{"custom_id": "keras#e83041fcd0c4c51c39a1f442e184fcdece019bc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 14 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 15041 | Contributors (this commit): 124 | Commits (past 90d): 28 | Contributors (cumulative): 144 | DMM Complexity: 1.0\n\nDIFF:\n@@ -631,9 +631,9 @@ class Model(Container):\n         \"\"\"\n         loss = loss or {}\n         self.optimizer = optimizers.get(optimizer)\n-        self.sample_weight_mode = sample_weight_mode\n         self.loss = loss\n         self.loss_weights = loss_weights\n+        self.sample_weight_mode = sample_weight_mode\n \n         # Prepare loss functions.\n         if isinstance(loss, dict):\n\n@@ -743,26 +743,47 @@ class Sequential(Model):\n                 metrics=None,\n                 sample_weight_mode=None,\n                 weighted_metrics=None,\n+                target_tensors=None,\n                 **kwargs):\n-        \"\"\"Configures the learning process.\n+        \"\"\"Configures the model for training.\n \n         # Arguments\n-            optimizer: str (name of optimizer) or optimizer object.\n+            optimizer: String (name of optimizer) or optimizer object.\n                 See [optimizers](/optimizers).\n-            loss: str (name of objective function) or objective function.\n+            loss: String (name of objective function) or objective function.\n                 See [losses](/losses).\n-            metrics: list of metrics to be evaluated by the model\n+                If the model has multiple outputs, you can use a different loss\n+                on each output by passing a dictionary or a list of losses.\n+                The loss value that will be minimized by the model\n+                will then be the sum of all individual losses.\n+            metrics: List of metrics to be evaluated by the model\n                 during training and testing.\n                 Typically you will use `metrics=['accuracy']`.\n-                See [metrics](/metrics).\n-            sample_weight_mode: if you need to do timestep-wise\n-                sample weighting (2D weights), set this to \"temporal\".\n-                \"None\" defaults to sample-wise weights (1D).\n-            weighted_metrics: list of metrics to be evaluated and weighted\n-                by sample_weight or class_weight during training and testing\n-            **kwargs: for Theano/CNTK backends, these are passed into\n-                K.function. When using the TensorFlow backend, these are\n-                passed into `tf.Session.run`.\n+                To specify different metrics for different outputs of a\n+                multi-output model, you could also pass a dictionary,\n+                such as `metrics={'output_a': 'accuracy'}`.\n+            sample_weight_mode: If you need to do timestep-wise\n+                sample weighting (2D weights), set this to `\"temporal\"`.\n+                `None` defaults to sample-wise weights (1D).\n+                If the model has multiple outputs, you can use a different\n+                `sample_weight_mode` on each output by passing a\n+                dictionary or a list of modes.\n+            weighted_metrics: List of metrics to be evaluated and weighted\n+                by sample_weight or class_weight during training and testing.\n+            target_tensors: By default, Keras will create placeholders for the\n+                model's target, which will be fed with the target data during\n+                training. If instead you would like to use your own\n+                target tensors (in turn, Keras will not expect external\n+                Numpy data for these targets at training time), you\n+                can specify them via the `target_tensors` argument. It can be\n+                a single tensor (for a single-output model), a list of tensors,\n+                or a dict mapping output names to target tensors.\n+            **kwargs: When using the Theano/CNTK backends, these arguments\n+                are passed into K.function. When using the TensorFlow backend,\n+                these arguments are passed into `tf.Session.run`.\n+\n+        # Raises\n+            ValueError: In case of invalid arguments for\n \n         # Example\n             ```python\n@@ -781,18 +802,19 @@ class Sequential(Model):\n                            metrics=metrics,\n                            sample_weight_mode=sample_weight_mode,\n                            weighted_metrics=weighted_metrics,\n+                           target_tensors=target_tensors,\n                            **kwargs)\n         self.optimizer = self.model.optimizer\n         self.loss = self.model.loss\n-        self.total_loss = self.model.total_loss\n-        self.loss_weights = self.model.loss_weights\n         self.metrics = self.model.metrics\n+        self.loss_weights = self.model.loss_weights\n+        self.sample_weight_mode = self.model.sample_weight_mode\n         self.weighted_metrics = self.model.weighted_metrics\n+        self.targets = self.model.targets\n         self.metrics_tensors = self.model.metrics_tensors\n         self.metrics_names = self.model.metrics_names\n-        self.sample_weight_mode = self.model.sample_weight_mode\n         self.sample_weights = self.model.sample_weights\n-        self.targets = self.model.targets\n+        self.total_loss = self.model.total_loss\n \n     def fit(self, x, y, batch_size=32, epochs=10, verbose=1, callbacks=None,\n             validation_split=0., validation_data=None, shuffle=True,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ca4f74ae57eab613f1a6018b873c26db18bf232", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 845 | Contributors (this commit): 22 | Commits (past 90d): 8 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -328,7 +328,15 @@ class Progbar(object):\n                 time_per_unit = 0\n             if self.target is not None and current <= self.target:\n                 eta = time_per_unit * (self.target - current)\n-                info = ' - ETA: %0.fs' % eta\n+\n+                if eta > 3600:\n+                    eta_format = '%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)\n+                elif eta > 60:\n+                    eta_format = '%d:%02d' % (eta // 60, eta % 60)\n+                else:\n+                    eta_format = '%ds' % eta\n+\n+                info = ' - ETA: %s' % eta_format\n             for k in self.unique_values:\n                 info += ' - %s:' % k\n                 if isinstance(self.sum_values[k], list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2d33d7f900d9ad4a0208e1a48635fbb7bd0c13c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2632 | Contributors (this commit): 42 | Commits (past 90d): 13 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -524,12 +524,10 @@ class ImageDataGenerator(object):\n             x = self.preprocessing_function(x)\n         if self.rescale:\n             x *= self.rescale\n-        # x is a single image, so it doesn't have image number at index 0\n-        img_channel_axis = self.channel_axis - 1\n         if self.samplewise_center:\n-            x -= np.mean(x, axis=img_channel_axis, keepdims=True)\n+            x -= np.mean(x, keepdims=True)\n         if self.samplewise_std_normalization:\n-            x /= (np.std(x, axis=img_channel_axis, keepdims=True) + 1e-7)\n+            x /= np.std(x, keepdims=True) + 1e-7\n \n         if self.featurewise_center:\n             if self.mean is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3fa689f1daca66e5ae98a43796b40be402e56531", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1069 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -547,20 +547,20 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n         use_multiprocessing: use multiprocessing if True, otherwise threading\n         wait_time: time to sleep in-between calls to `put()`\n         random_seed: Initial seed for workers,\n-            will be incremented by one for each workers.\n+            will be incremented by one for each worker.\n     \"\"\"\n \n     def __init__(self, generator,\n                  use_multiprocessing=False,\n                  wait_time=0.05,\n-                 random_seed=None):\n+                 seed=None):\n         self.wait_time = wait_time\n         self._generator = generator\n         self._use_multiprocessing = use_multiprocessing\n         self._threads = []\n         self._stop_event = None\n         self.queue = None\n-        self.random_seed = random_seed\n+        self.seed = seed\n \n     def start(self, workers=1, max_queue_size=10):\n         \"\"\"Kicks off threads which add data from the generator into the queue.\n@@ -597,11 +597,11 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n                 if self._use_multiprocessing:\n                     # Reset random seed else all children processes\n                     # share the same seed\n-                    np.random.seed(self.random_seed)\n+                    np.random.seed(self.seed)\n                     thread = multiprocessing.Process(target=data_generator_task)\n                     thread.daemon = True\n-                    if self.random_seed is not None:\n-                        self.random_seed += 1\n+                    if self.seed is not None:\n+                        self.seed += 1\n                 else:\n                     thread = threading.Thread(target=data_generator_task)\n                 self._threads.append(thread)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f148e40e1dd9b95c4cd2dd0d182b8729729a5c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 320 | Lines Deleted: 53 | Files Changed: 2 | Hunks: 27 | Methods Changed: 21 | Complexity Δ (Sum/Max): 30/18 | Churn Δ: 373 | Churn Cumulative: 9603 | Contributors (this commit): 48 | Commits (past 90d): 11 | Contributors (cumulative): 56 | DMM Complexity: 0.9241706161137441\n\nDIFF:\n@@ -1,7 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n import numpy as np\n-import functools\n import warnings\n \n from .. import backend as K\n@@ -200,7 +199,9 @@ class RNN(Layer):\n     # Arguments\n         cell: A RNN cell instance. A RNN cell is a class that has:\n             - a `call(input_at_t, states_at_t)` method, returning\n-                `(output_at_t, states_at_t_plus_1)`.\n+                `(output_at_t, states_at_t_plus_1)`. The call method of the\n+                cell can also take the optional argument `constants`, see\n+                section \"Note on passing external constants\" below.\n             - a `state_size` attribute. This can be a single integer\n                 (single state) in which case it is\n                 the size of the recurrent state\n@@ -292,6 +293,14 @@ class RNN(Layer):\n         `states` should be a numpy array or list of numpy arrays representing\n         the initial state of the RNN layer.\n \n+    # Note on passing external constants to RNNs\n+        You can pass \"external\" constants to the cell using the `constants`\n+        keyword argument of `RNN.__call__` (as well as `RNN.call`) method. This\n+        requires that the `cell.call` method accepts the same keyword argument\n+        `constants`. Such constants can be used to condition the cell\n+        transformation on additional static inputs (not changing over time),\n+        a.k.a. an attention mechanism.\n+\n     # Examples\n \n     ```python\n@@ -363,12 +372,10 @@ class RNN(Layer):\n \n         self.supports_masking = True\n         self.input_spec = [InputSpec(ndim=3)]\n-        if hasattr(self.cell.state_size, '__len__'):\n-            self.state_spec = [InputSpec(shape=(None, dim))\n-                               for dim in self.cell.state_size]\n-        else:\n-            self.state_spec = InputSpec(shape=(None, self.cell.state_size))\n+        self.state_spec = None\n         self._states = None\n+        self.constants_spec = None\n+        self._num_constants = None\n \n     @property\n     def states(self):\n@@ -415,6 +422,13 @@ class RNN(Layer):\n             return output_mask\n \n     def build(self, input_shape):\n+        # Note input_shape will be list of shapes of initial states and\n+        # constants if these are passed in __call__.\n+        if self._num_constants is not None:\n+            constants_shape = input_shape[-self._num_constants:]\n+        else:\n+            constants_shape = None\n+\n         if isinstance(input_shape, list):\n             input_shape = input_shape[0]\n \n@@ -422,13 +436,33 @@ class RNN(Layer):\n         input_dim = input_shape[-1]\n         self.input_spec[0] = InputSpec(shape=(batch_size, None, input_dim))\n \n-        if self.stateful:\n-            self.reset_states()\n-\n+        # allow cell (if layer) to build before we set or validate state_spec\n         if isinstance(self.cell, Layer):\n             step_input_shape = (input_shape[0],) + input_shape[2:]\n+            if constants_shape is not None:\n+                self.cell.build([step_input_shape] + constants_shape)\n+            else:\n                 self.cell.build(step_input_shape)\n \n+        # set or validate state_spec\n+        if hasattr(self.cell.state_size, '__len__'):\n+            state_size = list(self.cell.state_size)\n+        else:\n+            state_size = [self.cell.state_size]\n+\n+        if self.state_spec is not None:\n+            # initial_state was passed in call, check compatibility\n+            if not [spec.shape[-1] for spec in self.state_spec] == state_size:\n+                raise ValueError(\n+                    'an initial_state was passed that is not compatible with'\n+                    ' cell.state_size, state_spec: {}, cell.state_size:'\n+                    ' {}'.format(self.state_spec, self.cell.state_size))\n+        else:\n+            self.state_spec = [InputSpec(shape=(None, dim))\n+                               for dim in state_size]\n+        if self.stateful:\n+            self.reset_states()\n+\n     def get_initial_state(self, inputs):\n         # build an all-zero tensor of shape (samples, output_dim)\n         initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n@@ -440,62 +474,65 @@ class RNN(Layer):\n         else:\n             return [K.tile(initial_state, [1, self.cell.state_size])]\n \n-    def __call__(self, inputs, initial_state=None, **kwargs):\n-        # If there are multiple inputs, then\n-        # they should be the main input and `initial_state`\n-        # e.g. when loading model from file\n-        if isinstance(inputs, (list, tuple)) and len(inputs) > 1 and initial_state is None:\n-            initial_state = inputs[1:]\n-            inputs = inputs[0]\n+    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n+        inputs, initial_state, constants = self._standardize_args(\n+            inputs, initial_state, constants)\n \n-        # If `initial_state` is specified,\n-        # and if it a Keras tensor,\n-        # then add it to the inputs and temporarily\n-        # modify the input spec to include the state.\n-        if initial_state is None:\n+        if initial_state is None and constants is None:\n             return super(RNN, self).__call__(inputs, **kwargs)\n \n-        if not isinstance(initial_state, (list, tuple)):\n-            initial_state = [initial_state]\n+        # If any of `initial_state` or `constants` are specified and are Keras\n+        # tensors, then add them to the inputs and temporarily modify the\n+        # input_spec to include them.\n \n-        is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n-        for tensor in initial_state:\n+        additional_inputs = []\n+        additional_specs = []\n+        if initial_state is not None:\n+            kwargs['initial_state'] = initial_state\n+            additional_inputs += initial_state\n+            self.state_spec = [InputSpec(shape=K.int_shape(state))\n+                               for state in initial_state]\n+            additional_specs += self.state_spec\n+        if constants is not None:\n+            kwargs['constants'] = constants\n+            additional_inputs += constants\n+            self.constants_spec = [InputSpec(shape=K.int_shape(constant))\n+                                   for constant in constants]\n+            self._num_constants = len(constants)\n+            additional_specs += self.constants_spec\n+        # at this point additional_inputs cannot be empty\n+        is_keras_tensor = hasattr(additional_inputs[0], '_keras_history')\n+        for tensor in additional_inputs:\n             if hasattr(tensor, '_keras_history') != is_keras_tensor:\n-                raise ValueError('The initial state of an RNN layer cannot be'\n-                                 ' specified with a mix of Keras tensors and'\n-                                 ' non-Keras tensors')\n+                raise ValueError('The initial state or constants of an RNN'\n+                                 ' layer cannot be specified with a mix of'\n+                                 ' Keras tensors and non-Keras tensors')\n \n         if is_keras_tensor:\n-            # Compute the full input spec, including state\n-            input_spec = self.input_spec\n-            state_spec = self.state_spec\n-            if not isinstance(input_spec, list):\n-                input_spec = [input_spec]\n-            if not isinstance(state_spec, list):\n-                state_spec = [state_spec]\n-            self.input_spec = input_spec + state_spec\n-\n-            # Compute the full inputs, including state\n-            inputs = [inputs] + list(initial_state)\n-\n-            # Perform the call\n-            output = super(RNN, self).__call__(inputs, **kwargs)\n-\n-            # Restore original input spec\n-            self.input_spec = input_spec\n+            # Compute the full input spec, including state and constants\n+            full_input = [inputs] + additional_inputs\n+            full_input_spec = self.input_spec + additional_specs\n+            # Perform the call with temporarily replaced input_spec\n+            original_input_spec = self.input_spec\n+            self.input_spec = full_input_spec\n+            output = super(RNN, self).__call__(full_input, **kwargs)\n+            self.input_spec = original_input_spec\n             return output\n         else:\n-            kwargs['initial_state'] = initial_state\n             return super(RNN, self).__call__(inputs, **kwargs)\n \n-    def call(self, inputs, mask=None, training=None, initial_state=None):\n+    def call(self,\n+             inputs,\n+             mask=None,\n+             training=None,\n+             initial_state=None,\n+             constants=None):\n         # input shape: `(samples, time (padded with zeros), input_dim)`\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec and self.state_spec with complete input shapes.\n         if isinstance(inputs, list):\n-            initial_state = inputs[1:]\n             inputs = inputs[0]\n-        elif initial_state is not None:\n+        if initial_state is not None:\n             pass\n         elif self.stateful:\n             initial_state = self.states\n@@ -525,13 +562,27 @@ class RNN(Layer):\n                              'the time dimension by passing a `shape` '\n                              'or `batch_shape` argument to your Input layer.')\n \n+        kwargs = {}\n         if has_arg(self.cell.call, 'training'):\n-            step = functools.partial(self.cell.call, training=training)\n+            kwargs['training'] = training\n+\n+        if constants:\n+            if not has_arg(self.cell.call, 'constants'):\n+                raise ValueError('RNN cell does not support constants')\n+\n+            def step(inputs, states):\n+                constants = states[-self._num_constants:]\n+                states = states[:-self._num_constants]\n+                return self.cell.call(inputs, states, constants=constants,\n+                                      **kwargs)\n         else:\n-            step = self.cell.call\n+            def step(inputs, states):\n+                return self.cell.call(inputs, states, **kwargs)\n+\n         last_output, outputs, states = K.rnn(step,\n                                              inputs,\n                                              initial_state,\n+                                             constants=constants,\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n                                              unroll=self.unroll,\n@@ -560,6 +611,47 @@ class RNN(Layer):\n         else:\n             return output\n \n+    def _standardize_args(self, inputs, initial_state, constants):\n+        \"\"\"Brings the arguments of `__call__` that can contain input tensors to\n+        standard format.\n+\n+        When running a model loaded from file, the input tensors\n+        `initial_state` and `constants` can be passed to `RNN.__call__` as part\n+        of `inputs` instead of by the dedicated keyword arguments. This method\n+        makes sure the arguments are separated and that `initial_state` and\n+        `constants` are lists of tensors (or None).\n+\n+        # Arguments\n+            inputs: tensor or list/tuple of tensors\n+            initial_state: tensor or list of tensors or None\n+            constants: tensor or list of tensors or None\n+\n+        # Returns\n+            inputs: tensor\n+            initial_state: list of tensors or None\n+            constants: list of tensors or None\n+        \"\"\"\n+        if isinstance(inputs, list):\n+            assert initial_state is None and constants is None\n+            if self._num_constants is not None:\n+                constants = inputs[-self._num_constants:]\n+                inputs = inputs[:-self._num_constants]\n+            if len(inputs) > 1:\n+                initial_state = inputs[1:]\n+            inputs = inputs[0]\n+\n+        def to_list_or_none(x):\n+            if x is None or isinstance(x, list):\n+                return x\n+            if isinstance(x, tuple):\n+                return list(x)\n+            return [x]\n+\n+        initial_state = to_list_or_none(initial_state)\n+        constants = to_list_or_none(constants)\n+\n+        return inputs, initial_state, constants\n+\n     def reset_states(self, states=None):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n@@ -618,6 +710,9 @@ class RNN(Layer):\n                   'go_backwards': self.go_backwards,\n                   'stateful': self.stateful,\n                   'unroll': self.unroll}\n+        if self._num_constants is not None:\n+            config['num_constants'] = self._num_constants\n+\n         cell_config = self.cell.get_config()\n         config['cell'] = {'class_name': self.cell.__class__.__name__,\n                           'config': cell_config}\n@@ -629,7 +724,10 @@ class RNN(Layer):\n         from . import deserialize as deserialize_layer\n         cell = deserialize_layer(config.pop('cell'),\n                                  custom_objects=custom_objects)\n-        return cls(cell, **config)\n+        num_constants = config.pop('num_constants', None)\n+        layer = cls(cell, **config)\n+        layer._num_constants = num_constants\n+        return layer\n \n     @property\n     def trainable_weights(self):\n\n@@ -568,5 +568,174 @@ def test_batch_size_equal_one(layer_class):\n     model.train_on_batch(x, y)\n \n \n+def test_rnn_cell_with_constants_layer():\n+\n+    class RNNCellWithConstants(keras.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(RNNCellWithConstants, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            if not isinstance(input_shape, list):\n+                raise TypeError('expects constants shape')\n+            [input_shape, constant_shape] = input_shape\n+            # will (and should) raise if more than one constant passed\n+\n+            self.input_kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer='uniform',\n+                name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.constant_kernel = self.add_weight(\n+                shape=(constant_shape[-1], self.units),\n+                initializer='uniform',\n+                name='constant_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states, constants):\n+            [prev_output] = states\n+            [constant] = constants\n+            h_input = keras.backend.dot(inputs, self.input_kernel)\n+            h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\n+            h_const = keras.backend.dot(constant, self.constant_kernel)\n+            output = h_input + h_state + h_const\n+            return output, [output]\n+\n+        def get_config(self):\n+            config = {'units': self.units}\n+            base_config = super(RNNCellWithConstants, self).get_config()\n+            return dict(list(base_config.items()) + list(config.items()))\n+\n+    # Test basic case.\n+    x = keras.Input((None, 5))\n+    c = keras.Input((3,))\n+    cell = RNNCellWithConstants(32)\n+    layer = recurrent.RNN(cell)\n+    y = layer(x, constants=c)\n+    model = keras.models.Model([x, c], y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n+        np.zeros((6, 32))\n+    )\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    c_np = np.random.random((6, 3))\n+    y_np = model.predict([x_np, c_np])\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer(x, constants=c)\n+    model = keras.models.Model([x, c], y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict([x_np, c_np])\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # test flat list inputs\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer([x, c])\n+    model = keras.models.Model([x, c], y)\n+    model.set_weights(weights)\n+    y_np_3 = model.predict([x_np, c_np])\n+    assert_allclose(y_np, y_np_3, atol=1e-4)\n+\n+\n+def test_rnn_cell_with_constants_layer_passing_initial_state():\n+\n+    class RNNCellWithConstants(keras.layers.Layer):\n+\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(RNNCellWithConstants, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            if not isinstance(input_shape, list):\n+                raise TypeError('expects constants shape')\n+            [input_shape, constant_shape] = input_shape\n+            # will (and should) raise if more than one constant passed\n+\n+            self.input_kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer='uniform',\n+                name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.constant_kernel = self.add_weight(\n+                shape=(constant_shape[-1], self.units),\n+                initializer='uniform',\n+                name='constant_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states, constants):\n+            [prev_output] = states\n+            [constant] = constants\n+            h_input = keras.backend.dot(inputs, self.input_kernel)\n+            h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\n+            h_const = keras.backend.dot(constant, self.constant_kernel)\n+            output = h_input + h_state + h_const\n+            return output, [output]\n+\n+        def get_config(self):\n+            config = {'units': self.units}\n+            base_config = super(RNNCellWithConstants, self).get_config()\n+            return dict(list(base_config.items()) + list(config.items()))\n+\n+    # Test basic case.\n+    x = keras.Input((None, 5))\n+    c = keras.Input((3,))\n+    s = keras.Input((32,))\n+    cell = RNNCellWithConstants(32)\n+    layer = recurrent.RNN(cell)\n+    y = layer(x, initial_state=s, constants=c)\n+    model = keras.models.Model([x, s, c], y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 3))],\n+        np.zeros((6, 32))\n+    )\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    s_np = np.random.random((6, 32))\n+    c_np = np.random.random((6, 3))\n+    y_np = model.predict([x_np, s_np, c_np])\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer(x, initial_state=s, constants=c)\n+    model = keras.models.Model([x, s, c], y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict([x_np, s_np, c_np])\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # verify that state is used\n+    y_np_2_different_s = model.predict([x_np, s_np + 10., c_np])\n+    with pytest.raises(AssertionError):\n+        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n+\n+    # test flat list inputs\n+    with keras.utils.CustomObjectScope(custom_objects):\n+        layer = recurrent.RNN.from_config(config.copy())\n+    y = layer([x, s, c])\n+    model = keras.models.Model([x, s, c], y)\n+    model.set_weights(weights)\n+    y_np_3 = model.predict([x_np, s_np, c_np])\n+    assert_allclose(y_np, y_np_3, atol=1e-4)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34055f494f2fadbb4ca05d8a7aac0b1a723a1ab2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 531 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -40,7 +40,9 @@ class _CuDNNRNN(RNN):\n                                for dim in self.cell.state_size]\n         else:\n             self.state_spec = InputSpec(shape=(None, self.cell.state_size))\n+        self.constants_spec = None\n         self._states = None\n+        self._num_constants = None\n \n     def _canonical_to_params(self, weights, biases):\n         import tensorflow as tf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7144aeb796d7e1bc6523bca8688dd3160d54e810", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 157 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.75\n\nDIFF:\n@@ -10,6 +10,11 @@ def _get_available_devices():\n     return [x.name for x in local_device_protos]\n \n \n+def _normalize_device_name(name):\n+    name = name.lower().replace('device:', '')\n+    return name\n+\n+\n def multi_gpu_model(model, gpus):\n     \"\"\"Replicates a model on different GPUs.\n \n@@ -89,6 +94,7 @@ def multi_gpu_model(model, gpus):\n \n     target_devices = ['/cpu:0'] + ['/gpu:%d' % i for i in range(gpus)]\n     available_devices = _get_available_devices()\n+    available_devices = [_normalize_device_name(name) for name in available_devices]\n     for device in target_devices:\n         if device not in available_devices:\n             raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9fecc510372311e0bcf89077e12669016b93782a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 9288 | Contributors (this commit): 46 | Commits (past 90d): 11 | Contributors (cumulative): 47 | DMM Complexity: 0.5\n\nDIFF:\n@@ -36,10 +36,11 @@ class _CuDNNRNN(RNN):\n         self.supports_masking = False\n         self.input_spec = [InputSpec(ndim=3)]\n         if hasattr(self.cell.state_size, '__len__'):\n-            self.state_spec = [InputSpec(shape=(None, dim))\n-                               for dim in self.cell.state_size]\n+            state_size = self.cell.state_size\n         else:\n-            self.state_spec = InputSpec(shape=(None, self.cell.state_size))\n+            state_size = [self.cell.state_size]\n+        self.state_spec = [InputSpec(shape=(None, dim))\n+                           for dim in state_size]\n         self.constants_spec = None\n         self._states = None\n         self._num_constants = None\n\n@@ -454,8 +454,9 @@ class RNN(Layer):\n             # initial_state was passed in call, check compatibility\n             if not [spec.shape[-1] for spec in self.state_spec] == state_size:\n                 raise ValueError(\n-                    'an initial_state was passed that is not compatible with'\n-                    ' cell.state_size, state_spec: {}, cell.state_size:'\n+                    'An initial_state was passed that is not compatible with '\n+                    '`cell.state_size`. Received `state_spec`={}; '\n+                    'However `cell.state_size` is '\n                     '{}'.format(self.state_spec, self.cell.state_size))\n         else:\n             self.state_spec = [InputSpec(shape=(None, dim))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e48bc45d1b4f760f77e5f01f128c05011a6459fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1339 | Contributors (this commit): 19 | Commits (past 90d): 11 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -284,6 +284,7 @@ PAGES = [\n         'functions': [utils.to_categorical,\n                       utils.normalize,\n                       utils.get_file,\n+                      utils.print_summary,\n                       utils.plot_model,\n                       utils.multi_gpu_model],\n         'classes': [utils.CustomObjectScope,\n@@ -360,7 +361,7 @@ def get_class_signature(cls):\n     try:\n         class_signature = get_function_signature(cls.__init__)\n         class_signature = class_signature.replace('__init__', cls.__name__)\n-    except TypeError:\n+    except (TypeError, AttributeError):\n         # in case the class inherits from object and does not\n         # define __init__\n         class_signature = cls.__module__ + '.' + cls.__name__ + '()'\n@@ -406,8 +407,8 @@ def code_snippet(snippet):\n \n \n def process_class_docstring(docstring):\n-    docstring = re.sub(r'\\n    # (.*)\\n',\n-                       r'\\n    __\\1__\\n\\n',\n+    docstring = re.sub(r'\\n(\\s+)# (.*)\\n',\n+                       r'\\n\\1__\\2__\\n\\n',\n                        docstring)\n     docstring = re.sub(r'    ([^\\s\\\\\\(]+):(.*)\\n',\n                        r'    - __\\1__:\\2\\n',\n@@ -420,8 +421,8 @@ def process_class_docstring(docstring):\n \n \n def process_function_docstring(docstring):\n-    docstring = re.sub(r'\\n    # (.*)\\n',\n-                       r'\\n    __\\1__\\n\\n',\n+    docstring = re.sub(r'\\n(\\s+)# (.*)\\n',\n+                       r'\\n\\1__\\2__\\n\\n',\n                        docstring)\n     docstring = re.sub(r'    ([^\\s\\\\\\(]+):(.*)\\n',\n                        r'    - __\\1__:\\2\\n',\n\n@@ -18,6 +18,7 @@ from .generic_utils import serialize_keras_object\n from .generic_utils import deserialize_keras_object\n from .generic_utils import Progbar\n from .layer_utils import convert_all_kernels_in_model\n+from .layer_utils import print_summary\n from .vis_utils import plot_model\n from .np_utils import to_categorical\n from .np_utils import normalize\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cab77c8f23bf81eaa06aeeeb28a4da3b716f7bd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 85 | Churn Cumulative: 16029 | Contributors (this commit): 120 | Commits (past 90d): 44 | Contributors (cumulative): 156 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -16,6 +16,7 @@ from .. import backend as K\n from .. import initializers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n from ..utils.layer_utils import print_summary as print_layer_summary\n+from ..utils.layer_utils import count_params\n from ..utils.generic_utils import has_arg\n from ..utils import conv_utils\n from ..legacy import interfaces\n@@ -1269,7 +1270,7 @@ class Layer(object):\n                                    self.name + ', but the layer isn\\'t built. '\n                                    'You can build it manually via: `' +\n                                    self.name + '.build(batch_input_shape)`.')\n-        return sum([K.count_params(p) for p in self.weights])\n+        return count_params(self.weights)\n \n \n class InputLayer(Layer):\n\n@@ -22,6 +22,7 @@ from .. import optimizers\n from .. import losses\n from .. import metrics as metrics_module\n from ..utils.generic_utils import Progbar\n+from ..utils.layer_utils import count_params\n from .. import callbacks as cbks\n from ..legacy import interfaces\n \n@@ -954,9 +955,29 @@ class Model(Container):\n         trainable_weights = self.trainable_weights\n         self._collected_trainable_weights = trainable_weights\n \n+    def _check_trainable_weights_consistency(self):\n+        \"\"\"Check trainable weights count consistency.\n+\n+        This will raise a warning if `trainable_weights` and\n+        `_collected_trainable_weights` are consistent (i.e. have the same\n+        number of parameters).\n+        Inconsistency will typically arise when one modifies `model.trainable`\n+        without calling `model.compile` again.\n+        \"\"\"\n+        if not hasattr(self, '_collected_trainable_weights'):\n+            return\n+\n+        if (count_params(self.trainable_weights) !=\n+                count_params(self._collected_trainable_weights)):\n+            warnings.warn(UserWarning(\n+                'Discrepancy between trainable weights and collected trainable'\n+                ' weights, did you set `model.trainable` without calling'\n+                ' `model.compile` after ?'))\n+\n     def _make_train_function(self):\n         if not hasattr(self, 'train_function'):\n             raise RuntimeError('You must compile your model before using it.')\n+        self._check_trainable_weights_consistency()\n         if self.train_function is None:\n             inputs = self._feed_inputs + self._feed_targets + self._feed_sample_weights\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n\n@@ -5,6 +5,18 @@ from .. import backend as K\n import numpy as np\n \n \n+def count_params(weights):\n+    \"\"\"Count the total number of scalars composing the weights.\n+\n+    # Arguments\n+        weights: An iterable containing the weights on which to compute params\n+\n+    # Returns\n+        The total number of scalars composing the weights\n+    \"\"\"\n+    return int(np.sum([K.count_params(p) for p in set(weights)]))\n+\n+\n def print_summary(model, line_length=None, positions=None, print_fn=print):\n     \"\"\"Prints a summary of a model.\n \n@@ -134,8 +146,12 @@ def print_summary(model, line_length=None, positions=None, print_fn=print):\n         else:\n             print_fn('_' * line_length)\n \n-    trainable_count = int(\n-        np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n+    model._check_trainable_weights_consistency()\n+    if hasattr(model, '_collected_trainable_weights'):\n+        trainable_count = count_params(model._collected_trainable_weights)\n+    else:\n+        trainable_count = count_params(model.trainable_weights)\n+\n     non_trainable_count = int(\n         np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n \n\n@@ -937,5 +937,46 @@ def test_model_custom_target_tensors():\n                              [output_a_np, output_b_np])\n \n \n+@pytest.mark.skipif(sys.version_info < (3,), reason='Cannot catch warnings in python 2')\n+@keras_test\n+def test_trainable_weights_count_consistency():\n+    \"\"\"Tests the trainable weights consistency check of Model.\n+\n+    This verifies that a warning is shown if model.trainable is modified\n+    and the model is summarized/run without a new call to .compile()\n+\n+    Reproduce issue #8121\n+    \"\"\"\n+    a = Input(shape=(3,), name='input_a')\n+    model1 = Model(inputs=a, outputs=Dense(1)(a))\n+\n+    model1.trainable = False\n+    b = Input(shape=(3,), name='input_b')\n+    y = model1(b)\n+    model2 = Model(inputs=b, outputs=Dense(1)(y))\n+\n+    model2.compile(optimizer='adam', loss='mse')\n+\n+    model1.trainable = True\n+\n+    # Should warn on .summary()\n+    with pytest.warns(UserWarning) as w:\n+        model2.summary()\n+    warning_raised = any(['Discrepancy' in str(w_.message) for w_ in w])\n+    assert warning_raised, 'No warning raised when trainable is modified without .compile.'\n+\n+    # And on .fit()\n+    with pytest.warns(UserWarning) as w:\n+        model2.fit(x=np.zeros((5, 3)), y=np.zeros((5, 1)))\n+    warning_raised = any(['Discrepancy' in str(w_.message) for w_ in w])\n+    assert warning_raised, 'No warning raised when trainable is modified without .compile.'\n+\n+    # And shouldn't warn if we recompile\n+    model2.compile(optimizer='adam', loss='mse')\n+    with pytest.warns(None) as w:\n+        model2.summary()\n+    assert len(w) == 0, \"Warning raised even when .compile() is called after modifying .trainable\"\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#970488e8a9376753cefe5e9b147558e35474e714", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 65 | Files Changed: 1 | Hunks: 26 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 134 | Churn Cumulative: 699 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,47 +48,49 @@ Results\n - Running on GPU GeForce GTX Titan X Maxwell\n - Performance Comparisons - validation loss values during first 3 epochs:\n \n+Teacher model ...\n+(0) teacher_model:             0.0537   0.0354   0.0356\n+\n Experiment of Net2WiderNet ...\n-(1) teacher_model:             0.0537   0.0354   0.0356\n-(2) wider_random_pad:          0.0320   0.0317   0.0289\n-(3) wider_net2wider:           0.0271   0.0274   0.0270\n+(1) wider_random_pad:          0.0320   0.0317   0.0289\n+(2) wider_net2wider:           0.0271   0.0274   0.0270\n \n Experiment of Net2DeeperNet ...\n-(4) teacher_model:             0.0522   0.0386   0.0358\n-(5) deeper_random_init:        0.0682   0.0506   0.0468\n+(3) deeper_random_init:        0.0682   0.0506   0.0468\n (4) deeper_net2deeper:         0.0292   0.0294   0.0286\n '''\n \n from __future__ import print_function\n import numpy as np\n import keras\n+from keras import backend as K\n from keras.models import Sequential\n from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n from keras.optimizers import SGD\n from keras.datasets import mnist\n \n-if keras.backend.image_data_format() == 'channels_first':\n+if K.image_data_format() == 'channels_first':\n     input_shape = (1, 28, 28)  # image shape\n else:\n     input_shape = (28, 28, 1)  # image shape\n num_classes = 10  # number of classes\n+epochs = 3\n \n \n # load and pre-process data\n def preprocess_input(x):\n-    return x.reshape((-1, ) + input_shape) / 255.\n+    return x.astype('float32').reshape((-1,) + input_shape) / 255\n \n \n def preprocess_output(y):\n     return keras.utils.to_categorical(y)\n \n-(train_x, train_y), (validation_x, validation_y) = mnist.load_data()\n-train_x, validation_x = map(preprocess_input, [train_x, validation_x])\n-train_y, validation_y = map(preprocess_output, [train_y, validation_y])\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n+x_train, x_test = map(preprocess_input, [x_train, x_test])\n+y_train, y_test = map(preprocess_output, [y_train, y_test])\n print('Loading MNIST data...')\n-print('train_x shape:', train_x.shape, 'train_y shape:', train_y.shape)\n-print('validation_x shape:', validation_x.shape,\n-      'validation_y shape', validation_y.shape)\n+print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)\n+print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape)\n \n \n # knowledge transfer algorithms\n@@ -218,8 +220,11 @@ def copy_weights(teacher_model, student_model, layer_names):\n \n \n # methods to construct teacher_model and student_models\n-def make_teacher_model(train_data, validation_data, epochs=3):\n-    '''Train a simple CNN as teacher model.\n+def make_teacher_model(x_train, y_train,\n+                       x_test, y_test,\n+                       epochs):\n+    '''Train and benchmark performance of a simple CNN.\n+    (0) Teacher model\n     '''\n     model = Sequential()\n     model.add(Conv2D(64, 3, input_shape=input_shape,\n@@ -234,15 +239,16 @@ def make_teacher_model(train_data, validation_data, epochs=3):\n                   optimizer=SGD(lr=0.01, momentum=0.9),\n                   metrics=['accuracy'])\n \n-    train_x, train_y = train_data\n-    history = model.fit(train_x, train_y,\n+    model.fit(x_train, y_train,\n               epochs=epochs,\n-                        validation_data=validation_data)\n-    return model, history\n+              validation_data=(x_test, y_test))\n+    return model\n \n \n-def make_wider_student_model(teacher_model, train_data,\n-                             validation_data, init, epochs=3):\n+def make_wider_student_model(teacher_model,\n+                             x_train, y_train,\n+                             x_test, y_test,\n+                             init, epochs):\n     '''Train a wider student model based on teacher_model,\n        with either 'random-pad' (baseline) or 'net2wider'\n     '''\n@@ -284,15 +290,15 @@ def make_wider_student_model(teacher_model, train_data,\n                   optimizer=SGD(lr=0.001, momentum=0.9),\n                   metrics=['accuracy'])\n \n-    train_x, train_y = train_data\n-    history = model.fit(train_x, train_y,\n+    model.fit(x_train, y_train,\n               epochs=epochs,\n-                        validation_data=validation_data)\n-    return model, history\n+              validation_data=(x_test, y_test))\n \n \n-def make_deeper_student_model(teacher_model, train_data,\n-                              validation_data, init, epochs=3):\n+def make_deeper_student_model(teacher_model,\n+                              x_train, y_train,\n+                              x_test, y_test,\n+                              init, epochs):\n     '''Train a deeper student model based on teacher_model,\n        with either 'random-init' (baseline) or 'net2deeper'\n     '''\n@@ -333,60 +339,58 @@ def make_deeper_student_model(teacher_model, train_data,\n                   optimizer=SGD(lr=0.001, momentum=0.9),\n                   metrics=['accuracy'])\n \n-    train_x, train_y = train_data\n-    history = model.fit(train_x, train_y,\n+    model.fit(x_train, y_train,\n               epochs=epochs,\n-                        validation_data=validation_data)\n-    return model, history\n+              validation_data=(x_test, y_test))\n \n \n # experiments setup\n def net2wider_experiment():\n     '''Benchmark performances of\n-    (1) a teacher model,\n-    (2) a wider student model with `random_pad` initializer\n-    (3) a wider student model with `Net2WiderNet` initializer\n+    (1) a wider student model with `random_pad` initializer\n+    (2) a wider student model with `Net2WiderNet` initializer\n     '''\n-    train_data = (train_x, train_y)\n-    validation_data = (validation_x, validation_y)\n     print('\\nExperiment of Net2WiderNet ...')\n-    print('\\nbuilding teacher model ...')\n-    teacher_model, _ = make_teacher_model(train_data,\n-                                          validation_data,\n-                                          epochs=3)\n \n-    print('\\nbuilding wider student model by random padding ...')\n-    make_wider_student_model(teacher_model, train_data,\n-                             validation_data, 'random-pad',\n-                             epochs=3)\n-    print('\\nbuilding wider student model by net2wider ...')\n-    make_wider_student_model(teacher_model, train_data,\n-                             validation_data, 'net2wider',\n-                             epochs=3)\n+    print('\\n(1) building wider student model by random padding ...')\n+    make_wider_student_model(teacher_model,\n+                             x_train, y_train,\n+                             x_test, y_test,\n+                             init='random-pad',\n+                             epochs=epochs)\n+    print('\\n(2) building wider student model by net2wider ...')\n+    make_wider_student_model(teacher_model,\n+                             x_train, y_train,\n+                             x_test, y_test,\n+                             init='net2wider',\n+                             epochs=epochs)\n \n \n def net2deeper_experiment():\n     '''Benchmark performances of\n-    (1) a teacher model,\n-    (2) a deeper student model with `random_init` initializer\n-    (3) a deeper student model with `Net2DeeperNet` initializer\n+    (3) a deeper student model with `random_init` initializer\n+    (4) a deeper student model with `Net2DeeperNet` initializer\n     '''\n-    train_data = (train_x, train_y)\n-    validation_data = (validation_x, validation_y)\n     print('\\nExperiment of Net2DeeperNet ...')\n-    print('\\nbuilding teacher model ...')\n-    teacher_model, _ = make_teacher_model(train_data,\n-                                          validation_data,\n-                                          epochs=3)\n \n-    print('\\nbuilding deeper student model by random init ...')\n-    make_deeper_student_model(teacher_model, train_data,\n-                              validation_data, 'random-init',\n-                              epochs=3)\n-    print('\\nbuilding deeper student model by net2deeper ...')\n-    make_deeper_student_model(teacher_model, train_data,\n-                              validation_data, 'net2deeper',\n-                              epochs=3)\n+    print('\\n(3) building deeper student model by random init ...')\n+    make_deeper_student_model(teacher_model,\n+                              x_train, y_train,\n+                              x_test, y_test,\n+                              init='random-init',\n+                              epochs=epochs)\n+    print('\\n(4) building deeper student model by net2deeper ...')\n+    make_deeper_student_model(teacher_model,\n+                              x_train, y_train,\n+                              x_test, y_test,\n+                              init='net2deeper',\n+                              epochs=epochs)\n+\n+\n+print('\\n(0) building teacher model ...')\n+teacher_model = make_teacher_model(x_train, y_train,\n+                                   x_test, y_test,\n+                                   epochs=epochs)\n \n # run the experiments\n net2wider_experiment()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
