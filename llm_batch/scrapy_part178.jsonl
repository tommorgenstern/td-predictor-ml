{"custom_id": "scrapy#85b52b06542ee5905c174763de874bc9c376dfa5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 74 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -2,6 +2,7 @@ import os\n from six.moves import cPickle as pickle\n \n from scrapy import signals\n+from scrapy.utils.job import job_dir\n \n class SpiderState(object):\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n@@ -11,7 +12,7 @@ class SpiderState(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        obj = cls(crawler.settings.get('JOBDIR'))\n+        obj = cls(job_dir(crawler.settings))\n         crawler.signals.connect(obj.spider_closed, signal=signals.spider_closed)\n         crawler.signals.connect(obj.spider_opened, signal=signals.spider_opened)\n         return obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#86f61a990621797a6a9fe8cb67c2a88970b910b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 539 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -193,6 +193,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         # TODO: how do we fix this to return the actual argument names?\n         self.assertEqual(get_func_args(unicode.split), [])\n         self.assertEqual(get_func_args(\" \".join), [])\n+        self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -147,6 +147,8 @@ def get_func_args(func, stripself=False):\n     elif hasattr(func, '__call__'):\n         if inspect.isroutine(func):\n             return []\n+        elif getattr(func, '__name__', None) == '__call__':\n+            return []\n         else:\n             return get_func_args(func.__call__, True)\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af0635e2295b32553f2a3dc58201f8ffef1b9a03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 123 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -70,7 +70,7 @@ class DefaultsTest(ManagerTestCase):\n         req = Request('http://example.com')\n         body = '<p>You are being redirected</p>'\n         resp = Response(req.url, status=302, body=body, headers={\n-            'Content-Length': len(body),\n+            'Content-Length': str(len(body)),\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n             'Location': 'http://example.com/login',\n@@ -85,7 +85,7 @@ class DefaultsTest(ManagerTestCase):\n         req = Request('http://example.com')\n         body = '<p>You are being redirected</p>'\n         resp = Response(req.url, status=200, body=body, headers={\n-            'Content-Length': len(body),\n+            'Content-Length': str(len(body)),\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n             'Location': 'http://example.com/login',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebf76867bda7a45940322e5aa2e3b2bf3404164c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 205 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,9 +18,9 @@ About HTTP errors to consider:\n   indicate server overload, which would be something we want to retry\n \"\"\"\n \n-from twisted.internet.defer import TimeoutError as UserTimeoutError\n-from twisted.internet.error import TimeoutError as ServerTimeoutError, \\\n-        DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, \\\n+from twisted.internet import defer\n+from twisted.internet.error import TimeoutError, DNSLookupError, \\\n+        ConnectionRefusedError, ConnectionDone, ConnectError, \\\n         ConnectionLost, TCPTimedOutError\n \n from scrapy import log\n@@ -33,7 +33,7 @@ class RetryMiddleware(object):\n \n     # IOError is raised by the HttpCompression middleware when trying to\n     # decompress an empty response\n-    EXCEPTIONS_TO_RETRY = (ServerTimeoutError, UserTimeoutError, DNSLookupError,\n+    EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n                            ConnectionLost, TCPTimedOutError, ResponseFailed,\n                            IOError)\n\n@@ -1,7 +1,8 @@\n import unittest\n-from twisted.internet.error import TimeoutError as ServerTimeoutError, \\\n-    DNSLookupError, ConnectionRefusedError, ConnectionDone, ConnectError, \\\n-    ConnectionLost\n+from twisted.internet import defer\n+from twisted.internet.error import TimeoutError, DNSLookupError, \\\n+        ConnectionRefusedError, ConnectionDone, ConnectError, \\\n+        ConnectionLost, TCPTimedOutError\n \n from scrapy import optional_features\n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n@@ -41,7 +42,6 @@ class RetryTest(unittest.TestCase):\n \n     def test_dont_retry_exc(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n \n         r = self.mw.process_exception(req, DNSLookupError(), self.spider)\n         assert r is None\n@@ -64,9 +64,9 @@ class RetryTest(unittest.TestCase):\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_twistederrors(self):\n-        exceptions = [ServerTimeoutError, DNSLookupError,\n-                      ConnectionRefusedError, ConnectionDone, ConnectError,\n-                      ConnectionLost]\n+        exceptions = [defer.TimeoutError, TCPTimedOutError, TimeoutError,\n+                DNSLookupError, ConnectionRefusedError, ConnectionDone,\n+                ConnectError, ConnectionLost]\n         if 'http11' in optional_features:\n             exceptions.append(ResponseFailed)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbc9b37365bf572777ece68dfe48859bda3f825d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 43 | Churn Cumulative: 400 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,8 @@ See documentation in docs/topics/item.rst\n \"\"\"\n \n from pprint import pformat\n-from UserDict import DictMixin\n+from collections import MutableMapping\n+from abc import ABCMeta\n import six\n \n from scrapy.utils.trackref import object_ref\n@@ -20,7 +21,7 @@ class Field(dict):\n     \"\"\"Container of field metadata\"\"\"\n \n \n-class ItemMeta(type):\n+class ItemMeta(ABCMeta):\n \n     def __new__(mcs, class_name, bases, attrs):\n         fields = {}\n@@ -37,7 +38,7 @@ class ItemMeta(type):\n         return cls\n \n \n-class DictItem(DictMixin, BaseItem):\n+class DictItem(MutableMapping, BaseItem):\n \n     fields = {}\n \n@@ -71,6 +72,19 @@ class DictItem(DictMixin, BaseItem):\n                 (name, value))\n         super(DictItem, self).__setattr__(name, value)\n \n+    def __len__(self):\n+        return len(self._values)\n+\n+    def __iter__(self):\n+        for i in getattr(self, \"_values\", ()):\n+            yield i\n+\n+    def __hash__(self):\n+        if hasattr(self, \"_values\"):\n+            return hash(frozenset(self._values.items()))\n+        else:\n+            return 1\n+\n     def keys(self):\n         return self._values.keys()\n \n@@ -81,6 +95,5 @@ class DictItem(DictMixin, BaseItem):\n         return self.__class__(self)\n \n \n-class Item(DictItem):\n-\n-    __metaclass__ = ItemMeta\n+class Item(six.with_metaclass(ItemMeta, DictItem)):\n+    pass\n\n@@ -1,6 +1,7 @@\n import unittest\n \n from scrapy.item import Item, Field\n+import six\n \n \n class ItemTest(unittest.TestCase):\n@@ -52,8 +53,13 @@ class ItemTest(unittest.TestCase):\n         i['name'] = u'John Doe'\n         i['number'] = 123\n         itemrepr = repr(i)\n+\n+        if six.PY2:\n             self.assertEqual(itemrepr,\n                              \"{'name': u'John Doe', 'number': 123}\")\n+        else:\n+            self.assertEqual(itemrepr,\n+                             \"{'name': 'John Doe', 'number': 123}\")\n \n         i2 = eval(itemrepr)\n         self.assertEqual(i2['name'], 'John Doe')\n@@ -106,13 +112,13 @@ class ItemTest(unittest.TestCase):\n \n         i = TestItem()\n         i['name'] = u'John'\n-        self.assertEqual(i.keys(), ['name'])\n-        self.assertEqual(i.values(), ['John'])\n+        self.assertEqual(list(i.keys()), ['name'])\n+        self.assertEqual(list(i.values()), ['John'])\n \n         i['keys'] = u'Keys'\n         i['values'] = u'Values'\n-        self.assertSortedEqual(i.keys(), ['keys', 'values', 'name'])\n-        self.assertSortedEqual(i.values(), [u'Keys', u'Values', u'John'])\n+        self.assertSortedEqual(list(i.keys()), ['keys', 'values', 'name'])\n+        self.assertSortedEqual(list(i.values()), [u'Keys', u'Values', u'John'])\n \n     def test_metaclass_inheritance(self):\n         class BaseItem(Item):\n@@ -125,8 +131,8 @@ class ItemTest(unittest.TestCase):\n \n         i = TestItem()\n         i['keys'] = 3\n-        self.assertEqual(i.keys(), ['keys'])\n-        self.assertEqual(i.values(), [3])\n+        self.assertEqual(list(i.keys()), ['keys'])\n+        self.assertEqual(list(i.values()), [3])\n \n     def test_to_dict(self):\n         class TestItem(Item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d215a1b34b16e758e3a72a3181014f3c4ff00b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 12 | Churn Cumulative: 203 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -76,14 +76,9 @@ class DictItem(MutableMapping, BaseItem):\n         return len(self._values)\n \n     def __iter__(self):\n-        for i in getattr(self, \"_values\", ()):\n-            yield i\n+        return iter(self._values)\n \n-    def __hash__(self):\n-        if hasattr(self, \"_values\"):\n-            return hash(frozenset(self._values.items()))\n-        else:\n-            return 1\n+    __hash__ = BaseItem.__hash__\n \n     def keys(self):\n         return self._values.keys()\n@@ -95,5 +90,6 @@ class DictItem(MutableMapping, BaseItem):\n         return self.__class__(self)\n \n \n-class Item(six.with_metaclass(ItemMeta, DictItem)):\n+@six.add_metaclass(ItemMeta)\n+class Item(DictItem):\n     pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#242c08553f5bed2aedf2ff962fc83f68e13f2259", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 53 | Files Changed: 108 | Hunks: 38 | Methods Changed: 14 | Complexity Δ (Sum/Max): 437/57 | Churn Δ: 106 | Churn Cumulative: 551 | Contributors (this commit): 12 | Commits (past 90d): 110 | Contributors (cumulative): 121 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ from twisted.python import log\n \n from scrapy import optional_features\n \n-collect_ignore = [\"stats.py\"]\n+collect_ignore = [\"scrapy/stats.py\"]\n if 'django' not in optional_features:\n     collect_ignore.append(\"tests/test_djangoitem/models.py\")\n \n\n@@ -1,6 +1,6 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.tests.spiders import FollowAllSpider\n-from scrapy.tests.mockserver import MockServer\n+from tests.spiders import FollowAllSpider\n+from tests.mockserver import MockServer\n \n class Command(ScrapyCommand):\n \n\n@@ -1,5 +0,0 @@\n-EXTENSIONS = [\n-    'scrapy.tests.test_cmdline.extensions.TestExtension'\n-]\n-\n-TEST1 = 'default'\n\n@@ -50,7 +50,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n     The url passed can be a str or unicode, while the url returned is always a\n     str.\n \n-    For examples see the tests in scrapy.tests.test_utils_url\n+    For examples see the tests in tests/test_utils_url.py\n     \"\"\"\n \n     scheme, netloc, path, params, query, fragment = parse_url(url)\n\n@@ -1,5 +1,5 @@\n \"\"\"\n-scrapy.tests: this package contains all Scrapy unittests\n+tests: this package contains all Scrapy unittests\n \n To run all Scrapy unittests go to Scrapy main dir and type: \n \n\n\n\n@@ -179,7 +179,7 @@ class MockServer():\n \n     def __enter__(self):\n         from scrapy.utils.test import get_testenv\n-        self.proc = Popen([sys.executable, '-u', '-m', 'scrapy.tests.mockserver'],\n+        self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver'],\n                           stdout=PIPE, env=get_testenv())\n         self.proc.stdout.readline()\n \n\n\n@@ -1,8 +1,8 @@\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import docrawl\n-from scrapy.tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider\n-from scrapy.tests.mockserver import MockServer\n+from tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider\n+from tests.mockserver import MockServer\n \n \n class TestCloseSpider(TestCase):\n\n@@ -8,7 +8,7 @@ class CmdlineTest(unittest.TestCase):\n \n     def setUp(self):\n         self.env = get_testenv()\n-        self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'\n+        self.env['SCRAPY_SETTINGS_MODULE'] = 'tests.test_cmdline.settings'\n \n     def _execute(self, *new_args, **kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n\n\n@@ -0,0 +1,5 @@\n+EXTENSIONS = [\n+    'tests.test_cmdline.extensions.TestExtension'\n+]\n+\n+TEST1 = 'default'\n\n\n\n\n\n\n\n\n@@ -6,7 +6,7 @@ from scrapy.link import Link\n from scrapy.contrib.linkextractors.htmlparser import HtmlParserLinkExtractor\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n from scrapy.contrib.linkextractors.lxmlhtml import LxmlLinkExtractor\n-from scrapy.tests import get_testdata\n+from tests import get_testdata\n \n \n class LinkExtractorTestCase(unittest.TestCase):\n\n\n\n@@ -4,9 +4,9 @@ import mock\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import docrawl, get_testlog\n-from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n+from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n-from scrapy.tests.mockserver import MockServer\n+from tests.mockserver import MockServer\n from scrapy.http import Request\n \n \n\n\n@@ -4,7 +4,7 @@ from twisted.trial import unittest\n from scrapy.contrib.djangoitem import DjangoItem, Field\n from scrapy import optional_features\n \n-os.environ['DJANGO_SETTINGS_MODULE'] = 'scrapy.tests.test_djangoitem.settings'\n+os.environ['DJANGO_SETTINGS_MODULE'] = 'tests.test_djangoitem.settings'\n \n if 'django' in optional_features:\n     from .models import Person, IdentifiedPerson\n\n\n\n@@ -46,13 +46,13 @@ class OffDH(object):\n class LoadTestCase(unittest.TestCase):\n \n     def test_enabled_handler(self):\n-        handlers = {'scheme': 'scrapy.tests.test_downloader_handlers.DummyDH'}\n+        handlers = {'scheme': 'tests.test_downloader_handlers.DummyDH'}\n         dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n         self.assertIn('scheme', dh._handlers)\n         self.assertNotIn('scheme', dh._notconfigured)\n \n     def test_not_configured_handler(self):\n-        handlers = {'scheme': 'scrapy.tests.test_downloader_handlers.OffDH'}\n+        handlers = {'scheme': 'tests.test_downloader_handlers.OffDH'}\n         dh = DownloadHandlers(get_crawler({'DOWNLOAD_HANDLERS': handlers}))\n         self.assertNotIn('scheme', dh._handlers)\n         self.assertIn('scheme', dh._notconfigured)\n\n\n\n\n@@ -2,7 +2,7 @@ from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n from scrapy.spider import Spider\n-from scrapy.tests import get_testdata\n+from tests import get_testdata\n from scrapy.utils.test import assert_samelines\n \n \n\n\n\n\n@@ -120,7 +120,7 @@ class DbmStorageTest(DefaultStorageTest):\n \n class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n \n-    dbm_module = 'scrapy.tests.mocks.dummydbm'\n+    dbm_module = 'tests.mocks.dummydbm'\n \n     def _get_settings(self, **new_settings):\n         new_settings.setdefault('HTTPCACHE_DBM_MODULE', self.dbm_module)\n\n@@ -6,7 +6,7 @@ from gzip import GzipFile\n from scrapy.spider import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware\n-from scrapy.tests import tests_datadir\n+from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n \n\n\n\n\n\n\n\n\n@@ -21,7 +21,7 @@ from twisted.trial import unittest\n from scrapy import signals\n from scrapy.utils.test import get_crawler\n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.tests import tests_datadir\n+from tests import tests_datadir\n from scrapy.spider import Spider\n from scrapy.item import Item, Field\n from scrapy.contrib.linkextractors import LinkExtractor\n\n\n\n\n\n\n\n\n\n\n@@ -47,7 +47,7 @@ class TestMiddlewareManager(MiddlewareManager):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return ['scrapy.tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]\n+        return ['tests.test_middleware.%s' % x for x in ['M1', 'MOff', 'M3']]\n \n     def _add_middleware(self, mw):\n         super(TestMiddlewareManager, self)._add_middleware(mw)\n\n\n\n\n@@ -9,8 +9,8 @@ from netlib import http_auth\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import get_testlog, docrawl\n-from scrapy.tests.spiders import SimpleSpider\n-from scrapy.tests.mockserver import MockServer\n+from tests.spiders import SimpleSpider\n+from tests.mockserver import MockServer\n \n \n \n\n\n\n\n\n@@ -127,7 +127,7 @@ class SettingsTest(unittest.TestCase):\n \n         self.settings.attributes = {}\n         self.settings.setmodule(\n-            'scrapy.tests.test_settings.default_settings', 10)\n+            'tests.test_settings.default_settings', 10)\n \n         self.assertItemsEqual(six.iterkeys(self.settings.attributes),\n                               six.iterkeys(ctrl_attributes))\n\n\n\n@@ -59,11 +59,11 @@ class SpiderManagerTest(unittest.TestCase):\n             ['spider3'])\n \n     def test_load_spider_module(self):\n-        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider1'])\n+        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider1'])\n         assert len(self.spiderman._spiders) == 1\n \n     def test_load_base_spider(self):\n-        self.spiderman = SpiderManager(['scrapy.tests.test_spidermanager.test_spiders.spider0'])\n+        self.spiderman = SpiderManager(['tests.test_spidermanager.test_spiders.spider0'])\n         assert len(self.spiderman._spiders) == 0\n \n     def test_load_from_crawler(self):\n\n\n\n\n\n\n\n\n@@ -4,7 +4,7 @@ from twisted.trial.unittest import TestCase as TrialTestCase\n from twisted.internet import defer\n \n from scrapy.utils.test import docrawl, get_testlog\n-from scrapy.tests.mockserver import MockServer\n+from tests.mockserver import MockServer\n from scrapy.http import Response, Request\n from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError\n\n\n\n\n\n\n\n\n\n\n\n@@ -43,9 +43,9 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertEqual(len(w), 1)\n         self.assertEqual(\n             str(w[0].message),\n-            \"scrapy.tests.test_utils_deprecate.UserClass inherits from \"\n-            \"deprecated class scrapy.tests.test_utils_deprecate.Deprecated, \"\n-            \"please inherit from scrapy.tests.test_utils_deprecate.NewName.\"\n+            \"tests.test_utils_deprecate.UserClass inherits from \"\n+            \"deprecated class tests.test_utils_deprecate.Deprecated, \"\n+            \"please inherit from tests.test_utils_deprecate.NewName.\"\n             \" (warning only on first subclass, there may be others)\"\n         )\n         self.assertEqual(w[0].lineno, inspect.getsourcelines(UserClass)[1])\n@@ -120,8 +120,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertEqual(len(w), 1)\n         self.assertEqual(\n             str(w[0].message),\n-            \"scrapy.tests.test_utils_deprecate.Deprecated is deprecated, \"\n-            \"instantiate scrapy.tests.test_utils_deprecate.NewName instead.\"\n+            \"tests.test_utils_deprecate.Deprecated is deprecated, \"\n+            \"instantiate tests.test_utils_deprecate.NewName instead.\"\n         )\n         self.assertEqual(w[0].lineno, lineno)\n \n@@ -133,8 +133,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n                 pass\n \n         msg = str(w[0].message)\n-        self.assertIn(\"scrapy.tests.test_utils_deprecate.NewName\", msg)\n-        self.assertIn(\"scrapy.tests.test_utils_deprecate.Deprecated\", msg)\n+        self.assertIn(\"tests.test_utils_deprecate.NewName\", msg)\n+        self.assertIn(\"tests.test_utils_deprecate.Deprecated\", msg)\n \n     def test_issubclass(self):\n         with warnings.catch_warnings(record=True):\n\n@@ -1,7 +1,7 @@\n import unittest\n from os.path import join\n \n-from scrapy.tests import tests_datadir\n+from tests import tests_datadir\n from scrapy.utils.gz import gunzip\n \n SAMPLEDIR = join(tests_datadir, 'compressed')\n\n\n\n@@ -4,7 +4,7 @@ from twisted.trial import unittest\n from scrapy.utils.iterators import csviter, xmliter, _body_or_str\n from scrapy.contrib_exp.iterators import xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse, Response\n-from scrapy.tests import get_testdata\n+from tests import get_testdata\n \n FOOBAR_NL = u\"foo\" + os.linesep + u\"bar\"\n \n\n@@ -4,7 +4,7 @@ from io import BytesIO\n from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n     JsonRpcError, jsonrpc_errors\n from scrapy.utils.serialize import ScrapyJSONDecoder\n-from scrapy.tests.test_utils_serialize import CrawlerMock\n+from tests.test_utils_serialize import CrawlerMock\n \n class urllib_mock(object):\n     def __init__(self, result=None, error=None):\n\n@@ -16,25 +16,25 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n \n     def test_walk_modules(self):\n-        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules')\n+        mods = walk_modules('tests.test_utils_misc.test_walk_modules')\n         expected = [\n-            'scrapy.tests.test_utils_misc.test_walk_modules',\n-            'scrapy.tests.test_utils_misc.test_walk_modules.mod',\n-            'scrapy.tests.test_utils_misc.test_walk_modules.mod.mod0',\n-            'scrapy.tests.test_utils_misc.test_walk_modules.mod1',\n+            'tests.test_utils_misc.test_walk_modules',\n+            'tests.test_utils_misc.test_walk_modules.mod',\n+            'tests.test_utils_misc.test_walk_modules.mod.mod0',\n+            'tests.test_utils_misc.test_walk_modules.mod1',\n         ]\n         self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n \n-        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules.mod')\n+        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod')\n         expected = [\n-            'scrapy.tests.test_utils_misc.test_walk_modules.mod',\n-            'scrapy.tests.test_utils_misc.test_walk_modules.mod.mod0',\n+            'tests.test_utils_misc.test_walk_modules.mod',\n+            'tests.test_utils_misc.test_walk_modules.mod.mod0',\n         ]\n         self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n \n-        mods = walk_modules('scrapy.tests.test_utils_misc.test_walk_modules.mod1')\n+        mods = walk_modules('tests.test_utils_misc.test_walk_modules.mod1')\n         expected = [\n-            'scrapy.tests.test_utils_misc.test_walk_modules.mod1',\n+            'tests.test_utils_misc.test_walk_modules.mod1',\n         ]\n         self.assertEquals(set([m.__name__ for m in mods]), set(expected))\n \n\n\n\n\n\n\n\n\n\n\n\n\n@@ -28,8 +28,8 @@ class UtilsSpidersTestCase(unittest.TestCase):\n         self.assertEqual(list(iterate_spider_output([r, i, o])), [r, i, o])\n \n     def test_iter_spider_classes(self):\n-        import scrapy.tests.test_utils_spider\n-        it = iter_spider_classes(scrapy.tests.test_utils_spider)\n+        import tests.test_utils_spider\n+        it = iter_spider_classes(tests.test_utils_spider)\n         self.assertEqual(set(it), set([MySpider1, MySpider2]))\n \n if __name__ == \"__main__\":\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a41ef417d4661d4eeadbf16c2d3807f7bc6f42d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 92 | Churn Cumulative: 121 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,13 @@\n+import sys\n+import time\n+import subprocess\n+\n+from six.moves.urllib.parse import urlencode\n+\n+import scrapy\n from scrapy.command import ScrapyCommand\n-from tests.spiders import FollowAllSpider\n-from tests.mockserver import MockServer\n+from scrapy.contrib.linkextractors import LinkExtractor\n+\n \n class Command(ScrapyCommand):\n \n@@ -14,8 +21,41 @@ class Command(ScrapyCommand):\n         return \"Run quick benchmark test\"\n \n     def run(self, args, opts):\n-        with MockServer():\n-            spider = FollowAllSpider(total=100000)\n+        with _BenchServer():\n+            spider = _BenchSpider(total=100000)\n             crawler = self.crawler_process.create_crawler()\n             crawler.crawl(spider)\n             self.crawler_process.start()\n+\n+\n+class _BenchServer(object):\n+\n+    def __enter__(self):\n+        from scrapy.utils.test import get_testenv\n+        pargs = [sys.executable, '-u', '-m', 'scrapy.utils.benchserver']\n+        self.proc = subprocess.Popen(pargs, stdout=subprocess.PIPE,\n+                                     env=get_testenv())\n+        self.proc.stdout.readline()\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        self.proc.kill()\n+        self.proc.wait()\n+        time.sleep(0.2)\n+\n+\n+class _BenchSpider(scrapy.Spider):\n+    \"\"\"A spider that follows all links\"\"\"\n+    name = 'follow'\n+    total = 10000\n+    show = 20\n+    baseurl = 'http://localhost:8998'\n+    link_extractor = LinkExtractor()\n+\n+    def start_requests(self):\n+        qargs = {'total': self.total, 'show': self.show}\n+        url = '{}?{}'.format(self.baseurl, urlencode(qargs, doseq=1))\n+        return [scrapy.Request(url, dont_filter=True)]\n+\n+    def parse(self, response):\n+        for link in self.link_extractor.extract_links(response):\n+            yield scrapy.Request(link.url, callback=self.parse)\n\n@@ -0,0 +1,44 @@\n+import random\n+from six.moves.urllib.parse import urlencode\n+from twisted.web.server import Site\n+from twisted.web.resource import Resource\n+from twisted.internet import reactor\n+\n+\n+class Root(Resource):\n+\n+    isLeaf = True\n+\n+    def getChild(self, name, request):\n+        return self\n+\n+    def render(self, request):\n+        total = _getarg(request, 'total', 100, int)\n+        show = _getarg(request, 'show', 10, int)\n+        nlist = [random.randint(1, total) for _ in range(show)]\n+        request.write(\"<html><head></head><body>\")\n+        args = request.args.copy()\n+        for nl in nlist:\n+            args['n'] = nl\n+            argstr = urlencode(args, doseq=True)\n+            request.write(\"<a href='/follow?{0}'>follow {1}</a><br>\"\n+                          .format(argstr, nl))\n+        request.write(\"</body></html>\")\n+        return ''\n+\n+\n+def _getarg(request, name, default=None, type=str):\n+    return type(request.args[name][0]) \\\n+        if name in request.args else default\n+\n+\n+if __name__ == '__main__':\n+    root = Root()\n+    factory = Site(root)\n+    httpPort = reactor.listenTCP(8998, Site(root))\n+\n+    def _print_listening():\n+        httpHost = httpPort.getHost()\n+        print(\"Bench server at http://{}:{}\".format(httpHost.host, httpHost.port))\n+    reactor.callWhenRunning(_print_listening)\n+    reactor.run()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70ee66ba0c9599a684d2e57ffbbec7c9f472dfaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 36/36 | Churn Δ: 7 | Churn Cumulative: 9 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-\n+import six\n import pytest\n from twisted.python import log\n \n@@ -8,6 +8,9 @@ collect_ignore = [\"scrapy/stats.py\"]\n if 'django' not in optional_features:\n     collect_ignore.append(\"tests/test_djangoitem/models.py\")\n \n+if six.PY3:\n+    for fn in open('tests/py3-ignores.txt'):\n+        collect_ignore.append(fn.strip())\n \n class LogObservers:\n     \"\"\"Class for keeping track of log observers across test modules\"\"\"\n\n@@ -3,7 +3,7 @@ Some spiders used for testing and benchmarking\n \"\"\"\n \n import time\n-from urllib import urlencode\n+from six.moves.urllib.parse import urlencode\n \n from scrapy.spider import Spider\n from scrapy.http import Request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cd271eb8e9560b16ee6a18566c5b0e95f3adfda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 8 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,7 @@\n \"\"\"\n tests: this package contains all Scrapy unittests\n \n-To run all Scrapy unittests go to Scrapy main dir and type: \n-\n-    bin/runtests.sh\n-    \n-If you're in windows use runtests.bat instead.\n+see http://doc.scrapy.org/en/latest/contributing.html#running-tests\n \"\"\"\n \n import os\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40cc875d6dfa9c0f4633641a160fd90c9cbc55e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 43 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ based on different criteria.\n \n from mimetypes import MimeTypes\n from pkgutil import get_data\n-from io import BytesIO\n+from io import StringIO\n import six\n \n from scrapy.http import Response\n@@ -33,8 +33,8 @@ class ResponseTypes(object):\n     def __init__(self):\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n-        mimedata = get_data('scrapy', 'mime.types')\n-        self.mimetypes.readfp(BytesIO(mimedata))\n+        mimedata = get_data('scrapy', 'mime.types').decode('utf8')\n+        self.mimetypes.readfp(StringIO(mimedata))\n         for mimetype, cls in six.iteritems(self.CLASSES):\n             self.classes[mimetype] = load_object(cls)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1a108b447d49a20b90805aa50c48e80a19ce73e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1052 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -166,6 +166,8 @@ class ScrapyAgent(object):\n         url = urldefrag(request.url)[0]\n         method = request.method\n         headers = TxHeaders(request.headers)\n+        if isinstance(agent, self._TunnelingAgent):\n+            headers.removeHeader('Proxy-Authorization')\n         bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n \n         start_time = time()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#484a0159d0fe9eb4d7ebba93401d5ba0f26387c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 21 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n+import json\n import os\n-import subprocess\n import time\n \n from threading import Thread\n@@ -9,13 +9,11 @@ from netlib import http_auth\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import get_testlog, docrawl\n-from tests.spiders import SimpleSpider\n+from scrapy.http import Request\n+from tests.spiders import SimpleSpider, SingleRequestSpider\n from tests.mockserver import MockServer\n \n \n-\n-\n-\n class HTTPSProxy(controller.Master, Thread):\n \n     def __init__(self, port):\n@@ -79,6 +77,15 @@ class ProxyConnectTestCase(TestCase):\n         self._assert_got_tunnel_error()\n         os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n+    @defer.inlineCallbacks\n+    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n+        request = Request(\"https://localhost:8999/echo\")\n+        spider = SingleRequestSpider(seed=request)\n+        yield docrawl(spider)\n+        self._assert_got_response_code(200)\n+        echo = json.loads(spider.meta['responses'][0].body)\n+        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n+\n     @defer.inlineCallbacks\n     def test_https_noconnect_auth_error(self):\n         os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f42b44b50ec8dfbaa5077595a6ce738adbbb9647", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 44 | Files Changed: 2 | Hunks: 22 | Methods Changed: 18 | Complexity Δ (Sum/Max): 21/18 | Churn Δ: 123 | Churn Cumulative: 143 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import six\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n \n@@ -10,19 +11,29 @@ class Headers(CaselessDict):\n         super(Headers, self).__init__(seq)\n \n     def normkey(self, key):\n-        \"\"\"Headers must not be unicode\"\"\"\n-        if isinstance(key, unicode):\n-            return key.title().encode(self.encoding)\n-        return key.title()\n+        \"\"\"Normalize key to bytes\"\"\"\n+        return self._tobytes(key.title())\n \n     def normvalue(self, value):\n-        \"\"\"Headers must not be unicode\"\"\"\n+        \"\"\"Normalize values to bytes\"\"\"\n         if value is None:\n             value = []\n+        elif isinstance(value, (six.text_type, bytes)):\n+            value = [value]\n         elif not hasattr(value, '__iter__'):\n             value = [value]\n-        return [x.encode(self.encoding) if isinstance(x, unicode) else x \\\n-            for x in value]\n+\n+        return [self._tobytes(x) for x in value]\n+\n+    def _tobytes(self, x):\n+        if isinstance(x, bytes):\n+            return x\n+        elif isinstance(x, six.text_type):\n+            return x.encode(self.encoding)\n+        elif isinstance(x, int):\n+            return six.text_type(x).encode(self.encoding)\n+        else:\n+            raise TypeError('Unsupported value type: {}'.format(type(x)))\n \n     def __getitem__(self, key):\n         try:\n\n@@ -17,49 +17,47 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h.get('Accept'), None)\n         self.assertEqual(h.getlist('Accept'), [])\n \n-        self.assertEqual(h.get('Accept', '*/*'), '*/*')\n-        self.assertEqual(h.getlist('Accept', '*/*'), ['*/*'])\n-        self.assertEqual(h.getlist('Accept', ['text/html', 'images/jpeg']), ['text/html','images/jpeg'])\n+        self.assertEqual(h.get('Accept', '*/*'), b'*/*')\n+        self.assertEqual(h.getlist('Accept', '*/*'), [b'*/*'])\n+        self.assertEqual(h.getlist('Accept', ['text/html', 'images/jpeg']),\n+                         [b'text/html', b'images/jpeg'])\n \n     def test_single_value(self):\n         h = Headers()\n         h['Content-Type'] = 'text/html'\n-        self.assertEqual(h['Content-Type'], 'text/html')\n-        self.assertEqual(h.get('Content-Type'), 'text/html')\n-        self.assertEqual(h.getlist('Content-Type'), ['text/html'])\n+        self.assertEqual(h['Content-Type'], b'text/html')\n+        self.assertEqual(h.get('Content-Type'), b'text/html')\n+        self.assertEqual(h.getlist('Content-Type'), [b'text/html'])\n \n     def test_multivalue(self):\n         h = Headers()\n-\n         h['X-Forwarded-For'] = hlist = ['ip1', 'ip2']\n-        self.assertEqual(h['X-Forwarded-For'], 'ip2')\n-        self.assertEqual(h.get('X-Forwarded-For'), 'ip2')\n-        self.assertEqual(h.getlist('X-Forwarded-For'), hlist)\n+        self.assertEqual(h['X-Forwarded-For'], b'ip2')\n+        self.assertEqual(h.get('X-Forwarded-For'), b'ip2')\n+        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n         assert h.getlist('X-Forwarded-For') is not hlist\n \n     def test_encode_utf8(self):\n         h = Headers({u'key': u'\\xa3'}, encoding='utf-8')\n-        key, val = dict(h).items()[0]\n-        assert isinstance(key, str), key\n-        assert isinstance(val[0], str), val[0]\n-        self.assertEqual(val[0], '\\xc2\\xa3')\n+        key, val = dict(h).popitem()\n+        assert isinstance(key, bytes), key\n+        assert isinstance(val[0], bytes), val[0]\n+        self.assertEqual(val[0], b'\\xc2\\xa3')\n \n     def test_encode_latin1(self):\n         h = Headers({u'key': u'\\xa3'}, encoding='latin1')\n-        key, val = dict(h).items()[0]\n-        self.assertEqual(val[0], '\\xa3')\n+        key, val = dict(h).popitem()\n+        self.assertEqual(val[0], b'\\xa3')\n \n     def test_encode_multiple(self):\n         h = Headers({u'key': [u'\\xa3']}, encoding='utf-8')\n-        key, val = dict(h).items()[0]\n-        self.assertEqual(val[0], '\\xc2\\xa3')\n+        key, val = dict(h).popitem()\n+        self.assertEqual(val[0], b'\\xc2\\xa3')\n \n     def test_delete_and_contains(self):\n         h = Headers()\n-\n         h['Content-Type'] = 'text/html'\n         assert 'Content-Type' in h\n-\n         del h['Content-Type']\n         assert 'Content-Type' not in h\n \n@@ -72,7 +70,7 @@ class HeadersTest(unittest.TestCase):\n \n         h = Headers()\n         olist = h.setdefault('X-Forwarded-For', 'ip1')\n-        self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1'])\n+        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1'])\n         assert h.getlist('X-Forwarded-For') is olist\n \n     def test_iterables(self):\n@@ -80,18 +78,24 @@ class HeadersTest(unittest.TestCase):\n \n         h = Headers(idict)\n         self.assertDictEqual(dict(h),\n-                             {'Content-Type': ['text/html'], 'X-Forwarded-For': ['ip1', 'ip2']})\n-        self.assertSortedEqual(h.keys(), ['X-Forwarded-For', 'Content-Type'])\n-        self.assertSortedEqual(h.items(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n-        self.assertSortedEqual(h.iteritems(), [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n-\n-        self.assertSortedEqual(h.values(), ['ip2', 'text/html'])\n+                             {b'Content-Type': [b'text/html'],\n+                              b'X-Forwarded-For': [b'ip1', b'ip2']})\n+        self.assertSortedEqual(h.keys(),\n+                               [b'X-Forwarded-For', b'Content-Type'])\n+        self.assertSortedEqual(h.items(),\n+                               [(b'X-Forwarded-For', [b'ip1', b'ip2']),\n+                                (b'Content-Type', [b'text/html'])])\n+        self.assertSortedEqual(h.iteritems(),\n+                               [(b'X-Forwarded-For', [b'ip1', b'ip2']),\n+                                (b'Content-Type', [b'text/html'])])\n+        self.assertSortedEqual(h.values(), [b'ip2', b'text/html'])\n \n     def test_update(self):\n         h = Headers()\n-        h.update({'Content-Type': 'text/html', 'X-Forwarded-For': ['ip1', 'ip2']})\n-        self.assertEqual(h.getlist('Content-Type'), ['text/html'])\n-        self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1', 'ip2'])\n+        h.update({'Content-Type': 'text/html',\n+                  'X-Forwarded-For': ['ip1', 'ip2']})\n+        self.assertEqual(h.getlist('Content-Type'), [b'text/html'])\n+        self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n \n     def test_copy(self):\n         h1 = Headers({'header1': ['value1', 'value2']})\n@@ -104,25 +108,25 @@ class HeadersTest(unittest.TestCase):\n     def test_appendlist(self):\n         h1 = Headers({'header1': 'value1'})\n         h1.appendlist('header1', 'value3')\n-        self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])\n+        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])\n \n         h1 = Headers()\n         h1.appendlist('header1', 'value1')\n         h1.appendlist('header1', 'value3')\n-        self.assertEqual(h1.getlist('header1'), ['value1', 'value3'])\n+        self.assertEqual(h1.getlist('header1'), [b'value1', b'value3'])\n \n     def test_setlist(self):\n         h1 = Headers({'header1': 'value1'})\n-        self.assertEqual(h1.getlist('header1'), ['value1'])\n-        h1.setlist('header1', ['value2', 'value3'])\n-        self.assertEqual(h1.getlist('header1'), ['value2', 'value3'])\n+        self.assertEqual(h1.getlist('header1'), [b'value1'])\n+        h1.setlist('header1', [b'value2', b'value3'])\n+        self.assertEqual(h1.getlist('header1'), [b'value2', b'value3'])\n \n     def test_setlistdefault(self):\n         h1 = Headers({'header1': 'value1'})\n         h1.setlistdefault('header1', ['value2', 'value3'])\n         h1.setlistdefault('header2', ['value2', 'value3'])\n-        self.assertEqual(h1.getlist('header1'), ['value1'])\n-        self.assertEqual(h1.getlist('header2'), ['value2', 'value3'])\n+        self.assertEqual(h1.getlist('header1'), [b'value1'])\n+        self.assertEqual(h1.getlist('header2'), [b'value2', b'value3'])\n \n     def test_none_value(self):\n         h1 = Headers()\n@@ -131,3 +135,23 @@ class HeadersTest(unittest.TestCase):\n         h1.setdefault('foo', 'bar')\n         self.assertEqual(h1.get('foo'), None)\n         self.assertEqual(h1.getlist('foo'), [])\n+\n+    def test_int_value(self):\n+        h1 = Headers({'hey': 5})\n+        h1['foo'] = 1\n+        h1.setdefault('bar', 2)\n+        h1.setlist('buz', [1, 'dos', 3])\n+        self.assertEqual(h1.getlist('foo'), [b'1'])\n+        self.assertEqual(h1.getlist('bar'), [b'2'])\n+        self.assertEqual(h1.getlist('buz'), [b'1', b'dos', b'3'])\n+        self.assertEqual(h1.getlist('hey'), [b'5'])\n+\n+    def test_invalid_value(self):\n+        self.assertRaisesRegexp(TypeError, 'Unsupported value type',\n+                                Headers, {'foo': object()})\n+        self.assertRaisesRegexp(TypeError, 'Unsupported value type',\n+                                Headers().__setitem__, 'foo', object())\n+        self.assertRaisesRegexp(TypeError, 'Unsupported value type',\n+                                Headers().setdefault, 'foo', object())\n+        self.assertRaisesRegexp(TypeError, 'Unsupported value type',\n+                                Headers().setlist, 'foo', [object()])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#521303e27ffd2c5e877c692c4b7eedd073765c9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 8 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -426,7 +426,7 @@ class FTPTestCase(unittest.TestCase):\n         def _test(r):\n             self.assertEqual(r.status, 200)\n             self.assertEqual(r.body, 'I have the power!')\n-            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': [17]})\n+            self.assertEqual(r.headers, {'Local Filename': [''], 'Size': ['17']})\n         return self._add_test_callbacks(d, _test)\n \n     def test_ftp_download_notexist(self):\n@@ -446,7 +446,7 @@ class FTPTestCase(unittest.TestCase):\n \n         def _test(r):\n             self.assertEqual(r.body, local_fname)\n-            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': [17]})\n+            self.assertEqual(r.headers, {'Local Filename': ['/tmp/file.txt'], 'Size': ['17']})\n             self.assertTrue(os.path.exists(local_fname))\n             with open(local_fname) as f:\n                 self.assertEqual(f.read(), \"I have the power!\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#910d0d36c195adef30de45b08957cb10813577d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 112 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,11 @@\n-import sys\n import os\n-from six.moves.configparser import SafeConfigParser\n+import sys\n from operator import itemgetter\n \n+import six\n+from six.moves.configparser import SafeConfigParser\n+\n+\n def build_component_list(base, custom):\n     \"\"\"Compose a component list based on a custom and base dict of components\n     (typically middlewares or extensions), unless custom is already a list, in\n@@ -12,8 +15,9 @@ def build_component_list(base, custom):\n         return custom\n     compdict = base.copy()\n     compdict.update(custom)\n-    return [k for k, v in sorted(compdict.items(), key=itemgetter(1)) \\\n-        if v is not None]\n+    items = (x for x in six.iteritems(compdict) if x[1] is not None)\n+    return [x[0] for x in sorted(items, key=itemgetter(1))]\n+\n \n def arglist_to_dict(arglist):\n     \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n@@ -21,6 +25,7 @@ def arglist_to_dict(arglist):\n     \"\"\"\n     return dict(x.split('=', 1) for x in arglist)\n \n+\n def closest_scrapy_cfg(path='.', prevpath=None):\n     \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n     directory and its parents\n@@ -33,6 +38,7 @@ def closest_scrapy_cfg(path='.', prevpath=None):\n         return cfgfile\n     return closest_scrapy_cfg(os.path.dirname(path), path)\n \n+\n def init_env(project='default', set_syspath=True):\n     \"\"\"Initialize environment to use command-line tool from inside a project\n     dir. This sets the Scrapy settings module and modifies the Python path to\n@@ -47,6 +53,7 @@ def init_env(project='default', set_syspath=True):\n         if set_syspath and projdir not in sys.path:\n             sys.path.append(projdir)\n \n+\n def get_config(use_closest=True):\n     \"\"\"Get Scrapy config file as a SafeConfigParser\"\"\"\n     sources = get_sources(use_closest)\n@@ -54,8 +61,9 @@ def get_config(use_closest=True):\n     cfg.read(sources)\n     return cfg\n \n+\n def get_sources(use_closest=True):\n-    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg', \\\n+    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',\n                os.path.expanduser('~/.scrapy.cfg')]\n     if use_closest:\n         sources.append(closest_scrapy_cfg())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
