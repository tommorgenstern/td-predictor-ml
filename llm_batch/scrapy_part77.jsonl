{"custom_id": "scrapy#fe4683d6373081052c1befe6561cf6046f9610ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 315 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -95,6 +95,7 @@ class XmlItemExporter(BaseItemExporter):\n \n     def _export_xml_field(self, name, serialized_value):\n         self.xg.startElement(name, {})\n+        print '%r %r' % (name, serialized_value), self.encoding\n         self.xg.characters(serialized_value)\n         self.xg.endElement(name)\n \n\n@@ -1,4 +1,3 @@\n-import sys\n import warnings\n from functools import wraps\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#14c6c4623a87d4b287aeef65582191ec40e753c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 265 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -95,7 +95,6 @@ class XmlItemExporter(BaseItemExporter):\n \n     def _export_xml_field(self, name, serialized_value):\n         self.xg.startElement(name, {})\n-        print '%r %r' % (name, serialized_value), self.encoding\n         self.xg.characters(serialized_value)\n         self.xg.endElement(name)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8efe6eb55e83c77b6ee33faf3bf437792c8d2b3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 17 | Churn Cumulative: 151 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n-from scrapy.item import DictItem, Field\n+from scrapy.item import Field, Item, ItemMeta\n \n \n-class DjangoItemMeta(type):\n+class DjangoItemMeta(ItemMeta):\n \n     def  __new__(mcs, class_name, bases, attrs):\n         cls = super(DjangoItemMeta, mcs).__new__(mcs, class_name, bases, attrs)\n@@ -12,17 +12,14 @@ class DjangoItemMeta(type):\n             cls._model_meta = cls.django_model._meta\n             for model_field in cls._model_meta.fields:\n                 # XXX: for now we're treating each PK as autogenerated field\n-                if model_field != cls._model_meta.pk:\n+                if model_field != cls._model_meta.pk and \\\n+                   model_field.name not in cls.fields:\n                     cls.fields[model_field.name] = Field()\n                     cls._model_fields.append(model_field.name)\n-            # add declared fields\n-            for n, v in attrs.iteritems():\n-                if isinstance(v, Field):\n-                    cls.fields[n] = v\n         return cls\n \n \n-class DjangoItem(DictItem):\n+class DjangoItem(Item):\n \n     __metaclass__ = DjangoItemMeta\n \n\n@@ -17,7 +17,7 @@ class Field(dict):\n     \"\"\"Container of field metadata\"\"\"\n \n \n-class _ItemMeta(type):\n+class ItemMeta(type):\n \n     def __new__(mcs, class_name, bases, attrs):\n         fields = {}\n@@ -81,7 +81,7 @@ class DictItem(DictMixin, BaseItem):\n \n class Item(DictItem):\n \n-    __metaclass__ = _ItemMeta\n+    __metaclass__ = ItemMeta\n \n \n class ScrapedItem(BaseItem):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62e03419b797d7b69cc52aef3438522e0058de61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -12,8 +12,8 @@ class DjangoItemMeta(ItemMeta):\n             cls._model_meta = cls.django_model._meta\n             for model_field in cls._model_meta.fields:\n                 # XXX: for now we're treating each PK as autogenerated field\n-                if model_field != cls._model_meta.pk and \\\n-                   model_field.name not in cls.fields:\n+                if model_field != cls._model_meta.pk:\n+                    if model_field.name not in cls.fields:\n                         cls.fields[model_field.name] = Field()\n                     cls._model_fields.append(model_field.name)\n         return cls\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e163a981659e88fe102f7db7b3194c9dd0fe8db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -54,3 +54,12 @@ class DjangoItemTest(unittest.TestCase):\n         self.assertEqual(person.name, 'John')\n         self.assertEqual(person.age, '22')\n \n+    def test_override_save(self):\n+        i = OverrideFieldPersonItem()\n+\n+        i['name'] = 'John'\n+        person = i.save(commit=False)\n+\n+        self.assertEqual(person.name, 'John')\n+        self.assertEqual(person.age, 1)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#625137464f61ff611ab6fb644ba5d72bb0e6d971", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import textwrap\n+\n from scrapy.command import ScrapyCommand, cmdline\n \n class Command(ScrapyCommand):\n@@ -14,16 +16,25 @@ class Command(ScrapyCommand):\n         if not args:\n             return False\n \n-        commands = cmdline.builtin_commands_dict()\n-        commands.update(cmdline.custom_commands_dict())\n+        commands = cmdline.get_commands_dict()\n \n         cmdname = args[0]\n         if cmdname in commands:\n             cmd = commands[cmdname]\n             help = getattr(cmd, 'help', None) or getattr(cmd, 'long_desc', None)\n-            print \"%s: %s\" % (cmdname, cmd.short_desc())\n+            title = \"%s command\" % cmdname\n+            print title\n+            print \"-\" * len(title)\n+            print\n+            print cmd.short_desc()\n+            print\n             print \"usage: %s %s\" % (cmdname, cmd.syntax())\n             print\n-            print help()\n+            print \"\\n\".join(textwrap.wrap(help()))\n+            print\n+            print \"For a list of supported arguments use:\"\n+            print\n+            print \"   scrapy-ctl.py %s -h\" % cmdname\n+            print\n         else:\n             print \"Unknown command: %s\" % cmdname\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab0def3f8be47fbf860b0a7ec5306c7417371e24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 195 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -145,3 +145,6 @@ def run_command(cmd, args, opts):\n     else:\n         ret = cmd.run(args, opts)\n     return ret\n+\n+if __name__ == '__main__':\n+    execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#78481bf4d311eb1651b228ccbb24950529e5cc84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 400 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ from twisted.python.failure import Failure\n from twisted.internet import defer\n \n from scrapy.utils.defer import defer_result, defer_succeed, parallel\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, arg_to_iter\n from scrapy.utils.signal import send_catch_log\n from scrapy.core.exceptions import IgnoreRequest, DropItem\n from scrapy.core import signals\n@@ -131,7 +131,7 @@ class Scraper(object):\n         return request.deferred.addCallback(self._iterable_spider_output)\n \n     def _iterable_spider_output(self, result):\n-        return [result] if isinstance(result, (BaseItem, Request)) else result\n+        return [result] if isinstance(result, BaseItem) else arg_to_iter(result)\n \n     def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02de8cb6fa9af8f4a902dd6f8dcdf109dfb60bfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 79 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,10 +14,12 @@ class Settings(object):\n     default_settings_module = 'scrapy.conf.default_settings'\n \n     def __init__(self):\n-        self.set_settings_module()\n         self.defaults = {}\n         self.global_defaults = import_(self.default_settings_module)\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n+        settings_module_path = os.environ.get('SCRAPYSETTINGS_MODULE', \\\n+            'scrapy_settings')\n+        self.set_settings_module(settings_module_path)\n \n         # XXX: find a better solution for this hack\n         pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n@@ -35,10 +37,7 @@ class Settings(object):\n                 return self.defaults[opt_name]\n         return getattr(self.global_defaults, opt_name, None)\n \n-    def set_settings_module(self, settings_module_path=None):\n-        if settings_module_path is None:\n-            settings_module_path = os.environ.get('SCRAPYSETTINGS_MODULE', \\\n-                'scrapy_settings')\n+    def set_settings_module(self, settings_module_path):\n         self.settings_module_path = settings_module_path\n         try:\n             self.settings_module = import_(settings_module_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea4f16bae6002145faa94a7d6603187faef8bc3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 107 | Files Changed: 2 | Hunks: 7 | Methods Changed: 18 | Complexity Δ (Sum/Max): 4/20 | Churn Δ: 244 | Churn Cumulative: 267 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,21 +4,8 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-import os\n-import urllib\n-import urlparse\n-\n-from twisted.internet import reactor, threads\n-\n from scrapy.command import ScrapyCommand\n-from scrapy.spider import spiders\n-from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n-from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n-from scrapy.core.manager import scrapymanager\n-from scrapy.core.engine import scrapyengine\n-from scrapy.http import Request\n-from scrapy.fetcher import get_or_create_spider\n+from scrapy.shell import Shell\n \n class Command(ScrapyCommand):\n \n@@ -31,100 +18,16 @@ class Command(ScrapyCommand):\n         return \"Interactive scraping console\"\n \n     def long_desc(self):\n-        return \"Interactive console for scraping the given url. For scraping local files you can use a URL like file://path/to/file.html\"\n+        return \"Interactive console for scraping the given url. For scraping \" \\\n+            \"local files you can use a URL like file://path/to/file.html\"\n \n-    def update_vars(self):\n-        \"\"\" You can use this function to update the Scrapy objects that will be available in the shell\"\"\"\n+    def update_vars(self, vars):\n+        \"\"\"You can use this function to update the Scrapy objects that will be\n+        available in the shell\n+        \"\"\"\n         pass\n \n-    def get_url(self, url):\n-        url = url.strip()\n-        if url:\n-            u = urlparse.urlparse(url)\n-            if not u.scheme:\n-                path = os.path.abspath(url).replace(os.sep, '/')\n-                url = 'file://' + urllib.pathname2url(path)\n-                u = urlparse.urlparse(url)\n-\n-            if u.scheme not in ('http', 'https', 'file'):\n-                print \"Unsupported scheme '%s' in URL: <%s>\" % (u.scheme, url)\n-                return\n-            request = Request(url)\n-        else:\n-            request = self.user_ns['request']\n-\n-        spider = get_or_create_spider(url)\n-        print \"Fetching %s...\" % request\n-        response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, request, spider)\n-        if response:\n-            self.generate_vars(url, response, request)\n-            return True\n-\n-    def generate_vars(self, url=None, response=None, request=None):\n-        itemcls = load_object(settings['DEFAULT_ITEM_CLASS'])\n-        item = itemcls()\n-        self.vars['item'] = item\n-        if url:\n-            self.vars['xxs'] = XmlXPathSelector(response)\n-            self.vars['hxs'] = HtmlXPathSelector(response)\n-            self.vars['url'] = url\n-            self.vars['response'] = response\n-            self.vars['request'] = request\n-            self.vars['spider'] = spiders.fromurl(url)\n-        self.update_vars()\n-        self.user_ns.update(self.vars)\n-        self.print_vars()\n-        \n-    def print_vars(self):\n-        print '-' * 60\n-        print \"Available Scrapy objects:\"\n-        for key, val in self.vars.iteritems():\n-            print \"   %s: %s\" % (key, val)\n-        print \"Available commands:\"\n-        print \"   get [url]: Fetch a new URL or re-fetch current Request\"\n-        print \"   shelp: Prints this help.\"\n-        print '-' * 60\n-    \n     def run(self, args, opts):\n-        self.vars = {}\n-        self.user_ns = {}\n-        url = None\n-        if args:\n-            url = args[0]\n-\n-        print \"Welcome to Scrapy shell!\"\n-\n-        def _console_thread():\n-            \n-            def _get_magic(shell, arg):\n-                self.get_url(arg)\n-            def _help_magic(shell, _):\n-                self.print_vars()\n-                \n-            if url:\n-                result = self.get_url(url)\n-                if not result:\n-                    self.generate_vars()\n-            else:\n-                self.generate_vars()\n-            try: # use IPython if available\n-                import IPython\n-                shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n-                ip = shell.IP.getapi()\n-                ip.expose_magic(\"get\", _get_magic)\n-                ip.expose_magic(\"shelp\", _help_magic)\n-                shell.mainloop()\n-                reactor.callFromThread(scrapymanager.stop)\n-            except ImportError:\n-                import code\n-                try: # readline module is only available on unix systems\n-                    import readline\n-                except ImportError:\n-                    pass\n-                else:\n-                    import rlcompleter\n-                    readline.parse_and_bind(\"tab:complete\")\n-                code.interact(local=self.vars)\n-\n-        reactor.callInThread(_console_thread)\n-        scrapymanager.start()\n+        url = args[0] if args else None\n+        shell = Shell(self.update_vars)\n+        shell.start(url)\n\n@@ -0,0 +1,127 @@\n+\"\"\"\n+Scrapy Shell\n+\n+See documentation in docs/topics/shell.rst\n+\"\"\"\n+\n+import os\n+import urllib\n+import urlparse\n+import signal\n+\n+from twisted.internet import reactor, threads\n+\n+from scrapy.spider import spiders\n+from scrapy.selector import XmlXPathSelector, HtmlXPathSelector\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.response import open_in_browser\n+from scrapy.conf import settings\n+from scrapy.core.manager import scrapymanager\n+from scrapy.core.engine import scrapyengine\n+from scrapy.http import Request\n+from scrapy.fetcher import get_or_create_spider\n+\n+def relevant_var(varname):\n+    return varname not in ['shelp', 'fetch', 'view', '__builtins__', 'In', \\\n+        'Out', 'help'] and not varname.startswith('_')\n+\n+def parse_url(url):\n+    \"\"\"Parse url which can be a direct path to a direct file\"\"\"\n+    url = url.strip()\n+    if url:\n+        u = urlparse.urlparse(url)\n+        if not u.scheme:\n+            path = os.path.abspath(url).replace(os.sep, '/')\n+            url = 'file://' + urllib.pathname2url(path)\n+            u = urlparse.urlparse(url)\n+    return url\n+\n+class Shell(object):\n+\n+    requires_project = False\n+\n+    def __init__(self, update_vars):\n+        self.vars = {}\n+        self.update_vars = update_vars\n+        self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])\n+\n+    def fetch(self, request_or_url, print_help=False):\n+        if isinstance(request_or_url, Request):\n+            request = request_or_url\n+            url = request.url\n+        else:\n+            url = parse_url(request_or_url)\n+            request = Request(url)\n+        spider = get_or_create_spider(url)\n+        print \"Fetching %s...\" % request\n+        response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \\\n+            request, spider)\n+        if response:\n+            self.populate_vars(url, response, request)\n+            if print_help:\n+                self.print_help()\n+            else:\n+                print \"Done - use shelp() to see available objects\"\n+\n+    def populate_vars(self, url=None, response=None, request=None):\n+        item = self.item_class()\n+        self.vars['item'] = item\n+        if url:\n+            self.vars['xxs'] = XmlXPathSelector(response)\n+            self.vars['hxs'] = HtmlXPathSelector(response)\n+            self.vars['url'] = url\n+            self.vars['response'] = response\n+            self.vars['request'] = request\n+            self.vars['spider'] = spiders.fromurl(url)\n+\n+            self.vars['fetch'] = self.fetch\n+            self.vars['view'] = open_in_browser\n+            self.vars['shelp'] = self.print_help\n+\n+        self.update_vars(self.vars)\n+\n+    def print_help(self):\n+        print \"Available objects\"\n+        print \"=================\"\n+        print\n+        for k, v in self.vars.iteritems():\n+            if relevant_var(k):\n+                print \"  %-10s: %s\" % (k, v)\n+        print\n+        print \"Available shortcuts\"\n+        print \"===================\"\n+        print\n+        print \"  shelp()           : Prints this help.\"\n+        print \"  fetch(req_or_url) : Fetch a new request or URL and update objects\"\n+        print \"  view(response)    : View response in a browser\"\n+        print\n+\n+    def start(self, url):\n+        # disable accidental Ctrl-C key press from shutting down the engine\n+        signal.signal(signal.SIGINT, signal.SIG_IGN)\n+\n+        reactor.callInThread(self._console_thread, url)\n+        scrapymanager.start()\n+\n+    def _console_thread(self, url=None):\n+        self.populate_vars()\n+        if url:\n+            result = self.fetch(url, print_help=True)\n+        else:\n+            self.print_help()\n+        try: # use IPython if available\n+            import IPython\n+            shell = IPython.Shell.IPShell(argv=[], user_ns=self.vars)\n+            ip = shell.IP.getapi()\n+            shell.mainloop()\n+        except ImportError:\n+            import code\n+            try: # readline module is only available on unix systems\n+                import readline\n+            except ImportError:\n+                pass\n+            else:\n+                import rlcompleter\n+                readline.parse_and_bind(\"tab:complete\")\n+            code.interact(local=self.vars)\n+        reactor.callFromThread(scrapymanager.stop)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#609aed45539f0fb7e08fb5f39c89c827a4f1d94b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 51 | Churn Cumulative: 193 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,8 @@ log_level = DEBUG\n \n started = False\n \n+_prev_descriptors = (sys.stdout, sys.stderr)\n+\n def start(logfile=None, loglevel=None, logstdout=None):\n     \"\"\"Initialize and start logging facility\"\"\"\n     global log_level, started\n@@ -53,6 +55,13 @@ def start(logfile=None, loglevel=None, logstdout=None):\n         file = open(logfile, 'a') if logfile else sys.stderr\n         log.startLogging(file, setStdout=logstdout)\n \n+def _switch_descriptors():\n+    global _prev_descriptors\n+\n+    cur = (sys.stdout, sys.stderr)\n+    sys.stdout, sys.stderr = _prev_descriptors\n+    _prev_descriptors = cur\n+\n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n\n@@ -20,6 +20,7 @@ from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.http import Request\n from scrapy.fetcher import get_or_create_spider\n+from scrapy import log\n \n def relevant_var(varname):\n     return varname not in ['shelp', 'fetch', 'view', '__builtins__', 'In', \\\n@@ -40,10 +41,11 @@ class Shell(object):\n \n     requires_project = False\n \n-    def __init__(self, update_vars):\n+    def __init__(self, update_vars=None, nofetch=False):\n         self.vars = {}\n         self.update_vars = update_vars\n         self.item_class = load_object(settings['DEFAULT_ITEM_CLASS'])\n+        self.nofetch = nofetch\n \n     def fetch(self, request_or_url, print_help=False):\n         if isinstance(request_or_url, Request):\n@@ -74,10 +76,12 @@ class Shell(object):\n             self.vars['request'] = request\n             self.vars['spider'] = spiders.fromurl(url)\n \n+            if not self.nofetch:\n                 self.vars['fetch'] = self.fetch\n             self.vars['view'] = open_in_browser\n             self.vars['shelp'] = self.print_help\n \n+        if self.update_vars:\n             self.update_vars(self.vars)\n \n     def print_help(self):\n@@ -92,6 +96,7 @@ class Shell(object):\n         print \"===================\"\n         print\n         print \"  shelp()           : Prints this help.\"\n+        if not self.nofetch:\n             print \"  fetch(req_or_url) : Fetch a new request or URL and update objects\"\n         print \"  view(response)    : View response in a browser\"\n         print\n@@ -103,12 +108,20 @@ class Shell(object):\n         reactor.callInThread(self._console_thread, url)\n         scrapymanager.start()\n \n-    def _console_thread(self, url=None):\n-        self.populate_vars()\n-        if url:\n-            result = self.fetch(url, print_help=True)\n-        else:\n-            self.print_help()\n+    def inspect_response(self, response):\n+        print\n+        print \"Scrapy Shell\"\n+        print \"============\"\n+        print\n+        print \"Inspecting: %s\" % response\n+        print \"Use shelp() to see available objects\"\n+        print\n+        request = response.request\n+        url = request.url\n+        self.populate_vars(url, response, request)\n+        self._run_console()\n+\n+    def _run_console(self):\n         try: # use IPython if available\n             import IPython\n             shell = IPython.Shell.IPShell(argv=[], user_ns=self.vars)\n@@ -124,4 +137,19 @@ class Shell(object):\n                 import rlcompleter\n                 readline.parse_and_bind(\"tab:complete\")\n             code.interact(local=self.vars)\n+\n+    def _console_thread(self, url=None):\n+        self.populate_vars()\n+        if url:\n+            result = self.fetch(url, print_help=True)\n+        else:\n+            self.print_help()\n+        self._run_console()\n         reactor.callFromThread(scrapymanager.stop)\n+\n+def inspect_response(response):\n+    \"\"\"Open a shell to inspect the given response\"\"\"\n+    shell = Shell(nofetch=True)\n+    log._switch_descriptors()\n+    shell.inspect_response(response)\n+    log._switch_descriptors()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da20cf53f58da1b421fc7f79223a142b0cf6e3e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 82 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,8 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n \n+from __future__ import with_statement\n+\n import re\n import weakref\n import webbrowser\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0bfa1a910c7d3f9d718dd38852a426a312aca95d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 61 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,8 +13,6 @@ from scrapy import log\n from scrapy.conf import settings\n \n class SimpledbStatsCollector(StatsCollector):\n-    \"\"\"A simple in-memory stats collector which keeps scraping stats of last\n-    run. Those can be accessed through the ``domain_stats`` attribute\"\"\"\n \n     def __init__(self):\n         super(SimpledbStatsCollector, self).__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3616ccb4790d85fc9939676e27da068136ee4726", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 87 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python\n \n import os\n-os.environ.setdefault('SCRAPYSETTINGS_MODULE', 'googledir.settings')\n+os.environ.setdefault('SCRAPY_SETTINGS_MODULE', 'googledir.settings')\n \n from scrapy.command.cmdline import execute\n execute()\n\n@@ -17,8 +17,8 @@ class Settings(object):\n         self.defaults = {}\n         self.global_defaults = import_(self.default_settings_module)\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n-        settings_module_path = os.environ.get('SCRAPYSETTINGS_MODULE', \\\n-            'scrapy_settings')\n+        settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \\\n+            os.environ.get('SCRAPYSETTINGS_MODULE', 'scrapy_settings'))\n         self.set_settings_module(settings_module_path)\n \n         # XXX: find a better solution for this hack\n\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python\n \n import os\n-os.environ.setdefault('SCRAPYSETTINGS_MODULE', '${project_name}.settings')\n+os.environ.setdefault('SCRAPY_SETTINGS_MODULE', '${project_name}.settings')\n \n from scrapy.command.cmdline import execute\n execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b68d4bbef48369a9c2c384ec8244cd10116a076", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 126 | Churn Cumulative: 357 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 0.55\n\nDIFF:\n@@ -2,6 +2,7 @@ import shutil\n import string\n from os import listdir\n from os.path import join, dirname, abspath, exists\n+import sys\n \n import scrapy\n from scrapy.spider import spiders\n@@ -62,7 +63,7 @@ class Command(ScrapyCommand):\n         if spider and not opts.force:\n             print \"Spider '%s' already exists in module:\" % domain\n             print \"  %s\" % spider.__module__\n-            return\n+            sys.exit(1)\n \n         template_file = self._find_template(opts.template)\n         if template_file:\n\n@@ -3,6 +3,7 @@\n import os\n import string\n import re\n+import sys\n \n import scrapy\n from scrapy.command import ScrapyCommand\n@@ -44,7 +45,12 @@ class Command(ScrapyCommand):\n             else:\n                 message = 'Project names must contain only letters, numbers and underscores'\n             print \"Invalid project name: %s\\n\\n%s\" % (project_name, message)\n+            sys.exit(1)\n         else:\n+            if os.path.exists(project_name):\n+                print \"%s dir already exists\" % project_name\n+                sys.exit(1)\n+\n             project_root_path = project_name\n \n             roottpl = os.path.join(PROJECT_TEMPLATES_PATH, 'root')\n\n@@ -0,0 +1,117 @@\n+import os\n+from os.path import exists, join\n+import subprocess\n+from shutil import rmtree\n+import sys\n+from tempfile import mkdtemp\n+from unittest import TestCase\n+\n+import scrapy\n+from scrapy.conf import settings\n+\n+\n+class ProjectTest(TestCase):\n+    project_name = 'testproject'\n+\n+    def setUp(self):\n+        self.temp_path = mkdtemp()\n+        self.cwd = self.temp_path\n+\n+    def tearDown(self):\n+        rmtree(self.temp_path)\n+\n+    def call(self, new_args, **kwargs):\n+        out = os.tmpfile()\n+        args = [sys.executable, '-m', 'scrapy.command.cmdline']\n+        args.extend(new_args)\n+\n+        env = self.env if hasattr(self, 'env') else os.environ\n+\n+        return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n+                               env=env, **kwargs)\n+\n+\n+class ScrapyCtlStartprojectTest(ProjectTest):\n+    \n+    def test_startproject(self):\n+        proj_path = join(self.temp_path, self.project_name)\n+        proj_mod_path = join(proj_path, self.project_name)\n+\n+        ret = self.call(['startproject', self.project_name])\n+        self.assertEqual(ret, 0)\n+\n+        self.assertEqual(exists(join(proj_path, 'scrapy-ctl.py')), True)\n+        self.assertEqual(exists(join(proj_path, 'testproject')), True)\n+        self.assertEqual(exists(join(proj_mod_path, '__init__.py')), True)\n+        self.assertEqual(exists(join(proj_mod_path, 'items.py')), True)\n+        self.assertEqual(exists(join(proj_mod_path, 'pipelines.py')), True)\n+        self.assertEqual(exists(join(proj_mod_path, 'settings.py')), True)\n+        self.assertEqual(exists(join(proj_mod_path, 'spiders', '__init__.py')), True)\n+\n+        ret = self.call(['startproject', self.project_name])\n+        self.assertEqual(ret, 1)\n+\n+        ret = self.call(['startproject', 'wrong---project---name'])\n+        self.assertEqual(ret, 1)\n+\n+\n+class CommandTest(ProjectTest):\n+\n+    def setUp(self):\n+        super(CommandTest, self).setUp()\n+\n+        self.call(['startproject', self.project_name])\n+\n+        self.cwd = join(self.temp_path, self.project_name)\n+\n+        self.env = os.environ.copy()\n+        self.env.pop('SCRAPY_SETTINGS_DISABLED', None)\n+        self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n+\n+\n+class ScrapyCtlCommandsTest(CommandTest):\n+\n+    def test_crawl(self):\n+        ret = self.call(['crawl'])\n+        self.assertEqual(ret, 0)\n+\n+    def test_genspider_subcommands(self):\n+        ret = self.call(['genspider', '--list'])\n+        self.assertEqual(ret, 0)\n+\n+        ret = self.call(['genspider', '--dump'])\n+        self.assertEqual(ret, 0)\n+\n+        ret = self.call(['genspider', '--dump', '--template=basic'])\n+        self.assertEqual(ret, 0)\n+\n+    def test_list(self):\n+        ret = self.call(['list'])\n+        self.assertEqual(ret, 0)\n+\n+\n+class BaseGenspiderTest(CommandTest):\n+    template = 'basic'\n+\n+    def test_genspider(self):\n+        ret = self.call(['genspider', 'testspider', 'test.com',\n+                          '--template=%s' % self.template])\n+        self.assertEqual(ret, 0)\n+        self.assertEqual(os.path.exists(join(self.cwd, self.project_name, \\\n+            'spiders', 'testspider.py')), True)\n+\n+        ret = self.call(['genspider', 'otherspider', 'test.com'])\n+        self.assertEqual(ret, 1)\n+\n+    \n+class CrawlGenspiderTest(BaseGenspiderTest):\n+    template = 'crawl'\n+\n+\n+class CsvFeedGenspiderTest(BaseGenspiderTest):\n+    template = 'csvfeed'\n+\n+\n+class XMLFeedGenspiderTest(BaseGenspiderTest):\n+    template = 'xmlfeed'\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e7e321a07b3a07ed0612d8595e72aa3703e5abb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 23 | Churn Cumulative: 380 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n+import sys\n import shutil\n import string\n from os import listdir\n from os.path import join, dirname, abspath, exists\n-import sys\n \n import scrapy\n from scrapy.spider import spiders\n\n@@ -1,9 +1,9 @@\n #!/usr/bin/env python\n \n+import sys\n import os\n import string\n import re\n-import sys\n \n import scrapy\n from scrapy.command import ScrapyCommand\n\n@@ -1,16 +1,13 @@\n-import os\n-from os.path import exists, join\n-import subprocess\n-from shutil import rmtree\n import sys\n+import os\n+import subprocess\n+from os.path import exists, join\n+from shutil import rmtree\n from tempfile import mkdtemp\n-from unittest import TestCase\n-\n-import scrapy\n-from scrapy.conf import settings\n+import unittest\n \n \n-class ProjectTest(TestCase):\n+class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n \n     def setUp(self):\n@@ -25,7 +22,7 @@ class ProjectTest(TestCase):\n         args = [sys.executable, '-m', 'scrapy.command.cmdline']\n         args.extend(new_args)\n \n-        env = self.env if hasattr(self, 'env') else os.environ\n+        env = getattr(self, 'env', os.environ)\n \n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n                                env=env, **kwargs)\n@@ -115,3 +112,5 @@ class CsvFeedGenspiderTest(BaseGenspiderTest):\n class XMLFeedGenspiderTest(BaseGenspiderTest):\n     template = 'xmlfeed'\n \n+if __name__ == '__main__':\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee40929df490fd8bf9d3dbb97e6f49f0a6775a91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 37 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.5\n\nDIFF:\n@@ -19,10 +19,14 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", help=\"disable scraped item pipeline\")\n-        parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", help=\"restrict crawling only to the given urls\")\n-        parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", help=\"don't follow links (for use with URLs only)\")\n-        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", help=\"use the provided callback for starting to crawl the given url\")\n+        parser.add_option(\"--nopipeline\", dest=\"nopipeline\", action=\"store_true\", \\\n+            help=\"disable scraped item pipeline\")\n+        parser.add_option(\"--restrict\", dest=\"restrict\", action=\"store_true\", \\\n+            help=\"restrict crawling only to the given urls\")\n+        parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n+            help=\"don't follow links (for use with URLs only)\")\n+        parser.add_option(\"-c\", \"--callback\", dest=\"callback\", action=\"store\", \\\n+            help=\"use the provided callback for starting to crawl the given url\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -44,20 +48,24 @@ class Command(ScrapyCommand):\n                     urls = [a]\n                 else:\n                     spider = spiders.fromdomain(a)\n-                    urls = spider.start_urls if hasattr(spider.start_urls, '__iter__') else [spider.start_urls]\n+                    urls = spider.start_urls if hasattr(spider.start_urls, \\\n+                        '__iter__') else [spider.start_urls]\n \n                 if spider:\n                     if hasattr(spider, opts.callback):\n-                        requests.extend(Request(url=url, callback=getattr(spider, opts.callback)) for url in urls)\n+                        requests.extend(Request(url=url, callback=getattr(spider, \\\n+                            opts.callback)) for url in urls)\n                     else:\n-                        log.msg('Callback %s doesnt exist in spider %s' % (opts.callback, spider.domain_name), log.ERROR)\n+                        log.msg('Callback %s doesnt exist in spider %s' % \\\n+                            (opts.callback, spider.domain_name), log.ERROR)\n                 else:\n                     log.msg('Cannot find spider for %s' % a, log.ERROR)\n \n             if requests:\n                 args = requests\n             else:\n-                log.msg('Cannot create any requests with the provided arguments', log.ERROR)\n+                log.msg('Cannot create any requests with the provided arguments', \\\n+                    log.ERROR)\n                 return\n \n         scrapymanager.runonce(*args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83a2b580f7b0071040e9baf6d2f6d8b3ddf3f458", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 9 | Churn Cumulative: 9 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -19,16 +19,14 @@ class ProjectTest(unittest.TestCase):\n \n     def call(self, new_args, **kwargs):\n         out = os.tmpfile()\n+        env = getattr(self, 'env', os.environ)\n         args = [sys.executable, '-m', 'scrapy.command.cmdline']\n         args.extend(new_args)\n-\n-        env = getattr(self, 'env', os.environ)\n-\n         return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n             env=env, **kwargs)\n \n \n-class ScrapyCtlStartprojectTest(ProjectTest):\n+class StartprojectTest(ProjectTest):\n     \n     def test_startproject(self):\n         proj_path = join(self.temp_path, self.project_name)\n@@ -66,7 +64,7 @@ class CommandTest(ProjectTest):\n         self.env['SCRAPY_SETTINGS_MODULE'] = '%s.settings' % self.project_name\n \n \n-class ScrapyCtlCommandsTest(CommandTest):\n+class MiscCommandsTest(CommandTest):\n \n     def test_crawl(self):\n         ret = self.call(['crawl'])\n@@ -112,5 +110,6 @@ class CsvFeedGenspiderTest(BaseGenspiderTest):\n class XMLFeedGenspiderTest(BaseGenspiderTest):\n     template = 'xmlfeed'\n \n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7dd43da9a5a71bcfdc37dfeb54293112e764e628", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 27 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -35,13 +35,13 @@ class StartprojectTest(ProjectTest):\n         ret = self.call(['startproject', self.project_name])\n         self.assertEqual(ret, 0)\n \n-        self.assertEqual(exists(join(proj_path, 'scrapy-ctl.py')), True)\n-        self.assertEqual(exists(join(proj_path, 'testproject')), True)\n-        self.assertEqual(exists(join(proj_mod_path, '__init__.py')), True)\n-        self.assertEqual(exists(join(proj_mod_path, 'items.py')), True)\n-        self.assertEqual(exists(join(proj_mod_path, 'pipelines.py')), True)\n-        self.assertEqual(exists(join(proj_mod_path, 'settings.py')), True)\n-        self.assertEqual(exists(join(proj_mod_path, 'spiders', '__init__.py')), True)\n+        assert exists(join(proj_path, 'scrapy-ctl.py'))\n+        assert exists(join(proj_path, 'testproject'))\n+        assert exists(join(proj_mod_path, '__init__.py'))\n+        assert exists(join(proj_mod_path, 'items.py'))\n+        assert exists(join(proj_mod_path, 'pipelines.py'))\n+        assert exists(join(proj_mod_path, 'settings.py'))\n+        assert exists(join(proj_mod_path, 'spiders', '__init__.py'))\n \n         ret = self.call(['startproject', self.project_name])\n         self.assertEqual(ret, 1)\n@@ -92,8 +92,8 @@ class BaseGenspiderTest(CommandTest):\n         ret = self.call(['genspider', 'testspider', 'test.com',\n                           '--template=%s' % self.template])\n         self.assertEqual(ret, 0)\n-        self.assertEqual(os.path.exists(join(self.cwd, self.project_name, \\\n-            'spiders', 'testspider.py')), True)\n+        assert os.path.exists(join(self.cwd, self.project_name, 'spiders', \\\n+            'testspider.py'))\n \n         ret = self.call(['genspider', 'otherspider', 'test.com'])\n         self.assertEqual(ret, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#18e9fd977140ffc8813ba9ae89ae4957aaf0e98c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,6 +13,8 @@ class ProjectTest(unittest.TestCase):\n     def setUp(self):\n         self.temp_path = mkdtemp()\n         self.cwd = self.temp_path\n+        self.proj_path = join(self.temp_path, self.project_name)\n+        self.proj_mod_path = join(self.proj_path, self.project_name)\n \n     def tearDown(self):\n         rmtree(self.temp_path)\n@@ -29,19 +31,16 @@ class ProjectTest(unittest.TestCase):\n class StartprojectTest(ProjectTest):\n     \n     def test_startproject(self):\n-        proj_path = join(self.temp_path, self.project_name)\n-        proj_mod_path = join(proj_path, self.project_name)\n-\n         ret = self.call(['startproject', self.project_name])\n         self.assertEqual(ret, 0)\n \n-        assert exists(join(proj_path, 'scrapy-ctl.py'))\n-        assert exists(join(proj_path, 'testproject'))\n-        assert exists(join(proj_mod_path, '__init__.py'))\n-        assert exists(join(proj_mod_path, 'items.py'))\n-        assert exists(join(proj_mod_path, 'pipelines.py'))\n-        assert exists(join(proj_mod_path, 'settings.py'))\n-        assert exists(join(proj_mod_path, 'spiders', '__init__.py'))\n+        assert exists(join(self.proj_path, 'scrapy-ctl.py'))\n+        assert exists(join(self.proj_path, 'testproject'))\n+        assert exists(join(self.proj_mod_path, '__init__.py'))\n+        assert exists(join(self.proj_mod_path, 'items.py'))\n+        assert exists(join(self.proj_mod_path, 'pipelines.py'))\n+        assert exists(join(self.proj_mod_path, 'settings.py'))\n+        assert exists(join(self.proj_mod_path, 'spiders', '__init__.py'))\n \n         ret = self.call(['startproject', self.project_name])\n         self.assertEqual(ret, 1)\n@@ -92,8 +91,7 @@ class BaseGenspiderTest(CommandTest):\n         ret = self.call(['genspider', 'testspider', 'test.com',\n                           '--template=%s' % self.template])\n         self.assertEqual(ret, 0)\n-        assert os.path.exists(join(self.cwd, self.project_name, 'spiders', \\\n-            'testspider.py'))\n+        assert exists(join(self.proj_mod_path, 'spiders', 'testspider.py'))\n \n         ret = self.call(['genspider', 'otherspider', 'test.com'])\n         self.assertEqual(ret, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#046e63b1cc9a4f82588998e58ec9b7f281aba149", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.4\n\nDIFF:\n@@ -8,6 +8,7 @@ import os\n import sys\n from optparse import OptionGroup\n \n+import scrapy\n from scrapy.conf import settings\n \n class ScrapyCommand(object):\n@@ -47,6 +48,8 @@ class ScrapyCommand(object):\n         group = OptionGroup(parser, \"Global Options\")\n         group.add_option(\"-h\", \"--help\", action=\"store_true\", dest=\"help\", \\\n             help=\"print command help and options\")\n+        group.add_option(\"--version\", action=\"store_true\", dest=\"version\", \\\n+            help=\"print Scrapy version and exit\")\n         group.add_option(\"--logfile\", dest=\"logfile\", metavar=\"FILE\", \\\n             help=\"log file. if omitted stderr will be used\")\n         group.add_option(\"-L\", \"--loglevel\", dest=\"loglevel\", metavar=\"LEVEL\", \\\n@@ -75,6 +78,10 @@ class ScrapyCommand(object):\n         if opts.settings:\n             settings.set_settings_module(opts.settings)\n \n+        if opts.version:\n+            print scrapy.__version__\n+            sys.exit()\n+\n         if opts.logfile:\n             settings.overrides['LOG_ENABLED'] = True\n             settings.overrides['LOG_FILE'] = opts.logfile\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
