{"custom_id": "keras#af46ab0d4dc981b899fe6c82c1712e95f251a118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 24 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 49 | Churn Cumulative: 193 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.9\n\nDIFF:\n@@ -15,32 +15,39 @@ class Optimizer(object):\n     def get_updates(self, params, grads):\n         raise NotImplementedError\n \n-    def get_gradients(self, cost, params, regularizers, constraints):\n+    def get_gradients(self, cost, params, regularizers):\n         grads = T.grad(cost, params)\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n             norm = T.sqrt(sum([T.sum(g**2) for g in grads]))\n             grads = [clip_norm(g, c, norm) for g in grads]\n \n-        for p, g, r, c in zip(params, grads, regularizers, constraints):\n+        new_grads = []\n+        for p, g, r in zip(params, grads, regularizers):\n             g = r(g,p)\n-            p = c(p)\n+            new_grads.append(g)\n \n-        return grads, params\n+        return new_grads\n+\n+    def update_params(self, params, new_params, updates, constraint):\n+        new_params = constraint(new_params)\n+        updates.append((params, new_params))\n+        return updates\n \n \n class SGD(Optimizer):\n \n     def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n+        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n+        grads = self.get_gradients(cost, params, regularizers)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n-        for p, g in zip(params, grads):\n+        for p, g, c in zip(params, grads, constraints):\n             m = shared_zeros(p.get_value().shape) # momentum\n             v = self.momentum * m - lr * g # velocity\n             updates.append((m, v)) \n@@ -50,45 +57,47 @@ class SGD(Optimizer):\n             else:\n                 new_p = p + v\n \n-            updates.append((p, new_p))\n+            self.update_params(p, new_p, updates, c)\n         return updates\n \n \n class RMSprop(Optimizer):\n \n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n+        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n+        grads = self.get_gradients(cost, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a in zip(params, grads, accumulators):\n+        for p, g, a, c in zip(params, grads, accumulators, constraints):\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            updates.append((p, new_p))\n+            self.update_params(p, new_p, updates, c)\n         return updates\n \n \n class Adagrad(Optimizer):\n \n     def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n+        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n+        grads = self.get_gradients(cost, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a in zip(params, grads, accumulators):\n+        for p, g, a, c in zip(params, grads, accumulators, constraints):\n             new_a = a + g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            updates.append((p, new_p))\n+            self.update_params(p, new_p, updates, c)\n         return updates\n \n \n@@ -97,15 +106,16 @@ class Adadelta(Optimizer):\n         Reference: http://arxiv.org/abs/1212.5701\n     '''\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n+        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n+        grads = self.get_gradients(cost, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n+        for p, g, a, d_a, c in zip(params, grads, accumulators, delta_accumulators, constraints):\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n@@ -113,7 +123,7 @@ class Adadelta(Optimizer):\n             update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n \n             new_p = p - self.lr * update\n-            updates.append((p, new_p))\n+            self.update_params(p, new_p, updates, c)\n \n             # update delta_accumulator\n             new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n@@ -130,11 +140,12 @@ class Adam(Optimizer):\n         lambda is renamed kappa.\n     '''\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8, *args, **kwargs):\n+        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n     def get_updates(self, params, regularizers, constraints, cost):\n-        grads, params = self.get_gradients(cost, params, regularizers, constraints)\n+        grads = self.get_gradients(cost, params, regularizers)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n@@ -143,7 +154,7 @@ class Adam(Optimizer):\n         # the update below seems missing from the paper, but is obviously required\n         beta_2_t = self.beta_2 * (self.kappa**i) \n \n-        for p, g in zip(params, grads):\n+        for p, g, c in zip(params, grads, constraints):\n             m = theano.shared(p.get_value() * 0.) # zero init of moment\n             v = theano.shared(p.get_value() * 0.) # zero init of velocity\n \n@@ -157,7 +168,7 @@ class Adam(Optimizer):\n             \n             updates.append((m, m_t))\n             updates.append((v, v_t))\n-            updates.append((p, p_t))\n+            self.update_params(p, p_t, updates, c)\n         return updates\n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#865893bae2b63e643736ad41d4149082e161c273", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 45 | Churn Cumulative: 925 | Contributors (this commit): 10 | Commits (past 90d): 47 | Contributors (cumulative): 17 | DMM Complexity: 0.4444444444444444\n\nDIFF:\n@@ -5,7 +5,7 @@ import theano\n import theano.tensor as T\n \n from .. import activations, initializations\n-from ..regularizers import ident\n+from ..regularizers import identity\n from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n \n@@ -72,9 +72,11 @@ class Activation(Layer):\n     '''\n         Apply an activation function to an output.\n     '''\n-    def __init__(self, activation):\n+    def __init__(self, activation, target=0, beta=0.1):\n         super(Activation,self).__init__()\n         self.activation = activations.get(activation)\n+        self.target = target\n+        self.beta = beta\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -146,7 +148,7 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=ident, b_regularizer=ident, W_constraint=ident, b_constraint=ident):\n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=identity, b_regularizer=identity, W_constraint=identity, b_constraint=identity):\n         super(Dense,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n@@ -186,7 +188,7 @@ class TimeDistributedDense(Layer):\n        Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n \n     '''\n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=ident, b_regularizer=ident, W_constraint=ident, b_constraint=ident):\n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=identity, b_regularizer=identity, W_constraint=identity, b_constraint=identity):\n         super(TimeDistributedDense,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n\n@@ -62,7 +62,7 @@ class Sequential(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        updates = self.optimizer.get_updates(self.params, self.regularizer, self.constraint, train_loss)\n+        updates = self.optimizer.get_updates(self.params, self.regularizer, self.constraint, self.layers, train_loss)\n \n         self._train = theano.function([self.X, self.y], train_loss, \n             updates=updates, allow_input_downcast=True)\n\n@@ -11,11 +11,20 @@ def clip_norm(g, c, n):\n         g = T.switch(T.ge(n, c), g*c/n, g)\n     return g\n \n+def kl_divergence(p, p_hat):\n+    return p_hat - p + p*T.log(p/p_hat)\n+\n class Optimizer(object):\n     def get_updates(self, params, grads):\n         raise NotImplementedError\n \n-    def get_gradients(self, cost, params, regularizers):\n+    def get_gradients(self, cost, params, layers, regularizers):\n+        for layer in layers:\n+            if hasattr(layer, 'target') and layer.target > 0:\n+                avg_act = T.mean(layer.output(train=True), axis=0)\n+                kl_div = layer.beta*kl_divergence(layer.target, avg_act)\n+                cost = cost + T.sum(kl_div)\n+\n         grads = T.grad(cost, params)\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n@@ -42,8 +51,8 @@ class SGD(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, layers, cost):\n+        grads = self.get_gradients(cost, params, layers, regularizers)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n@@ -67,8 +76,8 @@ class RMSprop(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, layers, cost):\n+        grads = self.get_gradients(cost, params, layers, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -87,8 +96,8 @@ class Adagrad(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, layers, cost):\n+        grads = self.get_gradients(cost, params, layers, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -109,8 +118,8 @@ class Adadelta(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, layers, cost):\n+        grads = self.get_gradients(cost, params, layers, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n@@ -144,8 +153,8 @@ class Adam(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, cost):\n-        grads = self.get_gradients(cost, params, regularizers)\n+    def get_updates(self, params, regularizers, constraints, layers, cost):\n+        grads = self.get_gradients(cost, params, layers, regularizers)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n\n@@ -15,5 +15,5 @@ def l2(lam=.01):\n         return g\n     return l2wrap\n \n-def ident(g,*l):\n+def identity(g,*l):\n     return g\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#083bf1c71d84561b3ac76cda4c4a2897830d95e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -229,7 +229,7 @@ class ImageDataGenerator(object):\n             self.mean = np.mean(X, axis=0)\n             X -= self.mean\n         if self.featurewise_std_normalization:\n-            self.std = np.std(X)\n+            self.std = np.std(X, axis=0)\n             X /= self.std\n \n         if self.zca_whitening:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ca891b8e67a0a6eeea127b797fcd934e0049ec2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 169 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 169 | Churn Cumulative: 169 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.32857142857142857\n\nDIFF:\n@@ -0,0 +1,51 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import keras\n+from keras.datasets import mnist\n+import keras.models\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.regularizers import l2, l1\n+from keras.constraints import maxnorm\n+from keras.optimizers import SGD, Adam, RMSprop\n+\n+'''\n+    Train a (fairly simple) deep NN on the MNIST dataset.\n+\n+'''\n+\n+batch_size = 100\n+nb_classes = 10\n+nb_epoch = 200\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data(test_split=0.1)\n+print(X_train.shape[0], 'train samples')\n+print(X_test.shape[0], 'test samples')\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+model = Sequential()\n+model.add(Dense(784, 100, W_constraint=maxnorm(3)))\n+model.add(Activation('relu'))\n+model.add(Dropout(0.25))\n+model.add(Dense(100, 100))\n+model.add(Activation('relu'))\n+model.add(Dropout(0.25))\n+model.add(Dense(100, 10, W_constraint=maxnorm(3)))\n+model.add(Activation('softmax'))\n+\n+\n+# let's train the model using SGD + momentum (how original).\n+sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n+model.compile(loss='categorical_crossentropy', optimizer=sgd)\n+\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10)\n+score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n+print('Test score:', score)\n\n@@ -0,0 +1,118 @@\n+# -*- coding: utf-8 -*-\n+# modified from https://github.com/niboshi/mnist_loader.git\n+import os\n+import urllib\n+import urllib2\n+import StringIO\n+import gzip\n+import struct\n+import numpy as np\n+\n+def load_data():\n+    (X_train, y_train) = load(set_name='train')\n+    (X_test, y_test) = load(set_name='test')\n+\n+    return (X_train, y_train), (X_test, y_test)\n+    \n+    \n+def open_data_url(url, cache_dir=None):\n+    \"\"\"\n+    Opens an URL as a file-like object.\n+    \"\"\"\n+\n+    if cache_dir is None:\n+        cache_dir = os.path.expanduser(os.path.join('~', '.keras', 'datasets','mnist'))\n+\n+    cache_path = os.path.join(cache_dir, urllib.quote(url, safe=''))\n+\n+    if os.path.isfile(cache_path):\n+        return open(cache_path, 'rb')\n+    else:\n+        request = urllib2.Request(url)\n+        response = urllib2.urlopen(request)\n+        buf = StringIO.StringIO()\n+        block = 1024\n+        while True:\n+            data = response.read(block)\n+            if data is None:\n+                break\n+            if len(data) == 0:\n+                break\n+\n+            buf.write(data)\n+\n+        if not os.path.isdir(os.path.dirname(cache_path)):\n+            os.makedirs(os.path.dirname(cache_path))\n+        with open(cache_path, 'wb') as fo:\n+            fo.write(buf.getvalue())\n+\n+        buf.seek(0)\n+        return buf\n+\n+def read_data(file_in):\n+    \"\"\"\n+    Parses the IDX file format.\n+    \"\"\"\n+\n+    # Magic code\n+    magic = file_in.read(4)\n+    magic = [ord(_) for _ in magic]\n+    if len(magic) != 4 or magic[0] != 0 or magic[1] != 0:\n+        raise RuntimeError(\"Invalid magic number: [{}]\".format('-'.join(['{:02x}'.format(_) for _ in magic])))\n+\n+    # Type code\n+    type_code = magic[2]\n+    dtype_map = {\n+        0x08: np.uint8,\n+        0x09: np.int8,\n+        0x0B: np.int16,\n+        0x0C: np.int32,\n+        0x0D: np.float32,\n+        0x0E: np.float64,\n+    }\n+    dtype = dtype_map[type_code]\n+\n+    # Dimensions\n+    ndim = magic[3]\n+\n+    dims = []\n+    for idim in range(ndim):\n+        dim, = struct.unpack('>I', file_in.read(4))\n+        dims.append(dim)\n+\n+    # Data\n+    data = file_in.read()\n+    data = np.fromstring(data, dtype=dtype).reshape(tuple(dims))\n+\n+    return data\n+\n+def read_data_from_url(url, cache_dir=None):\n+    \"\"\"\n+    Extracts multidimensional data from an URL.\n+    \"\"\"\n+    return read_data(gzip.GzipFile(fileobj=open_data_url(url, cache_dir=cache_dir)))\n+\n+def load(set_name='train', cache_dir=None):\n+    \"\"\"\n+    Loads the MNIST data set.\n+    \"\"\"\n+\n+    if set_name == 'train':\n+        urls = (\n+            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n+            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n+        )\n+    elif set_name == 'test':\n+        urls = (\n+            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n+            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n+        )\n+    else:\n+        assert False, \"Invalid set name: {}\".format(set_name)\n+\n+    data_url, labels_url = urls\n+    data   = read_data_from_url(data_url, cache_dir=cache_dir)\n+    labels = read_data_from_url(labels_url, cache_dir=cache_dir)\n+\n+    return data, labels\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ca44264efeb4e62e7dd85b6be0aadb9333a4308", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 145 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 157 | Churn Cumulative: 208 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ from keras.layers.core import Dense, Dropout, Activation\n from keras.regularizers import l2, l1\n from keras.constraints import maxnorm\n from keras.optimizers import SGD, Adam, RMSprop\n+from keras.utils import np_utils, generic_utils\n \n '''\n     Train a (fairly simple) deep NN on the MNIST dataset.\n@@ -16,10 +17,16 @@ from keras.optimizers import SGD, Adam, RMSprop\n \n batch_size = 100\n nb_classes = 10\n-nb_epoch = 200\n+nb_epoch = 20\n \n # the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data(test_split=0.1)\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train=X_train.reshape(60000,784)\n+X_test=X_test.reshape(10000,784)\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n@@ -30,22 +37,19 @@ Y_test = np_utils.to_categorical(y_test, nb_classes)\n model = Sequential()\n model.add(Dense(784, 100, W_constraint=maxnorm(3)))\n model.add(Activation('relu'))\n-model.add(Dropout(0.25))\n+model.add(Dropout(0.1))\n model.add(Dense(100, 100))\n model.add(Activation('relu'))\n-model.add(Dropout(0.25))\n+model.add(Dropout(0.1))\n model.add(Dense(100, 10, W_constraint=maxnorm(3)))\n model.add(Activation('softmax'))\n \n \n # let's train the model using SGD + momentum (how original).\n-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n-model.compile(loss='categorical_crossentropy', optimizer=sgd)\n+rms = RMSprop()\n+model.compile(loss='categorical_crossentropy', optimizer=rms)\n \n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10)\n-score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2)\n+score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=2)\n print('Test score:', score)\n\n@@ -0,0 +1,129 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import keras\n+from keras.datasets import mnist\n+import keras.models\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.regularizers import l2, l1\n+from keras.constraints import maxnorm, nonneg\n+from keras.optimizers import SGD, Adam, RMSprop\n+from keras.utils import np_utils, generic_utils\n+import theano\n+import theano.tensor as T\n+import numpy as np\n+import scipy\n+\n+batch_size = 100\n+nb_classes = 10\n+nb_epoch = 10\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+X_train=X_train.reshape(60000,784)\n+X_test=X_test.reshape(10000,784)\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+model = Sequential()\n+model.add(Dense(784, 20, W_constraint=maxnorm(1)))\n+model.add(Activation('relu'))\n+model.add(Dropout(0.1))\n+model.add(Dense(20, 20, W_constraint=nonneg))\n+model.add(Activation('relu'))\n+model.add(Dropout(0.1))\n+model.add(Dense(20, 10, W_constraint=maxnorm(1)))\n+model.add(Activation('softmax'))\n+\n+\n+rms = RMSprop()\n+model.compile(loss='categorical_crossentropy', optimizer=rms)\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0)\n+\n+a=model.params[0].eval()\n+if np.isclose(np.max(np.sqrt(np.sum(a**2, axis=0))),1):\n+\tprint('Maxnorm test passed')\n+else:\n+\traise ValueError('Maxnorm test failed!')\n+\t\t\n+b=model.params[2].eval()\n+if np.min(b)==0 and np.min(a)!=0:\n+\tprint('Nonneg test passed')\n+else:\n+\traise ValueError('Nonneg test failed!')\n+\t\n+model = Sequential()\n+model.add(Dense(784, 20))\n+model.add(Activation('relu', target=.4))\n+model.add(Dropout(0.1))\n+model.add(Dense(20, 20))\n+model.add(Activation('relu', target=.3))\n+model.add(Dropout(0.1))\n+model.add(Dense(20, 10))\n+model.add(Activation('softmax'))\n+\n+\n+rms = RMSprop()\n+model.compile(loss='categorical_crossentropy', optimizer=rms)\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, show_accuracy=True, verbose=0)\n+\n+\n+\n+get_activations1 = theano.function([model.layers[0].input], model.layers[1].output(train=False), allow_input_downcast=True)\n+activations1 = get_activations1(X_train)\n+get_activations2 = theano.function([model.layers[0].input], model.layers[4].output(train=False), allow_input_downcast=True)\n+activations2 = get_activations2(X_train)\n+\n+if np.isclose(np.mean(activations1), .4, atol=.02) and np.isclose(np.mean(activations2), .3, atol=.02):\n+\tprint('KL penalty test passed')\n+else:\n+\traise ValueError('KL penalty test failed!')\n+\t\n+\n+\n+model = Sequential()\n+model.add(Dense(784, 20))\n+model.add(Activation('relu'))\n+model.add(Dense(20, 20, W_regularizer=l1(.01)))\n+model.add(Activation('relu'))\n+model.add(Dense(20, 10))\n+model.add(Activation('softmax'))\n+\n+\n+rms = RMSprop()\n+model.compile(loss='categorical_crossentropy', optimizer=rms)\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, show_accuracy=True, verbose=0)\n+\n+a=model.params[2].eval().reshape(400)\n+(D, p1) = scipy.stats.kurtosistest(a)\n+\n+model = Sequential()\n+model.add(Dense(784, 20))\n+model.add(Activation('relu'))\n+model.add(Dense(20, 20, W_regularizer=l2(.01)))\n+model.add(Activation('relu'))\n+model.add(Dense(20, 10))\n+model.add(Activation('softmax'))\n+\n+\n+rms = RMSprop()\n+model.compile(loss='categorical_crossentropy', optimizer=rms)\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, show_accuracy=True, verbose=0)\n+\n+a=model.params[2].eval().reshape(400)\n+(D, p2) = scipy.stats.kurtosistest(a)\n+\n+if p1<.01 and p2>.01:\n+\tprint('L1 and L2 regularization tests passed')\n+else:\n+\traise ValueError('L1 and L2 regularization tests failed!')\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83c6e73db56d283fb06f32e492020801aaa7fa05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 253 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,7 +84,9 @@ class Activation(Layer):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"activation\":self.activation.__name__}\n+            \"activation\":self.activation.__name__,\n+            \"target\":self.target,\n+            \"beta\":self.beta}\n \n \n class Reshape(Layer):\n@@ -104,7 +106,7 @@ class Reshape(Layer):\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"p\":self.p}\n+            \"dims\":self.dims}\n \n \n class Flatten(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#050fd9707b851d55c61b4da4b502ad37fde5e185", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 126 | Files Changed: 3 | Hunks: 22 | Methods Changed: 6 | Complexity Δ (Sum/Max): -16/0 | Churn Δ: 156 | Churn Cumulative: 383 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 0.8103448275862069\n\nDIFF:\n@@ -1,26 +1,27 @@\n from __future__ import absolute_import\n from __future__ import print_function\n-import keras\n from keras.datasets import mnist\n-import keras.models\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n from keras.regularizers import l2, l1\n from keras.constraints import maxnorm\n from keras.optimizers import SGD, Adam, RMSprop\n-from keras.utils import np_utils, generic_utils\n+from keras.utils import np_utils\n+import numpy as np\n \n '''\n-    Train a (fairly simple) deep NN on the MNIST dataset.\n-\n+    Train a simple deep NN on the MNIST dataset.\n '''\n \n-batch_size = 100\n+batch_size = 64\n nb_classes = 10\n nb_epoch = 20\n \n+np.random.seed(1337) # for reproducibility\n+\n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n X_train=X_train.reshape(60000,784)\n X_test=X_test.reshape(10000,784)\n X_train = X_train.astype(\"float32\")\n@@ -35,21 +36,19 @@ Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n model = Sequential()\n-model.add(Dense(784, 100, W_constraint=maxnorm(3)))\n+model.add(Dense(784, 128))\n model.add(Activation('relu'))\n-model.add(Dropout(0.1))\n-model.add(Dense(100, 100))\n+model.add(Dropout(0.2))\n+model.add(Dense(128, 128))\n model.add(Activation('relu'))\n-model.add(Dropout(0.1))\n-model.add(Dense(100, 10, W_constraint=maxnorm(3)))\n+model.add(Dropout(0.2))\n+model.add(Dense(128, 10))\n model.add(Activation('softmax'))\n \n-\n-# let's train the model using SGD + momentum (how original).\n rms = RMSprop()\n model.compile(loss='categorical_crossentropy', optimizer=rms)\n \n-\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2)\n-score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=2)\n-print('Test score:', score)\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))\n+score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+print('Test score:', score[0])\n+print('Test accuracy:', score[1])\n\n@@ -41,3 +41,4 @@ def get_file(fname, origin, untar=False):\n         return untar_fpath\n \n     return fpath\n+\n\n@@ -1,118 +1,22 @@\n # -*- coding: utf-8 -*-\n-# modified from https://github.com/niboshi/mnist_loader.git\n-import os\n-import urllib\n-import urllib2\n-import StringIO\n import gzip\n-import struct\n-import numpy as np\n+from .data_utils import get_file\n+import six.moves.cPickle\n+import sys\n \n-def load_data():\n-    (X_train, y_train) = load(set_name='train')\n-    (X_test, y_test) = load(set_name='test')\n+def load_data(path=\"mnist.pkl.gz\"):\n+    path = get_file(path, origin=\"https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\")\n \n-    return (X_train, y_train), (X_test, y_test)\n-    \n-    \n-def open_data_url(url, cache_dir=None):\n-    \"\"\"\n-    Opens an URL as a file-like object.\n-    \"\"\"\n-\n-    if cache_dir is None:\n-        cache_dir = os.path.expanduser(os.path.join('~', '.keras', 'datasets','mnist'))\n-\n-    cache_path = os.path.join(cache_dir, urllib.quote(url, safe=''))\n-\n-    if os.path.isfile(cache_path):\n-        return open(cache_path, 'rb')\n+    if path.endswith(\".gz\"):\n+        f = gzip.open(path, 'rb')\n     else:\n-        request = urllib2.Request(url)\n-        response = urllib2.urlopen(request)\n-        buf = StringIO.StringIO()\n-        block = 1024\n-        while True:\n-            data = response.read(block)\n-            if data is None:\n-                break\n-            if len(data) == 0:\n-                break\n+        f = open(path, 'rb')\n \n-            buf.write(data)\n-\n-        if not os.path.isdir(os.path.dirname(cache_path)):\n-            os.makedirs(os.path.dirname(cache_path))\n-        with open(cache_path, 'wb') as fo:\n-            fo.write(buf.getvalue())\n-\n-        buf.seek(0)\n-        return buf\n-\n-def read_data(file_in):\n-    \"\"\"\n-    Parses the IDX file format.\n-    \"\"\"\n-\n-    # Magic code\n-    magic = file_in.read(4)\n-    magic = [ord(_) for _ in magic]\n-    if len(magic) != 4 or magic[0] != 0 or magic[1] != 0:\n-        raise RuntimeError(\"Invalid magic number: [{}]\".format('-'.join(['{:02x}'.format(_) for _ in magic])))\n-\n-    # Type code\n-    type_code = magic[2]\n-    dtype_map = {\n-        0x08: np.uint8,\n-        0x09: np.int8,\n-        0x0B: np.int16,\n-        0x0C: np.int32,\n-        0x0D: np.float32,\n-        0x0E: np.float64,\n-    }\n-    dtype = dtype_map[type_code]\n-\n-    # Dimensions\n-    ndim = magic[3]\n-\n-    dims = []\n-    for idim in range(ndim):\n-        dim, = struct.unpack('>I', file_in.read(4))\n-        dims.append(dim)\n-\n-    # Data\n-    data = file_in.read()\n-    data = np.fromstring(data, dtype=dtype).reshape(tuple(dims))\n-\n-    return data\n-\n-def read_data_from_url(url, cache_dir=None):\n-    \"\"\"\n-    Extracts multidimensional data from an URL.\n-    \"\"\"\n-    return read_data(gzip.GzipFile(fileobj=open_data_url(url, cache_dir=cache_dir)))\n-\n-def load(set_name='train', cache_dir=None):\n-    \"\"\"\n-    Loads the MNIST data set.\n-    \"\"\"\n-\n-    if set_name == 'train':\n-        urls = (\n-            'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n-            'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n-        )\n-    elif set_name == 'test':\n-        urls = (\n-            'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n-            'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n-        )\n+    if sys.version_info < (3,):\n+        data = six.moves.cPickle.load(f)\n     else:\n-        assert False, \"Invalid set name: {}\".format(set_name)\n+        data = six.moves.cPickle.load(f, encoding=\"bytes\")\n \n-    data_url, labels_url = urls\n-    data   = read_data_from_url(data_url, cache_dir=cache_dir)\n-    labels = read_data_from_url(labels_url, cache_dir=cache_dir)\n-\n-    return data, labels\n+    f.close()\n \n+    return data # (X_train, y_train), (X_test, y_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3653f906ebf7d249bb7e0a07f991f495f56f13b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 53 | Files Changed: 2 | Hunks: 31 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 87 | Churn Cumulative: 744 | Contributors (this commit): 9 | Commits (past 90d): 33 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,16 +25,12 @@ class Sequential(object):\n     def __init__(self):\n         self.layers = []\n         self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n \n     def add(self, layer):\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n             self.layers[-1].connect(self.layers[-2])\n         self.params += [p for p in layer.params]\n-        self.regularizer += [r for r in layer.regularizer]\n-        self.constraint += [c for c in layer.constraint]\n \n     def compile(self, optimizer, loss, class_mode=\"categorical\"):\n         self.optimizer = optimizers.get(optimizer)\n@@ -62,7 +58,7 @@ class Sequential(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        updates = self.optimizer.get_updates(self.params, self.regularizer, self.constraint, self.layers, train_loss)\n+        updates = self.optimizer.get_updates(self.params, train_loss)\n \n         self._train = theano.function([self.X, self.y], train_loss, \n             updates=updates, allow_input_downcast=True)\n@@ -255,5 +251,3 @@ class Sequential(object):\n             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n             self.layers[k].set_weights(weights)\n         f.close()\n\\ No newline at end of file\n-\n-\n\n@@ -11,20 +11,11 @@ def clip_norm(g, c, n):\n         g = T.switch(T.ge(n, c), g*c/n, g)\n     return g\n \n-def kl_divergence(p, p_hat):\n-    return p_hat - p + p*T.log(p/p_hat)\n-\n class Optimizer(object):\n     def get_updates(self, params, grads):\n         raise NotImplementedError\n \n-    def get_gradients(self, cost, params, layers, regularizers):\n-        for layer in layers:\n-            if hasattr(layer, 'target') and layer.target > 0:\n-                avg_act = T.mean(layer.output(train=True), axis=0)\n-                kl_div = layer.beta*kl_divergence(layer.target, avg_act)\n-                cost = cost + T.sum(kl_div)\n-\n+    def get_gradients(self, cost, params):\n         grads = T.grad(cost, params)\n \n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n@@ -32,31 +23,34 @@ class Optimizer(object):\n             grads = [clip_norm(g, c, norm) for g in grads]\n \n         new_grads = []\n-        for p, g, r in zip(params, grads, regularizers):\n-            g = r(g,p)\n+        for p, g in zip(params, grads):\n+            if hasattr(self, 'l1') and self.l1 > 0:\n+                g += T.sgn(p) * self.l1\n+\n+            if hasattr(self, 'l2') and self.l2 > 0:\n+                g += p * self.l2\n+\n+            if hasattr(self, 'maxnorm') and self.maxnorm > 0:\n+                norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n+                desired = T.clip(norms, 0, self.maxnorm)\n+                p = p * (desired / (1e-7 + norms))\n+\n             new_grads.append(g)\n-\n         return new_grads\n \n-    def update_params(self, params, new_params, updates, constraint):\n-        new_params = constraint(new_params)\n-        updates.append((params, new_params))\n-        return updates\n-\n \n class SGD(Optimizer):\n \n     def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, *args, **kwargs):\n-        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n-        for p, g, c in zip(params, grads, constraints):\n+        for p, g in zip(params, grads):\n             m = shared_zeros(p.get_value().shape) # momentum\n             v = self.momentum * m - lr * g # velocity\n             updates.append((m, v)) \n@@ -65,48 +59,45 @@ class SGD(Optimizer):\n                 new_p = p + self.momentum * v - lr * g\n             else:\n                 new_p = p + v\n-\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, new_p))\n         return updates\n \n \n class RMSprop(Optimizer):\n \n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n-        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, c in zip(params, grads, accumulators, constraints):\n+        for p, g, a in zip(params, grads, accumulators):\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, new_p))\n         return updates\n \n \n class Adagrad(Optimizer):\n \n     def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n-        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, c in zip(params, grads, accumulators, constraints):\n+        for p, g, a in zip(params, grads, accumulators):\n             new_a = a + g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, new_p))\n         return updates\n \n \n@@ -115,16 +106,15 @@ class Adadelta(Optimizer):\n         Reference: http://arxiv.org/abs/1212.5701\n     '''\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n-        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n-        for p, g, a, d_a, c in zip(params, grads, accumulators, delta_accumulators, constraints):\n+        for p, g, a, d_a in zip(params, grads, accumulators, delta_accumulators):\n             new_a = self.rho * a + (1 - self.rho) * g ** 2 # update accumulator\n             updates.append((a, new_a))\n \n@@ -132,7 +122,7 @@ class Adadelta(Optimizer):\n             update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n \n             new_p = p - self.lr * update\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, new_p))\n \n             # update delta_accumulator\n             new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n@@ -143,18 +133,15 @@ class Adadelta(Optimizer):\n class Adam(Optimizer):\n     '''\n         Reference: http://arxiv.org/abs/1412.6980\n-\n         Default parameters follow those provided in the original paper\n-\n         lambda is renamed kappa.\n     '''\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8, *args, **kwargs):\n-        self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, cost):\n+        grads = self.get_gradients(cost, params)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n@@ -163,7 +150,7 @@ class Adam(Optimizer):\n         # the update below seems missing from the paper, but is obviously required\n         beta_2_t = self.beta_2 * (self.kappa**i) \n \n-        for p, g, c in zip(params, grads, constraints):\n+        for p, g in zip(params, grads):\n             m = theano.shared(p.get_value() * 0.) # zero init of moment\n             v = theano.shared(p.get_value() * 0.) # zero init of velocity\n \n@@ -177,7 +164,7 @@ class Adam(Optimizer):\n \n             updates.append((m, m_t))\n             updates.append((v, v_t))\n-            self.update_params(p, p_t, updates, c)\n+            updates.append((p, p_t))\n         return updates\n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b2acf76f352bc588a72e0b827d0b9667d43c2b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 45 | Files Changed: 5 | Hunks: 28 | Methods Changed: 20 | Complexity Δ (Sum/Max): 10/14 | Churn Δ: 113 | Churn Cumulative: 1147 | Contributors (this commit): 10 | Commits (past 90d): 56 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,3 +14,6 @@ def maxnorm(m=2):\n def nonneg(p):\n     p *= T.ge(p,0)\n     return p\n+\n+def identity(g):\n+    return g\n\\ No newline at end of file\n\n@@ -5,7 +5,6 @@ import theano\n import theano.tensor as T\n \n from .. import activations, initializations\n-from ..regularizers import identity\n from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n \n@@ -16,8 +15,6 @@ srng = RandomStreams()\n class Layer(object):\n     def __init__(self):\n         self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n \n     def connect(self, previous_layer):\n         self.previous_layer = previous_layer\n@@ -150,7 +147,9 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=identity, b_regularizer=identity, W_constraint=identity, b_constraint=identity):\n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n+        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+\n         super(Dense,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n@@ -163,8 +162,8 @@ class Dense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizer = [W_regularizer, b_regularizer]\n-        self.constraint = [W_constraint, b_constraint]\n+        self.regularizers = [W_regularizer, b_regularizer]\n+        self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -190,7 +189,9 @@ class TimeDistributedDense(Layer):\n        Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n \n     '''\n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, W_regularizer=identity, b_regularizer=identity, W_constraint=identity, b_constraint=identity):\n+    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n+        W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None):\n+\n         super(TimeDistributedDense,self).__init__()\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n@@ -203,8 +204,8 @@ class TimeDistributedDense(Layer):\n \n         self.params = [self.W, self.b]\n \n-        self.regularizer = [W_regularizer, b_regularizer]\n-        self.constraint = [W_constraint, b_constraint]\n+        self.regularizers = [W_regularizer, b_regularizer]\n+        self.constraints = [W_constraint, b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n\n@@ -6,6 +6,8 @@ import numpy as np\n \n from . import optimizers\n from . import objectives\n+from . import regularizers\n+from . import constraints\n import time, copy\n from .utils.generic_utils import Progbar\n from six.moves import range\n@@ -25,16 +27,37 @@ class Sequential(object):\n     def __init__(self):\n         self.layers = []\n         self.params = []\n-        self.regularizer = []\n-        self.constraint = []\n+        self.regularizers = []\n+        self.constraints = []\n \n     def add(self, layer):\n         self.layers.append(layer)\n         if len(self.layers) > 1:\n             self.layers[-1].connect(self.layers[-2])\n         self.params += [p for p in layer.params]\n-        self.regularizer += [r for r in layer.regularizer]\n-        self.constraint += [c for c in layer.constraint]\n+        \n+        if hasattr(layer, 'regularizers'):\n+            for r in layer.regularizers:\n+                if r:\n+                    self.regularizers.append(r)\n+                else:\n+                    self.regularizers.append(regularizers.identity)\n+        elif hasattr(layer, 'regularizer') and layer.regularizer:\n+            self.regularizers += [layer.regularizer for _ in range(len(layer.params))]\n+        else:\n+            self.regularizers += [regularizers.identity for _ in range(len(layer.params))]\n+\n+        if hasattr(layer, 'constraints'):\n+            for c in layer.constraints:\n+                if c:\n+                    self.constraints.append(c)\n+                else:\n+                    self.constraints.append(constraints.identity)\n+        elif hasattr(layer, 'constraint') and layer.constraint:\n+            self.constraints += [layer.constraint for _ in range(len(layer.params))]\n+        else:\n+            self.constraints += [constraints.identity for _ in range(len(layer.params))]\n+        \n \n     def compile(self, optimizer, loss, class_mode=\"categorical\"):\n         self.optimizer = optimizers.get(optimizer)\n@@ -62,7 +85,7 @@ class Sequential(object):\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n \n-        updates = self.optimizer.get_updates(self.params, self.regularizer, self.constraint, self.layers, train_loss)\n+        updates = self.optimizer.get_updates(self.params, self.regularizers, self.constraints, train_loss)\n \n         self._train = theano.function([self.X, self.y], train_loss, \n             updates=updates, allow_input_downcast=True)\n\n@@ -18,12 +18,12 @@ class Optimizer(object):\n     def get_updates(self, params, grads):\n         raise NotImplementedError\n \n-    def get_gradients(self, cost, params, layers, regularizers):\n-        for layer in layers:\n-            if hasattr(layer, 'target') and layer.target > 0:\n-                avg_act = T.mean(layer.output(train=True), axis=0)\n-                kl_div = layer.beta*kl_divergence(layer.target, avg_act)\n-                cost = cost + T.sum(kl_div)\n+    def get_gradients(self, cost, params, regularizers):\n+        # for layer in layers:\n+        #     if hasattr(layer, 'target') and layer.target > 0:\n+        #         avg_act = T.mean(layer.output(train=True), axis=0)\n+        #         kl_div = layer.beta*kl_divergence(layer.target, avg_act)\n+        #         cost = cost + T.sum(kl_div)\n \n         grads = T.grad(cost, params)\n \n@@ -38,11 +38,6 @@ class Optimizer(object):\n \n         return new_grads\n \n-    def update_params(self, params, new_params, updates, constraint):\n-        new_params = constraint(new_params)\n-        updates.append((params, new_params))\n-        return updates\n-\n \n class SGD(Optimizer):\n \n@@ -51,8 +46,8 @@ class SGD(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, regularizers, constraints, cost):\n+        grads = self.get_gradients(cost, params, regularizers)\n         lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n@@ -66,7 +61,7 @@ class SGD(Optimizer):\n             else:\n                 new_p = p + v\n \n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, c(new_p))) # apply constraints\n         return updates\n \n \n@@ -76,8 +71,8 @@ class RMSprop(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, regularizers, constraints, cost):\n+        grads = self.get_gradients(cost, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -86,7 +81,8 @@ class RMSprop(Optimizer):\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, c(new_p))) # apply constraints\n+            \n         return updates\n \n \n@@ -96,8 +92,8 @@ class Adagrad(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, regularizers, constraints, cost):\n+        grads = self.get_gradients(cost, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n \n@@ -106,7 +102,7 @@ class Adagrad(Optimizer):\n             updates.append((a, new_a))\n \n             new_p = p - self.lr * g / T.sqrt(new_a + self.epsilon)\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, c(new_p))) # apply constraints\n         return updates\n \n \n@@ -118,8 +114,8 @@ class Adadelta(Optimizer):\n         self.__dict__.update(kwargs)\n         self.__dict__.update(locals())\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, regularizers, constraints, cost):\n+        grads = self.get_gradients(cost, params, regularizers)\n         accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         delta_accumulators = [shared_zeros(p.get_value().shape) for p in params]\n         updates = []\n@@ -132,7 +128,7 @@ class Adadelta(Optimizer):\n             update = g * T.sqrt(d_a + self.epsilon) / T.sqrt(new_a + self.epsilon)\n \n             new_p = p - self.lr * update\n-            self.update_params(p, new_p, updates, c)\n+            updates.append((p, c(new_p))) # apply constraints\n \n             # update delta_accumulator\n             new_d_a = self.rho * d_a + (1 - self.rho) * update ** 2\n@@ -153,8 +149,8 @@ class Adam(Optimizer):\n         self.__dict__.update(locals())\n         self.iterations = shared_scalar(0)\n \n-    def get_updates(self, params, regularizers, constraints, layers, cost):\n-        grads = self.get_gradients(cost, params, layers, regularizers)\n+    def get_updates(self, params, regularizers, constraints, cost):\n+        grads = self.get_gradients(cost, params, regularizers)\n         updates = [(self.iterations, self.iterations+1.)]\n \n         i = self.iterations\n@@ -177,7 +173,7 @@ class Adam(Optimizer):\n             \n             updates.append((m, m_t))\n             updates.append((v, v_t))\n-            self.update_params(p, p_t, updates, c)\n+            updates.append((p, c(p_t))) # apply constraints\n         return updates\n \n # aliases\n\n@@ -3,17 +3,17 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n-def l1(lam=.01):\n+def l1(l=.01):\n     def l1wrap(g,p):\n-        g += T.sgn(p) * lam\n+        g += T.sgn(p) * l\n         return g\n     return l1wrap\n \n-def l2(lam=.01):\n+def l2(l=.01):\n     def l2wrap(g,p):\n-        g += p * lam\n+        g += p * l\n         return g\n     return l2wrap\n \n-def identity(g,*l):\n+def identity(g, p):\n     return g\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
