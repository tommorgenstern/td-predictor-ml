{"custom_id": "keras#3985029834daf54f58995feeebaa8610cc3b25cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 114 | Churn Cumulative: 4715 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,7 @@ import warnings\n from keras import backend\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n+from keras.engine import functional_utils\n from keras.engine import input_layer as input_layer_module\n from keras.engine import input_spec\n from keras.engine import node as node_module\n@@ -106,6 +107,14 @@ class Functional(training_lib.Model):\n       return\n     generic_utils.validate_kwargs(kwargs, {})\n     super(Functional, self).__init__(name=name, trainable=trainable)\n+    # Check if the inputs contain any intermediate `KerasTensor` (not created\n+    # by tf.keras.Input()). In this case we need to clone the `Node` and\n+    # `KerasTensor` objects to mimic rebuilding a new model from new inputs.\n+    # This feature is only enabled in TF2 not in v1 graph mode.\n+    if tf.compat.v1.executing_eagerly_outside_functions():\n+      if not all([functional_utils.is_input_keras_tensor(t)\n+                  for t in tf.nest.flatten(inputs)]):\n+        inputs, outputs = functional_utils.clone_graph_nodes(inputs, outputs)\n     self._init_graph_network(inputs, outputs)\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n\n@@ -634,15 +634,6 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n \n     model = training_lib.Model(inputs=[a, b], outputs=[c, d], name='model')\n \n-    # input is not an Input tensor\n-    j = layers.Input(shape=(32,), name='input_j')\n-    j = layers.Dense(32)(j)\n-    k = layers.Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-\n-    with self.assertRaises(Exception):\n-      training_lib.Model([j, k], [m, n])\n-\n     # disconnected graph\n     j = layers.Input(shape=(32,), name='input_j')\n     k = layers.Input(shape=(32,), name='input_k')\n\n@@ -167,7 +167,12 @@ def clone_graph_nodes(inputs, outputs):\n   cloned_inputs = tf.nest.pack_sequence_as(inputs, cloned_inputs)\n \n   for kt_output in tf.nest.flatten(outputs):\n-    cpy = keras_tensor.keras_tensor_from_type_spec(kt_output.type_spec)\n+    cpy = _clone_keras_tensor(kt_output)\n+    # We reuse the _keras_history here, which contains the old information. It\n+    # is used in the Node constructor to check if the tensor \"is_keras_tensor()\"\n+    # The history will be override by the Node constructor anyway for the\n+    # corresponding layer output anyway.\n+    cpy._keras_history = kt_output._keras_history  # pylint: disable=protected-access\n     cloned_outputs.append(cpy)\n     kt_id_mapping[id(kt_output)] = cpy\n   cloned_outputs = tf.nest.pack_sequence_as(outputs, cloned_outputs)\n@@ -216,6 +221,7 @@ def clone_keras_tensors(args, keras_tensor_mapping):\n       else:\n         # Create copy of keras_tensor if we haven't done it before\n         cpy = _clone_keras_tensor(obj)\n+        cpy._keras_history = obj._keras_history  # pylint: disable=protected-access\n         keras_tensor_mapping[id(obj)] = cpy\n       result.append(cpy)\n     else:\n@@ -224,7 +230,7 @@ def clone_keras_tensors(args, keras_tensor_mapping):\n \n \n def _clone_keras_tensor(kt):\n-  \"\"\"Create an idential keras_tensor based on the input.\n+  \"\"\"Create an identical keras_tensor based on the input.\n \n   We use keras_tensor_to_placeholder and keras_tensor_from_tensor to make sure\n   inferred shape are not lost during the copy.\n@@ -233,7 +239,7 @@ def _clone_keras_tensor(kt):\n     kt: the input KerasTensor.\n \n   Returns:\n-    An indential copy of the input KerasTensor.\n+    An identical copy of the input KerasTensor.\n   \"\"\"\n   # Create a scratch graph since we don't intend to use the placeholders.\n   with backend._scratch_graph() as scratch_graph:  # pylint: disable=protected-access\n\n@@ -14,6 +14,9 @@\n #,============================================================================\n \"\"\"Tests for functional_utils.\"\"\"\n \n+import collections\n+import os\n+\n from keras import keras_parameterized\n from keras import layers\n from keras import models\n@@ -117,18 +120,89 @@ class FunctionalModelSlideTest(keras_parameterized.TestCase):\n     layer2 = layers.Dense(16)\n     x = layer1(inputs)\n     y = layer2(x)\n-    cloned_inputs, cloned_outputs = functional_utils.clone_graph_nodes(x, y)\n-    # Make sure the inputs and outputs are cloned.\n-    self.assertIsNot(x, cloned_inputs)\n-    self.assertIsNot(y, cloned_outputs)\n+    model = models.Model(x, y)\n     # Make sure a new node is attached to layer2, which mimic y = layer2(x)\n     self.assertLen(layer2.inbound_nodes, 2)\n \n-    model = models.Model(cloned_inputs, cloned_outputs)\n     self.assertIsInstance(model, models.Model)\n+    # The model only contains 1 dense layer and 1 input layer.\n+    self.assertLen(model.layers, 2)\n+    self.assertIs(model.layers[1], layer2)\n \n     model.compile('rmsprop', 'mse')\n     model.fit(np.random.randn(batch_size, 32), np.random.randn(batch_size, 16))\n+    # Test for model saving\n+    output_path = os.path.join(self.get_temp_dir(), 'tf_keras_saved_model')\n+    model.save(output_path, save_format='tf')\n+    loaded_model = models.load_model(output_path)\n+    self.assertEqual(model.summary(), loaded_model.summary())\n+\n+    # Also make sure the orignal inputs and y can still be used to build model\n+    new_model = models.Model(inputs, y)\n+    # Make sure no new node is attached to layer2\n+    self.assertLen(layer2.inbound_nodes, 2)\n+\n+    self.assertLen(new_model.layers, 3)\n+    self.assertIs(new_model.layers[1], layer1)\n+    self.assertIs(new_model.layers[2], layer2)\n+\n+  def test_build_model_from_intermediate_tensor_with_complicated_model(self):\n+    # The topology is like below:\n+    # input1 -> dense1 -> a\n+    #                     + -> c - + --> d - + --> output\n+    # input2 -> dense1 -> b -------^         ^\n+    # input3 -> dense2 -> e -----------------|\n+    batch_size = 8\n+    input1 = input_layer_lib.Input((2,))\n+    input2 = input_layer_lib.Input((2,))\n+    input3 = input_layer_lib.Input((8,))\n+\n+    dense1 = layers.Dense(8, name='dense1')\n+    dense2 = layers.Dense(8, name='dense2')\n+\n+    # dense1 are shared between input1 and input2\n+    a = dense1(input1)\n+    b = dense1(input2)\n+\n+    c = layers.Add()([a, b])\n+    # d has a residual connection from b.\n+    d = layers.Add()([b, c])\n+    e = dense2(input3)\n+    output = layers.Add()([d, e])\n+\n+    # We skip the input2 here and use b instead.\n+    model = models.Model([input1, b, input3], output)\n+    # Make sure we have 8 layers, 3 for inputs, 2 for dense and 3 for Add.\n+    # Note that dense1 is still in use by input1.\n+    self.assertLen(model.layers, 8)\n+    # Since the layers are not ordered, let's check class of the layers to make\n+    # sure it match the expectation.\n+    class_count = collections.Counter([l.__class__ for l in model.layers])\n+    self.assertEqual(class_count[input_layer_lib.InputLayer], 3)\n+    self.assertEqual(class_count[layers.Dense], 2)\n+    self.assertEqual(class_count[layers.Add], 3)\n+\n+    model.compile('rmsprop', 'mse')\n+    model.fit([np.random.randn(batch_size, 2),\n+               np.random.randn(batch_size, 8),  # The shape of b is (batch, 8)\n+               np.random.randn(batch_size, 8)],\n+              np.random.randn(batch_size, 8))\n+    output_path = os.path.join(self.get_temp_dir(), 'tf_keras_saved_model')\n+    model.save(output_path, save_format='tf')\n+    loaded_model = models.load_model(output_path)\n+    self.assertEqual(model.summary(), loaded_model.summary())\n+\n+    model2 = models.Model([a, b], d)\n+    # 2 input layers and 2 Add layer.\n+    self.assertLen(model2.layers, 4)\n+    class_count = collections.Counter([l.__class__ for l in model2.layers])\n+    self.assertEqual(class_count[input_layer_lib.InputLayer], 2)\n+    self.assertEqual(class_count[layers.Add], 2)\n+\n+    model2.compile('rmsprop', 'mse')\n+    model2.fit([np.random.randn(batch_size, 8),\n+                np.random.randn(batch_size, 8)],\n+               np.random.randn(batch_size, 8))\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff38804195258385dff1ebe339ceddb6fd442f93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 223 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ import tensorflow.compat.v2 as tf\n \n class FunctionalModelSlideTest(keras_parameterized.TestCase):\n \n-  def testfind_nodes_by_inputs_and_outputs(self):\n+  def test_find_nodes_by_inputs_and_outputs(self):\n     inputs = input_layer_lib.Input((10,))\n     unconnected_inputs = input_layer_lib.Input((10,))\n     x = layers.Dense(8)(inputs)\n@@ -57,7 +57,7 @@ class FunctionalModelSlideTest(keras_parameterized.TestCase):\n       functional_utils.find_nodes_by_inputs_and_outputs(\n           [inputs, unconnected_inputs], output)\n \n-  def testfind_nodes_by_inputs_and_outputs_with_complicated_network(self):\n+  def test_find_nodes_by_inputs_and_outputs_with_complicated_network(self):\n     input1 = input_layer_lib.Input((10,))\n     input2 = input_layer_lib.Input((10,))\n     input3 = input_layer_lib.Input((10,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11aa8ed23905c8627d5807995654b3007ec487d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 33 | Churn Cumulative: 3383 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -323,6 +323,36 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       cbk = BackupAndRestore(self.get_temp_dir())\n       model.fit(np.ones((10, 1)), np.ones((10, 1)), epochs=0, callbacks=[cbk])\n \n+  def test_backup_restore_train_counter(self):\n+    model = keras.Sequential([keras.layers.Dense(1)])\n+    model.compile('sgd', 'mse')\n+    cbk = BackupAndRestore(self.get_temp_dir())\n+\n+    class InterruptingCallback(keras.callbacks.Callback):\n+      \"\"\"A callback to intentionally introduce interruption to training.\"\"\"\n+\n+      def on_epoch_end(self, epoch, log=None):\n+        logging.info(f'counter: {model._train_counter}')\n+        if epoch == 5 or epoch == 12:\n+          raise RuntimeError('Interruption')\n+\n+    log_dir = self.get_temp_dir()\n+\n+    # The following asserts that the train counter is fault tolerant.\n+    self.assertEqual(model._train_counter.numpy(), 0)\n+    try:\n+      model.fit(np.ones((10, 1)), np.ones((10, 1)), epochs=20,\n+                callbacks=[cbk, InterruptingCallback()])\n+    except RuntimeError:\n+      pass\n+    self.assertEqual(model._train_counter.numpy(), 6)\n+    try:\n+      model.fit(np.ones((10, 1)), np.ones((10, 1)), epochs=20,\n+                callbacks=[cbk, InterruptingCallback()])\n+    except RuntimeError:\n+      pass\n+    self.assertEqual(model._train_counter.numpy(), 13)\n+\n   @keras_parameterized.run_all_keras_modes\n   def test_callback_warning(self):\n \n\n@@ -52,7 +52,8 @@ class WorkerTrainingState:\n     # _ckpt_saved_epoch gets tracked and is included in the checkpoint file\n     # when backing up.\n     checkpoint = tf.train.Checkpoint(\n-        model=self._model, ckpt_saved_epoch=self._ckpt_saved_epoch)\n+        model=self._model, ckpt_saved_epoch=self._ckpt_saved_epoch,\n+        train_counter=self._model._train_counter)\n \n     # If this is single-worker training, checkpoint_dir are the same for\n     # write_checkpoint_manager and read_checkpoint_manager.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#117b4dce200258bd5dbcf89c627a5ea6bd6b6d94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 125 | Churn Cumulative: 276 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras initializers for TF 1.\"\"\"\n+# pylint:disable=g-classes-have-attributes\n \n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n@@ -69,8 +70,132 @@ class RandomUniform(tf.compat.v1.random_uniform_initializer):\n @keras_export(v1=['keras.initializers.TruncatedNormal',\n                   'keras.initializers.truncated_normal'])\n class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n+  \"\"\"Initializer that generates a truncated normal distribution.\n+\n+  These values are similar to values from a `random_normal_initializer`\n+  except that values more than two standard deviations from the mean\n+  are discarded and re-drawn. This is the recommended initializer for\n+  neural network weights and filters.\n+\n+  Args:\n+    mean: a python scalar or a scalar tensor. Mean of the random values to\n+      generate.\n+    stddev: a python scalar or a scalar tensor. Standard deviation of the\n+      random values to generate.\n+    seed: A Python integer. Used to create random seeds. See\n+      `tf.compat.v1.set_random_seed` for behavior.\n+    dtype: Default data type, used if no `dtype` argument is provided when\n+      calling the initializer. Only floating point types are supported.\n+\n+  @compatibility(TF2)\n+  Although it is a legacy compat.v1 api,\n+  `tf.compat.v1.keras.initializers.TruncatedNormal` is compatible with eager\n+  execution and `tf.function`.\n+\n+  To switch to native TF2, switch to using\n+  `tf.keras.initializers.TruncatedNormal` (not from `compat.v1`) and\n+  if you need to change the default dtype use\n+  `tf.keras.backend.set_floatx(float_dtype)`\n+  or pass the dtype when calling the initializer, rather than passing it\n+  when constructing the initializer.\n+\n+  Random seed behavior:\n+  Also be aware that as of TF 2.6, if you pass a seed to the TF2 initializer\n+  API it will reuse that same seed for every single initialization\n+  (unlike the TF1 intializer)\n+\n+  #### Structural Mapping to Native TF2\n+\n+  Before:\n+\n+  ```python\n+  initializer = tf.compat.v1.keras.initializers.TruncatedNormal(\n+    mean=mean,\n+    stddev=stddev,\n+    seed=seed,\n+    dtype=dtype)\n+\n+  weight_one = tf.Variable(initializer(shape_one))\n+  weight_two = tf.Variable(initializer(shape_two))\n+  ```\n+\n+  After:\n+\n+  ```python\n+  tf.random.set_seed(...) # set determinism globally if needed\n+\n+  initializer = tf.keras.initializers.TruncatedNormal(\n+    mean=mean,\n+    # seed=seed,  # Setting a seed in the native TF2 API\n+                  # causes it to produce the same initializations\n+                  # across multiple calls of the same initializer.\n+    stddev=stddev)\n+\n+  weight_one = tf.Variable(initializer(shape_one, dtype=dtype))\n+  weight_two = tf.Variable(initializer(shape_two, dtype=dtype))\n+  ```\n+\n+  #### How to Map Arguments\n+\n+  | TF1 Arg Name      | TF2 Arg Name    | Note                       |\n+  | :---------------- | :-------------- | :------------------------- |\n+  | `mean`            | `mean`          | No change to defaults |\n+  | `stddev`          | `stddev`        | No change to defaults |\n+  | `seed`            | `seed`          | Different random number generation |\n+  :                    :        : semantics (to change in a :\n+  :                    :        : future version). If set, the TF2 version :\n+  :                    :        : will use stateless random number :\n+  :                    :        : generation which will produce the exact :\n+  :                    :        : same initialization even across multiple :\n+  :                    :        : calls of the initializer instance. the :\n+  :                    :        : `compat.v1` version will generate new :\n+  :                    :        : initializations each time. Do not set :\n+  :                    :        : a seed if you need different          :\n+  :                    :        : initializations each time. Instead    :\n+  :                    :        : either set a global tf seed with\n+  :                    :        : `tf.random.set_seed` if you need :\n+  :                    :        : determinism, or initialize each weight :\n+  :                    :        : with a separate initializer instance  :\n+  :                    :        : and a different seed.                 :\n+  | `dtype`           | `dtype`  | The TF2 native api only takes it  |\n+  :                   :      : as a `__call__` arg, not a constructor arg. :\n+  | `partition_info`  | -    |  (`__call__` arg in TF1) Not supported      |\n+\n+  #### Example of fixed-seed behavior differences\n+\n+  `compat.v1` Fixed seed behavior:\n+\n+  >>> initializer = tf.compat.v1.keras.initializers.TruncatedNormal(seed=10)\n+  >>> a = initializer(shape=(2, 2))\n+  >>> b = initializer(shape=(2, 2))\n+  >>> tf.reduce_sum(a - b) == 0\n+  <tf.Tensor: shape=(), dtype=bool, numpy=False>\n+\n+  After:\n+\n+  >>> initializer = tf.keras.initializers.TruncatedNormal(seed=10)\n+  >>> a = initializer(shape=(2, 2))\n+  >>> b = initializer(shape=(2, 2))\n+  >>> tf.reduce_sum(a - b) == 0\n+  <tf.Tensor: shape=(), dtype=bool, numpy=True>\n+\n+  @end_compatibility\n+  \"\"\"\n \n   def __init__(self, mean=0.0, stddev=0.05, seed=None, dtype=tf.float32):\n+    \"\"\"Initializer that generates a truncated normal distribution.\n+\n+\n+    Args:\n+      mean: a python scalar or a scalar tensor. Mean of the random values to\n+        generate.\n+      stddev: a python scalar or a scalar tensor. Standard deviation of the\n+        random values to generate.\n+      seed: A Python integer. Used to create random seeds. See\n+        `tf.compat.v1.set_random_seed` for behavior.\n+      dtype: Default data type, used if no `dtype` argument is provided when\n+        calling the initializer. Only floating point types are supported.\n+    \"\"\"\n     super(TruncatedNormal, self).__init__(\n         mean=mean, stddev=stddev, seed=seed, dtype=dtype)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc91cef921e7ac542ff056122570e4e0b0518b21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 48 | Files Changed: 10 | Hunks: 21 | Methods Changed: 25 | Complexity Δ (Sum/Max): -4/3 | Churn Δ: 99 | Churn Cumulative: 28066 | Contributors (this commit): 114 | Commits (past 90d): 87 | Contributors (cumulative): 147 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -41,7 +41,7 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n       self.skipTest(\"Unused option.\")\n     model, _ = self._model_compile(strategy)\n     test_data = tf.data.Dataset.from_tensor_slices(\n-        [1., 2., 3., 1., 5., 1.]).repeat().batch(2)\n+        [[1.], [2.], [3.], [1.], [5.], [1.]]).repeat().batch(2)\n     model.predict(x=test_data, steps=3)\n \n   def testClusterCoordinatorSingleInstance(self, strategy, use_dataset_creator):\n\n@@ -166,7 +166,7 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n         all(predictions[0] == predictions[i] for i in [0, 1, 2, 4]))\n \n   def testModelPredictWithNumpyData(self, strategy):\n-    x = np.array([1., 2., 3., 1., 5., 1.])\n+    x = np.array([[1.], [2.], [3.], [1.], [5.], [1.]])\n     _, predictions = self._model_predict(strategy, test_data=x)\n \n     self.assertTrue(all(predictions[0] == predictions[i] for i in [0, 3, 5]))\n@@ -174,7 +174,7 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n         all(predictions[0] == predictions[i] for i in [0, 1, 2, 4]))\n \n   def testModelPredictWithTensorData(self, strategy):\n-    x = tf.constant([1., 2., 3., 1., 5., 1.])\n+    x = tf.constant([[1.], [2.], [3.], [1.], [5.], [1.]])\n     _, predictions = self._model_predict(strategy, test_data=x)\n     self.assertTrue(all(predictions[0] == predictions[i] for i in [0, 3, 5]))\n     self.assertFalse(\n@@ -232,7 +232,7 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n \n     def _dataset_fn(input_context):\n       del input_context\n-      x = tf.constant([1., 2., 3., 1., 5., 1.])\n+      x = tf.constant([[1.], [2.], [3.], [1.], [5.], [1.]])\n       return tf.data.Dataset.from_tensor_slices(x).repeat().batch(2)\n \n     _, predictions = self._model_predict(\n\n@@ -215,7 +215,7 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n       callbacks += default_callbacks\n \n     def create_test_data():\n-      x = tf.constant([1., 2., 3., 1., 5., 1.])\n+      x = tf.constant([[1.], [2.], [3.], [1.], [5.], [1.]])\n       return tf.data.Dataset.from_tensor_slices(x).repeat().batch(2)\n \n     if test_data is None:\n\n@@ -1443,30 +1443,15 @@ def _make_class_weight_map_fn(class_weight):\n     cw = tf.gather(class_weight_tensor, y_classes)\n     if sw is not None:\n       cw = tf.cast(cw, sw.dtype)\n-      sw, cw = expand_1d((sw, cw))\n       # `class_weight` and `sample_weight` are multiplicative.\n       sw = sw * cw\n     else:\n       sw = cw\n-\n     return x, y, sw\n \n   return _class_weights_map_fn\n \n \n-def expand_1d(data):\n-  \"\"\"Expands 1-dimensional `Tensor`s into 2-dimensional `Tensor`s.\"\"\"\n-\n-  def _expand_single_1d_tensor(t):\n-    # Leaves `CompositeTensor`s as-is.\n-    if (isinstance(t, tf.Tensor) and\n-        isinstance(t.shape, tf.TensorShape) and t.shape.rank == 1):\n-      return tf.expand_dims(t, axis=-1)\n-    return t\n-\n-  return tf.nest.map_structure(_expand_single_1d_tensor, data)\n-\n-\n def train_validation_split(arrays, validation_split):\n   \"\"\"Split arrays into train and validation subsets in deterministic order.\n \n\n@@ -1094,15 +1094,6 @@ class ListsOfScalarsDataAdapterTest(DataAdapterTestBase):\n     self.assertFalse(self.adapter_cls.can_handle([]))\n \n \n-class TestUtils(keras_parameterized.TestCase):\n-\n-  def test_expand_1d_sparse_tensors_untouched(self):\n-    st = tf.SparseTensor(\n-        indices=[[0], [10]], values=[1, 2], dense_shape=[10])\n-    st = data_adapter.expand_1d(st)\n-    self.assertEqual(st.shape.rank, 1)\n-\n-\n if __name__ == '__main__':\n   tf.compat.v1.enable_eager_execution()\n   tf.test.main()\n\n@@ -118,8 +118,8 @@ class Sequential(functional.Functional):\n     self._layer_call_argspecs = {}\n     self._created_nodes = set()\n     # Flag that indicate whether the sequential network topology has been\n-    # created. It is false when there isn't any layer, or the layers doesn't\n-    # have input shape.\n+    # created. It is false when there isn't any layer, or the layers don't\n+    # have an input shape.\n     self._graph_initialized = False\n \n     # Unfortunately some Sequential models using custom layers or FeatureColumn\n@@ -445,8 +445,8 @@ class Sequential(functional.Functional):\n   def input_spec(self):\n     if hasattr(self, '_manual_input_spec'):\n       return self._manual_input_spec\n-    if self.layers and hasattr(self.layers[0], 'input_spec'):\n-      return self.layers[0].input_spec\n+    if self._has_explicit_input_shape:\n+      return super().input_spec\n     return None\n \n   @input_spec.setter\n\n@@ -148,7 +148,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       itertools.chain(('_train_counter', '_test_counter', '_predict_counter',\n                        '_steps_per_execution'),\n                       base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES))  # pylint: disable=protected-access\n-  _SCALAR_UPRANKING_ON = True\n+  _SCALAR_UPRANKING_ON = False\n \n   def __new__(cls, *args, **kwargs):\n     # Signature detection\n@@ -776,9 +776,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       values of the `Model`'s metrics are returned. Example:\n       `{'loss': 0.2, 'accuracy': 0.7}`.\n     \"\"\"\n-    # These are the only transformations `Model.fit` applies to user-input\n-    # data when a `tf.data.Dataset` is provided.\n-    data = data_adapter.expand_1d(data)\n     x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n     # Run forward pass.\n     with tf.GradientTape() as tape:\n@@ -1265,7 +1262,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n       values of the `Model`'s metrics are returned.\n     \"\"\"\n-    data = data_adapter.expand_1d(data)\n     x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n \n     y_pred = self(x, training=False)\n@@ -1536,7 +1532,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       The result of one inference step, typically the output of calling the\n       `Model` on data.\n     \"\"\"\n-    data = data_adapter.expand_1d(data)\n     x, _, _ = data_adapter.unpack_x_y_sample_weight(data)\n     return self(x, training=False)\n \n\n@@ -197,8 +197,8 @@ class TrainingTest(keras_parameterized.TestCase):\n         layers_module.Dense(10, dtype=np.float64)\n     ]\n     model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n-    inputs = np.ones(10, dtype=np.float64)\n-    targets = np.ones(10, dtype=np.float64)\n+    inputs = np.ones(shape=(10, 1), dtype=np.float64)\n+    targets = np.ones(shape=(10, 1), dtype=np.float64)\n     model.compile(\n         'sgd',\n         loss=loss_fn,\n@@ -1111,8 +1111,9 @@ class TrainingTest(keras_parameterized.TestCase):\n         return inputs * 2\n \n     model = SubclassedModel()\n-    dataset_one = tf.data.Dataset.range(2).batch(2)\n-    dataset_two = tf.data.Dataset.range(3, 10).batch(2)\n+    dataset_one = tf.data.Dataset.from_tensor_slices([[0], [1]]).batch(2)\n+    dataset_two = tf.data.Dataset.from_tensor_slices(\n+        [[3], [4], [5], [6], [7], [8]]).batch(2)\n     self.assertAllEqual([[0], [2]], model.predict(dataset_one, steps=1))\n     self.assertAllEqual([[6], [8], [10], [12]],\n                         model.predict(dataset_two, steps=2))\n@@ -3892,5 +3893,37 @@ class TestBuildCustomModel(keras_parameterized.TestCase):\n     self.assertAllEqual(m1.get_weights(), m2.get_weights())\n \n \n+class ScalarDataModelTest(keras_parameterized.TestCase):\n+\n+  def test_scalar_loss_reduction(self):\n+\n+    class MyModel(training_module.Model):\n+\n+      def __init__(self):\n+        super().__init__()\n+        self.w = self.add_weight((), initializer='ones')\n+        self.b = self.add_weight((), initializer='zeros')\n+\n+      def call(self, inputs):\n+        return inputs * self.w + self.b\n+\n+    model = MyModel()\n+    model.compile(optimizer_v2.gradient_descent.SGD(1e-2),\n+                  loss='mse',\n+                  metrics=['binary_accuracy'])\n+    # learn y = x * 2 + 0.5\n+    x = np.array([3, 5, 5, 3, 5], dtype='float32')\n+    y = x * 2 + 0.5\n+    x2d = np.expand_dims(x, axis=-1)\n+    y2d = np.expand_dims(y, axis=-1)\n+    loss, acc = model.evaluate(x, y)\n+    loss2d, acc2d = model.evaluate(x2d, y2d)\n+    self.assertAllClose([loss, acc], [loss2d, acc2d], atol=1e-6)\n+    model.fit(x, y, epochs=20)\n+    preds = model.predict(x)\n+    self.assertEqual(preds.shape, (5,))\n+    self.assertAllClose(preds, y, atol=2e-1)\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -98,7 +98,8 @@ class WideDeepModel(keras_training.Model):\n       dnn_output = self.dnn_model(dnn_inputs, training=training)\n     else:\n       dnn_output = self.dnn_model(dnn_inputs)\n-    output = tf.nest.map_structure(lambda x, y: (x + y), linear_output, dnn_output)\n+    output = tf.nest.map_structure(\n+        lambda x, y: (x + y), linear_output, dnn_output)\n     if self.activation:\n       return tf.nest.map_structure(self.activation, output)\n     return output\n@@ -106,8 +107,6 @@ class WideDeepModel(keras_training.Model):\n   # This does not support gradient scaling and LossScaleOptimizer.\n   def train_step(self, data):\n     x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)\n-    x, y, sample_weight = data_adapter.expand_1d((x, y, sample_weight))\n-\n     with tf.GradientTape() as tape:\n       y_pred = self(x, training=True)\n       loss = self.compiled_loss(\n\n@@ -54,8 +54,8 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n       dnn_model = sequential.Sequential(\n           [core.Dense(units=1, kernel_initializer='zeros')])\n       wide_deep_model = wide_deep.WideDeepModel(linear_model, dnn_model)\n-      linear_inp = np.array([1.])\n-      dnn_inp = np.array([1.])\n+      linear_inp = np.array([[1.]])\n+      dnn_inp = np.array([[1.]])\n       inputs = [linear_inp, dnn_inp]\n       output = linear_inp + 2 * dnn_inp\n       linear_opt = gradient_descent.SGD(learning_rate=.1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f717f8cf0978b5e2fa3560870681e49dda248ef3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2030 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -231,7 +231,7 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n \n   The second variant is compatible with CuDNNGRU (GPU-only) and allows\n   inference on CPU. Thus it has separate biases for `kernel` and\n-  `recurrent_kernel`. To use this variant, set `'reset_after'=True` and\n+  `recurrent_kernel`. To use this variant, set `reset_after=True` and\n   `recurrent_activation='sigmoid'`.\n \n   For example:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf504f45533c5c01fd57fe8a1e50bc580b2b72d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 47 | Files Changed: 6 | Hunks: 45 | Methods Changed: 19 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 94 | Churn Cumulative: 21919 | Contributors (this commit): 76 | Commits (past 90d): 27 | Contributors (cumulative): 100 | DMM Complexity: None\n\nDIFF:\n@@ -233,21 +233,21 @@ from keras.layers.convolutional_recurrent import ConvLSTM1D\n from keras.layers.convolutional_recurrent import ConvLSTM2D\n from keras.layers.convolutional_recurrent import ConvLSTM3D\n \n-# CuDNN recurrent layers.\n+# cuDNN recurrent layers.\n from keras.layers.cudnn_recurrent import CuDNNLSTM\n from keras.layers.cudnn_recurrent import CuDNNGRU\n \n-# Wrapper functions\n+# Wrapper functions.\n from keras.layers.wrappers import Wrapper\n from keras.layers.wrappers import Bidirectional\n from keras.layers.wrappers import TimeDistributed\n \n-# # RNN Cell wrappers.\n+# RNN Cell wrappers.\n from keras.layers.rnn_cell_wrapper_v2 import DeviceWrapper\n from keras.layers.rnn_cell_wrapper_v2 import DropoutWrapper\n from keras.layers.rnn_cell_wrapper_v2 import ResidualWrapper\n \n-# Serialization functions\n+# Serialization functions.\n from keras.layers import serialization\n from keras.layers.serialization import deserialize\n from keras.layers.serialization import serialize\n\n@@ -245,7 +245,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n       gpu_model.set_weights(weights)\n       y_2 = gpu_model.predict(x_train)\n \n-    # Note that CuDNN uses 'sigmoid' as activation, so the GRU V2 uses\n+    # Note that cuDNN uses 'sigmoid' as activation, so the GRU V2 uses\n     # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve\n     # the same output.\n     with testing_utils.device(should_use_gpu=True):\n\n@@ -547,7 +547,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n       gpu_model.set_weights(weights)\n     y_2 = gpu_model.predict(x_train)\n \n-    # Note that CuDNN uses 'sigmoid' as activation, so the LSTM V2 uses\n+    # Note that cuDNN uses 'sigmoid' as activation, so the LSTM V2 uses\n     # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve\n     # the same output.\n     with testing_utils.device(should_use_gpu=True):\n@@ -1114,7 +1114,7 @@ class LSTMPerformanceTest(tf.test.Benchmark):\n                           extras=test_config)\n \n     logging.info('Expect the performance of LSTM V2 is within 80% of '\n-                 'CuDNN LSTM, got {0:.2f}%'.format(cudnn_vs_v2 * 100))\n+                 'cuDNN LSTM, got {0:.2f}%'.format(cudnn_vs_v2 * 100))\n     logging.info('Expect the performance of LSTM V2 is more than 5 times'\n                  ' of normal LSTM, got {0:.2f}'.format(v2_vs_normal))\n \n\n@@ -1740,7 +1740,7 @@ class GRUCell(DropoutRNNCellMixin, Layer):\n       the linear transformation of the recurrent state.\n     reset_after: GRU convention (whether to apply reset gate after or\n       before matrix multiplication). False = \"before\" (default),\n-      True = \"after\" (CuDNN compatible).\n+      True = \"after\" (cuDNN compatible).\n \n   Call arguments:\n     inputs: A 2D tensor.\n@@ -2056,7 +2056,7 @@ class GRU(RNN):\n       form.\n     reset_after: GRU convention (whether to apply reset gate after or\n       before matrix multiplication). False = \"before\" (default),\n-      True = \"after\" (CuDNN compatible).\n+      True = \"after\" (cuDNN compatible).\n \n   Call arguments:\n     inputs: A 3D tensor.\n\n@@ -149,7 +149,7 @@ class GRUCell(recurrent.GRUCell):\n       the linear transformation of the recurrent state. Default: 0.\n     reset_after: GRU convention (whether to apply reset gate after or\n       before matrix multiplication). False = \"before\",\n-      True = \"after\" (default and CuDNN compatible).\n+      True = \"after\" (default and cuDNN compatible).\n \n   Call arguments:\n     inputs: A 2D tensor, with shape of `[batch, feature]`.\n@@ -210,7 +210,7 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n   Based on available runtime hardware and constraints, this layer\n   will choose different implementations (cuDNN-based or pure-TensorFlow)\n   to maximize the performance. If a GPU is available and all\n-  the arguments to the layer meet the requirement of the CuDNN kernel\n+  the arguments to the layer meet the requirement of the cuDNN kernel\n   (see below for details), the layer will use a fast cuDNN implementation.\n \n   The requirements to use the cuDNN implementation are:\n@@ -311,7 +311,7 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n       form.\n     reset_after: GRU convention (whether to apply reset gate after or\n       before matrix multiplication). False = \"before\",\n-      True = \"after\" (default and CuDNN compatible).\n+      True = \"after\" (default and cuDNN compatible).\n \n   Call arguments:\n     inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n@@ -532,9 +532,9 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n   This implementation can be run on all types of hardware.\n \n   This implementation lifts out all the layer weights and make them function\n-  parameters. It has same number of tensor input params as the CuDNN\n+  parameters. It has same number of tensor input params as the cuDNN\n   counterpart. The RNN step logic has been simplified, eg dropout and mask is\n-  removed since CuDNN implementation does not support that.\n+  removed since cuDNN implementation does not support that.\n \n   Args:\n     inputs: Input tensor of GRU layer.\n@@ -610,7 +610,7 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n \n def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n             go_backwards, sequence_lengths):\n-  \"\"\"GRU with CuDNN implementation which is only available for GPU.\"\"\"\n+  \"\"\"GRU with cuDNN implementation which is only available for GPU.\"\"\"\n   if not time_major and mask is None:\n     inputs = tf.transpose(inputs, perm=(1, 0, 2))\n     seq_axis, batch_axis = (0, 1)\n@@ -627,8 +627,8 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n   bias = tf.split(backend.flatten(bias), 6)\n \n   if tf.sysconfig.get_build_info()['is_cuda_build']:\n-    # Note that the gate order for CuDNN is different from the canonical format.\n-    # canonical format is [z, r, h], whereas CuDNN is [r, z, h]. The swap need\n+    # Note that the gate order for cuDNN is different from the canonical format.\n+    # canonical format is [z, r, h], whereas cuDNN is [r, z, h]. The swap need\n     # to be done for kernel, recurrent_kernel, input_bias, recurrent_bias.\n     # z is update gate weights.\n     # r is reset gate weights.\n@@ -701,7 +701,7 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n \n   Under the hood, this function will create two TF function, one with the most\n   generic kernel and can run on all device condition, and the second one with\n-  CuDNN specific kernel, which can only run on GPU.\n+  cuDNN specific kernel, which can only run on GPU.\n \n   The first function will be called with normal_lstm_params, while the second\n   function is not called, but only registered in the graph. The Grappler will\n@@ -747,7 +747,7 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n   def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,\n                             mask, time_major, go_backwards, sequence_lengths,\n                             zero_output_for_mask):\n-    \"\"\"Use CuDNN kernel when mask is none or strictly right padded.\"\"\"\n+    \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n     if mask is None:\n       return gpu_gru(\n           inputs=inputs,\n@@ -815,7 +815,7 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n                                             gpu_gru_with_fallback,\n                                             supportive_attribute)\n \n-    # Call the normal GRU impl and register the CuDNN impl function. The\n+    # Call the normal GRU impl and register the cuDNN impl function. The\n     # grappler will kick in during session execution to optimize the graph.\n     last_output, outputs, new_h, runtime = defun_standard_gru(**params)\n     _function_register(defun_gpu_gru, **params)\n@@ -949,7 +949,7 @@ class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n   Based on available runtime hardware and constraints, this layer\n   will choose different implementations (cuDNN-based or pure-TensorFlow)\n   to maximize the performance. If a GPU is available and all\n-  the arguments to the layer meet the requirement of the CuDNN kernel\n+  the arguments to the layer meet the requirement of the cuDNN kernel\n   (see below for details), the layer will use a fast cuDNN implementation.\n \n   The requirements to use the cuDNN implementation are:\n@@ -1170,8 +1170,8 @@ class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n       # Use the new defun approach for backend implementation swap.\n       # Note that different implementations need to have same function\n       # signature, eg, the tensor parameters need to have same shape and dtypes.\n-      # Since the CuDNN has an extra set of bias, those bias will be passed to\n-      # both normal and CuDNN implementations.\n+      # Since the cuDNN has an extra set of bias, those bias will be passed to\n+      # both normal and cuDNN implementations.\n       self.reset_dropout_mask()\n       dropout_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n       if dropout_mask is not None:\n@@ -1276,12 +1276,12 @@ class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n \n \n def _canonical_to_params(weights, biases, shape, transpose_weights=False):\n-  \"\"\"Utility function convert variable to CuDNN compatible parameter.\n+  \"\"\"Utility function convert variable to cuDNN compatible parameter.\n \n-  Note that Keras weights for kernels are different from the CuDNN format. Eg.:\n+  Note that Keras weights for kernels are different from the cuDNN format. Eg.:\n \n   ```\n-    Keras                 CuDNN\n+    Keras                 cuDNN\n     [[0, 1, 2],  <--->  [[0, 2, 4],\n      [3, 4, 5]]          [1, 3, 5]]\n   ```\n@@ -1292,11 +1292,11 @@ def _canonical_to_params(weights, biases, shape, transpose_weights=False):\n   Args:\n     weights: list of weights for the individual kernels and recurrent kernels.\n     biases: list of biases for individual gate.\n-    shape: the shape for the converted variables that will be feed to CuDNN.\n+    shape: the shape for the converted variables that will be feed to cuDNN.\n     transpose_weights: boolean, whether to transpose the weights.\n \n   Returns:\n-    The converted weights that can be feed to CuDNN ops as param.\n+    The converted weights that can be feed to cuDNN ops as param.\n   \"\"\"\n   def convert(w):\n     return tf.transpose(w) if transpose_weights else w\n@@ -1314,12 +1314,12 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n   This implementation can be run on all types for hardware.\n \n   This implementation lifts out all the layer weights and make them function\n-  parameters. It has same number of tensor input params as the CuDNN\n+  parameters. It has same number of tensor input params as the cuDNN\n   counterpart. The RNN step logic has been simplified, eg dropout and mask is\n-  removed since CuDNN implementation does not support that.\n+  removed since cuDNN implementation does not support that.\n \n   Note that the first half of the bias tensor should be ignored by this impl.\n-  The CuDNN impl need an extra set of input gate bias. In order to make the both\n+  The cuDNN impl need an extra set of input gate bias. In order to make the both\n   function take same shape of parameter, that extra set of bias is also feed\n   here.\n \n@@ -1393,7 +1393,7 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n \n def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n              time_major, go_backwards, sequence_lengths):\n-  \"\"\"LSTM with either CuDNN or ROCm implementation which is only available for GPU.\n+  \"\"\"LSTM with either cuDNN or ROCm implementation which is only available for GPU.\n \n   Note that currently only right padded data is supported, or the result will be\n   polluted by the unmasked data which should be filtered.\n@@ -1440,7 +1440,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n \n   weights = tf.split(kernel, 4, axis=1)\n   weights += tf.split(recurrent_kernel, 4, axis=1)\n-  # CuDNN has an extra set of bias for inputs, we disable them (setting to 0),\n+  # cuDNN has an extra set of bias for inputs, we disable them (setting to 0),\n   # so that mathematically it is same as the canonical LSTM implementation.\n   full_bias = tf.concat((tf.zeros_like(bias), bias), 0)\n \n@@ -1524,7 +1524,7 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n \n   Under the hood, this function will create two TF function, one with the most\n   generic kernel and can run on all device condition, and the second one with\n-  CuDNN specific kernel, which can only run on GPU.\n+  cuDNN specific kernel, which can only run on GPU.\n \n   The first function will be called with normal_lstm_params, while the second\n   function is not called, but only registered in the graph. The Grappler will\n@@ -1572,7 +1572,7 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n   def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,\n                              bias, mask, time_major, go_backwards,\n                              sequence_lengths, zero_output_for_mask):\n-    \"\"\"Use CuDNN kernel when mask is none or strictly right padded.\"\"\"\n+    \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n     if mask is None:\n       return gpu_lstm(\n           inputs=inputs,\n@@ -1643,7 +1643,7 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n                                              gpu_lstm_with_fallback,\n                                              supportive_attribute)\n \n-    # Call the normal LSTM impl and register the CuDNN impl function. The\n+    # Call the normal LSTM impl and register the cuDNN impl function. The\n     # grappler will kick in during session execution to optimize the graph.\n     last_output, outputs, new_h, new_c, runtime = defun_standard_lstm(**params)\n     _function_register(defun_gpu_lstm, **params)\n@@ -1654,9 +1654,9 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n def is_sequence_right_padded(mask):\n   \"\"\"Check the mask tensor and see if it right padded.\n \n-  For CuDNN kernel, it uses the sequence length param to skip the tailing\n+  For cuDNN kernel, it uses the sequence length param to skip the tailing\n   timestep. If the data is left padded, or not a strict right padding (has\n-  masked value in the middle of the sequence), then CuDNN kernel won't be work\n+  masked value in the middle of the sequence), then cuDNN kernel won't be work\n   properly in those cases.\n \n   Left padded data: [[False, False, True, True, True]].\n\n@@ -229,7 +229,7 @@ def preprocess_weights_for_loading(layer,\n   \"\"\"Preprocess layer weights between different Keras formats.\n \n   Converts layers weights from Keras 1 format to Keras 2 and also weights of\n-  CuDNN layers in Keras 2.\n+  cuDNN layers in Keras 2.\n \n   Args:\n       layer: Layer instance.\n@@ -317,7 +317,7 @@ def preprocess_weights_for_loading(layer,\n \n   # Convert layers nested in Bidirectional/Model/Sequential.\n   # Both transformation should be ran for both Keras 1->2 conversion\n-  # and for conversion of CuDNN layers.\n+  # and for conversion of cuDNN layers.\n   if layer.__class__.__name__ == 'Bidirectional':\n     weights = convert_nested_bidirectional(weights)\n   if layer.__class__.__name__ == 'TimeDistributed':\n@@ -405,12 +405,12 @@ def preprocess_weights_for_loading(layer,\n       if layer.__class__.__name__ == 'ConvLSTM2D':\n         weights[1] = np.transpose(weights[1], (3, 2, 0, 1))\n \n-  # convert CuDNN layers\n+  # convert cuDNN layers\n   return _convert_rnn_weights(layer, weights)\n \n \n def _convert_rnn_weights(layer, weights):\n-  \"\"\"Converts weights for RNN layers between native and CuDNN format.\n+  \"\"\"Converts weights for RNN layers between native and cuDNN format.\n \n   Input kernels for each gate are transposed and converted between Fortran\n   and C layout, recurrent kernels are transposed. For LSTM biases are summed/\n@@ -448,12 +448,12 @@ def _convert_rnn_weights(layer, weights):\n     return np.hstack([func(k) for k in np.hsplit(kernels, n_gates)])\n \n   def transpose_input(from_cudnn):\n-    \"\"\"Makes a function that transforms input kernels from/to CuDNN format.\n+    \"\"\"Makes a function that transforms input kernels from/to cuDNN format.\n \n     It keeps the shape, but changes between the layout (Fortran/C). Eg.:\n \n     ```\n-    Keras                 CuDNN\n+    Keras                 cuDNN\n     [[0, 1, 2],  <--->  [[0, 2, 4],\n      [3, 4, 5]]          [1, 3, 5]]\n     ```\n@@ -461,7 +461,7 @@ def _convert_rnn_weights(layer, weights):\n     It can be passed to `transform_kernels()`.\n \n     Args:\n-        from_cudnn: `True` if source weights are in CuDNN format, `False`\n+        from_cudnn: `True` if source weights are in cuDNN format, `False`\n             if they're in plain Keras format.\n \n     Returns:\n@@ -498,7 +498,7 @@ def _convert_rnn_weights(layer, weights):\n \n       Args:\n         weights: Original weights.\n-        from_cudnn: Indicates whether original weights are from CuDNN layer.\n+        from_cudnn: Indicates whether original weights are from cuDNN layer.\n \n       Returns:\n         Updated weights compatible with LSTM.\n@@ -535,7 +535,7 @@ def _convert_rnn_weights(layer, weights):\n \n       Args:\n         weights: Original weights.\n-        from_cudnn: Indicates whether original weights are from CuDNN layer.\n+        from_cudnn: Indicates whether original weights are from cuDNN layer.\n \n       Returns:\n         Updated weights compatible with GRU.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
