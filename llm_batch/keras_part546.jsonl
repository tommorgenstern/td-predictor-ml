{"custom_id": "keras#2aa29d0ac8810c17776d1154d5338eaa3b05b3a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 955 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -281,6 +281,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         )\n \n \n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Not have other backend support yet.\",\n+)\n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n         x = KerasTensor([1, 2, 3])\n@@ -543,6 +547,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         )\n \n \n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Not have other backend support yet.\",\n+)\n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f21bf21eb43a90c78fc46b08e26dc1f487b05127", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 12 | Files Changed: 11 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 4383 | Contributors (this commit): 6 | Commits (past 90d): 114 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -434,7 +434,7 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n-                y_pred = self(data)\n+                self(data)\n                 break\n \n         # Container that configures and calls callbacks.\n\n@@ -58,7 +58,7 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n     def ndim(self):\n         return self.value.ndim\n \n-    def numpy(self):\n+    def numpy(self):  # noqa: F811\n         return self.value.numpy()\n \n     # Overload native accessor.\n\n@@ -4,6 +4,10 @@ from keras_core.losses.losses import LossFunctionWrapper\n from keras_core.losses.losses import MeanSquaredError\n \n \n+def deserialize(obj):\n+    raise NotImplementedError\n+\n+\n @keras_core_export(\"keras_core.losses.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras loss as a `function`/`Loss` class instance.\n\n@@ -54,7 +54,7 @@ class Loss:\n \n def standardize_reduction(reduction):\n     allowed = {\"sum_over_batch_size\", \"sum\", None}\n-    if not reduction in allowed:\n+    if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n             f\"Expected on of {allowed}. Received: \"\n\n@@ -6,6 +6,10 @@ from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n \n \n+def deserialize(obj):\n+    raise NotImplementedError\n+\n+\n @keras_core_export(\"keras_core.metrics.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n\n@@ -19,7 +19,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n             [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n         )\n \n-        update_op = mse_obj.update_state(y_true, y_pred)\n+        mse_obj.update_state(y_true, y_pred)\n         result = mse_obj.result()\n         self.assertAllClose(0.5, result, atol=1e-5)\n \n\n@@ -148,7 +148,6 @@ class Functional(Function, Model):\n \n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n-        names = [x.name for x in self._inputs]\n         adjusted = []\n         for x, ref_shape in zip(flat_inputs, flat_ref_shapes):\n             x_rank = len(x.shape)\n\n@@ -287,7 +287,7 @@ class Optimizer:\n     def _filter_empty_gradients(self, grads_and_vars):\n         filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n         if not filtered:\n-            raise ValueError(f\"No gradients provided for any variable.\")\n+            raise ValueError(\"No gradients provided for any variable.\")\n         if len(filtered) < len(grads_and_vars):\n             missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n             warnings.warn(\n\n@@ -30,5 +30,5 @@ class RegularizersTest(testing.TestCase):\n     def test_orthogonal_regularizer(self):\n         value = np.random.random((4, 4))\n         x = backend.Variable(value)\n-        y = regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n+        regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n         # TODO\n\n@@ -410,7 +410,7 @@ class CompileLoss(losses_module.Loss):\n                     raise ValueError(\n                         f\"When there is only a single output, the `loss_weights` argument \"\n                         \"must be a Python float. \"\n-                        f\"Received instead:\\loss_weights={loss_weights} of type {type(loss_weights)}\"\n+                        f\"Received instead: loss_weights={loss_weights} of type {type(loss_weights)}\"\n                     )\n                 flat_loss_weights.append(loss_weights)\n             else:\n@@ -447,7 +447,7 @@ class CompileLoss(losses_module.Loss):\n                         \"For a model with multiple outputs, \"\n                         f\"when providing the `loss_weights` argument as a list, \"\n                         \"it should have as many entries as the model has outputs. \"\n-                        f\"Received:\\loss_weights={loss_weights}\\nof length {len(loss_weights)} \"\n+                        f\"Received: loss_weights={loss_weights} of length {len(loss_weights)} \"\n                         f\"whereas the model has {len(y_pred)} outputs.\"\n                     )\n                 if not all(isinstance(e, float) for e in loss_weights):\n@@ -503,7 +503,7 @@ class CompileLoss(losses_module.Loss):\n                         raise ValueError(\n                             f\"In the dict argument `loss_weights`, key \"\n                             f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received:\\loss_weights={loss_weights}\"\n+                            f\"Received: loss_weights={loss_weights}\"\n                         )\n                     if not isinstance(loss_weights[name], float):\n                         raise ValueError(\n\n@@ -138,8 +138,8 @@ class Trainer:\n             loss = self._compile_loss(y, y_pred, sample_weight)\n             if loss is not None:\n                 losses.append(loss)\n-        for l in self.losses:\n-            losses.append(ops.cast(l, dtype=backend.floatx()))\n+        for loss in self.losses:\n+            losses.append(ops.cast(loss, dtype=backend.floatx()))\n         if len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7951fb1e6a1c03af233dd33e85a785851b23fd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 213 | Lines Deleted: 1821 | Files Changed: 17 | Hunks: 36 | Methods Changed: 68 | Complexity Δ (Sum/Max): -119/9 | Churn Δ: 2034 | Churn Cumulative: 9158 | Contributors (this commit): 6 | Commits (past 90d): 143 | Contributors (cumulative): 55 | DMM Complexity: 0.2890499194847021\n\nDIFF:\n@@ -61,45 +61,3 @@ def softmax(x):\n \n def log_softmax(x, axis=-1):\n     return jnn.log_softmax(x, axis=axis)\n-\n-\n-def max_pool(inputs, pool_size, strides, padding):\n-    # TODO: Implement `max_pool` with JAX ops.\n-    raise NotImplementedError\n-\n-\n-def average_pool(inputs, pool_size, strides, padding):\n-    # TODO: Implement `average_pool` with JAX ops.\n-    raise NotImplementedError\n-\n-\n-def conv(inputs, kernel, strides, padding, dilation_rate=None):\n-    # TODO: Add missing args.\n-    return jax.lax.conv_general_dilated(\n-        inputs, kernel, strides, padding, rhs_dilation=dilation_rate\n-    )\n-\n-\n-def depthwise_conv(inputs, filter, strides, padding):\n-    # TODO: Implement `depthwise_conv` with `conv_general_dilated`.\n-    raise NotImplementedError\n-\n-\n-def separable_conv(\n-    inputs, depthwise_kernel, pointwise_kernel, strides, padding\n-):\n-    # TODO: Implement `separable_conv` with `conv_general_dilated`.\n-    raise NotImplementedError\n-\n-\n-def conv_transpose(\n-    inputs,\n-    kernel,\n-    strides,\n-    output_padding,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    # TODO: Implement `conv_transpose`.\n-    raise NotImplementedError\n\n@@ -434,7 +434,7 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n-                self(data)\n+                y_pred = self(data)\n                 break\n \n         # Container that configures and calls callbacks.\n\n@@ -58,7 +58,7 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n     def ndim(self):\n         return self.value.ndim\n \n-    def numpy(self):  # noqa: F811\n+    def numpy(self):\n         return self.value.numpy()\n \n     # Overload native accessor.\n\n@@ -1,16 +1,17 @@\n import tensorflow as tf\n+from tensorflow import nn as tfnn\n \n \n def relu(x):\n-    return tf.nn.relu(x)\n+    return tfnn.relu(x)\n \n \n def relu6(x):\n-    return tf.nn.relu6(x)\n+    return tfnn.relu6(x)\n \n \n def sigmoid(x):\n-    return tf.nn.sigmoid(x)\n+    return tfnn.sigmoid(x)\n \n \n def softplus(x):\n@@ -18,11 +19,11 @@ def softplus(x):\n \n \n def softsign(x):\n-    return tf.nn.softsign(x)\n+    return tfnn.softsign(x)\n \n \n def silu(x, beta=1.0):\n-    return tf.nn.silu(x, beta=beta)\n+    return tfnn.silu(x, beta=beta)\n \n \n def swish(x):\n@@ -34,7 +35,7 @@ def log_sigmoid(x):\n \n \n def leaky_relu(x, negative_slope=0.2):\n-    return tf.nn.leaky_relu(x, alpha=negative_slope)\n+    return tfnn.leaky_relu(x, alpha=negative_slope)\n \n \n def hard_sigmoid(x):\n@@ -43,350 +44,20 @@ def hard_sigmoid(x):\n \n \n def elu(x):\n-    return tf.nn.elu(x)\n+    return tfnn.elu(x)\n \n \n def selu(x):\n-    return tf.nn.selu(x)\n+    return tfnn.selu(x)\n \n \n def gelu(x, approximate=True):\n-    return tf.nn.gelu(x, approximate)\n+    return tfnn.gelu(x, approximate)\n \n \n def softmax(x, axis=None):\n-    return tf.nn.softmax(x, axis=axis)\n+    return tfnn.softmax(x, axis=axis)\n \n \n def log_softmax(x, axis=None):\n-    return tf.nn.log_softmax(x, axis=axis)\n-\n-\n-def max_pool(\n-    inputs,\n-    pool_size,\n-    strides=None,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-):\n-    strides = pool_size if strides is None else strides\n-    padding = padding.upper()\n-    data_format = _convert_data_format(data_format, len(inputs.shape))\n-    return tf.nn.max_pool(inputs, pool_size, strides, padding, data_format)\n-\n-\n-def average_pool(\n-    inputs,\n-    pool_size,\n-    strides=None,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-):\n-    strides = pool_size if strides is None else strides\n-    padding = padding.upper()\n-    data_format = _convert_data_format(data_format, len(inputs.shape))\n-    return tf.nn.avg_pool(inputs, pool_size, strides, padding, data_format)\n-\n-\n-def _convert_data_format(data_format, ndim):\n-    if data_format == \"channels_last\":\n-        if ndim == 3:\n-            return \"NWC\"\n-        elif ndim == 4:\n-            return \"NHWC\"\n-        elif ndim == 5:\n-            return \"NDHWC\"\n-        else:\n-            raise ValueError(\n-                f\"Input rank not supported: {ndim}. \"\n-                \"Expected values are [3, 4, 5]\"\n-            )\n-    elif data_format == \"channels_first\":\n-        if ndim == 3:\n-            return \"NCW\"\n-        elif ndim == 4:\n-            return \"NCHW\"\n-        elif ndim == 5:\n-            return \"NCDHW\"\n-        else:\n-            raise ValueError(\n-                f\"Input rank not supported: {ndim}. \"\n-                \"Expected values are [3, 4, 5]\"\n-            )\n-    else:\n-        raise ValueError(\n-            f\"Invalid data_format: {data_format}. \"\n-            'Expected values are [\"channels_first\", \"channels_last\"]'\n-        )\n-\n-\n-def conv(\n-    inputs,\n-    kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    data_format=\"channel_last\",\n-    dilation_rate=1,\n-):\n-    \"\"\"General N-D convolution function.\n-\n-    Arg:\n-    \"\"\"\n-\n-    data_format = _convert_data_format(data_format, len(inputs.shape))\n-    padding = padding.upper()\n-\n-    return tf.nn.convolution(\n-        inputs,\n-        kernel,\n-        strides,\n-        padding,\n-        data_format=data_format,\n-        dilations=dilation_rate,\n-    )\n-\n-\n-def depthwise_conv(\n-    inputs,\n-    kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    num_spatial_dims = len(inputs.shape) - 2\n-    if num_spatial_dims > 2:\n-        raise ValueError(\n-            \"`inputs` rank must be 3 (1D conv) or 4 (2D conv). Received: \"\n-            \"{inputs.ndim}.\"\n-        )\n-    tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n-    padding = padding.upper()\n-    if isinstance(strides, int):\n-        strides = (strides,) * num_spatial_dims\n-    if isinstance(dilation_rate, int):\n-        dilation_rate = (dilation_rate,) * num_spatial_dims\n-    if num_spatial_dims == 1:\n-        # 1D depthwise conv.\n-        if data_format == \"channels_last\":\n-            strides = (1,) + strides * 2 + (1,)\n-            spatial_start_dim = 1\n-        else:\n-            strides = (1, 1) + strides * 2\n-            spatial_start_dim = 2\n-        inputs = tf.expand_dims(inputs, spatial_start_dim)\n-        kernel = tf.expand_dims(kernel, axis=0)\n-\n-        dilation_rate = None if dilation_rate is None else (1,) + dilation_rate\n-\n-        outputs = tf.nn.depthwise_conv2d(\n-            inputs,\n-            kernel,\n-            strides,\n-            padding,\n-            data_format=tf_data_format,\n-            dilations=dilation_rate,\n-        )\n-        return tf.squeeze(outputs, [spatial_start_dim])\n-\n-    if data_format == \"channels_last\":\n-        strides = (1,) + strides + (1,)\n-        spatial_start_dim = 1\n-    else:\n-        strides = (1, 1) + strides\n-        spatial_start_dim = 2\n-    return tf.nn.depthwise_conv2d(\n-        inputs,\n-        kernel,\n-        strides,\n-        padding,\n-        data_format=tf_data_format,\n-        dilations=dilation_rate,\n-    )\n-\n-\n-def separable_conv(\n-    inputs,\n-    depthwise_kernel,\n-    pointwise_kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    num_spatial_dims = len(inputs.shape) - 2\n-    if num_spatial_dims > 2:\n-        raise ValueError(\n-            \"`num_spatial_dims` must be 1 or 2. Received: \"\n-            f\"num_spatial_dims={num_spatial_dims}.\"\n-        )\n-    tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n-    padding = padding.upper()\n-    if isinstance(strides, int):\n-        strides = (strides,) * num_spatial_dims\n-    if isinstance(dilation_rate, int):\n-        dilation_rate = (dilation_rate,) * num_spatial_dims\n-    if num_spatial_dims == 1:\n-        # 1D depthwise conv.\n-        if data_format == \"channels_last\":\n-            strides = (1,) + strides * 2 + (1,)\n-            spatial_start_dim = 1\n-        else:\n-            strides = (1, 1) + strides * 2\n-            spatial_start_dim = 2\n-        inputs = tf.expand_dims(inputs, spatial_start_dim)\n-        depthwise_kernel = tf.expand_dims(depthwise_kernel, axis=0)\n-        pointwise_kernel = tf.expand_dims(pointwise_kernel, axis=0)\n-        dilation_rate = None if dilation_rate is None else (1,) + dilation_rate\n-\n-        outputs = tf.nn.separable_conv2d(\n-            inputs,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            strides,\n-            padding,\n-            data_format=tf_data_format,\n-            dilations=dilation_rate,\n-        )\n-        return tf.squeeze(outputs, [spatial_start_dim])\n-\n-    if data_format == \"channels_last\":\n-        strides = (1,) + strides + (1,)\n-        spatial_start_dim = 1\n-    else:\n-        strides = (1, 1) + strides\n-        spatial_start_dim = 2\n-    return tf.nn.separable_conv2d(\n-        inputs,\n-        depthwise_kernel,\n-        pointwise_kernel,\n-        strides,\n-        padding,\n-        data_format=tf_data_format,\n-        dilations=dilation_rate,\n-    )\n-\n-\n-def _deconv_output_length(\n-    input_length,\n-    kernel_size,\n-    padding,\n-    output_padding=None,\n-    stride=1,\n-    dilation=1,\n-):\n-    \"\"\"Determines output length of a transposed convolution given input length.\n-\n-    Args:\n-        input_length: Integer.\n-        kernel_size: Integer.\n-        padding: one of `\"same\"` or `\"valid\"`.\n-        output_padding: Integer, amount of padding along the output dimension.\n-          Can be set to `None` in which case the output length is inferred.\n-        stride: Integer.\n-        dilation: Integer.\n-\n-    Returns:\n-        The output length (integer).\n-    \"\"\"\n-    assert padding in {\"same\", \"valid\"}\n-    if input_length is None:\n-        return None\n-\n-    # Get the dilated kernel size\n-    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n-\n-    # Infer length if output padding is None, else compute the exact length\n-    if output_padding is None:\n-        if padding == \"valid\":\n-            length = input_length * stride + max(kernel_size - stride, 0)\n-        else:\n-            length = input_length * stride\n-    else:\n-        if padding == \"same\":\n-            pad = kernel_size // 2\n-        else:\n-            pad = 0\n-\n-        length = (\n-            (input_length - 1) * stride + kernel_size - 2 * pad + output_padding\n-        )\n-    return length\n-\n-\n-def compute_output_shape_conv_transpose(\n-    inputs,\n-    kernel,\n-    strides,\n-    padding,\n-    output_padding=None,\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    num_spatial_dims = len(inputs.shape) - 2\n-    kernel_spatial_shape = kernel.shape[:-2]\n-\n-    if isinstance(output_padding, int):\n-        output_padding = (output_padding,) * len(kernel_spatial_shape)\n-    if isinstance(strides, int):\n-        strides = (strides,) * num_spatial_dims\n-    if isinstance(dilation_rate, int):\n-        dilation_rate = (dilation_rate,) * num_spatial_dims\n-\n-    if data_format == \"channels_last\":\n-        inputs_spatial_shape = inputs.shape[1:-1]\n-    else:\n-        inputs_spatial_shape = inputs.shape[2:]\n-\n-    output_shape = []\n-    for i in range(num_spatial_dims):\n-        current_output_padding = (\n-            None if output_padding is None else output_padding[i]\n-        )\n-        output_shape.append(\n-            _deconv_output_length(\n-                inputs_spatial_shape[i],\n-                kernel_spatial_shape[i],\n-                padding=padding,\n-                output_padding=current_output_padding,\n-                stride=strides[i],\n-                dilation=dilation_rate[0],\n-            )\n-        )\n-\n-    if data_format == \"channels_last\":\n-        output_shape = [inputs.shape[0]] + output_shape + [kernel.shape[-2]]\n-    else:\n-        output_shape = [inputs.shape[0], kernel.shape[-1]] + output_shape\n-    return output_shape\n-\n-\n-def conv_transpose(\n-    inputs,\n-    kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    output_padding=None,\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n-    output_shape = compute_output_shape_conv_transpose(\n-        inputs,\n-        kernel,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n-    )\n-\n-    return tf.nn.conv_transpose(\n-        inputs,\n-        kernel,\n-        output_shape,\n-        strides,\n-        padding=padding.upper(),\n-        data_format=tf_data_format,\n-        dilations=dilation_rate,\n-    )\n+    return tfnn.log_softmax(x, axis=axis)\n\n@@ -4,10 +4,6 @@ from keras_core.losses.losses import LossFunctionWrapper\n from keras_core.losses.losses import MeanSquaredError\n \n \n-def deserialize(obj):\n-    raise NotImplementedError\n-\n-\n @keras_core_export(\"keras_core.losses.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras loss as a `function`/`Loss` class instance.\n\n@@ -54,7 +54,7 @@ class Loss:\n \n def standardize_reduction(reduction):\n     allowed = {\"sum_over_batch_size\", \"sum\", None}\n-    if reduction not in allowed:\n+    if not reduction in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n             f\"Expected on of {allowed}. Received: \"\n\n@@ -6,10 +6,6 @@ from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n \n \n-def deserialize(obj):\n-    raise NotImplementedError\n-\n-\n @keras_core_export(\"keras_core.metrics.get\")\n def get(identifier):\n     \"\"\"Retrieves a Keras metric as a `function`/`Metric` class instance.\n\n@@ -19,7 +19,7 @@ class MeanSquaredErrorTest(testing.TestCase):\n             [[0, 0, 1, 1, 0], [1, 1, 1, 1, 1], [0, 1, 0, 1, 0], [1, 1, 1, 1, 1]]\n         )\n \n-        mse_obj.update_state(y_true, y_pred)\n+        update_op = mse_obj.update_state(y_true, y_pred)\n         result = mse_obj.result()\n         self.assertAllClose(0.5, result, atol=1e-5)\n \n\n@@ -148,6 +148,7 @@ class Functional(Function, Model):\n \n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n+        names = [x.name for x in self._inputs]\n         adjusted = []\n         for x, ref_shape in zip(flat_inputs, flat_ref_shapes):\n             x_rank = len(x.shape)\n\n@@ -25,10 +25,6 @@ conv_transpose\n ctc ??\n \"\"\"\n \n-import math\n-\n-import numpy as np\n-\n from keras_core import backend\n from keras_core.backend import KerasTensor\n from keras_core.backend import any_symbolic_tensors\n@@ -259,719 +255,3 @@ def log_softmax(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return LogSoftmax(axis).symbolic_call(x)\n     return backend.nn.log_softmax(x, axis=axis)\n-\n-\n-class MaxPool(Operation):\n-    def __init__(\n-        self,\n-        pool_size,\n-        strides=None,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-    ):\n-        super().__init__()\n-        self.pool_size = pool_size\n-        self.strides = strides\n-        self.padding = padding\n-        self.data_format = data_format\n-\n-    def call(self, inputs):\n-        return backend.nn.max_pool(\n-            inputs,\n-            self.pool_size,\n-            self.strides,\n-            self.padding,\n-            self.data_format,\n-        )\n-\n-    def compute_output_spec(self, inputs):\n-        strides = self.pool_size if self.strides is None else self.strides\n-        input_shape = np.array(inputs.shape)\n-        if self.data_format == \"channels_last\":\n-            spatial_shape = input_shape[1:-1]\n-        else:\n-            spatial_shape = input_shape[2:]\n-        pool_size = np.array(self.pool_size)\n-        if self.padding == \"valid\":\n-            output_spatial_shape = (\n-                np.floor((spatial_shape - self.pool_size) / strides) + 1\n-            )\n-            negative_in_shape = np.all(output_spatial_shape < 0)\n-            if negative_in_shape:\n-                raise ValueError(\n-                    \"Computed output size would be negative. Received \"\n-                    f\"`inputs.shape={input_shape}` and `pool_size={pool_size}`.\"\n-                )\n-        elif self.padding == \"same\":\n-            output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n-        output_spatial_shape = [int(i) for i in output_spatial_shape]\n-        if self.data_format == \"channels_last\":\n-            output_shape = (\n-                [inputs.shape[0]]\n-                + list(output_spatial_shape)\n-                + [inputs.shape[-1]]\n-            )\n-        else:\n-            output_shape = inputs.shape[:2] + list(output_spatial_shape)\n-        return KerasTensor(output_shape, dtype=inputs.dtype)\n-\n-\n-def max_pool(\n-    inputs,\n-    pool_size,\n-    strides=None,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-):\n-    \"\"\"Max pooling operation.\n-\n-    Args:\n-        inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n-            `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n-        pool_size: int or tuple/list of integers of size\n-            `len(inputs_spatial_shape)`, specifying the size of the pooling\n-            window for each spatial dimension of the input tensor. If `pool_size`\n-            is int, then every spatial dimension shares the same `pool_size`.\n-        strides: int or tuple/list of integers of size\n-            `len(inputs_spatial_shape)`. The stride of the sliding window for\n-            each spatial dimension of the input tensor. If `strides` is int,\n-            then every spatial dimension shares the same `strides`.\n-        padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n-            left/right or up/down of the input such that output has the\n-            same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n-            `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n-\n-    Returns:\n-        A tensor of rank N+2, the result of the max pooling operation.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return MaxPool(\n-            pool_size,\n-            strides,\n-            padding,\n-            data_format,\n-        ).symbolic_call(inputs)\n-    return backend.nn.max_pool(inputs, pool_size, strides, padding, data_format)\n-\n-\n-class AveragePool(Operation):\n-    def __init__(\n-        self,\n-        pool_size,\n-        strides=None,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-    ):\n-        super().__init__()\n-        self.pool_size = pool_size\n-        self.strides = strides\n-        self.padding = padding\n-        self.data_format = data_format\n-\n-    def call(self, inputs):\n-        return backend.nn.average_pool(\n-            inputs,\n-            self.pool_size,\n-            self.strides,\n-            self.padding,\n-            self.data_format,\n-        )\n-\n-    def compute_output_spec(self, inputs):\n-        strides = self.pool_size if self.strides is None else self.strides\n-        input_shape = np.array(inputs.shape)\n-        if self.data_format == \"channels_last\":\n-            spatial_shape = input_shape[1:-1]\n-        else:\n-            spatial_shape = input_shape[2:]\n-        pool_size = np.array(self.pool_size)\n-        if self.padding == \"valid\":\n-            output_spatial_shape = (\n-                np.floor((spatial_shape - self.pool_size) / strides) + 1\n-            )\n-            negative_in_shape = np.all(output_spatial_shape < 0)\n-            if negative_in_shape:\n-                raise ValueError(\n-                    \"Computed output size would be negative. Received \"\n-                    f\"`inputs.shape={input_shape}` and `pool_size={pool_size}`.\"\n-                )\n-        elif self.padding == \"same\":\n-            output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n-        output_spatial_shape = [int(i) for i in output_spatial_shape]\n-        if self.data_format == \"channels_last\":\n-            output_shape = (\n-                [inputs.shape[0]] + output_spatial_shape + [inputs.shape[-1]]\n-            )\n-        else:\n-            output_shape = inputs.shape[:2] + output_spatial_shape\n-        return KerasTensor(output_shape, dtype=inputs.dtype)\n-\n-\n-def average_pool(\n-    inputs,\n-    pool_size,\n-    strides=None,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-):\n-    \"\"\"Average pooling operation.\n-\n-    Args:\n-        inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n-            `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n-        pool_size: int or tuple/list of integers of size\n-            `len(inputs_spatial_shape)`, specifying the size of the pooling\n-            window for each spatial dimension of the input tensor. If `pool_size`\n-            is int, then every spatial dimension shares the same `pool_size`.\n-        strides: int or tuple/list of integers of size\n-            `len(inputs_spatial_shape)`. The stride of the sliding window for\n-            each spatial dimension of the input tensor. If `strides` is int,\n-            then every spatial dimension shares the same `strides`.\n-        padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n-            left/right or up/down of the input such that output has the\n-            same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n-            `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n-\n-    Returns:\n-        A tensor of rank N+2, the result of the average pooling operation.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return AveragePool(\n-            pool_size,\n-            strides,\n-            padding,\n-            data_format,\n-        ).symbolic_call(inputs)\n-    return backend.nn.average_pool(\n-        inputs, pool_size, strides, padding, data_format\n-    )\n-\n-\n-class Conv(Operation):\n-    def __init__(\n-        self,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=\"channel_last\",\n-        dilation_rate=1,\n-    ):\n-        super().__init__()\n-        self.strides = strides\n-        self.padding = padding\n-        self.data_format = data_format\n-        self.dilation_rate = dilation_rate\n-\n-    def call(self, inputs, kernel):\n-        return backend.nn.conv(\n-            inputs,\n-            kernel,\n-            self.strides,\n-            self.padding,\n-            self.data_format,\n-            self.dilation_rate,\n-        )\n-\n-    def compute_output_spec(self, inputs, kernel):\n-        input_shape = inputs.shape\n-        if self.data_format == \"channels_last\":\n-            spatial_shape = input_shape[1:-1]\n-        else:\n-            spatial_shape = input_shape[2:]\n-        if len(kernel.shape) != len(input_shape):\n-            raise ValueError(\n-                \"Kernel shape must have the same length as input, but received \"\n-                f\"kernel of shape {kernel.shape} and \"\n-                f\"input of shape {input_shape}.\"\n-            )\n-        if isinstance(self.dilation_rate, int):\n-            dilation_rate = (self.dilation_rate,) * len(spatial_shape)\n-        else:\n-            dilation_rate = self.dilation_rate\n-        if len(dilation_rate) != len(spatial_shape):\n-            raise ValueError(\n-                \"Dilation must be None, scalar or tuple/list of length of \"\n-                \"inputs' spatial shape, but received \"\n-                f\"`dilation_rate={self.dilation_rate}` and input of shape {input_shape}.\"\n-            )\n-        spatial_shape = np.array(spatial_shape)\n-        kernel_spatial_shape = np.array(kernel.shape[:-2])\n-        dilation_rate = np.array(dilation_rate)\n-        if self.padding == \"valid\":\n-            output_spatial_shape = (\n-                np.floor(\n-                    (\n-                        spatial_shape\n-                        - dilation_rate * (kernel_spatial_shape - 1)\n-                        - 1\n-                    )\n-                    / self.strides\n-                )\n-                + 1\n-            )\n-            negative_in_shape = np.all(output_spatial_shape < 0)\n-            if negative_in_shape:\n-                raise ValueError(\n-                    \"Computed output size would be negative. Received \"\n-                    f\"`inputs shape={inputs.shape}`, \"\n-                    f\"`kernel spatial size={kernel.size}`, \"\n-                    f\"`dilation_rate={self.dilation_rate}`.\"\n-                )\n-        elif self.padding == \"same\":\n-            output_spatial_shape = (\n-                np.floor((spatial_shape - 1) / self.strides) + 1\n-            )\n-        output_spatial_shape = [int(i) for i in output_spatial_shape]\n-        if self.data_format == \"channels_last\":\n-            output_shape = (\n-                [inputs.shape[0]] + output_spatial_shape + [kernel.shape[-1]]\n-            )\n-        else:\n-            output_shape = [\n-                inputs.shape[0],\n-                kernel.shape[-1],\n-            ] + output_spatial_shape\n-        return KerasTensor(output_shape, dtype=inputs.dtype)\n-\n-\n-def conv(\n-    inputs,\n-    kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    \"\"\"General N-D convolution.\n-\n-    This ops supports 1D, 2D and 3D convolution.\n-\n-    Args:\n-        inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n-            `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n-        kernel: Tensor of rank N+2. `kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_output_channels],\n-            `num_input_channels` should match the number of channels in\n-            `inputs`.\n-        strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the strides of the convolution along each spatial\n-            dimension. If `strides` is int, then every spatial dimension shares\n-            the same `strides`.\n-        padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n-            left/right or up/down of the input such that output has the\n-            same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n-            `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n-        dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the dilation rate to use for dilated convolution. If\n-            `dilation_rate` is int, then every spatial dimension shares\n-            the same `dilation_rate`.\n-\n-    Returns:\n-        A tensor of rank N+2, the result of the conv operation.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return Conv(strides, padding, data_format, dilation_rate).symbolic_call(\n-            inputs, kernel\n-        )\n-    return backend.nn.conv(\n-        inputs, kernel, strides, padding, data_format, dilation_rate\n-    )\n-\n-\n-class DepthwiseConv(Operation):\n-    def __init__(\n-        self,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-        dilation_rate=1,\n-    ):\n-        super().__init__()\n-        self.strides = strides\n-        self.padding = padding\n-        self.data_format = data_format\n-        self.dilation_rate = dilation_rate\n-\n-    def call(self, inputs, kernel):\n-        return backend.nn.depthwise_conv(\n-            inputs,\n-            kernel,\n-            self.strides,\n-            self.padding,\n-            self.data_format,\n-            self.dilation_rate,\n-        )\n-\n-    def compute_output_spec(self, inputs, kernel):\n-        input_shape = inputs.shape\n-        if self.data_format == \"channels_last\":\n-            spatial_shape = input_shape[1:-1]\n-        else:\n-            spatial_shape = input_shape[2:]\n-        if len(kernel.shape) != len(inputs.shape):\n-            raise ValueError(\n-                \"Kernel shape must have the same length as input, but received \"\n-                f\"kernel of shape {kernel.shape} and \"\n-                f\"input of shape {input_shape}.\"\n-            )\n-        if isinstance(self.dilation_rate, int):\n-            dilation_rate = (self.dilation_rate,) * len(spatial_shape)\n-        else:\n-            dilation_rate = self.dilation_rate\n-        if len(dilation_rate) != len(spatial_shape):\n-            raise ValueError(\n-                \"Dilation must be None, scalar or tuple/list of length of \"\n-                \"inputs' spatial shape, but received \"\n-                f\"`dilation_rate={self.dilation_rate}` and input of shape {input_shape}.\"\n-            )\n-        spatial_shape = np.array(spatial_shape)\n-        kernel_spatial_shape = np.array(kernel.shape[:-2])\n-        dilation_rate = np.array(self.dilation_rate)\n-        if self.padding == \"valid\":\n-            output_spatial_shape = (\n-                np.floor(\n-                    (\n-                        spatial_shape\n-                        - dilation_rate * (kernel_spatial_shape - 1)\n-                        - 1\n-                    )\n-                    / self.strides\n-                )\n-                + 1\n-            )\n-            negative_in_shape = np.all(output_spatial_shape < 0)\n-            if negative_in_shape:\n-                raise ValueError(\n-                    \"Computed output size would be negative. Received \"\n-                    f\"`inputs shape={inputs.shape}`, \"\n-                    f\"`kernel spatial size={kernel.size}`, \"\n-                    f\"`dilation_rate={self.dilation_rate}`.\"\n-                )\n-        elif self.padding == \"same\":\n-            output_spatial_shape = (\n-                np.floor((spatial_shape - 1) / self.strides) + 1\n-            )\n-\n-        output_spatial_shape = [int(i) for i in output_spatial_shape]\n-        output_channels = kernel.shape[-1] * kernel.shape[-2]\n-        if self.data_format == \"channels_last\":\n-            output_shape = (\n-                [input_shape[0]] + output_spatial_shape + [output_channels]\n-            )\n-        else:\n-            output_shape = [\n-                input_shape[0],\n-                output_channels,\n-            ] + output_spatial_shape\n-        return KerasTensor(output_shape, dtype=inputs.dtype)\n-\n-\n-def depthwise_conv(\n-    inputs,\n-    kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    \"\"\"General N-D depthwise convolution.\n-\n-    This ops supports 1D and 2D depthwise convolution.\n-\n-    Args:\n-        inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n-            `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n-        kernel: Tensor of rank N+2. `kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_channels_multiplier],\n-            `num_input_channels` should match the number of channels in\n-            `inputs`.\n-        strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the strides of the convolution along each spatial\n-            dimension. If `strides` is int, then every spatial dimension shares\n-            the same `strides`.\n-        padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n-            left/right or up/down of the input such that output has the\n-            same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n-            `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n-        dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the dilation rate to use for dilated convolution. If\n-            `dilation_rate` is int, then every spatial dimension shares\n-            the same `dilation_rate`.\n-\n-    Returns:\n-        A tensor of rank N+2, the result of the depthwise conv operation.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return DepthwiseConv(\n-            strides, padding, data_format, dilation_rate\n-        ).symbolic_call(inputs, kernel)\n-    return backend.nn.depthwise_conv(\n-        inputs,\n-        kernel,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-    )\n-\n-\n-class SeparableConv(Operation):\n-    def __init__(\n-        self,\n-        strides=1,\n-        padding=\"valid\",\n-        data_format=\"channels_last\",\n-        dilation_rate=1,\n-    ):\n-        super().__init__()\n-        self.strides = strides\n-        self.padding = padding\n-        self.data_format = data_format\n-        self.dilation_rate = dilation_rate\n-\n-    def call(self, inputs, depthwise_kernel, pointwise_kernel):\n-        return backend.nn.separable_conv(\n-            inputs,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            self.strides,\n-            self.padding,\n-            self.data_format,\n-            self.dilation_rate,\n-        )\n-\n-    def compute_output_spec(self, inputs, depthwise_kernel, pointwise_kernel):\n-        output_shape = list(\n-            depthwise_conv(\n-                inputs,\n-                depthwise_kernel,\n-                self.strides,\n-                self.padding,\n-                self.data_format,\n-                self.dilation_rate,\n-            ).shape\n-        )\n-        if self.data_format == \"channels_last\":\n-            output_shape[-1] = pointwise_kernel.shape[-1]\n-        else:\n-            output_shape[1] = pointwise_kernel.shape[-1]\n-        return KerasTensor(output_shape, dtype=inputs.dtype)\n-\n-\n-def separable_conv(\n-    inputs,\n-    depthwise_kernel,\n-    pointwise_kernel,\n-    strides=1,\n-    padding=\"valid\",\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    \"\"\"General N-D separable convolution.\n-\n-    This ops supports 1D and 2D separable convolution. `separable_conv` is\n-    a depthwise conv followed by a pointwise conv.\n-\n-    Args:\n-        inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n-            `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n-        depthwise_kernel: Tensor of rank N+2. `depthwise_kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_channels_multiplier],\n-            `num_input_channels` should match the number of channels in\n-            `inputs`.\n-        pointwise_kernel: Tensor of rank N+2. `pointwise_kernel` has shape\n-            [ones_like(kernel_spatial_shape),\n-            num_input_channels * num_channels_multiplier, num_output_channels].\n-        strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the strides of the convolution along each spatial\n-            dimension. If `strides` is int, then every spatial dimension shares\n-            the same `strides`.\n-        padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n-            left/right or up/down of the input such that output has the\n-            same height/width dimension as the input when `strides=1`.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n-            `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n-        dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the dilation rate to use for dilated convolution. If\n-            `dilation_rate` is int, then every spatial dimension shares\n-            the same `dilation_rate`.\n-\n-    Returns:\n-        A tensor of rank N+2, the result of the depthwise conv operation.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return SeparableConv(\n-            strides,\n-            padding,\n-            data_format,\n-            dilation_rate,\n-        ).symbolic_call(inputs, depthwise_kernel, pointwise_kernel)\n-    return backend.nn.separable_conv(\n-        inputs,\n-        depthwise_kernel,\n-        pointwise_kernel,\n-        strides,\n-        padding,\n-        data_format,\n-        dilation_rate,\n-    )\n-\n-\n-class ConvTranspose(Operation):\n-    def __init__(\n-        self,\n-        strides,\n-        padding=\"valid\",\n-        output_padding=None,\n-        data_format=\"channels_last\",\n-        dilation_rate=1,\n-    ):\n-        super().__init__()\n-        self.strides = strides\n-        self.output_padding = output_padding\n-        self.padding = padding\n-        self.data_format = data_format\n-        self.dilation_rate = dilation_rate\n-\n-    def call(\n-        self,\n-        inputs,\n-        kernel,\n-    ):\n-        return backend.nn.conv_transpose(\n-            inputs,\n-            kernel,\n-            self.strides,\n-            self.output_padding,\n-            self.padding,\n-            self.data_format,\n-            self.dilation_rate,\n-        )\n-\n-    def compute_output_spec(self, inputs, kernel):\n-        output_shape = backend.nn.compute_output_shape_conv_transpose(\n-            inputs,\n-            kernel,\n-            self.strides,\n-            self.padding,\n-            self.output_padding,\n-            self.data_format,\n-            self.dilation_rate,\n-        )\n-        return KerasTensor(output_shape, dtype=inputs.dtype)\n-\n-\n-def conv_transpose(\n-    inputs,\n-    kernel,\n-    strides,\n-    padding=\"valid\",\n-    output_padding=None,\n-    data_format=\"channels_last\",\n-    dilation_rate=1,\n-):\n-    \"\"\"General N-D convolution transpose.\n-\n-    Also known as de-convolution. This ops supports 1D, 2D and 3D convolution.\n-\n-    Args:\n-        inputs: Tensor of rank N+2. `inputs` has shape\n-            [batch_size] + inputs_spatial_shape + [num_channels] if\n-            `data_format=\"channels_last\"`, or\n-            [batch_size, num_channels] + inputs_spatial_shape if\n-            `data_format=\"channels_first\"`. Pooling happens over the spatial\n-            dimensions only.\n-        kernel: Tensor of rank N+2. `kernel` has shape\n-            [kernel_spatial_shape, num_input_channels, num_output_channels],\n-            `num_input_channels` should match the number of channels in\n-            `inputs`.\n-        strides: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the strides of the convolution along each spatial\n-            dimension. If `strides` is int, then every spatial dimension shares\n-            the same `strides`.\n-        padding: string, either `\"valid\"` or `\"same\"`. `\"valid\"` means no\n-            padding is applied, and \"same\" results in padding evenly to the\n-            left/right or up/down of the input such that output has the\n-            same height/width dimension as the input when `strides=1`.\n-        output_padding: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the amount of padding along the height and width of\n-            the output tensor. Can be a single integer to specify the same\n-            value for all spatial dimensions. The amount of output padding\n-            along a given dimension must be lower than the stride along that\n-            same dimension. If set to None (default), the output shape is\n-            inferred.\n-        data_format: A string, either \"channels_last\" or `channels_first`.\n-            `data_format` determines the ordering of the dimensions in the\n-            inputs. If `data_format=\"channels_last\"`, inputs is of shape\n-            (batch_size, spatial_shape, channels) while if\n-            `data_format=\"channels_first\"`, inputs is of shape\n-            (batch_size, channels, spatial_shape).\n-        dilation_rate: int or int tuple/list of `len(inputs_spatial_shape)`,\n-            specifying the dilation rate to use for dilated convolution. If\n-            `dilation_rate` is int, then every spatial dimension shares\n-            the same `dilation_rate`.\n-\n-    Returns:\n-        A tensor of rank N+2, the result of the conv operation.\n-    \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return ConvTranspose(\n-            strides, padding, output_padding, data_format, dilation_rate\n-        ).symbolic_call(inputs, kernel)\n-    return backend.nn.conv_transpose(\n-        inputs,\n-        kernel,\n-        strides,\n-        padding,\n-        output_padding,\n-        data_format,\n-        dilation_rate,\n-    )\n\n@@ -1,6 +1,5 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import testing\n@@ -78,213 +77,7 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knn.log_softmax(x, axis=1).shape, (None, 2, 3))\n         self.assertEqual(knn.log_softmax(x, axis=-1).shape, (None, 2, 3))\n \n-    def test_max_pool(self):\n-        x = KerasTensor([None, 8, 3])\n-        self.assertEqual(knn.max_pool(x, 2, 1).shape, (None, 7, 3))\n-        self.assertEqual(\n-            knn.max_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 3)\n-        )\n \n-        x = KerasTensor([None, 8, 8, 3])\n-        self.assertEqual(knn.max_pool(x, 2, 1).shape, (None, 7, 7, 3))\n-        self.assertEqual(\n-            knn.max_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 4, 3)\n-        )\n-        self.assertEqual(\n-            knn.max_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n-            (None, 4, 4, 3),\n-        )\n-\n-    def test_average_pool(self):\n-        x = KerasTensor([None, 8, 3])\n-        self.assertEqual(knn.average_pool(x, 2, 1).shape, (None, 7, 3))\n-        self.assertEqual(\n-            knn.average_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 3)\n-        )\n-\n-        x = KerasTensor([None, 8, 8, 3])\n-        self.assertEqual(knn.average_pool(x, 2, 1).shape, (None, 7, 7, 3))\n-        self.assertEqual(\n-            knn.average_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 4, 3)\n-        )\n-        self.assertEqual(\n-            knn.average_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n-            (None, 4, 4, 3),\n-        )\n-\n-    def test_conv(self):\n-        # Test 1D conv.\n-        inputs_1d = KerasTensor([None, 20, 3])\n-        kernel = KerasTensor([4, 3, 2])\n-        self.assertEqual(\n-            knn.conv(inputs_1d, kernel, 1, padding=\"valid\").shape, (None, 17, 2)\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_1d, kernel, 1, padding=\"same\").shape, (None, 20, 2)\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_1d, kernel, (2,), dilation_rate=2).shape,\n-            (None, 7, 2),\n-        )\n-\n-        # Test 2D conv.\n-        inputs_2d = KerasTensor([None, 10, 10, 3])\n-        kernel = KerasTensor([2, 2, 3, 2])\n-        self.assertEqual(\n-            knn.conv(inputs_2d, kernel, 1, padding=\"valid\").shape,\n-            (None, 9, 9, 2),\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_2d, kernel, 1, padding=\"same\").shape,\n-            (None, 10, 10, 2),\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n-            (None, 4, 9, 2),\n-        )\n-\n-        # Test 3D conv.\n-        inputs_3d = KerasTensor([None, 8, 8, 8, 3])\n-        kernel = KerasTensor([3, 3, 3, 3, 2])\n-        self.assertEqual(\n-            knn.conv(inputs_3d, kernel, 1, padding=\"valid\").shape,\n-            (None, 6, 6, 6, 2),\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_3d, kernel, (2, 1, 2), padding=\"same\").shape,\n-            (None, 4, 8, 4, 2),\n-        )\n-        self.assertEqual(\n-            knn.conv(\n-                inputs_3d, kernel, 1, padding=\"valid\", dilation_rate=(1, 2, 2)\n-            ).shape,\n-            (None, 6, 4, 4, 2),\n-        )\n-\n-    def test_depthwise_conv(self):\n-        # Test 1D depthwise conv.\n-        inputs_1d = KerasTensor([None, 20, 3])\n-        kernel = KerasTensor([4, 3, 1])\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, 1, padding=\"valid\").shape,\n-            (None, 17, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, (1,), padding=\"same\").shape,\n-            (None, 20, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, 2, dilation_rate=2).shape,\n-            (None, 7, 3),\n-        )\n-\n-        # Test 2D depthwise conv.\n-        inputs_2d = KerasTensor([None, 10, 10, 3])\n-        kernel = KerasTensor([2, 2, 3, 1])\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, 1, padding=\"valid\").shape,\n-            (None, 9, 9, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, (1, 2), padding=\"same\").shape,\n-            (None, 10, 5, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, 2, dilation_rate=2).shape,\n-            (None, 4, 4, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(\n-                inputs_2d, kernel, 2, dilation_rate=(2, 1)\n-            ).shape,\n-            (None, 4, 5, 3),\n-        )\n-\n-    def test_separable_conv(self):\n-        # Test 1D separable conv.\n-        inputs_1d = KerasTensor([None, 20, 3])\n-        kernel = KerasTensor([4, 3, 2])\n-        pointwise_kernel = KerasTensor([1, 6, 5])\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_1d, kernel, pointwise_kernel, 1, padding=\"valid\"\n-            ).shape,\n-            (None, 17, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_1d, kernel, pointwise_kernel, 1, padding=\"same\"\n-            ).shape,\n-            (None, 20, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_1d, kernel, pointwise_kernel, 2, dilation_rate=2\n-            ).shape,\n-            (None, 7, 5),\n-        )\n-\n-        # Test 2D separable conv.\n-        inputs_2d = KerasTensor([None, 10, 10, 3])\n-        kernel = KerasTensor([2, 2, 3, 2])\n-        pointwise_kernel = KerasTensor([1, 1, 6, 5])\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_2d, kernel, pointwise_kernel, 1, padding=\"valid\"\n-            ).shape,\n-            (None, 9, 9, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_2d, kernel, pointwise_kernel, (1, 2), padding=\"same\"\n-            ).shape,\n-            (None, 10, 5, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_2d, kernel, pointwise_kernel, 2, dilation_rate=(2, 1)\n-            ).shape,\n-            (None, 4, 5, 5),\n-        )\n-\n-    def test_conv_transpose(self):\n-        inputs_1d = KerasTensor([None, 4, 3])\n-        kernel = KerasTensor([2, 5, 3])\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_1d, kernel, 2).shape, (None, 8, 5)\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_1d, kernel, 2, padding=\"same\").shape,\n-            (None, 8, 5),\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(\n-                inputs_1d, kernel, 5, padding=\"valid\", output_padding=4\n-            ).shape,\n-            (None, 21, 5),\n-        )\n-\n-        inputs_2d = KerasTensor([None, 4, 4, 3])\n-        kernel = KerasTensor([2, 2, 5, 3])\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_2d, kernel, 2).shape, (None, 8, 8, 5)\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_2d, kernel, (2, 2), padding=\"same\").shape,\n-            (None, 8, 8, 5),\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(\n-                inputs_2d, kernel, (5, 5), padding=\"valid\", output_padding=4\n-            ).shape,\n-            (None, 21, 21, 5),\n-        )\n-\n-\n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsStaticShapeTest(testing.TestCase):\n     def test_relu(self):\n         x = KerasTensor([1, 2, 3])\n@@ -350,207 +143,7 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knn.log_softmax(x, axis=1).shape, (1, 2, 3))\n         self.assertEqual(knn.log_softmax(x, axis=-1).shape, (1, 2, 3))\n \n-    def test_max_pool(self):\n-        x = KerasTensor([1, 8, 3])\n-        self.assertEqual(knn.max_pool(x, 2, 1).shape, (1, 7, 3))\n-        self.assertEqual(knn.max_pool(x, 2, 2, padding=\"same\").shape, (1, 4, 3))\n \n-        x = KerasTensor([1, 8, 8, 3])\n-        self.assertEqual(knn.max_pool(x, 2, 1).shape, (1, 7, 7, 3))\n-        self.assertEqual(\n-            knn.max_pool(x, 2, 2, padding=\"same\").shape, (1, 4, 4, 3)\n-        )\n-        self.assertEqual(\n-            knn.max_pool(x, (2, 2), (2, 2), padding=\"same\").shape, (1, 4, 4, 3)\n-        )\n-\n-    def test_average_pool(self):\n-        x = KerasTensor([1, 8, 3])\n-        self.assertEqual(knn.average_pool(x, 2, 1).shape, (1, 7, 3))\n-        self.assertEqual(\n-            knn.average_pool(x, 2, 2, padding=\"same\").shape, (1, 4, 3)\n-        )\n-\n-        x = KerasTensor([1, 8, 8, 3])\n-        self.assertEqual(knn.average_pool(x, 2, 1).shape, (1, 7, 7, 3))\n-        self.assertEqual(\n-            knn.average_pool(x, 2, 2, padding=\"same\").shape, (1, 4, 4, 3)\n-        )\n-        self.assertEqual(\n-            knn.average_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n-            (1, 4, 4, 3),\n-        )\n-\n-    def test_conv(self):\n-        # Test 1D conv.\n-        inputs_1d = KerasTensor([2, 20, 3])\n-        kernel = KerasTensor([4, 3, 2])\n-        self.assertEqual(\n-            knn.conv(inputs_1d, kernel, 1, padding=\"valid\").shape, (2, 17, 2)\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_1d, kernel, 1, padding=\"same\").shape, (2, 20, 2)\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_1d, kernel, (2,), dilation_rate=2).shape, (2, 7, 2)\n-        )\n-\n-        # Test 2D conv.\n-        inputs_2d = KerasTensor([2, 10, 10, 3])\n-        kernel = KerasTensor([2, 2, 3, 2])\n-        self.assertEqual(\n-            knn.conv(inputs_2d, kernel, 1, padding=\"valid\").shape, (2, 9, 9, 2)\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_2d, kernel, 1, padding=\"same\").shape, (2, 10, 10, 2)\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n-            (2, 4, 9, 2),\n-        )\n-\n-        # Test 3D conv.\n-        inputs_3d = KerasTensor([2, 8, 8, 8, 3])\n-        kernel = KerasTensor([3, 3, 3, 3, 2])\n-        self.assertEqual(\n-            knn.conv(inputs_3d, kernel, 1, padding=\"valid\").shape,\n-            (2, 6, 6, 6, 2),\n-        )\n-        self.assertEqual(\n-            knn.conv(inputs_3d, kernel, (2, 1, 2), padding=\"same\").shape,\n-            (2, 4, 8, 4, 2),\n-        )\n-        self.assertEqual(\n-            knn.conv(\n-                inputs_3d, kernel, 1, padding=\"valid\", dilation_rate=(1, 2, 2)\n-            ).shape,\n-            (2, 6, 4, 4, 2),\n-        )\n-\n-    def test_depthwise_conv(self):\n-        # Test 1D depthwise conv.\n-        inputs_1d = KerasTensor([2, 20, 3])\n-        kernel = KerasTensor([4, 3, 1])\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, 1, padding=\"valid\").shape,\n-            (2, 17, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, (1,), padding=\"same\").shape,\n-            (2, 20, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_1d, kernel, 2, dilation_rate=2).shape,\n-            (2, 7, 3),\n-        )\n-\n-        # Test 2D depthwise conv.\n-        inputs_2d = KerasTensor([2, 10, 10, 3])\n-        kernel = KerasTensor([2, 2, 3, 1])\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, 1, padding=\"valid\").shape,\n-            (2, 9, 9, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, (1, 2), padding=\"same\").shape,\n-            (2, 10, 5, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(inputs_2d, kernel, 2, dilation_rate=2).shape,\n-            (2, 4, 4, 3),\n-        )\n-        self.assertEqual(\n-            knn.depthwise_conv(\n-                inputs_2d, kernel, 2, dilation_rate=(2, 1)\n-            ).shape,\n-            (2, 4, 5, 3),\n-        )\n-\n-    def test_separable_conv(self):\n-        # Test 1D separable conv.\n-        inputs_1d = KerasTensor([2, 20, 3])\n-        kernel = KerasTensor([4, 3, 2])\n-        pointwise_kernel = KerasTensor([1, 6, 5])\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_1d, kernel, pointwise_kernel, 1, padding=\"valid\"\n-            ).shape,\n-            (2, 17, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_1d, kernel, pointwise_kernel, 1, padding=\"same\"\n-            ).shape,\n-            (2, 20, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_1d, kernel, pointwise_kernel, 2, dilation_rate=2\n-            ).shape,\n-            (2, 7, 5),\n-        )\n-\n-        # Test 2D separable conv.\n-        inputs_2d = KerasTensor([2, 10, 10, 3])\n-        kernel = KerasTensor([2, 2, 3, 2])\n-        pointwise_kernel = KerasTensor([1, 1, 6, 5])\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_2d, kernel, pointwise_kernel, 1, padding=\"valid\"\n-            ).shape,\n-            (2, 9, 9, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_2d, kernel, pointwise_kernel, (1, 2), padding=\"same\"\n-            ).shape,\n-            (2, 10, 5, 5),\n-        )\n-        self.assertEqual(\n-            knn.separable_conv(\n-                inputs_2d, kernel, pointwise_kernel, 2, dilation_rate=(2, 1)\n-            ).shape,\n-            (2, 4, 5, 5),\n-        )\n-\n-    def test_conv_transpose(self):\n-        inputs_1d = KerasTensor([2, 4, 3])\n-        kernel = KerasTensor([2, 5, 3])\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_1d, kernel, 2).shape, (2, 8, 5)\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_1d, kernel, 2, padding=\"same\").shape,\n-            (2, 8, 5),\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(\n-                inputs_1d, kernel, 5, padding=\"valid\", output_padding=4\n-            ).shape,\n-            (2, 21, 5),\n-        )\n-\n-        inputs_2d = KerasTensor([2, 4, 4, 3])\n-        kernel = KerasTensor([2, 2, 5, 3])\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_2d, kernel, 2).shape, (2, 8, 8, 5)\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(inputs_2d, kernel, (2, 2), padding=\"same\").shape,\n-            (2, 8, 8, 5),\n-        )\n-        self.assertEqual(\n-            knn.conv_transpose(\n-                inputs_2d, kernel, (5, 5), padding=\"valid\", output_padding=4\n-            ).shape,\n-            (2, 21, 21, 5),\n-        )\n-\n-\n-@pytest.mark.skipif(\n-    backend() != \"tensorflow\",\n-    reason=\"Not have other backend support yet.\",\n-)\n class NNOpsCorrectnessTest(testing.TestCase):\n     def test_relu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n@@ -661,295 +254,3 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.log_softmax(x, axis=-1),\n             [-2.407606, -1.4076059, -0.4076059],\n         )\n-\n-    def test_max_pool(self):\n-        # Test 1D max pooling.\n-        x = np.arange(120, dtype=float).reshape([2, 20, 3])\n-        self.assertAllClose(\n-            knn.max_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.max_pool1d(x, 2, 1, padding=\"VALID\"),\n-        )\n-        self.assertAllClose(\n-            knn.max_pool(x, 2, 2, padding=\"same\"),\n-            tf.nn.max_pool1d(x, 2, 2, padding=\"SAME\"),\n-        )\n-\n-        # Test 2D max pooling.\n-        x = np.arange(540, dtype=float).reshape([2, 10, 9, 3])\n-        self.assertAllClose(\n-            knn.max_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.max_pool2d(x, 2, 1, padding=\"VALID\"),\n-        )\n-        self.assertAllClose(\n-            knn.max_pool(x, 2, (2, 1), padding=\"same\"),\n-            tf.nn.max_pool2d(x, 2, (2, 1), padding=\"SAME\"),\n-        )\n-\n-    def test_average_pool(self):\n-        # Test 1D max pooling.\n-        x = np.arange(120, dtype=float).reshape([2, 20, 3])\n-        self.assertAllClose(\n-            knn.average_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.avg_pool1d(x, 2, 1, padding=\"VALID\"),\n-        )\n-        self.assertAllClose(\n-            knn.average_pool(x, 2, 2, padding=\"same\"),\n-            tf.nn.avg_pool1d(x, 2, 2, padding=\"SAME\"),\n-        )\n-\n-        # Test 2D max pooling.\n-        x = np.arange(540, dtype=float).reshape([2, 10, 9, 3])\n-        self.assertAllClose(\n-            knn.average_pool(x, 2, 1, padding=\"valid\"),\n-            tf.nn.avg_pool2d(x, 2, 1, padding=\"VALID\"),\n-        )\n-        self.assertAllClose(\n-            knn.average_pool(x, 2, (2, 1), padding=\"same\"),\n-            tf.nn.avg_pool2d(x, 2, (2, 1), padding=\"SAME\"),\n-        )\n-\n-    def test_conv(self):\n-        # Test 1D conv.\n-        inputs_1d = np.arange(120, dtype=float).reshape([2, 20, 3])\n-        kernel = np.arange(24, dtype=float).reshape([4, 3, 2])\n-\n-        outputs = knn.conv(inputs_1d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.conv1d(inputs_1d, kernel, 1, padding=\"VALID\")\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(inputs_1d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv1d(inputs_1d, kernel, 2, padding=\"SAME\")\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(\n-            inputs_1d, kernel, 1, padding=\"same\", dilation_rate=2\n-        )\n-        expected = tf.nn.conv1d(\n-            inputs_1d, kernel, 1, padding=\"SAME\", dilations=2\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        # Test 2D conv.\n-        inputs_2d = np.arange(600, dtype=float).reshape([2, 10, 10, 3])\n-        kernel = np.arange(24, dtype=float).reshape([2, 2, 3, 2])\n-\n-        outputs = knn.conv(inputs_2d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, 1, padding=\"VALID\")\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(inputs_2d, kernel, (1, 2), padding=\"valid\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, (1, 2), padding=\"VALID\")\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(inputs_2d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv2d(inputs_2d, kernel, 2, padding=\"SAME\")\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(\n-            inputs_2d, kernel, 1, padding=\"same\", dilation_rate=2\n-        )\n-        expected = tf.nn.conv2d(\n-            inputs_2d, kernel, 1, padding=\"SAME\", dilations=2\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(\n-            inputs_2d,\n-            kernel,\n-            1,\n-            padding=\"same\",\n-            dilation_rate=(2, 1),\n-        )\n-        expected = tf.nn.conv2d(\n-            inputs_2d,\n-            kernel,\n-            1,\n-            padding=\"SAME\",\n-            dilations=(2, 1),\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        # Test 3D conv.\n-        inputs_3d = np.arange(3072, dtype=float).reshape([2, 8, 8, 8, 3])\n-        kernel = np.arange(162, dtype=float).reshape([3, 3, 3, 3, 2])\n-\n-        outputs = knn.conv(inputs_3d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.conv3d(\n-            inputs_3d, kernel, (1, 1, 1, 1, 1), padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(\n-            inputs_3d,\n-            kernel,\n-            (1, 1, 1),\n-            padding=\"valid\",\n-            dilation_rate=(1, 1, 1),\n-        )\n-        expected = tf.nn.conv3d(\n-            inputs_3d,\n-            kernel,\n-            (1, 1, 1, 1, 1),\n-            padding=\"VALID\",\n-            dilations=(1, 1, 1, 1, 1),\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv(inputs_3d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv3d(\n-            inputs_3d, kernel, (1, 2, 2, 2, 1), padding=\"SAME\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-    def test_depthwise_conv(self):\n-        # Test 2D conv.\n-        inputs_2d = np.arange(600, dtype=float).reshape([2, 10, 10, 3])\n-        kernel = np.arange(24, dtype=float).reshape([2, 2, 3, 2])\n-\n-        outputs = knn.depthwise_conv(inputs_2d, kernel, 1, padding=\"valid\")\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 1, 1, 1), padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.depthwise_conv(inputs_2d, kernel, (1, 1), padding=\"valid\")\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 1, 1, 1), padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.depthwise_conv(inputs_2d, kernel, (2, 2), padding=\"same\")\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 2, 2, 1), padding=\"SAME\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.depthwise_conv(\n-            inputs_2d, kernel, 1, padding=\"same\", dilation_rate=(2, 2)\n-        )\n-        expected = tf.nn.depthwise_conv2d(\n-            inputs_2d, kernel, (1, 1, 1, 1), padding=\"SAME\", dilations=(2, 2)\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-    def test_separable_conv(self):\n-        # Test 2D conv.\n-        inputs_2d = np.arange(600, dtype=float).reshape([2, 10, 10, 3])\n-        depthwise_kernel = np.arange(24, dtype=float).reshape([2, 2, 3, 2])\n-        pointwise_kernel = np.arange(72, dtype=float).reshape([1, 1, 6, 12])\n-\n-        outputs = knn.separable_conv(\n-            inputs_2d, depthwise_kernel, pointwise_kernel, 1, padding=\"valid\"\n-        )\n-        expected = tf.nn.separable_conv2d(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            (1, 1, 1, 1),\n-            padding=\"VALID\",\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.separable_conv(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            (1, 1),\n-            padding=\"valid\",\n-        )\n-        expected = tf.nn.separable_conv2d(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            (1, 1, 1, 1),\n-            padding=\"VALID\",\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.separable_conv(\n-            inputs_2d, depthwise_kernel, pointwise_kernel, 2, padding=\"same\"\n-        )\n-        expected = tf.nn.separable_conv2d(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            (1, 2, 2, 1),\n-            padding=\"SAME\",\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.separable_conv(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            1,\n-            padding=\"same\",\n-            dilation_rate=(2, 2),\n-        )\n-        expected = tf.nn.separable_conv2d(\n-            inputs_2d,\n-            depthwise_kernel,\n-            pointwise_kernel,\n-            (1, 1, 1, 1),\n-            padding=\"SAME\",\n-            dilations=(2, 2),\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-    def test_conv_transpose(self):\n-        # Test 1D conv.\n-        inputs_1d = np.arange(24, dtype=float).reshape([2, 4, 3])\n-        kernel = np.arange(30, dtype=float).reshape([2, 5, 3])\n-        outputs = knn.conv_transpose(inputs_1d, kernel, 2, padding=\"valid\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 8, 5], 2, padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv_transpose(inputs_1d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 8, 5], 2, padding=\"SAME\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv_transpose(\n-            inputs_1d, kernel, 5, output_padding=4, padding=\"valid\"\n-        )\n-        expected = tf.nn.conv_transpose(\n-            inputs_1d, kernel, [2, 21, 5], 5, padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        # Test 2D conv.\n-        inputs_2d = np.arange(96, dtype=float).reshape([2, 4, 4, 3])\n-        kernel = np.arange(60, dtype=float).reshape([2, 2, 5, 3])\n-\n-        outputs = knn.conv_transpose(inputs_2d, kernel, (2, 2), padding=\"valid\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d, kernel, [2, 8, 8, 5], (2, 2), padding=\"VALID\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv_transpose(inputs_2d, kernel, 2, padding=\"same\")\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d, kernel, [2, 8, 8, 5], 2, padding=\"SAME\"\n-        )\n-        self.assertAllClose(outputs, expected)\n-\n-        outputs = knn.conv_transpose(\n-            inputs_2d,\n-            kernel,\n-            5,\n-            output_padding=4,\n-            padding=\"valid\",\n-            dilation_rate=(1, 1),\n-        )\n-        expected = tf.nn.conv_transpose(\n-            inputs_2d,\n-            kernel,\n-            [2, 21, 21, 5],\n-            5,\n-            padding=\"VALID\",\n-            dilations=(1, 1),\n-        )\n-        self.assertAllClose(outputs, expected)\n\n@@ -0,0 +1,144 @@\n+from keras_core import operations as ops\n+from keras_core.optimizers import optimizer\n+\n+\n+class Adam(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adam algorithm.\n+\n+    Adam optimization is a stochastic gradient descent method that is based on\n+    adaptive estimation of first-order and second-order moments.\n+\n+    According to\n+    [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n+    the method is \"*computationally\n+    efficient, has little memory requirement, invariant to diagonal rescaling of\n+    gradients, and is well suited for problems that are large in terms of\n+    data/parameters*\".\n+\n+    Args:\n+      learning_rate: A `Tensor`, floating point value, a schedule that is a\n+        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        learning rate. Defaults to `0.001`.\n+      beta_1: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n+      beta_2: A float value or a constant float tensor, or a callable\n+        that takes no arguments and returns the actual value to use. The\n+        exponential decay rate for the 2nd moment estimates. Defaults to\n+        `0.999`.\n+      epsilon: A small constant for numerical stability. This epsilon is\n+        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n+        `1e-7`.\n+      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n+        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n+      {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        amsgrad=False,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"Adam\",\n+        **kwargs\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+        self.amsgrad = amsgrad\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adam optimizer has 3 types of variables: momentums, velocities and\n+        velocity_hat (only set when amsgrad is applied),\n+\n+        Args:\n+          var_list: list of model variables to build Adam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._momentums = []\n+        self._velocities = []\n+        for var in var_list:\n+            self._momentums.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"m\"\n+                )\n+            )\n+            self._velocities.append(\n+                self.add_variable_from_reference(\n+                    model_variable=var, variable_name=\"v\"\n+                )\n+            )\n+        if self.amsgrad:\n+            self._velocity_hats = []\n+            for var in var_list:\n+                self._velocity_hats.append(\n+                    self.add_variable_from_reference(\n+                        model_variable=var, variable_name=\"vhat\"\n+                    )\n+                )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        beta_1_power = None\n+        beta_2_power = None\n+        lr = ops.cast(self.learning_rate, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        beta_1_power = ops.power(ops.cast(self.beta_1, variable.dtype), local_step)\n+        beta_2_power = ops.power(ops.cast(self.beta_2, variable.dtype), local_step)\n+\n+        m = self._momentums[self._get_variable_index(variable)]\n+        v = self._velocities[self._get_variable_index(variable)]\n+\n+        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n+\n+        m.assign(m + (gradient - m) * (1 - self.beta_1))\n+        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n+        if self.amsgrad:\n+            v_hat = self._velocity_hats[self._get_variable_index(variable)]\n+            v_hat.assign(ops.maximum(v_hat, v))\n+            v = v_hat\n+        variable.assign(m - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+                \"amsgrad\": self.amsgrad,\n+            }\n+        )\n+        return config\n+\n+\n+Adam.__doc__ = Adam.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\\ No newline at end of file\n\n@@ -0,0 +1,45 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core import testing\n+from keras_core.optimizers.adam import Adam\n+import numpy as np\n+\n+class AdamTest(testing.TestCase):\n+    def test_config(self):\n+        # TODO\n+        pass\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Adam(learning_rate=1, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Adam(learning_rate=1, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Adam(learning_rate=1, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1, 1.9760959)\n+        self.assertAlmostEqual(var2, 2.0)\n+        self.assertAlmostEqual(var3, 2.0)\n+\n+    def test_clip_norm(self):\n+        optimizer = Adam(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Adam(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n+\n\n@@ -287,7 +287,7 @@ class Optimizer:\n     def _filter_empty_gradients(self, grads_and_vars):\n         filtered = [(g, v) for g, v in grads_and_vars if g is not None]\n         if not filtered:\n-            raise ValueError(\"No gradients provided for any variable.\")\n+            raise ValueError(f\"No gradients provided for any variable.\")\n         if len(filtered) < len(grads_and_vars):\n             missing_grad_vars = [v for g, v in grads_and_vars if g is None]\n             warnings.warn(\n\n@@ -30,5 +30,5 @@ class RegularizersTest(testing.TestCase):\n     def test_orthogonal_regularizer(self):\n         value = np.random.random((4, 4))\n         x = backend.Variable(value)\n-        regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n+        y = regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n         # TODO\n\n@@ -410,7 +410,7 @@ class CompileLoss(losses_module.Loss):\n                     raise ValueError(\n                         f\"When there is only a single output, the `loss_weights` argument \"\n                         \"must be a Python float. \"\n-                        f\"Received instead: loss_weights={loss_weights} of type {type(loss_weights)}\"\n+                        f\"Received instead:\\loss_weights={loss_weights} of type {type(loss_weights)}\"\n                     )\n                 flat_loss_weights.append(loss_weights)\n             else:\n@@ -447,7 +447,7 @@ class CompileLoss(losses_module.Loss):\n                         \"For a model with multiple outputs, \"\n                         f\"when providing the `loss_weights` argument as a list, \"\n                         \"it should have as many entries as the model has outputs. \"\n-                        f\"Received: loss_weights={loss_weights} of length {len(loss_weights)} \"\n+                        f\"Received:\\loss_weights={loss_weights}\\nof length {len(loss_weights)} \"\n                         f\"whereas the model has {len(y_pred)} outputs.\"\n                     )\n                 if not all(isinstance(e, float) for e in loss_weights):\n@@ -503,7 +503,7 @@ class CompileLoss(losses_module.Loss):\n                         raise ValueError(\n                             f\"In the dict argument `loss_weights`, key \"\n                             f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received: loss_weights={loss_weights}\"\n+                            f\"Received:\\loss_weights={loss_weights}\"\n                         )\n                     if not isinstance(loss_weights[name], float):\n                         raise ValueError(\n\n@@ -138,8 +138,8 @@ class Trainer:\n             loss = self._compile_loss(y, y_pred, sample_weight)\n             if loss is not None:\n                 losses.append(loss)\n-        for loss in self.losses:\n-            losses.append(ops.cast(loss, dtype=backend.floatx()))\n+        for l in self.losses:\n+            losses.append(ops.cast(l, dtype=backend.floatx()))\n         if len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
