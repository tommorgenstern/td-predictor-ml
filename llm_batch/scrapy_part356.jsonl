{"custom_id": "scrapy#043a24410b2eeed554e8ce3a73b1e78fa53fa4d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1098 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -206,7 +206,7 @@ class BaseSettingsTest(unittest.TestCase):\n     )\n     def test_update_kwargs(self):\n         settings = BaseSettings({\"key\": 0})\n-        settings.update(key=1)\n+        settings.update(key=1)  # pylint: disable=unexpected-keyword-arg\n \n     @pytest.mark.xfail(\n         raises=AttributeError,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db86f91789d25e45c43f964fff31d3016a451a1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1275 | Contributors (this commit): 23 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -27,9 +27,10 @@ from scrapy.settings import default_settings\n _SettingsKeyT = Union[bool, float, int, str, None]\n \n if TYPE_CHECKING:\n-    # typing.Self requires Python 3.11\n     # https://github.com/python/typing/issues/445#issuecomment-1131458824\n     from _typeshed import SupportsItems\n+\n+    # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n     _SettingsInputT = Union[SupportsItems[_SettingsKeyT, Any], str, None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f5bbe3a8fababac6dfcfcf1a22aa7f7dba199ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 53 | Churn Cumulative: 211 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 0.75\n\nDIFF:\n@@ -4,11 +4,14 @@ conditions are met.\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n+import logging\n from collections import defaultdict\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n \n+logger = logging.getLogger(__name__)\n+\n \n class CloseSpider:\n     def __init__(self, crawler):\n@@ -19,6 +22,7 @@ class CloseSpider:\n             \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n             \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n             \"errorcount\": crawler.settings.getint(\"CLOSESPIDER_ERRORCOUNT\"),\n+            \"timeout_no_item\": crawler.settings.getint(\"CLOSESPIDER_TIMEOUT_NO_ITEM\"),\n         }\n \n         if not any(self.close_on.values()):\n@@ -34,6 +38,15 @@ class CloseSpider:\n             crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n         if self.close_on.get(\"itemcount\"):\n             crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n+        if self.close_on.get(\"timeout_no_item\"):\n+            self.timeout_no_item = self.close_on[\"timeout_no_item\"]\n+            self.items_in_period = 0\n+            crawler.signals.connect(\n+                self.spider_opened_no_item, signal=signals.spider_opened\n+            )\n+            crawler.signals.connect(\n+                self.item_scraped_no_item, signal=signals.item_scraped\n+            )\n         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n     @classmethod\n@@ -69,3 +82,31 @@ class CloseSpider:\n         task = getattr(self, \"task\", False)\n         if task and task.active():\n             task.cancel()\n+\n+        task_no_item = getattr(self, \"task_no_item\", False)\n+        if task_no_item.running:\n+            task_no_item.stop()\n+\n+    def spider_opened_no_item(self, spider):\n+        from twisted.internet import task\n+\n+        self.task_no_item = task.LoopingCall(self._count_items_produced, spider)\n+        self.task_no_item.start(self.timeout_no_item, now=False)\n+\n+        logger.info(\n+            f\"Spider will stop when no items are produced after \"\n+            f\"{self.timeout_no_item} seconds.\"\n+        )\n+\n+    def item_scraped_no_item(self, item, spider):\n+        self.items_in_period += 1\n+\n+    def _count_items_produced(self, spider):\n+        if self.items_in_period >= 1:\n+            self.items_in_period = 0\n+        else:\n+            logger.info(\n+                f\"Closing spider since no items were produced in the last \"\n+                f\"{self.timeout_no_item} seconds.\"\n+            )\n+            self.crawler.engine.close_spider(spider, \"closespider_timeout_no_item\")\n\n@@ -3,7 +3,7 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-from tests.spiders import ErrorSpider, FollowAllSpider, ItemSpider\n+from tests.spiders import DelaySpider, ErrorSpider, FollowAllSpider, ItemSpider\n \n \n class TestCloseSpider(TestCase):\n@@ -54,3 +54,13 @@ class TestCloseSpider(TestCase):\n         self.assertEqual(reason, \"closespider_timeout\")\n         total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n         self.assertTrue(total_seconds >= close_on)\n+\n+    @defer.inlineCallbacks\n+    def test_closespider_timeout_no_item(self):\n+        timeout = 1\n+        crawler = get_crawler(DelaySpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n+        yield crawler.crawl(n=3, total=10, mockserver=self.mockserver)\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_timeout_no_item\")\n+        total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n+        self.assertTrue(total_seconds >= timeout)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f787a27dc1f7551b98d322b6b93b3cb8bad4e2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 103 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -84,7 +84,7 @@ class CloseSpider:\n             task.cancel()\n \n         task_no_item = getattr(self, \"task_no_item\", False)\n-        if task_no_item.running:\n+        if task_no_item and task_no_item.running:\n             task_no_item.stop()\n \n     def spider_opened_no_item(self, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#368ab29ffc32b13d440f8adefdf06431b7ba7c32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 22 | Churn Cumulative: 884 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,6 +77,22 @@ class DelaySpider(MetaSpider):\n         self.t2_err = time.time()\n \n \n+class SlowSpider(DelaySpider):\n+    name = \"slow\"\n+\n+    def start_requests(self):\n+        # 1st response is fast\n+        url = self.mockserver.url(\"/delay?n=0&b=0\")\n+        yield Request(url, callback=self.parse, errback=self.errback)\n+\n+        # 2nd response is slow\n+        url = self.mockserver.url(f\"/delay?n={self.n}&b={self.b}\")\n+        yield Request(url, callback=self.parse, errback=self.errback)\n+\n+    def parse(self, response):\n+        yield Item()\n+\n+\n class SimpleSpider(MetaSpider):\n     name = \"simple\"\n \n\n@@ -3,7 +3,7 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-from tests.spiders import DelaySpider, ErrorSpider, FollowAllSpider, ItemSpider\n+from tests.spiders import ErrorSpider, FollowAllSpider, ItemSpider, SlowSpider\n \n \n class TestCloseSpider(TestCase):\n@@ -58,8 +58,8 @@ class TestCloseSpider(TestCase):\n     @defer.inlineCallbacks\n     def test_closespider_timeout_no_item(self):\n         timeout = 1\n-        crawler = get_crawler(DelaySpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n-        yield crawler.crawl(n=3, total=10, mockserver=self.mockserver)\n+        crawler = get_crawler(SlowSpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n+        yield crawler.crawl(n=3, mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n         self.assertEqual(reason, \"closespider_timeout_no_item\")\n         total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c34f34ecbc18c0829cd04a621b1d8239c25642b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 26 | Churn Cumulative: 788 | Contributors (this commit): 1 | Commits (past 90d): 17 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,15 +17,8 @@ class AddonManager:\n         self.crawler: \"Crawler\" = crawler\n         self.addons: List[Any] = []\n \n-    def add(self, addon: Any) -> None:\n-        \"\"\"Store an add-on.\n-\n-        :param addon: The add-on object (or path) to be stored\n-        :type addon: Python object, class or ``str``\n-\n-        :param config: The add-on configuration dictionary\n-        :type config: ``dict``\n-        \"\"\"\n+    def _add(self, addon: Any) -> None:\n+        \"\"\"Store an add-on.\"\"\"\n         if isinstance(addon, (type, str)):\n             addon = load_object(addon)\n         if isinstance(addon, type):\n@@ -33,7 +26,7 @@ class AddonManager:\n         self.addons.append(addon)\n \n     def load_settings(self, settings) -> None:\n-        \"\"\"Load add-ons and configurations from settings object.\n+        \"\"\"Load add-ons and configurations from a settings object.\n \n         This will load the addon for every add-on path in the\n         ``ADDONS`` setting.\n@@ -42,10 +35,9 @@ class AddonManager:\n             which to read the add-on configuration\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n-        paths = build_component_list(settings[\"ADDONS\"])\n-        addons = [load_object(path) for path in paths]\n-        for a in addons:\n-            self.add(a)\n+        addons = build_component_list(settings[\"ADDONS\"])\n+        for addon in build_component_list(settings[\"ADDONS\"]):\n+            self._add(addon)\n         logger.info(\n             \"Enabled addons:\\n%(addons)s\",\n             {\n\n@@ -43,12 +43,6 @@ class AddonTest(unittest.TestCase):\n \n \n class AddonManagerTest(unittest.TestCase):\n-    def test_add(self):\n-        crawler = get_crawler()\n-        manager = crawler.addons\n-        manager.add(\"tests.test_addons.GoodAddon\")\n-        self.assertIsInstance(manager.addons[0], GoodAddon)\n-\n     def test_load_settings(self):\n         settings_dict = {\n             \"ADDONS\": {\"tests.test_addons.GoodAddon\": 0},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#005c8cc5f00f41ad7d836eccf45dbaf39c03ebca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 616 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class AddonManager:\n     def load_settings(self, settings) -> None:\n         \"\"\"Load add-ons and configurations from a settings object.\n \n-        This will load the addon for every add-on path in the\n+        This will load the add-on for every add-on path in the\n         ``ADDONS`` setting.\n \n         :param settings: The :class:`~scrapy.settings.Settings` object from \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#583df9f7d063ac0f48eafccdd463f3195a084d9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 59 | Files Changed: 3 | Hunks: 11 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 80 | Churn Cumulative: 4321 | Contributors (this commit): 39 | Commits (past 90d): 6 | Contributors (cumulative): 67 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,10 @@\n+import platform\n+import sys\n from pathlib import Path\n \n import pytest\n+from twisted import version as twisted_version\n+from twisted.python.versions import Version\n from twisted.web.http import H2_ENABLED\n \n from scrapy.utils.reactor import install_reactor\n@@ -73,6 +77,18 @@ def only_not_asyncio(request, reactor_pytest):\n         pytest.skip(\"This test is only run without --reactor=asyncio\")\n \n \n+@pytest.fixture(autouse=True)\n+def requires_uvloop(request):\n+    if not request.node.get_closest_marker(\"requires_uvloop\"):\n+        return\n+    if sys.implementation.name == \"pypy\":\n+        pytest.skip(\"uvloop does not support pypy properly\")\n+    if platform.system() == \"Windows\":\n+        pytest.skip(\"uvloop does not support Windows\")\n+    if twisted_version == Version(\"twisted\", 21, 2, 0):\n+        pytest.skip(\"https://twistedmatrix.com/trac/ticket/10106\")\n+\n+\n def pytest_configure(config):\n     if config.getoption(\"--reactor\") == \"asyncio\":\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n@@ -18,8 +18,6 @@ from typing import Dict, Generator, Optional, Union\n from unittest import skipIf\n \n from pytest import mark\n-from twisted import version as twisted_version\n-from twisted.python.versions import Version\n from twisted.trial import unittest\n \n import scrapy\n@@ -802,17 +800,7 @@ class MySpider(scrapy.Spider):\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n         )\n \n-    @mark.skipif(\n-        sys.implementation.name == \"pypy\",\n-        reason=\"uvloop does not support pypy properly\",\n-    )\n-    @mark.skipif(\n-        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n-    )\n-    @mark.skipif(\n-        twisted_version == Version(\"twisted\", 21, 2, 0),\n-        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n-    )\n+    @mark.requires_uvloop\n     def test_custom_asyncio_loop_enabled_true(self):\n         log = self.get_log(\n             self.debug_log_spider,\n\n@@ -8,9 +8,7 @@ from pathlib import Path\n \n from packaging.version import parse as parse_version\n from pytest import mark, raises\n-from twisted import version as twisted_version\n from twisted.internet import defer\n-from twisted.python.versions import Version\n from twisted.trial import unittest\n from w3lib import __version__ as w3lib_version\n \n@@ -466,17 +464,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             log,\n         )\n \n-    @mark.skipif(\n-        sys.implementation.name == \"pypy\",\n-        reason=\"uvloop does not support pypy properly\",\n-    )\n-    @mark.skipif(\n-        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n-    )\n-    @mark.skipif(\n-        twisted_version == Version(\"twisted\", 21, 2, 0),\n-        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n-    )\n+    @mark.requires_uvloop\n     def test_custom_loop_asyncio(self):\n         log = self.run_script(\"asyncio_custom_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -485,17 +473,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    @mark.skipif(\n-        sys.implementation.name == \"pypy\",\n-        reason=\"uvloop does not support pypy properly\",\n-    )\n-    @mark.skipif(\n-        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n-    )\n-    @mark.skipif(\n-        twisted_version == Version(\"twisted\", 21, 2, 0),\n-        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n-    )\n+    @mark.requires_uvloop\n     def test_custom_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -505,17 +483,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n-    @mark.skipif(\n-        sys.implementation.name == \"pypy\",\n-        reason=\"uvloop does not support pypy properly\",\n-    )\n-    @mark.skipif(\n-        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n-    )\n-    @mark.skipif(\n-        twisted_version == Version(\"twisted\", 21, 2, 0),\n-        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n-    )\n+    @mark.requires_uvloop\n     def test_asyncio_enabled_reactor_same_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -524,17 +492,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    @mark.skipif(\n-        sys.implementation.name == \"pypy\",\n-        reason=\"uvloop does not support pypy properly\",\n-    )\n-    @mark.skipif(\n-        platform.system() == \"Windows\", reason=\"uvloop does not support Windows\"\n-    )\n-    @mark.skipif(\n-        twisted_version == Version(\"twisted\", 21, 2, 0),\n-        reason=\"https://twistedmatrix.com/trac/ticket/10106\",\n-    )\n+    @mark.requires_uvloop\n     def test_asyncio_enabled_reactor_different_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n         self.assertNotIn(\"Spider closed (finished)\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e058a05763b45086edaf8b52f067648c0c50ae21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 10 | Churn Cumulative: 8716 | Contributors (this commit): 59 | Commits (past 90d): 7 | Contributors (cumulative): 85 | DMM Complexity: 0.0\n\nDIFF:\n@@ -87,6 +87,8 @@ def requires_uvloop(request):\n         pytest.skip(\"uvloop does not support Windows\")\n     if twisted_version == Version(\"twisted\", 21, 2, 0):\n         pytest.skip(\"https://twistedmatrix.com/trac/ticket/10106\")\n+    if sys.version_info >= (3, 12):\n+        pytest.skip(\"uvloop doesn't support Python 3.12 yet\")\n \n \n def pytest_configure(config):\n\n@@ -125,6 +125,9 @@ class FileFeedStorageTest(unittest.TestCase):\n             path.unlink()\n \n \n+@pytest.mark.skipif(\n+    sys.version_info >= (3, 12), reason=\"pyftpdlib doesn't support Python 3.12 yet\"\n+)\n class FTPFeedStorageTest(unittest.TestCase):\n     def get_test_spider(self, settings=None):\n         class TestSpider(scrapy.Spider):\n\n@@ -1,6 +1,7 @@\n import dataclasses\n import os\n import random\n+import sys\n import time\n from datetime import datetime\n from io import BytesIO\n@@ -11,6 +12,7 @@ from unittest import mock\n from urllib.parse import urlparse\n \n import attr\n+import pytest\n from itemadapter import ItemAdapter\n from twisted.internet import defer\n from twisted.trial import unittest\n@@ -641,6 +643,9 @@ class TestGCSFilesStore(unittest.TestCase):\n                     store.bucket.get_blob.assert_called_with(expected_blob_path)\n \n \n+@pytest.mark.skipif(\n+    sys.version_info >= (3, 12), reason=\"pyftpdlib doesn't support Python 3.12 yet\"\n+)\n class TestFTPFileStore(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_persist(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a346732275b425e4fbebc3bdf133df961528df87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 280 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -18,6 +18,10 @@ def _py_files(folder):\n collect_ignore = [\n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n+    \"tests/ftpserver.py\",\n+    \"tests/mockserver.py\",\n+    \"tests/pipelines.py\",\n+    \"tests/spiders.py\",\n     # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n     *_py_files(\"tests/CrawlerProcess\"),\n     # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21b6dc5f9fbd2607d6f4df20256dcdd9d5b9dae4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 828 | Contributors (this commit): 37 | Commits (past 90d): 6 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -62,6 +62,7 @@ setup(\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n+        \"Programming Language :: Python :: 3.12\",\n         \"Programming Language :: Python :: Implementation :: CPython\",\n         \"Programming Language :: Python :: Implementation :: PyPy\",\n         \"Topic :: Internet :: WWW/HTTP\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53f8570786fbc7c90bc9990a22279d90a52025b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 811 | Contributors (this commit): 24 | Commits (past 90d): 4 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,7 +48,11 @@ def _get_commands_from_module(module, inproject):\n \n def _get_commands_from_entry_points(inproject, group=\"scrapy.commands\"):\n     cmds = {}\n-    for entry_point in entry_points().get(group, {}):\n+    if sys.version_info >= (3, 10):\n+        eps = entry_points(group=group)\n+    else:\n+        eps = entry_points().get(group, ())\n+    for entry_point in eps:\n         obj = entry_point.load()\n         if inspect.isclass(obj):\n             cmds[entry_point.name] = obj()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d91ea12d642503da95df43cf033eeeda3db1553", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 49 | Churn Cumulative: 1791 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -133,6 +133,13 @@ class JsonItemExporter(BaseItemExporter):\n         if self.indent is not None:\n             self.file.write(b\"\\n\")\n \n+    def _add_comma_after_first(self):\n+        if self.first_item:\n+            self.first_item = False\n+        else:\n+            self.file.write(b\",\")\n+            self._beautify_newline()\n+\n     def start_exporting(self):\n         self.file.write(b\"[\")\n         self._beautify_newline()\n@@ -142,14 +149,10 @@ class JsonItemExporter(BaseItemExporter):\n         self.file.write(b\"]\")\n \n     def export_item(self, item):\n-        if self.first_item:\n-            self.first_item = False\n-        else:\n-            self.file.write(b\",\")\n-            self._beautify_newline()\n         itemdict = dict(self._get_serialized_fields(item))\n-        data = self.encoder.encode(itemdict)\n-        self.file.write(to_bytes(data, self.encoding))\n+        data = to_bytes(self.encoder.encode(itemdict), self.encoding)\n+        self._add_comma_after_first()\n+        self.file.write(data)\n \n \n class XmlItemExporter(BaseItemExporter):\n\n@@ -599,6 +599,20 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n     def test_two_dict_items(self):\n         self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n \n+    def test_two_items_with_failure_between(self):\n+        i1 = TestItem(name=\"Joseph\\xa3\", age=\"22\")\n+        i2 = TestItem(\n+            name=\"Maria\", age=1j\n+        )  # Invalid datetimes didn't consistently fail between Python versions\n+        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        self.ie.start_exporting()\n+        self.ie.export_item(i1)\n+        self.assertRaises(TypeError, self.ie.export_item, i2)\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue()))\n+        self.assertEqual(exported, [dict(i1), dict(i3)])\n+\n     def test_nested_item(self):\n         i1 = self.item_class(name=\"Joseph\\xa3\", age=\"22\")\n         i2 = self.item_class(name=\"Maria\", age=i1)\n@@ -637,6 +651,24 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.assertEqual(exported, [item])\n \n \n+class JsonItemExporterToBytesTest(BaseItemExporterTest):\n+    def _get_exporter(self, **kwargs):\n+        kwargs[\"encoding\"] = \"latin\"\n+        return JsonItemExporter(self.output, **kwargs)\n+\n+    def test_two_items_with_failure_between(self):\n+        i1 = TestItem(name=\"Joseph\", age=\"22\")\n+        i2 = TestItem(name=\"\\u263a\", age=\"11\")\n+        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        self.ie.start_exporting()\n+        self.ie.export_item(i1)\n+        self.assertRaises(UnicodeEncodeError, self.ie.export_item, i2)\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(to_unicode(self.output.getvalue(), encoding=\"latin\"))\n+        self.assertEqual(exported, [dict(i1), dict(i3)])\n+\n+\n class JsonItemExporterDataclassTest(JsonItemExporterTest):\n     item_class = TestDataClass\n     custom_field_item_class = CustomFieldDataclass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7522aeed357d90ed95ee10d3b5a506f1b1049d1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 67 | Churn Cumulative: 4901 | Contributors (this commit): 61 | Commits (past 90d): 10 | Contributors (cumulative): 92 | DMM Complexity: 0.9803921568627451\n\nDIFF:\n@@ -9,7 +9,7 @@ import re\n import sys\n import warnings\n from datetime import datetime\n-from pathlib import Path\n+from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n from typing import IO, Any, Callable, List, Optional, Tuple, Union\n from urllib.parse import unquote, urlparse\n@@ -615,7 +615,7 @@ class FeedExporter:\n \n     def _storage_supported(self, uri, feed_options):\n         scheme = urlparse(uri).scheme\n-        if scheme in self.storages:\n+        if scheme in self.storages or PureWindowsPath(uri).drive:\n             try:\n                 self._get_storage(uri, feed_options)\n                 return True\n@@ -633,7 +633,7 @@ class FeedExporter:\n         It supports not passing the *feed_options* parameters to classes that\n         do not support it, and issuing a deprecation warning instead.\n         \"\"\"\n-        feedcls = self.storages[urlparse(uri).scheme]\n+        feedcls = self.storages.get(urlparse(uri).scheme, self.storages[\"file\"])\n         crawler = getattr(self, \"crawler\", None)\n \n         def build_instance(builder, *preargs):\n\n@@ -202,7 +202,8 @@ def feed_process_params_from_cli(\n     for element in output:\n         try:\n             feed_uri, feed_format = element.rsplit(\":\", 1)\n-        except ValueError:\n+            check_valid_format(feed_format)\n+        except (ValueError, UsageError):\n             feed_uri = element\n             feed_format = Path(element).suffix.replace(\".\", \"\")\n         else:\n\n@@ -907,6 +907,64 @@ class MySpider(scrapy.Spider):\n         log = self.get_log(spider_code, args=args)\n         self.assertIn(\"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\", log)\n \n+    @skipIf(platform.system() == \"Windows\", reason=\"Linux only\")\n+    def test_absolute_path_linux(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    start_urls = [\"data:,\"]\n+\n+    def parse(self, response):\n+        yield {\"hello\": \"world\"}\n+        \"\"\"\n+        temp_dir = mkdtemp()\n+\n+        args = [\"-o\", f\"{temp_dir}/output1.json:json\"]\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output1.json\",\n+            log,\n+        )\n+\n+        args = [\"-o\", f\"{temp_dir}/output2.json\"]\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}/output2.json\",\n+            log,\n+        )\n+\n+    @skipIf(platform.system() != \"Windows\", reason=\"Windows only\")\n+    def test_absolute_path_windows(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    start_urls = [\"data:,\"]\n+\n+    def parse(self, response):\n+        yield {\"hello\": \"world\"}\n+        \"\"\"\n+        temp_dir = mkdtemp()\n+\n+        args = [\"-o\", f\"{temp_dir}\\\\output1.json:json\"]\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output1.json\",\n+            log,\n+        )\n+\n+        args = [\"-o\", f\"{temp_dir}\\\\output2.json\"]\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\n+            f\"[scrapy.extensions.feedexport] INFO: Stored json feed (1 items) in: {temp_dir}\\\\output2.json\",\n+            log,\n+        )\n+\n \n @skipIf(platform.system() != \"Windows\", \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a1bf40c2f7ac0b5fdd4810336afddbfeb925981", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 46 | Churn Cumulative: 9027 | Contributors (this commit): 51 | Commits (past 90d): 10 | Contributors (cumulative): 86 | DMM Complexity: 0.9069767441860465\n\nDIFF:\n@@ -169,6 +169,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         secret_key=None,\n         acl=None,\n         endpoint_url=None,\n+        region_name=None,\n         *,\n         feed_options=None,\n         session_token=None,\n@@ -183,6 +184,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n         self.endpoint_url = endpoint_url\n+        self.region_name = region_name\n \n         if IS_BOTO3_AVAILABLE:\n             import boto3.session\n@@ -195,6 +197,7 @@ class S3FeedStorage(BlockingFeedStorage):\n                 aws_secret_access_key=self.secret_key,\n                 aws_session_token=self.session_token,\n                 endpoint_url=self.endpoint_url,\n+                region_name=self.region_name,\n             )\n         else:\n             warnings.warn(\n@@ -213,6 +216,7 @@ class S3FeedStorage(BlockingFeedStorage):\n                 aws_secret_access_key=self.secret_key,\n                 aws_session_token=self.session_token,\n                 endpoint_url=self.endpoint_url,\n+                region_name=self.region_name,\n             )\n \n         if feed_options and feed_options.get(\"overwrite\", True) is False:\n@@ -232,6 +236,7 @@ class S3FeedStorage(BlockingFeedStorage):\n             session_token=crawler.settings[\"AWS_SESSION_TOKEN\"],\n             acl=crawler.settings[\"FEED_STORAGE_S3_ACL\"] or None,\n             endpoint_url=crawler.settings[\"AWS_ENDPOINT_URL\"] or None,\n+            region_name=crawler.settings[\"AWS_REGION_NAME\"] or None,\n             feed_options=feed_options,\n         )\n \n\n@@ -349,6 +349,19 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n+    def test_init_with_region_name(self):\n+        region_name = \"ap-east-1\"\n+        storage = S3FeedStorage(\n+            \"s3://mybucket/export.csv\",\n+            \"access_key\",\n+            \"secret_key\",\n+            region_name=region_name,\n+        )\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.region_name, region_name)\n+        self.assertEqual(storage.s3_client._client_config.region_name, region_name)\n+\n     def test_from_crawler_without_acl(self):\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n@@ -377,6 +390,20 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.endpoint_url, None)\n \n+    def test_without_region_name(self):\n+        settings = {\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = S3FeedStorage.from_crawler(\n+            crawler,\n+            \"s3://mybucket/export.csv\",\n+        )\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.s3_client._client_config.region_name, \"us-east-1\")\n+\n     def test_from_crawler_with_acl(self):\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n@@ -404,6 +431,20 @@ class S3FeedStorageTest(unittest.TestCase):\n         self.assertEqual(storage.secret_key, \"secret_key\")\n         self.assertEqual(storage.endpoint_url, \"https://example.com\")\n \n+    def test_from_crawler_with_region_name(self):\n+        region_name = \"ap-east-1\"\n+        settings = {\n+            \"AWS_ACCESS_KEY_ID\": \"access_key\",\n+            \"AWS_SECRET_ACCESS_KEY\": \"secret_key\",\n+            \"AWS_REGION_NAME\": region_name,\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = S3FeedStorage.from_crawler(crawler, \"s3://mybucket/export.csv\")\n+        self.assertEqual(storage.access_key, \"access_key\")\n+        self.assertEqual(storage.secret_key, \"secret_key\")\n+        self.assertEqual(storage.region_name, region_name)\n+        self.assertEqual(storage.s3_client._client_config.region_name, region_name)\n+\n     @defer.inlineCallbacks\n     def test_store_without_acl(self):\n         storage = S3FeedStorage(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a689fe5baf35e50d47270e9a402b1d67b6e95ffe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1765 | Contributors (this commit): 42 | Commits (past 90d): 7 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -169,10 +169,10 @@ class S3FeedStorage(BlockingFeedStorage):\n         secret_key=None,\n         acl=None,\n         endpoint_url=None,\n-        region_name=None,\n         *,\n         feed_options=None,\n         session_token=None,\n+        region_name=None,\n     ):\n         if not is_botocore_available():\n             raise NotConfigured(\"missing botocore library\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4f0aa4fdfff802c153b1eeda711957d063aed60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 12 | Files Changed: 5 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 7881 | Contributors (this commit): 56 | Commits (past 90d): 12 | Contributors (cumulative): 122 | DMM Complexity: 1.0\n\nDIFF:\n@@ -228,14 +228,14 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n     def test_crawler_runner_bootstrap_successful(self):\n         runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n-        self.assertEqual(runner.bootstrap_failed, False)\n+        self.assertFalse(runner.bootstrap_failed)\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_successful_for_several(self):\n         runner = self._runner()\n         yield runner.crawl(NoRequestsSpider)\n         yield runner.crawl(NoRequestsSpider)\n-        self.assertEqual(runner.bootstrap_failed, False)\n+        self.assertFalse(runner.bootstrap_failed)\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_failed(self):\n@@ -248,7 +248,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         else:\n             self.fail(\"Exception should be raised from spider\")\n \n-        self.assertEqual(runner.bootstrap_failed, True)\n+        self.assertTrue(runner.bootstrap_failed)\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_failed_for_several(self):\n@@ -263,7 +263,7 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n         yield runner.crawl(NoRequestsSpider)\n \n-        self.assertEqual(runner.bootstrap_failed, True)\n+        self.assertTrue(runner.bootstrap_failed)\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_asyncio_enabled_true(self):\n\n@@ -748,7 +748,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n         yield crawler.crawl(seed=request)\n         # download_maxsize = 50 is enough for the gzipped response\n         failure = crawler.spider.meta.get(\"failure\")\n-        self.assertTrue(failure is None)\n+        self.assertIsNone(failure)\n         reason = crawler.spider.meta[\"close_reason\"]\n         self.assertTrue(reason, \"finished\")\n \n\n@@ -229,7 +229,7 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n         self.assertStatsEqual(\"httpcompression/response_count\", 1)\n-        self.assertStatsEqual(\"httpcompression/response_bytes\", 104)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n \n     def test_process_response_force_recalculate_encoding(self):\n         headers = {\n@@ -254,7 +254,7 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n         self.assertStatsEqual(\"httpcompression/response_count\", 1)\n-        self.assertStatsEqual(\"httpcompression/response_bytes\", 104)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n \n     def test_process_response_no_content_type_header(self):\n         headers = {\n@@ -277,7 +277,7 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding(\"gb2312\"))\n         self.assertStatsEqual(\"httpcompression/response_count\", 1)\n-        self.assertStatsEqual(\"httpcompression/response_bytes\", 104)\n+        self.assertStatsEqual(\"httpcompression/response_bytes\", len(plainbody))\n \n     def test_process_response_gzipped_contenttype(self):\n         response = self._getresponse(\"gzip\")\n\n@@ -468,8 +468,13 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         pipeline = UserDefinedFilesPipeline.from_settings(\n             Settings({\"FILES_STORE\": self.tempdir})\n         )\n-        self.assertEqual(pipeline.files_result_field, \"this\")\n-        self.assertEqual(pipeline.files_urls_field, \"that\")\n+        self.assertEqual(\n+            pipeline.files_result_field,\n+            UserDefinedFilesPipeline.DEFAULT_FILES_RESULT_FIELD,\n+        )\n+        self.assertEqual(\n+            pipeline.files_urls_field, UserDefinedFilesPipeline.DEFAULT_FILES_URLS_FIELD\n+        )\n \n     def test_user_defined_subclass_default_key_names(self):\n         \"\"\"Test situation when user defines subclass of FilesPipeline,\n\n@@ -598,8 +598,14 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         pipeline = UserDefinedImagePipeline.from_settings(\n             Settings({\"IMAGES_STORE\": self.tempdir})\n         )\n-        self.assertEqual(pipeline.images_result_field, \"something_else\")\n-        self.assertEqual(pipeline.images_urls_field, \"something\")\n+        self.assertEqual(\n+            pipeline.images_result_field,\n+            UserDefinedImagePipeline.DEFAULT_IMAGES_RESULT_FIELD,\n+        )\n+        self.assertEqual(\n+            pipeline.images_urls_field,\n+            UserDefinedImagePipeline.DEFAULT_IMAGES_URLS_FIELD,\n+        )\n \n     def test_user_defined_subclass_default_key_names(self):\n         \"\"\"Test situation when user defines subclass of ImagePipeline,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8c5b415597f9a58e5bf10ab1d8838bd7f730949", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 107 | Churn Cumulative: 281 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.8169014084507042\n\nDIFF:\n@@ -1,18 +1,24 @@\n+import itertools\n import unittest\n-from typing import Any, Dict, Optional\n+from typing import Any, Dict\n \n-from scrapy.crawler import Crawler\n-from scrapy.settings import BaseSettings\n+from scrapy import Spider\n+from scrapy.crawler import Crawler, CrawlerRunner\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.test import get_crawler\n \n \n-class GoodAddon:\n-    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:\n-        super().__init__()\n-        self.config = config or {}\n-\n+class SimpleAddon:\n     def update_settings(self, settings):\n-        settings.update(self.config, \"addon\")\n+        pass\n+\n+\n+def get_addon_cls(config: Dict[str, Any]) -> type:\n+    class AddonWithConfig:\n+        def update_settings(self, settings: BaseSettings):\n+            settings.update(config, priority=\"addon\")\n+\n+    return AddonWithConfig\n \n \n class CreateInstanceAddon:\n@@ -35,7 +41,7 @@ class AddonTest(unittest.TestCase):\n         settings.set(\"KEY1\", \"default\", priority=\"default\")\n         settings.set(\"KEY2\", \"project\", priority=\"project\")\n         addon_config = {\"KEY1\": \"addon\", \"KEY2\": \"addon\", \"KEY3\": \"addon\"}\n-        testaddon = GoodAddon(addon_config)\n+        testaddon = get_addon_cls(addon_config)()\n         testaddon.update_settings(settings)\n         self.assertEqual(settings[\"KEY1\"], \"addon\")\n         self.assertEqual(settings[\"KEY2\"], \"project\")\n@@ -45,11 +51,27 @@ class AddonTest(unittest.TestCase):\n class AddonManagerTest(unittest.TestCase):\n     def test_load_settings(self):\n         settings_dict = {\n-            \"ADDONS\": {\"tests.test_addons.GoodAddon\": 0},\n+            \"ADDONS\": {\"tests.test_addons.SimpleAddon\": 0},\n         }\n         crawler = get_crawler(settings_dict=settings_dict)\n         manager = crawler.addons\n-        self.assertIsInstance(manager.addons[0], GoodAddon)\n+        self.assertIsInstance(manager.addons[0], SimpleAddon)\n+\n+    def test_load_settings_order(self):\n+        # Get three addons with different settings\n+        addonlist = []\n+        for i in range(3):\n+            addon = get_addon_cls({\"KEY1\": i})\n+            addon.number = i\n+            addonlist.append(addon)\n+        # Test for every possible ordering\n+        for ordered_addons in itertools.permutations(addonlist):\n+            expected_order = [a.number for a in ordered_addons]\n+            settings = {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n+            crawler = get_crawler(settings_dict=settings)\n+            manager = crawler.addons\n+            self.assertEqual([a.number for a in manager.addons], expected_order)\n+            self.assertEqual(crawler.settings.getint(\"KEY1\"), expected_order[-1])\n \n     def test_create_instance(self):\n         settings_dict = {\n@@ -60,3 +82,64 @@ class AddonManagerTest(unittest.TestCase):\n         manager = crawler.addons\n         self.assertIsInstance(manager.addons[0], CreateInstanceAddon)\n         self.assertEqual(crawler.settings.get(\"MYADDON_KEY\"), \"val\")\n+\n+    def test_settings_priority(self):\n+        config = {\n+            \"KEY\": 15,  # priority=addon\n+        }\n+        settings_dict = {\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 15)\n+\n+        settings_dict = {\n+            \"KEY\": 20,  # priority=project\n+            \"ADDONS\": {get_addon_cls(config): 1},\n+        }\n+        settings = Settings(settings_dict)\n+        settings.set(\"KEY\", 0, priority=\"default\")\n+        runner = CrawlerRunner(settings)\n+        crawler = runner.create_crawler(Spider)\n+        self.assertEqual(crawler.settings.getint(\"KEY\"), 20)\n+\n+    def test_fallback_workflow(self):\n+        FALLBACK_SETTING = \"MY_FALLBACK_DOWNLOAD_HANDLER\"\n+\n+        class AddonWithFallback:\n+            def update_settings(self, settings):\n+                if not settings.get(FALLBACK_SETTING):\n+                    settings.set(\n+                        FALLBACK_SETTING,\n+                        settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"],\n+                        \"addon\",\n+                    )\n+                settings[\"DOWNLOAD_HANDLERS\"][\"https\"] = \"AddonHandler\"\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(\n+            crawler.settings.get(FALLBACK_SETTING),\n+            \"scrapy.core.downloader.handlers.http.HTTPDownloadHandler\",\n+        )\n+\n+        settings_dict = {\n+            \"ADDONS\": {AddonWithFallback: 1},\n+            \"DOWNLOAD_HANDLERS\": {\"https\": \"UserHandler\"},\n+        }\n+        crawler = get_crawler(settings_dict=settings_dict)\n+        self.assertEqual(\n+            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")[\"https\"], \"AddonHandler\"\n+        )\n+        self.assertEqual(crawler.settings.get(FALLBACK_SETTING), \"UserHandler\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
