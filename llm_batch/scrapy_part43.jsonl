{"custom_id": "scrapy#53983e6e234f8d040f4d1a1088693381c58d4513", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 38 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 44 | Churn Cumulative: 219 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -2,8 +2,8 @@\n \n from scrapy.contrib_exp import newitem \n \n-from scrapy.contrib_exp.newitem import extractors\n-from scrapy.contrib_exp import adaptors\n+from scrapy.contrib_exp.newitem.extractors import ItemExtractor, adaptor\n+from scrapy.contrib_exp.adaptors import extract, strip\n \n \n class GoogledirItem(newitem.Item):\n@@ -12,9 +12,9 @@ class GoogledirItem(newitem.Item):\n     description = newitem.StringField()\n \n \n-class GoogledirItemExtractor(extractors.ItemExtractor):\n+class GoogledirItemExtractor(ItemExtractor):\n     item_class = GoogledirItem\n \n-    name = extractors.ExtractorField([adaptors.extract, adaptors.strip])\n-    url = extractors.ExtractorField([adaptors.extract, adaptors.strip])\n-    description = extractors.ExtractorField([adaptors.extract, adaptors.strip])\n+    name = adaptor(extract, strip)\n+    url = adaptor(extract, strip)\n+    description = adaptor(extract, strip)\n\n@@ -38,38 +38,6 @@ class ItemExtractor(object):\n             return object.__getattribute__(self, name)\n \n \n-class ExtractorField(object):\n-    def __init__(self, funcs):\n-        if not hasattr(funcs, '__iter__'):\n-            raise TypeError(\n-                'You must initialize ExtractorField with a list of callables')\n-\n-        self._funcs = []\n-\n-        for func in funcs:\n-            func_args = get_func_args(func)\n-            takes_args = True if 'adaptor_args' in func_args else False\n-            self._funcs.append((func, takes_args))\n-\n-    def __call__(self, value, adaptor_args=None):\n-        values = [value]\n-\n-        for func, takes_args in self._funcs:\n-            next_round = []\n-\n-            for val in values:\n-                val = func(val, adaptor_args) if takes_args else func(val)\n-\n-                if isinstance(val, tuple):\n-                    next_round.extend(val)\n-                elif val is not None:\n-                    next_round.append(val)\n-\n-            values = next_round\n-\n-        return list(values)\n-\n-\n def adaptor(*funcs, **adaptor_args):\n     \"\"\"A pipe adaptor implementing the tree adaption logic\n     \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b26102745a19134cd608278169425d1466c40921", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 197 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,7 @@ class ItemExtractor(object):\n         return fe\n \n     def __setattr__(self, name, value):\n-        if (name.startswith('_') or name == 'item_instance'):\n+        if name.startswith('_') or name == 'item_instance':\n             return object.__setattr__(self, name, value)\n \n         try:\n@@ -32,16 +32,16 @@ class ItemExtractor(object):\n         setattr(self.item_instance, name, final)\n \n     def __getattribute__(self, name):\n-        if not (name.startswith('_') or name.startswith('item_')):\n-            return getattr(self.item_instance, name)\n-        else:\n+        if name.startswith('_') or name.startswith('item_'):\n             return object.__getattribute__(self, name)\n \n+        return getattr(self.item_instance, name)\n+\n \n def adaptor(*funcs, **adaptor_args):\n     \"\"\"A pipe adaptor implementing the tree adaption logic\n     \n-    It takes multiples unnamed arguments used as functions of the pipe, and a\n+    It takes multiples unnamed arguments used as functions of the pipe, and\n     keywords used as adaptor_args to be passed to functions that supports it\n \n     If an adaptor function returns a list of values, each value is used as\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8398212404e2f2ed1006042f20443f03763ed763", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 273 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n \n from scrapy.contrib_exp import newitem \n \n-from scrapy.contrib_exp.newitem.extractors import ItemExtractor, adaptor\n+from scrapy.contrib_exp.newitem.extractors import ItemAdaptor, adaptor\n from scrapy.contrib_exp.adaptors import extract, strip\n \n \n@@ -12,7 +12,7 @@ class GoogledirItem(newitem.Item):\n     description = newitem.StringField()\n \n \n-class GoogledirItemExtractor(ItemExtractor):\n+class GoogledirItemAdaptor(ItemAdaptor):\n     item_class = GoogledirItem\n \n     name = adaptor(extract, strip)\n\n@@ -5,7 +5,7 @@ from scrapy.xpath import HtmlXPathSelector\n from scrapy.contrib.spiders import CrawlSpider, rule\n from scrapy.utils.misc import items_to_csv\n \n-from googledir.items import GoogledirItem, GoogledirItemExtractor\n+from googledir.items import GoogledirItem, GoogledirItemAdaptor\n \n class GoogleDirectorySpider(CrawlSpider):\n     domain_name = 'google.com'\n@@ -24,7 +24,7 @@ class GoogleDirectorySpider(CrawlSpider):\n         links = hxs.x('//td[descendant::a[contains(@href, \"#pagerank\")]]/following-sibling::td/font')\n \n         for link in links:\n-            extractor = GoogledirItemExtractor()\n+            extractor = GoogledirItemAdaptor()\n             extractor.name =  link.x('a/text()')\n             extractor.url = link.x('a/@href')\n             extractor.description = link.x('font[2]/text()')\n\n@@ -1,7 +1,7 @@\n from scrapy.utils.python import get_func_args\n \n \n-class ItemExtractor(object):\n+class ItemAdaptor(object):\n     def __init__(self, response=None, item=None):\n         if item:\n             self.item_instance = item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5afa621d6ad0defd06c0b8970d485b0709b84c97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@\n \n from scrapy.contrib_exp import newitem \n \n-from scrapy.contrib_exp.newitem.extractors import ItemAdaptor, adaptor\n+from scrapy.contrib_exp.newitem.adaptors import ItemAdaptor, adaptor\n from scrapy.contrib_exp.adaptors import extract, strip\n \n \n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#119706ae232bd54de6dced19cae85b6aef16ac8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 9 | Churn Cumulative: 106 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -54,6 +54,7 @@ def headers_dict_to_raw(headers_dict):\n                 raw_lines.append(\"%s: %s\" % (key, v))\n     return '\\r\\n'.join(raw_lines)\n \n+\n class Headers(CaselessDict):\n     def __init__(self, dictorstr=None, fromdict=None, fromstr=None, encoding='utf-8'):\n         self.encoding = encoding\n@@ -85,13 +86,5 @@ class Headers(CaselessDict):\n             value = value.encode(self.encoding)\n         super(Headers, self).__setitem__(key, value)\n \n-    def tostring(self):\n-        return headers_dict_to_raw(self)\n-\n-    def rawsize(self):\n-        \"\"\"Estimated size of raw HTTP headers, in bytes\"\"\"\n-        # For each header line you have 4 extra chars: \": \" and CRLF\n-        return sum([len(k)+len(v)+4 for k, v in self.iteritems()])\n-\n     def to_string(self):\n         return headers_dict_to_raw(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d504a2b4283e4d013c2e624ace80bef989d30b5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n \n import unittest\n-from scrapy.contrib_exp.newitem.extractors import adaptor, ItemExtractor\n+from scrapy.contrib_exp.newitem.adaptors import adaptor, ItemAdaptor\n \n \n class TreeadaptTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4b55bfd203b22db2f101f804ddfbdb015ad90e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 7 | Methods Changed: 17 | Complexity Δ (Sum/Max): 5/8 | Churn Δ: 118 | Churn Cumulative: 820 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,83 @@\n+import unittest\n+\n+from scrapy.utils.datatypes import CaselessDict\n+\n+\n+class CaselessDictTest(unittest.TestCase):\n+\n+    def test_init(self):\n+        seq = {'red': 1, 'black': 3}\n+        d = CaselessDict(seq)\n+        self.assertEqual(d['red'], 1)\n+        self.assertEqual(d['black'], 3)\n+\n+        seq = (('red', 1), ('black', 3))\n+        d = CaselessDict(seq)\n+        self.assertEqual(d['red'], 1)\n+        self.assertEqual(d['black'], 3)\n+\n+    def test_caseless(self):\n+        d = CaselessDict()\n+        d['key_Lower'] = 1\n+        self.assertEqual(d['KEy_loWer'], 1)\n+        self.assertEqual(d.get('KEy_loWer'), 1)\n+\n+        d['KEY_LOWER'] = 3\n+        self.assertEqual(d['key_Lower'], 3)\n+        self.assertEqual(d.get('key_Lower'), 3)\n+\n+    def test_delete(self):\n+        d = CaselessDict({'key_lower': 1})\n+        del d['key_LOWER']\n+        self.assertRaises(KeyError, d.__getitem__, 'key_LOWER')\n+        self.assertRaises(KeyError, d.__getitem__, 'key_lower')\n+\n+    def test_getdefault(self):\n+        d = CaselessDict()\n+        self.assertEqual(d.get('c', 5), 5)\n+        d['c'] = 10\n+        self.assertEqual(d.get('c', 5), 10)\n+\n+    def test_setdefault(self):\n+        d = CaselessDict({'a': 1, 'b': 2})\n+\n+        r = d.setdefault('A', 5)\n+        self.assertEqual(r, 1)\n+        self.assertEqual(d['A'], 1)\n+\n+        r = d.setdefault('c', 5)\n+        self.assertEqual(r, 5)\n+        self.assertEqual(d['C'], 5)\n+\n+    def test_fromkeys(self):\n+        keys = ('a', 'b')\n+\n+        d = CaselessDict.fromkeys(keys)\n+        self.assertEqual(d['A'], None)\n+        self.assertEqual(d['B'], None)\n+\n+        d = CaselessDict.fromkeys(keys, 1)\n+        self.assertEqual(d['A'], 1)\n+        self.assertEqual(d['B'], 1)\n+\n+        instance = CaselessDict()\n+        d = instance.fromkeys(keys)\n+        self.assertEqual(d['A'], None)\n+        self.assertEqual(d['B'], None)\n+\n+        d = instance.fromkeys(keys, 1)\n+        self.assertEqual(d['A'], 1)\n+        self.assertEqual(d['B'], 1)\n+\n+    def test_contains(self):\n+        d = CaselessDict()\n+        d['a'] = 1\n+        assert 'a' in d\n+        assert d.has_key('a')\n+\n+    def test_pop(self):\n+        d = CaselessDict()\n+        d['a'] = 1\n+        self.assertEqual(d.pop('A'), 1)\n+        self.assertRaises(KeyError, d.pop, 'A')\n+\n\n@@ -184,15 +184,10 @@ class SiteNode(object):\n         return s\n \n class CaselessDict(dict):\n-    def __init__(self, other=None):\n-        if other:\n-            # Doesn't do keyword args\n-            if isinstance(other, dict):\n-                for k, v in other.items():\n-                    dict.__setitem__(self, self.normkey(k), v)\n-            else:\n-                for k, v in other:\n-                    dict.__setitem__(self, self.normkey(k), v)\n+    def __init__(self, seq=None):\n+        dict.__init__(self)\n+        if seq:\n+            self.update(seq)\n \n     def __getitem__(self, key):\n         return dict.__getitem__(self, self.normkey(key))\n@@ -205,31 +200,27 @@ class CaselessDict(dict):\n \n     def __contains__(self, key):\n         return dict.__contains__(self, self.normkey(key))\n+    has_key = __contains__\n \n     def normkey(self, key):\n         return key.lower()\n \n-    def has_key(self, key):\n-        return dict.has_key(self, self.normkey(key))\n-\n     def get(self, key, def_val=None):\n         return dict.get(self, self.normkey(key), def_val)\n \n     def setdefault(self, key, def_val=None):\n         return dict.setdefault(self, self.normkey(key), def_val)\n \n-    def update(self, other):\n-        for k, v in other.items():\n-            dict.__setitem__(self, self.normkey(k), v)\n+    def update(self, seq):\n+        items = seq.iteritems() if isinstance(seq, dict) else seq\n+        dict.update(self, ((self.normkey(k), v) for k, v in items))\n \n-    def fromkeys(self, iterable, value=None):\n-        d = CaselessDict()\n-        for k in iterable:\n-            dict.__setitem__(d, self.normkey(k), value)\n-        return d\n+    @classmethod\n+    def fromkeys(cls, keys, value=None):\n+        return cls((k, value) for k in keys)\n \n-    def pop(self, key, def_val=None):\n-        return dict.pop(self, self.normkey(key), def_val)\n+    def pop(self, key, *args):\n+        return dict.pop(self, self.normkey(key), *args)\n \n class PriorityQueue(object):\n     \"\"\"A simple priority queue\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#934535740bfdd1c643f875905f919ed6faecd778", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 82 | Files Changed: 5 | Hunks: 19 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/11 | Churn Δ: 201 | Churn Cumulative: 1478 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,10 +10,10 @@ from pydispatch import dispatcher\n from scrapy.core import signals\n from scrapy import log\n from scrapy.http import Response, Headers\n-from scrapy.http.headers import headers_dict_to_raw\n from scrapy.core.exceptions import NotConfigured, HttpException, IgnoreRequest\n from scrapy.core.downloader.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n+from scrapy.utils.http import headers_dict_to_raw, headers_raw_to_dict\n from scrapy.conf import settings\n \n class CacheMiddleware(object):\n@@ -149,7 +149,7 @@ class Cache(object):\n             responseheaders = f.read()\n \n         url = metadata['url']\n-        headers = Headers(responseheaders)\n+        headers = Headers(headers_raw_to_dict(responseheaders))\n         status = metadata['status']\n \n         respcls = responsetypes.from_args(headers=headers, url=url)\n\n@@ -1,90 +1,23 @@\n from scrapy.utils.datatypes import CaselessDict\n-\n-def headers_raw_to_dict(headers_raw):\n-    \"\"\"\n-    Convert raw headers (single multi-line string)\n-    to the dictionary.\n-    \n-    For example:\n-    >>> headers_raw_to_dict(\"Content-type: text/html\\\\n\\\\rAccept: gzip\\\\n\\\\n\")\n-    {'Content-type': ['text/html'], 'Accept': ['gzip']}\n-    \n-    Incorrect input:\n-    >>> headers_raw_to_dict(\"Content-typt gzip\\\\n\\\\n\")\n-    {}\n-    \n-    Argument is None:\n-    >>> headers_raw_to_dict(None)\n-    \"\"\"\n-    if headers_raw is None:\n-        return None \n-    return dict([\n-        (header_item[0].strip(), [header_item[1].strip()]) \n-        for header_item \n-        in [\n-            header.split(':', 1) \n-            for header \n-            in headers_raw.splitlines()] \n-        if len(header_item) == 2])\n-        \n-def headers_dict_to_raw(headers_dict):\n-    \"\"\"\n-    Returns a raw HTTP headers representation of headers\n-    \n-    For example:\n-    >>> headers_dict_to_raw({'Content-type': 'text/html', 'Accept': 'gzip'})\n-    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n-    >>> from twisted.python.util import InsensitiveDict\n-    >>> td = InsensitiveDict({'Content-type': ['text/html'], 'Accept': ['gzip']})\n-    >>> headers_dict_to_raw(td)\n-    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n-    \n-    Argument is None:\n-    >>> headers_dict_to_raw(None)\n-    \n-    \"\"\"\n-    if headers_dict is None:\n-        return None\n-    raw_lines = []\n-    for key, value in headers_dict.items():\n-        if isinstance(value, (str, unicode)):\n-            raw_lines.append(\"%s: %s\" % (key, value))\n-        elif isinstance(value, (list, tuple)):\n-            for v in value:\n-                raw_lines.append(\"%s: %s\" % (key, v))\n-    return '\\r\\n'.join(raw_lines)\n+from scrapy.utils.http import headers_dict_to_raw\n \n \n class Headers(CaselessDict):\n-    def __init__(self, dictorstr=None, fromdict=None, fromstr=None, encoding='utf-8'):\n+    def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n-\n-        if dictorstr is not None:\n-            if isinstance(dictorstr, dict):\n-                d = dictorstr\n-            elif isinstance(dictorstr, basestring):\n-                d = headers_raw_to_dict(dictorstr)\n-        elif fromdict is not None:\n-            d = fromdict\n-        elif fromstr is not None:\n-            d = headers_raw_to_dict(fromstr)\n-        else:\n-            d = {}\n-\n-        # can't use CaselessDict.__init__(self, d) because it doesn't call __setitem__\n-        for k,v in d.iteritems(): \n-            self.__setitem__(k.lower(), v) \n+        super(Headers, self).__init__(seq)\n \n     def normkey(self, key):\n-        return key.title()\n-\n-    def __setitem__(self, key, value):\n         \"\"\"Headers must not be unicode\"\"\"\n         if isinstance(key, unicode):\n             key = key.encode(self.encoding)\n+        return key.title()\n+\n+    def normvalue(self, value):\n+        \"\"\"Headers must not be unicode\"\"\"\n         if isinstance(value, unicode):\n             value = value.encode(self.encoding)\n-        super(Headers, self).__setitem__(key, value)\n+        return value\n \n     def to_string(self):\n         return headers_dict_to_raw(self)\n\n@@ -81,3 +81,41 @@ class CaselessDictTest(unittest.TestCase):\n         self.assertEqual(d.pop('A'), 1)\n         self.assertRaises(KeyError, d.pop, 'A')\n \n+    def test_normkey(self):\n+        class MyDict(CaselessDict):\n+            def normkey(self, key):\n+                return key.title()\n+\n+        d = MyDict()\n+        d['key-one'] = 2\n+        self.assertEqual(list(d.keys()), ['Key-One'])\n+\n+    def test_normvalue(self):\n+        class MyDict(CaselessDict):\n+            def normvalue(self, value):\n+                if value is not None:\n+                    return value + 1\n+\n+        d = MyDict({'key': 1})\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict()\n+        d['key'] = 1\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict()\n+        d.setdefault('key', 1)\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict()\n+        d.update({'key': 1})\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n+        d = MyDict.fromkeys(('key',), 1)\n+        self.assertEqual(d['key'], 2)\n+        self.assertEqual(d.get('key'), 2)\n+\n\n@@ -183,6 +183,7 @@ class SiteNode(object):\n             s += node.to_string(level+1)\n         return s\n \n+\n class CaselessDict(dict):\n     def __init__(self, seq=None):\n         dict.__init__(self)\n@@ -193,7 +194,7 @@ class CaselessDict(dict):\n         return dict.__getitem__(self, self.normkey(key))\n \n     def __setitem__(self, key, value):\n-        dict.__setitem__(self, self.normkey(key), value)\n+        dict.__setitem__(self, self.normkey(key), self.normvalue(value))\n \n     def __delitem__(self, key):\n         dict.__delitem__(self, self.normkey(key))\n@@ -203,17 +204,23 @@ class CaselessDict(dict):\n     has_key = __contains__\n \n     def normkey(self, key):\n+        \"\"\"Method to normalize dictionary key access\"\"\"\n         return key.lower()\n \n+    def normvalue(self, value):\n+        \"\"\"Method to normalize values prior to be setted\"\"\"\n+        return value\n+\n     def get(self, key, def_val=None):\n-        return dict.get(self, self.normkey(key), def_val)\n+        return dict.get(self, self.normkey(key), self.normvalue(def_val))\n \n     def setdefault(self, key, def_val=None):\n-        return dict.setdefault(self, self.normkey(key), def_val)\n+        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))\n \n     def update(self, seq):\n-        items = seq.iteritems() if isinstance(seq, dict) else seq\n-        dict.update(self, ((self.normkey(k), v) for k, v in items))\n+        seq = seq.iteritems() if isinstance(seq, dict) else seq\n+        iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n+        super(CaselessDict, self).update(iseq)\n \n     @classmethod\n     def fromkeys(cls, keys, value=None):\n@@ -222,6 +229,7 @@ class CaselessDict(dict):\n     def pop(self, key, *args):\n         return dict.pop(self, self.normkey(key), *args)\n \n+\n class PriorityQueue(object):\n     \"\"\"A simple priority queue\"\"\"\n \n\n@@ -0,0 +1,58 @@\n+\n+def headers_raw_to_dict(headers_raw):\n+    \"\"\"\n+    Convert raw headers (single multi-line string)\n+    to the dictionary.\n+\n+    For example:\n+    >>> headers_raw_to_dict(\"Content-type: text/html\\\\n\\\\rAccept: gzip\\\\n\\\\n\")\n+    {'Content-type': ['text/html'], 'Accept': ['gzip']}\n+\n+    Incorrect input:\n+    >>> headers_raw_to_dict(\"Content-typt gzip\\\\n\\\\n\")\n+    {}\n+\n+    Argument is None:\n+    >>> headers_raw_to_dict(None)\n+    \"\"\"\n+    if headers_raw is None:\n+        return None\n+    return dict([\n+        (header_item[0].strip(), [header_item[1].strip()])\n+        for header_item\n+        in [\n+            header.split(':', 1)\n+            for header\n+            in headers_raw.splitlines()]\n+        if len(header_item) == 2])\n+\n+\n+def headers_dict_to_raw(headers_dict):\n+    \"\"\"\n+    Returns a raw HTTP headers representation of headers\n+\n+    For example:\n+    >>> headers_dict_to_raw({'Content-type': 'text/html', 'Accept': 'gzip'})\n+    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n+    >>> from twisted.python.util import InsensitiveDict\n+    >>> td = InsensitiveDict({'Content-type': ['text/html'], 'Accept': ['gzip']})\n+    >>> headers_dict_to_raw(td)\n+    'Content-type: text/html\\\\r\\\\nAccept: gzip'\n+\n+    Argument is None:\n+    >>> headers_dict_to_raw(None)\n+\n+    \"\"\"\n+    if headers_dict is None:\n+        return None\n+    raw_lines = []\n+    for key, value in headers_dict.items():\n+        if isinstance(value, (str, unicode)):\n+            raw_lines.append(\"%s: %s\" % (key, value))\n+        elif isinstance(value, (list, tuple)):\n+            for v in value:\n+                raw_lines.append(\"%s: %s\" % (key, v))\n+    return '\\r\\n'.join(raw_lines)\n+\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#987ceb540e58590fe2ed9cf30f7e74e46900e6e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 700 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ class ScrapedItem(object):\n             for attr, value in data.iteritems():\n                 setattr(self, attr, value)\n         elif data is not None:\n-            raise UsageError(\"Initialize with dict, not %s\" % data.__class__.__name__)\n+            raise TypeError(\"Initialize with dict, not %s\" % data.__class__.__name__)\n \n     def __repr__(self):\n         \"\"\"\n\n@@ -0,0 +1,21 @@\n+import unittest\n+\n+from scrapy.item import ScrapedItem\n+\n+class ItemTestCase(unittest.TestCase):\n+\n+    def test_item(self):\n+        \n+        class MyItem(ScrapedItem):\n+            pass\n+\n+        item = MyItem()\n+        self.assertEqual(repr(item), 'MyItem({})')\n+\n+        item = ScrapedItem({'key': 'value'})\n+        self.assertEqual(item.key, 'value')\n+\n+        self.assertRaises(TypeError, ScrapedItem, 10)\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d3489ac100b9e894ccdb3a02da1f0a5e21cc480", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 193 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,8 @@ from scrapy.utils.http import headers_dict_to_raw\n \n \n class Headers(CaselessDict):\n+    \"\"\"Case insensitive http headers dictionary\"\"\"\n+\n     def __init__(self, seq=None, encoding='utf-8'):\n         self.encoding = encoding\n         super(Headers, self).__init__(seq)\n@@ -10,7 +12,7 @@ class Headers(CaselessDict):\n     def normkey(self, key):\n         \"\"\"Headers must not be unicode\"\"\"\n         if isinstance(key, unicode):\n-            key = key.encode(self.encoding)\n+            return key.title().encode(self.encoding)\n         return key.title()\n \n     def normvalue(self, value):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5de0816d9ff91100f8685f6ff2ca2fe95fe38201", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 35 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,13 @@ class UrlClassTest(unittest.TestCase):\n         self.assertEqual(None, u.username)\n         self.assertEqual(None, u.password)\n \n+        u = Url(\"http://scrapy.org:8080/wiki/info\")\n+        self.assertEqual(\"scrapy.org\", u.hostname)\n+        self.assertEqual(\"/wiki/info\", u.path)\n+        self.assertEqual(8080, u.port)\n+        self.assertEqual(None, u.username)\n+        self.assertEqual(None, u.password)\n+\n         u = Url(\"http://someuser:somepass@example.com/ticket/query?owner=pablo\")\n         self.assertEqual(\"someuser\", u.username)\n         self.assertEqual(\"somepass\", u.password)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82dc57e59b10b25f259f7dec82c3124bdb53686a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 129 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 10 | Methods Changed: 23 | Complexity Δ (Sum/Max): 24/17 | Churn Δ: 137 | Churn Cumulative: 1014 | Contributors (this commit): 3 | Commits (past 90d): 30 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,12 +21,12 @@ class RedirectMiddleware(object):\n         response = exception.response\n \n         if status in [302, 303]:\n-            redirected_url = urljoin(request.url, response.headers['location'][0])\n+            redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url, method='GET', body=None)\n             return self._redirect(redirected, request, spider, status)\n \n         if status in [301, 307]:\n-            redirected_url = urljoin(request.url, response.headers['location'][0])\n+            redirected_url = urljoin(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url)\n             return self._redirect(redirected, request, spider, status)\n \n\n@@ -97,7 +97,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n                 return # returning None force download\n \n             # check if last modified date did not expires\n-            last_modified = response.headers['Last-Modified'][0]\n+            last_modified = response.headers['Last-Modified']\n             modified_tuple = rfc822.parsedate_tz(last_modified)\n             modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n             age_seconds = time.time() - modified_stamp\n@@ -106,7 +106,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             if age_days > self.image_refresh_days:\n                 return # returning None force download\n \n-            etag = response.headers['Etag'][0].strip('\"')\n+            etag = response.headers['Etag'].strip('\"')\n             referer = request.headers.get('Referer')\n             log.msg('Image (uptodate) type=%s at <%s> referred from <%s>' % \\\n                     (self.MEDIA_TYPE, request.url, referer), level=log.DEBUG, domain=info.domain)\n\n@@ -55,9 +55,9 @@ class ResponseTypes(object):\n         headers\"\"\"\n         cls = Response\n         if 'Content-Type' in headers:\n-            cls = self.from_content_type(headers['Content-type'][0])\n+            cls = self.from_content_type(headers['Content-type'])\n         if cls is Response and 'Content-Disposition' in headers:\n-            cls = self.from_content_disposition(headers['Content-Disposition'][0])\n+            cls = self.from_content_disposition(headers['Content-Disposition'])\n         return cls\n \n     def from_filename(self, filename):\n\n@@ -19,7 +19,52 @@ class Headers(CaselessDict):\n         \"\"\"Headers must not be unicode\"\"\"\n         if isinstance(value, unicode):\n             value = value.encode(self.encoding)\n-        return value\n+\n+        if isinstance(value, (list, tuple)):\n+            return list(value)\n+        return [value]\n+\n+    def __getitem__(self, key):\n+        try:\n+            return super(Headers, self).__getitem__(key)[-1]\n+        except IndexError:\n+            return None\n+\n+    def get(self, key, def_val=None):\n+        try:\n+            return super(Headers, self).get(key, def_val)[-1]\n+        except IndexError:\n+            return None\n+\n+    def getlist(self, key, def_val=None):\n+        try:\n+            return super(Headers, self).__getitem__(key)\n+        except KeyError:\n+            if def_val is not None:\n+                return self.normvalue(def_val)\n+            return []\n+\n+    def setlist(self, key, list_):\n+        self[key] = list_\n+\n+    def setlistdefault(self, key, default_list=()):\n+        self.setdefault(key, default_list)\n+\n+    def appendlist(self, key, value):\n+        self.getlist(key).append(self.normvalue(value))\n+\n+    def items(self):\n+        return list(self.iteritems())\n+\n+    def iteritems(self):\n+        return ((k, self[k]) for k in self.keys())\n+\n+    def values(self):\n+        return [self[k] for k in self.keys()]\n+\n+    def lists(self):\n+        return super(Headers, self).items()\n \n     def to_string(self):\n         return headers_dict_to_raw(self)\n+\n\n@@ -50,7 +50,7 @@ class TextResponse(Response):\n             headers = self.headers\n         content_type = headers.get('Content-Type')\n         if content_type:\n-            encoding = self._ENCODING_RE.search(content_type[0])\n+            encoding = self._ENCODING_RE.search(content_type)\n             if encoding:\n                 return encoding.group(1)\n \n\n@@ -0,0 +1,76 @@\n+import unittest\n+\n+from scrapy.http import Headers\n+\n+class HeadersTest(unittest.TestCase):\n+    def test_basics(self):\n+        h = Headers({'Content-Type': 'text/html', 'Content-Length': 1234})\n+        assert h['Content-Type']\n+        assert h['Content-Length']\n+\n+        self.assertRaises(KeyError, h.__getitem__, 'Accept')\n+        self.assertEqual(h.get('Accept'), None)\n+        self.assertEqual(h.getlist('Accept'), [])\n+\n+        self.assertEqual(h.get('Accept', '*/*'), '*/*')\n+        self.assertEqual(h.getlist('Accept', '*/*'), ['*/*'])\n+        self.assertEqual(h.getlist('Accept', ['text/html', 'images/jpeg']), ['text/html','images/jpeg'])\n+\n+    def test_single_value(self):\n+        h = Headers()\n+        h['Content-Type'] = 'text/html'\n+        self.assertEqual(h['Content-Type'], 'text/html')\n+        self.assertEqual(h.get('Content-Type'), 'text/html')\n+        self.assertEqual(h.getlist('Content-Type'), ['text/html'])\n+\n+    def test_multivalue(self):\n+        h = Headers()\n+\n+        h['X-Forwarded-For'] = hlist = ['ip1', 'ip2']\n+        self.assertEqual(h['X-Forwarded-For'], 'ip2')\n+        self.assertEqual(h.get('X-Forwarded-For'), 'ip2')\n+        self.assertEqual(h.getlist('X-Forwarded-For'), hlist)\n+        assert h.getlist('X-Forwarded-For') is not hlist\n+\n+    def test_delete_and_contains(self):\n+        h = Headers()\n+\n+        h['Content-Type'] = 'text/html'\n+        assert 'Content-Type' in h\n+\n+        del h['Content-Type']\n+        assert 'Content-Type' not in h\n+\n+    def test_setdefault(self):\n+        h = Headers()\n+        hlist = ['ip1', 'ip2']\n+        olist = h.setdefault('X-Forwarded-For', hlist)\n+        assert h.getlist('X-Forwarded-For') is not hlist\n+        assert h.getlist('X-Forwarded-For') is olist\n+\n+        h = Headers()\n+        olist = h.setdefault('X-Forwarded-For', 'ip1')\n+        self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1'])\n+        assert h.getlist('X-Forwarded-For') is olist\n+\n+    def test_iterables(self):\n+        idict = {'Content-Type': 'text/html', 'X-Forwarded-For': ['ip1', 'ip2']}\n+\n+        h = Headers(idict)\n+        self.assertEqual(dict(h), {'Content-Type': ['text/html'], 'X-Forwarded-For': ['ip1', 'ip2']})\n+        self.assertEqual(h.keys(), ['X-Forwarded-For', 'Content-Type'])\n+        self.assertEqual(h.items(), [('X-Forwarded-For', 'ip2'), ('Content-Type', 'text/html')])\n+        self.assertEqual(list(h.iteritems()),\n+                [('X-Forwarded-For', 'ip2'), ('Content-Type', 'text/html')])\n+\n+        self.assertEqual(h.values(), ['ip2', 'text/html'])\n+        self.assertEqual(h.lists(),\n+                [('X-Forwarded-For', ['ip1', 'ip2']), ('Content-Type', ['text/html'])])\n+\n+    def test_update(self):\n+        h = Headers()\n+        h.update({'Content-Type': 'text/html', 'X-Forwarded-For': ['ip1', 'ip2']})\n+        self.assertEqual(h.getlist('Content-Type'), ['text/html'])\n+        self.assertEqual(h.getlist('X-Forwarded-For'), ['ip1', 'ip2'])\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de1b9e5ec9abb0ab38395d40ab3ae238dd6471da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 443 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n-import sys, os, time, datetime, pickle\n+from __future__ import with_statement\n+import sys, os, time, datetime, pickle, gzip\n \n from twisted.internet import protocol, reactor\n from twisted.spread import pb\n@@ -38,6 +39,15 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.procman.update_master(self.domain, \"running\")\n \n     def processEnded(self, reason):\n+        if settings.getbool('CLUSTER_WORKER_GZIP_LOGS'):\n+            try:\n+                with open(self.logfile) as f_in:\n+                    with gzip.open(\"%s.gz\" % self.logfile, \"wb\") as f_out:\n+                        f_out.writelines(f_in)\n+                os.remove(self.logfile)\n+                self.logfile = \"%s.gz\" % self.logfile\n+            except e:\n+                log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, self.domain, self.pid))\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n         del self.procman.running[self.domain]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6e76dadc654ddafa2969af5ca795d26797b6b39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 445 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n                 os.remove(self.logfile)\n                 self.logfile = \"%s.gz\" % self.logfile\n             except e:\n-                log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, self.domain, self.pid))\n+                log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, e, self.domain, self.pid))\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n         del self.procman.running[self.domain]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9fb0f5094ec39ee8fd5909b0075543605e4f9f80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 15 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,26 +9,27 @@ class ItemAdaptor(object):\n             self.item_instance = self.item_class()\n         \n         self._response = response\n-        self._field_extractors = self._get_field_extractors()\n+        self._field_adaptors = self._get_field_adaptors()\n \n-    def _get_field_extractors(self):\n-        fe = {}\n+    def _get_field_adaptors(self):\n+        fa = {}\n         for field in self.item_instance._fields.keys():\n             if self.__class__.__dict__.has_key(field):\n-                fe[field] = self.__class__.__dict__[field]\n-        return fe\n+                fa[field] = self.__class__.__dict__[field]\n+        return fa\n \n     def __setattr__(self, name, value):\n         if name.startswith('_') or name == 'item_instance':\n             return object.__setattr__(self, name, value)\n \n         try:\n-            fieldextractor = self._field_extractors[name]\n+            fieldadaptor = self._field_adaptors[name]\n         except KeyError:\n             raise AttributeError(name)\n \n         adaptor_args = {'response': self._response}\n-        final = fieldextractor(value, adaptor_args=adaptor_args)\n+        final = fieldadaptor(value, adaptor_args=adaptor_args)\n+        print \"Intentando asignar: %s\" % final\n         setattr(self.item_instance, name, final)\n \n     def __getattribute__(self, name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4675404fc1792305879ea429c70be1c281d6b2e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,7 +29,6 @@ class ItemAdaptor(object):\n \n         adaptor_args = {'response': self._response}\n         final = fieldadaptor(value, adaptor_args=adaptor_args)\n-        print \"Intentando asignar: %s\" % final\n         setattr(self.item_instance, name, final)\n \n     def __getattribute__(self, name):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f60224b9196546781a2367b91ce4252903e1c13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 447 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n                         f_out.writelines(f_in)\n                 os.remove(self.logfile)\n                 self.logfile = \"%s.gz\" % self.logfile\n-            except e:\n+            except Exception as e:\n                 log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, e, self.domain, self.pid))\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5af4f3a279c4081021423586b91a15867afda137", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 449 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n                         f_out.writelines(f_in)\n                 os.remove(self.logfile)\n                 self.logfile = \"%s.gz\" % self.logfile\n-            except Exception as e:\n+            except Exception, e:\n                 log.msg(\"failed to compress %s exception=%s (domain=%s, pid=%s)\" % (self.logfile, e, self.domain, self.pid))\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fa5c7bade3b6218f656840a31d7733791fea5d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 456 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-from __future__ import with_statement\n import sys, os, time, datetime, pickle, gzip\n \n from twisted.internet import protocol, reactor\n@@ -41,9 +40,11 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n     def processEnded(self, reason):\n         if settings.getbool('CLUSTER_WORKER_GZIP_LOGS'):\n             try:\n-                with open(self.logfile) as f_in:\n-                    with gzip.open(\"%s.gz\" % self.logfile, \"wb\") as f_out:\n+                f_in = open(self.logfile)\n+                f_out = gzip.open(\"%s.gz\" % self.logfile, \"wb\")\n                 f_out.writelines(f_in)\n+                f_in.close()\n+                f_out.close()\n                 os.remove(self.logfile)\n                 self.logfile = \"%s.gz\" % self.logfile\n             except Exception, e:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf3afb9490abcb36d6009bcaf6fbb2fb36066d05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 26 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,11 @@ class ItemAdaptor(object):\n     def _get_field_adaptors(self):\n         fa = {}\n         for field in self.item_instance._fields.keys():\n+            try:\n+                fa[field] = object.__getattribute__(self, field)\n+            except AttributeError:\n+                pass\n+\n             if self.__class__.__dict__.has_key(field):\n                 fa[field] = self.__class__.__dict__[field]\n         return fa\n\n@@ -1,6 +1,27 @@\n \n import unittest\n from scrapy.contrib_exp.newitem.adaptors import adaptor, ItemAdaptor\n+from scrapy.contrib_exp.newitem import Item, StringField\n+\n+\n+class TestItem(Item):\n+    name = StringField()\n+    url = StringField()\n+\n+\n+class ItemAdaptorTest(unittest.TestCase):\n+\n+    def test_inheritance(self):\n+        class TestAdaptor(ItemAdaptor):\n+            item_class = TestItem\n+            name = lambda v, adaptor_args: v.title()\n+\n+        class ChildTestAdaptor(TestAdaptor):\n+            url = lambda v, adaptor_args: v.lower()\n+\n+        ia = ChildTestAdaptor()\n+        assert 'url' in ia._field_adaptors\n+        assert 'name' in ia._field_adaptors\n \n \n class TreeadaptTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e8945a477a49bbbff59a6225ee08a964e180fae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 34 | Churn Cumulative: 78 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,8 +19,6 @@ class ItemAdaptor(object):\n             except AttributeError:\n                 pass\n \n-            if self.__class__.__dict__.has_key(field):\n-                fa[field] = self.__class__.__dict__[field]\n         return fa\n \n     def __setattr__(self, name, value):\n@@ -28,13 +26,17 @@ class ItemAdaptor(object):\n             return object.__setattr__(self, name, value)\n \n         try:\n-            fieldadaptor = self._field_adaptors[name]\n+            bounded_fa = self._field_adaptors[name]\n+            # unbound bounded field adaptor here because use of\n+            # @staticmethod decorator in class definition is not practical\n+            # for ItemAdaptor's\n+            fa = bounded_fa.__func__.__call__ \n         except KeyError:\n             raise AttributeError(name)\n \n         adaptor_args = {'response': self._response}\n-        final = fieldadaptor(value, adaptor_args=adaptor_args)\n-        setattr(self.item_instance, name, final)\n+        ovalue = fa(value, adaptor_args=adaptor_args)\n+        setattr(self.item_instance, name, ovalue)\n \n     def __getattribute__(self, name):\n         if name.startswith('_') or name.startswith('item_'):\n\n@@ -8,14 +8,20 @@ class TestItem(Item):\n     name = StringField()\n     url = StringField()\n \n-\n-class ItemAdaptorTest(unittest.TestCase):\n-\n-    def test_inheritance(self):\n class TestAdaptor(ItemAdaptor):\n     item_class = TestItem\n     name = lambda v, adaptor_args: v.title()\n \n+\n+class ItemAdaptorTest(unittest.TestCase):\n+\n+    def test_basic(self):\n+        ia = TestAdaptor()\n+        ia.name = 'marta'\n+        self.assertEqual(ia.item_instance.name, 'Marta')\n+        self.assertEqual(ia.name, 'Marta')\n+\n+    def test_inheritance(self):\n         class ChildTestAdaptor(TestAdaptor):\n             url = lambda v, adaptor_args: v.lower()\n \n@@ -24,6 +30,14 @@ class ItemAdaptorTest(unittest.TestCase):\n         assert 'name' in ia._field_adaptors\n \n \n+        ia.url = 'HTTP://scrapy.ORG'\n+        self.assertEqual(ia.url, 'http://scrapy.org')\n+\n+        ia.name = 'marta'\n+        self.assertEqual(ia.name, 'Marta')\n+\n+\n+\n class TreeadaptTest(unittest.TestCase):\n \n     def test_1_passtrough(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
