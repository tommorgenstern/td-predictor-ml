{"custom_id": "keras#2a3ef732f6d9f9378b88a96a56ec346323d8cab4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 6 | Files Changed: 5 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 119 | Churn Cumulative: 883 | Contributors (this commit): 11 | Commits (past 90d): 105 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,6 +111,7 @@ from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n from keras_core.layers.reshaping.up_sampling2d import UpSampling2D\n from keras_core.layers.reshaping.up_sampling3d import UpSampling3D\n+from keras_core.layers.reshaping.zero_padding1d import ZeroPadding1D\n from keras_core.layers.reshaping.zero_padding2d import ZeroPadding2D\n from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n from keras_core.layers.rnn.bidirectional import Bidirectional\n\n@@ -26,10 +26,12 @@ class Cropping1D(Layer):\n      [[8 9]]]\n \n     Args:\n-        cropping: Integer or tuple of integers of length 2.\n-            How many units should be trimmed off at the beginning and end of\n-            the cropping dimension (axis 1).\n-            If a single int is provided, the same value will be used for both.\n+        cropping: Int, or tuple of int (length 2), or dictionary.\n+            - If int: how many units should be trimmed off at the beginning and\n+              end of the cropping dimension (axis 1).\n+            - If tuple of 2 ints: how many units should be trimmed off at the\n+              beginning and end of the cropping dimension\n+              (`(left_crop, right_crop)`).\n \n     Input shape:\n         3D tensor with shape `(batch_size, axis_to_crop, features)`\n\n@@ -0,0 +1,69 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n+class ZeroPadding1D(Layer):\n+    \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 2, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> x\n+    [[[ 0  1  2]\n+      [ 3  4  5]]\n+     [[ 6  7  8]\n+      [ 9 10 11]]]\n+    >>> y = keras_core.layers.ZeroPadding1D(padding=2)(x)\n+    >>> y\n+    [[[ 0  0  0]\n+      [ 0  0  0]\n+      [ 0  1  2]\n+      [ 3  4  5]\n+      [ 0  0  0]\n+      [ 0  0  0]]\n+     [[ 0  0  0]\n+      [ 0  0  0]\n+      [ 6  7  8]\n+      [ 9 10 11]\n+      [ 0  0  0]\n+      [ 0  0  0]]]\n+\n+    Args:\n+        padding: Int, or tuple of int (length 2), or dictionary.\n+            - If int: how many zeros to add at the beginning and end of\n+              the padding dimension (axis 1).\n+            - If tuple of 2 ints: how many zeros to add at the beginning and the\n+              end of the padding dimension (`(left_pad, right_pad)`).\n+\n+    Input shape:\n+        3D tensor with shape `(batch_size, axis_to_pad, features)`\n+\n+    Output shape:\n+        3D tensor with shape `(batch_size, padded_axis, features)`\n+    \"\"\"\n+\n+    def __init__(self, padding=1, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        if isinstance(padding, int):\n+            padding = (padding, padding)\n+        self.padding = padding\n+        self.input_spec = InputSpec(ndim=3)\n+\n+    def compute_output_shape(self, input_shape):\n+        output_shape = list(input_shape)\n+        if input_shape[1] is not None:\n+            input_shape[1] += self.padding[0] + self.padding[1]\n+        return tuple(output_shape)\n+\n+    def call(self, inputs):\n+        all_dims_padding = ((0, 0), self.padding, (0, 0))\n+        return ops.pad(inputs, all_dims_padding)\n+\n+    def get_config(self):\n+        config = {\"padding\": self.padding}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,35 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n+    def test_zero_padding_1d(self):\n+        inputs = np.random.rand(1, 2, 3)\n+        outputs = layers.ZeroPadding1D(padding=(1, 2))(inputs)\n+\n+        for index in [0, -1, -2]:\n+            self.assertAllClose(outputs[:, index, :], 0.0)\n+        self.assertAllClose(outputs[:, 1:-2, :], inputs)\n+\n+    @parameterized.named_parameters((\"one_tuple\", (2, 2)), (\"one_int\", 2))\n+    def test_zero_padding_1d_with_same_padding(self, padding):\n+        inputs = np.random.rand(1, 2, 3)\n+        outputs = layers.ZeroPadding1D(padding=padding)(inputs)\n+\n+        for index in [0, 1, -1, -2]:\n+            self.assertAllClose(outputs[:, index, :], 0.0)\n+        self.assertAllClose(outputs[:, 2:-2, :], inputs)\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n+        input_layer = layers.Input(batch_shape=(1, None, 3))\n+        padded = layers.ZeroPadding1D((1, 2))(input_layer)\n+        self.assertEqual(padded.shape, (1, None, 3))\n\n@@ -67,8 +67,8 @@ class ZeroPadding2D(Layer):\n           `(batch_size, channels, padded_height, padded_width)`\n     \"\"\"\n \n-    def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n-        super().__init__(**kwargs)\n+    def __init__(self, padding=(1, 1), data_format=None, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = ((padding, padding), (padding, padding))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8852928f6e6e8f6d03cb4700a2be8173935086c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 489 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 7 | Methods Changed: 11 | Complexity Δ (Sum/Max): 36/36 | Churn Δ: 491 | Churn Cumulative: 1234 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 0.43805309734513276\n\nDIFF:\n@@ -4,6 +4,7 @@ from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import testing\n+from keras_core.applications import densenet\n from keras_core.applications import efficientnet\n from keras_core.applications import efficientnet_v2\n from keras_core.applications import mobilenet\n@@ -47,6 +48,10 @@ MODEL_LIST = [\n     (efficientnet_v2.EfficientNetV2S, 1280, efficientnet_v2),\n     (efficientnet_v2.EfficientNetV2M, 1280, efficientnet_v2),\n     (efficientnet_v2.EfficientNetV2L, 1280, efficientnet_v2),\n+    # densenet\n+    (densenet.DenseNet121, 1024, densenet),\n+    (densenet.DenseNet169, 1664, densenet),\n+    (densenet.DenseNet201, 1920, densenet),\n ]\n # Add names for `named_parameters`.\n MODEL_LIST = [(e[0].__name__, *e) for e in MODEL_LIST]\n\n@@ -0,0 +1,482 @@\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHTS_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/densenet/\"\n+)\n+DENSENET121_WEIGHT_PATH = (\n+    BASE_WEIGHTS_PATH + \"densenet121_weights_tf_dim_ordering_tf_kernels.h5\"\n+)\n+DENSENET121_WEIGHT_PATH_NO_TOP = (\n+    BASE_WEIGHTS_PATH\n+    + \"densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n+)\n+DENSENET169_WEIGHT_PATH = (\n+    BASE_WEIGHTS_PATH + \"densenet169_weights_tf_dim_ordering_tf_kernels.h5\"\n+)\n+DENSENET169_WEIGHT_PATH_NO_TOP = (\n+    BASE_WEIGHTS_PATH\n+    + \"densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n+)\n+DENSENET201_WEIGHT_PATH = (\n+    BASE_WEIGHTS_PATH + \"densenet201_weights_tf_dim_ordering_tf_kernels.h5\"\n+)\n+DENSENET201_WEIGHT_PATH_NO_TOP = (\n+    BASE_WEIGHTS_PATH\n+    + \"densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n+)\n+\n+\n+def dense_block(x, blocks, name):\n+    \"\"\"A dense block.\n+\n+    Args:\n+        x: input tensor.\n+        blocks: integer, the number of building blocks.\n+        name: string, block label.\n+\n+    Returns:\n+        Output tensor for the block.\n+    \"\"\"\n+    for i in range(blocks):\n+        x = conv_block(x, 32, name=name + \"_block\" + str(i + 1))\n+    return x\n+\n+\n+def transition_block(x, reduction, name):\n+    \"\"\"A transition block.\n+\n+    Args:\n+        x: input tensor.\n+        reduction: float, compression rate at transition layers.\n+        name: string, block label.\n+\n+    Returns:\n+        Output tensor for the block.\n+    \"\"\"\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+    x = layers.BatchNormalization(\n+        axis=bn_axis, epsilon=1.001e-5, name=name + \"_bn\"\n+    )(x)\n+    x = layers.Activation(\"relu\", name=name + \"_relu\")(x)\n+    x = layers.Conv2D(\n+        int(x.shape[bn_axis] * reduction),\n+        1,\n+        use_bias=False,\n+        name=name + \"_conv\",\n+    )(x)\n+    x = layers.AveragePooling2D(2, strides=2, name=name + \"_pool\")(x)\n+    return x\n+\n+\n+def conv_block(x, growth_rate, name):\n+    \"\"\"A building block for a dense block.\n+\n+    Args:\n+        x: input tensor.\n+        growth_rate: float, growth rate at dense layers.\n+        name: string, block label.\n+\n+    Returns:\n+        Output tensor for the block.\n+    \"\"\"\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+    x1 = layers.BatchNormalization(\n+        axis=bn_axis, epsilon=1.001e-5, name=name + \"_0_bn\"\n+    )(x)\n+    x1 = layers.Activation(\"relu\", name=name + \"_0_relu\")(x1)\n+    x1 = layers.Conv2D(\n+        4 * growth_rate, 1, use_bias=False, name=name + \"_1_conv\"\n+    )(x1)\n+    x1 = layers.BatchNormalization(\n+        axis=bn_axis, epsilon=1.001e-5, name=name + \"_1_bn\"\n+    )(x1)\n+    x1 = layers.Activation(\"relu\", name=name + \"_1_relu\")(x1)\n+    x1 = layers.Conv2D(\n+        growth_rate, 3, padding=\"same\", use_bias=False, name=name + \"_2_conv\"\n+    )(x1)\n+    x = layers.Concatenate(axis=bn_axis, name=name + \"_concat\")([x, x1])\n+    return x\n+\n+\n+def DenseNet(\n+    blocks,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the DenseNet architecture.\n+\n+    Reference:\n+    - [Densely Connected Convolutional Networks](\n+        https://arxiv.org/abs/1608.06993) (CVPR 2017)\n+\n+    This function returns a Keras image classification model,\n+    optionally loaded with weights pre-trained on ImageNet.\n+\n+    For image classification use cases, see\n+    [this page for detailed examples](\n+      https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+    For transfer learning use cases, make sure to read the\n+    [guide to transfer learning & fine-tuning](\n+      https://keras.io/guides/transfer_learning/).\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For DenseNet, call `keras_core.applications.densenet.preprocess_input`\n+    on your inputs before passing them to the model.\n+    `densenet.preprocess_input` will scale pixels between 0 and 1 and then\n+    will normalize each channel with respect to the ImageNet\n+    dataset statistics.\n+\n+    Args:\n+        blocks: numbers of building blocks for the four dense layers.\n+        include_top: whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: one of `None` (random initialization),\n+            `\"imagenet\"` (pre-training on ImageNet),\n+            or the path to the weights file to be loaded.\n+        input_tensor: optional Keras tensor\n+            (i.e. output of `layers.Input()`)\n+            to use as image input for the model.\n+        input_shape: optional shape tuple, only to be specified\n+            if `include_top` is False (otherwise the input shape\n+            has to be `(224, 224, 3)`\n+            (with `'channels_last'` data format)\n+            or `(3, 224, 224)` (with `'channels_first'` data format).\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 32.\n+            E.g. `(200, 200, 3)` would be one valid value.\n+        pooling: optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the\n+                last convolutional block.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional block, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n+        classes: optional number of classes to classify images\n+            into, only to be specified if `include_top` is `True`, and\n+            if no `weights` argument is specified.\n+        classifier_activation: A `str` or callable.\n+            The activation function to use\n+            on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer. When loading pretrained weights, `classifier_activation`\n+            can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights` as `\"imagenet\"` with `include_top`'\n+            \" as true, `classes` should be 1000\"\n+        )\n+\n+    # Determine proper input shape\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=224,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+\n+    x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n+    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\"conv1/conv\")(x)\n+    x = layers.BatchNormalization(\n+        axis=bn_axis, epsilon=1.001e-5, name=\"conv1/bn\"\n+    )(x)\n+    x = layers.Activation(\"relu\", name=\"conv1/relu\")(x)\n+    x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n+    x = layers.MaxPooling2D(3, strides=2, name=\"pool1\")(x)\n+\n+    x = dense_block(x, blocks[0], name=\"conv2\")\n+    x = transition_block(x, 0.5, name=\"pool2\")\n+    x = dense_block(x, blocks[1], name=\"conv3\")\n+    x = transition_block(x, 0.5, name=\"pool3\")\n+    x = dense_block(x, blocks[2], name=\"conv4\")\n+    x = transition_block(x, 0.5, name=\"pool4\")\n+    x = dense_block(x, blocks[3], name=\"conv5\")\n+\n+    x = layers.BatchNormalization(axis=bn_axis, epsilon=1.001e-5, name=\"bn\")(x)\n+    x = layers.Activation(\"relu\", name=\"relu\")(x)\n+\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes, activation=classifier_activation, name=\"predictions\"\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    if blocks == [6, 12, 24, 16]:\n+        model = Functional(inputs, x, name=\"densenet121\")\n+    elif blocks == [6, 12, 32, 32]:\n+        model = Functional(inputs, x, name=\"densenet169\")\n+    elif blocks == [6, 12, 48, 32]:\n+        model = Functional(inputs, x, name=\"densenet201\")\n+    else:\n+        model = Functional(inputs, x, name=\"densenet\")\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            if blocks == [6, 12, 24, 16]:\n+                weights_path = file_utils.get_file(\n+                    \"densenet121_weights_tf_dim_ordering_tf_kernels.h5\",\n+                    DENSENET121_WEIGHT_PATH,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"9d60b8095a5708f2dcce2bca79d332c7\",\n+                )\n+            elif blocks == [6, 12, 32, 32]:\n+                weights_path = file_utils.get_file(\n+                    \"densenet169_weights_tf_dim_ordering_tf_kernels.h5\",\n+                    DENSENET169_WEIGHT_PATH,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"d699b8f76981ab1b30698df4c175e90b\",\n+                )\n+            elif blocks == [6, 12, 48, 32]:\n+                weights_path = file_utils.get_file(\n+                    \"densenet201_weights_tf_dim_ordering_tf_kernels.h5\",\n+                    DENSENET201_WEIGHT_PATH,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"1ceb130c1ea1b78c3bf6114dbdfd8807\",\n+                )\n+        else:\n+            if blocks == [6, 12, 24, 16]:\n+                weights_path = file_utils.get_file(\n+                    \"densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n+                    DENSENET121_WEIGHT_PATH_NO_TOP,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"30ee3e1110167f948a6b9946edeeb738\",\n+                )\n+            elif blocks == [6, 12, 32, 32]:\n+                weights_path = file_utils.get_file(\n+                    \"densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n+                    DENSENET169_WEIGHT_PATH_NO_TOP,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"b8c4d4c20dd625c148057b9ff1c1176b\",\n+                )\n+            elif blocks == [6, 12, 48, 32]:\n+                weights_path = file_utils.get_file(\n+                    \"densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n+                    DENSENET201_WEIGHT_PATH_NO_TOP,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"c13680b51ded0fb44dff2d8f86ac8bb1\",\n+                )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.densenet.DenseNet121\",\n+        \"keras_core.applications.DenseNet121\",\n+    ]\n+)\n+def DenseNet121(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the Densenet121 architecture.\"\"\"\n+    return DenseNet(\n+        [6, 12, 24, 16],\n+        include_top,\n+        weights,\n+        input_tensor,\n+        input_shape,\n+        pooling,\n+        classes,\n+        classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.densenet.DenseNet169\",\n+        \"keras_core.applications.DenseNet169\",\n+    ]\n+)\n+def DenseNet169(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the Densenet169 architecture.\"\"\"\n+    return DenseNet(\n+        [6, 12, 32, 32],\n+        include_top,\n+        weights,\n+        input_tensor,\n+        input_shape,\n+        pooling,\n+        classes,\n+        classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.densenet.DenseNet201\",\n+        \"keras_core.applications.DenseNet201\",\n+    ]\n+)\n+def DenseNet201(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the Densenet201 architecture.\"\"\"\n+    return DenseNet(\n+        [6, 12, 48, 32],\n+        include_top,\n+        weights,\n+        input_tensor,\n+        input_shape,\n+        pooling,\n+        classes,\n+        classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.densenet.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    return imagenet_utils.preprocess_input(\n+        x, data_format=data_format, mode=\"torch\"\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.densenet.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n+    mode=\"\",\n+    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TORCH,\n+    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC,\n+)\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n+\n+DOC = \"\"\"\n+\n+Reference:\n+- [Densely Connected Convolutional Networks](\n+    https://arxiv.org/abs/1608.06993) (CVPR 2017)\n+\n+Optionally loads weights pre-trained on ImageNet.\n+Note that the data format convention used by the model is\n+the one specified in your Keras config at `~/.keras/keras.json`.\n+\n+Note: each Keras Application expects a specific kind of input preprocessing.\n+For DenseNet, call `keras_core.applications.densenet.preprocess_input`\n+on your inputs before passing them to the model.\n+\n+Args:\n+    include_top: whether to include the fully-connected\n+    layer at the top of the network.\n+    weights: one of `None` (random initialization),\n+    `\"imagenet\"` (pre-training on ImageNet),\n+    or the path to the weights file to be loaded.\n+    input_tensor: optional Keras tensor\n+    (i.e. output of `layers.Input()`)\n+    to use as image input for the model.\n+    input_shape: optional shape tuple, only to be specified\n+    if `include_top` is False (otherwise the input shape\n+    has to be `(224, 224, 3)` (with `'channels_last'` data format)\n+    or `(3, 224, 224)` (with `'channels_first'` data format).\n+    It should have exactly 3 inputs channels,\n+    and width and height should be no smaller than 32.\n+    E.g. `(200, 200, 3)` would be one valid value.\n+    pooling: Optional pooling mode for feature extraction\n+    when `include_top` is `False`.\n+    - `None` means that the output of the model will be\n+        the 4D tensor output of the\n+        last convolutional block.\n+    - `avg` means that global average pooling\n+        will be applied to the output of the\n+        last convolutional block, and thus\n+        the output of the model will be a 2D tensor.\n+    - `max` means that global max pooling will\n+        be applied.\n+    classes: optional number of classes to classify images\n+    into, only to be specified if `include_top` is `True`, and\n+    if no `weights` argument is specified.\n+    classifier_activation: A `str` or callable.\n+    The activation function to use\n+    on the \"top\" layer. Ignored unless `include_top=True`. Set\n+    `classifier_activation=None` to return the logits\n+    of the \"top\" layer. When loading pretrained weights,\n+    `classifier_activation` can only be `None` or `\"softmax\"`.\n+\n+Returns:\n+    A Keras model instance.\n+\"\"\"\n+\n+setattr(DenseNet121, \"__doc__\", DenseNet121.__doc__ + DOC)\n+setattr(DenseNet169, \"__doc__\", DenseNet169.__doc__ + DOC)\n+setattr(DenseNet201, \"__doc__\", DenseNet201.__doc__ + DOC)\n\n@@ -38,7 +38,6 @@ class Concatenate(Merge):\n         self._reshape_required = False\n \n     def build(self, input_shape):\n-        super().build(input_shape)\n         # Used purely for shape validation.\n         if len(input_shape) < 1 or not isinstance(input_shape[0], tuple):\n             raise ValueError(\n@@ -77,6 +76,7 @@ class Concatenate(Merge):\n                 )\n                 if len(unique_dims) > 1:\n                     raise ValueError(err_msg)\n+        self.built = True\n \n     def _merge_function(self, inputs):\n         return ops.concatenate(inputs, axis=self.axis)\n\n@@ -263,7 +263,6 @@ class Dot(Merge):\n         self._reshape_required = False\n \n     def build(self, input_shape):\n-        super().build(input_shape)\n         # Used purely for shape validation.\n         if not isinstance(input_shape[0], tuple) or len(input_shape) != 2:\n             raise ValueError(\n@@ -288,6 +287,7 @@ class Dot(Merge):\n                 f\"{shape2[axes[1]]} (at axis {axes[1]}). \"\n                 f\"Full input shapes: {shape1}, {shape2}\"\n             )\n+        self.built = True\n \n     def _merge_function(self, inputs):\n         if len(inputs) != 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96a6fae639b8b6fd08edc9828e31c0f78b1764e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 102 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n             input_shape=(2, 3),\n             expected_output_shape=(2, 3),\n             expected_num_trainable_weights=0,\n-            expected_num_non_trainable_weights=2,\n+            expected_num_non_trainable_weights=3,\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=True,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61e94375b0a4dbb2704d0332e1aa9ec45c1af851", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 37 | Files Changed: 12 | Hunks: 18 | Methods Changed: 12 | Complexity Δ (Sum/Max): -10/1 | Churn Δ: 119 | Churn Cumulative: 3408 | Contributors (this commit): 4 | Commits (past 90d): 58 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping1D\")\n@@ -42,9 +43,9 @@ class Cropping1D(Layer):\n \n     def __init__(self, cropping=(1, 1), name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        if isinstance(cropping, int):\n-            cropping = (cropping, cropping)\n-        self.cropping = cropping\n+        self.cropping = argument_validation.standardize_tuple(\n+            cropping, 2, \"cropping\", allow_zero=True\n+        )\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -56,6 +56,14 @@ class Cropping1DTest(testing.TestCase):\n         cropped = layers.Cropping1D((1, 2))(input_layer)\n         self.assertEqual(cropped.shape, (1, None, 7))\n \n+    def test_cropping_1d_errors_if_cropping_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.Cropping1D(cropping=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping1D(cropping=(1, 2, 3))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping1D(cropping=\"1\")\n+\n     def test_cropping_1d_errors_if_cropping_more_than_available(self):\n         with self.assertRaises(ValueError):\n             input_layer = layers.Input(batch_shape=(3, 5, 7))\n\n@@ -2,6 +2,7 @@ from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping2D\")\n@@ -65,12 +66,12 @@ class Cropping2D(Layer):\n                     \"`cropping` should have two elements. \"\n                     f\"Received: cropping={cropping}.\"\n                 )\n-            height_cropping = cropping[0]\n-            if isinstance(height_cropping, int):\n-                height_cropping = (height_cropping, height_cropping)\n-            width_cropping = cropping[1]\n-            if isinstance(width_cropping, int):\n-                width_cropping = (width_cropping, width_cropping)\n+            height_cropping = argument_validation.standardize_tuple(\n+                cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n+            )\n+            width_cropping = argument_validation.standardize_tuple(\n+                cropping[1], 2, \"2nd entry of cropping\", allow_zero=True\n+            )\n             self.cropping = (height_cropping, width_cropping)\n         else:\n             raise ValueError(\n\n@@ -97,3 +97,9 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n             layers.Cropping2D(cropping=(1, 2, 3))\n         with self.assertRaises(ValueError):\n             layers.Cropping2D(cropping=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=((1, 2), (3, 4, 5)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=((1, 2), (3, -4)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=((1, 2), \"3\"))\n\n@@ -2,6 +2,7 @@ from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping3D\")\n@@ -75,15 +76,15 @@ class Cropping3D(Layer):\n                 raise ValueError(\n                     f\"`cropping` should have 3 elements. Received: {cropping}.\"\n                 )\n-            dim1_cropping = cropping[0]\n-            if isinstance(dim1_cropping, int):\n-                dim1_cropping = (dim1_cropping, dim1_cropping)\n-            dim2_cropping = cropping[1]\n-            if isinstance(dim2_cropping, int):\n-                dim2_cropping = (dim2_cropping, dim2_cropping)\n-            dim3_cropping = cropping[2]\n-            if isinstance(dim3_cropping, int):\n-                dim3_cropping = (dim3_cropping, dim3_cropping)\n+            dim1_cropping = argument_validation.standardize_tuple(\n+                cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n+            )\n+            dim2_cropping = argument_validation.standardize_tuple(\n+                cropping[1], 2, \"2nd entry of cropping\", allow_zero=True\n+            )\n+            dim3_cropping = argument_validation.standardize_tuple(\n+                cropping[2], 2, \"3rd entry of cropping\", allow_zero=True\n+            )\n             self.cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n         else:\n             raise ValueError(\n\n@@ -159,3 +159,9 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             layers.Cropping3D(cropping=(1, 2, 3, 4))\n         with self.assertRaises(ValueError):\n             layers.Cropping3D(cropping=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.Cropping3D(cropping=((1, 2), (3, 4), (5, 6, 7)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping3D(cropping=((1, 2), (3, 4), (5, -6)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping3D(cropping=((1, 2), (3, 4), \"5\"))\n\n@@ -2,6 +2,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n@@ -48,9 +49,9 @@ class ZeroPadding1D(Layer):\n \n     def __init__(self, padding=1, name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        if isinstance(padding, int):\n-            padding = (padding, padding)\n-        self.padding = padding\n+        self.padding = argument_validation.standardize_tuple(\n+            padding, 2, \"padding\", allow_zero=True\n+        )\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -33,3 +33,11 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n         padded = layers.ZeroPadding1D((1, 2))(input_layer)\n         self.assertEqual(padded.shape, (1, None, 3))\n+\n+    def test_zero_padding_1d_errors_if_padding_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding1D(padding=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding1D(padding=(1, 2, 3))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding1D(padding=\"1\")\n\n@@ -3,6 +3,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding2D\")\n@@ -78,12 +79,12 @@ class ZeroPadding2D(Layer):\n                     \"`padding` should have two elements. \"\n                     f\"Received: padding={padding}.\"\n                 )\n-            height_padding = padding[0]\n-            if isinstance(height_padding, int):\n-                height_padding = (height_padding, height_padding)\n-            width_padding = padding[1]\n-            if isinstance(width_padding, int):\n-                width_padding = (width_padding, width_padding)\n+            height_padding = argument_validation.standardize_tuple(\n+                padding[0], 2, \"1st entry of padding\", allow_zero=True\n+            )\n+            width_padding = argument_validation.standardize_tuple(\n+                padding[1], 2, \"2nd entry of padding\", allow_zero=True\n+            )\n             self.padding = (height_padding, width_padding)\n         else:\n             raise ValueError(\n\n@@ -74,3 +74,9 @@ class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n             layers.ZeroPadding2D(padding=(1, 2, 3))\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding2D(padding=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding2D(padding=((1, 2), (3, 4, 5)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding2D(padding=((1, 2), (3, -4)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding2D(padding=((1, 2), \"3\"))\n\n@@ -3,6 +3,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding3D\")\n@@ -76,16 +77,15 @@ class ZeroPadding3D(Layer):\n                 raise ValueError(\n                     f\"`padding` should have 3 elements. Received: {padding}.\"\n                 )\n-            dim1_padding = padding[0]\n-            if isinstance(dim1_padding, int):\n-                dim1_padding = (dim1_padding, dim1_padding)\n-            dim2_padding = padding[1]\n-            if isinstance(dim2_padding, int):\n-                dim2_padding = (dim2_padding, dim2_padding)\n-            dim3_padding = padding[2]\n-            if isinstance(dim3_padding, int):\n-                dim3_padding = (dim3_padding, dim3_padding)\n-            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+            dim1_padding = argument_validation.standardize_tuple(\n+                padding[0], 2, \"1st entry of padding\", allow_zero=True\n+            )\n+            dim2_padding = argument_validation.standardize_tuple(\n+                padding[1], 2, \"2nd entry of padding\", allow_zero=True\n+            )\n+            dim3_padding = argument_validation.standardize_tuple(\n+                padding[2], 2, \"3rd entry of padding\", allow_zero=True\n+            )\n             self.padding = (dim1_padding, dim2_padding, dim3_padding)\n         else:\n             raise ValueError(\n\n@@ -82,3 +82,9 @@ class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n             layers.ZeroPadding3D(padding=(1, 2, 3, 4))\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding3D(padding=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=((1, 2), (3, 4), (5, 6, 7)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=((1, 2), (3, 4), (5, -6)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=((1, 2), (3, 4), \"5\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#785c83b25ab07b446b8615f05eb02b65d56fed31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1648 | Lines Deleted: 91 | Files Changed: 18 | Hunks: 29 | Methods Changed: 48 | Complexity Δ (Sum/Max): 115/48 | Churn Δ: 1739 | Churn Cumulative: 7277 | Contributors (this commit): 9 | Commits (past 90d): 127 | Contributors (cumulative): 42 | DMM Complexity: 0.14475271411338964\n\nDIFF:\n@@ -4,15 +4,19 @@ from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import testing\n+from keras_core.applications import convnext\n from keras_core.applications import densenet\n from keras_core.applications import efficientnet\n from keras_core.applications import efficientnet_v2\n+from keras_core.applications import inception_resnet_v2\n+from keras_core.applications import inception_v3\n from keras_core.applications import mobilenet\n from keras_core.applications import mobilenet_v2\n from keras_core.applications import mobilenet_v3\n from keras_core.applications import vgg16\n from keras_core.applications import vgg19\n from keras_core.applications import xception\n+from keras_core.saving import serialization_lib\n from keras_core.utils import file_utils\n from keras_core.utils import image_utils\n \n@@ -27,7 +31,10 @@ MODEL_LIST = [\n     (vgg19.VGG19, 512, vgg19),\n     # xception\n     (xception.Xception, 2048, xception),\n-    # mobilnet\n+    # inception\n+    (inception_v3.InceptionV3, 2048, inception_v3),\n+    (inception_resnet_v2.InceptionResNetV2, 1536, inception_resnet_v2),\n+    # mobilenet\n     (mobilenet.MobileNet, 1024, mobilenet),\n     (mobilenet_v2.MobileNetV2, 1280, mobilenet_v2),\n     (mobilenet_v3.MobileNetV3Small, 576, mobilenet_v3),\n@@ -52,6 +59,12 @@ MODEL_LIST = [\n     (densenet.DenseNet121, 1024, densenet),\n     (densenet.DenseNet169, 1664, densenet),\n     (densenet.DenseNet201, 1920, densenet),\n+    # convnext\n+    (convnext.ConvNeXtTiny, 768, convnext),\n+    (convnext.ConvNeXtSmall, 768, convnext),\n+    (convnext.ConvNeXtBase, 1024, convnext),\n+    (convnext.ConvNeXtLarge, 1536, convnext),\n+    (convnext.ConvNeXtXLarge, 2048, convnext),\n ]\n # Add names for `named_parameters`.\n MODEL_LIST = [(e[0].__name__, *e) for e in MODEL_LIST]\n@@ -111,8 +124,8 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"African_elephant\", names[:3])\n \n         # Can be serialized and deserialized\n-        config = model.get_config()\n-        reconstructed_model = model.__class__.from_config(config)\n+        config = serialization_lib.serialize_keras_object(model)\n+        reconstructed_model = serialization_lib.deserialize_keras_object(config)\n         self.assertEqual(len(model.weights), len(reconstructed_model.weights))\n \n     @parameterized.named_parameters(MODEL_LIST)\n\n@@ -0,0 +1,752 @@\n+import numpy as np\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import random\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.layers.layer import Layer\n+from keras_core.models import Functional\n+from keras_core.models import Sequential\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHTS_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/convnext/\"\n+)\n+\n+WEIGHTS_HASHES = {\n+    \"convnext_tiny\": (\n+        \"8ae6e78ce2933352b1ef4008e6dd2f17bc40771563877d156bc6426c7cf503ff\",\n+        \"d547c096cabd03329d7be5562c5e14798aa39ed24b474157cef5e85ab9e49ef1\",\n+    ),\n+    \"convnext_small\": (\n+        \"ce1277d8f1ee5a0ef0e171469089c18f5233860ceaf9b168049cb9263fd7483c\",\n+        \"6fc8009faa2f00c1c1dfce59feea9b0745eb260a7dd11bee65c8e20843da6eab\",\n+    ),\n+    \"convnext_base\": (\n+        \"52cbb006d3dadd03f6e095a8ca1aca47aecdd75acb4bc74bce1f5c695d0086e6\",\n+        \"40a20c5548a5e9202f69735ecc06c990e6b7c9d2de39f0361e27baeb24cb7c45\",\n+    ),\n+    \"convnext_large\": (\n+        \"070c5ed9ed289581e477741d3b34beffa920db8cf590899d6d2c67fba2a198a6\",\n+        \"96f02b6f0753d4f543261bc9d09bed650f24dd6bc02ddde3066135b63d23a1cd\",\n+    ),\n+    \"convnext_xlarge\": (\n+        \"c1f5ccab661354fc3a79a10fa99af82f0fbf10ec65cb894a3ae0815f17a889ee\",\n+        \"de3f8a54174130e0cecdc71583354753d557fcf1f4487331558e2a16ba0cfe05\",\n+    ),\n+}\n+\n+\n+MODEL_CONFIGS = {\n+    \"tiny\": {\n+        \"depths\": [3, 3, 9, 3],\n+        \"projection_dims\": [96, 192, 384, 768],\n+        \"default_size\": 224,\n+    },\n+    \"small\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [96, 192, 384, 768],\n+        \"default_size\": 224,\n+    },\n+    \"base\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [128, 256, 512, 1024],\n+        \"default_size\": 224,\n+    },\n+    \"large\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [192, 384, 768, 1536],\n+        \"default_size\": 224,\n+    },\n+    \"xlarge\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [256, 512, 1024, 2048],\n+        \"default_size\": 224,\n+    },\n+}\n+\n+BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n+\n+References:\n+- [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n+(CVPR 2022)\n+\n+For image classification use cases, see\n+[this page for detailed examples](\n+https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+For transfer learning use cases, make sure to read the\n+[guide to transfer learning & fine-tuning](\n+https://keras.io/guides/transfer_learning/).\n+\n+The `base`, `large`, and `xlarge` models were first pre-trained on the\n+ImageNet-21k dataset and then fine-tuned on the ImageNet-1k dataset. The\n+pre-trained parameters of the models were assembled from the\n+[official repository](https://github.com/facebookresearch/ConvNeXt). To get a\n+sense of how these parameters were converted to Keras compatible parameters,\n+please refer to\n+[this repository](https://github.com/sayakpaul/keras-convnext-conversion).\n+\n+Note: Each Keras Application expects a specific kind of input preprocessing.\n+For ConvNeXt, preprocessing is included in the model using a `Normalization`\n+layer.  ConvNeXt models expect their inputs to be float or uint8 tensors of\n+pixels with values in the [0-255] range.\n+\n+When calling the `summary()` method after instantiating a ConvNeXt model,\n+prefer setting the `expand_nested` argument `summary()` to `True` to better\n+investigate the instantiated model.\n+\n+Args:\n+    include_top: Whether to include the fully-connected\n+        layer at the top of the network. Defaults to `True`.\n+    weights: One of `None` (random initialization),\n+        `\"imagenet\"` (pre-training on ImageNet-1k), or the path to the weights\n+        file to be loaded. Defaults to `\"imagenet\"`.\n+    input_tensor: Optional Keras tensor\n+        (i.e. output of `layers.Input()`)\n+        to use as image input for the model.\n+    input_shape: Optional shape tuple, only to be specified\n+        if `include_top` is `False`.\n+        It should have exactly 3 inputs channels.\n+    pooling: Optional pooling mode for feature extraction\n+        when `include_top` is `False`. Defaults to None.\n+        - `None` means that the output of the model will be\n+        the 4D tensor output of the last convolutional layer.\n+        - `avg` means that global average pooling\n+        will be applied to the output of the\n+        last convolutional layer, and thus\n+        the output of the model will be a 2D tensor.\n+        - `max` means that global max pooling will\n+        be applied.\n+    classes: Optional number of classes to classify images\n+        into, only to be specified if `include_top` is `True`, and\n+        if no `weights` argument is specified. Defaults to 1000 (number of\n+        ImageNet classes).\n+    classifier_activation: A `str` or callable. The activation function to use\n+        on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        `classifier_activation=None` to return the logits of the \"top\" layer.\n+        Defaults to `\"softmax\"`.\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n+\n+Returns:\n+    A model instance.\n+\"\"\"\n+\n+\n+class StochasticDepth(Layer):\n+    \"\"\"Stochastic Depth module.\n+\n+    It performs batch-wise dropping rather than sample-wise. In libraries like\n+    `timm`, it's similar to `DropPath` layers that drops residual paths\n+    sample-wise.\n+\n+    References:\n+    - https://github.com/rwightman/pytorch-image-models\n+\n+    Args:\n+      drop_path_rate (float): Probability of dropping paths. Should be within\n+        [0, 1].\n+\n+    Returns:\n+      Tensor either with the residual path dropped or kept.\n+    \"\"\"\n+\n+    def __init__(self, drop_path_rate, **kwargs):\n+        super().__init__(**kwargs)\n+        self.drop_path_rate = drop_path_rate\n+\n+    def call(self, x, training=None):\n+        if training:\n+            keep_prob = 1 - self.drop_path_rate\n+            shape = (ops.shape(x)[0],) + (1,) * (len(ops.shape(x)) - 1)\n+            random_tensor = keep_prob + random.uniform(shape, 0, 1)\n+            random_tensor = ops.floor(random_tensor)\n+            return (x / keep_prob) * random_tensor\n+        return x\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"drop_path_rate\": self.drop_path_rate})\n+        return config\n+\n+\n+class LayerScale(Layer):\n+    \"\"\"Layer scale module.\n+\n+    References:\n+\n+    - https://arxiv.org/abs/2103.17239\n+\n+    Args:\n+        init_values (float): Initial value for layer scale. Should be within\n+            [0, 1].\n+        projection_dim (int): Projection dimensionality.\n+\n+    Returns:\n+        Tensor multiplied to the scale.\n+    \"\"\"\n+\n+    def __init__(self, init_values, projection_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.init_values = init_values\n+        self.projection_dim = projection_dim\n+\n+    def build(self, _):\n+        self.gamma = self.add_weight(\n+            shape=(self.projection_dim,),\n+            initializer=initializers.Constant(self.init_values),\n+            trainable=True,\n+        )\n+\n+    def call(self, x):\n+        return x * self.gamma\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"init_values\": self.init_values,\n+                \"projection_dim\": self.projection_dim,\n+            }\n+        )\n+        return config\n+\n+\n+def ConvNeXtBlock(\n+    projection_dim, drop_path_rate=0.0, layer_scale_init_value=1e-6, name=None\n+):\n+    \"\"\"ConvNeXt block.\n+\n+    References:\n+    - https://arxiv.org/abs/2201.03545\n+    - https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py\n+\n+    Notes:\n+        In the original ConvNeXt implementation (linked above), the authors use\n+        `Dense` layers for pointwise convolutions for increased efficiency.\n+        Following that, this implementation also uses the same.\n+\n+    Args:\n+        projection_dim (int): Number of filters for convolution layers. In the\n+            ConvNeXt paper, this is referred to as projection dimension.\n+        drop_path_rate (float): Probability of dropping paths. Should be within\n+            [0, 1].\n+        layer_scale_init_value (float): Layer scale value.\n+            Should be a small float number.\n+        name: name to path to the keras layer.\n+\n+    Returns:\n+        A function representing a ConvNeXtBlock block.\n+    \"\"\"\n+    if name is None:\n+        name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n+\n+    def apply(inputs):\n+        x = inputs\n+\n+        x = layers.Conv2D(\n+            filters=projection_dim,\n+            kernel_size=7,\n+            padding=\"same\",\n+            groups=projection_dim,\n+            name=name + \"_depthwise_conv\",\n+        )(x)\n+        x = layers.LayerNormalization(epsilon=1e-6, name=name + \"_layernorm\")(x)\n+        x = layers.Dense(4 * projection_dim, name=name + \"_pointwise_conv_1\")(x)\n+        x = layers.Activation(\"gelu\", name=name + \"_gelu\")(x)\n+        x = layers.Dense(projection_dim, name=name + \"_pointwise_conv_2\")(x)\n+\n+        if layer_scale_init_value is not None:\n+            x = LayerScale(\n+                layer_scale_init_value,\n+                projection_dim,\n+                name=name + \"_layer_scale\",\n+            )(x)\n+        if drop_path_rate:\n+            layer = StochasticDepth(\n+                drop_path_rate, name=name + \"_stochastic_depth\"\n+            )\n+        else:\n+            layer = layers.Activation(\"linear\", name=name + \"_identity\")\n+\n+        return inputs + layer(x)\n+\n+    return apply\n+\n+\n+def PreStem(name=None):\n+    \"\"\"Normalizes inputs with ImageNet-1k mean and std.\"\"\"\n+    if name is None:\n+        name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n+\n+    def apply(x):\n+        x = layers.Normalization(\n+            mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n+            variance=[\n+                (0.229 * 255) ** 2,\n+                (0.224 * 255) ** 2,\n+                (0.225 * 255) ** 2,\n+            ],\n+            name=name + \"_prestem_normalization\",\n+        )(x)\n+        return x\n+\n+    return apply\n+\n+\n+def Head(num_classes=1000, classifier_activation=None, name=None):\n+    \"\"\"Implementation of classification head of ConvNeXt.\n+\n+    Args:\n+        num_classes: number of classes for Dense layer\n+        classifier_activation: activation function for the Dense layer\n+        name: name prefix\n+\n+    Returns:\n+        Classification head function.\n+    \"\"\"\n+    if name is None:\n+        name = str(backend.get_uid(\"head\"))\n+\n+    def apply(x):\n+        x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n+        x = layers.LayerNormalization(\n+            epsilon=1e-6, name=name + \"_head_layernorm\"\n+        )(x)\n+        x = layers.Dense(\n+            num_classes,\n+            activation=classifier_activation,\n+            name=name + \"_head_dense\",\n+        )(x)\n+        return x\n+\n+    return apply\n+\n+\n+def ConvNeXt(\n+    depths,\n+    projection_dims,\n+    drop_path_rate=0.0,\n+    layer_scale_init_value=1e-6,\n+    default_size=224,\n+    model_name=\"convnext\",\n+    include_preprocessing=True,\n+    include_top=True,\n+    weights=None,\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates ConvNeXt architecture given specific configuration.\n+\n+    Args:\n+        depths: An iterable containing depths for each individual stages.\n+        projection_dims: An iterable containing output number of channels of\n+        each individual stages.\n+        drop_path_rate: Stochastic depth probability. If 0.0, then stochastic\n+            depth won't be used.\n+        layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling\n+            won't be used.\n+        default_size: Default input image size.\n+        model_name: An optional name for the model.\n+        include_preprocessing: boolean denoting whther to\n+            include preprocessing in the model.\n+            When `weights=\"imagenet\"` this should always be `True`.\n+            But for other models (e.g., randomly initialized) you should set it\n+            to `False` and apply preprocessing to data accordingly.\n+        include_top: Boolean denoting whether to include classification\n+            head to the model.\n+        weights: one of `None` (random initialization), `\"imagenet\"`\n+            (pre-training on ImageNet-1k),\n+            or the path to the weights file to be loaded.\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n+            use as image input for the model.\n+        input_shape: optional shape tuple, only to be specified if `include_top`\n+            is `False`. It should have exactly 3 inputs channels.\n+        pooling: optional pooling mode for feature extraction when `include_top`\n+            is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the last convolutional layer.\n+            - `avg` means that global average pooling will be applied\n+                to the output of the last convolutional layer,\n+                and thus the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will be applied.\n+        classes: optional number of classes to classify images into,\n+            only to be specified if `include_top` is `True`,\n+            and if no `weights` argument is specified.\n+        classifier_activation: A `str` or callable.\n+            The activation function to use\n+            on the \"top\" layer. Ignored unless `include_top=True`.\n+            Set `classifier_activation=None` to return the logits\n+            of the \"top\" layer.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights=\"imagenet\"` with `include_top=True`, '\n+            \"`classes` should be 1000. \"\n+            f\"Received classes={classes}\"\n+        )\n+\n+    # Determine proper input shape.\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=default_size,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)[0]\n+    else:\n+        inputs = img_input\n+\n+    x = inputs\n+    if include_preprocessing:\n+        channel_axis = (\n+            3 if backend.image_data_format() == \"channels_last\" else 1\n+        )\n+        num_channels = input_shape[channel_axis - 1]\n+        if num_channels == 3:\n+            x = PreStem(name=model_name)(x)\n+\n+    # Stem block.\n+    stem = Sequential(\n+        [\n+            layers.Conv2D(\n+                projection_dims[0],\n+                kernel_size=4,\n+                strides=4,\n+                name=model_name + \"_stem_conv\",\n+            ),\n+            layers.LayerNormalization(\n+                epsilon=1e-6, name=model_name + \"_stem_layernorm\"\n+            ),\n+        ],\n+        name=model_name + \"_stem\",\n+    )\n+\n+    # Downsampling blocks.\n+    downsample_layers = []\n+    downsample_layers.append(stem)\n+\n+    num_downsample_layers = 3\n+    for i in range(num_downsample_layers):\n+        downsample_layer = Sequential(\n+            [\n+                layers.LayerNormalization(\n+                    epsilon=1e-6,\n+                    name=model_name + \"_downsampling_layernorm_\" + str(i),\n+                ),\n+                layers.Conv2D(\n+                    projection_dims[i + 1],\n+                    kernel_size=2,\n+                    strides=2,\n+                    name=model_name + \"_downsampling_conv_\" + str(i),\n+                ),\n+            ],\n+            name=model_name + \"_downsampling_block_\" + str(i),\n+        )\n+        downsample_layers.append(downsample_layer)\n+\n+    # Stochastic depth schedule.\n+    # This is referred from the original ConvNeXt codebase:\n+    # https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py#L86\n+    depth_drop_rates = [\n+        float(x) for x in np.linspace(0.0, drop_path_rate, sum(depths))\n+    ]\n+\n+    # First apply downsampling blocks and then apply ConvNeXt stages.\n+    cur = 0\n+\n+    num_convnext_blocks = 4\n+    for i in range(num_convnext_blocks):\n+        x = downsample_layers[i](x)\n+        for j in range(depths[i]):\n+            x = ConvNeXtBlock(\n+                projection_dim=projection_dims[i],\n+                drop_path_rate=depth_drop_rates[cur + j],\n+                layer_scale_init_value=layer_scale_init_value,\n+                name=model_name + f\"_stage_{i}_block_{j}\",\n+            )(x)\n+        cur += depths[i]\n+\n+    if include_top:\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = Head(\n+            num_classes=classes,\n+            classifier_activation=classifier_activation,\n+            name=model_name,\n+        )(x)\n+\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D()(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D()(x)\n+        x = layers.LayerNormalization(epsilon=1e-6)(x)\n+\n+    model = Functional(inputs=inputs, outputs=x, name=model_name)\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            file_suffix = \".h5\"\n+            file_hash = WEIGHTS_HASHES[model_name][0]\n+        else:\n+            file_suffix = \"_notop.h5\"\n+            file_hash = WEIGHTS_HASHES[model_name][1]\n+        file_name = model_name + file_suffix\n+        weights_path = file_utils.get_file(\n+            file_name,\n+            BASE_WEIGHTS_PATH + file_name,\n+            cache_subdir=\"models\",\n+            file_hash=file_hash,\n+        )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+## Instantiating variants ##\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.convnext.ConvNeXtTiny\",\n+        \"keras_core.applications.ConvNeXtTiny\",\n+    ]\n+)\n+def ConvNeXtTiny(\n+    model_name=\"convnext_tiny\",\n+    include_top=True,\n+    include_preprocessing=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    return ConvNeXt(\n+        depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n+        projection_dims=MODEL_CONFIGS[\"tiny\"][\"projection_dims\"],\n+        drop_path_rate=0.0,\n+        layer_scale_init_value=1e-6,\n+        default_size=MODEL_CONFIGS[\"tiny\"][\"default_size\"],\n+        model_name=model_name,\n+        include_top=include_top,\n+        include_preprocessing=include_preprocessing,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.convnext.ConvNeXtSmall\",\n+        \"keras_core.applications.ConvNeXtSmall\",\n+    ]\n+)\n+def ConvNeXtSmall(\n+    model_name=\"convnext_small\",\n+    include_top=True,\n+    include_preprocessing=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    return ConvNeXt(\n+        depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n+        projection_dims=MODEL_CONFIGS[\"small\"][\"projection_dims\"],\n+        drop_path_rate=0.0,\n+        layer_scale_init_value=1e-6,\n+        default_size=MODEL_CONFIGS[\"small\"][\"default_size\"],\n+        model_name=model_name,\n+        include_top=include_top,\n+        include_preprocessing=include_preprocessing,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.convnext.ConvNeXtBase\",\n+        \"keras_core.applications.ConvNeXtBase\",\n+    ]\n+)\n+def ConvNeXtBase(\n+    model_name=\"convnext_base\",\n+    include_top=True,\n+    include_preprocessing=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    return ConvNeXt(\n+        depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n+        projection_dims=MODEL_CONFIGS[\"base\"][\"projection_dims\"],\n+        drop_path_rate=0.0,\n+        layer_scale_init_value=1e-6,\n+        default_size=MODEL_CONFIGS[\"base\"][\"default_size\"],\n+        model_name=model_name,\n+        include_top=include_top,\n+        include_preprocessing=include_preprocessing,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.convnext.ConvNeXtLarge\",\n+        \"keras_core.applications.ConvNeXtLarge\",\n+    ]\n+)\n+def ConvNeXtLarge(\n+    model_name=\"convnext_large\",\n+    include_top=True,\n+    include_preprocessing=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    return ConvNeXt(\n+        depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n+        projection_dims=MODEL_CONFIGS[\"large\"][\"projection_dims\"],\n+        drop_path_rate=0.0,\n+        layer_scale_init_value=1e-6,\n+        default_size=MODEL_CONFIGS[\"large\"][\"default_size\"],\n+        model_name=model_name,\n+        include_top=include_top,\n+        include_preprocessing=include_preprocessing,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.convnext.ConvNeXtXLarge\",\n+        \"keras_core.applications.ConvNeXtXLarge\",\n+    ]\n+)\n+def ConvNeXtXLarge(\n+    model_name=\"convnext_xlarge\",\n+    include_top=True,\n+    include_preprocessing=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    return ConvNeXt(\n+        depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n+        projection_dims=MODEL_CONFIGS[\"xlarge\"][\"projection_dims\"],\n+        drop_path_rate=0.0,\n+        layer_scale_init_value=1e-6,\n+        default_size=MODEL_CONFIGS[\"xlarge\"][\"default_size\"],\n+        model_name=model_name,\n+        include_top=include_top,\n+        include_preprocessing=include_preprocessing,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+ConvNeXtTiny.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtTiny\")\n+ConvNeXtSmall.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtSmall\")\n+ConvNeXtBase.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtBase\")\n+ConvNeXtLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtLarge\")\n+ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n+\n+\n+@keras_core_export(\"keras_core.applications.convnext.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    \"\"\"A placeholder method for backward compatibility.\n+\n+    The preprocessing logic has been included in the convnext model\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n+\n+    Args:\n+        x: A floating point `numpy.array` or a tensor.\n+        data_format: Optional data format of the image tensor/array. Defaults to\n+            None, in which case the global setting\n+            `keras_core.backend.image_data_format()` is used\n+            (unless you changed it, it defaults to `\"channels_last\"`).{mode}\n+\n+    Returns:\n+        Unchanged `numpy.array` or tensor.\n+    \"\"\"\n+    return x\n+\n+\n+@keras_core_export(\"keras_core.applications.convnext.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -0,0 +1,400 @@\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.layers.layer import Layer\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHT_URL = (\n+    \"https://storage.googleapis.com/tensorflow/\"\n+    \"keras-applications/inception_resnet_v2/\"\n+)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.inception_resnet_v2.InceptionResNetV2\",\n+        \"keras_core.applications.InceptionResNetV2\",\n+    ]\n+)\n+def InceptionResNetV2(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the Inception-ResNet v2 architecture.\n+\n+    Reference:\n+    - [Inception-v4, Inception-ResNet and the Impact of\n+       Residual Connections on Learning](https://arxiv.org/abs/1602.07261)\n+      (AAAI 2017)\n+\n+    This function returns a Keras image classification model,\n+    optionally loaded with weights pre-trained on ImageNet.\n+\n+    For image classification use cases, see\n+    [this page for detailed examples](\n+      https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+    For transfer learning use cases, make sure to read the\n+    [guide to transfer learning & fine-tuning](\n+      https://keras.io/guides/transfer_learning/).\n+\n+    Note: each Keras Application expects a specific kind of\n+    input preprocessing. For InceptionResNetV2, call\n+    `keras_core.applications.inception_resnet_v2.preprocess_input`\n+    on your inputs before passing them to the model.\n+    `inception_resnet_v2.preprocess_input`\n+    will scale input pixels between -1 and 1.\n+\n+    Args:\n+        include_top: whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: one of `None` (random initialization),\n+            `\"imagenet\"` (pre-training on ImageNet),\n+            or the path to the weights file to be loaded.\n+        input_tensor: optional Keras tensor\n+            (i.e. output of `layers.Input()`)\n+            to use as image input for the model.\n+        input_shape: optional shape tuple, only to be specified\n+            if `include_top` is `False` (otherwise the input shape\n+            has to be `(299, 299, 3)`\n+            (with `'channels_last'` data format)\n+            or `(3, 299, 299)` (with `'channels_first'` data format).\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 75.\n+            E.g. `(150, 150, 3)` would be one valid value.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model will be\n+                the 4D tensor output of the last convolutional block.\n+            - `'avg'` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional block, and thus\n+                the output of the model will be a 2D tensor.\n+            - `'max'` means that global max pooling will be applied.\n+        classes: optional number of classes to classify images\n+            into, only to be specified if `include_top` is `True`,\n+            and if no `weights` argument is specified.\n+        classifier_activation: A `str` or callable.\n+            The activation function to use on the \"top\" layer.\n+            Ignored unless `include_top=True`.\n+            Set `classifier_activation=None` to return the logits\n+            of the \"top\" layer. When loading pretrained weights,\n+            `classifier_activation` can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights=\"imagenet\"` with `include_top=True`, '\n+            \"`classes` should be 1000. \"\n+            f\"Received classes={classes}\"\n+        )\n+\n+    # Determine proper input shape\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=299,\n+        min_size=75,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    # Stem block: 35 x 35 x 192\n+    x = conv2d_bn(img_input, 32, 3, strides=2, padding=\"valid\")\n+    x = conv2d_bn(x, 32, 3, padding=\"valid\")\n+    x = conv2d_bn(x, 64, 3)\n+    x = layers.MaxPooling2D(3, strides=2)(x)\n+    x = conv2d_bn(x, 80, 1, padding=\"valid\")\n+    x = conv2d_bn(x, 192, 3, padding=\"valid\")\n+    x = layers.MaxPooling2D(3, strides=2)(x)\n+\n+    # Mixed 5b (Inception-A block): 35 x 35 x 320\n+    branch_0 = conv2d_bn(x, 96, 1)\n+    branch_1 = conv2d_bn(x, 48, 1)\n+    branch_1 = conv2d_bn(branch_1, 64, 5)\n+    branch_2 = conv2d_bn(x, 64, 1)\n+    branch_2 = conv2d_bn(branch_2, 96, 3)\n+    branch_2 = conv2d_bn(branch_2, 96, 3)\n+    branch_pool = layers.AveragePooling2D(3, strides=1, padding=\"same\")(x)\n+    branch_pool = conv2d_bn(branch_pool, 64, 1)\n+    branches = [branch_0, branch_1, branch_2, branch_pool]\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else 3\n+    x = layers.Concatenate(axis=channel_axis, name=\"mixed_5b\")(branches)\n+\n+    # 10x block35 (Inception-ResNet-A block): 35 x 35 x 320\n+    for block_idx in range(1, 11):\n+        x = inception_resnet_block(\n+            x, scale=0.17, block_type=\"block35\", block_idx=block_idx\n+        )\n+\n+    # Mixed 6a (Reduction-A block): 17 x 17 x 1088\n+    branch_0 = conv2d_bn(x, 384, 3, strides=2, padding=\"valid\")\n+    branch_1 = conv2d_bn(x, 256, 1)\n+    branch_1 = conv2d_bn(branch_1, 256, 3)\n+    branch_1 = conv2d_bn(branch_1, 384, 3, strides=2, padding=\"valid\")\n+    branch_pool = layers.MaxPooling2D(3, strides=2, padding=\"valid\")(x)\n+    branches = [branch_0, branch_1, branch_pool]\n+    x = layers.Concatenate(axis=channel_axis, name=\"mixed_6a\")(branches)\n+\n+    # 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088\n+    for block_idx in range(1, 21):\n+        x = inception_resnet_block(\n+            x, scale=0.1, block_type=\"block17\", block_idx=block_idx\n+        )\n+\n+    # Mixed 7a (Reduction-B block): 8 x 8 x 2080\n+    branch_0 = conv2d_bn(x, 256, 1)\n+    branch_0 = conv2d_bn(branch_0, 384, 3, strides=2, padding=\"valid\")\n+    branch_1 = conv2d_bn(x, 256, 1)\n+    branch_1 = conv2d_bn(branch_1, 288, 3, strides=2, padding=\"valid\")\n+    branch_2 = conv2d_bn(x, 256, 1)\n+    branch_2 = conv2d_bn(branch_2, 288, 3)\n+    branch_2 = conv2d_bn(branch_2, 320, 3, strides=2, padding=\"valid\")\n+    branch_pool = layers.MaxPooling2D(3, strides=2, padding=\"valid\")(x)\n+    branches = [branch_0, branch_1, branch_2, branch_pool]\n+    x = layers.Concatenate(axis=channel_axis, name=\"mixed_7a\")(branches)\n+\n+    # 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080\n+    for block_idx in range(1, 10):\n+        x = inception_resnet_block(\n+            x, scale=0.2, block_type=\"block8\", block_idx=block_idx\n+        )\n+    x = inception_resnet_block(\n+        x, scale=1.0, activation=None, block_type=\"block8\", block_idx=10\n+    )\n+\n+    # Final convolution block: 8 x 8 x 1536\n+    x = conv2d_bn(x, 1536, 1, name=\"conv_7b\")\n+\n+    if include_top:\n+        # Classification block\n+        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes, activation=classifier_activation, name=\"predictions\"\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D()(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D()(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Functional(inputs, x, name=\"inception_resnet_v2\")\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            fname = \"inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5\"\n+            weights_path = file_utils.get_file(\n+                fname,\n+                BASE_WEIGHT_URL + fname,\n+                cache_subdir=\"models\",\n+                file_hash=\"e693bd0210a403b3192acc6073ad2e96\",\n+            )\n+        else:\n+            fname = (\n+                \"inception_resnet_v2_weights_\"\n+                \"tf_dim_ordering_tf_kernels_notop.h5\"\n+            )\n+            weights_path = file_utils.get_file(\n+                fname,\n+                BASE_WEIGHT_URL + fname,\n+                cache_subdir=\"models\",\n+                file_hash=\"d19885ff4a710c122648d3b5c3b684e4\",\n+            )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+def conv2d_bn(\n+    x,\n+    filters,\n+    kernel_size,\n+    strides=1,\n+    padding=\"same\",\n+    activation=\"relu\",\n+    use_bias=False,\n+    name=None,\n+):\n+    \"\"\"Utility function to apply conv + BN.\n+\n+    Args:\n+        x: input tensor.\n+        filters: filters in `Conv2D`.\n+        kernel_size: kernel size as in `Conv2D`.\n+        strides: strides in `Conv2D`.\n+        padding: padding mode in `Conv2D`.\n+        activation: activation in `Conv2D`.\n+        use_bias: whether to use a bias in `Conv2D`.\n+        name: name of the ops; will become `name + '_ac'`\n+            for the activation and `name + '_bn'` for the batch norm layer.\n+\n+    Returns:\n+        Output tensor after applying `Conv2D` and `BatchNormalization`.\n+    \"\"\"\n+    x = layers.Conv2D(\n+        filters,\n+        kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        use_bias=use_bias,\n+        name=name,\n+    )(x)\n+    if not use_bias:\n+        bn_axis = 1 if backend.image_data_format() == \"channels_first\" else 3\n+        bn_name = None if name is None else name + \"_bn\"\n+        x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(\n+            x\n+        )\n+    if activation is not None:\n+        ac_name = None if name is None else name + \"_ac\"\n+        x = layers.Activation(activation, name=ac_name)(x)\n+    return x\n+\n+\n+class CustomScaleLayer(Layer):\n+    def __init__(self, scale, **kwargs):\n+        super().__init__(**kwargs)\n+        self.scale = scale\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"scale\": self.scale})\n+        return config\n+\n+    def call(self, inputs):\n+        return inputs[0] + inputs[1] * self.scale\n+\n+\n+def inception_resnet_block(x, scale, block_type, block_idx, activation=\"relu\"):\n+    \"\"\"Adds an Inception-ResNet block.\n+\n+    Args:\n+        x: input tensor.\n+        scale: scaling factor to scale the residuals\n+            (i.e., the output of passing `x` through an inception module)\n+            before adding them to the shortcut\n+            branch. Let `r` be the output from the residual branch,\n+            the output of this block will be `x + scale * r`.\n+        block_type: `'block35'`, `'block17'` or `'block8'`,\n+            determines the network structure in the residual branch.\n+        block_idx: an `int` used for generating layer names.\n+            The Inception-ResNet blocks are repeated many times\n+            in this network. We use `block_idx` to identify each\n+            of the repetitions. For example, the first\n+            Inception-ResNet-A block will have\n+            `block_type='block35', block_idx=0`, and the layer names\n+            will have a common prefix `'block35_0'`.\n+        activation: activation function to use at the end of the block.\n+\n+    Returns:\n+        Output tensor for the block.\n+    \"\"\"\n+    if block_type == \"block35\":\n+        branch_0 = conv2d_bn(x, 32, 1)\n+        branch_1 = conv2d_bn(x, 32, 1)\n+        branch_1 = conv2d_bn(branch_1, 32, 3)\n+        branch_2 = conv2d_bn(x, 32, 1)\n+        branch_2 = conv2d_bn(branch_2, 48, 3)\n+        branch_2 = conv2d_bn(branch_2, 64, 3)\n+        branches = [branch_0, branch_1, branch_2]\n+    elif block_type == \"block17\":\n+        branch_0 = conv2d_bn(x, 192, 1)\n+        branch_1 = conv2d_bn(x, 128, 1)\n+        branch_1 = conv2d_bn(branch_1, 160, [1, 7])\n+        branch_1 = conv2d_bn(branch_1, 192, [7, 1])\n+        branches = [branch_0, branch_1]\n+    elif block_type == \"block8\":\n+        branch_0 = conv2d_bn(x, 192, 1)\n+        branch_1 = conv2d_bn(x, 192, 1)\n+        branch_1 = conv2d_bn(branch_1, 224, [1, 3])\n+        branch_1 = conv2d_bn(branch_1, 256, [3, 1])\n+        branches = [branch_0, branch_1]\n+    else:\n+        raise ValueError(\n+            \"Unknown Inception-ResNet block type. \"\n+            'Expects \"block35\", \"block17\" or \"block8\", '\n+            \"but got: \" + str(block_type)\n+        )\n+\n+    block_name = block_type + \"_\" + str(block_idx)\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else 3\n+    mixed = layers.Concatenate(axis=channel_axis, name=block_name + \"_mixed\")(\n+        branches\n+    )\n+    up = conv2d_bn(\n+        mixed,\n+        x.shape[channel_axis],\n+        1,\n+        activation=None,\n+        use_bias=True,\n+        name=block_name + \"_conv\",\n+    )\n+\n+    x = CustomScaleLayer(scale)([x, up])\n+    if activation is not None:\n+        x = layers.Activation(activation, name=block_name + \"_ac\")(x)\n+    return x\n+\n+\n+@keras_core_export(\n+    \"keras_core.applications.inception_resnet_v2.preprocess_input\"\n+)\n+def preprocess_input(x, data_format=None):\n+    return imagenet_utils.preprocess_input(\n+        x, data_format=data_format, mode=\"tf\"\n+    )\n+\n+\n+@keras_core_export(\n+    \"keras_core.applications.inception_resnet_v2.decode_predictions\"\n+)\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n+    mode=\"\",\n+    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TF,\n+    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC,\n+)\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -0,0 +1,442 @@\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+WEIGHTS_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n+    \"inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\"\n+)\n+WEIGHTS_PATH_NO_TOP = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n+    \"inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n+)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.inception_v3.InceptionV3\",\n+        \"keras_core.applications.InceptionV3\",\n+    ]\n+)\n+def InceptionV3(\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    input_shape=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the Inception v3 architecture.\n+\n+    Reference:\n+    - [Rethinking the Inception Architecture for Computer Vision](\n+        http://arxiv.org/abs/1512.00567) (CVPR 2016)\n+\n+    This function returns a Keras image classification model,\n+    optionally loaded with weights pre-trained on ImageNet.\n+\n+    For image classification use cases, see\n+    [this page for detailed examples](\n+      https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+    For transfer learning use cases, make sure to read the\n+    [guide to transfer learning & fine-tuning](\n+      https://keras.io/guides/transfer_learning/).\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For `InceptionV3`, call\n+    `keras_core.applications.inception_v3.preprocess_input` on your inputs\n+    before passing them to the model.\n+    `inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n+\n+    Args:\n+        include_top: Boolean, whether to include the fully-connected\n+            layer at the top, as the last layer of the network.\n+            Defaults to `True`.\n+        weights: One of `None` (random initialization),\n+            `imagenet` (pre-training on ImageNet),\n+            or the path to the weights file to be loaded.\n+            Defaults to `\"imagenet\"`.\n+        input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n+            to use as image input for the model. `input_tensor` is useful for\n+            sharing inputs between multiple different networks.\n+            Defaults to `None`.\n+        input_shape: Optional shape tuple, only to be specified\n+            if `include_top` is False (otherwise the input shape\n+            has to be `(299, 299, 3)` (with `channels_last` data format)\n+            or `(3, 299, 299)` (with `channels_first` data format).\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 75.\n+            E.g. `(150, 150, 3)` would be one valid value.\n+            `input_shape` will be ignored if the `input_tensor` is provided.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` (default) means that the output of the model will be\n+                the 4D tensor output of the last convolutional block.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional block, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will be applied.\n+        classes: optional number of classes to classify images\n+            into, only to be specified if `include_top` is `True`, and\n+            if no `weights` argument is specified. Defaults to 1000.\n+        classifier_activation: A `str` or callable. The activation function\n+            to use on the \"top\" layer. Ignored unless `include_top=True`.\n+            Set `classifier_activation=None` to return the logits of the \"top\"\n+            layer. When loading pretrained weights, `classifier_activation`\n+            can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded; \"\n+            f\"Received: weights={weights}\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights=\"imagenet\"` with `include_top=True`, '\n+            \"`classes` should be 1000. \"\n+            f\"Received classes={classes}\"\n+        )\n+\n+    # Determine proper input shape\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=299,\n+        min_size=75,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    if backend.image_data_format() == \"channels_first\":\n+        channel_axis = 1\n+    else:\n+        channel_axis = 3\n+\n+    x = conv2d_bn(img_input, 32, 3, 3, strides=(2, 2), padding=\"valid\")\n+    x = conv2d_bn(x, 32, 3, 3, padding=\"valid\")\n+    x = conv2d_bn(x, 64, 3, 3)\n+    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n+\n+    x = conv2d_bn(x, 80, 1, 1, padding=\"valid\")\n+    x = conv2d_bn(x, 192, 3, 3, padding=\"valid\")\n+    x = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n+\n+    # mixed 0: 35 x 35 x 256\n+    branch1x1 = conv2d_bn(x, 64, 1, 1)\n+\n+    branch5x5 = conv2d_bn(x, 48, 1, 1)\n+    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+\n+    branch_pool = layers.AveragePooling2D(\n+        (3, 3), strides=(1, 1), padding=\"same\"\n+    )(x)\n+    branch_pool = conv2d_bn(branch_pool, 32, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+        axis=channel_axis,\n+        name=\"mixed0\",\n+    )\n+\n+    # mixed 1: 35 x 35 x 288\n+    branch1x1 = conv2d_bn(x, 64, 1, 1)\n+\n+    branch5x5 = conv2d_bn(x, 48, 1, 1)\n+    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+\n+    branch_pool = layers.AveragePooling2D(\n+        (3, 3), strides=(1, 1), padding=\"same\"\n+    )(x)\n+    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+        axis=channel_axis,\n+        name=\"mixed1\",\n+    )\n+\n+    # mixed 2: 35 x 35 x 288\n+    branch1x1 = conv2d_bn(x, 64, 1, 1)\n+\n+    branch5x5 = conv2d_bn(x, 48, 1, 1)\n+    branch5x5 = conv2d_bn(branch5x5, 64, 5, 5)\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+\n+    branch_pool = layers.AveragePooling2D(\n+        (3, 3), strides=(1, 1), padding=\"same\"\n+    )(x)\n+    branch_pool = conv2d_bn(branch_pool, 64, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch5x5, branch3x3dbl, branch_pool],\n+        axis=channel_axis,\n+        name=\"mixed2\",\n+    )\n+\n+    # mixed 3: 17 x 17 x 768\n+    branch3x3 = conv2d_bn(x, 384, 3, 3, strides=(2, 2), padding=\"valid\")\n+\n+    branch3x3dbl = conv2d_bn(x, 64, 1, 1)\n+    branch3x3dbl = conv2d_bn(branch3x3dbl, 96, 3, 3)\n+    branch3x3dbl = conv2d_bn(\n+        branch3x3dbl, 96, 3, 3, strides=(2, 2), padding=\"valid\"\n+    )\n+\n+    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n+    x = layers.concatenate(\n+        [branch3x3, branch3x3dbl, branch_pool], axis=channel_axis, name=\"mixed3\"\n+    )\n+\n+    # mixed 4: 17 x 17 x 768\n+    branch1x1 = conv2d_bn(x, 192, 1, 1)\n+\n+    branch7x7 = conv2d_bn(x, 128, 1, 1)\n+    branch7x7 = conv2d_bn(branch7x7, 128, 1, 7)\n+    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n+\n+    branch7x7dbl = conv2d_bn(x, 128, 1, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 1, 7)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 128, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+\n+    branch_pool = layers.AveragePooling2D(\n+        (3, 3), strides=(1, 1), padding=\"same\"\n+    )(x)\n+    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+        axis=channel_axis,\n+        name=\"mixed4\",\n+    )\n+\n+    # mixed 5, 6: 17 x 17 x 768\n+    for i in range(2):\n+        branch1x1 = conv2d_bn(x, 192, 1, 1)\n+\n+        branch7x7 = conv2d_bn(x, 160, 1, 1)\n+        branch7x7 = conv2d_bn(branch7x7, 160, 1, 7)\n+        branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n+\n+        branch7x7dbl = conv2d_bn(x, 160, 1, 1)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 1, 7)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 160, 7, 1)\n+        branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+\n+        branch_pool = layers.AveragePooling2D(\n+            (3, 3), strides=(1, 1), padding=\"same\"\n+        )(x)\n+        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+        x = layers.concatenate(\n+            [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+            axis=channel_axis,\n+            name=\"mixed\" + str(5 + i),\n+        )\n+\n+    # mixed 7: 17 x 17 x 768\n+    branch1x1 = conv2d_bn(x, 192, 1, 1)\n+\n+    branch7x7 = conv2d_bn(x, 192, 1, 1)\n+    branch7x7 = conv2d_bn(branch7x7, 192, 1, 7)\n+    branch7x7 = conv2d_bn(branch7x7, 192, 7, 1)\n+\n+    branch7x7dbl = conv2d_bn(x, 192, 1, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 7, 1)\n+    branch7x7dbl = conv2d_bn(branch7x7dbl, 192, 1, 7)\n+\n+    branch_pool = layers.AveragePooling2D(\n+        (3, 3), strides=(1, 1), padding=\"same\"\n+    )(x)\n+    branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+    x = layers.concatenate(\n+        [branch1x1, branch7x7, branch7x7dbl, branch_pool],\n+        axis=channel_axis,\n+        name=\"mixed7\",\n+    )\n+\n+    # mixed 8: 8 x 8 x 1280\n+    branch3x3 = conv2d_bn(x, 192, 1, 1)\n+    branch3x3 = conv2d_bn(branch3x3, 320, 3, 3, strides=(2, 2), padding=\"valid\")\n+\n+    branch7x7x3 = conv2d_bn(x, 192, 1, 1)\n+    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 1, 7)\n+    branch7x7x3 = conv2d_bn(branch7x7x3, 192, 7, 1)\n+    branch7x7x3 = conv2d_bn(\n+        branch7x7x3, 192, 3, 3, strides=(2, 2), padding=\"valid\"\n+    )\n+\n+    branch_pool = layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n+    x = layers.concatenate(\n+        [branch3x3, branch7x7x3, branch_pool], axis=channel_axis, name=\"mixed8\"\n+    )\n+\n+    # mixed 9: 8 x 8 x 2048\n+    for i in range(2):\n+        branch1x1 = conv2d_bn(x, 320, 1, 1)\n+\n+        branch3x3 = conv2d_bn(x, 384, 1, 1)\n+        branch3x3_1 = conv2d_bn(branch3x3, 384, 1, 3)\n+        branch3x3_2 = conv2d_bn(branch3x3, 384, 3, 1)\n+        branch3x3 = layers.concatenate(\n+            [branch3x3_1, branch3x3_2],\n+            axis=channel_axis,\n+            name=\"mixed9_\" + str(i),\n+        )\n+\n+        branch3x3dbl = conv2d_bn(x, 448, 1, 1)\n+        branch3x3dbl = conv2d_bn(branch3x3dbl, 384, 3, 3)\n+        branch3x3dbl_1 = conv2d_bn(branch3x3dbl, 384, 1, 3)\n+        branch3x3dbl_2 = conv2d_bn(branch3x3dbl, 384, 3, 1)\n+        branch3x3dbl = layers.concatenate(\n+            [branch3x3dbl_1, branch3x3dbl_2], axis=channel_axis\n+        )\n+\n+        branch_pool = layers.AveragePooling2D(\n+            (3, 3), strides=(1, 1), padding=\"same\"\n+        )(x)\n+        branch_pool = conv2d_bn(branch_pool, 192, 1, 1)\n+        x = layers.concatenate(\n+            [branch1x1, branch3x3, branch3x3dbl, branch_pool],\n+            axis=channel_axis,\n+            name=\"mixed\" + str(9 + i),\n+        )\n+    if include_top:\n+        # Classification block\n+        x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes, activation=classifier_activation, name=\"predictions\"\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D()(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D()(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+    # Create model.\n+    model = Functional(inputs, x, name=\"inception_v3\")\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            weights_path = file_utils.get_file(\n+                \"inception_v3_weights_tf_dim_ordering_tf_kernels.h5\",\n+                WEIGHTS_PATH,\n+                cache_subdir=\"models\",\n+                file_hash=\"9a0d58056eeedaa3f26cb7ebd46da564\",\n+            )\n+        else:\n+            weights_path = file_utils.get_file(\n+                \"inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\",\n+                WEIGHTS_PATH_NO_TOP,\n+                cache_subdir=\"models\",\n+                file_hash=\"bcbd6486424b2319ff4ef7d526e38f63\",\n+            )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+def conv2d_bn(\n+    x, filters, num_row, num_col, padding=\"same\", strides=(1, 1), name=None\n+):\n+    \"\"\"Utility function to apply conv + BN.\n+\n+    Args:\n+        x: input tensor.\n+        filters: filters in `Conv2D`.\n+        num_row: height of the convolution kernel.\n+        num_col: width of the convolution kernel.\n+        padding: padding mode in `Conv2D`.\n+        strides: strides in `Conv2D`.\n+        name: name of the ops; will become `name + '_conv'`\n+            for the convolution and `name + '_bn'` for the\n+            batch norm layer.\n+\n+    Returns:\n+        Output tensor after applying `Conv2D` and `BatchNormalization`.\n+    \"\"\"\n+    if name is not None:\n+        bn_name = name + \"_bn\"\n+        conv_name = name + \"_conv\"\n+    else:\n+        bn_name = None\n+        conv_name = None\n+    if backend.image_data_format() == \"channels_first\":\n+        bn_axis = 1\n+    else:\n+        bn_axis = 3\n+    x = layers.Conv2D(\n+        filters,\n+        (num_row, num_col),\n+        strides=strides,\n+        padding=padding,\n+        use_bias=False,\n+        name=conv_name,\n+    )(x)\n+    x = layers.BatchNormalization(axis=bn_axis, scale=False, name=bn_name)(x)\n+    x = layers.Activation(\"relu\", name=name)(x)\n+    return x\n+\n+\n+@keras_core_export(\"keras_core.applications.inception_v3.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    return imagenet_utils.preprocess_input(\n+        x, data_format=data_format, mode=\"tf\"\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.inception_v3.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n+    mode=\"\",\n+    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TF,\n+    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC,\n+)\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -1,7 +1,6 @@\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping1D\")\n@@ -43,9 +42,9 @@ class Cropping1D(Layer):\n \n     def __init__(self, cropping=(1, 1), name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.cropping = argument_validation.standardize_tuple(\n-            cropping, 2, \"cropping\", allow_zero=True\n-        )\n+        if isinstance(cropping, int):\n+            cropping = (cropping, cropping)\n+        self.cropping = cropping\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -56,14 +56,6 @@ class Cropping1DTest(testing.TestCase):\n         cropped = layers.Cropping1D((1, 2))(input_layer)\n         self.assertEqual(cropped.shape, (1, None, 7))\n \n-    def test_cropping_1d_errors_if_cropping_argument_invalid(self):\n-        with self.assertRaises(ValueError):\n-            layers.Cropping1D(cropping=(1,))\n-        with self.assertRaises(ValueError):\n-            layers.Cropping1D(cropping=(1, 2, 3))\n-        with self.assertRaises(ValueError):\n-            layers.Cropping1D(cropping=\"1\")\n-\n     def test_cropping_1d_errors_if_cropping_more_than_available(self):\n         with self.assertRaises(ValueError):\n             input_layer = layers.Input(batch_shape=(3, 5, 7))\n\n@@ -2,7 +2,6 @@ from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping2D\")\n@@ -66,12 +65,12 @@ class Cropping2D(Layer):\n                     \"`cropping` should have two elements. \"\n                     f\"Received: cropping={cropping}.\"\n                 )\n-            height_cropping = argument_validation.standardize_tuple(\n-                cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n-            )\n-            width_cropping = argument_validation.standardize_tuple(\n-                cropping[1], 2, \"2nd entry of cropping\", allow_zero=True\n-            )\n+            height_cropping = cropping[0]\n+            if isinstance(height_cropping, int):\n+                height_cropping = (height_cropping, height_cropping)\n+            width_cropping = cropping[1]\n+            if isinstance(width_cropping, int):\n+                width_cropping = (width_cropping, width_cropping)\n             self.cropping = (height_cropping, width_cropping)\n         else:\n             raise ValueError(\n\n@@ -97,9 +97,3 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n             layers.Cropping2D(cropping=(1, 2, 3))\n         with self.assertRaises(ValueError):\n             layers.Cropping2D(cropping=\"1\")\n-        with self.assertRaises(ValueError):\n-            layers.Cropping2D(cropping=((1, 2), (3, 4, 5)))\n-        with self.assertRaises(ValueError):\n-            layers.Cropping2D(cropping=((1, 2), (3, -4)))\n-        with self.assertRaises(ValueError):\n-            layers.Cropping2D(cropping=((1, 2), \"3\"))\n\n@@ -2,7 +2,6 @@ from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping3D\")\n@@ -76,15 +75,15 @@ class Cropping3D(Layer):\n                 raise ValueError(\n                     f\"`cropping` should have 3 elements. Received: {cropping}.\"\n                 )\n-            dim1_cropping = argument_validation.standardize_tuple(\n-                cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n-            )\n-            dim2_cropping = argument_validation.standardize_tuple(\n-                cropping[1], 2, \"2nd entry of cropping\", allow_zero=True\n-            )\n-            dim3_cropping = argument_validation.standardize_tuple(\n-                cropping[2], 2, \"3rd entry of cropping\", allow_zero=True\n-            )\n+            dim1_cropping = cropping[0]\n+            if isinstance(dim1_cropping, int):\n+                dim1_cropping = (dim1_cropping, dim1_cropping)\n+            dim2_cropping = cropping[1]\n+            if isinstance(dim2_cropping, int):\n+                dim2_cropping = (dim2_cropping, dim2_cropping)\n+            dim3_cropping = cropping[2]\n+            if isinstance(dim3_cropping, int):\n+                dim3_cropping = (dim3_cropping, dim3_cropping)\n             self.cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n         else:\n             raise ValueError(\n\n@@ -159,9 +159,3 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             layers.Cropping3D(cropping=(1, 2, 3, 4))\n         with self.assertRaises(ValueError):\n             layers.Cropping3D(cropping=\"1\")\n-        with self.assertRaises(ValueError):\n-            layers.Cropping3D(cropping=((1, 2), (3, 4), (5, 6, 7)))\n-        with self.assertRaises(ValueError):\n-            layers.Cropping3D(cropping=((1, 2), (3, 4), (5, -6)))\n-        with self.assertRaises(ValueError):\n-            layers.Cropping3D(cropping=((1, 2), (3, 4), \"5\"))\n\n@@ -2,7 +2,6 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n@@ -49,9 +48,9 @@ class ZeroPadding1D(Layer):\n \n     def __init__(self, padding=1, name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        self.padding = argument_validation.standardize_tuple(\n-            padding, 2, \"padding\", allow_zero=True\n-        )\n+        if isinstance(padding, int):\n+            padding = (padding, padding)\n+        self.padding = padding\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -33,11 +33,3 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n         padded = layers.ZeroPadding1D((1, 2))(input_layer)\n         self.assertEqual(padded.shape, (1, None, 3))\n-\n-    def test_zero_padding_1d_errors_if_padding_argument_invalid(self):\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding1D(padding=(1,))\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding1D(padding=(1, 2, 3))\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding1D(padding=\"1\")\n\n@@ -3,7 +3,6 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding2D\")\n@@ -79,12 +78,12 @@ class ZeroPadding2D(Layer):\n                     \"`padding` should have two elements. \"\n                     f\"Received: padding={padding}.\"\n                 )\n-            height_padding = argument_validation.standardize_tuple(\n-                padding[0], 2, \"1st entry of padding\", allow_zero=True\n-            )\n-            width_padding = argument_validation.standardize_tuple(\n-                padding[1], 2, \"2nd entry of padding\", allow_zero=True\n-            )\n+            height_padding = padding[0]\n+            if isinstance(height_padding, int):\n+                height_padding = (height_padding, height_padding)\n+            width_padding = padding[1]\n+            if isinstance(width_padding, int):\n+                width_padding = (width_padding, width_padding)\n             self.padding = (height_padding, width_padding)\n         else:\n             raise ValueError(\n\n@@ -74,9 +74,3 @@ class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n             layers.ZeroPadding2D(padding=(1, 2, 3))\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding2D(padding=\"1\")\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding2D(padding=((1, 2), (3, 4, 5)))\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding2D(padding=((1, 2), (3, -4)))\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding2D(padding=((1, 2), \"3\"))\n\n@@ -3,7 +3,6 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n-from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding3D\")\n@@ -77,15 +76,16 @@ class ZeroPadding3D(Layer):\n                 raise ValueError(\n                     f\"`padding` should have 3 elements. Received: {padding}.\"\n                 )\n-            dim1_padding = argument_validation.standardize_tuple(\n-                padding[0], 2, \"1st entry of padding\", allow_zero=True\n-            )\n-            dim2_padding = argument_validation.standardize_tuple(\n-                padding[1], 2, \"2nd entry of padding\", allow_zero=True\n-            )\n-            dim3_padding = argument_validation.standardize_tuple(\n-                padding[2], 2, \"3rd entry of padding\", allow_zero=True\n-            )\n+            dim1_padding = padding[0]\n+            if isinstance(dim1_padding, int):\n+                dim1_padding = (dim1_padding, dim1_padding)\n+            dim2_padding = padding[1]\n+            if isinstance(dim2_padding, int):\n+                dim2_padding = (dim2_padding, dim2_padding)\n+            dim3_padding = padding[2]\n+            if isinstance(dim3_padding, int):\n+                dim3_padding = (dim3_padding, dim3_padding)\n+            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n             self.padding = (dim1_padding, dim2_padding, dim3_padding)\n         else:\n             raise ValueError(\n\n@@ -82,9 +82,3 @@ class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n             layers.ZeroPadding3D(padding=(1, 2, 3, 4))\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding3D(padding=\"1\")\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding3D(padding=((1, 2), (3, 4), (5, 6, 7)))\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding3D(padding=((1, 2), (3, 4), (5, -6)))\n-        with self.assertRaises(ValueError):\n-            layers.ZeroPadding3D(padding=((1, 2), (3, 4), \"5\"))\n\n@@ -540,7 +540,7 @@ def deserialize_node(node_data, created_layers):\n             if layer is None:\n                 raise ValueError(f\"Unknown layer: {history[0]}\")\n             inbound_node_index = history[1]\n-            inbound_tensor_index = history[1]\n+            inbound_tensor_index = history[2]\n             if len(layer._inbound_nodes) <= inbound_node_index:\n                 raise ValueError(\n                     \"Layer node index out of bounds.\\n\"\n\n@@ -112,11 +112,6 @@ class Sequential(Model):\n         self._functional = Functional(inputs=inputs, outputs=outputs)\n         self.built = True\n \n-    def __call__(self, inputs, training=None, mask=None):\n-        if self._functional:\n-            return self._functional(inputs, training=training, mask=mask)\n-        return super().__call__(inputs, training=training, mask=mask)\n-\n     def call(self, inputs, training=None, mask=None):\n         if self._functional:\n             return self._functional.call(inputs, training=training, mask=mask)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
