{"custom_id": "keras#91435bffd38e8a74b9eb46a438fe60f551928754", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Î” (Sum/Max): 4/3 | Churn Î”: 44 | Churn Cumulative: 6820 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -947,6 +947,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     def __init__(\n         self,\n         from_logits=False,\n+        ignore_class=None,\n         reduction=\"sum_over_batch_size\",\n         name=\"sparse_categorical_crossentropy\",\n     ):\n@@ -955,14 +956,17 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n             name=name,\n             reduction=reduction,\n             from_logits=from_logits,\n+            ignore_class=ignore_class,\n         )\n         self.from_logits = from_logits\n+        self.ignore_class = ignore_class\n \n     def get_config(self):\n         return {\n             \"name\": self.name,\n             \"reduction\": self.reduction,\n             \"from_logits\": self.from_logits,\n+            \"ignore_class\": self.ignore_class,\n         }\n \n \n@@ -1659,7 +1663,9 @@ def categorical_focal_crossentropy(\n         \"keras_core.losses.sparse_categorical_crossentropy\",\n     ]\n )\n-def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n+def sparse_categorical_crossentropy(\n+    y_true, y_pred, from_logits=False, ignore_class=None, axis=-1\n+):\n     \"\"\"Computes the sparse categorical crossentropy loss.\n \n     Args:\n@@ -1667,6 +1673,11 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n         y_pred: The predicted values.\n         from_logits: Whether `y_pred` is expected to be a logits tensor. By\n             default, we assume that `y_pred` encodes a probability distribution.\n+        ignore_class: Optional integer. The ID of a class to be ignored during\n+            loss computation. This is useful, for example, in segmentation\n+            problems featuring a \"void\" class (commonly -1 or 255) in\n+            segmentation maps. By default (`ignore_class=None`), all classes are\n+            considered.\n         axis: Defaults to -1. The dimension along which the entropy is\n             computed.\n \n@@ -1682,13 +1693,33 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n     >>> loss\n     array([0.0513, 2.303], dtype=float32)\n     \"\"\"\n-    return ops.sparse_categorical_crossentropy(\n+\n+    if ignore_class is not None:\n+        res_shape = ops.shape(y_pred)[:-1]\n+        valid_mask = ops.not_equal(y_true, ops.cast(ignore_class, y_pred.dtype))\n+        y_true = y_true * ops.cast(valid_mask, y_true.dtype)\n+        y_pred = y_pred * ops.cast(\n+            ops.expand_dims(valid_mask, -1), y_pred.dtype\n+        )\n+\n+    res = ops.sparse_categorical_crossentropy(\n         y_true,\n         y_pred,\n         from_logits=from_logits,\n         axis=axis,\n     )\n \n+    if ignore_class is not None:\n+        valid_mask = ops.reshape(valid_mask, res_shape)\n+        res = ops.where(valid_mask, res, 0.0)\n+\n+        try:\n+            res._keras_mask = valid_mask\n+        except AttributeError:\n+            pass\n+\n+    return res\n+\n \n @keras_core_export(\n     [\n\n@@ -1157,6 +1157,15 @@ class SparseCategoricalCrossentropyTest(testing.TestCase):\n         loss = cce_obj(y_true, logits)\n         self.assertAllClose((0.001822, 0.000459, 0.169846), loss, 3)\n \n+    def test_ignore_class(self):\n+        y_true = np.array([[-1, 2]])\n+        logits = np.array([[[0.854, 0.698, 0.598], [0.088, 0.86, 0.018]]])\n+        cce_obj = losses.SparseCategoricalCrossentropy(\n+            from_logits=True, ignore_class=-1, reduction=None\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose([[0.0, 1.48012]], loss, 3)\n+\n \n class BinaryFocalCrossentropyTest(testing.TestCase):\n     def test_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04195f50480911735b8bbc54603cf47be854ca4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 507 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Î” (Sum/Max): 25/25 | Churn Î”: 507 | Churn Cumulative: 507 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.03571428571428571\n\nDIFF:\n@@ -0,0 +1,507 @@\n+\"\"\"\n+Title: Pretraining BERT with Hugging Face Transformers\n+Author: Sreyan Ghosh\n+Date created: 2022/07/01\n+Last modified: 2022/08/27\n+Description: Pretraining BERT using Hugging Face Transformers on NSP and MLM.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\"\"\"\n+\n+\"\"\"\n+### BERT (Bidirectional Encoder Representations from Transformers)\n+\n+In the field of computer vision, researchers have repeatedly shown the value of\n+transfer learning â€” pretraining a neural network model on a known task/dataset, for\n+instance ImageNet classification, and then performing fine-tuning â€” using the trained neural\n+network as the basis of a new specific-purpose model. In recent years, researchers\n+have shown that a similar technique can be useful in many natural language tasks.\n+\n+BERT makes use of Transformer, an attention mechanism that learns contextual relations\n+between words (or subwords) in a text. In its vanilla form, Transformer includes two\n+separate mechanisms â€” an encoder that reads the text input and a decoder that produces\n+a prediction for the task. Since BERTâ€™s goal is to generate a language model, only the\n+encoder mechanism is necessary. The detailed workings of Transformer are described in\n+a paper by Google.\n+\n+As opposed to directional models, which read the text input sequentially\n+(left-to-right or right-to-left), the Transformer encoder reads the entire\n+sequence of words at once. Therefore it is considered bidirectional, though\n+it would be more accurate to say that itâ€™s non-directional. This characteristic\n+allows the model to learn the context of a word based on all of its surroundings\n+(left and right of the word).\n+\n+When training language models, a challenge is defining a prediction goal.\n+Many models predict the next word in a sequence (e.g. `\"The child came home from _\"`),\n+a directional approach which inherently limits context learning. To overcome this\n+challenge, BERT uses two training strategies:\n+\n+### Masked Language Modeling (MLM)\n+\n+Before feeding word sequences into BERT, 15% of the words in each sequence are replaced\n+with a `[MASK]` token. The model then attempts to predict the original value of the masked\n+words, based on the context provided by the other, non-masked, words in the sequence.\n+\n+### Next Sentence Prediction (NSP)\n+\n+In the BERT training process, the model receives pairs of sentences as input and learns to\n+predict if the second sentence in the pair is the subsequent sentence in the original\n+document. During training, 50% of the inputs are a pair in which the second sentence is the\n+subsequent sentence in the original document, while in the other 50% a random sentence\n+from the corpus is chosen as the second sentence. The assumption is that the random sentence\n+will represent a disconnect from the first sentence.\n+\n+Though Google provides a pretrained BERT checkpoint for English, you may often need\n+to either pretrain the model from scratch for a different language, or do a\n+continued-pretraining to fit the model to a new domain. In this notebook, we pretrain\n+BERT from scratch optimizing both MLM and NSP objectves using ðŸ¤— Transformers on the `WikiText`\n+English dataset loaded from ðŸ¤— Datasets.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+\"\"\"\n+### Installing the requirements\n+\"\"\"\n+\n+\"\"\"shell\n+pip install git+https://github.com/huggingface/transformers.git\n+pip install datasets\n+pip install huggingface-hub\n+pip install nltk\n+\"\"\"\n+\n+\"\"\"\n+### Importing the necessary libraries\n+\"\"\"\n+\n+import nltk\n+import random\n+import logging\n+\n+import keras_core as keras\n+\n+nltk.download(\"punkt\")\n+# Set random seed\n+keras.utils.set_random_seed(42)\n+\n+\"\"\"\n+### Define certain variables\n+\"\"\"\n+\n+TOKENIZER_BATCH_SIZE = 256  # Batch-size to train the tokenizer on\n+TOKENIZER_VOCABULARY = 25000  # Total number of unique subwords the tokenizer can have\n+\n+BLOCK_SIZE = 128  # Maximum number of tokens in an input sample\n+NSP_PROB = 0.50  # Probability that the next sentence is the actual next sentence in NSP\n+SHORT_SEQ_PROB = 0.1  # Probability of generating shorter sequences to minimize the mismatch between pretraining and fine-tuning.\n+MAX_LENGTH = 512  # Maximum number of tokens in an input sample after padding\n+\n+MLM_PROB = 0.2  # Probability with which tokens are masked in MLM\n+\n+TRAIN_BATCH_SIZE = 2  # Batch-size for pretraining the model on\n+MAX_EPOCHS = 1  # Maximum number of epochs to train the model for\n+LEARNING_RATE = 1e-4  # Learning rate for training the model\n+\n+MODEL_CHECKPOINT = \"bert-base-cased\"  # Name of pretrained model from ðŸ¤— Model Hub\n+\n+\"\"\"\n+## Load the WikiText dataset\n+\"\"\"\n+\n+\"\"\"\n+We now download the `WikiText` language modeling dataset. It is a collection of over\n+100 million tokens extracted from the set of verified \"Good\" and \"Featured\" articles on\n+Wikipedia.\n+\n+We load the dataset from [ðŸ¤— Datasets](https://github.com/huggingface/datasets).\n+For the purpose of demonstration in this notebook, we work with only the `train`\n+split of the dataset. This can be easily done with the `load_dataset` function.\n+\"\"\"\n+\n+from datasets import load_dataset\n+\n+dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n+\n+\"\"\"\n+The dataset just has one column which is the raw text, and this is all we need for\n+pretraining BERT!\n+\"\"\"\n+\n+print(dataset)\n+\n+\"\"\"\n+## Training a new Tokenizer\n+\"\"\"\n+\n+\"\"\"\n+First we train our own tokenizer from scratch on our corpus, so that can we\n+can use it to train our language model from scratch.\n+\n+But why would you need to train a tokenizer? That's because Transformer models very\n+often use subword tokenization algorithms, and they need to be trained to identify the\n+parts of words that are often present in the corpus you are using.\n+\n+The ðŸ¤— Transformers `Tokenizer` (as the name indicates) will tokenize the inputs\n+(including converting the tokens to their corresponding IDs in the pretrained vocabulary)\n+and put it in a format the model expects, as well as generate the other inputs that model\n+requires.\n+\n+First we make a list of all the raw documents from the `WikiText` corpus:\n+\"\"\"\n+\n+all_texts = [\n+    doc for doc in dataset[\"train\"][\"text\"] if len(doc) > 0 and not doc.startswith(\" =\")\n+]\n+\n+\"\"\"\n+Next we make a `batch_iterator` function that will aid us to train our tokenizer.\n+\"\"\"\n+\n+\n+def batch_iterator():\n+    for i in range(0, len(all_texts), TOKENIZER_BATCH_SIZE):\n+        yield all_texts[i : i + TOKENIZER_BATCH_SIZE]\n+\n+\n+\"\"\"\n+In this notebook, we train a tokenizer with the exact same algorithms and\n+parameters as an existing one. For instance, we train a new version of the\n+`BERT-CASED` tokenzier on `Wikitext-2` using the same tokenization algorithm.\n+\n+First we need to load the tokenizer we want to use as a model:\n+\"\"\"\n+\n+from transformers import AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n+\n+\"\"\"\n+Now we train our tokenizer using the entire `train` split of the `Wikitext-2`\n+dataset.\n+\"\"\"\n+\n+tokenizer = tokenizer.train_new_from_iterator(\n+    batch_iterator(), vocab_size=TOKENIZER_VOCABULARY\n+)\n+\n+\"\"\"\n+So now we our done training our new tokenizer! Next we move on to the data\n+pre-processing steps.\n+\"\"\"\n+\n+\"\"\"\n+## Data Pre-processing\n+\"\"\"\n+\n+\"\"\"\n+For the sake of demonstrating the workflow, in this notebook we only take\n+small subsets of the entire WikiText `train` and `test` splits.\n+\"\"\"\n+\n+dataset[\"train\"] = dataset[\"train\"].select([i for i in range(1000)])\n+dataset[\"validation\"] = dataset[\"validation\"].select([i for i in range(1000)])\n+\n+\"\"\"\n+Before we can feed those texts to our model, we need to pre-process them and get them\n+ready for the task. As mentioned earlier, the BERT pretraining task includes two tasks\n+in total, the `NSP` task and the `MLM` task. ðŸ¤— Transformers have an easy to implement\n+`collator` called the `DataCollatorForLanguageModeling`. However, we need to get the\n+data ready for `NSP` manually.\n+\n+Next we write a simple function called the `prepare_train_features` that helps us in\n+the pre-processing and is compatible with ðŸ¤— Datasets. To summarize, our pre-processing\n+function should:\n+\n+- Get the dataset ready for the NSP task by creating pairs of sentences (A,B), where B\n+either actually follows A, or B is randomly sampled from somewhere else in the corpus.\n+It should also generate a corresponding label for each pair, which is 1 if B actually\n+follows A and 0 if not.\n+- Tokenize the text dataset into it's corresponding token ids that will be used for\n+embedding look-up in BERT\n+- Create additional inputs for the model like `token_type_ids`, `attention_mask`, etc.\n+\"\"\"\n+\n+# We define the maximum number of tokens after tokenization that each training sample\n+# will have\n+max_num_tokens = BLOCK_SIZE - tokenizer.num_special_tokens_to_add(pair=True)\n+\n+\n+def prepare_train_features(examples):\n+    \"\"\"Function to prepare features for NSP task\n+\n+    Arguments:\n+      examples: A dictionary with 1 key (\"text\")\n+        text: List of raw documents (str)\n+    Returns:\n+      examples:  A dictionary with 4 keys\n+        input_ids: List of tokenized, concatnated, and batched\n+          sentences from the individual raw documents (int)\n+        token_type_ids: List of integers (0 or 1) corresponding\n+          to: 0 for senetence no. 1 and padding, 1 for sentence\n+          no. 2\n+        attention_mask: List of integers (0 or 1) corresponding\n+          to: 1 for non-padded tokens, 0 for padded\n+        next_sentence_label: List of integers (0 or 1) corresponding\n+          to: 1 if the second sentence actually follows the first,\n+          0 if the senetence is sampled from somewhere else in the corpus\n+    \"\"\"\n+\n+    # Remove un-wanted samples from the training set\n+    examples[\"document\"] = [\n+        d.strip() for d in examples[\"text\"] if len(d) > 0 and not d.startswith(\" =\")\n+    ]\n+    # Split the documents from the dataset into it's individual sentences\n+    examples[\"sentences\"] = [\n+        nltk.tokenize.sent_tokenize(document) for document in examples[\"document\"]\n+    ]\n+    # Convert the tokens into ids using the trained tokenizer\n+    examples[\"tokenized_sentences\"] = [\n+        [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent)) for sent in doc]\n+        for doc in examples[\"sentences\"]\n+    ]\n+\n+    # Define the outputs\n+    examples[\"input_ids\"] = []\n+    examples[\"token_type_ids\"] = []\n+    examples[\"attention_mask\"] = []\n+    examples[\"next_sentence_label\"] = []\n+\n+    for doc_index, document in enumerate(examples[\"tokenized_sentences\"]):\n+        current_chunk = []  # a buffer stored current working segments\n+        current_length = 0\n+        i = 0\n+\n+        # We *usually* want to fill up the entire sequence since we are padding\n+        # to `block_size` anyways, so short sequences are generally wasted\n+        # computation. However, we *sometimes*\n+        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n+        # sequences to minimize the mismatch between pretraining and fine-tuning.\n+        # The `target_seq_length` is just a rough target however, whereas\n+        # `block_size` is a hard limit.\n+        target_seq_length = max_num_tokens\n+\n+        if random.random() < SHORT_SEQ_PROB:\n+            target_seq_length = random.randint(2, max_num_tokens)\n+\n+        while i < len(document):\n+            segment = document[i]\n+            current_chunk.append(segment)\n+            current_length += len(segment)\n+            if i == len(document) - 1 or current_length >= target_seq_length:\n+                if current_chunk:\n+                    # `a_end` is how many segments from `current_chunk` go into the `A`\n+                    # (first) sentence.\n+                    a_end = 1\n+                    if len(current_chunk) >= 2:\n+                        a_end = random.randint(1, len(current_chunk) - 1)\n+\n+                    tokens_a = []\n+                    for j in range(a_end):\n+                        tokens_a.extend(current_chunk[j])\n+\n+                    tokens_b = []\n+\n+                    if len(current_chunk) == 1 or random.random() < NSP_PROB:\n+                        is_random_next = True\n+                        target_b_length = target_seq_length - len(tokens_a)\n+\n+                        # This should rarely go for more than one iteration for large\n+                        # corpora. However, just to be careful, we try to make sure that\n+                        # the random document is not the same as the document\n+                        # we're processing.\n+                        for _ in range(10):\n+                            random_document_index = random.randint(\n+                                0, len(examples[\"tokenized_sentences\"]) - 1\n+                            )\n+                            if random_document_index != doc_index:\n+                                break\n+\n+                        random_document = examples[\"tokenized_sentences\"][\n+                            random_document_index\n+                        ]\n+                        random_start = random.randint(0, len(random_document) - 1)\n+                        for j in range(random_start, len(random_document)):\n+                            tokens_b.extend(random_document[j])\n+                            if len(tokens_b) >= target_b_length:\n+                                break\n+                        # We didn't actually use these segments so we \"put them back\" so\n+                        # they don't go to waste.\n+                        num_unused_segments = len(current_chunk) - a_end\n+                        i -= num_unused_segments\n+                    else:\n+                        is_random_next = False\n+                        for j in range(a_end, len(current_chunk)):\n+                            tokens_b.extend(current_chunk[j])\n+\n+                    input_ids = tokenizer.build_inputs_with_special_tokens(\n+                        tokens_a, tokens_b\n+                    )\n+                    # add token type ids, 0 for sentence a, 1 for sentence b\n+                    token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n+                        tokens_a, tokens_b\n+                    )\n+\n+                    padded = tokenizer.pad(\n+                        {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids},\n+                        padding=\"max_length\",\n+                        max_length=MAX_LENGTH,\n+                    )\n+\n+                    examples[\"input_ids\"].append(padded[\"input_ids\"])\n+                    examples[\"token_type_ids\"].append(padded[\"token_type_ids\"])\n+                    examples[\"attention_mask\"].append(padded[\"attention_mask\"])\n+                    examples[\"next_sentence_label\"].append(1 if is_random_next else 0)\n+                    current_chunk = []\n+                    current_length = 0\n+            i += 1\n+\n+    # We delete all the un-necessary columns from our dataset\n+    del examples[\"document\"]\n+    del examples[\"sentences\"]\n+    del examples[\"text\"]\n+    del examples[\"tokenized_sentences\"]\n+\n+    return examples\n+\n+\n+tokenized_dataset = dataset.map(\n+    prepare_train_features,\n+    batched=True,\n+    remove_columns=[\"text\"],\n+    num_proc=1,\n+)\n+\n+\"\"\"\n+For MLM we are going to use the same preprocessing as before for our dataset with\n+one additional step: we randomly mask some tokens (by replacing them by [MASK])\n+and the labels will be adjusted to only include the masked tokens\n+(we don't have to predict the non-masked tokens). If you use a tokenizer you trained\n+yourself, make sure the [MASK] token is among the special tokens you passed during training!\n+\n+To get the data ready for MLM, we simply use the `collator` called the\n+`DataCollatorForLanguageModeling` provided by the ðŸ¤— Transformers library on our dataset\n+that is already ready for the NSP task. The `collator` expects certain parameters.\n+We use the default ones from the original BERT paper in this notebook. The\n+`return_tensors='tf'` ensures that we get `tf.Tensor` objects back.\n+\"\"\"\n+\n+from transformers import DataCollatorForLanguageModeling\n+\n+collater = DataCollatorForLanguageModeling(\n+    tokenizer=tokenizer, mlm=True, mlm_probability=MLM_PROB, return_tensors=\"tf\"\n+)\n+\n+\"\"\"\n+Next we define our training set with which we train our model. Again, ðŸ¤— Datasets\n+provides us with the `to_tf_dataset` method which will help us integrate our dataset with\n+the `collator` defined above. The method expects certain parameters:\n+\n+- **columns**: the columns which will serve as our independant variables\n+- **label_cols**: the columns which will serve as our labels or dependant variables\n+- **batch_size**: our batch size for training\n+- **shuffle**: whether we want to shuffle our training dataset\n+- **collate_fn**: our collator function\n+\"\"\"\n+\n+train = tokenized_dataset[\"train\"].to_tf_dataset(\n+    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n+    label_cols=[\"labels\", \"next_sentence_label\"],\n+    batch_size=TRAIN_BATCH_SIZE,\n+    shuffle=True,\n+    collate_fn=collater,\n+)\n+\n+validation = tokenized_dataset[\"validation\"].to_tf_dataset(\n+    columns=[\"input_ids\", \"token_type_ids\", \"attention_mask\"],\n+    label_cols=[\"labels\", \"next_sentence_label\"],\n+    batch_size=TRAIN_BATCH_SIZE,\n+    shuffle=True,\n+    collate_fn=collater,\n+)\n+\n+\"\"\"\n+## Defining the model\n+\"\"\"\n+\n+\"\"\"\n+To define our model, first we need to define a config which will help us define certain\n+parameters of our model architecture. This includes parameters like number of transformer\n+layers, number of attention heads, hidden dimension, etc. For this notebook, we try\n+to define the exact config defined in the original BERT paper.\n+\n+We can easily achieve this using the `BertConfig` class from the ðŸ¤— Transformers library.\n+The `from_pretrained()` method expects the name of a model. Here we define the simplest\n+model with which we also trained our model, i.e., `bert-base-cased`.\n+\"\"\"\n+\n+from transformers import BertConfig\n+\n+config = BertConfig.from_pretrained(MODEL_CHECKPOINT)\n+\n+\"\"\"\n+For defining our model we use the `TFBertForPreTraining` class from the ðŸ¤— Transformers\n+library. This class internally handles everything starting from defining our model, to\n+unpacking our inputs and calculating the loss. So we need not do anything ourselves except\n+defining the model with the correct `config` we want!\n+\"\"\"\n+\n+from transformers import TFBertForPreTraining\n+\n+model = TFBertForPreTraining(config)\n+\n+\"\"\"\n+Now we define our optimizer and compile the model. The loss calculation is handled\n+internally and so we need not worry about that!\n+\"\"\"\n+\n+from keras.optimizers import Adam\n+\n+model.compile(optimizer=Adam(learning_rate=LEARNING_RATE))\n+\n+\"\"\"\n+Finally all steps are done and now we can start training our model!\n+\"\"\"\n+\n+model.fit(train, validation_data=validation, epochs=MAX_EPOCHS)\n+\n+\"\"\"\n+Our model has now been trained! We suggest to please train the model on the complete\n+dataset for atleast 50 epochs for decent performance. The pretrained model now acts as\n+a language model and is meant to be fine-tuned on a downstream task. Thus it can now be\n+fine-tuned on any downstream task like Question Answering, Text Classification\n+etc.!\n+\"\"\"\n+\n+\"\"\"\n+Now you can push this model to ðŸ¤— Model Hub and also share it with with all your friends,\n+family, favorite pets: they can all load it with the identifier\n+`\"your-username/the-name-you-picked\"` so for instance:\n+\n+```python\n+model.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n+tokenizer.push_to_hub(\"pretrained-bert\", organization=\"keras-io\")\n+```\n+And after you push your model this is how you can load it in the future!\n+\n+```python\n+from transformers import TFBertForPreTraining\n+\n+model = TFBertForPreTraining.from_pretrained(\"your-username/my-awesome-model\")\n+```\n+or, since it's a pretrained model and you would generally use it for fine-tuning\n+on a downstream task, you can also load it for some other task like:\n+\n+```python\n+from transformers import TFBertForSequenceClassification\n+\n+model = TFBertForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")\n+```\n+In this case, the pretraining head will be dropped and the model will just be initialized\n+with the transformer layers. A new task-specific head will be added with random weights.\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe212a4c6e8c18c806d33ccd25a903759bb6b8a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Î” (Sum/Max): 1/1 | Churn Î”: 33 | Churn Cumulative: 817 | Contributors (this commit): 8 | Commits (past 90d): 15 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,7 +170,10 @@ def apply_mask(sample_weight, mask, dtype, reduction):\n             #   = sum(loss * sample_weight * total / valid) / total\n             #   = sum(loss * sample_weight) / total * total / valid\n             #   = sum(loss * sample_weight) / valid\n-            total = ops.cast(ops.shape(mask)[0], dtype=dtype)\n+            total = ops.cast(\n+                ops.prod(ops.convert_to_tensor(ops.shape(mask), dtype=\"int32\")),\n+                dtype,\n+            )\n             valid = ops.sum(mask)  # May be 0!\n             mask *= total / (valid + backend.epsilon())\n \n\n@@ -117,6 +117,34 @@ class LossTest(testing.TestCase):\n             loss,\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy backend does not support masking.\",\n+    )\n+    def test_mask_and_sample_weight_rank2(self):\n+        # check loss of inputs with duplicate rows doesn't change\n+        sample_weight = np.array([0.4, 0.3, 0.2, 0.1])\n+        y_true = np.array([1.0, 0.0, 1.0, 0.0])\n+        y_pred = np.array([0.1, 0.2, 0.3, 0.4])\n+        mask = np.array([True, False, True, True])\n+\n+        mask = ops.convert_to_tensor(mask)\n+        y_true = ops.convert_to_tensor(y_true)\n+        y_pred = ops.convert_to_tensor(y_pred)\n+        y_pred._keras_mask = mask\n+\n+        loss_fn = ExampleLoss()\n+        rank1_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # duplicate rows\n+        mask = ops.tile(ops.expand_dims(mask, axis=0), (2, 1))\n+        y_true = ops.tile(ops.expand_dims(y_true, axis=0), (2, 1))\n+        y_pred = ops.tile(ops.expand_dims(y_pred, axis=0), (2, 1))\n+        sample_weight = ops.tile(ops.expand_dims(sample_weight, axis=0), (2, 1))\n+        y_pred._keras_mask = mask\n+        rank2_loss = loss_fn(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(rank1_loss, rank2_loss)\n+\n     # @testing.parametrize(\n     #     \"uprank\", [\"mask\", \"sample_weight\", \"y_true\", \"y_pred\"])\n     # TODO: use parameterization decorator\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0ca55af5114b7885faf864737fb932b6eab8a46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 138 | Lines Deleted: 63 | Files Changed: 2 | Hunks: 28 | Methods Changed: 12 | Complexity Î” (Sum/Max): 12/7 | Churn Î”: 201 | Churn Cumulative: 687 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,13 +1,23 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import layers\n from keras_core import testing\n \n \n class CenterCropTest(testing.TestCase, parameterized.TestCase):\n+    def np_center_crop(self, img, h_new, w_new):\n+        img = np.array(img)\n+        if img.ndim == 4:\n+            _, h, w = img.shape[:3]\n+        else:\n+            h, w = img.shape[:2]\n+        h_start = (h - h_new) // 2\n+        w_start = (w - w_new) // 2\n+        return img[..., h_start : h_start + h_new, w_start : w_start + w_new, :]\n+\n     @pytest.mark.requires_trainable_backend\n     def test_center_crop_basics(self):\n         self.run_layer_test(\n@@ -45,8 +55,8 @@ class CenterCropTest(testing.TestCase, parameterized.TestCase):\n         [\n             ((5, 7), \"channels_first\"),\n             ((5, 7), \"channels_last\"),\n-            ((15, 10), \"channels_first\"),\n-            ((10, 17), \"channels_last\"),\n+            ((4, 9), \"channels_first\"),\n+            ((9, 4), \"channels_last\"),\n         ]\n     )\n     def test_center_crop_correctness(self, size, data_format):\n@@ -63,12 +73,12 @@ class CenterCropTest(testing.TestCase, parameterized.TestCase):\n         if data_format == \"channels_first\":\n             img_transpose = np.transpose(img, (0, 2, 3, 1))\n \n-            ref_out = tf.transpose(\n-                tf.keras.layers.CenterCrop(size[0], size[1])(img_transpose),\n+            ref_out = np.transpose(\n+                self.np_center_crop(img_transpose, size[0], size[1]),\n                 (0, 3, 1, 2),\n             )\n         else:\n-            ref_out = tf.keras.layers.CenterCrop(size[0], size[1])(img)\n+            ref_out = self.np_center_crop(img, size[0], size[1])\n         self.assertAllClose(ref_out, out)\n \n         # unbatched case\n@@ -83,24 +93,64 @@ class CenterCropTest(testing.TestCase, parameterized.TestCase):\n         )(img)\n         if data_format == \"channels_first\":\n             img_transpose = np.transpose(img, (1, 2, 0))\n-            ref_out = tf.transpose(\n-                tf.keras.layers.CenterCrop(\n+            ref_out = np.transpose(\n+                self.np_center_crop(\n+                    img_transpose,\n                     size[0],\n                     size[1],\n-                )(img_transpose),\n+                ),\n                 (2, 0, 1),\n             )\n         else:\n-            ref_out = tf.keras.layers.CenterCrop(\n+            ref_out = self.np_center_crop(\n+                img,\n                 size[0],\n                 size[1],\n+            )\n+        self.assertAllClose(ref_out, out)\n+\n+    @parameterized.parameters(\n+        [\n+            ((15, 10), \"channels_first\"),\n+            ((10, 17), \"channels_last\"),\n+        ]\n+    )\n+    def test_input_smaller_than_crop_box(self, size, data_format):\n+        \"\"\"Output should equal resizing with crop_to_aspect ratio.\"\"\"\n+        # batched case\n+        if data_format == \"channels_first\":\n+            img = np.random.random((2, 3, 9, 11))\n+        else:\n+            img = np.random.random((2, 9, 11, 3))\n+        out = layers.CenterCrop(\n+            size[0],\n+            size[1],\n+            data_format=data_format,\n+        )(img)\n+        ref_out = layers.Resizing(\n+            size[0], size[1], data_format=data_format, crop_to_aspect_ratio=True\n+        )(img)\n+        self.assertAllClose(ref_out, out)\n+\n+        # unbatched case\n+        if data_format == \"channels_first\":\n+            img = np.random.random((3, 9, 11))\n+        else:\n+            img = np.random.random((9, 11, 3))\n+        out = layers.CenterCrop(\n+            size[0],\n+            size[1],\n+            data_format=data_format,\n+        )(img)\n+        ref_out = layers.Resizing(\n+            size[0], size[1], data_format=data_format, crop_to_aspect_ratio=True\n         )(img)\n         self.assertAllClose(ref_out, out)\n \n     def test_tf_data_compatibility(self):\n         layer = layers.CenterCrop(8, 9)\n         input_data = np.random.random((2, 10, 12, 3))\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertEqual(list(output.shape), [2, 8, 9, 3])\n@@ -111,6 +161,6 @@ class CenterCropTest(testing.TestCase, parameterized.TestCase):\n             np.random.rand(10, 10, 3),\n         ]\n         output = layers.CenterCrop(height=6, width=5)(images)\n-        ref_output = tf.keras.layers.CenterCrop(6, 5)(images)\n+        ref_output = self.np_center_crop(images, 6, 5)\n         self.assertListEqual(list(output.shape), [2, 6, 5, 3])\n         self.assertAllClose(ref_output, output)\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from tensorflow import data as tf_data\n \n from keras_core import Sequential\n from keras_core import backend\n@@ -89,72 +89,97 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n             run_training_check=False,\n         )\n \n-    @parameterized.parameters(\n-        [\n-            ((5, 7), \"channels_first\", True),\n-            ((5, 7), \"channels_last\", True),\n-            ((6, 8), \"channels_first\", False),\n-            ((6, 8), \"channels_last\", False),\n-        ]\n-    )\n-    def test_resizing_correctness(\n-        self, size, data_format, crop_to_aspect_ratio\n-    ):\n-        # batched case\n+    @parameterized.parameters([(\"channels_first\",), (\"channels_last\",)])\n+    def test_down_sampling_numeric(self, data_format):\n+        img = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(np.float32)\n         if data_format == \"channels_first\":\n-            img = np.random.random((2, 3, 9, 11))\n-        else:\n-            img = np.random.random((2, 9, 11, 3))\n+            img = img.transpose(0, 3, 1, 2)\n         out = layers.Resizing(\n-            size[0],\n-            size[1],\n-            data_format=data_format,\n-            crop_to_aspect_ratio=crop_to_aspect_ratio,\n+            height=2, width=2, interpolation=\"nearest\", data_format=data_format\n         )(img)\n-        if data_format == \"channels_first\":\n-            img_transpose = np.transpose(img, (0, 2, 3, 1))\n-\n-            ref_out = tf.transpose(\n-                tf.keras.layers.Resizing(\n-                    size[0], size[1], crop_to_aspect_ratio=crop_to_aspect_ratio\n-                )(img_transpose),\n-                (0, 3, 1, 2),\n+        ref_out = (\n+            np.asarray([[5, 7], [13, 15]])\n+            .astype(np.float32)\n+            .reshape((1, 2, 2, 1))\n         )\n-        else:\n-            ref_out = tf.keras.layers.Resizing(\n-                size[0], size[1], crop_to_aspect_ratio=crop_to_aspect_ratio\n-            )(img)\n+        if data_format == \"channels_first\":\n+            ref_out = ref_out.transpose(0, 3, 1, 2)\n         self.assertAllClose(ref_out, out)\n \n-        # unbatched case\n+    @parameterized.parameters([(\"channels_first\",), (\"channels_last\",)])\n+    def test_up_sampling_numeric(self, data_format):\n+        img = np.reshape(np.arange(0, 4), (1, 2, 2, 1)).astype(np.float32)\n         if data_format == \"channels_first\":\n-            img = np.random.random((3, 9, 11))\n-        else:\n-            img = np.random.random((9, 11, 3))\n+            img = img.transpose(0, 3, 1, 2)\n         out = layers.Resizing(\n-            size[0],\n-            size[1],\n+            height=4,\n+            width=4,\n+            interpolation=\"nearest\",\n             data_format=data_format,\n-            crop_to_aspect_ratio=crop_to_aspect_ratio,\n         )(img)\n-        if data_format == \"channels_first\":\n-            img_transpose = np.transpose(img, (1, 2, 0))\n-            ref_out = tf.transpose(\n-                tf.keras.layers.Resizing(\n-                    size[0], size[1], crop_to_aspect_ratio=crop_to_aspect_ratio\n-                )(img_transpose),\n-                (2, 0, 1),\n+        ref_out = (\n+            np.asarray([[0, 0, 1, 1], [0, 0, 1, 1], [2, 2, 3, 3], [2, 2, 3, 3]])\n+            .astype(np.float32)\n+            .reshape((1, 4, 4, 1))\n         )\n-        else:\n-            ref_out = tf.keras.layers.Resizing(\n-                size[0], size[1], crop_to_aspect_ratio=crop_to_aspect_ratio\n+        if data_format == \"channels_first\":\n+            ref_out = ref_out.transpose(0, 3, 1, 2)\n+        self.assertAllClose(ref_out, out)\n+\n+    @parameterized.parameters([(\"channels_first\",), (\"channels_last\",)])\n+    def test_crop_to_aspect_ratio(self, data_format):\n+        img = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(\"float32\")\n+        if data_format == \"channels_first\":\n+            img = img.transpose(0, 3, 1, 2)\n+        out = layers.Resizing(\n+            height=4,\n+            width=2,\n+            interpolation=\"nearest\",\n+            data_format=data_format,\n+            crop_to_aspect_ratio=True,\n         )(img)\n+        ref_out = (\n+            np.asarray(\n+                [\n+                    [1, 2],\n+                    [5, 6],\n+                    [9, 10],\n+                    [13, 14],\n+                ]\n+            )\n+            .astype(\"float32\")\n+            .reshape((1, 4, 2, 1))\n+        )\n+        if data_format == \"channels_first\":\n+            ref_out = ref_out.transpose(0, 3, 1, 2)\n+        self.assertAllClose(ref_out, out)\n+\n+    @parameterized.parameters([(\"channels_first\",), (\"channels_last\",)])\n+    def test_unbatched_image(self, data_format):\n+        img = np.reshape(np.arange(0, 16), (4, 4, 1)).astype(\"float32\")\n+        if data_format == \"channels_first\":\n+            img = img.transpose(2, 0, 1)\n+        out = layers.Resizing(\n+            2, 2, interpolation=\"nearest\", data_format=data_format\n+        )(img)\n+        ref_out = (\n+            np.asarray(\n+                [\n+                    [5, 7],\n+                    [13, 15],\n+                ]\n+            )\n+            .astype(\"float32\")\n+            .reshape((2, 2, 1))\n+        )\n+        if data_format == \"channels_first\":\n+            ref_out = ref_out.transpose(2, 0, 1)\n         self.assertAllClose(ref_out, out)\n \n     def test_tf_data_compatibility(self):\n         layer = layers.Resizing(8, 9)\n         input_data = np.random.random((2, 10, 12, 3))\n-        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertEqual(list(output.shape), [2, 8, 9, 3])\n@@ -169,7 +194,7 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n         layer = layers.Resizing(8, 9)\n         input_data = np.random.random((2, 10, 12, 3))\n         ds = (\n-            tf.data.Dataset.from_tensor_slices(input_data)\n+            tf_data.Dataset.from_tensor_slices(input_data)\n             .batch(2)\n             .map(Sequential([layer]))\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87e71284b592c4d4b226e89ad082a96c0140f06b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 56 | Files Changed: 2 | Hunks: 17 | Methods Changed: 5 | Complexity Î” (Sum/Max): 1/1 | Churn Î”: 109 | Churn Cumulative: 3838 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -493,46 +493,38 @@ class PrecisionTest(testing.TestCase):\n         expected_precision = tp / predicted_positives\n         self.assertAlmostEqual(expected_precision, result)\n \n-    def test_unweighted_class_id(self):\n+    def test_unweighted_class_id_should_throw_error_1d(self):\n         p_obj = metrics.Precision(class_id=2)\n \n         y_pred = np.array([0.2, 0.1, 0.6, 0, 0.2])\n         y_true = np.array([0, 1, 1, 0, 0])\n-        result = p_obj(y_true, y_pred)\n-        self.assertAlmostEqual(1, result)\n-        self.assertAlmostEqual(1, p_obj.true_positives)\n-        self.assertAlmostEqual(0, p_obj.false_positives)\n \n-        y_pred = np.array([0.2, 0.1, 0, 0, 0.2])\n-        y_true = np.array([0, 1, 1, 0, 0])\n-        result = p_obj(y_true, y_pred)\n-        self.assertAlmostEqual(1, result)\n-        self.assertAlmostEqual(1, p_obj.true_positives)\n-        self.assertAlmostEqual(0, p_obj.false_positives)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"When class_id is provided, y_pred must be a 2D array \"\n+            r\"with shape \\(num_samples, num_classes\\), found shape:.*\"\n+        ):\n+            p_obj(y_true, y_pred)\n \n-        y_pred = np.array([0.2, 0.1, 0.6, 0, 0.2])\n-        y_true = np.array([0, 1, 0, 0, 0])\n-        result = p_obj(y_true, y_pred)\n-        self.assertAlmostEqual(0.5, result)\n-        self.assertAlmostEqual(1, p_obj.true_positives)\n-        self.assertAlmostEqual(1, p_obj.false_positives)\n+    def test_unweighted_class_id_multiclass(self):\n+        p_obj = metrics.Precision(class_id=1)\n \n-    def test_unweighted_top_k_and_class_id(self):\n-        p_obj = metrics.Precision(class_id=2, top_k=2)\n+        y_pred = np.array([[0.1, 0.2, 0.7],\n+                           [0.5, 0.3, 0.2],\n+                           [0.2, 0.6, 0.2],\n+                           [0.7, 0.2, 0.1],\n+                           [0.1, 0.1, 0.8]])\n \n-        y_pred = np.array([0.2, 0.6, 0.3, 0, 0.2])\n-        y_true = np.array([0, 1, 1, 0, 0])\n-        result = p_obj(y_true, y_pred)\n-        self.assertAlmostEqual(1, result)\n-        self.assertAlmostEqual(1, p_obj.true_positives)\n-        self.assertAlmostEqual(0, p_obj.false_positives)\n+        y_true = np.array([[0., 0., 1.],\n+                           [1., 0., 0.],\n+                           [0., 1., 0.],\n+                           [1., 0., 0.],\n+                           [0., 0., 1.]])\n \n-        y_pred = np.array([1, 1, 0.9, 1, 1])\n-        y_true = np.array([0, 1, 1, 0, 0])\n         result = p_obj(y_true, y_pred)\n-        self.assertAlmostEqual(1, result)\n-        self.assertAlmostEqual(1, p_obj.true_positives)\n-        self.assertAlmostEqual(0, p_obj.false_positives)\n+        self.assertAlmostEqual(1.0, result)\n+        self.assertAlmostEqual(1.0, p_obj.true_positives)\n+        self.assertAlmostEqual(0.0, p_obj.false_positives)\n \n     def test_unweighted_top_k_and_threshold(self):\n         p_obj = metrics.Precision(thresholds=0.7, top_k=2)\n@@ -654,41 +646,38 @@ class RecallTest(testing.TestCase):\n         expected_recall = tp / positives\n         self.assertAlmostEqual(expected_recall, result)\n \n-    def test_unweighted_class_id(self):\n+    def test_unweighted_class_id_should_throw_error_1d(self):\n         r_obj = metrics.Recall(class_id=2)\n \n         y_pred = np.array([0.2, 0.1, 0.6, 0, 0.2])\n         y_true = np.array([0, 1, 1, 0, 0])\n-        self.assertAlmostEqual(1, r_obj(y_true, y_pred))\n-        self.assertAlmostEqual(1, r_obj.true_positives)\n-        self.assertAlmostEqual(0, r_obj.false_negatives)\n \n-        y_pred = np.array([0.2, 0.1, 0, 0, 0.2])\n-        y_true = np.array([0, 1, 1, 0, 0])\n-        self.assertAlmostEqual(0.5, r_obj(y_true, y_pred))\n-        self.assertAlmostEqual(1, r_obj.true_positives)\n-        self.assertAlmostEqual(1, r_obj.false_negatives)\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"When class_id is provided, y_pred must be a 2D array \"\n+            r\"with shape \\(num_samples, num_classes\\), found shape:.*\"\n+        ):\n+            r_obj(y_true, y_pred)\n \n-        y_pred = np.array([0.2, 0.1, 0.6, 0, 0.2])\n-        y_true = np.array([0, 1, 0, 0, 0])\n-        self.assertAlmostEqual(0.5, r_obj(y_true, y_pred))\n-        self.assertAlmostEqual(1, r_obj.true_positives)\n-        self.assertAlmostEqual(1, r_obj.false_negatives)\n+    def test_unweighted_class_id_multiclass(self):\n+        r_obj = metrics.Recall(class_id=1)\n \n-    def test_unweighted_top_k_and_class_id(self):\n-        r_obj = metrics.Recall(class_id=2, top_k=2)\n+        y_pred = np.array([[0.1, 0.2, 0.7],\n+                           [0.5, 0.3, 0.2],\n+                           [0.2, 0.6, 0.2],\n+                           [0.7, 0.2, 0.1],\n+                           [0.1, 0.1, 0.8]])\n \n-        y_pred = np.array([0.2, 0.6, 0.3, 0, 0.2])\n-        y_true = np.array([0, 1, 1, 0, 0])\n-        self.assertAlmostEqual(1, r_obj(y_true, y_pred))\n-        self.assertAlmostEqual(1, r_obj.true_positives)\n-        self.assertAlmostEqual(0, r_obj.false_negatives)\n+        y_true = np.array([[0., 0., 1.],\n+                           [1., 0., 0.],\n+                           [0., 1., 0.],\n+                           [1., 0., 0.],\n+                           [0., 0., 1.]])\n \n-        y_pred = np.array([1, 1, 0.9, 1, 1])\n-        y_true = np.array([0, 1, 1, 0, 0])\n-        self.assertAlmostEqual(0.5, r_obj(y_true, y_pred))\n-        self.assertAlmostEqual(1, r_obj.true_positives)\n-        self.assertAlmostEqual(1, r_obj.false_negatives)\n+        result = r_obj(y_true, y_pred)\n+        self.assertAlmostEqual(1.0, result)\n+        self.assertAlmostEqual(1.0, r_obj.true_positives)\n+        self.assertAlmostEqual(0.0, r_obj.false_negatives)\n \n     def test_unweighted_top_k_and_threshold(self):\n         r_obj = metrics.Recall(thresholds=0.7, top_k=2)\n\n@@ -462,7 +462,15 @@ def update_confusion_matrix_variables(\n \n     if top_k is not None:\n         y_pred = _filter_top_k(y_pred, top_k)\n+\n     if class_id is not None:\n+        if len(y_pred.shape) == 1:\n+            raise ValueError(\n+                \"When class_id is provided, y_pred must be a 2D array \"\n+                \"with shape (num_samples, num_classes), found shape: \"\n+                f\"{y_pred.shape}\"\n+            )\n+\n         # Preserve dimension to match with sample_weight\n         y_true = y_true[..., class_id, None]\n         y_pred = y_pred[..., class_id, None]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#339b45e626df266d88d597a6c14cbf79b0097bdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 2 | Churn Cumulative: 6004 | Contributors (this commit): 26 | Commits (past 90d): 80 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -436,7 +436,6 @@ class Layer(BackendLayer, Operation):\n             name: String name of the variable. Useful\n                 for debugging purposes.\n         \"\"\"\n-        # TODO: handle layout\n         self._check_super_called()\n         initializer = initializers.get(initializer)\n         with backend.name_scope(self.name, caller=self):\n\n@@ -751,6 +751,7 @@ class Arccosh(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n+@keras_core_export([\"keras_core.ops.arccosh\", \"keras_core.ops.numpy.arccosh\"])\n def arccosh(x):\n     \"\"\"Inverse hyperbolic cosine, element-wise.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2305a7916a5ed28d6310aecffbe3a2e702e20c85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d42700a528b6b8f515017d23b6840e41d5ea1a32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Î” (Sum/Max): 0/0 | Churn Î”: 4 | Churn Cumulative: 64 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -31,10 +31,6 @@ class DropoutTest(testing.TestCase):\n         self.assertAllClose(np.mean(outputs), 1.0, atol=0.02)\n         self.assertAllClose(np.max(outputs), 2.0)\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"jax\",\n-        reason=\"JAX does not support dynamic shapes\",\n-    )\n     def test_dropout_partial_noise_shape_dynamic(self):\n         inputs = np.ones((20, 5, 10))\n         layer = layers.Dropout(0.5, noise_shape=(None, 1, None))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
