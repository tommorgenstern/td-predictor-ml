{"custom_id": "keras#bd514cab64cb6623caba4d56b96eaf01273aa885", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 588 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n \"\"\"Commonly-used neural network operations not included in NumPy.\"\"\"\n \n+import warnings\n+\n from keras import backend\n from keras.api_export import keras_export\n from keras.backend import KerasTensor\n@@ -536,6 +538,14 @@ def softmax(x, axis=-1):\n     array([0.09003057, 0.24472847, 0.66524096], shape=(3,), dtype=float64)\n \n     \"\"\"\n+    if isinstance(axis, int) and backend.shape(x)[axis] == 1:\n+        warnings.warn(\n+            f\"You are using a softmax over axis {axis} \"\n+            f\"of a tensor of shape {backend.shape(x)}. This axis \"\n+            \"has size 1. The softmax operation will always return \"\n+            \"the value 1, which is likely not what you intended. \"\n+            \"Did you mean to use a sigmoid instead?\"\n+        )\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n     if isinstance(axis, tuple):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9adb76cd417451acca3c4697af1c1c02c6f491f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 3075 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: 0.5454545454545454\n\nDIFF:\n@@ -104,7 +104,7 @@ class BaseConv(Layer):\n         )\n         self.rank = rank\n         self.filters = filters\n-        self.groups = groups or 1\n+        self.groups = groups\n         self.kernel_size = standardize_tuple(kernel_size, rank, \"kernel_size\")\n         self.strides = standardize_tuple(strides, rank, \"strides\")\n         self.dilation_rate = standardize_tuple(\n@@ -129,6 +129,12 @@ class BaseConv(Layer):\n                 f\"positive value. Received filters={self.filters}.\"\n             )\n \n+        if self.groups <= 0:\n+            raise ValueError(\n+                \"The number of groups must be a positive integer. \"\n+                f\"Received: groups={self.groups}.\"\n+            )\n+\n         if self.filters is not None and self.filters % self.groups != 0:\n             raise ValueError(\n                 \"The number of filters must be evenly divisible by the \"\n\n@@ -503,6 +503,14 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n \n+        # `groups` is not strictly positive.\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The number of groups must be a positive integer. \"\n+            \"Received: groups=0.\",\n+        ):\n+            layers.Conv2D(filters=5, kernel_size=(2, 2), groups=0)\n+\n         # `filters` cannot be divided by `groups`.\n         with self.assertRaises(ValueError):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce7cdcef0dc14fccf3dccf46342484b8a742a55e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 656 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -200,12 +200,12 @@ def initialize(job_addresses, num_processes, process_id):\n                 f\"{len(job_addresses)} jobs, but num_processes is \"\n                 f\"{num_processes}\"\n             )\n-        corrdinator_address = job_addresses[0]\n+        coordinator_address = job_addresses[0]\n     else:\n-        corrdinator_address = job_addresses\n+        coordinator_address = job_addresses\n \n     jax.distributed.initialize(\n-        corrdinator_address=corrdinator_address,\n+        coordinator_address=coordinator_address,\n         num_processes=num_processes,\n         process_id=process_id,\n     )\n\n@@ -50,7 +50,7 @@ class JaxDistributionLibTest(testing.TestCase):\n     def test_initialize_with_all_job_addresses(self, mock_jax_initialze):\n         backend_dlib.initialize(\"10.0.0.1:1234,10.0.0.2:2345\", 2, 0)\n         mock_jax_initialze.assert_called_once_with(\n-            corrdinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n+            coordinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n         )\n \n     def test_initialize_validate_job_and_process(self):\n@@ -63,7 +63,7 @@ class JaxDistributionLibTest(testing.TestCase):\n     def test_initialize_with_coordinater_address(self, mock_jax_initialze):\n         backend_dlib.initialize(\"10.0.0.1:1234\", 2, 0)\n         mock_jax_initialze.assert_called_once_with(\n-            corrdinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n+            coordinator_address=\"10.0.0.1:1234\", num_processes=2, process_id=0\n         )\n \n     def test_distribute_tensor(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22a4ea757b247d9c090e6b30cec41ab5975a73f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 18 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 122 | Churn Cumulative: 4038 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -486,19 +486,38 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.Conv1D(filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \\d+ \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2D(filters=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} that \"\n+            r\"do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2D(filters=2, kernel_size=(2, 2), strides=(1, 0))\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.Conv2D(\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n@@ -512,7 +531,11 @@ class ConvBasicTest(testing.TestCase, parameterized.TestCase):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=0)\n \n         # `filters` cannot be divided by `groups`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The number of filters must be evenly divisible by the\"\n+            \" number of groups. Received: groups=2, filters=5.\",\n+        ):\n             layers.Conv2D(filters=5, kernel_size=(2, 2), groups=2)\n \n \n\n@@ -507,21 +507,40 @@ class ConvTransposeBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.Conv1DTranspose(filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2DTranspose(filters=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.Conv2DTranspose(\n                 filters=2, kernel_size=(2, 2), strides=(1, 0)\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.Conv2DTranspose(\n                 filters=2, kernel_size=(2, 2), strides=2, dilation_rate=(2, 1)\n             )\n\n@@ -293,21 +293,41 @@ class DepthwiseConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `depth_multiplier` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n             layers.DepthwiseConv1D(depth_multiplier=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of 2 \"\n+            r\"integers. Received kernel_size=\\(1, 0\\), including values \"\n+            r\"\\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.DepthwiseConv2D(depth_multiplier=2, kernel_size=(1, 0))\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.DepthwiseConv2D(\n                 depth_multiplier=2, kernel_size=(2, 2), strides=(1, 0)\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.DepthwiseConv2D(\n                 depth_multiplier=2,\n                 kernel_size=(2, 2),\n\n@@ -147,21 +147,40 @@ class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n \n     def test_bad_init_args(self):\n         # `depth_multiplier` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `depth_multiplier`. \"\n+            \"Expected a strictly positive value. Received \"\n+            \"depth_multiplier=0.\",\n+        ):\n             layers.SeparableConv1D(depth_multiplier=0, filters=1, kernel_size=1)\n \n         # `filters` is not positive.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Invalid value for argument `filters`. Expected a \"\n+            \"strictly positive value. Received filters=0.\",\n+        ):\n             layers.SeparableConv1D(depth_multiplier=1, filters=0, kernel_size=1)\n \n         # `kernel_size` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `kernel_size` argument must be a tuple of \"\n+            r\"\\d+ integers. Received kernel_size=\\(1, 0\\), including values\"\n+            r\" \\{0\\} that do not satisfy `value > 0`\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2, filters=2, kernel_size=(1, 0)\n             )\n \n         # `strides` has 0.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"The `strides` argument must be a tuple of \\d+ \"\n+            r\"integers. Received strides=\\(1, 0\\), including values \\{0\\} \"\n+            r\"that do not satisfy `value > 0`\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2,\n                 filters=2,\n@@ -170,7 +189,12 @@ class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n             )\n \n         # `dilation_rate > 1` while `strides > 1`.\n-        with self.assertRaises(ValueError):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"`strides > 1` not supported in conjunction with \"\n+            r\"`dilation_rate > 1`. Received: strides=\\(2, 2\\) and \"\n+            r\"dilation_rate=\\(2, 1\\)\",\n+        ):\n             layers.SeparableConv2D(\n                 depth_multiplier=2,\n                 filters=2,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fffa00400f1e63008f0e1e08f7538a3fbd22e9ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 77 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -17,15 +17,16 @@ class InputSpec:\n     A `None` entry in a shape is compatible with any dimension.\n \n     Args:\n-        dtype: Expected DataType of the input.\n+        dtype: Expected dtype of the input.\n         shape: Shape tuple, expected shape of the input\n-            (may include None for unchecked axes). Includes the batch size.\n+            (may include `None` for dynamic axes).\n+            Includes the batch size.\n         ndim: Integer, expected rank of the input.\n         max_ndim: Integer, maximum rank of the input.\n         min_ndim: Integer, minimum rank of the input.\n         axes: Dictionary mapping integer axes to\n             a specific dimension value.\n-        allow_last_axis_squeeze: If True, then allow inputs of rank N+1 as long\n+        allow_last_axis_squeeze: If `True`, allow inputs of rank N+1 as long\n             as the last axis of the input is 1, as well as inputs of rank N-1\n             as long as the last axis of the spec is 1.\n         name: Expected key corresponding to this input when passing data as\n@@ -36,7 +37,7 @@ class InputSpec:\n     ```python\n     class MyLayer(Layer):\n         def __init__(self):\n-            super(MyLayer, self).__init__()\n+            super().__init__()\n             # The layer will accept inputs with\n             # shape (*, 28, 28) & (*, 28, 28, 1)\n             # and raise an appropriate error message otherwise.\n\n@@ -156,7 +156,7 @@ class Mean(Metric):\n \n @keras_export(\"keras.metrics.MeanMetricWrapper\")\n class MeanMetricWrapper(Mean):\n-    \"\"\"Wrap a stateless metric function with the Mean metric.\n+    \"\"\"Wrap a stateless metric function with the `Mean` metric.\n \n     You could use this class to quickly build a mean metric from a function. The\n     function needs to have the signature `fn(y_true, y_pred)` and return a\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a6ef707046036c2fd81b497d1b480ed46983159", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 98 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -142,6 +142,8 @@ def shape(x):\n     tensor values when the shape is unknown (this is tf specific, as dynamic\n     shapes do not apply in other backends).\n     \"\"\"\n+    if isinstance(x, KerasTensor):\n+        return x.shape\n     if not tf.is_tensor(x):\n         x = tf.convert_to_tensor(x)\n     dynamic = tf.shape(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2ad117c44c5346691a646512ded3d25a5e3cb322", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 14 | Files Changed: 8 | Hunks: 16 | Methods Changed: 18 | Complexity Δ (Sum/Max): 16/8 | Churn Δ: 101 | Churn Cumulative: 3643 | Contributors (this commit): 25 | Commits (past 90d): 46 | Contributors (cumulative): 56 | DMM Complexity: 0.9666666666666667\n\nDIFF:\n@@ -31,6 +31,8 @@ class KerasVariable:\n         self._dtype = dtype\n         self._shape = None\n         self._initializer = None\n+        self._regularizer = None\n+        self._constraint = None\n         self._trainable = trainable\n         if isinstance(initializer, str):\n             from keras import initializers\n@@ -175,6 +177,38 @@ class KerasVariable:\n     def trainable(self, value):\n         self._trainable = value\n \n+    @property\n+    def regularizer(self):\n+        return self._regularizer\n+\n+    @regularizer.setter\n+    def regularizer(self, value):\n+        from keras.regularizers import Regularizer\n+\n+        if value is not None and not isinstance(value, Regularizer):\n+            raise ValueError(\n+                \"Invalid value for attribute `regularizer`. Expected an \"\n+                \"instance of `keras.regularizers.Regularizer`, or `None`. \"\n+                f\"Received: regularizer={value}\"\n+            )\n+        self._regularizer = value\n+\n+    @property\n+    def constraint(self):\n+        return self._constraint\n+\n+    @constraint.setter\n+    def constraint(self, value):\n+        from keras.constraints import Constraint\n+\n+        if value is not None and not isinstance(value, Constraint):\n+            raise ValueError(\n+                \"Invalid value for attribute `constraint`. Expected an \"\n+                \"instance of `keras.constraints.Constraint`, or `None`. \"\n+                f\"Received: constraint={value}\"\n+            )\n+        self._constraint = value\n+\n     def __repr__(self):\n         return (\n             f\"<KerasVariable shape={self.shape}, dtype={self.dtype}, \"\n\n@@ -164,7 +164,7 @@ class Dense(Layer):\n                 \"lora is already enabled. \"\n                 \"This can only be done once per layer.\"\n             )\n-        self._tracker.locked = False\n+        self._tracker.unlock()\n         self.lora_kernel_a = self.add_weight(\n             name=\"lora_kernel_a\",\n             shape=(self.kernel.shape[0], rank),\n@@ -178,7 +178,7 @@ class Dense(Layer):\n             regularizer=self.kernel_regularizer,\n         )\n         self.kernel.trainable = False\n-        self._tracker.locked = True\n+        self._tracker.lock()\n         self.lora_enabled = True\n \n     def save_own_variables(self, store):\n\n@@ -240,8 +240,7 @@ class EinsumDense(Layer):\n                 \"lora is already enabled. \"\n                 \"This can only be done once per layer.\"\n             )\n-\n-        self._tracker.locked = False\n+        self._tracker.unlock()\n         self.lora_kernel_a = self.add_weight(\n             name=\"lora_kernel_a\",\n             shape=(self.kernel.shape[:-1] + (rank,)),\n@@ -255,7 +254,7 @@ class EinsumDense(Layer):\n             regularizer=self.kernel_regularizer,\n         )\n         self.kernel.trainable = False\n-        self._tracker.locked = True\n+        self._tracker.lock()\n         self.lora_enabled = True\n \n     def save_own_variables(self, store):\n\n@@ -143,7 +143,7 @@ class Embedding(Layer):\n                 \"lora is already enabled. \"\n                 \"This can only be done once per layer.\"\n             )\n-        self._tracker.locked = False\n+        self._tracker.unlock()\n         self.lora_embeddings_a = self.add_weight(\n             name=\"lora_embeddings_a\",\n             shape=(self.embeddings.shape[0], rank),\n@@ -157,7 +157,7 @@ class Embedding(Layer):\n             regularizer=self.embeddings_regularizer,\n         )\n         self.embeddings.trainable = False\n-        self._tracker.locked = True\n+        self._tracker.lock()\n         self.lora_enabled = True\n \n     def save_own_variables(self, store):\n\n@@ -1062,13 +1062,14 @@ class Layer(BackendLayer, Operation):\n \n     def _get_regularization_losses(self):\n         weight_regularization_losses = []\n-        for v in self.trainable_weights:\n-            regularizer = getattr(v, \"regularizer\", None)\n-            if regularizer is None:\n+        for variable in self.trainable_weights:\n+            if variable.regularizer is None:\n                 continue\n             if backend.in_stateless_scope():\n-                v = backend.get_stateless_scope().get_current_value(v)\n-            weight_regularization_losses.append(regularizer(v))\n+                v = backend.get_stateless_scope().get_current_value(variable)\n+            else:\n+                v = variable\n+            weight_regularization_losses.append(variable.regularizer(v))\n         return weight_regularization_losses\n \n     @property\n@@ -1156,6 +1157,13 @@ class Layer(BackendLayer, Operation):\n         else:\n             self._tracker.add_to_store(\"non_trainable_variables\", variable)\n \n+    def _untrack_variable(self, variable):\n+        previous_lock_state = self._tracker.locked\n+        self._tracker.unlock()\n+        self._tracker.untrack(variable)\n+        if previous_lock_state is True:\n+            self._tracker.lock()\n+\n     def add_metric(self):\n         # Permanently disabled\n         raise NotImplementedError\n\n@@ -855,3 +855,31 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(layer.w5.shape, (2, 2))\n         self.assertEqual(layer.w5.dtype, \"float32\")\n         self.assertAllClose(backend.convert_to_numpy(layer.w5), np.ones((2, 2)))\n+\n+    def test_remove_weight(self):\n+        class MyLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.w = self.add_weight()\n+\n+            def custom_remove_w(self):\n+                self.w = self._untrack_variable(self.w)\n+\n+            def custom_change_dtype(self):\n+                self.w = self._untrack_variable(self.w)\n+                self.w = self.add_weight(\n+                    initializer=\"zeros\", dtype=\"int8\", trainable=False\n+                )\n+\n+        layer = MyLayer()\n+        self.assertEqual(len(layer.weights), 1)\n+        layer.custom_remove_w()\n+        self.assertEqual(len(layer.weights), 0)\n+        self.assertEqual(layer.w, None)\n+\n+        layer = MyLayer()\n+        self.assertEqual(layer.w.dtype, \"float32\")\n+        self.assertEqual(layer.w.trainable, True)\n+        layer.custom_change_dtype()\n+        self.assertEqual(layer.w.dtype, \"int8\")\n+        self.assertEqual(layer.w.trainable, False)\n\n@@ -330,7 +330,7 @@ class BaseOptimizer:\n             self._backend_apply_gradients(grads, trainable_variables)\n             # Apply variable constraints after applying gradients.\n             for variable in trainable_variables:\n-                if getattr(variable, \"constraint\", None) is not None:\n+                if variable.constraint is not None:\n                     variable.assign(variable.constraint(variable))\n \n     def _backend_apply_gradients(self, grads, trainable_variables):\n\n@@ -95,10 +95,14 @@ class Tracker:\n                 self.stored_ids[store_name].remove(id(value))\n                 python_utils.remove_by_id(self.config[store_name][1], value)\n \n-    def lock(self, msg):\n+    def lock(self, msg=None):\n         self.locked = True\n+        if msg is not None:\n             self._lock_violation_msg = msg\n \n+    def unlock(self):\n+        self.locked = False\n+\n     def add_to_store(self, store_name, value):\n         if self.locked:\n             raise ValueError(self._lock_violation_msg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d3057da1e5ada7ac97ffafd02dfa335cd9f3212", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 4 | Churn Cumulative: 159 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -379,7 +379,9 @@ PYTHON_DTYPES_MAP = {\n }\n \n \n-@keras_export(\"keras.backend.standardize_dtype\")\n+@keras_export(\n+    [\"keras.utils.standardize_dtype\", \"keras.backend.standardize_dtype\"]\n+)\n def standardize_dtype(dtype):\n     if dtype is None:\n         return config.floatx()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bde0e7cc88a425b4af316d206f7d743f5120323b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 89 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -110,7 +110,7 @@ class TorchModuleWrapper(Layer):\n         self.built = True\n \n     def call(self, *args, **kwargs):\n-        return self.module.forward(*args, **kwargs)\n+        return self.module(*args, **kwargs)\n \n     def save_own_variables(self, store):\n         \"\"\"Saves model's state from `state_dict`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#05dd51e1d9a6a9915474708f9d8027508fe36e86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 45 | Churn Cumulative: 964 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -76,7 +76,13 @@ class SpectralNormalization(Wrapper):\n \n     def call(self, inputs, training=False):\n         if training:\n-            self.normalize_weights()\n+            new_vector_u, new_kernel = ops.cond(\n+                ops.all(ops.equal(self.kernel.value, 0)),\n+                lambda: (self.vector_u.value, self.kernel.value),\n+                self.normalized_weights,\n+            )\n+            self.vector_u.assign(new_vector_u)\n+            self.kernel.assign(new_kernel)\n \n         output = self.layer(inputs)\n         return ops.cast(output, inputs.dtype)\n@@ -84,18 +90,16 @@ class SpectralNormalization(Wrapper):\n     def compute_output_shape(self, input_shape):\n         return self.layer.compute_output_shape(input_shape)\n \n-    def normalize_weights(self):\n+    def normalized_weights(self):\n         \"\"\"Generate spectral normalized weights.\n \n-        This method will update the value of `self.kernel` with the\n+        This method returns the updated value for `self.kernel` with the\n         spectral normalized value, so that the layer is ready for `call()`.\n         \"\"\"\n \n         weights = ops.reshape(self.kernel, [-1, self.kernel_shape[-1]])\n-        vector_u = self.vector_u\n+        vector_u = self.vector_u.value\n \n-        # check for zeroes weights\n-        if not all([w == 0.0 for w in weights]):\n         for _ in range(self.power_iterations):\n             vector_v = normalize(\n                 ops.matmul(vector_u, ops.transpose(weights)), axis=None\n@@ -106,12 +110,9 @@ class SpectralNormalization(Wrapper):\n         sigma = ops.matmul(\n             ops.matmul(vector_v, weights), ops.transpose(vector_u)\n         )\n-            self.vector_u.assign(ops.cast(vector_u, self.vector_u.dtype))\n-            self.kernel.assign(\n-                ops.cast(\n-                    ops.reshape(self.kernel / sigma, self.kernel_shape),\n-                    self.kernel.dtype,\n-                )\n+        kernel = ops.reshape(ops.divide(self.kernel, sigma), self.kernel_shape)\n+        return ops.cast(vector_u, self.vector_u.dtype), ops.cast(\n+            kernel, self.kernel.dtype\n         )\n \n     def get_config(self):\n\n@@ -77,9 +77,9 @@ class AlphaDropout(Layer):\n \n     def _get_concrete_noise_shape(self, inputs, noise_shape):\n         if noise_shape is None:\n-            return inputs.shape\n+            return ops.shape(inputs)\n \n-        concrete_inputs_shape = inputs.shape\n+        concrete_inputs_shape = ops.shape(inputs)\n         concrete_noise_shape = []\n         for i, value in enumerate(noise_shape):\n             concrete_noise_shape.append(\n\n@@ -392,7 +392,9 @@ class RNN(Layer):\n \n         # Prepopulate the dropout state so that the inner_loop is stateless\n         # this is particularly important for JAX backend.\n-        self._maybe_config_dropout_masks(self.cell, sequences, initial_state)\n+        self._maybe_config_dropout_masks(\n+            self.cell, sequences[:, 0, :], initial_state\n+        )\n \n         last_output, outputs, states = self.inner_loop(\n             sequences=sequences,\n@@ -426,18 +428,22 @@ class RNN(Layer):\n         return output\n \n     def _maybe_config_dropout_masks(self, cell, input_sequence, input_state):\n-        step_input = input_sequence[:, 0, :]\n         state = (\n             input_state[0]\n             if isinstance(input_state, (list, tuple))\n             else input_state\n         )\n         if isinstance(cell, DropoutRNNCell):\n-            cell.get_dropout_mask(step_input)\n+            cell.get_dropout_mask(input_sequence)\n             cell.get_recurrent_dropout_mask(state)\n         if isinstance(cell, StackedRNNCells):\n             for c, s in zip(cell.cells, input_state):\n                 self._maybe_config_dropout_masks(c, input_sequence, s)\n+                # Replicate the behavior of `StackedRNNCells.call` to compute\n+                # the inputs for the next cell.\n+                s = list(s) if tree.is_nested(s) else [s]\n+                cell_call_fn = c.__call__ if callable(c) else c.call\n+                input_sequence, _ = cell_call_fn(input_sequence, s)\n \n     def _maybe_reset_dropout_masks(self, cell):\n         if isinstance(cell, DropoutRNNCell):\n\n@@ -320,8 +320,8 @@ class TestCase(unittest.TestCase):\n                     super().__init__()\n                     self.layer = layer\n \n-                def call(self, x):\n-                    return self.layer(x)\n+                def call(self, x, training=False):\n+                    return self.layer(x, training=training)\n \n             model = TestModel(layer)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#671546ab769b9e8fe32180202a9057f2742321b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 8 | Files Changed: 7 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 41/20 | Churn Δ: 36 | Churn Cumulative: 1073 | Contributors (this commit): 9 | Commits (past 90d): 34 | Contributors (cumulative): 24 | DMM Complexity: 0.8421052631578947\n\nDIFF:\n@@ -371,7 +371,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self.make_train_function()\n         self.stop_training = False\n         callbacks.on_train_begin()\n-\n+        initial_epoch = self._initial_epoch or initial_epoch\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n\n@@ -314,6 +314,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         callbacks.on_train_begin()\n         training_logs = None\n         logs = None\n+        initial_epoch = self._initial_epoch or initial_epoch\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n\n@@ -233,7 +233,7 @@ class TorchTrainer(base_trainer.Trainer):\n         self.stop_training = False\n         self.make_train_function()\n         callbacks.on_train_begin()\n-\n+        initial_epoch = self._initial_epoch or initial_epoch\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n\n@@ -1,4 +1,4 @@\n-from keras.callbacks.backup_and_restore_callback import BackupAndRestore\n+from keras.callbacks.backup_and_restore import BackupAndRestore\n from keras.callbacks.callback import Callback\n from keras.callbacks.callback_list import CallbackList\n from keras.callbacks.csv_logger import CSVLogger\n\n@@ -1,3 +1,5 @@\n+import json\n+\n from keras.api_export import keras_export\n from keras.callbacks.callback import Callback\n from keras.utils import file_utils\n@@ -85,6 +87,9 @@ class BackupAndRestore(Callback):\n             raise ValueError(\"Empty `backup_dir` argument passed\")\n         self.backup_dir = backup_dir\n         self._weights_path = file_utils.join(backup_dir, \"latest.weights.h5\")\n+        self._training_metadata_path = file_utils.join(\n+            backup_dir, \"training_metadata.json\"\n+        )\n         if save_freq != \"epoch\" and not isinstance(save_freq, int):\n             raise ValueError(\n                 \"Invalid value for argument `save_freq`. \"\n@@ -96,9 +101,15 @@ class BackupAndRestore(Callback):\n         \"\"\"Get training state from temporary file and restore it.\"\"\"\n         if file_utils.exists(self._weights_path):\n             self.model.load_weights(self._weights_path)\n+        if file_utils.exists(self._training_metadata_path):\n+            with file_utils.File(self._training_metadata_path, \"r\") as f:\n+                training_metadata = json.loads(f.read())\n+            epoch = training_metadata[\"epoch\"]\n+            self.model._initial_epoch = epoch\n \n     def on_epoch_end(self, epoch, logs=None):\n-        self._current_epoch = epoch\n+        self._current_epoch = epoch + 1\n+        self._last_batch_seen = 0\n         if self.save_freq == \"epoch\":\n             self._save_model()\n \n@@ -119,6 +130,12 @@ class BackupAndRestore(Callback):\n         if not file_utils.exists(self.backup_dir):\n             file_utils.makedirs(self.backup_dir)\n         self.model.save_weights(filepath=self._weights_path, overwrite=True)\n+        with file_utils.File(self._training_metadata_path, \"w\") as f:\n+            training_metadata = {\n+                \"epoch\": self._current_epoch,\n+                \"batch\": self._last_batch_seen,\n+            }\n+            f.write(json.dumps(training_metadata))\n \n     def _should_save_on_batch(self, batch):\n         \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n\n@@ -101,7 +101,7 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n                 x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n             )\n \n-            self.assertEqual(cbk._current_epoch, 4)\n+            self.assertEqual(cbk._current_epoch, 5)\n             self.assertEqual(hist.epoch[-1], 4)\n             self.assertEqual(int(model.layers[0].counter.value), 17)\n \n@@ -135,16 +135,16 @@ class BackupAndRestoreCallbackTest(testing.TestCase):\n                 verbose=0,\n             )\n         except RuntimeError:\n-            self.assertEqual(cbk._current_epoch, 1)\n+            self.assertEqual(cbk._current_epoch, 2)\n             self.assertTrue(file_utils.exists(backup_dir))\n             self.assertEqual(int(model.layers[0].counter.value), 6)\n \n             hist = model.fit(\n                 x_train, y_train, batch_size=4, callbacks=[cbk], epochs=5\n             )\n-            self.assertEqual(cbk._current_epoch, 4)\n+            self.assertEqual(cbk._current_epoch, 5)\n             self.assertEqual(hist.epoch[-1], 4)\n-            self.assertEqual(int(model.layers[0].counter.value), 21)\n+            self.assertEqual(int(model.layers[0].counter.value), 5 * 3)\n \n     # Checking if after interruption, when model is deleted\n     @pytest.mark.requires_trainable_backend\n\n@@ -24,6 +24,8 @@ class Trainer:\n         self.compiled = False\n         self.loss = None\n         self.steps_per_execution = 1\n+        # Can be set by callbacks in on_train_begin\n+        self._initial_epoch = None\n \n     @traceback_utils.filter_traceback\n     @tracking.no_automatic_dependency_tracking\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9e6f0b05582d3b3e1862f6763abdaaf5f2e6244", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 4826 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -35,12 +35,12 @@ class RNNCellWithDropout(layers.Layer, DropoutRNNCell):\n     def call(self, inputs, states, training=False):\n         if training:\n             dp_mask = self.get_dropout_mask(inputs)\n-            inputs *= dp_mask\n+            inputs = inputs * dp_mask\n         prev_output = states[0]\n         h = ops.matmul(inputs, self.kernel)\n         if training:\n             rdp_mask = self.get_recurrent_dropout_mask(prev_output)\n-            prev_output *= rdp_mask\n+            prev_output = prev_output * rdp_mask\n         output = h + ops.matmul(prev_output, self.recurrent_kernel)\n         return output, [output]\n \n\n@@ -195,9 +195,9 @@ class GRUCell(Layer, DropoutRNNCell):\n                 )\n \n         if training and 0.0 < self.dropout < 1.0:\n-            inputs *= dp_mask\n+            inputs = inputs * dp_mask\n         if training and 0.0 < self.recurrent_dropout < 1.0:\n-            h_tm1 *= rec_dp_mask\n+            h_tm1 = h_tm1 * rec_dp_mask\n \n         if self.implementation == 1:\n             inputs_z = inputs\n\n@@ -158,13 +158,13 @@ class SimpleRNNCell(Layer, DropoutRNNCell):\n         rec_dp_mask = self.get_recurrent_dropout_mask(prev_output)\n \n         if training and dp_mask is not None:\n-            sequence *= dp_mask\n+            sequence = sequence * dp_mask\n         h = ops.matmul(sequence, self.kernel)\n         if self.bias is not None:\n             h += self.bias\n \n         if training and rec_dp_mask is not None:\n-            prev_output *= rec_dp_mask\n+            prev_output = prev_output * rec_dp_mask\n         output = h + ops.matmul(prev_output, self.recurrent_kernel)\n         if self.activation is not None:\n             output = self.activation(output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39b1e0cb73487a1fed70e9c44d6b1c6695d600c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 594 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1862,7 +1862,7 @@ class Normalize(Operation):\n         return KerasTensor(shape=x.shape)\n \n     def call(self, x):\n-        return _normalize(x, self.order, self.axis)\n+        return _normalize(x, axis=self.axis, order=self.order)\n \n \n @keras_export(\n@@ -1896,8 +1896,8 @@ def normalize(x, axis=-1, order=2):\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Normalize(axis, order).symbolic_call(x)\n-    return _normalize(x, axis, order)\n+        return Normalize(axis=axis, order=order).symbolic_call(x)\n+    return _normalize(x, axis=axis, order=order)\n \n \n def _normalize(x, axis=-1, order=2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af3106a83a0470c9636ba831c2347ae9aeaeb167", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 11/8 | Churn Δ: 89 | Churn Cumulative: 2006 | Contributors (this commit): 11 | Commits (past 90d): 12 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -455,8 +455,8 @@ def standardize_shape(shape):\n     for e in shape:\n         if e is None:\n             continue\n-        if config.backend() == \"jax\" and str(e) == \"b\":\n-            # JAX2TF tracing represents `None` dimensions as `b`\n+        if config.backend() == \"jax\" and \"_DimExpr\" in str(type(e)):\n+            # JAX2TF tracing uses JAX-native dimension expressions\n             continue\n         if not is_int_dtype(type(e)):\n             raise ValueError(\n\n@@ -1,5 +1,3 @@\n-from unittest.mock import patch\n-\n import numpy as np\n import pytest\n from absl.testing import parameterized\n@@ -614,13 +612,6 @@ class VariableBinaryOperationsTest(test_case.TestCase):\n         ):\n             standardize_dtype(invalid_dtype)\n \n-    @patch(\"keras.backend.config.backend\", return_value=\"jax\")\n-    def test_jax_backend_b_dimension(self, mock_backend):\n-        \"\"\"Test 'b' dimension handling with JAX backend.\"\"\"\n-        shape = (3, \"b\", 5)\n-        standardized_shape = standardize_shape(shape)\n-        self.assertEqual(standardized_shape, shape)\n-\n     def test_negative_shape_entry(self):\n         \"\"\"Test negative shape entry.\"\"\"\n         shape = (3, -1, 5)\n\n@@ -76,7 +76,7 @@ class ExportArchiveTest(testing.TestCase):\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\",\n-        reason=\"Registering a tf.function endpoint is only in TF backend.\",\n+        reason=\"This test is native to the TF backend.\",\n     )\n     def test_endpoint_registration_tf_function(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n@@ -175,6 +175,80 @@ class ExportArchiveTest(testing.TestCase):\n             export_archive.non_trainable_variables[0], tf.Variable\n         )\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"This test is native to the JAX backend.\",\n+    )\n+    def test_jax_multi_unknown_endpoint_registration(self):\n+        window_size = 100\n+\n+        X = np.random.random((1024, window_size, 1))\n+        Y = np.random.random((1024, window_size, 1))\n+\n+        model = models.Sequential(\n+            [\n+                layers.Dense(128, activation=\"relu\"),\n+                layers.Dense(64, activation=\"relu\"),\n+                layers.Dense(1, activation=\"relu\"),\n+            ]\n+        )\n+\n+        model.compile(optimizer=\"adam\", loss=\"mse\")\n+\n+        model.fit(X, Y, batch_size=32)\n+\n+        # build a JAX function\n+        def model_call(x):\n+            return model(x)\n+\n+        from jax import default_backend as jax_device\n+        from jax.experimental import jax2tf\n+\n+        native_jax_compatible = not (\n+            jax_device() == \"gpu\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+        )\n+        # now, convert JAX function\n+        converted_model_call = jax2tf.convert(\n+            model_call,\n+            native_serialization=native_jax_compatible,\n+            polymorphic_shapes=[\"(b, t, 1)\"],\n+        )\n+\n+        # you can now build a TF inference function\n+        @tf.function(\n+            input_signature=[\n+                tf.TensorSpec(shape=(None, None, 1), dtype=tf.float32)\n+            ],\n+            autograph=False,\n+        )\n+        def infer_fn(x):\n+            return converted_model_call(x)\n+\n+        ref_input = np.random.random((1024, window_size, 1))\n+        ref_output = infer_fn(ref_input)\n+\n+        # Export with TF inference function as endpoint\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model\")\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\"serve\", infer_fn)\n+        export_archive.write_out(temp_filepath)\n+\n+        # Reload and verify outputs\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertFalse(hasattr(revived_model, \"_tracked\"))\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input), atol=1e-6\n+        )\n+        self.assertLen(revived_model.variables, 6)\n+        self.assertLen(revived_model.trainable_variables, 6)\n+        self.assertLen(revived_model.non_trainable_variables, 0)\n+\n+        # Assert all variables wrapped as `tf.Variable`\n+        assert isinstance(export_archive.variables[0], tf.Variable)\n+        assert isinstance(export_archive.trainable_variables[0], tf.Variable)\n+\n     def test_layer_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c21e56e3b8202536b8b46ddb5a2aa391bce1499f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 56 | Files Changed: 24 | Hunks: 53 | Methods Changed: 30 | Complexity Δ (Sum/Max): 78/36 | Churn Δ: 112 | Churn Cumulative: 18091 | Contributors (this commit): 79 | Commits (past 90d): 86 | Contributors (cumulative): 207 | DMM Complexity: None\n\nDIFF:\n@@ -277,7 +277,7 @@ inputs = keras.Input(shape=(32,))\n outputs = keras.layers.Dense(1)(inputs)\n model = CustomModel(inputs, outputs)\n \n-# We don't passs a loss or metrics here.\n+# We don't pass a loss or metrics here.\n model.compile(optimizer=\"adam\")\n \n # Just use `fit` as usual -- you can use callbacks, etc.\n\n@@ -189,7 +189,7 @@ inputs = keras.Input(shape=(32,))\n outputs = keras.layers.Dense(1)(inputs)\n model = CustomModel(inputs, outputs)\n \n-# We don't passs a loss or metrics here.\n+# We don't pass a loss or metrics here.\n model.compile(optimizer=\"adam\")\n \n # Just use `fit` as usual -- you can use callbacks, etc.\n\n@@ -204,7 +204,7 @@ inputs = keras.Input(shape=(32,))\n outputs = keras.layers.Dense(1)(inputs)\n model = CustomModel(inputs, outputs)\n \n-# We don't passs a loss or metrics here.\n+# We don't pass a loss or metrics here.\n model.compile(optimizer=\"adam\")\n \n # Just use `fit` as usual -- you can use callbacks, etc.\n\n@@ -123,7 +123,7 @@ Here's our training loop, step by step:\n \n - We open a `for` loop that iterates over epochs\n - For each epoch, we open a `for` loop that iterates over the dataset, in batches\n-- For each batch, we call the model on the input data to retrive the predictions,\n+- For each batch, we call the model on the input data to retrieve the predictions,\n then we use them to compute a loss value\n - We call `loss.backward()` to \n - Outside the scope, we retrieve the gradients of the weights\n\n@@ -16,7 +16,7 @@ def _convert_conv_tranpose_padding_args_from_keras_to_jax(\n     kernel_size = (kernel_size - 1) * dilation_rate + 1\n \n     if padding.lower() == \"valid\":\n-        # If output_padding is None, we fill it so that the shape of the ouput\n+        # If output_padding is None, we fill it so that the shape of the output\n         # is `(input-1)*s + max(kernel_size, stride)`\n         output_padding = (\n             max(kernel_size, stride) - kernel_size\n@@ -28,12 +28,12 @@ def _convert_conv_tranpose_padding_args_from_keras_to_jax(\n \n     else:\n         if output_padding is None:\n-            # When output_padding is None, we want the shape of the ouput to\n+            # When output_padding is None, we want the shape of the output to\n             # be `input * s`, therefore a total padding of\n             # `stride + kernel_size - 2`\n             pad_len = stride + kernel_size - 2\n         else:\n-            # When output_padding is filled, we want the shape of the ouput to\n+            # When output_padding is filled, we want the shape of the output to\n             # be `(input-1)*stride + kernel_size%2 + output_padding`\n             pad_len = kernel_size + kernel_size % 2 - 2 + output_padding\n         left_pad = min(pad_len // 2 + pad_len % 2, kernel_size - 1)\n@@ -51,15 +51,15 @@ def _convert_conv_tranpose_padding_args_from_keras_to_torch(\n     `torch_output_padding` on the right.\n     Because in Torch the output_padding can only be added to the right,\n     consistency with Tensorflow is not always possible. In particular this is\n-    the case when both the Torch padding and output_padding values are stricly\n-    positive.\n+    the case when both the Torch padding and output_padding values are\n+    strictly positive.\n     \"\"\"\n     assert padding.lower() in {\"valid\", \"same\"}\n     original_kernel_size = kernel_size\n     kernel_size = (kernel_size - 1) * dilation_rate + 1\n \n     if padding.lower() == \"valid\":\n-        # If output_padding is None, we fill it so that the shape of the ouput\n+        # If output_padding is None, we fill it so that the shape of the output\n         # is `(i-1)*s + max(k, s)`\n         output_padding = (\n             max(kernel_size, stride) - kernel_size\n@@ -70,7 +70,7 @@ def _convert_conv_tranpose_padding_args_from_keras_to_torch(\n         torch_output_padding = output_padding\n \n     else:\n-        # When output_padding is None, we want the shape of the ouput to be\n+        # When output_padding is None, we want the shape of the output to be\n         # `input * s`, otherwise we use the value provided.\n         output_padding = (\n             stride - kernel_size % 2\n@@ -86,7 +86,7 @@ def _convert_conv_tranpose_padding_args_from_keras_to_torch(\n \n     if torch_padding > 0 and torch_output_padding > 0:\n         warnings.warn(\n-            f\"You might experience inconsistencies accross backends when \"\n+            f\"You might experience inconsistencies across backends when \"\n             f\"calling conv transpose with kernel_size={original_kernel_size}, \"\n             f\"stride={stride}, dilation_rate={dilation_rate}, \"\n             f\"padding={padding}, output_padding={output_padding}.\"\n\n@@ -122,7 +122,7 @@ class KerasVariable:\n             if value is not None:\n                 return self._maybe_autocast(value)\n         if self._value is None:\n-            # Unitialized variable. Return a placeholder.\n+            # Uninitialized variable. Return a placeholder.\n             # This is fine because it's only ever used\n             # in during shape inference / graph tracing\n             # (anything else would be a bug, to be fixed.)\n\n@@ -64,7 +64,7 @@ def leaky_relu(x, negative_slope=0.2):\n \n \n def hard_sigmoid(x):\n-    # python numbers will be promoted to float64 by np, so it's neccessary to\n+    # python numbers will be promoted to float64 by np, so it's necessary to\n     # first convert the python numbers to np scalars\n     x = x / np.array(6.0, x.dtype) + np.array(0.5, x.dtype)\n     return np.where(\n@@ -96,7 +96,7 @@ def selu(\n \n def gelu(x, approximate=True):\n     x = convert_to_tensor(x)\n-    # followd by JAX's implementation\n+    # followed by JAX's implementation\n     if approximate:\n         sqrt_2_over_pi = np.sqrt(2 / np.pi).astype(x.dtype)\n         cdf = np.array(0.5, x.dtype) * (\n\n@@ -143,7 +143,7 @@ def _transpose_spatial_inputs(inputs):\n \n \n def _transpose_spatial_outputs(outputs):\n-    # Undo the tranpose in `_transpose_spatial_inputs`.\n+    # Undo the transpose in `_transpose_spatial_inputs`.\n     num_spatial_dims = len(outputs.shape) - 2\n     if num_spatial_dims == 1:\n         outputs = tf.transpose(outputs, (0, 2, 1))\n\n@@ -306,7 +306,7 @@ def mean(x, axis=None, keepdims=False):\n             )\n             return tf.IndexedSlices(new_values, x.indices, new_dense_shape)\n         elif rank == len(axis) + 1:\n-            # `keepdims=False` and reducing against all axes exept 0, result is\n+            # `keepdims=False` and reducing against all axes except 0, result is\n             # a 1D tensor, which cannot be `IndexedSlices`. We have to scatter\n             # the computed means to construct the correct dense tensor.\n             return tf.scatter_nd(\n@@ -1140,7 +1140,7 @@ def nan_to_num(x):\n     # Replace NaN with 0\n     x = tf.where(tf.math.is_nan(x), tf.constant(0, dtype), x)\n \n-    # Replace positive infinitiy with dtype.max\n+    # Replace positive infinity with dtype.max\n     x = tf.where(tf.math.is_inf(x) & (x > 0), tf.constant(dtype.max, dtype), x)\n \n     # Replace negative infinity with dtype.min\n\n@@ -201,7 +201,7 @@ def _transpose_spatial_inputs(inputs):\n \n \n def _transpose_spatial_outputs(outputs):\n-    # Undo the tranpose in `_transpose_spatial_inputs`.\n+    # Undo the transpose in `_transpose_spatial_inputs`.\n     num_spatial_dims = len(outputs.shape) - 2\n     if num_spatial_dims == 1:\n         outputs = torch.permute(outputs, (0, 2, 1))\n@@ -567,7 +567,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     dims = output.dim()\n     if axis != -1 and axis != dims:\n         new_axes_order = list(range(dims))\n-        new_axes_order[axis] = -1  # Shifts output to axis positon\n+        new_axes_order[axis] = -1  # Shifts output to axis position\n         # Shift remaining axes with offset by 1 since output moved to `axis`.\n         for ax in range(axis + 1, dims):\n             new_axes_order[ax] -= 1\n\n@@ -153,7 +153,7 @@ class SwapEMAWeights(Callback):\n             self._swap_variables()\n             self._ema_weights_in_model = True\n             # We need to recover EMA weights from the previously swapped weights\n-            # in the last epoch. This is becuase, at the end of the fitting,\n+            # in the last epoch. This is because, at the end of the fitting,\n             # `finalize_variable_values` will be called to assign\n             # `_model_variables_moving_average` to `trainable_variables`.\n             if epoch == self.params[\"epochs\"] - 1:\n\n@@ -141,7 +141,7 @@ def get_word_index(path=\"reuters_word_index.json\"):\n \n     E.g. word index of 'the' is 1, but the in the actual training data, the\n     index of 'the' will be 1 + 3 = 4. Vice versa, to translate word indices in\n-    training data back to words using this mapping, indices need to substract 3.\n+    training data back to words using this mapping, indices need to subtract 3.\n \n     Args:\n         path: where to cache the data (relative to `~/.keras/dataset`).\n\n@@ -48,9 +48,9 @@ def initialize(job_addresses=None, num_processes=None, proceed_id=None):\n     Calling `initialize` will prepare the backend for execution on multi-host\n     GPU or TPUs. It should be called before any computations.\n \n-    Note that the parameters can also be injected via enviornment variables,\n+    Note that the parameters can also be injected via environment variables,\n     which can be better controlled by the launch script at startup time.\n-    For certain backend that also rely on the enviornment variables to\n+    For certain backend that also rely on the environment variables to\n     configure, Keras will properly forward them.\n \n     Args:\n@@ -58,18 +58,18 @@ def initialize(job_addresses=None, num_processes=None, proceed_id=None):\n             that will form the whole computation cluster. Note that for JAX\n             backend, only the address for job 0 (coodinator) is needed. For\n             certain runtime like cloud TPU, this value can be `None`, and the\n-            backend will figure it out with the TPU enviornment variables. You\n-            can also config this value via enviornment variable\n+            backend will figure it out with the TPU environment variables. You\n+            can also config this value via environment variable\n             `KERAS_DISTRIBUTION_JOB_ADDRESSES`.\n         num_processes: int. The number of worker/processes that will form the\n             whole computation cluster. For certain runtime like cloud TPU, this\n             value can be `None`, and the backend will figure it out with the TPU\n-            enviornment variables. You can also configure this value via\n-            enviornment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n+            environment variables. You can also configure this value via\n+            environment variable `KERAS_DISTRIBUTION_NUM_PROCESSES`.\n         process_id: int. The ID number of the current worker/process. The value\n             should be ranged from `0` to `num_processes - 1`. `0` will indicate\n             the current worker/process is the master/coordinate job. You can\n-            also configure this value via enviornment variable\n+            also configure this value via environment variable\n             `KERAS_DISTRIBUTION_PROCESS_ID`.\n \n         Example:\n@@ -93,7 +93,7 @@ def initialize(job_addresses=None, num_processes=None, proceed_id=None):\n             process_id=1)\n         ```\n \n-        or via the enviornment variables:\n+        or via the environment variables:\n         On process 0:\n         ```python\n         os.environ[\n@@ -223,7 +223,7 @@ class TensorLayout:\n \n     Args:\n         axes: tuple of strings that should map to the `axis_names` in\n-            a `DeviceMesh`. For any dimentions that doesn't need any sharding,\n+            a `DeviceMesh`. For any dimensions that doesn't need any sharding,\n             A `None` can be used a placeholder.\n         device_mesh: Optional `DeviceMesh` that will be used to create\n             the layout. The actual mapping of tensor to physical device\n@@ -320,7 +320,7 @@ class Distribution:\n         \"\"\"Retrieve the `TensorLayout` for the intermediate tensor.\n \n         Args:\n-            path: a string path for the correspoding tensor.\n+            path: a string path for the corresponding tensor.\n \n         return:\n             The `TensorLayout` for the intermediate tensor, which can be used\n@@ -367,7 +367,7 @@ class Distribution:\n class DataParallel(Distribution):\n     \"\"\"Distribution for data parallelism.\n \n-    You can choose to create this instance by either specifing\n+    You can choose to create this instance by either specifying\n     the `device_mesh` or `devices` arguments (but not both).\n \n     The `device_mesh` argument is expected to be a `DeviceMesh` instance,\n@@ -495,7 +495,7 @@ class ModelParallel(Distribution):\n     To construct a `ModelParallel` distribution, you need to provide a\n     `DeviceMesh` and a `LayoutMap`.\n \n-    1. `DeviceMesh` contains physcial device information. The axis names in\n+    1. `DeviceMesh` contains physical device information. The axis names in\n         the mesh will be used to map the variable and data layout.\n     2. `LayoutMap` contains the mapping between variable paths to their\n         corresponding `TensorLayout`.\n@@ -623,7 +623,7 @@ class ModelParallel(Distribution):\n         # model   | w0_0, w0_1, w1_0, w1_1 |\n         #   v     | w0_2, w0_3, w1_2, w1_3 |\n         #         --------------------------\n-        # In this case, the batch dim will be split acorss 2 workers.\n+        # In this case, the batch dim will be split across 2 workers.\n         #\n         # In the case that global batch dim is smaller than the worker size, eg\n         # {'batch': 1, 'model': 8}, then the mesh will be like below:\n@@ -650,12 +650,12 @@ class ModelParallel(Distribution):\n         if mesh_batch_dim_size < local_device_count:\n             # No sharding is needed in this case. The worker will have the\n             # global batch size, and data from the iterator will need to be\n-            # replicated for model dimention.\n+            # replicated for model dimension.\n             return dataset.prefetch(tf.data.AUTOTUNE)\n         else:\n             if mesh_batch_dim_size % local_device_count != 0:\n                 raise ValueError(\n-                    \"The Batch dimention of the mesh is not compatible \"\n+                    \"The Batch dimension of the mesh is not compatible \"\n                     \"with the local worker device count. Mesh batch \"\n                     f\"dim = {mesh_batch_dim_size} and local device \"\n                     f\"count = {local_device_count}\"\n\n@@ -799,7 +799,7 @@ class ConvTransposeCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             (kernel_size, 1, 1)\n         )\n \n-        # Exepected result\n+        # Expected result\n         expected_res = np_conv1d_transpose(\n             x=input,\n             kernel_weights=kernel_weights,\n\n@@ -752,7 +752,7 @@ class Layer(BackendLayer, Operation):\n         # across nested calls.\n         call_context = self._get_call_context()\n \n-        # This is the value explicity passed by the user\n+        # This is the value explicitly passed by the user\n         training = call_spec.user_arguments_dict.get(\"training\", None)\n         if training is None:\n             # Wasn't passed explicitly: use context value\n@@ -877,7 +877,7 @@ class Layer(BackendLayer, Operation):\n             trainable_variables: List of trainable variables of the model.\n             non_trainable_variables: List of non-trainable variables of the\n                 model.\n-            *args: Positional argumets to be passed to `call()`.\n+            *args: Positional arguments to be passed to `call()`.\n             return_losses: If `True`, `stateless_call()` will return the list of\n                 losses created during `call()` as part of its return values.\n             **kwargs: Keyword arguments to be passed to `call()`.\n@@ -1228,7 +1228,7 @@ class Layer(BackendLayer, Operation):\n                     \"your layer. It should create all variables used by the \"\n                     \"layer (e.g. by calling `layer.build()` on all its \"\n                     \"children layers).\\n\"\n-                    f\"Exception encoutered: ''{e}''\"\n+                    f\"Exception encountered: ''{e}''\"\n                 )\n         self.build(first_shape)\n \n\n@@ -293,7 +293,7 @@ def merge_summaries(prev_summary, next_summary, epsilon):\n     Args:\n         prev_summary: 2D `np.ndarray` summary to be merged with `next_summary`.\n         next_summary: 2D `np.ndarray` summary to be merged with `prev_summary`.\n-        epsilon: A float that determines the approxmiate desired precision.\n+        epsilon: A float that determines the approximate desired precision.\n \n     Returns:\n         A 2-D `np.ndarray` that is a merged summary. First column is the\n@@ -319,7 +319,7 @@ def compress_summary(summary, epsilon):\n \n     Args:\n         summary: 2D `np.ndarray` summary to be compressed.\n-        epsilon: A `'float32'` that determines the approxmiate desired\n+        epsilon: A `'float32'` that determines the approximate desired\n         precision.\n \n     Returns:\n\n@@ -240,7 +240,7 @@ class IntegerLookup(IndexLookup):\n             [1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)\n \n     To specify the idf weights for oov tokens, you will need to pass the entire\n-    vocabularly including the leading oov token.\n+    vocabulary including the leading oov token.\n \n     >>> vocab = [-1, 12, 36, 1138, 42]\n     >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n@@ -357,7 +357,7 @@ class IntegerLookup(IndexLookup):\n         self.supports_jit = False\n \n     def adapt(self, data, steps=None):\n-        \"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\n+        \"\"\"Computes a vocabulary of integer terms from tokens in a dataset.\n \n         Calling `adapt()` on an `IntegerLookup` layer is an alternative to\n         passing in a precomputed vocabulary  on construction via the\n@@ -367,7 +367,7 @@ class IntegerLookup(IndexLookup):\n         During `adapt()`, the layer will build a vocabulary of all integer\n         tokens seen in the dataset, sorted by occurrence count, with ties broken\n         by sort order of the tokens (high to low). At the end of `adapt()`, if\n-        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        `max_tokens` is set, the vocabulary will be truncated to `max_tokens`\n         size. For example, adapting a layer with `max_tokens=1000` will compute\n         the 1000 most frequent tokens occurring in the input dataset. If\n         `output_mode='tf-idf'`, `adapt()` will also learn the document\n\n@@ -238,7 +238,7 @@ class StringLookup(IndexLookup):\n            [1.0 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)\n \n     To specify the idf weights for oov values, you will need to pass the entire\n-    vocabularly including the leading oov token.\n+    vocabulary including the leading oov token.\n \n     >>> vocab = [\"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n     >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n@@ -338,7 +338,7 @@ class StringLookup(IndexLookup):\n         self.supports_jit = False\n \n     def adapt(self, data, steps=None):\n-        \"\"\"Computes a vocabulary of interger terms from tokens in a dataset.\n+        \"\"\"Computes a vocabulary of integer terms from tokens in a dataset.\n \n         Calling `adapt()` on a `StringLookup` layer is an alternative to passing\n         in a precomputed vocabulary on construction via the `vocabulary`\n@@ -348,7 +348,7 @@ class StringLookup(IndexLookup):\n         During `adapt()`, the layer will build a vocabulary of all string tokens\n         seen in the dataset, sorted by occurrence count, with ties broken by\n         sort order of the tokens (high to low). At the end of `adapt()`, if\n-        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        `max_tokens` is set, the vocabulary will be truncated to `max_tokens`\n         size. For example, adapting a layer with `max_tokens=1000` will compute\n         the 1000 most frequent tokens occurring in the input dataset. If\n         `output_mode='tf-idf'`, `adapt()` will also learn the document\n\n@@ -397,7 +397,7 @@ class TextVectorization(Layer):\n         During `adapt()`, the layer will build a vocabulary of all string tokens\n         seen in the dataset, sorted by occurrence count, with ties broken by\n         sort order of the tokens (high to low). At the end of `adapt()`, if\n-        `max_tokens` is set, the vocabulary wil be truncated to `max_tokens`\n+        `max_tokens` is set, the vocabulary will be truncated to `max_tokens`\n         size. For example, adapting a layer with `max_tokens=1000` will compute\n         the 1000 most frequent tokens occurring in the input dataset. If\n         `output_mode='tf-idf'`, `adapt()` will also learn the document\n\n@@ -234,7 +234,7 @@ def _find_replace_nested_dict(config, find, replace):\n \n \n def _resolve_compile_arguments_compat(obj, obj_config, module):\n-    \"\"\"Resolves backwards compatiblity issues with training config arguments.\n+    \"\"\"Resolves backwards compatibility issues with training config arguments.\n \n     This helper function accepts built-in Keras modules such as optimizers,\n     losses, and metrics to ensure an object being deserialized is compatible\n\n@@ -172,7 +172,7 @@ def _update_confusion_matrix_variables_optimized(\n             to be the same).\n         multi_label: Optional boolean indicating whether multidimensional\n             prediction/labels should be treated as multilabel responses, or\n-            flattened into a single label. When True, the valus of\n+            flattened into a single label. When True, the values of\n             `variables_to_update` must have a second dimension equal to the\n             number of labels in y_true and y_pred, and those tensors must not be\n             RaggedTensors.\n@@ -383,7 +383,7 @@ def update_confusion_matrix_variables(\n         dimension).\n       multi_label: Optional boolean indicating whether multidimensional\n         prediction/labels should be treated as multilabel responses, or\n-        flattened into a single label. When True, the valus of\n+        flattened into a single label. When True, the values of\n         `variables_to_update` must have a second dimension equal to the number\n         of labels in y_true and y_pred, and those tensors must not be\n         RaggedTensors.\n\n@@ -86,7 +86,7 @@ def validate_string_arg(\n     elif isinstance(value, str) and value in allowable_strings:\n         return\n     raise ValueError(\n-        f\"Unkown value for `{arg_name}` argument of {caller_name}. \"\n+        f\"Unknown value for `{arg_name}` argument of {caller_name}. \"\n         f\"Allowed values are: {allowable_strings}. Received: \"\n         f\"{arg_name}={value}\"\n     )\n\n@@ -49,7 +49,7 @@ def is_interactive_logging_enabled():\n \n     To switch between writing logs to stdout and `absl.logging`, you may use\n     `keras.config.enable_interactive_logging()` and\n-    `keras.config.disable_interactie_logging()`.\n+    `keras.config.disable_interactive_logging()`.\n \n     Returns:\n         Boolean, `True` if interactive logging is enabled,\n\n@@ -63,7 +63,7 @@ def packed_nest_with_indices(\n         index: Index at which to start reading from flat.\n         is_nested_fn: Function used to test if a value should\n             be treated as a nested structure.\n-        sequence_fn: Function used to generate a new strcuture instance.\n+        sequence_fn: Function used to generate a new structure instance.\n \n     Returns:\n         The tuple (new_index, child), where:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
