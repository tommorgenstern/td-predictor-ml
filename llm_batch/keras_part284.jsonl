{"custom_id": "keras#f4b5c496af3a02fc2cf4c4ebeb66af5653d10609", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2436 | Contributors (this commit): 33 | Commits (past 90d): 14 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ def get_class_signature(cls):\n         # in case the class inherits from object and does not\n         # define __init__\n         class_signature = \"{clean_module_name}.{cls_name}()\".format(\n-            clean_module_name=clean_module_name(cls.__module__),\n+            clean_module_name=cls.__module__,\n             cls_name=cls.__name__\n         )\n     return post_process_signature(class_signature)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#17eab763320e1cb310ffe0235e277ae63d1fbd97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 46 | Files Changed: 2 | Hunks: 23 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 87 | Churn Cumulative: 12599 | Contributors (this commit): 130 | Commits (past 90d): 14 | Contributors (cumulative): 133 | DMM Complexity: 0.0\n\nDIFF:\n@@ -46,6 +46,11 @@ _SESSION = None\n # either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n _GRAPH_LEARNING_PHASES = {}\n \n+# This boolean flag can be set to True to leave variable initialization\n+# up to the user.\n+# Change its value via `manual_variable_initialization(value)`.\n+_MANUAL_VAR_INIT = False\n+\n # This list holds the available devices.\n # It is populated when `_get_available_gpus()` is called for the first time.\n # We assume our devices don't change during our lifetime.\n@@ -57,10 +62,7 @@ def _is_tf_1():\n \n \n def get_graph():\n-    if hasattr(tf_keras_backend, 'get_graph'):\n     return tf_keras_backend.get_graph()\n-    else:\n-        return tf.get_default_graph()\n \n \n def tf_graph_op(func):\n@@ -95,21 +97,34 @@ def clear_session():\n     Useful to avoid clutter from old models / layers.\n     \"\"\"\n     tf_keras_backend.clear_session()\n+    if _is_tf_1():\n         global _SESSION\n-    global _GRAPH_LEARNING_PHASES\n         tf.reset_default_graph()\n-    reset_uids()\n         _SESSION = None\n-    with tf.name_scope(''):\n+    reset_uids()\n+    with get_graph().as_default(), tf.name_scope(''):\n         phase = tf.placeholder_with_default(\n             False,\n             shape=(),\n             name='keras_learning_phase')\n+    global _GRAPH_LEARNING_PHASES\n     _GRAPH_LEARNING_PHASES = {}\n-    _GRAPH_LEARNING_PHASES[tf.get_default_graph()] = phase\n+    _GRAPH_LEARNING_PHASES[get_graph()] = phase\n \n \n-manual_variable_initialization = tf_keras_backend.manual_variable_initialization\n+def manual_variable_initialization(value):\n+    \"\"\"Sets the manual variable initialization flag.\n+    This boolean flag determines whether\n+    variables should be initialized\n+    as they are instantiated (default), or if\n+    the user should handle the initialization\n+    (e.g. via `tf.initialize_all_variables()`).\n+    # Arguments\n+        value: Python boolean.\n+    \"\"\"\n+    global _MANUAL_VAR_INIT\n+    _MANUAL_VAR_INIT = value\n+\n \n learning_phase = tf_keras_backend.learning_phase\n learning_phase_scope = tf_keras_backend.learning_phase_scope\n@@ -132,10 +147,15 @@ def get_session():\n     # Returns\n         A TensorFlow session.\n     \"\"\"\n+    if not _is_tf_1():\n+        raise RuntimeError(\n+            '`set_session` is not available '\n+            'when using TensorFlow 2.0.')\n     if tf.executing_eagerly():\n         raise RuntimeError(\n-            '`get_session` is not available when '\n+            '`set_session` is not available when '\n             'TensorFlow is executing eagerly.')\n+\n     global _SESSION\n \n     default_session = tf.get_default_session()\n@@ -185,6 +205,10 @@ def set_session(session):\n     # Arguments\n         session: A TF Session.\n     \"\"\"\n+    if not _is_tf_1():\n+        raise RuntimeError(\n+            '`set_session` is not available '\n+            'when using TensorFlow 2.0.')\n     if tf.executing_eagerly():\n         raise RuntimeError(\n             '`get_session` is not available when '\n@@ -366,7 +390,6 @@ def variable(value, dtype=None, name=None, constraint=None):\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n-    with tf_ops.init_scope():\n     v = tf_keras_backend.variable(\n         value, dtype=dtype, name=name, constraint=constraint)\n     if hasattr(value, 'tocoo'):\n@@ -966,7 +989,7 @@ def update(x, new_x):\n     if _is_tf_1():\n         return tf.assign(x, new_x)\n     else:\n-        x.assign(new_x)\n+        return x.assign(new_x)\n \n \n @tf_graph_op\n@@ -983,7 +1006,7 @@ def update_add(x, increment):\n     if _is_tf_1():\n         return tf.assign_add(x, increment)\n     else:\n-        x.assign_add(increment)\n+        return x.assign_add(increment)\n \n \n @tf_graph_op\n@@ -1000,7 +1023,7 @@ def update_sub(x, decrement):\n     if _is_tf_1():\n         return tf.assign_sub(x, decrement)\n     else:\n-        x.assign_sub(decrement)\n+        return x.assign_sub(decrement)\n \n \n @tf_graph_op\n@@ -2680,17 +2703,7 @@ def set_value(x, value):\n         value: Value to set the tensor to, as a Numpy array\n             (of the same shape).\n     \"\"\"\n-    # TODO\n-    value = np.asarray(value, dtype=dtype(x))\n-    if hasattr(x, '_assign_placeholder'):\n-        assign_placeholder = x._assign_placeholder\n-        assign_op = x._assign_op\n-    else:\n-        assign_placeholder = tf.placeholder(value.dtype, shape=value.shape)\n-        assign_op = x.assign(assign_placeholder)\n-        x._assign_placeholder = assign_placeholder\n-        x._assign_op = assign_op\n-    get_session().run(assign_op, feed_dict={assign_placeholder: value})\n+    tf_keras_backend.set_value(x, value)\n \n \n def batch_set_value(tuples):\n@@ -2700,25 +2713,7 @@ def batch_set_value(tuples):\n         tuples: a list of tuples `(tensor, value)`.\n             `value` should be a Numpy array.\n     \"\"\"\n-    # TODO\n-    if tuples:\n-        assign_ops = []\n-        feed_dict = {}\n-        for x, value in tuples:\n-            value = np.asarray(value, dtype=dtype(x))\n-            tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])\n-            if hasattr(x, '_assign_placeholder'):\n-                assign_placeholder = x._assign_placeholder\n-                assign_op = x._assign_op\n-            else:\n-                assign_placeholder = tf.placeholder(tf_dtype,\n-                                                    shape=value.shape)\n-                assign_op = x.assign(assign_placeholder)\n-                x._assign_placeholder = assign_placeholder\n-                x._assign_op = assign_op\n-            assign_ops.append(assign_op)\n-            feed_dict[assign_placeholder] = value\n-        get_session().run(assign_ops, feed_dict=feed_dict)\n+    tf_keras_backend.batch_set_value(tuples)\n \n \n def get_variable_shape(x):\n\n@@ -31,7 +31,7 @@ def test_vector_classification():\n         layers.Dense(num_classes, activation='softmax')\n     ])\n     model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n+                  optimizer=keras.optimizers.Adam(1e-3),\n                   metrics=['accuracy'])\n     model.summary()\n     history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n@@ -56,7 +56,7 @@ def test_vector_classification_functional():\n     outputs = layers.Dense(num_classes, activation='softmax')(x)\n     model = keras.models.Model(inputs, outputs)\n     model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n-                  optimizer=keras.optimizers.RMSprop(),\n+                  optimizer=keras.optimizers.Adam(1e-3),\n                   metrics=['acc'])\n     history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n                         validation_data=(x_test, y_test),\n@@ -80,7 +80,7 @@ def test_vector_regression():\n         layers.Dense(num_classes)\n     ])\n \n-    model.compile(loss='hinge', optimizer='adagrad')\n+    model.compile(loss='hinge', optimizer=keras.optimizers.Adam(1e-3))\n     history = model.fit(x_train, y_train, epochs=20, batch_size=16,\n                         validation_data=(x_test, y_test), verbose=0)\n     assert (history.history['val_loss'][-1] < 0.9)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bbba9cede7cf9603598a78b6b27d87599dff8312", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 397 | Files Changed: 1 | Hunks: 21 | Methods Changed: 14 | Complexity Δ (Sum/Max): -22/0 | Churn Δ: 421 | Churn Cumulative: 12921 | Contributors (this commit): 129 | Commits (past 90d): 14 | Contributors (cumulative): 129 | DMM Complexity: 0.6197183098591549\n\nDIFF:\n@@ -10,25 +10,14 @@ from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import functional_ops\n from tensorflow.python.ops import ctc_ops as ctc\n-from tensorflow.python.client import device_lib\n-from tensorflow.core.protobuf import config_pb2\n \n import functools\n-from collections import defaultdict\n \n import numpy as np\n from distutils.version import StrictVersion\n-import os\n \n-from .common import floatx\n-from .common import epsilon\n from .common import normalize_data_format\n from ..utils.generic_utils import transpose_shape\n-from ..utils.generic_utils import has_arg\n-\n-# Legacy functions\n-from .common import set_image_dim_ordering\n-from .common import image_dim_ordering\n \n py_all = all\n py_any = any\n@@ -37,20 +26,6 @@ py_slice = slice\n \n # INTERNAL UTILS\n \n-# This is the default internal TF session used by Keras.\n-# It can be set manually via `set_session(sess)`.\n-_SESSION = None\n-\n-# This dictionary holds a mapping {graph: learning_phase}.\n-# A learning phase is a bool tensor used to run Keras models in\n-# either train mode (learning_phase == 1) or test mode (learning_phase == 0).\n-_GRAPH_LEARNING_PHASES = {}\n-\n-# This boolean flag can be set to True to leave variable initialization\n-# up to the user.\n-# Change its value via `manual_variable_initialization(value)`.\n-_MANUAL_VAR_INIT = False\n-\n # This list holds the available devices.\n # It is populated when `_get_available_gpus()` is called for the first time.\n # We assume our devices don't change during our lifetime.\n@@ -61,8 +36,19 @@ def _is_tf_1():\n     return tf.__version__.startswith('1.')\n \n \n-def get_graph():\n-    return tf_keras_backend.get_graph()\n+floatx = tf_keras_backend.floatx\n+epsilon = tf_keras_backend.epsilon\n+\n+get_graph = tf_keras_backend.get_graph\n+get_uid = tf_keras_backend.get_uid\n+reset_uids = tf_keras_backend.reset_uids\n+clear_session = tf_keras_backend.clear_session\n+manual_variable_initialization = tf_keras_backend.manual_variable_initialization\n+\n+learning_phase = tf_keras_backend.learning_phase\n+learning_phase_scope = tf_keras_backend.learning_phase_scope\n+set_learning_phase = tf_keras_backend.set_learning_phase\n+name_scope = tf.name_scope\n \n \n def tf_graph_op(func):\n@@ -73,64 +59,6 @@ def tf_graph_op(func):\n     return wrapper\n \n \n-def get_uid(prefix=''):\n-    \"\"\"Get the uid for the default graph.\n-\n-    # Arguments\n-        prefix: An optional prefix of the graph.\n-\n-    # Returns\n-        A unique identifier for the graph.\n-    \"\"\"\n-    return tf_keras_backend.get_uid(prefix)\n-\n-\n-def reset_uids():\n-    \"\"\"Resets graph identifiers.\n-    \"\"\"\n-    tf_keras_backend.reset_uids()\n-\n-\n-def clear_session():\n-    \"\"\"Destroys the current TF graph and creates a new one.\n-\n-    Useful to avoid clutter from old models / layers.\n-    \"\"\"\n-    tf_keras_backend.clear_session()\n-    if _is_tf_1():\n-        global _SESSION\n-        tf.reset_default_graph()\n-        _SESSION = None\n-    reset_uids()\n-    with get_graph().as_default(), tf.name_scope(''):\n-        phase = tf.placeholder_with_default(\n-            False,\n-            shape=(),\n-            name='keras_learning_phase')\n-    global _GRAPH_LEARNING_PHASES\n-    _GRAPH_LEARNING_PHASES = {}\n-    _GRAPH_LEARNING_PHASES[get_graph()] = phase\n-\n-\n-def manual_variable_initialization(value):\n-    \"\"\"Sets the manual variable initialization flag.\n-    This boolean flag determines whether\n-    variables should be initialized\n-    as they are instantiated (default), or if\n-    the user should handle the initialization\n-    (e.g. via `tf.initialize_all_variables()`).\n-    # Arguments\n-        value: Python boolean.\n-    \"\"\"\n-    global _MANUAL_VAR_INIT\n-    _MANUAL_VAR_INIT = value\n-\n-\n-learning_phase = tf_keras_backend.learning_phase\n-learning_phase_scope = tf_keras_backend.learning_phase_scope\n-set_learning_phase = tf_keras_backend.set_learning_phase\n-\n-\n def get_session():\n     \"\"\"Returns the TF session to be used by the backend.\n \n@@ -155,48 +83,7 @@ def get_session():\n         raise RuntimeError(\n             '`set_session` is not available when '\n             'TensorFlow is executing eagerly.')\n-\n-    global _SESSION\n-\n-    default_session = tf.get_default_session()\n-\n-    if default_session is not None:\n-        session = default_session\n-    else:\n-        if _SESSION is None:\n-            if not os.environ.get('OMP_NUM_THREADS'):\n-                config = tf.ConfigProto(allow_soft_placement=True)\n-            else:\n-                num_thread = int(os.environ.get('OMP_NUM_THREADS'))\n-                config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n-                                        inter_op_parallelism_threads=num_thread,\n-                                        allow_soft_placement=True)\n-            _SESSION = tf.Session(config=config)\n-        session = _SESSION\n-    if not _MANUAL_VAR_INIT:\n-        with session.graph.as_default():\n-            variables = tf.global_variables()\n-            candidate_vars = []\n-            for v in variables:\n-                if not getattr(v, '_keras_initialized', False):\n-                    candidate_vars.append(v)\n-            if candidate_vars:\n-                # This step is expensive, so we only run it on variables\n-                # not already marked as initialized.\n-                is_initialized = session.run(\n-                    [tf.is_variable_initialized(v) for v in candidate_vars])\n-                uninitialized_vars = []\n-                for flag, v in zip(is_initialized, candidate_vars):\n-                    if not flag:\n-                        uninitialized_vars.append(v)\n-                    v._keras_initialized = True\n-                if uninitialized_vars:\n-                    session.run(tf.variables_initializer(uninitialized_vars))\n-    # hack for list_devices() function.\n-    # list_devices() function is not available under tensorflow r1.3.\n-    if not hasattr(session, 'list_devices'):\n-        session.list_devices = lambda: device_lib.list_local_devices()\n-    return session\n+    return tf_keras_backend.get_session()\n \n \n def set_session(session):\n@@ -213,8 +100,7 @@ def set_session(session):\n         raise RuntimeError(\n             '`get_session` is not available when '\n             'TensorFlow is executing eagerly.')\n-    global _SESSION\n-    _SESSION = session\n+    tf_keras_backend.set_session(session)\n \n \n # DEVICE MANIPULATION AND PROBING\n@@ -358,9 +244,6 @@ def to_dense(tensor):\n         return tensor\n \n \n-name_scope = tf.name_scope\n-\n-\n def variable(value, dtype=None, name=None, constraint=None):\n     \"\"\"Instantiates a variable and returns it.\n \n@@ -390,6 +273,7 @@ def variable(value, dtype=None, name=None, constraint=None):\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n+    with tf_ops.init_scope():\n         v = tf_keras_backend.variable(\n             value, dtype=dtype, name=name, constraint=constraint)\n     if hasattr(value, 'tocoo'):\n@@ -986,10 +870,7 @@ def update(x, new_x):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    if _is_tf_1():\n-        return tf.assign(x, new_x)\n-    else:\n-        return x.assign(new_x)\n+    return tf_keras_backend.update(x, new_x)\n \n \n @tf_graph_op\n@@ -1003,10 +884,7 @@ def update_add(x, increment):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    if _is_tf_1():\n-        return tf.assign_add(x, increment)\n-    else:\n-        return x.assign_add(increment)\n+    return tf_keras_backend.update_add(x, increment)\n \n \n @tf_graph_op\n@@ -1020,10 +898,7 @@ def update_sub(x, decrement):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    if _is_tf_1():\n-        return tf.assign_sub(x, decrement)\n-    else:\n-        return x.assign_sub(decrement)\n+    return tf_keras_backend.update_sub(x, decrement)\n \n \n @tf_graph_op\n@@ -2675,8 +2550,7 @@ def get_value(x):\n     # Returns\n         A Numpy array.\n     \"\"\"\n-    # TODO\n-    return x.eval(session=get_session())\n+    return tf_keras_backend.get_value(x)\n \n \n def batch_get_value(ops):\n@@ -2689,10 +2563,7 @@ def batch_get_value(ops):\n         A list of Numpy arrays.\n     \"\"\"\n     # TODO\n-    if ops:\n-        return get_session().run(ops)\n-    else:\n-        return []\n+    return tf_keras_backend.batch_get_value(ops)\n \n \n def set_value(x, value):\n@@ -2753,247 +2624,6 @@ def print_tensor(x, message=''):\n \n # GRAPH MANIPULATION\n \n-# class Function(object):\n-#     \"\"\"Runs a computation graph.\n-\n-#     It's possible to pass arguments to `tf.Session.run()` via `session_kwargs`.\n-#     In particular additional operations via `fetches` argument and additional\n-#     tensor substitutions via `feed_dict` arguments. Note that given\n-#     substitutions are merged with substitutions from `inputs`. Even though\n-#     `feed_dict` is passed once in the constructor (called in `model.compile()`)\n-#     we can modify the values in the dictionary. Through this feed_dict we can\n-#     provide additional substitutions besides Keras inputs.\n-\n-#     # Arguments\n-#         inputs: Feed placeholders to the computation graph.\n-#         outputs: Output tensors to fetch.\n-#         updates: Additional update ops to be run at function call.\n-#         name: a name to help users identify what this function does.\n-#         session_kwargs: arguments to `tf.Session.run()`:\n-#             `fetches`, `feed_dict`,\n-#             `options`, `run_metadata`\n-#     \"\"\"\n-\n-#     def __init__(self, inputs, outputs,\n-#                  updates=None,\n-#                  name=None,\n-#                  **session_kwargs):\n-#         updates = updates or []\n-#         if not isinstance(inputs, (list, tuple)):\n-#             raise TypeError('`inputs` to a TensorFlow backend function '\n-#                             'should be a list or tuple.')\n-#         if not isinstance(outputs, (list, tuple)):\n-#             raise TypeError('`outputs` of a TensorFlow backend function '\n-#                             'should be a list or tuple.')\n-#         if not isinstance(updates, (list, tuple)):\n-#             raise TypeError('`updates` in a TensorFlow backend function '\n-#                             'should be a list or tuple.')\n-#         self.inputs = list(inputs)\n-#         self.outputs = list(outputs)\n-#         with tf.control_dependencies(self.outputs):\n-#             updates_ops = []\n-#             for update in updates:\n-#                 if isinstance(update, tuple):\n-#                     p, new_p = update\n-#                     updates_ops.append(tf.assign(p, new_p))\n-#                 else:\n-#                     # assumed already an op\n-#                     updates_ops.append(update)\n-#             self.updates_op = tf.group(*updates_ops)\n-#         self.name = name\n-#         # additional tensor substitutions\n-#         self.feed_dict = session_kwargs.pop('feed_dict', {})\n-#         # additional operations\n-#         self.fetches = session_kwargs.pop('fetches', [])\n-#         if not isinstance(self.fetches, list):\n-#             self.fetches = [self.fetches]\n-#         # The main use case of `fetches` being passed to a model is the ability\n-#         # to run custom updates\n-#         # (since the outputs of fetches are never returned).\n-#         # This requires us to wrap fetches in `identity` ops.\n-#         self.fetches = [tf.identity(x) for x in self.fetches]\n-#         # self.session_kwargs is used for _legacy_call\n-#         self.session_kwargs = session_kwargs.copy()\n-#         self.run_options = session_kwargs.pop('options', None)\n-#         self.run_metadata = session_kwargs.pop('run_metadata', None)\n-#         if session_kwargs:\n-#             raise ValueError('Some keys in session_kwargs are not '\n-#                              'supported at this '\n-#                              'time: %s', session_kwargs.keys())\n-#         self._callable_fn = None\n-#         self._feed_arrays = None\n-#         self._feed_symbols = None\n-#         self._symbol_vals = None\n-#         self._session = None\n-\n-#     def _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session):\n-#         \"\"\"Generates a callable that runs the graph.\n-\n-#         # Arguments\n-#             feed_arrays: List of input tensors to be fed\n-#                 Numpy arrays at runtime.\n-#             feed_symbols: List of input tensors to be fed\n-#                 symbolic tensors at runtime.\n-#             symbol_vals: List of symbolic tensors to be fed to `feed_symbols`.\n-#             session: Session to use to generate the callable.\n-\n-#         # Returns\n-#             Function that runs the graph according to the above options.\n-#         \"\"\"\n-#         # Prepare callable options.\n-#         callable_opts = config_pb2.CallableOptions()\n-#         # Handle external-data feed.\n-#         for x in feed_arrays:\n-#             callable_opts.feed.append(x.name)\n-#         if self.feed_dict:\n-#             for key in sorted(self.feed_dict.keys()):\n-#                 callable_opts.feed.append(key.name)\n-#         # Handle symbolic feed.\n-#         for x, y in zip(feed_symbols, symbol_vals):\n-#             connection = callable_opts.tensor_connection.add()\n-#             if x.dtype != y.dtype:\n-#                 y = tf.cast(y, dtype=x.dtype)\n-#             from_tensor = tf_ops._as_graph_element(y)\n-#             if from_tensor is None:\n-#                 from_tensor = y\n-#             connection.from_tensor = from_tensor.name  # Data tensor\n-#             connection.to_tensor = x.name  # Placeholder\n-#         # Handle fetches.\n-#         for x in self.outputs + self.fetches:\n-#             callable_opts.fetch.append(x.name)\n-#         # Handle updates.\n-#         callable_opts.target.append(self.updates_op.name)\n-#         # Handle run_options.\n-#         if self.run_options:\n-#             callable_opts.run_options.CopyFrom(self.run_options)\n-#         # Create callable.\n-#         callable_fn = session._make_callable_from_options(callable_opts)\n-#         # Cache parameters corresponding to the generated callable, so that\n-#         # we can detect future mismatches and refresh the callable.\n-#         self._callable_fn = callable_fn\n-#         self._feed_arrays = feed_arrays\n-#         self._feed_symbols = feed_symbols\n-#         self._symbol_vals = symbol_vals\n-#         self._session = session\n-\n-#     def _call(self, inputs):\n-#         if not isinstance(inputs, (list, tuple)):\n-#             raise TypeError('`inputs` should be a list or tuple.')\n-\n-#         session = get_session()\n-#         feed_arrays = []\n-#         array_vals = []\n-#         feed_symbols = []\n-#         symbol_vals = []\n-#         for tensor, value in zip(self.inputs, inputs):\n-#             if value is None:\n-#                 continue\n-#             if is_tensor(value):\n-#                 # Case: feeding symbolic tensor.\n-#                 feed_symbols.append(tensor)\n-#                 symbol_vals.append(value)\n-#             else:\n-#                 feed_arrays.append(tensor)\n-#                 # We need to do array conversion and type casting\n-#                 # at this level, since\n-#                 # `callable_fn` only supports exact matches.\n-#                 array_vals.append(\n-#                     np.asarray(value,\n-#                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n-#         if self.feed_dict:\n-#             for key in sorted(self.feed_dict.keys()):\n-#                 array_vals.append(\n-#                     np.asarray(self.feed_dict[key],\n-#                                dtype=tf.as_dtype(key.dtype).as_numpy_dtype))\n-\n-#         # Refresh callable if anything has changed.\n-#         if (self._callable_fn is None or\n-#                 feed_arrays != self._feed_arrays or\n-#                 symbol_vals != self._symbol_vals or\n-#                 feed_symbols != self._feed_symbols or\n-#                 session != self._session):\n-#             self._make_callable(feed_arrays,\n-#                                 feed_symbols,\n-#                                 symbol_vals,\n-#                                 session)\n-#         if self.run_metadata:\n-#             fetched = self._callable_fn(*array_vals, run_metadata=self.run_metadata)\n-#         else:\n-#             fetched = self._callable_fn(*array_vals)\n-#         return fetched[:len(self.outputs)]\n-\n-#     def _legacy_call(self, inputs):\n-#         if not isinstance(inputs, (list, tuple)):\n-#             raise TypeError('`inputs` should be a list or tuple.')\n-#         feed_dict = self.feed_dict.copy()\n-#         for tensor, value in zip(self.inputs, inputs):\n-#             if is_sparse(tensor):\n-#                 sparse_coo = value.tocoo()\n-#                 indices = np.concatenate(\n-#                     (np.expand_dims(sparse_coo.row, 1),\n-#                      np.expand_dims(sparse_coo.col, 1)), 1)\n-#                 value = (indices, sparse_coo.data, sparse_coo.shape)\n-#             feed_dict[tensor] = value\n-#         fetches = self.outputs + [self.updates_op] + self.fetches\n-#         session = get_session()\n-#         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n-#                               **self.session_kwargs)\n-#         return updated[:len(self.outputs)]\n-\n-#     def __call__(self, inputs):\n-#         if hasattr(get_session(), '_make_callable_from_options'):\n-#             if py_any(is_sparse(x) for x in self.inputs):\n-#                 if py_any(is_tensor(x) for x in inputs):\n-#                     raise ValueError(\n-#                         'Feeding from symbolic tensors is not '\n-#                         'supported with sparse inputs.')\n-#                 return self._legacy_call(inputs)\n-\n-#             # callable generated by Session._make_callable_from_options accepts\n-#             # `run_metadata` keyword argument since TF 1.10\n-#             if self.run_metadata:\n-#                 current_version = StrictVersion(tf.__version__.split('-')[0])\n-#                 if current_version < StrictVersion('1.10.0'):\n-#                     if py_any(is_tensor(x) for x in inputs):\n-#                         raise ValueError(\n-#                             'In order to feed symbolic tensors '\n-#                             'to a Keras model and set '\n-#                             '`run_metadata`, you need tensorflow 1.10 or higher.')\n-#                     return self._legacy_call(inputs)\n-\n-#             return self._call(inputs)\n-#         else:\n-#             if py_any(is_tensor(x) for x in inputs):\n-#                 raise ValueError(\n-#                     'In order to feed symbolic tensors to a Keras model '\n-#                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n-#             return self._legacy_call(inputs)\n-\n-\n-# def function(inputs, outputs, updates=None, **kwargs):\n-#     \"\"\"Instantiates a Keras function.\n-\n-#     # Arguments\n-#         inputs: List of placeholder tensors.\n-#         outputs: List of output tensors.\n-#         updates: List of update ops.\n-#         **kwargs: Passed to `tf.Session.run`.\n-\n-#     # Returns\n-#         Output values as Numpy arrays.\n-\n-#     # Raises\n-#         ValueError: if invalid kwargs are passed in.\n-#     \"\"\"\n-#     if kwargs:\n-#         for key in kwargs:\n-#             session_has_key = has_arg(tf.Session.run, key, True)\n-#             function_has_key = has_arg(Function.__init__, key, True)\n-#             if not (session_has_key or function_has_key):\n-#                 raise ValueError('Invalid argument \"%s\" passed to K.function '\n-#                                  'with TensorFlow backend' % key)\n-#     return Function(inputs, outputs, updates=updates, **kwargs)\n-\n def function(inputs, outputs, updates=None, **kwargs):\n     return tf_keras_backend.function(inputs, outputs,\n                                      updates=updates,\n@@ -3843,8 +3473,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n     padding = _preprocess_padding(padding)\n     x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n     x = tf.nn.convolution(\n-        input=x,\n-        filter=kernel,\n+        x, kernel,\n         dilation_rate=(dilation_rate,),\n         strides=(strides,),\n         padding=padding,\n@@ -3883,8 +3512,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n \n     padding = _preprocess_padding(padding)\n     x = tf.nn.convolution(\n-        input=x,\n-        filter=kernel,\n+        x, kernel,\n         dilation_rate=dilation_rate,\n         strides=strides,\n         padding=padding,\n@@ -4125,8 +3753,7 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n     x = tf.nn.convolution(\n-        input=x,\n-        filter=kernel,\n+        x, kernel,\n         dilation_rate=dilation_rate,\n         strides=strides,\n         padding=padding,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d857be62bbeabf1ba6374492d13d63a097891ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 326 | Lines Deleted: 367 | Files Changed: 10 | Hunks: 201 | Methods Changed: 44 | Complexity Δ (Sum/Max): -9/5 | Churn Δ: 693 | Churn Cumulative: 37674 | Contributors (this commit): 256 | Commits (past 90d): 39 | Contributors (cumulative): 365 | DMM Complexity: 1.0\n\nDIFF:\n@@ -147,6 +147,8 @@ from .load_backend import local_conv2d\n from .load_backend import backend\r\n from .load_backend import normalize_data_format\r\n from .load_backend import name_scope\r\n+from .load_backend import symbolic\r\n+from .load_backend import eager\r\n \r\n if backend() == 'theano':\r\n     from .load_backend import pattern_broadcast\r\n\n@@ -179,6 +179,16 @@ def normalize_data_format(value):\n     return data_format\n \n \n+def symbolic(func):\n+    \"\"\"Dummy decorator used in TensorFlow 2.0 to enter the Keras graph.\"\"\"\n+    return func\n+\n+\n+def eager(func):\n+    \"\"\"Dummy decorator used in TensorFlow 2.0 to exit the Keras graph.\"\"\"\n+    return func\n+\n+\n # Legacy methods\n \n def set_image_dim_ordering(dim_ordering):\n\n@@ -12,6 +12,7 @@ from .common import cast_to_floatx\n from .common import image_data_format\n from .common import set_image_data_format\n from .common import normalize_data_format\n+from .common import symbolic, eager\n \n # Set Keras base dir path given KERAS_HOME env variable, if applicable.\n # Otherwise either ~/.keras or /tmp.\n\n@@ -3,15 +3,20 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n+from tensorflow.python.eager import context\n from tensorflow.python.framework import ops as tf_ops\n+from tensorflow.python.ops import image_ops as tf_image_ops\n from tensorflow.python.keras import backend as tf_keras_backend\n from tensorflow.python.training import moving_averages\n from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import functional_ops\n from tensorflow.python.ops import ctc_ops as ctc\n+from .common import floatx, epsilon, image_data_format\n \n import functools\n+import threading\n+import contextlib\n \n import numpy as np\n from distutils.version import StrictVersion\n@@ -31,13 +36,23 @@ py_slice = slice\n # We assume our devices don't change during our lifetime.\n _LOCAL_DEVICES = None\n \n+_SYMBOLIC_SCOPE = threading.local()\n+_SYMBOLIC_SCOPE.value = True\n+\n \n def _is_tf_1():\n     return tf.__version__.startswith('1.')\n \n+tf_keras_backend.set_floatx(floatx())\n+tf_keras_backend.set_epsilon(epsilon())\n+tf_keras_backend.set_image_data_format(image_data_format())\n \n floatx = tf_keras_backend.floatx\n epsilon = tf_keras_backend.epsilon\n+image_data_format = tf_keras_backend.image_data_format\n+set_floatx = tf_keras_backend.set_floatx\n+set_epsilon = tf_keras_backend.set_floatx\n+set_image_data_format = tf_keras_backend.set_image_data_format\n \n get_graph = tf_keras_backend.get_graph\n get_uid = tf_keras_backend.get_uid\n@@ -45,20 +60,68 @@ reset_uids = tf_keras_backend.reset_uids\n clear_session = tf_keras_backend.clear_session\n manual_variable_initialization = tf_keras_backend.manual_variable_initialization\n \n-learning_phase = tf_keras_backend.learning_phase\n+# TODO\n learning_phase_scope = tf_keras_backend.learning_phase_scope\n-set_learning_phase = tf_keras_backend.set_learning_phase\n name_scope = tf.name_scope\n \n \n-def tf_graph_op(func):\n+def symbolic(func):\n     @functools.wraps(func)\n     def wrapper(*args, **kwargs):\n+        if _SYMBOLIC_SCOPE.value:\n             with get_graph().as_default():\n                 return func(*args, **kwargs)\n+        else:\n+            return func(*args, **kwargs)\n     return wrapper\n \n \n+def eager(func):\n+    global _SYMBOLIC_SCOPE\n+\n+    @functools.wraps(func)\n+    def wrapper(*args, **kwargs):\n+        prev_value = _SYMBOLIC_SCOPE.value\n+        try:\n+            _SYMBOLIC_SCOPE.value = False\n+            with context.eager_mode():\n+                out = func(*args, **kwargs)\n+        finally:\n+            _SYMBOLIC_SCOPE.value = prev_value\n+        return out\n+    return wrapper\n+\n+\n+@symbolic\n+def learning_phase():\n+    \"\"\"Returns the learning phase flag.\n+\n+    The learning phase flag is a bool tensor (0 = test, 1 = train)\n+    to be passed as input to any Keras function\n+    that uses a different behavior at train time and test time.\n+\n+    # Returns\n+        Learning phase (scalar integer tensor or Python integer).\n+    \"\"\"\n+    lp = tf_keras_backend.learning_phase()\n+    if isinstance(lp, tf_ops.Tensor) and lp.dtype.name == 'bool':\n+        return tf.cast(lp, 'int32')\n+    return lp\n+\n+\n+@symbolic\n+def set_learning_phase(value):\n+    \"\"\"Sets the learning phase to a fixed value.\n+\n+    # Arguments\n+        value: Learning phase value, either 0 or 1 (integers).\n+\n+    # Raises\n+        ValueError: if `value` is neither `0` nor `1`.\n+    \"\"\"\n+    tf_keras_backend.set_learning_phase(value)\n+\n+\n def get_session():\n     \"\"\"Returns the TF session to be used by the backend.\n \n@@ -77,11 +140,11 @@ def get_session():\n     \"\"\"\n     if not _is_tf_1():\n         raise RuntimeError(\n-            '`set_session` is not available '\n+            '`get_session` is not available '\n             'when using TensorFlow 2.0.')\n     if tf.executing_eagerly():\n         raise RuntimeError(\n-            '`set_session` is not available when '\n+            '`get_session` is not available when '\n             'TensorFlow is executing eagerly.')\n     return tf_keras_backend.get_session()\n \n@@ -98,7 +161,7 @@ def set_session(session):\n             'when using TensorFlow 2.0.')\n     if tf.executing_eagerly():\n         raise RuntimeError(\n-            '`get_session` is not available when '\n+            '`set_session` is not available when '\n             'TensorFlow is executing eagerly.')\n     tf_keras_backend.set_session(session)\n \n@@ -143,11 +206,11 @@ def _is_current_explicit_device(device_type):\n     # Raises\n         ValueError: If the `device_type` string indicates an unsupported device.\n     \"\"\"\n-    device_type = device_type.upper()\n-    if device_type not in ['CPU', 'GPU']:\n-        raise ValueError('`device_type` should be either \"CPU\" or \"GPU\".')\n+    device_type = device_type.lower()\n+    if device_type not in ['cpu', 'gpu']:\n+        raise ValueError('`device_type` should be either \"cpu\" or \"gpu\".')\n     device = _get_current_tf_device()\n-    return (device is not None and device.device_type == device_type.upper())\n+    return (device is not None and device.device_type.lower() == device_type)\n \n \n def _get_available_gpus():\n@@ -158,8 +221,12 @@ def _get_available_gpus():\n     \"\"\"\n     global _LOCAL_DEVICES\n     if _LOCAL_DEVICES is None:\n-        _LOCAL_DEVICES = get_session().list_devices()\n-    return [x.name for x in _LOCAL_DEVICES if x.device_type == 'GPU']\n+        if _is_tf_1():\n+            devices = get_session().list_devices()\n+            _LOCAL_DEVICES = [x.name for x in devices]\n+        else:\n+            _LOCAL_DEVICES = tf.config.experimental_list_devices()\n+    return [x for x in _LOCAL_DEVICES if 'device:gpu' in x.lower()]\n \n \n def _has_nchw_support():\n@@ -173,14 +240,14 @@ def _has_nchw_support():\n     # Returns\n         bool: if the current scope device placement would support nchw\n     \"\"\"\n-    explicitly_on_cpu = _is_current_explicit_device('CPU')\n+    explicitly_on_cpu = _is_current_explicit_device('cpu')\n     gpus_available = len(_get_available_gpus()) > 0\n     return (not explicitly_on_cpu and gpus_available)\n \n \n # VARIABLE MANIPULATION\n \n-@tf_graph_op\n+@symbolic\n def _to_tensor(x, dtype):\n     \"\"\"Convert the input `x` to a tensor of type `dtype`.\n \n@@ -217,7 +284,7 @@ def is_sparse(tensor):\n     return isinstance(tensor, tf.SparseTensor)\n \n \n-@tf_graph_op\n+@symbolic\n def to_dense(tensor):\n     \"\"\"Converts a sparse tensor into a dense tensor and returns it.\n \n@@ -273,7 +340,6 @@ def variable(value, dtype=None, name=None, constraint=None):\n     \"\"\"\n     if dtype is None:\n         dtype = floatx()\n-    with tf_ops.init_scope():\n     v = tf_keras_backend.variable(\n         value, dtype=dtype, name=name, constraint=constraint)\n     if hasattr(value, 'tocoo'):\n@@ -286,7 +352,6 @@ def variable(value, dtype=None, name=None, constraint=None):\n     return v\n \n \n-@tf_graph_op\n def constant(value, dtype=None, shape=None, name=None):\n     \"\"\"Creates a constant tensor.\n \n@@ -359,7 +424,7 @@ def is_tensor(x):\n     return isinstance(x, tf_ops._TensorLike) or tf_ops.is_dense_tensor_like(x)\n \n \n-@tf_graph_op\n+@symbolic\n def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     \"\"\"Instantiates a placeholder tensor and returns it.\n \n@@ -395,7 +460,7 @@ def placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None):\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def is_placeholder(x):\n     \"\"\"Returns whether `x` is a placeholder.\n \n@@ -534,10 +599,10 @@ def dtype(x):\n \n \n def eval(x):\n-    \"\"\"Evaluates the value of a variable.\n+    \"\"\"Evaluates the value of a tensor.\n \n     # Arguments\n-        x: A variable.\n+        x: A tensor.\n \n     # Returns\n         A Numpy array.\n@@ -552,11 +617,12 @@ def eval(x):\n     ```\n     {{np_implementation}}\n     \"\"\"\n-    # TODO\n-    return to_dense(x).eval(session=get_session())\n+    if hasattr(x, 'numpy'):\n+        return x.numpy()\n+    eval_fn = function([], [x])\n+    return eval_fn([])[0]\n \n \n-@tf_graph_op\n def zeros(shape, dtype=None, name=None):\n     \"\"\"Instantiates an all-zeros variable and returns it.\n \n@@ -590,7 +656,6 @@ def zeros(shape, dtype=None, name=None):\n         return v\n \n \n-@tf_graph_op\n def ones(shape, dtype=None, name=None):\n     \"\"\"Instantiates an all-ones variable and returns it.\n \n@@ -624,7 +689,6 @@ def ones(shape, dtype=None, name=None):\n         return v\n \n \n-@tf_graph_op\n def eye(size, dtype=None, name=None):\n     \"\"\"Instantiate an identity matrix and returns it.\n \n@@ -653,7 +717,7 @@ def eye(size, dtype=None, name=None):\n         return variable(tf.eye(size, dtype=dtype), dtype, name)\n \n \n-@tf_graph_op\n+@symbolic\n def zeros_like(x, dtype=None, name=None):\n     \"\"\"Instantiates an all-zeros variable of the same shape as another tensor.\n \n@@ -680,7 +744,7 @@ def zeros_like(x, dtype=None, name=None):\n     return tf.zeros_like(x, dtype=dtype, name=name)\n \n \n-@tf_graph_op\n+@symbolic\n def ones_like(x, dtype=None, name=None):\n     \"\"\"Instantiates an all-ones variable of the same shape as another tensor.\n \n@@ -707,7 +771,7 @@ def ones_like(x, dtype=None, name=None):\n     return tf.ones_like(x, dtype=dtype, name=name)\n \n \n-@tf_graph_op\n+@symbolic\n def identity(x, name=None):\n     \"\"\"Returns a tensor with the same content as the input tensor.\n \n@@ -757,7 +821,7 @@ def random_uniform_variable(shape, low, high,\n         seed = np.random.randint(10e8)\n     with tf_ops.init_scope():\n         value = tf.random_uniform_initializer(\n-            low, high, dtype=dtype, seed=seed)(shape)\n+            low, high, seed=seed)(shape, dtype=dtype)\n         return variable(value, dtype=dtype, name=name)\n \n \n@@ -795,7 +859,7 @@ def random_normal_variable(shape, mean, scale, dtype=None,\n         seed = np.random.randint(10e8)\n     with tf_ops.init_scope():\n         value = tf.random_normal_initializer(\n-            mean, scale, dtype=dtype, seed=seed)(shape)\n+            mean, scale, seed=seed)(shape, dtype=dtype)\n         return variable(value, dtype=dtype, name=name)\n \n \n@@ -823,7 +887,7 @@ def count_params(x):\n     return np.prod(int_shape(x))\n \n \n-@tf_graph_op\n+@symbolic\n def cast(x, dtype):\n     \"\"\"Casts a tensor to a different dtype and returns it.\n \n@@ -859,7 +923,7 @@ def cast(x, dtype):\n # UPDATES OPS\n \n \n-@tf_graph_op\n+@symbolic\n def update(x, new_x):\n     \"\"\"Update the value of `x` to `new_x`.\n \n@@ -873,7 +937,7 @@ def update(x, new_x):\n     return tf_keras_backend.update(x, new_x)\n \n \n-@tf_graph_op\n+@symbolic\n def update_add(x, increment):\n     \"\"\"Update the value of `x` by adding `increment`.\n \n@@ -887,7 +951,7 @@ def update_add(x, increment):\n     return tf_keras_backend.update_add(x, increment)\n \n \n-@tf_graph_op\n+@symbolic\n def update_sub(x, decrement):\n     \"\"\"Update the value of `x` by subtracting `decrement`.\n \n@@ -901,7 +965,7 @@ def update_sub(x, decrement):\n     return tf_keras_backend.update_sub(x, decrement)\n \n \n-@tf_graph_op\n+@symbolic\n def moving_average_update(x, value, momentum):\n     \"\"\"Compute the moving average of a variable.\n \n@@ -921,7 +985,7 @@ def moving_average_update(x, value, momentum):\n \n # LINEAR ALGEBRA\n \n-@tf_graph_op\n+@symbolic\n def dot(x, y):\n     \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n \n@@ -987,13 +1051,13 @@ def dot(x, y):\n         return tf.reshape(tf.matmul(xt, yt),\n                           x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n     if is_sparse(x):\n-        out = tf.sparse_tensor_dense_matmul(x, y)\n+        out = tf.matmul(x, y)\n     else:\n         out = tf.matmul(x, y)\n     return out\n \n \n-@tf_graph_op\n+@symbolic\n def batch_dot(x, y, axes=None):\n     \"\"\"Batchwise dot product.\n \n@@ -1198,7 +1262,7 @@ def batch_dot(x, y, axes=None):\n     return result\n \n \n-@tf_graph_op\n+@symbolic\n def transpose(x):\n     \"\"\"Transposes a tensor and returns it.\n \n@@ -1235,7 +1299,7 @@ def transpose(x):\n     return tf.transpose(x)\n \n \n-@tf_graph_op\n+@symbolic\n def gather(reference, indices):\n     \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n \n@@ -1254,7 +1318,7 @@ def gather(reference, indices):\n # ELEMENT-WISE OPERATIONS\n \n \n-@tf_graph_op\n+@symbolic\n def max(x, axis=None, keepdims=False):\n     \"\"\"Maximum value in a tensor.\n \n@@ -1276,7 +1340,7 @@ def max(x, axis=None, keepdims=False):\n     return tf.reduce_max(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def min(x, axis=None, keepdims=False):\n     \"\"\"Minimum value in a tensor.\n \n@@ -1298,7 +1362,7 @@ def min(x, axis=None, keepdims=False):\n     return tf.reduce_min(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def sum(x, axis=None, keepdims=False):\n     \"\"\"Sum of the values in a tensor, alongside the specified axis.\n \n@@ -1320,7 +1384,7 @@ def sum(x, axis=None, keepdims=False):\n     return tf.reduce_sum(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def prod(x, axis=None, keepdims=False):\n     \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n \n@@ -1342,7 +1406,7 @@ def prod(x, axis=None, keepdims=False):\n     return tf.reduce_prod(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def cumsum(x, axis=0):\n     \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n \n@@ -1357,7 +1421,7 @@ def cumsum(x, axis=0):\n     return tf.cumsum(x, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def cumprod(x, axis=0):\n     \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n \n@@ -1372,7 +1436,7 @@ def cumprod(x, axis=0):\n     return tf.cumprod(x, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def var(x, axis=None, keepdims=False):\n     \"\"\"Variance of a tensor, alongside the specified axis.\n \n@@ -1399,7 +1463,7 @@ def var(x, axis=None, keepdims=False):\n                           keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def std(x, axis=None, keepdims=False):\n     \"\"\"Standard deviation of a tensor, alongside the specified axis.\n \n@@ -1420,7 +1484,7 @@ def std(x, axis=None, keepdims=False):\n     return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n \n \n-@tf_graph_op\n+@symbolic\n def mean(x, axis=None, keepdims=False):\n     \"\"\"Mean of a tensor, alongside the specified axis.\n \n@@ -1443,7 +1507,7 @@ def mean(x, axis=None, keepdims=False):\n     return tf.reduce_mean(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def any(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical OR).\n \n@@ -1480,7 +1544,7 @@ def all(x, axis=None, keepdims=False):\n     return tf.reduce_all(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def argmax(x, axis=-1):\n     \"\"\"Returns the index of the maximum value along an axis.\n \n@@ -1495,7 +1559,7 @@ def argmax(x, axis=-1):\n     return tf.argmax(x, axis)\n \n \n-@tf_graph_op\n+@symbolic\n def argmin(x, axis=-1):\n     \"\"\"Returns the index of the minimum value along an axis.\n \n@@ -1510,7 +1574,7 @@ def argmin(x, axis=-1):\n     return tf.argmin(x, axis)\n \n \n-@tf_graph_op\n+@symbolic\n def square(x):\n     \"\"\"Element-wise square.\n \n@@ -1523,7 +1587,7 @@ def square(x):\n     return tf.square(x)\n \n \n-@tf_graph_op\n+@symbolic\n def abs(x):\n     \"\"\"Element-wise absolute value.\n \n@@ -1536,7 +1600,7 @@ def abs(x):\n     return tf.abs(x)\n \n \n-@tf_graph_op\n+@symbolic\n def sqrt(x):\n     \"\"\"Element-wise square root.\n \n@@ -1553,7 +1617,7 @@ def sqrt(x):\n     return tf.sqrt(x)\n \n \n-@tf_graph_op\n+@symbolic\n def exp(x):\n     \"\"\"Element-wise exponential.\n \n@@ -1566,7 +1630,7 @@ def exp(x):\n     return tf.exp(x)\n \n \n-@tf_graph_op\n+@symbolic\n def log(x):\n     \"\"\"Element-wise log.\n \n@@ -1579,7 +1643,7 @@ def log(x):\n     return tf.log(x)\n \n \n-@tf_graph_op\n+@symbolic\n def logsumexp(x, axis=None, keepdims=False):\n     \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n \n@@ -1604,7 +1668,7 @@ def logsumexp(x, axis=None, keepdims=False):\n     return tf.reduce_logsumexp(x, axis, keepdims)\n \n \n-@tf_graph_op\n+@symbolic\n def round(x):\n     \"\"\"Element-wise rounding to the closest integer.\n \n@@ -1619,7 +1683,7 @@ def round(x):\n     return tf.round(x)\n \n \n-@tf_graph_op\n+@symbolic\n def sign(x):\n     \"\"\"Element-wise sign.\n \n@@ -1632,7 +1696,7 @@ def sign(x):\n     return tf.sign(x)\n \n \n-@tf_graph_op\n+@symbolic\n def pow(x, a):\n     \"\"\"Element-wise exponentiation.\n \n@@ -1647,7 +1711,7 @@ def pow(x, a):\n     return tf.pow(x, a)\n \n \n-@tf_graph_op\n+@symbolic\n def clip(x, min_value, max_value):\n     \"\"\"Element-wise value clipping.\n \n@@ -1671,7 +1735,7 @@ def clip(x, min_value, max_value):\n     return tf.clip_by_value(x, min_value, max_value)\n \n \n-@tf_graph_op\n+@symbolic\n def equal(x, y):\n     \"\"\"Element-wise equality between two tensors.\n \n@@ -1687,7 +1751,7 @@ def equal(x, y):\n     return tf.equal(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def not_equal(x, y):\n     \"\"\"Element-wise inequality between two tensors.\n \n@@ -1703,7 +1767,7 @@ def not_equal(x, y):\n     return tf.not_equal(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def greater(x, y):\n     \"\"\"Element-wise truth value of (x > y).\n \n@@ -1719,7 +1783,7 @@ def greater(x, y):\n     return tf.greater(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def greater_equal(x, y):\n     \"\"\"Element-wise truth value of (x >= y).\n \n@@ -1735,7 +1799,7 @@ def greater_equal(x, y):\n     return tf.greater_equal(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def less(x, y):\n     \"\"\"Element-wise truth value of (x < y).\n \n@@ -1751,7 +1815,7 @@ def less(x, y):\n     return tf.less(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def less_equal(x, y):\n     \"\"\"Element-wise truth value of (x <= y).\n \n@@ -1767,7 +1831,7 @@ def less_equal(x, y):\n     return tf.less_equal(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def maximum(x, y):\n     \"\"\"Element-wise maximum of two tensors.\n \n@@ -1783,7 +1847,7 @@ def maximum(x, y):\n     return tf.maximum(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def minimum(x, y):\n     \"\"\"Element-wise minimum of two tensors.\n \n@@ -1799,7 +1863,7 @@ def minimum(x, y):\n     return tf.minimum(x, y)\n \n \n-@tf_graph_op\n+@symbolic\n def sin(x):\n     \"\"\"Computes sin of x element-wise.\n \n@@ -1812,7 +1876,7 @@ def sin(x):\n     return tf.sin(x)\n \n \n-@tf_graph_op\n+@symbolic\n def cos(x):\n     \"\"\"Computes cos of x element-wise.\n \n@@ -1938,7 +2002,7 @@ def _fused_normalize_batch_in_training(x, gamma, beta, reduction_axes,\n         data_format=tf_data_format)\n \n \n-@tf_graph_op\n+@symbolic\n def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n     \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n@@ -1973,7 +2037,7 @@ def normalize_batch_in_training(x, gamma, beta,\n                                                           epsilon=epsilon)\n \n \n-@tf_graph_op\n+@symbolic\n def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n     \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n \n@@ -2047,7 +2111,7 @@ def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n \n # SHAPE OPERATIONS\n \n-@tf_graph_op\n+@symbolic\n def concatenate(tensors, axis=-1):\n     \"\"\"Concatenates a list of tensors alongside the specified axis.\n \n@@ -2064,14 +2128,10 @@ def concatenate(tensors, axis=-1):\n             axis %= rank\n         else:\n             axis = 0\n-\n-    if py_all([is_sparse(x) for x in tensors]):\n-        return tf.sparse_concat(axis, tensors)\n-    else:\n-        return tf.concat([to_dense(x) for x in tensors], axis)\n+    return tf.concat(tensors, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def reshape(x, shape):\n     \"\"\"Reshapes a tensor to the specified shape.\n \n@@ -2085,7 +2145,7 @@ def reshape(x, shape):\n     return tf.reshape(x, shape)\n \n \n-@tf_graph_op\n+@symbolic\n def permute_dimensions(x, pattern):\n     \"\"\"Permutes axes in a tensor.\n \n@@ -2100,7 +2160,7 @@ def permute_dimensions(x, pattern):\n     return tf.transpose(x, perm=pattern)\n \n \n-@tf_graph_op\n+@symbolic\n def resize_images(x,\n                   height_factor,\n                   width_factor,\n@@ -2134,9 +2194,9 @@ def resize_images(x,\n     if data_format == 'channels_first':\n         x = permute_dimensions(x, [0, 2, 3, 1])\n     if interpolation == 'nearest':\n-        x = tf.image.resize_nearest_neighbor(x, new_shape)\n+        x = tf_image_ops.resize_nearest_neighbor(x, new_shape)\n     elif interpolation == 'bilinear':\n-        x = tf.image.resize_bilinear(x, new_shape)\n+        x = tf_image_ops.resize_bilinear(x, new_shape)\n     else:\n         raise ValueError('interpolation should be one '\n                          'of \"nearest\" or \"bilinear\".')\n@@ -2159,7 +2219,7 @@ def resize_images(x,\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n     \"\"\"Resizes the volume contained in a 5D tensor.\n \n@@ -2191,7 +2251,7 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n         raise ValueError('Unknown data_format: ' + str(data_format))\n \n \n-@tf_graph_op\n+@symbolic\n def repeat_elements(x, rep, axis):\n     \"\"\"Repeats the elements of a tensor along an axis, like `np.repeat`.\n \n@@ -2242,7 +2302,7 @@ def repeat_elements(x, rep, axis):\n     return x_rep\n \n \n-@tf_graph_op\n+@symbolic\n def repeat(x, n):\n     \"\"\"Repeats a 2D tensor.\n \n@@ -2262,7 +2322,7 @@ def repeat(x, n):\n     return tf.tile(x, pattern)\n \n \n-@tf_graph_op\n+@symbolic\n def arange(start, stop=None, step=1, dtype='int32'):\n     \"\"\"Creates a 1D tensor containing a sequence of integers.\n \n@@ -2300,7 +2360,7 @@ def arange(start, stop=None, step=1, dtype='int32'):\n     return result\n \n \n-@tf_graph_op\n+@symbolic\n def tile(x, n):\n     \"\"\"Creates a tensor by tiling `x` by `n`.\n \n@@ -2317,7 +2377,7 @@ def tile(x, n):\n     return tf.tile(x, n)\n \n \n-@tf_graph_op\n+@symbolic\n def flatten(x):\n     \"\"\"Flatten a tensor.\n \n@@ -2330,7 +2390,7 @@ def flatten(x):\n     return tf.reshape(x, [-1])\n \n \n-@tf_graph_op\n+@symbolic\n def batch_flatten(x):\n     \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n \n@@ -2346,7 +2406,7 @@ def batch_flatten(x):\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def expand_dims(x, axis=-1):\n     \"\"\"Adds a 1-sized dimension at index \"axis\".\n \n@@ -2360,7 +2420,7 @@ def expand_dims(x, axis=-1):\n     return tf.expand_dims(x, axis)\n \n \n-@tf_graph_op\n+@symbolic\n def squeeze(x, axis):\n     \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n \n@@ -2374,7 +2434,7 @@ def squeeze(x, axis):\n     return tf.squeeze(x, [axis])\n \n \n-@tf_graph_op\n+@symbolic\n def temporal_padding(x, padding=(1, 1)):\n     \"\"\"Pads the middle dimension of a 3D tensor.\n \n@@ -2391,7 +2451,7 @@ def temporal_padding(x, padding=(1, 1)):\n     return tf.pad(x, pattern)\n \n \n-@tf_graph_op\n+@symbolic\n def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n \n@@ -2420,7 +2480,7 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     return tf.pad(x, pattern)\n \n \n-@tf_graph_op\n+@symbolic\n def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     \"\"\"Pads 5D tensor with zeros along the depth, height, width dimensions.\n \n@@ -2463,7 +2523,7 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     return tf.pad(x, pattern)\n \n \n-@tf_graph_op\n+@symbolic\n def stack(x, axis=0):\n     \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n \n@@ -2479,7 +2539,7 @@ def stack(x, axis=0):\n     return tf.stack(x, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def one_hot(indices, num_classes):\n     \"\"\"Computes the one-hot representation of an integer tensor.\n \n@@ -2495,7 +2555,7 @@ def one_hot(indices, num_classes):\n     return tf.one_hot(indices, depth=num_classes, axis=-1)\n \n \n-@tf_graph_op\n+@symbolic\n def reverse(x, axes):\n     \"\"\"Reverses a tensor along the specified axes.\n \n@@ -2514,7 +2574,7 @@ def reverse(x, axes):\n     return tf.reverse(x, axes)\n \n \n-@tf_graph_op\n+@symbolic\n def slice(x, start, size):\n     \"\"\"Extracts a slice from a tensor.\n \n@@ -2550,7 +2610,10 @@ def get_value(x):\n     # Returns\n         A Numpy array.\n     \"\"\"\n-    return tf_keras_backend.get_value(x)\n+    if _is_tf_1():\n+        return x.eval(session=get_session((x,)))\n+    else:\n+        return x.numpy()\n \n \n def batch_get_value(ops):\n@@ -2599,6 +2662,7 @@ def get_variable_shape(x):\n     return int_shape(x)\n \n \n+@symbolic\n def print_tensor(x, message=''):\n     \"\"\"Prints `message` and the tensor value when evaluated.\n \n@@ -2630,7 +2694,7 @@ def function(inputs, outputs, updates=None, **kwargs):\n                                      **kwargs)\n \n \n-@tf_graph_op\n+@symbolic\n def gradients(loss, variables):\n     \"\"\"Returns the gradients of `loss` w.r.t. `variables`.\n \n@@ -2646,7 +2710,7 @@ def gradients(loss, variables):\n     return tf.gradients(loss, variables)\n \n \n-@tf_graph_op\n+@symbolic\n def stop_gradient(variables):\n     \"\"\"Returns `variables` but with zero gradient w.r.t. every other variable.\n \n@@ -2665,7 +2729,8 @@ def stop_gradient(variables):\n \n \n # CONTROL FLOW\n-@tf_graph_op  # TODO\n+\n+@symbolic\n def rnn(step_function, inputs, initial_states,\n         go_backwards=False, mask=None, constants=None,\n         unroll=False, input_length=None):\n@@ -2715,206 +2780,15 @@ def rnn(step_function, inputs, initial_states,\n \n     {{np_implementation}}\n     \"\"\"\n-    ndim = len(inputs.shape)\n-    if ndim < 3:\n-        raise ValueError('Input should be at least 3D.')\n-\n-    # Transpose to time-major, i.e.\n-    # from (batch, time, ...) to (time, batch, ...)\n-    axes = [1, 0] + list(range(2, ndim))\n-    inputs = tf.transpose(inputs, (axes))\n-\n-    if mask is not None:\n-        if mask.dtype != tf.bool:\n-            mask = tf.cast(mask, tf.bool)\n-        if len(mask.shape) != 2:\n-            raise ValueError(\n-                'mask should have `shape=(samples, time)`, '\n-                'got {}'.format(mask.shape))\n-        mask = tf.transpose(mask, [1, 0])\n-\n-        def get_matching_mask(mask_t, ref_tensor_t):\n-            # tf.where needs its condition tensor\n-            # to be the same shape as its two\n-            # result tensors\n-            ndim = len(ref_tensor_t.shape)\n-            for _ in range(ndim - 1):\n-                mask_t = expand_dims(mask_t)\n-            add_shape = tf.shape(ref_tensor_t)[1:]\n-            multiple = tf.concat([[1], add_shape], 0)\n-            return tf.tile(mask_t, multiple)\n-\n-    if constants is None:\n-        constants = []\n-\n-    uses_learning_phase = [False]\n-\n-    if unroll:\n-        if not inputs.shape[0]:\n-            raise ValueError('Unrolling requires a '\n-                             'fixed number of timesteps.')\n-        states = initial_states\n-        successive_states = []\n-        successive_outputs = []\n-\n-        input_list = tf.unstack(inputs)\n-        if go_backwards:\n-            input_list.reverse()\n-\n-        if mask is not None:\n-            mask_list = tf.unstack(mask)\n-            if go_backwards:\n-                mask_list.reverse()\n-\n-            for inp, mask_t in zip(input_list, mask_list):\n-                output, new_states = step_function(inp, states + constants)\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-\n-                if not successive_outputs:\n-                    prev_output = zeros_like(output)\n-                else:\n-                    prev_output = successive_outputs[-1]\n-\n-                output_mask_t = get_matching_mask(mask_t, output)\n-                output = tf.where(output_mask_t, output, prev_output)\n-\n-                return_states = []\n-                for state, new_state in zip(states, new_states):\n-                    state_mask_t = get_matching_mask(mask_t, new_state)\n-                    return_states.append(tf.where(state_mask_t,\n-                                                  new_state,\n-                                                  state))\n-                states = return_states\n-                successive_outputs.append(output)\n-                successive_states.append(states)\n-            last_output = successive_outputs[-1]\n-            new_states = successive_states[-1]\n-            outputs = tf.stack(successive_outputs)\n-        else:\n-            for inp in input_list:\n-                output, states = step_function(inp, states + constants)\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-                successive_outputs.append(output)\n-                successive_states.append(states)\n-            last_output = successive_outputs[-1]\n-            new_states = successive_states[-1]\n-            outputs = tf.stack(successive_outputs)\n-\n-    else:\n-        if go_backwards:\n-            inputs = reverse(inputs, 0)\n-\n-        states = tuple(initial_states)\n-\n-        time_steps = tf.shape(inputs)[0]\n-        output, _ = step_function(inputs[0], initial_states + constants)\n-        output_ta = tensor_array_ops.TensorArray(\n-            dtype=output.dtype,\n-            size=time_steps,\n-            tensor_array_name='output_ta')\n-        initial_output = zeros_like(output)\n-        input_ta = tensor_array_ops.TensorArray(\n-            dtype=inputs.dtype,\n-            size=time_steps,\n-            tensor_array_name='input_ta')\n-        input_ta = input_ta.unstack(inputs)\n-        time = tf.constant(0, dtype='int32', name='time')\n-        while_loop_kwargs = {\n-            'cond': lambda time, *_: time < time_steps,\n-            'parallel_iterations': 32,\n-            'swap_memory': True,\n-            'maximum_iterations': input_length}\n-\n-        if mask is not None:\n-            if go_backwards:\n-                mask = reverse(mask, 0)\n-\n-            mask_ta = tensor_array_ops.TensorArray(\n-                dtype=tf.bool,\n-                size=time_steps,\n-                tensor_array_name='mask_ta')\n-            mask_ta = mask_ta.unstack(mask)\n-\n-            def _step(time, output_ta_t, output_tm1, *states):\n-                \"\"\"RNN step function.\n-\n-                # Arguments\n-                    time: Current timestep value.\n-                    output_ta_t: TensorArray.\n-                    output_tm1: output Tensor from previous timestep\n-                    *states: List of states.\n-\n-                # Returns\n-                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n-                \"\"\"\n-                current_input = input_ta.read(time)\n-                mask_t = mask_ta.read(time)\n-                output, new_states = step_function(current_input,\n-                                                   tuple(states) +\n-                                                   tuple(constants))\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-                for state, new_state in zip(states, new_states):\n-                    new_state.set_shape(state.shape)\n-\n-                output_mask_t = get_matching_mask(mask_t, output)\n-                output = tf.where(output_mask_t, output, output_tm1)\n-\n-                new_states = [tf.where(get_matching_mask(mask_t, new_states[i]),\n-                                       new_states[i],\n-                                       states[i]) for i in range(len(states))]\n-\n-                output_ta_t = output_ta_t.write(time, output)\n-                return (time + 1, output_ta_t, output) + tuple(new_states)\n-\n-            final_outputs = control_flow_ops.while_loop(\n-                body=_step,\n-                loop_vars=(time, output_ta, initial_output) + states,\n-                **while_loop_kwargs)\n-            new_states = final_outputs[3:]  # skip output_tm1\n-        else:\n-            def _step(time, output_ta_t, *states):\n-                \"\"\"RNN step function.\n-\n-                # Arguments\n-                    time: Current timestep value.\n-                    output_ta_t: TensorArray.\n-                    *states: List of states.\n-\n-                # Returns\n-                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n-                \"\"\"\n-                current_input = input_ta.read(time)\n-                output, new_states = step_function(current_input,\n-                                                   tuple(states) +\n-                                                   tuple(constants))\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-                for state, new_state in zip(states, new_states):\n-                    new_state.set_shape(state.shape)\n-                output_ta_t = output_ta_t.write(time, output)\n-                return (time + 1, output_ta_t) + tuple(new_states)\n-\n-            final_outputs = control_flow_ops.while_loop(\n-                body=_step,\n-                loop_vars=(time, output_ta) + states,\n-                **while_loop_kwargs)\n-            new_states = final_outputs[2:]\n-\n-        last_time = final_outputs[0]\n-        output_ta = final_outputs[1]\n-        outputs = output_ta.stack()\n-        last_output = output_ta.read(last_time - 1)\n-\n-    axes = [1, 0] + list(range(2, len(outputs.shape)))\n-    outputs = tf.transpose(outputs, axes)\n-    last_output._uses_learning_phase = uses_learning_phase[0]\n-    return last_output, outputs, new_states\n+    return tf_keras_backend.rnn(step_function, inputs, initial_states,\n+                                go_backwards=go_backwards,\n+                                mask=mask,\n+                                constants=constants,\n+                                unroll=unroll,\n+                                input_length=input_length)\n \n \n-@tf_graph_op\n+@symbolic\n def switch(condition, then_expression, else_expression):\n     \"\"\"Switches between two operations depending on a scalar value.\n \n@@ -2979,7 +2853,7 @@ def switch(condition, then_expression, else_expression):\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def in_train_phase(x, alt, training=None):\n     \"\"\"Selects `x` in train phase, and `alt` otherwise.\n \n@@ -3023,7 +2897,7 @@ def in_train_phase(x, alt, training=None):\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def in_test_phase(x, alt, training=None):\n     \"\"\"Selects `x` in test phase, and `alt` otherwise.\n \n@@ -3046,7 +2920,7 @@ def in_test_phase(x, alt, training=None):\n \n # NN OPERATIONS\n \n-@tf_graph_op\n+@symbolic\n def relu(x, alpha=0., max_value=None, threshold=0.):\n     \"\"\"Rectified linear unit.\n \n@@ -3101,7 +2975,7 @@ def relu(x, alpha=0., max_value=None, threshold=0.):\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def elu(x, alpha=1.):\n     \"\"\"Exponential linear unit.\n \n@@ -3121,7 +2995,7 @@ def elu(x, alpha=1.):\n         return tf.where(x > 0, res, alpha * res)\n \n \n-@tf_graph_op\n+@symbolic\n def softmax(x, axis=-1):\n     \"\"\"Softmax of a tensor.\n \n@@ -3138,7 +3012,7 @@ def softmax(x, axis=-1):\n     return tf.nn.softmax(x, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def softplus(x):\n     \"\"\"Softplus of a tensor.\n \n@@ -3153,7 +3027,7 @@ def softplus(x):\n     return tf.nn.softplus(x)\n \n \n-@tf_graph_op\n+@symbolic\n def softsign(x):\n     \"\"\"Softsign of a tensor.\n \n@@ -3168,7 +3042,7 @@ def softsign(x):\n     return tf.nn.softsign(x)\n \n \n-@tf_graph_op\n+@symbolic\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n \n@@ -3195,7 +3069,7 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         target, output, from_logits=from_logits, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"Categorical crossentropy with integer targets.\n \n@@ -3222,7 +3096,7 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         target, output, from_logits=from_logits, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def binary_crossentropy(target, output, from_logits=False):\n     \"\"\"Binary crossentropy between an output tensor and a target tensor.\n \n@@ -3240,7 +3114,7 @@ def binary_crossentropy(target, output, from_logits=False):\n         target, output, from_logits=from_logits)\n \n \n-@tf_graph_op\n+@symbolic\n def sigmoid(x):\n     \"\"\"Element-wise sigmoid.\n \n@@ -3255,7 +3129,7 @@ def sigmoid(x):\n     return tf.nn.sigmoid(x)\n \n \n-@tf_graph_op\n+@symbolic\n def hard_sigmoid(x):\n     \"\"\"Segment-wise linear approximation of sigmoid.\n \n@@ -3274,7 +3148,7 @@ def hard_sigmoid(x):\n     return tf_keras_backend.hard_sigmoid(x)\n \n \n-@tf_graph_op\n+@symbolic\n def tanh(x):\n     \"\"\"Element-wise tanh.\n \n@@ -3289,7 +3163,7 @@ def tanh(x):\n     return tf.nn.tanh(x)\n \n \n-@tf_graph_op\n+@symbolic\n def dropout(x, level, noise_shape=None, seed=None):\n     \"\"\"Sets entries in `x` to zero at random, while scaling the entire tensor.\n \n@@ -3310,7 +3184,7 @@ def dropout(x, level, noise_shape=None, seed=None):\n     return tf.nn.dropout(x, rate=level, noise_shape=noise_shape, seed=seed)\n \n \n-@tf_graph_op\n+@symbolic\n def l2_normalize(x, axis=None):\n     \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n \n@@ -3326,7 +3200,7 @@ def l2_normalize(x, axis=None):\n     return tf.nn.l2_normalize(x, axis=axis)\n \n \n-@tf_graph_op\n+@symbolic\n def in_top_k(predictions, targets, k):\n     \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n \n@@ -3438,7 +3312,7 @@ def _preprocess_padding(padding):\n     return padding\n \n \n-@tf_graph_op\n+@symbolic\n def conv1d(x, kernel, strides=1, padding='valid',\n            data_format=None, dilation_rate=1):\n     \"\"\"1D convolution.\n@@ -3472,19 +3346,27 @@ def conv1d(x, kernel, strides=1, padding='valid',\n         padding = 'valid'\n     padding = _preprocess_padding(padding)\n     x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n+\n+    # TF 2 arg conversion\n+    kwargs = {}\n+    if _is_tf_1():\n+        kwargs['dilation_rate'] = (dilation_rate,)\n+    else:\n+        kwargs['dilations'] = (dilation_rate,)\n+\n     x = tf.nn.convolution(\n         x, kernel,\n-        dilation_rate=(dilation_rate,),\n         strides=(strides,),\n         padding=padding,\n-        data_format=tf_data_format)\n+        data_format=tf_data_format,\n+        **kwargs)\n \n     if data_format == 'channels_first' and tf_data_format == 'NWC':\n         x = tf.transpose(x, (0, 2, 1))  # NWC -> NCW\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def conv2d(x, kernel, strides=(1, 1), padding='valid',\n            data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution.\n@@ -3511,19 +3393,27 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n \n     padding = _preprocess_padding(padding)\n+\n+    # TF 2 arg conversion\n+    kwargs = {}\n+    if _is_tf_1():\n+        kwargs['dilation_rate'] = dilation_rate\n+    else:\n+        kwargs['dilations'] = dilation_rate\n+\n     x = tf.nn.convolution(\n         x, kernel,\n-        dilation_rate=dilation_rate,\n         strides=strides,\n         padding=padding,\n-        data_format=tf_data_format)\n+        data_format=tf_data_format,\n+        **kwargs)\n \n     if data_format == 'channels_first' and tf_data_format == 'NHWC':\n         x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D deconvolution (i.e. transposed convolution).\n@@ -3587,7 +3477,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                      padding='valid', data_format=None, dilation_rate=1):\n     \"\"\"1D convolution with separable filters.\n@@ -3631,11 +3521,18 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     pointwise_kernel = tf.expand_dims(pointwise_kernel, 0)\n     dilation_rate = (1,) + dilation_rate\n \n+    # TF 2 arg conversion\n+    kwargs = {}\n+    if _is_tf_1():\n+        kwargs['rate'] = dilation_rate\n+    else:\n+        kwargs['dilations'] = dilation_rate\n+\n     x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                                strides=strides,\n                                padding=padding,\n-                               rate=dilation_rate,\n-                               data_format=tf_data_format)\n+                               data_format=tf_data_format,\n+                               **kwargs)\n \n     x = tf.squeeze(x, [spatial_start_dim])\n \n@@ -3645,7 +3542,7 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution with separable filters.\n@@ -3676,17 +3573,24 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n     else:\n         strides = (1, 1) + strides\n \n+    # TF 2 arg conversion\n+    kwargs = {}\n+    if _is_tf_1():\n+        kwargs['rate'] = dilation_rate\n+    else:\n+        kwargs['dilations'] = dilation_rate\n+\n     x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                                strides=strides,\n                                padding=padding,\n-                               rate=dilation_rate,\n-                               data_format=tf_data_format)\n+                               data_format=tf_data_format,\n+                               **kwargs)\n     if data_format == 'channels_first' and tf_data_format == 'NHWC':\n         x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                      data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution with separable filters.\n@@ -3716,17 +3620,24 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n     else:\n         strides = (1, 1) + strides\n \n+    # TF 2 arg conversion\n+    kwargs = {}\n+    if _is_tf_1():\n+        kwargs['rate'] = dilation_rate\n+    else:\n+        kwargs['dilations'] = dilation_rate\n+\n     x = tf.nn.depthwise_conv2d(x, depthwise_kernel,\n                                strides=strides,\n                                padding=padding,\n-                               rate=dilation_rate,\n-                               data_format=tf_data_format)\n+                               data_format=tf_data_format,\n+                               **kwargs)\n     if data_format == 'channels_first' and tf_data_format == 'NHWC':\n         x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n            data_format=None, dilation_rate=(1, 1, 1)):\n     \"\"\"3D convolution.\n@@ -3752,18 +3663,26 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n \n     x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n+\n+    # TF 2 arg conversion\n+    kwargs = {}\n+    if _is_tf_1():\n+        kwargs['dilation_rate'] = dilation_rate\n+    else:\n+        kwargs['dilations'] = dilation_rate\n+\n     x = tf.nn.convolution(\n         x, kernel,\n-        dilation_rate=dilation_rate,\n         strides=strides,\n         padding=padding,\n-        data_format=tf_data_format)\n+        data_format=tf_data_format,\n+        **kwargs)\n     if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n         x = tf.transpose(x, (0, 4, 1, 2, 3))\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                      padding='valid', data_format=None):\n     \"\"\"3D deconvolution (i.e. transposed convolution).\n@@ -3815,7 +3734,7 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def pool2d(x, pool_size, strides=(1, 1),\n            padding='valid', data_format=None,\n            pool_mode='max'):\n@@ -3864,7 +3783,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n            data_format=None, pool_mode='max'):\n     \"\"\"3D Pooling.\n@@ -3912,7 +3831,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n     return x\n \n \n-@tf_graph_op\n+@symbolic\n def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     \"\"\"Apply 1D conv with un-shared weights.\n \n@@ -3952,7 +3871,7 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     return permute_dimensions(output, (1, 0, 2))\n \n \n-@tf_graph_op\n+@symbolic\n def local_conv2d(inputs,\n                  kernel,\n                  kernel_size,\n@@ -4022,7 +3941,7 @@ def local_conv2d(inputs,\n     return output\n \n \n-@tf_graph_op\n+@symbolic\n def bias_add(x, bias, data_format=None):\n     \"\"\"Adds a bias vector to a tensor.\n \n@@ -4085,7 +4004,7 @@ def bias_add(x, bias, data_format=None):\n \n # RANDOMNESS\n \n-@tf_graph_op\n+\n def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Returns a tensor with normal distribution of values.\n \n@@ -4109,7 +4028,6 @@ def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n \n \n-@tf_graph_op\n def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Returns a tensor with uniform distribution of values.\n \n@@ -4134,7 +4052,6 @@ def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n             shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n \n \n-@tf_graph_op\n def random_binomial(shape, p=0.0, dtype=None, seed=None):\n     \"\"\"Returns a tensor with random binomial distribution of values.\n \n@@ -4156,7 +4073,6 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n             shape, p=p, dtype=dtype, seed=seed)\n \n \n-@tf_graph_op\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Returns a tensor with truncated random normal distribution of values.\n \n@@ -4191,7 +4107,7 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n # in TensorFlow's CTC implementation\n \n \n-@tf_graph_op\n+@symbolic\n def ctc_label_dense_to_sparse(labels, label_lengths):\n     \"\"\"Converts CTC labels from dense to sparse.\n \n@@ -4233,7 +4149,7 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     return tf.SparseTensor(indices, vals_sparse, label_shape)\n \n \n-@tf_graph_op\n+@symbolic\n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     \"\"\"Runs CTC loss algorithm on each batch element.\n \n@@ -4261,7 +4177,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n                                        sequence_length=input_length), 1)\n \n \n-@tf_graph_op\n+@symbolic\n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                top_paths=1, merge_repeated=False):\n     \"\"\"Decodes the output of a softmax.\n@@ -4315,7 +4231,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n \n # HIGH ORDER FUNCTIONS\n \n-@tf_graph_op\n+@symbolic\n def map_fn(fn, elems, name=None, dtype=None):\n     \"\"\"Map the function fn over the elements elems and return the outputs.\n \n@@ -4331,7 +4247,7 @@ def map_fn(fn, elems, name=None, dtype=None):\n     return tf.map_fn(fn, elems, name=name, dtype=dtype)\n \n \n-@tf_graph_op\n+@symbolic\n def foldl(fn, elems, initializer=None, name=None):\n     \"\"\"Reduce elems using fn to combine them from left to right.\n \n@@ -4348,7 +4264,7 @@ def foldl(fn, elems, initializer=None, name=None):\n     return tf.foldl(fn, elems, initializer=initializer, name=name)\n \n \n-@tf_graph_op\n+@symbolic\n def foldr(fn, elems, initializer=None, name=None):\n     \"\"\"Reduce elems using fn to combine them from right to left.\n \n\n@@ -379,6 +379,7 @@ class Layer(object):\n         \"\"\"\n         return inputs\n \n+    @K.symbolic\n     def __call__(self, inputs, **kwargs):\n         \"\"\"Wrapper around self.call(), for handling internal references.\n \n\n@@ -987,7 +987,7 @@ class Model(Network):\n                 sample_weight=val_sample_weight,\n                 batch_size=batch_size)\n             if self._uses_dynamic_learning_phase():\n-                val_inputs = val_x + val_y + val_sample_weights + [0.]\n+                val_inputs = val_x + val_y + val_sample_weights + [0]\n             else:\n                 val_inputs = val_x + val_y + val_sample_weights\n \n@@ -1009,7 +1009,7 @@ class Model(Network):\n                 slice_arrays(sample_weights, 0, split_at),\n                 slice_arrays(sample_weights, split_at))\n             if self._uses_dynamic_learning_phase():\n-                val_inputs = val_x + val_y + val_sample_weights + [0.]\n+                val_inputs = val_x + val_y + val_sample_weights + [0]\n             else:\n                 val_inputs = val_x + val_y + val_sample_weights\n \n@@ -1020,7 +1020,7 @@ class Model(Network):\n \n         # Prepare input arrays and training function.\n         if self._uses_dynamic_learning_phase():\n-            fit_inputs = x + y + sample_weights + [1.]\n+            fit_inputs = x + y + sample_weights + [1]\n         else:\n             fit_inputs = x + y + sample_weights\n         self._make_train_function()\n@@ -1123,7 +1123,7 @@ class Model(Network):\n             batch_size=batch_size)\n         # Prepare inputs, delegate logic to `test_loop`.\n         if self._uses_dynamic_learning_phase():\n-            ins = x + y + sample_weights + [0.]\n+            ins = x + y + sample_weights + [0]\n         else:\n             ins = x + y + sample_weights\n         self._make_test_function()\n@@ -1184,7 +1184,7 @@ class Model(Network):\n \n         # Prepare inputs, delegate logic to `predict_loop`.\n         if self._uses_dynamic_learning_phase():\n-            ins = x + [0.]\n+            ins = x + [0]\n         else:\n             ins = x\n         self._make_predict_function()\n@@ -1237,7 +1237,7 @@ class Model(Network):\n             sample_weight=sample_weight,\n             class_weight=class_weight)\n         if self._uses_dynamic_learning_phase():\n-            ins = x + y + sample_weights + [1.]\n+            ins = x + y + sample_weights + [1]\n         else:\n             ins = x + y + sample_weights\n         self._make_train_function()\n@@ -1276,7 +1276,7 @@ class Model(Network):\n             x, y,\n             sample_weight=sample_weight)\n         if self._uses_dynamic_learning_phase():\n-            ins = x + y + sample_weights + [0.]\n+            ins = x + y + sample_weights + [0]\n         else:\n             ins = x + y + sample_weights\n         self._make_test_function()\n@@ -1294,7 +1294,7 @@ class Model(Network):\n         \"\"\"\n         x, _, _ = self._standardize_user_data(x)\n         if self._uses_dynamic_learning_phase():\n-            ins = x + [0.]\n+            ins = x + [0]\n         else:\n             ins = x\n         self._make_predict_function()\n\n@@ -186,7 +186,7 @@ def fit_loop(model, fit_function, fit_inputs,\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 try:\n-                    if isinstance(fit_inputs[-1], float):\n+                    if isinstance(fit_inputs[-1], int):\n                         # Do not slice the training phase flag.\n                         ins_batch = slice_arrays(\n                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n@@ -319,7 +319,7 @@ def predict_loop(model, f, ins,\n         index_array = np.arange(num_samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n-            if ins and isinstance(ins[-1], float):\n+            if ins and isinstance(ins[-1], int):\n                 # Do not slice the training phase flag.\n                 ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n             else:\n@@ -329,6 +329,7 @@ def predict_loop(model, f, ins,\n \n             batch_logs = {'batch': batch_index, 'size': len(batch_ids)}\n             callbacks._call_batch_hook('predict', 'begin', batch_index, batch_logs)\n+            print('ins_batch', ins_batch)\n             batch_outs = f(ins_batch)\n             batch_outs = to_list(batch_outs)\n             if batch_index == 0:\n@@ -458,7 +459,7 @@ def test_loop(model, f, ins,\n         index_array = np.arange(num_samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n-            if isinstance(ins[-1], float):\n+            if isinstance(ins[-1], int):\n                 # Do not slice the training phase flag.\n                 ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n             else:\n\n@@ -1892,6 +1892,7 @@ class LSTMCell(Layer):\n \n         if self.use_bias:\n             if self.unit_forget_bias:\n+                @K.eager\n                 def bias_initializer(_, *args, **kwargs):\n                     return K.concatenate([\n                         self.bias_initializer((self.units,), *args, **kwargs),\n\n@@ -82,6 +82,7 @@ class Optimizer(object):\n         self.weights = []\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         raise NotImplementedError\n \n@@ -180,6 +181,7 @@ class SGD(Optimizer):\n         self.nesterov = nesterov\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n@@ -252,6 +254,7 @@ class RMSprop(Optimizer):\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         accumulators = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]\n@@ -318,6 +321,7 @@ class Adagrad(Optimizer):\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         shapes = [K.int_shape(p) for p in params]\n@@ -391,6 +395,7 @@ class Adadelta(Optimizer):\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         shapes = [K.int_shape(p) for p in params]\n@@ -471,6 +476,7 @@ class Adam(Optimizer):\n         self.amsgrad = amsgrad\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n@@ -557,6 +563,7 @@ class Adamax(Optimizer):\n         self.initial_decay = decay\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n@@ -641,6 +648,7 @@ class Nadam(Optimizer):\n         self.schedule_decay = schedule_decay\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n         grads = self.get_gradients(loss, params)\n         self.updates = [K.update_add(self.iterations, 1)]\n@@ -708,7 +716,11 @@ class TFOptimizer(Optimizer):\n             self.iterations = K.variable(0, dtype='int64', name='iterations')\n \n     @interfaces.legacy_get_updates_support\n+    @K.symbolic\n     def get_updates(self, loss, params):\n+        if isinstance(self.optimizer, tf.keras.optimizers.Optimizer):\n+            return self.optimizer.get_updates(loss, params)\n+        else:\n             grads = self.optimizer.compute_gradients(loss, var_list=params)\n         self.updates = [K.update_add(self.iterations, 1)]\n         opt_update = self.optimizer.apply_gradients(\n@@ -718,12 +730,19 @@ class TFOptimizer(Optimizer):\n \n     @property\n     def weights(self):\n+        if isinstance(self.optimizer, tf.keras.optimizers.Optimizer):\n+            return self.optimizer.weights\n         raise NotImplementedError\n \n     def get_config(self):\n+        if isinstance(self.optimizer, tf.keras.optimizers.Optimizer):\n+            return self.optimizer.get_config\n         raise NotImplementedError\n \n+    @classmethod\n     def from_config(self, config):\n+        if isinstance(self.optimizer, tf.keras.optimizers.Optimizer):\n+            return self.optimizer.from_config\n         raise NotImplementedError\n \n \n@@ -791,10 +810,13 @@ def get(identifier):\n     # Raises\n         ValueError: If `identifier` cannot be interpreted.\n     \"\"\"\n-    if K.backend() == 'tensorflow' and tf.__version__.startswith('1.'):\n+    if K.backend() == 'tensorflow':\n         # Wrap TF optimizer instances\n+        if tf.__version__.startswith('1.'):\n             if isinstance(identifier, tf.train.Optimizer):\n                 return TFOptimizer(identifier)\n+        elif isinstance(identifier, tf.keras.optimizers.Optimizer):\n+            return TFOptimizer(identifier)\n     if isinstance(identifier, dict):\n         return deserialize(identifier)\n     elif isinstance(identifier, six.string_types):\n\n@@ -17,8 +17,13 @@ def test_tf_optimizer():\n     output_dim = 2\n     input_dim = 10\n     target = 0.8\n+\n+    if tf.__version__.startswith('1.'):\n         optimizer = tf.train.AdadeltaOptimizer(\n             learning_rate=1., rho=0.95, epsilon=1e-08)\n+    else:\n+        optimizer = tf.keras.optimizers.Adadelta(\n+            learning_rate=1., rho=0.95, epsilon=1e-08)\n \n     (x_train, y_train), (x_test, y_test) = get_test_data(\n         num_train=1000, num_test=200,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
