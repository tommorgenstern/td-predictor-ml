{"custom_id": "keras#0c2bdff313f7533f0d7e6670a906102cc2fb046d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 13 | Churn Cumulative: 334 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -367,7 +367,7 @@ class SavingTest(testing.TestCase):\n         )\n         self.assertEqual(\n             config_dict[\"compile_config\"][\"loss\"][\"config\"],\n-            \"my_mean_squared_error\",\n+            \"my_custom_package>my_mean_squared_error\",\n         )\n \n     @pytest.mark.requires_trainable_backend\n\n@@ -366,7 +366,7 @@ def _get_class_or_fn_config(obj):\n     \"\"\"Return the object's config depending on its type.\"\"\"\n     # Functions / lambdas:\n     if isinstance(obj, types.FunctionType):\n-        return obj.__name__\n+        return object_registration.get_registered_name(obj)\n     # All classes:\n     if hasattr(obj, \"get_config\"):\n         config = obj.get_config()\n@@ -781,15 +781,6 @@ def _retrieve_class_or_fn(\n                 if obj is not None:\n                     return obj\n \n-            # Retrieval of registered custom function in a package\n-            filtered_dict = {\n-                k: v\n-                for k, v in custom_objects.items()\n-                if k.endswith(full_config[\"config\"])\n-            }\n-            if filtered_dict:\n-                return next(iter(filtered_dict.values()))\n-\n         # Otherwise, attempt to retrieve the class object given the `module`\n         # and `class_name`. Import the module, find the class.\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9b942f642bb62d75513ae6712cc5c01b438959c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 11 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -15,10 +15,10 @@ class Activation(Layer):\n     Example:\n \n     >>> layer = keras.layers.Activation('relu')\n-    >>> layer([-3.0, -1.0, 0.0, 2.0])\n+    >>> layer(np.array([-3.0, -1.0, 0.0, 2.0]))\n     [0.0, 0.0, 0.0, 2.0]\n     >>> layer = keras.layers.Activation(keras.activations.relu)\n-    >>> layer([-3.0, -1.0, 0.0, 2.0])\n+    >>> layer(np.array([-3.0, -1.0, 0.0, 2.0]))\n     [0.0, 0.0, 0.0, 2.0]\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aed5b08e9f59975b295372a38b44d126ec153c86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 16/5 | Churn Δ: 125 | Churn Cumulative: 4228 | Contributors (this commit): 20 | Commits (past 90d): 80 | Contributors (cumulative): 98 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from keras.src.activations.activations import glu\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_silu as hard_swish\n+from keras.src.activations.activations import hard_tanh\n from keras.src.activations.activations import leaky_relu\n from keras.src.activations.activations import linear\n from keras.src.activations.activations import log_softmax\n\n@@ -75,6 +75,7 @@ from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n+from keras.src.ops.nn import hard_tanh\n from keras.src.ops.nn import leaky_relu\n from keras.src.ops.nn import log_sigmoid\n from keras.src.ops.nn import log_softmax\n\n@@ -21,6 +21,7 @@ from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n+from keras.src.ops.nn import hard_tanh\n from keras.src.ops.nn import leaky_relu\n from keras.src.ops.nn import log_sigmoid\n from keras.src.ops.nn import log_softmax\n\n@@ -15,6 +15,7 @@ from keras.src.activations.activations import glu\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n from keras.src.activations.activations import hard_silu as hard_swish\n+from keras.src.activations.activations import hard_tanh\n from keras.src.activations.activations import leaky_relu\n from keras.src.activations.activations import linear\n from keras.src.activations.activations import log_softmax\n\n@@ -75,6 +75,7 @@ from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n+from keras.src.ops.nn import hard_tanh\n from keras.src.ops.nn import leaky_relu\n from keras.src.ops.nn import log_sigmoid\n from keras.src.ops.nn import log_softmax\n\n@@ -21,6 +21,7 @@ from keras.src.ops.nn import glu\n from keras.src.ops.nn import hard_sigmoid\n from keras.src.ops.nn import hard_silu\n from keras.src.ops.nn import hard_silu as hard_swish\n+from keras.src.ops.nn import hard_tanh\n from keras.src.ops.nn import leaky_relu\n from keras.src.ops.nn import log_sigmoid\n from keras.src.ops.nn import log_softmax\n\n@@ -7,6 +7,7 @@ from keras.src.activations.activations import gelu\n from keras.src.activations.activations import glu\n from keras.src.activations.activations import hard_sigmoid\n from keras.src.activations.activations import hard_silu\n+from keras.src.activations.activations import hard_tanh\n from keras.src.activations.activations import leaky_relu\n from keras.src.activations.activations import linear\n from keras.src.activations.activations import log_softmax\n@@ -42,6 +43,7 @@ ALL_OBJECTS = {\n     exponential,\n     hard_sigmoid,\n     hard_silu,\n+    hard_tanh,\n     linear,\n     mish,\n     log_softmax,\n\n@@ -358,6 +358,21 @@ def tanh(x):\n     return ops.tanh(x)\n \n \n+@keras_export(\"keras.activations.hard_tanh\")\n+def hard_tanh(x):\n+    \"\"\"HardTanh activation function.\n+\n+    It is defined as:\n+    `hard_tanh(x) = -1 for x < -1`,\n+    `hard_tanh(x) = x for -1 <= x <= 1`,\n+    `hard_tanh(x) = 1 for x > 1`.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.hard_tanh(x)\n+\n+\n @keras_export(\"keras.activations.sigmoid\")\n def sigmoid(x):\n     \"\"\"Sigmoid activation function.\n\n@@ -613,6 +613,15 @@ class ActivationsTest(testing.TestCase):\n         expected = glu(x, axis=-2)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_hard_tanh(self):\n+        def hard_tanh(x):\n+            return np.clip(x, -1.0, 1.0)\n+\n+        x = np.random.random((2, 5))\n+        result = activations.hard_tanh(x[np.newaxis, :])[0]\n+        expected = hard_tanh(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_elu(self):\n         x = np.random.random((2, 5))\n         result = activations.elu(x[np.newaxis, :])[0]\n\n@@ -95,6 +95,11 @@ def glu(x, axis=-1):\n     return jnn.glu(x, axis=axis)\n \n \n+def hard_tanh(x):\n+    x = convert_to_tensor(x)\n+    return jnn.hard_tanh(x)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n\n@@ -132,6 +132,13 @@ def glu(x, axis=-1):\n     return x1 * (1 / (1 + np.exp(-x2)))\n \n \n+def hard_tanh(x):\n+    x = convert_to_tensor(x)\n+    min_val = np.asarray(-1.0, x.dtype)\n+    max_val = np.asarray(1.0, x.dtype)\n+    return np.array(np.clip(x, min_val, max_val), dtype=x.dtype)\n+\n+\n def softmax(x, axis=None):\n     exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n     return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n@@ -92,6 +92,10 @@ def glu(x, axis=-1):\n     return x1 * tf.sigmoid(x2)\n \n \n+def hard_tanh(x):\n+    return tf.clip_by_value(x, clip_value_min=-1.0, clip_value_max=1.0)\n+\n+\n def softmax(x, axis=-1):\n     logits = x\n     if axis is None:\n\n@@ -98,6 +98,11 @@ def glu(x, axis=-1):\n     return tnn.glu(x, dim=axis)\n \n \n+def hard_tanh(x):\n+    x = convert_to_tensor(x)\n+    return tnn.hardtanh(x, min_val=-1.0, max_val=1.0)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     dtype = backend.standardize_dtype(x.dtype)\n\n@@ -579,6 +579,45 @@ def glu(x, axis=-1):\n     return backend.nn.glu(x, axis=axis)\n \n \n+class HardTanh(Operation):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return backend.nn.hard_tanh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.hard_tanh\", \"keras.ops.nn.hard_tanh\"])\n+def hard_tanh(x):\n+    \"\"\"Applies the HardTanh function element-wise.\n+\n+    It is defined as:\n+\n+    `f(x) = -1 for x < -1`, `f(x) = x for -1 <= x <= 1`, `f(x) = 1 for x > 1`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as `x`\n+        where values are clamped between -1 and 1.\n+\n+    Example:\n+\n+    >>> x = x = np.array([-2., -1., 0., 1., 2.])\n+    >>> x_hard_tanh = keras.ops.hard_tanh(x)\n+    >>> print(x_hard_tanh)\n+    array([-1. -1.  0.  1.  1.], shape=(5,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return HardTanh().symbolic_call(x)\n+    return backend.nn.hard_tanh(x)\n+\n+\n class Softmax(Operation):\n     def __init__(self, axis=-1):\n         super().__init__()\n\n@@ -149,6 +149,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.glu(x).shape, (None, 2, 3))\n \n+    def test_hard_tanh(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.hard_tanh(x).shape, (None, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (None, 2, 3))\n@@ -802,6 +806,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.glu(x).shape, (1, 2, 3))\n \n+    def test_hard_tanh(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.hard_tanh(x).shape, (1, 2, 3))\n+\n     def test_softmax(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.softmax(x).shape, (1, 2, 3))\n@@ -1322,6 +1330,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [-0.8807971, 0.0, 0.98201376],\n         )\n \n+    def test_hard_tanh(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.hard_tanh(x),\n+            [-1.0, 0.0, 1.0, 1.0, 1.0],\n+        )\n+\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n@@ -2411,6 +2426,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_hard_tanh(self, dtype):\n+        import jax.nn as jnn\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnn.hard_tanh(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.hard_tanh(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.HardTanh().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_glu(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0587f886763f855bb8116b05f9c0f2ecab1a5885", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 907 | Contributors (this commit): 9 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,7 +48,8 @@ def add(x1, x2):\n         # `x2` non-squeezable dimension defined\n         and x2_squeeze_shape[0] is not None\n         # `x2` non-squeezable dimension match `x1` channel dimension\n-        and x2_squeeze_shape[0] in {x1.shape[1], x1.shape[-1]}\n+        and x2_squeeze_shape[0]\n+        in {x1.shape.as_list()[1], x1.shape.as_list()[-1]}\n     ):\n         if x1.shape[-1] == x2_squeeze_shape[0]:\n             data_format = \"NHWC\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#093b055aa0f93c96fb6afa321199f57847886107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 395 | Lines Deleted: 234 | Files Changed: 5 | Hunks: 46 | Methods Changed: 17 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 629 | Churn Cumulative: 1471 | Contributors (this commit): 12 | Commits (past 90d): 16 | Contributors (cumulative): 26 | DMM Complexity: 0.8846153846153846\n\nDIFF:\n@@ -1056,7 +1056,9 @@ def divide_no_nan(x1, x2):\n     )\n     x1 = convert_to_tensor(x1, dtype)\n     x2 = convert_to_tensor(x2, dtype)\n-    return np.where(x2 == 0, 0, np.divide(x1, x2))\n+    # No need for the double-where trick since we don't calculate gradients in\n+    # numpy backend.\n+    return np.where(x2 == 0, np.array(0, dtype=dtype), np.divide(x1, x2))\n \n \n def true_divide(x1, x2):\n\n@@ -11,14 +11,17 @@ from keras.src.utils.naming import auto_name\n class Loss(KerasSaveable):\n     \"\"\"Loss base class.\n \n-    This is the class to subclass in order to create new\n-    custom losses.\n+    This is the class to subclass in order to create new custom losses.\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`\n-            or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -96,7 +99,14 @@ class Loss(KerasSaveable):\n \n \n def standardize_reduction(reduction):\n-    allowed = {\"sum_over_batch_size\", \"sum\", None, \"none\", \"mean\"}\n+    allowed = {\n+        \"sum_over_batch_size\",\n+        \"sum\",\n+        None,\n+        \"none\",\n+        \"mean\",\n+        \"mean_with_sample_weight\",\n+    }\n     if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n@@ -127,7 +137,7 @@ def squeeze_or_expand_to_same_rank(x1, x2, expand_rank_1=True):\n     return x1, x2\n \n \n-def reduce_values(values, reduction=\"sum_over_batch_size\"):\n+def reduce_values(values, sample_weight=None, reduction=\"sum_over_batch_size\"):\n     if (\n         reduction is None\n         or reduction == \"none\"\n@@ -136,11 +146,17 @@ def reduce_values(values, reduction=\"sum_over_batch_size\"):\n     ):\n         return values\n     loss = ops.sum(values)\n-    if reduction in (\"mean\", \"sum_over_batch_size\"):\n-        loss /= ops.cast(\n-            ops.prod(ops.convert_to_tensor(ops.shape(values), dtype=\"int32\")),\n+    if reduction in (\"sum_over_batch_size\", \"mean\", \"mean_with_sample_weight\"):\n+        if reduction == \"mean_with_sample_weight\" and sample_weight is not None:\n+            divisor = ops.cast(ops.sum(sample_weight), loss.dtype)\n+        else:\n+            divisor = ops.cast(\n+                ops.prod(\n+                    ops.convert_to_tensor(ops.shape(values), dtype=\"int32\")\n+                ),\n                 loss.dtype,\n             )\n+        loss = ops.divide_no_nan(loss, divisor)\n     return loss\n \n \n@@ -173,7 +189,7 @@ def reduce_weighted_values(\n         values = values * sample_weight\n \n     # Apply reduction function to the individual weighted losses.\n-    loss = reduce_values(values, reduction)\n+    loss = reduce_values(values, sample_weight, reduction)\n     return loss\n \n \n\n@@ -271,7 +271,7 @@ class LossTest(testing.TestCase):\n \n         # `dtype` setter should raise AttributeError\n         with self.assertRaises(AttributeError):\n-            loss.dtype = \"bfloat16\"\n+            loss_fn.dtype = \"bfloat16\"\n \n     def test_default_dtype(self):\n         y_true = np.array([1.0, 0.0, 1.0, 0.0], dtype=\"float32\")\n\n@@ -56,8 +56,13 @@ class MeanSquaredError(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -92,8 +97,13 @@ class MeanAbsoluteError(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -128,8 +138,13 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -167,8 +182,13 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -215,8 +235,13 @@ class CosineSimilarity(LossFunctionWrapper):\n         axis: The axis along which the cosine similarity is computed\n             (the features axis). Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -264,9 +289,14 @@ class Huber(LossFunctionWrapper):\n     Args:\n         delta: A float, the point where the Huber loss function changes from a\n             quadratic to linear.\n-        reduction: Type of reduction to apply to loss. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`. Defaults to\n-            `\"sum_over_batch_size\"`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -283,7 +313,11 @@ class Huber(LossFunctionWrapper):\n         dtype=None,\n     ):\n         super().__init__(\n-            huber, name=name, reduction=reduction, dtype=dtype, delta=delta\n+            huber,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n+            delta=delta,\n         )\n \n     def get_config(self):\n@@ -303,9 +337,14 @@ class LogCosh(LossFunctionWrapper):\n     where x is the error `y_pred - y_true`.\n \n     Args:\n-        reduction: Type of reduction to apply to loss. Options are `\"sum\"`,\n-            `\"sum_over_batch_size\"` or `None`. Defaults to\n-            `\"sum_over_batch_size\"`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -315,7 +354,10 @@ class LogCosh(LossFunctionWrapper):\n     \"\"\"\n \n     def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"log_cosh\", dtype=None\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"log_cosh\",\n+        dtype=None,\n     ):\n         super().__init__(log_cosh, name=name, reduction=reduction, dtype=dtype)\n \n@@ -338,8 +380,13 @@ class Hinge(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -349,7 +396,10 @@ class Hinge(LossFunctionWrapper):\n     \"\"\"\n \n     def __init__(\n-        self, reduction=\"sum_over_batch_size\", name=\"hinge\", dtype=None\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"hinge\",\n+        dtype=None,\n     ):\n         super().__init__(hinge, name=name, reduction=reduction, dtype=dtype)\n \n@@ -372,8 +422,13 @@ class SquaredHinge(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -407,8 +462,13 @@ class CategoricalHinge(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -447,8 +507,13 @@ class KLDivergence(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -480,8 +545,13 @@ class Poisson(LossFunctionWrapper):\n \n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -525,8 +595,13 @@ class BinaryCrossentropy(LossFunctionWrapper):\n         axis: The axis along which to compute crossentropy (the features axis).\n             Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -609,13 +684,15 @@ class BinaryCrossentropy(LossFunctionWrapper):\n         self.axis = axis\n \n     def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n+        config = Loss.get_config(self)\n+        config.update(\n+            {\n                 \"from_logits\": self.from_logits,\n                 \"label_smoothing\": self.label_smoothing,\n                 \"axis\": self.axis,\n             }\n+        )\n+        return config\n \n \n @keras_export(\"keras.losses.BinaryFocalCrossentropy\")\n@@ -662,8 +739,13 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         axis: The axis along which to compute crossentropy (the features axis).\n             Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -787,9 +869,9 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n         self.gamma = gamma\n \n     def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n+        config = Loss.get_config(self)\n+        config.update(\n+            {\n                 \"from_logits\": self.from_logits,\n                 \"label_smoothing\": self.label_smoothing,\n                 \"axis\": self.axis,\n@@ -797,6 +879,8 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n                 \"alpha\": self.alpha,\n                 \"gamma\": self.gamma,\n             }\n+        )\n+        return config\n \n \n @keras_export(\"keras.losses.CategoricalCrossentropy\")\n@@ -820,8 +904,13 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n         axis: The axis along which to compute crossentropy (the features\n             axis). Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -887,13 +976,15 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n         self.axis = axis\n \n     def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n+        config = Loss.get_config(self)\n+        config.update(\n+            {\n                 \"from_logits\": self.from_logits,\n                 \"label_smoothing\": self.label_smoothing,\n                 \"axis\": self.axis,\n             }\n+        )\n+        return config\n \n \n @keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n@@ -958,8 +1049,13 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n         axis: The axis along which to compute crossentropy (the features\n             axis). Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -1032,15 +1128,17 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n         self.gamma = gamma\n \n     def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n+        config = Loss.get_config(self)\n+        config.update(\n+            {\n                 \"from_logits\": self.from_logits,\n                 \"label_smoothing\": self.label_smoothing,\n                 \"axis\": self.axis,\n                 \"alpha\": self.alpha,\n                 \"gamma\": self.gamma,\n             }\n+        )\n+        return config\n \n \n @keras_export(\"keras.losses.SparseCategoricalCrossentropy\")\n@@ -1063,8 +1161,13 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n         from_logits: Whether `y_pred` is expected to be a logits tensor. By\n             default, we assume that `y_pred` encodes a probability distribution.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -1125,12 +1228,179 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n         self.ignore_class = ignore_class\n \n     def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n+        config = Loss.get_config(self)\n+        config.update(\n+            {\n                 \"from_logits\": self.from_logits,\n                 \"ignore_class\": self.ignore_class,\n             }\n+        )\n+        return config\n+\n+\n+@keras_export(\"keras.losses.CTC\")\n+class CTC(LossFunctionWrapper):\n+    \"\"\"CTC (Connectionist Temporal Classification) loss.\n+\n+    Args:\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n+        name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n+    \"\"\"\n+\n+    def __init__(self, reduction=\"sum_over_batch_size\", name=\"ctc\", dtype=None):\n+        super().__init__(ctc, name=name, reduction=reduction, dtype=dtype)\n+\n+    def get_config(self):\n+        return Loss.get_config(self)\n+\n+\n+@keras_export(\"keras.losses.Dice\")\n+class Dice(LossFunctionWrapper):\n+    \"\"\"Computes the Dice loss value between `y_true` and `y_pred`.\n+\n+    Formula:\n+    ```python\n+    loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred))\n+    ```\n+\n+    Args:\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n+        name: Optional name for the loss instance.\n+        axis: Tuple for which dimensions the loss is calculated. Defaults to\n+            `None`.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n+\n+    Returns:\n+        Dice loss value.\n+\n+    Example:\n+\n+    >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]],\n+    ...           [[[1.0], [1.0]], [[0.0], [0.0]]]]\n+    >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]],\n+    ...           [[[0.4], [0.0]], [[0.0], [0.9]]]]\n+    >>> axis = (1, 2, 3)\n+    >>> loss = keras.losses.dice(y_true, y_pred, axis=axis)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n+\n+    >>> loss = keras.losses.dice(y_true, y_pred)\n+    >>> assert loss.shape == ()\n+    >>> loss\n+    array(0.6164384, shape=(), dtype=float32)\n+\n+    >>> y_true = np.array(y_true)\n+    >>> y_pred = np.array(y_pred)\n+    >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred)\n+    >>> assert loss.shape == (2,)\n+    >>> loss\n+    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"dice\",\n+        axis=None,\n+        dtype=None,\n+    ):\n+        super().__init__(\n+            dice, name=name, reduction=reduction, dtype=dtype, axis=axis\n+        )\n+        self.axis = axis\n+\n+    def get_config(self):\n+        config = Loss.get_config(self)\n+        config.update({\"axis\": self.axis})\n+        return config\n+\n+\n+@keras_export(\"keras.losses.Tversky\")\n+class Tversky(LossFunctionWrapper):\n+    \"\"\"Computes the Tversky loss value between `y_true` and `y_pred`.\n+\n+    This loss function is weighted by the alpha and beta coefficients\n+    that penalize false positives and false negatives.\n+\n+    With `alpha=0.5` and `beta=0.5`, the loss value becomes equivalent to\n+    Dice Loss.\n+\n+    Args:\n+        alpha: The coefficient controlling incidence of false positives.\n+            Defaults to `0.5`.\n+        beta: The coefficient controlling incidence of false negatives.\n+            Defaults to `0.5`.\n+        reduction: Type of reduction to apply to the loss. In almost all cases\n+            this should be `\"sum_over_batch_size\"`. Supported options are\n+            `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`,\n+            `\"mean_with_sample_weight\"` or `None`. `\"sum\"` sums the loss,\n+            `\"sum_over_batch_size\"` and `\"mean\"` sum the loss and divide by the\n+            sample size, and `\"mean_with_sample_weight\"` sums the loss and\n+            divides by the sum of the sample weights. `\"none\"` and `None`\n+            perform no aggregation. Defaults to `\"sum_over_batch_size\"`.\n+        name: Optional name for the loss instance.\n+        dtype: The dtype of the loss's computations. Defaults to `None`, which\n+            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n+            `\"float32\"` unless set to different value\n+            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n+            provided, then the `compute_dtype` will be utilized.\n+\n+    Returns:\n+        Tversky loss value.\n+\n+    Reference:\n+\n+    - [Salehi et al., 2017](https://arxiv.org/abs/1706.05721)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        alpha=0.5,\n+        beta=0.5,\n+        reduction=\"sum_over_batch_size\",\n+        name=\"tversky\",\n+        dtype=None,\n+    ):\n+        super().__init__(\n+            tversky,\n+            name=name,\n+            reduction=reduction,\n+            dtype=dtype,\n+            alpha=alpha,\n+            beta=beta,\n+        )\n+        self.alpha = alpha\n+        self.beta = beta\n+\n+    def get_config(self):\n+        config = Loss.get_config(self)\n+        config.update({\"alpha\": self.alpha, \"beta\": self.beta})\n+        return config\n \n \n def convert_binary_labels_to_hinge(y_true):\n@@ -2026,37 +2296,6 @@ def binary_focal_crossentropy(\n     return ops.mean(focal_bce, axis=axis)\n \n \n-@keras_export(\"keras.losses.CTC\")\n-class CTC(LossFunctionWrapper):\n-    \"\"\"CTC (Connectionist Temporal Classification) loss.\n-\n-    Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-        dtype: The dtype of the loss's computations. Defaults to `None`, which\n-            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n-            `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n-            provided, then the `compute_dtype` will be utilized.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"ctc\",\n-        dtype=None,\n-    ):\n-        super().__init__(ctc, name=name, reduction=reduction, dtype=dtype)\n-\n-    def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n-        }\n-\n-\n @keras_export(\"keras.losses.ctc\")\n def ctc(y_true, y_pred):\n     \"\"\"CTC (Connectionist Temporal Classification) loss.\n@@ -2095,81 +2334,6 @@ def ctc(y_true, y_pred):\n     )\n \n \n-@keras_export(\"keras.losses.Dice\")\n-class Dice(LossFunctionWrapper):\n-    \"\"\"Computes the Dice loss value between `y_true` and `y_pred`.\n-\n-    Formula:\n-    ```python\n-    loss = 1 - (2 * sum(y_true * y_pred)) / (sum(y_true) + sum(y_pred))\n-    ```\n-\n-    Args:\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-        axis: Tuple for which dimensions the loss is calculated. Defaults to\n-            `None`.\n-        dtype: The dtype of the loss's computations. Defaults to `None`, which\n-            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n-            `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n-            provided, then the `compute_dtype` will be utilized.\n-\n-    Returns:\n-        Dice loss value.\n-\n-    Example:\n-\n-    >>> y_true = [[[[1.0], [1.0]], [[0.0], [0.0]]],\n-    ...           [[[1.0], [1.0]], [[0.0], [0.0]]]]\n-    >>> y_pred = [[[[0.0], [1.0]], [[0.0], [1.0]]],\n-    ...           [[[0.4], [0.0]], [[0.0], [0.9]]]]\n-    >>> axis = (1, 2, 3)\n-    >>> loss = keras.losses.dice(y_true, y_pred, axis=axis)\n-    >>> assert loss.shape == (2,)\n-    >>> loss\n-    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n-\n-    >>> loss = keras.losses.dice(y_true, y_pred)\n-    >>> assert loss.shape == ()\n-    >>> loss\n-    array(0.6164384, shape=(), dtype=float32)\n-\n-    >>> y_true = np.array(y_true)\n-    >>> y_pred = np.array(y_pred)\n-    >>> loss = keras.losses.Dice(axis=axis, reduction=None)(y_true, y_pred)\n-    >>> assert loss.shape == (2,)\n-    >>> loss\n-    array([0.5, 0.75757575], shape=(2,), dtype=float32)\n-\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"dice\",\n-        axis=None,\n-        dtype=None,\n-    ):\n-        super().__init__(\n-            dice,\n-            name=name,\n-            reduction=reduction,\n-            dtype=dtype,\n-            axis=axis,\n-        )\n-        self.axis = axis\n-\n-    def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"reduction\": self.reduction,\n-            \"axis\": self.axis,\n-        }\n-\n-\n @keras_export(\"keras.losses.dice\")\n def dice(y_true, y_pred, axis=None):\n     \"\"\"Computes the Dice loss value between `y_true` and `y_pred`.\n@@ -2204,67 +2368,6 @@ def dice(y_true, y_pred, axis=None):\n     return 1 - dice\n \n \n-@keras_export(\"keras.losses.Tversky\")\n-class Tversky(LossFunctionWrapper):\n-    \"\"\"Computes the Tversky loss value between `y_true` and `y_pred`.\n-\n-    This loss function is weighted by the alpha and beta coefficients\n-    that penalize false positives and false negatives.\n-\n-    With `alpha=0.5` and `beta=0.5`, the loss value becomes equivalent to\n-    Dice Loss.\n-\n-    Args:\n-        alpha: The coefficient controlling incidence of false positives.\n-            Defaults to `0.5`.\n-        beta: The coefficient controlling incidence of false negatives.\n-            Defaults to `0.5`.\n-        reduction: Type of reduction to apply to the loss. In almost all cases\n-            this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n-        name: Optional name for the loss instance.\n-        dtype: The dtype of the loss's computations. Defaults to `None`, which\n-            means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n-            `\"float32\"` unless set to different value\n-            (via `keras.backend.set_floatx()`). If a `keras.DTypePolicy` is\n-            provided, then the `compute_dtype` will be utilized.\n-\n-    Returns:\n-        Tversky loss value.\n-\n-    Reference:\n-\n-    - [Salehi et al., 2017](https://arxiv.org/abs/1706.05721)\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        alpha=0.5,\n-        beta=0.5,\n-        reduction=\"sum_over_batch_size\",\n-        name=\"tversky\",\n-        dtype=None,\n-    ):\n-        super().__init__(\n-            tversky,\n-            name=name,\n-            reduction=reduction,\n-            dtype=dtype,\n-            alpha=alpha,\n-            beta=beta,\n-        )\n-        self.alpha = alpha\n-        self.beta = beta\n-\n-    def get_config(self):\n-        return {\n-            \"name\": self.name,\n-            \"alpha\": self.alpha,\n-            \"beta\": self.beta,\n-            \"reduction\": self.reduction,\n-        }\n-\n-\n @keras_export(\"keras.losses.tversky\")\n def tversky(y_true, y_pred, alpha=0.5, beta=0.5):\n     \"\"\"Computes the Tversky loss value between `y_true` and `y_pred`.\n\n@@ -78,6 +78,16 @@ class MeanSquaredErrorTest(testing.TestCase):\n         loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, 227.69998)\n \n+    def test_mean_with_sample_weight_reduction(self):\n+        mse_obj = losses.MeanSquaredError(reduction=\"mean_with_sample_weight\")\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        sample_weight = np.array([[1.2], [3.4]])\n+        loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(\n+            loss, (110 / 3 * 1.2 + 187 / 3 * 3.4) / (1.2 + 3.4)\n+        )\n+\n     def test_dtype_arg(self):\n         mse_obj = losses.MeanSquaredError(dtype=\"bfloat16\")\n         y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n@@ -153,6 +163,16 @@ class MeanAbsoluteErrorTest(testing.TestCase):\n         loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, 25.29999)\n \n+    def test_mean_with_sample_weight_reduction(self):\n+        mae_obj = losses.MeanAbsoluteError(reduction=\"mean_with_sample_weight\")\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        sample_weight = np.array([[1.2], [3.4]])\n+        loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(\n+            loss, (14 / 3 * 1.2 + 19 / 3 * 3.4) / (1.2 + 3.4)\n+        )\n+\n     def test_dtype_arg(self):\n         mae_obj = losses.MeanAbsoluteError(dtype=\"bfloat16\")\n         y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n@@ -221,6 +241,16 @@ class MeanAbsolutePercentageErrorTest(testing.TestCase):\n         loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, [621.8518, 352.6666])\n \n+    def test_mean_with_sample_weight_reduction(self):\n+        mape_obj = losses.MeanAbsolutePercentageError(\n+            reduction=\"mean_with_sample_weight\"\n+        )\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        sample_weight = np.array([[1.2], [3.4]])\n+        loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 183.865)\n+\n     def test_dtype_arg(self):\n         mape_obj = losses.MeanAbsolutePercentageError(dtype=\"bfloat16\")\n         y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n@@ -276,6 +306,16 @@ class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n         loss = msle_obj(y_true, y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0, 3)\n \n+    def test_mean_with_sample_weight_reduction(self):\n+        msle_obj = losses.MeanSquaredLogarithmicError(\n+            reduction=\"mean_with_sample_weight\"\n+        )\n+        y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n+        y_pred = np.array([[4, 8, 12], [8, 1, 3]], dtype=\"float32\")\n+        sample_weight = np.array([[1.2], [3.4]])\n+        loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(loss, 1.646)\n+\n     def test_dtype_arg(self):\n         msle_obj = losses.MeanSquaredLogarithmicError(dtype=\"bfloat16\")\n         y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
