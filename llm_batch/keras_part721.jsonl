{"custom_id": "keras#549e260ade2487ee00702a5a74ec5b751b7ea348", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 3585 | Contributors (this commit): 14 | Commits (past 90d): 45 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -220,17 +220,16 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n         # TODO `tf.keras` also outputs individual losses for outputs\n-        # TODO Align output names with 'bce', `mse`, `mae` of `tf.keras`\n         ref_keys = sorted(\n             [\n                 \"loss\",\n                 # \"output_a_loss\",\n-                \"output_a_binary_crossentropy\",\n-                \"output_a_mean_absolute_error\",\n-                \"output_a_mean_squared_error\",\n+                \"output_a_bce\",\n+                \"output_a_mae\",\n+                \"output_a_mse\",\n                 \"output_b_acc\",\n                 # \"output_b_loss\",\n-                \"output_b_mean_squared_error\",\n+                \"output_b_mse\",\n             ]\n         )\n         self.assertListEqual(hist_keys, ref_keys)\n\n@@ -79,13 +79,14 @@ def get_metric(identifier, y_true, y_pred):\n             )\n \n     if not isinstance(metric_obj, metrics_module.Metric):\n+        metric_obj = metrics_module.MeanMetricWrapper(metric_obj)\n+\n     if isinstance(identifier, str):\n         metric_name = identifier\n     else:\n         metric_name = get_object_name(metric_obj)\n-        metric_obj = metrics_module.MeanMetricWrapper(\n-            metric_obj, name=metric_name\n-        )\n+    metric_obj.name = metric_name\n+\n     return metric_obj\n \n \n\n@@ -198,7 +198,7 @@ class TestCompileMetrics(testing.TestCase):\n \n     def test_name_conversions(self):\n         compile_metrics = CompileMetrics(\n-            metrics=[\"acc\", \"accuracy\"],\n+            metrics=[\"acc\", \"accuracy\", \"mse\"],\n             weighted_metrics=[],\n         )\n         y_true = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n@@ -207,9 +207,10 @@ class TestCompileMetrics(testing.TestCase):\n         compile_metrics.update_state(y_true, y_pred, sample_weight=None)\n         result = compile_metrics.result()\n         self.assertTrue(isinstance(result, dict))\n-        self.assertEqual(len(result), 2)\n+        self.assertEqual(len(result), 3)\n         self.assertAllClose(result[\"acc\"], 0.333333)\n         self.assertAllClose(result[\"accuracy\"], 0.333333)\n+        self.assertTrue(\"mse\" in result)\n \n \n class TestCompileLoss(testing.TestCase, parameterized.TestCase):\n\n@@ -525,7 +525,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n                 assert keys == [\"outputs\"]\n \n         model = ExampleModel(units=3)\n-        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n+        model.compile(\n+            optimizer=\"adam\", loss=\"mse\", metrics=[\"mean_absolute_error\"]\n+        )\n         x = np.ones((16, 4))\n         y = np.zeros((16, 3))\n         x_test = np.ones((16, 4))\n@@ -651,12 +653,16 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         inputs = layers.Input((2,))\n         outputs = layers.Dense(3)(inputs)\n         model = keras_core.Model(inputs, outputs)\n-        model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"])\n+        model.compile(\n+            optimizer=\"sgd\", loss=\"mse\", metrics=[\"mean_squared_error\"]\n+        )\n         history_1 = model.fit(np.ones((3, 2)), np.ones((3, 3))).history\n         eval_out_1 = model.evaluate(\n             np.ones((3, 2)), np.ones((3, 3)), return_dict=True\n         )\n-        model.compile(optimizer=\"sgd\", loss=\"mse\", metrics=[\"mae\"])\n+        model.compile(\n+            optimizer=\"sgd\", loss=\"mse\", metrics=[\"mean_absolute_error\"]\n+        )\n         history_2 = model.fit(np.ones((3, 2)), np.ones((3, 3))).history\n         eval_out_2 = model.evaluate(\n             np.ones((3, 2)), np.ones((3, 3)), return_dict=True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a65b6e57aca479cd320f9b38fc0db6e161c70685", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 386 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -160,24 +160,24 @@ class RandomRotation(TFDataLayer):\n     \"\"\"\n \n     def _get_rotation_matrix(self, inputs):\n-        input_shape = len(inputs.shape)\n-        if input_shape == 4:\n+        shape = self.backend.core.shape(inputs)\n+        if len(shape) == 4:\n             if self.data_format == \"channels_last\":\n-                batch_size = inputs.shape[0]\n-                image_height = inputs.shape[1]\n-                image_width = inputs.shape[2]\n+                batch_size = shape[0]\n+                image_height = shape[1]\n+                image_width = shape[2]\n             else:\n-                batch_size = inputs.shape[1]\n-                image_height = inputs.shape[2]\n-                image_width = inputs.shape[3]\n+                batch_size = shape[1]\n+                image_height = shape[2]\n+                image_width = shape[3]\n         else:\n             batch_size = 1\n             if self.data_format == \"channels_last\":\n-                image_height = inputs.shape[0]\n-                image_width = inputs.shape[1]\n+                image_height = shape[0]\n+                image_width = shape[1]\n             else:\n-                image_height = inputs.shape[1]\n-                image_width = inputs.shape[2]\n+                image_height = shape[1]\n+                image_width = shape[2]\n \n         lower = self._factor[0] * 2.0 * self.backend.convert_to_tensor(np.pi)\n         upper = self._factor[1] * 2.0 * self.backend.convert_to_tensor(np.pi)\n@@ -215,7 +215,7 @@ class RandomRotation(TFDataLayer):\n             ],\n             axis=1,\n         )\n-        if input_shape == 3:\n+        if len(shape) == 3:\n             outputs = self.backend.numpy.squeeze(outputs, axis=0)\n         return outputs\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b02dddeaa2943b1085d3dfd8ee4d69c9046f619", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 22 | Files Changed: 18 | Hunks: 34 | Methods Changed: 23 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 111 | Churn Cumulative: 10953 | Contributors (this commit): 23 | Commits (past 90d): 164 | Contributors (cumulative): 102 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -86,6 +86,7 @@ class ReLU(ops.Operation):\n         clip_max = max_value is not None\n         if threshold != 0:\n             # computes x for x > threshold else 0\n+            threshold = ops.cast(threshold, dtype=x.dtype)\n             x = x * backend.cast(\n                 backend.numpy.greater(x, threshold), dtype=x.dtype\n             )\n@@ -97,7 +98,9 @@ class ReLU(ops.Operation):\n             x = backend.nn.relu(x)\n \n         if clip_max:\n-            x = backend.numpy.clip(x, 0.0, max_value)\n+            min_value = ops.cast(0.0, dtype=x.dtype)\n+            max_value = ops.cast(max_value, dtype=x.dtype)\n+            x = backend.numpy.clip(x, min_value, max_value)\n \n         if negative_slope != 0.0:\n             x -= negative_slope * negative_part\n\n@@ -477,6 +477,7 @@ class AutocastScope:\n     \"\"\"\n \n     def __init__(self, dtype):\n+        if dtype is not None:\n             dtype = standardize_dtype(dtype)\n             if not is_float_dtype(dtype):\n                 raise ValueError(\n@@ -490,7 +491,7 @@ class AutocastScope:\n     def maybe_cast(self, value):\n         from keras_core import backend\n \n-        if is_float_dtype(value.dtype):\n+        if self.dtype is not None and is_float_dtype(value.dtype):\n             return backend.cast(value, dtype=self.dtype)\n         return value\n \n\n@@ -101,6 +101,11 @@ def affine_transform(\n             f\"transform.shape={transform.shape}\"\n         )\n \n+    # scipy.ndimage.map_coordinates lacks support for half precision.\n+    input_dtype = image.dtype\n+    if input_dtype == \"float16\":\n+        image = image.astype(\"float32\")\n+\n     # unbatched case\n     need_squeeze = False\n     if len(image.shape) == 3:\n@@ -165,4 +170,6 @@ def affine_transform(\n         affined = np.transpose(affined, (0, 3, 1, 2))\n     if need_squeeze:\n         affined = np.squeeze(affined, axis=0)\n+    if input_dtype == \"float16\":\n+        affined = affined.astype(input_dtype)\n     return affined\n\n@@ -47,7 +47,7 @@ def resize(\n             resized = tf.transpose(resized, (0, 3, 1, 2))\n         elif len(image.shape) == 3:\n             resized = tf.transpose(resized, (2, 0, 1))\n-    return resized\n+    return tf.cast(resized, image.dtype)\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = (\n\n@@ -243,6 +243,7 @@ class MultiHeadAttention(Layer):\n             activity_regularizer=self._activity_regularizer,\n             kernel_constraint=self._kernel_constraint,\n             bias_constraint=self._bias_constraint,\n+            dtype=self.dtype_policy,\n         )\n         # Create new clone of kernel/bias initializer, so that we don't reuse\n         # the initializer instance, which could lead to same init value since\n@@ -311,8 +312,10 @@ class MultiHeadAttention(Layer):\n                 attn_scores_rank - len(self._attention_axes), attn_scores_rank\n             )\n         )\n-        self._softmax = Softmax(axis=norm_axes)\n-        self._dropout_layer = Dropout(rate=self._dropout)\n+        self._softmax = Softmax(axis=norm_axes, dtype=self.dtype_policy)\n+        self._dropout_layer = Dropout(\n+            rate=self._dropout, dtype=self.dtype_policy\n+        )\n         self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n@@ -358,7 +361,9 @@ class MultiHeadAttention(Layer):\n         # Note: Applying scalar multiply at the smaller end of einsum improves\n         # XLA performance, but may introduce slight numeric differences in\n         # the Transformer attention head.\n-        query = ops.multiply(query, self._inverse_sqrt_key_dim)\n+        query = ops.multiply(\n+            query, ops.cast(self._inverse_sqrt_key_dim, query.dtype)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n\n@@ -1,6 +1,7 @@\n import pytest\n \n from keras_core import layers\n+from keras_core import ops\n from keras_core import testing\n \n \n@@ -8,7 +9,7 @@ class ExampleWrapper(layers.Wrapper):\n     \"\"\"Simple Wrapper subclass.\"\"\"\n \n     def call(self, inputs, **kwargs):\n-        return self.layer(inputs, **kwargs)\n+        return ops.cast(self.layer(inputs, **kwargs), self.compute_dtype)\n \n \n class WrapperTest(testing.TestCase):\n\n@@ -739,10 +739,20 @@ class Layer(BackendLayer, Operation):\n         # 7. Call the layer.\n         try:\n             with backend.name_scope(self.name, caller=self):\n-                if self.autocast and self.compute_dtype != self.variable_dtype:\n-                    # For mixed precision, we automatically cast layer variables\n-                    # (float ones only) to the compute dtype upon access.\n-                    with backend.AutocastScope(self.compute_dtype):\n+                current_scope = backend.get_autocast_scope()\n+                new_scope = None\n+                if current_scope is not None:\n+                    # Clear or update the current scope if necessary.\n+                    if not self.autocast:\n+                        new_scope = backend.AutocastScope(None)\n+                    elif current_scope.dtype != self.compute_dtype:\n+                        new_scope = backend.AutocastScope(self.compute_dtype)\n+                elif self.compute_dtype != self.variable_dtype:\n+                    # Enter a new scope if our dtypes are \"mixed\".\n+                    new_scope = backend.AutocastScope(self.compute_dtype)\n+\n+                if new_scope is not None:\n+                    with new_scope:\n                         outputs = super().__call__(*args, **kwargs)\n                 else:\n                     outputs = super().__call__(*args, **kwargs)\n\n@@ -236,7 +236,7 @@ class BatchNormalization(Layer):\n             beta = ops.reshape(self.beta, broadcast_shape)\n             beta = ops.cast(beta, outputs.dtype)\n             outputs = outputs + beta\n-        return outputs\n+        return ops.cast(outputs, input_dtype)\n \n     def get_config(self):\n         base_config = super().get_config()\n\n@@ -79,7 +79,7 @@ class SpectralNormalization(Wrapper):\n             self.normalize_weights()\n \n         output = self.layer(inputs)\n-        return output\n+        return ops.cast(output, inputs.dtype)\n \n     def compute_output_shape(self, input_shape):\n         return self.layer.compute_output_shape(input_shape)\n\n@@ -28,6 +28,7 @@ class SpectralNormalizationTest(testing.TestCase):\n             layers.Conv2D(\n                 1, (2, 2), kernel_initializer=initializers.Constant(value=1)\n             ),\n+            power_iterations=8,\n         )\n \n         result = sn_wrapper(images, training=False)\n\n@@ -105,6 +105,8 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n \n \n\n@@ -104,6 +104,8 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n \n \n\n@@ -23,6 +23,8 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n             run_training_check=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n         self.run_layer_test(\n             layers.HashedCrossing,\n@@ -35,6 +37,8 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n             run_training_check=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n \n     def test_correctness(self):\n\n@@ -164,14 +164,12 @@ class Normalization(Layer):\n             self.adapt_mean = self.add_weight(\n                 name=\"mean\",\n                 shape=mean_and_var_shape,\n-                dtype=self.compute_dtype,\n                 initializer=\"zeros\",\n                 trainable=False,\n             )\n             self.adapt_variance = self.add_weight(\n                 name=\"variance\",\n                 shape=mean_and_var_shape,\n-                dtype=self.compute_dtype,\n                 initializer=\"ones\",\n                 trainable=False,\n             )\n\n@@ -67,6 +67,7 @@ class RandomContrast(TFDataLayer):\n                 minval=1.0 - self.lower,\n                 maxval=1.0 + self.upper,\n                 seed=seed_generator,\n+                dtype=self.compute_dtype,\n             )\n \n             outputs = self._adjust_constrast(inputs, factor)\n\n@@ -222,6 +222,8 @@ class Bidirectional(Wrapper):\n         y_rev = self.backward_layer(\n             backward_inputs, initial_state=backward_state, **kwargs\n         )\n+        y = ops.cast(y, self.compute_dtype)\n+        y_rev = ops.cast(y_rev, self.compute_dtype)\n \n         if self.return_state:\n             states = y[1:] + y_rev[1:]\n\n@@ -4,15 +4,12 @@ from keras_core import backend\n from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras_core.saving import serialization_lib\n from keras_core.utils import tracking\n \n \n-class DropoutRNNCellMixin:\n-    pass\n-\n-\n @keras_core_export(\"keras_core.layers.RNN\")\n class RNN(Layer):\n     \"\"\"Base class for recurrent layers.\n@@ -305,7 +302,7 @@ class RNN(Layer):\n             init_state = get_initial_state_fn(batch_size=batch_size)\n         else:\n             return [\n-                ops.zeros((batch_size, d), dtype=self.compute_dtype)\n+                ops.zeros((batch_size, d), dtype=self.cell.compute_dtype)\n                 for d in self.state_size\n             ]\n \n@@ -387,7 +384,9 @@ class RNN(Layer):\n         # Note that states may be deeply nested\n         # (e.g. in the stacked cells case).\n         initial_state = tree.map_structure(\n-            lambda x: backend.convert_to_tensor(x, dtype=self.compute_dtype),\n+            lambda x: backend.convert_to_tensor(\n+                x, dtype=self.cell.compute_dtype\n+            ),\n             initial_state,\n         )\n \n@@ -397,6 +396,11 @@ class RNN(Layer):\n             mask=mask,\n             training=training,\n         )\n+        last_output = ops.cast(last_output, self.compute_dtype)\n+        outputs = ops.cast(outputs, self.compute_dtype)\n+        states = tree.map_structure(\n+            lambda x: ops.cast(x, dtype=self.compute_dtype), states\n+        )\n         self._maybe_reset_dropout_masks(self.cell)\n \n         if self.stateful:\n@@ -418,7 +422,7 @@ class RNN(Layer):\n         return output\n \n     def _maybe_reset_dropout_masks(self, cell):\n-        if isinstance(cell, DropoutRNNCellMixin):\n+        if isinstance(cell, DropoutRNNCell):\n             cell.reset_dropout_mask()\n             cell.reset_recurrent_dropout_mask()\n         if isinstance(cell, StackedRNNCells):\n\n@@ -9,6 +9,8 @@ import tree\n from keras_core import backend\n from keras_core import ops\n from keras_core import utils\n+from keras_core.backend.common import is_float_dtype\n+from keras_core.backend.common import standardize_dtype\n from keras_core.models import Model\n from keras_core.utils import traceback_utils\n \n@@ -123,6 +125,7 @@ class TestCase(unittest.TestCase):\n         expected_mask_shape=None,\n         custom_objects=None,\n         run_training_check=True,\n+        run_mixed_precision_check=True,\n     ):\n         \"\"\"Run basic checks on a layer.\n \n@@ -159,6 +162,8 @@ class TestCase(unittest.TestCase):\n                 considered during deserialization.\n             run_training_check: Whether to attempt to train the layer\n                 (if an input shape or input data was provided).\n+            run_mixed_precision_check: Whether to test the layer with a mixed\n+                precision dtype policy.\n         \"\"\"\n         if input_shape is not None and input_data is not None:\n             raise ValueError(\n@@ -337,6 +342,27 @@ class TestCase(unittest.TestCase):\n             if run_training_check:\n                 run_training_step(layer, input_data, output_data)\n \n+            # Never test mixed precision on torch CPU. Torch lacks support.\n+            if run_mixed_precision_check and backend.backend() == \"torch\":\n+                import torch\n+\n+                run_mixed_precision_check = torch.cuda.is_available()\n+\n+            if run_mixed_precision_check:\n+                layer = layer_cls(**{**init_kwargs, \"dtype\": \"mixed_float16\"})\n+                if isinstance(input_data, dict):\n+                    output_data = layer(**input_data, **call_kwargs)\n+                else:\n+                    output_data = layer(input_data, **call_kwargs)\n+                for tensor in tree.flatten(output_data):\n+                    dtype = standardize_dtype(tensor.dtype)\n+                    if is_float_dtype(dtype):\n+                        self.assertEqual(dtype, \"float16\")\n+                for weight in layer.weights:\n+                    dtype = standardize_dtype(weight.dtype)\n+                    if is_float_dtype(dtype):\n+                        self.assertEqual(dtype, \"float32\")\n+\n \n def create_keras_tensors(input_shape, dtype):\n     from keras_core.backend.common import keras_tensor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4a5303bc3795d81620bd22082a02b0b6d06ae3c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 90 | Files Changed: 19 | Hunks: 37 | Methods Changed: 26 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 130 | Churn Cumulative: 11116 | Contributors (this commit): 23 | Commits (past 90d): 182 | Contributors (cumulative): 105 | DMM Complexity: 0.9767441860465116\n\nDIFF:\n@@ -86,7 +86,6 @@ class ReLU(ops.Operation):\n         clip_max = max_value is not None\n         if threshold != 0:\n             # computes x for x > threshold else 0\n-            threshold = ops.cast(threshold, dtype=x.dtype)\n             x = x * backend.cast(\n                 backend.numpy.greater(x, threshold), dtype=x.dtype\n             )\n@@ -98,9 +97,7 @@ class ReLU(ops.Operation):\n             x = backend.nn.relu(x)\n \n         if clip_max:\n-            min_value = ops.cast(0.0, dtype=x.dtype)\n-            max_value = ops.cast(max_value, dtype=x.dtype)\n-            x = backend.numpy.clip(x, min_value, max_value)\n+            x = backend.numpy.clip(x, 0.0, max_value)\n \n         if negative_slope != 0.0:\n             x -= negative_slope * negative_part\n\n@@ -477,7 +477,6 @@ class AutocastScope:\n     \"\"\"\n \n     def __init__(self, dtype):\n-        if dtype is not None:\n         dtype = standardize_dtype(dtype)\n         if not is_float_dtype(dtype):\n             raise ValueError(\n@@ -491,7 +490,7 @@ class AutocastScope:\n     def maybe_cast(self, value):\n         from keras_core import backend\n \n-        if self.dtype is not None and is_float_dtype(value.dtype):\n+        if is_float_dtype(value.dtype):\n             return backend.cast(value, dtype=self.dtype)\n         return value\n \n\n@@ -101,11 +101,6 @@ def affine_transform(\n             f\"transform.shape={transform.shape}\"\n         )\n \n-    # scipy.ndimage.map_coordinates lacks support for half precision.\n-    input_dtype = image.dtype\n-    if input_dtype == \"float16\":\n-        image = image.astype(\"float32\")\n-\n     # unbatched case\n     need_squeeze = False\n     if len(image.shape) == 3:\n@@ -170,6 +165,4 @@ def affine_transform(\n         affined = np.transpose(affined, (0, 3, 1, 2))\n     if need_squeeze:\n         affined = np.squeeze(affined, axis=0)\n-    if input_dtype == \"float16\":\n-        affined = affined.astype(input_dtype)\n     return affined\n\n@@ -47,7 +47,7 @@ def resize(\n             resized = tf.transpose(resized, (0, 3, 1, 2))\n         elif len(image.shape) == 3:\n             resized = tf.transpose(resized, (2, 0, 1))\n-    return tf.cast(resized, image.dtype)\n+    return resized\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = (\n\n@@ -243,7 +243,6 @@ class MultiHeadAttention(Layer):\n             activity_regularizer=self._activity_regularizer,\n             kernel_constraint=self._kernel_constraint,\n             bias_constraint=self._bias_constraint,\n-            dtype=self.dtype_policy,\n         )\n         # Create new clone of kernel/bias initializer, so that we don't reuse\n         # the initializer instance, which could lead to same init value since\n@@ -312,10 +311,8 @@ class MultiHeadAttention(Layer):\n                 attn_scores_rank - len(self._attention_axes), attn_scores_rank\n             )\n         )\n-        self._softmax = Softmax(axis=norm_axes, dtype=self.dtype_policy)\n-        self._dropout_layer = Dropout(\n-            rate=self._dropout, dtype=self.dtype_policy\n-        )\n+        self._softmax = Softmax(axis=norm_axes)\n+        self._dropout_layer = Dropout(rate=self._dropout)\n         self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n@@ -361,9 +358,7 @@ class MultiHeadAttention(Layer):\n         # Note: Applying scalar multiply at the smaller end of einsum improves\n         # XLA performance, but may introduce slight numeric differences in\n         # the Transformer attention head.\n-        query = ops.multiply(\n-            query, ops.cast(self._inverse_sqrt_key_dim, query.dtype)\n-        )\n+        query = ops.multiply(query, self._inverse_sqrt_key_dim)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n\n@@ -1,7 +1,6 @@\n import pytest\n \n from keras_core import layers\n-from keras_core import ops\n from keras_core import testing\n \n \n@@ -9,7 +8,7 @@ class ExampleWrapper(layers.Wrapper):\n     \"\"\"Simple Wrapper subclass.\"\"\"\n \n     def call(self, inputs, **kwargs):\n-        return ops.cast(self.layer(inputs, **kwargs), self.compute_dtype)\n+        return self.layer(inputs, **kwargs)\n \n \n class WrapperTest(testing.TestCase):\n\n@@ -18,6 +18,7 @@ And some more magic:\n import collections\n import inspect\n import warnings\n+from functools import wraps\n \n import tree\n \n@@ -206,6 +207,22 @@ class Layer(BackendLayer, Operation):\n     ```\n     \"\"\"\n \n+    def __new__(cls, *args, **kwargs):\n+        # Wrap the user-provided build method in the build_decorator\n+        # to add name scope support and serialization support.\n+        obj = super().__new__(cls, *args, **kwargs)\n+\n+        original_build_method = obj.build\n+\n+        @wraps(original_build_method)\n+        def build_wrapper(*args, **kwargs):\n+            with backend.name_scope(obj.name, caller=obj):\n+                original_build_method(*args, **kwargs)\n+            obj.built = True\n+\n+        obj.build = build_wrapper\n+        return obj\n+\n     def __init__(\n         self,\n         *,\n@@ -739,20 +756,10 @@ class Layer(BackendLayer, Operation):\n         # 7. Call the layer.\n         try:\n             with backend.name_scope(self.name, caller=self):\n-                current_scope = backend.get_autocast_scope()\n-                new_scope = None\n-                if current_scope is not None:\n-                    # Clear or update the current scope if necessary.\n-                    if not self.autocast:\n-                        new_scope = backend.AutocastScope(None)\n-                    elif current_scope.dtype != self.compute_dtype:\n-                        new_scope = backend.AutocastScope(self.compute_dtype)\n-                elif self.compute_dtype != self.variable_dtype:\n-                    # Enter a new scope if our dtypes are \"mixed\".\n-                    new_scope = backend.AutocastScope(self.compute_dtype)\n-\n-                if new_scope is not None:\n-                    with new_scope:\n+                if self.autocast and self.compute_dtype != self.variable_dtype:\n+                    # For mixed precision, we automatically cast layer variables\n+                    # (float ones only) to the compute dtype upon access.\n+                    with backend.AutocastScope(self.compute_dtype):\n                         outputs = super().__call__(*args, **kwargs)\n                 else:\n                     outputs = super().__call__(*args, **kwargs)\n\n@@ -236,7 +236,7 @@ class BatchNormalization(Layer):\n             beta = ops.reshape(self.beta, broadcast_shape)\n             beta = ops.cast(beta, outputs.dtype)\n             outputs = outputs + beta\n-        return ops.cast(outputs, input_dtype)\n+        return outputs\n \n     def get_config(self):\n         base_config = super().get_config()\n\n@@ -79,7 +79,7 @@ class SpectralNormalization(Wrapper):\n             self.normalize_weights()\n \n         output = self.layer(inputs)\n-        return ops.cast(output, inputs.dtype)\n+        return output\n \n     def compute_output_shape(self, input_shape):\n         return self.layer.compute_output_shape(input_shape)\n\n@@ -28,7 +28,6 @@ class SpectralNormalizationTest(testing.TestCase):\n             layers.Conv2D(\n                 1, (2, 2), kernel_initializer=initializers.Constant(value=1)\n             ),\n-            power_iterations=8,\n         )\n \n         result = sn_wrapper(images, training=False)\n\n@@ -105,8 +105,6 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n-            # Incomplete op support on tensorflow.\n-            run_mixed_precision_check=False,\n         )\n \n \n\n@@ -104,8 +104,6 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n-            # Incomplete op support on tensorflow.\n-            run_mixed_precision_check=False,\n         )\n \n \n\n@@ -23,8 +23,6 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n             run_training_check=False,\n-            # Incomplete op support on tensorflow.\n-            run_mixed_precision_check=False,\n         )\n         self.run_layer_test(\n             layers.HashedCrossing,\n@@ -37,8 +35,6 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n             run_training_check=False,\n-            # Incomplete op support on tensorflow.\n-            run_mixed_precision_check=False,\n         )\n \n     def test_correctness(self):\n\n@@ -164,12 +164,14 @@ class Normalization(Layer):\n             self.adapt_mean = self.add_weight(\n                 name=\"mean\",\n                 shape=mean_and_var_shape,\n+                dtype=self.compute_dtype,\n                 initializer=\"zeros\",\n                 trainable=False,\n             )\n             self.adapt_variance = self.add_weight(\n                 name=\"variance\",\n                 shape=mean_and_var_shape,\n+                dtype=self.compute_dtype,\n                 initializer=\"ones\",\n                 trainable=False,\n             )\n\n@@ -67,7 +67,6 @@ class RandomContrast(TFDataLayer):\n                 minval=1.0 - self.lower,\n                 maxval=1.0 + self.upper,\n                 seed=seed_generator,\n-                dtype=self.compute_dtype,\n             )\n \n             outputs = self._adjust_constrast(inputs, factor)\n\n@@ -222,8 +222,6 @@ class Bidirectional(Wrapper):\n         y_rev = self.backward_layer(\n             backward_inputs, initial_state=backward_state, **kwargs\n         )\n-        y = ops.cast(y, self.compute_dtype)\n-        y_rev = ops.cast(y_rev, self.compute_dtype)\n \n         if self.return_state:\n             states = y[1:] + y_rev[1:]\n\n@@ -4,12 +4,15 @@ from keras_core import backend\n from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n-from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras_core.saving import serialization_lib\n from keras_core.utils import tracking\n \n \n+class DropoutRNNCellMixin:\n+    pass\n+\n+\n @keras_core_export(\"keras_core.layers.RNN\")\n class RNN(Layer):\n     \"\"\"Base class for recurrent layers.\n@@ -302,7 +305,7 @@ class RNN(Layer):\n             init_state = get_initial_state_fn(batch_size=batch_size)\n         else:\n             return [\n-                ops.zeros((batch_size, d), dtype=self.cell.compute_dtype)\n+                ops.zeros((batch_size, d), dtype=self.compute_dtype)\n                 for d in self.state_size\n             ]\n \n@@ -384,9 +387,7 @@ class RNN(Layer):\n         # Note that states may be deeply nested\n         # (e.g. in the stacked cells case).\n         initial_state = tree.map_structure(\n-            lambda x: backend.convert_to_tensor(\n-                x, dtype=self.cell.compute_dtype\n-            ),\n+            lambda x: backend.convert_to_tensor(x, dtype=self.compute_dtype),\n             initial_state,\n         )\n \n@@ -396,11 +397,6 @@ class RNN(Layer):\n             mask=mask,\n             training=training,\n         )\n-        last_output = ops.cast(last_output, self.compute_dtype)\n-        outputs = ops.cast(outputs, self.compute_dtype)\n-        states = tree.map_structure(\n-            lambda x: ops.cast(x, dtype=self.compute_dtype), states\n-        )\n         self._maybe_reset_dropout_masks(self.cell)\n \n         if self.stateful:\n@@ -422,7 +418,7 @@ class RNN(Layer):\n         return output\n \n     def _maybe_reset_dropout_masks(self, cell):\n-        if isinstance(cell, DropoutRNNCell):\n+        if isinstance(cell, DropoutRNNCellMixin):\n             cell.reset_dropout_mask()\n             cell.reset_recurrent_dropout_mask()\n         if isinstance(cell, StackedRNNCells):\n\n@@ -18,7 +18,7 @@ class VariableMappingTest(testing.TestCase):\n         model.build((None, 1))\n         model.optimizer.build(model.trainable_variables)\n         variable_map = model._get_variable_map()\n-        self.assertIn(\"dense_1/bias\", variable_map)\n+        self.assertIn(\"sequential/dense_1/bias\", variable_map)\n         self.assertIn(\"adam/learning_rate\", variable_map)\n \n         model = saving_lib_test._get_subclassed_model()\n\n@@ -9,8 +9,6 @@ import tree\n from keras_core import backend\n from keras_core import ops\n from keras_core import utils\n-from keras_core.backend.common import is_float_dtype\n-from keras_core.backend.common import standardize_dtype\n from keras_core.models import Model\n from keras_core.utils import traceback_utils\n \n@@ -125,7 +123,6 @@ class TestCase(unittest.TestCase):\n         expected_mask_shape=None,\n         custom_objects=None,\n         run_training_check=True,\n-        run_mixed_precision_check=True,\n     ):\n         \"\"\"Run basic checks on a layer.\n \n@@ -162,8 +159,6 @@ class TestCase(unittest.TestCase):\n                 considered during deserialization.\n             run_training_check: Whether to attempt to train the layer\n                 (if an input shape or input data was provided).\n-            run_mixed_precision_check: Whether to test the layer with a mixed\n-                precision dtype policy.\n         \"\"\"\n         if input_shape is not None and input_data is not None:\n             raise ValueError(\n@@ -342,27 +337,6 @@ class TestCase(unittest.TestCase):\n             if run_training_check:\n                 run_training_step(layer, input_data, output_data)\n \n-            # Never test mixed precision on torch CPU. Torch lacks support.\n-            if run_mixed_precision_check and backend.backend() == \"torch\":\n-                import torch\n-\n-                run_mixed_precision_check = torch.cuda.is_available()\n-\n-            if run_mixed_precision_check:\n-                layer = layer_cls(**{**init_kwargs, \"dtype\": \"mixed_float16\"})\n-                if isinstance(input_data, dict):\n-                    output_data = layer(**input_data, **call_kwargs)\n-                else:\n-                    output_data = layer(input_data, **call_kwargs)\n-                for tensor in tree.flatten(output_data):\n-                    dtype = standardize_dtype(tensor.dtype)\n-                    if is_float_dtype(dtype):\n-                        self.assertEqual(dtype, \"float16\")\n-                for weight in layer.weights:\n-                    dtype = standardize_dtype(weight.dtype)\n-                    if is_float_dtype(dtype):\n-                        self.assertEqual(dtype, \"float32\")\n-\n \n def create_keras_tensors(input_shape, dtype):\n     from keras_core.backend.common import keras_tensor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab35b9667ed2e764c36d7489dddd9c2566d26df9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 22 | Files Changed: 18 | Hunks: 34 | Methods Changed: 23 | Complexity Δ (Sum/Max): 15/8 | Churn Δ: 111 | Churn Cumulative: 11192 | Contributors (this commit): 23 | Commits (past 90d): 198 | Contributors (cumulative): 104 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -86,6 +86,7 @@ class ReLU(ops.Operation):\n         clip_max = max_value is not None\n         if threshold != 0:\n             # computes x for x > threshold else 0\n+            threshold = ops.cast(threshold, dtype=x.dtype)\n             x = x * backend.cast(\n                 backend.numpy.greater(x, threshold), dtype=x.dtype\n             )\n@@ -97,7 +98,9 @@ class ReLU(ops.Operation):\n             x = backend.nn.relu(x)\n \n         if clip_max:\n-            x = backend.numpy.clip(x, 0.0, max_value)\n+            min_value = ops.cast(0.0, dtype=x.dtype)\n+            max_value = ops.cast(max_value, dtype=x.dtype)\n+            x = backend.numpy.clip(x, min_value, max_value)\n \n         if negative_slope != 0.0:\n             x -= negative_slope * negative_part\n\n@@ -477,6 +477,7 @@ class AutocastScope:\n     \"\"\"\n \n     def __init__(self, dtype):\n+        if dtype is not None:\n             dtype = standardize_dtype(dtype)\n             if not is_float_dtype(dtype):\n                 raise ValueError(\n@@ -490,7 +491,7 @@ class AutocastScope:\n     def maybe_cast(self, value):\n         from keras_core import backend\n \n-        if is_float_dtype(value.dtype):\n+        if self.dtype is not None and is_float_dtype(value.dtype):\n             return backend.cast(value, dtype=self.dtype)\n         return value\n \n\n@@ -101,6 +101,11 @@ def affine_transform(\n             f\"transform.shape={transform.shape}\"\n         )\n \n+    # scipy.ndimage.map_coordinates lacks support for half precision.\n+    input_dtype = image.dtype\n+    if input_dtype == \"float16\":\n+        image = image.astype(\"float32\")\n+\n     # unbatched case\n     need_squeeze = False\n     if len(image.shape) == 3:\n@@ -165,4 +170,6 @@ def affine_transform(\n         affined = np.transpose(affined, (0, 3, 1, 2))\n     if need_squeeze:\n         affined = np.squeeze(affined, axis=0)\n+    if input_dtype == \"float16\":\n+        affined = affined.astype(input_dtype)\n     return affined\n\n@@ -47,7 +47,7 @@ def resize(\n             resized = tf.transpose(resized, (0, 3, 1, 2))\n         elif len(image.shape) == 3:\n             resized = tf.transpose(resized, (2, 0, 1))\n-    return resized\n+    return tf.cast(resized, image.dtype)\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = (\n\n@@ -243,6 +243,7 @@ class MultiHeadAttention(Layer):\n             activity_regularizer=self._activity_regularizer,\n             kernel_constraint=self._kernel_constraint,\n             bias_constraint=self._bias_constraint,\n+            dtype=self.dtype_policy,\n         )\n         # Create new clone of kernel/bias initializer, so that we don't reuse\n         # the initializer instance, which could lead to same init value since\n@@ -311,8 +312,10 @@ class MultiHeadAttention(Layer):\n                 attn_scores_rank - len(self._attention_axes), attn_scores_rank\n             )\n         )\n-        self._softmax = Softmax(axis=norm_axes)\n-        self._dropout_layer = Dropout(rate=self._dropout)\n+        self._softmax = Softmax(axis=norm_axes, dtype=self.dtype_policy)\n+        self._dropout_layer = Dropout(\n+            rate=self._dropout, dtype=self.dtype_policy\n+        )\n         self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n \n     def _masked_softmax(self, attention_scores, attention_mask=None):\n@@ -358,7 +361,9 @@ class MultiHeadAttention(Layer):\n         # Note: Applying scalar multiply at the smaller end of einsum improves\n         # XLA performance, but may introduce slight numeric differences in\n         # the Transformer attention head.\n-        query = ops.multiply(query, self._inverse_sqrt_key_dim)\n+        query = ops.multiply(\n+            query, ops.cast(self._inverse_sqrt_key_dim, query.dtype)\n+        )\n \n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n\n@@ -1,6 +1,7 @@\n import pytest\n \n from keras_core import layers\n+from keras_core import ops\n from keras_core import testing\n \n \n@@ -8,7 +9,7 @@ class ExampleWrapper(layers.Wrapper):\n     \"\"\"Simple Wrapper subclass.\"\"\"\n \n     def call(self, inputs, **kwargs):\n-        return self.layer(inputs, **kwargs)\n+        return ops.cast(self.layer(inputs, **kwargs), self.compute_dtype)\n \n \n class WrapperTest(testing.TestCase):\n\n@@ -756,10 +756,20 @@ class Layer(BackendLayer, Operation):\n         # 7. Call the layer.\n         try:\n             with backend.name_scope(self.name, caller=self):\n-                if self.autocast and self.compute_dtype != self.variable_dtype:\n-                    # For mixed precision, we automatically cast layer variables\n-                    # (float ones only) to the compute dtype upon access.\n-                    with backend.AutocastScope(self.compute_dtype):\n+                current_scope = backend.get_autocast_scope()\n+                new_scope = None\n+                if current_scope is not None:\n+                    # Clear or update the current scope if necessary.\n+                    if not self.autocast:\n+                        new_scope = backend.AutocastScope(None)\n+                    elif current_scope.dtype != self.compute_dtype:\n+                        new_scope = backend.AutocastScope(self.compute_dtype)\n+                elif self.compute_dtype != self.variable_dtype:\n+                    # Enter a new scope if our dtypes are \"mixed\".\n+                    new_scope = backend.AutocastScope(self.compute_dtype)\n+\n+                if new_scope is not None:\n+                    with new_scope:\n                         outputs = super().__call__(*args, **kwargs)\n                 else:\n                     outputs = super().__call__(*args, **kwargs)\n\n@@ -236,7 +236,7 @@ class BatchNormalization(Layer):\n             beta = ops.reshape(self.beta, broadcast_shape)\n             beta = ops.cast(beta, outputs.dtype)\n             outputs = outputs + beta\n-        return outputs\n+        return ops.cast(outputs, input_dtype)\n \n     def get_config(self):\n         base_config = super().get_config()\n\n@@ -79,7 +79,7 @@ class SpectralNormalization(Wrapper):\n             self.normalize_weights()\n \n         output = self.layer(inputs)\n-        return output\n+        return ops.cast(output, inputs.dtype)\n \n     def compute_output_shape(self, input_shape):\n         return self.layer.compute_output_shape(input_shape)\n\n@@ -28,6 +28,7 @@ class SpectralNormalizationTest(testing.TestCase):\n             layers.Conv2D(\n                 1, (2, 2), kernel_initializer=initializers.Constant(value=1)\n             ),\n+            power_iterations=8,\n         )\n \n         result = sn_wrapper(images, training=False)\n\n@@ -105,6 +105,8 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n \n \n\n@@ -104,6 +104,8 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n             expected_num_non_trainable_weights=0,\n             expected_num_losses=0,\n             supports_masking=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n \n \n\n@@ -23,6 +23,8 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n             run_training_check=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n         self.run_layer_test(\n             layers.HashedCrossing,\n@@ -35,6 +37,8 @@ class HashedCrossingTest(testing.TestCase):\n             expected_num_losses=0,\n             supports_masking=False,\n             run_training_check=False,\n+            # Incomplete op support on tensorflow.\n+            run_mixed_precision_check=False,\n         )\n \n     def test_correctness(self):\n\n@@ -164,14 +164,12 @@ class Normalization(Layer):\n             self.adapt_mean = self.add_weight(\n                 name=\"mean\",\n                 shape=mean_and_var_shape,\n-                dtype=self.compute_dtype,\n                 initializer=\"zeros\",\n                 trainable=False,\n             )\n             self.adapt_variance = self.add_weight(\n                 name=\"variance\",\n                 shape=mean_and_var_shape,\n-                dtype=self.compute_dtype,\n                 initializer=\"ones\",\n                 trainable=False,\n             )\n\n@@ -67,6 +67,7 @@ class RandomContrast(TFDataLayer):\n                 minval=1.0 - self.lower,\n                 maxval=1.0 + self.upper,\n                 seed=seed_generator,\n+                dtype=self.compute_dtype,\n             )\n \n             outputs = self._adjust_constrast(inputs, factor)\n\n@@ -222,6 +222,8 @@ class Bidirectional(Wrapper):\n         y_rev = self.backward_layer(\n             backward_inputs, initial_state=backward_state, **kwargs\n         )\n+        y = ops.cast(y, self.compute_dtype)\n+        y_rev = ops.cast(y_rev, self.compute_dtype)\n \n         if self.return_state:\n             states = y[1:] + y_rev[1:]\n\n@@ -4,15 +4,12 @@ from keras_core import backend\n from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras_core.saving import serialization_lib\n from keras_core.utils import tracking\n \n \n-class DropoutRNNCellMixin:\n-    pass\n-\n-\n @keras_core_export(\"keras_core.layers.RNN\")\n class RNN(Layer):\n     \"\"\"Base class for recurrent layers.\n@@ -305,7 +302,7 @@ class RNN(Layer):\n             init_state = get_initial_state_fn(batch_size=batch_size)\n         else:\n             return [\n-                ops.zeros((batch_size, d), dtype=self.compute_dtype)\n+                ops.zeros((batch_size, d), dtype=self.cell.compute_dtype)\n                 for d in self.state_size\n             ]\n \n@@ -387,7 +384,9 @@ class RNN(Layer):\n         # Note that states may be deeply nested\n         # (e.g. in the stacked cells case).\n         initial_state = tree.map_structure(\n-            lambda x: backend.convert_to_tensor(x, dtype=self.compute_dtype),\n+            lambda x: backend.convert_to_tensor(\n+                x, dtype=self.cell.compute_dtype\n+            ),\n             initial_state,\n         )\n \n@@ -397,6 +396,11 @@ class RNN(Layer):\n             mask=mask,\n             training=training,\n         )\n+        last_output = ops.cast(last_output, self.compute_dtype)\n+        outputs = ops.cast(outputs, self.compute_dtype)\n+        states = tree.map_structure(\n+            lambda x: ops.cast(x, dtype=self.compute_dtype), states\n+        )\n         self._maybe_reset_dropout_masks(self.cell)\n \n         if self.stateful:\n@@ -418,7 +422,7 @@ class RNN(Layer):\n         return output\n \n     def _maybe_reset_dropout_masks(self, cell):\n-        if isinstance(cell, DropoutRNNCellMixin):\n+        if isinstance(cell, DropoutRNNCell):\n             cell.reset_dropout_mask()\n             cell.reset_recurrent_dropout_mask()\n         if isinstance(cell, StackedRNNCells):\n\n@@ -9,6 +9,8 @@ import tree\n from keras_core import backend\n from keras_core import ops\n from keras_core import utils\n+from keras_core.backend.common import is_float_dtype\n+from keras_core.backend.common import standardize_dtype\n from keras_core.models import Model\n from keras_core.utils import traceback_utils\n \n@@ -123,6 +125,7 @@ class TestCase(unittest.TestCase):\n         expected_mask_shape=None,\n         custom_objects=None,\n         run_training_check=True,\n+        run_mixed_precision_check=True,\n     ):\n         \"\"\"Run basic checks on a layer.\n \n@@ -159,6 +162,8 @@ class TestCase(unittest.TestCase):\n                 considered during deserialization.\n             run_training_check: Whether to attempt to train the layer\n                 (if an input shape or input data was provided).\n+            run_mixed_precision_check: Whether to test the layer with a mixed\n+                precision dtype policy.\n         \"\"\"\n         if input_shape is not None and input_data is not None:\n             raise ValueError(\n@@ -337,6 +342,27 @@ class TestCase(unittest.TestCase):\n             if run_training_check:\n                 run_training_step(layer, input_data, output_data)\n \n+            # Never test mixed precision on torch CPU. Torch lacks support.\n+            if run_mixed_precision_check and backend.backend() == \"torch\":\n+                import torch\n+\n+                run_mixed_precision_check = torch.cuda.is_available()\n+\n+            if run_mixed_precision_check:\n+                layer = layer_cls(**{**init_kwargs, \"dtype\": \"mixed_float16\"})\n+                if isinstance(input_data, dict):\n+                    output_data = layer(**input_data, **call_kwargs)\n+                else:\n+                    output_data = layer(input_data, **call_kwargs)\n+                for tensor in tree.flatten(output_data):\n+                    dtype = standardize_dtype(tensor.dtype)\n+                    if is_float_dtype(dtype):\n+                        self.assertEqual(dtype, \"float16\")\n+                for weight in layer.weights:\n+                    dtype = standardize_dtype(weight.dtype)\n+                    if is_float_dtype(dtype):\n+                        self.assertEqual(dtype, \"float32\")\n+\n \n def create_keras_tensors(input_shape, dtype):\n     from keras_core.backend.common import keras_tensor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
