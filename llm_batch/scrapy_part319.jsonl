{"custom_id": "scrapy#722a33a2ac8ebc22bb7a7056598898dcb76e98a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 838 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -458,7 +458,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         request = Request('https://example.com')\n         spider = self.get_spider()\n         with LogCapture(attributes=('spider',)) as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n             )\n@@ -468,7 +468,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         request = Request('https://example.com')\n         spider = self.get_spider()\n         with LogCapture(attributes=('spider',)) as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n                 max_retry_times=0,\n@@ -480,7 +480,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         spider = self.get_spider()\n         expected_reason = 'because'\n         with LogCapture() as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n                 reason=expected_reason,\n@@ -503,7 +503,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         expected_reason = NotImplementedError()\n         expected_reason_string = 'builtins.NotImplementedError'\n         with LogCapture() as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n                 reason=expected_reason,\n@@ -528,7 +528,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         expected_reason = NotImplementedError\n         expected_reason_string = 'builtins.NotImplementedError'\n         with LogCapture() as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n                 reason=expected_reason,\n@@ -553,7 +553,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         expected_reason = IgnoreRequest()\n         expected_reason_string = 'scrapy.exceptions.IgnoreRequest'\n         with LogCapture() as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n                 reason=expected_reason,\n@@ -578,7 +578,7 @@ class GetRetryRequestTest(unittest.TestCase):\n         expected_reason = IgnoreRequest\n         expected_reason_string = 'scrapy.exceptions.IgnoreRequest'\n         with LogCapture() as log:\n-            new_request = get_retry_request(\n+            get_retry_request(\n                 request,\n                 spider=spider,\n                 reason=expected_reason,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f7665c4cfb955ca4e81d7bdb249cd88ada788c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 840 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -398,7 +398,7 @@ class GetRetryRequestTest(unittest.TestCase):\n     def test_no_spider(self):\n         request = Request('https://example.com')\n         with self.assertRaises(TypeError):\n-            get_retry_request(request)\n+            get_retry_request(request)  # pylint: disable=missing-kwoa\n \n     def test_max_retry_times_setting(self):\n         max_retry_times = 0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6326178bc5824e8c08d84b6543de6977a16fb8d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 34 | Churn Cumulative: 2435 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,13 +33,16 @@ from scrapy.spiders import Spider\n logger = logging.getLogger(__name__)\n \n \n+PROTOCOL_NAME = b\"h2\"\n+\n+\n class InvalidNegotiatedProtocol(H2Error):\n \n-    def __init__(self, negotiated_protocol: str) -> None:\n+    def __init__(self, negotiated_protocol: bytes) -> None:\n         self.negotiated_protocol = negotiated_protocol\n \n     def __str__(self) -> str:\n-        return f'InvalidNegotiatedProtocol: Expected h2 as negotiated protocol, received {self.negotiated_protocol!r}'\n+        return (f\"Expected {PROTOCOL_NAME!r}, received {self.negotiated_protocol!r}\")\n \n \n class RemoteTerminatedConnection(H2Error):\n@@ -52,7 +55,7 @@ class RemoteTerminatedConnection(H2Error):\n         self.terminate_event = event\n \n     def __str__(self) -> str:\n-        return f'RemoteTerminatedConnection: Received GOAWAY frame from {self.remote_ip_address!r}'\n+        return f'Received GOAWAY frame from {self.remote_ip_address!r}'\n \n \n class MethodNotAllowed405(H2Error):\n@@ -60,7 +63,7 @@ class MethodNotAllowed405(H2Error):\n         self.remote_ip_address = remote_ip_address\n \n     def __str__(self) -> str:\n-        return f\"MethodNotAllowed405: Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"\n+        return f\"Received 'HTTP/2.0 405 Method Not Allowed' from {self.remote_ip_address!r}\"\n \n \n @implementer(IHandshakeListener)\n@@ -231,13 +234,12 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.transport.loseConnection()\n \n     def handshakeCompleted(self) -> None:\n-        \"\"\"We close the connection with InvalidNegotiatedProtocol exception\n-        when the connection was not made via h2 protocol\"\"\"\n-        protocol = self.transport.negotiatedProtocol\n-        if protocol is not None and protocol != b\"h2\":\n-            # Here we have not initiated the connection yet\n-            # So, no need to send a GOAWAY frame to the remote\n-            self._lose_connection_with_error([InvalidNegotiatedProtocol(protocol.decode(\"utf-8\"))])\n+        \"\"\"\n+        Close the connection if it's not made via the expected protocol\n+        \"\"\"\n+        if self.transport.negotiatedProtocol is not None and self.transport.negotiatedProtocol != PROTOCOL_NAME:\n+            # we have not initiated the connection yet, no need to send a GOAWAY frame to the remote peer\n+            self._lose_connection_with_error([InvalidNegotiatedProtocol(self.transport.negotiatedProtocol)])\n \n     def _check_received_data(self, data: bytes) -> None:\n         \"\"\"Checks for edge cases where the connection to remote fails\n@@ -414,4 +416,4 @@ class H2ClientFactory(Factory):\n         return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n \n     def acceptableProtocols(self) -> List[bytes]:\n-        return [b'h2']\n+        return [PROTOCOL_NAME]\n\n@@ -5,6 +5,7 @@ import re\n import shutil\n import string\n from ipaddress import IPv4Address\n+from unittest import mock\n from urllib.parse import urlencode\n \n from h2.exceptions import InvalidBodyLengthError\n@@ -381,6 +382,13 @@ class Https2ClientProtocolTestCase(TestCase):\n             200\n         )\n \n+    @inlineCallbacks\n+    def test_invalid_negotiated_protocol(self):\n+        with mock.patch(\"scrapy.core.http2.protocol.PROTOCOL_NAME\", return_value=b\"not-h2\"):\n+            request = Request(url=self.get_url('/status?n=200'))\n+            with self.assertRaises(ResponseFailed):\n+                yield self.make_request(request)\n+\n     def test_cancel_request(self):\n         request = Request(url=self.get_url('/get-data-html-large'))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7605f19ec429fd3adb6a18da8e48143f99f6bfec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 34 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,6 +73,21 @@ class Https2TestCase(Https11TestCase):\n     def test_download_broken_chunked_content_allow_data_loss_via_setting(self):\n         raise unittest.SkipTest(self.HTTP2_DATALOSS_SKIP_REASON)\n \n+    def test_concurrent_requests_same_domain(self):\n+        spider = Spider('foo')\n+\n+        request1 = Request(self.getURL('file'))\n+        d1 = self.download_request(request1, spider)\n+        d1.addCallback(lambda r: r.body)\n+        d1.addCallback(self.assertEqual, b\"0123456789\")\n+\n+        request2 = Request(self.getURL('echo'), method='POST')\n+        d2 = self.download_request(request2, spider)\n+        d2.addCallback(lambda r: r.headers['Content-Length'])\n+        d2.addCallback(self.assertEqual, b\"79\")\n+\n+        return defer.DeferredList([d1, d2])\n+\n \n class Https2WrongHostnameTestCase(Https2TestCase):\n     tls_log_message = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bd29f32dee445c77e7f2427a3047b7c74505efb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 1404 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -70,7 +70,7 @@ class MethodNotAllowed405(H2Error):\n class H2ClientProtocol(Protocol, TimeoutMixin):\n     IDLE_TIMEOUT = 240\n \n-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Optional[Deferred] = None) -> None:\n+    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Deferred) -> None:\n         \"\"\"\n         Arguments:\n             uri -- URI of the base url to which HTTP/2 Connection will be made.\n@@ -308,7 +308,6 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         if not reason.check(connectionDone):\n             self._conn_lost_errors.append(reason)\n \n-        if self._conn_lost_deferred:\n         self._conn_lost_deferred.callback(self._conn_lost_errors)\n \n         for stream in self.streams.values():\n@@ -407,7 +406,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n @implementer(IProtocolNegotiationFactory)\n class H2ClientFactory(Factory):\n-    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Optional[Deferred] = None) -> None:\n+    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Deferred) -> None:\n         self.uri = uri\n         self.settings = settings\n         self.conn_lost_deferred = conn_lost_deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ba31cd1a268475db1f3cc64d4e6febb477e8f77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 1479 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -431,7 +431,8 @@ class Stream:\n             errors.insert(0, InactiveStreamClosed(self._request))\n             self._deferred_response.errback(ResponseFailed(errors))\n \n-        elif reason is StreamCloseReason.INVALID_HOSTNAME:\n+        else:\n+            assert reason is StreamCloseReason.INVALID_HOSTNAME\n             self._deferred_response.errback(InvalidHostname(\n                 self._request,\n                 str(self._protocol.metadata['uri'].host, 'utf-8'),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#510109420733e93db0d525302555224a8364ed5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 43 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from unittest import mock\n \n+from pytest import mark\n from twisted.internet import defer, error, reactor\n from twisted.trial import unittest\n from twisted.web import server\n@@ -88,6 +89,14 @@ class Https2TestCase(Https11TestCase):\n \n         return defer.DeferredList([d1, d2])\n \n+    @mark.xfail(reason=\"https://github.com/python-hyper/h2/issues/1247\")\n+    def test_connect_request(self):\n+        request = Request(self.getURL('file'), method='CONNECT')\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.body)\n+        d.addCallback(self.assertEqual, b'')\n+        return d\n+\n \n class Https2WrongHostnameTestCase(Https2TestCase):\n     tls_log_message = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12064d799b8f15eef770a615237932b880b594a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 67 | Churn Cumulative: 1589 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.7592592592592593\n\nDIFF:\n@@ -220,11 +220,24 @@ class Stream:\n                 (':path', path),\n             ]\n \n-        for name, value in self._request.headers.items():\n-            headers.append((str(name, 'utf-8'), str(value[0], 'utf-8')))\n+        content_length = str(len(self._request.body))\n+        headers.append(('Content-Length', content_length))\n \n-        if b'Content-Length' not in self._request.headers.keys():\n-            headers.append(('Content-Length', str(len(self._request.body))))\n+        content_length_name = self._request.headers.normkey(b'Content-Length')\n+        for name, values in self._request.headers.items():\n+            for value in values:\n+                value = str(value, 'utf-8')\n+                if name == content_length_name:\n+                    if value != content_length:\n+                        logger.warning(\n+                            'Ignoring bad Content-Length header %r of request %r, '\n+                            'sending %r instead',\n+                            value,\n+                            self._request,\n+                            content_length,\n+                        )\n+                    continue\n+                headers.append((str(name, 'utf-8'), value))\n \n         return headers\n \n\n@@ -1,6 +1,8 @@\n+import json\n from unittest import mock\n \n from pytest import mark\n+from testfixtures import LogCapture\n from twisted.internet import defer, error, reactor\n from twisted.trial import unittest\n from twisted.web import server\n@@ -97,6 +99,50 @@ class Https2TestCase(Https11TestCase):\n         d.addCallback(self.assertEqual, b'')\n         return d\n \n+    def test_custom_content_length_good(self):\n+        request = Request(self.getURL('contentlength'))\n+        custom_content_length = str(len(request.body))\n+        request.headers['Content-Length'] = custom_content_length\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.text)\n+        d.addCallback(self.assertEqual, custom_content_length)\n+        return d\n+\n+    def test_custom_content_length_bad(self):\n+        request = Request(self.getURL('contentlength'))\n+        actual_content_length = str(len(request.body))\n+        bad_content_length = str(len(request.body)+1)\n+        request.headers['Content-Length'] = bad_content_length\n+        log = LogCapture()\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: r.text)\n+        d.addCallback(self.assertEqual, actual_content_length)\n+        d.addCallback(\n+            lambda _: log.check_present(\n+                (\n+                    'scrapy.core.http2.stream',\n+                    'WARNING',\n+                    f'Ignoring bad Content-Length header '\n+                    f'{bad_content_length!r} of request {request}, sending '\n+                    f'{actual_content_length!r} instead',\n+                )\n+            )\n+        )\n+        d.addCallback(\n+            lambda _: log.uninstall()\n+        )\n+        return d\n+\n+    def test_duplicate_header(self):\n+        request = Request(self.getURL('echo'))\n+        header, value1, value2 = 'Custom-Header', 'foo', 'bar'\n+        request.headers.appendlist(header, value1)\n+        request.headers.appendlist(header, value2)\n+        d = self.download_request(request, Spider('foo'))\n+        d.addCallback(lambda r: json.loads(r.text)['headers'][header])\n+        d.addCallback(self.assertEqual, [value1, value2])\n+        return d\n+\n \n class Https2WrongHostnameTestCase(Https2TestCase):\n     tls_log_message = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3894ebb1497b32959c405201f2e010292cf65098", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 36 | Churn Cumulative: 162 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,6 +34,26 @@ for argument in safe_to_ignore_arguments:\n     curl_parser.add_argument(*argument, action='store_true')\n \n \n+def _parse_headers_and_cookies(parsed_args):\n+    headers = []\n+    cookies = {}\n+    for header in parsed_args.headers or ():\n+        name, val = header.split(':', 1)\n+        name = name.strip()\n+        val = val.strip()\n+        if name.title() == 'Cookie':\n+            for name, morsel in SimpleCookie(val).items():\n+                cookies[name] = morsel.value\n+        else:\n+            headers.append((name, val))\n+\n+    if parsed_args.auth:\n+        user, password = parsed_args.auth.split(':', 1)\n+        headers.append(('Authorization', basic_auth_header(user, password)))\n+\n+    return headers, cookies\n+\n+\n def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n     \"\"\"Convert a cURL command syntax to Request kwargs.\n \n@@ -70,21 +90,7 @@ def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n \n     result = {'method': method.upper(), 'url': url}\n \n-    headers = []\n-    cookies = {}\n-    for header in parsed_args.headers or ():\n-        name, val = header.split(':', 1)\n-        name = name.strip()\n-        val = val.strip()\n-        if name.title() == 'Cookie':\n-            for name, morsel in SimpleCookie(val).items():\n-                cookies[name] = morsel.value\n-        else:\n-            headers.append((name, val))\n-\n-    if parsed_args.auth:\n-        user, password = parsed_args.auth.split(':', 1)\n-        headers.append(('Authorization', basic_auth_header(user, password)))\n+    headers, cookies = _parse_headers_and_cookies(parsed_args)\n \n     if headers:\n         result['headers'] = headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e62355271fa39e67b06f00f5601cdb848c7894e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 60 | Churn Cumulative: 1241 | Contributors (this commit): 14 | Commits (past 90d): 10 | Contributors (cumulative): 21 | DMM Complexity: 0.9411764705882353\n\nDIFF:\n@@ -29,7 +29,8 @@ from scrapy.utils.response import response_status_message\n from scrapy.core.downloader.handlers.http11 import TunnelError\n from scrapy.utils.python import global_object_name\n \n-logger = logging.getLogger(__name__)\n+\n+retry_logger = logging.getLogger(__name__)\n \n \n def get_retry_request(\n@@ -39,6 +40,8 @@ def get_retry_request(\n     reason='unspecified',\n     max_retry_times=None,\n     priority_adjust=None,\n+    logger=retry_logger,\n+    stats_base_key='retry',\n ):\n     \"\"\"\n     Returns a new :class:`~scrapy.Request` object to retry the specified\n@@ -76,6 +79,11 @@ def get_retry_request(\n     *priority_adjust* is a number that determines how the priority of the new\n     request changes in relation to *request*. If not specified, the number is\n     read from the :setting:`RETRY_PRIORITY_ADJUST` setting.\n+\n+    *logger* is the logging.Logger object to be used when logging messages\n+\n+    *stats_base_key* is a string to be used as the base key for the\n+    retry-related job stats\n     \"\"\"\n     settings = spider.crawler.settings\n     stats = spider.crawler.stats\n@@ -102,11 +110,11 @@ def get_retry_request(\n         if isinstance(reason, Exception):\n             reason = global_object_name(reason.__class__)\n \n-        stats.inc_value('retry/count')\n-        stats.inc_value(f'retry/reason_count/{reason}')\n+        stats.inc_value(f'{stats_base_key}/count')\n+        stats.inc_value(f'{stats_base_key}/reason_count/{reason}')\n         return new_request\n     else:\n-        stats.inc_value('retry/max_reached')\n+        stats.inc_value(f'{stats_base_key}/max_reached')\n         logger.error(\n             \"Gave up retrying %(request)s (failed %(retry_times)d times): \"\n             \"%(reason)s\",\n\n@@ -1,4 +1,7 @@\n+import logging\n import unittest\n+\n+from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.internet.error import (\n     ConnectError,\n@@ -9,17 +12,12 @@ from twisted.internet.error import (\n )\n from twisted.web.client import ResponseFailed\n \n-from scrapy.downloadermiddlewares.retry import (\n-    get_retry_request,\n-    RetryMiddleware,\n-)\n+from scrapy.downloadermiddlewares.retry import get_retry_request, RetryMiddleware\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n-from testfixtures import LogCapture\n-\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n@@ -597,6 +595,40 @@ class GetRetryRequestTest(unittest.TestCase):\n             )\n         )\n \n+    def test_custom_logger(self):\n+        logger = logging.getLogger(\"custom-logger\")\n+        request = Request(\"https://example.com\")\n+        spider = self.get_spider()\n+        expected_reason = \"because\"\n+        with LogCapture() as log:\n+            get_retry_request(\n+                request,\n+                spider=spider,\n+                reason=expected_reason,\n+                logger=logger,\n+            )\n+        log.check_present(\n+            (\n+                \"custom-logger\",\n+                \"DEBUG\",\n+                f\"Retrying {request} (failed 1 times): {expected_reason}\",\n+            )\n+        )\n+\n+    def test_custom_stats_key(self):\n+        request = Request(\"https://example.com\")\n+        spider = self.get_spider()\n+        expected_reason = \"because\"\n+        stats_key = \"custom_retry\"\n+        get_retry_request(\n+            request,\n+            spider=spider,\n+            reason=expected_reason,\n+            stats_base_key=stats_key,\n+        )\n+        for stat in (f\"{stats_key}/count\", f\"{stats_key}/reason_count/{expected_reason}\"):\n+            self.assertEqual(spider.crawler.stats.get_value(stat), 1)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3bea5e1a974b7cd99f75edb4ff728a9d7163a805", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 1508 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -362,14 +362,6 @@ class Stream:\n         self._protocol.conn.reset_stream(self.stream_id, ErrorCodes.REFUSED_STREAM)\n         self.close(reason)\n \n-    def _is_data_lost(self) -> bool:\n-        assert self.metadata['stream_closed_server']\n-\n-        expected_size = self._response['flow_controlled_size']\n-        received_body_size = int(self._response['headers'][b'Content-Length'])\n-\n-        return expected_size != received_body_size\n-\n     def close(\n         self,\n         reason: StreamCloseReason,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c160882306a8e33f92815cc12dc8979f04f8f5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 216 | Lines Deleted: 65 | Files Changed: 7 | Hunks: 26 | Methods Changed: 34 | Complexity Δ (Sum/Max): 13/8 | Churn Δ: 281 | Churn Cumulative: 4505 | Contributors (this commit): 37 | Commits (past 90d): 10 | Contributors (cumulative): 84 | DMM Complexity: 0.7722772277227723\n\nDIFF:\n@@ -382,6 +382,29 @@ class ScrapyAgent:\n         return result\n \n     def _cb_bodyready(self, txresponse, request):\n+        headers_received_result = self._crawler.signals.send_catch_log(\n+            signal=signals.headers_received,\n+            headers=Headers(txresponse.headers.getAllRawHeaders()),\n+            body_length=txresponse.length,\n+            request=request,\n+            spider=self._crawler.spider,\n+        )\n+        for handler, result in headers_received_result:\n+            if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n+                logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n+                             {\"request\": request, \"handler\": handler.__qualname__})\n+                txresponse._transport.stopProducing()\n+                with suppress(AttributeError):\n+                    txresponse._transport._producer.loseConnection()\n+                return {\n+                    \"txresponse\": txresponse,\n+                    \"body\": b\"\",\n+                    \"flags\": [\"download_stopped\"],\n+                    \"certificate\": None,\n+                    \"ip_address\": None,\n+                    \"failure\": result if result.value.fail else None,\n+                }\n+\n         # deliverBody hangs for responses without body\n         if txresponse.length == 0:\n             return {\n@@ -529,6 +552,7 @@ class _ResponseReader(protocol.Protocol):\n             if isinstance(result, Failure) and isinstance(result.value, StopDownload):\n                 logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                              {\"request\": self._request, \"handler\": handler.__qualname__})\n+                self.transport.stopProducing()\n                 self.transport._producer.loseConnection()\n                 failure = result if result.value.fail else None\n                 self._finish_response(flags=[\"download_stopped\"], failure=failure)\n\n@@ -17,6 +17,7 @@ request_reached_downloader = object()\n request_left_downloader = object()\n response_received = object()\n response_downloaded = object()\n+headers_received = object()\n bytes_received = object()\n item_scraped = object()\n item_dropped = object()\n\n@@ -390,3 +390,32 @@ class BytesReceivedErrbackSpider(BytesReceivedCallbackSpider):\n     def bytes_received(self, data, request, spider):\n         self.meta[\"bytes_received\"] = data\n         raise StopDownload(fail=True)\n+\n+\n+class HeadersReceivedCallbackSpider(MetaSpider):\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = super().from_crawler(crawler, *args, **kwargs)\n+        crawler.signals.connect(spider.headers_received, signals.headers_received)\n+        return spider\n+\n+    def start_requests(self):\n+        yield Request(self.mockserver.url(\"/status\"), errback=self.errback)\n+\n+    def parse(self, response):\n+        self.meta[\"response\"] = response\n+\n+    def errback(self, failure):\n+        self.meta[\"failure\"] = failure\n+\n+    def headers_received(self, headers, body_length, request, spider):\n+        self.meta[\"headers_received\"] = headers\n+        raise StopDownload(fail=False)\n+\n+\n+class HeadersReceivedErrbackSpider(HeadersReceivedCallbackSpider):\n+\n+    def headers_received(self, headers, body_length, request, spider):\n+        self.meta[\"headers_received\"] = headers\n+        raise StopDownload(fail=True)\n\n@@ -35,6 +35,8 @@ from tests.spiders import (\n     DelaySpider,\n     DuplicateStartRequestsSpider,\n     FollowAllSpider,\n+    HeadersReceivedCallbackSpider,\n+    HeadersReceivedErrbackSpider,\n     SimpleSpider,\n     SingleRequestSpider,\n )\n@@ -496,7 +498,7 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertEqual(str(ip_address), gethostbyname(expected_netloc))\n \n     @defer.inlineCallbacks\n-    def test_stop_download_callback(self):\n+    def test_bytes_received_stop_download_callback(self):\n         crawler = self.runner.create_crawler(BytesReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n@@ -505,7 +507,7 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertLess(len(crawler.spider.meta[\"response\"].body), crawler.spider.full_response_length)\n \n     @defer.inlineCallbacks\n-    def test_stop_download_errback(self):\n+    def test_bytes_received_stop_download_errback(self):\n         crawler = self.runner.create_crawler(BytesReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIsNone(crawler.spider.meta.get(\"response\"))\n@@ -518,3 +520,23 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertLess(\n             len(crawler.spider.meta[\"failure\"].value.response.body),\n             crawler.spider.full_response_length)\n+\n+    @defer.inlineCallbacks\n+    def test_headers_received_stop_download_callback(self):\n+        crawler = self.runner.create_crawler(HeadersReceivedCallbackSpider)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertIsNone(crawler.spider.meta.get(\"failure\"))\n+        self.assertIsInstance(crawler.spider.meta[\"response\"], Response)\n+        self.assertEqual(crawler.spider.meta[\"response\"].headers, crawler.spider.meta.get(\"headers_received\"))\n+\n+    @defer.inlineCallbacks\n+    def test_headers_received_stop_download_errback(self):\n+        crawler = self.runner.create_crawler(HeadersReceivedErrbackSpider)\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertIsNone(crawler.spider.meta.get(\"response\"))\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"], Failure)\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"].value, StopDownload)\n+        self.assertIsInstance(crawler.spider.meta[\"failure\"].value.response, Response)\n+        self.assertEqual(\n+            crawler.spider.meta[\"failure\"].value.response.headers,\n+            crawler.spider.meta.get(\"headers_received\"))\n\n@@ -19,14 +19,12 @@ from urllib.parse import urlparse\n import attr\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n-from testfixtures import LogCapture\n from twisted.internet import defer, reactor\n from twisted.trial import unittest\n from twisted.web import server, static, util\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.exceptions import StopDownload\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n@@ -143,6 +141,7 @@ class CrawlerRun:\n         self.reqreached = []\n         self.itemerror = []\n         self.itemresp = []\n+        self.headers = {}\n         self.bytes = defaultdict(lambda: list())\n         self.signals_caught = {}\n         self.spider_class = spider_class\n@@ -165,6 +164,7 @@ class CrawlerRun:\n         self.crawler = get_crawler(self.spider_class)\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n         self.crawler.signals.connect(self.item_error, signals.item_error)\n+        self.crawler.signals.connect(self.headers_received, signals.headers_received)\n         self.crawler.signals.connect(self.bytes_received, signals.bytes_received)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n@@ -183,6 +183,7 @@ class CrawlerRun:\n             if not name.startswith('_'):\n                 disconnect_all(signal)\n         self.deferred.callback(None)\n+        return self.crawler.stop()\n \n     def geturl(self, path):\n         return f\"http://localhost:{self.portno}{path}\"\n@@ -197,6 +198,9 @@ class CrawlerRun:\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n+    def headers_received(self, headers, body_length, request, spider):\n+        self.headers[request] = headers\n+\n     def bytes_received(self, data, request, spider):\n         self.bytes[request].append(data)\n \n@@ -220,18 +224,7 @@ class CrawlerRun:\n         self.signals_caught[sig] = signalargs\n \n \n-class StopDownloadCrawlerRun(CrawlerRun):\n-    \"\"\"\n-    Make sure raising the StopDownload exception stops the download of the response body\n-    \"\"\"\n-\n-    def bytes_received(self, data, request, spider):\n-        super().bytes_received(data, request, spider)\n-        raise StopDownload(fail=False)\n-\n-\n class EngineTest(unittest.TestCase):\n-\n     @defer.inlineCallbacks\n     def test_crawler(self):\n \n@@ -241,8 +234,8 @@ class EngineTest(unittest.TestCase):\n             self.run = CrawlerRun(spider)\n             yield self.run.run()\n             self._assert_visited_urls()\n-            self._assert_scheduled_requests(urls_to_visit=9)\n-            self._assert_downloaded_responses()\n+            self._assert_scheduled_requests(count=9)\n+            self._assert_downloaded_responses(count=9)\n             self._assert_scraped_items()\n             self._assert_signals_caught()\n             self._assert_bytes_received()\n@@ -251,7 +244,7 @@ class EngineTest(unittest.TestCase):\n     def test_crawler_dupefilter(self):\n         self.run = CrawlerRun(TestDupeFilterSpider)\n         yield self.run.run()\n-        self._assert_scheduled_requests(urls_to_visit=8)\n+        self._assert_scheduled_requests(count=8)\n         self._assert_dropped_requests()\n \n     @defer.inlineCallbacks\n@@ -267,8 +260,8 @@ class EngineTest(unittest.TestCase):\n         urls_expected = {self.run.geturl(p) for p in must_be_visited}\n         assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n \n-    def _assert_scheduled_requests(self, urls_to_visit=None):\n-        self.assertEqual(urls_to_visit, len(self.run.reqplug))\n+    def _assert_scheduled_requests(self, count=None):\n+        self.assertEqual(count, len(self.run.reqplug))\n \n         paths_expected = ['/item999.html', '/item2.html', '/item1.html']\n \n@@ -286,10 +279,10 @@ class EngineTest(unittest.TestCase):\n     def _assert_dropped_requests(self):\n         self.assertEqual(len(self.run.reqdropped), 1)\n \n-    def _assert_downloaded_responses(self):\n+    def _assert_downloaded_responses(self, count):\n         # response tests\n-        self.assertEqual(9, len(self.run.respplug))\n-        self.assertEqual(9, len(self.run.reqreached))\n+        self.assertEqual(count, len(self.run.respplug))\n+        self.assertEqual(count, len(self.run.reqreached))\n \n         for response, _ in self.run.respplug:\n             if self.run.getpath(response.url) == '/item999.html':\n@@ -323,6 +316,13 @@ class EngineTest(unittest.TestCase):\n                 self.assertEqual('Item 2 name', item['name'])\n                 self.assertEqual('200', item['price'])\n \n+    def _assert_headers_received(self):\n+        for headers in self.run.headers.values():\n+            self.assertIn(b\"Server\", headers)\n+            self.assertIn(b\"TwistedWeb\", headers[b\"Server\"])\n+            self.assertIn(b\"Date\", headers)\n+            self.assertIn(b\"Content-Type\", headers)\n+\n     def _assert_bytes_received(self):\n         self.assertEqual(9, len(self.run.bytes))\n         for request, data in self.run.bytes.items():\n@@ -371,6 +371,7 @@ class EngineTest(unittest.TestCase):\n         assert signals.spider_opened in self.run.signals_caught\n         assert signals.spider_idle in self.run.signals_caught\n         assert signals.spider_closed in self.run.signals_caught\n+        assert signals.headers_received in self.run.signals_caught\n \n         self.assertEqual({'spider': self.run.spider},\n                          self.run.signals_caught[signals.spider_opened])\n@@ -403,48 +404,6 @@ class EngineTest(unittest.TestCase):\n         self.assertEqual(len(e.open_spiders), 0)\n \n \n-class StopDownloadEngineTest(EngineTest):\n-\n-    @defer.inlineCallbacks\n-    def test_crawler(self):\n-        for spider in TestSpider, DictItemsSpider:\n-            self.run = StopDownloadCrawlerRun(spider)\n-            with LogCapture() as log:\n-                yield self.run.run()\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n-                                   \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/redirected> \"\n-                                   \"from signal handler\"\n-                                   \" StopDownloadCrawlerRun.bytes_received\"))\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n-                                   \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/> \"\n-                                   \"from signal handler\"\n-                                   \" StopDownloadCrawlerRun.bytes_received\"))\n-                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n-                                   \"DEBUG\",\n-                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/numbers> \"\n-                                   \"from signal handler\"\n-                                   \" StopDownloadCrawlerRun.bytes_received\"))\n-            self._assert_visited_urls()\n-            self._assert_scheduled_requests(urls_to_visit=9)\n-            self._assert_downloaded_responses()\n-            self._assert_signals_caught()\n-            self._assert_bytes_received()\n-\n-    def _assert_bytes_received(self):\n-        self.assertEqual(9, len(self.run.bytes))\n-        for request, data in self.run.bytes.items():\n-            joined_data = b\"\".join(data)\n-            self.assertTrue(len(data) == 1)  # signal was fired only once\n-            if self.run.getpath(request.url) == \"/numbers\":\n-                # Received bytes are not the complete response. The exact amount depends\n-                # on the buffer size, which can vary, so we only check that the amount\n-                # of received bytes is strictly less than the full response.\n-                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n-                self.assertTrue(len(joined_data) < len(b\"\".join(numbers)))\n-\n-\n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n         start_test_site(debug=True)\n\n@@ -0,0 +1,60 @@\n+from testfixtures import LogCapture\n+from twisted.internet import defer\n+\n+from scrapy.exceptions import StopDownload\n+\n+from tests.test_engine import (\n+    AttrsItemsSpider,\n+    DataClassItemsSpider,\n+    DictItemsSpider,\n+    TestSpider,\n+    CrawlerRun,\n+    EngineTest,\n+)\n+\n+\n+class BytesReceivedCrawlerRun(CrawlerRun):\n+    def bytes_received(self, data, request, spider):\n+        super().bytes_received(data, request, spider)\n+        raise StopDownload(fail=False)\n+\n+\n+class BytesReceivedEngineTest(EngineTest):\n+    @defer.inlineCallbacks\n+    def test_crawler(self):\n+        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n+            if spider is None:\n+                continue\n+            self.run = BytesReceivedCrawlerRun(spider)\n+            with LogCapture() as log:\n+                yield self.run.run()\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/redirected> \"\n+                                   \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/> \"\n+                                   \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/numbers> \"\n+                                   \"from signal handler BytesReceivedCrawlerRun.bytes_received\"))\n+            self._assert_visited_urls()\n+            self._assert_scheduled_requests(count=9)\n+            self._assert_downloaded_responses(count=9)\n+            self._assert_signals_caught()\n+            self._assert_headers_received()\n+            self._assert_bytes_received()\n+\n+    def _assert_bytes_received(self):\n+        self.assertEqual(9, len(self.run.bytes))\n+        for request, data in self.run.bytes.items():\n+            joined_data = b\"\".join(data)\n+            self.assertTrue(len(data) == 1)  # signal was fired only once\n+            if self.run.getpath(request.url) == \"/numbers\":\n+                # Received bytes are not the complete response. The exact amount depends\n+                # on the buffer size, which can vary, so we only check that the amount\n+                # of received bytes is strictly less than the full response.\n+                numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n+                self.assertTrue(len(joined_data) < len(b\"\".join(numbers)))\n\n@@ -0,0 +1,56 @@\n+from testfixtures import LogCapture\n+from twisted.internet import defer\n+\n+from scrapy.exceptions import StopDownload\n+\n+from tests.test_engine import (\n+    AttrsItemsSpider,\n+    DataClassItemsSpider,\n+    DictItemsSpider,\n+    TestSpider,\n+    CrawlerRun,\n+    EngineTest,\n+)\n+\n+\n+class HeadersReceivedCrawlerRun(CrawlerRun):\n+    def headers_received(self, headers, body_length, request, spider):\n+        super().headers_received(headers, body_length, request, spider)\n+        raise StopDownload(fail=False)\n+\n+\n+class HeadersReceivedEngineTest(EngineTest):\n+    @defer.inlineCallbacks\n+    def test_crawler(self):\n+        for spider in (TestSpider, DictItemsSpider, AttrsItemsSpider, DataClassItemsSpider):\n+            if spider is None:\n+                continue\n+            self.run = HeadersReceivedCrawlerRun(spider)\n+            with LogCapture() as log:\n+                yield self.run.run()\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/redirected> from\"\n+                                   \" signal handler HeadersReceivedCrawlerRun.headers_received\"))\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/> from signal\"\n+                                   \" handler HeadersReceivedCrawlerRun.headers_received\"))\n+                log.check_present((\"scrapy.core.downloader.handlers.http11\",\n+                                   \"DEBUG\",\n+                                   f\"Download stopped for <GET http://localhost:{self.run.portno}/numbers> from\"\n+                                   \" signal handler HeadersReceivedCrawlerRun.headers_received\"))\n+            self._assert_visited_urls()\n+            self._assert_downloaded_responses(count=6)\n+            self._assert_signals_caught()\n+            self._assert_bytes_received()\n+            self._assert_headers_received()\n+\n+    def _assert_bytes_received(self):\n+        self.assertEqual(0, len(self.run.bytes))\n+\n+    def _assert_visited_urls(self):\n+        must_be_visited = [\"/\", \"/redirect\", \"/redirected\"]\n+        urls_visited = {rp[0].url for rp in self.run.respplug}\n+        urls_expected = {self.run.geturl(p) for p in must_be_visited}\n+        assert urls_expected <= urls_visited, f\"URLs not visited: {list(urls_expected - urls_visited)}\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e5ea7924c7e3f5dd958d13db73e04c6348c99ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 11 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 58 | Churn Cumulative: 73 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,22 +14,27 @@ logger = logging.getLogger(__name__)\n \n class UrlLengthMiddleware:\n \n-    def __init__(self, maxlength):\n+    def __init__(self, maxlength, stats):\n         self.maxlength = maxlength\n+        self.stats = stats\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_crawler(cls, crawler):\n+        settings = crawler.settings\n         maxlength = settings.getint('URLLENGTH_LIMIT')\n         if not maxlength:\n             raise NotConfigured\n-        return cls(maxlength)\n+        return cls(maxlength, crawler.stats)\n \n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n             if isinstance(request, Request) and len(request.url) > self.maxlength:\n-                logger.debug(\"Ignoring link (url length > %(maxlength)d): %(url)s \",\n+                logger.info(\n+                    \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n                     {'maxlength': self.maxlength, 'url': request.url},\n-                             extra={'spider': spider})\n+                    extra={'spider': spider}\n+                )\n+                self.stats.inc_value('urllength/request_ignored_count', spider=spider)\n                 return False\n             else:\n                 return True\n\n@@ -1,20 +1,45 @@\n from unittest import TestCase\n \n+from testfixtures import LogCapture\n+\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n+from scrapy.statscollectors import StatsCollector\n+from scrapy.utils.test import get_crawler\n \n \n class TestUrlLengthMiddleware(TestCase):\n \n-    def test_process_spider_output(self):\n-        res = Response('http://scrapytest.org')\n+    def setUp(self):\n+        crawler = get_crawler(Spider)\n+        self.spider = crawler._create_spider('foo')\n \n-        short_url_req = Request('http://scrapytest.org/')\n-        long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n-        reqs = [short_url_req, long_url_req]\n+        self.stats = StatsCollector(crawler)\n+        self.stats.open_spider(self.spider)\n \n-        mw = UrlLengthMiddleware(maxlength=25)\n-        spider = Spider('foo')\n-        out = list(mw.process_spider_output(res, reqs, spider))\n-        self.assertEqual(out, [short_url_req])\n+        self.maxlength = 25\n+        self.mw = UrlLengthMiddleware(maxlength=self.maxlength, stats=self.stats)\n+\n+        self.response = Response('http://scrapytest.org')\n+        self.short_url_req = Request('http://scrapytest.org/')\n+        self.long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n+        self.reqs = [self.short_url_req, self.long_url_req]\n+\n+    def tearDown(self):\n+        self.stats.close_spider(self.spider, '')\n+\n+    def process_spider_output(self):\n+        return list(self.mw.process_spider_output(self.response, self.reqs, self.spider))\n+\n+    def test_middleware_works(self):\n+        self.assertEqual(self.process_spider_output(), [self.short_url_req])\n+\n+    def test_logging(self):\n+        with LogCapture() as log:\n+            self.process_spider_output()\n+\n+        ric = self.stats.get_value('urllength/request_ignored_count', spider=self.spider)\n+        self.assertEqual(ric, 1)\n+\n+        self.assertIn(f'Ignoring link (url length > {self.maxlength})', str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
