{"custom_id": "keras#5ce2df68f46fe8fc8ca62eaebd45ec6a812fbe95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 81 | Churn Cumulative: 638 | Contributors (this commit): 8 | Commits (past 90d): 35 | Contributors (cumulative): 12 | DMM Complexity: 0.45652173913043476\n\nDIFF:\n@@ -7,3 +7,4 @@ from keras_core.backend.common.variables import is_int_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.random import random\n+convert\n\\ No newline at end of file\n\n@@ -1,9 +1,12 @@\n import numpy as np\n \n+from keras_core import ops\n+from keras_core import random\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n+from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n@@ -52,7 +55,9 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -60,13 +65,20 @@ class RandomCrop(Layer):\n             )\n \n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n+        self.data_format = (\n+            \"channels_last\" if data_format is None else \"channels_first\"\n         )\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n@@ -74,8 +86,47 @@ class RandomCrop(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n+            inputs = backend.convert_to_tensor(backend.convert_to_numpy(inputs))\n+\n+        input_shape = ops.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = ops.expand_dims(inputs, axis=0) if not is_batched else inputs\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            dtype = input_shape.dtype\n+            rands = random.uniform([2], 0, dtype.max, dtype, seed=self.seed)\n+            h_start = rands[0] % (h_diff + 1)\n+            w_start = rands[1] % (w_diff + 1)\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs, [self.height, self.width]\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return ops.cast(outputs, self.compute_dtype)\n+\n+        outputs = ops.cond(\n+            tf.reduce_all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n         if (\n             backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n@@ -83,10 +134,15 @@ class RandomCrop(Layer):\n             outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = tf.TensorShape(input_shape).as_list()\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\"height\": self.height, \"width\": self.width, \"seed\": self.seed}\n+        )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f340d8fe049802199aac1776fd4767b401dcb7c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 14 | Churn Cumulative: 632 | Contributors (this commit): 7 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from keras_core import ops\n from keras_core import random\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n from keras_core.utils import image_utils\n@@ -11,7 +12,7 @@ from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -68,9 +69,7 @@ class RandomCrop(Layer):\n         self.height = height\n         self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.data_format = (\n-            \"channels_last\" if data_format is None else \"channels_first\"\n-        )\n+        self.data_format = backend.standardize_data_format(data_format)\n \n         if self.data_format == \"channels_first\":\n             self.heigh_axis = -2\n@@ -143,6 +142,11 @@ class RandomCrop(Layer):\n     def get_config(self):\n         config = super().get_config()\n         config.update(\n-            {\"height\": self.height, \"width\": self.width, \"seed\": self.seed}\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n         )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3e29542c926cad5edb04f7e803f079a0c6a3cf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 639 | Contributors (this commit): 7 | Commits (past 90d): 22 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,10 +2,10 @@ import numpy as np\n \n from keras_core import ops\n from keras_core import random\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n@@ -69,6 +69,7 @@ class RandomCrop(TFDataLayer):\n         self.height = height\n         self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n+        self.seed_generator = SeedGenerator(seed)\n         self.data_format = backend.standardize_data_format(data_format)\n \n         if self.data_format == \"channels_first\":\n@@ -96,7 +97,9 @@ class RandomCrop(TFDataLayer):\n \n         def random_crop():\n             dtype = input_shape.dtype\n-            rands = random.uniform([2], 0, dtype.max, dtype, seed=self.seed)\n+            rands = random.uniform(\n+                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n+            )\n             h_start = rands[0] % (h_diff + 1)\n             w_start = rands[1] % (w_diff + 1)\n             if self.data_format == \"channels_last\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#26aeed285c3a9abb60da946afd2babf8aa39f521", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 647 | Contributors (this commit): 7 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,13 +118,16 @@ class RandomCrop(TFDataLayer):\n \n         def resize():\n             outputs = image_utils.smart_resize(\n-                inputs, [self.height, self.width]\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=backend.backend(),\n             )\n             # smart_resize will always output float32, so we need to re-cast.\n             return ops.cast(outputs, self.compute_dtype)\n \n         outputs = ops.cond(\n-            tf.reduce_all((training, h_diff >= 0, w_diff >= 0)),\n+            ops.all((training, h_diff >= 0, w_diff >= 0)),\n             random_crop,\n             resize,\n         )\n@@ -152,4 +155,5 @@ class RandomCrop(TFDataLayer):\n                 \"data_format\": self.data_format,\n             }\n         )\n+        ops.all\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd711ff0eafbd02fc11d30ed28633d3f74f1a83f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 649 | Contributors (this commit): 7 | Commits (past 90d): 24 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -140,7 +140,7 @@ class RandomCrop(TFDataLayer):\n         return outputs\n \n     def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = tf.TensorShape(input_shape).as_list()\n+        input_shape = list(input_shape)\n         input_shape[self.height_axis] = self.height\n         input_shape[self.width_axis] = self.width\n         return tuple(input_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6bdbc2ef85ea1d32ee7b8dc166cc7b8b6e60a8ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 650 | Contributors (this commit): 7 | Commits (past 90d): 25 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -155,5 +155,4 @@ class RandomCrop(TFDataLayer):\n                 \"data_format\": self.data_format,\n             }\n         )\n-        ops.all\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1af8b72a05f446300d5621902833b3ba1b2f28cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 668 | Contributors (this commit): 7 | Commits (past 90d): 26 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -86,11 +86,11 @@ class RandomCrop(TFDataLayer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = backend.convert_to_tensor(backend.convert_to_numpy(inputs))\n+            inputs = self.backend.convert_to_tensor(backend.convert_to_numpy(inputs))\n \n-        input_shape = ops.shape(inputs)\n+        input_shape = self.backend.shape(inputs)\n         is_batched = len(input_shape) > 3\n-        inputs = ops.expand_dims(inputs, axis=0) if not is_batched else inputs\n+        inputs = self.backend.expand_dims(inputs, axis=0) if not is_batched else inputs\n \n         h_diff = input_shape[self.height_axis] - self.height\n         w_diff = input_shape[self.width_axis] - self.width\n@@ -121,22 +121,22 @@ class RandomCrop(TFDataLayer):\n                 inputs,\n                 [self.height, self.width],\n                 data_format=self.data_format,\n-                backend_module=backend.backend(),\n+                backend_module=self.backend,\n             )\n             # smart_resize will always output float32, so we need to re-cast.\n-            return ops.cast(outputs, self.compute_dtype)\n+            return self.backend.cast(outputs, self.compute_dtype)\n \n-        outputs = ops.cond(\n-            ops.all((training, h_diff >= 0, w_diff >= 0)),\n+        outputs = self.backend.cond(\n+            self.backend.all((training, h_diff >= 0, w_diff >= 0)),\n             random_crop,\n             resize,\n         )\n \n         if (\n-            backend.backend() != \"tensorflow\"\n+            self.backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = backend.convert_to_tensor(outputs)\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n     def compute_output_shape(self, input_shape, *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43950938cf00f69f80349665e60e13b4f5a7f93a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 86 | Files Changed: 4 | Hunks: 17 | Methods Changed: 10 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 115 | Churn Cumulative: 4745 | Contributors (this commit): 9 | Commits (past 90d): 67 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,4 +7,3 @@ from keras_core.backend.common.variables import is_int_dtype\n from keras_core.backend.common.variables import standardize_dtype\n from keras_core.backend.common.variables import standardize_shape\n from keras_core.random import random\n-convert\n\\ No newline at end of file\n\n@@ -1,18 +1,14 @@\n import numpy as np\n \n-from keras_core import ops\n-from keras_core import random\n-from keras_core.random.seed_generator import SeedGenerator\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -56,9 +52,7 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -66,19 +60,13 @@ class RandomCrop(TFDataLayer):\n             )\n \n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n@@ -86,73 +74,19 @@ class RandomCrop(TFDataLayer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = self.backend.convert_to_tensor(backend.convert_to_numpy(inputs))\n-\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = self.backend.expand_dims(inputs, axis=0) if not is_batched else inputs\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            dtype = input_shape.dtype\n-            rands = random.uniform(\n-                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n-            )\n-            h_start = rands[0] % (h_diff + 1)\n-            w_start = rands[1] % (w_diff + 1)\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n         if (\n-            self.backend.backend() != \"tensorflow\"\n+            backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = self.backend.convert_to_tensor(outputs)\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.gru(\n+                out = backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,6 +536,11 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.lstm(\n+                out = backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,6 +516,11 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfbbe6db38c0bb4b118f1e08aa16092dbd1c7bda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 29 | Files Changed: 3 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/6 | Churn Δ: 114 | Churn Cumulative: 4838 | Contributors (this commit): 8 | Commits (past 90d): 54 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,18 @@\n import numpy as np\n \n+from keras_core import ops\n+from keras_core import random\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras_core.utils import backend_utils\n+from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,7 +56,9 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -60,13 +66,19 @@ class RandomCrop(Layer):\n             )\n \n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.seed_generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n@@ -74,19 +86,73 @@ class RandomCrop(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n+            inputs = self.backend.convert_to_tensor(backend.convert_to_numpy(inputs))\n+\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = self.backend.expand_dims(inputs, axis=0) if not is_batched else inputs\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            dtype = input_shape.dtype\n+            rands = random.uniform(\n+                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n+            )\n+            h_start = rands[0] % (h_diff + 1)\n+            w_start = rands[1] % (w_diff + 1)\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n         if (\n-            backend.backend() != \"tensorflow\"\n+            self.backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = backend.convert_to_tensor(outputs)\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                out = backend.gru(\n+                return backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,11 +536,6 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n-                # We disable jit_compile for the model in this case,\n-                # since cuDNN ops aren't XLA compatible.\n-                if backend.backend() == \"tensorflow\":\n-                    self.supports_jit = False\n-                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                out = backend.lstm(\n+                return backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,11 +516,6 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n-                # We disable jit_compile for the model in this case,\n-                # since cuDNN ops aren't XLA compatible.\n-                if backend.backend() == \"tensorflow\":\n-                    self.supports_jit = False\n-                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f69a2bceee4120934e4d60f50cccfdb5aef4fff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 91 | Files Changed: 8 | Hunks: 24 | Methods Changed: 17 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 198 | Churn Cumulative: 7954 | Contributors (this commit): 14 | Commits (past 90d): 93 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1 @@\n+from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -0,0 +1,24 @@\n+import torch\n+\n+from keras_core.optimizers.base_optimizer import BaseOptimizer\n+\n+\n+class TorchOptimizer(BaseOptimizer):\n+    def __new__(cls, *args, **kwargs):\n+        # Import locally to avoid circular imports.\n+        from keras_core import optimizers\n+        from keras_core.backend.torch.optimizers import torch_sgd\n+\n+        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n+        if cls in OPTIMIZERS:\n+            return OPTIMIZERS[cls](*args, **kwargs)\n+        return super().__new__(cls)\n+\n+    def _apply_weight_decay(self, variables):\n+        if self.weight_decay is None:\n+            return\n+\n+        torch._foreach_mul_(\n+            [v.value for v in variables if self._use_weight_decay(v)],\n+            1 - self.weight_decay * self._get_current_learning_rate(),\n+        )\n\n@@ -0,0 +1,43 @@\n+import torch\n+\n+from keras_core import optimizers\n+\n+\n+class SGD(optimizers.SGD):\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        grads, trainable_variables = zip(*grads_and_vars)\n+\n+        self._parallel_update_step(\n+            grads,\n+            [v.value for v in trainable_variables],\n+            self._get_current_learning_rate(),\n+        )\n+        self.iterations.assign(self.iterations + 1)\n+\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        if self.momentum != 0:\n+            bufs = [\n+                self.momentums[self._get_variable_index(variable.value)]\n+                for variable in variables\n+            ]\n+\n+            for i in range(len(bufs)):\n+                if bufs[i] is None:\n+                    bufs[i] = torch.clone(grads[i]).detach()\n+\n+            torch._foreach_mul_(bufs, self.momentum)\n+            torch._foreach_add_(bufs, grads, alpha=-learning_rate)\n+\n+            if self.nesterov:\n+                torch._foreach_add_(variables, grads, alpha=-learning_rate)\n+                torch._foreach_add_(variables, bufs, alpha=self.momentum)\n+            else:\n+                torch._foreach_add_(variables, bufs)\n+\n+        else:\n+            torch._foreach_add_(variables, grads, alpha=-learning_rate)\n\n@@ -1,18 +1,14 @@\n import numpy as np\n \n-from keras_core import ops\n-from keras_core import random\n-from keras_core.random.seed_generator import SeedGenerator\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -56,9 +52,7 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -66,19 +60,13 @@ class RandomCrop(TFDataLayer):\n             )\n \n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n@@ -86,73 +74,19 @@ class RandomCrop(TFDataLayer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = self.backend.convert_to_tensor(backend.convert_to_numpy(inputs))\n-\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = self.backend.expand_dims(inputs, axis=0) if not is_batched else inputs\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            dtype = input_shape.dtype\n-            rands = random.uniform(\n-                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n-            )\n-            h_start = rands[0] % (h_diff + 1)\n-            w_start = rands[1] % (w_diff + 1)\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n         if (\n-            self.backend.backend() != \"tensorflow\"\n+            backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = self.backend.convert_to_tensor(outputs)\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.gru(\n+                out = backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,6 +536,11 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.lstm(\n+                out = backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,6 +516,11 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -3,9 +3,13 @@ from keras_core.api_export import keras_core_export\n from keras_core.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n+    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n \n-    BackendOptimizer = tf_optimizer.TFOptimizer\n+    BackendOptimizer = TFOptimizer\n+elif backend.backend() == \"torch\":\n+    from keras_core.backend.torch.optimizers import TorchOptimizer\n+\n+    BackendOptimizer = TorchOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -21,7 +21,7 @@ class SGDTest(testing.TestCase):\n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n         self.assertEqual(len(optimizer.variables), 2)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n@@ -32,7 +32,7 @@ class SGDTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -56,8 +56,8 @@ class SGDTest(testing.TestCase):\n         optimizer = SGD(nesterov=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b265498d532f868a9effce9eab196fa3dfc06ad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 107 | Files Changed: 8 | Hunks: 24 | Methods Changed: 17 | Complexity Δ (Sum/Max): -10/6 | Churn Δ: 203 | Churn Cumulative: 8157 | Contributors (this commit): 14 | Commits (past 90d): 101 | Contributors (cumulative): 41 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1 +0,0 @@\n-from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -1,24 +0,0 @@\n-import torch\n-\n-from keras_core.optimizers.base_optimizer import BaseOptimizer\n-\n-\n-class TorchOptimizer(BaseOptimizer):\n-    def __new__(cls, *args, **kwargs):\n-        # Import locally to avoid circular imports.\n-        from keras_core import optimizers\n-        from keras_core.backend.torch.optimizers import torch_sgd\n-\n-        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n-        if cls in OPTIMIZERS:\n-            return OPTIMIZERS[cls](*args, **kwargs)\n-        return super().__new__(cls)\n-\n-    def _apply_weight_decay(self, variables):\n-        if self.weight_decay is None:\n-            return\n-\n-        torch._foreach_mul_(\n-            [v.value for v in variables if self._use_weight_decay(v)],\n-            1 - self.weight_decay * self._get_current_learning_rate(),\n-        )\n\n@@ -1,43 +0,0 @@\n-import torch\n-\n-from keras_core import optimizers\n-\n-\n-class SGD(optimizers.SGD):\n-    def _internal_apply_gradients(self, grads_and_vars):\n-        grads, trainable_variables = zip(*grads_and_vars)\n-\n-        self._parallel_update_step(\n-            grads,\n-            [v.value for v in trainable_variables],\n-            self._get_current_learning_rate(),\n-        )\n-        self.iterations.assign(self.iterations + 1)\n-\n-    def _parallel_update_step(\n-        self,\n-        grads,\n-        variables,\n-        learning_rate,\n-    ):\n-        if self.momentum != 0:\n-            bufs = [\n-                self.momentums[self._get_variable_index(variable.value)]\n-                for variable in variables\n-            ]\n-\n-            for i in range(len(bufs)):\n-                if bufs[i] is None:\n-                    bufs[i] = torch.clone(grads[i]).detach()\n-\n-            torch._foreach_mul_(bufs, self.momentum)\n-            torch._foreach_add_(bufs, grads, alpha=-learning_rate)\n-\n-            if self.nesterov:\n-                torch._foreach_add_(variables, grads, alpha=-learning_rate)\n-                torch._foreach_add_(variables, bufs, alpha=self.momentum)\n-            else:\n-                torch._foreach_add_(variables, bufs)\n-\n-        else:\n-            torch._foreach_add_(variables, grads, alpha=-learning_rate)\n\n@@ -1,14 +1,17 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import random\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n+from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,7 +55,9 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -60,13 +65,19 @@ class RandomCrop(Layer):\n             )\n \n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.seed_generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n@@ -74,19 +85,79 @@ class RandomCrop(Layer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n+            inputs = self.backend.convert_to_tensor(\n+                backend.convert_to_numpy(inputs)\n+            )\n+\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            dtype = input_shape.dtype\n+            rands = random.uniform(\n+                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n+            )\n+            h_start = rands[0] % (h_diff + 1)\n+            w_start = rands[1] % (w_diff + 1)\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n         if (\n-            backend.backend() != \"tensorflow\"\n+            self.backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = backend.convert_to_tensor(outputs)\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                out = backend.gru(\n+                return backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,11 +536,6 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n-                # We disable jit_compile for the model in this case,\n-                # since cuDNN ops aren't XLA compatible.\n-                if backend.backend() == \"tensorflow\":\n-                    self.supports_jit = False\n-                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                out = backend.lstm(\n+                return backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,11 +516,6 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n-                # We disable jit_compile for the model in this case,\n-                # since cuDNN ops aren't XLA compatible.\n-                if backend.backend() == \"tensorflow\":\n-                    self.supports_jit = False\n-                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -3,13 +3,9 @@ from keras_core.api_export import keras_core_export\n from keras_core.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n+    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n \n-    BackendOptimizer = TFOptimizer\n-elif backend.backend() == \"torch\":\n-    from keras_core.backend.torch.optimizers import TorchOptimizer\n-\n-    BackendOptimizer = TorchOptimizer\n+    BackendOptimizer = tf_optimizer.TFOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -21,7 +21,7 @@ class SGDTest(testing.TestCase):\n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n         self.assertEqual(len(optimizer.variables), 2)\n-        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n@@ -32,7 +32,7 @@ class SGDTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            ops.zeros(()),\n+            np.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -56,8 +56,8 @@ class SGDTest(testing.TestCase):\n         optimizer = SGD(nesterov=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = ops.arange(0.1, 1.1, 0.1)\n-        first_grads = ops.full((10,), 0.01)\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
