{"custom_id": "keras#65b048455ba3fdd756b0b615ad608eafece05e91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -19/0 | Churn Δ: 17 | Churn Cumulative: 937 | Contributors (this commit): 12 | Commits (past 90d): 28 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -69,10 +69,25 @@ class Convolution1D(Layer):\n         X = T.reshape(X, (X.shape[0], X.shape[1], X.shape[2], 1)).dimshuffle(0, 2, 1, 3)\n \n         border_mode = self.border_mode\n+        if on_gpu() and dnn.dnn_available():\n+            if border_mode == 'same':\n+                assert(self.subsample_length == 1)\n+                pad_x = (self.filter_length - self.subsample_length) // 2\n+                conv_out = dnn.dnn_conv(img=X,\n+                                        kerns=self.W,\n+                                        border_mode=(pad_x, 0))\n+            else:\n+                conv_out = dnn.dnn_conv(img=X,\n+                                        kerns=self.W,\n+                                        border_mode=border_mode,\n+                                        subsample=self.subsample)\n+        else:\n             if border_mode == 'same':\n                 border_mode = 'full'\n \n-        conv_out = T.nnet.conv.conv2d(X, self.W, border_mode=border_mode, subsample=self.subsample)\n+            conv_out = T.nnet.conv.conv2d(X, self.W,\n+                                          border_mode=border_mode,\n+                                          subsample=self.subsample)\n             if self.border_mode == 'same':\n                 shift_x = (self.filter_length - 1) // 2\n                 conv_out = conv_out[:, :, shift_x:X.shape[2] + shift_x, :]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc6280f34d84c2d44f1cc856d732c353b4b543ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 22 | Files Changed: 5 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 21/19 | Churn Δ: 43 | Churn Cumulative: 3421 | Contributors (this commit): 29 | Commits (past 90d): 76 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -364,7 +364,7 @@ class ZeroPadding1D(Layer):\n         the padding dimension (axis 1).\n     \"\"\"\n     def __init__(self, padding=1):\n-        super(ZeroPadding2D, self).__init__()\n+        super(ZeroPadding1D, self).__init__()\n         self.padding = padding\n         self.input = T.tensor4()\n \n@@ -423,8 +423,8 @@ class ZeroPadding2D(Layer):\n         out = T.zeros(out_shape)\n         indices = (slice(None),\n                    slice(None),\n-                   slice(pad[0], input_shape[2] + self.padding[0]),\n-                   slice(pad[1], input_shape[3] + self.padding[1]))\n+                   slice(self.padding[0], input_shape[2] + self.padding[0]),\n+                   slice(self.padding[1], input_shape[3] + self.padding[1]))\n         return T.set_subtensor(out[indices], X)\n \n     def get_config(self):\n\n@@ -49,6 +49,9 @@ class Layer(object):\n         else:\n             raise NotImplementedError\n \n+    def set_input_shape(self, input_shape):\n+        self._input_shape = input_shape\n+\n     @property\n     def output_shape(self):\n         # default assumption: tensor shape unchanged.\n\n@@ -45,7 +45,6 @@ class TestConvolutions(unittest.TestCase):\n \n     def test_maxpooling_1d(self):\n         nb_samples = 9\n-\n         nb_steps = 7\n         input_dim = 10\n \n@@ -61,7 +60,6 @@ class TestConvolutions(unittest.TestCase):\n \n     def test_convolution_2d(self):\n         nb_samples = 8\n-\n         nb_filter = 9\n         stack_size = 7\n         nb_row = 10\n@@ -100,16 +98,15 @@ class TestConvolutions(unittest.TestCase):\n \n     def test_maxpooling_2d(self):\n         nb_samples = 9\n-\n         stack_size = 7\n         input_nb_row = 11\n         input_nb_col = 12\n-        poolsize = (3, 3)\n+        pool_size = (3, 3)\n \n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n         for ignore_border in [True, False]:\n             for stride in [(1, 1), (2, 2)]:\n-                layer = convolutional.MaxPooling2D(stride=stride, ignore_border=ignore_border, poolsize=poolsize)\n+                layer = convolutional.MaxPooling2D(stride=stride, ignore_border=ignore_border, pool_size=pool_size)\n                 layer.input = theano.shared(value=input)\n                 for train in [True, False]:\n                     layer.get_output(train).eval()\n@@ -118,13 +115,12 @@ class TestConvolutions(unittest.TestCase):\n \n     def test_zero_padding_2d(self):\n         nb_samples = 9\n-\n         stack_size = 7\n         input_nb_row = 11\n         input_nb_col = 12\n \n         input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-        layer = convolutional.ZeroPadding2D(pad=(2,2))\n+        layer = convolutional.ZeroPadding2D(padding=(2, 2))\n         layer.input = theano.shared(value=input)\n         for train in [True, False]:\n             out = layer.get_output(train).eval()\n@@ -137,7 +133,6 @@ class TestConvolutions(unittest.TestCase):\n \n     def test_upsample_1d(self):\n         nb_samples = 9\n-\n         nb_steps = 7\n         input_dim = 10\n \n@@ -153,7 +148,6 @@ class TestConvolutions(unittest.TestCase):\n \n     def test_upsample_2d(self):\n         nb_samples = 9\n-\n         stack_size = 7\n         input_nb_row = 11\n         input_nb_col = 12\n\n@@ -72,7 +72,7 @@ class TestConfigParams(unittest.TestCase):\n         self._runner(layer)\n \n     def test_reshape(self):\n-        layer = core.Reshape(10, 10)\n+        layer = core.Reshape(dims=(10, 10))\n         self._runner(layer)\n \n     def test_flatten(self):\n@@ -121,8 +121,7 @@ class TestMasking(unittest.TestCase):\n         func = theano.function([layer.input], layer.get_output_mask())\n         self.assertTrue(np.all(\n             # get mask for this input\n-            func(np.array(\n-            [[[1], [2], [3], [0]],\n+            func(np.array([[[1], [2], [3], [0]],\n                           [[0], [4], [5], [0]]], dtype=np.int32)) ==\n             # This is the expected output mask, one dimension less\n             np.array([[1, 1, 1, 0], [0, 1, 1, 0]])))\n@@ -133,8 +132,7 @@ class TestMasking(unittest.TestCase):\n         func = theano.function([layer.input], layer.get_output_mask())\n         self.assertTrue(np.all(\n             # get mask for this input, if not all the values are 5, shouldn't masked\n-            func(np.array(\n-            [[[1, 1], [2, 1], [3, 1], [5, 5]],\n+            func(np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n                           [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)) ==\n             # This is the expected output mask, one dimension less\n             np.array([[1, 1, 1, 0], [1, 1, 1, 1]])))\n@@ -145,8 +143,7 @@ class TestMasking(unittest.TestCase):\n         func = theano.function([layer.input], layer.get_output())\n         self.assertTrue(np.all(\n             # get output for this input, replace padding with 0\n-            func(np.array(\n-            [[[1, 1], [2, 1], [3, 1], [5, 5]],\n+            func(np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n                           [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)) ==\n             # This is the expected output\n             np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n\n@@ -23,7 +23,7 @@ class TestShapeInference(unittest.TestCase):\n     # # Core #\n     # ########\n     def test_Reshape(self):\n-        layer = Reshape(2, 3)\n+        layer = Reshape(dims=(2, 3))\n         input_data = np.random.random((2, 6))\n         check_layer_output_shape(layer, input_data)\n \n@@ -91,9 +91,9 @@ class TestShapeInference(unittest.TestCase):\n     def test_MaxPooling2D(self):\n         for ignore_border in [True, False]:\n             for stride in [(1, 1), (2, 2)]:\n-                for poolsize in [(2, 2), (3, 3), (4, 4)]:\n+                for pool_size in [(2, 2), (3, 3), (4, 4)]:\n                     for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n-                        layer = MaxPooling2D(poolsize=poolsize, stride=stride, ignore_border=ignore_border)\n+                        layer = MaxPooling2D(pool_size=pool_size, stride=stride, ignore_border=ignore_border)\n                         input_data = np.random.random(input_data_shape)\n                         check_layer_output_shape(layer, input_data)\n \n@@ -107,6 +107,11 @@ class TestShapeInference(unittest.TestCase):\n         input_data = np.random.random((2, 1, 2, 3))\n         check_layer_output_shape(layer, input_data)\n \n+    def test_ZeroPadding1D(self):\n+        layer = ZeroPadding1D(1)\n+        input_data = np.random.random((2, 2, 1))\n+        check_layer_output_shape(layer, input_data)\n+\n     def test_ZeroPadding2D(self):\n         layer = ZeroPadding2D((1, 2))\n         input_data = np.random.random((2, 1, 2, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca4fc2e72f4a841e9230d9e4241893f0381f3fdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 951 | Contributors (this commit): 12 | Commits (past 90d): 30 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -261,7 +261,9 @@ class Convolution2D(Layer):\n class MaxPooling1D(Layer):\n     def __init__(self, pool_length=2, stride=1, ignore_border=True):\n         super(MaxPooling1D, self).__init__()\n-        if type(stride) is not int or not stride:\n+        if stride is None:\n+            stride = pool_length\n+        if type(stride) not stride:\n             raise Exception('\"stride\" argument in MaxPooling1D should be an int > 0.')\n         self.pool_length = pool_length\n         self.stride = stride\n@@ -292,10 +294,12 @@ class MaxPooling1D(Layer):\n \n \n class MaxPooling2D(Layer):\n-    def __init__(self, poolsize=(2, 2), stride=(1, 1), ignore_border=True):\n+    def __init__(self, poolsize=(2, 2), stride=(2, 2), ignore_border=True):\n         super(MaxPooling2D, self).__init__()\n         self.input = T.tensor4()\n         self.poolsize = tuple(poolsize)\n+        if stride is None:\n+            stride = self.poolsize\n         self.stride = tuple(stride)\n         self.ignore_border = ignore_border\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0bcee4963ea05b8a03c250b6a8858cd5d3ede8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 955 | Contributors (this commit): 12 | Commits (past 90d): 31 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -259,7 +259,7 @@ class Convolution2D(Layer):\n \n \n class MaxPooling1D(Layer):\n-    def __init__(self, pool_length=2, stride=1, ignore_border=True):\n+    def __init__(self, pool_length=2, stride=None, ignore_border=True):\n         super(MaxPooling1D, self).__init__()\n         if stride is None:\n             stride = pool_length\n@@ -294,7 +294,7 @@ class MaxPooling1D(Layer):\n \n \n class MaxPooling2D(Layer):\n-    def __init__(self, poolsize=(2, 2), stride=(2, 2), ignore_border=True):\n+    def __init__(self, poolsize=(2, 2), stride=None, ignore_border=True):\n         super(MaxPooling2D, self).__init__()\n         self.input = T.tensor4()\n         self.poolsize = tuple(poolsize)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7e0ba1c3966ddf0bcf864d7f15323f5327afbb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 957 | Contributors (this commit): 12 | Commits (past 90d): 32 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -263,8 +263,6 @@ class MaxPooling1D(Layer):\n         super(MaxPooling1D, self).__init__()\n         if stride is None:\n             stride = pool_length\n-        if type(stride) not stride:\n-            raise Exception('\"stride\" argument in MaxPooling1D should be an int > 0.')\n         self.pool_length = pool_length\n         self.stride = stride\n         self.st = (self.stride, 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4564dab62a1307bb44573489c4db4d61bf10f59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 78 | Files Changed: 7 | Hunks: 63 | Methods Changed: 67 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 197 | Churn Cumulative: 4591 | Contributors (this commit): 36 | Commits (past 90d): 99 | Contributors (cumulative): 64 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,8 +6,8 @@ import numpy as np\n \n \n class LeakyReLU(MaskedLayer):\n-    def __init__(self, alpha=0.3):\n-        super(LeakyReLU, self).__init__()\n+    def __init__(self, alpha=0.3, **kwargs):\n+        super(LeakyReLU, self).__init__(**kwargs)\n         self.alpha = alpha\n \n     def get_output(self, train):\n@@ -25,8 +25,8 @@ class PReLU(MaskedLayer):\n             Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n                 http://arxiv.org/pdf/1502.01852v1.pdf\n     '''\n-    def __init__(self, input_shape, init='zero', weights=None):\n-        super(PReLU, self).__init__()\n+    def __init__(self, input_shape, init='zero', weights=None, **kwargs):\n+        super(PReLU, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.alphas = self.init(input_shape)\n         self.params = [self.alphas]\n@@ -55,9 +55,10 @@ class ParametricSoftplus(MaskedLayer):\n             Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n             http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143\n     '''\n-    def __init__(self, input_shape, alpha_init=0.2, beta_init=5.0, weights=None):\n+    def __init__(self, input_shape, alpha_init=0.2,\n+                 beta_init=5.0, weights=None, **kwargs):\n \n-        super(ParametricSoftplus, self).__init__()\n+        super(ParametricSoftplus, self).__init__(**kwargs)\n         self.alpha_init = alpha_init\n         self.beta_init = beta_init\n         self.alphas = sharedX(alpha_init * np.ones(input_shape))\n@@ -87,8 +88,8 @@ class ThresholdedLinear(MaskedLayer):\n             Zero-Bias Autoencoders and the Benefits of Co-Adapting Features\n             http://arxiv.org/pdf/1402.3337.pdf\n     '''\n-    def __init__(self, theta=1.0):\n-        super(ThresholdedLinear, self).__init__()\n+    def __init__(self, theta=1.0, **kwargs):\n+        super(ThresholdedLinear, self).__init__(**kwargs)\n         self.theta = theta\n \n     def get_output(self, train):\n@@ -108,8 +109,8 @@ class ThresholdedReLu(MaskedLayer):\n             Zero-Bias Autoencoders and the Benefits of Co-Adapting Features\n             http://arxiv.org/pdf/1402.3337.pdf\n     '''\n-    def __init__(self, theta=1.0):\n-        super(ThresholdedReLu, self).__init__()\n+    def __init__(self, theta=1.0, **kwargs):\n+        super(ThresholdedReLu, self).__init__(**kwargs)\n         self.theta = theta\n \n     def get_output(self, train):\n\n@@ -45,16 +45,18 @@ def pool_output_length(input_length, pool_size, ignore_border, stride):\n \n \n class Convolution1D(Layer):\n+    input_ndim = 3\n+\n     def __init__(self, input_dim, nb_filter, filter_length,\n                  init='uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None):\n+                 W_constraint=None, b_constraint=None, **kwargs):\n \n         if border_mode not in {'valid', 'full', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n \n-        super(Convolution1D, self).__init__()\n+        super(Convolution1D, self).__init__(**kwargs)\n         self.nb_filter = nb_filter\n         self.input_dim = input_dim\n         self.filter_length = filter_length\n@@ -150,16 +152,18 @@ class Convolution1D(Layer):\n \n \n class Convolution2D(Layer):\n+    input_ndim = 4\n+\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col,\n                  init='glorot_uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None):\n+                 W_constraint=None, b_constraint=None, **kwargs):\n \n         if border_mode not in {'valid', 'full', 'same'}:\n             raise Exception('Invalid border mode for Convolution2D:', border_mode)\n \n-        super(Convolution2D, self).__init__()\n+        super(Convolution2D, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.subsample = tuple(subsample)\n@@ -259,8 +263,10 @@ class Convolution2D(Layer):\n \n \n class MaxPooling1D(Layer):\n-    def __init__(self, pool_length=2, stride=None, ignore_border=True):\n-        super(MaxPooling1D, self).__init__()\n+    input_ndim = 3\n+\n+    def __init__(self, pool_length=2, stride=None, ignore_border=True, **kwargs):\n+        super(MaxPooling1D, self).__init__(**kwargs)\n         if stride is None:\n             stride = pool_length\n         self.pool_length = pool_length\n@@ -292,8 +298,10 @@ class MaxPooling1D(Layer):\n \n \n class MaxPooling2D(Layer):\n-    def __init__(self, pool_size=(2, 2), stride=None, ignore_border=True):\n-        super(MaxPooling2D, self).__init__()\n+    input_ndim = 4\n+\n+    def __init__(self, pool_size=(2, 2), stride=None, ignore_border=True, **kwargs):\n+        super(MaxPooling2D, self).__init__(**kwargs)\n         self.input = T.tensor4()\n         self.pool_size = tuple(pool_size)\n         if stride is None:\n@@ -321,8 +329,10 @@ class MaxPooling2D(Layer):\n \n \n class UpSample1D(Layer):\n-    def __init__(self, length=2):\n-        super(UpSample1D, self).__init__()\n+    input_ndim = 3\n+\n+    def __init__(self, length=2, **kwargs):\n+        super(UpSample1D, self).__init__(**kwargs)\n         self.length = length\n         self.input = T.tensor3()\n \n@@ -342,8 +352,10 @@ class UpSample1D(Layer):\n \n \n class UpSample2D(Layer):\n-    def __init__(self, size=(2, 2)):\n-        super(UpSample2D, self).__init__()\n+    input_ndim = 4\n+\n+    def __init__(self, size=(2, 2), **kwargs):\n+        super(UpSample2D, self).__init__(**kwargs)\n         self.input = T.tensor4()\n         self.size = tuple(size)\n \n@@ -380,8 +392,10 @@ class ZeroPadding1D(Layer):\n         How many zeros to add at the beginning and end of\n         the padding dimension (axis 1).\n     \"\"\"\n-    def __init__(self, padding=1):\n-        super(ZeroPadding1D, self).__init__()\n+    input_ndim = 3\n+\n+    def __init__(self, padding=1, **kwargs):\n+        super(ZeroPadding1D, self).__init__(**kwargs)\n         self.padding = padding\n         self.input = T.tensor4()\n \n@@ -417,8 +431,10 @@ class ZeroPadding2D(Layer):\n         How many zeros to add at the beginning and end of\n         the 2 padding dimensions (axis 3 and 4).\n     \"\"\"\n-    def __init__(self, padding=(1, 1)):\n-        super(ZeroPadding2D, self).__init__()\n+    input_ndim = 4\n+\n+    def __init__(self, padding=(1, 1), **kwargs):\n+        super(ZeroPadding2D, self).__init__(**kwargs)\n         self.padding = tuple(padding)\n         self.input = T.tensor4()\n \n\n@@ -9,7 +9,7 @@ from collections import OrderedDict\n import copy\n \n from .. import activations, initializations, regularizers, constraints\n-from ..utils.theano_utils import shared_zeros, floatX\n+from ..utils.theano_utils import shared_zeros, floatX, ndim_tensor\n from ..utils.generic_utils import make_tuple\n from ..regularizers import ActivityRegularizer, Regularizer\n \n@@ -18,7 +18,9 @@ from six.moves import zip\n \n \n class Layer(object):\n-    def __init__(self):\n+    def __init__(self, **kwargs):\n+        if 'input_shape' in kwargs:\n+            self.set_input_shape(kwargs['input_shape'])\n         self.params = []\n \n     def init_updates(self):\n@@ -50,7 +52,15 @@ class Layer(object):\n             raise NotImplementedError\n \n     def set_input_shape(self, input_shape):\n+        if type(input_shape) not in [tuple, list]:\n+            raise Exception('Invalid input shape - input_shape should be a tuple of int.')\n+        input_shape = (None,) + tuple(input_shape)\n+        if hasattr(self, 'input_ndim') and self.input_ndim:\n+            if self.input_ndim != len(input_shape):\n+                raise Exception('Invalid input shape - Layer expects input ndim=' +\n+                                str(self.input_ndim) + ', was provided with input shape ' + str(input_shape))\n         self._input_shape = input_shape\n+        self.input = ndim_tensor(len(self._input_shape))\n \n     @property\n     def output_shape(self):\n@@ -170,8 +180,8 @@ class Masking(MaskedLayer):\n     otherwise it is 1.\n \n     \"\"\"\n-    def __init__(self, mask_value=0.):\n-        super(Masking, self).__init__()\n+    def __init__(self, mask_value=0., **kwargs):\n+        super(Masking, self).__init__(**kwargs)\n         self.mask_value = mask_value\n         self.input = T.tensor3()\n \n@@ -195,7 +205,10 @@ class TimeDistributedMerge(Layer):\n     Tensor input dimensions:   (nb_sample, time, features)\n     Tensor output dimensions:  (nb_sample, features)\n     '''\n-    def __init__(self, mode='sum'):\n+    input_ndim = 3\n+\n+    def __init__(self, mode='sum', **kwargs):\n+        super(TimeDistributedMerge, self).__init__(**kwargs)\n         self.mode = mode\n         self.params = []\n         self.regularizers = []\n@@ -336,8 +349,8 @@ class Dropout(MaskedLayer):\n     '''\n         Hinton's dropout.\n     '''\n-    def __init__(self, p):\n-        super(Dropout, self).__init__()\n+    def __init__(self, p, **kwargs):\n+        super(Dropout, self).__init__(**kwargs)\n         self.p = p\n         self.srng = RandomStreams(seed=np.random.randint(10e6))\n \n@@ -360,8 +373,8 @@ class Activation(MaskedLayer):\n     '''\n         Apply an activation function to an output.\n     '''\n-    def __init__(self, activation, target=0, beta=0.1):\n-        super(Activation, self).__init__()\n+    def __init__(self, activation, target=0, beta=0.1, **kwargs):\n+        super(Activation, self).__init__(**kwargs)\n         self.activation = activations.get(activation)\n         self.target = target\n         self.beta = beta\n@@ -383,8 +396,8 @@ class Reshape(Layer):\n         Can't be used as first layer in a model (no fixed input!)\n         First dimension is assumed to be nb_samples.\n     '''\n-    def __init__(self, dims):\n-        super(Reshape, self).__init__()\n+    def __init__(self, dims, **kwargs):\n+        super(Reshape, self).__init__(**kwargs)\n         self.dims = tuple(dims)\n \n     @property\n@@ -405,8 +418,8 @@ class Permute(Layer):\n     '''\n         Permute the dimensions of the input according to the given tuple.\n     '''\n-    def __init__(self, dims):\n-        super(Permute, self).__init__()\n+    def __init__(self, dims, **kwargs):\n+        super(Permute, self).__init__(**kwargs)\n         self.dims = tuple(dims)\n \n     @property\n@@ -432,8 +445,8 @@ class Flatten(Layer):\n         Reshape input to flat shape.\n         First dimension is assumed to be nb_samples.\n     '''\n-    def __init__(self):\n-        super(Flatten, self).__init__()\n+    def __init__(self, **kwargs):\n+        super(Flatten, self).__init__(**kwargs)\n \n     @property\n     def output_shape(self):\n@@ -454,8 +467,8 @@ class RepeatVector(Layer):\n         Dimensions of input are assumed to be (nb_samples, dim).\n         Return tensor of shape (nb_samples, n, dim).\n     '''\n-    def __init__(self, n):\n-        super(RepeatVector, self).__init__()\n+    def __init__(self, n, **kwargs):\n+        super(RepeatVector, self).__init__(**kwargs)\n         self.n = n\n \n     @property\n@@ -478,11 +491,13 @@ class Dense(Layer):\n     '''\n         Just your regular fully connected NN layer.\n     '''\n+    input_ndim = 2\n+\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, name=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None):\n+                 W_constraint=None, b_constraint=None, **kwargs):\n \n-        super(Dense, self).__init__()\n+        super(Dense, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -551,8 +566,8 @@ class ActivityRegularization(Layer):\n         Layer that passes through its input unchanged, but applies an update\n         to the cost function based on the activity.\n     '''\n-    def __init__(self, l1=0., l2=0.):\n-        super(ActivityRegularization, self).__init__()\n+    def __init__(self, l1=0., l2=0., **kwargs):\n+        super(ActivityRegularization, self).__init__(**kwargs)\n         self.l1 = l1\n         self.l2 = l2\n \n@@ -577,11 +592,13 @@ class TimeDistributedDense(MaskedLayer):\n        Tensor output dimensions:  (nb_sample, time_dimension, output_dim)\n \n     '''\n+    input_ndim = 3\n+\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None):\n+                 W_constraint=None, b_constraint=None, **kwargs):\n \n-        super(TimeDistributedDense, self).__init__()\n+        super(TimeDistributedDense, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.input_dim = input_dim\n@@ -650,9 +667,8 @@ class AutoEncoder(Layer):\n         else:\n             same as decoder output\n     '''\n-    def __init__(self, encoder, decoder, output_reconstruction=True, weights=None):\n-\n-        super(AutoEncoder, self).__init__()\n+    def __init__(self, encoder, decoder, output_reconstruction=True, weights=None, **kwargs):\n+        super(AutoEncoder, self).__init__(**kwargs)\n \n         self.output_reconstruction = output_reconstruction\n         self.encoder = encoder\n@@ -729,11 +745,13 @@ class MaxoutDense(Layer):\n         Max-out layer, nb_feature is the number of pieces in the piecewise linear approx.\n         Refer to http://arxiv.org/pdf/1302.4389.pdf\n     '''\n+    input_ndim = 2\n+\n     def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None):\n+                 W_constraint=None, b_constraint=None, **kwargs):\n \n-        super(MaxoutDense, self).__init__()\n+        super(MaxoutDense, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n\n@@ -17,11 +17,13 @@ class Embedding(Layer):\n         @input_dim: size of vocabulary (highest input integer + 1)\n         @out_dim: size of dense representation\n     '''\n+    input_ndim = 2\n+\n     def __init__(self, input_dim, output_dim, init='uniform',\n                  W_regularizer=None, activity_regularizer=None, W_constraint=None,\n-                 mask_zero=False, weights=None):\n+                 mask_zero=False, weights=None, **kwargs):\n \n-        super(Embedding, self).__init__()\n+        super(Embedding, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n@@ -100,10 +102,12 @@ class WordContextProduct(Layer):\n             Efficient Estimation of Word reprensentations in Vector Space\n             http://arxiv.org/pdf/1301.3781v3.pdf\n     '''\n-    def __init__(self, input_dim, proj_dim=128,\n-                 init='uniform', activation='sigmoid', weights=None):\n+    input_ndim = 2\n \n-        super(WordContextProduct, self).__init__()\n+    def __init__(self, input_dim, proj_dim=128,\n+                 init='uniform', activation='sigmoid', weights=None, **kwargs):\n+\n+        super(WordContextProduct, self).__init__(**kwargs)\n         self.input_dim = input_dim\n         self.proj_dim = proj_dim\n         self.init = initializations.get(init)\n\n@@ -10,8 +10,8 @@ class GaussianNoise(MaskedLayer):\n     '''\n         Corruption process with GaussianNoise\n     '''\n-    def __init__(self, sigma):\n-        super(GaussianNoise, self).__init__()\n+    def __init__(self, sigma, **kwargs):\n+        super(GaussianNoise, self).__init__(**kwargs)\n         self.sigma = sigma\n         self.srng = RandomStreams(seed=np.random.randint(10e6))\n \n@@ -36,8 +36,8 @@ class GaussianDropout(MaskedLayer):\n             Srivastava, Hinton, et al. 2014\n             http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n     '''\n-    def __init__(self, p):\n-        super(GaussianDropout, self).__init__()\n+    def __init__(self, p, **kwargs):\n+        super(GaussianDropout, self).__init__(**kwargs)\n         self.p = p\n         self.srng = RandomStreams(seed=np.random.randint(10e6))\n \n\n@@ -16,8 +16,8 @@ class BatchNormalization(Layer):\n \n             momentum: momentum term in the computation of a running estimate of the mean and std of the data\n     '''\n-    def __init__(self, input_shape, epsilon=1e-6, mode=0, momentum=0.9, weights=None):\n-        super(BatchNormalization, self).__init__()\n+    def __init__(self, input_shape, epsilon=1e-6, mode=0, momentum=0.9, weights=None, **kwargs):\n+        super(BatchNormalization, self).__init__(**kwargs)\n         self.init = initializations.get(\"uniform\")\n         self._input_shape = input_shape\n         self.epsilon = epsilon\n@@ -77,10 +77,10 @@ class LRN2D(Layer):\n     License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n     \"\"\"\n \n-    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5):\n+    def __init__(self, alpha=1e-4, k=2, beta=0.75, n=5, **kwargs):\n         if n % 2 == 0:\n             raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n-        super(LRN2D, self).__init__()\n+        super(LRN2D, self).__init__(**kwargs)\n         self.alpha = alpha\n         self.k = k\n         self.beta = beta\n\n@@ -11,6 +11,8 @@ from six.moves import range\n \n \n class Recurrent(MaskedLayer):\n+    input_ndim = 3\n+\n     def get_output_mask(self, train=None):\n         if self.return_sequences:\n             return super(Recurrent, self).get_output_mask(train)\n@@ -53,9 +55,9 @@ class SimpleRNN(Recurrent):\n     '''\n     def __init__(self, input_dim, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n-                 truncate_gradient=-1, return_sequences=False):\n+                 truncate_gradient=-1, return_sequences=False, **kwargs):\n \n-        super(SimpleRNN, self).__init__()\n+        super(SimpleRNN, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.input_dim = input_dim\n@@ -127,7 +129,7 @@ class SimpleDeepRNN(Recurrent):\n     def __init__(self, input_dim, output_dim, depth=3,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False):\n+                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n \n         super(SimpleDeepRNN, self).__init__()\n         self.init = initializations.get(init)\n@@ -226,7 +228,7 @@ class GRU(Recurrent):\n     def __init__(self, input_dim, output_dim=128,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False):\n+                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n \n         super(GRU, self).__init__()\n         self.input_dim = input_dim\n@@ -331,7 +333,7 @@ class LSTM(Recurrent):\n     def __init__(self, input_dim, output_dim=128,\n                  init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one',\n                  activation='tanh', inner_activation='hard_sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False):\n+                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n \n         super(LSTM, self).__init__()\n         self.input_dim = input_dim\n@@ -446,9 +448,9 @@ class JZS1(Recurrent):\n     def __init__(self, input_dim, output_dim=128,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False):\n+                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n \n-        super(JZS1, self).__init__()\n+        super(JZS1, self).__init__(**kwargs)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n@@ -552,9 +554,9 @@ class JZS2(Recurrent):\n     def __init__(self, input_dim, output_dim=128,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False):\n+                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n \n-        super(JZS2, self).__init__()\n+        super(JZS2, self).__init__(**kwargs)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n@@ -659,9 +661,9 @@ class JZS3(Recurrent):\n     def __init__(self, input_dim, output_dim=128,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n-                 weights=None, truncate_gradient=-1, return_sequences=False):\n+                 weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n \n-        super(JZS3, self).__init__()\n+        super(JZS3, self).__init__(**kwargs)\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11eaaeb6955eb963ad52673d63d51335ee73d173", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 309 | Contributors (this commit): 9 | Commits (past 90d): 16 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,9 +5,9 @@ import sys\n import six\n \n \n-def get_from_module(identifier, module_params, module_name, instantiate=False, kwargs=None, custom_layers={}):\n+def get_from_module(identifier, module_params, module_name, instantiate=False, kwargs=None):\n     if isinstance(identifier, six.string_types):\n-        res = module_params.get(identifier, custom_layers.get(identifier))\n+        res = module_params.get(identifier)\n         if not res:\n             raise Exception('Invalid ' + str(module_name) + ': ' + str(identifier))\n         if instantiate and not kwargs:\n\n@@ -125,4 +125,7 @@ def print_layer_shapes(model, input_shapes):\n \n from .generic_utils import get_from_module\n def get_layer(identifier, kwargs=None, custom_layers={}):\n-    return get_from_module(identifier, globals(), 'layer', instantiate=True, kwargs=kwargs, custom_layers=custom_layers)\n+    # Insert custom layers into globals so they can be accessed by `get_from_module`.\n+    for cls_key in custom_layers:\n+        globals()[cls_key] = custom_layers[cls_key]\n+    return get_from_module(identifier, globals(), 'layer', instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35d66d672b2bdb5048b39153c60070c5c1f05fc4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 269 | Lines Deleted: 238 | Files Changed: 4 | Hunks: 120 | Methods Changed: 35 | Complexity Δ (Sum/Max): 18/8 | Churn Δ: 507 | Churn Cumulative: 4607 | Contributors (this commit): 33 | Commits (past 90d): 78 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,8 @@ if on_gpu():\n \n \n def conv_output_length(input_length, filter_size, border_mode, stride):\n+    if input_length is None:\n+        return None\n     assert border_mode in {'same', 'full', 'valid'}\n     if border_mode == 'same':\n         output_length = input_length\n@@ -25,6 +27,8 @@ def conv_output_length(input_length, filter_size, border_mode, stride):\n \n \n def pool_output_length(input_length, pool_size, ignore_border, stride):\n+    if input_length is None:\n+        return None\n     if ignore_border:\n         output_length = input_length - pool_size + 1\n         output_length = (output_length + stride - 1) // stride\n@@ -55,47 +59,50 @@ class Convolution1D(Layer):\n \n         if border_mode not in {'valid', 'full', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n-\n-        super(Convolution1D, self).__init__(**kwargs)\n         self.nb_filter = nb_filter\n-        self.input_dim = input_dim\n         self.filter_length = filter_length\n-        self.subsample_length = subsample_length\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n-        self.subsample = (subsample_length, 1)\n         self.border_mode = border_mode\n+        self.subsample_length = subsample_length\n \n-        self.input = T.tensor3()\n-        self.W_shape = (nb_filter, input_dim, filter_length, 1)\n-        self.W = self.init(self.W_shape)\n-        self.b = shared_zeros((nb_filter,))\n-\n-        self.params = [self.W, self.b]\n-\n-        self.regularizers = []\n+        self.subsample = (subsample_length, 1)\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        self.initial_weights = weights\n+        super(Convolution1D, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n+        self.input = T.tensor3()\n+        self.W_shape = (nb_filter, input_dim, filter_length, 1)\n+        self.W = self.init(self.W_shape)\n+        self.b = shared_zeros((nb_filter,))\n+        self.params = [self.W, self.b]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     @property\n     def output_shape(self):\n@@ -137,7 +144,6 @@ class Convolution1D(Layer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"nb_filter\": self.nb_filter,\n                 \"filter_length\": self.filter_length,\n                 \"init\": self.init.__name__,\n@@ -154,7 +160,7 @@ class Convolution1D(Layer):\n class Convolution2D(Layer):\n     input_ndim = 4\n \n-    def __init__(self, nb_filter, stack_size, nb_row, nb_col,\n+    def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample=(1, 1),\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n@@ -162,48 +168,49 @@ class Convolution2D(Layer):\n \n         if border_mode not in {'valid', 'full', 'same'}:\n             raise Exception('Invalid border mode for Convolution2D:', border_mode)\n-\n-        super(Convolution2D, self).__init__(**kwargs)\n-        self.init = initializations.get(init)\n-        self.activation = activations.get(activation)\n-        self.subsample = tuple(subsample)\n-        self.border_mode = border_mode\n         self.nb_filter = nb_filter\n-        self.stack_size = stack_size\n-\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-\n-        self.input = T.tensor4()\n-        self.W_shape = (nb_filter, stack_size, nb_row, nb_col)\n-        self.W = self.init(self.W_shape)\n-        self.b = shared_zeros((nb_filter,))\n-\n-        self.params = [self.W, self.b]\n-\n-        self.regularizers = []\n+        self.init = initializations.get(init)\n+        self.activation = activations.get(activation)\n+        self.border_mode = border_mode\n+        self.subsample = tuple(subsample)\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        self.initial_weights = weights\n+        super(Convolution2D, self).__init__(**kwargs)\n+\n+    def build(self):\n+        stack_size = self.input_shape[1]\n+        self.input = T.tensor4()\n+        self.W_shape = (nb_filter, stack_size, nb_row, nb_col)\n+        self.W = self.init(self.W_shape)\n+        self.b = shared_zeros((nb_filter,))\n+        self.params = [self.W, self.b]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     @property\n     def output_shape(self):\n@@ -248,7 +255,6 @@ class Convolution2D(Layer):\n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n                 \"nb_filter\": self.nb_filter,\n-                \"stack_size\": self.stack_size,\n                 \"nb_row\": self.nb_row,\n                 \"nb_col\": self.nb_col,\n                 \"init\": self.init.__name__,\n\n@@ -31,6 +31,16 @@ class Layer(object):\n         if not self.supports_masked_input() and layer.get_output_mask() is not None:\n             raise Exception(\"Cannot connect non-masking layer to layer with masked output\")\n         self.previous = layer\n+        self.build()\n+\n+    def build(self):\n+        '''Instantiation of layer weights.\n+\n+        Called after `set_previous`, or after `set_input_shape`,\n+        once the layer has a defined input shape.\n+        Must be implemented on all layers that have weights.\n+        '''\n+        pass\n \n     @property\n     def nb_input(self):\n@@ -49,7 +59,7 @@ class Layer(object):\n         elif hasattr(self, '_input_shape'):\n             return self._input_shape\n         else:\n-            raise NotImplementedError\n+            raise Exception('Layer is not connected.')\n \n     def set_input_shape(self, input_shape):\n         if type(input_shape) not in [tuple, list]:\n@@ -61,6 +71,7 @@ class Layer(object):\n                                 str(self.input_ndim) + ', was provided with input shape ' + str(input_shape))\n         self._input_shape = input_shape\n         self.input = ndim_tensor(len(self._input_shape))\n+        self.build()\n \n     @property\n     def output_shape(self):\n@@ -102,6 +113,7 @@ class Layer(object):\n         return None\n \n     def set_weights(self, weights):\n+        assert len(self.params) == len(weights), 'Provided weight array does not match layer weights.'\n         for p, w in zip(self.params, weights):\n             if p.eval().shape != w.shape:\n                 raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (p.eval().shape, w.shape))\n@@ -493,51 +505,49 @@ class Dense(Layer):\n     '''\n     input_ndim = 2\n \n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, name=None,\n+    def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, **kwargs):\n-\n-        super(Dense, self).__init__(**kwargs)\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n \n-        self.input = T.matrix()\n-        self.W = self.init((self.input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim))\n-\n-        self.params = [self.W, self.b]\n-\n-        self.regularizers = []\n         self.W_regularizer = regularizers.get(W_regularizer)\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        self.initial_weights = weights\n+        super(Dense, self).__init__(**kwargs)\n \n-        if name is not None:\n-            self.set_name(name)\n+    def build(self):\n+        input_dim = self.input_shape[1]\n \n-    def set_name(self, name):\n-        self.W.name = '%s_W' % name\n-        self.b.name = '%s_b' % name\n+        self.input = T.matrix()\n+        self.W = self.init((input_dim, self.output_dim))\n+        self.b = shared_zeros((self.output_dim))\n+\n+        self.params = [self.W, self.b]\n+\n+        self.regularizers = []\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     @property\n     def output_shape(self):\n@@ -550,7 +560,6 @@ class Dense(Layer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"activation\": self.activation.__name__,\n@@ -594,45 +603,49 @@ class TimeDistributedDense(MaskedLayer):\n     '''\n     input_ndim = 3\n \n-    def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None,\n+    def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, **kwargs):\n-\n-        super(TimeDistributedDense, self).__init__(**kwargs)\n+        self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n-        self.input_dim = input_dim\n-        self.output_dim = output_dim\n-\n-        self.input = T.tensor3()\n-        self.W = self.init((self.input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim))\n-\n-        self.params = [self.W, self.b]\n-\n-        self.regularizers = []\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        self.initial_weights = weights\n+        super(TimeDistributedDense, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[1]\n+\n+        self.input = T.tensor3()\n+        self.W = self.init((input_dim, self.output_dim))\n+        self.b = shared_zeros((self.output_dim))\n+\n+        self.params = [self.W, self.b]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     @property\n     def output_shape(self):\n@@ -646,7 +659,6 @@ class TimeDistributedDense(MaskedLayer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"activation\": self.activation.__name__,\n@@ -747,45 +759,49 @@ class MaxoutDense(Layer):\n     '''\n     input_ndim = 2\n \n-    def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n+    def __init__(self, output_dim, nb_feature=4, init='glorot_uniform', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None, **kwargs):\n-\n-        super(MaxoutDense, self).__init__(**kwargs)\n-        self.init = initializations.get(init)\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.nb_feature = nb_feature\n-\n-        self.input = T.matrix()\n-        self.W = self.init((self.nb_feature, self.input_dim, self.output_dim))\n-        self.b = shared_zeros((self.nb_feature, self.output_dim))\n-\n-        self.params = [self.W, self.b]\n-\n-        self.regularizers = []\n+        self.init = initializations.get(init)\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n         self.constraints = [self.W_constraint, self.b_constraint]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        self.initial_weights = weights\n+        super(MaxoutDense, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[1]\n+\n+        self.input = T.matrix()\n+        self.W = self.init((self.nb_feature, input_dim, self.output_dim))\n+        self.b = shared_zeros((self.nb_feature, self.output_dim))\n+\n+        self.params = [self.W, self.b]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     @property\n     def output_shape(self):\n@@ -799,7 +815,6 @@ class MaxoutDense(Layer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"nb_feature\": self.nb_feature,\n\n@@ -16,13 +16,16 @@ class BatchNormalization(Layer):\n \n             momentum: momentum term in the computation of a running estimate of the mean and std of the data\n     '''\n-    def __init__(self, input_shape, epsilon=1e-6, mode=0, momentum=0.9, weights=None, **kwargs):\n-        super(BatchNormalization, self).__init__(**kwargs)\n+    def __init__(self, epsilon=1e-6, mode=0, momentum=0.9, weights=None, **kwargs):\n         self.init = initializations.get(\"uniform\")\n-        self._input_shape = input_shape\n         self.epsilon = epsilon\n         self.mode = mode\n         self.momentum = momentum\n+        super(BatchNormalization, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_shape = self.input_shape  # starts with samples axis\n+        input_shape = input_shape[1:]\n         self.input = ndim_tensor(len(input_shape) + 1)\n \n         self.gamma = self.init((input_shape))\n@@ -66,9 +69,9 @@ class BatchNormalization(Layer):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_shape\": self._input_shape,\n                 \"epsilon\": self.epsilon,\n-                \"mode\": self.mode}\n+                \"mode\": self.mode,\n+                \"momentum\": self.momentum}\n \n \n class LRN2D(Layer):\n\n@@ -53,27 +53,30 @@ class SimpleRNN(Recurrent):\n         included for demonstration purposes\n         (demonstrates how to use theano.scan to build a basic RNN).\n     '''\n-    def __init__(self, input_dim, output_dim,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal', activation='sigmoid', weights=None,\n                  truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(SimpleRNN, self).__init__(**kwargs)\n+        self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n-        self.input_dim = input_dim\n-        self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n         self.activation = activations.get(activation)\n         self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+        super(SimpleRNN, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n \n-        self.W = self.init((self.input_dim, self.output_dim))\n+        self.W = self.init((input_dim, self.output_dim))\n         self.U = self.inner_init((self.output_dim, self.output_dim))\n         self.b = shared_zeros((self.output_dim))\n         self.params = [self.W, self.U, self.b]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self, x_t, mask_tm1, h_tm1, u):\n         '''\n@@ -107,7 +110,6 @@ class SimpleRNN(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"inner_init\": self.inner_init.__name__,\n@@ -126,30 +128,32 @@ class SimpleDeepRNN(Recurrent):\n         This demonstrates how to build RNNs with arbitrary lookback.\n         Also (probably) not a super useful model.\n     '''\n-    def __init__(self, input_dim, output_dim, depth=3,\n+    def __init__(self, output_dim, depth=3,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n                  weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(SimpleDeepRNN, self).__init__()\n+        self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n-        self.input_dim = input_dim\n-        self.output_dim = output_dim\n         self.truncate_gradient = truncate_gradient\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.depth = depth\n         self.return_sequences = return_sequences\n-        self.input = T.tensor3()\n+        self.initial_weights = weights\n+        super(SimpleDeepRNN, self).__init__(**kwargs)\n \n-        self.W = self.init((self.input_dim, self.output_dim))\n+    def build(self):\n+        input_dim = self.input_shape[2]\n+        self.input = T.tensor3()\n+        self.W = self.init((input_dim, self.output_dim))\n         self.Us = [self.inner_init((self.output_dim, self.output_dim)) for _ in range(self.depth)]\n         self.b = shared_zeros((self.output_dim))\n         self.params = [self.W] + self.Us + [self.b]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self, x_t, *args):\n         o = x_t\n@@ -192,7 +196,6 @@ class SimpleDeepRNN(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"depth\": self.depth,\n                 \"init\": self.init.__name__,\n@@ -225,32 +228,33 @@ class GRU(Recurrent):\n             Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n                 http://arxiv.org/pdf/1412.3555v1.pdf\n     '''\n-    def __init__(self, input_dim, output_dim=128,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n                  weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(GRU, self).__init__()\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+        super(GRU, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n \n-        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.W_z = self.init((input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n         self.b_z = shared_zeros((self.output_dim))\n \n-        self.W_r = self.init((self.input_dim, self.output_dim))\n+        self.W_r = self.init((input_dim, self.output_dim))\n         self.U_r = self.inner_init((self.output_dim, self.output_dim))\n         self.b_r = shared_zeros((self.output_dim))\n \n-        self.W_h = self.init((self.input_dim, self.output_dim))\n+        self.W_h = self.init((input_dim, self.output_dim))\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n@@ -260,8 +264,9 @@ class GRU(Recurrent):\n             self.W_h, self.U_h, self.b_h,\n         ]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self,\n               xz_t, xr_t, xh_t, mask_tm1,\n@@ -295,7 +300,6 @@ class GRU(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"inner_init\": self.inner_init.__name__,\n@@ -330,37 +334,38 @@ class LSTM(Recurrent):\n             Supervised sequence labelling with recurrent neural networks\n                 http://www.cs.toronto.edu/~graves/preprint.pdf\n     '''\n-    def __init__(self, input_dim, output_dim=128,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one',\n                  activation='tanh', inner_activation='hard_sigmoid',\n                  weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(LSTM, self).__init__()\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.forget_bias_init = initializations.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+        super(LSTM, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n \n-        self.W_i = self.init((self.input_dim, self.output_dim))\n+        self.W_i = self.init((input_dim, self.output_dim))\n         self.U_i = self.inner_init((self.output_dim, self.output_dim))\n         self.b_i = shared_zeros((self.output_dim))\n \n-        self.W_f = self.init((self.input_dim, self.output_dim))\n+        self.W_f = self.init((input_dim, self.output_dim))\n         self.U_f = self.inner_init((self.output_dim, self.output_dim))\n         self.b_f = self.forget_bias_init((self.output_dim))\n \n-        self.W_c = self.init((self.input_dim, self.output_dim))\n+        self.W_c = self.init((input_dim, self.output_dim))\n         self.U_c = self.inner_init((self.output_dim, self.output_dim))\n         self.b_c = shared_zeros((self.output_dim))\n \n-        self.W_o = self.init((self.input_dim, self.output_dim))\n+        self.W_o = self.init((input_dim, self.output_dim))\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n         self.b_o = shared_zeros((self.output_dim))\n \n@@ -371,8 +376,9 @@ class LSTM(Recurrent):\n             self.W_o, self.U_o, self.b_o,\n         ]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self,\n               xi_t, xf_t, xo_t, xc_t, mask_tm1,\n@@ -414,7 +420,6 @@ class LSTM(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"inner_init\": self.inner_init.__name__,\n@@ -445,27 +450,28 @@ class JZS1(Recurrent):\n             An Empirical Exploration of Recurrent Network Architectures\n                 http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n     '''\n-    def __init__(self, input_dim, output_dim=128,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n                  weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(JZS1, self).__init__(**kwargs)\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+        super(JZS1, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n \n-        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.W_z = self.init((input_dim, self.output_dim))\n         self.b_z = shared_zeros((self.output_dim))\n \n-        self.W_r = self.init((self.input_dim, self.output_dim))\n+        self.W_r = self.init((input_dim, self.output_dim))\n         self.U_r = self.inner_init((self.output_dim, self.output_dim))\n         self.b_r = shared_zeros((self.output_dim))\n \n@@ -473,11 +479,11 @@ class JZS1(Recurrent):\n         self.b_h = shared_zeros((self.output_dim))\n \n         # P_h used to project X onto different dimension, using sparse random projections\n-        if self.input_dim == self.output_dim:\n+        if input_dim == self.output_dim:\n             self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n         else:\n-            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n-            P = 1 / np.sqrt(self.input_dim) * P\n+            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n+            P = 1 / np.sqrt(input_dim) * P\n             self.Pmat = theano.shared(P, name=None)\n \n         self.params = [\n@@ -487,8 +493,9 @@ class JZS1(Recurrent):\n             self.Pmat\n         ]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self,\n               xz_t, xr_t, xh_t, mask_tm1,\n@@ -521,7 +528,6 @@ class JZS1(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"inner_init\": self.inner_init.__name__,\n@@ -551,40 +557,40 @@ class JZS2(Recurrent):\n             An Empirical Exploration of Recurrent Network Architectures\n                 http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n     '''\n-    def __init__(self, input_dim, output_dim=128,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n                  weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(JZS2, self).__init__(**kwargs)\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        super(JZS2, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n \n-        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.W_z = self.init((input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n         self.b_z = shared_zeros((self.output_dim))\n \n         self.U_r = self.inner_init((self.output_dim, self.output_dim))\n         self.b_r = shared_zeros((self.output_dim))\n \n-        self.W_h = self.init((self.input_dim, self.output_dim))\n+        self.W_h = self.init((input_dim, self.output_dim))\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n         # P_h used to project X onto different dimension, using sparse random projections\n-        if self.input_dim == self.output_dim:\n+        if input_dim == self.output_dim:\n             self.Pmat = theano.shared(np.identity(self.output_dim, dtype=theano.config.floatX), name=None)\n         else:\n-            P = np.random.binomial(1, 0.5, size=(self.input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n-            P = 1 / np.sqrt(self.input_dim) * P\n+            P = np.random.binomial(1, 0.5, size=(input_dim, self.output_dim)).astype(theano.config.floatX) * 2 - 1\n+            P = 1 / np.sqrt(input_dim) * P\n             self.Pmat = theano.shared(P, name=None)\n \n         self.params = [\n@@ -594,8 +600,9 @@ class JZS2(Recurrent):\n             self.Pmat\n         ]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self,\n               xz_t, xr_t, xh_t, mask_tm1,\n@@ -628,7 +635,6 @@ class JZS2(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"inner_init\": self.inner_init.__name__,\n@@ -658,32 +664,33 @@ class JZS3(Recurrent):\n             An Empirical Exploration of Recurrent Network Architectures\n                 http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\n     '''\n-    def __init__(self, input_dim, output_dim=128,\n+    def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='tanh', inner_activation='sigmoid',\n                  weights=None, truncate_gradient=-1, return_sequences=False, **kwargs):\n-\n-        super(JZS3, self).__init__(**kwargs)\n-        self.input_dim = input_dim\n         self.output_dim = output_dim\n-        self.truncate_gradient = truncate_gradient\n-        self.return_sequences = return_sequences\n-\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.truncate_gradient = truncate_gradient\n+        self.return_sequences = return_sequences\n+        self.initial_weights = weights\n+        super(JZS3, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n         self.input = T.tensor3()\n \n-        self.W_z = self.init((self.input_dim, self.output_dim))\n+        self.W_z = self.init((input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n         self.b_z = shared_zeros((self.output_dim))\n \n-        self.W_r = self.init((self.input_dim, self.output_dim))\n+        self.W_r = self.init((input_dim, self.output_dim))\n         self.U_r = self.inner_init((self.output_dim, self.output_dim))\n         self.b_r = shared_zeros((self.output_dim))\n \n-        self.W_h = self.init((self.input_dim, self.output_dim))\n+        self.W_h = self.init((input_dim, self.output_dim))\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = shared_zeros((self.output_dim))\n \n@@ -693,8 +700,9 @@ class JZS3(Recurrent):\n             self.W_h, self.U_h, self.b_h,\n         ]\n \n-        if weights is not None:\n-            self.set_weights(weights)\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n \n     def _step(self,\n               xz_t, xr_t, xh_t, mask_tm1,\n@@ -728,7 +736,6 @@ class JZS3(Recurrent):\n \n     def get_config(self):\n         return {\"name\": self.__class__.__name__,\n-                \"input_dim\": self.input_dim,\n                 \"output_dim\": self.output_dim,\n                 \"init\": self.init.__name__,\n                 \"inner_init\": self.inner_init.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
