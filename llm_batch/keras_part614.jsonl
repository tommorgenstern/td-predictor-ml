{"custom_id": "keras#761ba0e4bba8e0a735885787ffedd231462b9160", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1654 | Lines Deleted: 19 | Files Changed: 9 | Hunks: 24 | Methods Changed: 29 | Complexity Δ (Sum/Max): 163/71 | Churn Δ: 1673 | Churn Cumulative: 3250 | Contributors (this commit): 11 | Commits (past 90d): 98 | Contributors (cumulative): 19 | DMM Complexity: 0.29450757575757575\n\nDIFF:\n@@ -1,20 +1,35 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import testing\n+from keras_core.applications import mobilenet\n+from keras_core.applications import mobilenet_v2\n+from keras_core.applications import mobilenet_v3\n from keras_core.applications import vgg16\n from keras_core.applications import vgg19\n from keras_core.applications import xception\n from keras_core.utils import file_utils\n from keras_core.utils import image_utils\n \n+try:\n+    import PIL\n+except ImportError:\n+    PIL = None\n+\n MODEL_LIST = [\n     # cls, last_dim\n     (vgg16.VGG16, 512, vgg16),\n     (vgg19.VGG19, 512, vgg19),\n     (xception.Xception, 2048, xception),\n+    (mobilenet.MobileNet, 1024, mobilenet),\n+    (mobilenet_v2.MobileNetV2, 1280, mobilenet_v2),\n+    (mobilenet_v3.MobileNetV3Small, 576, mobilenet_v3),\n+    (mobilenet_v3.MobileNetV3Large, 960, mobilenet_v3),\n ]\n+# Add names for `named_parameters`.\n+MODEL_LIST = [(e[0].__name__, *e) for e in MODEL_LIST]\n \n \n def _get_elephant(target_size):\n@@ -36,7 +51,7 @@ def _get_elephant(target_size):\n \n \n class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters(MODEL_LIST)\n+    @parameterized.named_parameters(MODEL_LIST)\n     def test_application_notop_variable_input_channels(self, app, last_dim, _):\n         # Test compatibility with 1 channel\n         if backend.image_data_format() == \"channels_first\":\n@@ -56,7 +71,8 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n         output_shape = list(model.outputs[0].shape)\n         self.assertEqual(output_shape, [None, None, None, last_dim])\n \n-    @parameterized.parameters(MODEL_LIST)\n+    @parameterized.named_parameters(MODEL_LIST)\n+    @pytest.mark.skipif(PIL is None, reason=\"Requires PIL.\")\n     def test_application_base(self, app, _, app_module):\n         # Can be instantiated with default arguments\n         model = app(weights=\"imagenet\")\n@@ -74,19 +90,19 @@ class ApplicationsTest(testing.TestCase, parameterized.TestCase):\n         reconstructed_model = model.__class__.from_config(config)\n         self.assertEqual(len(model.weights), len(reconstructed_model.weights))\n \n-    @parameterized.parameters(MODEL_LIST)\n+    @parameterized.named_parameters(MODEL_LIST)\n     def test_application_notop_custom_input_shape(self, app, last_dim, _):\n         model = app(weights=None, include_top=False, input_shape=(123, 123, 3))\n         output_shape = list(model.outputs[0].shape)\n         self.assertEqual(output_shape[-1], last_dim)\n \n-    @parameterized.parameters(MODEL_LIST)\n+    @parameterized.named_parameters(MODEL_LIST)\n     def test_application_pooling(self, app, last_dim, _):\n         model = app(weights=None, include_top=False, pooling=\"max\")\n         output_shape = list(model.outputs[0].shape)\n         self.assertEqual(output_shape, [None, last_dim])\n \n-    @parameterized.parameters(MODEL_LIST)\n+    @parameterized.named_parameters(MODEL_LIST)\n     def test_application_classifier_activation(self, app, *_):\n         model = app(\n             weights=None, include_top=True, classifier_activation=\"softmax\"\n\n@@ -419,7 +419,7 @@ def correct_pad(inputs, kernel_size):\n       A tuple.\n     \"\"\"\n     img_dim = 2 if backend.image_data_format() == \"channels_first\" else 1\n-    input_size = backend.int_shape(inputs)[img_dim : (img_dim + 2)]\n+    input_size = inputs.shape[img_dim : (img_dim + 2)]\n     if isinstance(kernel_size, int):\n         kernel_size = (kernel_size, kernel_size)\n     if input_size[0] is None:\n\n@@ -0,0 +1,430 @@\n+import warnings\n+\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHT_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/\"\n+)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.mobilenet.MobileNet\",\n+        \"keras_core.applications.MobileNet\",\n+    ]\n+)\n+def MobileNet(\n+    input_shape=None,\n+    alpha=1.0,\n+    depth_multiplier=1,\n+    dropout=1e-3,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the MobileNet architecture.\n+\n+    Reference:\n+    - [MobileNets: Efficient Convolutional Neural Networks\n+       for Mobile Vision Applications](\n+        https://arxiv.org/abs/1704.04861)\n+\n+    This function returns a Keras image classification model,\n+    optionally loaded with weights pre-trained on ImageNet.\n+\n+    For image classification use cases, see\n+    [this page for detailed examples](\n+    https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+    For transfer learning use cases, make sure to read the\n+    [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For MobileNet, call `keras_core.applications.mobilenet.preprocess_input`\n+    on your inputs before passing them to the model.\n+    `mobilenet.preprocess_input` will scale input pixels between -1 and 1.\n+\n+    Args:\n+        input_shape: Optional shape tuple, only to be specified if `include_top`\n+            is `False` (otherwise the input shape has to be `(224, 224, 3)`\n+            (with `\"channels_last\"` data format) or `(3, 224, 224)`\n+            (with `\"channels_first\"` data format).\n+            It should have exactly 3 inputs channels, and width and\n+            height should be no smaller than 32. E.g. `(200, 200, 3)` would\n+            be one valid value. Defaults to `None`.\n+            `input_shape` will be ignored if the `input_tensor` is provided.\n+        alpha: Controls the width of the network. This is known as the width\n+            multiplier in the MobileNet paper.\n+            - If `alpha < 1.0`, proportionally decreases the number\n+                of filters in each layer.\n+            - If `alpha > 1.0`, proportionally increases the number\n+                of filters in each layer.\n+            - If `alpha == 1`, default number of filters from the paper\n+                are used at each layer. Defaults to `1.0`.\n+        depth_multiplier: Depth multiplier for depthwise convolution.\n+            This is called the resolution multiplier in the MobileNet paper.\n+            Defaults to `1.0`.\n+        dropout: Dropout rate. Defaults to `0.001`.\n+        include_top: Boolean, whether to include the fully-connected layer\n+            at the top of the network. Defaults to `True`.\n+        weights: One of `None` (random initialization), `\"imagenet\"`\n+            (pre-training on ImageNet), or the path to the weights file\n+            to be loaded. Defaults to `\"imagenet\"`.\n+        input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n+            to use as image input for the model. `input_tensor` is useful\n+            for sharing inputs between multiple different networks.\n+            Defaults to `None`.\n+        pooling: Optional pooling mode for feature extraction when `include_top`\n+            is `False`.\n+            - `None` (default) means that the output of the model will be\n+                the 4D tensor output of the last convolutional block.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional block, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will be applied.\n+        classes: Optional number of classes to classify images into,\n+            only to be specified if `include_top` is `True`, and if\n+            no `weights` argument is specified. Defaults to `1000`.\n+        classifier_activation: A `str` or callable. The activation function\n+            to use on the \"top\" layer. Ignored unless `include_top=True`.\n+            Set `classifier_activation=None` to return the logits of the \"top\"\n+            layer. When loading pretrained weights, `classifier_activation`\n+            can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), 'imagenet' \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded. \"\n+            f\"Received weights={weights}\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            \"If using `weights='imagenet'` with `include_top=True`, \"\n+            \"`classes` should be 1000.  \"\n+            f\"Received classes={classes}\"\n+        )\n+\n+    # Determine proper input shape and default size.\n+    if input_shape is None:\n+        default_size = 224\n+    else:\n+        if backend.image_data_format() == \"channels_first\":\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            rows = input_shape[0]\n+            cols = input_shape[1]\n+\n+        if rows == cols and rows in [128, 160, 192, 224]:\n+            default_size = rows\n+        else:\n+            default_size = 224\n+\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=default_size,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if backend.image_data_format() == \"channels_last\":\n+        row_axis, col_axis = (0, 1)\n+    else:\n+        row_axis, col_axis = (1, 2)\n+    rows = input_shape[row_axis]\n+    cols = input_shape[col_axis]\n+\n+    if weights == \"imagenet\":\n+        if depth_multiplier != 1:\n+            raise ValueError(\n+                \"If imagenet weights are being loaded, \"\n+                \"depth multiplier must be 1.  \"\n+                f\"Received depth_multiplier={depth_multiplier}\"\n+            )\n+\n+        if alpha not in [0.25, 0.50, 0.75, 1.0]:\n+            raise ValueError(\n+                \"If imagenet weights are being loaded, \"\n+                \"alpha can be one of\"\n+                \"`0.25`, `0.50`, `0.75` or `1.0` only.  \"\n+                f\"Received alpha={alpha}\"\n+            )\n+\n+        if rows != cols or rows not in [128, 160, 192, 224]:\n+            rows = 224\n+            warnings.warn(\n+                \"`input_shape` is undefined or non-square, \"\n+                \"or `rows` is not in [128, 160, 192, 224]. \"\n+                \"Weights for input shape (224, 224) will be \"\n+                \"loaded as the default.\",\n+                stacklevel=2,\n+            )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n+    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1)\n+\n+    x = _depthwise_conv_block(\n+        x, 128, alpha, depth_multiplier, strides=(2, 2), block_id=2\n+    )\n+    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3)\n+\n+    x = _depthwise_conv_block(\n+        x, 256, alpha, depth_multiplier, strides=(2, 2), block_id=4\n+    )\n+    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5)\n+\n+    x = _depthwise_conv_block(\n+        x, 512, alpha, depth_multiplier, strides=(2, 2), block_id=6\n+    )\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10)\n+    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11)\n+\n+    x = _depthwise_conv_block(\n+        x, 1024, alpha, depth_multiplier, strides=(2, 2), block_id=12\n+    )\n+    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n+\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D(keepdims=True)(x)\n+        x = layers.Dropout(dropout, name=\"dropout\")(x)\n+        x = layers.Conv2D(classes, (1, 1), padding=\"same\", name=\"conv_preds\")(x)\n+        x = layers.Reshape((classes,), name=\"reshape_2\")(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Activation(\n+            activation=classifier_activation, name=\"predictions\"\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D()(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D()(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Functional(inputs, x, name=f\"mobilenet_{alpha:0.2f}_{rows}\")\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if alpha == 1.0:\n+            alpha_text = \"1_0\"\n+        elif alpha == 0.75:\n+            alpha_text = \"7_5\"\n+        elif alpha == 0.50:\n+            alpha_text = \"5_0\"\n+        else:\n+            alpha_text = \"2_5\"\n+\n+        if include_top:\n+            model_name = \"mobilenet_%s_%d_tf.h5\" % (alpha_text, rows)\n+            weight_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = file_utils.get_file(\n+                model_name, weight_path, cache_subdir=\"models\"\n+            )\n+        else:\n+            model_name = \"mobilenet_%s_%d_tf_no_top.h5\" % (alpha_text, rows)\n+            weight_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = file_utils.get_file(\n+                model_name, weight_path, cache_subdir=\"models\"\n+            )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n+    \"\"\"Adds an initial convolution layer (with batch normalization and relu6).\n+\n+    Args:\n+        inputs: Input tensor of shape `(rows, cols, 3)` (with `channels_last`\n+            data format) or (3, rows, cols) (with `channels_first` data format).\n+            It should have exactly 3 inputs channels, and width and height\n+            should be no smaller than 32. E.g. `(224, 224, 3)` would be\n+            one valid value.\n+        filters: Integer, the dimensionality of the output space (i.e. the\n+            number of output filters in the convolution).\n+        alpha: controls the width of the network. - If `alpha` < 1.0,\n+            proportionally decreases the number of filters in each layer.\n+            - If `alpha` > 1.0, proportionally increases the number of filters\n+                in each layer.\n+            - If `alpha` = 1, default number of filters from the paper are\n+                used at each layer.\n+        kernel: An integer or tuple/list of 2 integers, specifying the width\n+            and height of the 2D convolution window.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions.\n+        strides: An integer or tuple/list of 2 integers, specifying the strides\n+            of the convolution along the width and height.\n+            Can be a single integer to specify the same value for all\n+            spatial dimensions. Specifying any stride value != 1 is\n+            incompatible with specifying any `dilation_rate`\n+            value != 1. # Input shape\n+        4D tensor with shape: `(samples, channels, rows, cols)` if\n+            data_format='channels_first'\n+        or 4D tensor with shape: `(samples, rows, cols, channels)` if\n+            data_format='channels_last'. # Output shape\n+        4D tensor with shape: `(samples, filters, new_rows, new_cols)`\n+            if data_format='channels_first'\n+        or 4D tensor with shape: `(samples, new_rows, new_cols, filters)`\n+            if data_format='channels_last'. `rows` and `cols` values\n+            might have changed due to stride.\n+\n+    Returns:\n+        Output tensor of block.\n+    \"\"\"\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n+    filters = int(filters * alpha)\n+    x = layers.Conv2D(\n+        filters,\n+        kernel,\n+        padding=\"same\",\n+        use_bias=False,\n+        strides=strides,\n+        name=\"conv1\",\n+    )(inputs)\n+    x = layers.BatchNormalization(axis=channel_axis, name=\"conv1_bn\")(x)\n+    return layers.ReLU(6.0, name=\"conv1_relu\")(x)\n+\n+\n+def _depthwise_conv_block(\n+    inputs,\n+    pointwise_conv_filters,\n+    alpha,\n+    depth_multiplier=1,\n+    strides=(1, 1),\n+    block_id=1,\n+):\n+    \"\"\"Adds a depthwise convolution block.\n+\n+    A depthwise convolution block consists of a depthwise conv,\n+    batch normalization, relu6, pointwise convolution,\n+    batch normalization and relu6 activation.\n+\n+    Args:\n+        inputs: Input tensor of shape `(rows, cols, channels)` (with\n+            `channels_last` data format) or (channels, rows, cols) (with\n+            `channels_first` data format).\n+        pointwise_conv_filters: Integer, the dimensionality of the output space\n+            (i.e. the number of output filters in the pointwise convolution).\n+        alpha: controls the width of the network. - If `alpha` < 1.0,\n+            proportionally decreases the number of filters in each layer.\n+            - If `alpha` > 1.0, proportionally increases the number of filters\n+                in each layer.\n+            - If `alpha` = 1, default number of filters from the paper are\n+                used at each layer.\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel. The total number of depthwise convolution\n+            output channels will be equal to `filters_in * depth_multiplier`.\n+        strides: An integer or tuple/list of 2 integers, specifying the strides\n+            of the convolution along the width and height.\n+            Can be a single integer to specify the same value for\n+            all spatial dimensions. Specifying any stride value != 1 is\n+            incompatible with specifying any `dilation_rate` value != 1.\n+        block_id: Integer, a unique identification designating the block number.\n+            # Input shape\n+        4D tensor with shape: `(batch, channels, rows, cols)` if\n+            data_format='channels_first'\n+        or 4D tensor with shape: `(batch, rows, cols, channels)` if\n+            data_format='channels_last'. # Output shape\n+        4D tensor with shape: `(batch, filters, new_rows, new_cols)` if\n+            data_format='channels_first'\n+        or 4D tensor with shape: `(batch, new_rows, new_cols, filters)` if\n+            data_format='channels_last'. `rows` and `cols` values might have\n+            changed due to stride.\n+\n+    Returns:\n+        Output tensor of block.\n+    \"\"\"\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n+    pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n+\n+    if strides == (1, 1):\n+        x = inputs\n+    else:\n+        x = layers.ZeroPadding2D(\n+            ((0, 1), (0, 1)), name=\"conv_pad_%d\" % block_id\n+        )(inputs)\n+    x = layers.DepthwiseConv2D(\n+        (3, 3),\n+        padding=\"same\" if strides == (1, 1) else \"valid\",\n+        depth_multiplier=depth_multiplier,\n+        strides=strides,\n+        use_bias=False,\n+        name=\"conv_dw_%d\" % block_id,\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis, name=\"conv_dw_%d_bn\" % block_id\n+    )(x)\n+    x = layers.ReLU(6.0, name=\"conv_dw_%d_relu\" % block_id)(x)\n+\n+    x = layers.Conv2D(\n+        pointwise_conv_filters,\n+        (1, 1),\n+        padding=\"same\",\n+        use_bias=False,\n+        strides=(1, 1),\n+        name=\"conv_pw_%d\" % block_id,\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis, name=\"conv_pw_%d_bn\" % block_id\n+    )(x)\n+    return layers.ReLU(6.0, name=\"conv_pw_%d_relu\" % block_id)(x)\n+\n+\n+@keras_core_export(\"keras_core.applications.mobilenet.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    return imagenet_utils.preprocess_input(\n+        x, data_format=data_format, mode=\"tf\"\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.mobilenet.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n+    mode=\"\",\n+    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TF,\n+    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC,\n+)\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -0,0 +1,502 @@\n+import warnings\n+\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHT_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/\"\n+)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.mobilenet_v2.MobileNetV2\",\n+        \"keras_core.applications.MobileNetV2\",\n+    ]\n+)\n+def MobileNetV2(\n+    input_shape=None,\n+    alpha=1.0,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates the MobileNetV2 architecture.\n+\n+    MobileNetV2 is very similar to the original MobileNet,\n+    except that it uses inverted residual blocks with\n+    bottlenecking features. It has a drastically lower\n+    parameter count than the original MobileNet.\n+    MobileNets support any input size greater\n+    than 32 x 32, with larger image sizes\n+    offering better performance.\n+\n+    Reference:\n+    - [MobileNetV2: Inverted Residuals and Linear Bottlenecks](\n+        https://arxiv.org/abs/1801.04381) (CVPR 2018)\n+\n+    This function returns a Keras image classification model,\n+    optionally loaded with weights pre-trained on ImageNet.\n+\n+    For image classification use cases, see\n+    [this page for detailed examples](\n+      https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+    For transfer learning use cases, make sure to read the\n+    [guide to transfer learning & fine-tuning](\n+      https://keras.io/guides/transfer_learning/).\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For MobileNetV2, call\n+    `keras_core.applications.mobilenet_v2.preprocess_input`\n+    on your inputs before passing them to the model.\n+    `mobilenet_v2.preprocess_input` will scale input pixels between -1 and 1.\n+\n+    Args:\n+        input_shape: Optional shape tuple, only to be specified if `include_top`\n+            is `False` (otherwise the input shape has to be `(224, 224, 3)`\n+            (with `\"channels_last\"` data format) or `(3, 224, 224)`\n+            (with `\"channels_first\"` data format).\n+            It should have exactly 3 inputs channels, and width and\n+            height should be no smaller than 32. E.g. `(200, 200, 3)` would\n+            be one valid value. Defaults to `None`.\n+            `input_shape` will be ignored if the `input_tensor` is provided.\n+        alpha: Controls the width of the network. This is known as the width\n+            multiplier in the MobileNet paper.\n+            - If `alpha < 1.0`, proportionally decreases the number\n+                of filters in each layer.\n+            - If `alpha > 1.0`, proportionally increases the number\n+                of filters in each layer.\n+            - If `alpha == 1`, default number of filters from the paper\n+                are used at each layer. Defaults to `1.0`.\n+        include_top: Boolean, whether to include the fully-connected layer\n+            at the top of the network. Defaults to `True`.\n+        weights: One of `None` (random initialization), `\"imagenet\"`\n+            (pre-training on ImageNet), or the path to the weights file\n+            to be loaded. Defaults to `\"imagenet\"`.\n+        input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n+            to use as image input for the model. `input_tensor` is useful\n+            for sharing inputs between multiple different networks.\n+            Defaults to `None`.\n+        pooling: Optional pooling mode for feature extraction when `include_top`\n+            is `False`.\n+            - `None` (default) means that the output of the model will be\n+                the 4D tensor output of the last convolutional block.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional block, and thus\n+                the output of the model will be a 2D tensor.\n+            - `max` means that global max pooling will be applied.\n+        classes: Optional number of classes to classify images into,\n+            only to be specified if `include_top` is `True`, and if\n+            no `weights` argument is specified. Defaults to `1000`.\n+        classifier_activation: A `str` or callable. The activation function\n+            to use on the \"top\" layer. Ignored unless `include_top=True`.\n+            Set `classifier_activation=None` to return the logits of the \"top\"\n+            layer. When loading pretrained weights, `classifier_activation`\n+            can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.  \"\n+            f\"Received `weights={weights}`\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights` as `\"imagenet\"` with `include_top` '\n+            f\"as true, `classes` should be 1000. Received `classes={classes}`\"\n+        )\n+\n+    # Determine proper input shape and default size.\n+    # If both input_shape and input_tensor are used, they should match\n+    if input_shape is not None and input_tensor is not None:\n+        try:\n+            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n+        except ValueError:\n+            try:\n+                is_input_t_tensor = backend.is_keras_tensor(\n+                    operation_utils.get_source_inputs(input_tensor)\n+                )\n+            except ValueError:\n+                raise ValueError(\n+                    f\"input_tensor: {input_tensor}\"\n+                    \"is not type input_tensor. \"\n+                    f\"Received `type(input_tensor)={type(input_tensor)}`\"\n+                )\n+        if is_input_t_tensor:\n+            if backend.image_data_format() == \"channels_first\":\n+                if input_tensor.shape[1] != input_shape[1]:\n+                    raise ValueError(\n+                        \"input_shape[1] must equal shape(input_tensor)[1] \"\n+                        \"when `image_data_format` is `channels_first`; \"\n+                        \"Received `input_tensor.shape=\"\n+                        f\"{input_tensor.shape}`\"\n+                        f\", `input_shape={input_shape}`\"\n+                    )\n+            else:\n+                if input_tensor.shape[2] != input_shape[1]:\n+                    raise ValueError(\n+                        \"input_tensor.shape[2] must equal input_shape[1]; \"\n+                        \"Received `input_tensor.shape=\"\n+                        f\"{input_tensor.shape}`, \"\n+                        f\"`input_shape={input_shape}`\"\n+                    )\n+        else:\n+            raise ValueError(\n+                \"input_tensor is not a Keras tensor; \"\n+                f\"Received `input_tensor={input_tensor}`\"\n+            )\n+\n+    # If input_shape is None, infer shape from input_tensor.\n+    if input_shape is None and input_tensor is not None:\n+        try:\n+            backend.is_keras_tensor(input_tensor)\n+        except ValueError:\n+            raise ValueError(\n+                \"input_tensor must be a valid Keras tensor type; \"\n+                f\"Received {input_tensor} of type {type(input_tensor)}\"\n+            )\n+\n+        if input_shape is None and not backend.is_keras_tensor(input_tensor):\n+            default_size = 224\n+        elif input_shape is None and backend.is_keras_tensor(input_tensor):\n+            if backend.image_data_format() == \"channels_first\":\n+                rows = input_tensor.shape[2]\n+                cols = input_tensor.shape[3]\n+            else:\n+                rows = input_tensor.shape[1]\n+                cols = input_tensor.shape[2]\n+\n+            if rows == cols and rows in [96, 128, 160, 192, 224]:\n+                default_size = rows\n+            else:\n+                default_size = 224\n+\n+    # If input_shape is None and no input_tensor\n+    elif input_shape is None:\n+        default_size = 224\n+\n+    # If input_shape is not None, assume default size.\n+    else:\n+        if backend.image_data_format() == \"channels_first\":\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            rows = input_shape[0]\n+            cols = input_shape[1]\n+\n+        if rows == cols and rows in [96, 128, 160, 192, 224]:\n+            default_size = rows\n+        else:\n+            default_size = 224\n+\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=default_size,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if backend.image_data_format() == \"channels_last\":\n+        row_axis, col_axis = (0, 1)\n+    else:\n+        row_axis, col_axis = (1, 2)\n+    rows = input_shape[row_axis]\n+    cols = input_shape[col_axis]\n+\n+    if weights == \"imagenet\":\n+        if alpha not in [0.35, 0.50, 0.75, 1.0, 1.3, 1.4]:\n+            raise ValueError(\n+                \"If imagenet weights are being loaded, \"\n+                \"alpha must be one of `0.35`, `0.50`, `0.75`, \"\n+                \"`1.0`, `1.3` or `1.4` only;\"\n+                f\" Received `alpha={alpha}`\"\n+            )\n+\n+        if rows != cols or rows not in [96, 128, 160, 192, 224]:\n+            rows = 224\n+            warnings.warn(\n+                \"`input_shape` is undefined or non-square, \"\n+                \"or `rows` is not in [96, 128, 160, 192, 224]. \"\n+                \"Weights for input shape (224, 224) will be \"\n+                \"loaded as the default.\",\n+                stacklevel=2,\n+            )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n+\n+    first_block_filters = _make_divisible(32 * alpha, 8)\n+    x = layers.Conv2D(\n+        first_block_filters,\n+        kernel_size=3,\n+        strides=(2, 2),\n+        padding=\"same\",\n+        use_bias=False,\n+        name=\"Conv1\",\n+    )(img_input)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"bn_Conv1\"\n+    )(x)\n+    x = layers.ReLU(6.0, name=\"Conv1_relu\")(x)\n+\n+    x = _inverted_res_block(\n+        x, filters=16, alpha=alpha, stride=1, expansion=1, block_id=0\n+    )\n+\n+    x = _inverted_res_block(\n+        x, filters=24, alpha=alpha, stride=2, expansion=6, block_id=1\n+    )\n+    x = _inverted_res_block(\n+        x, filters=24, alpha=alpha, stride=1, expansion=6, block_id=2\n+    )\n+\n+    x = _inverted_res_block(\n+        x, filters=32, alpha=alpha, stride=2, expansion=6, block_id=3\n+    )\n+    x = _inverted_res_block(\n+        x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=4\n+    )\n+    x = _inverted_res_block(\n+        x, filters=32, alpha=alpha, stride=1, expansion=6, block_id=5\n+    )\n+\n+    x = _inverted_res_block(\n+        x, filters=64, alpha=alpha, stride=2, expansion=6, block_id=6\n+    )\n+    x = _inverted_res_block(\n+        x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=7\n+    )\n+    x = _inverted_res_block(\n+        x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=8\n+    )\n+    x = _inverted_res_block(\n+        x, filters=64, alpha=alpha, stride=1, expansion=6, block_id=9\n+    )\n+\n+    x = _inverted_res_block(\n+        x, filters=96, alpha=alpha, stride=1, expansion=6, block_id=10\n+    )\n+    x = _inverted_res_block(\n+        x, filters=96, alpha=alpha, stride=1, expansion=6, block_id=11\n+    )\n+    x = _inverted_res_block(\n+        x, filters=96, alpha=alpha, stride=1, expansion=6, block_id=12\n+    )\n+\n+    x = _inverted_res_block(\n+        x, filters=160, alpha=alpha, stride=2, expansion=6, block_id=13\n+    )\n+    x = _inverted_res_block(\n+        x, filters=160, alpha=alpha, stride=1, expansion=6, block_id=14\n+    )\n+    x = _inverted_res_block(\n+        x, filters=160, alpha=alpha, stride=1, expansion=6, block_id=15\n+    )\n+\n+    x = _inverted_res_block(\n+        x, filters=320, alpha=alpha, stride=1, expansion=6, block_id=16\n+    )\n+\n+    # no alpha applied to last conv as stated in the paper:\n+    # if the width multiplier is greater than 1 we increase the number of output\n+    # channels.\n+    if alpha > 1.0:\n+        last_block_filters = _make_divisible(1280 * alpha, 8)\n+    else:\n+        last_block_filters = 1280\n+\n+    x = layers.Conv2D(\n+        last_block_filters, kernel_size=1, use_bias=False, name=\"Conv_1\"\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"Conv_1_bn\"\n+    )(x)\n+    x = layers.ReLU(6.0, name=\"out_relu\")(x)\n+\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D()(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes, activation=classifier_activation, name=\"predictions\"\n+        )(x)\n+\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D()(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D()(x)\n+\n+    # Ensure that the model takes into account any potential predecessors of\n+    # `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Functional(inputs, x, name=f\"mobilenetv2_{alpha:0.2f}_{rows}\")\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if include_top:\n+            model_name = (\n+                \"mobilenet_v2_weights_tf_dim_ordering_tf_kernels_\"\n+                + str(float(alpha))\n+                + \"_\"\n+                + str(rows)\n+                + \".h5\"\n+            )\n+            weight_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = file_utils.get_file(\n+                model_name, weight_path, cache_subdir=\"models\"\n+            )\n+        else:\n+            model_name = (\n+                \"mobilenet_v2_weights_tf_dim_ordering_tf_kernels_\"\n+                + str(float(alpha))\n+                + \"_\"\n+                + str(rows)\n+                + \"_no_top\"\n+                + \".h5\"\n+            )\n+            weight_path = BASE_WEIGHT_PATH + model_name\n+            weights_path = file_utils.get_file(\n+                model_name, weight_path, cache_subdir=\"models\"\n+            )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n+    \"\"\"Inverted ResNet block.\"\"\"\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n+\n+    in_channels = inputs.shape[channel_axis]\n+    pointwise_conv_filters = int(filters * alpha)\n+    # Ensure the number of filters on the last 1x1 convolution is divisible by\n+    # 8.\n+    pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n+    x = inputs\n+    prefix = f\"block_{block_id}_\"\n+\n+    if block_id:\n+        # Expand with a pointwise 1x1 convolution.\n+        x = layers.Conv2D(\n+            expansion * in_channels,\n+            kernel_size=1,\n+            padding=\"same\",\n+            use_bias=False,\n+            activation=None,\n+            name=prefix + \"expand\",\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=channel_axis,\n+            epsilon=1e-3,\n+            momentum=0.999,\n+            name=prefix + \"expand_BN\",\n+        )(x)\n+        x = layers.ReLU(6.0, name=prefix + \"expand_relu\")(x)\n+    else:\n+        prefix = \"expanded_conv_\"\n+\n+    # Depthwise 3x3 convolution.\n+    if stride == 2:\n+        x = layers.ZeroPadding2D(\n+            padding=imagenet_utils.correct_pad(x, 3), name=prefix + \"pad\"\n+        )(x)\n+    x = layers.DepthwiseConv2D(\n+        kernel_size=3,\n+        strides=stride,\n+        activation=None,\n+        use_bias=False,\n+        padding=\"same\" if stride == 1 else \"valid\",\n+        name=prefix + \"depthwise\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis,\n+        epsilon=1e-3,\n+        momentum=0.999,\n+        name=prefix + \"depthwise_BN\",\n+    )(x)\n+\n+    x = layers.ReLU(6.0, name=prefix + \"depthwise_relu\")(x)\n+\n+    # Project with a pointwise 1x1 convolution.\n+    x = layers.Conv2D(\n+        pointwise_filters,\n+        kernel_size=1,\n+        padding=\"same\",\n+        use_bias=False,\n+        activation=None,\n+        name=prefix + \"project\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis,\n+        epsilon=1e-3,\n+        momentum=0.999,\n+        name=prefix + \"project_BN\",\n+    )(x)\n+\n+    if in_channels == pointwise_filters and stride == 1:\n+        return layers.Add(name=prefix + \"add\")([inputs, x])\n+    return x\n+\n+\n+def _make_divisible(v, divisor, min_value=None):\n+    if min_value is None:\n+        min_value = divisor\n+    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n+    # Make sure that round down does not go down by more than 10%.\n+    if new_v < 0.9 * v:\n+        new_v += divisor\n+    return new_v\n+\n+\n+@keras_core_export(\"keras_core.applications.mobilenet_v2.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    return imagenet_utils.preprocess_input(\n+        x, data_format=data_format, mode=\"tf\"\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.mobilenet_v2.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n+    mode=\"\",\n+    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TF,\n+    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC,\n+)\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -0,0 +1,679 @@\n+import warnings\n+\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHT_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/\"\n+)\n+WEIGHTS_HASHES = {\n+    \"large_224_0.75_float\": (\n+        \"765b44a33ad4005b3ac83185abf1d0eb\",\n+        \"40af19a13ebea4e2ee0c676887f69a2e\",\n+    ),\n+    \"large_224_1.0_float\": (\n+        \"59e551e166be033d707958cf9e29a6a7\",\n+        \"07fb09a5933dd0c8eaafa16978110389\",\n+    ),\n+    \"large_minimalistic_224_1.0_float\": (\n+        \"675e7b876c45c57e9e63e6d90a36599c\",\n+        \"ec5221f64a2f6d1ef965a614bdae7973\",\n+    ),\n+    \"small_224_0.75_float\": (\n+        \"cb65d4e5be93758266aa0a7f2c6708b7\",\n+        \"ebdb5cc8e0b497cd13a7c275d475c819\",\n+    ),\n+    \"small_224_1.0_float\": (\n+        \"8768d4c2e7dee89b9d02b2d03d65d862\",\n+        \"d3e8ec802a04aa4fc771ee12a9a9b836\",\n+    ),\n+    \"small_minimalistic_224_1.0_float\": (\n+        \"99cd97fb2fcdad2bf028eb838de69e37\",\n+        \"cde8136e733e811080d9fcd8a252f7e4\",\n+    ),\n+}\n+\n+\n+BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n+\n+Reference:\n+- [Searching for MobileNetV3](\n+    https://arxiv.org/pdf/1905.02244.pdf) (ICCV 2019)\n+\n+The following table describes the performance of MobileNets v3:\n+------------------------------------------------------------------------\n+MACs stands for Multiply Adds\n+\n+|Classification Checkpoint|MACs(M)|Parameters(M)|Top1 Accuracy|Pixel1 CPU(ms)|\n+|---|---|---|---|---|\n+| mobilenet_v3_large_1.0_224              | 217 | 5.4 |   75.6   |   51.2  |\n+| mobilenet_v3_large_0.75_224             | 155 | 4.0 |   73.3   |   39.8  |\n+| mobilenet_v3_large_minimalistic_1.0_224 | 209 | 3.9 |   72.3   |   44.1  |\n+| mobilenet_v3_small_1.0_224              | 66  | 2.9 |   68.1   |   15.8  |\n+| mobilenet_v3_small_0.75_224             | 44  | 2.4 |   65.4   |   12.8  |\n+| mobilenet_v3_small_minimalistic_1.0_224 | 65  | 2.0 |   61.9   |   12.2  |\n+\n+For image classification use cases, see\n+[this page for detailed examples](\n+https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+For transfer learning use cases, make sure to read the\n+[guide to transfer learning & fine-tuning](\n+https://keras.io/guides/transfer_learning/).\n+\n+Note: each Keras Application expects a specific kind of input preprocessing.\n+For MobileNetV3, by default input preprocessing is included as a part of the\n+model (as a `Rescaling` layer), and thus\n+`keras_core.applications.mobilenet_v3.preprocess_input` is actually a\n+pass-through function. In this use case, MobileNetV3 models expect their\n+inputs to be float tensors of pixels with values in the `[0-255]` range.\n+At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n+layer) can be disabled by setting `include_preprocessing` argument to `False`.\n+With preprocessing disabled MobileNetV3 models expect their inputs to be float\n+tensors of pixels with values in the `[-1, 1]` range.\n+\n+Args:\n+    input_shape: Optional shape tuple, to be specified if you would\n+        like to use a model with an input image resolution that is not\n+        `(224, 224, 3)`.\n+        It should have exactly 3 inputs channels.\n+        You can also omit this option if you would like\n+        to infer input_shape from an input_tensor.\n+        If you choose to include both input_tensor and input_shape then\n+        input_shape will be used if they match, if the shapes\n+        do not match then we will throw an error.\n+        E.g. `(160, 160, 3)` would be one valid value.\n+    alpha: controls the width of the network. This is known as the\n+        depth multiplier in the MobileNetV3 paper, but the name is kept for\n+        consistency with MobileNetV1 in Keras.\n+        - If `alpha < 1.0`, proportionally decreases the number\n+            of filters in each layer.\n+        - If `alpha > 1.0`, proportionally increases the number\n+            of filters in each layer.\n+        - If `alpha == 1`, default number of filters from the paper\n+            are used at each layer.\n+    minimalistic: In addition to large and small models this module also\n+        contains so-called minimalistic models, these models have the same\n+        per-layer dimensions characteristic as MobilenetV3 however, they don't\n+        utilize any of the advanced blocks (squeeze-and-excite units,\n+        hard-swish, and 5x5 convolutions).\n+        While these models are less efficient on CPU, they\n+        are much more performant on GPU/DSP.\n+    include_top: Boolean, whether to include the fully-connected\n+        layer at the top of the network. Defaults to `True`.\n+    weights: String, one of `None` (random initialization),\n+        'imagenet' (pre-training on ImageNet),\n+        or the path to the weights file to be loaded.\n+    input_tensor: Optional Keras tensor (i.e. output of\n+        `layers.Input()`)\n+        to use as image input for the model.\n+    pooling: String, optional pooling mode for feature extraction\n+        when `include_top` is `False`.\n+        - `None` means that the output of the model\n+            will be the 4D tensor output of the\n+            last convolutional block.\n+        - `avg` means that global average pooling\n+            will be applied to the output of the\n+            last convolutional block, and thus\n+            the output of the model will be a\n+            2D tensor.\n+        - `max` means that global max pooling will\n+            be applied.\n+    classes: Integer, optional number of classes to classify images\n+        into, only to be specified if `include_top` is True, and\n+        if no `weights` argument is specified.\n+    dropout_rate: fraction of the input units to drop on the last layer.\n+    classifier_activation: A `str` or callable. The activation function to use\n+        on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        `classifier_activation=None` to return the logits of the \"top\" layer.\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n+    include_preprocessing: Boolean, whether to include the preprocessing\n+        layer (`Rescaling`) at the bottom of the network. Defaults to `True`.\n+\n+Call arguments:\n+    inputs: A floating point `numpy.array` or backend-native tensor,\n+        4D with 3 color channels, with values in the range `[0, 255]`\n+        if `include_preprocessing` is `True` and in the range `[-1, 1]`\n+        otherwise.\n+\n+Returns:\n+    A model instance.\n+\"\"\"\n+\n+\n+def MobileNetV3(\n+    stack_fn,\n+    last_point_ch,\n+    input_shape=None,\n+    alpha=1.0,\n+    model_type=\"large\",\n+    minimalistic=False,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    classes=1000,\n+    pooling=None,\n+    dropout_rate=0.2,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.  \"\n+            f\"Received weights={weights}\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights` as `\"imagenet\"` with `include_top` '\n+            \"as true, `classes` should be 1000.  \"\n+            f\"Received classes={classes}\"\n+        )\n+\n+    # Determine proper input shape and default size.\n+    # If both input_shape and input_tensor are used, they should match\n+    if input_shape is not None and input_tensor is not None:\n+        try:\n+            is_input_t_tensor = backend.is_keras_tensor(input_tensor)\n+        except ValueError:\n+            try:\n+                is_input_t_tensor = backend.is_keras_tensor(\n+                    operation_utils.get_source_inputs(input_tensor)\n+                )\n+            except ValueError:\n+                raise ValueError(\n+                    \"input_tensor: \",\n+                    input_tensor,\n+                    \"is not type input_tensor.  \"\n+                    f\"Received type(input_tensor)={type(input_tensor)}\",\n+                )\n+        if is_input_t_tensor:\n+            if backend.image_data_format() == \"channels_first\":\n+                if input_tensor.shape[1] != input_shape[1]:\n+                    raise ValueError(\n+                        \"When backend.image_data_format()=channels_first, \"\n+                        \"input_shape[1] must equal \"\n+                        \"input_tensor.shape[1].  Received \"\n+                        f\"input_shape={input_shape}, \"\n+                        \"input_tensor.shape=\"\n+                        f\"{input_tensor.shape}\"\n+                    )\n+            else:\n+                if input_tensor.shape[2] != input_shape[1]:\n+                    raise ValueError(\n+                        \"input_shape[1] must equal \"\n+                        \"input_tensor.shape[2].  Received \"\n+                        f\"input_shape={input_shape}, \"\n+                        \"input_tensor.shape=\"\n+                        f\"{input_tensor.shape}\"\n+                    )\n+        else:\n+            raise ValueError(\n+                \"input_tensor specified: \",\n+                input_tensor,\n+                \"is not a keras tensor\",\n+            )\n+\n+    # If input_shape is None, infer shape from input_tensor\n+    if input_shape is None and input_tensor is not None:\n+        try:\n+            backend.is_keras_tensor(input_tensor)\n+        except ValueError:\n+            raise ValueError(\n+                \"input_tensor: \",\n+                input_tensor,\n+                \"is type: \",\n+                type(input_tensor),\n+                \"which is not a valid type\",\n+            )\n+\n+        if backend.is_keras_tensor(input_tensor):\n+            if backend.image_data_format() == \"channels_first\":\n+                rows = input_tensor.shape[2]\n+                cols = input_tensor.shape[3]\n+                input_shape = (3, cols, rows)\n+            else:\n+                rows = input_tensor.shape[1]\n+                cols = input_tensor.shape[2]\n+                input_shape = (cols, rows, 3)\n+    # If input_shape is None and input_tensor is None using standard shape\n+    if input_shape is None and input_tensor is None:\n+        input_shape = (None, None, 3)\n+\n+    if backend.image_data_format() == \"channels_last\":\n+        row_axis, col_axis = (0, 1)\n+    else:\n+        row_axis, col_axis = (1, 2)\n+    rows = input_shape[row_axis]\n+    cols = input_shape[col_axis]\n+    if rows and cols and (rows < 32 or cols < 32):\n+        raise ValueError(\n+            \"Input size must be at least 32x32; Received `input_shape=\"\n+            f\"{input_shape}`\"\n+        )\n+    if weights == \"imagenet\":\n+        if (\n+            not minimalistic\n+            and alpha not in [0.75, 1.0]\n+            or minimalistic\n+            and alpha != 1.0\n+        ):\n+            raise ValueError(\n+                \"If imagenet weights are being loaded, \"\n+                \"alpha can be one of `0.75`, `1.0` for non minimalistic \"\n+                \"or `1.0` for minimalistic only.\"\n+            )\n+\n+        if rows != cols or rows != 224:\n+            warnings.warn(\n+                \"`input_shape` is undefined or non-square, \"\n+                \"or `rows` is not 224. \"\n+                \"Weights for input shape (224, 224) will be \"\n+                \"loaded as the default.\",\n+                stacklevel=2,\n+            )\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n+\n+    if minimalistic:\n+        kernel = 3\n+        activation = relu\n+        se_ratio = None\n+    else:\n+        kernel = 5\n+        activation = hard_swish\n+        se_ratio = 0.25\n+\n+    x = img_input\n+    if include_preprocessing:\n+        x = layers.Rescaling(scale=1.0 / 127.5, offset=-1.0)(x)\n+    x = layers.Conv2D(\n+        16,\n+        kernel_size=3,\n+        strides=(2, 2),\n+        padding=\"same\",\n+        use_bias=False,\n+        name=\"Conv\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"Conv/BatchNorm\"\n+    )(x)\n+    x = activation(x)\n+\n+    x = stack_fn(x, kernel, activation, se_ratio)\n+\n+    last_conv_ch = _depth(x.shape[channel_axis] * 6)\n+\n+    # if the width multiplier is greater than 1 we\n+    # increase the number of output channels\n+    if alpha > 1.0:\n+        last_point_ch = _depth(last_point_ch * alpha)\n+    x = layers.Conv2D(\n+        last_conv_ch,\n+        kernel_size=1,\n+        padding=\"same\",\n+        use_bias=False,\n+        name=\"Conv_1\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"Conv_1/BatchNorm\"\n+    )(x)\n+    x = activation(x)\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D(keepdims=True)(x)\n+        x = layers.Conv2D(\n+            last_point_ch,\n+            kernel_size=1,\n+            padding=\"same\",\n+            use_bias=True,\n+            name=\"Conv_2\",\n+        )(x)\n+        x = activation(x)\n+\n+        if dropout_rate > 0:\n+            x = layers.Dropout(dropout_rate)(x)\n+        x = layers.Conv2D(\n+            classes, kernel_size=1, padding=\"same\", name=\"Logits\"\n+        )(x)\n+        x = layers.Flatten()(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Activation(\n+            activation=classifier_activation, name=\"Predictions\"\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D(name=\"max_pool\")(x)\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    # Create model.\n+    model = Functional(inputs, x, name=\"MobilenetV3\" + model_type)\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        model_name = \"{}{}_224_{}_float\".format(\n+            model_type, \"_minimalistic\" if minimalistic else \"\", str(alpha)\n+        )\n+        if include_top:\n+            file_name = \"weights_mobilenet_v3_\" + model_name + \".h5\"\n+            file_hash = WEIGHTS_HASHES[model_name][0]\n+        else:\n+            file_name = \"weights_mobilenet_v3_\" + model_name + \"_no_top_v2.h5\"\n+            file_hash = WEIGHTS_HASHES[model_name][1]\n+        weights_path = file_utils.get_file(\n+            file_name,\n+            BASE_WEIGHT_PATH + file_name,\n+            cache_subdir=\"models\",\n+            file_hash=file_hash,\n+        )\n+        model.load_weights(weights_path)\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    return model\n+\n+\n+@keras_core_export(\"keras_core.applications.MobileNetV3Small\")\n+def MobileNetV3Small(\n+    input_shape=None,\n+    alpha=1.0,\n+    minimalistic=False,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    classes=1000,\n+    pooling=None,\n+    dropout_rate=0.2,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    def stack_fn(x, kernel, activation, se_ratio):\n+        def depth(d):\n+            return _depth(d * alpha)\n+\n+        x = _inverted_res_block(x, 1, depth(16), 3, 2, se_ratio, relu, 0)\n+        x = _inverted_res_block(x, 72.0 / 16, depth(24), 3, 2, None, relu, 1)\n+        x = _inverted_res_block(x, 88.0 / 24, depth(24), 3, 1, None, relu, 2)\n+        x = _inverted_res_block(\n+            x, 4, depth(40), kernel, 2, se_ratio, activation, 3\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(40), kernel, 1, se_ratio, activation, 4\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(40), kernel, 1, se_ratio, activation, 5\n+        )\n+        x = _inverted_res_block(\n+            x, 3, depth(48), kernel, 1, se_ratio, activation, 6\n+        )\n+        x = _inverted_res_block(\n+            x, 3, depth(48), kernel, 1, se_ratio, activation, 7\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(96), kernel, 2, se_ratio, activation, 8\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(96), kernel, 1, se_ratio, activation, 9\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(96), kernel, 1, se_ratio, activation, 10\n+        )\n+        return x\n+\n+    return MobileNetV3(\n+        stack_fn,\n+        1024,\n+        input_shape,\n+        alpha,\n+        \"small\",\n+        minimalistic,\n+        include_top,\n+        weights,\n+        input_tensor,\n+        classes,\n+        pooling,\n+        dropout_rate,\n+        classifier_activation,\n+        include_preprocessing,\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.MobileNetV3Large\")\n+def MobileNetV3Large(\n+    input_shape=None,\n+    alpha=1.0,\n+    minimalistic=False,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    classes=1000,\n+    pooling=None,\n+    dropout_rate=0.2,\n+    classifier_activation=\"softmax\",\n+    include_preprocessing=True,\n+):\n+    def stack_fn(x, kernel, activation, se_ratio):\n+        def depth(d):\n+            return _depth(d * alpha)\n+\n+        x = _inverted_res_block(x, 1, depth(16), 3, 1, None, relu, 0)\n+        x = _inverted_res_block(x, 4, depth(24), 3, 2, None, relu, 1)\n+        x = _inverted_res_block(x, 3, depth(24), 3, 1, None, relu, 2)\n+        x = _inverted_res_block(x, 3, depth(40), kernel, 2, se_ratio, relu, 3)\n+        x = _inverted_res_block(x, 3, depth(40), kernel, 1, se_ratio, relu, 4)\n+        x = _inverted_res_block(x, 3, depth(40), kernel, 1, se_ratio, relu, 5)\n+        x = _inverted_res_block(x, 6, depth(80), 3, 2, None, activation, 6)\n+        x = _inverted_res_block(x, 2.5, depth(80), 3, 1, None, activation, 7)\n+        x = _inverted_res_block(x, 2.3, depth(80), 3, 1, None, activation, 8)\n+        x = _inverted_res_block(x, 2.3, depth(80), 3, 1, None, activation, 9)\n+        x = _inverted_res_block(\n+            x, 6, depth(112), 3, 1, se_ratio, activation, 10\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(112), 3, 1, se_ratio, activation, 11\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(160), kernel, 2, se_ratio, activation, 12\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(160), kernel, 1, se_ratio, activation, 13\n+        )\n+        x = _inverted_res_block(\n+            x, 6, depth(160), kernel, 1, se_ratio, activation, 14\n+        )\n+        return x\n+\n+    return MobileNetV3(\n+        stack_fn,\n+        1280,\n+        input_shape,\n+        alpha,\n+        \"large\",\n+        minimalistic,\n+        include_top,\n+        weights,\n+        input_tensor,\n+        classes,\n+        pooling,\n+        dropout_rate,\n+        classifier_activation,\n+        include_preprocessing,\n+    )\n+\n+\n+MobileNetV3Small.__doc__ = BASE_DOCSTRING.format(name=\"MobileNetV3Small\")\n+MobileNetV3Large.__doc__ = BASE_DOCSTRING.format(name=\"MobileNetV3Large\")\n+\n+\n+def relu(x):\n+    return layers.ReLU()(x)\n+\n+\n+def hard_sigmoid(x):\n+    return layers.ReLU(6.0)(x + 3.0) * (1.0 / 6.0)\n+\n+\n+def hard_swish(x):\n+    return layers.Multiply()([x, hard_sigmoid(x)])\n+\n+\n+# This function is taken from the original tf repo.\n+# It ensures that all layers have a channel number that is divisible by 8\n+# It can be seen here:\n+# https://github.com/tensorflow/models/blob/master/research/\n+# slim/nets/mobilenet/mobilenet.py\n+\n+\n+def _depth(v, divisor=8, min_value=None):\n+    if min_value is None:\n+        min_value = divisor\n+    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n+    # Make sure that round down does not go down by more than 10%.\n+    if new_v < 0.9 * v:\n+        new_v += divisor\n+    return new_v\n+\n+\n+def _se_block(inputs, filters, se_ratio, prefix):\n+    x = layers.GlobalAveragePooling2D(\n+        keepdims=True, name=prefix + \"squeeze_excite/AvgPool\"\n+    )(inputs)\n+    x = layers.Conv2D(\n+        _depth(filters * se_ratio),\n+        kernel_size=1,\n+        padding=\"same\",\n+        name=prefix + \"squeeze_excite/Conv\",\n+    )(x)\n+    x = layers.ReLU(name=prefix + \"squeeze_excite/Relu\")(x)\n+    x = layers.Conv2D(\n+        filters,\n+        kernel_size=1,\n+        padding=\"same\",\n+        name=prefix + \"squeeze_excite/Conv_1\",\n+    )(x)\n+    x = hard_sigmoid(x)\n+    x = layers.Multiply(name=prefix + \"squeeze_excite/Mul\")([inputs, x])\n+    return x\n+\n+\n+def _inverted_res_block(\n+    x, expansion, filters, kernel_size, stride, se_ratio, activation, block_id\n+):\n+    channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n+    shortcut = x\n+    prefix = \"expanded_conv/\"\n+    infilters = x.shape[channel_axis]\n+    if block_id:\n+        # Expand\n+        prefix = f\"expanded_conv_{block_id}/\"\n+        x = layers.Conv2D(\n+            _depth(infilters * expansion),\n+            kernel_size=1,\n+            padding=\"same\",\n+            use_bias=False,\n+            name=prefix + \"expand\",\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=channel_axis,\n+            epsilon=1e-3,\n+            momentum=0.999,\n+            name=prefix + \"expand/BatchNorm\",\n+        )(x)\n+        x = activation(x)\n+\n+    if stride == 2:\n+        x = layers.ZeroPadding2D(\n+            padding=imagenet_utils.correct_pad(x, kernel_size),\n+            name=prefix + \"depthwise/pad\",\n+        )(x)\n+    x = layers.DepthwiseConv2D(\n+        kernel_size,\n+        strides=stride,\n+        padding=\"same\" if stride == 1 else \"valid\",\n+        use_bias=False,\n+        name=prefix + \"depthwise\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis,\n+        epsilon=1e-3,\n+        momentum=0.999,\n+        name=prefix + \"depthwise/BatchNorm\",\n+    )(x)\n+    x = activation(x)\n+\n+    if se_ratio:\n+        x = _se_block(x, _depth(infilters * expansion), se_ratio, prefix)\n+\n+    x = layers.Conv2D(\n+        filters,\n+        kernel_size=1,\n+        padding=\"same\",\n+        use_bias=False,\n+        name=prefix + \"project\",\n+    )(x)\n+    x = layers.BatchNormalization(\n+        axis=channel_axis,\n+        epsilon=1e-3,\n+        momentum=0.999,\n+        name=prefix + \"project/BatchNorm\",\n+    )(x)\n+\n+    if stride == 1 and infilters == filters:\n+        x = layers.Add(name=prefix + \"Add\")([shortcut, x])\n+    return x\n+\n+\n+@keras_core_export(\"keras_core.applications.mobilenet_v3.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    \"\"\"A placeholder method for backward compatibility.\n+\n+    The preprocessing logic has been included in the mobilenet_v3 model\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n+\n+    Args:\n+        x: A floating point `numpy.array` or a tensor.\n+        data_format: Optional data format of the image tensor/array.\n+            `None` means the global setting\n+            `keras_core.config.image_data_format()` is used\n+            (unless you changed it, it uses `\"channels_last\"`).\n+            Defaults to `None`.\n+\n+    Returns:\n+        Unchanged `numpy.array` or tensor.\n+    \"\"\"\n+    return x\n+\n+\n+@keras_core_export(\"keras_core.applications.mobilenet_v3.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -88,8 +88,8 @@ def VGG16(\n         classifier_activation: A `str` or callable. The activation function to\n             use on the \"top\" layer. Ignored unless `include_top=True`. Set\n             `classifier_activation=None` to return the logits of the \"top\"\n-            layer.  When loading pretrained weights, `classifier_activation` can\n-            only be `None` or `\"softmax\"`.\n+            layer.  When loading pretrained weights, `classifier_activation`\n+            can only be `None` or `\"softmax\"`.\n \n     Returns:\n         A model instance.\n@@ -97,7 +97,7 @@ def VGG16(\n     if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n-            \"`None` (random initialization), `imagenet` \"\n+            \"`None` (random initialization), 'imagenet' \"\n             \"(pre-training on ImageNet), \"\n             \"or the path to the weights file to be loaded.  Received: \"\n             f\"weights={weights}\"\n@@ -105,10 +105,11 @@ def VGG16(\n \n     if weights == \"imagenet\" and include_top and classes != 1000:\n         raise ValueError(\n-            'If using `weights` as `\"imagenet\"` with `include_top` '\n-            \"as true, `classes` should be 1000.  \"\n-            f\"Received `classes={classes}`\"\n+            \"If using `weights='imagenet'` with `include_top=True`, \"\n+            \"`classes` should be 1000.  \"\n+            f\"Received classes={classes}\"\n         )\n+\n     # Determine proper input shape\n     input_shape = imagenet_utils.obtain_input_shape(\n         input_shape,\n\n@@ -97,7 +97,7 @@ def VGG19(\n     if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n-            \"`None` (random initialization), `imagenet` \"\n+            \"`None` (random initialization), 'imagenet' \"\n             \"(pre-training on ImageNet), \"\n             \"or the path to the weights file to be loaded.  Received: \"\n             f\"weights={weights}\"\n@@ -105,10 +105,11 @@ def VGG19(\n \n     if weights == \"imagenet\" and include_top and classes != 1000:\n         raise ValueError(\n-            'If using `weights` as `\"imagenet\"` with `include_top` '\n-            \"as true, `classes` should be 1000.  \"\n-            f\"Received `classes={classes}`\"\n+            \"If using `weights='imagenet'` with `include_top=True`, \"\n+            \"`classes` should be 1000.  \"\n+            f\"Received classes={classes}\"\n         )\n+\n     # Determine proper input shape\n     input_shape = imagenet_utils.obtain_input_shape(\n         input_shape,\n\n@@ -95,15 +95,16 @@ def Xception(\n     if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n-            \"`None` (random initialization), `imagenet` \"\n+            \"`None` (random initialization), 'imagenet' \"\n             \"(pre-training on ImageNet), \"\n             \"or the path to the weights file to be loaded.\"\n         )\n \n     if weights == \"imagenet\" and include_top and classes != 1000:\n         raise ValueError(\n-            'If using `weights` as `\"imagenet\"` with `include_top`'\n-            \" as true, `classes` should be 1000\"\n+            \"If using `weights='imagenet'` with `include_top=True`, \"\n+            \"`classes` should be 1000.  \"\n+            f\"Received classes={classes}\"\n         )\n \n     # Determine proper input shape\n\n@@ -1,4 +1,9 @@\n from keras_core.layers.activations.activation import Activation\n+from keras_core.layers.activations.elu import ELU\n+from keras_core.layers.activations.leaky_relu import LeakyReLU\n+from keras_core.layers.activations.prelu import PReLU\n+from keras_core.layers.activations.relu import ReLU\n+from keras_core.layers.activations.softmax import Softmax\n from keras_core.layers.attention.additive_attention import AdditiveAttention\n from keras_core.layers.attention.attention import Attention\n from keras_core.layers.attention.multi_head_attention import MultiHeadAttention\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
