{"custom_id": "scrapy#de7789e52df5d8dc8595570d36a60ae92b4fcb50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 22 | Churn Cumulative: 297 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -143,11 +143,11 @@ class XmlItemExporter(BaseItemExporter):\n \n     def _beautify_newline(self, new_item=False):\n         if self.indent is not None and (self.indent > 0 or new_item):\n-            self._xg_characters('\\n')\n+            self.xg.characters('\\n')\n \n     def _beautify_indent(self, depth=1):\n         if self.indent:\n-            self._xg_characters(' ' * self.indent * depth)\n+            self.xg.characters(' ' * self.indent * depth)\n \n     def start_exporting(self):\n         self.xg.startDocument()\n@@ -182,26 +182,12 @@ class XmlItemExporter(BaseItemExporter):\n                 self._export_xml_field('value', value, depth=depth+1)\n             self._beautify_indent(depth=depth)\n         elif isinstance(serialized_value, six.text_type):\n-            self._xg_characters(serialized_value)\n+            self.xg.characters(serialized_value)\n         else:\n-            self._xg_characters(str(serialized_value))\n+            self.xg.characters(str(serialized_value))\n         self.xg.endElement(name)\n         self._beautify_newline()\n \n-    # Workaround for https://bugs.python.org/issue17606\n-    # Before Python 2.7.4 xml.sax.saxutils required bytes;\n-    # since 2.7.4 it requires unicode. The bug is likely to be\n-    # fixed in 2.7.6, but 2.7.6 will still support unicode,\n-    # and Python 3.x will require unicode, so \">= 2.7.4\" should be fine.\n-    if sys.version_info[:3] >= (2, 7, 4):\n-        def _xg_characters(self, serialized_value):\n-            if not isinstance(serialized_value, six.text_type):\n-                serialized_value = serialized_value.decode(self.encoding)\n-            return self.xg.characters(serialized_value)\n-    else:  # pragma: no cover\n-        def _xg_characters(self, serialized_value):\n-            return self.xg.characters(serialized_value)\n-\n \n class CsvItemExporter(BaseItemExporter):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2898fdcf91cbbfa9d801196924960d3a3764b93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 174 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,7 @@ from scrapy.utils.decorators import deprecated\n #   (regression or bug-fix compared to Python 3.4)\n # - read1(), which fetches data before raising EOFError on next call\n #   works here but is only available from Python>=3.3\n+@deprecated('GzipFile.read1')\n def read1(gzf, size=-1):\n     return gzf.read1(size)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cea2f5e244f30591bae667abf2b2689e566ab343", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 40 | Files Changed: 8 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): 12/13 | Churn Δ: 49 | Churn Cumulative: 2122 | Contributors (this commit): 41 | Commits (past 90d): 26 | Contributors (cumulative): 79 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ and extract the potentially compressed responses that may arrive.\n \n import bz2\n import gzip\n+from io import BytesIO\n import zipfile\n import tarfile\n import logging\n@@ -11,11 +12,6 @@ from tempfile import mktemp\n \n import six\n \n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n-\n from scrapy.responsetypes import responsetypes\n \n logger = logging.getLogger(__name__)\n\n@@ -3,12 +3,8 @@ Mail sending helpers\n \n See documentation in docs/topics/email.rst\n \"\"\"\n-import logging\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n from io import BytesIO\n+import logging\n \n from email.utils import COMMASPACE, formatdate\n from six.moves.email_mime_multipart import MIMEMultipart\n\n@@ -5,6 +5,7 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n+from io import BytesIO\n import mimetypes\n import os\n import os.path\n@@ -15,12 +16,6 @@ from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n \n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n-\n from twisted.internet import defer, threads\n \n from scrapy.pipelines.media import MediaPipeline\n\n@@ -5,12 +5,8 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n-import six\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n from io import BytesIO\n+import six\n \n from PIL import Image\n \n\n@@ -1,12 +1,7 @@\n-import struct\n-\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n from gzip import GzipFile\n-\n+from io import BytesIO\n import re\n+import struct\n \n from scrapy.utils.decorators import deprecated\n \n\n@@ -1,11 +1,7 @@\n import re\n import csv\n-import logging\n-try:\n-    from cStringIO import StringIO as BytesIO\n-except ImportError:\n-    from io import BytesIO\n from io import StringIO\n+import logging\n import six\n \n from scrapy.http import TextResponse, Response\n\n@@ -1,3 +1,4 @@\n+from io import StringIO\n import json\n import os\n import pstats\n@@ -7,10 +8,6 @@ from subprocess import Popen, PIPE\n import sys\n import tempfile\n import unittest\n-try:\n-    from cStringIO import StringIO\n-except ImportError:\n-    from io import StringIO\n \n from scrapy.utils.test import get_testenv\n \n\n@@ -144,9 +144,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n         # The Failure should encapsulate a FileException ...\n         self.assertEqual(failure.value, file_exc)\n-        # ... and if we're running on Python 3 ...\n-        if sys.version_info.major >= 3:\n-            # ... it should have the returnValue exception set as its context\n+        # ... and it should have the returnValue exception set as its context\n         self.assertEqual(failure.value.__context__, def_gen_return_exc)\n \n         # Let's calculate the request fingerprint and fake some runtime data...\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b70b051a6cec699c3cb14c3ac9cf530671a8487", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 10 | Churn Cumulative: 1048 | Contributors (this commit): 23 | Commits (past 90d): 8 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ class Response(object_ref):\n     @property\n     def text(self):\n         \"\"\"For subclasses of TextResponse, this will return the body\n-        as text (unicode object in Python 2 and str in Python 3)\n+        as str\n         \"\"\"\n         raise AttributeError(\"Response content isn't text\")\n \n\n@@ -90,7 +90,7 @@ def to_unicode(text, encoding=None, errors='strict'):\n     if isinstance(text, six.text_type):\n         return text\n     if not isinstance(text, (bytes, six.text_type)):\n-        raise TypeError('to_unicode must receive a bytes, str or unicode '\n+        raise TypeError('to_unicode must receive a bytes or str '\n                         'object, got %s' % type(text).__name__)\n     if encoding is None:\n         encoding = 'utf-8'\n@@ -103,16 +103,16 @@ def to_bytes(text, encoding=None, errors='strict'):\n     if isinstance(text, bytes):\n         return text\n     if not isinstance(text, six.string_types):\n-        raise TypeError('to_bytes must receive a unicode, str or bytes '\n+        raise TypeError('to_bytes must receive a str or bytes '\n                         'object, got %s' % type(text).__name__)\n     if encoding is None:\n         encoding = 'utf-8'\n     return text.encode(encoding, errors)\n \n \n+@deprecated('to_unicode')\n def to_native_str(text, encoding=None, errors='strict'):\n-    \"\"\" Return str representation of ``text``\n-    (bytes in Python 2.x and unicode in Python 3.x). \"\"\"\n+    \"\"\" Return str representation of ``text``. \"\"\"\n     return to_unicode(text, encoding, errors)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ac4b430ae0165f25d8cd2fb1ac25ac9f307df8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1201 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import os\n-import six\n import shutil\n import tempfile\n import contextlib\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75b1d051d99cda17558a210326e77046eec851a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 44 | Files Changed: 18 | Hunks: 35 | Methods Changed: 0 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 61 | Churn Cumulative: 7665 | Contributors (this commit): 58 | Commits (past 90d): 66 | Contributors (cumulative): 159 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,6 @@\n import base64\n from six.moves.urllib.parse import unquote, urlunparse\n from six.moves.urllib.request import getproxies, proxy_bypass\n-try:\n-    from urllib2 import _parse_proxy\n-except ImportError:\n from urllib.request import _parse_proxy\n \n from scrapy.exceptions import NotConfigured\n\n@@ -3,10 +3,7 @@ This module provides some commonly used processors for Item Loaders.\n \n See documentation in docs/topics/loaders.rst\n \"\"\"\n-try:\n from collections import ChainMap\n-except ImportError:\n-    from scrapy.utils.datatypes import MergeDict as ChainMap\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.loader.common import wrap_loader_context\n\n@@ -21,11 +21,6 @@ if 'COV_CORE_CONFIG' in os.environ:\n     os.environ['COV_CORE_CONFIG'] = os.path.join(_sourceroot,\n                                                  os.environ['COV_CORE_CONFIG'])\n \n-try:\n-    import unittest.mock as mock\n-except ImportError:\n-    import mock\n-\n tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n                              'sample_data')\n \n\n@@ -2,11 +2,8 @@ import os\n import six\n import shutil\n import tempfile\n-import contextlib\n-try:\n from unittest import mock\n-except ImportError:\n-    import mock\n+import contextlib\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n\n@@ -1,3 +1,5 @@\n+from unittest import mock\n+\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n@@ -7,7 +9,6 @@ from scrapy.exceptions import _InvalidOutput\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.utils.test import get_crawler\n from scrapy.utils.python import to_bytes\n-from tests import mock\n \n \n class ManagerTestCase(TestCase):\n\n@@ -1,5 +1,8 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+\n+from unittest import mock\n+\n from twisted.internet import reactor, error\n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n from twisted.python import failure\n@@ -9,7 +12,6 @@ from scrapy.downloadermiddlewares.robotstxt import (RobotsTxtMiddleware,\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.settings import Settings\n-from tests import mock\n from tests.test_robotstxt_interface import rerp_available, reppy_available\n \n \n\n@@ -1,8 +1,3 @@\n-try:\n-    import unittest.mock as mock\n-except ImportError:\n-    import mock\n-\n from twisted.trial import unittest\n from twisted.conch.telnet import ITelnetProtocol\n from twisted.cred import credentials\n\n@@ -7,6 +7,7 @@ from io import BytesIO\n import tempfile\n import shutil\n import string\n+from unittest import mock\n from six.moves.urllib.parse import urljoin, urlparse, quote\n from six.moves.urllib.request import pathname2url\n \n@@ -15,7 +16,6 @@ from twisted.trial import unittest\n from twisted.internet import defer\n from scrapy.crawler import CrawlerRunner\n from scrapy.settings import Settings\n-from tests import mock\n from tests.mockserver import MockServer\n from w3lib.url import path_to_file_uri\n \n\n@@ -3,6 +3,7 @@ import cgi\n import unittest\n import re\n import json\n+from unittest import mock\n from urllib.parse import unquote_to_bytes\n import warnings\n \n@@ -13,8 +14,6 @@ from six.moves.urllib.parse import urlparse, parse_qs, unquote\n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_native_str\n \n-from tests import mock\n-\n \n class RequestTest(unittest.TestCase):\n \n\n@@ -1,12 +1,12 @@\n import sys\n import unittest\n+from unittest import mock\n from warnings import catch_warnings\n \n import six\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n-from tests import mock\n \n \n PY36_PLUS = (sys.version_info.major >= 3) and (sys.version_info.minor >= 6)\n\n@@ -1,10 +1,9 @@\n import os\n import random\n import time\n-import hashlib\n-import warnings\n from tempfile import mkdtemp\n from shutil import rmtree\n+from unittest import mock\n from six.moves.urllib.parse import urlparse\n from six import BytesIO\n \n@@ -15,13 +14,10 @@ from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GC\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n from scrapy.utils.test import assert_gcs_environ, get_gcs_content_and_delete\n from scrapy.utils.boto import is_botocore\n \n-from tests import mock\n-\n \n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n\n@@ -1,10 +1,9 @@\n import six\n import unittest\n-import warnings\n+from unittest import mock\n \n from scrapy.settings import (BaseSettings, Settings, SettingsAttribute,\n                              SETTINGS_PRIORITIES, get_settings_priority)\n-from tests import mock\n from . import default_settings\n \n \n\n@@ -1,5 +1,6 @@\n import gzip\n import inspect\n+from unittest import mock\n import warnings\n from io import BytesIO\n \n@@ -17,8 +18,6 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.test import get_crawler\n \n-from tests import mock\n-\n \n class SpiderTest(unittest.TestCase):\n \n\n@@ -1,3 +1,5 @@\n+from unittest import mock\n+\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n@@ -6,7 +8,6 @@ from scrapy.http import Request, Response\n from scrapy.exceptions import _InvalidOutput\n from scrapy.utils.test import get_crawler\n from scrapy.core.spidermw import SpiderMiddlewareManager\n-from tests import mock\n \n \n class SpiderMiddlewareTestCase(TestCase):\n\n@@ -1,10 +1,6 @@\n from datetime import datetime\n import unittest\n-\n-try:\n from unittest import mock\n-except ImportError:\n-    import mock\n \n from scrapy.extensions.corestats import CoreStats\n from scrapy.spiders import Spider\n\n@@ -2,12 +2,11 @@\n from __future__ import absolute_import\n import inspect\n import unittest\n+from unittest import mock\n import warnings\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n \n-from tests import mock\n-\n \n class MyWarning(UserWarning):\n     pass\n\n@@ -1,11 +1,11 @@\n import sys\n import os\n import unittest\n+from unittest import mock\n \n from scrapy.item import Item, Field\n from scrapy.utils.misc import arg_to_iter, create_instance, load_object, set_environ, walk_modules\n \n-from tests import mock\n \n __doctests__ = ['scrapy.utils.misc']\n \n\n@@ -1,7 +1,7 @@\n import six\n import unittest\n+from unittest import mock\n from scrapy.utils import trackref\n-from tests import mock\n \n \n class Foo(trackref.object_ref):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#397e8835564614608647c855c62c36232020f078", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 58 | Files Changed: 20 | Hunks: 51 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 9347 | Contributors (this commit): 77 | Commits (past 90d): 90 | Contributors (cumulative): 185 | DMM Complexity: None\n\nDIFF:\n@@ -174,7 +174,7 @@ def tunnel_request_data(host, port, proxy_auth_header=None):\n     r\"\"\"\n     Return binary content of a CONNECT request.\n \n-    >>> from scrapy.utils.python import to_native_str as s\n+    >>> from scrapy.utils.python import to_unicode as s\n     >>> s(tunnel_request_data(\"example.com\", 8080))\n     'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n     >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n\n@@ -6,7 +6,7 @@ from collections import defaultdict\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n \n logger = logging.getLogger(__name__)\n \n@@ -53,7 +53,7 @@ class CookiesMiddleware(object):\n \n     def _debug_cookie(self, request, spider):\n         if self.debug:\n-            cl = [to_native_str(c, errors='replace')\n+            cl = [to_unicode(c, errors='replace')\n                   for c in request.headers.getlist('Cookie')]\n             if cl:\n                 cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n@@ -62,7 +62,7 @@ class CookiesMiddleware(object):\n \n     def _debug_set_cookie(self, response, spider):\n         if self.debug:\n-            cl = [to_native_str(c, errors='replace')\n+            cl = [to_unicode(c, errors='replace')\n                   for c in response.headers.getlist('Set-Cookie')]\n             if cl:\n                 cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n\n@@ -5,15 +5,12 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \"\"\"\n \n import logging\n-import sys\n-import re\n \n from twisted.internet.defer import Deferred, maybeDeferred\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_native_str\n from scrapy.utils.misc import load_object\n \n logger = logging.getLogger(__name__)\n\n@@ -12,7 +12,7 @@ from six.moves import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n \n from scrapy.utils.serialize import ScrapyJSONEncoder\n-from scrapy.utils.python import to_bytes, to_unicode, to_native_str, is_listlike\n+from scrapy.utils.python import to_bytes, to_unicode, is_listlike\n from scrapy.item import BaseItem\n from scrapy.exceptions import ScrapyDeprecationWarning\n import warnings\n@@ -232,7 +232,7 @@ class CsvItemExporter(BaseItemExporter):\n     def _build_row(self, values):\n         for s in values:\n             try:\n-                yield to_native_str(s, self.encoding)\n+                yield to_unicode(s, self.encoding)\n             except TypeError:\n                 yield s\n \n\n@@ -3,7 +3,7 @@ from six.moves.http_cookiejar import (\n     CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n )\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n \n \n class CookieJar(object):\n@@ -165,13 +165,13 @@ class WrappedRequest(object):\n         return name in self.request.headers\n \n     def get_header(self, name, default=None):\n-        return to_native_str(self.request.headers.get(name, default),\n+        return to_unicode(self.request.headers.get(name, default),\n                              errors='replace')\n \n     def header_items(self):\n         return [\n-            (to_native_str(k, errors='replace'),\n-             [to_native_str(x, errors='replace') for x in v])\n+            (to_unicode(k, errors='replace'),\n+             [to_unicode(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]\n \n@@ -189,7 +189,7 @@ class WrappedResponse(object):\n \n     # python3 cookiejars calls get_all\n     def get_all(self, name, default=None):\n-        return [to_native_str(v, errors='replace')\n+        return [to_unicode(v, errors='replace')\n                 for v in self.response.headers.getlist(name)]\n     # python2 cookiejars calls getheaders\n     getheaders = get_all\n\n@@ -16,7 +16,7 @@ from w3lib.html import strip_html5_whitespace\n from scrapy.http.request import Request\n from scrapy.http.response import Response\n from scrapy.utils.response import get_base_url\n-from scrapy.utils.python import memoizemethod_noargs, to_native_str\n+from scrapy.utils.python import memoizemethod_noargs, to_unicode\n \n \n class TextResponse(Response):\n@@ -32,7 +32,7 @@ class TextResponse(Response):\n \n     def _set_url(self, url):\n         if isinstance(url, six.text_type):\n-            self._url = to_native_str(url, self.encoding)\n+            self._url = to_unicode(url, self.encoding)\n         else:\n             super(TextResponse, self)._set_url(url)\n \n@@ -81,11 +81,11 @@ class TextResponse(Response):\n     @memoizemethod_noargs\n     def _headers_encoding(self):\n         content_type = self.headers.get(b'Content-Type', b'')\n-        return http_content_type_encoding(to_native_str(content_type))\n+        return http_content_type_encoding(to_unicode(content_type))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n-            content_type = to_native_str(self.headers.get(b'Content-Type', b''))\n+            content_type = to_unicode(self.headers.get(b'Content-Type', b''))\n             benc, ubody = html_to_unicode(content_type, self.body,\n                     auto_detect_fun=self._auto_detect_fun,\n                     default_encoding=self._DEFAULT_ENCODING)\n\n@@ -10,7 +10,7 @@ from w3lib.url import canonicalize_url\n \n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n-from scrapy.utils.python import unique as unique_list, to_native_str\n+from scrapy.utils.python import unique as unique_list, to_unicode\n from scrapy.utils.response import get_base_url\n from scrapy.linkextractors import FilteringLinkExtractor\n \n@@ -67,7 +67,7 @@ class LxmlParserLinkExtractor(object):\n                 url = self.process_attr(attr_val)\n                 if url is None:\n                     continue\n-            url = to_native_str(url, encoding=response_encoding)\n+            url = to_unicode(url, encoding=response_encoding)\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n             link = Link(url, _collect_string_content(el) or u'',\n\n@@ -10,7 +10,7 @@ import six\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n-from scrapy.utils.python import binary_is_text, to_bytes, to_native_str\n+from scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n \n \n class ResponseTypes(object):\n@@ -55,12 +55,12 @@ class ResponseTypes(object):\n         header \"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n+        mimetype = to_unicode(content_type).split(';')[0].strip().lower()\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n         try:\n-            filename = to_native_str(content_disposition,\n+            filename = to_unicode(content_disposition,\n                 encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n\n@@ -3,14 +3,14 @@ import logging\n from abc import ABCMeta, abstractmethod\n from six import with_metaclass\n \n-from scrapy.utils.python import to_native_str, to_unicode\n+from scrapy.utils.python import to_unicode\n \n logger = logging.getLogger(__name__)\n \n def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n     try:\n         if to_native_str_type:\n-            robotstxt_body = to_native_str(robotstxt_body)\n+            robotstxt_body = to_unicode(robotstxt_body)\n         else:\n             robotstxt_body = robotstxt_body.decode('utf-8')\n     except UnicodeDecodeError:\n@@ -66,8 +66,8 @@ class PythonRobotParser(RobotParser):\n         return o\n \n     def allowed(self, url, user_agent):\n-        user_agent = to_native_str(user_agent)\n-        url = to_native_str(url)\n+        user_agent = to_unicode(user_agent)\n+        url = to_unicode(url)\n         return self.rp.can_fetch(user_agent, url)\n \n \n\n@@ -10,8 +10,7 @@ from w3lib.url import safe_url_string\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n-from scrapy.utils.python import to_native_str\n-from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n from scrapy.utils.url import strip_url\n \n@@ -322,7 +321,7 @@ class RefererMiddleware(object):\n             if isinstance(resp_or_url, Response):\n                 policy_header = resp_or_url.headers.get('Referrer-Policy')\n                 if policy_header is not None:\n-                    policy_name = to_native_str(policy_header.decode('latin1'))\n+                    policy_name = to_unicode(policy_header.decode('latin1'))\n         if policy_name is None:\n             return self.default_policy()\n \n\n@@ -4,7 +4,7 @@ Helper functions for serializing (and deserializing) requests.\n import six\n \n from scrapy.http import Request\n-from scrapy.utils.python import to_unicode, to_native_str\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n \n \n@@ -54,7 +54,7 @@ def request_from_dict(d, spider=None):\n         eb = _get_method(spider, eb)\n     request_cls = load_object(d['_class']) if '_class' in d else Request\n     return request_cls(\n-        url=to_native_str(d['url']),\n+        url=to_unicode(d['url']),\n         callback=cb,\n         errback=eb,\n         method=d['method'],\n\n@@ -9,7 +9,7 @@ import weakref\n from six.moves.urllib.parse import urlunparse\n \n from w3lib.http import basic_auth_header\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes, to_unicode\n \n from w3lib.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n@@ -97,4 +97,4 @@ def referer_str(request):\n     referrer = request.headers.get('Referer')\n     if referrer is None:\n         return referrer\n-    return to_native_str(referrer, errors='replace')\n+    return to_unicode(referrer, errors='replace')\n\n@@ -8,7 +8,7 @@ import webbrowser\n import tempfile\n \n from twisted.web import http\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes, to_unicode\n from w3lib import html\n \n \n@@ -36,7 +36,7 @@ def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n     \"\"\"\n     message = http.RESPONSES.get(int(status), \"Unknown Status\")\n-    return '%s %s' % (status, to_native_str(message))\n+    return '%s %s' % (status, to_unicode(message))\n \n \n def response_httprepr(response):\n\n@@ -3,7 +3,7 @@\n import OpenSSL\n import OpenSSL._util as pyOpenSSLutil\n \n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n \n \n # The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.\n@@ -12,7 +12,7 @@ SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, 'SSL_OP_NO_TLSv1_3', 0)\n \n \n def ffi_buf_to_string(buf):\n-    return to_native_str(pyOpenSSLutil.ffi.string(buf))\n+    return to_unicode(pyOpenSSLutil.ffi.string(buf))\n \n \n def x509name_to_string(x509name):\n\n@@ -1,17 +1,16 @@\n import os\n from os.path import join, abspath\n-from twisted.trial import unittest\n from twisted.internet import defer\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n from tests.test_commands import CommandTest\n \n \n def _textmode(bstr):\n     \"\"\"Normalize input the same as writing to a file\n     and reading from it in text mode\"\"\"\n-    return to_native_str(bstr).replace(os.linesep, '\\n')\n+    return to_unicode(bstr).replace(os.linesep, '\\n')\n \n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n     command = 'parse'\n\n@@ -10,13 +10,10 @@ from contextlib import contextmanager\n from threading import Timer\n \n from twisted.trial import unittest\n-from twisted.internet import defer\n \n import scrapy\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n-from scrapy.utils.testsite import SiteTest\n-from scrapy.utils.testproc import ProcessTest\n from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n@@ -56,7 +53,7 @@ class ProjectTest(unittest.TestCase):\n         finally:\n             timer.cancel()\n \n-        return p, to_native_str(stdout), to_native_str(stderr)\n+        return p, to_unicode(stdout), to_unicode(stderr)\n \n \n class StartprojectTest(ProjectTest):\n\n@@ -26,8 +26,7 @@ from scrapy.extensions.feedexport import (\n     S3FeedStorage, StdoutFeedStorage,\n     BlockingFeedStorage)\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n-from scrapy.utils.python import to_native_str\n-from scrapy.utils.project import get_project_settings\n+from scrapy.utils.python import to_unicode\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -456,7 +455,7 @@ class FeedExportTest(unittest.TestCase):\n         settings.update({'FEED_FORMAT': 'csv'})\n         data = yield self.exported_data(items, settings)\n \n-        reader = csv.DictReader(to_native_str(data).splitlines())\n+        reader = csv.DictReader(to_unicode(data).splitlines())\n         got_rows = list(reader)\n         if ordered:\n             self.assertEqual(reader.fieldnames, header)\n@@ -470,7 +469,7 @@ class FeedExportTest(unittest.TestCase):\n         settings = settings or {}\n         settings.update({'FEED_FORMAT': 'jl'})\n         data = yield self.exported_data(items, settings)\n-        parsed = [json.loads(to_native_str(line)) for line in data.splitlines()]\n+        parsed = [json.loads(to_unicode(line)) for line in data.splitlines()]\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         self.assertEqual(rows, parsed)\n \n\n@@ -7,12 +7,11 @@ from unittest import mock\n from urllib.parse import unquote_to_bytes\n import warnings\n \n-import six\n from six.moves import xmlrpc_client as xmlrpclib\n from six.moves.urllib.parse import urlparse, parse_qs, unquote\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes, to_unicode\n \n \n class RequestTest(unittest.TestCase):\n@@ -349,8 +348,8 @@ class FormRequestTest(RequestTest):\n     request_class = FormRequest\n \n     def assertQueryEqual(self, first, second, msg=None):\n-        first = to_native_str(first).split(\"&\")\n-        second = to_native_str(second).split(\"&\")\n+        first = to_unicode(first).split(\"&\")\n+        second = to_unicode(second).split(\"&\")\n         return self.assertEqual(sorted(first), sorted(second), msg)\n \n     def test_empty_formdata(self):\n\n@@ -7,7 +7,7 @@ from w3lib.encoding import resolve_encoding\n from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n                          XmlResponse, Headers)\n from scrapy.selector import Selector\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n from scrapy.exceptions import NotSupported\n from scrapy.link import Link\n from tests import get_testdata\n@@ -204,11 +204,11 @@ class TextResponseTest(BaseResponseTest):\n         assert isinstance(resp.url, str)\n \n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='utf-8')\n-        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\\xc2\\xa3'))\n+        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\\xc2\\xa3'))\n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='latin-1')\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n         resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]})\n-        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\\xc2\\xa3'))\n+        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\\xc2\\xa3'))\n         resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n \n\n@@ -1,6 +1,5 @@\n # coding=utf-8\n from twisted.trial import unittest\n-from scrapy.utils.python import to_native_str\n \n \n def reppy_available():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7299e91b1f1b231095e2a7ddbce5ad980ae7da8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 3 | Churn Cumulative: 157 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,8 +80,6 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_mixin_private_callback_serialization(self):\n-        if sys.version_info[0] < 3:\n-            return\n         r = Request(\"http://www.example.com\",\n                     callback=self.spider._TestSpiderMixin__mixin_callback,\n                     errback=self.spider.handle_error)\n@@ -119,7 +117,6 @@ class RequestSerializationTest(unittest.TestCase):\n     def test_private_name_mangling(self):\n         self._assert_mangles_to(\n             self.spider, '_TestSpider__parse_item_private')\n-        if sys.version_info[0] >= 3:\n         self._assert_mangles_to(\n             self.spider, '_TestSpiderMixin__mixin_callback')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f02c3d1dcf3e4880388d19e961e7911be5dc54ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 247 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -206,8 +206,7 @@ class MockServer():\n \n     def __exit__(self, exc_type, exc_value, traceback):\n         self.proc.kill()\n-        self.proc.wait()\n-        time.sleep(0.2)\n+        self.proc.communicate()\n \n     def url(self, path, is_secure=False):\n         host = self.http_address.replace('0.0.0.0', '127.0.0.1')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5eb59b92d3311f702933753d85b15e6d698faa1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 385 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class ProxyConnectTestCase(TestCase):\n         with LogCapture() as l:\n             yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200, l)\n-        echo = json.loads(crawler.spider.meta['responses'][0].body)\n+        echo = json.loads(crawler.spider.meta['responses'][0].body.decode('utf-8'))\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n     @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0c5c724969ebc35970daaf0203969dc2eb00d56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 387 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class ProxyConnectTestCase(TestCase):\n         with LogCapture() as l:\n             yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200, l)\n-        echo = json.loads(crawler.spider.meta['responses'][0].body.decode('utf-8'))\n+        echo = json.loads(crawler.spider.meta['responses'][0].text)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n     @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c9963ab049e49a1d490d9505ad22ae9c1415421", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 389 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n-    @pytest.mark.xfail(reason='Python 3 fails this earlier')\n+    @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info.minor >= 6)\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b0cdf7f3ed94e81612cceacfab6d81d54117865", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 392 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,8 +48,7 @@ sys.exit(mitmdump())\n \n     def stop(self):\n         self.proc.kill()\n-        self.proc.wait()\n-        time.sleep(0.2)\n+        self.proc.communicate()\n \n \n def _wrong_credentials(proxy_url):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#350aa67c3dc8997ef9d3aac9ef3f596c83758e1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 118 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ collect_ignore = [\n ]\n \n \n-for line in open('tests/py3-ignores.txt'):\n+for line in open('tests/ignores.txt'):\n     file_path = line.strip()\n     if file_path and file_path[0] != '#':\n         collect_ignore.append(file_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#415526d922b6c70bfd5872c8d67d69fcce0ee1fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 35 | Files Changed: 29 | Hunks: 29 | Methods Changed: 0 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 35 | Churn Cumulative: 6278 | Contributors (this commit): 58 | Commits (past 90d): 38 | Contributors (cumulative): 225 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n #!/usr/bin/env python\n-from __future__ import print_function\n from time import time\n from collections import deque\n from twisted.web.server import Site, NOT_DONE_YET\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import sys\n import os\n import optparse\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import time\n import sys\n from collections import defaultdict\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import sys\n from w3lib.url import is_url\n \n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import os\n import shutil\n import string\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n from scrapy.commands import ScrapyCommand\n \n class Command(ScrapyCommand):\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import json\n import logging\n \n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import json\n \n from scrapy.commands import ScrapyCommand\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import re\n import os\n import string\n\n@@ -1,5 +1,3 @@\n-from __future__ import print_function\n-\n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.utils.versions import scrapy_components_versions\n\n@@ -1,4 +1,3 @@\n-from __future__ import absolute_import\n import random\n import warnings\n from time import time\n\n@@ -1,3 +1,2 @@\n-from __future__ import absolute_import\n from .http10 import HTTP10DownloadHandler\n from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n\n@@ -1,5 +1,4 @@\n # -*- coding: utf-8 -*-\n-from __future__ import absolute_import\n import re\n import logging\n \n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import os\n import logging\n \n\n@@ -1,5 +1,3 @@\n-from __future__ import print_function\n-\n import gzip\n import logging\n import os\n\n@@ -1,5 +1,3 @@\n-from __future__ import print_function\n-\n import functools\n import logging\n from collections import defaultdict\n\n@@ -2,7 +2,6 @@\n This module implements a class which returns the appropriate Response class\n based on different criteria.\n \"\"\"\n-from __future__ import absolute_import\n from mimetypes import MimeTypes\n from pkgutil import get_data\n from io import StringIO\n\n@@ -3,8 +3,6 @@\n See documentation in docs/topics/shell.rst\n \n \"\"\"\n-from __future__ import print_function\n-\n import os\n import signal\n import warnings\n\n@@ -1,4 +1,3 @@\n-from __future__ import absolute_import\n from pydispatch import dispatcher\n from scrapy.utils import signal as _signal\n \n\n@@ -1,5 +1,4 @@\n # -*- coding: utf-8 -*-\n-from __future__ import absolute_import\n from collections import defaultdict\n import traceback\n import warnings\n\n@@ -1,7 +1,5 @@\n \"\"\"Boto/botocore helpers\"\"\"\n \n-from __future__ import absolute_import\n-\n from scrapy.exceptions import NotConfigured\n \n \n\n@@ -2,7 +2,6 @@\n pprint and pformat wrappers with colorization support\n \"\"\"\n \n-from __future__ import print_function\n import sys\n from pprint import pformat as pformat_\n \n\n@@ -1,6 +1,5 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n-from __future__ import print_function\n from time import time # used in global tests code\n \n def get_engine_status(engine):\n\n@@ -1,5 +1,3 @@\n-\n-from __future__ import absolute_import\n import signal\n \n from twisted.internet import reactor\n\n@@ -3,7 +3,6 @@ This module provides some useful functions for working with\n scrapy.http.Request objects\n \"\"\"\n \n-from __future__ import print_function\n import hashlib\n import weakref\n from six.moves.urllib.parse import urlunparse\n\n@@ -2,7 +2,6 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n-from __future__ import absolute_import\n import os\n \n from importlib import import_module\n\n@@ -1,4 +1,3 @@\n-from __future__ import absolute_import\n import sys\n import os\n \n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n from six.moves.urllib.parse import urljoin\n \n from twisted.internet import reactor\n\n@@ -9,7 +9,6 @@ and no performance penalty at all when disabled (as object_ref becomes just an\n alias to object in that case).\n \"\"\"\n \n-from __future__ import print_function\n import weakref\n from time import time\n from operator import itemgetter\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0bfaef37abe029c98072e7b1a1bf346ab79016c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 10 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 3182 | Contributors (this commit): 34 | Commits (past 90d): 39 | Contributors (cumulative): 88 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import sys, time, random, os, json\n from six.moves.urllib.parse import urlencode\n from subprocess import Popen, PIPE\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import time\n import tempfile\n import shutil\n\n@@ -1,6 +1,4 @@\n # -*- coding: utf-8 -*-\n-from __future__ import absolute_import\n-\n from unittest import mock\n \n from twisted.internet import reactor, error\n\n@@ -10,7 +10,6 @@ module with the ``runserver`` argument::\n     python test_engine.py runserver\n \"\"\"\n \n-from __future__ import print_function\n import sys, os, re\n from six.moves.urllib.parse import urlparse\n \n\n@@ -1,4 +1,3 @@\n-from __future__ import absolute_import\n import re\n import json\n import marshal\n\n@@ -1,4 +1,3 @@\n-from __future__ import absolute_import\n import os\n import csv\n import json\n\n@@ -1,5 +1,3 @@\n-from __future__ import print_function\n-\n import sys\n \n from testfixtures import LogCapture\n\n@@ -1,5 +1,4 @@\n # -*- coding: utf-8 -*-\n-from __future__ import absolute_import\n import inspect\n import unittest\n from unittest import mock\n\n@@ -1,5 +1,4 @@\n # -*- coding: utf-8 -*-\n-from __future__ import print_function\n import sys\n import logging\n import unittest\n\n@@ -1,4 +1,3 @@\n-from __future__ import print_function\n import unittest\n from scrapy.http import Request\n from scrapy.utils.request import request_fingerprint, _fingerprint_cache, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
