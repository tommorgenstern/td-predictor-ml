{"custom_id": "keras#6168147964a3570cc9431886048b97a1cfd7ae7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 971 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -187,16 +187,21 @@ def standardize_data_format(data_format):\n # Set Keras base dir path given KERAS_HOME env variable, if applicable.\n # Otherwise either ~/.keras or /tmp.\n if \"KERAS_HOME\" in os.environ:\n-    _keras_dir = os.environ.get(\"KERAS_HOME\")\n+    _KERAS_DIR = os.environ.get(\"KERAS_HOME\")\n else:\n     _keras_base_dir = os.path.expanduser(\"~\")\n     if not os.access(_keras_base_dir, os.W_OK):\n         _keras_base_dir = \"/tmp\"\n-    _keras_dir = os.path.join(_keras_base_dir, \".keras\")\n+    _KERAS_DIR = os.path.join(_keras_base_dir, \".keras\")\n+\n+\n+def keras_home():\n+    # Private accessor for the keras home location.\n+    return _KERAS_DIR\n \n \n # Attempt to read Keras config file.\n-_config_path = os.path.expanduser(os.path.join(_keras_dir, \"keras.json\"))\n+_config_path = os.path.expanduser(os.path.join(_KERAS_DIR, \"keras.json\"))\n if os.path.exists(_config_path):\n     try:\n         with open(_config_path) as f:\n@@ -217,9 +222,9 @@ if os.path.exists(_config_path):\n     _BACKEND = _backend\n \n # Save config file, if possible.\n-if not os.path.exists(_keras_dir):\n+if not os.path.exists(_KERAS_DIR):\n     try:\n-        os.makedirs(_keras_dir)\n+        os.makedirs(_KERAS_DIR)\n     except OSError:\n         # Except permission denied and potential race conditions\n         # in multi-threaded environments.\n\n@@ -9,6 +9,7 @@ import warnings\n import zipfile\n from urllib.request import urlretrieve\n \n+from keras_core.backend import config\n from keras_core.api_export import keras_core_export\n from keras_core.utils import io_utils\n from keras_core.utils.module_utils import gfile\n@@ -184,7 +185,8 @@ def get_file(\n             The default `\"auto\"` corresponds to `[\"tar\", \"zip\"]`.\n             None or an empty list will return no matches found.\n         cache_dir: Location to store cached files, when None it\n-            defaults to the default directory `~/.keras/`.\n+            defaults ether `$KERAS_HOME` if the `KERAS_HOME` environment\n+            variable is set or `~/.keras/`.\n \n     Returns:\n         Path to the downloaded file.\n@@ -204,7 +206,7 @@ def get_file(\n         )\n \n     if cache_dir is None:\n-        cache_dir = os.path.join(os.path.expanduser(\"~\"), \".keras\")\n+        cache_dir = config.keras_home()\n     if md5_hash is not None and file_hash is None:\n         file_hash = md5_hash\n         hash_algorithm = \"md5\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#60d4fbce3272c8cd0752af28787a8d3f8f19a941", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 489 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,8 +9,8 @@ import warnings\n import zipfile\n from urllib.request import urlretrieve\n \n-from keras_core.backend import config\n from keras_core.api_export import keras_core_export\n+from keras_core.backend import config\n from keras_core.utils import io_utils\n from keras_core.utils.module_utils import gfile\n from keras_core.utils.progbar import Progbar\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06053a120ac51c5afd1bdbdae3e06f63b23cc43f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 22 | Files Changed: 3 | Hunks: 17 | Methods Changed: 9 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 122 | Churn Cumulative: 1326 | Contributors (this commit): 7 | Commits (past 90d): 26 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -276,7 +276,7 @@ def can_convert_arrays(arrays):\n     )\n \n \n-def convert_to_arrays(arrays, dtype=None):\n+def convert_to_arrays(arrays):\n     \"\"\"Process array-like inputs.\n \n     This function:\n@@ -291,26 +291,31 @@ def convert_to_arrays(arrays, dtype=None):\n     Returns:\n         Structure of NumPy `ndarray`s.\n     \"\"\"\n-    dtype = dtype or backend.floatx()\n \n     def convert_single_array(x):\n         if x is None:\n             return x\n         if pandas is not None:\n             if isinstance(x, pandas.Series):\n-                x = np.expand_dims(x.to_numpy(dtype=dtype), axis=-1)\n+                x = np.expand_dims(x.to_numpy(), axis=-1)\n             elif isinstance(x, pandas.DataFrame):\n-                x = x.to_numpy(dtype=dtype)\n+                x = x.to_numpy()\n         if is_tf_ragged_tensor(x):\n             from keras_core.utils.module_utils import tensorflow as tf\n \n-            return tf.cast(x, dtype=dtype)\n+            # Convert floats to floatx.\n+            if (\n+                backend.is_float_dtype(x.dtype)\n+                and not backend.standardize_dtype(x.dtype) == backend.floatx()\n+            ):\n+                x = tf.cast(x, backend.floatx())\n+            return x\n         if not isinstance(x, np.ndarray):\n             # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n             # `torch.Tensor`, as well as any other tensor-like object that has\n             # added numpy support.\n             if hasattr(x, \"__array__\"):\n-                x = backend.convert_to_numpy(x).astype(dtype)\n+                x = backend.convert_to_numpy(x)\n             else:\n                 raise ValueError(\n                     \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n@@ -320,8 +325,12 @@ def convert_to_arrays(arrays, dtype=None):\n                 )\n         if x.dtype == object:\n             return x\n-        if not x.dtype == dtype:\n-            x = x.astype(dtype)\n+        # Convert floats to floatx.\n+        if (\n+            backend.is_float_dtype(x.dtype)\n+            and not backend.standardize_dtype(x.dtype) == backend.floatx()\n+        ):\n+            x = x.astype(backend.floatx())\n         return x\n \n     arrays = tree.map_structure(convert_single_array, arrays)\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pandas\n+import pytest\n import tensorflow as tf\n from absl.testing import parameterized\n \n@@ -9,30 +10,29 @@ from keras_core.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters([(\"np\",), (\"tf\",), (\"backend\",), (\"pandas\",)])\n-    def test_basic_flow(self, array_type):\n+    def make_array(self, array_type, shape, dtype=\"float32\"):\n         if array_type == \"np\":\n-            x = np.random.random((34, 4))\n-            y = np.random.random((34, 2))\n+            return np.ones(shape, dtype=dtype)\n         elif array_type == \"tf\":\n-            x = tf.random.normal((34, 4))\n-            y = tf.random.normal((34, 2))\n+            return tf.ones(shape, dtype=dtype)\n         elif array_type == \"backend\":\n             if backend.backend() == \"jax\":\n                 import jax\n \n-                x = jax.numpy.ones((34, 4))\n-                y = jax.numpy.ones((34, 2))\n+                return jax.numpy.ones(shape, dtype=dtype)\n             elif backend.backend() == \"torch\":\n                 import torch\n \n-                x = torch.ones((34, 4))\n-                y = torch.ones((34, 2))\n+                return torch.tensor(np.ones(shape, dtype=dtype))\n             else:\n-                return  # skip TF already addressed\n+                return tf.ones(shape, dtype=dtype)\n         elif array_type == \"pandas\":\n-            x = pandas.DataFrame(np.random.random((34, 4)))\n-            y = pandas.DataFrame(np.random.random((34, 2)))\n+            return pandas.DataFrame(np.ones(shape, dtype=dtype))\n+\n+    @parameterized.parameters([(\"np\",), (\"tf\",), (\"backend\",), (\"pandas\",)])\n+    def test_basic_flow(self, array_type):\n+        x = self.make_array(array_type, (34, 4))\n+        y = self.make_array(array_type, (34, 2))\n \n         adapter = array_data_adapter.ArrayDataAdapter(\n             x,\n@@ -185,3 +185,72 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n     def test_errors(self):\n         # TODO\n         pass\n+\n+    @parameterized.parameters([(\"np\",), (\"tf\",), (\"backend\",), (\"pandas\",)])\n+    def test_integer_inputs(self, array_type):\n+        x1 = self.make_array(array_type, (4, 4), dtype=\"float64\")\n+        x2 = self.make_array(array_type, (4, 4), dtype=\"int32\")\n+        y = self.make_array(array_type, (4, 2))\n+\n+        adapter = array_data_adapter.ArrayDataAdapter(\n+            (x1, x2),\n+            y=y,\n+            sample_weight=None,\n+            batch_size=4,\n+            steps=None,\n+            shuffle=False,\n+        )\n+\n+        (x1, x2), y = next(adapter.get_numpy_iterator())\n+        self.assertEqual(x1.dtype, backend.floatx())\n+        self.assertEqual(x2.dtype, \"int32\")\n+\n+    def test_pandas_series(self):\n+        x = pandas.Series(np.ones((10,)))\n+        y = np.ones((10,))\n+\n+        adapter = array_data_adapter.ArrayDataAdapter(\n+            x,\n+            y=y,\n+            sample_weight=None,\n+            batch_size=4,\n+            steps=None,\n+            shuffle=False,\n+        )\n+\n+        self.assertEqual(adapter.num_batches, 3)\n+        self.assertEqual(adapter.batch_size, 4)\n+        self.assertEqual(adapter.has_partial_batch, True)\n+        self.assertEqual(adapter.partial_batch_size, 2)\n+\n+        x, y = next(adapter.get_numpy_iterator())\n+        self.assertEqual(x.dtype, backend.floatx())\n+        self.assertIsInstance(x, np.ndarray)\n+        self.assertEqual(x.shape, (4, 1))\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"Only tensorflow supports raggeds\",\n+    )\n+    def test_tf_ragged(self):\n+        x = tf.ragged.constant([[1, 2], [1, 2, 3], [1, 2], [1], []], \"float64\")\n+        y = np.ones((5,))\n+\n+        adapter = array_data_adapter.ArrayDataAdapter(\n+            x,\n+            y=y,\n+            sample_weight=None,\n+            batch_size=2,\n+            steps=None,\n+            shuffle=False,\n+        )\n+\n+        self.assertEqual(adapter.num_batches, 3)\n+        self.assertEqual(adapter.batch_size, 2)\n+        self.assertEqual(adapter.has_partial_batch, True)\n+        self.assertEqual(adapter.partial_batch_size, 1)\n+\n+        x, y = next(adapter.get_numpy_iterator())\n+        self.assertEqual(x.dtype, backend.floatx())\n+        self.assertIsInstance(x, tf.RaggedTensor)\n+        self.assertEqual(x.shape, (2, None))\n\n@@ -205,7 +205,7 @@ def train_validation_split(arrays, validation_split):\n \n \n def class_weight_to_sample_weights(y, class_weight):\n-    sample_weight = np.ones(shape=(y.shape[0],), dtype=y.dtype)\n+    sample_weight = np.ones(shape=(y.shape[0],), dtype=backend.floatx())\n     if len(y.shape) > 1:\n         if y.shape[-1] != 1:\n             y = np.argmax(y, axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ba83b43f26c41e013d44c75b84cd5fff516772f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 38 | Churn Cumulative: 1729 | Contributors (this commit): 14 | Commits (past 90d): 34 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,11 +12,10 @@ class SpectralNormalizationTest(testing.TestCase):\n         self.run_layer_test(\n             layers.SpectralNormalization,\n             init_kwargs={\"layer\": layers.Dense(2)},\n-            input_shape=None,\n             input_data=np.random.uniform(size=(10, 3, 4)),\n             expected_output_shape=(10, 3, 2),\n             expected_num_trainable_weights=2,\n-            expected_num_non_trainable_weights=2,\n+            expected_num_non_trainable_weights=1,\n             expected_num_seed_generators=0,\n             expected_num_losses=0,\n             supports_masking=False,\n\n@@ -131,9 +131,9 @@ class HashedCrossing(Layer):\n             return input_shape[0]\n \n         if self.output_mode == \"one_hot\" and input_shape[0][-1] != 1:\n-            output_shape = tuple(input_shape[0]) + (self.num_bins,)\n-        output_shape = tuple(input_shape[0])[:-1] + (self.num_bins,)\n-        return output_shape\n+            return tuple(input_shape[0]) + (self.num_bins,)\n+\n+        return tuple(input_shape[0])[:-1] + (self.num_bins,)\n \n     def call(self, inputs):\n         self._check_at_least_two_inputs(inputs)\n\n@@ -15,7 +15,7 @@ class HashedCrossingTest(testing.TestCase):\n                 \"num_bins\": 3,\n                 \"output_mode\": \"int\",\n             },\n-            input_data=([1, 2], [4, 5]),\n+            input_data=(np.array([1, 2]), np.array([4, 5])),\n             expected_output_shape=(2,),\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n@@ -29,7 +29,7 @@ class HashedCrossingTest(testing.TestCase):\n         self.run_layer_test(\n             layers.HashedCrossing,\n             init_kwargs={\"num_bins\": 4, \"output_mode\": \"one_hot\"},\n-            input_data=([1, 2], [4, 5]),\n+            input_data=(np.array([1, 2]), np.array([4, 5])),\n             expected_output_shape=(2, 4),\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n\n@@ -329,15 +329,29 @@ class TestCase(unittest.TestCase):\n                 model.fit(input_data, output_data, verbose=0)\n \n         # Build test.\n-        if input_shape is not None:\n-            layer = layer_cls(**init_kwargs)\n-            if isinstance(input_shape, dict):\n-                layer.build(**input_shape)\n+        if input_data is not None or input_shape is not None:\n+            if input_shape is None:\n+                build_shape = tree.map_structure(\n+                    lambda x: ops.shape(x), input_data\n+                )\n             else:\n-                layer.build(input_shape)\n+                build_shape = input_shape\n+            layer = layer_cls(**init_kwargs)\n+            if isinstance(build_shape, dict):\n+                layer.build(**build_shape)\n+            else:\n+                layer.build(build_shape)\n             run_build_asserts(layer)\n \n             # Symbolic call test.\n+            if input_shape is None:\n+                keras_tensor_inputs = tree.map_structure(\n+                    lambda x: create_keras_tensors(\n+                        ops.shape(x), x.dtype, input_sparse\n+                    ),\n+                    input_data,\n+                )\n+            else:\n                 keras_tensor_inputs = create_keras_tensors(\n                     input_shape, input_dtype, input_sparse\n                 )\n@@ -405,6 +419,7 @@ def create_keras_tensors(input_shape, dtype, sparse):\n             )\n             for k, v in input_shape.items()\n         }\n+    raise ValueError(f\"Unsupported type for `input_shape`: {type(input_shape)}\")\n \n \n def create_eager_tensors(input_shape, dtype, sparse):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68a46c30131d8ea32a2b6961f2228c5cd7e8854d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 484 | Contributors (this commit): 8 | Commits (past 90d): 23 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -212,7 +212,7 @@ def fori_loop(lower, upper, body_fun, init_val):\n \n \n def stop_gradient(x):\n-    pass\n+    return x\n \n \n def unstack(x, num=None, axis=0):\n\n@@ -264,6 +264,11 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertEqual(model.layers[0].w.numpy(), 0.0)\n         self.assertNotEqual(model.layers[0].b.numpy(), 0.0)\n \n+    def test_stop_gradient_return(self):\n+        x = ops.random.uniform(shape=(2, 4), dtype=\"float32\")\n+        y = ops.stop_gradient(x)\n+        self.assertAllClose(x, y)\n+\n     def test_shape(self):\n         x = np.ones((2, 3, 7, 1))\n         self.assertAllEqual(core.shape(x), (2, 3, 7, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32a607023670b069361f12eaa5788dd92affafcb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 20 | Churn Cumulative: 531 | Contributors (this commit): 11 | Commits (past 90d): 19 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -134,6 +134,11 @@ def compute_output_spec(fn, *args, **kwargs):\n \n             def convert_keras_tensor_to_tf(x):\n                 if isinstance(x, KerasTensor):\n+                    if x.sparse:\n+                        return tf.compat.v1.sparse_placeholder(\n+                            shape=x.shape, dtype=x.dtype\n+                        )\n+                    else:\n                         return tf.compat.v1.placeholder(\n                             shape=x.shape, dtype=x.dtype\n                         )\n@@ -154,7 +159,9 @@ def compute_output_spec(fn, *args, **kwargs):\n \n             def convert_tf_to_keras_tensor(x):\n                 if tf.is_tensor(x):\n-                    return KerasTensor(x.shape, x.dtype)\n+                    return KerasTensor(\n+                        x.shape, x.dtype, sparse=isinstance(x, tf.SparseTensor)\n+                    )\n                 return x\n \n             output_spec = tf.nest.map_structure(\n\n@@ -1,7 +1,4 @@\n-import tree\n-\n from keras_core.api_export import keras_core_export\n-from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.layers.layer import Layer\n \n \n@@ -19,11 +16,3 @@ class Identity(Layer):\n \n     def call(self, inputs):\n         return inputs\n-\n-    def compute_output_spec(self, inputs):\n-        return tree.map_structure(\n-            lambda input: KerasTensor(\n-                shape=input.shape, dtype=input.dtype, sparse=input.sparse\n-            ),\n-            inputs,\n-        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8361f6fd0e7ff1e54e67acb426b973f486413711", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 35 | Churn Cumulative: 1372 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -308,7 +308,7 @@ def print_summary(\n     non_trainable_count = count_params(model.non_trainable_weights)\n     non_trainable_memory_size = weight_memory_size(model.non_trainable_weights)\n \n-    if model.compiled and model.optimizer.built:\n+    if model.compiled and model.optimizer and model.optimizer.built:\n         optimizer_weight_count = count_params(model.optimizer.variables)\n         optimizer_memory_size = weight_memory_size(model.optimizer.variables)\n     else:\n\n@@ -1,7 +1,6 @@\n-import os\n-\n import numpy as np\n import pytest\n+from absl.testing import parameterized\n \n from keras_core import layers\n from keras_core import models\n@@ -9,34 +8,34 @@ from keras_core import testing\n from keras_core.utils import summary_utils\n \n \n-class SummaryUtilsTest(testing.TestCase):\n+class SummaryUtilsTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters([(\"adam\",), (None,)])\n     @pytest.mark.requires_trainable_backend\n-    def test_print_model_summary(self):\n+    def test_print_model_summary(self, optimizer):\n         inputs = layers.Input((2,))\n         outputs = layers.Dense(3)(inputs)\n         model = models.Model(inputs, outputs)\n-        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mse\"])\n+        model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mse\"])\n+        if optimizer:\n             # Trigger the optimizer weights creation\n             model.fit(x=np.zeros([4, 2]), y=np.zeros([4, 3]))\n \n-        file_name = \"model_1.txt\"\n-        temp_dir = self.get_temp_dir()\n-        fpath = os.path.join(temp_dir, file_name)\n-        writer = open(fpath, \"w\")\n+        summary_content = []\n \n-        def print_to_file(text, line_break=False):\n-            print(text, file=writer)\n+        def print_to_variable(text, line_break=False):\n+            summary_content.append(text)\n \n         try:\n-            summary_utils.print_summary(model, print_fn=print_to_file)\n-            writer.close()\n-            self.assertTrue(os.path.exists(fpath))\n-            with open(fpath, \"r\") as reader:\n-                summary_content = reader.read()\n-            # self.assertEqual(len(lines), 15)\n+            summary_utils.print_summary(model, print_fn=print_to_variable)\n+            summary_content = \"\\n\".join(summary_content)\n+            if optimizer:\n                 self.assertIn(\"Total params: 29\", summary_content)\n                 self.assertIn(\"Trainable params: 9\", summary_content)\n                 self.assertIn(\"Non-trainable params: 0\", summary_content)\n                 self.assertIn(\"Optimizer params: 20\", summary_content)\n+            else:\n+                self.assertIn(\"Total params: 9\", summary_content)\n+                self.assertIn(\"Trainable params: 9\", summary_content)\n+                self.assertIn(\"Non-trainable params: 0\", summary_content)\n         except ImportError:\n             pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea8ffc1c27d388a1983d93a1c8ad378767f69284", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 1376 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 9 | DMM Complexity: 0.25\n\nDIFF:\n@@ -311,9 +311,11 @@ def print_summary(\n     if model.compiled and model.optimizer and model.optimizer.built:\n         optimizer_weight_count = count_params(model.optimizer.variables)\n         optimizer_memory_size = weight_memory_size(model.optimizer.variables)\n+        optimizer_built = True\n     else:\n         optimizer_weight_count = 0\n         optimizer_memory_size = 0\n+        optimizer_built = False\n \n     total_count = trainable_count + non_trainable_count + optimizer_weight_count\n     total_memory_size = (\n@@ -349,6 +351,7 @@ def print_summary(\n         + highlight_number(f\"{non_trainable_count:,}\")\n         + f\" ({readable_memory_size(non_trainable_memory_size)})\"\n     )\n+    if optimizer_built:\n         console.print(\n             bold_text(\" Optimizer params: \")\n             + highlight_number(f\"{optimizer_weight_count:,}\")\n\n@@ -37,5 +37,6 @@ class SummaryUtilsTest(testing.TestCase, parameterized.TestCase):\n                 self.assertIn(\"Total params: 9\", summary_content)\n                 self.assertIn(\"Trainable params: 9\", summary_content)\n                 self.assertIn(\"Non-trainable params: 0\", summary_content)\n+                self.assertNotIn(\"Optimizer params\", summary_content)\n         except ImportError:\n             pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85710f4491be451e1af159c5373958233c49fca1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 807 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core.api_export import keras_core_export\n from keras_core.backend import config\n from keras_core.backend.common import global_state\n from keras_core.backend.common.name_scope import current_path\n@@ -398,6 +399,7 @@ PYTHON_DTYPES_MAP = {\n }\n \n \n+@keras_core_export(\"keras_core.backend.standardize_dtype\")\n def standardize_dtype(dtype):\n     if dtype is None:\n         return config.floatx()\n@@ -454,11 +456,13 @@ def shape_equal(a, b):\n     return True\n \n \n+@keras_core_export(\"keras_core.backend.is_float_dtype\")\n def is_float_dtype(dtype):\n     dtype = standardize_dtype(dtype)\n     return dtype.startswith(\"float\") or dtype.startswith(\"bfloat\")\n \n \n+@keras_core_export(\"keras_core.backend.is_int_dtype\")\n def is_int_dtype(dtype):\n     dtype = standardize_dtype(dtype)\n     return dtype.startswith(\"int\") or dtype.startswith(\"uint\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d64651901631bd2bfb75d4491ec0da8f6cf09b50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 102 | Churn Cumulative: 1902 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 10 | DMM Complexity: 0.40425531914893614\n\nDIFF:\n@@ -7,6 +7,7 @@ from keras_core import backend\n from keras_core import utils\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.legacy.saving import legacy_h5_format\n from keras_core.models.variable_mapping import map_trackable_variables\n from keras_core.saving import saving_api\n from keras_core.saving import saving_lib\n@@ -263,7 +264,7 @@ class Model(Trainer, Layer):\n         )\n \n     @traceback_utils.filter_traceback\n-    def save(self, filepath, overwrite=True, save_format=\"keras\"):\n+    def save(self, filepath, overwrite=True, **kwargs):\n         \"\"\"Saves a model as a `.keras` file.\n \n         Args:\n@@ -301,21 +302,29 @@ class Model(Trainer, Layer):\n \n         Thus models can be reinstantiated in the exact same state.\n         \"\"\"\n-        if save_format in [\"h5\", \"tf\"]:\n+        include_optimizer = kwargs.pop(\"include_optimizer\", True)\n+        save_format = kwargs.pop(\"save_format\", None)\n+        if kwargs:\n             raise ValueError(\n-                \"`'h5'` and `'t5'` formats are no longer supported via the \"\n-                \"`save_format` option. Please use the new `'keras'` format. \"\n+                \"The following argument(s) are not supported: \"\n+                f\"{list(kwargs.keys())}\"\n+            )\n+        if save_format:\n+            if str(filepath).endswith((\".h5\", \".hdf5\")) or str(\n+                filepath\n+            ).endswith(\".keras\"):\n+                warnings.warn(\n+                    \"The `save_format` argument is deprecated in Keras Core. \"\n+                    \"We recommend removing this argument as it can be inferred \"\n+                    \"from the file path. \"\n                     f\"Received: save_format={save_format}\"\n                 )\n-        if save_format not in [\"keras\", \"keras_v3\"]:\n+            else:\n                 raise ValueError(\n-                \"Unknown `save_format` value. Only the `'keras'` format is \"\n-                f\"currently supported. Received: save_format={save_format}\"\n-            )\n-        if not str(filepath).endswith(\".keras\"):\n-            raise ValueError(\n-                \"The filename must end in `.keras`. \"\n-                f\"Received: filepath={filepath}\"\n+                    \"The `save_format` argument is deprecated in Keras Core. \"\n+                    \"Please remove this argument and pass a file path with \"\n+                    \"either `.keras` or `.h5` extension.\"\n+                    f\"Received: save_format={save_format}\"\n                 )\n         try:\n             exists = os.path.exists(filepath)\n@@ -325,7 +334,28 @@ class Model(Trainer, Layer):\n             proceed = io_utils.ask_to_proceed_with_overwrite(filepath)\n             if not proceed:\n                 return\n+        if str(filepath).endswith(\".keras\"):\n             saving_lib.save_model(self, filepath)\n+        elif str(filepath).endswith((\".h5\", \".hdf5\")):\n+            # Deprecation warnings\n+            warnings.warn(\n+                \"You are saving your model as an HDF5 file via `model.save()`. \"\n+                \"This file format is considered legacy. \"\n+                \"We recommend using instead the native Keras format, \"\n+                \"e.g. `model.save('my_model.keras')`.\"\n+            )\n+            legacy_h5_format.save_model_to_hdf5(\n+                self, filepath, overwrite, include_optimizer\n+            )\n+        else:\n+            raise ValueError(\n+                \"Invalid filepath extension for saving. \"\n+                \"Please add either a `.keras` extension for the native Keras \"\n+                f\"format (recommended) or a `.h5` extension. \"\n+                \"Use `tf.saved_model.save()` if you want to export a \"\n+                \"SavedModel for use with TFLite/TFServing/etc. \"\n+                f\"Received: filepath={filepath}.\"\n+            )\n \n     @traceback_utils.filter_traceback\n     def save_weights(self, filepath, overwrite=True):\n\n@@ -402,15 +402,6 @@ class SavingTest(testing.TestCase):\n     #         self.assertIn(str(temp_filepath), mock_re_match.call_args.args)\n     #         self.assertIn(str(temp_filepath), mock_copy.call_args.args)\n \n-    def test_load_model_api_endpoint(self):\n-        temp_filepath = Path(os.path.join(self.get_temp_dir(), \"mymodel.keras\"))\n-        model = _get_basic_functional_model()\n-        ref_input = np.random.random((2, 4))\n-        ref_output = model.predict(ref_input)\n-        model.save(temp_filepath)\n-        model = keras_core.saving.load_model(temp_filepath)\n-        self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n-\n     def test_save_load_weights_only(self):\n         temp_filepath = Path(\n             os.path.join(self.get_temp_dir(), \"mymodel.weights.h5\")\n@@ -530,7 +521,10 @@ class SavingTest(testing.TestCase):\n             np.array(new_model.layers[2].kernel), new_layer_kernel_value\n         )\n \n-    def test_api_errors(self):\n+\n+@pytest.mark.requires_trainable_backend\n+class SavingAPITest(testing.TestCase):\n+    def test_saving_api_errors(self):\n         from keras_core.saving import saving_api\n \n         model = _get_basic_functional_model()\n@@ -559,6 +553,40 @@ class SavingTest(testing.TestCase):\n         with self.assertRaisesRegex(ValueError, \"File format not supported\"):\n             _ = saving_api.load_model(temp_filepath)\n \n+    def test_model_api_endpoint(self):\n+        temp_filepath = Path(os.path.join(self.get_temp_dir(), \"mymodel.keras\"))\n+        model = _get_basic_functional_model()\n+        ref_input = np.random.random((2, 4))\n+        ref_output = model.predict(ref_input)\n+        model.save(temp_filepath)\n+        model = keras_core.saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n+\n+    def test_model_api_endpoint_h5(self):\n+        temp_filepath = Path(os.path.join(self.get_temp_dir(), \"mymodel.h5\"))\n+        model = _get_basic_functional_model()\n+        ref_input = np.random.random((2, 4))\n+        ref_output = model.predict(ref_input)\n+        model.save(temp_filepath)\n+        model = keras_core.saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(ref_input), ref_output, atol=1e-6)\n+\n+    def test_model_api_errors(self):\n+        model = _get_basic_functional_model()\n+\n+        # Saving API errors\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel\")\n+        with self.assertRaisesRegex(ValueError, \"argument is deprecated\"):\n+            model.save(temp_filepath, save_format=\"keras\")\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.notkeras\")\n+        with self.assertRaisesRegex(ValueError, \"Invalid filepath extension\"):\n+            model.save(temp_filepath)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n+        with self.assertRaisesRegex(ValueError, \"are not supported\"):\n+            model.save(temp_filepath, invalid_arg=\"hello\")\n+\n \n # def test_safe_mode(self):\n #     temp_filepath = os.path.join(self.get_temp_dir(), \"unsafe_model.keras\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34d687274f619c60f633844d26167d21530b5d62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 21 | Churn Cumulative: 165 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,19 +1,20 @@\n from keras_core import backend\n from keras_core import ops\n \n+DTYPE_TO_SIZE = {\n+    **{f\"float{i}\": i for i in (16, 32, 64)},\n+    **{f\"int{i}\": i for i in (8, 16, 32, 64)},\n+    **{f\"uint{i}\": i for i in (8, 16, 32, 64)},\n+    \"bfloat16\": 16,\n+    \"bool\": 1,\n+}\n+\n \n def dtype_size(dtype):\n-    if dtype in (\"bfloat16\", \"float16\"):\n-        return 16\n-    if dtype in (\"float32\", \"int32\"):\n-        return 32\n-    if dtype in (\"float64\", \"int64\"):\n-        return 64\n-    if dtype == \"uint8\":\n-        return 8\n-    if dtype == \"bool\":\n-        return 1\n+    size = DTYPE_TO_SIZE.get(dtype, None)\n+    if size is None:\n         raise ValueError(f\"Invalid dtype: {dtype}\")\n+    return size\n \n \n def is_float(dtype):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16c43bca968c178163f837936ac2e42e0a0b75bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 298 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 4 | Methods Changed: 47 | Complexity Δ (Sum/Max): 51/23 | Churn Δ: 298 | Churn Cumulative: 312 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,142 @@\n+import os\n+import sys\n+from io import StringIO\n+\n+from keras_core.testing import test_case\n+from keras_core.utils.code_stats import count_loc\n+\n+\n+class TestCountLoc(test_case.TestCase):\n+    def setUp(self):\n+        self.test_dir = \"test_directory\"\n+        os.makedirs(self.test_dir, exist_ok=True)\n+\n+    def tearDown(self):\n+        for root, dirs, files in os.walk(self.test_dir, topdown=False):\n+            for name in files:\n+                os.remove(os.path.join(root, name))\n+            for name in dirs:\n+                os.rmdir(os.path.join(root, name))\n+\n+    def create_file(self, filename, content):\n+        with open(\n+            os.path.join(self.test_dir, filename), \"w\", encoding=\"utf-8\"\n+        ) as f:\n+            f.write(content)\n+\n+    def test_count_loc_valid_python(self):\n+        self.create_file(\n+            \"sample.py\", \"# This is a test file\\n\\nprint('Hello')\\n\"\n+        )\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 1)\n+\n+    def test_exclude_test_files(self):\n+        self.create_file(\"sample_test.py\", \"print('Hello')\\n\")\n+        loc = count_loc(self.test_dir, exclude=(\"_test\",))\n+        self.assertEqual(loc, 0)\n+\n+    def test_other_extensions(self):\n+        self.create_file(\"sample.txt\", \"Hello\\n\")\n+        loc = count_loc(self.test_dir, extensions=(\".py\",))\n+        self.assertEqual(loc, 0)\n+\n+    def test_comment_lines(self):\n+        self.create_file(\n+            \"sample.py\", \"# Comment\\nprint('Hello')\\n# Another comment\\n\"\n+        )\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 1)\n+\n+    def test_empty_file(self):\n+        self.create_file(\"empty.py\", \"\")\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 0)\n+\n+    def test_whitespace_only(self):\n+        self.create_file(\"whitespace.py\", \"     \\n\\t\\n\")\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 0)\n+\n+    def test_inline_comments_after_code(self):\n+        content = 'print(\"Hello\") # This is an inline comment'\n+        self.create_file(\"inline_comment_sample.py\", content)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 1)  # The comment shouldn't affect the count\n+\n+    def test_directory_structure(self):\n+        content1 = 'print(\"Hello from file1\")'\n+        content2 = 'print(\"Hello from file2\")'\n+        os.mkdir(os.path.join(self.test_dir, \"subdir\"))\n+        self.create_file(\"sample1.py\", content1)\n+        self.create_file(os.path.join(\"subdir\", \"sample2.py\"), content2)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 2)  # Both files should be counted\n+\n+    def test_normal_directory_name(self):\n+        content = 'print(\"Hello from a regular directory\")'\n+        os.makedirs(os.path.join(self.test_dir, \"some_test_dir\"))\n+        self.create_file(os.path.join(\"some_test_dir\", \"sample.py\"), content)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 1)  # Should count normally\n+\n+    def test_exclude_directory_name(self):\n+        content = 'print(\"Hello from an excluded directory\")'\n+        os.makedirs(os.path.join(self.test_dir, \"dir_test\"))\n+        self.create_file(os.path.join(\"dir_test\", \"sample.py\"), content)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 0)\n+        # Shouldn't count the file in dir_test due to the exclusion pattern\n+\n+    def test_verbose_output(self):\n+        content = 'print(\"Hello\")'\n+        self.create_file(\"sample.py\", content)\n+        original_stdout = sys.stdout\n+        sys.stdout = StringIO()\n+        count_loc(self.test_dir, verbose=1)\n+        output = sys.stdout.getvalue()\n+        sys.stdout = original_stdout\n+        self.assertIn(\"Count LoCs in\", output)\n+\n+    def test_multiline_string_same_line(self):\n+        content = '''\"\"\"This is a multiline string ending on the same line\"\"\"\n+        print(\"Outside string\")'''\n+        self.create_file(\"same_line_multiline.py\", content)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 1)  # Only the print statement should count\n+\n+    def test_multiline_string_ends_on_same_line(self):\n+        content = '\"\"\"a multiline string end on same line\"\"\"\\nprint(\"Outstr\")'\n+        self.create_file(\"same_line_multiline.py\", content)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 1)  # Only the print statement should count\n+\n+    def test_multiline_string_ends_in_middle_of_line(self):\n+        content = '''print(\"Start\")\n+        \"\"\"This is a multiline string ending in the middle of a line\"\"\"\n+        \"\"\"This is another multiline string.\"\"\"\n+        print(\"End\")'''\n+        self.create_file(\"multiline_in_middle.py\", content)\n+        loc = count_loc(self.test_dir)\n+        self.assertEqual(loc, 2)  # Both print statements should count\n+\n+    def test_line_starting_with_triple_quotes_not_ending(self):\n+        content = '\"\"\"\\nThis is a multiline string\\n'\n+        self.create_file(\"test_file_2.py\", content)\n+        path = os.path.join(self.test_dir, \"test_file_2.py\")\n+        self.assertEqual(count_loc(path), 0)\n+        # Because it's part of a multiline string\n+\n+    def test_line_starting_and_ending_with_triple_quotes(self):\n+        content = '\"\"\"This is a one-liner docstring.\"\"\"\\n'\n+        self.create_file(\"test_file_3.py\", content)\n+        path = os.path.join(self.test_dir, \"test_file_3.py\")\n+        self.assertEqual(count_loc(path), 0)\n+        # This is still considered a comment/docstring\n+\n+    def test_string_open_true_line_starting_with_triple_quotes(self):\n+        content = '\"\"\"\\nEnd of the multiline string.\"\"\"\\n'\n+        self.create_file(\"test_file_4.py\", content)\n+        path = os.path.join(self.test_dir, \"test_file_4.py\")\n+        self.assertEqual(count_loc(path), 0)\n+        # Entire content is a multiline string/comment\n\n@@ -0,0 +1,57 @@\n+from unittest.mock import patch\n+\n+from keras_core.testing import test_case\n+from keras_core.utils import io_utils\n+\n+\n+class TestIoUtils(test_case.TestCase):\n+    def test_enable_interactive_logging(self):\n+        io_utils.enable_interactive_logging()\n+        self.assertTrue(io_utils.is_interactive_logging_enabled())\n+\n+    def test_disable_interactive_logging(self):\n+        io_utils.disable_interactive_logging()\n+        self.assertFalse(io_utils.is_interactive_logging_enabled())\n+\n+    def test_set_logging_verbosity_valid(self):\n+        valid_levels = [\"FATAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"]\n+        for level in valid_levels:\n+            io_utils.set_logging_verbosity(level)\n+\n+    def test_set_logging_verbosity_invalid(self):\n+        with self.assertRaises(ValueError):\n+            io_utils.set_logging_verbosity(\"INVALID\")\n+\n+    @patch(\"builtins.input\", side_effect=[\"y\"])\n+    def test_ask_to_proceed_with_overwrite_yes(self, _):\n+        self.assertTrue(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n+\n+    @patch(\"builtins.input\", side_effect=[\"n\"])\n+    def test_ask_to_proceed_with_overwrite_no(self, _):\n+        self.assertFalse(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n+\n+    @patch(\"sys.stdout.write\")\n+    def test_print_msg_interactive_with_line_break(self, mock_write):\n+        io_utils.enable_interactive_logging()\n+        io_utils.print_msg(\"Hello\", line_break=True)\n+        mock_write.assert_called_once_with(\"Hello\\n\")\n+\n+    @patch(\"sys.stdout.write\")\n+    def test_print_msg_interactive_without_line_break(self, mock_write):\n+        io_utils.enable_interactive_logging()\n+        io_utils.print_msg(\"Hello\", line_break=False)\n+        mock_write.assert_called_once_with(\"Hello\")\n+\n+    @patch(\"absl.logging.info\")\n+    def test_print_msg_non_interactive(self, mock_logging):\n+        io_utils.disable_interactive_logging()\n+        io_utils.print_msg(\"Hello\")\n+        mock_logging.assert_called_once_with(\"Hello\")\n+\n+    @patch(\"builtins.input\", side_effect=[\"invalid\", \"invalid\", \"y\"])\n+    def test_ask_to_proceed_with_overwrite_invalid_then_yes(self, _):\n+        self.assertTrue(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n+\n+    @patch(\"builtins.input\", side_effect=[\"invalid\", \"n\"])\n+    def test_ask_to_proceed_with_overwrite_invalid_then_no(self, _):\n+        self.assertFalse(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n\n@@ -3,6 +3,11 @@ from keras_core.utils import naming\n \n \n class NamingUtilsTest(test_case.TestCase):\n+    def test_uniquify_unique_name(self):\n+        name = \"the_unique_name\"\n+        unique_name = naming.uniquify(name)\n+        self.assertEqual(unique_name, name)\n+\n     def test_auto_name(self):\n         self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name\")\n         self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name_1\")\n@@ -12,3 +17,97 @@ class NamingUtilsTest(test_case.TestCase):\n         self.assertEqual(naming.get_uid(\"very_unique_name\"), 1)\n         self.assertEqual(naming.get_uid(\"very_unique_name\"), 2)\n         self.assertEqual(naming.get_uid(\"very_unique_name\"), 3)\n+\n+    def test_uniquify_non_unique_name(self):\n+        name = \"non_unique_name\"\n+        naming.uniquify(name)\n+        unique_name = naming.uniquify(name)\n+        self.assertEqual(unique_name, name + \"_1\")\n+\n+    def test_to_snake_case_snake_case_name(self):\n+        name = \"snake_case_name\"\n+        snake_case_name = naming.to_snake_case(name)\n+        self.assertEqual(snake_case_name, name)\n+\n+    def test_get_uid_existing_prefix(self):\n+        prefix = \"existing_prefix\"\n+        naming.get_uid(prefix)\n+        uid = naming.get_uid(prefix)\n+        self.assertEqual(uid, 2)\n+\n+    def test_reset_uids(self):\n+        naming.get_uid(\"unique_name\")\n+        naming.reset_uids()\n+        uid = naming.get_uid(\"unique_name\")\n+        self.assertEqual(uid, 1)\n+\n+    def test_get_object_name_no_name_attribute(self):\n+        class ObjectWithoutName:\n+            __name__ = \"ObjectWithoutName\"\n+\n+        obj = ObjectWithoutName()\n+        object_name = naming.get_object_name(obj)\n+        self.assertEqual(object_name, \"object_without_name\")\n+\n+    def test_get_object_name_no_name_or_class_attribute(self):\n+        class ObjectWithoutNameOrClass:\n+            pass\n+\n+        obj = ObjectWithoutNameOrClass()\n+        object_name = naming.get_object_name(obj)\n+        self.assertEqual(object_name, \"object_without_name_or_class\")\n+\n+    def test_uniquify_already_uniquified_name(self):\n+        name = \"unique_name\"\n+        unique_name = naming.uniquify(name)\n+        new_unique_name = naming.uniquify(unique_name)\n+        self.assertEqual(new_unique_name, unique_name)\n+\n+    def test_to_snake_case_capital_after_any_character(self):\n+        name = \"myVariableNameHere\"\n+        snake_case_name = naming.to_snake_case(name)\n+        self.assertEqual(snake_case_name, \"my_variable_name_here\")\n+\n+    def test_to_snake_case_lower_before_upper(self):\n+        name = \"convertTHIS\"\n+        snake_case_name = naming.to_snake_case(name)\n+        self.assertEqual(snake_case_name, \"convert_this\")\n+\n+    def test_to_snake_case_already_snake_cased(self):\n+        name = \"already_snake_cased\"\n+        snake_case_name = naming.to_snake_case(name)\n+        self.assertEqual(snake_case_name, name)\n+\n+    def test_to_snake_case_no_changes(self):\n+        name = \"lowercase\"\n+        snake_case_name = naming.to_snake_case(name)\n+        self.assertEqual(snake_case_name, name)\n+\n+    def test_to_snake_case_single_uppercase_word(self):\n+        name = \"UPPERCASE\"\n+        snake_case_name = naming.to_snake_case(name)\n+        self.assertEqual(snake_case_name, \"uppercase\")\n+\n+    def test_get_object_name_for_keras_objects(self):\n+        class MockKerasObject:\n+            name = \"mock_object\"\n+\n+        obj = MockKerasObject()\n+        result = naming.get_object_name(obj)\n+        self.assertEqual(\n+            result, \"mock_object\", f\"Expected 'mock_object' but got {result}\"\n+        )\n+\n+    # Test for function objects that have a `__name__` attribute.\n+    def test_get_object_name_for_functions(self):\n+        def mock_function():\n+            pass\n+\n+        result = naming.get_object_name(mock_function)\n+        # Assumes to_snake_case works correctly.\n+        expected_name = naming.to_snake_case(mock_function.__name__)\n+        self.assertEqual(\n+            result,\n+            expected_name,\n+            f\"Expected '{expected_name}' but got {result}\",\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fecf8e893515d499dfd2b44b3e5a95f4e0bc8c71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 331 | Files Changed: 5 | Hunks: 15 | Methods Changed: 50 | Complexity Δ (Sum/Max): -48/4 | Churn Δ: 350 | Churn Cumulative: 1383 | Contributors (this commit): 6 | Commits (past 90d): 15 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n import numpy as np\n \n-from keras_core import backend\n+from keras_core import backend, ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.utils import argument_validation\n@@ -17,13 +17,9 @@ class Discretization(Layer):\n     contiguous ranges and output an integer index indicating which range each\n     element was placed in.\n \n-    **Note:** This layer uses TensorFlow internally. It cannot\n-    be used as part of the compiled computation graph of a model with\n-    any backend other than TensorFlow.\n-    It can however be used with any backend when running eagerly.\n-    It can also always be used as part of an input preprocessing pipeline\n-    with any backend (outside the model itself), which is how we recommend\n-    to use this layer.\n+    This layer can also always be used as part of an input preprocessing\n+    pipeline with any backend (outside the model itself), which is how we\n+    recommend to use this layer.\n \n     **Note:** This layer is safe to use inside a `tf.data` pipeline\n     (independently of which backend you're using).\n@@ -78,14 +74,14 @@ class Discretization(Layer):\n \n     Examples:\n \n-    Bucketize float values based on provided buckets.\n+    Digitize float values based on provided buckets.\n     >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n     >>> layer = Discretization(bin_boundaries=[0., 1., 2.])\n     >>> layer(input)\n     array([[0, 2, 3, 1],\n            [1, 3, 2, 1]])\n \n-    Bucketize float values based on a number of buckets to compute.\n+    Digitize float values based on a number of buckets to compute.\n     >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n     >>> layer = Discretization(num_bins=4, epsilon=0.01)\n     >>> layer.adapt(input)\n@@ -241,20 +237,15 @@ class Discretization(Layer):\n         if not isinstance(\n             inputs,\n             (\n-                tf.Tensor,\n-                tf.SparseTensor,\n-                tf.RaggedTensor,\n                 np.ndarray,\n                 backend.KerasTensor,\n             ),\n         ):\n-            inputs = tf.convert_to_tensor(\n-                backend.convert_to_numpy(inputs), dtype=self.input_dtype\n+            inputs = backend.convert_to_tensor(\n+                inputs, dtype=self.input_dtype\n             )\n \n-        from keras_core.backend.tensorflow.numpy import digitize\n-\n-        indices = digitize(inputs, self.bin_boundaries)\n+        indices = ops.digitize(inputs, self.bin_boundaries)\n \n         outputs = tf_utils.encode_categorical_inputs(\n             indices,\n@@ -370,7 +361,3 @@ def compress_summary(summary, epsilon):\n     )\n     summary = np.stack((new_bins, new_weights))\n     return summary.astype(\"float32\")\n-\n-\n-def bucketize(inputs, boundaries):\n-    return tf.raw_ops.Bucketize(input=inputs, boundaries=boundaries)\n\n@@ -1,142 +0,0 @@\n-import os\n-import sys\n-from io import StringIO\n-\n-from keras_core.testing import test_case\n-from keras_core.utils.code_stats import count_loc\n-\n-\n-class TestCountLoc(test_case.TestCase):\n-    def setUp(self):\n-        self.test_dir = \"test_directory\"\n-        os.makedirs(self.test_dir, exist_ok=True)\n-\n-    def tearDown(self):\n-        for root, dirs, files in os.walk(self.test_dir, topdown=False):\n-            for name in files:\n-                os.remove(os.path.join(root, name))\n-            for name in dirs:\n-                os.rmdir(os.path.join(root, name))\n-\n-    def create_file(self, filename, content):\n-        with open(\n-            os.path.join(self.test_dir, filename), \"w\", encoding=\"utf-8\"\n-        ) as f:\n-            f.write(content)\n-\n-    def test_count_loc_valid_python(self):\n-        self.create_file(\n-            \"sample.py\", \"# This is a test file\\n\\nprint('Hello')\\n\"\n-        )\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 1)\n-\n-    def test_exclude_test_files(self):\n-        self.create_file(\"sample_test.py\", \"print('Hello')\\n\")\n-        loc = count_loc(self.test_dir, exclude=(\"_test\",))\n-        self.assertEqual(loc, 0)\n-\n-    def test_other_extensions(self):\n-        self.create_file(\"sample.txt\", \"Hello\\n\")\n-        loc = count_loc(self.test_dir, extensions=(\".py\",))\n-        self.assertEqual(loc, 0)\n-\n-    def test_comment_lines(self):\n-        self.create_file(\n-            \"sample.py\", \"# Comment\\nprint('Hello')\\n# Another comment\\n\"\n-        )\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 1)\n-\n-    def test_empty_file(self):\n-        self.create_file(\"empty.py\", \"\")\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 0)\n-\n-    def test_whitespace_only(self):\n-        self.create_file(\"whitespace.py\", \"     \\n\\t\\n\")\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 0)\n-\n-    def test_inline_comments_after_code(self):\n-        content = 'print(\"Hello\") # This is an inline comment'\n-        self.create_file(\"inline_comment_sample.py\", content)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 1)  # The comment shouldn't affect the count\n-\n-    def test_directory_structure(self):\n-        content1 = 'print(\"Hello from file1\")'\n-        content2 = 'print(\"Hello from file2\")'\n-        os.mkdir(os.path.join(self.test_dir, \"subdir\"))\n-        self.create_file(\"sample1.py\", content1)\n-        self.create_file(os.path.join(\"subdir\", \"sample2.py\"), content2)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 2)  # Both files should be counted\n-\n-    def test_normal_directory_name(self):\n-        content = 'print(\"Hello from a regular directory\")'\n-        os.makedirs(os.path.join(self.test_dir, \"some_test_dir\"))\n-        self.create_file(os.path.join(\"some_test_dir\", \"sample.py\"), content)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 1)  # Should count normally\n-\n-    def test_exclude_directory_name(self):\n-        content = 'print(\"Hello from an excluded directory\")'\n-        os.makedirs(os.path.join(self.test_dir, \"dir_test\"))\n-        self.create_file(os.path.join(\"dir_test\", \"sample.py\"), content)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 0)\n-        # Shouldn't count the file in dir_test due to the exclusion pattern\n-\n-    def test_verbose_output(self):\n-        content = 'print(\"Hello\")'\n-        self.create_file(\"sample.py\", content)\n-        original_stdout = sys.stdout\n-        sys.stdout = StringIO()\n-        count_loc(self.test_dir, verbose=1)\n-        output = sys.stdout.getvalue()\n-        sys.stdout = original_stdout\n-        self.assertIn(\"Count LoCs in\", output)\n-\n-    def test_multiline_string_same_line(self):\n-        content = '''\"\"\"This is a multiline string ending on the same line\"\"\"\n-        print(\"Outside string\")'''\n-        self.create_file(\"same_line_multiline.py\", content)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 1)  # Only the print statement should count\n-\n-    def test_multiline_string_ends_on_same_line(self):\n-        content = '\"\"\"a multiline string end on same line\"\"\"\\nprint(\"Outstr\")'\n-        self.create_file(\"same_line_multiline.py\", content)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 1)  # Only the print statement should count\n-\n-    def test_multiline_string_ends_in_middle_of_line(self):\n-        content = '''print(\"Start\")\n-        \"\"\"This is a multiline string ending in the middle of a line\"\"\"\n-        \"\"\"This is another multiline string.\"\"\"\n-        print(\"End\")'''\n-        self.create_file(\"multiline_in_middle.py\", content)\n-        loc = count_loc(self.test_dir)\n-        self.assertEqual(loc, 2)  # Both print statements should count\n-\n-    def test_line_starting_with_triple_quotes_not_ending(self):\n-        content = '\"\"\"\\nThis is a multiline string\\n'\n-        self.create_file(\"test_file_2.py\", content)\n-        path = os.path.join(self.test_dir, \"test_file_2.py\")\n-        self.assertEqual(count_loc(path), 0)\n-        # Because it's part of a multiline string\n-\n-    def test_line_starting_and_ending_with_triple_quotes(self):\n-        content = '\"\"\"This is a one-liner docstring.\"\"\"\\n'\n-        self.create_file(\"test_file_3.py\", content)\n-        path = os.path.join(self.test_dir, \"test_file_3.py\")\n-        self.assertEqual(count_loc(path), 0)\n-        # This is still considered a comment/docstring\n-\n-    def test_string_open_true_line_starting_with_triple_quotes(self):\n-        content = '\"\"\"\\nEnd of the multiline string.\"\"\"\\n'\n-        self.create_file(\"test_file_4.py\", content)\n-        path = os.path.join(self.test_dir, \"test_file_4.py\")\n-        self.assertEqual(count_loc(path), 0)\n-        # Entire content is a multiline string/comment\n\n@@ -1,20 +1,19 @@\n from keras_core import backend\n from keras_core import ops\n \n-DTYPE_TO_SIZE = {\n-    **{f\"float{i}\": i for i in (16, 32, 64)},\n-    **{f\"int{i}\": i for i in (8, 16, 32, 64)},\n-    **{f\"uint{i}\": i for i in (8, 16, 32, 64)},\n-    \"bfloat16\": 16,\n-    \"bool\": 1,\n-}\n-\n \n def dtype_size(dtype):\n-    size = DTYPE_TO_SIZE.get(dtype, None)\n-    if size is None:\n+    if dtype in (\"bfloat16\", \"float16\"):\n+        return 16\n+    if dtype in (\"float32\", \"int32\"):\n+        return 32\n+    if dtype in (\"float64\", \"int64\"):\n+        return 64\n+    if dtype == \"uint8\":\n+        return 8\n+    if dtype == \"bool\":\n+        return 1\n     raise ValueError(f\"Invalid dtype: {dtype}\")\n-    return size\n \n \n def is_float(dtype):\n\n@@ -1,57 +0,0 @@\n-from unittest.mock import patch\n-\n-from keras_core.testing import test_case\n-from keras_core.utils import io_utils\n-\n-\n-class TestIoUtils(test_case.TestCase):\n-    def test_enable_interactive_logging(self):\n-        io_utils.enable_interactive_logging()\n-        self.assertTrue(io_utils.is_interactive_logging_enabled())\n-\n-    def test_disable_interactive_logging(self):\n-        io_utils.disable_interactive_logging()\n-        self.assertFalse(io_utils.is_interactive_logging_enabled())\n-\n-    def test_set_logging_verbosity_valid(self):\n-        valid_levels = [\"FATAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\"]\n-        for level in valid_levels:\n-            io_utils.set_logging_verbosity(level)\n-\n-    def test_set_logging_verbosity_invalid(self):\n-        with self.assertRaises(ValueError):\n-            io_utils.set_logging_verbosity(\"INVALID\")\n-\n-    @patch(\"builtins.input\", side_effect=[\"y\"])\n-    def test_ask_to_proceed_with_overwrite_yes(self, _):\n-        self.assertTrue(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n-\n-    @patch(\"builtins.input\", side_effect=[\"n\"])\n-    def test_ask_to_proceed_with_overwrite_no(self, _):\n-        self.assertFalse(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n-\n-    @patch(\"sys.stdout.write\")\n-    def test_print_msg_interactive_with_line_break(self, mock_write):\n-        io_utils.enable_interactive_logging()\n-        io_utils.print_msg(\"Hello\", line_break=True)\n-        mock_write.assert_called_once_with(\"Hello\\n\")\n-\n-    @patch(\"sys.stdout.write\")\n-    def test_print_msg_interactive_without_line_break(self, mock_write):\n-        io_utils.enable_interactive_logging()\n-        io_utils.print_msg(\"Hello\", line_break=False)\n-        mock_write.assert_called_once_with(\"Hello\")\n-\n-    @patch(\"absl.logging.info\")\n-    def test_print_msg_non_interactive(self, mock_logging):\n-        io_utils.disable_interactive_logging()\n-        io_utils.print_msg(\"Hello\")\n-        mock_logging.assert_called_once_with(\"Hello\")\n-\n-    @patch(\"builtins.input\", side_effect=[\"invalid\", \"invalid\", \"y\"])\n-    def test_ask_to_proceed_with_overwrite_invalid_then_yes(self, _):\n-        self.assertTrue(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n-\n-    @patch(\"builtins.input\", side_effect=[\"invalid\", \"n\"])\n-    def test_ask_to_proceed_with_overwrite_invalid_then_no(self, _):\n-        self.assertFalse(io_utils.ask_to_proceed_with_overwrite(\"test_path\"))\n\n@@ -3,11 +3,6 @@ from keras_core.utils import naming\n \n \n class NamingUtilsTest(test_case.TestCase):\n-    def test_uniquify_unique_name(self):\n-        name = \"the_unique_name\"\n-        unique_name = naming.uniquify(name)\n-        self.assertEqual(unique_name, name)\n-\n     def test_auto_name(self):\n         self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name\")\n         self.assertEqual(naming.auto_name(\"unique_name\"), \"unique_name_1\")\n@@ -17,97 +12,3 @@ class NamingUtilsTest(test_case.TestCase):\n         self.assertEqual(naming.get_uid(\"very_unique_name\"), 1)\n         self.assertEqual(naming.get_uid(\"very_unique_name\"), 2)\n         self.assertEqual(naming.get_uid(\"very_unique_name\"), 3)\n-\n-    def test_uniquify_non_unique_name(self):\n-        name = \"non_unique_name\"\n-        naming.uniquify(name)\n-        unique_name = naming.uniquify(name)\n-        self.assertEqual(unique_name, name + \"_1\")\n-\n-    def test_to_snake_case_snake_case_name(self):\n-        name = \"snake_case_name\"\n-        snake_case_name = naming.to_snake_case(name)\n-        self.assertEqual(snake_case_name, name)\n-\n-    def test_get_uid_existing_prefix(self):\n-        prefix = \"existing_prefix\"\n-        naming.get_uid(prefix)\n-        uid = naming.get_uid(prefix)\n-        self.assertEqual(uid, 2)\n-\n-    def test_reset_uids(self):\n-        naming.get_uid(\"unique_name\")\n-        naming.reset_uids()\n-        uid = naming.get_uid(\"unique_name\")\n-        self.assertEqual(uid, 1)\n-\n-    def test_get_object_name_no_name_attribute(self):\n-        class ObjectWithoutName:\n-            __name__ = \"ObjectWithoutName\"\n-\n-        obj = ObjectWithoutName()\n-        object_name = naming.get_object_name(obj)\n-        self.assertEqual(object_name, \"object_without_name\")\n-\n-    def test_get_object_name_no_name_or_class_attribute(self):\n-        class ObjectWithoutNameOrClass:\n-            pass\n-\n-        obj = ObjectWithoutNameOrClass()\n-        object_name = naming.get_object_name(obj)\n-        self.assertEqual(object_name, \"object_without_name_or_class\")\n-\n-    def test_uniquify_already_uniquified_name(self):\n-        name = \"unique_name\"\n-        unique_name = naming.uniquify(name)\n-        new_unique_name = naming.uniquify(unique_name)\n-        self.assertEqual(new_unique_name, unique_name)\n-\n-    def test_to_snake_case_capital_after_any_character(self):\n-        name = \"myVariableNameHere\"\n-        snake_case_name = naming.to_snake_case(name)\n-        self.assertEqual(snake_case_name, \"my_variable_name_here\")\n-\n-    def test_to_snake_case_lower_before_upper(self):\n-        name = \"convertTHIS\"\n-        snake_case_name = naming.to_snake_case(name)\n-        self.assertEqual(snake_case_name, \"convert_this\")\n-\n-    def test_to_snake_case_already_snake_cased(self):\n-        name = \"already_snake_cased\"\n-        snake_case_name = naming.to_snake_case(name)\n-        self.assertEqual(snake_case_name, name)\n-\n-    def test_to_snake_case_no_changes(self):\n-        name = \"lowercase\"\n-        snake_case_name = naming.to_snake_case(name)\n-        self.assertEqual(snake_case_name, name)\n-\n-    def test_to_snake_case_single_uppercase_word(self):\n-        name = \"UPPERCASE\"\n-        snake_case_name = naming.to_snake_case(name)\n-        self.assertEqual(snake_case_name, \"uppercase\")\n-\n-    def test_get_object_name_for_keras_objects(self):\n-        class MockKerasObject:\n-            name = \"mock_object\"\n-\n-        obj = MockKerasObject()\n-        result = naming.get_object_name(obj)\n-        self.assertEqual(\n-            result, \"mock_object\", f\"Expected 'mock_object' but got {result}\"\n-        )\n-\n-    # Test for function objects that have a `__name__` attribute.\n-    def test_get_object_name_for_functions(self):\n-        def mock_function():\n-            pass\n-\n-        result = naming.get_object_name(mock_function)\n-        # Assumes to_snake_case works correctly.\n-        expected_name = naming.to_snake_case(mock_function.__name__)\n-        self.assertEqual(\n-            result,\n-            expected_name,\n-            f\"Expected '{expected_name}' but got {result}\",\n-        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
