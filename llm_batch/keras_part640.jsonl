{"custom_id": "keras#bd56569ccf5c31c373569626b65461a04136de95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 569 | Contributors (this commit): 6 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -471,6 +471,8 @@ def tan(x):\n \n \n def tensordot(x1, x2, axes=2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n     return jnp.tensordot(x1, x2, axes=axes)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4a2bf1329eefba50bfc477a4539078f50cf8598", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 226 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 226 | Churn Cumulative: 226 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,226 @@\n+\"\"\"\n+Title: Timeseries classification from scratch\n+Author: [hfawaz](https://github.com/hfawaz/)\n+Date created: 2020/07/21\n+Last modified: 2021/07/16\n+Description: Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example shows how to do timeseries classification from scratch, starting from raw\n+CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n+[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/).\n+\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\n+\"\"\"\n+import keras_core as keras\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+\"\"\"\n+## Load the data: the FordA dataset\n+\n+### Dataset description\n+\n+The dataset we are using here is called FordA.\n+The data comes from the UCR archive.\n+The dataset contains 3601 training instances and another 1320 testing instances.\n+Each timeseries corresponds to a measurement of engine noise captured by a motor sensor.\n+For this task, the goal is to automatically detect the presence of a specific issue with\n+the engine. The problem is a balanced binary classification task. The full description of\n+this dataset can be found [here](http://www.j-wichard.de/publications/FordPaper.pdf).\n+\n+### Read the TSV data\n+\n+We will use the `FordA_TRAIN` file for training and the\n+`FordA_TEST` file for testing. The simplicity of this dataset\n+allows us to demonstrate effectively how to use ConvNets for timeseries classification.\n+In this file, the first column corresponds to the label.\n+\"\"\"\n+\n+\n+def readucr(filename):\n+    data = np.loadtxt(filename, delimiter=\"\\t\")\n+    y = data[:, 0]\n+    x = data[:, 1:]\n+    return x, y.astype(int)\n+\n+\n+root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n+\n+x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n+x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n+\n+\"\"\"\n+## Visualize the data\n+\n+Here we visualize one timeseries example for each class in the dataset.\n+\n+\"\"\"\n+\n+classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n+\n+plt.figure()\n+for c in classes:\n+    c_x_train = x_train[y_train == c]\n+    plt.plot(c_x_train[0], label=\"class \" + str(c))\n+plt.legend(loc=\"best\")\n+plt.show()\n+plt.close()\n+\n+\"\"\"\n+## Standardize the data\n+\n+Our timeseries are already in a single length (500). However, their values are\n+usually in various ranges. This is not ideal for a neural network;\n+in general we should seek to make the input values normalized.\n+For this specific dataset, the data is already z-normalized: each timeseries sample\n+has a mean equal to zero and a standard deviation equal to one. This type of\n+normalization is very common for timeseries classification problems, see\n+[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).\n+\n+Note that the timeseries data used here are univariate, meaning we only have one channel\n+per timeseries example.\n+We will therefore transform the timeseries into a multivariate one with one channel\n+using a simple reshaping via numpy.\n+This will allow us to construct a model that is easily applicable to multivariate time\n+series.\n+\"\"\"\n+\n+x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n+x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n+\n+\"\"\"\n+Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\n+the number of classes beforehand.\n+\"\"\"\n+\n+num_classes = len(np.unique(y_train))\n+\n+\"\"\"\n+Now we shuffle the training set because we will be using the `validation_split` option\n+later when training.\n+\"\"\"\n+\n+idx = np.random.permutation(len(x_train))\n+x_train = x_train[idx]\n+y_train = y_train[idx]\n+\n+\"\"\"\n+Standardize the labels to positive integers.\n+The expected labels will then be 0 and 1.\n+\"\"\"\n+\n+y_train[y_train == -1] = 0\n+y_test[y_test == -1] = 0\n+\n+\"\"\"\n+## Build a model\n+\n+We build a Fully Convolutional Neural Network originally proposed in\n+[this paper](https://arxiv.org/abs/1611.06455).\n+The implementation is based on the TF 2 version provided\n+[here](https://github.com/hfawaz/dl-4-tsc/).\n+The following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found\n+via random search using [KerasTuner](https://github.com/keras-team/keras-tuner).\n+\n+\"\"\"\n+\n+\n+def make_model(input_shape):\n+    input_layer = keras.layers.Input(input_shape)\n+\n+    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n+    conv1 = keras.layers.BatchNormalization()(conv1)\n+    conv1 = keras.layers.ReLU()(conv1)\n+\n+    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n+    conv2 = keras.layers.BatchNormalization()(conv2)\n+    conv2 = keras.layers.ReLU()(conv2)\n+\n+    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n+    conv3 = keras.layers.BatchNormalization()(conv3)\n+    conv3 = keras.layers.ReLU()(conv3)\n+\n+    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n+\n+    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n+\n+    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n+\n+\n+model = make_model(input_shape=x_train.shape[1:])\n+keras.utils.plot_model(model, show_shapes=True)\n+\n+\"\"\"\n+## Train the model\n+\n+\"\"\"\n+\n+epochs = 500\n+batch_size = 32\n+\n+callbacks = [\n+    keras.callbacks.ModelCheckpoint(\n+        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n+    ),\n+    keras.callbacks.ReduceLROnPlateau(\n+        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n+    ),\n+    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n+]\n+model.compile(\n+    optimizer=\"adam\",\n+    loss=\"sparse_categorical_crossentropy\",\n+    metrics=[\"sparse_categorical_accuracy\"]\n+)\n+history = model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=batch_size,\n+    epochs=epochs,\n+    callbacks=callbacks,\n+    validation_split=0.2,\n+    verbose=1,\n+)\n+\n+\"\"\"\n+## Evaluate model on test data\n+\"\"\"\n+\n+model = keras.models.load_model(\"best_model.keras\")\n+\n+test_loss, test_acc = model.evaluate(x_test, y_test)\n+\n+print(\"Test accuracy\", test_acc)\n+print(\"Test loss\", test_loss)\n+\n+\"\"\"\n+## Plot the model's training and validation loss\n+\"\"\"\n+\n+metric = \"sparse_categorical_accuracy\"\n+plt.figure()\n+plt.plot(history.history[metric])\n+plt.plot(history.history[\"val_\" + metric])\n+plt.title(\"model \" + metric)\n+plt.ylabel(metric, fontsize=\"large\")\n+plt.xlabel(\"epoch\", fontsize=\"large\")\n+plt.legend([\"train\", \"val\"], loc=\"best\")\n+plt.show()\n+plt.close()\n+\n+\"\"\"\n+We can see how the training accuracy reaches almost 0.95 after 100 epochs.\n+However, by observing the validation accuracy we can see how the network still needs\n+training until it reaches almost 0.97 for both the validation and the training accuracy\n+after 200 epochs. Beyond the 200th epoch, if we continue on training, the validation\n+accuracy will start decreasing while the training accuracy will continue on increasing:\n+the model starts overfitting.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d4ad236f2c7b0c98ce8045c353a3ada7692e0ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 2301 | Files Changed: 12 | Hunks: 17 | Methods Changed: 54 | Complexity Δ (Sum/Max): -46/10 | Churn Δ: 2426 | Churn Cumulative: 7054 | Contributors (this commit): 12 | Commits (past 90d): 73 | Contributors (cumulative): 43 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,342 +0,0 @@\n-\"\"\"\n-Title: OCR model for reading Captchas\n-Author: [A_K_Nain](https://twitter.com/A_K_Nain)\n-Date created: 2020/06/14\n-Last modified: 2020/06/26\n-Description: How to implement an OCR model using CNNs, RNNs and CTC loss.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-This example demonstrates a simple OCR model built with the Functional API. Apart from\n-combining CNN and RNN, it also illustrates how you can instantiate a new layer\n-and use it as an \"Endpoint layer\" for implementing CTC loss. For a detailed\n-guide to layer subclassing, please check out\n-[this page](https://keras.io/guides/making_new_layers_and_models_via_subclassing/)\n-in the developer guides.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import os\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-from pathlib import Path\n-from collections import Counter\n-\n-import tensorflow as tf\n-from tensorflow import keras\n-from tensorflow.keras import layers\n-\n-\n-\"\"\"\n-## Load the data: [Captcha Images](https://www.kaggle.com/fournierp/captcha-version-2-images)\n-Let's download the data.\n-\"\"\"\n-\n-\n-\"\"\"shell\n-curl -LO https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip\n-unzip -qq captcha_images_v2.zip\n-\"\"\"\n-\n-\n-\"\"\"\n-The dataset contains 1040 captcha files as `png` images. The label for each sample is a string,\n-the name of the file (minus the file extension).\n-We will map each character in the string to an integer for training the model. Similary,\n-we will need to map the predictions of the model back to strings. For this purpose\n-we will maintain two dictionaries, mapping characters to integers, and integers to characters,\n-respectively.\n-\"\"\"\n-\n-\n-# Path to the data directory\n-data_dir = Path(\"./captcha_images_v2/\")\n-\n-# Get list of all the images\n-images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n-labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n-characters = set(char for label in labels for char in label)\n-characters = sorted(list(characters))\n-\n-print(\"Number of images found: \", len(images))\n-print(\"Number of labels found: \", len(labels))\n-print(\"Number of unique characters: \", len(characters))\n-print(\"Characters present: \", characters)\n-\n-# Batch size for training and validation\n-batch_size = 16\n-\n-# Desired image dimensions\n-img_width = 200\n-img_height = 50\n-\n-# Factor by which the image is going to be downsampled\n-# by the convolutional blocks. We will be using two\n-# convolution blocks and each block will have\n-# a pooling layer which downsample the features by a factor of 2.\n-# Hence total downsampling factor would be 4.\n-downsample_factor = 4\n-\n-# Maximum length of any captcha in the dataset\n-max_length = max([len(label) for label in labels])\n-\n-\n-\"\"\"\n-## Preprocessing\n-\"\"\"\n-\n-\n-# Mapping characters to integers\n-char_to_num = layers.StringLookup(vocabulary=list(characters), mask_token=None)\n-\n-# Mapping integers back to original characters\n-num_to_char = layers.StringLookup(\n-    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n-)\n-\n-\n-def split_data(images, labels, train_size=0.9, shuffle=True):\n-    # 1. Get the total size of the dataset\n-    size = len(images)\n-    # 2. Make an indices array and shuffle it, if required\n-    indices = np.arange(size)\n-    if shuffle:\n-        np.random.shuffle(indices)\n-    # 3. Get the size of training samples\n-    train_samples = int(size * train_size)\n-    # 4. Split data into training and validation sets\n-    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n-    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n-    return x_train, x_valid, y_train, y_valid\n-\n-\n-# Splitting data into training and validation sets\n-x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n-\n-\n-def encode_single_sample(img_path, label):\n-    # 1. Read image\n-    img = tf.io.read_file(img_path)\n-    # 2. Decode and convert to grayscale\n-    img = tf.io.decode_png(img, channels=1)\n-    # 3. Convert to float32 in [0, 1] range\n-    img = tf.image.convert_image_dtype(img, tf.float32)\n-    # 4. Resize to the desired size\n-    img = tf.image.resize(img, [img_height, img_width])\n-    # 5. Transpose the image because we want the time\n-    # dimension to correspond to the width of the image.\n-    img = tf.transpose(img, perm=[1, 0, 2])\n-    # 6. Map the characters in label to numbers\n-    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n-    # 7. Return a dict as our model is expecting two inputs\n-    return {\"image\": img, \"label\": label}\n-\n-\n-\"\"\"\n-## Create `Dataset` objects\n-\"\"\"\n-\n-\n-train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n-train_dataset = (\n-    train_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n-    .batch(batch_size)\n-    .prefetch(buffer_size=tf.data.AUTOTUNE)\n-)\n-\n-validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n-validation_dataset = (\n-    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n-    .batch(batch_size)\n-    .prefetch(buffer_size=tf.data.AUTOTUNE)\n-)\n-\n-\"\"\"\n-## Visualize the data\n-\"\"\"\n-\n-\n-_, ax = plt.subplots(4, 4, figsize=(10, 5))\n-for batch in train_dataset.take(1):\n-    images = batch[\"image\"]\n-    labels = batch[\"label\"]\n-    for i in range(16):\n-        img = (images[i] * 255).numpy().astype(\"uint8\")\n-        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n-        ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n-        ax[i // 4, i % 4].set_title(label)\n-        ax[i // 4, i % 4].axis(\"off\")\n-plt.show()\n-\n-\"\"\"\n-## Model\n-\"\"\"\n-\n-\n-class CTCLayer(layers.Layer):\n-    def __init__(self, name=None):\n-        super().__init__(name=name)\n-        self.loss_fn = keras.backend.ctc_batch_cost\n-\n-    def call(self, y_true, y_pred):\n-        # Compute the training-time loss value and add it\n-        # to the layer using `self.add_loss()`.\n-        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n-        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n-        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n-\n-        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n-        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n-\n-        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n-        self.add_loss(loss)\n-\n-        # At test time, just return the computed predictions\n-        return y_pred\n-\n-\n-def build_model():\n-    # Inputs to the model\n-    input_img = layers.Input(\n-        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n-    )\n-    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n-\n-    # First conv block\n-    x = layers.Conv2D(\n-        32,\n-        (3, 3),\n-        activation=\"relu\",\n-        kernel_initializer=\"he_normal\",\n-        padding=\"same\",\n-        name=\"Conv1\",\n-    )(input_img)\n-    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n-\n-    # Second conv block\n-    x = layers.Conv2D(\n-        64,\n-        (3, 3),\n-        activation=\"relu\",\n-        kernel_initializer=\"he_normal\",\n-        padding=\"same\",\n-        name=\"Conv2\",\n-    )(x)\n-    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n-\n-    # We have used two max pool with pool size and strides 2.\n-    # Hence, downsampled feature maps are 4x smaller. The number of\n-    # filters in the last layer is 64. Reshape accordingly before\n-    # passing the output to the RNN part of the model\n-    new_shape = ((img_width // 4), (img_height // 4) * 64)\n-    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n-    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n-    x = layers.Dropout(0.2)(x)\n-\n-    # RNNs\n-    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n-    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n-\n-    # Output layer\n-    x = layers.Dense(\n-        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n-    )(x)\n-\n-    # Add CTC layer for calculating CTC loss at each step\n-    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n-\n-    # Define the model\n-    model = keras.models.Model(\n-        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n-    )\n-    # Optimizer\n-    opt = keras.optimizers.Adam()\n-    # Compile the model and return\n-    model.compile(optimizer=opt)\n-    return model\n-\n-\n-# Get the model\n-model = build_model()\n-model.summary()\n-\n-\"\"\"\n-## Training\n-\"\"\"\n-\n-\n-epochs = 1\n-early_stopping_patience = 10\n-# Add early stopping\n-early_stopping = keras.callbacks.EarlyStopping(\n-    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n-)\n-\n-# Train the model\n-history = model.fit(\n-    train_dataset,\n-    validation_data=validation_dataset,\n-    epochs=epochs,\n-    callbacks=[early_stopping],\n-)\n-\n-\n-\"\"\"\n-## Inference\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/ocr-for-captcha)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/ocr-for-captcha).\n-\"\"\"\n-\n-\n-# Get the prediction model by extracting layers till the output layer\n-prediction_model = keras.models.Model(\n-    model.get_layer(name=\"image\").input, model.get_layer(name=\"dense2\").output\n-)\n-prediction_model.summary()\n-\n-\n-# A utility function to decode the output of the network\n-def decode_batch_predictions(pred):\n-    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n-    # Use greedy search. For complex tasks, you can use beam search\n-    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n-        :, :max_length\n-    ]\n-    # Iterate over the results and get back the text\n-    output_text = []\n-    for res in results:\n-        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n-        output_text.append(res)\n-    return output_text\n-\n-\n-#  Let's check results on some validation samples\n-for batch in validation_dataset.take(1):\n-    batch_images = batch[\"image\"]\n-    batch_labels = batch[\"label\"]\n-\n-    preds = prediction_model.predict(batch_images)\n-    pred_texts = decode_batch_predictions(preds)\n-\n-    orig_texts = []\n-    for label in batch_labels:\n-        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n-        orig_texts.append(label)\n-\n-    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n-    for i in range(len(pred_texts)):\n-        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n-        img = img.T\n-        title = f\"Prediction: {pred_texts[i]}\"\n-        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n-        ax[i // 4, i % 4].set_title(title)\n-        ax[i // 4, i % 4].axis(\"off\")\n-plt.show()\n\\ No newline at end of file\n\n@@ -1,503 +0,0 @@\n-\"\"\"\n-Title: Object detection with Vision Transformers\n-Author: [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/)\n-Date created: 2022/03/27\n-Last modified: 2022/03/27\n-Description: A simple Keras implementation of object detection using Vision Transformers.\n-Accelerator: GPU\n-\"\"\"\n-\n-\"\"\"\n-## Introduction\n-\n-The article\n-[Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n-architecture by Alexey Dosovitskiy et al.\n-demonstrates that a pure transformer applied directly to sequences of image\n-patches can perform well on object detection tasks.\n-\n-In this Keras example, we implement an object detection ViT\n-and we train it on the\n-[Caltech 101 dataset](http://www.vision.caltech.edu/datasets/)\n-to detect an airplane in the given image.\n-\n-This example requires TensorFlow 2.4 or higher.\n-\"\"\"\n-\n-\"\"\"\n-## Imports and setup\n-\"\"\"\n-\n-import numpy as np\n-import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import cv2\n-import os\n-import scipy.io\n-import shutil\n-\n-\"\"\"\n-## Prepare dataset\n-\n-We use the [Caltech 101 Dataset](https://data.caltech.edu/records/mzrjq-6wc02).\n-\"\"\"\n-\n-# Path to images and annotations\n-path_images = \"./101_ObjectCategories/airplanes/\"\n-path_annot = \"./Annotations/Airplanes_Side_2/\"\n-\n-path_to_downloaded_file = keras.utils.get_file(\n-    fname=\"caltech_101_zipped\",\n-    origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",\n-    extract=True,\n-    archive_format=\"zip\",  # downloaded file format\n-    cache_dir=\"./\",  # cache and extract in current directory\n-)\n-\n-# Extracting tar files found inside main zip file\n-shutil.unpack_archive(\"./datasets/caltech-101/101_ObjectCategories.tar.gz\", \"./\")\n-shutil.unpack_archive(\"./datasets/caltech-101/Annotations.tar\", \"./\")\n-\n-# list of paths to images and annotations\n-image_paths = [\n-    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n-]\n-annot_paths = [\n-    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n-]\n-\n-image_paths.sort()\n-annot_paths.sort()\n-\n-image_size = 224  # resize input images to this size\n-\n-images, targets = [], []\n-\n-# loop over the annotations and images, preprocess them and store in lists\n-for i in range(0, len(annot_paths)):\n-    # Access bounding box coordinates\n-    annot = scipy.io.loadmat(path_annot + annot_paths[i])[\"box_coord\"][0]\n-\n-    top_left_x, top_left_y = annot[2], annot[0]\n-    bottom_right_x, bottom_right_y = annot[3], annot[1]\n-\n-    image = keras.utils.load_img(\n-        path_images + image_paths[i],\n-    )\n-    (w, h) = image.size[:2]\n-\n-    # resize train set images\n-    if i < int(len(annot_paths) * 0.8):\n-        # resize image if it is for training dataset\n-        image = image.resize((image_size, image_size))\n-\n-    # convert image to array and append to list\n-    images.append(keras.utils.img_to_array(image))\n-\n-    # apply relative scaling to bounding boxes as per given image and append to list\n-    targets.append(\n-        (\n-            float(top_left_x) / w,\n-            float(top_left_y) / h,\n-            float(bottom_right_x) / w,\n-            float(bottom_right_y) / h,\n-        )\n-    )\n-\n-# Convert the list to numpy array, split to train and test dataset\n-(x_train), (y_train) = (\n-    np.asarray(images[: int(len(images) * 0.8)]),\n-    np.asarray(targets[: int(len(targets) * 0.8)]),\n-)\n-(x_test), (y_test) = (\n-    np.asarray(images[int(len(images) * 0.8) :]),\n-    np.asarray(targets[int(len(targets) * 0.8) :]),\n-)\n-\n-\"\"\"\n-## Implement multilayer-perceptron (MLP)\n-\n-We use the code from the Keras example\n-[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/)\n-as a reference.\n-\"\"\"\n-\n-\n-def mlp(x, hidden_units, dropout_rate):\n-    for units in hidden_units:\n-        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n-        x = layers.Dropout(dropout_rate)(x)\n-    return x\n-\n-\n-\"\"\"\n-## Implement the patch creation layer\n-\"\"\"\n-\n-\n-class Patches(layers.Layer):\n-    def __init__(self, patch_size):\n-        super().__init__()\n-        self.patch_size = patch_size\n-\n-    #     Override function to avoid error while saving model\n-    def get_config(self):\n-        config = super().get_config().copy()\n-        config.update(\n-            {\n-                \"input_shape\": input_shape,\n-                \"patch_size\": patch_size,\n-                \"num_patches\": num_patches,\n-                \"projection_dim\": projection_dim,\n-                \"num_heads\": num_heads,\n-                \"transformer_units\": transformer_units,\n-                \"transformer_layers\": transformer_layers,\n-                \"mlp_head_units\": mlp_head_units,\n-            }\n-        )\n-        return config\n-\n-    def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=[1, self.patch_size, self.patch_size, 1],\n-            strides=[1, self.patch_size, self.patch_size, 1],\n-            rates=[1, 1, 1, 1],\n-            padding=\"VALID\",\n-        )\n-        # return patches\n-        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])\n-\n-\n-\"\"\"\n-## Display patches for an input image\n-\"\"\"\n-\n-patch_size = 32  # Size of the patches to be extracted from the input images\n-\n-plt.figure(figsize=(4, 4))\n-plt.imshow(x_train[0].astype(\"uint8\"))\n-plt.axis(\"off\")\n-\n-patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n-print(f\"Image size: {image_size} X {image_size}\")\n-print(f\"Patch size: {patch_size} X {patch_size}\")\n-print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n-\n-\n-n = int(np.sqrt(patches.shape[1]))\n-plt.figure(figsize=(4, 4))\n-for i, patch in enumerate(patches[0]):\n-    ax = plt.subplot(n, n, i + 1)\n-    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n-    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n-    plt.axis(\"off\")\n-\n-\"\"\"\n-## Implement the patch encoding layer\n-\n-The `PatchEncoder` layer linearly transforms a patch by projecting it into a\n-vector of size `projection_dim`. It also adds a learnable position\n-embedding to the projected vector.\n-\"\"\"\n-\n-\n-class PatchEncoder(layers.Layer):\n-    def __init__(self, num_patches, projection_dim):\n-        super().__init__()\n-        self.num_patches = num_patches\n-        self.projection = layers.Dense(units=projection_dim)\n-        self.position_embedding = layers.Embedding(\n-            input_dim=num_patches, output_dim=projection_dim\n-        )\n-\n-    # Override function to avoid error while saving model\n-    def get_config(self):\n-        config = super().get_config().copy()\n-        config.update(\n-            {\n-                \"input_shape\": input_shape,\n-                \"patch_size\": patch_size,\n-                \"num_patches\": num_patches,\n-                \"projection_dim\": projection_dim,\n-                \"num_heads\": num_heads,\n-                \"transformer_units\": transformer_units,\n-                \"transformer_layers\": transformer_layers,\n-                \"mlp_head_units\": mlp_head_units,\n-            }\n-        )\n-        return config\n-\n-    def call(self, patch):\n-        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n-        encoded = self.projection(patch) + self.position_embedding(positions)\n-        return encoded\n-\n-\n-\"\"\"\n-## Build the ViT model\n-\n-The ViT model has multiple Transformer blocks.\n-The `MultiHeadAttention` layer is used for self-attention,\n-applied to the sequence of image patches. The encoded patches (skip connection)\n-and self-attention layer outputs are normalized and fed into a\n-multilayer perceptron (MLP).\n-The model outputs four dimensions representing\n-the bounding box coordinates of an object.\n-\"\"\"\n-\n-\n-def create_vit_object_detector(\n-    input_shape,\n-    patch_size,\n-    num_patches,\n-    projection_dim,\n-    num_heads,\n-    transformer_units,\n-    transformer_layers,\n-    mlp_head_units,\n-):\n-    inputs = layers.Input(shape=input_shape)\n-    # Create patches\n-    patches = Patches(patch_size)(inputs)\n-    # Encode patches\n-    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n-\n-    # Create multiple layers of the Transformer block.\n-    for _ in range(transformer_layers):\n-        # Layer normalization 1.\n-        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n-        # Create a multi-head attention layer.\n-        attention_output = layers.MultiHeadAttention(\n-            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n-        )(x1, x1)\n-        # Skip connection 1.\n-        x2 = layers.Add()([attention_output, encoded_patches])\n-        # Layer normalization 2.\n-        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n-        # MLP\n-        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n-        # Skip connection 2.\n-        encoded_patches = layers.Add()([x3, x2])\n-\n-    # Create a [batch_size, projection_dim] tensor.\n-    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n-    representation = layers.Flatten()(representation)\n-    representation = layers.Dropout(0.3)(representation)\n-    # Add MLP.\n-    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n-\n-    bounding_box = layers.Dense(4)(\n-        features\n-    )  # Final four neurons that output bounding box\n-\n-    # return Keras model.\n-    return keras.Model(inputs=inputs, outputs=bounding_box)\n-\n-\n-\"\"\"\n-## Run the experiment\n-\"\"\"\n-\n-\n-def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n-    optimizer = keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    )\n-\n-    # Compile model.\n-    model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n-\n-    checkpoint_filepath = \"logs/model.weights.h5\"\n-    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n-        checkpoint_filepath,\n-        monitor=\"val_loss\",\n-        save_best_only=True,\n-        save_weights_only=True,\n-    )\n-\n-    history = model.fit(\n-        x=x_train,\n-        y=y_train,\n-        batch_size=batch_size,\n-        epochs=num_epochs,\n-        validation_split=0.1,\n-        callbacks=[\n-            checkpoint_callback,\n-            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10),\n-        ],\n-    )\n-\n-    return history\n-\n-\n-input_shape = (image_size, image_size, 3)  # input image shape\n-learning_rate = 0.001\n-weight_decay = 0.0001\n-batch_size = 32\n-num_epochs = 1\n-num_patches = (image_size // patch_size) ** 2\n-projection_dim = 64\n-num_heads = 4\n-# Size of the transformer layers\n-transformer_units = [\n-    projection_dim * 2,\n-    projection_dim,\n-]\n-transformer_layers = 4\n-mlp_head_units = [2048, 1024, 512, 64, 32]  # Size of the dense layers\n-\n-\n-history = []\n-num_patches = (image_size // patch_size) ** 2\n-\n-vit_object_detector = create_vit_object_detector(\n-    input_shape,\n-    patch_size,\n-    num_patches,\n-    projection_dim,\n-    num_heads,\n-    transformer_units,\n-    transformer_layers,\n-    mlp_head_units,\n-)\n-\n-# Train model\n-history = run_experiment(\n-    vit_object_detector, learning_rate, weight_decay, batch_size, num_epochs\n-)\n-\n-\n-\"\"\"\n-## Evaluate the model\n-\"\"\"\n-\n-import matplotlib.patches as patches\n-\n-# Saves the model in current path\n-vit_object_detector.save(\"vit_object_detector.keras\", save_format=\"keras\")\n-\n-\n-# To calculate IoU (intersection over union, given two bounding boxes)\n-def bounding_box_intersection_over_union(box_predicted, box_truth):\n-    # get (x, y) coordinates of intersection of bounding boxes\n-    top_x_intersect = max(box_predicted[0], box_truth[0])\n-    top_y_intersect = max(box_predicted[1], box_truth[1])\n-    bottom_x_intersect = min(box_predicted[2], box_truth[2])\n-    bottom_y_intersect = min(box_predicted[3], box_truth[3])\n-\n-    # calculate area of the intersection bb (bounding box)\n-    intersection_area = max(0, bottom_x_intersect - top_x_intersect + 1) * max(\n-        0, bottom_y_intersect - top_y_intersect + 1\n-    )\n-\n-    # calculate area of the prediction bb and ground-truth bb\n-    box_predicted_area = (box_predicted[2] - box_predicted[0] + 1) * (\n-        box_predicted[3] - box_predicted[1] + 1\n-    )\n-    box_truth_area = (box_truth[2] - box_truth[0] + 1) * (\n-        box_truth[3] - box_truth[1] + 1\n-    )\n-\n-    # calculate intersection over union by taking intersection\n-    # area and dividing it by the sum of predicted bb and ground truth\n-    # bb areas subtracted by  the interesection area\n-\n-    # return ioU\n-    return intersection_area / float(\n-        box_predicted_area + box_truth_area - intersection_area\n-    )\n-\n-\n-i, mean_iou = 0, 0\n-\n-# Compare results for 10 images in the test set\n-for input_image in x_test[:10]:\n-    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 15))\n-    im = input_image\n-\n-    # Display the image\n-    ax1.imshow(im.astype(\"uint8\"))\n-    ax2.imshow(im.astype(\"uint8\"))\n-\n-    input_image = cv2.resize(\n-        input_image, (image_size, image_size), interpolation=cv2.INTER_AREA\n-    )\n-    input_image = np.expand_dims(input_image, axis=0)\n-    preds = vit_object_detector.predict(input_image)[0]\n-\n-    (h, w) = (im).shape[0:2]\n-\n-    top_left_x, top_left_y = int(preds[0] * w), int(preds[1] * h)\n-\n-    bottom_right_x, bottom_right_y = int(preds[2] * w), int(preds[3] * h)\n-\n-    box_predicted = [top_left_x, top_left_y, bottom_right_x, bottom_right_y]\n-    # Create the bounding box\n-    rect = patches.Rectangle(\n-        (top_left_x, top_left_y),\n-        bottom_right_x - top_left_x,\n-        bottom_right_y - top_left_y,\n-        facecolor=\"none\",\n-        edgecolor=\"red\",\n-        linewidth=1,\n-    )\n-    # Add the bounding box to the image\n-    ax1.add_patch(rect)\n-    ax1.set_xlabel(\n-        \"Predicted: \"\n-        + str(top_left_x)\n-        + \", \"\n-        + str(top_left_y)\n-        + \", \"\n-        + str(bottom_right_x)\n-        + \", \"\n-        + str(bottom_right_y)\n-    )\n-\n-    top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n-\n-    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n-\n-    box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n-\n-    mean_iou += bounding_box_intersection_over_union(box_predicted, box_truth)\n-    # Create the bounding box\n-    rect = patches.Rectangle(\n-        (top_left_x, top_left_y),\n-        bottom_right_x - top_left_x,\n-        bottom_right_y - top_left_y,\n-        facecolor=\"none\",\n-        edgecolor=\"red\",\n-        linewidth=1,\n-    )\n-    # Add the bounding box to the image\n-    ax2.add_patch(rect)\n-    ax2.set_xlabel(\n-        \"Target: \"\n-        + str(top_left_x)\n-        + \", \"\n-        + str(top_left_y)\n-        + \", \"\n-        + str(bottom_right_x)\n-        + \", \"\n-        + str(bottom_right_y)\n-        + \"\\n\"\n-        + \"IoU\"\n-        + str(bounding_box_intersection_over_union(box_predicted, box_truth))\n-    )\n-    i = i + 1\n-\n-print(\"mean_iou: \" + str(mean_iou / len(x_test[:10])))\n-plt.show()\n-\n-\"\"\"\n-This example demonstrates that a pure Transformer can be trained\n-to predict the bounding boxes of an object in a given image,\n-thus extending the use of Transformers to object detection tasks.\n-The model can be improved further by tuning hyper-parameters and pre-training.\n-\"\"\"\n\\ No newline at end of file\n\n@@ -1,673 +0,0 @@\n-\"\"\"\n-Title: Semi-supervised image classification using contrastive pretraining with SimCLR\n-Author: [András Béres](https://www.linkedin.com/in/andras-beres-789190210)\n-Date created: 2021/04/24\n-Last modified: 2021/04/24\n-Description: Contrastive pretraining with SimCLR for semi-supervised image classification on the STL-10 dataset.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-### Semi-supervised learning\n-\n-Semi-supervised learning is a machine learning paradigm that deals with\n-**partially labeled datasets**. When applying deep learning in the real world,\n-one usually has to gather a large dataset to make it work well. However, while\n-the cost of labeling scales linearly with the dataset size (labeling each\n-example takes a constant time), model performance only scales\n-[sublinearly](https://arxiv.org/abs/2001.08361) with it. This means that\n-labeling more and more samples becomes less and less cost-efficient, while\n-gathering unlabeled data is generally cheap, as it is usually readily available\n-in large quantities.\n-\n-Semi-supervised learning offers to solve this problem by only requiring a\n-partially labeled dataset, and by being label-efficient by utilizing the\n-unlabeled examples for learning as well.\n-\n-In this example, we will pretrain an encoder with contrastive learning on the\n-[STL-10](https://ai.stanford.edu/~acoates/stl10/) semi-supervised dataset using\n-no labels at all, and then fine-tune it using only its labeled subset.\n-\n-### Contrastive learning\n-\n-On the highest level, the main idea behind contrastive learning is to **learn\n-representations that are invariant to image augmentations** in a self-supervised\n-manner. One problem with this objective is that it has a trivial degenerate\n-solution: the case where the representations are constant, and do not depend at all on the\n-input images.\n-\n-Contrastive learning avoids this trap by modifying the objective in the\n-following way: it pulls representations of augmented versions/views of the same\n-image closer to each other (contracting positives), while simultaneously pushing\n-different images away from each other (contrasting negatives) in representation\n-space.\n-\n-One such contrastive approach is [SimCLR](https://arxiv.org/abs/2002.05709),\n-which essentially identifies the core components needed to optimize this\n-objective, and can achieve high performance by scaling this simple approach.\n-\n-Another approach is [SimSiam](https://arxiv.org/abs/2011.10566)\n-([Keras example](https://keras.io/examples/vision/simsiam/)),\n-whose main difference from\n-SimCLR is that the former does not use any negatives in its loss. Therefore, it does not\n-explicitly prevent the trivial solution, and, instead, avoids it implicitly by\n-architecture design (asymmetric encoding paths using a predictor network and\n-batch normalization (BatchNorm) are applied in the final layers).\n-\n-For further reading about SimCLR, check out\n-[the official Google AI blog post](https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html),\n-and for an overview of self-supervised learning across both vision and language\n-check out\n-[this blog post](https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/).\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-# Make sure we are able to handle large datasets\n-import resource\n-low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n-resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n-\n-import math\n-import matplotlib.pyplot as plt\n-import tensorflow as tf\n-import tensorflow_datasets as tfds\n-\n-import keras_core as keras\n-from keras_core import layers\n-\n-\"\"\"\n-## Hyperparameter setup\n-\"\"\"\n-# Dataset hyperparameters\n-unlabeled_dataset_size = 100000\n-labeled_dataset_size = 5000\n-image_size = 96\n-image_channels = 3\n-\n-# Algorithm hyperparameters\n-num_epochs = 1\n-batch_size = 525  # Corresponds to 200 steps per epoch\n-width = 128\n-temperature = 0.1\n-# Stronger augmentations for contrastive, weaker ones for supervised training\n-contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n-classification_augmentation = {\"min_area\": 0.75, \"brightness\": 0.3, \"jitter\": 0.1}\n-\n-\"\"\"\n-## Dataset\n-\n-During training we will simultaneously load a large batch of unlabeled images along with a\n-smaller batch of labeled images.\n-\"\"\"\n-\n-\n-def prepare_dataset():\n-    # Labeled and unlabeled samples are loaded synchronously\n-    # with batch sizes selected accordingly\n-    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n-    unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n-    labeled_batch_size = labeled_dataset_size // steps_per_epoch\n-    print(\n-        f\"batch size is {unlabeled_batch_size} (unlabeled) + {labeled_batch_size} (labeled)\"\n-    )\n-\n-    # Turning off shuffle to lower resource usage\n-    unlabeled_train_dataset = (\n-        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False)\n-        .shuffle(buffer_size=10 * unlabeled_batch_size)\n-        .batch(unlabeled_batch_size)\n-    )\n-    labeled_train_dataset = (\n-        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=False)\n-        .shuffle(buffer_size=10 * labeled_batch_size)\n-        .batch(labeled_batch_size)\n-    )\n-    test_dataset = (\n-        tfds.load(\"stl10\", split=\"test\", as_supervised=True)\n-        .batch(batch_size)\n-        .prefetch(buffer_size=tf.data.AUTOTUNE)\n-    )\n-\n-    # Labeled and unlabeled datasets are zipped together\n-    train_dataset = tf.data.Dataset.zip(\n-        (unlabeled_train_dataset, labeled_train_dataset)\n-    ).prefetch(buffer_size=tf.data.AUTOTUNE)\n-\n-    return train_dataset, labeled_train_dataset, test_dataset\n-\n-\n-# Load STL10 dataset\n-train_dataset, labeled_train_dataset, test_dataset = prepare_dataset()\n-\n-\"\"\"\n-## Image augmentations\n-\n-The two most important image augmentations for contrastive learning are the\n-following:\n-\n-- Cropping: forces the model to encode different parts of the same image\n-similarly, we implement it with the\n-[RandomTranslation](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_translation/)\n-and\n-[RandomZoom](https://keras.io/api/layers/preprocessing_layers/image_preprocessing/random_zoom/)\n-layers\n-- Color jitter: prevents a trivial color histogram-based solution to the task by\n-distorting color histograms. A principled way to implement that is by affine\n-transformations in color space.\n-\n-In this example we use random horizontal flips as well. Stronger augmentations\n-are applied for contrastive learning, along with weaker ones for supervised\n-classification to avoid overfitting on the few labeled examples.\n-\n-We implement random color jitter as a custom preprocessing layer. Using\n-preprocessing layers for data augmentation has the following two advantages:\n-\n-- The data augmentation will run on GPU in batches, so the training will not be\n-bottlenecked by the data pipeline in environments with constrained CPU\n-resources (such as a Colab Notebook, or a personal machine)\n-- Deployment is easier as the data preprocessing pipeline is encapsulated in the\n-model, and does not have to be reimplemented when deploying it\n-\"\"\"\n-\n-\n-# Distorts the color distibutions of images\n-class RandomColorAffine(layers.Layer):\n-    def __init__(self, brightness=0, jitter=0, **kwargs):\n-        super().__init__(**kwargs)\n-\n-        self.brightness = brightness\n-        self.jitter = jitter\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update({\"brightness\": self.brightness, \"jitter\": self.jitter})\n-        return config\n-\n-    def call(self, images, training=True):\n-        if training:\n-            batch_size = tf.shape(images)[0]\n-\n-            # Same for all colors\n-            brightness_scales = 1 + tf.random.uniform(\n-                (batch_size, 1, 1, 1), minval=-self.brightness, maxval=self.brightness\n-            )\n-            # Different for all colors\n-            jitter_matrices = tf.random.uniform(\n-                (batch_size, 1, 3, 3), minval=-self.jitter, maxval=self.jitter\n-            )\n-\n-            color_transforms = (\n-                tf.eye(3, batch_shape=[batch_size, 1]) * brightness_scales\n-                + jitter_matrices\n-            )\n-            images = tf.clip_by_value(tf.matmul(images, color_transforms), 0, 1)\n-        return images\n-\n-\n-# Image augmentation module\n-def get_augmenter(min_area, brightness, jitter):\n-    zoom_factor = 1.0 - math.sqrt(min_area)\n-    return keras.Sequential(\n-        [\n-            keras.Input(shape=(image_size, image_size, image_channels)),\n-            layers.Rescaling(1 / 255, dtype=\"uint8\"),\n-            layers.RandomFlip(\"horizontal\"),\n-            layers.RandomTranslation(zoom_factor / 2, zoom_factor / 2),\n-            layers.RandomZoom((-zoom_factor, 0.0), (-zoom_factor, 0.0)),\n-            RandomColorAffine(brightness, jitter),\n-        ]\n-    )\n-\n-\n-def visualize_augmentations(num_images):\n-    # Sample a batch from a dataset\n-    images = next(iter(train_dataset))[0][0][:num_images]\n-\n-    # Apply augmentations\n-    augmented_images = zip(\n-        images,\n-        get_augmenter(**classification_augmentation)(images),\n-        get_augmenter(**contrastive_augmentation)(images),\n-        get_augmenter(**contrastive_augmentation)(images),\n-    )\n-    row_titles = [\n-        \"Original:\",\n-        \"Weakly augmented:\",\n-        \"Strongly augmented:\",\n-        \"Strongly augmented:\",\n-    ]\n-    plt.figure(figsize=(num_images * 2.2, 4 * 2.2), dpi=100)\n-    for column, image_row in enumerate(augmented_images):\n-        for row, image in enumerate(image_row):\n-            plt.subplot(4, num_images, row * num_images + column + 1)\n-            plt.imshow(image)\n-            if column == 0:\n-                plt.title(row_titles[row], loc=\"left\")\n-            plt.axis(\"off\")\n-    plt.tight_layout()\n-\n-\n-visualize_augmentations(num_images=8)\n-\n-\"\"\"\n-## Encoder architecture\n-\"\"\"\n-\n-\n-# Define the encoder architecture\n-def get_encoder():\n-    return keras.Sequential(\n-        [\n-            keras.Input(shape=(image_size, image_size, image_channels)),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n-            layers.Flatten(),\n-            layers.Dense(width, activation=\"relu\"),\n-        ],\n-        name=\"encoder\",\n-    )\n-\n-\n-\"\"\"\n-## Supervised baseline model\n-\n-A baseline supervised model is trained using random initialization.\n-\"\"\"\n-\n-# Baseline supervised training with random initialization\n-baseline_model = keras.Sequential(\n-    [\n-        keras.Input(shape=(image_size, image_size, image_channels)),\n-        get_augmenter(**classification_augmentation),\n-        get_encoder(),\n-        layers.Dense(10),\n-    ],\n-    name=\"baseline_model\",\n-)\n-baseline_model.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n-)\n-\n-baseline_history = baseline_model.fit(\n-    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n-)\n-\n-print(\n-    \"Maximal validation accuracy: {:.2f}%\".format(\n-        max(baseline_history.history[\"val_acc\"]) * 100\n-    )\n-)\n-\n-\"\"\"\n-## Self-supervised model for contrastive pretraining\n-\n-We pretrain an encoder on unlabeled images with a contrastive loss.\n-A nonlinear projection head is attached to the top of the encoder, as it\n-improves the quality of representations of the encoder.\n-\n-We use the InfoNCE/NT-Xent/N-pairs loss, which can be interpreted in the\n-following way:\n-\n-1. We treat each image in the batch as if it had its own class.\n-2. Then, we have two examples (a pair of augmented views) for each \"class\".\n-3. Each view's representation is compared to every possible pair's one (for both\n-  augmented versions).\n-4. We use the temperature-scaled cosine similarity of compared representations as\n-  logits.\n-5. Finally, we use categorical cross-entropy as the \"classification\" loss\n-\n-The following two metrics are used for monitoring the pretraining performance:\n-\n-- [Contrastive accuracy (SimCLR Table 5)](https://arxiv.org/abs/2002.05709):\n-Self-supervised metric, the ratio of cases in which the representation of an\n-image is more similar to its differently augmented version's one, than to the\n-representation of any other image in the current batch. Self-supervised\n-metrics can be used for hyperparameter tuning even in the case when there are\n-no labeled examples.\n-- [Linear probing accuracy](https://arxiv.org/abs/1603.08511): Linear probing is\n-a popular metric to evaluate self-supervised classifiers. It is computed as\n-the accuracy of a logistic regression classifier trained on top of the\n-encoder's features. In our case, this is done by training a single dense layer\n-on top of the frozen encoder. Note that contrary to traditional approach where\n-the classifier is trained after the pretraining phase, in this example we\n-train it during pretraining. This might slightly decrease its accuracy, but\n-that way we can monitor its value during training, which helps with\n-experimentation and debugging.\n-\n-Another widely used supervised metric is the\n-[KNN accuracy](https://arxiv.org/abs/1805.01978), which is the accuracy of a KNN\n-classifier trained on top of the encoder's features, which is not implemented in\n-this example.\n-\"\"\"\n-\n-\n-# Define the contrastive model with model-subclassing\n-class ContrastiveModel(keras.Model):\n-    def __init__(self):\n-        super().__init__()\n-\n-        self.temperature = temperature\n-        self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n-        self.classification_augmenter = get_augmenter(**classification_augmentation)\n-        self.encoder = get_encoder()\n-        # Non-linear MLP as projection head\n-        self.projection_head = keras.Sequential(\n-            [\n-                keras.Input(shape=(width,)),\n-                layers.Dense(width, activation=\"relu\"),\n-                layers.Dense(width),\n-            ],\n-            name=\"projection_head\",\n-        )\n-        # Single dense layer for linear probing\n-        self.linear_probe = keras.Sequential(\n-            [layers.Input(shape=(width,)), layers.Dense(10)], name=\"linear_probe\"\n-        )\n-\n-        self.encoder.summary()\n-        self.projection_head.summary()\n-        self.linear_probe.summary()\n-\n-    def compile(self, contrastive_optimizer, probe_optimizer, **kwargs):\n-        super().compile(**kwargs)\n-\n-        self.contrastive_optimizer = contrastive_optimizer\n-        self.probe_optimizer = probe_optimizer\n-\n-        # self.contrastive_loss will be defined as a method\n-        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n-\n-        self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n-        self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n-            name=\"c_acc\"\n-        )\n-        self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n-        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n-\n-    @property\n-    def metrics(self):\n-        return [\n-            self.contrastive_loss_tracker,\n-            self.contrastive_accuracy,\n-            self.probe_loss_tracker,\n-            self.probe_accuracy,\n-        ]\n-\n-    def contrastive_loss(self, projections_1, projections_2):\n-        # InfoNCE loss (information noise-contrastive estimation)\n-        # NT-Xent loss (normalized temperature-scaled cross entropy)\n-\n-        # Cosine similarity: the dot product of the l2-normalized feature vectors\n-        projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n-        projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n-        similarities = (\n-            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n-        )\n-\n-        # The similarity between the representations of two augmented views of the\n-        # same image should be higher than their similarity with other views\n-        batch_size = tf.shape(projections_1)[0]\n-        contrastive_labels = tf.range(batch_size)\n-        self.contrastive_accuracy.update_state(contrastive_labels, similarities)\n-        self.contrastive_accuracy.update_state(\n-            contrastive_labels, tf.transpose(similarities)\n-        )\n-\n-        # The temperature-scaled similarities are used as logits for cross-entropy\n-        # a symmetrized version of the loss is used here\n-        loss_1_2 = keras.losses.sparse_categorical_crossentropy(\n-            contrastive_labels, similarities, from_logits=True\n-        )\n-        loss_2_1 = keras.losses.sparse_categorical_crossentropy(\n-            contrastive_labels, tf.transpose(similarities), from_logits=True\n-        )\n-        return (loss_1_2 + loss_2_1) / 2\n-\n-    def train_step(self, data):\n-        (unlabeled_images, _), (labeled_images, labels) = data\n-\n-        # Both labeled and unlabeled images are used, without labels\n-        images = tf.concat((unlabeled_images, labeled_images), axis=0)\n-        # Each image is augmented twice, differently\n-        augmented_images_1 = self.contrastive_augmenter(images, training=True)\n-        augmented_images_2 = self.contrastive_augmenter(images, training=True)\n-        with tf.GradientTape() as tape:\n-            features_1 = self.encoder(augmented_images_1, training=True)\n-            features_2 = self.encoder(augmented_images_2, training=True)\n-            # The representations are passed through a projection mlp\n-            projections_1 = self.projection_head(features_1, training=True)\n-            projections_2 = self.projection_head(features_2, training=True)\n-            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n-        gradients = tape.gradient(\n-            contrastive_loss,\n-            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n-        )\n-        self.contrastive_optimizer.apply_gradients(\n-            zip(\n-                gradients,\n-                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n-            )\n-        )\n-        self.contrastive_loss_tracker.update_state(contrastive_loss)\n-\n-        # Labels are only used in evalutation for an on-the-fly logistic regression\n-        preprocessed_images = self.classification_augmenter(\n-            labeled_images, training=True\n-        )\n-        with tf.GradientTape() as tape:\n-            # the encoder is used in inference mode here to avoid regularization\n-            # and updating the batch normalization paramers if they are used\n-            features = self.encoder(preprocessed_images, training=False)\n-            class_logits = self.linear_probe(features, training=True)\n-            probe_loss = self.probe_loss(labels, class_logits)\n-        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n-        self.probe_optimizer.apply_gradients(\n-            zip(gradients, self.linear_probe.trainable_weights)\n-        )\n-        self.probe_loss_tracker.update_state(probe_loss)\n-        self.probe_accuracy.update_state(labels, class_logits)\n-\n-        return {m.name: m.result() for m in self.metrics}\n-\n-    def test_step(self, data):\n-        labeled_images, labels = data\n-\n-        # For testing the components are used with a training=False flag\n-        preprocessed_images = self.classification_augmenter(\n-            labeled_images, training=False\n-        )\n-        features = self.encoder(preprocessed_images, training=False)\n-        class_logits = self.linear_probe(features, training=False)\n-        probe_loss = self.probe_loss(labels, class_logits)\n-        self.probe_loss_tracker.update_state(probe_loss)\n-        self.probe_accuracy.update_state(labels, class_logits)\n-\n-        # Only the probe metrics are logged at test time\n-        return {m.name: m.result() for m in self.metrics[2:]}\n-\n-\n-# Contrastive pretraining\n-pretraining_model = ContrastiveModel()\n-pretraining_model.compile(\n-    contrastive_optimizer=keras.optimizers.Adam(),\n-    probe_optimizer=keras.optimizers.Adam(),\n-)\n-\n-pretraining_history = pretraining_model.fit(\n-    train_dataset, epochs=num_epochs, validation_data=test_dataset\n-)\n-print(\n-    \"Maximal validation accuracy: {:.2f}%\".format(\n-        max(pretraining_history.history[\"val_p_acc\"]) * 100\n-    )\n-)\n-\n-\"\"\"\n-## Supervised finetuning of the pretrained encoder\n-\n-We then finetune the encoder on the labeled examples, by attaching\n-a single randomly initalized fully connected classification layer on its top.\n-\"\"\"\n-\n-# Supervised finetuning of the pretrained encoder\n-finetuning_model = keras.Sequential(\n-    [\n-        layers.Input(shape=(image_size, image_size, image_channels)),\n-        get_augmenter(**classification_augmentation),\n-        pretraining_model.encoder,\n-        layers.Dense(10),\n-    ],\n-    name=\"finetuning_model\",\n-)\n-finetuning_model.compile(\n-    optimizer=keras.optimizers.Adam(),\n-    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n-)\n-\n-finetuning_history = finetuning_model.fit(\n-    labeled_train_dataset, epochs=num_epochs, validation_data=test_dataset\n-)\n-print(\n-    \"Maximal validation accuracy: {:.2f}%\".format(\n-        max(finetuning_history.history[\"val_acc\"]) * 100\n-    )\n-)\n-\n-\"\"\"\n-## Comparison against the baseline\n-\"\"\"\n-\n-\n-# The classification accuracies of the baseline and the pretraining + finetuning process:\n-def plot_training_curves(pretraining_history, finetuning_history, baseline_history):\n-    for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):\n-        plt.figure(figsize=(8, 5), dpi=100)\n-        plt.plot(\n-            baseline_history.history[f\"val_{metric_key}\"], label=\"supervised baseline\"\n-        )\n-        plt.plot(\n-            pretraining_history.history[f\"val_p_{metric_key}\"],\n-            label=\"self-supervised pretraining\",\n-        )\n-        plt.plot(\n-            finetuning_history.history[f\"val_{metric_key}\"],\n-            label=\"supervised finetuning\",\n-        )\n-        plt.legend()\n-        plt.title(f\"Classification {metric_name} during training\")\n-        plt.xlabel(\"epochs\")\n-        plt.ylabel(f\"validation {metric_name}\")\n-\n-\n-plot_training_curves(pretraining_history, finetuning_history, baseline_history)\n-\n-\"\"\"\n-By comparing the training curves, we can see that when using contrastive\n-pretraining, a higher validation accuracy can be reached, paired with a lower\n-validation loss, which means that the pretrained network was able to generalize\n-better when seeing only a small amount of labeled examples.\n-\"\"\"\n-\n-\"\"\"\n-## Improving further\n-\n-### Architecture\n-\n-The experiment in the original paper demonstrated that increasing the width and depth of the\n-models improves performance at a higher rate than for supervised learning. Also,\n-using a [ResNet-50](https://keras.io/api/applications/resnet/#resnet50-function)\n-encoder is quite standard in the literature. However keep in mind, that more\n-powerful models will not only increase training time but will also require more\n-memory and will limit the maximal batch size you can use.\n-\n-It has [been](https://arxiv.org/abs/1905.09272)\n-[reported](https://arxiv.org/abs/1911.05722) that the usage of BatchNorm layers\n-could sometimes degrade performance, as it introduces an intra-batch dependency\n-between samples, which is why I did not have used them in this example. In my\n-experiments however, using BatchNorm, especially in the projection head,\n-improves performance.\n-\n-### Hyperparameters\n-\n-The hyperparameters used in this example have been tuned manually for this task and\n-architecture. Therefore, without changing them, only marginal gains can be expected\n-from further hyperparameter tuning.\n-\n-However for a different task or model architecture these would need tuning, so\n-here are my notes on the most important ones:\n-\n-- **Batch size**: since the objective can be interpreted as a classification\n-over a batch of images (loosely speaking), the batch size is actually a more\n-important hyperparameter than usual. The higher, the better.\n-- **Temperature**: the temperature defines the \"softness\" of the softmax\n-distribution that is used in the cross-entropy loss, and is an important\n-hyperparameter. Lower values generally lead to a higher contrastive accuracy.\n-A recent trick (in [ALIGN](https://arxiv.org/abs/2102.05918)) is to learn\n-the temperature's value as well (which can be done by defining it as a\n-tf.Variable, and applying gradients on it). Even though this provides a good baseline\n-value, in my experiments the learned temperature was somewhat lower\n-than optimal, as it is optimized with respect to the contrastive loss, which is not a\n-perfect proxy for representation quality.\n-- **Image augmentation strength**: during pretraining stronger augmentations\n-increase the difficulty of the task, however after a point too strong\n-augmentations will degrade performance. During finetuning stronger\n-augmentations reduce overfitting while in my experience too strong\n-augmentations decrease the performance gains from pretraining. The whole data\n-augmentation pipeline can be seen as an important hyperparameter of the\n-algorithm, implementations of other custom image augmentation layers in Keras\n-can be found in\n-[this repository](https://github.com/beresandras/image-augmentation-layers-keras).\n-- **Learning rate schedule**: a constant schedule is used here, but it is\n-quite common in the literature to use a\n-[cosine decay schedule](https://www.tensorflow.org/api_docs/python/tf/keras/experimental/CosineDecay),\n-which can further improve performance.\n-- **Optimizer**: Adam is used in this example, as it provides good performance\n-with default parameters. SGD with momentum requires more tuning, however it\n-could slightly increase performance.\n-\"\"\"\n-\n-\"\"\"\n-## Related works\n-\n-Other instance-level (image-level) contrastive learning methods:\n-\n-- [MoCo](https://arxiv.org/abs/1911.05722)\n-([v2](https://arxiv.org/abs/2003.04297),\n-[v3](https://arxiv.org/abs/2104.02057)): uses a momentum-encoder as well,\n-whose weights are an exponential moving average of the target encoder\n-- [SwAV](https://arxiv.org/abs/2006.09882): uses clustering instead of pairwise\n-comparison\n-- [BarlowTwins](https://arxiv.org/abs/2103.03230): uses a cross\n-correlation-based objective instead of pairwise comparison\n-\n-Keras implementations of **MoCo** and **BarlowTwins** can be found in\n-[this repository](https://github.com/beresandras/contrastive-classification-keras),\n-which includes a Colab notebook.\n-\n-There is also a new line of works, which optimize a similar objective, but\n-without the use of any negatives:\n-\n-- [BYOL](https://arxiv.org/abs/2006.07733): momentum-encoder + no negatives\n-- [SimSiam](https://arxiv.org/abs/2011.10566)\n-([Keras example](https://keras.io/examples/vision/simsiam/)):\n-no momentum-encoder + no negatives\n-\n-In my experience, these methods are more brittle (they can collapse to a constant\n-representation, I could not get them to work using this encoder architecture).\n-Even though they are generally more dependent on the\n-[model](https://generallyintelligent.ai/understanding-self-supervised-contrastive-learning.html)\n-[architecture](https://arxiv.org/abs/2010.10241), they can improve\n-performance at smaller batch sizes.\n-\n-You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/semi-supervised-classification-simclr)\n-and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/semi-supervised-classification).\n-\"\"\"\n\\ No newline at end of file\n\n@@ -1,547 +0,0 @@\n-\"\"\"\n-Title: Image classification with Swin Transformers\n-Author: [Rishit Dagli](https://twitter.com/rishit_dagli)\n-Date created: 2021/09/08\n-Last modified: 2021/09/08\n-Description: Image classification using Swin Transformers, a general-purpose backbone for computer vision.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-This example implements [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n-by Liu et al. for image classification, and demonstrates it on the\n-[CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n-\n-Swin Transformer (**S**hifted **Win**dow Transformer) can serve as a general-purpose backbone\n-for computer vision. Swin Transformer is a hierarchical Transformer whose\n-representations are computed with _shifted windows_. The shifted window scheme\n-brings greater efficiency by limiting self-attention computation to\n-non-overlapping local windows while also allowing for cross-window connections.\n-This architecture has the flexibility to model information at various scales and has\n-a linear computational complexity with respect to image size.\n-\n-This example requires TensorFlow 2.5 or higher.\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\"\"\"\n-\n-import matplotlib.pyplot as plt\n-import numpy as np\n-import tensorflow as tf\n-import keras_core as keras\n-from keras_core import layers\n-\n-\"\"\"\n-## Prepare the data\n-\n-We load the CIFAR-100 dataset through `tf.keras.datasets`,\n-normalize the images, and convert the integer labels to one-hot encoded vectors.\n-\"\"\"\n-\n-num_classes = 100\n-input_shape = (32, 32, 3)\n-\n-(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n-x_train, x_test = x_train / 255.0, x_test / 255.0\n-y_train = keras.utils.numerical_utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.numerical_utils.to_categorical(y_test, num_classes)\n-print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n-print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n-\n-plt.figure(figsize=(10, 10))\n-for i in range(25):\n-    plt.subplot(5, 5, i + 1)\n-    plt.xticks([])\n-    plt.yticks([])\n-    plt.grid(False)\n-    plt.imshow(x_train[i])\n-plt.show()\n-\n-\"\"\"\n-## Configure the hyperparameters\n-\n-A key parameter to pick is the `patch_size`, the size of the input patches.\n-In order to use each pixel as an individual input, you can set `patch_size` to `(1, 1)`.\n-Below, we take inspiration from the original paper settings\n-for training on ImageNet-1K, keeping most of the original settings for this example.\n-\"\"\"\n-\n-patch_size = (2, 2)  # 2-by-2 sized patches\n-dropout_rate = 0.03  # Dropout rate\n-num_heads = 8  # Attention heads\n-embed_dim = 64  # Embedding dimension\n-num_mlp = 256  # MLP layer size\n-qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n-window_size = 2  # Size of attention window\n-shift_size = 1  # Size of shifting window\n-image_dimension = 32  # Initial image size\n-\n-num_patch_x = input_shape[0] // patch_size[0]\n-num_patch_y = input_shape[1] // patch_size[1]\n-\n-learning_rate = 1e-3\n-batch_size = 128\n-num_epochs = 1\n-validation_split = 0.1\n-weight_decay = 0.0001\n-label_smoothing = 0.1\n-\n-\"\"\"\n-## Helper functions\n-\n-We create two helper functions to help us get a sequence of\n-patches from the image, merge patches, and apply dropout.\n-\"\"\"\n-\n-\n-def window_partition(x, window_size):\n-    _, height, width, channels = x.shape\n-    patch_num_y = height // window_size\n-    patch_num_x = width // window_size\n-    x = tf.reshape(\n-        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n-    )\n-    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n-    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n-    return windows\n-\n-\n-def window_reverse(windows, window_size, height, width, channels):\n-    patch_num_y = height // window_size\n-    patch_num_x = width // window_size\n-    x = tf.reshape(\n-        windows,\n-        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n-    )\n-    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n-    x = tf.reshape(x, shape=(-1, height, width, channels))\n-    return x\n-\n-\n-class DropPath(layers.Layer):\n-    def __init__(self, drop_prob=None, **kwargs):\n-        super().__init__(**kwargs)\n-        self.drop_prob = drop_prob\n-\n-    def call(self, x):\n-        input_shape = tf.shape(x)\n-        batch_size = input_shape[0]\n-        rank = x.shape.rank\n-        shape = (batch_size,) + (1,) * (rank - 1)\n-        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n-        path_mask = tf.floor(random_tensor)\n-        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n-        return output\n-\n-\n-\"\"\"\n-## Window based multi-head self-attention\n-\n-Usually Transformers perform global self-attention, where the relationships between\n-a token and all other tokens are computed. The global computation leads to quadratic\n-complexity with respect to the number of tokens. Here, as the [original paper](https://arxiv.org/abs/2103.14030)\n-suggests, we compute self-attention within local windows, in a non-overlapping manner.\n-Global self-attention leads to quadratic computational complexity in the number of patches,\n-whereas window-based self-attention leads to linear complexity and is easily scalable.\n-\"\"\"\n-\n-\n-class WindowAttention(layers.Layer):\n-    def __init__(\n-        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n-    ):\n-        super().__init__(**kwargs)\n-        self.dim = dim\n-        self.window_size = window_size\n-        self.num_heads = num_heads\n-        self.scale = (dim // num_heads) ** -0.5\n-        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n-        self.dropout = layers.Dropout(dropout_rate)\n-        self.proj = layers.Dense(dim)\n-\n-    def build(self, input_shape):\n-        num_window_elements = (2 * self.window_size[0] - 1) * (\n-            2 * self.window_size[1] - 1\n-        )\n-        self.relative_position_bias_table = self.add_weight(\n-            shape=(num_window_elements, self.num_heads),\n-            initializer=tf.initializers.Zeros(),\n-            trainable=True,\n-        )\n-        coords_h = np.arange(self.window_size[0])\n-        coords_w = np.arange(self.window_size[1])\n-        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n-        coords = np.stack(coords_matrix)\n-        coords_flatten = coords.reshape(2, -1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n-        relative_coords = relative_coords.transpose([1, 2, 0])\n-        relative_coords[:, :, 0] += self.window_size[0] - 1\n-        relative_coords[:, :, 1] += self.window_size[1] - 1\n-        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n-        relative_position_index = relative_coords.sum(-1)\n-\n-        self.relative_position_index = tf.Variable(\n-            initial_value=lambda: tf.convert_to_tensor(relative_position_index), trainable=False\n-        )\n-\n-    def call(self, x, mask=None):\n-        _, size, channels = x.shape\n-        head_dim = channels // self.num_heads\n-        x_qkv = self.qkv(x)\n-        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n-        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n-        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n-        q = q * self.scale\n-        k = tf.transpose(k, perm=(0, 1, 3, 2))\n-        attn = q @ k\n-\n-        num_window_elements = self.window_size[0] * self.window_size[1]\n-        relative_position_index_flat = tf.reshape(\n-            self.relative_position_index, shape=(-1,)\n-        )\n-        relative_position_bias = tf.gather(\n-            self.relative_position_bias_table, relative_position_index_flat\n-        )\n-        relative_position_bias = tf.reshape(\n-            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n-        )\n-        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n-        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n-\n-        if mask is not None:\n-            nW = mask.shape[0]\n-            mask_float = tf.cast(\n-                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n-            )\n-            attn = (\n-                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n-                + mask_float\n-            )\n-            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n-            attn = keras.activations.softmax(attn, axis=-1)\n-        else:\n-            attn = keras.activations.softmax(attn, axis=-1)\n-        attn = self.dropout(attn)\n-\n-        x_qkv = attn @ v\n-        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n-        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n-        x_qkv = self.proj(x_qkv)\n-        x_qkv = self.dropout(x_qkv)\n-        return x_qkv\n-\n-\n-\"\"\"\n-## The complete Swin Transformer model\n-\n-Finally, we put together the complete Swin Transformer by replacing the standard multi-head\n-attention (MHA) with shifted windows attention. As suggested in the\n-original paper, we create a model comprising of a shifted window-based MHA\n-layer, followed by a 2-layer MLP with GELU nonlinearity in between, applying\n-`LayerNormalization` before each MSA layer and each MLP, and a residual\n-connection after each of these layers.\n-\n-Notice that we only create a simple MLP with 2 Dense and\n-2 Dropout layers. Often you will see models using ResNet-50 as the MLP which is\n-quite standard in the literature. However in this paper the authors use a\n-2-layer MLP with GELU nonlinearity in between.\n-\"\"\"\n-\n-\n-class SwinTransformer(layers.Layer):\n-    def __init__(\n-        self,\n-        dim,\n-        num_patch,\n-        num_heads,\n-        window_size=7,\n-        shift_size=0,\n-        num_mlp=1024,\n-        qkv_bias=True,\n-        dropout_rate=0.0,\n-        **kwargs,\n-    ):\n-        super().__init__(**kwargs)\n-\n-        self.dim = dim  # number of input dimensions\n-        self.num_patch = num_patch  # number of embedded patches\n-        self.num_heads = num_heads  # number of attention heads\n-        self.window_size = window_size  # size of window\n-        self.shift_size = shift_size  # size of window shift\n-        self.num_mlp = num_mlp  # number of MLP nodes\n-\n-        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n-        self.attn = WindowAttention(\n-            dim,\n-            window_size=(self.window_size, self.window_size),\n-            num_heads=num_heads,\n-            qkv_bias=qkv_bias,\n-            dropout_rate=dropout_rate,\n-        )\n-        self.drop_path = DropPath(dropout_rate)\n-        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n-\n-        self.mlp = keras.Sequential(\n-            [\n-                layers.Dense(num_mlp),\n-                layers.Activation(keras.activations.gelu),\n-                layers.Dropout(dropout_rate),\n-                layers.Dense(dim),\n-                layers.Dropout(dropout_rate),\n-            ]\n-        )\n-\n-        if min(self.num_patch) < self.window_size:\n-            self.shift_size = 0\n-            self.window_size = min(self.num_patch)\n-\n-    def build(self, input_shape):\n-        if self.shift_size == 0:\n-            self.attn_mask = None\n-        else:\n-            height, width = self.num_patch\n-            h_slices = (\n-                slice(0, -self.window_size),\n-                slice(-self.window_size, -self.shift_size),\n-                slice(-self.shift_size, None),\n-            )\n-            w_slices = (\n-                slice(0, -self.window_size),\n-                slice(-self.window_size, -self.shift_size),\n-                slice(-self.shift_size, None),\n-            )\n-            mask_array = np.zeros((1, height, width, 1))\n-            count = 0\n-            for h in h_slices:\n-                for w in w_slices:\n-                    mask_array[:, h, w, :] = count\n-                    count += 1\n-            mask_array = tf.convert_to_tensor(mask_array)\n-\n-            # mask array to windows\n-            mask_windows = window_partition(mask_array, self.window_size)\n-            mask_windows = tf.reshape(\n-                mask_windows, shape=[-1, self.window_size * self.window_size]\n-            )\n-            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n-                mask_windows, axis=2\n-            )\n-            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n-            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n-            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n-\n-    def call(self, x):\n-        height, width = self.num_patch\n-        _, num_patches_before, channels = x.shape\n-        x_skip = x\n-        x = self.norm1(x)\n-        x = tf.reshape(x, shape=(-1, height, width, channels))\n-        if self.shift_size > 0:\n-            shifted_x = tf.roll(\n-                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n-            )\n-        else:\n-            shifted_x = x\n-\n-        x_windows = window_partition(shifted_x, self.window_size)\n-        x_windows = tf.reshape(\n-            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n-        )\n-        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n-\n-        attn_windows = tf.reshape(\n-            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n-        )\n-        shifted_x = window_reverse(\n-            attn_windows, self.window_size, height, width, channels\n-        )\n-        if self.shift_size > 0:\n-            x = tf.roll(\n-                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n-            )\n-        else:\n-            x = shifted_x\n-\n-        x = tf.reshape(x, shape=(-1, height * width, channels))\n-        x = self.drop_path(x)\n-        x = x_skip + x\n-        x_skip = x\n-        x = self.norm2(x)\n-        x = self.mlp(x)\n-        x = self.drop_path(x)\n-        x = x_skip + x\n-        return x\n-\n-\n-\"\"\"\n-## Model training and evaluation\n-\n-### Extract and embed patches\n-\n-We first create 3 layers to help us extract, embed and merge patches from the\n-images on top of which we will later use the Swin Transformer class we built.\n-\"\"\"\n-\n-\n-class PatchExtract(layers.Layer):\n-    def __init__(self, patch_size, **kwargs):\n-        super().__init__(**kwargs)\n-        self.patch_size_x = patch_size[0]\n-        self.patch_size_y = patch_size[0]\n-\n-    def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n-            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n-            rates=(1, 1, 1, 1),\n-            padding=\"VALID\",\n-        )\n-        patch_dim = patches.shape[-1]\n-        patch_num = patches.shape[1]\n-        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n-\n-\n-class PatchEmbedding(layers.Layer):\n-    def __init__(self, num_patch, embed_dim, **kwargs):\n-        super().__init__(**kwargs)\n-        self.num_patch = num_patch\n-        self.proj = layers.Dense(embed_dim)\n-        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n-\n-    def call(self, patch):\n-        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n-        return self.proj(patch) + self.pos_embed(pos)\n-\n-\n-class PatchMerging(keras.layers.Layer):\n-    def __init__(self, num_patch, embed_dim):\n-        super().__init__()\n-        self.num_patch = num_patch\n-        self.embed_dim = embed_dim\n-        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n-\n-    def call(self, x):\n-        height, width = self.num_patch\n-        _, _, C = x.shape\n-        x = tf.reshape(x, shape=(-1, height, width, C))\n-        x0 = x[:, 0::2, 0::2, :]\n-        x1 = x[:, 1::2, 0::2, :]\n-        x2 = x[:, 0::2, 1::2, :]\n-        x3 = x[:, 1::2, 1::2, :]\n-        x = tf.concat((x0, x1, x2, x3), axis=-1)\n-        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n-        return self.linear_trans(x)\n-\n-\n-\"\"\"\n-### Build the model\n-\n-We put together the Swin Transformer model.\n-\"\"\"\n-\n-input = layers.Input(input_shape)\n-x = layers.RandomCrop(image_dimension, image_dimension)(input)\n-x = layers.RandomFlip(\"horizontal\")(x)\n-x = PatchExtract(patch_size)(x)\n-x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n-x = SwinTransformer(\n-    dim=embed_dim,\n-    num_patch=(num_patch_x, num_patch_y),\n-    num_heads=num_heads,\n-    window_size=window_size,\n-    shift_size=0,\n-    num_mlp=num_mlp,\n-    qkv_bias=qkv_bias,\n-    dropout_rate=dropout_rate,\n-)(x)\n-x = SwinTransformer(\n-    dim=embed_dim,\n-    num_patch=(num_patch_x, num_patch_y),\n-    num_heads=num_heads,\n-    window_size=window_size,\n-    shift_size=shift_size,\n-    num_mlp=num_mlp,\n-    qkv_bias=qkv_bias,\n-    dropout_rate=dropout_rate,\n-)(x)\n-x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n-x = layers.GlobalAveragePooling1D()(x)\n-output = layers.Dense(num_classes, activation=\"softmax\")(x)\n-\n-\"\"\"\n-### Train on CIFAR-100\n-\n-We train the model on CIFAR-100. Here, we only train the model\n-for 40 epochs to keep the training time short in this example.\n-In practice, you should train for 150 epochs to reach convergence.\n-\"\"\"\n-\n-model = keras.Model(input, output)\n-model.compile(\n-    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n-    optimizer=keras.optimizers.AdamW(\n-        learning_rate=learning_rate, weight_decay=weight_decay\n-    ),\n-    metrics=[\n-        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n-        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n-    ],\n-)\n-\n-history = model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=num_epochs,\n-    validation_split=validation_split,\n-)\n-\n-\"\"\"\n-Let's visualize the training progress of the model.\n-\"\"\"\n-\n-plt.plot(history.history[\"loss\"], label=\"train_loss\")\n-plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n-plt.xlabel(\"Epochs\")\n-plt.ylabel(\"Loss\")\n-plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n-plt.legend()\n-plt.grid()\n-plt.show()\n-\n-\"\"\"\n-Let's display the final results of the training on CIFAR-100.\n-\"\"\"\n-\n-loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n-print(f\"Test loss: {round(loss, 2)}\")\n-print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n-print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n-\n-\"\"\"\n-The Swin Transformer model we just trained has just 152K parameters, and it gets\n-us to ~75% test top-5 accuracy within just 40 epochs without any signs of overfitting\n-as well as seen in above graph. This means we can train this network for longer\n-(perhaps with a bit more regularization) and obtain even better performance.\n-This performance can further be improved by additional techniques like cosine\n-decay learning rate schedule, other data augmentation techniques. While experimenting,\n-I tried training the model for 150 epochs with a slightly higher dropout and greater\n-embedding dimensions which pushes the performance to ~72% test accuracy on CIFAR-100\n-as you can see in the screenshot.\n-\n-![Results of training for longer](https://i.imgur.com/9vnQesZ.png)\n-\n-The authors present a top-1 accuracy of 87.3% on ImageNet. The authors also present\n-a number of experiments to study how input sizes, optimizers etc. affect the final\n-performance of this model. The authors further present using this model for object detection,\n-semantic segmentation and instance segmentation as well and report competitive results\n-for these. You are strongly advised to also check out the\n-[original paper](https://arxiv.org/abs/2103.14030).\n-\n-This example takes inspiration from the official\n-[PyTorch](https://github.com/microsoft/Swin-Transformer) and\n-[TensorFlow](https://github.com/VcampSoldiers/Swin-Transformer-Tensorflow) implementations.\n-\"\"\"\n\\ No newline at end of file\n\n@@ -1,226 +0,0 @@\n-\"\"\"\n-Title: Timeseries classification from scratch\n-Author: [hfawaz](https://github.com/hfawaz/)\n-Date created: 2020/07/21\n-Last modified: 2021/07/16\n-Description: Training a timeseries classifier from scratch on the FordA dataset from the UCR/UEA archive.\n-Accelerator: GPU\n-\"\"\"\n-\"\"\"\n-## Introduction\n-\n-This example shows how to do timeseries classification from scratch, starting from raw\n-CSV timeseries files on disk. We demonstrate the workflow on the FordA dataset from the\n-[UCR/UEA archive](https://www.cs.ucr.edu/%7Eeamonn/time_series_data_2018/).\n-\n-\"\"\"\n-\n-\"\"\"\n-## Setup\n-\n-\"\"\"\n-import keras_core as keras\n-import numpy as np\n-import matplotlib.pyplot as plt\n-\n-\"\"\"\n-## Load the data: the FordA dataset\n-\n-### Dataset description\n-\n-The dataset we are using here is called FordA.\n-The data comes from the UCR archive.\n-The dataset contains 3601 training instances and another 1320 testing instances.\n-Each timeseries corresponds to a measurement of engine noise captured by a motor sensor.\n-For this task, the goal is to automatically detect the presence of a specific issue with\n-the engine. The problem is a balanced binary classification task. The full description of\n-this dataset can be found [here](http://www.j-wichard.de/publications/FordPaper.pdf).\n-\n-### Read the TSV data\n-\n-We will use the `FordA_TRAIN` file for training and the\n-`FordA_TEST` file for testing. The simplicity of this dataset\n-allows us to demonstrate effectively how to use ConvNets for timeseries classification.\n-In this file, the first column corresponds to the label.\n-\"\"\"\n-\n-\n-def readucr(filename):\n-    data = np.loadtxt(filename, delimiter=\"\\t\")\n-    y = data[:, 0]\n-    x = data[:, 1:]\n-    return x, y.astype(int)\n-\n-\n-root_url = \"https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/\"\n-\n-x_train, y_train = readucr(root_url + \"FordA_TRAIN.tsv\")\n-x_test, y_test = readucr(root_url + \"FordA_TEST.tsv\")\n-\n-\"\"\"\n-## Visualize the data\n-\n-Here we visualize one timeseries example for each class in the dataset.\n-\n-\"\"\"\n-\n-classes = np.unique(np.concatenate((y_train, y_test), axis=0))\n-\n-plt.figure()\n-for c in classes:\n-    c_x_train = x_train[y_train == c]\n-    plt.plot(c_x_train[0], label=\"class \" + str(c))\n-plt.legend(loc=\"best\")\n-plt.show()\n-plt.close()\n-\n-\"\"\"\n-## Standardize the data\n-\n-Our timeseries are already in a single length (500). However, their values are\n-usually in various ranges. This is not ideal for a neural network;\n-in general we should seek to make the input values normalized.\n-For this specific dataset, the data is already z-normalized: each timeseries sample\n-has a mean equal to zero and a standard deviation equal to one. This type of\n-normalization is very common for timeseries classification problems, see\n-[Bagnall et al. (2016)](https://link.springer.com/article/10.1007/s10618-016-0483-9).\n-\n-Note that the timeseries data used here are univariate, meaning we only have one channel\n-per timeseries example.\n-We will therefore transform the timeseries into a multivariate one with one channel\n-using a simple reshaping via numpy.\n-This will allow us to construct a model that is easily applicable to multivariate time\n-series.\n-\"\"\"\n-\n-x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n-x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n-\n-\"\"\"\n-Finally, in order to use `sparse_categorical_crossentropy`, we will have to count\n-the number of classes beforehand.\n-\"\"\"\n-\n-num_classes = len(np.unique(y_train))\n-\n-\"\"\"\n-Now we shuffle the training set because we will be using the `validation_split` option\n-later when training.\n-\"\"\"\n-\n-idx = np.random.permutation(len(x_train))\n-x_train = x_train[idx]\n-y_train = y_train[idx]\n-\n-\"\"\"\n-Standardize the labels to positive integers.\n-The expected labels will then be 0 and 1.\n-\"\"\"\n-\n-y_train[y_train == -1] = 0\n-y_test[y_test == -1] = 0\n-\n-\"\"\"\n-## Build a model\n-\n-We build a Fully Convolutional Neural Network originally proposed in\n-[this paper](https://arxiv.org/abs/1611.06455).\n-The implementation is based on the TF 2 version provided\n-[here](https://github.com/hfawaz/dl-4-tsc/).\n-The following hyperparameters (kernel_size, filters, the usage of BatchNorm) were found\n-via random search using [KerasTuner](https://github.com/keras-team/keras-tuner).\n-\n-\"\"\"\n-\n-\n-def make_model(input_shape):\n-    input_layer = keras.layers.Input(input_shape)\n-\n-    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n-    conv1 = keras.layers.BatchNormalization()(conv1)\n-    conv1 = keras.layers.ReLU()(conv1)\n-\n-    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n-    conv2 = keras.layers.BatchNormalization()(conv2)\n-    conv2 = keras.layers.ReLU()(conv2)\n-\n-    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n-    conv3 = keras.layers.BatchNormalization()(conv3)\n-    conv3 = keras.layers.ReLU()(conv3)\n-\n-    gap = keras.layers.GlobalAveragePooling1D()(conv3)\n-\n-    output_layer = keras.layers.Dense(num_classes, activation=\"softmax\")(gap)\n-\n-    return keras.models.Model(inputs=input_layer, outputs=output_layer)\n-\n-\n-model = make_model(input_shape=x_train.shape[1:])\n-keras.utils.plot_model(model, show_shapes=True)\n-\n-\"\"\"\n-## Train the model\n-\n-\"\"\"\n-\n-epochs = 500\n-batch_size = 32\n-\n-callbacks = [\n-    keras.callbacks.ModelCheckpoint(\n-        \"best_model.keras\", save_best_only=True, monitor=\"val_loss\"\n-    ),\n-    keras.callbacks.ReduceLROnPlateau(\n-        monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001\n-    ),\n-    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1),\n-]\n-model.compile(\n-    optimizer=\"adam\",\n-    loss=\"sparse_categorical_crossentropy\",\n-    metrics=[\"sparse_categorical_accuracy\"]\n-)\n-history = model.fit(\n-    x_train,\n-    y_train,\n-    batch_size=batch_size,\n-    epochs=epochs,\n-    callbacks=callbacks,\n-    validation_split=0.2,\n-    verbose=1,\n-)\n-\n-\"\"\"\n-## Evaluate model on test data\n-\"\"\"\n-\n-model = keras.models.load_model(\"best_model.keras\")\n-\n-test_loss, test_acc = model.evaluate(x_test, y_test)\n-\n-print(\"Test accuracy\", test_acc)\n-print(\"Test loss\", test_loss)\n-\n-\"\"\"\n-## Plot the model's training and validation loss\n-\"\"\"\n-\n-metric = \"sparse_categorical_accuracy\"\n-plt.figure()\n-plt.plot(history.history[metric])\n-plt.plot(history.history[\"val_\" + metric])\n-plt.title(\"model \" + metric)\n-plt.ylabel(metric, fontsize=\"large\")\n-plt.xlabel(\"epoch\", fontsize=\"large\")\n-plt.legend([\"train\", \"val\"], loc=\"best\")\n-plt.show()\n-plt.close()\n-\n-\"\"\"\n-We can see how the training accuracy reaches almost 0.95 after 100 epochs.\n-However, by observing the validation accuracy we can see how the network still needs\n-training until it reaches almost 0.97 for both the validation and the training accuracy\n-after 200 epochs. Beyond the 200th epoch, if we continue on training, the validation\n-accuracy will start decreasing while the training accuracy will continue on increasing:\n-the model starts overfitting.\n-\"\"\"\n\n@@ -35,3 +35,13 @@ def logsumexp(x, axis=None, keepdims=False):\n         jnp.log(jnp.sum(jnp.exp(x - max_x), axis=axis, keepdims=True)) + max_x\n     )\n     return jnp.squeeze(result) if not keepdims else result\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    return jax.numpy.linalg.qr(x, mode=mode)\n\n@@ -471,8 +471,6 @@ def tan(x):\n \n \n def tensordot(x1, x2, axes=2):\n-    x1 = convert_to_tensor(x1)\n-    x2 = convert_to_tensor(x2)\n     return jnp.tensordot(x1, x2, axes=axes)\n \n \n\n@@ -21,3 +21,15 @@ def in_top_k(targets, predictions, k):\n \n def logsumexp(x, axis=None, keepdims=False):\n     return tf.math.reduce_logsumexp(x, axis=axis, keepdims=keepdims)\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    if mode == \"reduced\":\n+        return tf.linalg.qr(x)\n+    return tf.linalg.qr(x, full_matrices=True)\n\n@@ -64,3 +64,13 @@ def logsumexp(x, axis=None, keepdims=False):\n         + max_x\n     )\n     return torch.squeeze(result, dim=axis) if not keepdims else result\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    return torch.linalg.qr(x, mode=mode)\n\n@@ -1,8 +1,5 @@\n import math\n \n-import numpy as np\n-\n-from keras_core import backend\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.backend import random\n@@ -652,13 +649,12 @@ class OrthogonalInitializer(Initializer):\n         # Generate a random matrix\n         a = random.normal(flat_shape, seed=self.seed, dtype=dtype)\n         # Compute the qr factorization\n-        q, r = np.linalg.qr(a)\n+        q, r = ops.qr(a)\n         # Make Q uniform\n-        d = np.diag(r)\n-        q *= np.sign(d)\n+        d = ops.diag(r)\n+        q *= ops.sign(d)\n         if num_rows < num_cols:\n-            q = np.transpose(q)\n-        q = backend.convert_to_tensor(q)\n+            q = ops.transpose(q)\n         return self.gain * ops.reshape(q, shape)\n \n     def get_config(self):\n\n@@ -106,3 +106,52 @@ def logsumexp(x, axis=None, keepdims=False):\n     if any_symbolic_tensors((x,)):\n         return Logsumexp(axis, keepdims).symbolic_call(x)\n     return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n+\n+\n+class Qr(Operation):\n+    def __init__(self, mode=\"reduced\"):\n+        super().__init__()\n+        if mode not in {\"reduced\", \"complete\"}:\n+            raise ValueError(\n+                \"`mode` argument value not supported. \"\n+                \"Expected one of {'reduced', 'complete'}. \"\n+                f\"Received: mode={mode}\"\n+            )\n+        self.mode = mode\n+\n+    def compute_output_spec(self, x):\n+        if len(x.shape) < 2:\n+            raise ValueError(\n+                \"Input should have rank >= 2. Received: \"\n+                f\"input.shape = {x.shape}\"\n+            )\n+        m = x.shape[-2]\n+        n = x.shape[-1]\n+        if m is None or n is None:\n+            raise ValueError(\n+                \"Input should have its last 2 dimensions \"\n+                \"fully-defined. Received: \"\n+                f\"input.shape = {x.shape}\"\n+            )\n+        k = min(m, n)\n+        base = tuple(x.shape[:-2])\n+        if self.mode == \"reduced\":\n+            return (\n+                KerasTensor(shape=base + (m, k), dtype=x.dtype),\n+                KerasTensor(shape=base + (k, n), dtype=x.dtype),\n+            )\n+        # 'complete' mode.\n+        return (\n+            KerasTensor(shape=base + (m, m), dtype=x.dtype),\n+            KerasTensor(shape=base + (m, n), dtype=x.dtype),\n+        )\n+\n+    def call(self, x):\n+        return backend.math.qr(x, mode=self.mode)\n+\n+\n+@keras_core_export(\"keras_core.operations.qr\")\n+def qr(x, mode=\"reduced\"):\n+    if any_symbolic_tensors((x,)):\n+        return Qr(mode=mode).symbolic_call(x)\n+    return backend.math.qr(x, mode=mode)\n\n@@ -38,6 +38,22 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         result = kmath.logsumexp(x)\n         self.assertEqual(result.shape, ())\n \n+    def test_qr(self):\n+        x = KerasTensor((None, 4, 3), dtype=\"float32\")\n+        q, r = kmath.qr(x, mode=\"reduced\")\n+        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\"reduced\")\n+        qref_shape = (None,) + qref.shape[1:]\n+        rref_shape = (None,) + rref.shape[1:]\n+        self.assertEqual(q.shape, qref_shape)\n+        self.assertEqual(r.shape, rref_shape)\n+\n+        q, r = kmath.qr(x, mode=\"complete\")\n+        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\"complete\")\n+        qref_shape = (None,) + qref.shape[1:]\n+        rref_shape = (None,) + rref.shape[1:]\n+        self.assertEqual(q.shape, qref_shape)\n+        self.assertEqual(r.shape, rref_shape)\n+\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -72,6 +88,18 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         result = kmath.logsumexp(x)\n         self.assertEqual(result.shape, ())\n \n+    def test_qr(self):\n+        x = KerasTensor((4, 3), dtype=\"float32\")\n+        q, r = kmath.qr(x, mode=\"reduced\")\n+        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\"reduced\")\n+        self.assertEqual(q.shape, qref.shape)\n+        self.assertEqual(r.shape, rref.shape)\n+\n+        q, r = kmath.qr(x, mode=\"complete\")\n+        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\"complete\")\n+        self.assertEqual(q.shape, qref.shape)\n+        self.assertEqual(r.shape, rref.shape)\n+\n \n class MathOpsCorrectnessTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -180,3 +208,15 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         outputs = kmath.logsumexp(x, axis=1)\n         expected = np.log(np.sum(np.exp(x), axis=1))\n         self.assertAllClose(outputs, expected)\n+\n+    def test_qr(self):\n+        x = np.random.random((4, 5))\n+        q, r = kmath.qr(x, mode=\"reduced\")\n+        qref, rref = np.linalg.qr(x, mode=\"reduced\")\n+        self.assertAllClose(qref, q)\n+        self.assertAllClose(rref, r)\n+\n+        q, r = kmath.qr(x, mode=\"complete\")\n+        qref, rref = np.linalg.qr(x, mode=\"complete\")\n+        self.assertAllClose(qref, q)\n+        self.assertAllClose(rref, r)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
