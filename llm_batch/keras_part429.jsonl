{"custom_id": "keras#4b5ea4914745b3f173b10175f4dc88663a4d35c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 217 | Files Changed: 4 | Hunks: 7 | Methods Changed: 13 | Complexity Δ (Sum/Max): -40/0 | Churn Δ: 229 | Churn Cumulative: 3735 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 20 | DMM Complexity: 0.514018691588785\n\nDIFF:\n@@ -1,59 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Contains keras-specific LossScale functionality.\n-\n-This functions cannot be in the non-keras loss_scale.py file since they depend\n-on keras, and files outside of keras should not depend on files inside keras.\n-\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from keras.utils import generic_utils\n-\n-\n-def serialize(loss_scale):\n-  return generic_utils.serialize_keras_object(loss_scale)\n-\n-\n-def deserialize(config, custom_objects=None):\n-  loss_scale_module_objects = {\n-      'FixedLossScale': tf.mixed_precision.experimental.FixedLossScale,\n-      'DynamicLossScale': tf.mixed_precision.experimental.DynamicLossScale,\n-  }\n-\n-  return generic_utils.deserialize_keras_object(\n-      config,\n-      module_objects=loss_scale_module_objects,\n-      custom_objects=custom_objects,\n-      printable_module_name='loss scale'\n-  )\n-\n-\n-def get(identifier):\n-  \"\"\"Get a loss scale object.\"\"\"\n-  if isinstance(identifier, dict):\n-    return deserialize(identifier)\n-\n-  if isinstance(identifier, (int, float)):\n-    return tf.mixed_precision.experimental.FixedLossScale(identifier)\n-  if identifier == 'dynamic':\n-    return tf.mixed_precision.experimental.DynamicLossScale()\n-  if isinstance(identifier, tf.mixed_precision.experimental.LossScale):\n-    return identifier\n-  elif identifier is None:\n-    return None\n-  else:\n-    raise ValueError('Could not interpret loss scale identifier: %s' %\n-                     identifier)\n\n@@ -1,152 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Benchmarks for LossScaleOptimizer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import time\n-from keras.mixed_precision import loss_scale_optimizer\n-from keras.optimizers.optimizer_v2 import adam\n-\n-\n-def _get_strategy(num_gpus):\n-  if num_gpus > 1:\n-    return tf.distribute.MirroredStrategy(\n-        ['/GPU:%d' % i for i in range(num_gpus)])\n-  else:\n-    return tf.distribute.get_strategy()  # The default strategy\n-\n-\n-class LossScaleBenchmark(tf.test.Benchmark):\n-  \"\"\"Benchmark for loss scaling.\"\"\"\n-\n-  def _benchmark(self, gradient_type, num_gpus, mode, loss_scaling):\n-    \"\"\"Benchmarks loss scaling.\n-\n-    We run a simple model with several scalar variables. The loss is the sum of\n-    all variables. The model is simple because we want to measure only the\n-    performance of loss scaling, not the performance of the model itself.\n-\n-    Args:\n-      gradient_type: \"optimizer\" or \"gradient_tape\". How gradients are computed.\n-        \"optimizer\" uses Optimizer.minimize. \"gradient_tape\" uses\n-        GradientTape.gradient along with LossScaleOptimizer.get_scaled_loss and\n-        LossScaleOptimizer.get_unscaled_gradients.\n-      num_gpus: The number of GPUs to use. Must be at least 1.\n-      mode: \"eager\" or \"tf_function\". \"tf_function\" causes all computations to\n-        be wrapped in a tf.function, while \"eager\" runs computations eagerly.\n-      loss_scaling: \"fixed\", \"dynamic\", or None. The type of loss scaling to\n-        use. None means use no loss scaling, which is useful as a baseline to\n-        see how much slower loss scaling is in comparison.\n-    \"\"\"\n-    ls_str = loss_scaling or 'no_loss_scaling'\n-    name = '%s_%d_GPU_%s_%s' % (gradient_type, num_gpus, mode, ls_str)\n-    with tf.__internal__.eager_context.eager_mode(), _get_strategy(num_gpus).scope() as strategy:\n-      opt = adam.Adam()\n-      if loss_scaling == 'fixed':\n-        loss_scale = tf.mixed_precision.experimental.FixedLossScale(2.)\n-      elif loss_scaling == 'dynamic':\n-        # Make increment_period so high that it's effectively infinite. This\n-        # means the loss scale will never change. Any performance overhead\n-        # from increasing/decreasing the loss scale is typically negligible\n-        # since it happens infrequently, so we only benchmark the common case\n-        # of the loss scale not changing.\n-        increment_period = 1000000\n-        loss_scale = tf.mixed_precision.experimental.DynamicLossScale(\n-            initial_loss_scale=2., increment_period=increment_period)\n-      else:\n-        assert loss_scaling is None\n-        loss_scale = None\n-      if loss_scale:\n-        opt = loss_scale_optimizer.LossScaleOptimizer(opt, loss_scale)\n-\n-      num_vars = 200\n-      num_warmup_iters = 1\n-      num_iters = 20\n-      # By using scalar variables, we reduce overhead of the actual GPU work of\n-      # multiplying variables, dividing gradients, and checking gradients for\n-      # NaNs. Measuring these overheads isn't very useful as there is little we\n-      # can do to reduce them (one such way would be to fuse dividing gradients\n-      # and checking them for NaNs). We still have all other overheads, such as\n-      # all-reducing the `is_finite` values and having a tf.cond or\n-      # tf.while_loop based on whether gradients are NaNs. Currently, these\n-      # other overheads are much more significant than the GPU work.\n-      var_list = [\n-          tf.Variable(i, dtype='float32') for i in range(num_vars)]\n-\n-      def get_loss():\n-        return tf.add_n(var_list)\n-\n-      if gradient_type == 'gradient_tape':\n-        if loss_scale is None:\n-          def minimize_fn():\n-            with tf.GradientTape() as tape:\n-              loss = get_loss()\n-            grads = tape.gradient(loss, var_list)\n-            return opt.apply_gradients(zip(grads, var_list))\n-        else:\n-          def minimize_fn():\n-            with tf.GradientTape() as tape:\n-              loss = get_loss()\n-              scaled_loss = opt.get_scaled_loss(loss)\n-            scaled_grads = tape.gradient(scaled_loss, var_list)\n-            grads = opt.get_unscaled_gradients(scaled_grads)\n-            return opt.apply_gradients(zip(grads, var_list))\n-      else:\n-        assert gradient_type == 'optimizer'\n-        def minimize_fn():\n-          return opt.minimize(get_loss, var_list)\n-\n-      def run_fn():\n-        strategy.run(minimize_fn)\n-      if mode == 'tf_function':\n-        run_fn = tf.function(run_fn)\n-\n-      for _ in range(num_warmup_iters):\n-        run_fn()\n-\n-      start = time.time()\n-      for _ in range(num_iters):\n-        run_fn()\n-      end = time.time()\n-      self.report_benchmark(iters=num_iters,\n-                            wall_time=(end - start) / num_iters, name=name)\n-\n-  def _gpus_to_test_with(self):\n-    num_gpus = len(tf.config.list_logical_devices('GPU'))\n-    gpus_to_test_with = []\n-    if num_gpus >= 1:\n-      gpus_to_test_with.append(1)\n-    if num_gpus >= 2:\n-      gpus_to_test_with.append(2)\n-    if num_gpus >= 8:\n-      gpus_to_test_with.append(8)\n-    return gpus_to_test_with\n-\n-  def benchmark_optimizer(self):\n-    for num_gpus in self._gpus_to_test_with():\n-      for mode in 'eager', 'tf_function':\n-        for loss_scaling in None, 'fixed', 'dynamic':\n-          self._benchmark('optimizer', num_gpus, mode, loss_scaling)\n-\n-  def benchmark_gradient_tape(self):\n-    for num_gpus in self._gpus_to_test_with():\n-      for mode in 'eager', 'tf_function':\n-        for loss_scaling in None, 'fixed', 'dynamic':\n-          self._benchmark('gradient_tape', num_gpus, mode, loss_scaling)\n-\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n@@ -16,10 +16,10 @@\n \n from keras import backend\n from keras import optimizers\n-from keras.mixed_precision import loss_scale as keras_loss_scale_module\n from keras.optimizers.optimizer_experimental import optimizer as optimizer_experimental\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.optimizers.optimizer_v2 import utils as optimizer_utils\n+from keras.utils import generic_utils\n \n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n@@ -819,12 +819,19 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n       # If loss_scale is in config, we assume we are deserializing a\n       # LossScaleOptimizer from TF 2.3 or below. We convert the config so it\n       # can be deserialized in the current LossScaleOptimizer.\n-      loss_scale = keras_loss_scale_module.deserialize(\n-          config.pop('loss_scale'))\n-      if isinstance(loss_scale, tf.mixed_precision.experimental.FixedLossScale):\n+      loss_scale = generic_utils.deserialize_keras_object(\n+          config.pop('loss_scale'),\n+          module_objects={\n+              'FixedLossScale': tf.compat.v1.mixed_precision.FixedLossScale,\n+              'DynamicLossScale': tf.compat.v1.mixed_precision.DynamicLossScale,\n+          },\n+          printable_module_name='loss scale')\n+\n+      if isinstance(loss_scale, tf.compat.v1.mixed_precision.FixedLossScale):\n         config['dynamic'] = False\n         config['initial_scale'] = loss_scale._loss_scale_value  # pylint: disable=protected-access\n-      elif isinstance(loss_scale, tf.mixed_precision.experimental.DynamicLossScale):\n+      elif isinstance(loss_scale,\n+                      tf.compat.v1.mixed_precision.DynamicLossScale):\n         config['dynamic'] = True\n         config['initial_scale'] = loss_scale.initial_loss_scale\n         config['dynamic_growth_steps'] = loss_scale.increment_period\n\n@@ -465,7 +465,6 @@ def _policy_equivalent_to_dtype(policy):\n   # We use type() instead of isinstance because a subclass of Policy is never\n   # equivalent to a dtype.\n   return (type(policy) == Policy and  # pylint: disable=unidiomatic-typecheck\n-          list(policy.get_config().keys()) == ['name'] and\n           (policy.name == '_infer' or _is_convertible_to_dtype(policy.name)))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0bcd03c807f010e51805eac998561c6e5bb5ecd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 117/117 | Churn Δ: 12 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -186,7 +186,7 @@ class SGD(Optimizer):\n \n     lr = self.lr\n     if self.initial_decay > 0:\n-      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n+      lr = lr * (\n           1. /\n           (1. +\n            self.decay * tf.cast(self.iterations,\n@@ -261,7 +261,7 @@ class RMSprop(Optimizer):\n \n     lr = self.lr\n     if self.initial_decay > 0:\n-      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n+      lr = lr * (\n           1. /\n           (1. +\n            self.decay * tf.cast(self.iterations,\n@@ -337,7 +337,7 @@ class Adagrad(Optimizer):\n \n     lr = self.lr\n     if self.initial_decay > 0:\n-      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n+      lr = lr * (\n           1. /\n           (1. +\n            self.decay * tf.cast(self.iterations,\n@@ -419,7 +419,7 @@ class Adadelta(Optimizer):\n \n     lr = self.lr\n     if self.initial_decay > 0:\n-      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n+      lr = lr * (\n           1. /\n           (1. +\n            self.decay * tf.cast(self.iterations,\n@@ -516,7 +516,7 @@ class Adam(Optimizer):\n \n     lr = self.lr\n     if self.initial_decay > 0:\n-      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n+      lr = lr * (\n           1. /\n           (1. +\n            self.decay * tf.cast(self.iterations,\n@@ -612,7 +612,7 @@ class Adamax(Optimizer):\n \n     lr = self.lr\n     if self.initial_decay > 0:\n-      lr = lr * (  # pylint: disable=g-no-augmented-assignment\n+      lr = lr * (\n           1. /\n           (1. +\n            self.decay * tf.cast(self.iterations,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a341b0eb2c4f7440d49795aa105b7869e716b5cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 152 | Files Changed: 9 | Hunks: 19 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 228 | Churn Cumulative: 1017 | Contributors (this commit): 3 | Commits (past 90d): 28 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -51,21 +51,7 @@ class Adadelta(optimizer.Optimizer):\n     rho: A `Tensor` or a floating point value. The decay rate. Defaults to 0.95.\n     epsilon: Small floating point value used to maintain numerical stability.\n       Defaults to 1e-7.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"Adadelta\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Reference:\n     - [Zeiler, 2012](http://arxiv.org/abs/1212.5701)\n@@ -154,3 +140,6 @@ class Adadelta(optimizer.Optimizer):\n         'epsilon': self.epsilon,\n     })\n     return config\n+\n+Adadelta.__doc__ = Adadelta.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -45,21 +45,7 @@ class Adagrad(optimizer.Optimizer):\n       Starting value for the accumulators (per-parameter momentum values).\n       Must be non-negative.\n     epsilon: Small floating point value used to maintain numerical stability.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"Adagrad\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Reference:\n     - [Duchi et al., 2011](\n@@ -136,3 +122,7 @@ class Adagrad(optimizer.Optimizer):\n         'epsilon': self.epsilon,\n     })\n     return config\n+\n+\n+Adagrad.__doc__ = Adagrad.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -54,21 +54,7 @@ class Adam(optimizer.Optimizer):\n       1e-7.\n     amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n       the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"Adam\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Reference:\n     - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n@@ -210,3 +196,7 @@ class Adam(optimizer.Optimizer):\n         'amsgrad': self.amsgrad,\n     })\n     return config\n+\n+\n+Adam.__doc__ = Adam.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -63,21 +63,7 @@ class Adamax(optimizer.Optimizer):\n     beta_2: A float value or a constant float tensor. The exponential decay\n       rate for the exponentially weighted infinity norm.\n     epsilon: A small constant for numerical stability.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"Adamax\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Reference:\n     - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n@@ -179,3 +165,7 @@ class Adamax(optimizer.Optimizer):\n         'epsilon': self.epsilon,\n     })\n     return config\n+\n+\n+Adamax.__doc__ = Adamax.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -59,21 +59,7 @@ class AdamW(optimizer.Optimizer):\n       1e-7.\n     amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n       the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"Adam\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Reference:\n     - [Loshchilov et al., 2019](https://arxiv.org/abs/1711.05101)\n@@ -228,3 +214,7 @@ class AdamW(optimizer.Optimizer):\n         'amsgrad': self.amsgrad,\n     })\n     return config\n+\n+\n+AdamW.__doc__ = AdamW.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -87,21 +87,7 @@ class Ftrl(optimizer.Optimizer):\n       When input is sparse shrinkage will only happen on the active weights.\n     beta: A float value, representing the beta value from the paper. Defaults to\n       0.0.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"Ftrl\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n   \"\"\"\n \n   def __init__(self,\n@@ -240,3 +226,7 @@ class Ftrl(optimizer.Optimizer):\n             self.beta,\n     })\n     return config\n+\n+\n+Ftrl.__doc__ = Ftrl.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -513,6 +513,39 @@ class _BaseOptimizer(tf.Module):\n     return cls(**config)\n \n \n+base_optimizer_keyword_args = \"\"\"name: String. The name to use\n+      for momentum accumulator weights created by\n+      the optimizer.\n+    clipnorm: Float. If set, the gradient of each weight is individually\n+      clipped so that its norm is no higher than this value.\n+    clipvalue: Float. If set, the gradient of each weight is clipped to be no\n+      higher than this value.\n+    global_clipnorm: Float. If set, the gradient of all weights is clipped so\n+      that their global norm is no higher than this value.\n+    use_ema: Boolean, defaults to False. If True, exponential moving average\n+      (EMA) is applied. EMA consists of computing an exponential moving\n+      average of the weights of the model (as the weight values change after\n+      each training batch), and periodically overwriting the weights with\n+      their moving average.\n+    ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`. This is\n+      the momentum to use when computing the EMA of the model's weights:\n+      `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n+      current_variable_value`.\n+    ema_overwrite_frequency: Int or None, defaults to None. Only used if\n+      `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations, we\n+      overwrite the model variable by its moving average. If None, the optimizer\n+       does not overwrite model variables in the middle of training, and you\n+      need to explicitly overwrite the variables at the end of training\n+      by calling `optimizer.finalize_variable_values()` (which updates the model\n+      variables in-place). When using the built-in `fit()` training loop, this\n+      happens automatically after the last epoch, and you don't need to do\n+      anything.\n+    jit_compile: Boolean, defaults to False. If True, the optimizer will use XLA\n+      compilation. `jit_compile` cannot be True when training with\n+      `tf.distribute.experimental.ParameterServerStrategy`.\n+    **kwargs: keyword arguments only used for backward compatibility.\"\"\"\n+\n+\n # pylint: disable=g-classes-have-attributes\n @keras_export(\"keras.optimizers.experimental.Optimizer\", v1=[])\n class Optimizer(_BaseOptimizer):\n@@ -522,38 +555,7 @@ class Optimizer(_BaseOptimizer):\n   optimizer, please subclass this class instead of _BaseOptimizer.\n \n   Args:\n-    name: string. The name to use for momentum accumulator weights created by\n-      the optimizer.\n-    clipnorm: float. If set, the gradient of each weight is individually\n-      clipped so that its norm is no higher than this value.\n-    clipvalue: float. If set, the gradient of each weight is clipped to be no\n-      higher than this value.\n-    global_clipnorm: float. If set, the gradient of all weights is clipped so\n-      that their global norm is no higher than this value.\n-    use_ema: boolean, default to False. If True, exponential moving average\n-      (EMA) is applied. EMA consists of computing an exponential moving\n-      average of the weights of the model (as the weight values change after\n-      each training batch), and periodically overwriting the weights with\n-      their moving average.\n-    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n-      the momentum to use when computing the EMA of the model's weights:\n-        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n-        current_variable_value`.\n-    ema_overwrite_frequency: int or None, default to None. Only used if\n-      `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations, we\n-      overwrite the model variable by its moving average. If None, the optimizer\n-       does not overwrite model variables in the middle of training, and you\n-      need to explicitly overwrite the variables at the end of training\n-      by calling `optimizer.finalize_variable_values()` (which updates the model\n-      variables in-place). When using the built-in `fit()` training loop, this\n-      happens automatically after the last epoch, and you don't need to do\n-      anything.\n-    jit_compile: bool, default to False. If True, the optimizer will use XLA\n-      acceleration. `jit_compile` can only be False when using Parameter\n-      Server Strategy.\n-    **kwargs: keyword arguments only used for backward compatibility with\n-      `optimizer_v2.OptimizerV2`. Any new code using\n-      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+    {{base_optimizer_keyword_args}}\n \n   ### Usage\n \n@@ -881,3 +883,6 @@ tf.__internal__.saved_model.load.register_revived_type(\n             min_producer_version=1,\n             min_consumer_version=1)\n     ])\n+\n+Optimizer.__doc__ = Optimizer.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", base_optimizer_keyword_args)\n\n@@ -53,21 +53,7 @@ class RMSprop(optimizer.Optimizer):\n       variance of the gradient; if False, by the uncentered second moment.\n       Setting this to `True` may help with training, but is slightly more\n       expensive in terms of computation and memory. Defaults to `False`.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"RMSprop\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Usage:\n \n@@ -207,3 +193,7 @@ class RMSprop(optimizer.Optimizer):\n         'centered': self.centered,\n     })\n     return config\n+\n+\n+RMSprop.__doc__ = RMSprop.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -58,21 +58,7 @@ class SGD(optimizer.Optimizer):\n       descent.\n     nesterov: boolean. Whether to apply Nesterov momentum.\n       Defaults to `False`.\n-    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n-    clipvalue: see the `clipvalue` argument of\n-      `optimizer_experimental.Optimizer`.\n-    global_clipnorm: see the `global_clipnorm` argument of\n-      `optimizer_experimental.Optimizer`.\n-    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n-    ema_momentum: see the `ema_momentum` argument of\n-      `optimizer_experimental.Optimizer`.\n-    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n-      `optimizer_experimental.Optimizer`.\n-    jit_compile: see the `jit_compile` argument of\n-      `optimizer_experimental.Optimizer`.\n-    name: Optional name prefix for the operations created when applying\n-      gradients. Defaults to `\"SGD\"`.\n-    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+    {{base_optimizer_keyword_args}}\n \n   Usage:\n \n@@ -203,3 +189,7 @@ class SGD(optimizer.Optimizer):\n         'nesterov': self.nesterov,\n     })\n     return config\n+\n+\n+SGD.__doc__ = SGD.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9577fa87b74247e22448479080fc2f4089cc2a43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 311 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -27,6 +27,7 @@ from keras.optimizers.optimizer_experimental import adagrad as adagrad_experimen\n from keras.optimizers.optimizer_experimental import adam as adam_experimental\n from keras.optimizers.optimizer_experimental import adamax as adamax_experimental\n from keras.optimizers.optimizer_experimental import adamw as adamw_experimental\n+from keras.optimizers.optimizer_experimental import ftrl as ftrl_experimental\n from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_experimental\n from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n from keras.optimizers.optimizer_v1 import Optimizer\n\n@@ -17,10 +17,13 @@\n from keras.optimizers.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n # pylint: disable=g-classes-have-attributes\n @generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.Ftrl', v1=[])\n class Ftrl(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the FTRL algorithm.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d882bb8e21fa63ee21a5e1323275f7e97f5037d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 16 | Methods Changed: 12 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 48 | Churn Cumulative: 2545 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 0.4\n\nDIFF:\n@@ -457,12 +457,13 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n           'go_backwards':\n               self.go_backwards,\n           'sequence_lengths':\n-              sequence_lengths\n+              sequence_lengths,\n+          'return_sequences':\n+              self.return_sequences\n       }\n       normal_gru_kwargs = gpu_gru_kwargs.copy()\n       normal_gru_kwargs.update({\n           'zero_output_for_mask': self.zero_output_for_mask,\n-          'return_sequences': self.return_sequences,\n       })\n \n       if tf.executing_eagerly():\n@@ -579,7 +580,7 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n \n \n def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n-            go_backwards, sequence_lengths):\n+            go_backwards, sequence_lengths, return_sequences):\n   \"\"\"GRU with cuDNN implementation which is only available for GPU.\"\"\"\n   if mask is not None:\n     sequence_lengths = gru_lstm_utils.calculate_sequence_by_mask(\n@@ -649,7 +650,7 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n         is_training=True, rnn_mode='gru')\n \n   last_output = outputs[-1]\n-  if not time_major and sequence_lengths is None:\n+  if not time_major and sequence_lengths is None and return_sequences:\n     outputs = tf.transpose(outputs, perm=[1, 0, 2])\n   h = tf.squeeze(h, axis=seq_axis)\n \n@@ -662,6 +663,10 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n   if sequence_lengths is not None:\n     last_output = h\n \n+  # Match CPU return format\n+  if not return_sequences:\n+    outputs = tf.expand_dims(last_output, axis=0 if time_major else 1)\n+\n   return last_output, outputs, h, gru_lstm_utils.runtime(\n       gru_lstm_utils.RUNTIME_GPU)\n \n@@ -734,7 +739,8 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n           mask=mask,\n           time_major=time_major,\n           go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n+          sequence_lengths=sequence_lengths,\n+          return_sequences=return_sequences)\n \n     def cudnn_gru_fn():\n       return gpu_gru(\n@@ -746,7 +752,8 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n           mask=mask,\n           time_major=time_major,\n           go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n+          sequence_lengths=sequence_lengths,\n+          return_sequences=return_sequences)\n \n     def standard_gru_fn():\n       return standard_gru(\n\n@@ -431,12 +431,13 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n             'go_backwards':\n                 self.go_backwards,\n             'sequence_lengths':\n-                row_lengths\n+                row_lengths,\n+            'return_sequences': \n+                self.return_sequences\n         }\n         normal_lstm_kwargs = gpu_lstm_kwargs.copy()\n         normal_lstm_kwargs.update({\n             'zero_output_for_mask': self.zero_output_for_mask,\n-            'return_sequences': self.return_sequences,\n         })\n \n         if tf.executing_eagerly():\n@@ -575,7 +576,7 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n \n \n def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n-             time_major, go_backwards, sequence_lengths):\n+             time_major, go_backwards, sequence_lengths, return_sequences):\n   \"\"\"LSTM with either cuDNN or ROCm implementation which is only available for GPU.\n \n   Note that currently only right padded data is supported, or the result will be\n@@ -600,12 +601,17 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n     sequence_lengths: The lengths of all sequences coming from a variable length\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n+    return_sequences: Boolean. If True, return the recurrent outputs for all\n+      timesteps in the sequence. If False, only return the output for the \n+      last timestep, matching the CPU function output format.\n \n   Returns:\n     last_output: Output tensor for the last timestep, which has shape\n       [batch, units].\n-    outputs: Output tensor for all timesteps, which has shape\n-      [batch, time, units].\n+    outputs: \n+      - If `return_sequences=True`: output tensor for all timesteps, \n+        which has shape [batch, time, units].\n+      - Else, a tensor equal to `last_output`\n     state_0: The cell output, which has same shape as init_h.\n     state_1: The cell hidden state, which has same shape as init_c.\n     runtime: Constant string tensor which indicate real runtime hardware. This\n@@ -684,7 +690,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n         is_training=True, rnn_mode='lstm')\n \n   last_output = outputs[-1]\n-  if not time_major and sequence_lengths is None:\n+  if not time_major and sequence_lengths is None and return_sequences:\n     outputs = tf.transpose(outputs, perm=[1, 0, 2])\n   h = tf.squeeze(h, axis=seq_axis)\n   c = tf.squeeze(c, axis=seq_axis)\n@@ -697,6 +703,11 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n   # the last_output, since it is numerically same as the output.\n   if sequence_lengths is not None:\n     last_output = h\n+  \n+  # Match CPU return format\n+  if not return_sequences:\n+    outputs = tf.expand_dims(last_output, axis=0 if time_major else 1)\n+\n   return last_output, outputs, h, c, gru_lstm_utils.runtime(\n       gru_lstm_utils.RUNTIME_GPU)\n \n@@ -774,7 +785,8 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n           mask=mask,\n           time_major=time_major,\n           go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n+          sequence_lengths=sequence_lengths,\n+          return_sequences=return_sequences)\n \n     def cudnn_lstm_fn():\n       return gpu_lstm(\n@@ -787,7 +799,8 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n           mask=mask,\n           time_major=time_major,\n           go_backwards=go_backwards,\n-          sequence_lengths=sequence_lengths)\n+          sequence_lengths=sequence_lengths,\n+          return_sequences=return_sequences)\n \n     def stardard_lstm_fn():\n       return standard_lstm(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88c26e2fbb92a93d884f405ebbec7eb1ad3daf50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 10354 | Contributors (this commit): 13 | Commits (past 90d): 17 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -4442,7 +4442,8 @@ def rnn(step_function,\n               - If `return_all_outputs=True`: a tensor with shape\n                 `(samples, time, ...)` where each entry `outputs[s, t]` is the\n                 output of the step function at time `t` for sample `s`\n-              - Else, a tensor equal to `last_output`\n+              - Else, a tensor equal to `last_output` with shape \n+                `(samples, 1, ...)`\n           new_states: list of tensors, latest states returned by\n               the step function, of shape `(samples, ...)`.\n \n\n@@ -530,7 +530,7 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n     outputs: \n       - If `return_sequences=True`: output tensor for all timesteps, \n         which has shape [batch, time, units].\n-      - Else, a tensor equal to `last_output`\n+      - Else, a tensor equal to `last_output` with shape [batch, 1, units]\n     state_0: the cell output, which has same shape as init_h.\n     runtime: constant string tensor which indicate real runtime hardware. This\n       value is for testing purpose and should be used by user.\n\n@@ -531,7 +531,7 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n     outputs: \n       - If `return_sequences=True`: output tensor for all timesteps, \n         which has shape [batch, time, units].\n-      - Else, a tensor equal to `last_output`\n+      - Else, a tensor equal to `last_output` with shape [batch, 1, units]\n     state_0: the cell output, which has same shape as init_h.\n     state_1: the cell hidden state, which has same shape as init_c.\n     runtime: constant string tensor which indicate real runtime hardware. This\n@@ -611,7 +611,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n     outputs: \n       - If `return_sequences=True`: output tensor for all timesteps, \n         which has shape [batch, time, units].\n-      - Else, a tensor equal to `last_output`\n+      - Else, a tensor equal to `last_output` with shape [batch, 1, units]\n     state_0: The cell output, which has same shape as init_h.\n     state_1: The cell hidden state, which has same shape as init_c.\n     runtime: Constant string tensor which indicate real runtime hardware. This\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af70910ffc1d8a3591bfea76728029e100e92a7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 28 | Files Changed: 3 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 43 | Churn Cumulative: 18714 | Contributors (this commit): 124 | Commits (past 90d): 37 | Contributors (cumulative): 133 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -43,7 +43,6 @@ from keras.saving.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n-from keras.utils import object_identity\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n from keras.utils import version_utils\n@@ -277,7 +276,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     self._training_state = None\n     self._saved_model_inputs_spec = None\n     self._saved_model_arg_spec = None\n-    self._trackable_saver = saver_with_op_caching(self)\n+    self._checkpoint = tf.train.Checkpoint(root=weakref.ref(self))\n \n     self._steps_per_execution = None\n \n@@ -2541,11 +2540,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       with h5py.File(filepath, 'w') as f:\n         hdf5_format.save_weights_to_hdf5_group(f, self)\n     else:\n-      if tf.executing_eagerly():\n-        session = None\n-      else:\n-        session = backend.get_session()\n-      self._trackable_saver.save(filepath, session=session, options=options)\n+      if not tf.executing_eagerly():\n+        # Call `get_session` to initialize any uninitialized variables.\n+        backend.get_session()\n+      self._checkpoint.write(filepath, options=options)\n+\n       # Record this checkpoint so it's visible from tf.train.latest_checkpoint.\n       tf.__internal__.train.update_checkpoint_state(\n           save_dir=os.path.dirname(filepath),\n@@ -2621,7 +2620,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     filepath, save_format = _detect_save_format(filepath)\n     if save_format == 'tf':\n-      status = self._trackable_saver.restore(filepath, options)\n+      status = self._checkpoint.read(filepath, options)\n       if by_name:\n         raise NotImplementedError(\n             'Weights may only be loaded based on topology into Models when '\n@@ -3492,16 +3491,6 @@ def _is_per_replica_instance(obj):\n           isinstance(obj, tf.__internal__.CompositeTensor))\n \n \n-def saver_with_op_caching(obj):\n-  if tf.executing_eagerly():\n-    saveables_cache = None\n-  else:\n-    saveables_cache = object_identity.ObjectIdentityWeakKeyDictionary()\n-  return tf.__internal__.tracking.TrackableSaver(\n-      tf.__internal__.tracking.ObjectGraphView(\n-          weakref.ref(obj), saveables_cache=saveables_cache))\n-\n-\n def disable_multi_worker(method):\n   \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"\n \n\n@@ -808,10 +808,9 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n         self._set_sentinels(root)\n         with self.assertRaises(AssertionError):\n           self._check_sentinels(root)\n-        object_saver = tf.__internal__.tracking.TrackableSaver(\n-            tf.__internal__.tracking.ObjectGraphView(root))\n+        object_saver = tf.train.Checkpoint(root=root)\n         self._set_sentinels(root)\n-        status = object_saver.restore(save_path)\n+        status = object_saver.read(save_path)\n         if tf.executing_eagerly():\n           self._check_sentinels(root)\n         if tf.executing_eagerly():\n@@ -830,14 +829,14 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n         status.run_restore_ops()\n         self._check_sentinels(root)\n         self._set_sentinels(root)\n-        status = object_saver.restore(save_path)\n+        status = object_saver.read(save_path)\n         status.initialize_or_restore()\n         status.assert_nontrivial_match()\n         self._check_sentinels(root)\n         # Check that there is no error when keys are missing from the name-based\n         # checkpoint.\n         root.not_in_name_checkpoint = tf.Variable([1.])\n-        status = object_saver.restore(save_path)\n+        status = object_saver.read(save_path)\n         with self.assertRaises(AssertionError):\n           status.assert_existing_objects_matched()\n \n\n@@ -619,10 +619,9 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n         self._set_sentinels(root)\n         with self.assertRaises(AssertionError):\n           self._check_sentinels(root)\n-        object_saver = tf.__internal__.tracking.TrackableSaver(\n-            tf.__internal__.tracking.ObjectGraphView(root))\n+        object_saver = tf.train.Checkpoint(root=root)\n         self._set_sentinels(root)\n-        status = object_saver.restore(save_path)\n+        status = object_saver.read(save_path)\n         if tf.executing_eagerly():\n           self._check_sentinels(root)\n         if tf.executing_eagerly():\n@@ -641,13 +640,13 @@ class CheckpointCompatibilityTests(test_combinations.TestCase):\n         status.run_restore_ops()\n         self._check_sentinels(root)\n         self._set_sentinels(root)\n-        status = object_saver.restore(save_path)\n+        status = object_saver.read(save_path)\n         status.initialize_or_restore()\n         self._check_sentinels(root)\n         # Check that there is no error when keys are missing from the name-based\n         # checkpoint.\n         root.not_in_name_checkpoint = tf.Variable([1.])\n-        status = object_saver.restore(save_path)\n+        status = object_saver.read(save_path)\n         with self.assertRaises(AssertionError):\n           status.assert_existing_objects_matched()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4e09d4f07b3f7a5c5c5ce37793706500882e5fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1379 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -187,7 +187,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n             `classifier_activation=None` to return the logits of the \"top\" layer.\n         include_preprocessing: Boolean, whether to include the preprocessing layer\n             (`Rescaling`) at the bottom of the network. Defaults to `True`.\n-            Note: if weights are 'imagenet' then Input image is normalized by ImageNet mean and standard deviation.\n+            Note: Input image is normalized by ImageNet mean and standard deviation.\n \n     Returns:\n         A `keras.Model` instance.\n@@ -564,7 +564,7 @@ def ResNetRS(\n             `classifier_activation=None` to return the logits of the \"top\" layer.\n         include_preprocessing: Boolean, whether to include the preprocessing layer\n             (`Rescaling`) at the bottom of the network. Defaults to `True`.\n-            Note: if weights are 'imagenet' then Input image is normalized by ImageNet mean and standard deviation.\n+            Note: Input image is normalized by ImageNet mean and standard deviation.\n \n     Returns:\n         A `tf.keras.Model` instance.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
