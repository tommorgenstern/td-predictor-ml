{"custom_id": "keras#bb626c120ea56b35b3fde9057877df461af0b02b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 983 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@\n from __future__ import absolute_import\n import sys\n from six.moves import cPickle\n-from six.moves import range\n \n \n def load_batch(fpath, label_key='labels'):\n\n@@ -2,7 +2,6 @@ from __future__ import absolute_import\n import copy\n import inspect\n import types\n-import numpy as np\n \n from ..utils.np_utils import to_categorical\n from ..models import Sequential\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#595d67ad7d919b44f3c4cabcf68c756c35b14c9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 837 | Contributors (this commit): 12 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -248,9 +248,12 @@ class ImageDataGenerator(object):\n         self.batch_index = 0\n \n     def _flow_index(self, N, batch_size=32, shuffle=False, seed=None):\n+        # ensure self.batch_index is 0\n+        self.reset()\n+\n         while 1:\n-            index_array = np.arange(N)\n             if self.batch_index == 0:\n+                index_array = np.arange(N)\n                 if shuffle:\n                     if seed is not None:\n                         np.random.seed(seed + self.total_batches_seen)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7cb41fc5cc5cef878cb8bfce40285f6cecdf1f24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2701 | Contributors (this commit): 8 | Commits (past 90d): 30 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -2273,7 +2273,7 @@ class Container(Layer):\n         for layer in flattened_layers:\n             g = f.create_group(layer.name)\n             symbolic_weights = layer.trainable_weights + layer.non_trainable_weights\n-            weight_values = layer.get_weights()\n+            weight_values = K.batch_get_value(symbolic_weights)\n             weight_names = []\n             for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n                 if hasattr(w, 'name') and w.name:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8327b37a0bd84290a6646692e2e75f45fba646e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 224 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ def euclidean_distance(vects):\n \n def eucl_dist_output_shape(shapes):\n     shape1, shape2 = shapes\n-    return shape1\n+    return (shape1[0], 1)\n \n \n def contrastive_loss(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39357b3045fd73a066b93ccfff2d16763e211b40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 585 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class Embedding(Layer):\n       model = Sequential()\n       model.add(Embedding(1000, 64, input_length=10))\n       # the model will take as input an integer matrix of size (batch, input_length).\n-      # the largest integer (i.e. word index) in the input should be no larger than 1000 (vocabulary size).\n+      # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n       # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n \n       input_array = np.random.randint(1000, size=(32, 10))\n@@ -28,7 +28,7 @@ class Embedding(Layer):\n     ```\n \n     # Arguments\n-      input_dim: int >= 0. Size of the vocabulary, ie.\n+      input_dim: int > 0. Size of the vocabulary, ie.\n           1 + maximum integer index occurring in the input data.\n       output_dim: int >= 0. Dimension of the dense embedding.\n       init: name of initialization function for the weights\n@@ -46,6 +46,8 @@ class Embedding(Layer):\n           This is useful for [recurrent layers](recurrent.md) which may take\n           variable length input. If this is `True` then all subsequent layers\n           in the model need to support masking or an exception will be raised.\n+          If mask_zero is set to True, as a consequence, index 0 cannot be\n+          used in the vocabulary (input_dim should equal |vocabulary| + 2).\n       input_length: Length of input sequences, when it is constant.\n           This argument is required if you are going to connect\n           `Flatten` then `Dense` layers upstream\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1b11b4eeb6bd4b163ce351af680c5a14d823443d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1626 | Contributors (this commit): 18 | Commits (past 90d): 38 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -499,15 +499,21 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n     positive integers.\n     '''\n     if dim_ordering == 'th':\n+        original_shape = int_shape(X)\n         new_shape = tf.shape(X)[2:]\n         new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n         X = permute_dimensions(X, [0, 2, 3, 1])\n         X = tf.image.resize_nearest_neighbor(X, new_shape)\n-        return permute_dimensions(X, [0, 3, 1, 2])\n+        X = permute_dimensions(X, [0, 3, 1, 2])\n+        X.set_shape((None, None, original_shape[2] * height_factor, original_shape[3] * width_factor))\n+        return X\n     elif dim_ordering == 'tf':\n+        original_shape = int_shape(X)\n         new_shape = tf.shape(X)[1:3]\n         new_shape *= tf.constant(np.array([height_factor, width_factor]).astype('int32'))\n-        return tf.image.resize_nearest_neighbor(X, new_shape)\n+        X = tf.image.resize_nearest_neighbor(X, new_shape)\n+        X.set_shape((None, original_shape[1] * height_factor, original_shape[2] * width_factor, None))\n+        return X\n     else:\n         raise Exception('Invalid dim_ordering: ' + dim_ordering)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5308033936aa05bec65f11a6769ffbc904d0cac3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1173 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -184,7 +184,7 @@ class RMSprop(Optimizer):\n             # update accumulator\n             new_a = self.rho * a + (1. - self.rho) * K.square(g)\n             self.updates.append((a, new_a))\n-            new_p = p - self.lr * g / K.sqrt(new_a + self.epsilon)\n+            new_p = p - self.lr * g / (K.sqrt(new_a) + self.epsilon)\n \n             # apply constraints\n             if p in constraints:\n@@ -211,7 +211,7 @@ class Adagrad(Optimizer):\n         lr: float >= 0. Learning rate.\n         epsilon: float >= 0.\n     '''\n-    def __init__(self, lr=0.01, epsilon=1e-6, **kwargs):\n+    def __init__(self, lr=0.01, epsilon=1e-8, **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n@@ -225,7 +225,7 @@ class Adagrad(Optimizer):\n         for p, g, a in zip(params, grads, self.weights):\n             new_a = a + K.square(g)  # update accumulator\n             self.updates.append((a, new_a))\n-            new_p = p - self.lr * g / K.sqrt(new_a + self.epsilon)\n+            new_p = p - self.lr * g / (K.sqrt(new_a) + self.epsilon)\n             # apply constraints\n             if p in constraints:\n                 c = constraints[p]\n@@ -255,7 +255,7 @@ class Adadelta(Optimizer):\n     # References\n         - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n     '''\n-    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, **kwargs):\n+    def __init__(self, lr=1.0, rho=0.95, epsilon=1e-8, **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5ae6f32dd1136226e7bd7a63a2120c317668d1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 109 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ def test_temporal_classification():\n     '''\n     np.random.seed(1337)\n     (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n+                                                         nb_test=500,\n                                                          input_shape=(3, 5),\n                                                          classification=True,\n                                                          nb_class=2)\n@@ -35,12 +35,12 @@ def test_temporal_classification():\n                   input_shape=(X_train.shape[1], X_train.shape[2]),\n                   activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n-                  optimizer='adadelta',\n+                  optimizer='adagrad',\n                   metrics=['accuracy'])\n-    history = model.fit(X_train, y_train, nb_epoch=5, batch_size=16,\n+    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=32,\n                         validation_data=(X_test, y_test),\n                         verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.9)\n+    assert(history.history['val_acc'][-1] >= 0.85)\n \n \n def test_temporal_regression():\n@@ -182,4 +182,5 @@ def test_masked_temporal():\n     assert(np.abs(history.history['val_loss'][-1] - ground_truth) < 0.06)\n \n if __name__ == '__main__':\n-    pytest.main([__file__])\n+    # pytest.main([__file__])\n+    test_temporal_classification()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#610ccba9f5c030e6106520359c27dc9c226670d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 34 | Files Changed: 17 | Hunks: 18 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 64 | Churn Cumulative: 3922 | Contributors (this commit): 27 | Commits (past 90d): 60 | Contributors (cumulative): 62 | DMM Complexity: None\n\nDIFF:\n@@ -29,8 +29,7 @@ Five digits inverted:\n from __future__ import print_function\n from keras.models import Sequential\n from keras.engine.training import slice_X\n-from keras.layers.core import Activation, TimeDistributedDense, RepeatVector\n-from keras.layers import recurrent\n+from keras.layers import Activation, TimeDistributedDense, RepeatVector, recurrent\n import numpy as np\n from six.moves import range\n \n\n@@ -12,7 +12,7 @@ backend (`K`), our code can run both on TensorFlow and Theano.\n \n from __future__ import print_function\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Layer, Activation\n+from keras.layers import Dense, Dropout, Layer, Activation\n from keras.datasets import mnist\n from keras import backend as K\n from keras.utils import np_utils\n\n@@ -16,8 +16,8 @@ Time per epoch: 3s on CPU (core i7).\n from __future__ import print_function\n from keras.models import Sequential\n from keras.layers.embeddings import Embedding\n-from keras.layers.core import Activation, Dense, Merge, Permute, Dropout\n-from keras.layers.recurrent import LSTM\n+from keras.layers import Activation, Dense, Merge, Permute, Dropout\n+from keras.layers import LSTM\n from keras.utils.data_utils import get_file\n from keras.preprocessing.sequence import pad_sequences\n from functools import reduce\n\n@@ -66,7 +66,7 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.utils.data_utils import get_file\n from keras.layers.embeddings import Embedding\n-from keras.layers.core import Dense, Merge, Dropout, RepeatVector\n+from keras.layers import Dense, Merge, Dropout, RepeatVector\n from keras.layers import recurrent\n from keras.models import Sequential\n from keras.preprocessing.sequence import pad_sequences\n\n@@ -15,8 +15,8 @@ from __future__ import print_function\n from keras.datasets import cifar10\n from keras.preprocessing.image import ImageDataGenerator\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Flatten\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.layers import Dense, Dropout, Activation, Flatten\n+from keras.layers import Convolution2D, MaxPooling2D\n from keras.optimizers import SGD\n from keras.utils import np_utils\n \n\n@@ -24,7 +24,7 @@ import h5py\n import os\n \n from keras.models import Sequential\n-from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n from keras import backend as K\n \n parser = argparse.ArgumentParser(description='Deep Dreams with Keras.')\n\n@@ -12,9 +12,9 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Lambda\n-from keras.layers.embeddings import Embedding\n-from keras.layers.convolutional import Convolution1D\n+from keras.layers import Dense, Dropout, Activation, Lambda\n+from keras.layers import Embedding\n+from keras.layers import Convolution1D\n from keras.datasets import imdb\n from keras import backend as K\n \n\n@@ -9,10 +9,10 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n-from keras.layers.convolutional import Convolution1D, MaxPooling1D\n+from keras.layers import Dense, Dropout, Activation\n+from keras.layers import Embedding\n+from keras.layers import LSTM, GRU, SimpleRNN\n+from keras.layers import Convolution1D, MaxPooling1D\n from keras.datasets import imdb\n \n \n\n@@ -19,9 +19,8 @@ np.random.seed(1337)  # for reproducibility\n from keras.preprocessing import sequence\n from keras.utils import np_utils\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n+from keras.layers import Dense, Dropout, Activation, Embedding\n+from keras.layers import LSTM, SimpleRNN, GRU\n from keras.datasets import imdb\n \n max_features = 20000\n\n@@ -12,8 +12,8 @@ has at least ~100k characters. ~1M is better.\n \n from __future__ import print_function\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Dropout\n-from keras.layers.recurrent import LSTM\n+from keras.layers import Dense, Activation, Dropout\n+from keras.layers import LSTM\n from keras.utils.data_utils import get_file\n import numpy as np\n import random\n\n@@ -11,8 +11,8 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Flatten\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.layers import Dense, Dropout, Activation, Flatten\n+from keras.layers import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n \n batch_size = 128\n\n@@ -17,9 +17,9 @@ from __future__ import print_function\n \n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n+from keras.layers import Dense, Activation\n+from keras.layers import SimpleRNN\n from keras.initializations import normal, identity\n-from keras.layers.recurrent import SimpleRNN\n from keras.optimizers import RMSprop\n from keras.utils import np_utils\n \n\n@@ -9,8 +9,8 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Flatten\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.layers import Dense, Dropout, Activation, Flatten\n+from keras.layers import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n from keras.wrappers.scikit_learn import KerasClassifier\n from sklearn.grid_search import GridSearchCV\n\n@@ -19,8 +19,8 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation, Flatten\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.layers import Dense, Dropout, Activation, Flatten\n+from keras.layers import Convolution2D, MaxPooling2D\n from keras.utils import np_utils\n \n \n\n@@ -58,7 +58,7 @@ import argparse\n import h5py\n \n from keras.models import Sequential\n-from keras.layers.convolutional import Convolution2D, ZeroPadding2D, MaxPooling2D\n+from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D\n from keras import backend as K\n \n parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n\n@@ -8,8 +8,7 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.datasets import reuters\n from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.layers.normalization import BatchNormalization\n+from keras.layers import Dense, Dropout, Activation\n from keras.utils import np_utils\n from keras.preprocessing.text import Tokenizer\n \n\n@@ -5,8 +5,7 @@ from __future__ import print_function\n import numpy as np\n import matplotlib.pyplot as plt\n from keras.models import Sequential\n-from keras.layers.core import Dense\n-from keras.layers.recurrent import LSTM\n+from keras.layers import Dense, LSTM\n \n \n # since we are using stateful rnn tsteps can be set to 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b581eb3f27a49c57f861ae64a3dfdd6f8fba53b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1175 | Contributors (this commit): 13 | Commits (past 90d): 8 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -168,7 +168,7 @@ class RMSprop(Optimizer):\n         rho: float >= 0.\n         epsilon: float >= 0. Fuzz factor.\n     '''\n-    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, **kwargs):\n+    def __init__(self, lr=0.001, rho=0.9, epsilon=1e-8, **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n         self.lr = K.variable(lr)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aea00258e7c8548bc0b9b91731fe606ce79509f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 583 | Contributors (this commit): 9 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class BatchNormalization(Layer):\n         Same shape as input.\n \n     # References\n-        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)\n+        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.html)\n     '''\n     def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.9,\n                  weights=None, beta_init='zero', gamma_init='one', **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3695bc2db5ed9aa1c1a2fe90160df83202c343e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 2705 | Contributors (this commit): 8 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1298,8 +1298,6 @@ class Merge(Layer):\n                     break\n                 output_shape[self.concat_axis] += shape[self.concat_axis]\n             return tuple(output_shape)\n-        elif self.mode == 'join':\n-            return None\n         elif self.mode == 'dot':\n             shape1 = list(input_shapes[0])\n             shape2 = list(input_shapes[1])\n@@ -1400,7 +1398,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n \n     # Arguments\n         mode: string or lambda/function. If string, must be one\n-            of: 'sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot'.\n+            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n         concat_axis: integer, axis to use in mode `concat`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#15d0b0ea08833b8f2db19e2da1ac428bad69d4cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 13 | Churn Cumulative: 2278 | Contributors (this commit): 19 | Commits (past 90d): 48 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -545,6 +545,8 @@ def repeat(x, n):\n \n \n def tile(x, n):\n+    if not hasattr(n, 'shape') and not hasattr(n, '__len__'):\n+        n = [n]\n     return tf.tile(x, n)\n \n \n\n@@ -91,6 +91,17 @@ class TestBackend(object):\n                 assert_allclose(np_rep, th_rep, atol=1e-05)\n                 assert_allclose(np_rep, tf_rep, atol=1e-05)\n \n+    def test_tile(self):\n+        shape = (3, 4)\n+        arr = np.arange(np.prod(shape)).reshape(shape)\n+        arr_th = KTH.variable(arr)\n+        arr_tf = KTF.variable(arr)\n+\n+        n = (2, 1)\n+        th_rep = KTH.eval(KTH.tile(arr_th, n))\n+        tf_rep = KTF.eval(KTF.tile(arr_tf, n))\n+        assert_allclose(tf_rep, th_rep, atol=1e-05)\n+\n     def test_value_manipulation(self):\n         val = np.random.random((4, 2))\n         xth = KTH.variable(val)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7ff7cde925f803919d954f5cb73a685b8f14b0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 98 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,98 @@\n+'''This script demonstrates how to build a variational autoencoder with Keras.\n+\n+Reference: \"Auto-Encoding Variational Bayes\" https://arxiv.org/abs/1312.6114\n+'''\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+from keras.layers import Input, Dense, Lambda\n+from keras.models import Model\n+from keras import backend as K\n+from keras import objectives\n+from keras.datasets import mnist\n+\n+batch_size = 16\n+original_dim = 784\n+latent_dim = 2\n+intermediate_dim = 128\n+epsilon_std = 0.01\n+nb_epoch = 40\n+\n+x = Input(batch_shape=(batch_size, original_dim))\n+h = Dense(intermediate_dim, activation='relu')(x)\n+z_mean = Dense(latent_dim)(h)\n+z_log_sigma = Dense(latent_dim)(h)\n+\n+def sampling(args):\n+    z_mean, z_log_sigma = args\n+    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n+                              mean=0., std=epsilon_std)\n+    return z_mean + K.exp(z_log_sigma) * epsilon\n+\n+# note that \"output_shape\" isn't necessary with the TensorFlow backend\n+# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n+z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n+\n+# we instantiate these layers separately so as to reuse them later\n+decoder_h = Dense(intermediate_dim, activation='relu')\n+decoder_mean = Dense(original_dim, activation='sigmoid')\n+h_decoded = decoder_h(z)\n+x_decoded_mean = decoder_mean(h_decoded)\n+\n+def vae_loss(x, x_decoded_mean):\n+    xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n+    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n+    return xent_loss + kl_loss\n+\n+vae = Model(x, x_decoded_mean)\n+vae.compile(optimizer='rmsprop', loss=vae_loss)\n+\n+# train the VAE on MNIST digits\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n+\n+x_train = x_train.astype('float32') / 255.\n+x_test = x_test.astype('float32') / 255.\n+x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n+x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n+\n+vae.fit(x_train, x_train,\n+        shuffle=True,\n+        nb_epoch=nb_epoch,\n+        batch_size=batch_size,\n+        validation_data=(x_test, x_test))\n+\n+# build a model to project inputs on the latent space\n+encoder = Model(x, z_mean)\n+\n+# display a 2D plot of the digit classes in the latent space\n+x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n+plt.figure(figsize=(6, 6))\n+plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n+plt.colorbar()\n+plt.show()\n+\n+# build a digit generator that can sample from the learned distribution\n+decoder_input = Input(shape=(latent_dim,))\n+_h_decoded = decoder_h(decoder_input)\n+_x_decoded_mean = decoder_mean(_h_decoded)\n+generator = Model(decoder_input, _x_decoded_mean)\n+\n+# display a 2D manifold of the digits\n+n = 15  # figure with 15x15 digits\n+digit_size = 28\n+figure = np.zeros((digit_size * n, digit_size * n))\n+# we will sample n points within [-15, 15] standard deviations\n+grid_x = np.linspace(-15, 15, n)\n+grid_y = np.linspace(-15, 15, n)\n+\n+for i, yi in enumerate(grid_x):\n+    for j, xi in enumerate(grid_y):\n+        z_sample = np.array([[xi, yi]]) * epsilon_std\n+        x_decoded = generator.predict(z_sample)\n+        digit = x_decoded[0].reshape(digit_size, digit_size)\n+        figure[i * digit_size: (i + 1) * digit_size,\n+               j * digit_size: (j + 1) * digit_size] = digit\n+\n+plt.figure(figsize=(10, 10))\n+plt.imshow(figure)\n+plt.show()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c2a573ebfef0fce5061c611bacbcae4dd5adc73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 197 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n from __future__ import absolute_import\r\n-__version__ = '1.0.2'\r\n from . import backend\r\n from . import datasets\r\n from . import engine\r\n@@ -15,3 +14,5 @@ from . import models\n from . import objectives\r\n from . import optimizers\r\n from . import regularizers\r\n+\r\n+__version__ = '1.0.3'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='1.0.2',\n+      version='1.0.3',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/1.0.2',\n+      download_url='https://github.com/fchollet/keras/tarball/1.0.3',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0ed174f2c6feed1091e38cbfb477853ea1212e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2711 | Contributors (this commit): 9 | Commits (past 90d): 32 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1015,9 +1015,9 @@ def Input(shape=None, batch_shape=None,\n         ```\n     '''\n     if not batch_shape:\n-        assert shape, ('Please provide to Input either an `input_shape`' +\n-                       ' or `batch_input_shape` argument. Note that ' +\n-                       '`input_shape` does not include the batch '\n+        assert shape, ('Please provide to Input either a `shape`' +\n+                       ' or a `batch_shape` argument. Note that ' +\n+                       '`shape` does not include the batch '\n                        'dimension.')\n         batch_shape = (None,) + tuple(shape)\n     input_layer = InputLayer(batch_input_shape=batch_shape,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2091347a712a35a820669b623ca22eac90aefc0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2712 | Contributors (this commit): 10 | Commits (past 90d): 33 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1220,6 +1220,7 @@ class Merge(Layer):\n             l2 = inputs[1]\n             denominator = K.sqrt(K.batch_dot(l1, l1, self.dot_axes) *\n                                  K.batch_dot(l2, l2, self.dot_axes))\n+            denominator = K.maximum(denominator, K.epsilon())\n             output = K.batch_dot(l1, l2, self.dot_axes) / denominator\n             output = K.expand_dims(output, 1)\n             return output\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa91994166422b9cd3d07721f4efd25549c7e7ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 36 | Churn Cumulative: 4601 | Contributors (this commit): 18 | Commits (past 90d): 67 | Contributors (cumulative): 20 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -2106,8 +2106,6 @@ class Container(Layer):\n         return output_tensors, output_masks, output_shapes\n \n     def get_config(self):\n-        '''TODO: add keras version information\n-        '''\n         config = {\n             'name': self.name,\n         }\n@@ -2347,6 +2345,26 @@ class Container(Layer):\n             K.batch_set_value(weight_value_tuples)\n         f.close()\n \n+    def _updated_config(self):\n+        '''shared between different serialization methods'''\n+        from keras import __version__ as keras_version\n+\n+        config = self.get_config()\n+        model_config = {\n+            'class_name': self.__class__.__name__,\n+            'config': config,\n+            'keras_version': keras_version\n+        }\n+\n+        if hasattr(self, 'optimizer'):\n+            model_config['optimizer'] = self.optimizer.get_config()\n+            model_config['loss'] = self.loss.__class__.__name__\n+            model_config['sample_weight_mode'] = self.sample_weight_mode\n+\n+        if hasattr(self, 'loss_weights'):\n+            model_config['loss_weights'] = self.loss_weights\n+        return model_config\n+\n     def to_json(self, **kwargs):\n         '''Returns a JSON string containing the network configuration.\n \n@@ -2366,11 +2384,7 @@ class Container(Layer):\n \n             raise TypeError('Not JSON Serializable')\n \n-        config = self.get_config()\n-        model_config = {\n-            'class_name': self.__class__.__name__,\n-            'config': config,\n-        }\n+        model_config = self._updated_config()\n         return json.dumps(model_config, default=get_json_type, **kwargs)\n \n     def to_yaml(self, **kwargs):\n@@ -2384,12 +2398,7 @@ class Container(Layer):\n         functions / classes.\n         '''\n         import yaml\n-        config = self.get_config()\n-        model_config = {\n-            'class_name': self.__class__.__name__,\n-            'config': config,\n-        }\n-        return yaml.dump(model_config, **kwargs)\n+        return yaml.dump(self._updated_config(), **kwargs)\n \n     def summary(self):\n         from keras.utils.layer_utils import print_summary\n\n@@ -452,6 +452,7 @@ class Model(Container):\n         self.optimizer = optimizers.get(optimizer)\n         self.sample_weight_mode = sample_weight_mode\n         self.loss = loss\n+        self.loss_weights = loss_weights\n \n         # prepare loss weights\n         if loss_weights is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8a717f5b6c709cfd521548fc525f2c20c2b6b78f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 110 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -21,17 +21,17 @@ nb_epoch = 40\n x = Input(batch_shape=(batch_size, original_dim))\n h = Dense(intermediate_dim, activation='relu')(x)\n z_mean = Dense(latent_dim)(h)\n-z_log_sigma = Dense(latent_dim)(h)\n+z_log_std = Dense(latent_dim)(h)\n \n def sampling(args):\n-    z_mean, z_log_sigma = args\n+    z_mean, z_log_std = args\n     epsilon = K.random_normal(shape=(batch_size, latent_dim),\n                               mean=0., std=epsilon_std)\n-    return z_mean + K.exp(z_log_sigma) * epsilon\n+    return z_mean + K.exp(z_log_std) * epsilon\n \n # note that \"output_shape\" isn't necessary with the TensorFlow backend\n-# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n-z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n+# so you could write `Lambda(sampling)([z_mean, z_log_std])`\n+z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_std])\n \n # we instantiate these layers separately so as to reuse them later\n decoder_h = Dense(intermediate_dim, activation='relu')\n@@ -41,7 +41,7 @@ x_decoded_mean = decoder_mean(h_decoded)\n \n def vae_loss(x, x_decoded_mean):\n     xent_loss = objectives.binary_crossentropy(x, x_decoded_mean)\n-    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n+    kl_loss = - 0.5 * K.mean(1 + z_log_std - K.square(z_mean) - K.exp(z_log_std), axis=-1)\n     return xent_loss + kl_loss\n \n vae = Model(x, x_decoded_mean)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ebbc4d9fb83e36b1f19ffce1b8e5c0386eeb214a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 1555 | Contributors (this commit): 16 | Commits (past 90d): 12 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -462,8 +462,7 @@ class TensorBoard(Callback):\n                                          layer.output)\n         self.merged = tf.merge_all_summaries()\n         if self.write_graph:\n-            tf_version = tuple(int(i) for i in tf.__version__.split('.'))\n-            if tf_version >= (0, 8, 0):\n+            if tf.__version__ >= '0.8.0':\n                 self.writer = tf.train.SummaryWriter(self.log_dir,\n                                                      self.sess.graph)\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2fb8b2786817b4014c077c13e99efb551fe35c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 788 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -94,8 +94,13 @@ def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n             pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))\n \n \n-path = get_file('babi-tasks-v1-2.tar.gz',\n-                origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n+try:\n+    path = get_file('babi-tasks-v1-2.tar.gz', origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n+except:\n+    print('Error downloading dataset, please download it manually:\\n'\n+          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n+          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n+    raise\n tar = tarfile.open(path)\n \n challenges = {\n\n@@ -146,7 +146,13 @@ BATCH_SIZE = 32\n EPOCHS = 40\n print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE, QUERY_HIDDEN_SIZE))\n \n+try:\n     path = get_file('babi-tasks-v1-2.tar.gz', origin='http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n+except:\n+    print('Error downloading dataset, please download it manually:\\n'\n+          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n+          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n+    raise\n tar = tarfile.open(path)\n # Default QA1 with 1000 samples\n # challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f8acfe4bf340ccae58ade4cbfddb0513fa1d4dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 3380 | Contributors (this commit): 21 | Commits (past 90d): 39 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -2400,7 +2400,7 @@ class Container(Layer):\n         import yaml\n         return yaml.dump(self._updated_config(), **kwargs)\n \n-    def summary(self):\n+    def summary(self, line_length=100, positions=[.33, .55, .67, 1.]):\n         from keras.utils.layer_utils import print_summary\n \n         if hasattr(self, 'flattened_layers'):\n@@ -2409,7 +2409,7 @@ class Container(Layer):\n         else:\n             flattened_layers = self.layers\n \n-        print_summary(flattened_layers, getattr(self, 'container_nodes', None))\n+        print_summary(flattened_layers, getattr(self, 'container_nodes', None), line_length=line_length, positions=positions)\n \n \n def get_source_inputs(tensor, layer=None, node_index=None):\n\n@@ -35,9 +35,11 @@ def layer_from_config(config, custom_objects={}):\n     return layer_class.from_config(config['config'])\n \n \n-def print_summary(layers, relevant_nodes=None):\n-    line_length = 100  # total length of printed lines\n-    positions = [35, 55, 67, 100]  # absolute positions of log elements in each line\n+def print_summary(layers, relevant_nodes=None, line_length=100, positions=[.33, .55, .67, 1.]):\n+    # line_length: total length of printed lines\n+    # positions: relative or absolute positions of log elements in each line\n+    if positions[-1] <= 1:\n+        positions = [int(line_length * p) for p in positions]\n     # header names for the different log elements\n     to_display = ['Layer (type)', 'Output Shape', 'Param #', 'Connected to']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0bfc24adc41840d937f6f485bb69591532cc538", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 20 | Methods Changed: 23 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 78 | Churn Cumulative: 374 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -12,11 +12,13 @@ def get_fans(shape, dim_ordering='th'):\n         # TH kernel shape: (depth, input_depth, ...)\n         # TF kernel shape: (..., input_depth, depth)\n         if dim_ordering == 'th':\n-            fan_in = np.prod(shape[1:])\n-            fan_out = shape[0]\n+            receptive_field_size = np.prod(shape[2:])\n+            fan_in = shape[1] * receptive_field_size\n+            fan_out = shape[0] * receptive_field_size\n         elif dim_ordering == 'tf':\n-            fan_in = np.prod(shape[:-1])\n-            fan_out = shape[-1]\n+            receptive_field_size = np.prod(shape[:2])\n+            fan_in = shape[-2] * receptive_field_size\n+            fan_out = shape[-1] * receptive_field_size\n         else:\n             raise Exception('Invalid dim_ordering: ' + dim_ordering)\n     else:\n\n@@ -4,8 +4,14 @@ import numpy as np\n from keras import initializations\n from keras import backend as K\n \n-SHAPE = (100, 100)\n+# 2D tensor test fixture\n+FC_SHAPE = (100, 100)\n \n+# 4D convolution in th order. This shape has the same effective shape as FC_SHAPE\n+CONV_SHAPE = (25, 25, 2, 2)\n+\n+# The equivalent shape of both test fixtures\n+SHAPE = (100, 100)\n \n def _runner(init, shape, target_mean=None, target_std=None,\n             target_max=None, target_min=None):\n@@ -22,62 +28,78 @@ def _runner(init, shape, target_mean=None, target_std=None,\n         assert abs(output.min() - target_min) < lim\n \n \n-def test_uniform():\n-    _runner(initializations.uniform, SHAPE, target_mean=0.,\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_uniform(tensor_shape):\n+    _runner(initializations.uniform, tensor_shape, target_mean=0.,\n             target_max=0.05, target_min=-0.05)\n \n \n-def test_normal():\n-    _runner(initializations.normal, SHAPE, target_mean=0., target_std=0.05)\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_normal(tensor_shape):\n+    _runner(initializations.normal, tensor_shape, target_mean=0., target_std=0.05)\n \n \n-def test_lecun_uniform():\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_lecun_uniform(tensor_shape):\n     scale = np.sqrt(3. / SHAPE[0])\n-    _runner(initializations.lecun_uniform, SHAPE,\n+    _runner(initializations.lecun_uniform, tensor_shape,\n             target_mean=0., target_max=scale, target_min=-scale)\n \n \n-def test_glorot_uniform():\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_glorot_uniform(tensor_shape):\n     scale = np.sqrt(6. / (SHAPE[0] + SHAPE[1]))\n-    _runner(initializations.glorot_uniform, SHAPE, target_mean=0.,\n+    _runner(initializations.glorot_uniform, tensor_shape, target_mean=0.,\n             target_max=scale, target_min=-scale)\n \n \n-def test_glorot_normal():\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_glorot_normal(tensor_shape):\n     scale = np.sqrt(2. / (SHAPE[0] + SHAPE[1]))\n-    _runner(initializations.glorot_normal, SHAPE,\n+    _runner(initializations.glorot_normal, tensor_shape,\n             target_mean=0., target_std=scale)\n \n \n-def test_he_uniform():\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_he_uniform(tensor_shape):\n     scale = np.sqrt(6. / SHAPE[0])\n-    _runner(initializations.he_uniform, SHAPE, target_mean=0.,\n+    _runner(initializations.he_uniform, tensor_shape, target_mean=0.,\n             target_max=scale, target_min=-scale)\n \n \n-def test_he_normal():\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_he_normal(tensor_shape):\n     scale = np.sqrt(2. / SHAPE[0])\n-    _runner(initializations.he_normal, SHAPE,\n+    _runner(initializations.he_normal, tensor_shape,\n             target_mean=0., target_std=scale)\n \n \n-def test_orthogonal():\n-    _runner(initializations.orthogonal, SHAPE,\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_orthogonal(tensor_shape):\n+    _runner(initializations.orthogonal, tensor_shape,\n             target_mean=0.)\n \n \n-def test_identity():\n-    _runner(initializations.identity, SHAPE,\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_identity(tensor_shape):\n+    if len(tensor_shape) > 2:\n+        with pytest.raises(Exception):\n+            _runner(initializations.identity, tensor_shape,\n+                    target_mean=1./SHAPE[0], target_max=1.)\n+    else:\n+        _runner(initializations.identity, tensor_shape,\n                 target_mean=1./SHAPE[0], target_max=1.)\n \n \n-def test_zero():\n-    _runner(initializations.zero, SHAPE,\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_zero(tensor_shape):\n+    _runner(initializations.zero, tensor_shape,\n             target_mean=0., target_max=0.)\n \n \n-def test_one():\n-    _runner(initializations.one, SHAPE,\n+@pytest.mark.parametrize('tensor_shape', [FC_SHAPE, CONV_SHAPE], ids=['FC', 'CONV'])\n+def test_one(tensor_shape):\n+    _runner(initializations.one, tensor_shape,\n             target_mean=1., target_max=1.)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
