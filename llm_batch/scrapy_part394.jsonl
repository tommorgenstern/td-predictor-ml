{"custom_id": "scrapy#a195af304d2823cc686fd7354790e52239208d82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 25 | Files Changed: 2 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 75 | Churn Cumulative: 3122 | Contributors (this commit): 31 | Commits (past 90d): 9 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,36 +1,47 @@\n \"\"\"\n This module contains general purpose URL functions not found in the standard\n library.\n-\n-Some of the functions that used to be imported from this module have been moved\n-to the w3lib.url module. Always import those from there instead.\n \"\"\"\n \n from __future__ import annotations\n \n import re\n-from typing import TYPE_CHECKING, Union, cast\n+import warnings\n+from importlib import import_module\n+from typing import TYPE_CHECKING, Union\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n-# scrapy.utils.url was moved to w3lib.url and import * ensures this\n-# move doesn't break old code\n-from w3lib.url import *  # pylint: disable=unused-wildcard-import,wildcard-import\n-from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n+from w3lib.url import __all__ as _public_w3lib_objects\n+from w3lib.url import add_or_replace_parameter as _add_or_replace_parameter\n+from w3lib.url import any_to_uri as _any_to_uri\n+from w3lib.url import parse_url as _parse_url\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+\n+def __getattr__(name: str):\n+    if name in (\"_unquotepath\", \"_safe_chars\", \"parse_url\", *_public_w3lib_objects):\n+        obj_type = \"attribute\" if name == \"_safe_chars\" else \"function\"\n+        warnings.warn(\n+            f\"The scrapy.utils.url.{name} {obj_type} is deprecated, use w3lib.url.{name} instead.\",\n+            ScrapyDeprecationWarning,\n+        )\n+        return getattr(import_module(\"w3lib.url\"), name)\n+\n+    raise AttributeError\n \n-from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n     from collections.abc import Iterable\n \n     from scrapy import Spider\n \n-\n UrlT = Union[str, bytes, ParseResult]\n \n \n def url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n     \"\"\"Return True if the url belongs to any of the given domains\"\"\"\n-    host = parse_url(url).netloc.lower()\n+    host = _parse_url(url).netloc.lower()\n     if not host:\n         return False\n     domains = [d.lower() for d in domains]\n@@ -46,21 +57,10 @@ def url_is_from_spider(url: UrlT, spider: type[Spider]) -> bool:\n \n def url_has_any_extension(url: UrlT, extensions: Iterable[str]) -> bool:\n     \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n-    lowercase_path = parse_url(url).path.lower()\n+    lowercase_path = _parse_url(url).path.lower()\n     return any(lowercase_path.endswith(ext) for ext in extensions)\n \n \n-def parse_url(  # pylint: disable=function-redefined\n-    url: UrlT, encoding: str | None = None\n-) -> ParseResult:\n-    \"\"\"Return urlparsed url from the given argument (which could be an already\n-    parsed url)\n-    \"\"\"\n-    if isinstance(url, ParseResult):\n-        return url\n-    return cast(ParseResult, urlparse(to_unicode(url, encoding)))\n-\n-\n def escape_ajax(url: str) -> str:\n     \"\"\"\n     Return the crawlable url\n@@ -86,7 +86,7 @@ def escape_ajax(url: str) -> str:\n     defrag, frag = urldefrag(url)\n     if not frag.startswith(\"!\"):\n         return url\n-    return add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n+    return _add_or_replace_parameter(defrag, \"_escaped_fragment_\", frag[1:])\n \n \n def add_http_if_no_scheme(url: str) -> str:\n@@ -146,7 +146,7 @@ def guess_scheme(url: str) -> str:\n     \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n     http:// otherwise.\"\"\"\n     if _is_filesystem_path(url):\n-        return any_to_uri(url)\n+        return _any_to_uri(url)\n     return add_http_if_no_scheme(url)\n \n \n\n@@ -1,10 +1,14 @@\n import unittest\n+import warnings\n+\n+import pytest\n \n from scrapy.linkextractors import IGNORED_EXTENSIONS\n from scrapy.spiders import Spider\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import (\n     _is_filesystem_path,\n+    _public_w3lib_objects,\n     add_http_if_no_scheme,\n     guess_scheme,\n     strip_url,\n@@ -607,5 +611,26 @@ class IsPathTestCase(unittest.TestCase):\n             )\n \n \n+@pytest.mark.parametrize(\n+    \"obj_name\",\n+    [\n+        \"_unquotepath\",\n+        \"_safe_chars\",\n+        \"parse_url\",\n+        *_public_w3lib_objects,\n+    ],\n+)\n+def test_deprecated_imports_from_w3lib(obj_name):\n+    with warnings.catch_warnings(record=True) as warns:\n+        obj_type = \"attribute\" if obj_name == \"_safe_chars\" else \"function\"\n+        message = f\"The scrapy.utils.url.{obj_name} {obj_type} is deprecated, use w3lib.url.{obj_name} instead.\"\n+\n+        from importlib import import_module\n+\n+        getattr(import_module(\"scrapy.utils.url\"), obj_name)\n+\n+        assert message in warns[0].message.args\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc484efd43b0f8ba0dc89904a7d38086775c44ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 3573 | Contributors (this commit): 47 | Commits (past 90d): 7 | Contributors (cumulative): 65 | DMM Complexity: None\n\nDIFF:\n@@ -17,9 +17,14 @@ from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.internet.protocol import Factory, Protocol, connectionDone\n from twisted.python.failure import Failure\n-from twisted.web.client import URI, Agent, HTTPConnectionPool\n+from twisted.web.client import (\n+    URI,\n+    Agent,\n+    HTTPConnectionPool,\n+    ResponseDone,\n+    ResponseFailed,\n+)\n from twisted.web.client import Response as TxResponse\n-from twisted.web.client import ResponseDone, ResponseFailed\n from twisted.web.http import PotentialDataLoss, _DataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer, IPolicyForHTTPS\n\n@@ -2,9 +2,8 @@ from __future__ import annotations\n \n import re\n import time\n-from http.cookiejar import Cookie\n+from http.cookiejar import Cookie, CookiePolicy, DefaultCookiePolicy\n from http.cookiejar import CookieJar as _CookieJar\n-from http.cookiejar import CookiePolicy, DefaultCookiePolicy\n from typing import TYPE_CHECKING, Any, cast\n \n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -1,9 +1,8 @@\n from urllib.parse import urlparse\n \n from twisted.internet import reactor\n-from twisted.names import cache\n+from twisted.names import cache, resolve\n from twisted.names import hosts as hostsModule\n-from twisted.names import resolve\n from twisted.names.client import Resolver\n from twisted.python.runtime import platform\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7af7b282d6d3b36689cc192e6f78c065e32fb89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 1818 | Contributors (this commit): 34 | Commits (past 90d): 16 | Contributors (cumulative): 59 | DMM Complexity: 0.25\n\nDIFF:\n@@ -131,25 +131,26 @@ class CookiesMiddleware:\n         decoded = {}\n         flags = set()\n         for key in (\"name\", \"value\", \"path\", \"domain\"):\n-            if cookie.get(key) is None:\n+            value = cookie.get(key)\n+            if value is None:\n                 if key in (\"name\", \"value\"):\n                     msg = f\"Invalid cookie found in request {request}: {cookie} ('{key}' is missing)\"\n                     logger.warning(msg)\n                     return None\n                 continue\n-            # https://github.com/python/mypy/issues/7178, https://github.com/python/mypy/issues/9168\n-            if isinstance(cookie[key], (bool, float, int, str)):  # type: ignore[literal-required]\n-                decoded[key] = str(cookie[key])  # type: ignore[literal-required]\n+            if isinstance(value, (bool, float, int, str)):\n+                decoded[key] = str(value)\n             else:\n+                assert isinstance(value, bytes)\n                 try:\n-                    decoded[key] = cookie[key].decode(\"utf8\")  # type: ignore[literal-required]\n+                    decoded[key] = value.decode(\"utf8\")\n                 except UnicodeDecodeError:\n                     logger.warning(\n                         \"Non UTF-8 encoded cookie found in request %s: %s\",\n                         request,\n                         cookie,\n                     )\n-                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")  # type: ignore[literal-required]\n+                    decoded[key] = value.decode(\"latin1\", errors=\"replace\")\n         for flag in (\"secure\",):\n             value = cookie.get(flag, _UNSET)\n             if value is _UNSET or not value:\n\n@@ -75,7 +75,7 @@ class TelnetConsole(protocol.ServerFactory):\n     def stop_listening(self) -> None:\n         self.port.stopListening()\n \n-    def protocol(self) -> telnet.TelnetTransport:  # type: ignore[override]\n+    def protocol(self) -> telnet.TelnetTransport:\n         # these import twisted.internet.reactor\n         from twisted.conch import manhole, telnet\n         from twisted.conch.insults import insults\n\n@@ -44,10 +44,10 @@ if TYPE_CHECKING:\n \n \n class VerboseCookie(TypedDict):\n-    name: str\n-    value: str\n-    domain: NotRequired[str]\n-    path: NotRequired[str]\n+    name: str | bytes\n+    value: str | bytes | bool | float | int\n+    domain: NotRequired[str | bytes]\n+    path: NotRequired[str | bytes]\n     secure: NotRequired[bool]\n \n \n\n@@ -26,13 +26,15 @@ class Sitemap:\n         )\n         self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # noqa: S320\n         rt = self._root.tag\n-        self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n+        assert isinstance(rt, str)\n+        self.type = rt.split(\"}\", 1)[1] if \"}\" in rt else rt\n \n     def __iter__(self) -> Iterator[dict[str, Any]]:\n         for elem in self._root.getchildren():\n             d: dict[str, Any] = {}\n             for el in elem.getchildren():\n                 tag = el.tag\n+                assert isinstance(tag, str)\n                 name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n \n                 if name == \"link\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#838ff99f37d88214829018c6a7dd2a84fdb418b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 51 | Files Changed: 27 | Hunks: 38 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 103 | Churn Cumulative: 22386 | Contributors (this commit): 133 | Commits (past 90d): 121 | Contributors (cumulative): 487 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,14 +13,14 @@ from scrapy.selector import Selector\n from scrapy.spiders import Spider\n \n __all__ = [\n+    \"Field\",\n+    \"FormRequest\",\n+    \"Item\",\n+    \"Request\",\n+    \"Selector\",\n+    \"Spider\",\n     \"__version__\",\n     \"version_info\",\n-    \"Spider\",\n-    \"Request\",\n-    \"FormRequest\",\n-    \"Selector\",\n-    \"Item\",\n-    \"Field\",\n ]\n \n \n\n@@ -39,9 +39,8 @@ class Command(BaseRunSpiderCommand):\n         else:\n             self.crawler_process.start()\n \n-            if (\n-                self.crawler_process.bootstrap_failed\n-                or hasattr(self.crawler_process, \"has_exception\")\n+            if self.crawler_process.bootstrap_failed or (\n+                hasattr(self.crawler_process, \"has_exception\")\n                 and self.crawler_process.has_exception\n             ):\n                 self.exitcode = 1\n\n@@ -269,7 +269,7 @@ class Command(BaseRunSpiderCommand):\n         assert self.crawler_process\n         assert self.spidercls\n         self.crawler_process.crawl(self.spidercls, **opts.spargs)\n-        self.pcrawler = list(self.crawler_process.crawlers)[0]\n+        self.pcrawler = next(iter(self.crawler_process.crawlers))\n         self.crawler_process.start()\n \n         if not self.first_response:\n\n@@ -20,7 +20,7 @@ def _import_file(filepath: str | PathLike[str]) -> ModuleType:\n     if abspath.suffix not in (\".py\", \".pyw\"):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n     dirname = str(abspath.parent)\n-    sys.path = [dirname] + sys.path\n+    sys.path = [dirname, *sys.path]\n     try:\n         module = import_module(abspath.stem)\n     finally:\n\n@@ -101,12 +101,14 @@ class BaseRedirectMiddleware:\n         if ttl and redirects <= self.max_redirect_times:\n             redirected.meta[\"redirect_times\"] = redirects\n             redirected.meta[\"redirect_ttl\"] = ttl - 1\n-            redirected.meta[\"redirect_urls\"] = request.meta.get(\"redirect_urls\", []) + [\n-                request.url\n+            redirected.meta[\"redirect_urls\"] = [\n+                *request.meta.get(\"redirect_urls\", []),\n+                request.url,\n+            ]\n+            redirected.meta[\"redirect_reasons\"] = [\n+                *request.meta.get(\"redirect_reasons\", []),\n+                reason,\n             ]\n-            redirected.meta[\"redirect_reasons\"] = request.meta.get(\n-                \"redirect_reasons\", []\n-            ) + [reason]\n             redirected.dont_filter = request.dont_filter\n             redirected.priority = request.priority + self.priority_adjust\n             logger.debug(\n\n@@ -25,13 +25,13 @@ if TYPE_CHECKING:\n \n __all__ = [\n     \"BaseItemExporter\",\n-    \"PprintItemExporter\",\n-    \"PickleItemExporter\",\n     \"CsvItemExporter\",\n-    \"XmlItemExporter\",\n-    \"JsonLinesItemExporter\",\n     \"JsonItemExporter\",\n+    \"JsonLinesItemExporter\",\n     \"MarshalItemExporter\",\n+    \"PickleItemExporter\",\n+    \"PprintItemExporter\",\n+    \"XmlItemExporter\",\n ]\n \n \n\n@@ -77,4 +77,4 @@ class Debugger:\n \n     def _enter_debugger(self, signum: int, frame: FrameType | None) -> None:\n         assert frame\n-        Pdb().set_trace(frame.f_back)  # noqa: T100\n+        Pdb().set_trace(frame.f_back)\n\n@@ -20,7 +20,7 @@ if TYPE_CHECKING:\n \n \n class JsonRequest(Request):\n-    attributes: tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n+    attributes: tuple[str, ...] = (*Request.attributes, \"dumps_kwargs\")\n \n     def __init__(\n         self, *args: Any, dumps_kwargs: dict[str, Any] | None = None, **kwargs: Any\n\n@@ -43,7 +43,7 @@ class TextResponse(Response):\n     _DEFAULT_ENCODING = \"ascii\"\n     _cached_decoded_json = _NONE\n \n-    attributes: tuple[str, ...] = Response.attributes + (\"encoding\",)\n+    attributes: tuple[str, ...] = (*Response.attributes, \"encoding\")\n \n     def __init__(self, *args: Any, **kwargs: Any):\n         self._encoding: str | None = kwargs.pop(\"encoding\", None)\n\n@@ -24,7 +24,7 @@ class Link:\n                     of the anchor tag.\n     \"\"\"\n \n-    __slots__ = [\"url\", \"text\", \"fragment\", \"nofollow\"]\n+    __slots__ = [\"fragment\", \"nofollow\", \"text\", \"url\"]\n \n     def __init__(\n         self, url: str, text: str = \"\", fragment: str = \"\", nofollow: bool = False\n\n@@ -76,8 +76,8 @@ class LogFormatter:\n         self, request: Request, response: Response, spider: Spider\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n-        request_flags = f\" {str(request.flags)}\" if request.flags else \"\"\n-        response_flags = f\" {str(response.flags)}\" if response.flags else \"\"\n+        request_flags = f\" {request.flags!s}\" if request.flags else \"\"\n+        response_flags = f\" {response.flags!s}\" if response.flags else \"\"\n         return {\n             \"level\": logging.DEBUG,\n             \"msg\": CRAWLEDMSG,\n\n@@ -127,8 +127,7 @@ class MediaPipeline(ABC):\n         if (\n             not base_class_name\n             or class_name == base_class_name\n-            or settings\n-            and not settings.get(formatted_key)\n+            or (settings and not settings.get(formatted_key))\n         ):\n             return key\n         return formatted_key\n\n@@ -195,8 +195,7 @@ class StrictOriginPolicy(ReferrerPolicy):\n         if (\n             self.tls_protected(response_url)\n             and self.potentially_trustworthy(request_url)\n-            or not self.tls_protected(response_url)\n-        ):\n+        ) or not self.tls_protected(response_url):\n             return self.origin_referrer(response_url)\n         return None\n \n@@ -249,8 +248,7 @@ class StrictOriginWhenCrossOriginPolicy(ReferrerPolicy):\n         if (\n             self.tls_protected(response_url)\n             and self.potentially_trustworthy(request_url)\n-            or not self.tls_protected(response_url)\n-        ):\n+        ) or not self.tls_protected(response_url):\n             return self.origin_referrer(response_url)\n         return None\n \n@@ -282,7 +280,7 @@ class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n     using ``file://`` or ``s3://`` scheme.\n     \"\"\"\n \n-    NOREFERRER_SCHEMES: tuple[str, ...] = LOCAL_SCHEMES + (\"file\", \"s3\")\n+    NOREFERRER_SCHEMES: tuple[str, ...] = (*LOCAL_SCHEMES, \"file\", \"s3\")\n     name: str = POLICY_SCRAPY_DEFAULT\n \n \n\n@@ -252,7 +252,9 @@ def is_generator_with_return_value(callable: Callable[..., Any]) -> bool:\n \n     def returns_none(return_node: ast.Return) -> bool:\n         value = return_node.value\n-        return value is None or isinstance(value, ast.Constant) and value.value is None\n+        return value is None or (\n+            isinstance(value, ast.Constant) and value.value is None\n+        )\n \n     if inspect.isgeneratorfunction(callable):\n         func = callable\n\n@@ -100,7 +100,7 @@ def install_reactor(reactor_path: str, event_loop_path: str | None = None) -> No\n             asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n-        installer_path = module + [\"install\"]\n+        installer_path = [*module, \"install\"]\n         installer = load_object(\".\".join(installer_path))\n         with suppress(error.ReactorAlreadyInstalledError):\n             installer()\n\n@@ -229,7 +229,8 @@ def request_to_curl(request: Request) -> str:\n             cookies = f\"--cookie '{cookie}'\"\n         elif isinstance(request.cookies, list):\n             cookie = \"; \".join(\n-                f\"{list(c.keys())[0]}={list(c.values())[0]}\" for c in request.cookies\n+                f\"{next(iter(c.keys()))}={next(iter(c.values()))}\"\n+                for c in request.cookies\n             )\n             cookies = f\"--cookie '{cookie}'\"\n \n\n@@ -31,7 +31,7 @@ class ProcessTest:\n         if settings is not None:\n             env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n         assert self.command\n-        cmd = self.prefix + [self.command] + list(args)\n+        cmd = [*self.prefix, self.command, *args]\n         pp = TestProcessProtocol()\n         pp.deferred.addCallback(self._process_finished, cmd, check_code)\n         reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n\n@@ -51,7 +51,7 @@ def url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n def url_is_from_spider(url: UrlT, spider: type[Spider]) -> bool:\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n     return url_is_from_any_domain(\n-        url, [spider.name] + list(getattr(spider, \"allowed_domains\", []))\n+        url, [spider.name, *getattr(spider, \"allowed_domains\", [])]\n     )\n \n \n\n@@ -11,7 +11,7 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.settings.default_settings import LOG_VERSIONS\n from scrapy.utils.ssl import get_openssl_version\n \n-_DEFAULT_SOFTWARE = [\"Scrapy\"] + LOG_VERSIONS\n+_DEFAULT_SOFTWARE = [\"Scrapy\", *LOG_VERSIONS]\n \n \n def _version(item):\n\n@@ -21,7 +21,7 @@ class CmdlineTest(unittest.TestCase):\n \n     def _execute(self, *new_args, **kwargs):\n         encoding = sys.stdout.encoding or \"utf-8\"\n-        args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n+        args = (sys.executable, \"-m\", \"scrapy.cmdline\", *new_args)\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n         comm = proc.communicate()[0].strip()\n         return comm.decode(encoding)\n\n@@ -87,13 +87,13 @@ class ProjectTest(unittest.TestCase):\n \n     def call(self, *new_args, **kwargs):\n         with TemporaryFile() as out:\n-            args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n+            args = (sys.executable, \"-m\", \"scrapy.cmdline\", *new_args)\n             return subprocess.call(\n                 args, stdout=out, stderr=out, cwd=self.cwd, env=self.env, **kwargs\n             )\n \n     def proc(self, *new_args, **popen_kwargs):\n-        args = (sys.executable, \"-m\", \"scrapy.cmdline\") + new_args\n+        args = (sys.executable, \"-m\", \"scrapy.cmdline\", *new_args)\n         p = subprocess.Popen(\n             args,\n             cwd=popen_kwargs.pop(\"cwd\", self.cwd),\n\n@@ -647,7 +647,7 @@ class ScriptRunnerMixin:\n \n     def get_script_args(self, script_name: str, *script_args: str) -> list[str]:\n         script_path = self.script_dir / script_name\n-        return [sys.executable, str(script_path)] + list(script_args)\n+        return [sys.executable, str(script_path), *script_args]\n \n     def run_script(self, script_name: str, *script_args: str) -> str:\n         args = self.get_script_args(script_name, *script_args)\n\n@@ -114,7 +114,7 @@ class RetryTest(unittest.TestCase):\n     def test_exception_to_retry_added(self):\n         exc = ValueError\n         settings_dict = {\n-            \"RETRY_EXCEPTIONS\": list(RETRY_EXCEPTIONS) + [exc],\n+            \"RETRY_EXCEPTIONS\": [*RETRY_EXCEPTIONS, exc],\n         }\n         crawler = get_crawler(Spider, settings_dict=settings_dict)\n         mw = RetryMiddleware.from_crawler(crawler)\n\n@@ -31,7 +31,7 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n     def start_requests(self):\n         self.times = {None: []}\n \n-        slots = list(self.custom_settings.get(\"DOWNLOAD_SLOTS\", {}).keys()) + [None]\n+        slots = [*self.custom_settings.get(\"DOWNLOAD_SLOTS\", {}), None]\n \n         for slot in slots:\n             url = self.mockserver.url(f\"/?downloader_slot={slot}\")\n\n@@ -116,7 +116,7 @@ class BaseItemExporterTest(unittest.TestCase):\n         )\n \n         ie = self._get_exporter(fields_to_export=[\"name\"], encoding=\"latin-1\")\n-        _, name = list(ie._get_serialized_fields(self.i))[0]\n+        _, name = next(iter(ie._get_serialized_fields(self.i)))\n         assert isinstance(name, str)\n         self.assertEqual(name, \"John\\xa3\")\n \n\n@@ -960,7 +960,7 @@ class XmlResponseTest(TextResponseTest):\n \n \n class CustomResponse(TextResponse):\n-    attributes = TextResponse.attributes + (\"foo\", \"bar\")\n+    attributes = (*TextResponse.attributes, \"foo\", \"bar\")\n \n     def __init__(self, *args, **kwargs) -> None:\n         self.foo = kwargs.pop(\"foo\", None)\n\n@@ -61,11 +61,11 @@ Foo                                 1   oldest: 0s ago\\n\\n\"\"\",\n         )\n \n     def test_get_oldest(self):\n-        o1 = Foo()  # noqa: F841\n+        o1 = Foo()\n \n         o1_time = time()\n \n-        o2 = Bar()  # noqa: F841\n+        o2 = Bar()\n \n         o3_time = time()\n         if o3_time <= o1_time:\n@@ -80,9 +80,9 @@ Foo                                 1   oldest: 0s ago\\n\\n\"\"\",\n         self.assertIsNone(trackref.get_oldest(\"XXX\"))\n \n     def test_iter_all(self):\n-        o1 = Foo()  # noqa: F841\n+        o1 = Foo()\n         o2 = Bar()  # noqa: F841\n-        o3 = Foo()  # noqa: F841\n+        o3 = Foo()\n         self.assertEqual(\n             set(trackref.iter_all(\"Foo\")),\n             {o1, o3},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f44ca39fa23f07c857a34309d79db6394c5faefb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 4140 | Contributors (this commit): 22 | Commits (past 90d): 9 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,7 +34,7 @@ def _path_safe(text: str) -> str:\n     # as we replace some letters we can get collision for different slots\n     # add we add unique part\n     unique_slot = hashlib.md5(text.encode(\"utf8\")).hexdigest()  # noqa: S324\n-    return \"-\".join([pathable_slot, unique_slot])\n+    return f\"{pathable_slot}-{unique_slot}\"\n \n \n class QueueProtocol(Protocol):\n\n@@ -891,14 +891,7 @@ class TestSettingsPolicyByName(TestCase):\n         # test parsing without space(s) after the comma\n         settings1 = Settings(\n             {\n-                \"REFERRER_POLICY\": \",\".join(\n-                    [\n-                        \"some-custom-unknown-policy\",\n-                        POLICY_SAME_ORIGIN,\n-                        POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n-                        \"another-custom-unknown-policy\",\n-                    ]\n-                )\n+                \"REFERRER_POLICY\": f\"some-custom-unknown-policy,{POLICY_SAME_ORIGIN},{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},another-custom-unknown-policy\"\n             }\n         )\n         mw1 = RefererMiddleware(settings1)\n@@ -907,13 +900,7 @@ class TestSettingsPolicyByName(TestCase):\n         # test parsing with space(s) after the comma\n         settings2 = Settings(\n             {\n-                \"REFERRER_POLICY\": \",    \".join(\n-                    [\n-                        POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n-                        \"another-custom-unknown-policy\",\n-                        POLICY_UNSAFE_URL,\n-                    ]\n-                )\n+                \"REFERRER_POLICY\": f\"{POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN},    another-custom-unknown-policy,    {POLICY_UNSAFE_URL}\"\n             }\n         )\n         mw2 = RefererMiddleware(settings2)\n@@ -922,13 +909,7 @@ class TestSettingsPolicyByName(TestCase):\n     def test_multiple_policy_tokens_all_invalid(self):\n         settings = Settings(\n             {\n-                \"REFERRER_POLICY\": \",\".join(\n-                    [\n-                        \"some-custom-unknown-policy\",\n-                        \"another-custom-unknown-policy\",\n-                        \"yet-another-custom-unknown-policy\",\n-                    ]\n-                )\n+                \"REFERRER_POLICY\": \"some-custom-unknown-policy,another-custom-unknown-policy,yet-another-custom-unknown-policy\"\n             }\n         )\n         with self.assertRaises(RuntimeError):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#273620488ced7dd1a3c8a1c5022d17f0cc7f9496", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 16 | Files Changed: 7 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 7218 | Contributors (this commit): 61 | Commits (past 90d): 35 | Contributors (cumulative): 138 | DMM Complexity: 1.0\n\nDIFF:\n@@ -154,7 +154,7 @@ class Command(ScrapyCommand):\n             spiders_dir = Path(spiders_module.__file__).parent.resolve()\n         else:\n             spiders_module = None\n-            spiders_dir = Path(\".\")\n+            spiders_dir = Path()\n         spider_file = f\"{spiders_dir / module}.py\"\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n\n@@ -1,6 +1,5 @@\n from __future__ import annotations\n \n-import os\n import re\n import string\n from importlib.util import find_spec\n@@ -28,9 +27,9 @@ TEMPLATES_TO_RENDER: tuple[tuple[str, ...], ...] = (\n IGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n \n \n-def _make_writable(path: str | os.PathLike) -> None:\n-    current_permissions = os.stat(path).st_mode\n-    os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)\n+def _make_writable(path: Path) -> None:\n+    current_permissions = path.stat().st_mode\n+    path.chmod(current_permissions | OWNER_WRITE_PERMISSION)\n \n \n class Command(ScrapyCommand):\n\n@@ -32,6 +32,7 @@ from __future__ import annotations\n \n import re\n from io import BytesIO\n+from pathlib import Path\n from typing import TYPE_CHECKING, Any, BinaryIO\n from urllib.parse import unquote\n \n@@ -56,9 +57,11 @@ if TYPE_CHECKING:\n \n \n class ReceivedDataProtocol(Protocol):\n-    def __init__(self, filename: str | None = None):\n-        self.__filename: str | None = filename\n-        self.body: BinaryIO = open(filename, \"wb\") if filename else BytesIO()\n+    def __init__(self, filename: bytes | None = None):\n+        self.__filename: bytes | None = filename\n+        self.body: BinaryIO = (\n+            Path(filename.decode()).open(\"wb\") if filename else BytesIO()\n+        )\n         self.size: int = 0\n \n     def dataReceived(self, data: bytes) -> None:\n@@ -66,7 +69,7 @@ class ReceivedDataProtocol(Protocol):\n         self.size += len(data)\n \n     @property\n-    def filename(self) -> str | None:\n+    def filename(self) -> bytes | None:\n         return self.__filename\n \n     def close(self) -> None:\n@@ -128,8 +131,8 @@ class FTPDownloadHandler:\n     ) -> Response:\n         self.result = result\n         protocol.close()\n-        headers = {\"local filename\": protocol.filename or \"\", \"size\": protocol.size}\n-        body = to_bytes(protocol.filename or protocol.body.read())\n+        headers = {\"local filename\": protocol.filename or b\"\", \"size\": protocol.size}\n+        body = protocol.filename or protocol.body.read()\n         respcls = responsetypes.from_args(url=request.url, body=body)\n         # hints for Headers-related types may need to be fixed to not use AnyStr\n         return respcls(url=request.url, status=200, body=body, headers=headers)  # type: ignore[arg-type]\n\n@@ -17,7 +17,7 @@ if TYPE_CHECKING:\n class ProcessTest:\n     command: str | None = None\n     prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n-    cwd = os.getcwd()  # trial chdirs to temp dir\n+    cwd = os.getcwd()  # trial chdirs to temp dir  # noqa: PTH109\n \n     def execute(\n         self,\n\n@@ -1,5 +1,4 @@\n import logging\n-import os\n import platform\n import re\n import signal\n@@ -643,7 +642,6 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n class ScriptRunnerMixin:\n     script_dir: Path\n-    cwd = os.getcwd()\n \n     def get_script_args(self, script_name: str, *script_args: str) -> list[str]:\n         script_path = self.script_dir / script_name\n\n@@ -116,7 +116,7 @@ class FileTestCase(unittest.TestCase):\n \n     def tearDown(self):\n         os.close(self.fd)\n-        os.remove(self.tmpname)\n+        Path(self.tmpname).unlink()\n \n     def test_download(self):\n         def _test(response):\n\n@@ -12,7 +12,7 @@ from scrapy.utils.project import data_path, get_project_settings\n \n @contextlib.contextmanager\n def inside_a_project():\n-    prev_dir = os.getcwd()\n+    prev_dir = Path.cwd()\n     project_dir = tempfile.mkdtemp()\n \n     try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c87354cd46afdba35ca104a62b0c2bbfa6bd6f64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 109 | Files Changed: 34 | Hunks: 58 | Methods Changed: 40 | Complexity Δ (Sum/Max): -11/2 | Churn Δ: 192 | Churn Cumulative: 45388 | Contributors (this commit): 200 | Commits (past 90d): 171 | Contributors (cumulative): 735 | DMM Complexity: 0.7428571428571429\n\nDIFF:\n@@ -90,12 +90,10 @@ def _get_commands_dict(\n \n \n def _pop_command_name(argv: list[str]) -> str | None:\n-    i = 0\n-    for arg in argv[1:]:\n+    for i, arg in enumerate(argv[1:]):\n         if not arg.startswith(\"-\"):\n             del argv[i]\n             return arg\n-        i += 1\n     return None\n \n \n\n@@ -174,13 +174,12 @@ class Command(BaseRunSpiderCommand):\n         display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n \n     def print_requests(self, lvl: int | None = None, colour: bool = True) -> None:\n-        if lvl is None:\n-            if self.requests:\n+        if lvl is not None:\n+            requests = self.requests.get(lvl, [])\n+        elif self.requests:\n             requests = self.requests[max(self.requests)]\n         else:\n             requests = []\n-        else:\n-            requests = self.requests.get(lvl, [])\n \n         print(\"# Requests \", \"-\" * 65)\n         display.pprint(requests, colorize=colour)\n\n@@ -95,10 +95,7 @@ class Command(ScrapyCommand):\n \n         project_name = args[0]\n \n-        if len(args) == 2:\n-            project_dir = Path(args[1])\n-        else:\n-            project_dir = Path(args[0])\n+        project_dir = Path(args[-1])\n \n         if (project_dir / \"scrapy.cfg\").exists():\n             self.exitcode = 1\n\n@@ -424,10 +424,7 @@ class ScrapyAgent:\n         headers = TxHeaders(request.headers)\n         if isinstance(agent, self._TunnelingAgent):\n             headers.removeHeader(b\"Proxy-Authorization\")\n-        if request.body:\n-            bodyproducer = _RequestBodyProducer(request.body)\n-        else:\n-            bodyproducer = None\n+        bodyproducer = _RequestBodyProducer(request.body) if request.body else None\n         start_time = time()\n         d: Deferred[TxResponse] = agent.request(\n             method, to_bytes(url, encoding=\"ascii\"), headers, bodyproducer\n\n@@ -291,9 +291,7 @@ class ExecutionEngine:\n             return False\n         if self.slot.start_requests is not None:  # not all start requests are handled\n             return False\n-        if self.slot.scheduler.has_pending_requests():\n-            return False\n-        return True\n+        return not self.slot.scheduler.has_pending_requests()\n \n     def crawl(self, request: Request) -> None:\n         \"\"\"Inject the request into the spider <-> downloader pipeline\"\"\"\n@@ -388,8 +386,7 @@ class ExecutionEngine:\n         )\n         self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.spider = spider\n-        if hasattr(scheduler, \"open\"):\n-            if d := scheduler.open(spider):\n+        if hasattr(scheduler, \"open\") and (d := scheduler.open(spider)):\n             yield d\n         yield self.scraper.open_spider(spider)\n         assert self.crawler.stats\n\n@@ -198,10 +198,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         # chain, they went through it already from the process_spider_exception method\n         recovered: MutableChain[_T] | MutableAsyncChain[_T]\n         last_result_is_async = isinstance(result, AsyncIterable)\n-        if last_result_is_async:\n-            recovered = MutableAsyncChain()\n-        else:\n-            recovered = MutableChain()\n+        recovered = MutableAsyncChain() if last_result_is_async else MutableChain()\n \n         # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.\n         # 1. def foo. Sync iterables are passed as is, async ones are downgraded.\n\n@@ -1,5 +1,6 @@\n from __future__ import annotations\n \n+import contextlib\n import logging\n import pprint\n import signal\n@@ -503,7 +504,6 @@ class CrawlerProcess(CrawlerRunner):\n     def _stop_reactor(self, _: Any = None) -> None:\n         from twisted.internet import reactor\n \n-        try:\n+        # raised if already stopped or in shutdown stage\n+        with contextlib.suppress(RuntimeError):\n             reactor.stop()\n-        except RuntimeError:  # raised if already stopped or in shutdown stage\n-            pass\n\n@@ -42,7 +42,10 @@ class HttpAuthMiddleware:\n         self, request: Request, spider: Spider\n     ) -> Request | Response | None:\n         auth = getattr(self, \"auth\", None)\n-        if auth and b\"Authorization\" not in request.headers:\n-            if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n+        if (\n+            auth\n+            and b\"Authorization\" not in request.headers\n+            and (not self.domain or url_is_from_any_domain(request.url, [self.domain]))\n+        ):\n             request.headers[b\"Authorization\"] = auth\n         return None\n\n@@ -51,10 +51,7 @@ class HttpProxyMiddleware:\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n \n-        if user:\n-            creds = self._basic_auth_header(user, password)\n-        else:\n-            creds = None\n+        creds = self._basic_auth_header(user, password) if user else None\n \n         return creds, proxy_url\n \n\n@@ -81,10 +81,7 @@ class BaseItemExporter:\n             include_empty = self.export_empty_fields\n \n         if self.fields_to_export is None:\n-            if include_empty:\n-                field_iter = item.field_names()\n-            else:\n-                field_iter = item.keys()\n+            field_iter = item.field_names() if include_empty else item.keys()\n         elif isinstance(self.fields_to_export, Mapping):\n             if include_empty:\n                 field_iter = self.fields_to_export.items()\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/extensions.rst\n \n from __future__ import annotations\n \n+import contextlib\n import logging\n import signal\n import sys\n@@ -69,11 +70,9 @@ class StackTraceDump:\n \n class Debugger:\n     def __init__(self) -> None:\n-        try:\n-            signal.signal(signal.SIGUSR2, self._enter_debugger)  # type: ignore[attr-defined]\n-        except AttributeError:\n         # win32 platforms don't support SIGUSR signals\n-            pass\n+        with contextlib.suppress(AttributeError):\n+            signal.signal(signal.SIGUSR2, self._enter_debugger)  # type: ignore[attr-defined]\n \n     def _enter_debugger(self, signum: int, frame: FrameType | None) -> None:\n         assert frame\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/feed-exports.rst\n \n from __future__ import annotations\n \n+import contextlib\n import logging\n import re\n import sys\n@@ -642,10 +643,8 @@ class FeedExporter:\n         )\n         d = {}\n         for k, v in conf.items():\n-            try:\n+            with contextlib.suppress(NotConfigured):\n                 d[k] = load_object(v)\n-            except NotConfigured:\n-                pass\n         return d\n \n     def _exporter_supported(self, format: str) -> bool:\n\n@@ -89,10 +89,7 @@ class RFC2616Policy:\n             return False\n         cc = self._parse_cachecontrol(request)\n         # obey user-agent directive \"Cache-Control: no-store\"\n-        if b\"no-store\" in cc:\n-            return False\n-        # Any other is eligible for caching\n-        return True\n+        return b\"no-store\" not in cc\n \n     def should_cache_response(self, response: Response, request: Request) -> bool:\n         # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1\n\n@@ -151,10 +151,7 @@ class PeriodicLog:\n                 return False\n         if exclude and not include:\n             return True\n-        for p in include:\n-            if p in stat_name:\n-                return True\n-        return False\n+        return any(p in stat_name for p in include)\n \n     def spider_closed(self, spider: Spider, reason: str) -> None:\n         self.log()\n\n@@ -64,8 +64,7 @@ class CookieJar:\n                 cookies += self.jar._cookies_for_domain(host, wreq)  # type: ignore[attr-defined]\n \n         attrs = self.jar._cookie_attrs(cookies)  # type: ignore[attr-defined]\n-        if attrs:\n-            if not wreq.has_header(\"Cookie\"):\n+        if attrs and not wreq.has_header(\"Cookie\"):\n             wreq.add_unredirected_header(\"Cookie\", \"; \".join(attrs))\n \n         self.processed += 1\n\n@@ -29,7 +29,7 @@ class JsonRequest(Request):\n         dumps_kwargs.setdefault(\"sort_keys\", True)\n         self._dumps_kwargs: dict[str, Any] = dumps_kwargs\n \n-        body_passed = kwargs.get(\"body\", None) is not None\n+        body_passed = kwargs.get(\"body\") is not None\n         data: Any = kwargs.pop(\"data\", None)\n         data_passed: bool = data is not None\n \n@@ -61,7 +61,7 @@ class JsonRequest(Request):\n     def replace(\n         self, *args: Any, cls: type[Request] | None = None, **kwargs: Any\n     ) -> Request:\n-        body_passed = kwargs.get(\"body\", None) is not None\n+        body_passed = kwargs.get(\"body\") is not None\n         data: Any = kwargs.pop(\"data\", None)\n         data_passed: bool = data is not None\n \n\n@@ -41,8 +41,11 @@ _collect_string_content = etree.XPath(\"string()\")\n \n \n def _nons(tag: Any) -> Any:\n-    if isinstance(tag, str):\n-        if tag[0] == \"{\" and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE:\n+    if (\n+        isinstance(tag, str)\n+        and tag[0] == \"{\"\n+        and tag[1 : len(XHTML_NAMESPACE) + 1] == XHTML_NAMESPACE\n+    ):\n         return tag.split(\"}\")[-1]\n     return tag\n \n@@ -230,9 +233,7 @@ class LxmlLinkExtractor:\n             parsed_url, self.deny_extensions\n         ):\n             return False\n-        if self.restrict_text and not _matches(link.text, self.restrict_text):\n-            return False\n-        return True\n+        return not self.restrict_text or _matches(link.text, self.restrict_text)\n \n     def matches(self, url: str) -> bool:\n         if self.allow_domains and not url_is_from_any_domain(url, self.allow_domains):\n\n@@ -111,11 +111,9 @@ class MailSender:\n     ) -> Deferred[None] | None:\n         from twisted.internet import reactor\n \n-        msg: MIMEBase\n-        if attachs:\n-            msg = MIMEMultipart()\n-        else:\n-            msg = MIMENonMultipart(*mimetype.split(\"/\", 1))\n+        msg: MIMEBase = (\n+            MIMEMultipart() if attachs else MIMENonMultipart(*mimetype.split(\"/\", 1))\n+        )\n \n         to = list(arg_to_iter(to))\n         cc = list(arg_to_iter(cc))\n\n@@ -553,10 +553,8 @@ class FilesPipeline(MediaPipeline):\n         ftp_store.USE_ACTIVE_MODE = settings.getbool(\"FEED_STORAGE_FTP_ACTIVE\")\n \n     def _get_store(self, uri: str) -> FilesStoreProtocol:\n-        if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n-            scheme = \"file\"\n-        else:\n-            scheme = urlparse(uri).scheme\n+        # to support win32 paths like: C:\\\\some\\dir\n+        scheme = \"file\" if Path(uri).is_absolute() else urlparse(uri).scheme\n         store_cls = self.STORE_SCHEMES[scheme]\n         return store_cls(uri)\n \n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/shell.rst\n \n from __future__ import annotations\n \n+import contextlib\n import os\n import signal\n from typing import TYPE_CHECKING, Any\n@@ -143,12 +144,10 @@ class Shell:\n             else:\n                 request.meta[\"handle_httpstatus_all\"] = True\n         response = None\n-        try:\n+        with contextlib.suppress(IgnoreRequest):\n             response, spider = threads.blockingCallFromThread(\n                 reactor, self._schedule, request, spider\n             )\n-        except IgnoreRequest:\n-            pass\n         self.populate_vars(response, request, spider)\n \n     def populate_vars(\n\n@@ -360,8 +360,7 @@ class RefererMiddleware:\n         - otherwise, the policy from settings is used.\n         \"\"\"\n         policy_name = request.meta.get(\"referrer_policy\")\n-        if policy_name is None:\n-            if isinstance(resp_or_url, Response):\n+        if policy_name is None and isinstance(resp_or_url, Response):\n             policy_header = resp_or_url.headers.get(\"Referrer-Policy\")\n             if policy_header is not None:\n                 policy_name = to_unicode(policy_header.decode(\"latin1\"))\n\n@@ -1,3 +1,4 @@\n+import contextlib\n import zlib\n from io import BytesIO\n from warnings import warn\n@@ -37,10 +38,8 @@ else:\n             return decompressor.process(data)\n \n \n-try:\n+with contextlib.suppress(ImportError):\n     import zstandard\n-except ImportError:\n-    pass\n \n \n _CHUNK_SIZE = 65536  # 64 KiB\n\n@@ -8,6 +8,7 @@ This module must not depend on any module outside the Standard Library.\n from __future__ import annotations\n \n import collections\n+import contextlib\n import warnings\n import weakref\n from collections import OrderedDict\n@@ -173,10 +174,9 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n         self.data: LocalCache = LocalCache(limit=limit)\n \n     def __setitem__(self, key: _KT, value: _VT) -> None:\n-        try:\n+        # if raised, key is not weak-referenceable, skip caching\n+        with contextlib.suppress(TypeError):\n             super().__setitem__(key, value)\n-        except TypeError:\n-            pass  # key is not weak-referenceable, skip caching\n \n     def __getitem__(self, key: _KT) -> _VT | None:  # type: ignore[override]\n         try:\n\n@@ -36,7 +36,7 @@ def send_catch_log(\n     dont_log = named.pop(\"dont_log\", ())\n     dont_log = tuple(dont_log) if isinstance(dont_log, Sequence) else (dont_log,)\n     dont_log += (StopDownload,)\n-    spider = named.get(\"spider\", None)\n+    spider = named.get(\"spider\")\n     responses: list[tuple[TypingAny, TypingAny]] = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         result: TypingAny\n@@ -88,7 +88,7 @@ def send_catch_log_deferred(\n         return failure\n \n     dont_log = named.pop(\"dont_log\", None)\n-    spider = named.get(\"spider\", None)\n+    spider = named.get(\"spider\")\n     dfds: list[Deferred[tuple[TypingAny, TypingAny]]] = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         d: Deferred[TypingAny] = maybeDeferred_coro(\n\n@@ -173,13 +173,19 @@ def strip_url(\n         parsed_url.username or parsed_url.password\n     ):\n         netloc = netloc.split(\"@\")[-1]\n-    if strip_default_port and parsed_url.port:\n-        if (parsed_url.scheme, parsed_url.port) in (\n+\n+    if (\n+        strip_default_port\n+        and parsed_url.port\n+        and (parsed_url.scheme, parsed_url.port)\n+        in (\n             (\"http\", 80),\n             (\"https\", 443),\n             (\"ftp\", 21),\n+        )\n     ):\n         netloc = netloc.replace(f\":{parsed_url.port}\", \"\")\n+\n     return urlunparse(\n         (\n             parsed_url.scheme,\n\n@@ -166,8 +166,10 @@ class AddonManagerTest(unittest.TestCase):\n             def update_settings(self, settings):\n                 pass\n \n-        with patch(\"scrapy.addons.logger\") as logger_mock:\n-            with patch(\"scrapy.addons.build_from_crawler\") as build_from_crawler_mock:\n+        with (\n+            patch(\"scrapy.addons.logger\") as logger_mock,\n+            patch(\"scrapy.addons.build_from_crawler\") as build_from_crawler_mock,\n+        ):\n             settings_dict = {\n                 \"ADDONS\": {LoggedAddon: 1},\n             }\n\n@@ -530,8 +530,9 @@ class Http11TestCase(HttpTestCase):\n         d = self.download_request(request, Spider(\"foo\"))\n \n         def checkDataLoss(failure):\n-            if failure.check(ResponseFailed):\n-                if any(r.check(_DataLoss) for r in failure.value.reasons):\n+            if failure.check(ResponseFailed) and any(\n+                r.check(_DataLoss) for r in failure.value.reasons\n+            ):\n                 return None\n             return failure\n \n\n@@ -756,7 +756,7 @@ class FeedExportTest(FeedExportTestBase):\n                 )\n \n         finally:\n-            for file_path in FEEDS.keys():\n+            for file_path in FEEDS:\n                 if not Path(file_path).exists():\n                     continue\n \n@@ -1229,15 +1229,13 @@ class FeedExportTest(FeedExportTestBase):\n \n         class CustomFilter2(scrapy.extensions.feedexport.ItemFilter):\n             def accepts(self, item):\n-                if \"foo\" not in item.fields:\n-                    return False\n-                return True\n+                return \"foo\" in item.fields\n \n         class CustomFilter3(scrapy.extensions.feedexport.ItemFilter):\n             def accepts(self, item):\n-                if isinstance(item, tuple(self.item_classes)) and item[\"foo\"] == \"bar1\":\n-                    return True\n-                return False\n+                return (\n+                    isinstance(item, tuple(self.item_classes)) and item[\"foo\"] == \"bar1\"\n+                )\n \n         formats = {\n             \"json\": b'[\\n{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n]',\n\n@@ -1488,10 +1488,7 @@ def _buildresponse(body, **kwargs):\n \n \n def _qs(req, encoding=\"utf-8\", to_unicode=False):\n-    if req.method == \"POST\":\n-        qs = req.body\n-    else:\n-        qs = req.url.partition(\"?\")[2]\n+    qs = req.body if req.method == \"POST\" else req.url.partition(\"?\")[2]\n     uqs = unquote_to_bytes(qs)\n     if to_unicode:\n         uqs = uqs.decode(encoding)\n\n@@ -634,8 +634,10 @@ class TestGCSFilesStore(unittest.TestCase):\n             import google.cloud.storage  # noqa: F401\n         except ModuleNotFoundError:\n             raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n-        with mock.patch(\"google.cloud.storage\") as _:\n-            with mock.patch(\"scrapy.pipelines.files.time\") as _:\n+        with (\n+            mock.patch(\"google.cloud.storage\"),\n+            mock.patch(\"scrapy.pipelines.files.time\"),\n+        ):\n             uri = \"gs://my_bucket/my_prefix/\"\n             store = GCSFilesStore(uri)\n             store.bucket = mock.Mock()\n\n@@ -170,7 +170,7 @@ class BaseSettingsTest(unittest.TestCase):\n \n         self.assertCountEqual(self.settings.attributes.keys(), ctrl_attributes.keys())\n \n-        for key in ctrl_attributes.keys():\n+        for key in ctrl_attributes:\n             attr = self.settings.attributes[key]\n             ctrl_attr = ctrl_attributes[key]\n             self.assertEqual(attr.value, ctrl_attr.value)\n\n@@ -1,3 +1,4 @@\n+import contextlib\n import shutil\n import sys\n import tempfile\n@@ -22,10 +23,8 @@ module_dir = Path(__file__).resolve().parent\n \n \n def _copytree(source: Path, target: Path):\n-    try:\n+    with contextlib.suppress(shutil.Error):\n         shutil.copytree(source, target)\n-    except shutil.Error:\n-        pass\n \n \n class SpiderLoaderTest(unittest.TestCase):\n\n@@ -259,8 +259,10 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertIn(\"foo.Bar\", str(w[1].message))\n \n     def test_inspect_stack(self):\n-        with mock.patch(\"inspect.stack\", side_effect=IndexError):\n-            with warnings.catch_warnings(record=True) as w:\n+        with (\n+            mock.patch(\"inspect.stack\", side_effect=IndexError),\n+            warnings.catch_warnings(record=True) as w,\n+        ):\n             DeprecatedName = create_deprecated_class(\"DeprecatedName\", NewName)\n \n             class SubClass(DeprecatedName):\n\n@@ -366,7 +366,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         # explicit type check cuz' we no like stinkin' autocasting! yarrr\n         for result_row in result:\n-            self.assertTrue(all(isinstance(k, str) for k in result_row.keys()))\n+            self.assertTrue(all(isinstance(k, str) for k in result_row))\n             self.assertTrue(all(isinstance(v, str) for v in result_row.values()))\n \n     def test_csviter_delimiter(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
