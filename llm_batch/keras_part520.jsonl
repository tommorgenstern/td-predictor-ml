{"custom_id": "keras#7fbfd50dbd8ac1f8d1c10742858981771d1f9ca3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 113 | Churn Cumulative: 4788 | Contributors (this commit): 42 | Commits (past 90d): 2 | Contributors (cumulative): 51 | DMM Complexity: 0.6875\n\nDIFF:\n@@ -123,6 +123,42 @@ def count_params(weights):\n     return int(sum(np.prod(p) for p in standardized_weight_shapes))\n \n \n+def weight_memory_size(weights):\n+    \"\"\"Calculate the memory footprint for weights based on their dtypes.\n+\n+    Args:\n+        weights: An iterable contains the weights to compute weight size.\n+\n+    Returns:\n+        The total memory size (in Bytes) of the weights.\n+    \"\"\"\n+    unique_weights = {id(w): w for w in weights}.values()\n+\n+    total_memory_size = 0\n+    for w in unique_weights:\n+        # Ignore TrackableWeightHandlers, which will not have a shape defined.\n+        if not hasattr(w, \"shape\"):\n+            continue\n+        elif None in w.shape.as_list():\n+            continue\n+        weight_shape = np.prod(w.shape.as_list())\n+        per_param_size = w.dtype.size\n+        total_memory_size += weight_shape * per_param_size\n+    return total_memory_size\n+\n+\n+def readable_weight_memory_size(weight_memory_size):\n+    \"\"\"Convert the weight memory size (Bytes) to a readable string.\"\"\"\n+    units = [\"Byte\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n+    scale = 1024\n+    for unit in units:\n+        if weight_memory_size / scale < 1:\n+            return \"{:.2f} {}\".format(weight_memory_size, unit)\n+        else:\n+            weight_memory_size /= scale\n+    return \"{:.2f} {}\".format(weight_memory_size, units[-1])\n+\n+\n def get_layer_index_bound_by_layer_name(model, layer_range=None):\n     \"\"\"Get the layer indexes from the model based on layer names.\n \n@@ -432,14 +468,30 @@ def print_summary(\n \n     if hasattr(model, \"_collected_trainable_weights\"):\n         trainable_count = count_params(model._collected_trainable_weights)\n+        trainable_memory_size = weight_memory_size(\n+            model._collected_trainable_weights\n+        )\n     else:\n         trainable_count = count_params(model.trainable_weights)\n+        trainable_memory_size = weight_memory_size(model.trainable_weights)\n \n     non_trainable_count = count_params(model.non_trainable_weights)\n+    non_trainable_memory_size = weight_memory_size(model.non_trainable_weights)\n \n-    print_fn(f\"Total params: {trainable_count + non_trainable_count:,}\")\n-    print_fn(f\"Trainable params: {trainable_count:,}\")\n-    print_fn(f\"Non-trainable params: {non_trainable_count:,}\")\n+    total_memory_size = trainable_memory_size + non_trainable_memory_size\n+\n+    print_fn(\n+        f\"Total params: {trainable_count + non_trainable_count} \"\n+        f\"({readable_weight_memory_size(total_memory_size)})\"\n+    )\n+    print_fn(\n+        f\"Trainable params: {trainable_count} \"\n+        f\"({readable_weight_memory_size(trainable_memory_size)})\"\n+    )\n+    print_fn(\n+        f\"Non-trainable params: {non_trainable_count} \"\n+        f\"({readable_weight_memory_size(non_trainable_memory_size)})\"\n+    )\n     print_fn(\"_\" * line_length)\n \n \n\n@@ -27,6 +27,7 @@ import timeit\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n+from absl.testing import parameterized\n \n import keras\n from keras.testing_infra import test_utils\n@@ -50,7 +51,7 @@ class MyPickleableObject(tf.__internal__.tracking.AutoTrackable):\n         return id(self)\n \n \n-class LayerUtilsTest(tf.test.TestCase):\n+class LayerUtilsTest(tf.test.TestCase, parameterized.TestCase):\n     def test_print_summary(self):\n         model = keras.Sequential()\n         model.add(\n@@ -148,9 +149,9 @@ class LayerUtilsTest(tf.test.TestCase):\n                 \"|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n\"  # noqa: E501\n                 \"¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\\n\"  # noqa: E501\n                 \"=================================================================\\n\"  # noqa: E501\n-                \"Total params: 24\\n\"\n-                \"Trainable params: 18\\n\"\n-                \"Non-trainable params: 6\\n\"\n+                \"Total params: 24 (96.00 Byte)\\n\"\n+                \"Trainable params: 18 (72.00 Byte)\\n\"\n+                \"Non-trainable params: 6 (24.00 Byte)\\n\"\n                 \"_________________________________________________________________\\n\"  # noqa: E501\n             )\n \n@@ -276,9 +277,9 @@ class LayerUtilsTest(tf.test.TestCase):\n                 \" dense (Dense)               (None, 5)                 65        Y          \\n\"  # noqa: E501\n                 \"                                                                            \\n\"  # noqa: E501\n                 \"============================================================================\\n\"  # noqa: E501\n-                \"Total params: 127\\n\"\n-                \"Trainable params: 65\\n\"\n-                \"Non-trainable params: 62\\n\"\n+                \"Total params: 127 (508.00 Byte)\\n\"\n+                \"Trainable params: 65 (260.00 Byte)\\n\"\n+                \"Non-trainable params: 62 (248.00 Byte)\\n\"\n                 \"____________________________________________________________________________\\n\"  # noqa: E501\n                 \"____________________________________________________________________________\\n\"  # noqa: E501\n             )\n@@ -350,9 +351,9 @@ class LayerUtilsTest(tf.test.TestCase):\n                 \"|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n\"  # noqa: E501\n                 \"¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\\n\"  # noqa: E501\n                 \"============================================================================\\n\"  # noqa: E501\n-                \"Total params: 24\\n\"\n-                \"Trainable params: 6\\n\"\n-                \"Non-trainable params: 18\\n\"\n+                \"Total params: 24 (96.00 Byte)\\n\"\n+                \"Trainable params: 6 (24.00 Byte)\\n\"\n+                \"Non-trainable params: 18 (72.00 Byte)\\n\"\n                 \"____________________________________________________________________________\\n\"  # noqa: E501\n             )\n \n@@ -460,9 +461,9 @@ class LayerUtilsTest(tf.test.TestCase):\n                 \"|¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯|\\n\"  # noqa: E501\n                 \"¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯¯\\n\"  # noqa: E501\n                 \"=================================================================\\n\"  # noqa: E501\n-                \"Total params: 24\\n\"\n-                \"Trainable params: 18\\n\"\n-                \"Non-trainable params: 6\\n\"\n+                \"Total params: 24 (96.00 Byte)\\n\"\n+                \"Trainable params: 18 (72.00 Byte)\\n\"\n+                \"Non-trainable params: 6 (24.00 Byte)\\n\"\n                 \"_________________________________________________________________\\n\"  # noqa: E501\n             )\n \n@@ -476,6 +477,34 @@ class LayerUtilsTest(tf.test.TestCase):\n         except ImportError:\n             pass\n \n+    def test_weight_memory_size(self):\n+        v1 = tf.Variable(tf.zeros(shape=(1, 2), dtype=tf.float32))\n+        v2 = tf.Variable(tf.zeros(shape=(2, 3), dtype=tf.float64))\n+        v3 = tf.Variable(tf.zeros(shape=(4, 5), dtype=tf.int16))\n+        v4 = tf.Variable(tf.zeros(shape=(6,), dtype=tf.uint8))\n+\n+        weights = [v1, v1, v2, v3, v4]\n+        weight_memory_size = layer_utils.weight_memory_size(weights)\n+        expected_memory_size = 1 * 2 * 4 + 2 * 3 * 8 + 4 * 5 * 2 + 6 * 1\n+        self.assertEqual(weight_memory_size, expected_memory_size)\n+\n+    @parameterized.parameters(\n+        (0, \"0.00 Byte\"),\n+        (1000, \"1000.00 Byte\"),\n+        (1024, \"1.00 KB\"),\n+        (1024 * 2 - 1, \"2.00 KB\"),\n+        (1024 * 2 + 1, \"2.00 KB\"),\n+        (1024**2 + 1, \"1.00 MB\"),\n+        (1024**3 - 1, \"1024.00 MB\"),\n+        (1024**3, \"1.00 GB\"),\n+        (1024**4, \"1.00 TB\"),\n+        (1024**5, \"1.00 PB\"),\n+        (1024**5 * 1.41415, \"1.41 PB\"),\n+    )\n+    def test_readable_weight_memory_size(self, size, expected_result):\n+        result = layer_utils.readable_weight_memory_size(size)\n+        self.assertEqual(result, expected_result)\n+\n     def test_property_cache(self):\n         test_counter = collections.Counter()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5f2d2e9e4c1a10c37aed932b8b2e02678be12fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 59 | Churn Cumulative: 32090 | Contributors (this commit): 157 | Commits (past 90d): 12 | Contributors (cumulative): 179 | DMM Complexity: None\n\nDIFF:\n@@ -2292,6 +2292,21 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         return self.add_weight(*args, **kwargs)\n \n     def get_build_config(self):\n+        \"\"\"Returns a dictionary with the layer's input shape.\n+\n+        This method returns a config dict that can be used by\n+        `build_from_config(config)` to create all states (e.g. Variables and\n+        Lookup tables) needed by the layer.\n+\n+        By default, the config only contains the input shape that the layer\n+        was built with. If you're writing a custom layer that creates state in\n+        an unusual way, you should override this method to make sure this state\n+        is already created when Keras attempts to load its value upon model\n+        loading.\n+\n+        Returns:\n+            A dict containing the input shape associated with the layer.\n+        \"\"\"\n         if self._build_input_shape is not None:\n \n             def convert_tensorshapes(x):\n@@ -2306,6 +2321,16 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             }\n \n     def build_from_config(self, config):\n+        \"\"\"Builds the layer's states with the supplied config dict.\n+\n+        By default, this method calls the `build(config[\"input_shape\"])` method,\n+        which creates weights based on the layer's input shape in the supplied\n+        config. If your config contains other information needed to load the\n+        layer's state, you should override this method.\n+\n+        Args:\n+            config: Dict containing the input shape associated with this layer.\n+        \"\"\"\n         input_shape = config[\"input_shape\"]\n         if input_shape is not None:\n             self.build(input_shape)\n@@ -3500,13 +3525,27 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         object.__setattr__(self, \"__dict__\", state)\n \n     def save_own_variables(self, store):\n-        \"\"\"Experimental method for saving the state of this layer object.\"\"\"\n+        \"\"\"Saves the state of the layer.\n+\n+        You can override this method to take full control of how the state of\n+        the layer is saved upon calling `model.save()`.\n+\n+        Args:\n+            store: Dict where the state of the model will be saved.\n+        \"\"\"\n         all_vars = self._trainable_weights + self._non_trainable_weights\n         for i, v in enumerate(all_vars):\n             store[f\"{i}\"] = v.numpy()\n \n     def load_own_variables(self, store):\n-        \"\"\"Experimental method for loading the state of this layer object.\"\"\"\n+        \"\"\"Loads the state of the layer.\n+\n+        You can override this method to take full control of how the state of\n+        the layer is loaded upon calling `keras.models.load_model()`.\n+\n+        Args:\n+            store: Dict from which the state of the model will be loaded.\n+        \"\"\"\n         self._update_trackables()\n         all_vars = self._trainable_weights + self._non_trainable_weights\n         if len(store.keys()) != len(all_vars):\n\n@@ -3369,10 +3369,26 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         return result\n \n     def get_compile_config(self):\n+        \"\"\"Returns a serialized config with information for compiling the model.\n+\n+        This method returns a config dictionary containing all the information\n+        (optimizer, loss, metrics, etc.) with which the model was compiled.\n+\n+        Returns:\n+            A dict containing information for compiling the model.\n+        \"\"\"\n         if self._is_compiled and hasattr(self, \"_compile_config\"):\n             return self._compile_config.serialize()\n \n     def compile_from_config(self, config):\n+        \"\"\"Compiles the model with the information given in config.\n+\n+        This method uses the information in the config (optimizer, loss,\n+        metrics, etc.) to compile the model.\n+\n+        Args:\n+            config: Dict containing information for compiling the model.\n+        \"\"\"\n         has_overridden_compile = self.__class__.compile != Model.compile\n         if has_overridden_compile:\n             logging.warning(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#574340318979ea8d7863a9c184205aa712fcc699", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 175 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,7 @@ from tensorflow.python.distribute.experimental import (\n )\n \n \n+@test_utils.run_v2_only\n class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n     def setUp(self):\n         super().setUp()\n@@ -70,7 +71,6 @@ class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n         synchronized=[True, False],\n         renorm=[True, False],\n     )\n-    @test_utils.run_v2_only\n     def test_batch_normalization_with_dtensor_strategy(\n         self, training, synchronized, renorm\n     ):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#066922d5726e4b1cec4e2c7691181dfb6e621a49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1407 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -465,7 +465,7 @@ def _init_state_variable_for_rng(model, layout_map):\n             # When the keras_generator is not built yet. Call the init function\n             # with DTensor device to init all the variable with default\n             # replicated layout.\n-            with dtensor.run_on(layout_map.get_default_mesh()):\n+            with dtensor.default_mesh(layout_map.get_default_mesh()):\n                 keras_generator._maybe_init()\n \n \n\n@@ -161,7 +161,7 @@ def call_with_layout(fn, layout, *args, **kwargs):\n       The output of fn, with potential relayout with the layout specified.\n     \"\"\"\n     if layout:\n-        with dtensor.run_on(layout.mesh):\n+        with dtensor.default_mesh(layout.mesh):\n             result = fn(*args, **kwargs)\n             return dtensor.relayout(result, layout)\n     return fn(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9bb8d735b252582132644226d2e57d3976c1ce47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 21252 | Contributors (this commit): 142 | Commits (past 90d): 9 | Contributors (cumulative): 142 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2160,6 +2160,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     steps=data_handler.inferred_steps,\n                 )\n \n+            # Initialize to prevent errors if 0 epochs are evaluated.\n+            logs = {}\n+\n             test_function_runner = self._get_test_function_runner(callbacks)\n             self._test_counter.assign(0)\n             callbacks.on_test_begin()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#288c907410078295cefe787f9dd2153f5f47143c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 21256 | Contributors (this commit): 142 | Commits (past 90d): 10 | Contributors (cumulative): 142 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1742,7 +1742,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 if logs is None:\n                     raise ValueError(\n                         \"Unexpected result of `train_function` \"\n-                        \"(Empty logs). Please use \"\n+                        \"(Empty logs). This could be due to issues in input \"\n+                        \"pipeline that resulted in an empty dataset. \"\n+                        \"Otherwise, please use \"\n                         \"`Model.compile(..., run_eagerly=True)`, or \"\n                         \"`tf.config.run_functions_eagerly(True)` for more \"\n                         \"information of where went wrong, or file a \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6f173fb91d60140cae0d8256411b74776ad78a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 220 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 18/14 | Churn Δ: 225 | Churn Cumulative: 5013 | Contributors (this commit): 42 | Commits (past 90d): 4 | Contributors (cumulative): 51 | DMM Complexity: 0.5333333333333333\n\nDIFF:\n@@ -147,7 +147,107 @@ def weight_memory_size(weights):\n     return total_memory_size\n \n \n-def readable_weight_memory_size(weight_memory_size):\n+def dtensor_variable_summary(weights):\n+    \"\"\"Group and calculate DTensor based weights memory size.\n+\n+    Since DTensor weights can be sharded across multiple device, the result\n+    will be grouped by the layout/sharding spec for the variables, so that\n+    the accurate per-device memory size can be calculated.\n+\n+    Args:\n+        weights: An iterable contains the weights to compute weight size.\n+\n+    Returns:\n+        total_weight_count, total_memory_size and per_sharing_spec_result which\n+        is a dict with normalized layout spec as key and tuple of weight count\n+        and weight size as value.\n+    \"\"\"\n+    unique_weights = {id(w): w for w in weights}.values()\n+    total_weight_count = 0\n+    total_memory_size = 0\n+    per_sharing_spec_result = {}\n+    for w in unique_weights:\n+        # Ignore TrackableWeightHandlers, which will not have a shape defined.\n+        if not hasattr(w, \"shape\"):\n+            continue\n+        if not isinstance(w, tf.experimental.dtensor.DVariable):\n+            continue\n+        layout = w.layout\n+        # Remove all the duplication axis, and sort the column name.\n+        # 1D replicated and 2D replicated variable will still be fully\n+        # replicated, and [batch, model] sharding will have same memory\n+        # footprint as the [model, batch] layout.\n+        reduced_sharding_spec = list(sorted(set(layout.sharding_specs)))\n+        if tf.experimental.dtensor.UNSHARDED in reduced_sharding_spec:\n+            reduced_sharding_spec.remove(tf.experimental.dtensor.UNSHARDED)\n+        reduced_sharding_spec = tuple(reduced_sharding_spec)  # For dict key\n+        weight_count, memory_size = per_sharing_spec_result.get(\n+            reduced_sharding_spec, (0, 0)\n+        )\n+        reduced_weight_shape = np.prod(w.shape.as_list())\n+        per_param_size = w.dtype.size\n+        weight_count += reduced_weight_shape\n+        memory_size += reduced_weight_shape * per_param_size\n+        per_sharing_spec_result[reduced_sharding_spec] = (\n+            weight_count,\n+            memory_size,\n+        )\n+        total_weight_count += reduced_weight_shape\n+        total_memory_size += reduced_weight_shape * per_param_size\n+    return total_weight_count, total_memory_size, per_sharing_spec_result\n+\n+\n+def print_dtensor_variable_summary(model, print_fn, line_length):\n+    if getattr(model, \"_layout_map\", None) is not None:\n+        mesh = model._layout_map.get_default_mesh()\n+    elif hasattr(model, \"distribute_strategy\") and hasattr(\n+        model.distribute_strategy, \"_mesh\"\n+    ):\n+        mesh = model.distribute_strategy._mesh\n+    else:\n+        # Not running with DTensor\n+        mesh = None\n+    if mesh:\n+        (\n+            total_weight_count,\n+            total_memory_size,\n+            per_sharing_spec_result,\n+        ) = dtensor_variable_summary(model.weights)\n+        total_per_device_memory_size = 0\n+        for sharding_spec in sorted(per_sharing_spec_result.keys()):\n+            count, memory_size = per_sharing_spec_result[sharding_spec]\n+            if len(sharding_spec) == 0:\n+                print_fn(\n+                    f\"{count} / {total_weight_count} params \"\n+                    f\"({readable_memory_size(memory_size)}) \"\n+                    \"are fully replicated\"\n+                )\n+                per_device_size = memory_size\n+            else:\n+                sharding_factor = np.prod(\n+                    [mesh.dim_size(s) for s in sharding_spec]\n+                )\n+                per_device_size = memory_size / sharding_factor\n+                print_fn(\n+                    f\"{count} / {total_weight_count} params \"\n+                    f\"({readable_memory_size(memory_size)}) are sharded based \"\n+                    f\"on spec '{sharding_spec}' and across {sharding_factor} \"\n+                    f\"devices.\"\n+                )\n+            total_per_device_memory_size += per_device_size\n+        print_fn(\n+            \"Overall per device memory usage: \"\n+            f\"{readable_memory_size(total_per_device_memory_size)}\"\n+        )\n+        print_fn(\n+            \"Overall sharding factor: {:.2f}\".format(\n+                total_memory_size / total_per_device_memory_size\n+            )\n+        )\n+        print_fn(\"_\" * line_length)\n+\n+\n+def readable_memory_size(weight_memory_size):\n     \"\"\"Convert the weight memory size (Bytes) to a readable string.\"\"\"\n     units = [\"Byte\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n     scale = 1024\n@@ -482,18 +582,20 @@ def print_summary(\n \n     print_fn(\n         f\"Total params: {trainable_count + non_trainable_count} \"\n-        f\"({readable_weight_memory_size(total_memory_size)})\"\n+        f\"({readable_memory_size(total_memory_size)})\"\n     )\n     print_fn(\n         f\"Trainable params: {trainable_count} \"\n-        f\"({readable_weight_memory_size(trainable_memory_size)})\"\n+        f\"({readable_memory_size(trainable_memory_size)})\"\n     )\n     print_fn(\n         f\"Non-trainable params: {non_trainable_count} \"\n-        f\"({readable_weight_memory_size(non_trainable_memory_size)})\"\n+        f\"({readable_memory_size(non_trainable_memory_size)})\"\n     )\n     print_fn(\"_\" * line_length)\n \n+    print_dtensor_variable_summary(model, print_fn, line_length)\n+\n \n def convert_dense_weights_data_format(\n     dense, previous_feature_map_shape, target_data_format=\"channels_first\"\n\n@@ -16,6 +16,7 @@\n \n import collections\n import contextlib\n+import io\n import multiprocessing.dummy\n import os\n import pickle\n@@ -30,9 +31,15 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n+from keras import backend\n+from keras import layers\n+from keras.dtensor import dtensor_api as dtensor\n+from keras.dtensor import layout_map as layout_map_lib\n+from keras.dtensor import test_util\n from keras.testing_infra import test_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n+from keras.utils import tf_utils\n \n _PICKLEABLE_CALL_COUNT = collections.Counter()\n \n@@ -52,6 +59,12 @@ class MyPickleableObject(tf.__internal__.tracking.AutoTrackable):\n \n \n class LayerUtilsTest(tf.test.TestCase, parameterized.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        # Reset the UID so that all the layer/model ID will always start with 1.\n+        # This will help remove the undetermined IDs from the model.summary()\n+        backend.reset_uids()\n+\n     def test_print_summary(self):\n         model = keras.Sequential()\n         model.add(\n@@ -502,7 +515,7 @@ class LayerUtilsTest(tf.test.TestCase, parameterized.TestCase):\n         (1024**5 * 1.41415, \"1.41 PB\"),\n     )\n     def test_readable_weight_memory_size(self, size, expected_result):\n-        result = layer_utils.readable_weight_memory_size(size)\n+        result = layer_utils.readable_memory_size(size)\n         self.assertEqual(result, expected_result)\n \n     def test_property_cache(self):\n@@ -794,5 +807,105 @@ class LayerUtilsTest(tf.test.TestCase, parameterized.TestCase):\n         )\n \n \n+@test_utils.run_v2_only\n+class DTensorVariableSummaryTest(test_util.DTensorBaseTest):\n+    def setUp(self):\n+        super().setUp()\n+        backend.reset_uids()\n+        backend.enable_tf_random_generator()\n+        tf_utils.set_random_seed(1337)\n+        global_ids = test_util.create_device_ids_array((2, 2))\n+        local_device_ids = np.ravel(global_ids).tolist()\n+        mesh_dict = {\n+            \"CPU\": dtensor.Mesh(\n+                [\"batch\", \"model\"],\n+                global_ids,\n+                local_device_ids,\n+                test_util.create_device_list((2, 2), \"CPU\"),\n+            )\n+        }\n+        self.mesh = self.configTestMesh(mesh_dict)\n+        self.replicated_2d = dtensor.Layout.replicated(self.mesh, rank=2)\n+        self.replicated_1d = dtensor.Layout.replicated(self.mesh, rank=1)\n+        self.sharded_2d = dtensor.Layout([\"model\", \"batch\"], self.mesh)\n+        self.sharded_1d = dtensor.Layout([\"model\"], self.mesh)\n+\n+    def test_model_summary(self):\n+        layout_map = layout_map_lib.LayoutMap(mesh=self.mesh)\n+        layout_map[\"d1.kernel\"] = self.replicated_2d\n+        layout_map[\"d1.bias\"] = self.replicated_1d\n+        layout_map[\"d2.kernel\"] = self.sharded_2d\n+        layout_map[\"d2.bias\"] = self.sharded_1d\n+\n+        with layout_map.scope():\n+            inputs = layers.Input((10,), batch_size=10)\n+            x = layers.Dense(20, name=\"d1\")(inputs)\n+            x = layers.Dropout(0.1)(x)\n+            output = layers.Dense(30, name=\"d2\")(x)\n+\n+            model = keras.Model(inputs, output)\n+\n+        # For dtype = float32, following value are expected from memory stats\n+        expected_result = {}\n+        replicated_var_count = 10 * 20 + 20  # For d1 kernel and bias\n+        model_batch_shard_var_count = 30 * 20  # For d2 kernel\n+        model_shard_var_count = 30  # For d2 bias\n+        expected_result[()] = (replicated_var_count, replicated_var_count * 4)\n+        expected_result[(\"batch\", \"model\")] = (\n+            model_batch_shard_var_count,\n+            model_batch_shard_var_count * 4,\n+        )\n+        expected_result[(\"model\",)] = (\n+            model_shard_var_count,\n+            model_shard_var_count * 4,\n+        )\n+\n+        expected_total_weight_count = (\n+            replicated_var_count\n+            + model_batch_shard_var_count\n+            + model_shard_var_count\n+        )\n+        expected_total_memory_size = expected_total_weight_count * 4\n+\n+        (\n+            total_weight_count,\n+            total_memory_size,\n+            per_sharing_spec_result,\n+        ) = layer_utils.dtensor_variable_summary(model.weights)\n+\n+        self.assertEqual(total_weight_count, expected_total_weight_count)\n+        self.assertEqual(total_memory_size, expected_total_memory_size)\n+        self.assertDictEqual(per_sharing_spec_result, expected_result)\n+\n+        output_buffer = io.StringIO()\n+\n+        def print_to_buffer(content):\n+            output_buffer.write(content)\n+\n+        model.summary(print_fn=print_to_buffer)\n+\n+        self.assertRegex(\n+            output_buffer.getvalue(),\n+            f\"{replicated_var_count} / {expected_total_weight_count} params \"\n+            \".* are fully replicated\",\n+        )\n+        self.assertRegex(\n+            output_buffer.getvalue(),\n+            f\"{model_batch_shard_var_count} / {expected_total_weight_count} \"\n+            r\"params .* are sharded based on spec .*batch.*model\"\n+            r\".* across 4 devices\",\n+        )\n+        self.assertRegex(\n+            output_buffer.getvalue(),\n+            f\"{model_shard_var_count} / {expected_total_weight_count} \"\n+            r\"params .* are sharded based on spec .*model\"\n+            r\".* across 2 devices\",\n+        )\n+        self.assertIn(\n+            \"Overall per device memory usage: 1.50 KB\", output_buffer.getvalue()\n+        )\n+        self.assertIn(\"Overall sharding factor: 2.21\", output_buffer.getvalue())\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ada508af51f16f283a9bfcc5fca07e1f619a424", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 1128 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -571,12 +571,20 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n                     \"dtype_to_cast_to=float16>\",\n                 )\n \n-    def test_repr_distributed(self):\n-        strategy = tf.distribute.MirroredStrategy([\"/cpu:1\", \"/cpu:2\"])\n-        with strategy.scope():\n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            distribution=[\n+                tf.__internal__.distribute.combinations.mirrored_strategy_with_two_cpus,  # noqa: E501\n+            ]\n+        )\n+    )\n+    def test_repr_distributed(self, distribution):\n+        with distribution.scope():\n             x = get_var(1.0, tf.float32)\n             x = autocast_variable.create_autocast_variable(x)\n-            use_policy = getattr(strategy.extended, \"_use_var_policy\", False)\n+            use_policy = getattr(\n+                distribution.extended, \"_use_var_policy\", False\n+            )\n             if use_policy:\n                 self.assertRegex(\n                     repr(x).replace(\"\\n\", \" \"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b342b3a329508bfe3becdf02b1f6e1a05d545c66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 115 | Churn Cumulative: 6617 | Contributors (this commit): 26 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1688,6 +1688,121 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         cb_list.on_predict_batch_end(logs)\n         cb_list.on_predict_end(logs)\n \n+    def _run_fit_with_ModelCheckpoint_with_steps_per_execution(\n+        self,\n+        model,\n+        savepath,\n+        save_freq,\n+        train_samples,\n+        steps_per_execution,\n+        epochs,\n+        check_ckpt_epochs,\n+        check_ckpt_batchs,\n+    ):\n+        assert len(check_ckpt_epochs) == len(check_ckpt_batchs)\n+\n+        (x_train, y_train), _ = test_utils.get_test_data(\n+            train_samples=train_samples,\n+            test_samples=0,\n+            input_shape=(INPUT_DIM,),\n+            num_classes=NUM_CLASSES,\n+        )\n+        y_train = np_utils.to_categorical(y_train)\n+\n+        model.compile(\n+            loss=\"categorical_crossentropy\",\n+            optimizer=\"rmsprop\",\n+            steps_per_execution=steps_per_execution,\n+        )\n+\n+        self.assertFalse(os.path.exists(savepath))\n+\n+        callback = keras.callbacks.ModelCheckpoint(\n+            filepath=os.path.join(savepath, \"ckpt_{epoch}_{batch}\"),\n+            save_freq=save_freq,\n+        )\n+\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=1,\n+            epochs=epochs,\n+            verbose=0,\n+            callbacks=[callback],\n+        )\n+\n+        self.assertTrue(os.path.exists(savepath))\n+\n+        for i in range(len(check_ckpt_epochs)):\n+            epoch = check_ckpt_epochs[i]\n+            batch = check_ckpt_batchs[i]\n+            ckpt_name = \"ckpt_\" + str(epoch) + \"_\" + str(batch)\n+            ckpt_path = os.path.join(savepath, ckpt_name)\n+            self.assertTrue(os.path.exists(ckpt_path))\n+            self.assertIn(\"saved_model.pb\", os.listdir(ckpt_path))\n+\n+        shutil.rmtree(savepath)\n+\n+    @test_combinations.run_with_all_model_types\n+    def test_fit_with_ModelCheckpoint_with_steps_per_execution(self):\n+        layers = [\n+            keras.layers.Dense(\n+                NUM_HIDDEN, input_dim=INPUT_DIM, activation=\"relu\"\n+            ),\n+            keras.layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n+        ]\n+        model = test_utils.get_model_from_layers(\n+            layers, input_shape=(INPUT_DIM,)\n+        )\n+\n+        temp_dir = self.get_temp_dir()\n+        savepath = os.path.join(temp_dir, \"checkpoint\")\n+\n+        for steps_per_execution in [None, 7]:\n+            self._run_fit_with_ModelCheckpoint_with_steps_per_execution(\n+                model,\n+                savepath,\n+                save_freq=7,\n+                train_samples=7,\n+                steps_per_execution=steps_per_execution,\n+                epochs=1,\n+                check_ckpt_epochs=[1],\n+                check_ckpt_batchs=[7],\n+            )\n+\n+            self._run_fit_with_ModelCheckpoint_with_steps_per_execution(\n+                model,\n+                savepath,\n+                save_freq=7,\n+                train_samples=7,\n+                steps_per_execution=steps_per_execution,\n+                epochs=2,\n+                check_ckpt_epochs=[1, 2],\n+                check_ckpt_batchs=[7, 7],\n+            )\n+\n+            self._run_fit_with_ModelCheckpoint_with_steps_per_execution(\n+                model,\n+                savepath,\n+                save_freq=14,\n+                train_samples=7,\n+                steps_per_execution=steps_per_execution,\n+                epochs=2,\n+                check_ckpt_epochs=[2],\n+                check_ckpt_batchs=[7],\n+            )\n+\n+            self._run_fit_with_ModelCheckpoint_with_steps_per_execution(\n+                model,\n+                savepath,\n+                save_freq=7,\n+                train_samples=14,\n+                steps_per_execution=steps_per_execution,\n+                epochs=2,\n+                check_ckpt_epochs=[1, 1, 2, 2],\n+                check_ckpt_batchs=[7, 14, 7, 14],\n+            )\n+\n     def test_verbose_2_logging(self):\n         data = np.random.random((100, 1))\n         labels = np.where(data > 0.5, 1, 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3538622ace799cf1d9a5dd279af90098965665c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 17 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 17413 | Contributors (this commit): 46 | Commits (past 90d): 22 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -5605,6 +5605,7 @@ def categorical_focal_crossentropy(\n     parameter. When `gamma` = 0, there is no focal effect on the categorical\n     crossentropy. And if alpha = 1, at the same time the loss is equivalent\n     to the categorical crossentropy.\n+\n     Args:\n         target: A tensor with the same shape as `output`.\n         output: A tensor.\n@@ -5616,13 +5617,16 @@ def categorical_focal_crossentropy(\n             reference. It helps to gradually reduce the importance given to\n             simple examples in a smooth manner.\n         from_logits: Whether `output` is expected to be a logits tensor. By\n-        default, we consider that `output` encodes a probability distribution.\n+            default, we consider that `output` encodes a probability\n+            distribution.\n+\n     Returns:\n         A tensor.\n     \"\"\"\n     target = tf.convert_to_tensor(target)\n     output = tf.convert_to_tensor(output)\n     target.shape.assert_is_compatible_with(output.shape)\n+\n     output, from_logits = _get_logits(\n         output, from_logits, \"Softmax\", \"categorical_focal_crossentropy\"\n     )\n@@ -5633,11 +5637,13 @@ def categorical_focal_crossentropy(\n         lambda: output,\n     )\n \n+    # scale preds so that the class probas of each sample sum to 1\n     output = output / tf.reduce_sum(output, axis=axis, keepdims=True)\n \n     epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n     output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n \n+    # Calculate cross entropy\n     cce = -target * tf.math.log(output)\n \n     # Calculate factors\n\n@@ -945,29 +945,40 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     crossentropy. And if alpha = 1, at the same time the loss is equivalent to\n     the categorical crossentropy.\n \n+    Use this crossentropy loss function when there are two or more label\n+    classes and if you want to handle class imbalance without using\n+    `class_weights`.\n+    We expect labels to be provided in a `one_hot` representation.\n+\n     In the snippet below, there is `# classes` floating pointing values per\n     example. The shape of both `y_pred` and `y_true` are\n     `[batch_size, num_classes]`.\n+\n     Standalone usage:\n+\n     >>> y_true = [[0., 1., 0.], [0., 0., 1.]]\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n     >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n     >>> cce = tf.keras.losses.CategoricalFocalCrossentropy()\n     >>> cce(y_true, y_pred).numpy()\n     0.23315276\n+\n     >>> # Calling with 'sample_weight'.\n     >>> cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()\n     0.1632\n+\n     >>> # Using 'sum' reduction type.\n     >>> cce = tf.keras.losses.CategoricalFocalCrossentropy(\n     ...     reduction=tf.keras.losses.Reduction.SUM)\n     >>> cce(y_true, y_pred).numpy()\n     0.46631\n+\n     >>> # Using 'none' reduction type.\n     >>> cce = tf.keras.losses.CategoricalFocalCrossentropy(\n     ...     reduction=tf.keras.losses.Reduction.NONE)\n     >>> cce(y_true, y_pred).numpy()\n     array([3.2058331e-05, 4.6627346e-01], dtype=float32)\n+\n     Usage with the `compile()` API:\n     ```python\n     model.compile(optimizer='sgd',\n@@ -2154,6 +2165,7 @@ def categorical_focal_crossentropy(\n     axis=-1,\n ):\n     \"\"\"Computes the categorical focal crossentropy loss.\n+\n     Standalone usage:\n     >>> y_true = [[0, 1, 0], [0, 0, 1]]\n     >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]\n@@ -2161,6 +2173,7 @@ def categorical_focal_crossentropy(\n     >>> assert loss.shape == (2,)\n     >>> loss.numpy()\n     array([2.63401289e-04, 6.75912094e-01], dtype=float32)\n+\n     Args:\n       y_true: Tensor of one-hot true targets.\n       y_pred: Tensor of predicted targets.\n@@ -2179,6 +2192,7 @@ def categorical_focal_crossentropy(\n         and `0.9 + 0.1 / num_classes` for target labels.\n       axis: Defaults to -1. The dimension along which the entropy is\n         computed.\n+\n     Returns:\n       Categorical focal crossentropy loss value.\n     \"\"\"\n@@ -2240,6 +2254,8 @@ def _ragged_tensor_categorical_focal_crossentropy(\n     number of elements independent of the batch. E.g. if the RaggedTensor\n     has 2 batches with [2, 1] values respectively the resulting loss is\n     the sum of the individual loss values divided by 3.\n+\n+    Args:\n       alpha: A weight balancing factor for all classes, default is `0.25` as\n         mentioned in the reference. It can be a list of floats or a scalar.\n         In the multi-class case, alpha may be set by inverse class frequency by\n@@ -2255,6 +2271,7 @@ def _ragged_tensor_categorical_focal_crossentropy(\n         and `0.9 + 0.1 / num_classes` for target labels.\n       axis: Defaults to -1. The dimension along which the entropy is\n         computed.\n+\n     Returns:\n       Categorical focal crossentropy loss value.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0822af67236c8576cac7caac35f98eac739f507", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3175 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -328,7 +328,7 @@ def PreStem(name=None):\n \n \n def Head(num_classes=1000, classifier_activation=None, name=None):\n-    \"\"\"Implementation of classification head of RegNet.\n+    \"\"\"Implementation of classification head of ConvNeXt.\n \n     Args:\n       num_classes: number of classes for Dense layer\n@@ -336,7 +336,7 @@ def Head(num_classes=1000, classifier_activation=None, name=None):\n       name: name prefix\n \n     Returns:\n-      Classification head function.\n+      Tensor of logits or softmax values as the output.\n     \"\"\"\n     if name is None:\n         name = str(backend.get_uid(\"head\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea13c2b937d2e31f78bbbcd1d0ae5191d4525dc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1290 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -20,6 +20,7 @@ from keras.utils import object_identity\n \n # isort: off\n from tensorflow.python.data.util import structure\n+from tensorflow.python.util.tf_export import keras_export\n \n \n # Tensorflow tensors have a maximum rank of 254\n@@ -29,6 +30,7 @@ from tensorflow.python.data.util import structure\n _MAX_TENSOR_RANK = 254\n \n \n+@keras_export(\"keras.__internal__.KerasTensor\", v1=[])\n class KerasTensor:\n     \"\"\"A representation of a Keras in/output during Functional API construction.\n \n@@ -451,6 +453,7 @@ class KerasTensor:\n KerasTensor._overload_all_operators(tf.Tensor)\n \n \n+@keras_export(\"keras.__internal__.SparseKerasTensor\", v1=[])\n class SparseKerasTensor(KerasTensor):\n     \"\"\"A specialized KerasTensor representation for `tf.sparse.SparseTensor`s.\n \n@@ -471,6 +474,7 @@ class SparseKerasTensor(KerasTensor):\n         )\n \n \n+@keras_export(\"keras.__internal__.RaggedKerasTensor\", v1=[])\n class RaggedKerasTensor(KerasTensor):\n     \"\"\"A specialized KerasTensor representation for `tf.RaggedTensor`s.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7a2034e897f7b39dbdff799b49ca7feda0ff6cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 94 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.723404255319149\n\nDIFF:\n@@ -314,7 +314,9 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n                 \"No endpoints have been set yet. Call add_endpoint().\"\n             )\n         if not self._trackables:\n-            raise ValueError(\"No assets are being tracked. Call track().\")\n+            tvs, ntvs = self._get_variables_used_by_endpoints()\n+            if tvs or ntvs:\n+                raise ValueError(\"No assets are being tracked. Call `track()`.\")\n         signatures = {}\n         for name in self._endpoint_names:\n             signatures[name] = self._get_concrete_fn(name)\n@@ -346,6 +348,10 @@ class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n             traces = getattr(self, endpoint)._trackable_children(\"saved_model\")\n             return list(traces.values())[0]\n \n+    def _get_variables_used_by_endpoints(self):\n+        fns = [self._get_concrete_fn(name) for name in self._endpoint_names]\n+        return _list_variables_used_by_fns(fns)\n+\n \n def export_model(model, filepath):\n     export_archive = ExportArchive()\n@@ -458,27 +464,11 @@ class ReloadedLayer(base_layer.Layer):\n         all_fns = [self.call_endpoint_fn]\n         if call_training_endpoint:\n             all_fns.append(self.call_training_endpoint_fn)\n-        trainable_variables_ids = set()\n-        non_trainable_variables_ids = set()\n-        for fn in all_fns:\n-            # The function may or may not be already a concrete function\n-            if hasattr(fn, \"concrete_functions\"):\n-                concrete_functions = fn.concrete_functions\n-            else:\n-                concrete_functions = [fn]\n-            for concrete_fn in concrete_functions:\n-                for v in concrete_fn.trainable_variables:\n-                    if id(v) not in trainable_variables_ids:\n+        tvs, ntvs = _list_variables_used_by_fns(all_fns)\n+        for v in tvs:\n             self._add_existing_weight(v, trainable=True)\n-                        trainable_variables_ids.add(id(v))\n-\n-                for v in concrete_fn.variables:\n-                    if (\n-                        id(v) not in trainable_variables_ids\n-                        and id(v) not in non_trainable_variables_ids\n-                    ):\n+        for v in ntvs:\n             self._add_existing_weight(v, trainable=False)\n-                        non_trainable_variables_ids.add(id(v))\n         self.built = True\n \n     def _add_existing_weight(self, weight, trainable):\n@@ -519,3 +509,31 @@ def _print_signature(fn, name):\n     lines = [f\"* Endpoint '{name}'\"] + lines[1:]\n     endpoint = \"\\n\".join(lines)\n     return endpoint\n+\n+\n+def _list_variables_used_by_fns(fns):\n+    trainable_variables = []\n+    non_trainable_variables = []\n+    trainable_variables_ids = set()\n+    non_trainable_variables_ids = set()\n+    for fn in fns:\n+        if hasattr(fn, \"concrete_functions\"):\n+            concrete_functions = fn.concrete_functions\n+        elif hasattr(fn, \"get_concrete_function\"):\n+            concrete_functions = [fn.get_concrete_function()]\n+        else:\n+            concrete_functions = [fn]\n+        for concrete_fn in concrete_functions:\n+            for v in concrete_fn.trainable_variables:\n+                if id(v) not in trainable_variables_ids:\n+                    trainable_variables.append(v)\n+                    trainable_variables_ids.add(id(v))\n+\n+            for v in concrete_fn.variables:\n+                if (\n+                    id(v) not in trainable_variables_ids\n+                    and id(v) not in non_trainable_variables_ids\n+                ):\n+                    non_trainable_variables.append(v)\n+                    non_trainable_variables_ids.add(id(v))\n+    return trainable_variables, non_trainable_variables\n\n@@ -370,6 +370,42 @@ class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n                 my_endpoint,\n             )\n \n+    def test_export_no_assets(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+\n+        # Case where there are assets but they aren't tracked.\n+        model = keras.Sequential([keras.layers.Dense(2)])\n+        model(tf.random.normal((2, 3)))\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.add_endpoint(\n+            \"call\",\n+            model.call,\n+            input_signature=[\n+                tf.TensorSpec(\n+                    shape=(None, 3),\n+                    dtype=tf.float32,\n+                )\n+            ],\n+        )\n+        with self.assertRaisesRegex(ValueError, \"No assets\"):\n+            export_archive.write_out(temp_filepath)\n+\n+        # Case where there are legitimately no assets.\n+        model = keras.Sequential([keras.layers.Flatten()])\n+        model(tf.random.normal((2, 3)))\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.add_endpoint(\n+            \"call\",\n+            model.call,\n+            input_signature=[\n+                tf.TensorSpec(\n+                    shape=(None, 3),\n+                    dtype=tf.float32,\n+                )\n+            ],\n+        )\n+        export_archive.write_out(temp_filepath)\n+\n     @test_combinations.run_with_all_model_types\n     def test_model_export_method(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
