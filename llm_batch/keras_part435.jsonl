{"custom_id": "keras#7d74c9f955c0e56639f9bb6f5caf3332c17d0efc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 111 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -52,7 +52,7 @@ class SharpnessAwareMinimizationTest(tf.test.TestCase, parameterized.TestCase):\n           loss=keras.losses.BinaryCrossentropy(from_logits=True),\n       )\n \n-      sam_model.fit(data, label)\n+      sam_model.fit(data, label, steps_per_epoch=1)\n \n   def test_save_sam(self):\n     model = keras.Sequential([\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ed234b4d98f15c6d87bf6f471631c0e7a4ad213", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2032 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -958,7 +958,7 @@ class KerasSequenceAdapter(GeneratorDataAdapter):\n     return generator_fn\n \n   def get_size(self):\n-    return self._size\n+    return len(self._keras_sequence)\n \n   def should_recreate_iterator(self):\n     return True\n@@ -1138,6 +1138,8 @@ class DataHandler:\n     self._insufficient_data = False\n     self._model = model\n \n+    self._steps_per_epoch = steps_per_epoch\n+\n     # `steps_per_execution_value` is the cached initial value.\n     # `steps_per_execution` is mutable and may be changed by the DataAdapter\n     # to handle partial executions.\n@@ -1195,6 +1197,7 @@ class DataHandler:\n           break\n         if self._adapter.should_recreate_iterator():\n           data_iterator = iter(self._dataset)\n+          self._inferred_steps = self._infer_steps(self._steps_per_epoch, self._dataset)\n         yield epoch, data_iterator\n         self._adapter.on_epoch_end()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c392b5ad96fb47a05019e6dda42d2af1f1ec08e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 18 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 52 | Churn Cumulative: 14905 | Contributors (this commit): 95 | Commits (past 90d): 18 | Contributors (cumulative): 120 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@\n # ==============================================================================\n \"\"\"Utilities to preprocess data before training.\n \n-Warning: `tf.keras.preprocessing` APIs do not operate on tensors and are\n+Deprecated: `tf.keras.preprocessing` APIs do not operate on tensors and are\n not recommended for new code. Prefer loading data with either\n `tf.keras.utils.text_dataset_from_directory` or\n `tf.keras.utils.image_dataset_from_directory`, and then transforming the output\n\n@@ -18,8 +18,8 @@\n # pylint: disable=g-direct-tensorflow-import\n \"\"\"Utilies for image preprocessing and augmentation.\n \n-Warning: `tf.keras.preprocessing.image` APIs do not operate on tensors and are\n-not recommended for new code. Prefer loading data with\n+Deprecated: `tf.keras.preprocessing.image` APIs do not operate on tensors and\n+are not recommended for new code. Prefer loading data with\n `tf.keras.utils.image_dataset_from_directory`, and then transforming the output\n `tf.data.Dataset` with preprocessing layers. For more information, see the\n tutorials for [loading images](\n@@ -335,7 +335,7 @@ def load_img(path,\n class Iterator(data_utils.Sequence):\n   \"\"\"Base class for image data iterators.\n \n-  Warning: `tf.keras.preprocessing.image.Iterator` is not recommended for\n+  Deprecated: `tf.keras.preprocessing.image.Iterator` is not recommended for\n   new code. Prefer loading images with\n   `tf.keras.utils.image_dataset_from_directory` and transforming the output\n   `tf.data.Dataset` with preprocessing layers. For more information, see the\n@@ -686,8 +686,8 @@ class BatchFromFilesMixin():\n class DirectoryIterator(BatchFromFilesMixin, Iterator):\n   \"\"\"Iterator capable of reading images from a directory on disk.\n \n-  Warning: `tf.keras.preprocessing.image.DirectoryIterator` is not recommended\n-  for new code. Prefer loading images with\n+  Deprecated: `tf.keras.preprocessing.image.DirectoryIterator` is not\n+  recommended for new code. Prefer loading images with\n   `tf.keras.utils.image_dataset_from_directory` and transforming the output\n   `tf.data.Dataset` with preprocessing layers. For more information, see the\n   tutorials for [loading images](\n@@ -838,8 +838,8 @@ class DirectoryIterator(BatchFromFilesMixin, Iterator):\n class NumpyArrayIterator(Iterator):\n   \"\"\"Iterator yielding data from a Numpy array.\n \n-  Warning: `tf.keras.preprocessing.image.NumpyArrayIterator` is not recommended\n-  for new code. Prefer loading images with\n+  Deprecated: `tf.keras.preprocessing.image.NumpyArrayIterator` is not\n+  recommended for new code. Prefer loading images with\n   `tf.keras.utils.image_dataset_from_directory` and transforming the output\n   `tf.data.Dataset` with preprocessing layers. For more information, see the\n   tutorials for [loading images](\n@@ -1296,8 +1296,8 @@ def flip_axis(x, axis):\n class ImageDataGenerator():\n   \"\"\"Generate batches of tensor image data with real-time data augmentation.\n \n-  Warning: `tf.keras.preprocessing.image.ImageDataGenerator` is not recommended\n-  for new code. Prefer loading images with\n+  Deprecated: `tf.keras.preprocessing.image.ImageDataGenerator` is not\n+  recommended for new code. Prefer loading images with\n   `tf.keras.utils.image_dataset_from_directory` and transforming the output\n   `tf.data.Dataset` with preprocessing layers. For more information, see the\n   tutorials for [loading images](\n@@ -2202,7 +2202,7 @@ def random_rotation(x, rg, row_axis=1, col_axis=2, channel_axis=0,\n                     fill_mode='nearest', cval=0., interpolation_order=1):\n   \"\"\"Performs a random rotation of a Numpy image tensor.\n \n-  Warning: `tf.keras.preprocessing.image.random_rotation` does not operate on\n+  Deprecated: `tf.keras.preprocessing.image.random_rotation` does not operate on\n   tensors and is not recommended for new code. Prefer\n   `tf.keras.layers.RandomRotation` which provides equivalent functionality as a\n   preprocessing layer. For more information, see the tutorial for\n@@ -2245,7 +2245,7 @@ def random_shift(x, wrg, hrg, row_axis=1, col_axis=2, channel_axis=0,\n                  fill_mode='nearest', cval=0., interpolation_order=1):\n   \"\"\"Performs a random spatial shift of a Numpy image tensor.\n \n-  Warning: `tf.keras.preprocessing.image.random_shift` does not operate on\n+  Deprecated: `tf.keras.preprocessing.image.random_shift` does not operate on\n   tensors and is not recommended for new code. Prefer\n   `tf.keras.layers.RandomTranslation` which provides equivalent functionality as\n   a preprocessing layer. For more information, see the tutorial for\n@@ -2327,7 +2327,7 @@ def random_zoom(x, zoom_range, row_axis=1, col_axis=2, channel_axis=0,\n                 fill_mode='nearest', cval=0., interpolation_order=1):\n   \"\"\"Performs a random spatial zoom of a Numpy image tensor.\n \n-  Warning: `tf.keras.preprocessing.image.random_zoom` does not operate on\n+  Deprecated: `tf.keras.preprocessing.image.random_zoom` does not operate on\n   tensors and is not recommended for new code. Prefer\n   `tf.keras.layers.RandomZoom` which provides equivalent functionality as\n   a preprocessing layer. For more information, see the tutorial for\n@@ -2448,8 +2448,8 @@ def apply_brightness_shift(x, brightness, scale=True):\n def random_brightness(x, brightness_range, scale=True):\n   \"\"\"Performs a random brightness shift.\n \n-  Warning: `tf.keras.preprocessing.image.random_brightness` does not operate on\n-  tensors and is not recommended for new code. Prefer\n+  Deprecated: `tf.keras.preprocessing.image.random_brightness` does not operate\n+  on tensors and is not recommended for new code. Prefer\n   `tf.keras.layers.RandomBrightness` which provides equivalent functionality as\n   a preprocessing layer. For more information, see the tutorial for\n   [augmenting images](\n\n@@ -14,7 +14,7 @@\n # ==============================================================================\n \"\"\"Utilities for preprocessing sequence data.\n \n-Warning: `tf.keras.preprocessing.sequence` APIs are not recommended for new\n+Deprecated: `tf.keras.preprocessing.sequence` APIs are not recommended for new\n code. Prefer `tf.keras.utils.timeseries_dataset_from_array` and\n the `tf.data` APIs which provide a much more flexible mechanisms for dealing\n with sequences. See the [tf.data guide](https://www.tensorflow.org/guide/data)\n@@ -56,7 +56,7 @@ def _remove_long_seq(maxlen, seq, label):\n class TimeseriesGenerator(data_utils.Sequence):\n   \"\"\"Utility class for generating batches of temporal data.\n \n-  Warning: `tf.keras.preprocessing.sequence.TimeseriesGenerator` does not\n+  Deprecated: `tf.keras.preprocessing.sequence.TimeseriesGenerator` does not\n   operate on tensors and is not recommended for new code. Prefer using a\n   `tf.data.Dataset` which provides a more efficient and flexible mechanism for\n   batching, shuffling, and windowing input. See the\n\n@@ -14,7 +14,7 @@\n # ==============================================================================\n \"\"\"Utilities for text input preprocessing.\n \n-Warning: `tf.keras.preprocessing.text` APIs are not recommended for new code.\n+Deprecated: `tf.keras.preprocessing.text` APIs are not recommended for new code.\n Prefer `tf.keras.utils.text_dataset_from_directory` and\n `tf.keras.layers.TextVectorization` which provide a more efficient approach\n for preprocessing text input. For an introduction to these APIs, see\n@@ -43,8 +43,8 @@ def text_to_word_sequence(input_text,\n                           split=' '):\n   r\"\"\"Converts a text to a sequence of words (or tokens).\n \n-  Warning: `tf.keras.preprocessing.text.text_to_word_sequence` does not operate\n-  on tensors and is not recommended for new code. Prefer\n+  Deprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\n+  operate on tensors and is not recommended for new code. Prefer\n   `tf.strings.regex_replace` and `tf.strings.split` which provide equivalent\n   functionality and accept `tf.Tensor` input. For an overview of text handling\n   in Tensorflow, see the [text loading tutorial]\n@@ -88,7 +88,7 @@ def one_hot(input_text,\n             analyzer=None):\n   r\"\"\"One-hot encodes a text into a list of word indexes of size `n`.\n \n-  Warning: `tf.keras.text.preprocessing.one_hot` does not operate on tensors\n+  Deprecated: `tf.keras.text.preprocessing.one_hot` does not operate on tensors\n   and is not recommended for new code. Prefer `tf.keras.layers.Hashing` with\n   `output_mode='one_hot'` which provides equivalent functionality through a\n   layer which accepts `tf.Tensor` input. See the [preprocessing layer guide]\n@@ -136,7 +136,7 @@ def hashing_trick(text,\n                   analyzer=None):\n   r\"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n \n-  Warning: `tf.keras.text.preprocessing.hashing_trick` does not operate on\n+  Deprecated: `tf.keras.text.preprocessing.hashing_trick` does not operate on\n   tensors and is not recommended for new code. Prefer `tf.keras.layers.Hashing`\n   which provides equivalent functionality through a layer which accepts\n   `tf.Tensor` input. See the [preprocessing layer guide]\n@@ -185,8 +185,8 @@ def hashing_trick(text,\n class Tokenizer(object):\n   \"\"\"Text tokenization utility class.\n \n-  Warning: `tf.keras.preprocessing.text.Tokenizer` does not operate on tensors\n-  and is not recommended for new code. Prefer\n+  Deprecated: `tf.keras.preprocessing.text.Tokenizer` does not operate on\n+  tensors and is not recommended for new code. Prefer\n   `tf.keras.layers.TextVectorization` which provides equivalent functionality\n   through a layer which accepts `tf.Tensor` input. See the\n   [text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)\n@@ -547,8 +547,8 @@ class Tokenizer(object):\n def tokenizer_from_json(json_string):\n   \"\"\"Parses a JSON tokenizer configuration and returns a tokenizer instance.\n \n-  Warning: `tf.keras.preprocessing.text.Tokenizer` does not operate on tensors\n-  and is not recommended for new code. Prefer\n+  Deprecated: `tf.keras.preprocessing.text.Tokenizer` does not operate on\n+  tensors and is not recommended for new code. Prefer\n   `tf.keras.layers.TextVectorization` which provides equivalent functionality\n   through a layer which accepts `tf.Tensor` input. See the\n   [text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c557822aa2caa7f9e81aae81bf8e6947ee5438cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2047 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n   converted to `tf.data.Dataset` if possible.\n \n   Note that since this class is mainly targeted for TF 2.0, it might have a lot\n-  of assumptions under the hood, eg eager context by default, distribution\n+  of assumptions under the hood, e.g. eager context by default, distribution\n   strategy, etc. In the meantime, some legacy feature support might be dropped,\n   eg, Iterator from dataset API in v1, etc.\n \n@@ -107,7 +107,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n           `distribution_strategy` is passed, the created dataset need to respect\n           the strategy.\n         DataAdapter might choose to ignore any keyword argument if it doesn't\n-        use it, or raise exception if any required argument is not provide.\n+        use it, or raise exception if any required argument is not provided.\n     \"\"\"\n     if not self.can_handle(x, y):\n       raise ValueError(\"{} Cannot handle input {}, {}\".format(\n@@ -119,11 +119,11 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n \n     Note that the dataset returned does not repeat for epoch, so caller might\n     need to create new iterator for the same dataset at the beginning of the\n-    epoch. This behavior might change in future.\n+    epoch. This behavior might change in the future.\n \n     Returns:\n       An tf.dataset.Dataset. Caller might use the dataset in different\n-      context, eg iter(dataset) in eager to get the value directly, or in graph\n+      context, e.g. iter(dataset) in eager to get the value directly, or in graph\n       mode, provide the iterator tensor to Keras model function.\n     \"\"\"\n     raise NotImplementedError\n@@ -135,7 +135,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n     For certain type of the data input, the number of batches is known, eg for\n     Numpy data, the size is same as (number_of_element / batch_size). Whereas\n     for dataset or python generator, the size is unknown since it may or may not\n-    have a end state.\n+    have an end state.\n \n     Returns:\n       int, the number of batches for the dataset, or None if it is unknown. The\n@@ -590,7 +590,7 @@ class CompositeTensorDataAdapter(DataAdapter):\n       dataset = dataset.shuffle(num_samples)\n \n     # If batch_size is not passed but steps is, calculate from the input data.\n-    # Default to 32 for backwards compat.\n+    # Default to 32 for backwards compatibility.\n     if not batch_size:\n       batch_size = int(math.ceil(num_samples / steps)) if steps else 32\n \n@@ -978,7 +978,7 @@ ALL_ADAPTER_CLS = [\n \n \n def select_data_adapter(x, y):\n-  \"\"\"Selects a data adapter than can handle a given x and y.\"\"\"\n+  \"\"\"Selects a data adapter that can handle a given x and y.\"\"\"\n   adapter_cls = [cls for cls in ALL_ADAPTER_CLS if cls.can_handle(x, y)]\n   if not adapter_cls:\n     # TODO(scottzhu): This should be a less implementation-specific error.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8d60d2ff1102951111621c3c0d977bc9484fa8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 27 | Churn Cumulative: 514 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 8 | DMM Complexity: 0.875\n\nDIFF:\n@@ -131,12 +131,14 @@ class AdamW(optimizer.Optimizer):\n \n     Args:\n       var_list: list of model variables to build AdamW variables on.\n+      exclude_from_weight_decay: list of model variables that will be excluded from weight decay.\n     \"\"\"\n     super().build(var_list)\n     if hasattr(self, '_built') and self._built:\n       return\n     self._built = True\n-    self._exclude_from_weight_decay = var_list or []\n+    if not hasattr(self, '_exclude_from_weight_decay'):\n+      self._exclude_from_weight_decay = exclude_from_weight_decay or []\n     self._momentums = []\n     self._velocities = []\n     for var in var_list:\n\n@@ -111,6 +111,23 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     clipped_grad = optimizer._clip_gradients(grad)\n     self.assertAllEqual(clipped_grad[0], [1.0, 1.0])\n \n+  def testWeightDecay(self):\n+    grads, var1, var2, var3 = tf.zeros(()), tf.Variable(2.0), tf.Variable(2.0), tf.Variable(2.0)\n+    optimizer_1 = adamw_new.AdamW(learning_rate=0.001, weight_decay=0.004)\n+    optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+    optimizer_2 = adamw_new.AdamW(learning_rate=0.001, weight_decay=0.004)\n+    optimizer_2.exclude_from_weight_decay([var2])\n+    optimizer_2.apply_gradients(zip([grads], [var2]))\n+\n+    optimizer_3 = adamw_new.AdamW(learning_rate=0.001, weight_decay=0.004)\n+    optimizer_3.build([var3], exclude_from_weight_decay=[var3])\n+    optimizer_3.apply_gradients(zip([grads], [var3]))\n+\n+    self.assertAllClose(var1, 8e-6)\n+    self.assertEqual(var2, 2.0)\n+    self.assertEqual(var3, 2.0)\n+\n   def testClipGlobalNorm(self):\n     optimizer = adam_new.Adam(global_clipnorm=1)\n     grad = [\n@@ -280,8 +297,6 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         [keras.layers.Input(shape=(1,)),\n          keras.layers.Dense(1)])\n     optimizer = optimizer_fn()\n-    if type(optimizer) is adamw_new.AdamW:\n-      optimizer.exclude_from_weight_decay(model.layers[-1].weights[-1])\n     optimizer.clipnorm = 0.1\n     x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n@@ -381,8 +396,6 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n           [keras.layers.Input(shape=(1,)),\n            keras.layers.Dense(1)])\n       optimizer = optimizer_fn()\n-      if type(optimizer) is adamw_new.AdamW:\n-        optimizer.exclude_from_weight_decay(model.layers[-1].weights[-1])\n       x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n       y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n       model.compile(loss=\"mse\", optimizer=optimizer)\n@@ -407,8 +420,6 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n           [keras.layers.Input(shape=(1,)),\n            keras.layers.Dense(1)])\n       optimizer = optimizer_fn()\n-      if type(optimizer) is adamw_new.AdamW:\n-        optimizer.exclude_from_weight_decay(model.layers[-1].weights[-1])\n \n       def per_worker_dataset_fn():\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#143b08ad3aaf9adbc297d4bc17921234677226f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 16911 | Contributors (this commit): 126 | Commits (past 90d): 33 | Contributors (cumulative): 126 | DMM Complexity: None\n\nDIFF:\n@@ -612,7 +612,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           machine learning.\n           `jit_compile` is not enabled for by default.\n           This option cannot be enabled with `run_eagerly=True`.\n-          Note that `jit_compile=True` is\n+          Note that `jit_compile=True`\n           may not necessarily work for all models.\n           For more information on supported operations please refer to the\n           [XLA documentation](https://www.tensorflow.org/xla).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#013adbae4d35448c49ccf36938cdf9a684a8ddd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2542 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -800,7 +800,8 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n     # self._optimizer.apply_gradients does not take\n     # experimental_aggregate_gradients.\n     return self._optimizer.apply_gradients(\n-        list(zip(grads, wrapped_vars.value)), name,\n+        list(zip(grads, wrapped_vars.value)),\n+        name=name,\n         experimental_aggregate_gradients=False)\n \n   def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d022b8c987e470c33f8aac86606f6f7369f64feb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 199 | Lines Deleted: 195 | Files Changed: 6 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/18 | Churn Δ: 394 | Churn Cumulative: 7688 | Contributors (this commit): 61 | Commits (past 90d): 15 | Contributors (cumulative): 87 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ from keras.distribute.strategy_combinations import all_strategies\n from keras.distribute.strategy_combinations import multi_worker_mirrored_strategies\n from keras.distribute.strategy_combinations import strategies_minus_tpu\n from keras.mixed_precision import policy\n-from keras.preprocessing import sequence\n+from keras.utils import data_utils\n \n _RANDOM_SEED = 1337\n _EVAL_STEPS = 20\n@@ -607,7 +607,7 @@ class TestDistributionStrategyEmbeddingModelCorrectnessBase(\n       labels.append(label)\n       features.append(word_ids)\n \n-    features = sequence.pad_sequences(\n+    features = data_utils.pad_sequences(\n         features, maxlen=max_words)\n     x_train = np.asarray(features, dtype=np.float32)\n     y_train = np.asarray(labels, dtype=np.int32).reshape((count, 1))\n\n@@ -231,130 +231,6 @@ class TimeseriesGenerator(data_utils.Sequence):\n     return json.dumps(timeseries_generator_config, **kwargs)\n \n \n-@keras_export('keras.utils.pad_sequences',\n-              'keras.preprocessing.sequence.pad_sequences')\n-def pad_sequences(sequences, maxlen=None, dtype='int32',\n-                  padding='pre', truncating='pre', value=0.):\n-  \"\"\"Pads sequences to the same length.\n-\n-  This function transforms a list (of length `num_samples`)\n-  of sequences (lists of integers)\n-  into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n-  `num_timesteps` is either the `maxlen` argument if provided,\n-  or the length of the longest sequence in the list.\n-\n-  Sequences that are shorter than `num_timesteps`\n-  are padded with `value` until they are `num_timesteps` long.\n-\n-  Sequences longer than `num_timesteps` are truncated\n-  so that they fit the desired length.\n-\n-  The position where padding or truncation happens is determined by\n-  the arguments `padding` and `truncating`, respectively.\n-  Pre-padding or removing values from the beginning of the sequence is the\n-  default.\n-\n-  >>> sequence = [[1], [2, 3], [4, 5, 6]]\n-  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence)\n-  array([[0, 0, 1],\n-         [0, 2, 3],\n-         [4, 5, 6]], dtype=int32)\n-\n-  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)\n-  array([[-1, -1,  1],\n-         [-1,  2,  3],\n-         [ 4,  5,  6]], dtype=int32)\n-\n-  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\n-  array([[1, 0, 0],\n-         [2, 3, 0],\n-         [4, 5, 6]], dtype=int32)\n-\n-  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)\n-  array([[0, 1],\n-         [2, 3],\n-         [5, 6]], dtype=int32)\n-\n-  Args:\n-      sequences: List of sequences (each sequence is a list of integers).\n-      maxlen: Optional Int, maximum length of all sequences. If not provided,\n-          sequences will be padded to the length of the longest individual\n-          sequence.\n-      dtype: (Optional, defaults to int32). Type of the output sequences.\n-          To pad sequences with variable length strings, you can use `object`.\n-      padding: String, 'pre' or 'post' (optional, defaults to 'pre'):\n-          pad either before or after each sequence.\n-      truncating: String, 'pre' or 'post' (optional, defaults to 'pre'):\n-          remove values from sequences larger than\n-          `maxlen`, either at the beginning or at the end of the sequences.\n-      value: Float or String, padding value. (Optional, defaults to 0.)\n-\n-  Returns:\n-      Numpy array with shape `(len(sequences), maxlen)`\n-\n-  Raises:\n-      ValueError: In case of invalid values for `truncating` or `padding`,\n-          or in case of invalid shape for a `sequences` entry.\n-  \"\"\"\n-  if not hasattr(sequences, '__len__'):\n-    raise ValueError('`sequences` must be iterable.')\n-  num_samples = len(sequences)\n-\n-  lengths = []\n-  sample_shape = ()\n-  flag = True\n-\n-  # take the sample shape from the first non empty sequence\n-  # checking for consistency in the main loop below.\n-\n-  for x in sequences:\n-    try:\n-      lengths.append(len(x))\n-      if flag and len(x):\n-        sample_shape = np.asarray(x).shape[1:]\n-        flag = False\n-    except TypeError as e:\n-      raise ValueError('`sequences` must be a list of iterables. '\n-                       'Found non-iterable: ' + str(x)) from e\n-\n-  if maxlen is None:\n-    maxlen = np.max(lengths)\n-\n-  is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n-      dtype, np.unicode_)\n-  if isinstance(value, str) and dtype != object and not is_dtype_str:\n-    raise ValueError(\n-        \"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n-        'You should set `dtype=object` for variable length strings.'.format(\n-            dtype, type(value)))\n-\n-  x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n-  for idx, s in enumerate(sequences):\n-    if not len(s):  # pylint: disable=g-explicit-length-test\n-      continue  # empty list/array was found\n-    if truncating == 'pre':\n-      trunc = s[-maxlen:]  # pylint: disable=invalid-unary-operand-type\n-    elif truncating == 'post':\n-      trunc = s[:maxlen]\n-    else:\n-      raise ValueError('Truncating type \"%s\" ' 'not understood' % truncating)\n-\n-    # check `trunc` has expected shape\n-    trunc = np.asarray(trunc, dtype=dtype)\n-    if trunc.shape[1:] != sample_shape:\n-      raise ValueError('Shape of sample %s of sequence at position %s '\n-                       'is different from expected shape %s' %\n-                       (trunc.shape[1:], idx, sample_shape))\n-\n-    if padding == 'post':\n-      x[idx, :len(trunc)] = trunc\n-    elif padding == 'pre':\n-      x[idx, -len(trunc):] = trunc\n-    else:\n-      raise ValueError('Padding type \"%s\" not understood' % padding)\n-  return x\n-\n-\n @keras_export('keras.preprocessing.sequence.make_sampling_table')\n def make_sampling_table(size, sampling_factor=1e-5):\n   \"\"\"Generates a word rank-based probabilistic sampling table.\n\n@@ -23,75 +23,6 @@ import tensorflow.compat.v2 as tf\n \n class TestSequence(tf.test.TestCase):\n \n-  def test_pad_sequences(self):\n-    a = [[1], [1, 2], [1, 2, 3]]\n-\n-    # test padding\n-    b = sequence.pad_sequences(a, maxlen=3, padding='pre')\n-    self.assertAllClose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n-    b = sequence.pad_sequences(a, maxlen=3, padding='post')\n-    self.assertAllClose(b, [[1, 0, 0], [1, 2, 0], [1, 2, 3]])\n-\n-    # test truncating\n-    b = sequence.pad_sequences(a, maxlen=2, truncating='pre')\n-    self.assertAllClose(b, [[0, 1], [1, 2], [2, 3]])\n-    b = sequence.pad_sequences(a, maxlen=2, truncating='post')\n-    self.assertAllClose(b, [[0, 1], [1, 2], [1, 2]])\n-\n-    # test value\n-    b = sequence.pad_sequences(a, maxlen=3, value=1)\n-    self.assertAllClose(b, [[1, 1, 1], [1, 1, 2], [1, 2, 3]])\n-\n-  def test_pad_sequences_str(self):\n-    a = [['1'], ['1', '2'], ['1', '2', '3']]\n-\n-    # test padding\n-    b = sequence.pad_sequences(\n-        a, maxlen=3, padding='pre', value='pad', dtype=object)\n-    self.assertAllEqual(\n-        b, [['pad', 'pad', '1'], ['pad', '1', '2'], ['1', '2', '3']])\n-    b = sequence.pad_sequences(\n-        a, maxlen=3, padding='post', value='pad', dtype='<U3')\n-    self.assertAllEqual(\n-        b, [['1', 'pad', 'pad'], ['1', '2', 'pad'], ['1', '2', '3']])\n-\n-    # test truncating\n-    b = sequence.pad_sequences(\n-        a, maxlen=2, truncating='pre', value='pad', dtype=object)\n-    self.assertAllEqual(b, [['pad', '1'], ['1', '2'], ['2', '3']])\n-    b = sequence.pad_sequences(\n-        a, maxlen=2, truncating='post', value='pad', dtype='<U3')\n-    self.assertAllEqual(b, [['pad', '1'], ['1', '2'], ['1', '2']])\n-\n-    with self.assertRaisesRegex(ValueError,\n-                                '`dtype` int32 is not compatible with '):\n-      sequence.pad_sequences(a, maxlen=2, truncating='post', value='pad')\n-\n-  def test_pad_sequences_vector(self):\n-    a = [[[1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2], [3, 3]]]\n-\n-    # test padding\n-    b = sequence.pad_sequences(a, maxlen=3, padding='pre')\n-    self.assertAllClose(b, [[[0, 0], [0, 0], [1, 1]], [[0, 0], [2, 1], [2, 2]],\n-                            [[3, 1], [3, 2], [3, 3]]])\n-    b = sequence.pad_sequences(a, maxlen=3, padding='post')\n-    self.assertAllClose(b, [[[1, 1], [0, 0], [0, 0]], [[2, 1], [2, 2], [0, 0]],\n-                            [[3, 1], [3, 2], [3, 3]]])\n-\n-    # test truncating\n-    b = sequence.pad_sequences(a, maxlen=2, truncating='pre')\n-    self.assertAllClose(b,\n-                        [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 2], [3, 3]]])\n-\n-    b = sequence.pad_sequences(a, maxlen=2, truncating='post')\n-    self.assertAllClose(b,\n-                        [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2]]])\n-\n-    # test value\n-    b = sequence.pad_sequences(a, maxlen=3, value=1)\n-    self.assertAllClose(b, [[[1, 1], [1, 1], [1, 1]], [[1, 1], [2, 1], [2, 2]],\n-                            [[3, 1], [3, 2], [3, 3]]])\n-\n   def test_make_sampling_table(self):\n     a = sequence.make_sampling_table(3)\n     self.assertAllClose(\n\n@@ -31,6 +31,7 @@ from keras.utils.data_utils import Sequence\n from keras.utils.data_utils import GeneratorEnqueuer\n from keras.utils.data_utils import OrderedEnqueuer\n from keras.utils.data_utils import SequenceEnqueuer\n+from keras.utils.data_utils import pad_sequences\n \n # Serialization related\n from keras.utils.generic_utils import custom_object_scope\n\n@@ -927,3 +927,127 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n             'Keras requires a thread-safe generator when '\n             '`use_multiprocessing=False, workers > 1`. ')\n       raise e\n+\n+\n+@keras_export('keras.utils.pad_sequences',\n+              'keras.preprocessing.sequence.pad_sequences')\n+def pad_sequences(sequences, maxlen=None, dtype='int32',\n+                  padding='pre', truncating='pre', value=0.):\n+  \"\"\"Pads sequences to the same length.\n+\n+  This function transforms a list (of length `num_samples`)\n+  of sequences (lists of integers)\n+  into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n+  `num_timesteps` is either the `maxlen` argument if provided,\n+  or the length of the longest sequence in the list.\n+\n+  Sequences that are shorter than `num_timesteps`\n+  are padded with `value` until they are `num_timesteps` long.\n+\n+  Sequences longer than `num_timesteps` are truncated\n+  so that they fit the desired length.\n+\n+  The position where padding or truncation happens is determined by\n+  the arguments `padding` and `truncating`, respectively.\n+  Pre-padding or removing values from the beginning of the sequence is the\n+  default.\n+\n+  >>> sequence = [[1], [2, 3], [4, 5, 6]]\n+  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence)\n+  array([[0, 0, 1],\n+         [0, 2, 3],\n+         [4, 5, 6]], dtype=int32)\n+\n+  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)\n+  array([[-1, -1,  1],\n+         [-1,  2,  3],\n+         [ 4,  5,  6]], dtype=int32)\n+\n+  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\n+  array([[1, 0, 0],\n+         [2, 3, 0],\n+         [4, 5, 6]], dtype=int32)\n+\n+  >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)\n+  array([[0, 1],\n+         [2, 3],\n+         [5, 6]], dtype=int32)\n+\n+  Args:\n+      sequences: List of sequences (each sequence is a list of integers).\n+      maxlen: Optional Int, maximum length of all sequences. If not provided,\n+          sequences will be padded to the length of the longest individual\n+          sequence.\n+      dtype: (Optional, defaults to `\"int32\"`). Type of the output sequences.\n+          To pad sequences with variable length strings, you can use `object`.\n+      padding: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n+          pad either before or after each sequence.\n+      truncating: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n+          remove values from sequences larger than\n+          `maxlen`, either at the beginning or at the end of the sequences.\n+      value: Float or String, padding value. (Optional, defaults to 0.)\n+\n+  Returns:\n+      Numpy array with shape `(len(sequences), maxlen)`\n+\n+  Raises:\n+      ValueError: In case of invalid values for `truncating` or `padding`,\n+          or in case of invalid shape for a `sequences` entry.\n+  \"\"\"\n+  if not hasattr(sequences, '__len__'):\n+    raise ValueError('`sequences` must be iterable.')\n+  num_samples = len(sequences)\n+\n+  lengths = []\n+  sample_shape = ()\n+  flag = True\n+\n+  # take the sample shape from the first non empty sequence\n+  # checking for consistency in the main loop below.\n+\n+  for x in sequences:\n+    try:\n+      lengths.append(len(x))\n+      if flag and len(x):\n+        sample_shape = np.asarray(x).shape[1:]\n+        flag = False\n+    except TypeError as e:\n+      raise ValueError('`sequences` must be a list of iterables. '\n+                       f'Found non-iterable: {str(x)}') from e\n+\n+  if maxlen is None:\n+    maxlen = np.max(lengths)\n+\n+  is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n+      dtype, np.unicode_)\n+  if isinstance(value, str) and dtype != object and not is_dtype_str:\n+    raise ValueError(\n+        f'`dtype` {dtype} is not compatible with `value`\\'s type: '\n+        f'{type(value)}\\nYou should set `dtype=object` for variable length '\n+        'strings.')\n+\n+  x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n+  for idx, s in enumerate(sequences):\n+    if not len(s):  # pylint: disable=g-explicit-length-test\n+      continue  # empty list/array was found\n+    if truncating == 'pre':\n+      trunc = s[-maxlen:]  # pylint: disable=invalid-unary-operand-type\n+    elif truncating == 'post':\n+      trunc = s[:maxlen]\n+    else:\n+      raise ValueError(f'Truncating type \"{truncating}\" not understood')\n+\n+    # check `trunc` has expected shape\n+    trunc = np.asarray(trunc, dtype=dtype)\n+    if trunc.shape[1:] != sample_shape:\n+      raise ValueError(f'Shape of sample {trunc.shape[1:]} of sequence at '\n+                       f'position {idx} is different from expected shape '\n+                       f'{sample_shape}')\n+\n+    if padding == 'post':\n+      x[idx, :len(trunc)] = trunc\n+    elif padding == 'pre':\n+      x[idx, -len(trunc):] = trunc\n+    else:\n+      raise ValueError(f'Padding type \"{padding}\" not understood')\n+  return x\n\n@@ -345,6 +345,78 @@ class TestEnqueuers(tf.test.TestCase):\n     enqueuer.stop()\n \n \n+class PadSequencesTest(tf.test.TestCase):\n+\n+  def test_pad_sequences(self):\n+    a = [[1], [1, 2], [1, 2, 3]]\n+\n+    # test padding\n+    b = data_utils.pad_sequences(a, maxlen=3, padding='pre')\n+    self.assertAllClose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n+    b = data_utils.pad_sequences(a, maxlen=3, padding='post')\n+    self.assertAllClose(b, [[1, 0, 0], [1, 2, 0], [1, 2, 3]])\n+\n+    # test truncating\n+    b = data_utils.pad_sequences(a, maxlen=2, truncating='pre')\n+    self.assertAllClose(b, [[0, 1], [1, 2], [2, 3]])\n+    b = data_utils.pad_sequences(a, maxlen=2, truncating='post')\n+    self.assertAllClose(b, [[0, 1], [1, 2], [1, 2]])\n+\n+    # test value\n+    b = data_utils.pad_sequences(a, maxlen=3, value=1)\n+    self.assertAllClose(b, [[1, 1, 1], [1, 1, 2], [1, 2, 3]])\n+\n+  def test_pad_sequences_str(self):\n+    a = [['1'], ['1', '2'], ['1', '2', '3']]\n+\n+    # test padding\n+    b = data_utils.pad_sequences(\n+        a, maxlen=3, padding='pre', value='pad', dtype=object)\n+    self.assertAllEqual(\n+        b, [['pad', 'pad', '1'], ['pad', '1', '2'], ['1', '2', '3']])\n+    b = data_utils.pad_sequences(\n+        a, maxlen=3, padding='post', value='pad', dtype='<U3')\n+    self.assertAllEqual(\n+        b, [['1', 'pad', 'pad'], ['1', '2', 'pad'], ['1', '2', '3']])\n+\n+    # test truncating\n+    b = data_utils.pad_sequences(\n+        a, maxlen=2, truncating='pre', value='pad', dtype=object)\n+    self.assertAllEqual(b, [['pad', '1'], ['1', '2'], ['2', '3']])\n+    b = data_utils.pad_sequences(\n+        a, maxlen=2, truncating='post', value='pad', dtype='<U3')\n+    self.assertAllEqual(b, [['pad', '1'], ['1', '2'], ['1', '2']])\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                '`dtype` int32 is not compatible with '):\n+      data_utils.pad_sequences(a, maxlen=2, truncating='post', value='pad')\n+\n+  def test_pad_sequences_vector(self):\n+    a = [[[1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2], [3, 3]]]\n+\n+    # test padding\n+    b = data_utils.pad_sequences(a, maxlen=3, padding='pre')\n+    self.assertAllClose(b, [[[0, 0], [0, 0], [1, 1]], [[0, 0], [2, 1], [2, 2]],\n+                            [[3, 1], [3, 2], [3, 3]]])\n+    b = data_utils.pad_sequences(a, maxlen=3, padding='post')\n+    self.assertAllClose(b, [[[1, 1], [0, 0], [0, 0]], [[2, 1], [2, 2], [0, 0]],\n+                            [[3, 1], [3, 2], [3, 3]]])\n+\n+    # test truncating\n+    b = data_utils.pad_sequences(a, maxlen=2, truncating='pre')\n+    self.assertAllClose(b,\n+                        [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 2], [3, 3]]])\n+\n+    b = data_utils.pad_sequences(a, maxlen=2, truncating='post')\n+    self.assertAllClose(b,\n+                        [[[0, 0], [1, 1]], [[2, 1], [2, 2]], [[3, 1], [3, 2]]])\n+\n+    # test value\n+    b = data_utils.pad_sequences(a, maxlen=3, value=1)\n+    self.assertAllClose(b, [[[1, 1], [1, 1], [1, 1]], [[1, 1], [2, 1], [2, 2]],\n+                            [[3, 1], [3, 2], [3, 3]]])\n+\n+\n if __name__ == '__main__':\n   # Bazel sets these environment variables to very long paths.\n   # Tempfile uses them to create long paths, and in turn multiprocessing\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b77f9c1ae725f11299f21072e4185331da89f5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 55 | Churn Cumulative: 3892 | Contributors (this commit): 8 | Commits (past 90d): 19 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1445,7 +1445,7 @@ class RandomContrast(base_layer.BaseRandomLayer):\n \n \n @keras_export('keras.layers.RandomBrightness', v1=[])\n-class RandomBrightness(base_layer.BaseRandomLayer):\n+class RandomBrightness(BaseImageAugmentationLayer):\n   \"\"\"A preprocessing layer which randomly adjusts brightness during training.\n \n   This layer will randomly increase/reduce the brightness for the input RGB\n@@ -1515,6 +1515,23 @@ class RandomBrightness(base_layer.BaseRandomLayer):\n     self._set_value_range(value_range)\n     self._seed = seed\n \n+  def augment_image(self, image, transformation=None):\n+    return self._brightness_adjust(image, transformation['rgb_delta'])\n+\n+  def get_random_transformation(self,\n+                                image=None,\n+                                label=None,\n+                                bounding_box=None):\n+    rgb_delta_shape = (1, 1, 1)\n+    random_rgb_delta = self._random_generator.random_uniform(\n+        shape=rgb_delta_shape,\n+        minval=self._factor[0],\n+        maxval=self._factor[1],\n+    )\n+    random_rgb_delta = random_rgb_delta * (\n+        self._value_range[1] - self._value_range[0])\n+    return {'rgb_delta': random_rgb_delta}\n+\n   def _set_value_range(self, value_range):\n     if not isinstance(value_range, (tuple, list)):\n       raise ValueError(\n@@ -1542,35 +1559,17 @@ class RandomBrightness(base_layer.BaseRandomLayer):\n     if input_number > 1.0 or input_number < -1.0:\n       raise ValueError(self._FACTOR_VALIDATION_ERROR + f'Got {input_number}')\n \n-  def call(self, inputs, training=True):\n-    if training:\n-      return self._brightness_adjust(inputs)\n-    else:\n-      return inputs\n-\n-  def _brightness_adjust(self, images):\n-    images = utils.ensure_tensor(images, self.compute_dtype)\n-    rank = images.shape.rank\n-    if rank == 3:\n-      rgb_delta_shape = (1, 1, 1)\n-    elif rank == 4:\n-      # Keep only the batch dim. This will ensure to have same adjustment\n-      # with in one image, but different across the images.\n-      rgb_delta_shape = [tf.shape(images)[0], 1, 1, 1]\n-    else:\n+  def _brightness_adjust(self, image, rgb_delta):\n+    image = utils.ensure_tensor(image, self.compute_dtype)\n+    rank = image.shape.rank\n+    if rank != 3:\n       raise ValueError(\n-          'Expected the input image to be rank 3 or 4. Got '\n-          f'inputs.shape = {images.shape}')\n-    rgb_delta = self._random_generator.random_uniform(\n-        shape=rgb_delta_shape,\n-        minval=self._factor[0],\n-        maxval=self._factor[1],\n-    )\n-    rgb_delta = rgb_delta * (self._value_range[1] - self._value_range[0])\n-    rgb_delta = tf.cast(rgb_delta, images.dtype)\n-    images += rgb_delta\n+          'Expected the input image to be rank 3. Got '\n+          f'inputs.shape = {image.shape}')\n+    rgb_delta = tf.cast(rgb_delta, image.dtype)\n+    image += rgb_delta\n     return tf.clip_by_value(\n-        images, self._value_range[0], self._value_range[1])\n+        image, self._value_range[0], self._value_range[1])\n \n   def get_config(self):\n     config = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f0f0adaabf1bc83edf06359482b169e6c5f4455", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1023 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -809,8 +809,8 @@ class TrackableWeightHandler:\n     self._trackable = trackable\n     self._distribute_strategy = tf.distribute.get_strategy()\n \n-    # TODO(b/141682913): Figure out why this is private and fix it.\n-    saveables = trackable._gather_saveables_for_checkpoint().values()  # pylint: disable=protected-access\n+    saveables = tf.__internal__.tracking.saveable_objects_from_trackable(\n+        trackable).values()\n     # 'Saveables' won't exist when we're passed a legacy TF1 table like\n     # a StaticHashTable.\n     if not saveables:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b27ac0aeedb2f0b4b48aad38094ecbab5b3ddcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 689 | Lines Deleted: 717 | Files Changed: 7 | Hunks: 43 | Methods Changed: 27 | Complexity Δ (Sum/Max): -3/42 | Churn Δ: 1406 | Churn Cumulative: 16060 | Contributors (this commit): 74 | Commits (past 90d): 23 | Contributors (cumulative): 93 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,13 +31,13 @@ from keras.applications import mobilenet_v3\n from keras.applications import nasnet\n from keras.applications import regnet\n from keras.applications import resnet\n+from keras.applications import resnet_rs\n from keras.applications import resnet_v2\n from keras.applications import vgg16\n from keras.applications import vgg19\n from keras.applications import xception\n-from keras.applications import resnet_rs\n-from keras.preprocessing import image\n from keras.utils import data_utils\n+from keras.utils import image_utils\n \n \n ARG_TO_MODEL = {\n@@ -110,8 +110,8 @@ def _get_elephant(target_size):\n   if target_size[0] is None:\n     target_size = (299, 299)\n   test_image = data_utils.get_file('elephant.jpg', TEST_IMAGE_PATH)\n-  img = image.load_img(test_image, target_size=tuple(target_size))\n-  x = image.img_to_array(img)\n+  img = image_utils.load_img(test_image, target_size=tuple(target_size))\n+  x = image_utils.img_to_array(img)\n   return np.expand_dims(x, axis=0)\n \n \n\n@@ -30,15 +30,14 @@ https://www.tensorflow.org/guide/keras/preprocessing_layers).\n \"\"\"\n \n import collections\n-import io\n import multiprocessing\n import os\n-import pathlib\n import threading\n import warnings\n \n from keras import backend\n from keras.utils import data_utils\n+from keras.utils import image_utils\n import numpy as np\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -49,288 +48,11 @@ try:\n except ImportError:\n   pass\n try:\n-  from PIL import Image as pil_image\n   from PIL import ImageEnhance\n except ImportError:\n-  pil_image = None\n   ImageEnhance = None\n \n \n-if pil_image is not None:\n-  _PIL_INTERPOLATION_METHODS = {\n-      'nearest': pil_image.NEAREST,\n-      'bilinear': pil_image.BILINEAR,\n-      'bicubic': pil_image.BICUBIC,\n-      'hamming': pil_image.HAMMING,\n-      'box': pil_image.BOX,\n-      'lanczos': pil_image.LANCZOS,\n-  }\n-\n-\n-@keras_export('keras.utils.array_to_img',\n-              'keras.preprocessing.image.array_to_img')\n-def array_to_img(x, data_format=None, scale=True, dtype=None):\n-  \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n-\n-  Usage:\n-\n-  ```python\n-  from PIL import Image\n-  img = np.random.random(size=(100, 100, 3))\n-  pil_img = tf.keras.preprocessing.image.array_to_img(img)\n-  ```\n-\n-\n-  Args:\n-      x: Input data, in any form that can be converted to a Numpy array.\n-      data_format: Image data format, can be either \"channels_first\" or\n-        \"channels_last\". Defaults to `None`, in which case the global setting\n-        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n-        it defaults to \"channels_last\").\n-      scale: Whether to rescale the image such that minimum and maximum values\n-        are 0 and 255 respectively. Defaults to `True`.\n-      dtype: Dtype to use. Default to `None`, in which case the global setting\n-        `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n-        to \"float32\")\n-\n-  Returns:\n-      A PIL Image instance.\n-\n-  Raises:\n-      ImportError: if PIL is not available.\n-      ValueError: if invalid `x` or `data_format` is passed.\n-  \"\"\"\n-\n-  if data_format is None:\n-    data_format = backend.image_data_format()\n-  if dtype is None:\n-    dtype = backend.floatx()\n-  if pil_image is None:\n-    raise ImportError('Could not import PIL.Image. '\n-                      'The use of `array_to_img` requires PIL.')\n-  x = np.asarray(x, dtype=dtype)\n-  if x.ndim != 3:\n-    raise ValueError('Expected image array to have rank 3 (single image). '\n-                     f'Got array with shape: {x.shape}')\n-\n-  if data_format not in {'channels_first', 'channels_last'}:\n-    raise ValueError(f'Invalid data_format: {data_format}')\n-\n-  # Original Numpy array x has format (height, width, channel)\n-  # or (channel, height, width)\n-  # but target PIL image has format (width, height, channel)\n-  if data_format == 'channels_first':\n-    x = x.transpose(1, 2, 0)\n-  if scale:\n-    x = x - np.min(x)\n-    x_max = np.max(x)\n-    if x_max != 0:\n-      x /= x_max\n-    x *= 255\n-  if x.shape[2] == 4:\n-    # RGBA\n-    return pil_image.fromarray(x.astype('uint8'), 'RGBA')\n-  elif x.shape[2] == 3:\n-    # RGB\n-    return pil_image.fromarray(x.astype('uint8'), 'RGB')\n-  elif x.shape[2] == 1:\n-    # grayscale\n-    if np.max(x) > 255:\n-      # 32-bit signed integer grayscale image. PIL mode \"I\"\n-      return pil_image.fromarray(x[:, :, 0].astype('int32'), 'I')\n-    return pil_image.fromarray(x[:, :, 0].astype('uint8'), 'L')\n-  else:\n-    raise ValueError(f'Unsupported channel number: {x.shape[2]}')\n-\n-\n-@keras_export('keras.utils.img_to_array',\n-              'keras.preprocessing.image.img_to_array')\n-def img_to_array(img, data_format=None, dtype=None):\n-  \"\"\"Converts a PIL Image instance to a Numpy array.\n-\n-  Usage:\n-\n-  ```python\n-  from PIL import Image\n-  img_data = np.random.random(size=(100, 100, 3))\n-  img = tf.keras.preprocessing.image.array_to_img(img_data)\n-  array = tf.keras.preprocessing.image.img_to_array(img)\n-  ```\n-\n-\n-  Args:\n-      img: Input PIL Image instance.\n-      data_format: Image data format, can be either \"channels_first\" or\n-        \"channels_last\". Defaults to `None`, in which case the global setting\n-        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n-        it defaults to \"channels_last\").\n-      dtype: Dtype to use. Default to `None`, in which case the global setting\n-        `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n-        to \"float32\")\n-\n-  Returns:\n-      A 3D Numpy array.\n-\n-  Raises:\n-      ValueError: if invalid `img` or `data_format` is passed.\n-  \"\"\"\n-\n-  if data_format is None:\n-    data_format = backend.image_data_format()\n-  if dtype is None:\n-    dtype = backend.floatx()\n-  if data_format not in {'channels_first', 'channels_last'}:\n-    raise ValueError(f'Unknown data_format: {data_format}')\n-  # Numpy array x has format (height, width, channel)\n-  # or (channel, height, width)\n-  # but original PIL image has format (width, height, channel)\n-  x = np.asarray(img, dtype=dtype)\n-  if len(x.shape) == 3:\n-    if data_format == 'channels_first':\n-      x = x.transpose(2, 0, 1)\n-  elif len(x.shape) == 2:\n-    if data_format == 'channels_first':\n-      x = x.reshape((1, x.shape[0], x.shape[1]))\n-    else:\n-      x = x.reshape((x.shape[0], x.shape[1], 1))\n-  else:\n-    raise ValueError(f'Unsupported image shape: {x.shape}')\n-  return x\n-\n-\n-@keras_export('keras.utils.save_img', 'keras.preprocessing.image.save_img')\n-def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n-  \"\"\"Saves an image stored as a Numpy array to a path or file object.\n-\n-  Args:\n-      path: Path or file object.\n-      x: Numpy array.\n-      data_format: Image data format, either \"channels_first\" or\n-        \"channels_last\".\n-      file_format: Optional file format override. If omitted, the format to use\n-        is determined from the filename extension. If a file object was used\n-        instead of a filename, this parameter should always be used.\n-      scale: Whether to rescale image values to be within `[0, 255]`.\n-      **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.\n-  \"\"\"\n-  if data_format is None:\n-    data_format = backend.image_data_format()\n-  img = array_to_img(x, data_format=data_format, scale=scale)\n-  if img.mode == 'RGBA' and (file_format == 'jpg' or file_format == 'jpeg'):\n-    warnings.warn('The JPG format does not support '\n-                  'RGBA images, converting to RGB.')\n-    img = img.convert('RGB')\n-  img.save(path, format=file_format, **kwargs)\n-\n-\n-@keras_export('keras.utils.load_img', 'keras.preprocessing.image.load_img')\n-def load_img(path,\n-             grayscale=False,\n-             color_mode='rgb',\n-             target_size=None,\n-             interpolation='nearest',\n-             keep_aspect_ratio=False):\n-  \"\"\"Loads an image into PIL format.\n-\n-  Usage:\n-\n-  ```\n-  image = tf.keras.preprocessing.image.load_img(image_path)\n-  input_arr = tf.keras.preprocessing.image.img_to_array(image)\n-  input_arr = np.array([input_arr])  # Convert single image to a batch.\n-  predictions = model.predict(input_arr)\n-  ```\n-\n-  Args:\n-      path: Path to image file.\n-      grayscale: DEPRECATED use `color_mode=\"grayscale\"`.\n-      color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\". The desired\n-        image format.\n-      target_size: Either `None` (default to original size) or tuple of ints\n-        `(img_height, img_width)`.\n-      interpolation: Interpolation method used to resample the image if the\n-        target size is different from that of the loaded image. Supported\n-        methods are \"nearest\", \"bilinear\", and \"bicubic\". If PIL version 1.1.3\n-        or newer is installed, \"lanczos\" is also supported. If PIL version 3.4.0\n-        or newer is installed, \"box\" and \"hamming\" are also supported. By\n-        default, \"nearest\" is used.\n-      keep_aspect_ratio: Boolean, whether to resize images to a target\n-              size without aspect ratio distortion. The image is cropped in\n-              the center with target aspect ratio before resizing.\n-\n-  Returns:\n-      A PIL Image instance.\n-\n-  Raises:\n-      ImportError: if PIL is not available.\n-      ValueError: if interpolation method is not supported.\n-  \"\"\"\n-  if grayscale:\n-    warnings.warn('grayscale is deprecated. Please use '\n-                  'color_mode = \"grayscale\"')\n-    color_mode = 'grayscale'\n-  if pil_image is None:\n-    raise ImportError('Could not import PIL.Image. '\n-                      'The use of `load_img` requires PIL.')\n-  if isinstance(path, io.BytesIO):\n-    img = pil_image.open(path)\n-  elif isinstance(path, (pathlib.Path, bytes, str)):\n-    if isinstance(path, pathlib.Path):\n-      path = str(path.resolve())\n-    with open(path, 'rb') as f:\n-      img = pil_image.open(io.BytesIO(f.read()))\n-  else:\n-    raise TypeError('path should be path-like or io.BytesIO'\n-                    ', not {}'.format(type(path)))\n-\n-  if color_mode == 'grayscale':\n-    # if image is not already an 8-bit, 16-bit or 32-bit grayscale image\n-    # convert it to an 8-bit grayscale image.\n-    if img.mode not in ('L', 'I;16', 'I'):\n-      img = img.convert('L')\n-  elif color_mode == 'rgba':\n-    if img.mode != 'RGBA':\n-      img = img.convert('RGBA')\n-  elif color_mode == 'rgb':\n-    if img.mode != 'RGB':\n-      img = img.convert('RGB')\n-  else:\n-    raise ValueError('color_mode must be \"grayscale\", \"rgb\", or \"rgba\"')\n-  if target_size is not None:\n-    width_height_tuple = (target_size[1], target_size[0])\n-    if img.size != width_height_tuple:\n-      if interpolation not in _PIL_INTERPOLATION_METHODS:\n-        raise ValueError('Invalid interpolation method {} specified. Supported '\n-                         'methods are {}'.format(\n-                             interpolation,\n-                             ', '.join(_PIL_INTERPOLATION_METHODS.keys())))\n-      resample = _PIL_INTERPOLATION_METHODS[interpolation]\n-\n-      if keep_aspect_ratio:\n-        width, height = img.size\n-        target_width, target_height = width_height_tuple\n-\n-        crop_height = (width * target_height) // target_width\n-        crop_width = (height * target_width) // target_height\n-\n-        # Set back to input height / width\n-        # if crop_height / crop_width is not smaller.\n-        crop_height = min(height, crop_height)\n-        crop_width = min(width, crop_width)\n-\n-        crop_box_hstart = (height - crop_height) // 2\n-        crop_box_wstart = (width - crop_width) // 2\n-        crop_box_wend = crop_box_wstart + crop_width\n-        crop_box_hend = crop_box_hstart + crop_height\n-        crop_box = [\n-            crop_box_wstart, crop_box_hstart, crop_box_wend, crop_box_hend\n-        ]\n-        img = img.resize(width_height_tuple, resample, box=crop_box)\n-      else:\n-        img = img.resize(width_height_tuple, resample)\n-  return img\n-\n-\n @keras_export('keras.preprocessing.image.Iterator')\n class Iterator(data_utils.Sequence):\n   \"\"\"Base class for image data iterators.\n@@ -612,13 +334,13 @@ class BatchFromFilesMixin():\n     # self.filepaths is dynamic, is better to call it once outside the loop\n     filepaths = self.filepaths\n     for i, j in enumerate(index_array):\n-      img = load_img(\n+      img = image_utils.load_img(\n           filepaths[j],\n           color_mode=self.color_mode,\n           target_size=self.target_size,\n           interpolation=self.interpolation,\n           keep_aspect_ratio=self.keep_aspect_ratio)\n-      x = img_to_array(img, data_format=self.data_format)\n+      x = image_utils.img_to_array(img, data_format=self.data_format)\n       # Pillow images should be closed after `load_img`,\n       # but not PIL images.\n       if hasattr(img, 'close'):\n@@ -631,7 +353,7 @@ class BatchFromFilesMixin():\n     # optionally save augmented images to disk for debugging purposes\n     if self.save_to_dir:\n       for i, j in enumerate(index_array):\n-        img = array_to_img(batch_x[i], self.data_format, scale=True)\n+        img = image_utils.array_to_img(batch_x[i], self.data_format, scale=True)\n         fname = '{prefix}_{index}_{hash}.{format}'.format(\n             prefix=self.save_prefix,\n             index=j,\n@@ -991,7 +713,7 @@ class NumpyArrayIterator(Iterator):\n \n     if self.save_to_dir:\n       for i, j in enumerate(index_array):\n-        img = array_to_img(batch_x[i], self.data_format, scale=True)\n+        img = image_utils.array_to_img(batch_x[i], self.data_format, scale=True)\n         fname = '{prefix}_{index}_{hash}.{format}'.format(\n             prefix=self.save_prefix,\n             index=j,\n@@ -2435,10 +2157,10 @@ def apply_brightness_shift(x, brightness, scale=True):\n                       'Install PIL or Pillow.')\n   x_min, x_max = np.min(x), np.max(x)\n   local_scale = (x_min < 0) or (x_max > 255)\n-  x = array_to_img(x, scale=local_scale or scale)\n+  x = image_utils.array_to_img(x, scale=local_scale or scale)\n   x = imgenhancer_Brightness = ImageEnhance.Brightness(x)\n   x = imgenhancer_Brightness.enhance(brightness)\n-  x = img_to_array(x)\n+  x = image_utils.img_to_array(x)\n   if not scale and local_scale:\n     x = x / 255 * (x_max - x_min) + x_min\n   return x\n\n@@ -14,9 +14,7 @@\n # ==============================================================================\n \"\"\"Tests for image preprocessing utils.\"\"\"\n \n-import io\n import os\n-import pathlib\n import random\n import shutil\n import tempfile\n@@ -27,6 +25,7 @@ from keras.engine import sequential\n from keras.preprocessing import image\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n+from keras.utils import image_utils\n import numpy as np\n import pandas as pd\n import tensorflow.compat.v2 as tf\n@@ -97,7 +96,7 @@ class TestImage(test_combinations.TestCase):\n     for test_images in _generate_test_images():\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n \n       images = np.vstack(img_list)\n       generator = image.ImageDataGenerator(\n@@ -226,10 +225,11 @@ class TestImage(test_combinations.TestCase):\n \n     # Test image loading util\n     fname = os.path.join(temp_dir, filenames[0])\n-    _ = image.load_img(fname)\n-    _ = image.load_img(fname, grayscale=True)\n-    _ = image.load_img(fname, target_size=(10, 10))\n-    _ = image.load_img(fname, target_size=(10, 10), interpolation='bilinear')\n+    _ = image_utils.load_img(fname)\n+    _ = image_utils.load_img(fname, grayscale=True)\n+    _ = image_utils.load_img(fname, target_size=(10, 10))\n+    _ = image_utils.load_img(\n+        fname, target_size=(10, 10), interpolation='bilinear')\n \n     # create iterator\n     generator = image.ImageDataGenerator()\n@@ -348,38 +348,6 @@ class TestImage(test_combinations.TestCase):\n   def test_directory_iterator_with_validation_split_50_percent(self):\n     self.directory_iterator_with_validation_split_test_helper(0.50)\n \n-  def test_img_utils(self):\n-    if PIL is None:\n-      return  # Skip test if PIL is not available.\n-\n-    height, width = 10, 8\n-\n-    # Test channels_first data format\n-    x = np.random.random((3, height, width))\n-    img = image.array_to_img(x, data_format='channels_first')\n-    self.assertEqual(img.size, (width, height))\n-    x = image.img_to_array(img, data_format='channels_first')\n-    self.assertEqual(x.shape, (3, height, width))\n-    # Test 2D\n-    x = np.random.random((1, height, width))\n-    img = image.array_to_img(x, data_format='channels_first')\n-    self.assertEqual(img.size, (width, height))\n-    x = image.img_to_array(img, data_format='channels_first')\n-    self.assertEqual(x.shape, (1, height, width))\n-\n-    # Test channels_last data format\n-    x = np.random.random((height, width, 3))\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 3))\n-    # Test 2D\n-    x = np.random.random((height, width, 1))\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 1))\n-\n   def test_batch_standardize(self):\n     if PIL is None:\n       return  # Skip test if PIL is not available.\n@@ -388,7 +356,7 @@ class TestImage(test_combinations.TestCase):\n     for test_images in _generate_test_images():\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n \n       images = np.vstack(img_list)\n       generator = image.ImageDataGenerator(\n@@ -428,382 +396,6 @@ class TestImage(test_combinations.TestCase):\n     _ = image.random_channel_shift(x, 2.)\n \n \n-@test_utils.run_v2_only\n-class TestImageLoading(test_combinations.TestCase):\n-\n-  def test_validate_filename(self):\n-    tmpdir = self.create_tempdir()\n-    valid_extensions = ('png', 'jpg')\n-    filename = tmpdir.create_file('test.png').full_path\n-    self.assertTrue(image.validate_filename(str(filename), valid_extensions))\n-\n-    filename = tmpdir.create_file('test.PnG').full_path\n-    self.assertTrue(image.validate_filename(str(filename), valid_extensions))\n-\n-    filename = tmpdir.create_file('test.some_extension').full_path\n-    self.assertFalse(image.validate_filename(str(filename), valid_extensions))\n-    self.assertFalse(\n-        image.validate_filename('some_test_file.png', valid_extensions))\n-\n-  def test_load_img(self):\n-    tmpdir = self.create_tempdir()\n-    filename_rgb = os.path.join(tmpdir.full_path, 'rgb_utils.png')\n-    filename_rgba = os.path.join(tmpdir.full_path, 'rgba_utils.png')\n-    filename_grayscale_8bit = os.path.join(tmpdir.full_path,\n-                                           'grayscale_8bit_utils.png')\n-    filename_grayscale_16bit = os.path.join(tmpdir.full_path,\n-                                            'grayscale_16bit_utils.tiff')\n-    filename_grayscale_32bit = os.path.join(tmpdir.full_path,\n-                                            'grayscale_32bit_utils.tiff')\n-\n-    original_rgb_array = np.array(\n-        255 * np.random.rand(100, 100, 3), dtype=np.uint8)\n-    original_rgb = image.array_to_img(original_rgb_array, scale=False)\n-    original_rgb.save(filename_rgb)\n-\n-    original_rgba_array = np.array(\n-        255 * np.random.rand(100, 100, 4), dtype=np.uint8)\n-    original_rgba = image.array_to_img(original_rgba_array, scale=False)\n-    original_rgba.save(filename_rgba)\n-\n-    original_grayscale_8bit_array = np.array(\n-        255 * np.random.rand(100, 100, 1), dtype=np.uint8)\n-    original_grayscale_8bit = image.array_to_img(\n-        original_grayscale_8bit_array, scale=False)\n-    original_grayscale_8bit.save(filename_grayscale_8bit)\n-\n-    original_grayscale_16bit_array = np.array(\n-        np.random.randint(-2147483648, 2147483647, (100, 100, 1)),\n-        dtype=np.int16)\n-    original_grayscale_16bit = image.array_to_img(\n-        original_grayscale_16bit_array, scale=False, dtype='int16')\n-    original_grayscale_16bit.save(filename_grayscale_16bit)\n-\n-    original_grayscale_32bit_array = np.array(\n-        np.random.randint(-2147483648, 2147483647, (100, 100, 1)),\n-        dtype=np.int32)\n-    original_grayscale_32bit = image.array_to_img(\n-        original_grayscale_32bit_array, scale=False, dtype='int32')\n-    original_grayscale_32bit.save(filename_grayscale_32bit)\n-\n-    # Test that loaded image is exactly equal to original.\n-\n-    loaded_im = image.load_img(filename_rgb)\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, original_rgb_array.shape)\n-    self.assertAllClose(loaded_im_array, original_rgb_array)\n-\n-    loaded_im = image.load_img(filename_rgba, color_mode='rgba')\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, original_rgba_array.shape)\n-    self.assertAllClose(loaded_im_array, original_rgba_array)\n-\n-    loaded_im = image.load_img(filename_rgb, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(\n-        loaded_im_array.shape,\n-        (original_rgb_array.shape[0], original_rgb_array.shape[1], 1))\n-\n-    loaded_im = image.load_img(filename_grayscale_8bit, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, original_grayscale_8bit_array.shape)\n-    self.assertAllClose(loaded_im_array, original_grayscale_8bit_array)\n-\n-    loaded_im = image.load_img(filename_grayscale_16bit, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int16')\n-    self.assertEqual(loaded_im_array.shape,\n-                     original_grayscale_16bit_array.shape)\n-    self.assertAllClose(loaded_im_array, original_grayscale_16bit_array)\n-    # test casting int16 image to float32\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertAllClose(loaded_im_array, original_grayscale_16bit_array)\n-\n-    loaded_im = image.load_img(filename_grayscale_32bit, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int32')\n-    self.assertEqual(loaded_im_array.shape,\n-                     original_grayscale_32bit_array.shape)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-    # test casting int32 image to float32\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-\n-    # Test that nothing is changed when target size is equal to original.\n-\n-    loaded_im = image.load_img(filename_rgb, target_size=(100, 100))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, original_rgb_array.shape)\n-    self.assertAllClose(loaded_im_array, original_rgb_array)\n-\n-    loaded_im = image.load_img(\n-        filename_rgba, color_mode='rgba', target_size=(100, 100))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, original_rgba_array.shape)\n-    self.assertAllClose(loaded_im_array, original_rgba_array)\n-\n-    loaded_im = image.load_img(\n-        filename_rgb, color_mode='grayscale', target_size=(100, 100))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(\n-        loaded_im_array.shape,\n-        (original_rgba_array.shape[0], original_rgba_array.shape[1], 1))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_8bit, color_mode='grayscale', target_size=(100, 100))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, original_grayscale_8bit_array.shape)\n-    self.assertAllClose(loaded_im_array, original_grayscale_8bit_array)\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_16bit,\n-        color_mode='grayscale',\n-        target_size=(100, 100))\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int16')\n-    self.assertEqual(loaded_im_array.shape,\n-                     original_grayscale_16bit_array.shape)\n-    self.assertAllClose(loaded_im_array, original_grayscale_16bit_array)\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_32bit,\n-        color_mode='grayscale',\n-        target_size=(100, 100))\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int32')\n-    self.assertEqual(loaded_im_array.shape,\n-                     original_grayscale_32bit_array.shape)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-\n-    # Test down-sampling with bilinear interpolation.\n-\n-    loaded_im = image.load_img(filename_rgb, target_size=(25, 25))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 3))\n-\n-    loaded_im = image.load_img(\n-        filename_rgba, color_mode='rgba', target_size=(25, 25))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 4))\n-\n-    loaded_im = image.load_img(\n-        filename_rgb, color_mode='grayscale', target_size=(25, 25))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_8bit, color_mode='grayscale', target_size=(25, 25))\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_16bit, color_mode='grayscale', target_size=(25, 25))\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int16')\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_32bit, color_mode='grayscale', target_size=(25, 25))\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int32')\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    # Test down-sampling with nearest neighbor interpolation.\n-\n-    loaded_im_nearest = image.load_img(\n-        filename_rgb, target_size=(25, 25), interpolation='nearest')\n-    loaded_im_array_nearest = image.img_to_array(loaded_im_nearest)\n-    self.assertEqual(loaded_im_array_nearest.shape, (25, 25, 3))\n-    self.assertTrue(np.any(loaded_im_array_nearest != loaded_im_array))\n-\n-    loaded_im_nearest = image.load_img(\n-        filename_rgba,\n-        color_mode='rgba',\n-        target_size=(25, 25),\n-        interpolation='nearest')\n-    loaded_im_array_nearest = image.img_to_array(loaded_im_nearest)\n-    self.assertEqual(loaded_im_array_nearest.shape, (25, 25, 4))\n-    self.assertTrue(np.any(loaded_im_array_nearest != loaded_im_array))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_8bit,\n-        color_mode='grayscale',\n-        target_size=(25, 25),\n-        interpolation='nearest')\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_16bit,\n-        color_mode='grayscale',\n-        target_size=(25, 25),\n-        interpolation='nearest')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int16')\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    loaded_im = image.load_img(\n-        filename_grayscale_32bit,\n-        color_mode='grayscale',\n-        target_size=(25, 25),\n-        interpolation='nearest')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype='int32')\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n-\n-    # Test different path type\n-    with open(filename_grayscale_32bit, 'rb') as f:\n-      path_ = io.BytesIO(f.read())  # io.Bytesio\n-    loaded_im = image.load_img(path_, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype=np.int32)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-\n-    path_ = filename_grayscale_32bit  # str\n-    loaded_im = image.load_img(path_, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype=np.int32)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-\n-    path_ = filename_grayscale_32bit.encode()  # bytes\n-    loaded_im = image.load_img(path_, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype=np.int32)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-\n-    path_ = pathlib.Path(\n-        os.path.join(tmpdir.full_path, 'grayscale_32bit_utils.tiff'))\n-    loaded_im = image.load_img(path_, color_mode='grayscale')\n-    loaded_im_array = image.img_to_array(loaded_im, dtype=np.int32)\n-    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n-\n-    # Check that exception is raised if interpolation not supported.\n-\n-    loaded_im = image.load_img(filename_rgb, interpolation='unsupported')\n-    with self.assertRaises(ValueError):\n-      loaded_im = image.load_img(\n-          filename_rgb, target_size=(25, 25), interpolation='unsupported')\n-\n-    # Check that the aspect ratio of a square is the same\n-\n-    filename_red_square = os.path.join(tmpdir.full_path, 'red_square_utils.png')\n-    arr = np.zeros((50, 100, 3), dtype=np.uint8)  # rectangle image 100x50\n-    arr[20:30, 45:55, 0] = 255  # red square 10x10\n-    red_square_array = np.array(arr)\n-    red_square = image.array_to_img(red_square_array, scale=False)\n-    red_square.save(filename_red_square)\n-\n-    loaded_im = image.load_img(\n-        filename_red_square, target_size=(25, 25), keep_aspect_ratio=True)\n-    loaded_im_array = image.img_to_array(loaded_im)\n-    self.assertEqual(loaded_im_array.shape, (25, 25, 3))\n-\n-    red_channel_arr = loaded_im_array[:, :, 0].astype(np.bool)\n-    square_width = np.sum(np.sum(red_channel_arr, axis=0))\n-    square_height = np.sum(np.sum(red_channel_arr, axis=1))\n-    aspect_ratio_result = square_width / square_height\n-\n-    # original square had 1:1 ratio\n-    self.assertNear(aspect_ratio_result, 1.0, 0.01)\n-\n-  def test_array_to_img_and_img_to_array(self):\n-    height, width = 10, 8\n-\n-    # Test the data format\n-    # Test RGB 3D\n-    x = np.random.random((3, height, width))\n-    img = image.array_to_img(x, data_format='channels_first')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_first')\n-    self.assertEqual(x.shape, (3, height, width))\n-\n-    # Test RGBA 3D\n-    x = np.random.random((4, height, width))\n-    img = image.array_to_img(x, data_format='channels_first')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_first')\n-    self.assertEqual(x.shape, (4, height, width))\n-\n-    # Test 2D\n-    x = np.random.random((1, height, width))\n-    img = image.array_to_img(x, data_format='channels_first')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_first')\n-    self.assertEqual(x.shape, (1, height, width))\n-\n-    # grayscale 32-bit signed integer\n-    x = np.array(\n-        np.random.randint(-2147483648, 2147483647, (1, height, width)),\n-        dtype=np.int32)\n-    img = image.array_to_img(x, data_format='channels_first')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_first')\n-    self.assertEqual(x.shape, (1, height, width))\n-\n-    # Test tf data format\n-    # Test RGB 3D\n-    x = np.random.random((height, width, 3))\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 3))\n-\n-    # Test RGBA 3D\n-    x = np.random.random((height, width, 4))\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 4))\n-\n-    # Test 2D\n-    x = np.random.random((height, width, 1))\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 1))\n-\n-    # grayscale 16-bit signed integer\n-    x = np.array(\n-        np.random.randint(-2147483648, 2147483647, (height, width, 1)),\n-        dtype=np.int16)\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 1))\n-\n-    # grayscale 32-bit signed integer\n-    x = np.array(\n-        np.random.randint(-2147483648, 2147483647, (height, width, 1)),\n-        dtype=np.int32)\n-    img = image.array_to_img(x, data_format='channels_last')\n-    self.assertEqual(img.size, (width, height))\n-\n-    x = image.img_to_array(img, data_format='channels_last')\n-    self.assertEqual(x.shape, (height, width, 1))\n-\n-    # Test invalid use case\n-    with self.assertRaises(ValueError):\n-      x = np.random.random((height, width))  # not 3D\n-      img = image.array_to_img(x, data_format='channels_first')\n-\n-    with self.assertRaises(ValueError):\n-      x = np.random.random((height, width, 3))\n-      # unknown data_format\n-      img = image.array_to_img(x, data_format='channels')\n-\n-    with self.assertRaises(ValueError):\n-      # neither RGB, RGBA, or gray-scale\n-      x = np.random.random((height, width, 5))\n-      img = image.array_to_img(x, data_format='channels_last')\n-\n-    with self.assertRaises(ValueError):\n-      x = np.random.random((height, width, 3))\n-      # unknown data_format\n-      img = image.img_to_array(x, data_format='channels')\n-\n-    with self.assertRaises(ValueError):\n-      # neither RGB, RGBA, or gray-scale\n-      x = np.random.random((height, width, 5, 3))\n-      img = image.img_to_array(x, data_format='channels_last')\n-\n-\n @test_utils.run_v2_only\n class TestDirectoryIterator(test_combinations.TestCase):\n \n@@ -1002,7 +594,7 @@ class TestNumpyArrayIterator(test_combinations.TestCase):\n     for test_images in all_test_images:\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n       images = np.vstack(img_list)\n       dsize = images.shape[0]\n \n@@ -1864,7 +1456,7 @@ class TestImageDataGenerator(test_combinations.TestCase):\n     for test_images in all_test_images:\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n \n       image.ImageDataGenerator(\n           featurewise_center=True,\n@@ -1890,7 +1482,7 @@ class TestImageDataGenerator(test_combinations.TestCase):\n     for test_images in all_test_images:\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n \n       images = np.vstack(img_list)\n       labels = np.concatenate(\n@@ -2031,7 +1623,7 @@ class TestImageDataGenerator(test_combinations.TestCase):\n     for test_images in all_test_images:\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n \n       images = np.vstack(img_list)\n       dsize = images.shape[0]\n@@ -2154,7 +1746,7 @@ class TestImageDataGenerator(test_combinations.TestCase):\n     for test_images in all_test_images:\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n \n       images = np.vstack(img_list)\n       generator = image.ImageDataGenerator(\n@@ -2250,7 +1842,7 @@ class TestImageDataGenerator(test_combinations.TestCase):\n     for test_images in all_test_images:\n       img_list = []\n       for im in test_images:\n-        img_list.append(image.img_to_array(im)[None, ...])\n+        img_list.append(image_utils.img_to_array(im)[None, ...])\n       images = np.vstack(img_list)\n \n       # featurewise_center test\n\n@@ -26,6 +26,12 @@ from keras.utils.vis_utils import plot_model\n from keras.utils.np_utils import normalize\n from keras.utils.np_utils import to_categorical\n \n+# Image related\n+from keras.utils.image_utils import array_to_img\n+from keras.utils.image_utils import img_to_array\n+from keras.utils.image_utils import load_img\n+from keras.utils.image_utils import save_img\n+\n # Sequence related\n from keras.utils.data_utils import Sequence\n from keras.utils.data_utils import GeneratorEnqueuer\n\n@@ -22,8 +22,8 @@ import shutil\n import numpy as np\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n-from keras.preprocessing import image as image_preproc\n from keras.utils import image_dataset\n+from keras.utils import image_utils\n \n try:\n   import PIL  # pylint:disable=g-import-not-at-top\n@@ -44,7 +44,7 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n         img = np.random.randint(0, 256, size=(height, width, 4))\n       else:\n         img = np.random.randint(0, 256, size=(height, width, 3))\n-      img = image_preproc.array_to_img(img)\n+      img = image_utils.array_to_img(img)\n       imgs.append(img)\n     return imgs\n \n\n@@ -14,15 +14,36 @@\n # ==============================================================================\n \"\"\"Utilities related to image handling.\"\"\"\n # pylint: disable=g-direct-tensorflow-import\n+# pylint: disable=g-import-not-at-top\n \n+import io\n+import pathlib\n+import warnings\n+\n+from keras import backend\n import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n+try:\n+  from PIL import Image as pil_image\n+except ImportError:\n+  pil_image = None\n+\n+\n+if pil_image is not None:\n+  _PIL_INTERPOLATION_METHODS = {\n+      'nearest': pil_image.NEAREST,\n+      'bilinear': pil_image.BILINEAR,\n+      'bicubic': pil_image.BICUBIC,\n+      'hamming': pil_image.HAMMING,\n+      'box': pil_image.BOX,\n+      'lanczos': pil_image.LANCZOS,\n+  }\n \n ResizeMethod = tf.image.ResizeMethod\n \n-_RESIZE_METHODS = {\n+_TF_INTERPOLATION_METHODS = {\n     'bilinear': ResizeMethod.BILINEAR,\n     'nearest': ResizeMethod.NEAREST_NEIGHBOR,\n     'bicubic': ResizeMethod.BICUBIC,\n@@ -152,10 +173,272 @@ def smart_resize(x, size, interpolation='bilinear'):\n \n def get_interpolation(interpolation):\n   interpolation = interpolation.lower()\n-  if interpolation not in _RESIZE_METHODS:\n+  if interpolation not in _TF_INTERPOLATION_METHODS:\n     raise NotImplementedError(\n         'Value not recognized for `interpolation`: {}. Supported values '\n-        'are: {}'.format(interpolation, _RESIZE_METHODS.keys()))\n-  return _RESIZE_METHODS[interpolation]\n+        'are: {}'.format(interpolation, _TF_INTERPOLATION_METHODS.keys()))\n+  return _TF_INTERPOLATION_METHODS[interpolation]\n \n \n+@keras_export('keras.utils.array_to_img',\n+              'keras.preprocessing.image.array_to_img')\n+def array_to_img(x, data_format=None, scale=True, dtype=None):\n+  \"\"\"Converts a 3D Numpy array to a PIL Image instance.\n+\n+  Usage:\n+\n+  ```python\n+  from PIL import Image\n+  img = np.random.random(size=(100, 100, 3))\n+  pil_img = tf.keras.preprocessing.image.array_to_img(img)\n+  ```\n+\n+\n+  Args:\n+      x: Input data, in any form that can be converted to a Numpy array.\n+      data_format: Image data format, can be either `\"channels_first\"` or\n+        `\"channels_last\"`. Defaults to `None`, in which case the global setting\n+        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+        it defaults to `\"channels_last\"`).\n+      scale: Whether to rescale the image such that minimum and maximum values\n+        are 0 and 255 respectively. Defaults to `True`.\n+      dtype: Dtype to use. Default to `None`, in which case the global setting\n+        `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n+        to `\"float32\"`)\n+\n+  Returns:\n+      A PIL Image instance.\n+\n+  Raises:\n+      ImportError: if PIL is not available.\n+      ValueError: if invalid `x` or `data_format` is passed.\n+  \"\"\"\n+\n+  if data_format is None:\n+    data_format = backend.image_data_format()\n+  if dtype is None:\n+    dtype = backend.floatx()\n+  if pil_image is None:\n+    raise ImportError('Could not import PIL.Image. '\n+                      'The use of `array_to_img` requires PIL.')\n+  x = np.asarray(x, dtype=dtype)\n+  if x.ndim != 3:\n+    raise ValueError('Expected image array to have rank 3 (single image). '\n+                     f'Got array with shape: {x.shape}')\n+\n+  if data_format not in {'channels_first', 'channels_last'}:\n+    raise ValueError(f'Invalid data_format: {data_format}')\n+\n+  # Original Numpy array x has format (height, width, channel)\n+  # or (channel, height, width)\n+  # but target PIL image has format (width, height, channel)\n+  if data_format == 'channels_first':\n+    x = x.transpose(1, 2, 0)\n+  if scale:\n+    x = x - np.min(x)\n+    x_max = np.max(x)\n+    if x_max != 0:\n+      x /= x_max\n+    x *= 255\n+  if x.shape[2] == 4:\n+    # RGBA\n+    return pil_image.fromarray(x.astype('uint8'), 'RGBA')\n+  elif x.shape[2] == 3:\n+    # RGB\n+    return pil_image.fromarray(x.astype('uint8'), 'RGB')\n+  elif x.shape[2] == 1:\n+    # grayscale\n+    if np.max(x) > 255:\n+      # 32-bit signed integer grayscale image. PIL mode \"I\"\n+      return pil_image.fromarray(x[:, :, 0].astype('int32'), 'I')\n+    return pil_image.fromarray(x[:, :, 0].astype('uint8'), 'L')\n+  else:\n+    raise ValueError(f'Unsupported channel number: {x.shape[2]}')\n+\n+\n+@keras_export('keras.utils.img_to_array',\n+              'keras.preprocessing.image.img_to_array')\n+def img_to_array(img, data_format=None, dtype=None):\n+  \"\"\"Converts a PIL Image instance to a Numpy array.\n+\n+  Usage:\n+\n+  ```python\n+  from PIL import Image\n+  img_data = np.random.random(size=(100, 100, 3))\n+  img = tf.keras.preprocessing.image.array_to_img(img_data)\n+  array = tf.keras.preprocessing.image.img_to_array(img)\n+  ```\n+\n+\n+  Args:\n+      img: Input PIL Image instance.\n+      data_format: Image data format, can be either `\"channels_first\"` or\n+        `\"channels_last\"`. Defaults to `None`, in which case the global setting\n+        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+        it defaults to `\"channels_last\"`).\n+      dtype: Dtype to use. Default to `None`, in which case the global setting\n+        `tf.keras.backend.floatx()` is used (unless you changed it, it defaults\n+        to `\"float32\"`).\n+\n+  Returns:\n+      A 3D Numpy array.\n+\n+  Raises:\n+      ValueError: if invalid `img` or `data_format` is passed.\n+  \"\"\"\n+\n+  if data_format is None:\n+    data_format = backend.image_data_format()\n+  if dtype is None:\n+    dtype = backend.floatx()\n+  if data_format not in {'channels_first', 'channels_last'}:\n+    raise ValueError(f'Unknown data_format: {data_format}')\n+  # Numpy array x has format (height, width, channel)\n+  # or (channel, height, width)\n+  # but original PIL image has format (width, height, channel)\n+  x = np.asarray(img, dtype=dtype)\n+  if len(x.shape) == 3:\n+    if data_format == 'channels_first':\n+      x = x.transpose(2, 0, 1)\n+  elif len(x.shape) == 2:\n+    if data_format == 'channels_first':\n+      x = x.reshape((1, x.shape[0], x.shape[1]))\n+    else:\n+      x = x.reshape((x.shape[0], x.shape[1], 1))\n+  else:\n+    raise ValueError(f'Unsupported image shape: {x.shape}')\n+  return x\n+\n+\n+@keras_export('keras.utils.save_img', 'keras.preprocessing.image.save_img')\n+def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n+  \"\"\"Saves an image stored as a Numpy array to a path or file object.\n+\n+  Args:\n+      path: Path or file object.\n+      x: Numpy array.\n+      data_format: Image data format, either `\"channels_first\"` or\n+        `\"channels_last\"`.\n+      file_format: Optional file format override. If omitted, the format to use\n+        is determined from the filename extension. If a file object was used\n+        instead of a filename, this parameter should always be used.\n+      scale: Whether to rescale image values to be within `[0, 255]`.\n+      **kwargs: Additional keyword arguments passed to `PIL.Image.save()`.\n+  \"\"\"\n+  if data_format is None:\n+    data_format = backend.image_data_format()\n+  img = array_to_img(x, data_format=data_format, scale=scale)\n+  if img.mode == 'RGBA' and (file_format == 'jpg' or file_format == 'jpeg'):\n+    warnings.warn('The JPG format does not support '\n+                  'RGBA images, converting to RGB.')\n+    img = img.convert('RGB')\n+  img.save(path, format=file_format, **kwargs)\n+\n+\n+@keras_export('keras.utils.load_img', 'keras.preprocessing.image.load_img')\n+def load_img(path,\n+             grayscale=False,\n+             color_mode='rgb',\n+             target_size=None,\n+             interpolation='nearest',\n+             keep_aspect_ratio=False):\n+  \"\"\"Loads an image into PIL format.\n+\n+  Usage:\n+\n+  ```\n+  image = tf.keras.preprocessing.image.load_img(image_path)\n+  input_arr = tf.keras.preprocessing.image.img_to_array(image)\n+  input_arr = np.array([input_arr])  # Convert single image to a batch.\n+  predictions = model.predict(input_arr)\n+  ```\n+\n+  Args:\n+      path: Path to image file.\n+      grayscale: DEPRECATED use `color_mode=\"grayscale\"`.\n+      color_mode: One of `\"grayscale\"`, `\"rgb\"`, `\"rgba\"`. Default: `\"rgb\"`.\n+        The desired image format.\n+      target_size: Either `None` (default to original size) or tuple of ints\n+        `(img_height, img_width)`.\n+      interpolation: Interpolation method used to resample the image if the\n+        target size is different from that of the loaded image. Supported\n+        methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`. If PIL version\n+        1.1.3 or newer is installed, `\"lanczos\"` is also supported. If PIL\n+        version 3.4.0 or newer is installed, `\"box\"` and `\"hamming\"` are also\n+        supported. By default, `\"nearest\"` is used.\n+      keep_aspect_ratio: Boolean, whether to resize images to a target\n+              size without aspect ratio distortion. The image is cropped in\n+              the center with target aspect ratio before resizing.\n+\n+  Returns:\n+      A PIL Image instance.\n+\n+  Raises:\n+      ImportError: if PIL is not available.\n+      ValueError: if interpolation method is not supported.\n+  \"\"\"\n+  if grayscale:\n+    warnings.warn('grayscale is deprecated. Please use '\n+                  'color_mode = \"grayscale\"')\n+    color_mode = 'grayscale'\n+  if pil_image is None:\n+    raise ImportError('Could not import PIL.Image. '\n+                      'The use of `load_img` requires PIL.')\n+  if isinstance(path, io.BytesIO):\n+    img = pil_image.open(path)\n+  elif isinstance(path, (pathlib.Path, bytes, str)):\n+    if isinstance(path, pathlib.Path):\n+      path = str(path.resolve())\n+    with open(path, 'rb') as f:\n+      img = pil_image.open(io.BytesIO(f.read()))\n+  else:\n+    raise TypeError('path should be path-like or io.BytesIO'\n+                    ', not {}'.format(type(path)))\n+\n+  if color_mode == 'grayscale':\n+    # if image is not already an 8-bit, 16-bit or 32-bit grayscale image\n+    # convert it to an 8-bit grayscale image.\n+    if img.mode not in ('L', 'I;16', 'I'):\n+      img = img.convert('L')\n+  elif color_mode == 'rgba':\n+    if img.mode != 'RGBA':\n+      img = img.convert('RGBA')\n+  elif color_mode == 'rgb':\n+    if img.mode != 'RGB':\n+      img = img.convert('RGB')\n+  else:\n+    raise ValueError('color_mode must be \"grayscale\", \"rgb\", or \"rgba\"')\n+  if target_size is not None:\n+    width_height_tuple = (target_size[1], target_size[0])\n+    if img.size != width_height_tuple:\n+      if interpolation not in _PIL_INTERPOLATION_METHODS:\n+        raise ValueError('Invalid interpolation method {} specified. Supported '\n+                         'methods are {}'.format(\n+                             interpolation,\n+                             ', '.join(_PIL_INTERPOLATION_METHODS.keys())))\n+      resample = _PIL_INTERPOLATION_METHODS[interpolation]\n+\n+      if keep_aspect_ratio:\n+        width, height = img.size\n+        target_width, target_height = width_height_tuple\n+\n+        crop_height = (width * target_height) // target_width\n+        crop_width = (height * target_width) // target_height\n+\n+        # Set back to input height / width\n+        # if crop_height / crop_width is not smaller.\n+        crop_height = min(height, crop_height)\n+        crop_width = min(width, crop_width)\n+\n+        crop_box_hstart = (height - crop_height) // 2\n+        crop_box_wstart = (width - crop_width) // 2\n+        crop_box_wend = crop_box_wstart + crop_width\n+        crop_box_hend = crop_box_hstart + crop_height\n+        crop_box = [\n+            crop_box_wstart, crop_box_hstart, crop_box_wend, crop_box_hend\n+        ]\n+        img = img.resize(width_height_tuple, resample, box=crop_box)\n+      else:\n+        img = img.resize(width_height_tuple, resample)\n+  return img\n\n@@ -14,6 +14,10 @@\n # ==============================================================================\n \"\"\"Tests for image_utils.\"\"\"\n \n+import io\n+import os\n+import pathlib\n+\n from absl.testing import parameterized\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n@@ -64,5 +68,370 @@ class TestImageUtils(test_combinations.TestCase):\n       image_utils.smart_resize(np.random.random((2, 4, 4, 5, 3)), size=(10, 5))\n \n \n+@test_utils.run_v2_only\n+class TestImageLoading(test_combinations.TestCase):\n+\n+  def test_load_img(self):\n+    tmpdir = self.create_tempdir()\n+    filename_rgb = os.path.join(tmpdir.full_path, 'rgb_utils.png')\n+    filename_rgba = os.path.join(tmpdir.full_path, 'rgba_utils.png')\n+    filename_grayscale_8bit = os.path.join(tmpdir.full_path,\n+                                           'grayscale_8bit_utils.png')\n+    filename_grayscale_16bit = os.path.join(tmpdir.full_path,\n+                                            'grayscale_16bit_utils.tiff')\n+    filename_grayscale_32bit = os.path.join(tmpdir.full_path,\n+                                            'grayscale_32bit_utils.tiff')\n+\n+    original_rgb_array = np.array(\n+        255 * np.random.rand(100, 100, 3), dtype=np.uint8)\n+    original_rgb = image_utils.array_to_img(original_rgb_array, scale=False)\n+    original_rgb.save(filename_rgb)\n+\n+    original_rgba_array = np.array(\n+        255 * np.random.rand(100, 100, 4), dtype=np.uint8)\n+    original_rgba = image_utils.array_to_img(original_rgba_array, scale=False)\n+    original_rgba.save(filename_rgba)\n+\n+    original_grayscale_8bit_array = np.array(\n+        255 * np.random.rand(100, 100, 1), dtype=np.uint8)\n+    original_grayscale_8bit = image_utils.array_to_img(\n+        original_grayscale_8bit_array, scale=False)\n+    original_grayscale_8bit.save(filename_grayscale_8bit)\n+\n+    original_grayscale_16bit_array = np.array(\n+        np.random.randint(-2147483648, 2147483647, (100, 100, 1)),\n+        dtype=np.int16)\n+    original_grayscale_16bit = image_utils.array_to_img(\n+        original_grayscale_16bit_array, scale=False, dtype='int16')\n+    original_grayscale_16bit.save(filename_grayscale_16bit)\n+\n+    original_grayscale_32bit_array = np.array(\n+        np.random.randint(-2147483648, 2147483647, (100, 100, 1)),\n+        dtype=np.int32)\n+    original_grayscale_32bit = image_utils.array_to_img(\n+        original_grayscale_32bit_array, scale=False, dtype='int32')\n+    original_grayscale_32bit.save(filename_grayscale_32bit)\n+\n+    # Test that loaded image is exactly equal to original.\n+\n+    loaded_im = image_utils.load_img(filename_rgb)\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, original_rgb_array.shape)\n+    self.assertAllClose(loaded_im_array, original_rgb_array)\n+\n+    loaded_im = image_utils.load_img(filename_rgba, color_mode='rgba')\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, original_rgba_array.shape)\n+    self.assertAllClose(loaded_im_array, original_rgba_array)\n+\n+    loaded_im = image_utils.load_img(filename_rgb, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(\n+        loaded_im_array.shape,\n+        (original_rgb_array.shape[0], original_rgb_array.shape[1], 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_8bit, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, original_grayscale_8bit_array.shape)\n+    self.assertAllClose(loaded_im_array, original_grayscale_8bit_array)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_16bit, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int16')\n+    self.assertEqual(loaded_im_array.shape,\n+                     original_grayscale_16bit_array.shape)\n+    self.assertAllClose(loaded_im_array, original_grayscale_16bit_array)\n+    # test casting int16 image to float32\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertAllClose(loaded_im_array, original_grayscale_16bit_array)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_32bit, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int32')\n+    self.assertEqual(loaded_im_array.shape,\n+                     original_grayscale_32bit_array.shape)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+    # test casting int32 image to float32\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+\n+    # Test that nothing is changed when target size is equal to original.\n+\n+    loaded_im = image_utils.load_img(filename_rgb, target_size=(100, 100))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, original_rgb_array.shape)\n+    self.assertAllClose(loaded_im_array, original_rgb_array)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_rgba, color_mode='rgba', target_size=(100, 100))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, original_rgba_array.shape)\n+    self.assertAllClose(loaded_im_array, original_rgba_array)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_rgb, color_mode='grayscale', target_size=(100, 100))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(\n+        loaded_im_array.shape,\n+        (original_rgba_array.shape[0], original_rgba_array.shape[1], 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_8bit, color_mode='grayscale', target_size=(100, 100))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, original_grayscale_8bit_array.shape)\n+    self.assertAllClose(loaded_im_array, original_grayscale_8bit_array)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_16bit,\n+        color_mode='grayscale',\n+        target_size=(100, 100))\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int16')\n+    self.assertEqual(loaded_im_array.shape,\n+                     original_grayscale_16bit_array.shape)\n+    self.assertAllClose(loaded_im_array, original_grayscale_16bit_array)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_32bit,\n+        color_mode='grayscale',\n+        target_size=(100, 100))\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int32')\n+    self.assertEqual(loaded_im_array.shape,\n+                     original_grayscale_32bit_array.shape)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+\n+    # Test down-sampling with bilinear interpolation.\n+\n+    loaded_im = image_utils.load_img(filename_rgb, target_size=(25, 25))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 3))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_rgba, color_mode='rgba', target_size=(25, 25))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 4))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_rgb, color_mode='grayscale', target_size=(25, 25))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_8bit, color_mode='grayscale', target_size=(25, 25))\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_16bit, color_mode='grayscale', target_size=(25, 25))\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int16')\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_32bit, color_mode='grayscale', target_size=(25, 25))\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int32')\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    # Test down-sampling with nearest neighbor interpolation.\n+\n+    loaded_im_nearest = image_utils.load_img(\n+        filename_rgb, target_size=(25, 25), interpolation='nearest')\n+    loaded_im_array_nearest = image_utils.img_to_array(loaded_im_nearest)\n+    self.assertEqual(loaded_im_array_nearest.shape, (25, 25, 3))\n+    self.assertTrue(np.any(loaded_im_array_nearest != loaded_im_array))\n+\n+    loaded_im_nearest = image_utils.load_img(\n+        filename_rgba,\n+        color_mode='rgba',\n+        target_size=(25, 25),\n+        interpolation='nearest')\n+    loaded_im_array_nearest = image_utils.img_to_array(loaded_im_nearest)\n+    self.assertEqual(loaded_im_array_nearest.shape, (25, 25, 4))\n+    self.assertTrue(np.any(loaded_im_array_nearest != loaded_im_array))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_8bit,\n+        color_mode='grayscale',\n+        target_size=(25, 25),\n+        interpolation='nearest')\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_16bit,\n+        color_mode='grayscale',\n+        target_size=(25, 25),\n+        interpolation='nearest')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int16')\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    loaded_im = image_utils.load_img(\n+        filename_grayscale_32bit,\n+        color_mode='grayscale',\n+        target_size=(25, 25),\n+        interpolation='nearest')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype='int32')\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 1))\n+\n+    # Test different path type\n+    with open(filename_grayscale_32bit, 'rb') as f:\n+      path_ = io.BytesIO(f.read())  # io.Bytesio\n+    loaded_im = image_utils.load_img(path_, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype=np.int32)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+\n+    path_ = filename_grayscale_32bit  # str\n+    loaded_im = image_utils.load_img(path_, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype=np.int32)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+\n+    path_ = filename_grayscale_32bit.encode()  # bytes\n+    loaded_im = image_utils.load_img(path_, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype=np.int32)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+\n+    path_ = pathlib.Path(\n+        os.path.join(tmpdir.full_path, 'grayscale_32bit_utils.tiff'))\n+    loaded_im = image_utils.load_img(path_, color_mode='grayscale')\n+    loaded_im_array = image_utils.img_to_array(loaded_im, dtype=np.int32)\n+    self.assertAllClose(loaded_im_array, original_grayscale_32bit_array)\n+\n+    # Check that exception is raised if interpolation not supported.\n+\n+    loaded_im = image_utils.load_img(filename_rgb, interpolation='unsupported')\n+    with self.assertRaises(ValueError):\n+      loaded_im = image_utils.load_img(\n+          filename_rgb, target_size=(25, 25), interpolation='unsupported')\n+\n+    # Check that the aspect ratio of a square is the same\n+\n+    filename_red_square = os.path.join(tmpdir.full_path, 'red_square_utils.png')\n+    arr = np.zeros((50, 100, 3), dtype=np.uint8)  # rectangle image 100x50\n+    arr[20:30, 45:55, 0] = 255  # red square 10x10\n+    red_square_array = np.array(arr)\n+    red_square = image_utils.array_to_img(red_square_array, scale=False)\n+    red_square.save(filename_red_square)\n+\n+    loaded_im = image_utils.load_img(\n+        filename_red_square, target_size=(25, 25), keep_aspect_ratio=True)\n+    loaded_im_array = image_utils.img_to_array(loaded_im)\n+    self.assertEqual(loaded_im_array.shape, (25, 25, 3))\n+\n+    red_channel_arr = loaded_im_array[:, :, 0].astype(np.bool)\n+    square_width = np.sum(np.sum(red_channel_arr, axis=0))\n+    square_height = np.sum(np.sum(red_channel_arr, axis=1))\n+    aspect_ratio_result = square_width / square_height\n+\n+    # original square had 1:1 ratio\n+    self.assertNear(aspect_ratio_result, 1.0, 0.01)\n+\n+  def test_array_to_img_and_img_to_array(self):\n+    height, width = 10, 8\n+\n+    # Test the data format\n+    # Test RGB 3D\n+    x = np.random.random((3, height, width))\n+    img = image_utils.array_to_img(x, data_format='channels_first')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_first')\n+    self.assertEqual(x.shape, (3, height, width))\n+\n+    # Test RGBA 3D\n+    x = np.random.random((4, height, width))\n+    img = image_utils.array_to_img(x, data_format='channels_first')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_first')\n+    self.assertEqual(x.shape, (4, height, width))\n+\n+    # Test 2D\n+    x = np.random.random((1, height, width))\n+    img = image_utils.array_to_img(x, data_format='channels_first')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_first')\n+    self.assertEqual(x.shape, (1, height, width))\n+\n+    # grayscale 32-bit signed integer\n+    x = np.array(\n+        np.random.randint(-2147483648, 2147483647, (1, height, width)),\n+        dtype=np.int32)\n+    img = image_utils.array_to_img(x, data_format='channels_first')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_first')\n+    self.assertEqual(x.shape, (1, height, width))\n+\n+    # Test tf data format\n+    # Test RGB 3D\n+    x = np.random.random((height, width, 3))\n+    img = image_utils.array_to_img(x, data_format='channels_last')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_last')\n+    self.assertEqual(x.shape, (height, width, 3))\n+\n+    # Test RGBA 3D\n+    x = np.random.random((height, width, 4))\n+    img = image_utils.array_to_img(x, data_format='channels_last')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_last')\n+    self.assertEqual(x.shape, (height, width, 4))\n+\n+    # Test 2D\n+    x = np.random.random((height, width, 1))\n+    img = image_utils.array_to_img(x, data_format='channels_last')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_last')\n+    self.assertEqual(x.shape, (height, width, 1))\n+\n+    # grayscale 16-bit signed integer\n+    x = np.array(\n+        np.random.randint(-2147483648, 2147483647, (height, width, 1)),\n+        dtype=np.int16)\n+    img = image_utils.array_to_img(x, data_format='channels_last')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_last')\n+    self.assertEqual(x.shape, (height, width, 1))\n+\n+    # grayscale 32-bit signed integer\n+    x = np.array(\n+        np.random.randint(-2147483648, 2147483647, (height, width, 1)),\n+        dtype=np.int32)\n+    img = image_utils.array_to_img(x, data_format='channels_last')\n+    self.assertEqual(img.size, (width, height))\n+\n+    x = image_utils.img_to_array(img, data_format='channels_last')\n+    self.assertEqual(x.shape, (height, width, 1))\n+\n+    # Test invalid use case\n+    with self.assertRaises(ValueError):\n+      x = np.random.random((height, width))  # not 3D\n+      img = image_utils.array_to_img(x, data_format='channels_first')\n+\n+    with self.assertRaises(ValueError):\n+      x = np.random.random((height, width, 3))\n+      # unknown data_format\n+      img = image_utils.array_to_img(x, data_format='channels')\n+\n+    with self.assertRaises(ValueError):\n+      # neither RGB, RGBA, or gray-scale\n+      x = np.random.random((height, width, 5))\n+      img = image_utils.array_to_img(x, data_format='channels_last')\n+\n+    with self.assertRaises(ValueError):\n+      x = np.random.random((height, width, 3))\n+      # unknown data_format\n+      img = image_utils.img_to_array(x, data_format='channels')\n+\n+    with self.assertRaises(ValueError):\n+      # neither RGB, RGBA, or gray-scale\n+      x = np.random.random((height, width, 5, 3))\n+      img = image_utils.img_to_array(x, data_format='channels_last')\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
