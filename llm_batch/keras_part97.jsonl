{"custom_id": "keras#edbec2dbc93cbfc020a6bf8a99cccfd7e67d5096", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 858 | Contributors (this commit): 12 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -441,11 +441,3 @@ class ImageDataGenerator(object):\n             sigma = np.dot(flatX.T, flatX) / flatX.shape[1]\n             U, S, V = linalg.svd(sigma)\n             self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + 10e-7))), U.T)\n-\n-\n-class GraphImageDataGenerator(ImageDataGenerator):\n-    '''Example of how to build a generator for a Graph model\n-    '''\n-    def next(self):\n-        bX, bY = super(GraphImageDataGenerator, self).next()\n-        return {'input': bX, 'output': bY}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d4c85018a21d51450055ebd1e7d730010e890ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 10415 | Contributors (this commit): 46 | Commits (past 90d): 20 | Contributors (cumulative): 46 | DMM Complexity: None\n\nDIFF:\n@@ -702,11 +702,6 @@ class MaxoutDense(Layer):\n             or alternatively, Theano function to use for weights\n             initialization. This parameter is only relevant\n             if you don't pass a `weights` argument.\n-        activation: name of activation function to use\n-            (see [activations](../activations.md)),\n-            or alternatively, elementwise Theano function.\n-            If you don't specify anything, no activation is applied\n-            (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n             The list should have 2 elements, of shape `(input_dim, output_dim)`\n             and (output_dim,) for weights and biases respectively.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5b99c7512fe6ba5067481ec6136e2abefda1be0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 8 | Churn Cumulative: 7608 | Contributors (this commit): 44 | Commits (past 90d): 23 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -461,6 +461,8 @@ class Sequential(Model):\n     def predict_on_batch(self, x):\n         '''Returns predictions for a single batch of samples.\n         '''\n+        if self.model is None:\n+            self.build()\n         return self.model.predict_on_batch(x)\n \n     def train_on_batch(self, x, y, class_weight=None,\n@@ -481,6 +483,8 @@ class Sequential(Model):\n             The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         '''\n+        if self.model is None:\n+            raise Exception('The model needs to be compiled before being used.')\n         if 'accuracy' in kwargs:\n             kwargs.pop('accuracy')\n             warnings.warn('The \"accuracy\" argument is deprecated, '\n@@ -511,6 +515,8 @@ class Sequential(Model):\n             The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         '''\n+        if self.model is None:\n+            raise Exception('The model needs to be compiled before being used.')\n         if 'accuracy' in kwargs:\n             kwargs.pop('accuracy')\n             warnings.warn('The \"accuracy\" argument is deprecated, '\n@@ -696,7 +702,7 @@ class Sequential(Model):\n             A Numpy array of predictions.\n         '''\n         if self.model is None:\n-            raise Exception('The model needs to be compiled before being used.')\n+            self.build()\n         return self.model.predict_generator(generator, val_samples,\n                                             max_q_size=max_q_size)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e18e345b0168ff31d08a062714f0a3e556afaca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 229 | Lines Deleted: 77 | Files Changed: 1 | Hunks: 13 | Methods Changed: 16 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 306 | Churn Cumulative: 1164 | Contributors (this commit): 12 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.3673469387755102\n\nDIFF:\n@@ -3,6 +3,7 @@ Can easily be extended to include new transformations,\n new preprocessing methods, etc...\n '''\n from __future__ import absolute_import\n+from __future__ import print_function\n \n import numpy as np\n import re\n@@ -135,7 +136,6 @@ def array_to_img(x, dim_ordering=K.image_dim_ordering(), scale=True):\n         raise Exception('Unsupported channel number: ', x.shape[2])\n \n \n-# only used by tests/keras/preprocessing/test_image.py to convert PIL.Image to numpy array\n def img_to_array(img, dim_ordering=K.image_dim_ordering()):\n     if dim_ordering not in ['th', 'tf']:\n         raise Exception('Unknown dim_ordering: ', dim_ordering)\n@@ -154,13 +154,15 @@ def img_to_array(img, dim_ordering=K.image_dim_ordering()):\n     return x\n \n \n-def load_img(path, grayscale=False):\n+def load_img(path, grayscale=False, target_size=None):\n     from PIL import Image\n     img = Image.open(path)\n     if grayscale:\n         img = img.convert('L')\n     else:  # Ensure 3 channel even when loaded image is grayscale\n         img = img.convert('RGB')\n+    if target_size:\n+        img = img.resize(target_size)\n     return img\n \n \n@@ -216,23 +218,24 @@ class ImageDataGenerator(object):\n                  cval=0.,\n                  horizontal_flip=False,\n                  vertical_flip=False,\n+                 rescale=None,\n                  dim_ordering=K.image_dim_ordering()):\n         self.__dict__.update(locals())\n         self.mean = None\n         self.std = None\n         self.principal_components = None\n-        self.lock = threading.Lock()\n+        self.rescale = rescale\n \n         if dim_ordering not in {'tf', 'th'}:\n             raise Exception('dim_ordering should be \"tf\" (channel after row and '\n                             'column) or \"th\" (channel before row and column). '\n                             'Received arg: ', dim_ordering)\n         self.dim_ordering = dim_ordering\n-        if dim_ordering == \"th\":\n+        if dim_ordering == 'th':\n             self.channel_index = 1\n             self.row_index = 2\n             self.col_index = 3\n-        if dim_ordering == \"tf\":\n+        if dim_ordering == 'tf':\n             self.channel_index = 3\n             self.row_index = 1\n             self.col_index = 2\n@@ -246,82 +249,30 @@ class ImageDataGenerator(object):\n                             'a tuple or list of two floats. '\n                             'Received arg: ', zoom_range)\n \n-        self.batch_index = 0\n-        self.total_batches_seen = 0\n-\n-    def reset(self):\n-        self.batch_index = 0\n-\n-    def _flow_index(self, N, batch_size=32, shuffle=False, seed=None):\n-        # ensure self.batch_index is 0\n-        self.reset()\n-\n-        while 1:\n-            if self.batch_index == 0:\n-                index_array = np.arange(N)\n-                if shuffle:\n-                    if seed is not None:\n-                        np.random.seed(seed + self.total_batches_seen)\n-                    index_array = np.random.permutation(N)\n-\n-            current_index = (self.batch_index * batch_size) % N\n-            if N >= current_index + batch_size:\n-                current_batch_size = batch_size\n-                self.batch_index += 1\n-            else:\n-                current_batch_size = N - current_index\n-                self.batch_index = 0\n-            self.total_batches_seen += 1\n-            yield (index_array[current_index: current_index + current_batch_size],\n-                   current_index, current_batch_size)\n-\n-    def flow(self, X, y, batch_size=32, shuffle=False, seed=None,\n+    def flow(self, X, y, batch_size=32, shuffle=True, seed=None,\n              save_to_dir=None, save_prefix='', save_format='jpeg'):\n-        assert len(X) == len(y)\n-        self.X = X\n-        self.y = y\n-        self.save_to_dir = save_to_dir\n-        self.save_prefix = save_prefix\n-        self.save_format = save_format\n-        self.reset()\n-        self.flow_generator = self._flow_index(X.shape[0], batch_size,\n-                                               shuffle, seed)\n-        return self\n+        return NumpyArrayIterator(\n+            X, y, self,\n+            batch_size=batch_size, shuffle=shuffle, seed=seed,\n+            dim_ordering=self.dim_ordering,\n+            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n \n-    def __iter__(self):\n-        # needed if we want to do something like:\n-        # for x, y in data_gen.flow(...):\n-        return self\n-\n-    def next(self):\n-        # for python 2.x.\n-        # Keeps under lock only the mechanism which advances\n-        # the indexing of each batch\n-        # see # http://anandology.com/blog/using-iterators-and-generators/\n-        with self.lock:\n-            index_array, current_index, current_batch_size = next(self.flow_generator)\n-        # The transformation of images is not under thread lock so it can be done in parallel\n-        bX = np.zeros(tuple([current_batch_size] + list(self.X.shape)[1:]))\n-        for i, j in enumerate(index_array):\n-            x = self.X[j]\n-            x = self.random_transform(x.astype('float32'))\n-            x = self.standardize(x)\n-            bX[i] = x\n-        if self.save_to_dir:\n-            for i in range(current_batch_size):\n-                img = array_to_img(bX[i], self.dim_ordering, scale=True)\n-                fname = '{prefix}_{index}.{format}'.format(prefix=self.save_prefix,\n-                                                           index=current_index + i,\n-                                                           format=self.save_format)\n-                img.save(os.path.join(self.save_to_dir, fname))\n-        bY = self.y[index_array]\n-        return bX, bY\n-\n-    def __next__(self):\n-        # for python 3.x.\n-        return self.next()\n+    def flow_from_directory(self, directory,\n+                            target_size=(256, 256), color_mode='rgb',\n+                            classes=None, class_mode='categorical',\n+                            batch_size=32, shuffle=True, seed=None,\n+                            save_to_dir=None, save_prefix='', save_format='jpeg'):\n+        return DirectoryIterator(\n+            directory, self,\n+            target_size=target_size, color_mode=color_mode,\n+            classes=classes, class_mode=class_mode,\n+            dim_ordering=self.dim_ordering,\n+            batch_size=batch_size, shuffle=shuffle, seed=seed,\n+            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n \n     def standardize(self, x):\n+        if self.rescale:\n+            x *= self.rescale\n         # x is a single image, so it doesn't have image number at index 0\n         img_channel_index = self.channel_index - 1\n         if self.samplewise_center:\n@@ -441,3 +392,204 @@ class ImageDataGenerator(object):\n             sigma = np.dot(flatX.T, flatX) / flatX.shape[1]\n             U, S, V = linalg.svd(sigma)\n             self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + 10e-7))), U.T)\n+\n+\n+class Iterator(object):\n+\n+    def __init__(self, N, batch_size, shuffle, seed):\n+        self.N = N\n+        self.batch_size = batch_size\n+        self.shuffle = shuffle\n+        self.batch_index = 0\n+        self.total_batches_seen = 0\n+        self.lock = threading.Lock()\n+        self.index_generator = self._flow_index(N, batch_size, shuffle, seed)\n+\n+    def reset(self):\n+        self.batch_index = 0\n+\n+    def _flow_index(self, N, batch_size=32, shuffle=False, seed=None):\n+        # ensure self.batch_index is 0\n+        self.reset()\n+        while 1:\n+            if self.batch_index == 0:\n+                index_array = np.arange(N)\n+                if shuffle:\n+                    if seed is not None:\n+                        np.random.seed(seed + self.total_batches_seen)\n+                    index_array = np.random.permutation(N)\n+\n+            current_index = (self.batch_index * batch_size) % N\n+            if N >= current_index + batch_size:\n+                current_batch_size = batch_size\n+                self.batch_index += 1\n+            else:\n+                current_batch_size = N - current_index\n+                self.batch_index = 0\n+            self.total_batches_seen += 1\n+            yield (index_array[current_index: current_index + current_batch_size],\n+                   current_index, current_batch_size)\n+\n+    def __iter__(self):\n+        # needed if we want to do something like:\n+        # for x, y in data_gen.flow(...):\n+        return self\n+\n+\n+class NumpyArrayIterator(Iterator):\n+\n+    def __init__(self, X, y, image_data_generator,\n+                 batch_size=32, shuffle=False, seed=None,\n+                 dim_ordering=K.image_dim_ordering(),\n+                 save_to_dir=None, save_prefix='', save_format='jpeg'):\n+        if len(X) != len(y):\n+            raise Exception('X (images tensor) and y (labels) '\n+                            'should have the same length. '\n+                            'Found: X.shape = %s, y.shape = %s' % (np.asarray(X).shape, np.asarray(y).shape))\n+        self.X = X\n+        self.y = y\n+        self.image_data_generator = image_data_generator\n+        self.dim_ordering\n+        self.save_to_dir = save_to_dir\n+        self.save_prefix = save_prefix\n+        self.save_format = save_format\n+        super(NumpyArrayIterator, self).__init__(X.shape[0], batch_size, shuffle, seed)\n+\n+    def next(self):\n+        # for python 2.x.\n+        # Keeps under lock only the mechanism which advances\n+        # the indexing of each batch\n+        # see http://anandology.com/blog/using-iterators-and-generators/\n+        with self.lock:\n+            index_array, current_index, current_batch_size = next(self.index_generator)\n+        # The transformation of images is not under thread lock so it can be done in parallel\n+        batch_x = np.zeros(tuple([current_batch_size] + list(self.X.shape)[1:]))\n+        for i, j in enumerate(index_array):\n+            x = self.X[j]\n+            x = self.image_data_generator.random_transform(x.astype('float32'))\n+            x = self.image_data_generator.standardize(x)\n+            batch_x[i] = x\n+        if self.save_to_dir:\n+            for i in range(current_batch_size):\n+                img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n+                fname = '{prefix}_{index}.{format}'.format(prefix=self.save_prefix,\n+                                                           index=current_index + i,\n+                                                           format=self.save_format)\n+                img.save(os.path.join(self.save_to_dir, fname))\n+        batch_y = self.y[index_array]\n+        return batch_x, batch_y\n+\n+\n+class DirectoryIterator(Iterator):\n+\n+    def __init__(self, directory, image_data_generator,\n+                 target_size=(256, 256), color_mode='rgb',\n+                 dim_ordering=K.image_dim_ordering,\n+                 classes=None, class_mode='categorical',\n+                 batch_size=32, shuffle=True, seed=None,\n+                 save_to_dir=None, save_prefix='', save_format='jpeg'):\n+        self.directory = directory\n+        self.image_data_generator = image_data_generator\n+        self.target_size = tuple(target_size)\n+        if color_mode not in {'rgb', 'grayscale'}:\n+            raise ValueError('Invalid color mode:', color_mode,\n+                             '; expected \"rgb\" or \"grayscale\".')\n+        self.color_mode = color_mode\n+        self.dim_ordering = dim_ordering\n+        if self.color_mode == 'rgb':\n+            if self.dim_ordering == 'tf':\n+                self.image_shape = self.target_size + (3,)\n+            else:\n+                self.image_shape = (3,) + self.target_size\n+        else:\n+            if self.dim_ordering == 'tf':\n+                self.image_shape = self.target_size + (1,)\n+            else:\n+                self.image_shape = (1,) + self.target_size\n+        self.classes = classes\n+        if class_mode not in {'categorical', 'binary', 'sparse', None}:\n+            raise ValueError('Invalid class_mode:', class_mode,\n+                             '; expected one of \"categorical\", '\n+                             '\"binary\", \"sparse\", or None.')\n+        self.class_mode = class_mode\n+        self.save_to_dir = save_to_dir\n+        self.save_prefix = save_prefix\n+        self.save_format = save_format\n+\n+        white_list_formats = {'png', 'jpg', 'jpeg', 'bmp'}\n+\n+        # first, count the number of samples and classes\n+        self.nb_sample = 0\n+\n+        if not classes:\n+            classes = []\n+            for subdir in os.listdir(directory):\n+                if os.path.isdir(os.path.join(directory, subdir)):\n+                    classes.append(subdir)\n+        self.nb_class = len(classes)\n+        self.class_indices = dict(zip(classes, range(len(classes))))\n+\n+        for subdir in classes:\n+            subpath = os.path.join(directory, subdir)\n+            for fname in os.listdir(subpath):\n+                is_valid = False\n+                for extension in white_list_formats:\n+                    if fname.endswith('.' + extension):\n+                        is_valid = True\n+                        break\n+                if is_valid:\n+                    self.nb_sample += 1\n+        print('Found %d images belonging to %d classes.' % (self.nb_sample, self.nb_class))\n+\n+        # second, build an index of the images in the different class subfolders\n+        self.filenames = []\n+        self.classes = np.zeros((self.nb_sample,), dtype='int32')\n+        i = 0\n+        for subdir in classes:\n+            subpath = os.path.join(directory, subdir)\n+            for fname in os.listdir(subpath):\n+                is_valid = False\n+                for extension in white_list_formats:\n+                    if fname.endswith('.' + extension):\n+                        is_valid = True\n+                        break\n+                if is_valid:\n+                    self.classes[i] = self.class_indices[subdir]\n+                    self.filenames.append(os.path.join(subdir, fname))\n+                    i += 1\n+        super(DirectoryIterator, self).__init__(self.nb_sample, batch_size, shuffle, seed)\n+\n+    def next(self):\n+        with self.lock:\n+            index_array, current_index, current_batch_size = next(self.index_generator)\n+        # The transformation of images is not under thread lock so it can be done in parallel\n+        batch_x = np.zeros((current_batch_size,) + self.image_shape)\n+        grayscale = self.color_mode == 'grayscale'\n+        # build batch of image data\n+        for i, j in enumerate(index_array):\n+            fname = self.filenames[j]\n+            img = load_img(os.path.join(self.directory, fname), grayscale=grayscale, target_size=self.target_size)\n+            x = img_to_array(img, dim_ordering=self.dim_ordering)\n+            x = self.image_data_generator.random_transform(x)\n+            x = self.image_data_generator.standardize(x)\n+            batch_x[i] = x\n+        # optionally save augmented images to disk for debugging purposes\n+        if self.save_to_dir:\n+            for i in range(current_batch_size):\n+                img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n+                fname = '{prefix}_{index}.{format}'.format(prefix=self.save_prefix,\n+                                                           index=current_index + i,\n+                                                           format=self.save_format)\n+                img.save(os.path.join(self.save_to_dir, fname))\n+        # build batch of labels\n+        if self.class_mode == 'sparse':\n+            batch_y = self.classes[index_array]\n+        elif self.class_mode == 'binary':\n+            batch_y = self.classes[index_array].astype('float32')\n+        elif self.class_mode == 'categorical':\n+            batch_y = np.zeros((len(batch_x), self.nb_class), dtype='float32')\n+            for i, label in enumerate(self.classes[index_array]):\n+                batch_y[i, label] = 1.\n+        else:\n+            return batch_x\n+        return batch_x, batch_y\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3bba409d9e26ec7f7a964362667a6e8c0e08fc89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1169 | Contributors (this commit): 12 | Commits (past 90d): 15 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -196,6 +196,9 @@ class ImageDataGenerator(object):\n             'constant'. Default is 0.\n         horizontal_flip: whether to randomly flip images horizontally.\n         vertical_flip: whether to randomly flip images vertically.\n+        rescale: rescaling factor. If None or 0, no rescaling is applied,\n+            otherwise we multiply the data by the value provided (before applying\n+            any other transformation).\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode it is at index 3.\n             It defaults to the `image_dim_ordering` value found in your\n@@ -449,7 +452,7 @@ class NumpyArrayIterator(Iterator):\n         self.X = X\n         self.y = y\n         self.image_data_generator = image_data_generator\n-        self.dim_ordering\n+        self.dim_ordering = dim_ordering\n         self.save_to_dir = save_to_dir\n         self.save_prefix = save_prefix\n         self.save_format = save_format\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dfc128b89a7b7f3fbebd0ad7b8a6a843e8dc2f26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 1172 | Contributors (this commit): 12 | Commits (past 90d): 16 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -438,6 +438,9 @@ class Iterator(object):\n         # for x, y in data_gen.flow(...):\n         return self\n \n+    def __next__(self, *args, **kwargs):\n+        return self.next(*args, **kwargs)\n+\n \n class NumpyArrayIterator(Iterator):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b71f6ba8644681b51ce4962ad2e0543adbfd4d28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 1184 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 12 | DMM Complexity: 0.75\n\nDIFF:\n@@ -252,7 +252,7 @@ class ImageDataGenerator(object):\n                             'a tuple or list of two floats. '\n                             'Received arg: ', zoom_range)\n \n-    def flow(self, X, y, batch_size=32, shuffle=True, seed=None,\n+    def flow(self, X, y=None, batch_size=32, shuffle=True, seed=None,\n              save_to_dir=None, save_prefix='', save_format='jpeg'):\n         return NumpyArrayIterator(\n             X, y, self,\n@@ -448,7 +448,7 @@ class NumpyArrayIterator(Iterator):\n                  batch_size=32, shuffle=False, seed=None,\n                  dim_ordering=K.image_dim_ordering(),\n                  save_to_dir=None, save_prefix='', save_format='jpeg'):\n-        if len(X) != len(y):\n+        if y is not None and len(X) != len(y):\n             raise Exception('X (images tensor) and y (labels) '\n                             'should have the same length. '\n                             'Found: X.shape = %s, y.shape = %s' % (np.asarray(X).shape, np.asarray(y).shape))\n@@ -478,10 +478,13 @@ class NumpyArrayIterator(Iterator):\n         if self.save_to_dir:\n             for i in range(current_batch_size):\n                 img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n-                fname = '{prefix}_{index}.{format}'.format(prefix=self.save_prefix,\n+                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n                                                                   index=current_index + i,\n+                                                                  hash=np.random.randint(1e4),\n                                                                   format=self.save_format)\n                 img.save(os.path.join(self.save_to_dir, fname))\n+        if self.y is None:\n+            return batch_x\n         batch_y = self.y[index_array]\n         return batch_x, batch_y\n \n@@ -583,8 +586,9 @@ class DirectoryIterator(Iterator):\n         if self.save_to_dir:\n             for i in range(current_batch_size):\n                 img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n-                fname = '{prefix}_{index}.{format}'.format(prefix=self.save_prefix,\n+                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n                                                            index=current_index + i,\n+                                                           hash=np.random.randint(1e4),\n                                                            format=self.save_format)\n                 img.save(os.path.join(self.save_to_dir, fname))\n         # build batch of labels\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd3cfb196b2be22c274cc7b7f0abbd1c8e87a2c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 421 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,7 +9,7 @@ if not pydot.find_graphviz():\n                        ' and graphviz for `pydotprint` to work.')\n \n \n-def model_to_dot(model, show_shapes=False):\n+def model_to_dot(model, show_shapes=False, show_layer_names=True):\n     dot = pydot.Dot()\n     dot.set('rankdir', 'TB')\n     dot.set('concentrate', True)\n@@ -24,7 +24,10 @@ def model_to_dot(model, show_shapes=False):\n     # first, populate the nodes of the graph\n     for layer in layers:\n         layer_id = str(id(layer))\n+        if show_layer_names:\n             label = str(layer.name) + ' (' + layer.__class__.__name__ + ')'\n+        else:\n+            label = layer.__class__.__name__\n \n         if show_shapes:\n             # Build the label that will actually contain a table with the\n@@ -59,6 +62,6 @@ def model_to_dot(model, show_shapes=False):\n     return dot\n \n \n-def plot(model, to_file='model.png', show_shapes=False):\n-    dot = model_to_dot(model, show_shapes)\n+def plot(model, to_file='model.png', show_shapes=False, show_layer_names=True):\n+    dot = model_to_dot(model, show_shapes, show_layer_names)\n     dot.write_png(to_file)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#62053e68e2895f990295bae4bda3e671767849fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 203 | Contributors (this commit): 8 | Commits (past 90d): 13 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -15,4 +15,4 @@ from . import objectives\n from . import optimizers\r\n from . import regularizers\r\n \r\n-__version__ = '1.0.3'\r\n+__version__ = '1.0.4'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='1.0.3',\n+      version='1.0.4',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/1.0.3',\n+      download_url='https://github.com/fchollet/keras/tarball/1.0.4',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97e31b6090b809cc1987f7d16a1dc55379edaf42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 100 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 102 | Churn Cumulative: 846 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -82,6 +82,7 @@ from keras import constraints\n from keras import activations\n from keras import regularizers\n \n+\n EXCLUDE = {\n     'Optimizer',\n     'Wrapper',\n@@ -334,6 +335,7 @@ def process_function_docstring(docstring):\n print('Cleaning up existing sources directory.')\n if os.path.exists('sources'):\n     shutil.rmtree('sources')\n+\n print('Populating sources directory with templates.')\n for subdir, dirs, fnames in os.walk('templates'):\n     for fname in fnames:\n@@ -418,103 +420,3 @@ for page_data in PAGES:\n     if not os.path.exists(subdir):\n         os.makedirs(subdir)\n     open(path, 'w').write(mkdown)\n-\n-\n-# covered_so_far = set()\n-# for module, module_name in MODULES:\n-#     class_pages = []\n-#     for name in dir(module):\n-#         if name in SKIP:\n-#             continue\n-#         if name[0] == '_':\n-#             continue\n-#         module_member = getattr(module, name)\n-#         if module_member in covered_so_far:\n-#             continue\n-#         if inspect.isclass(module_member):\n-#             cls = module_member\n-#             if cls.__module__ == module_name:\n-\n-#                 try:\n-#                     class_signature = get_function_signature(cls.__init__)\n-#                     class_signature = class_signature.replace('__init__', cls.__name__)\n-#                 except:\n-#                     # in case the class inherits from object and does not\n-#                     # define __init__\n-#                     class_signature = module_name + '.' + cls.__name__ + '()'\n-\n-#                 functions = []\n-#                 functions_not_defined_here = []\n-#                 for name in dir(cls):\n-#                     if name in SKIP:\n-#                         continue\n-#                     if name[0] == '_':\n-#                         continue\n-#                     cls_member = getattr(cls, name)\n-#                     if inspect.isfunction(cls_member):\n-#                         function = cls_member\n-#                         signature = inspect.getargspec(function)\n-#                         defaults = signature.defaults\n-#                         args = signature.args[1:]\n-#                         if defaults:\n-#                             kwargs = zip(args[-len(defaults):], defaults)\n-#                             args = args[:-len(defaults)]\n-#                         else:\n-#                             kwargs = []\n-\n-#                         defined_by = get_earliest_class_that_defined_member(function.__name__, cls)\n-#                         if cls == defined_by:\n-#                             functions.append(function)\n-#                         else:\n-#                             functions_not_defined_here.append((function, defined_by))\n-\n-#                 blocks = []\n-#                 blocks.append('<span style=\"float:right;\">' + class_to_source_link(cls) + '</span>')\n-#                 blocks.append('# ' + cls.__name__ + '\\n')\n-#                 blocks.append(code_snippet(class_signature))\n-#                 docstring = cls.__doc__\n-#                 if docstring:\n-#                     blocks.append(process_class_docstring(docstring))\n-\n-#                 if cls.__name__ in INCLUDE_functionS_FOR:\n-#                     if functions or functions_not_defined_here:\n-#                         blocks.append('### functions\\n')\n-#                         for function in functions:\n-#                             signature = get_function_signature(function)\n-#                             signature = signature.replace(module_name + '.', '')\n-#                             blocks.append(code_snippet(signature))\n-#                             docstring = function.__doc__\n-#                             if docstring:\n-#                                 blocks.append(process_function_docstring(docstring))\n-#                         for function, defined_by in functions_not_defined_here:\n-#                             signature = get_function_signature(function)\n-#                             function_module_name = function.__module__\n-#                             signature = signature.replace(function_module_name + '.', '')\n-#                             link = '[' + defined_by.__name__ + '](' + class_to_docs_link(defined_by) + ')'\n-#                             blocks.append(code_snippet(signature))\n-#                             blocks.append('Defined by ' + link + '.\\n')\n-\n-#                 mkdown = '\\n'.join(blocks)\n-#                 class_pages.append((id(cls), mkdown))\n-#                 covered_so_far.add(module_member)\n-\n-#     class_pages.sort(key=lambda x: x[0])\n-#     class_pages = [x[1] for x in class_pages]\n-#     module_page = '\\n----\\n\\n'.join(class_pages)\n-\n-#     # save module page.\n-#     # Either insert content into existing page,\n-#     # or create page otherwise\n-#     path = 'sources/' + module_name.replace('.', '/')[6:] + '.md'\n-#     if os.path.exists(path):\n-#         template = open(path).read()\n-#         assert '{{autogenerated}}' in template, ('Template found for ' + path +\n-#                                                  ' but missing {{autogenerated}} tag.')\n-#         module_page = template.replace('{{autogenerated}}', module_page)\n-#         print('...inserting autogenerated content into template:', path)\n-#     else:\n-#         print('...creating new page with autogenerated content:', path)\n-#     subdir = os.path.dirname(path)\n-#     if not os.path.exists(subdir):\n-#         os.makedirs(subdir)\n-#     open(path, 'w').write(module_page)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af5c5b6a55528a255500b733439a66d25b5647ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 42 | Files Changed: 20 | Hunks: 41 | Methods Changed: 18 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 84 | Churn Cumulative: 40772 | Contributors (this commit): 134 | Commits (past 90d): 267 | Contributors (cumulative): 301 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ class CharacterTable(object):\n     Given a set of characters:\n     + Encode them to a one hot integer representation\n     + Decode the one hot integer representation to their character output\n-    + Decode a vector of probabilties to their character output\n+    + Decode a vector of probabilities to their character output\n     '''\n     def __init__(self, chars, maxlen):\n         self.chars = sorted(set(chars))\n\n@@ -9,7 +9,7 @@ e.g.:\n python deep_dream.py img/mypic.jpg results/dream\n ```\n \n-It is preferrable to run this script on GPU, for speed.\n+It is preferable to run this script on GPU, for speed.\n If running on CPU, prefer the TensorFlow backend (much faster).\n \n Example results: http://i.imgur.com/FX6ROg9.jpg\n\n@@ -14,7 +14,7 @@ e.g.:\n python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n ```\n \n-It is preferrable to run this script on GPU, for speed.\n+It is preferable to run this script on GPU, for speed.\n If running on CPU, prefer the TensorFlow backend (much faster).\n \n Example result: https://twitter.com/fchollet/status/686631033085677568\n@@ -34,7 +34,7 @@ the pixels of the combination image, giving it visual coherence.\n \n - The style loss is where the deep learning keeps in --that one is defined\n using a deep convolutional neural network. Precisely, it consists in a sum of\n-L2 distances betwen the Gram matrices of the representations of\n+L2 distances between the Gram matrices of the representations of\n the base image and the style reference image, extracted from\n different layers of a convnet (trained on ImageNet). The general idea\n is to capture color/texture information at different spatial\n\n@@ -328,7 +328,7 @@ def std(x, axis=None, keepdims=False):\n \n \n def mean(x, axis=None, keepdims=False):\n-    '''Mean of a tensor, alongside the specificied axis.\n+    '''Mean of a tensor, alongside the specified axis.\n     '''\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n@@ -971,7 +971,7 @@ def dropout(x, level, seed=None):\n \n \n def l2_normalize(x, axis):\n-    '''Normalizes a tensor wrt the L2 norm alonside the specified axis.\n+    '''Normalizes a tensor wrt the L2 norm alongside the specified axis.\n     '''\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n\n@@ -397,7 +397,7 @@ def temporal_padding(x, padding=1):\n     '''Pad the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n \n-    Appologies for the inane API, but Theano makes this\n+    Apologies for the inane API, but Theano makes this\n     really hard.\n     '''\n     input_shape = x.shape\n\n@@ -426,11 +426,11 @@ class TensorBoard(Callback):\n \n     # Arguments\n         log_dir: the path of the directory where to save the log\n-            files to be parsed by tensorboard\n+            files to be parsed by Tensorboard\n         histogram_freq: frequency (in epochs) at which to compute activation\n             histograms for the layers of the model. If set to 0,\n             histograms won't be computed.\n-        write_graph: whether to visualize the graph in tensorboard. The log file can\n+        write_graph: whether to visualize the graph in Tensorboard. The log file can\n             become quite large when write_graph is set to True.\n     '''\n \n\n@@ -281,7 +281,7 @@ class Layer(object):\n         self.outbound_nodes = []\n \n         # these properties will be set upon call of self.build(),\n-        # which itself will be calld upon self.add_inbound_node if necessary.\n+        # which itself will be called upon self.add_inbound_node if necessary.\n         self.trainable_weights = []\n         self.non_trainable_weights = []\n         self.regularizers = []\n@@ -512,7 +512,7 @@ class Layer(object):\n                 where to connect the current layer.\n             tensor_indices: integer or list of integers.\n                 The output of the inbound node might be a list/tuple\n-                of tensor, and we might only be interested in one sepcific entry.\n+                of tensor, and we might only be interested in one specific entry.\n                 This index allows you to specify the index of the entry in the output list\n                 (if applicable). \"None\" means that we take all outputs (as a list).\n         '''\n@@ -1003,7 +1003,7 @@ def Input(shape=None, batch_shape=None,\n             Should be unique in a model (do not reuse the same name twice).\n             It will be autogenerated if it isn't provided.\n         dtype: The data type expected by the input, as a string\n-            (`float32`, `flaot64`, `int32`...)\n+            (`float32`, `float64`, `int32`...)\n \n     # Example usage\n \n@@ -1233,8 +1233,8 @@ class Merge(Layer):\n     def __call__(self, inputs, mask=None):\n         '''We disable successive calls to __call__ for Merge layers.\n         Although there is no technical obstacle to\n-        making it possible to __call__ a Merge intance many times\n-        (it is just a layer), it would make for a rather unelegant API.\n+        making it possible to __call__ a Merge instance many times\n+        (it is just a layer), it would make for a rather inelegant API.\n         '''\n         if type(inputs) is not list:\n             raise Exception('Merge can only be called on a list of tensors, '\n@@ -1272,7 +1272,7 @@ class Merge(Layer):\n             return self.call(inputs, mask)\n \n     def get_output_shape_for(self, input_shape):\n-        assert type(input_shape) is list  # must have mutiple input shape tuples\n+        assert type(input_shape) is list  # must have multiple input shape tuples\n         # case: callable self._output_shape\n         if hasattr(self.mode, '__call__'):\n             if hasattr(self._output_shape, '__call__'):\n\n@@ -825,7 +825,7 @@ class Model(Container):\n             verbose: verbosity mode.\n \n         # Returns\n-            Array of prections (if the model has a single output)\n+            Array of predictions (if the model has a single output)\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n         '''\n@@ -969,7 +969,7 @@ class Model(Container):\n                 at the end of each epoch. The model will not be trained on this data.\n                 This could be a tuple (x_val, y_val) or a tuple (val_x, val_y, val_sample_weights).\n             shuffle: boolean, whether to shuffle the training data before each epoch.\n-            class_weight: optional dictionary mapping classe indices (integers) to\n+            class_weight: optional dictionary mapping class indices (integers) to\n                 a weight (float) to apply to the model's loss for the samples\n                 from this class during training.\n                 This can be useful to tell the model to \"pay more attention\" to\n@@ -1144,7 +1144,7 @@ class Model(Container):\n                 with shape (samples, sequence_length),\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n-            class_weight: optional dictionary mapping classe indices (integers) to\n+            class_weight: optional dictionary mapping class indices (integers) to\n                 a weight (float) to apply to the model's loss for the samples\n                 from this class during training.\n                 This can be useful to tell the model to \"pay more attention\" to\n\n@@ -51,7 +51,7 @@ class PReLU(Layer):\n \n     # Arguments\n         init: initialization function for the weights.\n-        weights: initial weights, as a list of a single numpy array.\n+        weights: initial weights, as a list of a single Numpy array.\n \n     # References\n         - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/pdf/1502.01852v1.pdf)\n\n@@ -394,7 +394,7 @@ class Convolution3D(Layer):\n \n     # Arguments\n         nb_filter: Number of convolution filters to use.\n-        kernel_dim1: Length of the first dimension in the covolution kernel.\n+        kernel_dim1: Length of the first dimension in the convolution kernel.\n         kernel_dim2: Length of the second dimension in the convolution kernel.\n         kernel_dim3: Length of the third dimension in the convolution kernel.\n         init: name of initialization function for the weights of the layer\n@@ -407,7 +407,7 @@ class Convolution3D(Layer):\n             or alternatively, elementwise Theano function.\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n-        weights: list of numpy arrays to set as initial weights.\n+        weights: list of Numpy arrays to set as initial weights.\n         border_mode: 'valid' or 'same'.\n         subsample: tuple of length 3. Factor by which to subsample output.\n             Also called strides elsewhere.\n\n@@ -161,7 +161,7 @@ class Reshape(Layer):\n         '''Find and replace a single missing dimension in an output shape\n         given an input shape.\n \n-        A near direct port of the internal numpy function _fix_unknown_dimension\n+        A near direct port of the internal Numpy function _fix_unknown_dimension\n         in numpy/core/src/multiarray/shape.c\n \n         # Arguments\n@@ -537,7 +537,7 @@ class Dense(Layer):\n             or alternatively, elementwise Theano function.\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n-        weights: list of numpy arrays to set as initial weights.\n+        weights: list of Numpy arrays to set as initial weights.\n             The list should have 2 elements, of shape `(input_dim, output_dim)`\n             and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n@@ -702,7 +702,7 @@ class MaxoutDense(Layer):\n             or alternatively, Theano function to use for weights\n             initialization. This parameter is only relevant\n             if you don't pass a `weights` argument.\n-        weights: list of numpy arrays to set as initial weights.\n+        weights: list of Numpy arrays to set as initial weights.\n             The list should have 2 elements, of shape `(input_dim, output_dim)`\n             and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n@@ -834,7 +834,7 @@ class Highway(Layer):\n             or alternatively, elementwise Theano function.\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n-        weights: list of numpy arrays to set as initial weights.\n+        weights: list of Numpy arrays to set as initial weights.\n             The list should have 2 elements, of shape `(input_dim, output_dim)`\n             and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n@@ -983,7 +983,7 @@ class TimeDistributedDense(Layer):\n             or alternatively, elementwise Theano function.\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n-        weights: list of numpy arrays to set as initial weights.\n+        weights: list of Numpy arrays to set as initial weights.\n             The list should have 2 elements, of shape `(input_dim, output_dim)`\n             and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n\n@@ -35,7 +35,7 @@ class Embedding(Layer):\n           of the layer (see: [initializations](../initializations.md)),\n           or alternatively, Theano function to use for weights initialization.\n           This parameter is only relevant if you don't pass a `weights` argument.\n-      weights: list of numpy arrays to set as initial weights.\n+      weights: list of Numpy arrays to set as initial weights.\n           The list should have 1 element, of shape `(input_dim, output_dim)`.\n       W_regularizer: instance of the [regularizers](../regularizers.md) module\n         (eg. L1 or L2 regularization), applied to the embedding matrix.\n\n@@ -4,7 +4,7 @@ from .. import backend as K\n \n \n class GaussianNoise(Layer):\n-    '''Apply to the input an additive zero-centred gaussian noise with\n+    '''Apply to the input an additive zero-centered Gaussian noise with\n     standard deviation `sigma`. This is useful to mitigate overfitting\n     (you could see it as a kind of random data augmentation).\n     Gaussian Noise (GS) is a natural choice as corruption process\n@@ -42,7 +42,7 @@ class GaussianNoise(Layer):\n \n \n class GaussianDropout(Layer):\n-    '''Apply to the input an multiplicative one-centred gaussian noise\n+    '''Apply to the input an multiplicative one-centered Gaussian noise\n     with standard deviation `sqrt(p/(1-p))`.\n \n     As it is a regularization layer, it is only active at training time.\n\n@@ -33,7 +33,7 @@ class BatchNormalization(Layer):\n             exponential average of the mean and standard deviation\n             of the data, for feature-wise normalization.\n         weights: Initialization weights.\n-            List of 2 numpy arrays, with shapes:\n+            List of 2 Numpy arrays, with shapes:\n             `[(input_shape,), (input_shape,)]`\n         beta_init: name of initialization function for shift parameter\n             (see [initializations](../initializations.md)), or alternatively,\n\n@@ -66,7 +66,7 @@ class Recurrent(Layer):\n     ```\n \n     # Arguments\n-        weights: list of numpy arrays to set as initial weights.\n+        weights: list of Numpy arrays to set as initial weights.\n             The list should have 3 elements, of shapes:\n             `[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]`.\n         return_sequences: Boolean. Whether to return the last output\n\n@@ -384,7 +384,7 @@ class Graph(Model):\n \n         # Arguments\n             data: dictionary mapping input names and outputs names to\n-                appropriate numpy arrays. All arrays should contain\n+                appropriate Numpy arrays. All arrays should contain\n                 the same number of samples.\n             batch_size: int. Number of samples per gradient update.\n             nb_epoch: int.\n@@ -395,7 +395,7 @@ class Graph(Model):\n             validation_split: float (0. < x < 1). Fraction of the data to\n                 use as held-out validation data.\n             validation_data: dictionary mapping input names and outputs names\n-                to appropriate numpy arrays to be used as\n+                to appropriate Numpy arrays to be used as\n                 held-out validation data.\n                 All arrays should contain the same number of samples.\n                 Will override validation_split.\n@@ -560,7 +560,7 @@ class Graph(Model):\n             verbose: verbosity mode, 0, 1, or 2.\n             callbacks: list of callbacks to be called during training.\n             validation_data: dictionary mapping input names and outputs names\n-                to appropriate numpy arrays to be used as\n+                to appropriate Numpy arrays to be used as\n                 held-out validation data, or a generator yielding such\n                 dictionaries. All arrays should contain the same number\n                 of samples. If a generator, will be called until more than\n@@ -582,7 +582,7 @@ class Graph(Model):\n                 while 1:\n                     f = open(path)\n                     for line in f:\n-                        # create numpy arrays of input data\n+                        # create Numpy arrays of input data\n                         # and labels, from each line in the file\n                         x1, x2, y = process_line(line)\n                         yield ({'input_1': x1, 'input_2': x2, 'output': y})\n\n@@ -615,7 +615,7 @@ class Sequential(Model):\n                 while 1:\n                     f = open(path)\n                     for line in f:\n-                        # create numpy arrays of input data\n+                        # create Numpy arrays of input data\n                         # and labels, from each line in the file\n                         x, y = process_line(line)\n                         yield (x, y)\n\n@@ -100,7 +100,7 @@ def skipgrams(sequence, vocabulary_size,\n     '''Take a sequence (list of indexes of words),\n     returns couples of [word_index, other_word index] and labels (1s or 0s),\n     where label = 1 if 'other_word' belongs to the context of 'word',\n-    and label=0 if 'other_word' is ramdomly sampled\n+    and label=0 if 'other_word' is randomly sampled\n \n     # Arguments\n         vocabulary_size: int. maximum possible word index + 1\n@@ -113,7 +113,7 @@ def skipgrams(sequence, vocabulary_size,\n             if True labels will be categorical eg. [[1,0],[0,1],[0,1] .. ]\n \n     # Returns\n-        couples, lables: where `couples` are int pairs and\n+        couples, labels: where `couples` are int pairs and\n             `labels` are either 0 or 1.\n \n     # Notes\n\n@@ -53,7 +53,7 @@ def categorical_probas_to_classes(p):\n \n \n def convert_kernel(kernel, dim_ordering='th'):\n-    '''Converts a kernel matrix (numpy array)\n+    '''Converts a kernel matrix (Numpy array)\n     from Theano format to TensorFlow format\n     (or reciprocally, since the transformation\n     is its own inverse).\n\n@@ -28,7 +28,7 @@ class BaseWrapper(object):\n \n     `sk_params` takes both model parameters and fitting parameters. Legal model\n     parameters are the arguments of `build_fn`. Note that like all other\n-    estimators in scikit-learn, 'build_fn' should provide defalult values for\n+    estimators in scikit-learn, 'build_fn' should provide default values for\n     its arguments, so that you could create the estimator without passing any\n     values to `sk_params`.\n \n@@ -153,10 +153,10 @@ class BaseWrapper(object):\n \n         # Arguments\n             fn : arbitrary function\n-            override: dictionary, values to overrid sk_params\n+            override: dictionary, values to override sk_params\n \n         # Returns\n-            res : dictionary dictionary containing variabls\n+            res : dictionary dictionary containing variables\n                 in both sk_params and fn's arguments.\n         '''\n         res = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
