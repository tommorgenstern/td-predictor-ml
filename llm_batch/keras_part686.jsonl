{"custom_id": "keras#fe5fad8d55ab642eaac6ccf39047fd307c601653", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 772 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -458,6 +458,39 @@ class Amax(Operation):\n \n @keras_core_export([\"keras_core.ops.amax\", \"keras_core.ops.numpy.amax\"])\n def amax(x, axis=None, keepdims=False):\n+    \"\"\"Return the maximum of an array or maximum value along an axis.\n+    \n+    Args:\n+        x: Input tensor.\n+        axis: None or int or tuple of ints, optional \n+              Axis along which the maximum value of a array is found.\n+              By default (`axis=None`) find the maximum value in all the \n+              dimensions of the input array.`axis` may be negative, in which \n+              case it counts for the last to the first axis.     \n+        keep_dims: If `True`, axes which are reduced are left in the result as\n+              dimensions with size one. With this option, the result will\n+              broadcast correctly against the input array. Default is `False`.\n+\n+    Returns:\n+        An array with the maximum value. If `axis` is None, the result is a scalar\n+        value representing the maximum element in the entire array. If `axis` is\n+        given, the result is an array with the maximum values along the specified\n+        axis.\n+\n+    Examples:\n+        >>> x = keras_core.ops.convert_to_tensor([[1, 3, 5],[2, 3, 6]])\n+        >>> keras_core.ops.amax(x)\n+        array(6, dtype=int32)\n+\n+        >>> x = keras_core.ops.convert_to_tensor([[1,6,8],[1,5,2]])\n+        >>> keras_core.ops.amax(x, axis=0)\n+        array([1,6,8], dtype=int32)\n+\n+        >>> x = keras_core.ops.convert_to_tensor([[1,6,8],[1,5,2]])\n+        >>> keras_core.ops.amax(x, axis=1, keepdims=True)\n+        array([[8],\n+              [5]], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Amax(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.amax(x, axis=axis, keepdims=keepdims)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9fd2a7a06bfd02ccf25dd37ce6739f80746988a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 989 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import unittest.mock\n+\n import numpy as np\n import tensorflow as tf\n from absl.testing import parameterized\n@@ -12,9 +13,7 @@ from keras_core import utils\n class MockedRandomFlip(layers.RandomFlip):\n     def call(self, inputs, training=True):\n         with unittest.mock.patch.object(\n-                self.backend.random,\n-                \"uniform\",\n-                return_value=0.1\n+            self.backend.random, \"uniform\", return_value=0.1\n         ):\n             out = super().call(inputs, training=training)\n         return out\n\n@@ -458,26 +458,25 @@ class Amax(Operation):\n \n @keras_core_export([\"keras_core.ops.amax\", \"keras_core.ops.numpy.amax\"])\n def amax(x, axis=None, keepdims=False):\n-    \"\"\"Return the maximum of an array or maximum value along an axis.\n+    \"\"\"Returns the maximum of an array or maximum value along an axis.\n \n     Args:\n         x: Input tensor.\n-        axis: None or int or tuple of ints, optional \n-              Axis along which the maximum value of a array is found.\n-              By default (`axis=None`) find the maximum value in all the \n-              dimensions of the input array.`axis` may be negative, in which \n-              case it counts for the last to the first axis.     \n+        axis: Axis along which to compute the maximum.\n+            By default (`axis=None`), find the maximum value in all the\n+            dimensions of the input array.\n         keep_dims: If `True`, axes which are reduced are left in the result as\n-              dimensions with size one. With this option, the result will\n-              broadcast correctly against the input array. Default is `False`.\n+            dimensions that are broadcast to the size of the original\n+            input tensor. Defaults to `False`.\n \n     Returns:\n-        An array with the maximum value. If `axis` is None, the result is a scalar\n+        An array with the maximum value. If `axis=None`, the result is a scalar\n         value representing the maximum element in the entire array. If `axis` is\n-        given, the result is an array with the maximum values along the specified\n-        axis.\n+        given, the result is an array with the maximum values along\n+        the specified axis.\n \n     Examples:\n+\n     >>> x = keras_core.ops.convert_to_tensor([[1, 3, 5], [2, 3, 6]])\n     >>> keras_core.ops.amax(x)\n     array(6, dtype=int32)\n@@ -488,8 +487,7 @@ def amax(x, axis=None, keepdims=False):\n \n     >>> x = keras_core.ops.convert_to_tensor([[1, 6, 8], [1, 5, 2]])\n     >>> keras_core.ops.amax(x, axis=1, keepdims=True)\n-        array([[8],\n-              [5]], dtype=int32)\n+    array([[8], [5]], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Amax(axis=axis, keepdims=keepdims).symbolic_call(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93e0b0b5968f03c95f89d1f709c8e4ecf9ed62f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 420 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 32/32 | Churn Δ: 420 | Churn Cumulative: 420 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7222222222222222\n\nDIFF:\n@@ -0,0 +1,420 @@\n+\"\"\"\n+Title: Video Classification with Transformers\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Converted to Keras Core by: [Soumik Rakshit](http://github.com/soumik12345)\n+Date created: 2021/06/08\n+Last modified: 2023/22/07\n+Description: Training a video classifier with hybrid transformers.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+This example is a follow-up to the\n+[Video Classification with a CNN-RNN Architecture](https://keras.io/examples/vision/video_classification/)\n+example. This time, we will be using a Transformer-based model\n+([Vaswani et al.](https://arxiv.org/abs/1706.03762)) to classify videos. You can follow\n+[this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11)\n+in case you need an introduction to Transformers (with code). After reading this\n+example, you will know how to develop hybrid Transformer-based models for video\n+classification that operate on CNN feature maps.\n+\"\"\"\n+\n+\"\"\"shell\n+pip install -q git+https://github.com/keras-team/keras-core\n+pip install -q git+https://github.com/tensorflow/docs\n+\"\"\"\n+\n+\"\"\"\n+## Data collection\n+\n+As done in the [predecessor](https://keras.io/examples/vision/video_classification/) to\n+this example, we will be using a subsampled version of the\n+[UCF101 dataset](https://www.crcv.ucf.edu/data/UCF101.php),\n+a well-known benchmark dataset. In case you want to operate on a larger subsample or\n+even the entire dataset, please refer to\n+[this notebook](https://colab.research.google.com/github/sayakpaul/Action-Recognition-in-TensorFlow/blob/main/Data_Preparation_UCF101.ipynb).\n+\"\"\"\n+\n+\"\"\"shell\n+wget -q https://github.com/sayakpaul/Action-Recognition-in-TensorFlow/releases/download/v1.0.0/ucf101_top5.tar.gz\n+tar -xf ucf101_top5.tar.gz\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core.applications.densenet import DenseNet121\n+\n+from tensorflow_docs.vis import embed\n+\n+import matplotlib.pyplot as plt\n+import pandas as pd\n+import numpy as np\n+import imageio\n+import cv2\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+MAX_SEQ_LENGTH = 20\n+NUM_FEATURES = 1024\n+IMG_SIZE = 128\n+\n+EPOCHS = 5\n+\n+\"\"\"\n+## Data preparation\n+\n+We will mostly be following the same data preparation steps in this example, except for\n+the following changes:\n+\n+* We reduce the image size to 128x128 instead of 224x224 to speed up computation.\n+* Instead of using a pre-trained [InceptionV3](https://arxiv.org/abs/1512.00567) network,\n+we use a pre-trained\n+[DenseNet121](http://openaccess.thecvf.com/content_cvpr_2017/papers/Huang_Densely_Connected_Convolutional_CVPR_2017_paper.pdf)\n+for feature extraction.\n+* We directly pad shorter videos to length `MAX_SEQ_LENGTH`.\n+\n+First, let's load up the\n+[DataFrames](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n+\"\"\"\n+\n+train_df = pd.read_csv(\"train.csv\")\n+test_df = pd.read_csv(\"test.csv\")\n+\n+print(f\"Total videos for training: {len(train_df)}\")\n+print(f\"Total videos for testing: {len(test_df)}\")\n+\n+center_crop_layer = layers.CenterCrop(IMG_SIZE, IMG_SIZE)\n+\n+\n+def crop_center(frame):\n+    cropped = center_crop_layer(frame[None, ...])\n+    cropped = keras.ops.convert_to_numpy(cropped)\n+    cropped = keras.ops.squeeze(cropped)\n+    return cropped\n+\n+\n+# Following method is modified from this tutorial:\n+# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n+def load_video(path, max_frames=0, offload_to_cpu=False):\n+    cap = cv2.VideoCapture(path)\n+    frames = []\n+    try:\n+        while True:\n+            ret, frame = cap.read()\n+            if not ret:\n+                break\n+            frame = frame[:, :, [2, 1, 0]]\n+            frame = crop_center(frame)\n+            if offload_to_cpu and keras.backend.backend() == \"torch\":\n+                frame = frame.to(\"cpu\")\n+            frames.append(frame)\n+\n+            if len(frames) == max_frames:\n+                break\n+    finally:\n+        cap.release()\n+    if offload_to_cpu and keras.backend.backend() == \"torch\":\n+        return np.array([frame.to(\"cpu\").numpy() for frame in frames])\n+    return np.array(frames)\n+\n+\n+def build_feature_extractor():\n+    feature_extractor = DenseNet121(\n+        weights=\"imagenet\",\n+        include_top=False,\n+        pooling=\"avg\",\n+        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n+    )\n+    preprocess_input = keras.applications.densenet.preprocess_input\n+\n+    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n+    preprocessed = preprocess_input(inputs)\n+\n+    outputs = feature_extractor(preprocessed)\n+    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n+\n+\n+feature_extractor = build_feature_extractor()\n+\n+\n+# Label preprocessing with StringLookup.\n+label_processor = keras.layers.StringLookup(\n+    num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]), mask_token=None\n+)\n+print(label_processor.get_vocabulary())\n+\n+\n+def prepare_all_videos(df, root_dir):\n+    num_samples = len(df)\n+    video_paths = df[\"video_name\"].values.tolist()\n+    labels = df[\"tag\"].values\n+    labels = label_processor(labels[..., None]).numpy()\n+\n+    # `frame_features` are what we will feed to our sequence model.\n+    frame_features = np.zeros(\n+        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n+    )\n+\n+    # For each video.\n+    for idx, path in enumerate(video_paths):\n+        # Gather all its frames and add a batch dimension.\n+        frames = load_video(os.path.join(root_dir, path))\n+\n+        # Pad shorter videos.\n+        if len(frames) < MAX_SEQ_LENGTH:\n+            diff = MAX_SEQ_LENGTH - len(frames)\n+            padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n+            frames = np.concatenate(frames, padding)\n+\n+        frames = frames[None, ...]\n+\n+        # Initialize placeholder to store the features of the current video.\n+        temp_frame_features = np.zeros(\n+            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n+        )\n+\n+        # Extract features from the frames of the current video.\n+        for i, batch in enumerate(frames):\n+            video_length = batch.shape[0]\n+            length = min(MAX_SEQ_LENGTH, video_length)\n+            for j in range(length):\n+                if np.mean(batch[j, :]) > 0.0:\n+                    temp_frame_features[i, j, :] = feature_extractor.predict(\n+                        batch[None, j, :]\n+                    )\n+\n+                else:\n+                    temp_frame_features[i, j, :] = 0.0\n+\n+        frame_features[idx,] = temp_frame_features.squeeze()\n+\n+    return frame_features, labels\n+\n+\n+\"\"\"\n+Calling `prepare_all_videos()` on `train_df` and `test_df` takes ~20 minutes to\n+complete. For this reason, to save time, here we download already preprocessed NumPy arrays:\n+\"\"\"\n+\n+\"\"\"shell\n+!wget -q https://git.io/JZmf4 -O top5_data_prepared.tar.gz\n+!tar -xf top5_data_prepared.tar.gz\n+\"\"\"\n+\n+train_data, train_labels = np.load(\"train_data.npy\"), np.load(\n+    \"train_labels.npy\"\n+)\n+test_data, test_labels = np.load(\"test_data.npy\"), np.load(\"test_labels.npy\")\n+\n+print(f\"Frame features in train set: {train_data.shape}\")\n+\n+\"\"\"\n+## Building the Transformer-based model\n+\n+We will be building on top of the code shared in\n+[this book chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11) of\n+[Deep Learning with Python (Second ed.)](https://www.manning.com/books/deep-learning-with-python)\n+by François Chollet.\n+\n+First, self-attention layers that form the basic blocks of a Transformer are\n+order-agnostic. Since videos are ordered sequences of frames, we need our\n+Transformer model to take into account order information.\n+We do this via **positional encoding**.\n+We simply embed the positions of the frames present inside videos with an\n+[`Embedding` layer](https://keras.io/api/layers/core_layers/embedding). We then\n+add these positional embeddings to the precomputed CNN feature maps.\n+\"\"\"\n+\n+\n+class PositionalEmbedding(layers.Layer):\n+    def __init__(self, sequence_length, output_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.position_embeddings = layers.Embedding(\n+            input_dim=sequence_length, output_dim=output_dim\n+        )\n+        self.sequence_length = sequence_length\n+        self.output_dim = output_dim\n+\n+    def call(self, inputs):\n+        # The inputs are of shape: `(batch_size, frames, num_features)`\n+        inputs = keras.backend.cast(inputs, self.compute_dtype)\n+        length = keras.backend.shape(inputs)[1]\n+        positions = keras.ops.numpy.arange(start=0, stop=length, step=1)\n+        embedded_positions = self.position_embeddings(positions)\n+        return inputs + embedded_positions\n+\n+\n+\"\"\"\n+Now, we can create a subclassed layer for the Transformer.\n+\"\"\"\n+\n+\n+class TransformerEncoder(layers.Layer):\n+    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n+        super().__init__(**kwargs)\n+        self.embed_dim = embed_dim\n+        self.dense_dim = dense_dim\n+        self.num_heads = num_heads\n+        self.attention = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim, dropout=0.3\n+        )\n+        self.dense_proj = keras.Sequential(\n+            [\n+                layers.Dense(dense_dim, activation=keras.activations.gelu),\n+                layers.Dense(embed_dim),\n+            ]\n+        )\n+        self.layernorm_1 = layers.LayerNormalization()\n+        self.layernorm_2 = layers.LayerNormalization()\n+\n+    def call(self, inputs, mask=None):\n+        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n+        proj_input = self.layernorm_1(inputs + attention_output)\n+        proj_output = self.dense_proj(proj_input)\n+        return self.layernorm_2(proj_input + proj_output)\n+\n+\n+\"\"\"\n+## Utility functions for training\n+\"\"\"\n+\n+\n+def get_compiled_model(shape):\n+    sequence_length = MAX_SEQ_LENGTH\n+    embed_dim = NUM_FEATURES\n+    dense_dim = 4\n+    num_heads = 1\n+    classes = len(label_processor.get_vocabulary())\n+\n+    inputs = keras.Input(shape=shape)\n+    x = PositionalEmbedding(\n+        sequence_length, embed_dim, name=\"frame_position_embedding\"\n+    )(inputs)\n+    x = TransformerEncoder(\n+        embed_dim, dense_dim, num_heads, name=\"transformer_layer\"\n+    )(x)\n+    x = layers.GlobalMaxPooling1D()(x)\n+    x = layers.Dropout(0.5)(x)\n+    outputs = layers.Dense(classes, activation=\"softmax\")(x)\n+    model = keras.Model(inputs, outputs)\n+\n+    model.compile(\n+        optimizer=\"adam\",\n+        loss=\"sparse_categorical_crossentropy\",\n+        metrics=[\"accuracy\"],\n+    )\n+    return model\n+\n+\n+def run_experiment():\n+    filepath = \"/tmp/video_classifier.weights.h5\"\n+    checkpoint = keras.callbacks.ModelCheckpoint(\n+        filepath, save_weights_only=True, save_best_only=True, verbose=1\n+    )\n+\n+    model = get_compiled_model(train_data.shape[1:])\n+    history = model.fit(\n+        train_data,\n+        train_labels,\n+        validation_split=0.15,\n+        epochs=EPOCHS,\n+        callbacks=[checkpoint],\n+    )\n+\n+    model.load_weights(filepath)\n+    _, accuracy = model.evaluate(test_data, test_labels)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+\n+    return model\n+\n+\n+\"\"\"\n+## Model training and inference\n+\"\"\"\n+\n+trained_model = run_experiment()\n+\n+\"\"\"\n+**Note**: This model has ~4.23 Million parameters, which is way more than the sequence\n+model (99918 parameters) we used in the prequel of this example.  This kind of\n+Transformer model works best with a larger dataset and a longer pre-training schedule.\n+\"\"\"\n+\n+\n+def prepare_single_video(frames):\n+    frame_features = np.zeros(\n+        shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n+    )\n+\n+    # Pad shorter videos.\n+    if len(frames) < MAX_SEQ_LENGTH:\n+        diff = MAX_SEQ_LENGTH - len(frames)\n+        padding = np.zeros((diff, IMG_SIZE, IMG_SIZE, 3))\n+        frames = np.concatenate(frames, padding)\n+\n+    frames = frames[None, ...]\n+\n+    # Extract features from the frames of the current video.\n+    for i, batch in enumerate(frames):\n+        video_length = batch.shape[0]\n+        length = min(MAX_SEQ_LENGTH, video_length)\n+        for j in range(length):\n+            if np.mean(batch[j, :]) > 0.0:\n+                frame_features[i, j, :] = feature_extractor.predict(\n+                    batch[None, j, :]\n+                )\n+            else:\n+                frame_features[i, j, :] = 0.0\n+\n+    return frame_features\n+\n+\n+def predict_action(path):\n+    class_vocab = label_processor.get_vocabulary()\n+\n+    frames = load_video(os.path.join(\"test\", path), offload_to_cpu=True)\n+    frame_features = prepare_single_video(frames)\n+    probabilities = trained_model.predict(frame_features)[0]\n+\n+    plot_x_axis, plot_y_axis = [], []\n+\n+    for i in np.argsort(probabilities)[::-1]:\n+        plot_x_axis.append(class_vocab[i])\n+        plot_y_axis.append(probabilities[i])\n+        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n+\n+    plt.bar(plot_x_axis, plot_y_axis, label=plot_x_axis)\n+    plt.xlabel(\"class_label\")\n+    plt.xlabel(\"Probability\")\n+    plt.show()\n+\n+    return frames\n+\n+\n+# This utility is for visualization.\n+# Referenced from:\n+# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n+def to_gif(images):\n+    converted_images = images.astype(np.uint8)\n+    imageio.mimsave(\"animation.gif\", converted_images, fps=10)\n+    return embed.embed_file(\"animation.gif\")\n+\n+\n+test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n+print(f\"Test video path: {test_video}\")\n+test_frames = predict_action(test_video)\n+to_gif(test_frames[:MAX_SEQ_LENGTH])\n+\n+\"\"\"\n+The performance of our model is far from optimal, because it was trained on a\n+small dataset.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07f2383708f271a2c86f4e8bfcfdca91e6aa4eea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 140 | Churn Cumulative: 2490 | Contributors (this commit): 12 | Commits (past 90d): 51 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,14 @@\n-import numpy as np\n-\n from keras_core import backend\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras_core.utils import image_utils\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,41 +52,121 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer RandomCrop requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n-            )\n-\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.seed_generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.numpy.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            input_height, input_width = (\n+                input_shape[self.height_axis],\n+                input_shape[self.width_axis],\n+            )\n+\n+            h_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_height - self.height + 1),\n+                    dtype=inputs.dtype,\n+                    seed=self.seed_generator,\n+                ),\n+                \"int32\",\n+            )\n+            w_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_width - self.width + 1),\n+                    dtype=inputs.dtype,\n+                    seed=self.seed_generator,\n+                ),\n+                \"int32\",\n+            )\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n+        outputs = (\n+            self.backend.numpy.squeeze(outputs, axis=0)\n+            if not is_batched\n+            else outputs\n+        )\n+\n+        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d823be2a619f57f68da8c6202800c0ce15c07a9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 153/77 | Churn Δ: 50 | Churn Cumulative: 4170 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -135,7 +135,6 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n         self._run(fn, 20)\n \n     def benchmark_layers_embeddings_embedding_overhead(self):\n-\n         layer = tf.keras.layers.Embedding(1, 1)\n         x = tf.zeros((1, 1), dtype=\"int32\")\n \n@@ -148,7 +147,6 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n class KerasLayerCallOverheadBenchmarks(\n     MicroBenchmarksBase, metaclass=tf.__internal__.test.ParameterizedBenchmark\n ):\n-\n     # The set of layers for benchmarking. To add benchmarks for new layers,\n     # please add the parameter configs to \"_benchmark_paramters\".\n \n@@ -225,7 +223,6 @@ class KerasLayerCallOverheadBenchmarks(\n     ]\n \n     def benchmark_layer(self, layer, input_shape, kwargs=None):\n-\n         x = tf.ones(input_shape)\n \n         def fn():\n\n@@ -261,7 +261,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         layer = MyLayer()\n         outputs = layer(inputs)\n         model = Model(inputs, outputs)\n-        self.assertEqual(len(model.losses), 1)\n+        self.assertLen(model.losses, 1)\n         model.compile(\"sgd\", \"mse\", run_eagerly=test_utils.should_run_eagerly())\n         loss = model.train_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n         self.assertEqual(loss, 2 * 3)\n@@ -373,7 +373,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         m = Sequential([shared_layer])\n         m2 = Sequential([shared_layer, m])\n         m2(tf.constant([1, 2, 3]))\n-        self.assertEqual(len(m2.losses), 2)\n+        self.assertLen(m2.losses, 2)\n         self.assertAllClose(m2.losses, [6, 12])\n \n     @test_combinations.run_all_keras_modes\n@@ -394,23 +394,23 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         x1 = tf.ones((1, 1))\n         _ = l(x1)\n         if not tf.executing_eagerly():\n-            self.assertEqual(len(l.get_losses_for(x1)), 2)\n-            self.assertEqual(len(l.get_losses_for(None)), 1)\n+            self.assertLen(l.get_losses_for(x1), 2)\n+            self.assertLen(l.get_losses_for(None), 1)\n \n         x2 = tf.ones((1, 1))\n         _ = l(x2)\n         if not tf.executing_eagerly():\n-            self.assertEqual(len(l.get_losses_for(x1)), 2)\n-            self.assertEqual(len(l.get_losses_for(x2)), 2)\n-            self.assertEqual(len(l.get_losses_for(None)), 1)\n+            self.assertLen(l.get_losses_for(x1), 2)\n+            self.assertLen(l.get_losses_for(x2), 2)\n+            self.assertLen(l.get_losses_for(None), 1)\n \n         outputs = l(inputs)\n         model = Model(inputs, outputs)\n         if not tf.executing_eagerly():\n-            self.assertEqual(len(model.losses), 7)\n-            self.assertEqual(len(l.get_losses_for(x1)), 2)\n-            self.assertEqual(len(l.get_losses_for(x2)), 2)\n-            self.assertEqual(len(l.get_losses_for(None)), 1)\n+            self.assertLen(model.losses, 7)\n+            self.assertLen(l.get_losses_for(x1), 2)\n+            self.assertLen(l.get_losses_for(x2), 2)\n+            self.assertLen(l.get_losses_for(None), 1)\n \n         x3 = tf.ones((1, 1))\n         model(x3)\n@@ -418,12 +418,12 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n         model(x4)\n         if tf.executing_eagerly():\n             # Eager losses are cleared every `__call__`.\n-            self.assertEqual(len(model.losses), 3)\n+            self.assertLen(model.losses, 3)\n         else:\n-            self.assertEqual(len(model.losses), 11)\n-            self.assertEqual(len(model.get_losses_for(x3)), 2)\n-            self.assertEqual(len(model.get_losses_for(x4)), 2)\n-            self.assertEqual(len(model.get_losses_for(None)), 1)\n+            self.assertLen(model.losses, 11)\n+            self.assertLen(l.get_losses_for(x3), 2)\n+            self.assertLen(l.get_losses_for(x4), 2)\n+            self.assertLen(l.get_losses_for(None), 1)\n \n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_invalid_constant_input(self):\n\n@@ -121,10 +121,12 @@ class ModelSubclassingTest(test_combinations.TestCase):\n             model.weights,\n             \"Model should have no weights since it has not been built.\",\n         )\n-        with self.assertRaisesRegex(\n-            ValueError, \"input shape is not one of the valid types\"\n-        ):\n-            model.build(input_shape=tf.compat.v1.Dimension(input_dim))\n+        self.assertRaises(\n+            ValueError,\n+            model.build,\n+            input_shape=tf.compat.v1.Dimension(input_dim),\n+            msg=\"input shape is not one of the valid types\",\n+        )\n \n     def test_embed_dtype_with_subclass_build(self):\n         class Embedding(keras.layers.Layer):\n@@ -512,9 +514,9 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         model(x)\n \n         if tf.executing_eagerly():\n-            self.assertEqual(0, len(model.updates))\n+            self.assertLen(model.updates, 0)\n         else:\n-            self.assertEqual(2, len(model.updates))\n+            self.assertLen(model.updates, 2)\n \n \n class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n@@ -557,7 +559,6 @@ class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n             _ = model.evaluate(steps=10, verbose=0)\n \n     def test_updates_and_losses_for_nested_models_in_subclassed_model(self):\n-\n         # Case 1: deferred-build sequential nested in subclass.\n         class TestModel1(keras.Model):\n             def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ddeff03e7322fc79ead78ded0156fb1b352233b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 2289 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -121,12 +121,10 @@ class ModelSubclassingTest(test_combinations.TestCase):\n             model.weights,\n             \"Model should have no weights since it has not been built.\",\n         )\n-        self.assertRaises(\n-            ValueError,\n-            model.build,\n-            input_shape=tf.compat.v1.Dimension(input_dim),\n-            msg=\"input shape is not one of the valid types\",\n-        )\n+        with self.assertRaisesRegex(\n+            ValueError, \"input shape is not one of the valid types\"\n+        ):\n+            model.build(input_shape=tf.compat.v1.Dimension(input_dim))\n \n     def test_embed_dtype_with_subclass_build(self):\n         class Embedding(keras.layers.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f1c152afbff108c086248860e5aecff5923199b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 409 | Lines Deleted: 110 | Files Changed: 2 | Hunks: 12 | Methods Changed: 15 | Complexity Δ (Sum/Max): 16/21 | Churn Δ: 519 | Churn Cumulative: 3009 | Contributors (this commit): 13 | Commits (past 90d): 53 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,379 @@\n+\"\"\"\n+Title: CutMix data augmentation for image classification\n+Author: [Sayan Nath](https://twitter.com/sayannath2350)\n+Converted to Keras Core By: [Piyush Thakur](https://github.com/cosmo3769)\n+Date created: 2021/06/08\n+Last modified: 2023/07/18\n+Description: Data augmentation with CutMix for image classification on CIFAR-10.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\"\"\"\n+\n+\"\"\"\n+_CutMix_ is a data augmentation technique that addresses the issue of information loss\n+and inefficiency present in regional dropout strategies.\n+Instead of removing pixels and filling them with black or grey pixels or Gaussian noise,\n+you replace the removed regions with a patch from another image,\n+while the ground truth labels are mixed proportionally to the number of pixels of combined images.\n+CutMix was proposed in\n+[CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features](https://arxiv.org/abs/1905.04899)\n+(Yun et al., 2019)\n+\n+It's implemented via the following formulas:\n+\n+<img src=\"https://i.imgur.com/cGvd13V.png\" width=\"200\"/>\n+\n+where `M` is the binary mask which indicates the cutout and the fill-in\n+regions from the two randomly drawn images and `λ` (in `[0, 1]`) is drawn from a\n+[`Beta(α, α)` distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n+\n+The coordinates of bounding boxes are:\n+\n+<img src=\"https://i.imgur.com/eNisep4.png\" width=\"150\"/>\n+\n+which indicates the cutout and fill-in regions in case of the images.\n+The bounding box sampling is represented by:\n+\n+<img src=\"https://i.imgur.com/Snph9aj.png\" width=\"200\"/>\n+\n+where `rx, ry` are randomly drawn from a uniform distribution with upper bound.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import pandas as pd\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+from keras_core import layers\n+import keras_core as keras\n+\n+keras.utils.set_random_seed(42)\n+\n+\"\"\"\n+## Load the CIFAR-10 dataset\n+\n+In this example, we will use the\n+[CIFAR-10 image classification dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n+\"\"\"\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+y_train = keras.utils.to_categorical(y_train, num_classes=10)\n+y_test = keras.utils.to_categorical(y_test, num_classes=10)\n+\n+print(x_train.shape)\n+print(y_train.shape)\n+print(x_test.shape)\n+print(y_test.shape)\n+\n+class_names = [\n+    \"Airplane\",\n+    \"Automobile\",\n+    \"Bird\",\n+    \"Cat\",\n+    \"Deer\",\n+    \"Dog\",\n+    \"Frog\",\n+    \"Horse\",\n+    \"Ship\",\n+    \"Truck\",\n+]\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+AUTO = tf.data.AUTOTUNE\n+BATCH_SIZE = 32\n+IMG_SIZE = 32\n+\n+\"\"\"\n+## Define the image preprocessing function\n+\"\"\"\n+\n+\n+def preprocess_image(image, label):\n+    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n+    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0\n+    label = tf.cast(label, tf.float32)\n+    return image, label\n+\n+\n+\"\"\"\n+## Convert the data into TensorFlow `Dataset` objects\n+\"\"\"\n+\n+train_ds_one = (\n+    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+    .shuffle(1024)\n+    .map(preprocess_image, num_parallel_calls=AUTO)\n+)\n+train_ds_two = (\n+    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+    .shuffle(1024)\n+    .map(preprocess_image, num_parallel_calls=AUTO)\n+)\n+\n+train_ds_simple = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+\n+test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+\n+train_ds_simple = (\n+    train_ds_simple.map(preprocess_image, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+# Combine two shuffled datasets from the same training data.\n+train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n+\n+test_ds = (\n+    test_ds.map(preprocess_image, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+\"\"\"\n+## Define the CutMix data augmentation function\n+\n+The CutMix function takes two `image` and `label` pairs to perform the augmentation. It samples `λ(l)` from the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) and returns a bounding box from `get_box` function. We then crop the second image (`image2`) and pad this image in the final padded image at the same location.\n+\"\"\"\n+\n+\n+def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n+    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n+    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n+    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n+\n+\n+@tf.function\n+def get_box(lambda_value):\n+    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n+\n+    cut_w = IMG_SIZE * cut_rat  # rw\n+    cut_w = tf.cast(cut_w, tf.int32)\n+\n+    cut_h = IMG_SIZE * cut_rat  # rh\n+    cut_h = tf.cast(cut_h, tf.int32)\n+\n+    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n+    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n+\n+    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n+    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n+    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n+    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n+\n+    target_h = bby2 - boundaryy1\n+    if target_h == 0:\n+        target_h += 1\n+\n+    target_w = bbx2 - boundaryx1\n+    if target_w == 0:\n+        target_w += 1\n+\n+    return boundaryx1, boundaryy1, target_h, target_w\n+\n+\n+@tf.function\n+def cutmix(train_ds_one, train_ds_two):\n+    (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n+\n+    alpha = [0.25]\n+    beta = [0.25]\n+\n+    # Get a sample from the Beta distribution\n+    lambda_value = sample_beta_distribution(1, alpha, beta)\n+\n+    # Define Lambda\n+    lambda_value = lambda_value[0][0]\n+\n+    # Get the bounding box offsets, heights and widths\n+    boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n+\n+    # Get a patch from the second image (`image2`)\n+    crop2 = tf.image.crop_to_bounding_box(\n+        image2, boundaryy1, boundaryx1, target_h, target_w\n+    )\n+    # Pad the `image2` patch (`crop2`) with the same offset\n+    image2 = tf.image.pad_to_bounding_box(\n+        crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n+    )\n+    # Get a patch from the first image (`image1`)\n+    crop1 = tf.image.crop_to_bounding_box(\n+        image1, boundaryy1, boundaryx1, target_h, target_w\n+    )\n+    # Pad the `image1` patch (`crop1`) with the same offset\n+    img1 = tf.image.pad_to_bounding_box(\n+        crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n+    )\n+\n+    # Modify the first image by subtracting the patch from `image1`\n+    # (before applying the `image2` patch)\n+    image1 = image1 - img1\n+    # Add the modified `image1` and `image2`  together to get the CutMix image\n+    image = image1 + image2\n+\n+    # Adjust Lambda in accordance to the pixel ration\n+    lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n+    lambda_value = tf.cast(lambda_value, tf.float32)\n+\n+    # Combine the labels of both images\n+    label = lambda_value * label1 + (1 - lambda_value) * label2\n+    return image, label\n+\n+\n+\"\"\"\n+**Note**: we are combining two images to create a single one.\n+\n+## Visualize the new dataset after applying the CutMix augmentation\n+\"\"\"\n+\n+# Create the new dataset using our `cutmix` utility\n+train_ds_cmu = (\n+    train_ds.shuffle(1024)\n+    .map(cutmix, num_parallel_calls=AUTO)\n+    .batch(BATCH_SIZE)\n+    .prefetch(AUTO)\n+)\n+\n+# Let's preview 9 samples from the dataset\n+image_batch, label_batch = next(iter(train_ds_cmu))\n+plt.figure(figsize=(10, 10))\n+for i in range(9):\n+    ax = plt.subplot(3, 3, i + 1)\n+    plt.title(class_names[np.argmax(label_batch[i])])\n+    plt.imshow(image_batch[i])\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Define a ResNet-20 model\n+\"\"\"\n+\n+\n+def resnet_layer(\n+    inputs,\n+    num_filters=16,\n+    kernel_size=3,\n+    strides=1,\n+    activation=\"relu\",\n+    batch_normalization=True,\n+    conv_first=True,\n+):\n+    conv = layers.Conv2D(\n+        num_filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=\"same\",\n+        kernel_initializer=\"he_normal\",\n+        kernel_regularizer=keras.regularizers.L2(1e-4),\n+    )\n+    x = inputs\n+    if conv_first:\n+        x = conv(x)\n+        if batch_normalization:\n+            x = layers.BatchNormalization()(x)\n+        if activation is not None:\n+            x = layers.Activation(activation)(x)\n+    else:\n+        if batch_normalization:\n+            x = layers.BatchNormalization()(x)\n+        if activation is not None:\n+            x = layers.Activation(activation)(x)\n+        x = conv(x)\n+    return x\n+\n+\n+def resnet_v20(input_shape, depth, num_classes=10):\n+    if (depth - 2) % 6 != 0:\n+        raise ValueError(\"depth should be 6n+2 (eg 20, 32, 44 in [a])\")\n+    # Start model definition.\n+    num_filters = 16\n+    num_res_blocks = int((depth - 2) / 6)\n+\n+    inputs = layers.Input(shape=input_shape)\n+    x = resnet_layer(inputs=inputs)\n+    # Instantiate the stack of residual units\n+    for stack in range(3):\n+        for res_block in range(num_res_blocks):\n+            strides = 1\n+            if stack > 0 and res_block == 0:  # first layer but not first stack\n+                strides = 2  # downsample\n+            y = resnet_layer(inputs=x, num_filters=num_filters, strides=strides)\n+            y = resnet_layer(inputs=y, num_filters=num_filters, activation=None)\n+            if stack > 0 and res_block == 0:  # first layer but not first stack\n+                # linear projection residual shortcut connection to match\n+                # changed dims\n+                x = resnet_layer(\n+                    inputs=x,\n+                    num_filters=num_filters,\n+                    kernel_size=1,\n+                    strides=strides,\n+                    activation=None,\n+                    batch_normalization=False,\n+                )\n+            x = layers.add([x, y])\n+            x = layers.Activation(\"relu\")(x)\n+        num_filters *= 2\n+\n+    # Add classifier on top.\n+    # v1 does not use BN after last shortcut connection-ReLU\n+    x = layers.AveragePooling2D(pool_size=8)(x)\n+    y = layers.Flatten()(x)\n+    outputs = layers.Dense(\n+        num_classes, activation=\"softmax\", kernel_initializer=\"he_normal\"\n+    )(y)\n+\n+    # Instantiate model.\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+def training_model():\n+    return resnet_v20((32, 32, 3), 20)\n+\n+\n+initial_model = training_model()\n+initial_model.save_weights(\"initial_weights.weights.h5\")\n+\n+\"\"\"\n+## Train the model with the dataset augmented by CutMix\n+\"\"\"\n+\n+model = training_model()\n+model.load_weights(\"initial_weights.weights.h5\")\n+\n+model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+model.fit(train_ds_cmu, validation_data=test_ds, epochs=15)\n+\n+test_loss, test_accuracy = model.evaluate(test_ds)\n+print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n+\n+\"\"\"\n+## Train the model using the original non-augmented dataset\n+\"\"\"\n+\n+model = training_model()\n+model.load_weights(\"initial_weights.weights.h5\")\n+model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+model.fit(train_ds_simple, validation_data=test_ds, epochs=15)\n+\n+test_loss, test_accuracy = model.evaluate(test_ds)\n+print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n+\n+\"\"\"\n+## Notes\n+\n+In this example, we trained our model for 15 epochs.\n+In our experiment, the model with CutMix achieves a better accuracy on the CIFAR-10 dataset\n+(76.92% in our experiment) compared to the model that doesn't use the augmentation (72.23%).\n+You may notice it takes less time to train the model with the CutMix augmentation.\n+\n+You can experiment further with the CutMix technique by following the\n+[original paper](https://arxiv.org/abs/1905.04899).\n+\"\"\"\n\\ No newline at end of file\n\n@@ -1,14 +1,14 @@\n+import numpy as np\n+\n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n+from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,121 +52,41 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+        if not tf.available:\n+            raise ImportError(\n+                \"Layer RandomCrop requires TensorFlow. \"\n+                \"Install it via `pip install tensorflow`.\"\n+            )\n+\n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.numpy.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            input_height, input_width = (\n-                input_shape[self.height_axis],\n-                input_shape[self.width_axis],\n-            )\n-\n-            h_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_height - self.height + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                \"int32\",\n-            )\n-            w_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_width - self.width + 1),\n-                    dtype=inputs.dtype,\n-                    seed=self.seed_generator,\n-                ),\n-                \"int32\",\n-            )\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n-        outputs = (\n-            self.backend.numpy.squeeze(outputs, axis=0)\n-            if not is_batched\n-            else outputs\n-        )\n-\n-        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n-            outputs = self.backend.convert_to_tensor(outputs)\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n+        if (\n+            backend.backend() != \"tensorflow\"\n+            and not backend_utils.in_tf_graph()\n+        ):\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
