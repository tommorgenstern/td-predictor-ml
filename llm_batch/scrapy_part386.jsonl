{"custom_id": "scrapy#ceedb026f8c8ccb049187baa14202f98b2a3a60c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 23 | Files Changed: 3 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 30 | Churn Cumulative: 3037 | Contributors (this commit): 28 | Commits (past 90d): 6 | Contributors (cumulative): 42 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,21 +10,11 @@ import binascii\n import logging\n import os\n import pprint\n-import traceback\n from typing import TYPE_CHECKING, Any, Dict, List\n \n from twisted.internet import protocol\n from twisted.internet.tcp import Port\n \n-try:\n-    from twisted.conch import manhole, telnet\n-    from twisted.conch.insults import insults\n-\n-    TWISTED_CONCH_AVAILABLE = True\n-except (ImportError, SyntaxError):\n-    _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n-    TWISTED_CONCH_AVAILABLE = False\n-\n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.decorators import defers\n@@ -33,6 +23,8 @@ from scrapy.utils.reactor import listen_tcp\n from scrapy.utils.trackref import print_live_refs\n \n if TYPE_CHECKING:\n+    from twisted.conch import telnet\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -50,11 +42,7 @@ class TelnetConsole(protocol.ServerFactory):\n     def __init__(self, crawler: Crawler):\n         if not crawler.settings.getbool(\"TELNETCONSOLE_ENABLED\"):\n             raise NotConfigured\n-        if not TWISTED_CONCH_AVAILABLE:\n-            raise NotConfigured(\n-                \"TELNETCONSOLE_ENABLED setting is True but required twisted \"\n-                \"modules failed to import:\\n\" + _TWISTED_CONCH_TRACEBACK\n-            )\n+\n         self.crawler: Crawler = crawler\n         self.noisy: bool = False\n         self.portrange: List[int] = [\n@@ -88,6 +76,10 @@ class TelnetConsole(protocol.ServerFactory):\n         self.port.stopListening()\n \n     def protocol(self) -> telnet.TelnetTransport:  # type: ignore[override]\n+        # these import twisted.internet.reactor\n+        from twisted.conch import manhole, telnet\n+        from twisted.conch.insults import insults\n+\n         class Portal:\n             \"\"\"An implementation of IPortal\"\"\"\n \n\n@@ -21,7 +21,6 @@ import scrapy\n from scrapy import Spider\n from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.extensions import telnet\n from scrapy.extensions.throttle import AutoThrottle\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n@@ -482,7 +481,6 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n                 \"LOG_FILE\": str(log_file),\n                 # settings to avoid extra warnings\n                 \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n-                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n         configure_logging()\n@@ -516,8 +514,6 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n             custom_settings = {\n                 \"LOG_FILE\": str(log_file),\n                 \"LOG_FILE_APPEND\": False,\n-                # disable telnet if not available to avoid an extra warning\n-                \"TELNETCONSOLE_ENABLED\": telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n         configure_logging()\n\n@@ -11,7 +11,6 @@ import pytest\n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n-from scrapy.extensions import telnet\n from scrapy.utils.log import (\n     LogCounterHandler,\n     SpiderLoggerAdapter,\n@@ -70,9 +69,6 @@ class TopLevelFormatterTest(unittest.TestCase):\n class LogCounterHandlerTest(unittest.TestCase):\n     def setUp(self):\n         settings = {\"LOG_LEVEL\": \"WARNING\"}\n-        if not telnet.TWISTED_CONCH_AVAILABLE:\n-            # disable it to avoid the extra warning\n-            settings[\"TELNETCONSOLE_ENABLED\"] = False\n         self.logger = logging.getLogger(\"test\")\n         self.logger.setLevel(logging.NOTSET)\n         self.logger.propagate = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b8604bb5d8bffbbd5c78783022965fa2606c131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 38 | Churn Cumulative: 319 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: 0.5757575757575758\n\nDIFF:\n@@ -12,6 +12,7 @@ from typing import TYPE_CHECKING, Any, DefaultDict, Dict\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n+from scrapy.signalmanager import dispatcher\n \n if TYPE_CHECKING:\n     from twisted.python.failure import Failure\n@@ -36,6 +37,9 @@ class CloseSpider:\n             \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n             \"errorcount\": crawler.settings.getint(\"CLOSESPIDER_ERRORCOUNT\"),\n             \"timeout_no_item\": crawler.settings.getint(\"CLOSESPIDER_TIMEOUT_NO_ITEM\"),\n+            \"pagecount_no_item\": crawler.settings.getint(\n+                \"CLOSESPIDER_PAGECOUNT_NO_ITEM\"\n+            ),\n         }\n \n         if not any(self.close_on.values()):\n@@ -60,6 +64,19 @@ class CloseSpider:\n             crawler.signals.connect(\n                 self.item_scraped_no_item, signal=signals.item_scraped\n             )\n+        if self.close_on.get(\"pagecount_no_item\"):\n+            if self.page_count not in dispatcher.getReceivers(\n+                signal=signals.response_received\n+            ):\n+                crawler.signals.connect(\n+                    self.page_count, signal=signals.response_received\n+                )\n+\n+            if self.item_scraped not in dispatcher.getReceivers(\n+                signal=signals.item_scraped\n+            ):\n+                crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n+\n         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n     @classmethod\n@@ -74,9 +91,16 @@ class CloseSpider:\n \n     def page_count(self, response: Response, request: Request, spider: Spider) -> None:\n         self.counter[\"pagecount\"] += 1\n+        self.counter[\"pagecount_since_last_item\"] += 1\n         if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n             assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_pagecount\")\n+        if self.close_on[\"pagecount_no_item\"] and (\n+            self.counter[\"pagecount_since_last_item\"]\n+            >= self.close_on[\"pagecount_no_item\"]\n+        ):\n+            assert self.crawler.engine\n+            self.crawler.engine.close_spider(spider, \"closespider_pagecount_no_item\")\n \n     def spider_opened(self, spider: Spider) -> None:\n         from twisted.internet import reactor\n@@ -91,6 +115,7 @@ class CloseSpider:\n \n     def item_scraped(self, item: Any, spider: Spider) -> None:\n         self.counter[\"itemcount\"] += 1\n+        self.counter[\"pagecount_since_last_item\"] = 0\n         if self.counter[\"itemcount\"] == self.close_on[\"itemcount\"]:\n             assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_itemcount\")\n\n@@ -34,6 +34,19 @@ class TestCloseSpider(TestCase):\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n         self.assertTrue(pagecount >= close_on)\n \n+    @defer.inlineCallbacks\n+    def test_closespider_pagecount_no_item(self):\n+        close_on = 5\n+        crawler = get_crawler(\n+            FollowAllSpider,\n+            {\"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on},\n+        )\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_pagecount_no_item\")\n+        pagecount = crawler.stats.get_value(\"response_received_count\")\n+        self.assertTrue(pagecount >= close_on)\n+\n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n         close_on = 5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6352f9f66f655f11332fe6c52ed71ebb2e55bf4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/5 | Churn Δ: 59 | Churn Cumulative: 1187 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,6 @@ from typing import TYPE_CHECKING, Any, DefaultDict, Dict\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n-from scrapy.signalmanager import dispatcher\n \n if TYPE_CHECKING:\n     from twisted.python.failure import Failure\n@@ -49,11 +48,11 @@ class CloseSpider:\n \n         if self.close_on.get(\"errorcount\"):\n             crawler.signals.connect(self.error_count, signal=signals.spider_error)\n-        if self.close_on.get(\"pagecount\"):\n+        if self.close_on.get(\"pagecount\") or self.close_on.get(\"pagecount_no_item\"):\n             crawler.signals.connect(self.page_count, signal=signals.response_received)\n         if self.close_on.get(\"timeout\"):\n             crawler.signals.connect(self.spider_opened, signal=signals.spider_opened)\n-        if self.close_on.get(\"itemcount\"):\n+        if self.close_on.get(\"itemcount\") or self.close_on.get(\"pagecount_no_item\"):\n             crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n         if self.close_on.get(\"timeout_no_item\"):\n             self.timeout_no_item: int = self.close_on[\"timeout_no_item\"]\n@@ -64,18 +63,6 @@ class CloseSpider:\n             crawler.signals.connect(\n                 self.item_scraped_no_item, signal=signals.item_scraped\n             )\n-        if self.close_on.get(\"pagecount_no_item\"):\n-            if self.page_count not in dispatcher.getReceivers(\n-                signal=signals.response_received\n-            ):\n-                crawler.signals.connect(\n-                    self.page_count, signal=signals.response_received\n-                )\n-\n-            if self.item_scraped not in dispatcher.getReceivers(\n-                signal=signals.item_scraped\n-            ):\n-                crawler.signals.connect(self.item_scraped, signal=signals.item_scraped)\n \n         crawler.signals.connect(self.spider_closed, signal=signals.spider_closed)\n \n@@ -95,6 +82,7 @@ class CloseSpider:\n         if self.counter[\"pagecount\"] == self.close_on[\"pagecount\"]:\n             assert self.crawler.engine\n             self.crawler.engine.close_spider(spider, \"closespider_pagecount\")\n+            return\n         if self.close_on[\"pagecount_no_item\"] and (\n             self.counter[\"pagecount_since_last_item\"]\n             >= self.close_on[\"pagecount_no_item\"]\n\n@@ -283,6 +283,23 @@ class ItemSpider(FollowAllSpider):\n             yield {}\n \n \n+class MaxItemsSpider(ItemSpider):\n+    def __init__(self, max_items=10, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.max_items = max_items\n+        self.items_scraped = 0\n+\n+    def parse(self, response):\n+        for item_or_req in super().parse(response):\n+            if isinstance(item_or_req, Request):\n+                yield item_or_req\n+            else:\n+                if self.items_scraped >= self.max_items:\n+                    continue\n+                self.items_scraped += 1\n+                yield item_or_req\n+\n+\n class DefaultError(Exception):\n     pass\n \n\n@@ -3,7 +3,13 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-from tests.spiders import ErrorSpider, FollowAllSpider, ItemSpider, SlowSpider\n+from tests.spiders import (\n+    ErrorSpider,\n+    FollowAllSpider,\n+    ItemSpider,\n+    MaxItemsSpider,\n+    SlowSpider,\n+)\n \n \n class TestCloseSpider(TestCase):\n@@ -37,15 +43,23 @@ class TestCloseSpider(TestCase):\n     @defer.inlineCallbacks\n     def test_closespider_pagecount_no_item(self):\n         close_on = 5\n+        close_on_pagecount = 20\n+        max_items = 5\n         crawler = get_crawler(\n-            FollowAllSpider,\n-            {\"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on},\n+            MaxItemsSpider,\n+            {\n+                \"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on,\n+                \"CLOSESPIDER_PAGECOUNT\": close_on_pagecount,\n+            },\n         )\n-        yield crawler.crawl(mockserver=self.mockserver)\n+        yield crawler.crawl(max_items=max_items, mockserver=self.mockserver)\n         reason = crawler.spider.meta[\"close_reason\"]\n         self.assertEqual(reason, \"closespider_pagecount_no_item\")\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n-        self.assertTrue(pagecount >= close_on)\n+        itemcount = crawler.stats.get_value(\"item_scraped_count\")\n+        self.assertEqual(itemcount, max_items)\n+        self.assertLess(pagecount, close_on_pagecount)\n+        self.assertTrue((pagecount - itemcount) >= close_on)\n \n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a40d5281cfb8fdaf7d7edce80d3addbddef897a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 23 | Churn Cumulative: 1002 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -283,21 +283,18 @@ class ItemSpider(FollowAllSpider):\n             yield {}\n \n \n-class MaxItemsSpider(ItemSpider):\n+class MaxItemsKeepCrawlingSpider(FollowAllSpider):\n     def __init__(self, max_items=10, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.max_items = max_items\n         self.items_scraped = 0\n \n     def parse(self, response):\n-        for item_or_req in super().parse(response):\n-            if isinstance(item_or_req, Request):\n-                yield item_or_req\n-            else:\n-                if self.items_scraped >= self.max_items:\n-                    continue\n+        for request in super().parse(response):\n+            yield request\n+            if self.items_scraped < self.max_items:\n+                yield Item()\n                 self.items_scraped += 1\n-                yield item_or_req\n \n \n class DefaultError(Exception):\n\n@@ -7,7 +7,7 @@ from tests.spiders import (\n     ErrorSpider,\n     FollowAllSpider,\n     ItemSpider,\n-    MaxItemsSpider,\n+    MaxItemsKeepCrawlingSpider,\n     SlowSpider,\n )\n \n@@ -43,13 +43,11 @@ class TestCloseSpider(TestCase):\n     @defer.inlineCallbacks\n     def test_closespider_pagecount_no_item(self):\n         close_on = 5\n-        close_on_pagecount = 20\n         max_items = 5\n         crawler = get_crawler(\n-            MaxItemsSpider,\n+            MaxItemsKeepCrawlingSpider,\n             {\n                 \"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on,\n-                \"CLOSESPIDER_PAGECOUNT\": close_on_pagecount,\n             },\n         )\n         yield crawler.crawl(max_items=max_items, mockserver=self.mockserver)\n@@ -58,8 +56,8 @@ class TestCloseSpider(TestCase):\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n         self.assertEqual(itemcount, max_items)\n-        self.assertLess(pagecount, close_on_pagecount)\n-        self.assertTrue((pagecount - itemcount) >= close_on)\n+        self.assertLessEqual(pagecount, close_on + itemcount)\n+        self.assertGreater(pagecount, itemcount)\n \n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f0fad16f5d86134bcf72964f6e453541031eb06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 1023 | Contributors (this commit): 20 | Commits (past 90d): 8 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -283,15 +283,19 @@ class ItemSpider(FollowAllSpider):\n             yield {}\n \n \n-class MaxItemsKeepCrawlingSpider(FollowAllSpider):\n-    def __init__(self, max_items=10, *args, **kwargs):\n+class MaxItemsAndRequestsSpider(FollowAllSpider):\n+    def __init__(self, max_items=10, max_requests=10, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.max_items = max_items\n-        self.items_scraped = 0\n+        self.max_requests = max_requests\n \n     def parse(self, response):\n+        self.items_scraped = 0\n+        self.pages_crawled = 1  # account for the start url\n         for request in super().parse(response):\n+            if self.pages_crawled < self.max_requests:\n                 yield request\n+                self.pages_crawled += 1\n             if self.items_scraped < self.max_items:\n                 yield Item()\n                 self.items_scraped += 1\n\n@@ -7,7 +7,7 @@ from tests.spiders import (\n     ErrorSpider,\n     FollowAllSpider,\n     ItemSpider,\n-    MaxItemsKeepCrawlingSpider,\n+    MaxItemsAndRequestsSpider,\n     SlowSpider,\n )\n \n@@ -44,20 +44,21 @@ class TestCloseSpider(TestCase):\n     def test_closespider_pagecount_no_item(self):\n         close_on = 5\n         max_items = 5\n+        max_requests = close_on + max_items\n         crawler = get_crawler(\n-            MaxItemsKeepCrawlingSpider,\n+            MaxItemsAndRequestsSpider,\n             {\n                 \"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on,\n             },\n         )\n-        yield crawler.crawl(max_items=max_items, mockserver=self.mockserver)\n+        yield crawler.crawl(\n+            max_items=max_items, max_requests=max_requests, mockserver=self.mockserver\n+        )\n         reason = crawler.spider.meta[\"close_reason\"]\n         self.assertEqual(reason, \"closespider_pagecount_no_item\")\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n-        self.assertEqual(itemcount, max_items)\n         self.assertLessEqual(pagecount, close_on + itemcount)\n-        self.assertGreater(pagecount, itemcount)\n \n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cdbcb4f63922f09194fab7d211ba297319b5135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 191 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,6 +60,23 @@ class TestCloseSpider(TestCase):\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n         self.assertLessEqual(pagecount, close_on + itemcount)\n \n+    @defer.inlineCallbacks\n+    def test_closespider_pagecount_no_item_with_pagecount(self):\n+        close_on_pagecount_no_item = 5\n+        close_on_pagecount = 20\n+        crawler = get_crawler(\n+            FollowAllSpider,\n+            {\n+                \"CLOSESPIDER_PAGECOUNT_NO_ITEM\": close_on_pagecount_no_item,\n+                \"CLOSESPIDER_PAGECOUNT\": close_on_pagecount,\n+            },\n+        )\n+        yield crawler.crawl(mockserver=self.mockserver)\n+        reason = crawler.spider.meta[\"close_reason\"]\n+        self.assertEqual(reason, \"closespider_pagecount_no_item\")\n+        pagecount = crawler.stats.get_value(\"response_received_count\")\n+        self.assertLess(pagecount, close_on_pagecount)\n+\n     @defer.inlineCallbacks\n     def test_closespider_errorcount(self):\n         close_on = 5\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5794071f9679c89ef4ee75e8a627274b2464b65b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 2807 | Contributors (this commit): 44 | Commits (past 90d): 17 | Contributors (cumulative): 69 | DMM Complexity: None\n\nDIFF:\n@@ -211,7 +211,7 @@ class MediaPipeline(ABC):\n             # minimize cached information for failure\n             result.cleanFailure()\n             result.frames = []\n-            result.stack = None\n+            result.stack = []\n \n             # This code fixes a memory leak by avoiding to keep references to\n             # the Request and Response objects on the Media Pipeline cache.\n\n@@ -19,6 +19,7 @@ from typing import (\n     Callable,\n     Coroutine,\n     Dict,\n+    Generic,\n     Iterable,\n     Iterator,\n     List,\n@@ -144,7 +145,7 @@ def parallel(\n     return DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n-class _AsyncCooperatorAdapter(Iterator[Deferred]):\n+class _AsyncCooperatorAdapter(Iterator[Deferred], Generic[_T]):\n     \"\"\"A class that wraps an async iterable into a normal iterator suitable\n     for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n     it calls the callable directly in the callback, instead of providing a more\n\n@@ -128,7 +128,7 @@ def configure_logging(\n         settings = Settings(settings)\n \n     if settings.getbool(\"LOG_STDOUT\"):\n-        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))  # type: ignore[assignment]\n+        sys.stdout = StreamLogger(logging.getLogger(\"stdout\"))\n \n     if install_root_handler:\n         install_scrapy_root_handler(settings)\n\n@@ -20,7 +20,7 @@ def x509name_to_string(x509name: X509Name) -> str:\n     # from OpenSSL.crypto.X509Name.__repr__\n     result_buffer: Any = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n     pyOpenSSLutil.lib.X509_NAME_oneline(\n-        x509name._name, result_buffer, len(result_buffer)  # type: ignore[attr-defined]\n+        x509name._name, result_buffer, len(result_buffer)\n     )\n \n     return ffi_buf_to_string(result_buffer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ce0342beb1a5b588f353e52fe03d5e0ec84d938", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 16 | Files Changed: 6 | Hunks: 21 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/2 | Churn Δ: 112 | Churn Cumulative: 9920 | Contributors (this commit): 55 | Commits (past 90d): 19 | Contributors (cumulative): 126 | DMM Complexity: 0.8548387096774194\n\nDIFF:\n@@ -24,6 +24,7 @@ from typing import (\n     cast,\n )\n \n+from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n from twisted.internet.task import LoopingCall\n from twisted.python.failure import Failure\n@@ -194,7 +195,7 @@ class ExecutionEngine:\n \n         if self.slot.start_requests is not None and not self._needs_backout():\n             try:\n-                request = next(self.slot.start_requests)\n+                request_or_item = next(self.slot.start_requests)\n             except StopIteration:\n                 self.slot.start_requests = None\n             except Exception:\n@@ -205,7 +206,16 @@ class ExecutionEngine:\n                     extra={\"spider\": self.spider},\n                 )\n             else:\n-                self.crawl(request)\n+                if isinstance(request_or_item, Request):\n+                    self.crawl(request_or_item)\n+                elif is_item(request_or_item):\n+                    self.scraper.start_itemproc(request_or_item, response=None)\n+                else:\n+                    logger.error(\n+                        f\"Got {request_or_item!r} among start requests. Only \"\n+                        f\"requests and items are supported. It will be \"\n+                        f\"ignored.\"\n+                    )\n \n         if self.spider_is_idle() and self.slot.close_if_idle:\n             self._spider_idle()\n\n@@ -313,15 +313,11 @@ class Scraper:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider\n         \"\"\"\n-        assert self.slot is not None  # typing\n         if isinstance(output, Request):\n             assert self.crawler.engine is not None  # typing\n             self.crawler.engine.crawl(request=output)\n         elif is_item(output):\n-            self.slot.itemproc_size += 1\n-            dfd = self.itemproc.process_item(output, spider)\n-            dfd.addBoth(self._itemproc_finished, output, response, spider)\n-            return dfd\n+            return self.start_itemproc(output, response=response)\n         elif output is None:\n             pass\n         else:\n@@ -333,6 +329,19 @@ class Scraper:\n             )\n         return None\n \n+    def start_itemproc(self, item, *, response: Optional[Response]) -> Deferred[Any]:\n+        \"\"\"Send *item* to the item pipelines for processing.\n+\n+        *response* is the source of the item data. If the item does not come\n+        from response data, e.g. it was hard-coded, set it to ``None``.\n+        \"\"\"\n+        assert self.slot is not None  # typing\n+        assert self.crawler.spider is not None  # typing\n+        self.slot.itemproc_size += 1\n+        dfd = self.itemproc.process_item(item, self.crawler.spider)\n+        dfd.addBoth(self._itemproc_finished, item, response, self.crawler.spider)\n+        return dfd\n+\n     def _log_download_errors(\n         self,\n         spider_failure: Failure,\n@@ -373,7 +382,7 @@ class Scraper:\n         return None\n \n     def _itemproc_finished(\n-        self, output: Any, item: Any, response: Response, spider: Spider\n+        self, output: Any, item: Any, response: Optional[Response], spider: Spider\n     ) -> Deferred[Any]:\n         \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\"\"\"\n         assert self.slot is not None  # typing\n\n@@ -9,6 +9,7 @@ from twisted.python.failure import Failure\n # working around https://github.com/sphinx-doc/sphinx/issues/10400\n from scrapy import Request, Spider  # noqa: TC001\n from scrapy.http import Response  # noqa: TC001\n+from scrapy.utils.python import global_object_name\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n@@ -92,11 +93,13 @@ class LogFormatter:\n         }\n \n     def scraped(\n-        self, item: Any, response: Union[Response, Failure], spider: Spider\n+        self, item: Any, response: Union[Response, Failure, None], spider: Spider\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n         src: Any\n-        if isinstance(response, Failure):\n+        if response is None:\n+            src = f\"{global_object_name(spider.__class__)}.start_requests\"\n+        elif isinstance(response, Failure):\n             src = response.getErrorMessage()\n         else:\n             src = response\n@@ -110,7 +113,11 @@ class LogFormatter:\n         }\n \n     def dropped(\n-        self, item: Any, exception: BaseException, response: Response, spider: Spider\n+        self,\n+        item: Any,\n+        exception: BaseException,\n+        response: Optional[Response],\n+        spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n         return {\n@@ -123,7 +130,11 @@ class LogFormatter:\n         }\n \n     def item_error(\n-        self, item: Any, exception: BaseException, response: Response, spider: Spider\n+        self,\n+        item: Any,\n+        exception: BaseException,\n+        response: Optional[Response],\n+        spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item causes an error while it is passing\n         through the item pipeline.\n\n@@ -346,6 +346,19 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n         yield from super().parse(response)\n \n \n+class StartRequestsItemSpider(FollowAllSpider):\n+    def start_requests(self):\n+        yield {\"name\": \"test item\"}\n+\n+\n+class StartRequestsGoodAndBadOutput(FollowAllSpider):\n+    def start_requests(self):\n+        yield {\"a\": \"a\"}\n+        yield Request(\"data:,a\")\n+        yield \"data:,b\"\n+        yield object()\n+\n+\n class SingleRequestSpider(MetaSpider):\n     seed = None\n     callback_func = None\n\n@@ -1,5 +1,6 @@\n import json\n import logging\n+import re\n import unittest\n from ipaddress import IPv4Address\n from socket import gethostbyname\n@@ -49,6 +50,8 @@ from tests.spiders import (\n     HeadersReceivedErrbackSpider,\n     SimpleSpider,\n     SingleRequestSpider,\n+    StartRequestsGoodAndBadOutput,\n+    StartRequestsItemSpider,\n )\n \n \n@@ -184,6 +187,39 @@ class CrawlTestCase(TestCase):\n         self.assertIsNotNone(record.exc_info)\n         self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n+    @defer.inlineCallbacks\n+    def test_start_requests_items(self):\n+        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n+            crawler = get_crawler(StartRequestsItemSpider)\n+            yield crawler.crawl(mockserver=self.mockserver)\n+\n+        self.assertEqual(len(log.records), 0)\n+\n+    @defer.inlineCallbacks\n+    def test_start_requests_unsupported_output(self):\n+        with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n+            crawler = get_crawler(StartRequestsGoodAndBadOutput)\n+            yield crawler.crawl(mockserver=self.mockserver)\n+\n+        self.assertEqual(len(log.records), 2)\n+        self.assertEqual(\n+            log.records[0].msg,\n+            (\n+                \"Got 'data:,b' among start requests. Only requests and items \"\n+                \"are supported. It will be ignored.\"\n+            ),\n+        )\n+        self.assertTrue(\n+            re.match(\n+                (\n+                    r\"^Got <object object at 0x[0-9a-fA-F]+> among start \"\n+                    r\"requests\\. Only requests and items are supported\\. It \"\n+                    r\"will be ignored\\.$\"\n+                ),\n+                log.records[1].msg,\n+            )\n+        )\n+\n     @defer.inlineCallbacks\n     def test_start_requests_laziness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n\n@@ -1,5 +1,5 @@\n import collections.abc\n-from typing import Optional\n+from typing import Optional, Union\n from unittest import mock\n \n from testfixtures import LogCapture\n@@ -112,7 +112,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n     Should work for process_spider_output and, when it's supported, process_start_requests.\n     \"\"\"\n \n-    ITEM_TYPE: type\n+    ITEM_TYPE: Union[type, tuple]\n     RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects\n \n     @staticmethod\n@@ -328,12 +328,13 @@ class ProcessStartRequestsSimpleMiddleware:\n class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n     \"\"\"process_start_requests tests for simple start_requests\"\"\"\n \n-    ITEM_TYPE = Request\n+    ITEM_TYPE = (Request, dict)\n     MW_SIMPLE = ProcessStartRequestsSimpleMiddleware\n \n     def _start_requests(self):\n-        for i in range(3):\n+        for i in range(2):\n             yield Request(f\"https://example.com/{i}\", dont_filter=True)\n+        yield {\"name\": \"test item\"}\n \n     @defer.inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b85e5a66ede0a2255b335ee4869836e9d30c580a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 94 | Churn Cumulative: 1813 | Contributors (this commit): 72 | Commits (past 90d): 4 | Contributors (cumulative): 95 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,6 +35,20 @@ class CallbackKeywordArgumentsContract(Contract):\n         return args\n \n \n+class MetadataContract(Contract):\n+    \"\"\"Contract to key metadata arguments for the request.\n+    The value should be JSON-encoded dictionary, e.g.:\n+\n+    @meta {\"arg1\": \"some value\"}\n+    \"\"\"\n+\n+    name = \"meta\"\n+\n+    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+        args[\"meta\"] = json.loads(\" \".join(self.args))\n+        return args\n+\n+\n class ReturnsContract(Contract):\n     \"\"\"Contract to check the output of a callback\n \n\n@@ -333,6 +333,7 @@ SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n     \"scrapy.contracts.default.UrlContract\": 1,\n     \"scrapy.contracts.default.CallbackKeywordArgumentsContract\": 1,\n+    \"scrapy.contracts.default.MetadataContract\": 1,\n     \"scrapy.contracts.default.ReturnsContract\": 2,\n     \"scrapy.contracts.default.ScrapesContract\": 3,\n }\n\n@@ -8,6 +8,7 @@ from scrapy import FormRequest\n from scrapy.contracts import Contract, ContractsManager\n from scrapy.contracts.default import (\n     CallbackKeywordArgumentsContract,\n+    MetadataContract,\n     ReturnsContract,\n     ScrapesContract,\n     UrlContract,\n@@ -29,6 +30,10 @@ class ResponseMock:\n     url = \"http://scrapy.org\"\n \n \n+class ResponseMetaMock(ResponseMock):\n+    meta = None\n+\n+\n class CustomSuccessContract(Contract):\n     name = \"custom_success_contract\"\n \n@@ -195,6 +200,33 @@ class TestSpider(Spider):\n         \"\"\"\n         pass\n \n+    def returns_request_meta(self, response):\n+        \"\"\"method which returns request\n+        @url https://example.org\n+        @meta {\"cookiejar\": \"session1\"}\n+        @returns requests 1\n+        \"\"\"\n+        return Request(\n+            \"https://example.org\", meta=response.meta, callback=self.returns_item_meta\n+        )\n+\n+    def returns_item_meta(self, response):\n+        \"\"\"method which returns item\n+        @url http://scrapy.org\n+        @meta {\"key\": \"example\"}\n+        @returns items 1 1\n+        \"\"\"\n+        return TestItem(name=\"example\", url=response.url)\n+\n+    def returns_error_missing_meta(self, response):\n+        \"\"\"method which depends of metadata be defined\n+\n+        @url http://scrapy.org\n+        @returns items 1\n+        \"\"\"\n+        key = response.meta[\"key\"]\n+        yield {key: \"value\"}\n+\n \n class CustomContractSuccessSpider(Spider):\n     name = \"custom_contract_success_spider\"\n@@ -224,6 +256,7 @@ class ContractsManagerTest(unittest.TestCase):\n     contracts = [\n         UrlContract,\n         CallbackKeywordArgumentsContract,\n+        MetadataContract,\n         ReturnsContract,\n         ScrapesContract,\n         CustomFormContract,\n@@ -328,6 +361,52 @@ class ContractsManagerTest(unittest.TestCase):\n         request.callback(response, **request.cb_kwargs)\n         self.should_error()\n \n+    def test_meta(self):\n+        spider = TestSpider()\n+\n+        # extract contracts correctly\n+        contracts = self.conman.extract_contracts(spider.returns_request_meta)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        )\n+\n+        contracts = self.conman.extract_contracts(spider.returns_item_meta)\n+        self.assertEqual(len(contracts), 3)\n+        self.assertEqual(\n+            frozenset(type(x) for x in contracts),\n+            frozenset([UrlContract, MetadataContract, ReturnsContract]),\n+        )\n+\n+        response = ResponseMetaMock()\n+\n+        # returns_request\n+        request = self.conman.from_method(spider.returns_request_meta, self.results)\n+        assert request.meta[\"cookiejar\"] == \"session1\"\n+        response.meta = request.meta\n+        request.callback(response)\n+        assert response.meta[\"cookiejar\"] == \"session1\"\n+        self.should_succeed()\n+\n+        response = ResponseMetaMock()\n+\n+        # returns_item\n+        request = self.conman.from_method(spider.returns_item_meta, self.results)\n+        assert request.meta[\"key\"] == \"example\"\n+        response.meta = request.meta\n+        request.callback(ResponseMetaMock)\n+        assert response.meta[\"key\"] == \"example\"\n+        self.should_succeed()\n+\n+        response = ResponseMetaMock()\n+\n+        request = self.conman.from_method(\n+            spider.returns_error_missing_meta, self.results\n+        )\n+        request.callback(response)\n+        self.should_error()\n+\n     def test_returns(self):\n         spider = TestSpider()\n         response = ResponseMock()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddbdfeb699a2308ca600781b2d1549cbee62725c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 311 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ class CallbackKeywordArgumentsContract(Contract):\n \n \n class MetadataContract(Contract):\n-    \"\"\"Contract to key metadata arguments for the request.\n+    \"\"\"Contract to set metadata arguments for the request.\n     The value should be JSON-encoded dictionary, e.g.:\n \n     @meta {\"arg1\": \"some value\"}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67ab8d4650c1e9212c9508803c7b5265e166cbaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 511 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -116,6 +116,24 @@ class Command(ScrapyCommand):\n             if opts.edit:\n                 self.exitcode = os.system(f'scrapy edit \"{name}\"')  # nosec\n \n+    def _generate_template_variables(\n+        self,\n+        module: str,\n+        name: str,\n+        url: str,\n+        template_name: str,\n+    ):\n+        capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n+        return {\n+            \"project_name\": self.settings.get(\"BOT_NAME\"),\n+            \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n+            \"module\": module,\n+            \"name\": name,\n+            \"url\": url,\n+            \"domain\": extract_domain(url),\n+            \"classname\": f\"{capitalized_module}Spider\",\n+        }\n+\n     def _genspider(\n         self,\n         module: str,\n@@ -125,17 +143,7 @@ class Command(ScrapyCommand):\n         template_file: Union[str, os.PathLike],\n     ) -> None:\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n-        capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n-        domain = extract_domain(url)\n-        tvars = {\n-            \"project_name\": self.settings.get(\"BOT_NAME\"),\n-            \"ProjectName\": string_camelcase(self.settings.get(\"BOT_NAME\")),\n-            \"module\": module,\n-            \"name\": name,\n-            \"url\": url,\n-            \"domain\": domain,\n-            \"classname\": f\"{capitalized_module}Spider\",\n-        }\n+        tvars = self._generate_template_variables(module, name, url, template_name)\n         if self.settings.get(\"NEWSPIDER_MODULE\"):\n             spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n             assert spiders_module.__file__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f260f819e0794708868ed447ae154caa74d965f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 4609 | Contributors (this commit): 39 | Commits (past 90d): 5 | Contributors (cumulative): 57 | DMM Complexity: 0.0\n\nDIFF:\n@@ -39,7 +39,6 @@ from scrapy.settings import Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import build_from_crawler, load_object\n-from scrapy.utils.python import global_object_name\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n@@ -325,10 +324,6 @@ class ExecutionEngine:\n         )\n         for handler, result in request_scheduled_result:\n             if isinstance(result, Failure) and isinstance(result.value, IgnoreRequest):\n-                logger.debug(\n-                    f\"Signal handler {global_object_name(handler)} dropped \"\n-                    f\"request {request} before it reached the scheduler.\"\n-                )\n                 return\n         if not self.slot.scheduler.enqueue_request(request):  # type: ignore[union-attr]\n             self.signals.send_catch_log(\n\n@@ -499,7 +499,6 @@ def test_request_scheduled_signal(caplog):\n     assert scheduler.enqueued == [\n         keep_request\n     ], f\"{scheduler.enqueued!r} != [{keep_request!r}]\"\n-    assert \"dropped request <GET https://drop.example>\" in caplog.text\n     crawler.signals.disconnect(signal_handler, request_scheduled)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e139d22db9f3becc0a7e19e79daa1da3bb65383f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 726 | Contributors (this commit): 22 | Commits (past 90d): 1 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n import functools\n import operator\n import platform\n+import sys\n \n from twisted.trial import unittest\n \n@@ -238,16 +239,18 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertEqual(get_func_args(str.split, stripself=True), [\"sep\", \"maxsplit\"])\n         self.assertEqual(get_func_args(\" \".join, stripself=True), [\"iterable\"])\n \n-        if platform.python_implementation() == \"CPython\":\n-            # This didn't work on older versions of CPython: https://github.com/python/cpython/issues/86951\n+        if sys.version_info >= (3, 13) or platform.python_implementation() == \"PyPy\":\n+            # the correct and correctly extracted signature\n+            self.assertEqual(\n+                get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n+            )\n+        elif platform.python_implementation() == \"CPython\":\n+            # [\"args\", \"kwargs\"] is a correct result for the pre-3.13 incorrect function signature\n+            # [] is an incorrect result on even older CPython (https://github.com/python/cpython/issues/86951)\n             self.assertIn(\n                 get_func_args(operator.itemgetter(2), stripself=True),\n                 [[], [\"args\", \"kwargs\"]],\n             )\n-        elif platform.python_implementation() == \"PyPy\":\n-            self.assertEqual(\n-                get_func_args(operator.itemgetter(2), stripself=True), [\"obj\"]\n-            )\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#46cddc6ecfbe9a0750676143ef789acf6c2e637d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 30 | Churn Cumulative: 752 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -64,7 +64,7 @@ class SpiderLoader:\n             try:\n                 for module in walk_modules(name):\n                     self._load_spiders(module)\n-            except ImportError:\n+            except (ImportError, SyntaxError):\n                 if self.warn_only:\n                     warnings.warn(\n                         f\"\\n{traceback.format_exc()}Could not load spiders \"\n\n@@ -4,6 +4,7 @@ import tempfile\n import warnings\n from pathlib import Path\n from tempfile import mkdtemp\n+from unittest import mock\n \n from twisted.trial import unittest\n from zope.interface.verify import verifyObject\n@@ -136,6 +137,33 @@ class SpiderLoaderTest(unittest.TestCase):\n             spiders = spider_loader.list()\n             self.assertEqual(spiders, [])\n \n+    def test_syntax_error_exception(self):\n+        module = \"tests.test_spiderloader.test_spiders.spider1\"\n+        with mock.patch.object(SpiderLoader, \"_load_spiders\") as m:\n+            m.side_effect = SyntaxError\n+            settings = Settings({\"SPIDER_MODULES\": [module]})\n+            self.assertRaises(SyntaxError, SpiderLoader.from_settings, settings)\n+\n+    def test_syntax_error_warning(self):\n+        with warnings.catch_warnings(record=True) as w, mock.patch.object(\n+            SpiderLoader, \"_load_spiders\"\n+        ) as m:\n+            m.side_effect = SyntaxError\n+            module = \"tests.test_spiderloader.test_spiders.spider1\"\n+            settings = Settings(\n+                {\"SPIDER_MODULES\": [module], \"SPIDER_LOADER_WARN_ONLY\": True}\n+            )\n+            spider_loader = SpiderLoader.from_settings(settings)\n+            if str(w[0].message).startswith(\"_SixMetaPathImporter\"):\n+                # needed on 3.10 because of https://github.com/benjaminp/six/issues/349,\n+                # at least until all six versions we can import (including botocore.vendored.six)\n+                # are updated to 1.16.0+\n+                w.pop(0)\n+            self.assertIn(\"Could not load spiders from module\", str(w[0].message))\n+\n+            spiders = spider_loader.list()\n+            self.assertEqual(spiders, [])\n+\n \n class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8c133fcf7e4f19d55d60dc6a090d75dab6db1a72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 842 | Contributors (this commit): 37 | Commits (past 90d): 1 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,6 @@ install_requires = [\n     \"zope.interface>=5.1.0\",\n     \"protego>=0.1.15\",\n     \"itemadapter>=0.1.0\",\n-    \"setuptools\",\n     \"packaging\",\n     \"tldextract\",\n     \"lxml>=4.4.1\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
