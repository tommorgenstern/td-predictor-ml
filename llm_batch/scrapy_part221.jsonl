{"custom_id": "scrapy#33a39b368ffab6641997e7611d588487176716de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 61 | Churn Cumulative: 940 | Contributors (this commit): 33 | Commits (past 90d): 9 | Contributors (cumulative): 48 | DMM Complexity: 0.975\n\nDIFF:\n@@ -33,9 +33,9 @@ class BaseItemExporter(object):\n         If dont_fail is set, it won't raise an exception on unexpected options\n         (useful for using with keyword arguments in subclasses constructors)\n         \"\"\"\n+        self.encoding = options.pop('encoding', None)\n         self.fields_to_export = options.pop('fields_to_export', None)\n         self.export_empty_fields = options.pop('export_empty_fields', False)\n-        self.encoding = options.pop('encoding', 'utf-8')\n         if not dont_fail and options:\n             raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))\n \n@@ -84,11 +84,13 @@ class JsonLinesItemExporter(BaseItemExporter):\n     def __init__(self, file, **kwargs):\n         self._configure(kwargs, dont_fail=True)\n         self.file = file\n+        kwargs.setdefault('ensure_ascii', not self.encoding)\n         self.encoder = ScrapyJSONEncoder(**kwargs)\n \n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n-        self.file.write(to_bytes(self.encoder.encode(itemdict) + '\\n'))\n+        data = self.encoder.encode(itemdict) + '\\n'\n+        self.file.write(to_bytes(data, self.encoding))\n \n \n class JsonItemExporter(BaseItemExporter):\n@@ -96,6 +98,7 @@ class JsonItemExporter(BaseItemExporter):\n     def __init__(self, file, **kwargs):\n         self._configure(kwargs, dont_fail=True)\n         self.file = file\n+        kwargs.setdefault('ensure_ascii', not self.encoding)\n         self.encoder = ScrapyJSONEncoder(**kwargs)\n         self.first_item = True\n \n@@ -111,7 +114,8 @@ class JsonItemExporter(BaseItemExporter):\n         else:\n             self.file.write(b',\\n')\n         itemdict = dict(self._get_serialized_fields(item))\n-        self.file.write(to_bytes(self.encoder.encode(itemdict)))\n+        data = self.encoder.encode(itemdict)\n+        self.file.write(to_bytes(data, self.encoding))\n \n \n class XmlItemExporter(BaseItemExporter):\n@@ -120,6 +124,8 @@ class XmlItemExporter(BaseItemExporter):\n         self.item_element = kwargs.pop('item_element', 'item')\n         self.root_element = kwargs.pop('root_element', 'items')\n         self._configure(kwargs)\n+        if not self.encoding:\n+            self.encoding = 'utf-8'\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n \n     def start_exporting(self):\n@@ -169,9 +175,16 @@ class CsvItemExporter(BaseItemExporter):\n \n     def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):\n         self._configure(kwargs, dont_fail=True)\n+        if not self.encoding:\n+            self.encoding = 'utf-8'\n         self.include_headers_line = include_headers_line\n-        file = file if six.PY2 else io.TextIOWrapper(file, line_buffering=True)\n-        self.csv_writer = csv.writer(file, **kwargs)\n+        self.stream = io.TextIOWrapper(\n+            file,\n+            line_buffering=False,\n+            write_through=True,\n+            encoding=self.encoding\n+        ) if six.PY3 else file\n+        self.csv_writer = csv.writer(self.stream, **kwargs)\n         self._headers_not_written = True\n         self._join_multivalued = join_multivalued\n \n@@ -200,7 +213,7 @@ class CsvItemExporter(BaseItemExporter):\n     def _build_row(self, values):\n         for s in values:\n             try:\n-                yield to_native_str(s)\n+                yield to_native_str(s, self.encoding)\n             except TypeError:\n                 yield s\n \n@@ -263,6 +276,8 @@ class PythonItemExporter(BaseItemExporter):\n             warnings.warn(\n                 \"PythonItemExporter will drop support for binary export in the future\",\n                 ScrapyDeprecationWarning)\n+        if not self.encoding:\n+            self.encoding = 'utf-8'\n \n     def serialize_field(self, field, name, value):\n         serializer = field.get('serializer', self._serialize_value)\n\n@@ -162,6 +162,7 @@ class FeedExporter(object):\n         if not self.urifmt:\n             raise NotConfigured\n         self.format = settings['FEED_FORMAT'].lower()\n+        self.export_encoding = settings['FEED_EXPORT_ENCODING']\n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n         if not self._storage_supported(self.urifmt):\n@@ -185,7 +186,8 @@ class FeedExporter(object):\n         uri = self.urifmt % self._get_uri_params(spider)\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n-        exporter = self._get_exporter(file, fields_to_export=self.export_fields)\n+        exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n+            encoding=self.export_encoding)\n         exporter.start_exporting()\n         self.slot = SpiderSlot(file, exporter, storage, uri)\n \n\n@@ -139,6 +139,7 @@ FEED_URI = None\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n FEED_FORMAT = 'jsonlines'\n FEED_STORE_EMPTY = False\n+FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n FEED_STORAGES = {}\n FEED_STORAGES_BASE = {\n\n@@ -371,3 +371,32 @@ class FeedExportTest(unittest.TestCase):\n             ]\n             yield self.assertExported(items, ['egg', 'baz'], rows,\n                                       settings=settings, ordered=True)\n+\n+    @defer.inlineCallbacks\n+    def test_export_encoding(self):\n+        items = [dict({'foo': u'Test\\xd6'})]\n+        header = ['foo']\n+        \n+        formats = {\n+            'json': u'[\\n{\"foo\": \"Test\\\\u00d6\"}\\n]'.encode('utf-8'),\n+            'jsonlines': u'{\"foo\": \"Test\\\\u00d6\"}\\n'.encode('utf-8'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('utf-8'),\n+            'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n+        }\n+        \n+        for format in formats:\n+            settings = {'FEED_FORMAT': format}\n+            data = yield self.exported_data(items, settings)\n+            self.assertEqual(formats[format], data)\n+            \n+        formats = {\n+            'json': u'[\\n{\"foo\": \"Test\\xd6\"}\\n]'.encode('latin-1'),\n+            'jsonlines': u'{\"foo\": \"Test\\xd6\"}\\n'.encode('latin-1'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('latin-1'),\n+            'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n+        }\n+        \n+        for format in formats:\n+            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_ENCODING': 'latin-1'}\n+            data = yield self.exported_data(items, settings)\n+            self.assertEqual(formats[format], data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#859bcf48206369d4ff40f85bd55bbfdcadf65e25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 63 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -50,7 +50,7 @@ try:\n \n                 except ValueError as e:\n                     logger.warning(\n-                        'SSL/TLS verification failed for hostname \"{}\"; {}'.format(\n+                        'Ignoring remote certificate verification failure for hostname \"{}\"; {}'.format(\n                             self._hostnameASCII, e))\n \n except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#005cf949b8618259427c811da35e89f29c73feed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 80 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,9 +34,15 @@ try:\n                                              VerificationError)\n \n     class ScrapyClientTLSOptions(ClientTLSOptions):\n-        # same as Twisted's ClientTLSOptions,\n-        # except that VerificationError is caught\n-        # and doesn't close the connection\n+        \"\"\"\n+        SSL Client connection creator ignoring certificate verification errors\n+        (for genuinely invalid certificates or bugs in verification code).\n+\n+        Same as Twisted's private _sslverify.ClientTLSOptions,\n+        except that VerificationError and ValueError exceptions are caught,\n+        so that the connection is not closed, only logging warnings.\n+        \"\"\"\n+\n         def _identityVerifyingInfoCallback(self, connection, where, ret):\n             if where & SSL_CB_HANDSHAKE_START:\n                 _maybeSetHostNameIndication(connection, self._hostnameBytes)\n@@ -50,8 +56,9 @@ try:\n \n                 except ValueError as e:\n                     logger.warning(\n-                        'Ignoring remote certificate verification failure for hostname \"{}\"; {}'.format(\n-                            self._hostnameASCII, e))\n+                        'Ignoring error while verifying certificate '\n+                        'from host \"{}\" (exception: {})'.format(\n+                            self._hostnameASCII, repr(e)))\n \n except ImportError:\n     # ImportError should not matter for older Twisted versions\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ceecf3b26c542ffcd74d6a81233d58ed54ea3268", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 195 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,22 +48,23 @@ class ImagesPipeline(FilesPipeline):\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n-        super(ImagesPipeline, self).__init__(store_uri, settings=settings, download_func=download_func)\n+        super(ImagesPipeline, self).__init__(store_uri, settings=settings,\n+                                             download_func=download_func)\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n-        cls_name = \"ImagesPipeline\"\n+        resolve = functools.partial(self._key_for_pipe,\n+                                    base_class_name=\"ImagesPipeline\")\n         self.expires = settings.getint(\n-            self._key_for_pipe('IMAGES_EXPIRES', cls_name), self.EXPIRES\n+            resolve(\"IMAGES_EXPIRES\"), self.EXPIRES\n         )\n+\n         if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n             self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD\n         if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n             self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD\n \n-        resolve = functools.partial(self._key_for_pipe, base_class_name=cls_name)\n-\n         self.images_urls_field = settings.get(\n             resolve('IMAGES_URLS_FIELD'),\n             self.IMAGES_URLS_FIELD\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de64a1f68afe24a246152f778b7f3b236b9ae439", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 209 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -79,6 +79,7 @@ class Command(ScrapyCommand):\n \n         if len(args) == 2:\n             project_dir = args[1]\n+\n         if exists(join(project_dir, 'scrapy.cfg')):\n             self.exitcode = 1\n             print('Error: scrapy.cfg already exists in %s' % abspath(project_dir))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe088925a3eff9d1de5682efaa33a391c6dc7744", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 217 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -51,6 +51,14 @@ class Command(ScrapyCommand):\n         return False\n \n     def _copytree(self, src, dst):\n+        \"\"\"\n+        Since the original function always creates the directory, to resolve\n+        the issue a new function had to be created. It's a simple copy and\n+        was reduced for this case.\n+\n+        More info at:\n+        https://github.com/scrapy/scrapy/pull/2005\n+        \"\"\"\n         ignore = IGNORE\n         names = os.listdir(src)\n         ignored_names = ignore(src, names)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f72991a96b9b61d07c76fb690b901c396ae8e048", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 674 | Contributors (this commit): 30 | Commits (past 90d): 14 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,6 +88,8 @@ class ImagesPipeline(FilesPipeline):\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n+        s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n+\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n \n\n@@ -177,6 +177,8 @@ HTTPCACHE_GZIP = False\n \n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n+IMAGES_STORE_S3_ACL = 'private'\n+\n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n ITEM_PIPELINES = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f193c52acd23a8a6539da9873ccbda27ee6475a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 199 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ class ImagesPipeline(FilesPipeline):\n     # ImagesPipeline. They may be overridden by settings.\n     MIN_WIDTH = 0\n     MIN_HEIGHT = 0\n-    EXPIRES = 0\n+    EXPIRES = 90\n     THUMBS = {}\n     DEFAULT_IMAGES_URLS_FIELD = 'image_urls'\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c3e3b484edfb30240b980a09a3719165f4f4a15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 419 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -221,7 +221,7 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n     default_pipeline_settings = dict(\n         MIN_WIDTH=0,\n         MIN_HEIGHT=0,\n-        EXPIRES=0,\n+        EXPIRES=90,\n         THUMBS={},\n         IMAGES_URLS_FIELD='image_urls',\n         IMAGES_RESULT_FIELD='images'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e17fdd72766eea6bc81fcfb9e3c24a9929eeeba5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 376 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.7142857142857143\n\nDIFF:\n@@ -14,7 +14,9 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n     TIME_FORMAT = \"%H:%M:%S\"\n \n     def default(self, o):\n-        if isinstance(o, datetime.datetime):\n+        if isinstance(o, set):\n+            return list(o)\n+        elif isinstance(o, datetime.datetime):\n             return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n         elif isinstance(o, datetime.date):\n             return o.strftime(self.DATE_FORMAT)\n\n@@ -23,9 +23,14 @@ class JsonEncoderTestCase(unittest.TestCase):\n         ts = \"10:11:12\"\n         dec = Decimal(\"1000.12\")\n         decs = \"1000.12\"\n+        s = {'foo'}\n+        ss = ['foo']\n+        dt_set = {dt}\n+        dt_sets = [dts]\n \n         for input, output in [('foo', 'foo'), (d, ds), (t, ts), (dt, dts),\n-                              (dec, decs), (['foo', d], ['foo', ds])]:\n+                              (dec, decs), (['foo', d], ['foo', ds]), (s, ss),\n+                              (dt_set, dt_sets)]:\n             self.assertEqual(self.encoder.encode(input), json.dumps(output))\n \n     def test_encode_deferred(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#472a8a47d0b213b45c9d3fc390628c8516650e81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 865 | Contributors (this commit): 34 | Commits (past 90d): 19 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class Scheduler(object):\n         pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n-        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS')\n+        logunser = settings.getbool('SCHEDULER_DEBUG')\n         return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                    stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)\n \n@@ -86,9 +86,13 @@ class Scheduler(object):\n             self.dqs.push(reqd, -request.priority)\n         except ValueError as e: # non serializable request\n             if self.logunser:\n-                logger.error(\"Unable to serialize request: %(request)s - reason: %(reason)s\",\n-                             {'request': request, 'reason': e},\n+                msg = (\"Unable to serialize request: %(request)s - reason: %(reason)s\"\n+                       \" - no more unserializable requests will be logged\"\n+                       \" (stats being collected)\")\n+                logger.error(msg, {'request': request, 'reason': e},\n                              exc_info=True, extra={'spider': self.spider})\n+                self.logunser = False\n+            self.stats.inc_value('scheduler/unserializable', spider=self.spider)\n             return\n         else:\n             return True\n\n@@ -191,7 +191,7 @@ LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n \n-LOG_UNSERIALIZABLE_REQUESTS = False\n+SCHEDULER_DEBUG = False\n \n LOGSTATS_INTERVAL = 60.0\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2d9e5937c6cd40e7fdafeddd5e54242815763b9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 435 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ class Scheduler(object):\n         pqclass = load_object(settings['SCHEDULER_PRIORITY_QUEUE'])\n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n-        logunser = settings.getbool('SCHEDULER_DEBUG')\n+        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS', settings.getbool('SCHEDULER_DEBUG'))\n         return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                    stats=crawler.stats, pqclass=pqclass, dqclass=dqclass, mqclass=mqclass)\n \n\n@@ -14,6 +14,7 @@ DEPRECATED_SETTINGS = [\n     ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n     ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n     ('REDIRECT_MAX_METAREFRESH_DELAY', 'use METAREFRESH_MAXDELAY instead'),\n+    ('LOG_UNSERIALIZABLE_REQUESTS', 'use SCHEDULER_DEBUG instead'),\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c77b6d033a01a6279734e1626ca7e49dc8d0784", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 397 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,13 +86,14 @@ class Scheduler(object):\n             self.dqs.push(reqd, -request.priority)\n         except ValueError as e:  # non serializable request\n             if self.logunser:\n-                msg = (\"Unable to serialize request: %(request)s - reason: %(reason)s\"\n-                       \" - no more unserializable requests will be logged\"\n-                       \" (stats being collected)\")\n+                msg = (\"Unable to serialize request: %(request)s - reason:\"\n+                       \" %(reason)s - no more unserializable requests will be\"\n+                       \" logged (stats being collected)\")\n                 logger.error(msg, {'request': request, 'reason': e},\n                              exc_info=True, extra={'spider': self.spider})\n                 self.logunser = False\n-            self.stats.inc_value('scheduler/unserializable', spider=self.spider)\n+            self.stats.inc_value('scheduler/unserializable',\n+                                 spider=self.spider)\n             return\n         else:\n             return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2e64525544c8fa5ba0d9ad85aedac0801b7e3de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 400 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-\"\"\"Helper functions which doesn't fit anywhere else\"\"\"\n+\"\"\"Helper functions which don't fit anywhere else\"\"\"\n import re\n import hashlib\n from importlib import import_module\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#643dbeffcf6951991fdeed0069690c55684e66ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 8 | Churn Cumulative: 1235 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 0.5\n\nDIFF:\n@@ -46,6 +46,10 @@ class Selector(_ParselSelector, object_ref):\n     selectorlist_cls = SelectorList\n \n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n+        if not(response is None or text is None):\n+           raise ValueError('%s.__init__() received both response and text'\n+                            % self.__class__.__name__)\n+\n         st = _st(response, type or self._default_type)\n \n         if _root is not None:\n\n@@ -123,6 +123,10 @@ class SelectorTestCase(unittest.TestCase):\n             sel.xpath('//p').extract_unquoted()\n             self.assertSubstring('Use .extract() instead', str(w[-1].message))\n \n+    def test_selector_bad_args(self):\n+        with self.assertRaisesRegexp(ValueError, 'received both response and text'):\n+            Selector(TextResponse(url='http://example.com', body=b''), text=u'')\n+\n \n class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#625c69fdc73c74cdb4172c4642842ed4f6f31432", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 593 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -175,7 +175,7 @@ class Command(ScrapyCommand):\n                     cb = cb_method\n                 else:\n                     logger.error('Cannot find callback %(callback)r in spider: %(spider)s',\n-                                 {'callback': callback, 'spider': spider.name})\n+                                 {'callback': cb, 'spider': spider.name})\n                     return\n \n             # parse items and requests\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9437fd3d97430d39ea408bca7c200ec042e1ab7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 220 | Contributors (this commit): 8 | Commits (past 90d): 15 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,7 +43,7 @@ def gunzip(data):\n             # contains the whole page content\n             if output or getattr(f, 'extrabuf', None):\n                 try:\n-                    output += f.extrabuf\n+                    output += f.extrabuf[-f.extrasize:]\n                 finally:\n                     break\n             else:\n\n@@ -1,6 +1,8 @@\n import unittest\n from os.path import join\n \n+from w3lib.encoding import html_to_unicode\n+\n from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy.http import Response, Headers\n from tests import tests_datadir\n@@ -66,3 +68,11 @@ class GunzipTest(unittest.TestCase):\n         hdrs = Headers({\"Content-Type\": \"application/x-gzip;charset=utf-8\"})\n         r1 = Response(\"http://www.example.com\", headers=hdrs)\n         self.assertTrue(is_gzipped(r1))\n+\n+    def test_gunzip_illegal_eof(self):\n+        with open(join(SAMPLEDIR, 'unexpected-eof.gz'), 'rb') as f:\n+            text = html_to_unicode('charset=cp1252', gunzip(f.read()))[1]\n+            with open(join(SAMPLEDIR, 'unexpected-eof-output.txt'), 'rb') as o:\n+                expected_text = o.read().decode(\"utf-8\")\n+                self.assertEqual(len(text), len(expected_text))\n+                self.assertEqual(text, expected_text)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb3b806467c1a79ad0ff3c8012e9366cc6066f2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 384 | Files Changed: 5 | Hunks: 10 | Methods Changed: 29 | Complexity Δ (Sum/Max): -47/0 | Churn Δ: 392 | Churn Cumulative: 2171 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 45 | DMM Complexity: 0.11764705882352941\n\nDIFF:\n@@ -9,10 +9,11 @@ import re\n \n from six.moves.urllib.parse import urlparse\n from parsel.csstranslator import HTMLTranslator\n+from w3lib.url import canonicalize_url\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import (\n-    canonicalize_url, url_is_from_any_domain, url_has_any_extension,\n+    url_is_from_any_domain, url_has_any_extension,\n )\n \n \n\n@@ -11,7 +11,7 @@ from six.moves.urllib.parse import urlunparse\n from w3lib.http import basic_auth_header\n from scrapy.utils.python import to_bytes, to_native_str\n \n-from scrapy.utils.url import canonicalize_url\n+from w3lib.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n \n \n\n@@ -7,18 +7,13 @@ to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n import re\n-import six\n-from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n-                                    urlparse, parse_qsl, urlencode,\n-                                    quote, unquote)\n-if not six.PY2:\n-    from urllib.parse import unquote_to_bytes\n+from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse)\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n from w3lib.url import *\n-from w3lib.url import _safe_chars\n-from scrapy.utils.python import to_bytes, to_native_str, to_unicode\n+from w3lib.url import _safe_chars, _unquotepath\n+from scrapy.utils.python import to_unicode\n \n \n def url_is_from_any_domain(url, domains):\n@@ -40,121 +35,6 @@ def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n \n \n-def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n-    # IDNA encoding can fail for too long labels (>63 characters)\n-    # or missing labels (e.g. http://.example.com)\n-    try:\n-        netloc = parts.netloc.encode('idna')\n-    except UnicodeError:\n-        netloc = parts.netloc\n-\n-    return (\n-        to_native_str(parts.scheme),\n-        to_native_str(netloc),\n-\n-        # default encoding for path component SHOULD be UTF-8\n-        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n-        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n-\n-        # encoding of query and fragment follows page encoding\n-        # or form-charset (if known and passed)\n-        quote(to_bytes(parts.query, encoding), _safe_chars),\n-        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n-    )\n-\n-\n-def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n-                     encoding=None):\n-    \"\"\"Canonicalize the given url by applying the following procedures:\n-\n-    - sort query arguments, first by key, then by value\n-    - percent encode paths ; non-ASCII characters are percent-encoded\n-      using UTF-8 (RFC-3986)\n-    - percent encode query arguments ; non-ASCII characters are percent-encoded\n-      using passed `encoding` (UTF-8 by default)\n-    - normalize all spaces (in query arguments) '+' (plus symbol)\n-    - normalize percent encodings case (%2f -> %2F)\n-    - remove query arguments with blank values (unless `keep_blank_values` is True)\n-    - remove fragments (unless `keep_fragments` is True)\n-\n-    The url passed can be bytes or unicode, while the url returned is\n-    always a native str (bytes in Python 2, unicode in Python 3).\n-\n-    For examples see the tests in tests/test_utils_url.py\n-    \"\"\"\n-    # If supplied `encoding` is not compatible with all characters in `url`,\n-    # fallback to UTF-8 as safety net.\n-    # UTF-8 can handle all Unicode characters,\n-    # so we should be covered regarding URL normalization,\n-    # if not for proper URL expected by remote website.\n-    try:\n-        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n-            parse_url(url), encoding=encoding)\n-    except UnicodeEncodeError as e:\n-        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n-            parse_url(url), encoding='utf8')\n-\n-    # 1. decode query-string as UTF-8 (or keep raw bytes),\n-    #    sort values,\n-    #    and percent-encode them back\n-    if six.PY2:\n-        keyvals = parse_qsl(query, keep_blank_values)\n-    else:\n-        # Python3's urllib.parse.parse_qsl does not work as wanted\n-        # for percent-encoded characters that do not match passed encoding,\n-        # they get lost.\n-        #\n-        # e.g., 'q=b%a3' becomes [('q', 'b\\ufffd')]\n-        # (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),\n-        #      instead of \\xa3 that you get with Python2's parse_qsl)\n-        #\n-        # what we want here is to keep raw bytes, and percent encode them\n-        # so as to preserve whatever encoding what originally used.\n-        #\n-        # See https://tools.ietf.org/html/rfc3987#section-6.4:\n-        #\n-        # For example, it is possible to have a URI reference of\n-        # \"http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9\", where the\n-        # document name is encoded in iso-8859-1 based on server settings, but\n-        # where the fragment identifier is encoded in UTF-8 according to\n-        # [XPointer]. The IRI corresponding to the above URI would be (in XML\n-        # notation)\n-        # \"http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;\".\n-        # Similar considerations apply to query parts.  The functionality of\n-        # IRIs (namely, to be able to include non-ASCII characters) can only be\n-        # used if the query part is encoded in UTF-8.\n-        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n-    keyvals.sort()\n-    query = urlencode(keyvals)\n-\n-    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)\n-    #    and percent-encode path again (this normalizes to upper-case %XX)\n-    uqp = _unquotepath(path)\n-    path = quote(uqp, _safe_chars) or '/'\n-\n-    fragment = '' if not keep_fragments else fragment\n-\n-    # every part should be safe already\n-    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n-\n-\n-def _unquotepath(path):\n-    for reserved in ('2f', '2F', '3f', '3F'):\n-        path = path.replace('%' + reserved, '%25' + reserved.upper())\n-\n-    if six.PY2:\n-        # in Python 2, '%a3' becomes '\\xa3', which is what we want\n-        return unquote(path)\n-    else:\n-        # in Python 3,\n-        # standard lib's unquote() does not work for non-UTF-8\n-        # percent-escaped characters, they get lost.\n-        # e.g., '%a3' becomes 'REPLACEMENT CHARACTER' (U+FFFD)\n-        #\n-        # unquote_to_bytes() returns raw bytes instead\n-        return unquote_to_bytes(path)\n-\n-\n def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n@@ -164,59 +44,6 @@ def parse_url(url, encoding=None):\n     return urlparse(to_unicode(url, encoding))\n \n \n-if not six.PY2:\n-    from urllib.parse import _coerce_args, unquote_to_bytes\n-\n-    def parse_qsl_to_bytes(qs, keep_blank_values=False, strict_parsing=False):\n-        \"\"\"Parse a query given as a string argument.\n-\n-        Data are returned as a list of name, value pairs as bytes.\n-\n-        Arguments:\n-\n-        qs: percent-encoded query string to be parsed\n-\n-        keep_blank_values: flag indicating whether blank values in\n-            percent-encoded queries should be treated as blank strings.  A\n-            true value indicates that blanks should be retained as blank\n-            strings.  The default false value indicates that blank values\n-            are to be ignored and treated as if they were  not included.\n-\n-        strict_parsing: flag indicating what to do with parsing errors. If\n-            false (the default), errors are silently ignored. If true,\n-            errors raise a ValueError exception.\n-\n-        \"\"\"\n-        # This code is the same as Python3's parse_qsl()\n-        # (at https://hg.python.org/cpython/rev/c38ac7ab8d9a)\n-        # except for the unquote(s, encoding, errors) calls replaced\n-        # with unquote_to_bytes(s)\n-        qs, _coerce_result = _coerce_args(qs)\n-        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n-        r = []\n-        for name_value in pairs:\n-            if not name_value and not strict_parsing:\n-                continue\n-            nv = name_value.split('=', 1)\n-            if len(nv) != 2:\n-                if strict_parsing:\n-                    raise ValueError(\"bad query field: %r\" % (name_value,))\n-                # Handle case of a control-name with no equal sign\n-                if keep_blank_values:\n-                    nv.append('')\n-                else:\n-                    continue\n-            if len(nv[1]) or keep_blank_values:\n-                name = nv[0].replace('+', ' ')\n-                name = unquote_to_bytes(name)\n-                name = _coerce_result(name)\n-                value = nv[1].replace('+', ' ')\n-                value = unquote_to_bytes(value)\n-                value = _coerce_result(value)\n-                r.append((name, value))\n-        return r\n-\n-\n def escape_ajax(url):\n     \"\"\"\n     Return the crawleable url according to:\n\n@@ -42,7 +42,7 @@ setup(\n     ],\n     install_requires=[\n         'Twisted>=10.0.0',\n-        'w3lib>=1.14.2',\n+        'w3lib>=1.15.0',\n         'queuelib',\n         'lxml',\n         'pyOpenSSL',\n\n@@ -6,8 +6,7 @@ from six.moves.urllib.parse import urlparse\n \n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              canonicalize_url, add_http_if_no_scheme,\n-                              guess_scheme, parse_url)\n+                              add_http_if_no_scheme, guess_scheme, parse_url)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -77,209 +76,6 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n \n-class CanonicalizeUrlTest(unittest.TestCase):\n-\n-    def test_canonicalize_url(self):\n-        # simplest case\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/\"),\n-                                          \"http://www.example.com/\")\n-\n-    def test_return_str(self):\n-        assert isinstance(canonicalize_url(u\"http://www.example.com\"), str)\n-        assert isinstance(canonicalize_url(b\"http://www.example.com\"), str)\n-\n-    def test_append_missing_path(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com\"),\n-                                          \"http://www.example.com/\")\n-\n-    def test_typical_usage(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?a=1&b=2&c=3\"),\n-                                          \"http://www.example.com/do?a=1&b=2&c=3\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?c=1&b=2&a=3\"),\n-                                          \"http://www.example.com/do?a=3&b=2&c=1\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?&a=1\"),\n-                                          \"http://www.example.com/do?a=1\")\n-\n-    def test_sorting(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?c=3&b=5&b=2&a=50\"),\n-                                          \"http://www.example.com/do?a=50&b=2&b=5&c=3\")\n-\n-    def test_keep_blank_values(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&a=2\", keep_blank_values=False),\n-                                          \"http://www.example.com/do?a=2\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&a=2\"),\n-                                          \"http://www.example.com/do?a=2&b=\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&c&a=2\", keep_blank_values=False),\n-                                          \"http://www.example.com/do?a=2\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?b=&c&a=2\"),\n-                                          \"http://www.example.com/do?a=2&b=&c=\")\n-\n-        self.assertEqual(canonicalize_url(u'http://www.example.com/do?1750,4'),\n-                                           'http://www.example.com/do?1750%2C4=')\n-\n-    def test_spaces(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a space&a=1\"),\n-                                          \"http://www.example.com/do?a=1&q=a+space\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a+space&a=1\"),\n-                                          \"http://www.example.com/do?a=1&q=a+space\")\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a%20space&a=1\"),\n-                                          \"http://www.example.com/do?a=1&q=a+space\")\n-\n-    def test_canonicalize_url_unicode_path(self):\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé\"),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9\")\n-\n-    def test_canonicalize_url_unicode_query_string(self):\n-        # default encoding for path and query is UTF-8\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?q=résumé\"),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n-\n-        # passed encoding will affect query string\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?q=résumé\", encoding='latin1'),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%E9sum%E9\")\n-\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?country=Россия\", encoding='cp1251'),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?country=%D0%EE%F1%F1%E8%FF\")\n-\n-    def test_canonicalize_url_unicode_query_string_wrong_encoding(self):\n-        # trying to encode with wrong encoding\n-        # fallback to UTF-8\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?currency=€\", encoding='latin1'),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?currency=%E2%82%AC\")\n-\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?country=Россия\", encoding='latin1'),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?country=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F\")\n-\n-    def test_normalize_percent_encoding_in_paths(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/r%c3%a9sum%c3%a9\"),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9\")\n-\n-        # non-UTF8 encoded sequences: they should be kept untouched, only upper-cased\n-        # 'latin1'-encoded sequence in path\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do\"),\n-                                          \"http://www.example.com/a%A3do\")\n-\n-        # 'latin1'-encoded path, UTF-8 encoded query string\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\"),\n-                                          \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n-\n-        # 'latin1'-encoded path and query string\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do?q=r%e9sum%e9\"),\n-                                          \"http://www.example.com/a%A3do?q=r%E9sum%E9\")\n-\n-    def test_normalize_percent_encoding_in_query_arguments(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?k=b%a3\"),\n-                                          \"http://www.example.com/do?k=b%A3\")\n-\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/do?k=r%c3%a9sum%c3%a9\"),\n-                                          \"http://www.example.com/do?k=r%C3%A9sum%C3%A9\")\n-\n-    def test_non_ascii_percent_encoding_in_paths(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a do?a=1\"),\n-                                          \"http://www.example.com/a%20do?a=1\"),\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a %20do?a=1\"),\n-                                          \"http://www.example.com/a%20%20do?a=1\"),\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/a do£.html?a=1\"),\n-                                          \"http://www.example.com/a%20do%C2%A3.html?a=1\")\n-        self.assertEqual(canonicalize_url(b\"http://www.example.com/a do\\xc2\\xa3.html?a=1\"),\n-                                          \"http://www.example.com/a%20do%C2%A3.html?a=1\")\n-\n-    def test_non_ascii_percent_encoding_in_query_arguments(self):\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/do?price=£500&a=5&z=3\"),\n-                                          u\"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n-        self.assertEqual(canonicalize_url(b\"http://www.example.com/do?price=\\xc2\\xa3500&a=5&z=3\"),\n-                                          \"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n-        self.assertEqual(canonicalize_url(b\"http://www.example.com/do?price(\\xc2\\xa3)=500&a=1\"),\n-                                          \"http://www.example.com/do?a=1&price%28%C2%A3%29=500\")\n-\n-    def test_urls_with_auth_and_ports(self):\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com:81/do?now=1\"),\n-                                          u\"http://user:pass@www.example.com:81/do?now=1\")\n-\n-    def test_remove_fragments(self):\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\"),\n-                                          u\"http://user:pass@www.example.com/do?a=1\")\n-        self.assertEqual(canonicalize_url(u\"http://user:pass@www.example.com/do?a=1#frag\", keep_fragments=True),\n-                                          u\"http://user:pass@www.example.com/do?a=1#frag\")\n-\n-    def test_dont_convert_safe_characters(self):\n-        # dont convert safe characters to percent encoding representation\n-        self.assertEqual(canonicalize_url(\n-            \"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html\"),\n-            \"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html\")\n-\n-    def test_safe_characters_unicode(self):\n-        # urllib.quote uses a mapping cache of encoded characters. when parsing\n-        # an already percent-encoded url, it will fail if that url was not\n-        # percent-encoded as utf-8, that's why canonicalize_url must always\n-        # convert the urls to string. the following test asserts that\n-        # functionality.\n-        self.assertEqual(canonicalize_url(u'http://www.example.com/caf%E9-con-leche.htm'),\n-                                           'http://www.example.com/caf%E9-con-leche.htm')\n-\n-    def test_domains_are_case_insensitive(self):\n-        self.assertEqual(canonicalize_url(\"http://www.EXAMPLE.com/\"),\n-                                          \"http://www.example.com/\")\n-\n-    def test_canonicalize_idns(self):\n-        self.assertEqual(canonicalize_url(u'http://www.bücher.de?q=bücher'),\n-                                           'http://www.xn--bcher-kva.de/?q=b%C3%BCcher')\n-        # Japanese (+ reordering query parameters)\n-        self.assertEqual(canonicalize_url(u'http://はじめよう.みんな/?query=サ&maxResults=5'),\n-                                           'http://xn--p8j9a0d9c9a.xn--q9jyb4c/?maxResults=5&query=%E3%82%B5')\n-\n-    def test_quoted_slash_and_question_sign(self):\n-        self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC+rocks%3f/?yeah=1\"),\n-                         \"http://foo.com/AC%2FDC+rocks%3F/?yeah=1\")\n-        self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC/\"),\n-                         \"http://foo.com/AC%2FDC/\")\n-\n-    def test_canonicalize_urlparsed(self):\n-        # canonicalize_url() can be passed an already urlparse'd URL\n-        self.assertEqual(canonicalize_url(urlparse(u\"http://www.example.com/résumé?q=résumé\")),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n-        self.assertEqual(canonicalize_url(urlparse('http://www.example.com/caf%e9-con-leche.htm')),\n-                                          'http://www.example.com/caf%E9-con-leche.htm')\n-        self.assertEqual(canonicalize_url(urlparse(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\")),\n-                                          \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n-\n-    def test_canonicalize_parse_url(self):\n-        # parse_url() wraps urlparse and is used in link extractors\n-        self.assertEqual(canonicalize_url(parse_url(u\"http://www.example.com/résumé?q=résumé\")),\n-                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n-        self.assertEqual(canonicalize_url(parse_url('http://www.example.com/caf%e9-con-leche.htm')),\n-                                          'http://www.example.com/caf%E9-con-leche.htm')\n-        self.assertEqual(canonicalize_url(parse_url(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\")),\n-                                          \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n-\n-    def test_canonicalize_url_idempotence(self):\n-        for url, enc in [(u'http://www.bücher.de/résumé?q=résumé', 'utf8'),\n-                         (u'http://www.example.com/résumé?q=résumé', 'latin1'),\n-                         (u'http://www.example.com/résumé?country=Россия', 'cp1251'),\n-                         (u'http://はじめよう.みんな/?query=サ&maxResults=5', 'iso2022jp')]:\n-            canonicalized = canonicalize_url(url, encoding=enc)\n-\n-            # if we canonicalize again, we ge the same result\n-            self.assertEqual(canonicalize_url(canonicalized, encoding=enc), canonicalized)\n-\n-            # without encoding, already canonicalized URL is canonicalized identically\n-            self.assertEqual(canonicalize_url(canonicalized), canonicalized)\n-\n-    def test_canonicalize_url_idna_exceptions(self):\n-        # missing DNS label\n-        self.assertEqual(\n-            canonicalize_url(u\"http://.example.com/résumé?q=résumé\"),\n-            \"http://.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n-\n-        # DNS label too long\n-        self.assertEqual(\n-            canonicalize_url(\n-                u\"http://www.{label}.com/résumé?q=résumé\".format(\n-                    label=u\"example\"*11)),\n-            \"http://www.{label}.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\".format(\n-                    label=u\"example\"*11))\n-\n-\n class AddHttpIfNoScheme(unittest.TestCase):\n \n     def test_add_scheme(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
