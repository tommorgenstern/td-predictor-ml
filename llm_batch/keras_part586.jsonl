{"custom_id": "keras#981cb286e64155ea3feb0f504b7238d3e1019a4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 588 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 17 | Complexity Δ (Sum/Max): 39/30 | Churn Δ: 589 | Churn Cumulative: 1406 | Contributors (this commit): 6 | Commits (past 90d): 28 | Contributors (cumulative): 8 | DMM Complexity: 0.7713414634146342\n\nDIFF:\n@@ -0,0 +1,318 @@\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.metrics.metric import Metric\n+\n+\n+@keras_core_export(\"keras_core.metrics.FBetaScore\")\n+class FBetaScore(Metric):\n+    \"\"\"Computes F-Beta score.\n+\n+    Formula:\n+\n+    ```python\n+    b2 = beta ** 2\n+    f_beta_score = (1 + b2) * (precision * recall) / (precision * b2 + recall)\n+    ```\n+    This is the weighted harmonic mean of precision and recall.\n+    Its output range is `[0, 1]`. It works for both multi-class\n+    and multi-label classification.\n+\n+    Args:\n+        average: Type of averaging to be performed across per-class results\n+            in the multi-class case.\n+            Acceptable values are `None`, `\"micro\"`, `\"macro\"` and\n+            `\"weighted\"`. Default value is `None`.\n+            If `None`, no averaging is performed and `result()` will return\n+            the score for each class.\n+            If `\"micro\"`, compute metrics globally by counting the total\n+            true positives, false negatives and false positives.\n+            If `\"macro\"`, compute metrics for each label,\n+            and return their unweighted mean.\n+            This does not take label imbalance into account.\n+            If `\"weighted\"`, compute metrics for each label,\n+            and return their average weighted by support\n+            (the number of true instances for each label).\n+            This alters `\"macro\"` to account for label imbalance.\n+            It can result in an score that is not between precision and recall.\n+        beta: Determines the weight of given to recall\n+            in the harmonic mean between precision and recall (see pseudocode\n+            equation above). Default value is 1.\n+        threshold: Elements of `y_pred` greater than `threshold` are\n+            converted to be 1, and the rest 0. If `threshold` is\n+            `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.\n+        name: Optional. String name of the metric instance.\n+        dtype: Optional. Data type of the metric result.\n+\n+    Returns:\n+        F-Beta Score: float.\n+\n+    Example:\n+\n+    >>> metric = keras_core.metrics.FBetaScore(beta=2.0, threshold=0.5)\n+    >>> y_true = np.array([[1, 1, 1],\n+    ...                    [1, 0, 0],\n+    ...                    [1, 1, 0]], np.int32)\n+    >>> y_pred = np.array([[0.2, 0.6, 0.7],\n+    ...                    [0.2, 0.6, 0.6],\n+    ...                    [0.6, 0.8, 0.0]], np.float32)\n+    >>> metric.update_state(y_true, y_pred)\n+    >>> result = metric.result()\n+    >>> result\n+    [0.3846154 , 0.90909094, 0.8333334 ]\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        average=None,\n+        beta=1.0,\n+        threshold=None,\n+        name=\"fbeta_score\",\n+        dtype=None,\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+\n+        if average not in (None, \"micro\", \"macro\", \"weighted\"):\n+            raise ValueError(\n+                \"Invalid `average` argument value. Expected one of: \"\n+                \"{None, 'micro', 'macro', 'weighted'}. \"\n+                f\"Received: average={average}\"\n+            )\n+\n+        if not isinstance(beta, float):\n+            raise ValueError(\n+                \"Invalid `beta` argument value. \"\n+                \"It should be a Python float. \"\n+                f\"Received: beta={beta} of type '{type(beta)}'\"\n+            )\n+        if beta <= 0.0:\n+            raise ValueError(\n+                \"Invalid `beta` argument value. \"\n+                \"It should be > 0. \"\n+                f\"Received: beta={beta}\"\n+            )\n+\n+        if threshold is not None:\n+            if not isinstance(threshold, float):\n+                raise ValueError(\n+                    \"Invalid `threshold` argument value. \"\n+                    \"It should be a Python float. \"\n+                    f\"Received: threshold={threshold} \"\n+                    f\"of type '{type(threshold)}'\"\n+                )\n+            if threshold > 1.0 or threshold <= 0.0:\n+                raise ValueError(\n+                    \"Invalid `threshold` argument value. \"\n+                    \"It should verify 0 < threshold <= 1. \"\n+                    f\"Received: threshold={threshold}\"\n+                )\n+\n+        self.average = average\n+        self.beta = beta\n+        self.threshold = threshold\n+        self.axis = None\n+        self._built = False\n+\n+        if self.average != \"micro\":\n+            self.axis = 0\n+\n+    def _build(self, y_true_shape, y_pred_shape):\n+        if len(y_pred_shape) != 2 or len(y_true_shape) != 2:\n+            raise ValueError(\n+                \"FBetaScore expects 2D inputs with shape \"\n+                \"(batch_size, output_dim). Received input \"\n+                f\"shapes: y_pred.shape={y_pred_shape} and \"\n+                f\"y_true.shape={y_true_shape}.\"\n+            )\n+        if y_pred_shape[-1] is None or y_true_shape[-1] is None:\n+            raise ValueError(\n+                \"FBetaScore expects 2D inputs with shape \"\n+                \"(batch_size, output_dim), with output_dim fully \"\n+                \"defined (not None). Received input \"\n+                f\"shapes: y_pred.shape={y_pred_shape} and \"\n+                f\"y_true.shape={y_true_shape}.\"\n+            )\n+        num_classes = y_pred_shape[-1]\n+        if self.average != \"micro\":\n+            init_shape = num_classes\n+        else:\n+            init_shape = ()\n+\n+        def _add_zeros_variable(name):\n+            return self.add_variable(\n+                name=name,\n+                shape=init_shape,\n+                initializer=initializers.Zeros(),\n+                dtype=self.dtype,\n+            )\n+\n+        self.true_positives = _add_zeros_variable(\"true_positives\")\n+        self.false_positives = _add_zeros_variable(\"false_positives\")\n+        self.false_negatives = _add_zeros_variable(\"false_negatives\")\n+        self.intermediate_weights = _add_zeros_variable(\"intermediate_weights\")\n+        self._built = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = ops.convert_to_tensor(y_true, dtype=self.dtype)\n+        y_pred = ops.convert_to_tensor(y_pred, dtype=self.dtype)\n+        if not self._built:\n+            self._build(y_true.shape, y_pred.shape)\n+\n+        if self.threshold is None:\n+            threshold = ops.max(y_pred, axis=-1, keepdims=True)\n+            # make sure [0, 0, 0] doesn't become [1, 1, 1]\n+            # Use abs(x) > eps, instead of x != 0 to check for zero\n+            y_pred = ops.logical_and(\n+                y_pred >= threshold, ops.abs(y_pred) > 1e-9\n+            )\n+        else:\n+            y_pred = y_pred > self.threshold\n+\n+        y_pred = ops.cast(y_pred, dtype=self.dtype)\n+        y_true = ops.cast(y_true, dtype=self.dtype)\n+        if sample_weight is not None:\n+            sample_weight = ops.convert_to_tensor(\n+                sample_weight, dtype=self.dtype\n+            )\n+\n+        def _weighted_sum(val, sample_weight):\n+            if sample_weight is not None:\n+                val = ops.multiply(val, ops.expand_dims(sample_weight, 1))\n+            return ops.sum(val, axis=self.axis)\n+\n+        self.true_positives.assign(\n+            self.true_positives + _weighted_sum(y_pred * y_true, sample_weight)\n+        )\n+        self.false_positives.assign(\n+            self.false_positives\n+            + _weighted_sum(y_pred * (1 - y_true), sample_weight)\n+        )\n+        self.false_negatives.assign(\n+            self.false_negatives\n+            + _weighted_sum((1 - y_pred) * y_true, sample_weight)\n+        )\n+        self.intermediate_weights.assign(\n+            self.intermediate_weights + _weighted_sum(y_true, sample_weight)\n+        )\n+\n+    def result(self):\n+        precision = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_positives + backend.epsilon(),\n+        )\n+        recall = ops.divide(\n+            self.true_positives,\n+            self.true_positives + self.false_negatives + backend.epsilon(),\n+        )\n+\n+        precision = ops.convert_to_tensor(precision, dtype=self.dtype)\n+        recall = ops.convert_to_tensor(recall, dtype=self.dtype)\n+\n+        mul_value = precision * recall\n+        add_value = ((self.beta**2) * precision) + recall\n+        mean = ops.divide(mul_value, add_value + backend.epsilon())\n+        f1_score = mean * (1 + (self.beta**2))\n+\n+        if self.average == \"weighted\":\n+            weights = ops.divide(\n+                self.intermediate_weights,\n+                ops.sum(self.intermediate_weights) + backend.epsilon(),\n+            )\n+            f1_score = ops.sum(f1_score * weights)\n+\n+        elif self.average is not None:  # [micro, macro]\n+            f1_score = ops.mean(f1_score)\n+\n+        return f1_score\n+\n+    def get_config(self):\n+        \"\"\"Returns the serializable config of the metric.\"\"\"\n+\n+        config = {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"average\": self.average,\n+            \"beta\": self.beta,\n+            \"threshold\": self.threshold,\n+        }\n+\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    def reset_state(self):\n+        for v in self.variables:\n+            v.assign(ops.zeros(v.shape, dtype=v.dtype))\n+\n+\n+@keras_core_export(\"keras_core.metrics.F1Score\")\n+class F1Score(FBetaScore):\n+    r\"\"\"Computes F-1 Score.\n+\n+    Formula:\n+\n+    ```python\n+    f1_score = 2 * (precision * recall) / (precision + recall)\n+    ```\n+    This is the harmonic mean of precision and recall.\n+    Its output range is `[0, 1]`. It works for both multi-class\n+    and multi-label classification.\n+\n+    Args:\n+        average: Type of averaging to be performed on data.\n+            Acceptable values are `None`, `\"micro\"`, `\"macro\"`\n+            and `\"weighted\"`. Default value is `None`.\n+            If `None`, no averaging is performed and `result()` will return\n+            the score for each class.\n+            If `\"micro\"`, compute metrics globally by counting the total\n+            true positives, false negatives and false positives.\n+            If `\"macro\"`, compute metrics for each label,\n+            and return their unweighted mean.\n+            This does not take label imbalance into account.\n+            If `\"weighted\"`, compute metrics for each label,\n+            and return their average weighted by support\n+            (the number of true instances for each label).\n+            This alters `\"macro\"` to account for label imbalance.\n+            It can result in an score that is not between precision and recall.\n+        threshold: Elements of `y_pred` greater than `threshold` are\n+            converted to be 1, and the rest 0. If `threshold` is\n+            `None`, the argmax of `y_pred` is converted to 1, and the rest to 0.\n+        name: Optional. String name of the metric instance.\n+        dtype: Optional. Data type of the metric result.\n+\n+    Returns:\n+        F-1 Score: float.\n+\n+    Example:\n+\n+    >>> metric = keras_core.metrics.F1Score(threshold=0.5)\n+    >>> y_true = np.array([[1, 1, 1],\n+    ...                    [1, 0, 0],\n+    ...                    [1, 1, 0]], np.int32)\n+    >>> y_pred = np.array([[0.2, 0.6, 0.7],\n+    ...                    [0.2, 0.6, 0.6],\n+    ...                    [0.6, 0.8, 0.0]], np.float32)\n+    >>> metric.update_state(y_true, y_pred)\n+    >>> result = metric.result()\n+    array([0.5      , 0.8      , 0.6666667], dtype=float32)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        average=None,\n+        threshold=None,\n+        name=\"f1_score\",\n+        dtype=None,\n+    ):\n+        super().__init__(\n+            average=average,\n+            beta=1.0,\n+            threshold=threshold,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        del base_config[\"beta\"]\n+        return base_config\n\n@@ -0,0 +1,269 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import testing\n+from keras_core.metrics import f_score_metrics\n+\n+\n+class FBetaScoreTest(parameterized.TestCase, testing.TestCase):\n+    def _run_test(\n+        self,\n+        y_true,\n+        y_pred,\n+        sample_weights,\n+        average,\n+        beta,\n+        threshold,\n+        reference_result,\n+    ):\n+        fbeta = f_score_metrics.FBetaScore(\n+            average, beta, threshold, dtype=\"float32\"\n+        )\n+        fbeta.update_state(y_true, y_pred, sample_weights)\n+        result = fbeta.result()\n+        self.assertAllClose(result, reference_result, atol=1e-6)\n+\n+    def test_config(self):\n+        fbeta_obj = f_score_metrics.FBetaScore(\n+            beta=0.5, threshold=0.3, average=None, dtype=\"float32\"\n+        )\n+        self.assertEqual(fbeta_obj.beta, 0.5)\n+        self.assertEqual(fbeta_obj.average, None)\n+        self.assertEqual(fbeta_obj.threshold, 0.3)\n+        self.assertEqual(fbeta_obj.dtype, \"float32\")\n+\n+        # Check save and restore config\n+        fbeta_obj2 = f_score_metrics.FBetaScore.from_config(\n+            fbeta_obj.get_config()\n+        )\n+        self.assertEqual(fbeta_obj2.beta, 0.5)\n+        self.assertEqual(fbeta_obj2.average, None)\n+        self.assertEqual(fbeta_obj2.threshold, 0.3)\n+        self.assertEqual(fbeta_obj2.dtype, \"float32\")\n+\n+    @parameterized.parameters(\n+        (\"micro\", 0.5),\n+        (\"micro\", 1.0),\n+        (\"micro\", 2.0),\n+        (\"macro\", 0.5),\n+        (\"macro\", 1.0),\n+        (\"macro\", 2.0),\n+        (\"weighted\", 0.5),\n+        (\"weighted\", 1.0),\n+        (\"weighted\", 2.0),\n+    )\n+    def test_fbeta_perfect_score(self, average, beta):\n+        y_true = [[1, 1, 1], [1, 0, 0], [1, 1, 0]]\n+        y_pred = [[0.7, 0.7, 0.7], [1, 0, 0], [0.9, 0.8, 0]]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            None,\n+            average=average,\n+            beta=beta,\n+            threshold=0.66,\n+            reference_result=1.0,\n+        )\n+\n+    @parameterized.parameters(\n+        (\"micro\", 0.5),\n+        (\"micro\", 1.0),\n+        (\"micro\", 2.0),\n+        (\"macro\", 0.5),\n+        (\"macro\", 1.0),\n+        (\"macro\", 2.0),\n+        (\"weighted\", 0.5),\n+        (\"weighted\", 1.0),\n+        (\"weighted\", 2.0),\n+    )\n+    def test_fbeta_worst_score(self, average, beta):\n+        y_true = [[0, 0, 0], [0, 1, 0], [0, 0, 1]]\n+        y_pred = [[0.7, 0.7, 0.7], [1, 0, 0], [0.9, 0.8, 0]]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            None,\n+            average=average,\n+            beta=beta,\n+            threshold=0.66,\n+            reference_result=0.0,\n+        )\n+\n+    @parameterized.parameters(\n+        # average, beta, result\n+        (None, 0.5, [0.71428573, 0.5, 0.833334]),\n+        (None, 1.0, [0.8, 0.5, 0.6666667]),\n+        (None, 2.0, [0.9090904, 0.5, 0.555556]),\n+        (\"micro\", 0.5, 0.6666667),\n+        (\"micro\", 1.0, 0.6666667),\n+        (\"micro\", 2.0, 0.6666667),\n+        (\"macro\", 0.5, 0.6825397),\n+        (\"macro\", 1.0, 0.6555555),\n+        (\"macro\", 2.0, 0.6548822),\n+        (\"weighted\", 0.5, 0.6825397),\n+        (\"weighted\", 1.0, 0.6555555),\n+        (\"weighted\", 2.0, 0.6548822),\n+    )\n+    def test_fbeta_random_score(self, average, beta, result):\n+        y_pred = [[0.7, 0.7, 0.7], [1, 0, 0], [0.9, 0.8, 0]]\n+        y_true = [[0, 0, 1], [1, 1, 0], [1, 1, 1]]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            None,\n+            average=average,\n+            beta=beta,\n+            threshold=0.66,\n+            reference_result=result,\n+        )\n+\n+    @parameterized.parameters(\n+        # average, beta, result\n+        (None, 0.5, [0.9090904, 0.555556, 1.0]),\n+        (None, 1.0, [0.8, 0.6666667, 1.0]),\n+        (None, 2.0, [0.71428573, 0.833334, 1.0]),\n+        (\"micro\", 0.5, 0.833334),\n+        (\"micro\", 1.0, 0.833334),\n+        (\"micro\", 2.0, 0.833334),\n+        (\"macro\", 0.5, 0.821549),\n+        (\"macro\", 1.0, 0.822222),\n+        (\"macro\", 2.0, 0.849206),\n+        (\"weighted\", 0.5, 0.880471),\n+        (\"weighted\", 1.0, 0.844445),\n+        (\"weighted\", 2.0, 0.829365),\n+    )\n+    def test_fbeta_random_score_none(self, average, beta, result):\n+        y_true = [\n+            [1, 0, 0],\n+            [0, 1, 0],\n+            [0, 0, 1],\n+            [1, 0, 0],\n+            [1, 0, 0],\n+            [0, 0, 1],\n+        ]\n+        y_pred = [\n+            [0.9, 0.1, 0],\n+            [0.2, 0.6, 0.2],\n+            [0, 0, 1],\n+            [0.4, 0.3, 0.3],\n+            [0, 0.9, 0.1],\n+            [0, 0, 1],\n+        ]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            None,\n+            average=average,\n+            beta=beta,\n+            threshold=None,\n+            reference_result=result,\n+        )\n+\n+    @parameterized.parameters(\n+        # average, beta, sample_weights, result\n+        (None, 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.909091, 0.555556, 1.0]),\n+        (None, 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0]),\n+        (None, 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], [0.9375, 0.714286, 1.0]),\n+        (None, 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.8, 0.666667, 1.0]),\n+        (None, 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0]),\n+        (None, 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], [0.857143, 0.8, 1.0]),\n+        (None, 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.714286, 0.833333, 1.0]),\n+        (None, 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0]),\n+        (None, 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], [0.789474, 0.909091, 1.0]),\n+        (\"micro\", 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.833333),\n+        (\"micro\", 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n+        (\"micro\", 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.9),\n+        (\"micro\", 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.833333),\n+        (\"micro\", 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n+        (\"micro\", 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.9),\n+        (\"micro\", 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.833333),\n+        (\"micro\", 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n+        (\"micro\", 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.9),\n+        (\"macro\", 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.821549),\n+        (\"macro\", 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 0.666667),\n+        (\"macro\", 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.883929),\n+        (\"macro\", 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.822222),\n+        (\"macro\", 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 0.666667),\n+        (\"macro\", 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.885714),\n+        (\"macro\", 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.849206),\n+        (\"macro\", 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 0.666667),\n+        (\"macro\", 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.899522),\n+        (\"weighted\", 0.5, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.880471),\n+        (\"weighted\", 0.5, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n+        (\"weighted\", 0.5, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.917857),\n+        (\"weighted\", 1.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.844444),\n+        (\"weighted\", 1.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n+        (\"weighted\", 1.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.902857),\n+        (\"weighted\", 2.0, [1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 0.829365),\n+        (\"weighted\", 2.0, [1.0, 0.0, 1.0, 1.0, 0.0, 1.0], 1.0),\n+        (\"weighted\", 2.0, [0.5, 1.0, 1.0, 1.0, 0.5, 1.0], 0.897608),\n+    )\n+    def test_fbeta_weighted_random_score_none(\n+        self, average, beta, sample_weights, result\n+    ):\n+        y_true = [\n+            [1, 0, 0],\n+            [0, 1, 0],\n+            [0, 0, 1],\n+            [1, 0, 0],\n+            [1, 0, 0],\n+            [0, 0, 1],\n+        ]\n+        y_pred = [\n+            [0.9, 0.1, 0],\n+            [0.2, 0.6, 0.2],\n+            [0, 0, 1],\n+            [0.4, 0.3, 0.3],\n+            [0, 0.9, 0.1],\n+            [0, 0, 1],\n+        ]\n+        self._run_test(\n+            y_true,\n+            y_pred,\n+            sample_weights,\n+            average=average,\n+            beta=beta,\n+            threshold=None,\n+            reference_result=result,\n+        )\n+\n+\n+class F1ScoreTest(testing.TestCase):\n+    def test_config(self):\n+        f1_obj = f_score_metrics.F1Score(dtype=\"float32\")\n+        config = f1_obj.get_config()\n+        self.assertNotIn(\"beta\", config)\n+\n+        # Check save and restore config\n+        f1_obj = f_score_metrics.F1Score.from_config(config)\n+        self.assertEqual(f1_obj.average, None)\n+        self.assertEqual(f1_obj.dtype, \"float32\")\n+\n+    def test_correctness(self):\n+        f1 = f_score_metrics.F1Score()\n+        fbeta = f_score_metrics.FBetaScore(beta=1.0)\n+\n+        y_true = np.array(\n+            [\n+                [1, 0, 0],\n+                [0, 1, 0],\n+                [0, 0, 1],\n+                [1, 0, 0],\n+                [1, 0, 0],\n+                [0, 0, 1],\n+            ]\n+        )\n+        y_pred = np.array(\n+            [\n+                [0.9, 0.1, 0],\n+                [0.2, 0.6, 0.2],\n+                [0, 0, 1],\n+                [0.4, 0.3, 0.3],\n+                [0, 0.9, 0.1],\n+                [0, 0, 1],\n+            ]\n+        )\n+\n+        fbeta.update_state(y_true, y_pred)\n+        f1.update_state(y_true, y_pred)\n+        self.assertAllClose(fbeta.result(), f1.result(), atol=1e-6)\n\n@@ -203,7 +203,7 @@ class RootMeanSquaredError(reduction_metrics.Mean):\n     >>> m.reset_state()\n     >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n     ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n+    >>> m.result()\n     0.70710677\n \n     Usage with `compile()` API:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6a1077272b5a0ee5569bf55e3bb8bf567e45794", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 565 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -278,6 +278,8 @@ class LayerTest(testing.TestCase):\n         x._keras_mask = backend.numpy.ones((4,))\n         layer(x)\n \n+        layer(backend.numpy.ones((4, 4)), mask=backend.numpy.ones((4,)))\n+\n         class NestedInputMaskedLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n@@ -297,6 +299,11 @@ class LayerTest(testing.TestCase):\n         x2._keras_mask = backend.numpy.ones((4,))\n         layer([x1, x2])\n \n+        layer(\n+            [backend.numpy.ones((4, 4)), backend.numpy.ones((4, 4))],\n+            mask=[backend.numpy.ones((4,)), backend.numpy.ones((4,))],\n+        )\n+\n         class PositionalInputsMaskedLayer(layers.Layer):\n             def __init__(self):\n                 super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#387302903523b3ae4fb1d2f1b63009c0486b1893", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 155 | Lines Deleted: 0 | Files Changed: 7 | Hunks: 10 | Methods Changed: 13 | Complexity Δ (Sum/Max): 20/7 | Churn Δ: 155 | Churn Cumulative: 301 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import jax\n+import jax.numpy as jnp\n \n \n def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n@@ -20,3 +21,12 @@ def in_top_k(targets, predictions, k):\n     targets = targets[..., None]\n     mask = targets == topk_indices\n     return jax.numpy.any(mask, axis=1)\n+\n+\n+def logsumexp(x, axis=None, keepdims=False):\n+    max_x = jnp.max(x, axis=axis, keepdims=True)\n+    result = (\n+        jnp.log(jnp.sum(jnp.exp(x - max_x), axis=axis, keepdims=keepdims))\n+        + max_x\n+    )\n+    return jnp.squeeze(result) if not keepdims else result\n\n@@ -14,3 +14,7 @@ def top_k(x, k, sorted=True):\n \n def in_top_k(targets, predictions, k):\n     return tf.math.in_top_k(targets, predictions, k)\n+\n+\n+def logsumexp(x, axis=None, keepdims=False):\n+    return tf.math.reduce_logsumexp(x, axis=axis, keepdims=keepdims)\n\n@@ -2,3 +2,4 @@ from keras_core.layers.activations.elu import ELU\n from keras_core.layers.activations.leaky_relu import LeakyReLU\n from keras_core.layers.activations.prelu import PReLU\n from keras_core.layers.activations.relu import ReLU\n+from keras_core.layers.activations.softmax import Softmax\n\n@@ -0,0 +1,66 @@\n+from keras_core import activations\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Softmax\")\n+class Softmax(Layer):\n+    \"\"\"Softmax activation layer.\n+\n+    Formula:\n+    ``` python\n+    exp_x = exp(x - max(x))\n+    f(x) = exp_x / sum(exp_x)\n+    ```\n+\n+    Example:\n+    >>>softmax_layer = Softmax()\n+    >>>input = np.array([1.0, 2.0, 1.0])\n+    >>>result = softmax_layer(input)\n+    [0.21194157, 0.5761169, 0.21194157]\n+\n+\n+    Args:\n+        axis: Integer, or list of Integers, axis along which the softmax\n+            normalization is applied.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+\n+    Call arguments:\n+        inputs: The inputs, or logits to the softmax layer.\n+        mask: A boolean mask of the same shape as `inputs`. The mask\n+            specifies 1 to keep and 0 to mask. Defaults to `None`.\n+\n+    Returns:\n+        Softmaxed output with the same shape as `inputs`.\n+    \"\"\"\n+\n+    def __init__(self, axis=-1, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.axis = axis\n+\n+    def call(self, inputs, mask=None):\n+        if mask is not None:\n+            adder = (1.0 - backend.cast(mask, inputs.dtype)) * (-1e9)\n+            inputs += adder\n+        if isinstance(self.axis, (tuple, list)):\n+            if len(self.axis) > 1:\n+                return backend.numpy.exp(\n+                    inputs\n+                    - backend.math.logsumexp(\n+                        inputs, axis=self.axis, keepdims=True\n+                    )\n+                )\n+            else:\n+                return activations.softmax(inputs, axis=self.axis[0])\n+        return activations.softmax(inputs, axis=self.axis)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"axis\": self.axis})\n+        return config\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n\n@@ -0,0 +1,47 @@\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core.layers.activations import softmax\n+\n+\n+class SoftmaxTest(testing.TestCase):\n+    def test_softmax(self):\n+        self.run_layer_test(\n+            softmax.Softmax,\n+            init_kwargs={},\n+            input_shape=(2, 3, 4),\n+            supports_masking=True,\n+        )\n+\n+    def test_softmax_correctness(self):\n+        softmax_layer = softmax.Softmax()\n+        input = np.array([[1.0, 2.0, 1.0], [1.0, 2.0, 1.0]])\n+        expected_output = np.array(\n+            [\n+                [0.21194157, 0.5761169, 0.21194157],\n+                [0.21194157, 0.5761169, 0.21194157],\n+            ]\n+        )\n+        result = softmax_layer(input)\n+        self.assertAllClose(result, expected_output)\n+\n+    def test_softmax_correctness_with_mask(self):\n+        softmax_layer = softmax.Softmax(axis=(1, 0))\n+        input = np.array([[1.0, 2.0, 1.0], [1.0, 2.0, 1.0]])\n+        expected_output = np.array(\n+            [[0.21194154, 0.0, 0.21194154], [0.0, 0.57611686, 0.0]]\n+        )\n+        result = softmax_layer(input, mask=[[1.0, 0.0, 1.0], [0.0, 1.0, 0.0]])\n+        self.assertAllClose(result, expected_output)\n+\n+    def test_softmax_correctness_with_axis(self):\n+        softmax_layer = softmax.Softmax(axis=(1))\n+        input = np.array([[1.0, 2.0, 1.0], [1.0, 2.0, 1.0]])\n+        expected_output = np.array(\n+            [\n+                [0.21194157, 0.5761169, 0.21194157],\n+                [0.21194157, 0.5761169, 0.21194157],\n+            ]\n+        )\n+        result = softmax_layer(input)\n+        self.assertAllClose(result, expected_output)\n\n@@ -41,3 +41,14 @@ def in_top_k(targets, predictions, k):\n     if any_symbolic_tensors((targets, predictions)):\n         return InTopK().symbolic_call(targets, predictions, k)\n     return backend.math.in_top_k(targets, predictions, k)\n+\n+\n+def logsumexp(x, axis=None, keepdims=False):\n+    if any_symbolic_tensors((x,)):\n+        return Logsumexp().symbolic_call(x, axis=axis, keepdims=keepdims)\n+    return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n+\n+\n+class Logsumexp(Operation):\n+    def call(self, x, axis=None, keepdims=False):\n+        return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n\n@@ -18,6 +18,11 @@ class MathOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(values.shape, (None, 2, 1))\n         self.assertEqual(indices.shape, (None, 2, 1))\n \n+    def test_logsumexp(self):\n+        x = KerasTensor([None, 2, 3], dtype=\"float32\")\n+        result = kmath.logsumexp(x)\n+        self.assertEqual(result.shape, ())\n+\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     def test_topk(self):\n@@ -26,6 +31,11 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(values.shape, (1, 2, 1))\n         self.assertEqual(indices.shape, (1, 2, 1))\n \n+    def test_logsumexp(self):\n+        x = KerasTensor([1, 2, 3], dtype=\"float32\")\n+        result = kmath.logsumexp(x)\n+        self.assertEqual(result.shape, ())\n+\n \n class MathOpsCorrectnessTest(testing.TestCase):\n     def test_topk(self):\n@@ -33,3 +43,9 @@ class MathOpsCorrectnessTest(testing.TestCase):\n         values, indices = kmath.top_k(x, k=2)\n         self.assertAllClose(values, [4, 3])\n         self.assertAllClose(indices, [1, 4])\n+\n+    def test_logsumexp(self):\n+        x_np = np.random.rand(5, 5)\n+        y_tf_np = kmath.logsumexp(x_np)\n+        y_np = np.log(np.sum(np.exp(x_np)))\n+        self.assertAllClose(y_tf_np, y_np)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2623e2e9816a51bc37eaae8e6561c508fcec28f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 375 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 14 | Complexity Δ (Sum/Max): 42/38 | Churn Δ: 375 | Churn Cumulative: 476 | Contributors (this commit): 8 | Commits (past 90d): 47 | Contributors (cumulative): 10 | DMM Complexity: 0.7729257641921398\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras_core.layers.activations.activation import Activation\n+from keras_core.layers.attention.attention import Attention\n from keras_core.layers.convolutional.conv1d import Conv1D\n from keras_core.layers.convolutional.conv2d import Conv2D\n from keras_core.layers.convolutional.conv3d import Conv3D\n\n@@ -0,0 +1,275 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Attention\")\n+class Attention(Layer):\n+    \"\"\"Dot-product attention layer, a.k.a. Luong-style attention.\n+\n+    Inputs are a list with 2 or 3 elements:\n+    1. A query tensor of shape `(batch_size, Tq, dim)`.\n+    2. A value tensor of shape `(batch_size, Tv, dim)`.\n+    3. A optional key tensor of shape `(batch_size, Tv, dim)`. If none supplied,\n+        the value tensor will be used as a key.\n+\n+    The calculation follows the steps:\n+    1. Calculate attention scores using query and key with shape\n+        `(batch_size, Tq, Tv)`.\n+    2. Use scores to calculate a softmax distribution with shape\n+        `(batch_size, Tq, Tv)`.\n+    3. Use the softmax distribution to create a linear combination of value\n+        with shape `(batch_size, Tq, dim)`.\n+\n+    Args:\n+        use_scale: If `True`, will create a scalar variable to scale the\n+            attention scores.\n+        dropout: Float between 0 and 1. Fraction of the units to drop for the\n+            attention scores. Defaults to 0.0.\n+        score_mode: Function to use to compute attention scores, one of\n+            `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the\n+            query and key vectors. `\"concat\"` refers to the hyperbolic tangent\n+            of the concatenation of the query and key vectors.\n+\n+    Call Args:\n+        inputs: List of the following tensors:\n+            - query: Query `Tensor` of shape `(batch_size, Tq, dim)`.\n+            - value: Value `Tensor` of shape `(batch_size, Tv, dim)`.\n+            - key: Optional key `Tensor` of shape `(batch_size, Tv, dim)`. If\n+                not given, will use `value` for both `key` and `value`, which is\n+                the most common case.\n+        mask: List of the following tensors:\n+            - query_mask: A boolean mask `Tensor` of shape `(batch_size, Tq)`.\n+                If given, the output will be zero at the positions where\n+                `mask==False`.\n+            - value_mask: A boolean mask `Tensor` of shape `(batch_size, Tv)`.\n+                If given, will apply the mask such that values at positions\n+                 where `mask==False` do not contribute to the result.\n+        return_attention_scores: bool, it `True`, returns the attention scores\n+            (after masking and softmax) as an additional output argument.\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding dropout) or in inference mode (no dropout).\n+        use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds\n+            a mask such that position `i` cannot attend to positions `j > i`.\n+            This prevents the flow of information from the future towards the\n+            past. Defaults to `False`.\n+\n+    Output:\n+        Attention outputs of shape `(batch_size, Tq, dim)`.\n+        (Optional) Attention scores after masking and softmax with shape\n+            `(batch_size, Tq, Tv)`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        use_scale=False,\n+        score_mode=\"dot\",\n+        dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.use_scale = use_scale\n+        self.score_mode = score_mode\n+        self.dropout = dropout\n+        if self.score_mode not in [\"dot\", \"concat\"]:\n+            raise ValueError(\n+                \"Invalid value for argument score_mode. \"\n+                \"Expected one of {'dot', 'concat'}. \"\n+                f\"Received: score_mode={score_mode}\"\n+            )\n+\n+    def build(self, input_shape):\n+        self.scale = None\n+        self.concat_score_weight = None\n+        if self.use_scale:\n+            self.scale = self.add_weight(\n+                name=\"scale\",\n+                shape=(),\n+                initializer=\"ones\",\n+                dtype=self.dtype,\n+                trainable=True,\n+            )\n+        if self.score_mode == \"concat\":\n+            self.concat_score_weight = self.add_weight(\n+                name=\"concat_score_weight\",\n+                shape=(),\n+                initializer=\"ones\",\n+                dtype=self.dtype,\n+                trainable=True,\n+            )\n+        self.built = True\n+\n+    def _calculate_scores(self, query, key):\n+        \"\"\"Calculates attention scores as a query-key dot product.\n+\n+        Args:\n+            query: Query tensor of shape `(batch_size, Tq, dim)`.\n+            key: Key tensor of shape `(batch_size, Tv, dim)`.\n+\n+        Returns:\n+            Tensor of shape `(batch_size, Tq, Tv)`.\n+        \"\"\"\n+        if self.score_mode == \"dot\":\n+            scores = ops.matmul(query, ops.transpose(key, axes=[0, 2, 1]))\n+            if self.scale is not None:\n+                scores *= self.scale\n+        elif self.score_mode == \"concat\":\n+            # Reshape tensors to enable broadcasting.\n+            # Reshape into [batch_size, Tq, 1, dim].\n+            q_reshaped = ops.expand_dims(query, axis=-2)\n+            # Reshape into [batch_size, 1, Tv, dim].\n+            k_reshaped = ops.expand_dims(key, axis=-3)\n+            if self.scale is not None:\n+                scores = self.concat_score_weight * ops.sum(\n+                    ops.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1\n+                )\n+            else:\n+                scores = self.concat_score_weight * ops.sum(\n+                    ops.tanh(q_reshaped + k_reshaped), axis=-1\n+                )\n+\n+        return scores\n+\n+    def _apply_scores(self, scores, value, scores_mask=None, training=False):\n+        \"\"\"Applies attention scores to the given value tensor.\n+\n+        To use this method in your attention layer, follow the steps:\n+\n+        * Use `query` tensor of shape `(batch_size, Tq)` and `key` tensor of\n+            shape `(batch_size, Tv)` to calculate the attention `scores`.\n+        * Pass `scores` and `value` tensors to this method. The method applies\n+            `scores_mask`, calculates\n+            `attention_distribution = softmax(scores)`, then returns\n+            `matmul(attention_distribution, value).\n+        * Apply `query_mask` and return the result.\n+\n+        Args:\n+            scores: Scores float tensor of shape `(batch_size, Tq, Tv)`.\n+            value: Value tensor of shape `(batch_size, Tv, dim)`.\n+            scores_mask: A boolean mask `Tensor` of shape `(batch_size, 1, Tv)`\n+                or `(batch_size, Tq, Tv)`. If given, scores at positions where\n+                `scores_mask==False` do not contribute to the result. It must\n+                contain at least one `True` value in each line along the last\n+                dimension.\n+            training: Python boolean indicating whether the layer should behave\n+                in training mode (adding dropout) or in inference mode\n+                (no dropout).\n+\n+        Returns:\n+            Tensor of shape `(batch_size, Tq, dim)`.\n+            Attention scores after masking and softmax with shape\n+                `(batch_size, Tq, Tv)`.\n+        \"\"\"\n+        if scores_mask is not None:\n+            padding_mask = ops.logical_not(scores_mask)\n+            # Bias so padding positions do not contribute to attention\n+            # distribution.  Note 65504. is the max float16 value.\n+            max_value = 65504.0 if scores.dtype == \"float16\" else 1.0e9\n+            scores -= max_value * ops.cast(padding_mask, dtype=scores.dtype)\n+\n+        weights = ops.softmax(scores, axis=-1)\n+        if training and self.dropout > 0:\n+            weights = backend.random.dropout(\n+                weights,\n+                self.dropout,\n+                noise_shape=self.noise_shape,\n+                seed=self.seed_generator,\n+            )\n+        return ops.matmul(weights, value), weights\n+\n+    def _calculate_score_mask(self, scores, v_mask, use_causal_mask):\n+        if v_mask is not None:\n+            # Mask of shape [batch_size, 1, Tv].\n+            v_mask = ops.expand_dims(v_mask, axis=-2)\n+        if not use_causal_mask:\n+            return v_mask\n+\n+        # Creates a lower triangular mask, so position i cannot attend to\n+        # positions j>i. This prevents the flow of information from the\n+        # future into the past.\n+        score_shape = ops.shape(scores)\n+        # causal_mask_shape = [1, Tq, Tv].\n+        mask_shape = (1, score_shape[-2], score_shape[-1])\n+        ones_mask = ops.ones(shape=mask_shape, dtype=\"int32\")\n+        row_index = ops.cumsum(ones_mask, axis=-2)\n+        col_index = ops.cumsum(ones_mask, axis=-1)\n+        causal_mask = ops.greater_equal(row_index, col_index)\n+\n+        if v_mask is not None:\n+            return causal_mask\n+        return ops.logical_and(v_mask, causal_mask)\n+\n+    def call(\n+        self,\n+        inputs,\n+        mask=None,\n+        training=False,\n+        return_attention_scores=False,\n+        use_causal_mask=False,\n+    ):\n+        self._validate_call_args(inputs=inputs, mask=mask)\n+        q = inputs[0]\n+        v = inputs[1]\n+        k = inputs[2] if len(inputs) > 2 else v\n+        q_mask = mask[0] if mask else None\n+        v_mask = mask[1] if mask else None\n+        scores = self._calculate_scores(query=q, key=k)\n+        scores_mask = self._calculate_score_mask(\n+            scores, v_mask, use_causal_mask\n+        )\n+        result, attention_scores = self._apply_scores(\n+            scores=scores, value=v, scores_mask=scores_mask, training=training\n+        )\n+        if q_mask is not None:\n+            # Mask of shape [batch_size, Tq, 1].\n+            q_mask = ops.expand_dims(q_mask, axis=-1)\n+            result *= ops.cast(q_mask, dtype=result.dtype)\n+        if return_attention_scores:\n+            return result, attention_scores\n+        return result\n+\n+    def compute_mask(self, inputs, mask=None):\n+        self._validate_call_args(inputs=inputs, mask=mask)\n+        if mask is None or mask[0] is None:\n+            return None\n+        return ops.convert_to_tensor(mask[0])\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape[0]\n+\n+    def _validate_call_args(self, inputs, mask):\n+        \"\"\"Validates arguments of the call method.\"\"\"\n+        class_name = self.__class__.__name__\n+        if not isinstance(inputs, list):\n+            raise ValueError(\n+                f\"{class_name} layer must be called on a list of inputs, \"\n+                \"namely [query, value] or [query, value, key]. \"\n+                f\"Received: inputs={inputs}.\"\n+            )\n+        if len(inputs) < 2 or len(inputs) > 3:\n+            raise ValueError(\n+                f\"{class_name} layer accepts inputs list of length 2 or 3, \"\n+                \"namely [query, value] or [query, value, key]. \"\n+                f\"Received length: {len(inputs)}.\"\n+            )\n+        if mask is not None:\n+            if not isinstance(mask, list):\n+                raise ValueError(\n+                    f\"{class_name} layer mask must be a list, \"\n+                    f\"namely [query_mask, value_mask]. Received: mask={mask}.\"\n+                )\n+            if len(mask) < 2 or len(mask) > 3:\n+                raise ValueError(\n+                    f\"{class_name} layer accepts mask list of length 2 or 3. \"\n+                    f\"Received: inputs={inputs}, mask={mask}.\"\n+                )\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"use_scale\": self.use_scale,\n+            \"score_mode\": self.score_mode,\n+            \"dropout\": self.dropout,\n+        }\n+        return {**base_config, **config}\n\n@@ -0,0 +1,99 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class DenseTest(testing.TestCase):\n+    def test_attention_basics(self):\n+        # No scale, no concat.\n+        self.run_layer_test(\n+            layers.Attention,\n+            init_kwargs={\n+                \"score_mode\": \"dot\",\n+                \"dropout\": 0.5,\n+            },\n+            input_shape=[(2, 3, 4), (2, 4, 4)],\n+            expected_output_shape=(2, 3, 4),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+        # Sale and concat.\n+        self.run_layer_test(\n+            layers.Attention,\n+            init_kwargs={\n+                \"use_scale\": True,\n+                \"score_mode\": \"concat\",\n+                \"dropout\": 0.5,\n+            },\n+            input_shape=[(2, 3, 4), (2, 4, 4)],\n+            expected_output_shape=(2, 3, 4),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_attention_correctness(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+\n+        # Dot.\n+        layer = layers.Attention(score_mode=\"dot\")\n+        output, scores = layer(\n+            [query, value, key],\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(\n+            output, [[[2.462, 3.462], [1.538, 2.538]]], atol=1e-3\n+        )\n+        self.assertAllClose(\n+            scores, [[[0.269, 0.731], [0.731, 0.269]]], atol=1e-3\n+        )\n+\n+        # Concat.\n+        layer = layers.Attention(score_mode=\"concat\")\n+        output, scores = layer(\n+            [query, value, key],\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(\n+            output, [[[1.727, 2.727], [2.272, 3.272]]], atol=1e-3\n+        )\n+        self.assertAllClose(\n+            scores, [[[0.636, 0.363], [0.363, 0.636]]], atol=1e-3\n+        )\n+\n+    def test_attention_with_mask(self):\n+        layer = layers.Attention()\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        value = np.array([[[1.0, 1.0], [1.0, 1.0]]])\n+        query_mask = np.array([[True, False]])\n+        value_mask = np.array([[True, False]])\n+        output, scores = layer(\n+            [query, value],\n+            mask=[query_mask, value_mask],\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(output, [[[1.0, 1.0], [0.0, 0.0]]])\n+        self.assertAllClose(scores, [[[1.0, 0.0], [1.0, 0.0]]])\n+\n+    def test_attention_errors(self):\n+        layer = layers.Attention()\n+        tensor = np.array([[[1.0, 1.0], [1.0, 1.0]]])\n+        with self.assertRaisesRegex(ValueError, \"must be called on a list\"):\n+            layer(tensor)\n+\n+        with self.assertRaisesRegex(ValueError, \"length 2 or 3\"):\n+            layer([tensor, tensor, tensor, tensor])\n+\n+        with self.assertRaisesRegex(ValueError, \"layer mask must be a list\"):\n+            layer([tensor, tensor], mask=tensor)\n+\n+        with self.assertRaisesRegex(ValueError, \"length 2 or 3\"):\n+            layer([tensor, tensor], mask=[tensor])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#388645a08276a7ee8e9950e03136c8c1f002d553", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 24 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 469 | Contributors (this commit): 5 | Commits (past 90d): 38 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,45 +1,76 @@\n from keras_core.api_export import keras_core_export\n from keras_core.losses.loss import Loss\n+from keras_core.losses.losses import BinaryCrossentropy\n+from keras_core.losses.losses import CategoricalCrossentropy\n from keras_core.losses.losses import CategoricalHinge\n+from keras_core.losses.losses import CosineSimilarity\n from keras_core.losses.losses import Hinge\n+from keras_core.losses.losses import Huber\n from keras_core.losses.losses import KLDivergence\n+from keras_core.losses.losses import LogCosh\n from keras_core.losses.losses import LossFunctionWrapper\n from keras_core.losses.losses import MeanAbsoluteError\n from keras_core.losses.losses import MeanAbsolutePercentageError\n from keras_core.losses.losses import MeanSquaredError\n from keras_core.losses.losses import MeanSquaredLogarithmicError\n from keras_core.losses.losses import Poisson\n+from keras_core.losses.losses import SparseCategoricalCrossentropy\n from keras_core.losses.losses import SquaredHinge\n+from keras_core.losses.losses import binary_crossentropy\n+from keras_core.losses.losses import categorical_crossentropy\n from keras_core.losses.losses import categorical_hinge\n+from keras_core.losses.losses import cosine_similarity\n from keras_core.losses.losses import hinge\n+from keras_core.losses.losses import huber\n from keras_core.losses.losses import kl_divergence\n+from keras_core.losses.losses import log_cosh\n from keras_core.losses.losses import mean_absolute_error\n from keras_core.losses.losses import mean_absolute_percentage_error\n from keras_core.losses.losses import mean_squared_error\n from keras_core.losses.losses import mean_squared_logarithmic_error\n from keras_core.losses.losses import poisson\n+from keras_core.losses.losses import sparse_categorical_crossentropy\n from keras_core.losses.losses import squared_hinge\n from keras_core.saving import serialization_lib\n \n ALL_OBJECTS = {\n+    # Base\n     Loss,\n     LossFunctionWrapper,\n+    # Probabilistic\n     KLDivergence,\n+    Poisson,\n+    BinaryCrossentropy,\n+    CategoricalCrossentropy,\n+    SparseCategoricalCrossentropy,\n+    # Regression\n     MeanSquaredError,\n     MeanAbsoluteError,\n     MeanAbsolutePercentageError,\n     MeanSquaredLogarithmicError,\n+    CosineSimilarity,\n+    LogCosh,\n+    Huber,\n+    # Hinge\n     Hinge,\n-    Poisson,\n     SquaredHinge,\n     CategoricalHinge,\n+    # Probabilistic\n     kl_divergence,\n+    poisson,\n+    binary_crossentropy,\n+    categorical_crossentropy,\n+    sparse_categorical_crossentropy,\n+    # Regression\n     mean_squared_error,\n     mean_absolute_error,\n     mean_absolute_percentage_error,\n     mean_squared_logarithmic_error,\n+    cosine_similarity,\n+    log_cosh,\n+    huber,\n+    # Hinge\n     hinge,\n-    poisson,\n     squared_hinge,\n     categorical_hinge,\n }\n\n@@ -16,6 +16,8 @@ from keras_core.metrics.confusion_metrics import SensitivityAtSpecificity\n from keras_core.metrics.confusion_metrics import SpecificityAtSensitivity\n from keras_core.metrics.confusion_metrics import TrueNegatives\n from keras_core.metrics.confusion_metrics import TruePositives\n+from keras_core.metrics.f_score_metrics import F1Score\n+from keras_core.metrics.f_score_metrics import FBetaScore\n from keras_core.metrics.hinge_metrics import CategoricalHinge\n from keras_core.metrics.hinge_metrics import Hinge\n from keras_core.metrics.hinge_metrics import SquaredHinge\n@@ -30,7 +32,14 @@ from keras_core.metrics.probabilistic_metrics import (\n from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n from keras_core.metrics.reduction_metrics import Sum\n+from keras_core.metrics.regression_metrics import CosineSimilarity\n+from keras_core.metrics.regression_metrics import LogCoshError\n+from keras_core.metrics.regression_metrics import MeanAbsoluteError\n+from keras_core.metrics.regression_metrics import MeanAbsolutePercentageError\n from keras_core.metrics.regression_metrics import MeanSquaredError\n+from keras_core.metrics.regression_metrics import MeanSquaredLogarithmicError\n+from keras_core.metrics.regression_metrics import R2Score\n+from keras_core.metrics.regression_metrics import RootMeanSquaredError\n from keras_core.saving import serialization_lib\n \n ALL_OBJECTS = {\n@@ -41,6 +50,13 @@ ALL_OBJECTS = {\n     MeanMetricWrapper,\n     # Regression\n     MeanSquaredError,\n+    RootMeanSquaredError,\n+    MeanAbsoluteError,\n+    MeanAbsolutePercentageError,\n+    MeanSquaredLogarithmicError,\n+    CosineSimilarity,\n+    LogCoshError,\n+    R2Score,\n     # Classification\n     AUC,\n     FalseNegatives,\n@@ -70,6 +86,9 @@ ALL_OBJECTS = {\n     SparseCategoricalAccuracy,\n     TopKCategoricalAccuracy,\n     SparseTopKCategoricalAccuracy,\n+    # F-Score\n+    F1Score,\n+    FBetaScore,\n }\n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n ALL_OBJECTS_DICT.update(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
