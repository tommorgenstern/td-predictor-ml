{"custom_id": "keras#b2f989889cf7f68f8573eed70875e283dbdaf2a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 279 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -99,7 +99,7 @@ def conv_output_length(input_length, filter_size,\n     if input_length is None:\n         return None\n     assert padding in {'same', 'valid', 'full', 'causal'}\n-    dilated_filter_size = filter_size + (filter_size - 1) * (dilation - 1)\n+    dilated_filter_size = (filter_size - 1) * dilation + 1\n     if padding == 'same':\n         output_length = input_length\n     elif padding == 'valid':\n@@ -157,7 +157,7 @@ def deconv_length(dim_size, stride_size, kernel_size, padding,\n         return None\n \n     # Get the dilated kernel size\n-    kernel_size = kernel_size + (kernel_size - 1) * (dilation - 1)\n+    kernel_size = (kernel_size - 1) * dilation + 1\n \n     # Infer length if output padding is None, else compute the exact length\n     if output_padding is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa230033d4da69595962816ffc752b5816062fa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 2385 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -491,8 +491,6 @@ class ConvLSTM2DCell(Layer):\n             incompatible with specifying any `strides` value != 1.\n         activation: Activation function to use\n             (see [activations](../activations.md)).\n-            If you don't specify anything, no activation is applied\n-            (ie. \"linear\" activation: `a(x) = x`).\n         recurrent_activation: Activation function to use\n             for the recurrent step\n             (see [activations](../activations.md)).\n@@ -817,8 +815,6 @@ class ConvLSTM2D(ConvRNN2D):\n             incompatible with specifying any `strides` value != 1.\n         activation: Activation function to use\n             (see [activations](../activations.md)).\n-            If you don't specify anything, no activation is applied\n-            (ie. \"linear\" activation: `a(x) = x`).\n         recurrent_activation: Activation function to use\n             for the recurrent step\n             (see [activations](../activations.md)).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#910e1247b452f29c0297ec6e3bf52b28b24fefcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 468 | Lines Deleted: 245 | Files Changed: 10 | Hunks: 56 | Methods Changed: 31 | Complexity Δ (Sum/Max): 31/39 | Churn Δ: 713 | Churn Cumulative: 20394 | Contributors (this commit): 161 | Commits (past 90d): 31 | Contributors (cumulative): 249 | DMM Complexity: 0.71875\n\nDIFF:\n@@ -17,6 +17,7 @@ from functools import wraps\n import numpy as np\n \n from .. import backend as K\n+from .. import losses\n from .. import optimizers\n from ..utils.io_utils import H5Dict\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n@@ -347,7 +348,10 @@ def _deserialize_model(h5dict, custom_objects=None, compile=True):\n                                            custom_objects=custom_objects)\n \n         # Recover loss functions and metrics.\n-        loss = convert_custom_objects(training_config['loss'])\n+        loss_config = training_config['loss']  # Deserialize loss class.\n+        if isinstance(loss_config, dict) and 'class_name' in loss_config:\n+            loss_config = losses.get(loss_config)\n+        loss = convert_custom_objects(loss_config)\n         metrics = convert_custom_objects(training_config['metrics'])\n         sample_weight_mode = training_config['sample_weight_mode']\n         loss_weights = training_config['loss_weights']\n\n@@ -10,17 +10,7 @@ import numpy as np\n \n from .network import Network\n from .base_layer import Layer\n-from .training_utils import collect_metrics\n-from .training_utils import check_array_length_consistency\n-from .training_utils import check_loss_and_target_compatibility\n-from .training_utils import check_generator_arguments\n-from .training_utils import standardize_class_weights\n-from .training_utils import standardize_input_data\n-from .training_utils import standardize_sample_weights\n-from .training_utils import standardize_weights\n-from .training_utils import weighted_masked_objective\n-from .training_utils import get_static_batch_size\n-from .training_utils import is_generator_or_sequence\n+from . import training_utils\n from . import training_arrays\n from . import training_generator\n from .. import backend as K\n@@ -51,8 +41,8 @@ class Model(Network):\n         # Arguments\n             optimizer: String (name of optimizer) or optimizer instance.\n                 See [optimizers](/optimizers).\n-            loss: String (name of objective function) or objective function.\n-                See [losses](/losses).\n+            loss: String (name of objective function) or objective function or\n+                `Loss` instance. See [losses](/losses).\n                 If the model has multiple outputs, you can use a different loss\n                 on each output by passing a dictionary or a list of losses.\n                 The loss value that will be minimized by the model\n@@ -98,7 +88,7 @@ class Model(Network):\n                 `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n         \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n-        self.loss = loss or []\n+        self.loss = loss or {}\n         self.metrics = metrics or []\n         self.loss_weights = loss_weights\n         self.sample_weight_mode = sample_weight_mode\n@@ -113,80 +103,30 @@ class Model(Network):\n             return\n         self._is_compiled = True\n \n-        # Prepare loss functions.\n-        if isinstance(loss, dict):\n-            for name in loss:\n-                if name not in self.output_names:\n-                    raise ValueError('Unknown entry in loss '\n-                                     'dictionary: \"' + name + '\". '\n-                                     'Only expected the following keys: ' +\n-                                     str(self.output_names))\n-            loss_functions = []\n-            for name in self.output_names:\n-                if name not in loss:\n-                    warnings.warn('Output \"' + name +\n-                                  '\" missing from loss dictionary. '\n-                                  'We assume this was done on purpose, '\n-                                  'and we will not be expecting '\n-                                  'any data to be passed to \"' + name +\n-                                  '\" during training.', stacklevel=2)\n-                loss_functions.append(losses.get(loss.get(name)))\n-        elif isinstance(loss, list):\n-            if len(loss) != len(self.outputs):\n-                raise ValueError('When passing a list as loss, '\n-                                 'it should have one entry per model outputs. '\n-                                 'The model has ' + str(len(self.outputs)) +\n-                                 ' outputs, but you passed loss=' +\n-                                 str(loss))\n-            loss_functions = [losses.get(l) for l in loss]\n-        else:\n-            loss_function = losses.get(loss)\n-            loss_functions = [loss_function for _ in range(len(self.outputs))]\n-        self.loss_functions = loss_functions\n-        weighted_losses = [\n-            weighted_masked_objective(fn) for fn in loss_functions]\n-        skip_target_indices = []\n-        skip_target_weighing_indices = []\n+        # Prepare list of loss functions, same size as model outputs.\n+        self.loss_functions = training_utils.prepare_loss_functions(\n+            self.loss, self.output_names)\n+\n         self._feed_outputs = []\n         self._feed_output_names = []\n         self._feed_output_shapes = []\n         self._feed_loss_fns = []\n-        for i in range(len(weighted_losses)):\n-            if weighted_losses[i] is None:\n+\n+        # if loss function is None, then this output will be skipped during total\n+        # loss calculation and feed targets preparation.\n+        skip_target_indices = []\n+        skip_target_weighing_indices = []\n+        for i, loss_function in enumerate(self.loss_functions):\n+            if loss_function is None:\n                 skip_target_indices.append(i)\n                 skip_target_weighing_indices.append(i)\n \n         # Prepare output masks.\n-        masks = self.compute_mask(self.inputs, mask=None)\n-        if masks is None:\n-            masks = [None for _ in self.outputs]\n-        masks = to_list(masks)\n+        masks = [getattr(x, '_keras_mask', None) for x in self.outputs]\n \n-        # Prepare loss weights.\n-        if loss_weights is None:\n-            loss_weights_list = [1. for _ in range(len(self.outputs))]\n-        elif isinstance(loss_weights, dict):\n-            for name in loss_weights:\n-                if name not in self.output_names:\n-                    raise ValueError('Unknown entry in loss_weights '\n-                                     'dictionary: \"' + name + '\". '\n-                                     'Only expected the following keys: ' +\n-                                     str(self.output_names))\n-            loss_weights_list = []\n-            for name in self.output_names:\n-                loss_weights_list.append(loss_weights.get(name, 1.))\n-        elif isinstance(loss_weights, list):\n-            if len(loss_weights) != len(self.outputs):\n-                raise ValueError('When passing a list as loss_weights, '\n-                                 'it should have one entry per model output. '\n-                                 'The model has ' + str(len(self.outputs)) +\n-                                 ' outputs, but you passed loss_weights=' +\n-                                 str(loss_weights))\n-            loss_weights_list = loss_weights\n-        else:\n-            raise TypeError('Could not interpret loss_weights argument: ' +\n-                            str(loss_weights) +\n-                            ' - expected a list of dicts.')\n+        # Prepare list loss weights, same size of model outputs.\n+        self.loss_weights_list = training_utils.prepare_loss_weights(\n+            self.output_names, loss_weights)\n \n         # Prepare targets of model.\n         self.targets = []\n@@ -250,124 +190,18 @@ class Model(Network):\n                 self.targets.append(target)\n \n         # Prepare sample weights.\n-        sample_weights = []\n-        sample_weight_modes = []\n-        if isinstance(sample_weight_mode, dict):\n-            for name in sample_weight_mode:\n-                if name not in self.output_names:\n-                    raise ValueError('Unknown entry in '\n-                                     'sample_weight_mode dictionary: \"' +\n-                                     name + '\". '\n-                                     'Only expected the following keys: ' +\n-                                     str(self.output_names))\n-            for i, name in enumerate(self.output_names):\n-                if i in skip_target_weighing_indices:\n-                    weight = None\n-                    sample_weight_modes.append(None)\n-                else:\n-                    if name not in sample_weight_mode:\n-                        raise ValueError('Output \"' + name +\n-                                         '\" missing from sample_weight_modes '\n-                                         'dictionary')\n-                    if sample_weight_mode.get(name) == 'temporal':\n-                        weight = K.placeholder(ndim=2,\n-                                               name=name + '_sample_weights')\n-                        sample_weight_modes.append('temporal')\n-                    else:\n-                        weight = K.placeholder(ndim=1,\n-                                               name=name + '_sample_weights')\n-                        sample_weight_modes.append(None)\n-                sample_weights.append(weight)\n-        elif isinstance(sample_weight_mode, list):\n-            if len(sample_weight_mode) != len(self.outputs):\n-                raise ValueError('When passing a list as sample_weight_mode, '\n-                                 'it should have one entry per model output. '\n-                                 'The model has ' + str(len(self.outputs)) +\n-                                 ' outputs, but you passed '\n-                                 'sample_weight_mode=' +\n-                                 str(sample_weight_mode))\n-            for i in range(len(self.output_names)):\n-                if i in skip_target_weighing_indices:\n-                    weight = None\n-                    sample_weight_modes.append(None)\n-                else:\n-                    mode = sample_weight_mode[i]\n-                    name = self.output_names[i]\n-                    if mode == 'temporal':\n-                        weight = K.placeholder(ndim=2,\n-                                               name=name + '_sample_weights')\n-                        sample_weight_modes.append('temporal')\n-                    else:\n-                        weight = K.placeholder(ndim=1,\n-                                               name=name + '_sample_weights')\n-                        sample_weight_modes.append(None)\n-                sample_weights.append(weight)\n-        else:\n-            for i, name in enumerate(self.output_names):\n-                if i in skip_target_weighing_indices:\n-                    sample_weight_modes.append(None)\n-                    sample_weights.append(None)\n-                else:\n-                    if sample_weight_mode == 'temporal':\n-                        sample_weights.append(\n-                            K.placeholder(ndim=2,\n-                                          name=name + '_sample_weights'))\n-                        sample_weight_modes.append('temporal')\n-                    else:\n-                        sample_weights.append(\n-                            K.placeholder(ndim=1,\n-                                          name=name + '_sample_weights'))\n-                        sample_weight_modes.append(None)\n-        self.sample_weight_modes = sample_weight_modes\n-        self._feed_sample_weight_modes = []\n-        for i in range(len(self.outputs)):\n-            if i not in skip_target_weighing_indices:\n-                self._feed_sample_weight_modes.append(\n-                    self.sample_weight_modes[i])\n+        self._set_sample_weight_attributes(\n+            sample_weight_mode, skip_target_weighing_indices)\n \n         # Prepare metrics.\n         self.metrics_names = ['loss']\n         self.metrics_tensors = []\n \n-        # Compute total loss.\n-        total_loss = None\n-        with K.name_scope('loss'):\n-            for i in range(len(self.outputs)):\n-                if i in skip_target_indices:\n-                    continue\n-                y_true = self.targets[i]\n-                y_pred = self.outputs[i]\n-                weighted_loss = weighted_losses[i]\n-                sample_weight = sample_weights[i]\n-                mask = masks[i]\n-                loss_weight = loss_weights_list[i]\n-                with K.name_scope(self.output_names[i] + '_loss'):\n-                    output_loss = weighted_loss(y_true, y_pred,\n-                                                sample_weight, mask)\n-                if len(self.outputs) > 1:\n-                    self.metrics_tensors.append(output_loss)\n-                    self.metrics_names.append(self.output_names[i] + '_loss')\n-                if total_loss is None:\n-                    total_loss = loss_weight * output_loss\n-                else:\n-                    total_loss += loss_weight * output_loss\n-            if total_loss is None:\n-                if not self.losses:\n-                    raise ValueError('The model cannot be compiled '\n-                                     'because it has no loss to optimize.')\n-                else:\n-                    total_loss = 0.\n-\n-            # Add regularization penalties\n-            # and other layer-specific losses.\n-            for loss_tensor in self.losses:\n-                total_loss += loss_tensor\n-\n         # List of same size as output_names.\n         # contains tuples (metrics for output, names of metrics).\n-        nested_metrics = collect_metrics(metrics, self.output_names)\n-        nested_weighted_metrics = collect_metrics(weighted_metrics,\n-                                                  self.output_names)\n+        nested_metrics = training_utils.collect_metrics(metrics, self.output_names)\n+        nested_weighted_metrics = training_utils.collect_metrics(\n+            weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n         self.stateful_metric_functions = []\n@@ -406,11 +240,13 @@ class Model(Network):\n                             suffix = 'acc'\n                     elif metric in ('crossentropy', 'ce'):\n                             suffix = 'ce'\n-                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n+                    weighted_metric_fn = training_utils.weighted_masked_objective(\n+                        metric_fn)\n                     metric_name = metric_name_prefix + suffix\n                 else:\n                     metric_fn = metrics_module.get(metric)\n-                    weighted_metric_fn = weighted_masked_objective(metric_fn)\n+                    weighted_metric_fn = training_utils.weighted_masked_objective(\n+                        metric_fn)\n                     # Get metric name as string\n                     if hasattr(metric_fn, 'name'):\n                         metric_name = metric_fn.name\n@@ -449,19 +285,18 @@ class Model(Network):\n \n                 y_true = self.targets[i]\n                 y_pred = self.outputs[i]\n-                weights = sample_weights[i]\n+                weights = self.sample_weights[i]\n                 output_metrics = nested_metrics[i]\n                 output_weighted_metrics = nested_weighted_metrics[i]\n                 handle_metrics(output_metrics)\n                 handle_metrics(output_weighted_metrics, weights=weights)\n \n-        # Prepare gradient updates and state updates.\n-        self.total_loss = total_loss\n-        self.sample_weights = sample_weights\n-        self._feed_sample_weights = []\n-        for i in range(len(self.sample_weights)):\n-            if i not in skip_target_weighing_indices:\n-                self._feed_sample_weights.append(sample_weights[i])\n+        # Compute total loss.\n+        # Used to keep track of the total loss value (stateless).\n+        # eg., total_loss = loss_weight_1 * output_1_loss_fn(...) +\n+        #                   loss_weight_2 * output_2_loss_fn(...) +\n+        #                   layer losses.\n+        self.total_loss = self._prepare_total_loss(skip_target_indices, masks)\n \n         # Functions for train, test and predict will\n         # be compiled lazily when required.\n@@ -750,7 +585,7 @@ class Model(Network):\n             feed_input_shapes = self._feed_input_shapes\n \n         # Standardize the inputs.\n-        x = standardize_input_data(\n+        x = training_utils.standardize_input_data(\n             x,\n             feed_input_names,\n             feed_input_shapes,\n@@ -770,25 +605,29 @@ class Model(Network):\n                 feed_output_shapes = []\n                 for output_shape, loss_fn in zip(self._feed_output_shapes,\n                                                  self._feed_loss_fns):\n-                    if loss_fn is losses.sparse_categorical_crossentropy:\n+                    if ((isinstance(loss_fn, losses.LossFunctionWrapper) and\n+                         loss_fn.fn == losses.sparse_categorical_crossentropy)) or (\n+                            isinstance(\n+                                loss_fn, losses.SparseCategoricalCrossentropy)):\n                         if K.image_data_format() == 'channels_first' and len(\n                                 output_shape) in [4, 5]:\n                             feed_output_shapes.append(\n                                 (output_shape[0], 1) + output_shape[2:])\n                         else:\n                             feed_output_shapes.append(output_shape[:-1] + (1,))\n-                    elif (not hasattr(loss_fn, '__name__') or\n-                            getattr(losses, loss_fn.__name__, None) is None):\n-                        # If `loss_fn` is not a function (e.g. callable class)\n-                        # or if it not in the `losses` module, then\n-                        # it is a user-defined loss and we make no assumptions\n-                        # about it.\n+                    elif (not isinstance(loss_fn, losses.Loss) or\n+                            (isinstance(loss_fn, losses.LossFunctionWrapper) and\n+                             (getattr(losses, loss_fn.fn.__name__, None) is None))):\n+                        # If the given loss is not an instance of the `Loss` class\n+                        # (custom class) or if the loss function that is wrapped is\n+                        # not in the `losses` module, then it is a user-defined loss\n+                        # and we make no assumptions about it.\n                         feed_output_shapes.append(None)\n                     else:\n                         feed_output_shapes.append(output_shape)\n \n             # Standardize the outputs.\n-            y = standardize_input_data(\n+            y = training_utils.standardize_input_data(\n                 y,\n                 feed_output_names,\n                 feed_output_shapes,\n@@ -797,23 +636,23 @@ class Model(Network):\n \n             # Generate sample-wise weight values given the `sample_weight` and\n             # `class_weight` arguments.\n-            sample_weights = standardize_sample_weights(\n+            sample_weights = training_utils.standardize_sample_weights(\n                 sample_weight, feed_output_names)\n-            class_weights = standardize_class_weights(\n+            class_weights = training_utils.standardize_class_weights(\n                 class_weight, feed_output_names)\n             sample_weights = [\n-                standardize_weights(ref, sw, cw, mode)\n+                training_utils.standardize_weights(ref, sw, cw, mode)\n                 for (ref, sw, cw, mode) in\n                 zip(y, sample_weights, class_weights,\n                     feed_sample_weight_modes)\n             ]\n             # Check that all arrays have the same length.\n             if check_array_lengths:\n-                check_array_length_consistency(x, y, sample_weights)\n+                training_utils.check_array_length_consistency(x, y, sample_weights)\n             if self._is_graph_network:\n                 # Additional checks to avoid users mistakenly\n                 # using improper loss fns.\n-                check_loss_and_target_compatibility(\n+                training_utils.check_loss_and_target_compatibility(\n                     y, self._feed_loss_fns, feed_output_shapes)\n         else:\n             y = []\n@@ -830,6 +669,65 @@ class Model(Network):\n                                  str(x[0].shape[0]) + ' samples')\n         return x, y, sample_weights\n \n+    def _prepare_total_loss(self, skip_target_indices=None, masks=None):\n+        \"\"\"Computes total loss from loss functions.\n+\n+        # Arguments\n+            skip_target_indices: A list of indices of model outputs where loss\n+                function is None.\n+            masks: List of mask values corresponding to each model output.\n+\n+        # Returns\n+            A list of loss weights of python floats.\n+        \"\"\"\n+        skip_target_indices = skip_target_indices or []\n+        total_loss = None\n+        with K.name_scope('loss'):\n+            zipped_inputs = zip(self.targets, self.outputs, self.loss_functions,\n+                                self.sample_weights, masks, self.loss_weights_list)\n+            for i, (y_true, y_pred, loss_fn, sample_weight, mask,\n+                    loss_weight) in enumerate(zipped_inputs):\n+                if i in skip_target_indices:\n+                    continue\n+                loss_name = self.output_names[i] + '_loss'\n+                with K.name_scope(loss_name):\n+                    if mask is not None:\n+                        mask = math_ops.cast(mask, y_pred.dtype)\n+                        # Update weights with mask.\n+                        if sample_weight is None:\n+                            sample_weight = mask\n+                        else:\n+                            # Update dimensions of weights to match with mask.\n+                            mask, _, sample_weight = (\n+                                losses_utils.squeeze_or_expand_dimensions(\n+                                    mask, None, sample_weight))\n+                            sample_weight *= mask\n+\n+                    output_loss = loss_fn(\n+                        y_true, y_pred, sample_weight=sample_weight)\n+\n+                if len(self.outputs) > 1:\n+                    self.metrics_tensors.append(output_loss)\n+                    self.metrics_names.append(self.output_names[i] + '_loss')\n+\n+                if total_loss is None:\n+                    total_loss = loss_weight * output_loss\n+                else:\n+                    total_loss += loss_weight * output_loss\n+\n+            if total_loss is None:\n+                if not self.losses:\n+                    raise ValueError('The model cannot be compiled '\n+                                     'because it has no loss to optimize.')\n+                else:\n+                    total_loss = 0.\n+\n+            # Add regularization penalties and other layer-specific losses.\n+            for loss_tensor in self.losses:\n+                total_loss += loss_tensor\n+\n+        return total_loss\n+\n     def _get_callback_model(self):\n         \"\"\"Returns the Callback Model for this Model.\"\"\"\n         if hasattr(self, 'callback_model') and self.callback_model:\n@@ -862,14 +760,14 @@ class Model(Network):\n                 is passed, or if the specified batch size does not match the\n                 exepected size defined in the Input Layer.\n         \"\"\"\n-        if batch_size is not None and is_generator_or_sequence(x):\n+        if batch_size is not None and training_utils.is_generator_or_sequence(x):\n             raise ValueError('The `batch_size` argument must not be specified when'\n                              ' using a generator or Sequence as an input.')\n \n         layers = super(Model, self).layers  # Avoids the override in Sequential.\n         if layers:\n             first_layer = layers[0]\n-            static_batch_size = get_static_batch_size(first_layer)\n+            static_batch_size = training_utils.get_static_batch_size(first_layer)\n             if static_batch_size is not None:\n \n                 # Check `batch_size` argument is consistent with InputLayer.\n@@ -888,6 +786,24 @@ class Model(Network):\n             batch_size = 32\n         return batch_size\n \n+    def _set_sample_weight_attributes(self, sample_weight_mode,\n+                                      skip_target_weighing_indices):\n+        \"\"\"Sets sample weight related attributes on the model.\"\"\"\n+        sample_weights, sample_weight_modes = training_utils.prepare_sample_weights(\n+            self.output_names, sample_weight_mode, skip_target_weighing_indices)\n+        self.sample_weights = sample_weights\n+        self.sample_weight_modes = sample_weight_modes\n+        self._feed_sample_weight_modes = [\n+            sample_weight_modes[i]\n+            for i in range(len(self.outputs))\n+            if i not in skip_target_weighing_indices\n+        ]\n+        self._feed_sample_weights = [\n+            sample_weights[i]\n+            for i in range(len(sample_weights))\n+            if i not in skip_target_weighing_indices\n+        ]\n+\n     def fit(self,\n             x=None,\n             y=None,\n@@ -1063,8 +979,8 @@ class Model(Network):\n \n         # Case 1: generator-like. Input is Python generator,\n         # or Sequence object, or iterator.\n-        if is_generator_or_sequence(x):\n-            check_generator_arguments(\n+        if training_utils.is_generator_or_sequence(x):\n+            training_utils.check_generator_arguments(\n                 y, sample_weight, validation_split=validation_split)\n             return self.fit_generator(\n                 x,\n@@ -1266,8 +1182,8 @@ class Model(Network):\n         batch_size = self._validate_or_infer_batch_size(batch_size, steps, x)\n \n         # Case 1: generator-like. Input is Python generator, or Sequence object.\n-        if is_generator_or_sequence(x):\n-            check_generator_arguments(y, sample_weight)\n+        if training_utils.is_generator_or_sequence(x):\n+            training_utils.check_generator_arguments(y, sample_weight)\n             return self.evaluate_generator(\n                 x,\n                 steps=steps,\n@@ -1362,7 +1278,7 @@ class Model(Network):\n         batch_size = self._validate_or_infer_batch_size(batch_size, steps, x)\n \n         # Case 1: generator-like. Input is Python generator, or Sequence object.\n-        if is_generator_or_sequence(x):\n+        if training_utils.is_generator_or_sequence(x):\n             return self.predict_generator(\n                 x,\n                 steps=steps,\n\n@@ -8,12 +8,14 @@ import inspect\n import collections\n import copy\n import numpy as np\n+import six\n import warnings\n \n from .. import backend as K\n from .. import losses\n from ..utils import Sequence\n-from ..utils.generic_utils import to_list\n+from ..utils import generic_utils\n+from ..utils import losses_utils\n \n \n def standardize_single_array(x):\n@@ -252,7 +254,8 @@ def check_array_length_consistency(inputs, targets, weights=None):\n def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n     \"\"\"Does validation on the compatibility of targets and loss functions.\n \n-    This helps prevent users from using loss functions incorrectly.\n+    This helps prevent users from using loss functions incorrectly. This check\n+    is purely for UX purposes.\n \n     # Arguments\n         targets: list of Numpy arrays of targets.\n@@ -263,13 +266,16 @@ def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n         ValueError: if a loss function or target array\n             is incompatible with an output.\n     \"\"\"\n-    key_losses = {losses.mean_squared_error,\n-                  losses.binary_crossentropy,\n-                  losses.categorical_crossentropy}\n+    key_loss_fns = {\n+        losses.mean_squared_error, losses.binary_crossentropy,\n+        losses.categorical_crossentropy\n+    }\n+    key_loss_classes = (losses.MeanSquaredError, losses.BinaryCrossentropy,\n+                        losses.CategoricalCrossentropy)\n     for y, loss, shape in zip(targets, loss_fns, output_shapes):\n         if y is None or loss is None:\n             continue\n-        if loss is losses.categorical_crossentropy:\n+        if losses.is_categorical_crossentropy(loss):\n             if y.shape[-1] == 1:\n                 raise ValueError(\n                     'You are passing a target array of shape ' + str(y.shape) +\n@@ -287,15 +293,20 @@ def check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n                     'Alternatively, you can use the loss function '\n                     '`sparse_categorical_crossentropy` instead, '\n                     'which does expect integer targets.')\n-        if loss in key_losses:\n+        is_loss_wrapper = isinstance(loss, losses.LossFunctionWrapper)\n+        if (isinstance(loss, key_loss_classes) or (is_loss_wrapper and\n+                                                   (loss.fn in key_loss_fns))):\n             for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n                 if out_dim is not None and target_dim != out_dim:\n+                    loss_name = loss.name\n+                    if loss_name is None:\n+                        loss_type = loss.fn if is_loss_wrapper else type(loss)\n+                        loss_name = loss_type.__name__\n                     raise ValueError(\n                         'A target array with shape ' + str(y.shape) +\n                         ' was passed for an output of shape ' + str(shape) +\n-                        ' while using as loss `' + loss.__name__ + '`. '\n-                        'This loss expects '\n-                        'targets to have the same shape '\n+                        ' while using as loss `' + loss_name + '`. '\n+                        'This loss expects targets to have the same shape '\n                         'as the output.')\n \n \n@@ -347,7 +358,7 @@ def collect_metrics(metrics, output_names):\n                           .format(unknown_output_names, output_names))\n         for name in output_names:\n             output_metrics = metrics.get(name, [])\n-            output_metrics = to_list(output_metrics)\n+            output_metrics = generic_utils.to_list(output_metrics)\n             nested_metrics.append(output_metrics)\n         return nested_metrics\n     else:\n@@ -713,3 +724,189 @@ def get_input_shape_and_dtype(layer):\n     if hasattr(layer, '_batch_input_shape'):\n         return layer._batch_input_shape, layer.dtype\n     return None, None\n+\n+\n+def get_loss_function(loss):\n+    \"\"\"Returns the loss corresponding to the loss input in `compile` API.\"\"\"\n+    if loss is None or isinstance(loss, losses.Loss):\n+        return loss\n+\n+    # Deserialize loss configuration, if needed.\n+    if isinstance(loss, collections.Mapping):\n+        loss = losses.get(loss)\n+\n+    # Custom callable class.\n+    if callable(loss) and not hasattr(loss, '__name__'):\n+        return loss\n+\n+    # Wrap loss function with signature `(y_true, y_pred, **kwargs)`\n+    # in `LossFunctionWrapper` class.\n+    loss_fn = losses.get(loss)\n+\n+    # For losses which are given as strings/functions in the compile API,\n+    # we always set the loss reduction type to be `SUM_OVER_BATCH_SIZE`..\n+    return losses.LossFunctionWrapper(\n+        loss_fn,\n+        name=loss_fn.__name__,\n+        reduction=losses_utils.Reduction.SUM_OVER_BATCH_SIZE)\n+\n+\n+def get_output_sample_weight_and_mode(skip_target_weighing_indices,\n+                                      sample_weight_mode, output_name,\n+                                      output_index):\n+    \"\"\"Returns the sample weight and weight mode for a single output.\"\"\"\n+    if output_index in skip_target_weighing_indices:\n+        return None, None\n+\n+    if sample_weight_mode == 'temporal':\n+        shape = [None, None]\n+        mode = 'temporal'\n+    else:\n+        shape = [None]\n+        mode = None\n+    weight = K.placeholder(\n+        shape=shape,\n+        name=output_name + '_sample_weights')\n+    return weight, mode\n+\n+\n+def prepare_sample_weights(output_names, sample_weight_mode,\n+                           skip_target_weighing_indices):\n+    \"\"\"Prepares sample weights for the model.\n+\n+    # Arguments\n+        output_names: List of model output names.\n+        sample_weight_mode: sample weight mode user input passed from compile API.\n+        skip_target_weighing_indices: Indices of output for which sample weights\n+            should be skipped.\n+\n+    # Returns\n+        A pair of list of sample weights and sample weight modes\n+            (one for each output).\n+\n+    # Raises\n+        ValueError: In case of invalid `sample_weight_mode` input.\n+    \"\"\"\n+    sample_weights = []\n+    sample_weight_modes = []\n+    if isinstance(sample_weight_mode, dict):\n+        unknown_output = set(sample_weight_mode.keys()) - set(output_names)\n+        if unknown_output:\n+            raise ValueError(\n+                'Unknown entry in '\n+                'sample_weight_mode dictionary: \"' + str(unknown_output) +\n+                '\". Only expected the following keys: ' + str(output_names))\n+        for i, name in enumerate(output_names):\n+            if (i not in skip_target_weighing_indices and\n+                    name not in sample_weight_mode):\n+                raise ValueError(\n+                    'Output missing from sample_weight_modes dictionary')\n+            weight, mode = get_output_sample_weight_and_mode(\n+                skip_target_weighing_indices,\n+                sample_weight_mode.get(name),\n+                name,\n+                i)\n+            sample_weights.append(weight)\n+            sample_weight_modes.append(mode)\n+    elif isinstance(sample_weight_mode, list):\n+        if len(sample_weight_mode) != len(output_names):\n+            raise ValueError('When passing a list as sample_weight_mode, '\n+                             'it should have one entry per model output. '\n+                             'The model has ' + str(len(output_names)) +\n+                             ' outputs, but you passed ' +\n+                             str(len(sample_weight_mode)) + 'sample_weight_modes')\n+        for i, name in enumerate(output_names):\n+            weight, mode = get_output_sample_weight_and_mode(\n+                skip_target_weighing_indices, sample_weight_mode[i], name, i)\n+            sample_weights.append(weight)\n+            sample_weight_modes.append(mode)\n+    else:\n+        for i, name in enumerate(output_names):\n+            weight, mode = get_output_sample_weight_and_mode(\n+                skip_target_weighing_indices, sample_weight_mode, name, i)\n+            sample_weights.append(weight)\n+            sample_weight_modes.append(mode)\n+    return sample_weights, sample_weight_modes\n+\n+\n+def prepare_loss_functions(loss, output_names):\n+    \"\"\"Converts loss to a list of loss functions.\n+\n+    # Arguments\n+        loss: String (name of objective function), objective function or\n+            `Loss` instance. If the model has multiple outputs, you can use\n+            a different loss on each output by passing a dictionary or a\n+            list of losses. The loss value that will be minimized by the model\n+            will then be the sum of all individual losses.\n+        output_names: List of model output names.\n+\n+    # Returns\n+        A list of loss objective functions.\n+\n+    # Raises:\n+        ValueError: If loss is a dict with keys not in model output names,\n+            or if loss is a list with len not equal to model outputs.\n+    \"\"\"\n+    if isinstance(loss, collections.Mapping):\n+        generic_utils.check_for_unexpected_keys('loss', loss, output_names)\n+        loss_functions = []\n+        for name in output_names:\n+            if name not in loss:\n+                warnings.warn(\n+                    'Output {0} missing from loss dictionary. We assume '\n+                    'this was done on purpose. The fit and evaluate APIs will not '\n+                    'be expecting any data to be passed to {0}.'.format(name))\n+            loss_functions.append(get_loss_function(loss.get(name, None)))\n+    elif isinstance(loss, six.string_types):\n+        loss_functions = [get_loss_function(loss) for _ in output_names]\n+    elif isinstance(loss, collections.Sequence):\n+        if len(loss) != len(output_names):\n+            raise ValueError('When passing a list as loss, it should have one entry '\n+                             'per model outputs. The model has {} outputs, but you '\n+                             'passed loss={}'.format(len(output_names), loss))\n+        loss_functions = [get_loss_function(l) for l in loss]\n+    else:\n+        loss_functions = [get_loss_function(loss) for _ in range(len(output_names))]\n+\n+    return loss_functions\n+\n+\n+def prepare_loss_weights(output_names, loss_weights=None):\n+    \"\"\"Converts loss weights to a list of loss weights.\n+\n+    # Arguments\n+        output_names: List of model output names.\n+        loss_weights: Optional list or dictionary specifying scalar coefficients\n+            (Python floats) to weight the loss contributions of different model\n+            outputs. The loss value that will be minimized by the model will then be\n+            the *weighted sum* of all individual losses, weighted by the\n+            `loss_weights` coefficients. If a list, it is expected to have a 1:1\n+            mapping to the model's outputs. If a dict, it is expected to map\n+            output names (strings) to scalar coefficients.\n+\n+    # Returns\n+        A list of loss weights of python floats.\n+\n+    # Raises\n+        ValueError: If loss weight is a dict with key not in model output names,\n+            or if loss is a list with len not equal to model outputs.\n+    \"\"\"\n+    if loss_weights is None:\n+        weights_list = [1.] * len(output_names)\n+    elif isinstance(loss_weights, collections.Mapping):\n+        generic_utils.check_for_unexpected_keys('loss_weights', loss_weights,\n+                                                output_names)\n+        weights_list = [loss_weights.get(name, 1.) for name in output_names]\n+    elif isinstance(loss_weights, list):\n+        if len(loss_weights) != len(output_names):\n+            raise ValueError('When passing a list as loss_weights, '\n+                             'it should have one entry per model output. '\n+                             'The model has ' + str(len(output_names)) +\n+                             ' outputs, but you passed loss_weights=' +\n+                             str(loss_weights))\n+        weights_list = loss_weights\n+    else:\n+        raise TypeError('Could not interpret loss_weights argument: ' +\n+                        str(loss_weights) + ' - expected a list of dicts.')\n+\n+    return weights_list\n\n@@ -557,12 +557,12 @@ cosine = cosine_proximity\n \n \n def is_categorical_crossentropy(loss):\n-    return (isinstance(loss, CategoricalCrossentropy or\n+    return (isinstance(loss, CategoricalCrossentropy) or\n             (isinstance(loss, LossFunctionWrapper) and\n                 loss.fn == categorical_crossentropy) or\n             (hasattr(loss, '__name__') and\n                 loss.__name__ == 'categorical_crossentropy') or\n-            loss == 'categorical_crossentropy'))\n+            loss == 'categorical_crossentropy')\n \n \n def serialize(loss):\n\n@@ -613,3 +613,11 @@ def transpose_shape(shape, target_format, spatial_axes):\n         raise ValueError('The `data_format` argument must be one of '\n                          '\"channels_first\", \"channels_last\". Received: ' +\n                          str(target_format))\n+\n+\n+def check_for_unexpected_keys(name, input_dict, expected_values):\n+    unknown = set(input_dict.keys()).difference(expected_values)\n+    if unknown:\n+        raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\n+                         'following keys: {}'.format(name, list(unknown),\n+                                                     expected_values))\n\n@@ -128,7 +128,38 @@ def compute_weighted_loss(losses,\n         losses, _, sample_weight = squeeze_or_expand_dimensions(\n             losses, None, sample_weight)\n \n-        weighted_losses = losses * sample_weight\n+        # Broadcast weights if possible.\n+        weights_shape = K.int_shape(sample_weight)\n+        losses_shape = K.int_shape(losses)\n+        if losses_shape != weights_shape:\n+            weights_rank = K.ndim(sample_weight)\n+            losses_rank = K.ndim(losses)\n+\n+            # Raise error if ndim of weights is > losses.\n+            if weights_rank > losses_rank:\n+                raise ValueError(\n+                    'Incompatible shapes: `losses` {} vs `sample_weight` {}'.format(\n+                        losses_shape, weights_shape))\n+\n+            # Expand dim of weights to match ndim of losses, if required.\n+            for i in range(weights_rank, losses_rank):\n+                sample_weight = K.expand_dims(sample_weight, axis=i)\n+\n+            for i in range(weights_rank):\n+                if (weights_shape[i] is not None and losses_shape[i] is not None and\n+                        weights_shape[i] != losses_shape[i]):\n+                    # Cannot be broadcasted.\n+                    if weights_shape[i] != 1:\n+                        raise ValueError(\n+                            'Incompatible shapes: `losses` {} vs '\n+                            '`sample_weight` {}'.format(\n+                                losses_shape, weights_shape))\n+                    sample_weight = K.repeat_elements(\n+                        sample_weight, losses_shape[i], axis=i)\n+\n+        # Apply weights to losses.\n+        weighted_losses = sample_weight * losses\n+\n         # Apply reduction function to the individual weighted losses.\n         loss = reduce_weighted_loss(weighted_losses, reduction)\n         # Convert the result back to the input type.\n\n@@ -9,6 +9,7 @@ import types\n \n import numpy as np\n \n+from .. import losses\n from ..utils.np_utils import to_categorical\n from ..utils.generic_utils import has_arg\n from ..utils.generic_utils import to_list\n@@ -140,10 +141,8 @@ class BaseWrapper(object):\n         else:\n             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n \n-        loss_name = self.model.loss\n-        if hasattr(loss_name, '__name__'):\n-            loss_name = loss_name.__name__\n-        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n+        if (losses.is_categorical_crossentropy(self.model.loss) and\n+                len(y.shape) != 2):\n             y = to_categorical(y)\n \n         fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n\n@@ -10,7 +10,7 @@ from flaky import flaky\n \n import keras\n from keras import losses\n-from keras.layers import Activation, Dense, Dropout, Conv2D, Concatenate\n+from keras.layers import Layer, Activation, Dense, Dropout, Conv2D, Concatenate\n from keras.engine import Input\n from keras.engine.training import Model\n from keras.engine import training_utils\n@@ -665,6 +665,34 @@ def test_fit_generator_shape():\n     assert np.shape(out) == shape_0\n \n \n+def test_training_with_loss_instance():\n+    a = Input(shape=(3,), name='input_a')\n+    b = Input(shape=(3,), name='input_b')\n+\n+    dense = Dense(4, name='dense')\n+    c = dense(a)\n+    d = dense(b)\n+    e = Dropout(0.5, name='dropout')(c)\n+\n+    model = Model([a, b], [d, e])\n+    loss_weights = [1., 0.5]\n+    model.compile(\n+        'sgd',\n+        loss=losses.MeanSquaredError(),\n+        metrics=['mae'],\n+        loss_weights=loss_weights)\n+\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n+\n+    output_d_np = np.random.random((10, 4))\n+    output_e_np = np.random.random((10, 4))\n+\n+    model.fit([input_a_np, input_b_np], [output_d_np, output_e_np],\n+              epochs=1,\n+              batch_size=5)\n+\n+\n @pytest.mark.skipif(sys.version_info < (3,),\n                     reason='Cannot catch warnings in python 2')\n def test_warnings():\n@@ -788,7 +816,7 @@ def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n     with pytest.raises(ValueError) as exc:\n         training_utils.check_loss_and_target_compatibility(\n-            [a], [losses.categorical_crossentropy], [a.shape])\n+            [a], [losses.CategoricalCrossentropy()], [a.shape])\n \n     assert 'You are passing a target array' in str(exc)\n \n@@ -797,7 +825,7 @@ def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n     with pytest.raises(ValueError) as exc:\n         training_utils.check_loss_and_target_compatibility(\n-            [a], [losses.categorical_crossentropy], [(2, 3, 6)])\n+            [a], [losses.CategoricalCrossentropy()], [(2, 3, 6)])\n \n     assert 'targets to have the same shape' in str(exc)\n \n@@ -1723,5 +1751,27 @@ def test_validation_freq():\n     assert val_counter.val_runs == 3\n \n \n+def test_loss_correctness():\n+    class Bias(Layer):\n+\n+        def build(self, input_shape):\n+            self.bias = self.add_weight('bias', (1,), initializer='zeros')\n+\n+        def call(self, inputs):\n+            return inputs + self.bias\n+\n+    inp = Input(shape=(1,))\n+    out = Bias()(inp)\n+    model = Model(inp, out)\n+    model.compile(\n+        keras.optimizers.SGD(lr=0.1),\n+        loss=keras.losses.MeanAbsoluteError())\n+\n+    x = np.array([[0.], [1.], [2.]])\n+    y = np.array([[0.5], [2.], [3.5]])\n+    history = model.fit(x, y, batch_size=3, epochs=5)\n+    np.allclose(history.history['loss'], [1., 0.9, 0.8, 0.7, 0.6])\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -28,7 +28,7 @@ class MSE_MAE_loss:\n     def __init__(self, mse_fraction):\n         self.mse_fraction = mse_fraction\n \n-    def __call__(self, y_true, y_pred):\n+    def __call__(self, y_true, y_pred, sample_weight=None):\n         return (self.mse_fraction * losses.mse(y_true, y_pred) +\n                 (1 - self.mse_fraction) * losses.mae(y_true, y_pred))\n \n@@ -124,6 +124,24 @@ class TestLossFunctions:\n             loaded_model = keras.models.load_model(model_filename)\n             loaded_model.predict(np.random.rand(128, 2))\n \n+    def test_loss_wrapper(self):\n+        loss_fn = losses.get('mse')\n+        mse_obj = losses.LossFunctionWrapper(loss_fn, name=loss_fn.__name__)\n+\n+        assert mse_obj.name == 'mean_squared_error'\n+        assert (mse_obj.reduction == losses_utils.Reduction.SUM_OVER_BATCH_SIZE)\n+\n+        y_true = K.constant([[1., 9.], [2., 5.]])\n+        y_pred = K.constant([[4., 8.], [12., 3.]])\n+        sample_weight = K.constant([1.2, 0.5])\n+        loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # mse = [((4 - 1)^2 + (8 - 9)^2) / 2, ((12 - 2)^2 + (3 - 5)^2) / 2]\n+        # mse = [5, 52]\n+        # weighted_mse = [5 * 1.2, 52 * 0.5] = [6, 26]\n+        # reduced_weighted_mse = (6 + 26) / 2 =\n+        np.allclose(K.eval(loss), 16, atol=1e-2)\n+\n \n class TestMeanSquaredError:\n \n\n@@ -38,7 +38,7 @@ def test_sequential_model_saving():\n     model.add(Dense(2, input_shape=(3,)))\n     model.add(RepeatVector(3))\n     model.add(TimeDistributed(Dense(3)))\n-    model.compile(loss=losses.MSE,\n+    model.compile(loss=losses.MeanSquaredError(),\n                   optimizer=optimizers.RMSprop(lr=0.0001),\n                   metrics=[metrics.categorical_accuracy],\n                   sample_weight_mode='temporal')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
