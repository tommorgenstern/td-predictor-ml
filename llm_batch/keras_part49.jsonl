{"custom_id": "keras#1a7c6627ac89cbbc8612ea4df59755c3109f6684", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 3136 | Contributors (this commit): 26 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -659,7 +659,7 @@ class Graph(Model, containers.Graph):\n             val_f = self._test\n         if validation_data:\n             # can't use sample weights with validation data at this point\n-            y_val = [standardize_y(data[name]) for name in self.output_order]\n+            y_val = [standardize_y(validation_data[name]) for name in self.output_order]\n             sample_weight = [standardize_weights(y_val[i]) for i in range(len(y_val))]\n             val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order] + sample_weight\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b39b5f36b997868f9105d8bc76188f8837f50db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3497 | Files Changed: 29 | Hunks: 27 | Methods Changed: 161 | Complexity Δ (Sum/Max): -265/0 | Churn Δ: 3497 | Churn Cumulative: 8042 | Contributors (this commit): 21 | Commits (past 90d): 114 | Contributors (cumulative): 77 | DMM Complexity: 0.05532170775706555\n\nDIFF:\n@@ -1,221 +0,0 @@\n-\n-'''\n-    We loop over words in a dataset, and for each word, we look at a context window around the word.\n-    We generate pairs of (pivot_word, other_word_from_same_context) with label 1,\n-    and pairs of (pivot_word, random_word) with label 0 (skip-gram method).\n-\n-    We use the layer WordContextProduct to learn embeddings for the word couples,\n-    and compute a proximity score between the embeddings (= p(context|word)),\n-    trained with our positive and negative labels.\n-\n-    We then use the weights computed by WordContextProduct to encode words\n-    and demonstrate that the geometry of the embedding space\n-    captures certain useful semantic properties.\n-\n-    Read more about skip-gram in this particularly gnomic paper by Mikolov et al.:\n-        http://arxiv.org/pdf/1301.3781v3.pdf\n-\n-    Note: you should run this on GPU, otherwise training will be quite slow.\n-    On a EC2 GPU instance, expect 3 hours per 10e6 comments (~10e8 words) per epoch with dim_proj=256.\n-    Should be much faster on a modern GPU.\n-\n-    GPU command:\n-        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python skipgram_word_embeddings.py\n-\n-    Dataset: 5,845,908 Hacker News comments.\n-    Obtain the dataset at:\n-        https://mega.co.nz/#F!YohlwD7R!wec0yNO86SeaNGIYQBOR0A\n-        (HNCommentsAll.1perline.json.bz2)\n-'''\n-from __future__ import absolute_import\n-from __future__ import print_function\n-\n-import numpy as np\n-import theano\n-from six.moves import cPickle\n-import os, re, json\n-\n-from keras.preprocessing import sequence, text\n-from keras.optimizers import SGD, RMSprop, Adagrad\n-from keras.utils import np_utils, generic_utils\n-from keras.models import Sequential\n-from keras.layers.embeddings import WordContextProduct, Embedding\n-from six.moves import range\n-from six.moves import zip\n-\n-max_features = 50000  # vocabulary size: top 50,000 most common words in data\n-skip_top = 100  # ignore top 100 most common words\n-nb_epoch = 1\n-dim_proj = 256  # embedding space dimension\n-\n-save = True\n-load_model = False\n-load_tokenizer = False\n-train_model = True\n-save_dir = os.path.expanduser(\"~/.keras/models\")\n-if not os.path.exists(save_dir):\n-    os.makedirs(save_dir)\n-model_load_fname = \"HN_skipgram_model.pkl\"\n-model_save_fname = \"HN_skipgram_model.pkl\"\n-tokenizer_fname = \"HN_tokenizer.pkl\"\n-\n-data_path = os.path.expanduser(\"~/\")+\"HNCommentsAll.1perline.json\"\n-\n-# text preprocessing utils\n-html_tags = re.compile(r'<.*?>')\n-to_replace = [('&#x27;', \"'\")]\n-hex_tags = re.compile(r'&.*?;')\n-\n-\n-def clean_comment(comment):\n-    c = str(comment.encode(\"utf-8\"))\n-    c = html_tags.sub(' ', c)\n-    for tag, char in to_replace:\n-        c = c.replace(tag, char)\n-    c = hex_tags.sub(' ', c)\n-    return c\n-\n-\n-def text_generator(path=data_path):\n-    f = open(path)\n-    for i, l in enumerate(f):\n-        comment_data = json.loads(l)\n-        comment_text = comment_data[\"comment_text\"]\n-        comment_text = clean_comment(comment_text)\n-        if i % 10000 == 0:\n-            print(i)\n-        yield comment_text\n-    f.close()\n-\n-# model management\n-if load_tokenizer:\n-    print('Load tokenizer...')\n-    tokenizer = cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n-else:\n-    print(\"Fit tokenizer...\")\n-    tokenizer = text.Tokenizer(nb_words=max_features)\n-    tokenizer.fit_on_texts(text_generator())\n-    if save:\n-        print(\"Save tokenizer...\")\n-        if not os.path.exists(save_dir):\n-            os.makedirs(save_dir)\n-        cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"wb\"))\n-\n-# training process\n-if train_model:\n-    if load_model:\n-        print('Load model...')\n-        model = cPickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n-    else:\n-        print('Build model...')\n-        model = Sequential()\n-        model.add(WordContextProduct(max_features, proj_dim=dim_proj, init=\"uniform\"))\n-        model.compile(loss='mse', optimizer='rmsprop')\n-\n-    sampling_table = sequence.make_sampling_table(max_features)\n-\n-    for e in range(nb_epoch):\n-        print('-'*40)\n-        print('Epoch', e)\n-        print('-'*40)\n-\n-        progbar = generic_utils.Progbar(tokenizer.document_count)\n-        samples_seen = 0\n-        losses = []\n-\n-        for i, seq in enumerate(tokenizer.texts_to_sequences_generator(text_generator())):\n-            # get skipgram couples for one text in the dataset\n-            couples, labels = sequence.skipgrams(seq, max_features, window_size=4, negative_samples=1., sampling_table=sampling_table)\n-            if couples:\n-                # one gradient update per sentence (one sentence = a few 1000s of word couples)\n-                X = np.array(couples, dtype=\"int32\")\n-                loss = model.train_on_batch(X, labels)\n-                losses.append(loss)\n-                if len(losses) % 100 == 0:\n-                    progbar.update(i, values=[(\"loss\", np.mean(losses))])\n-                    losses = []\n-                samples_seen += len(labels)\n-        print('Samples seen:', samples_seen)\n-    print(\"Training completed!\")\n-\n-    if save:\n-        print(\"Saving model...\")\n-        if not os.path.exists(save_dir):\n-            os.makedirs(save_dir)\n-        cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"wb\"))\n-\n-\n-print(\"It's test time!\")\n-\n-# recover the embedding weights trained with skipgram:\n-weights = model.layers[0].get_weights()[0]\n-\n-# we no longer need this\n-del model\n-\n-weights[:skip_top] = np.zeros((skip_top, dim_proj))\n-norm_weights = np_utils.normalize(weights)\n-\n-word_index = tokenizer.word_index\n-reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])\n-\n-\n-def embed_word(w):\n-    i = word_index.get(w)\n-    if (not i) or (i < skip_top) or (i >= max_features):\n-        return None\n-    return norm_weights[i]\n-\n-\n-def closest_to_point(point, nb_closest=10):\n-    proximities = np.dot(norm_weights, point)\n-    tups = list(zip(list(range(len(proximities))), proximities))\n-    tups.sort(key=lambda x: x[1], reverse=True)\n-    return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]\n-\n-\n-def closest_to_word(w, nb_closest=10):\n-    i = word_index.get(w)\n-    if (not i) or (i < skip_top) or (i >= max_features):\n-        return []\n-    return closest_to_point(norm_weights[i].T, nb_closest)\n-\n-\n-''' the resuls in comments below were for:\n-    5.8M HN comments\n-    dim_proj = 256\n-    nb_epoch = 2\n-    optimizer = rmsprop\n-    loss = mse\n-    max_features = 50000\n-    skip_top = 100\n-    negative_samples = 1.\n-    window_size = 4\n-    and frequency subsampling of factor 10e-5.\n-'''\n-\n-words = [\n-    \"article\",  # post, story, hn, read, comments\n-    \"3\",  # 6, 4, 5, 2\n-    \"two\",  # three, few, several, each\n-    \"great\",  # love, nice, working, looking\n-    \"data\",  # information, memory, database\n-    \"money\",  # company, pay, customers, spend\n-    \"years\",  # ago, year, months, hours, week, days\n-    \"android\",  # ios, release, os, mobile, beta\n-    \"javascript\",  # js, css, compiler, library, jquery, ruby\n-    \"look\",  # looks, looking\n-    \"business\",  # industry, professional, customers\n-    \"company\",  # companies, startup, founders, startups\n-    \"after\",  # before, once, until\n-    \"own\",  # personal, our, having\n-    \"us\",  # united, country, american, tech, diversity, usa, china, sv\n-    \"using\",  # javascript, js, tools (lol)\n-    \"here\",  # hn, post, comments\n-]\n-\n-for w in words:\n-    res = closest_to_word(w)\n-    print('====', w)\n-    for r in res:\n-        print(r)\n\n@@ -1,44 +0,0 @@\n-from __future__ import absolute_import\n-import numpy as np\n-import theano\n-import theano.tensor as T\n-\n-\n-def floatX(X):\n-    return np.asarray(X, dtype=theano.config.floatX)\n-\n-\n-def sharedX(X, dtype=theano.config.floatX, name=None):\n-    return theano.shared(np.asarray(X, dtype=dtype), name=name)\n-\n-\n-def shared_zeros(shape, dtype=theano.config.floatX, name=None):\n-    return sharedX(np.zeros(shape), dtype=dtype, name=name)\n-\n-\n-def shared_scalar(val=0., dtype=theano.config.floatX, name=None):\n-    return theano.shared(np.cast[dtype](val))\n-\n-\n-def shared_ones(shape, dtype=theano.config.floatX, name=None):\n-    return sharedX(np.ones(shape), dtype=dtype, name=name)\n-\n-\n-def alloc_zeros_matrix(*dims):\n-    return T.alloc(np.cast[theano.config.floatX](0.), *dims)\n-\n-\n-def ndim_tensor(ndim):\n-    if ndim == 1:\n-        return T.vector()\n-    elif ndim == 2:\n-        return T.matrix()\n-    elif ndim == 3:\n-        return T.tensor3()\n-    elif ndim == 4:\n-        return T.tensor4()\n-    return T.matrix()\n-\n-\n-def on_gpu():\n-    return theano.config.device[:3] == 'gpu'\n\n\n@@ -1,331 +0,0 @@\n-import unittest\n-from numpy.testing import assert_allclose\n-import numpy as np\n-\n-from keras.backend import theano_backend as KTH\n-from keras.backend import tensorflow_backend as KTF\n-\n-\n-def check_single_tensor_operation(function_name, input_shape, **kwargs):\n-    val = np.random.random(input_shape) - 0.5\n-    xth = KTH.variable(val)\n-    xtf = KTF.variable(val)\n-\n-    zth = KTH.eval(getattr(KTH, function_name)(xth, **kwargs))\n-    ztf = KTF.eval(getattr(KTF, function_name)(xtf, **kwargs))\n-\n-    assert zth.shape == ztf.shape\n-    assert_allclose(zth, ztf, atol=1e-06)\n-\n-\n-def check_two_tensor_operation(function_name, x_input_shape,\n-                               y_input_shape, **kwargs):\n-    xval = np.random.random(x_input_shape) - 0.5\n-    xth = KTH.variable(xval)\n-    xtf = KTF.variable(xval)\n-\n-    yval = np.random.random(y_input_shape) - 0.5\n-    yth = KTH.variable(yval)\n-    ytf = KTF.variable(yval)\n-\n-    zth = KTH.eval(getattr(KTH, function_name)(xth, yth, **kwargs))\n-    ztf = KTF.eval(getattr(KTF, function_name)(xtf, ytf, **kwargs))\n-\n-    assert zth.shape == ztf.shape\n-    assert_allclose(zth, ztf, atol=1e-06)\n-\n-\n-class TestBackend(unittest.TestCase):\n-\n-    def test_linear_operations(self):\n-        check_two_tensor_operation('dot', (4, 2), (2, 4))\n-        check_single_tensor_operation('transpose', (4, 2))\n-\n-    def test_shape_operations(self):\n-        # concatenate\n-        xval = np.random.random((4, 3))\n-        xth = KTH.variable(xval)\n-        xtf = KTF.variable(xval)\n-        yval = np.random.random((4, 2))\n-        yth = KTH.variable(yval)\n-        ytf = KTF.variable(yval)\n-        zth = KTH.eval(KTH.concatenate([xth, yth], axis=-1))\n-        ztf = KTF.eval(KTF.concatenate([xtf, ytf], axis=-1))\n-        assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-06)\n-\n-        check_single_tensor_operation('reshape', (4, 2), shape=(8, 1))\n-        check_single_tensor_operation('permute_dimensions', (4, 2, 3),\n-                                      pattern=(2, 0, 1))\n-        check_single_tensor_operation('repeat', (4, 1), n=3)\n-        check_single_tensor_operation('flatten', (4, 1))\n-        check_single_tensor_operation('expand_dims', (4, 3), dim=-1)\n-        check_single_tensor_operation('expand_dims', (4, 3, 2), dim=1)\n-        check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n-\n-    def test_value_manipulation(self):\n-        val = np.random.random((4, 2))\n-        xth = KTH.variable(val)\n-        xtf = KTF.variable(val)\n-\n-        # get_value\n-        valth = KTH.get_value(xth)\n-        valtf = KTF.get_value(xtf)\n-        assert valtf.shape == valth.shape\n-        assert_allclose(valth, valtf, atol=1e-06)\n-\n-        # set_value\n-        val = np.random.random((4, 2))\n-        KTH.set_value(xth, val)\n-        KTF.set_value(xtf, val)\n-\n-        valth = KTH.get_value(xth)\n-        valtf = KTF.get_value(xtf)\n-        assert valtf.shape == valth.shape\n-        assert_allclose(valth, valtf, atol=1e-06)\n-\n-        # count_params\n-        assert KTH.count_params(xth) == KTF.count_params(xtf)\n-\n-    def test_elementwise_operations(self):\n-        check_single_tensor_operation('max', (4, 2))\n-        check_single_tensor_operation('max', (4, 2), axis=1, keepdims=True)\n-\n-        check_single_tensor_operation('min', (4, 2))\n-        check_single_tensor_operation('min', (4, 2), axis=1, keepdims=True)\n-\n-        check_single_tensor_operation('mean', (4, 2))\n-        check_single_tensor_operation('mean', (4, 2), axis=1, keepdims=True)\n-        check_single_tensor_operation('mean', (4, 2, 3), axis=-1, keepdims=True)\n-\n-        check_single_tensor_operation('std', (4, 2))\n-        check_single_tensor_operation('std', (4, 2), axis=1, keepdims=True)\n-\n-        check_single_tensor_operation('prod', (4, 2))\n-        check_single_tensor_operation('prod', (4, 2), axis=1, keepdims=True)\n-\n-        # does not work yet, wait for bool <-> int casting in TF (coming soon)\n-        # check_single_tensor_operation('any', (4, 2))\n-        # check_single_tensor_operation('any', (4, 2), axis=1, keepdims=True)\n-\n-        check_single_tensor_operation('argmax', (4, 2))\n-        check_single_tensor_operation('argmax', (4, 2), axis=1)\n-\n-        check_single_tensor_operation('argmin', (4, 2))\n-        check_single_tensor_operation('argmin', (4, 2), axis=1)\n-\n-        check_single_tensor_operation('square', (4, 2))\n-        check_single_tensor_operation('abs', (4, 2))\n-        check_single_tensor_operation('sqrt', (4, 2))\n-        check_single_tensor_operation('exp', (4, 2))\n-        check_single_tensor_operation('log', (4, 2))\n-        check_single_tensor_operation('round', (4, 2))\n-        check_single_tensor_operation('pow', (4, 2), a=3)\n-        check_single_tensor_operation('clip', (4, 2), min_value=0.4,\n-                                      max_value=0.6)\n-\n-        # two-tensor ops\n-        check_two_tensor_operation('equal', (4, 2), (4, 2))\n-        check_two_tensor_operation('maximum', (4, 2), (4, 2))\n-        check_two_tensor_operation('minimum', (4, 2), (4, 2))\n-\n-    def test_gradient(self):\n-        val = np.random.random((4, 2))\n-        xth = KTH.variable(val)\n-        xtf = KTF.variable(val)\n-\n-        expth = xth * KTH.exp(xth)\n-        exptf = xtf * KTF.exp(xtf)\n-        lossth = KTH.sum(expth)\n-        losstf = KTF.sum(exptf)\n-\n-        gradth = KTH.gradients(lossth, [expth])\n-        gradtf = KTF.gradients(losstf, [exptf])\n-\n-        zth = KTH.eval(gradth[0])\n-        ztf = KTF.eval(gradtf[0])\n-        assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-06)\n-\n-    def test_function(self):\n-        val = np.random.random((4, 2))\n-        input_val = np.random.random((4, 2))\n-\n-        xth = KTH.variable(val)\n-        xtf = KTF.variable(val)\n-        yth = KTH.placeholder(ndim=2)\n-        ytf = KTF.placeholder(ndim=2)\n-\n-        exp_th = KTH.square(xth) + yth\n-        exp_tf = KTF.square(xtf) + ytf\n-\n-        update_th = xth * 2\n-        update_tf = xtf * 2\n-        fth = KTH.function([yth], [exp_th], updates=[(xth, update_th)])\n-        ftf = KTF.function([ytf], [exp_tf], updates=[(xtf, update_tf)])\n-\n-        function_outputs_th = fth([input_val])[0]\n-        function_outputs_tf = ftf([input_val])[0]\n-        assert function_outputs_th.shape == function_outputs_tf.shape\n-        assert_allclose(function_outputs_th, function_outputs_tf, atol=1e-06)\n-\n-        new_val_th = KTH.get_value(xth)\n-        new_val_tf = KTF.get_value(xtf)\n-        assert new_val_th.shape == new_val_tf.shape\n-        assert_allclose(new_val_th, new_val_tf, atol=1e-06)\n-\n-    def test_rnn(self):\n-        # implement a simple RNN\n-        input_dim = 8\n-        output_dim = 4\n-        timesteps = 5\n-\n-        input_val = np.random.random((32, timesteps, input_dim))\n-        init_state_val = np.random.random((32, output_dim))\n-        W_i_val = np.random.random((input_dim, output_dim))\n-        W_o_val = np.random.random((output_dim, output_dim))\n-\n-        def rnn_step_fn(input_dim, output_dim, K):\n-            W_i = K.variable(W_i_val)\n-            W_o = K.variable(W_o_val)\n-\n-            def step_function(x, states):\n-                assert len(states) == 1\n-                prev_output = states[0]\n-                output = K.dot(x, W_i) + K.dot(prev_output, W_o)\n-                return output, [output]\n-            return step_function\n-\n-        th_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTH)\n-        inputs = KTH.variable(input_val)\n-        initial_states = [KTH.variable(init_state_val)]\n-        last_output, outputs, new_states = KTH.rnn(th_rnn_step_fn, inputs,\n-                                                   initial_states,\n-                                                   go_backwards=False,\n-                                                   masking=False)\n-        th_last_output = KTH.eval(last_output)\n-        th_outputs = KTH.eval(outputs)\n-        assert len(new_states) == 1\n-        th_state = KTH.eval(new_states[0])\n-\n-        tf_rnn_step_fn = rnn_step_fn(input_dim, output_dim, KTF)\n-        inputs = KTF.variable(input_val)\n-        initial_states = [KTF.variable(init_state_val)]\n-        last_output, outputs, new_states = KTF.rnn(tf_rnn_step_fn, inputs,\n-                                                   initial_states,\n-                                                   go_backwards=False,\n-                                                   masking=False)\n-        tf_last_output = KTF.eval(last_output)\n-        tf_outputs = KTF.eval(outputs)\n-        assert len(new_states) == 1\n-        tf_state = KTF.eval(new_states[0])\n-\n-        assert_allclose(tf_last_output, th_last_output, atol=1e-06)\n-        assert_allclose(tf_outputs, th_outputs, atol=1e-06)\n-        assert_allclose(tf_state, th_state, atol=1e-06)\n-\n-    def test_switch(self):\n-        val = np.random.random()\n-        xth = KTH.variable(val)\n-        xth = KTH.switch(xth >= 0.5, xth * 0.1, xth * 0.2)\n-\n-        xtf = KTF.variable(val)\n-        xtf = KTF.switch(xtf >= 0.5, xtf * 0.1, xtf * 0.2)\n-\n-        zth = KTH.eval(xth)\n-        ztf = KTF.eval(xtf)\n-\n-        assert zth.shape == ztf.shape\n-        assert_allclose(zth, ztf, atol=1e-06)\n-\n-    def test_nn_operations(self):\n-        check_single_tensor_operation('relu', (4, 2), alpha=0.1, max_value=0.5)\n-        check_single_tensor_operation('softmax', (4, 10))\n-        check_single_tensor_operation('softplus', (4, 10))\n-\n-        check_single_tensor_operation('sigmoid', (4, 2))\n-        check_single_tensor_operation('hard_sigmoid', (4, 2))\n-        check_single_tensor_operation('tanh', (4, 2))\n-\n-        # dropout\n-        val = np.random.random((20, 20))\n-        xth = KTH.variable(val)\n-        xtf = KTF.variable(val)\n-        zth = KTH.eval(KTH.dropout(xth, level=0.2))\n-        ztf = KTF.eval(KTF.dropout(xtf, level=0.2))\n-        assert zth.shape == ztf.shape\n-        # dropout patterns are different, only check mean\n-        assert np.abs(zth.mean() - ztf.mean()) < 0.05\n-\n-        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), from_logits=True)\n-        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), from_logits=True)\n-        check_two_tensor_operation('binary_crossentropy', (4, 2), (4, 2), from_logits=False)\n-\n-        check_two_tensor_operation('categorical_crossentropy', (4, 2), (4, 2), from_logits=False)\n-\n-    # def test_conv2d(self):\n-    #     '''conv2d works \"properly\" with Theano and TF but outputs different\n-    #     values in each case. Cause unclear (input / kernel shape format?)\n-    #     '''\n-    #     # TH kernel shape: (depth, input_depth, rows, cols)\n-    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 2, 2),\n-    #                                strides=(1, 1), border_mode='valid')\n-    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 2, 2),\n-    #                                strides=(1, 1), border_mode='same')\n-\n-    #     # TF kernel shape: (rows, cols, input_depth, depth)\n-    #     check_two_tensor_operation('conv2d', (5, 10, 12, 3), (2, 2, 3, 4),\n-    #                                strides=(1, 1), border_mode='valid', dim_ordering='tf')\n-    #     check_two_tensor_operation('conv2d', (5, 10, 12, 3), (2, 2, 3, 4),\n-    #                                strides=(1, 1), border_mode='same', dim_ordering='tf')\n-\n-    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 3, 3),\n-    #                                strides=(1, 1), border_mode='valid')\n-    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 3, 3),\n-    #                                strides=(1, 1), border_mode='same')\n-\n-    #     check_two_tensor_operation('conv2d', (5, 3, 10, 12), (4, 3, 3, 3),\n-    #                                strides=(2, 2), border_mode='valid')\n-\n-    # def test_maxpool2d(self):\n-    #     '''maxpool2d works \"properly\" with Theano and TF but outputs different\n-    #     values in each case. Cause unclear (input shape format?)\n-    #     '''\n-    #     check_single_tensor_operation('maxpool2d', (5, 3, 10, 12), pool_size=(2, 2),\n-    #                                   strides=(1, 1), border_mode='valid')\n-\n-    #     check_single_tensor_operation('maxpool2d', (5, 3, 9, 11), pool_size=(2, 2),\n-    #                                   strides=(1, 1), border_mode='valid')\n-\n-    #     check_single_tensor_operation('maxpool2d', (5, 3, 9, 11), pool_size=(2, 3),\n-    #                                   strides=(1, 1), border_mode='valid')\n-\n-    def test_random_normal(self):\n-        mean = 0.\n-        std = 1.\n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n-\n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n-\n-    def test_random_uniform(self):\n-        mean = 0.\n-        std = 1.\n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n-\n-        rand = KTF.get_value(KTF.random_normal((1000, 1000), mean=mean, std=std))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,173 +0,0 @@\n-import unittest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras.layers import convolutional\n-\n-\n-class TestConvolutions(unittest.TestCase):\n-    def test_convolution_1d(self):\n-        nb_samples = 9\n-        nb_steps = 7\n-        input_dim = 10\n-        filter_length = 6\n-        nb_filter = 5\n-\n-        weights_in = [np.ones((nb_filter, input_dim, filter_length, 1)),\n-                      np.ones(nb_filter)]\n-\n-        input = np.ones((nb_samples, nb_steps, input_dim))\n-        for weight in [None, weights_in]:\n-            for border_mode in ['valid', 'same']:\n-                for subsample_length in [1]:\n-                    if border_mode == 'same' and subsample_length != 1:\n-                        continue\n-                    for W_regularizer in [None, 'l2']:\n-                        for b_regularizer in [None, 'l2']:\n-                            for act_regularizer in [None, 'l2']:\n-                                layer = convolutional.Convolution1D(\n-                                    nb_filter, filter_length,\n-                                    weights=weight,\n-                                    border_mode=border_mode,\n-                                    W_regularizer=W_regularizer,\n-                                    b_regularizer=b_regularizer,\n-                                    activity_regularizer=act_regularizer,\n-                                    subsample_length=subsample_length,\n-                                    input_shape=(None, input_dim))\n-\n-                            layer.input = K.variable(input)\n-                            for train in [True, False]:\n-                                out = K.eval(layer.get_output(train))\n-                                assert input.shape[0] == out.shape[0]\n-                                if border_mode == 'same' and subsample_length == 1:\n-                                    assert input.shape[1] == out.shape[1]\n-\n-                            config = layer.get_config()\n-\n-    def test_maxpooling_1d(self):\n-        nb_samples = 9\n-        nb_steps = 7\n-        input_dim = 10\n-\n-        input = np.ones((nb_samples, nb_steps, input_dim))\n-        for stride in [1, 2]:\n-            layer = convolutional.MaxPooling1D(stride=stride,\n-                                               border_mode='valid')\n-            layer.input = K.variable(input)\n-            for train in [True, False]:\n-                K.eval(layer.get_output(train))\n-\n-            config = layer.get_config()\n-\n-    def test_convolution_2d(self):\n-        nb_samples = 8\n-        nb_filter = 9\n-        stack_size = 7\n-        nb_row = 10\n-        nb_col = 6\n-\n-        input_nb_row = 11\n-        input_nb_col = 12\n-\n-        weights_in = [np.ones((nb_filter, stack_size, nb_row, nb_col)), np.ones(nb_filter)]\n-\n-        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-        for weight in [None, weights_in]:\n-            for border_mode in ['valid', 'same']:\n-                for subsample in [(1, 1), (2, 2)]:\n-                    if border_mode == 'same' and subsample != (1, 1):\n-                        continue\n-                    for W_regularizer in [None, 'l2']:\n-                        for b_regularizer in [None, 'l2']:\n-                            for act_regularizer in [None, 'l2']:\n-                                layer = convolutional.Convolution2D(\n-                                    nb_filter, nb_row, nb_col,\n-                                    weights=weight,\n-                                    border_mode=border_mode,\n-                                    W_regularizer=W_regularizer,\n-                                    b_regularizer=b_regularizer,\n-                                    activity_regularizer=act_regularizer,\n-                                    subsample=subsample,\n-                                    input_shape=(stack_size, None, None))\n-\n-                                layer.input = K.variable(input)\n-                                for train in [True, False]:\n-                                    out = K.eval(layer.get_output(train))\n-                                    if border_mode == 'same' and subsample == (1, 1):\n-                                        assert out.shape[2:] == input.shape[2:]\n-\n-                                config = layer.get_config()\n-\n-    def test_maxpooling_2d(self):\n-        nb_samples = 9\n-        stack_size = 7\n-        input_nb_row = 11\n-        input_nb_col = 12\n-        pool_size = (3, 3)\n-\n-        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-        for strides in [(1, 1), (2, 2)]:\n-            layer = convolutional.MaxPooling2D(strides=strides,\n-                                               border_mode='valid',\n-                                               pool_size=pool_size)\n-            layer.input = K.variable(input)\n-            for train in [True, False]:\n-                K.eval(layer.get_output(train))\n-\n-            config = layer.get_config()\n-\n-    def test_zero_padding_2d(self):\n-        nb_samples = 9\n-        stack_size = 7\n-        input_nb_row = 11\n-        input_nb_col = 12\n-\n-        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-        layer = convolutional.ZeroPadding2D(padding=(2, 2))\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            out = K.eval(layer.get_output(train))\n-            for offset in [0, 1, -1, -2]:\n-                assert_allclose(out[:, :, offset, :], 0.)\n-                assert_allclose(out[:, :, :, offset], 0.)\n-            assert_allclose(out[:, :, 2:-2, 2:-2], 1.)\n-\n-        config = layer.get_config()\n-\n-    def test_upsample_1d(self):\n-        nb_samples = 9\n-        nb_steps = 7\n-        input_dim = 10\n-\n-        input = np.ones((nb_samples, nb_steps, input_dim))\n-        for length in [2, 3, 9]:\n-            layer = convolutional.UpSample1D(length=length)\n-            layer.input = K.variable(input)\n-            for train in [True, False]:\n-                out = K.eval(layer.get_output(train))\n-                assert out.shape[1] == length * nb_steps\n-\n-            config = layer.get_config()\n-\n-    def test_upsample_2d(self):\n-        nb_samples = 9\n-        stack_size = 7\n-        input_nb_row = 11\n-        input_nb_col = 12\n-\n-        input = np.ones((nb_samples, stack_size, input_nb_row, input_nb_col))\n-\n-        for length_row in [2, 3, 9]:\n-            for length_col in [2, 3, 9]:\n-                layer = convolutional.UpSample2D(size=(length_row, length_col))\n-                layer.input = K.variable(input)\n-                for train in [True, False]:\n-                    out = K.eval(layer.get_output(train))\n-                    assert out.shape[2] == length_row * input_nb_row\n-                    assert out.shape[3] == length_col * input_nb_col\n-\n-            config = layer.get_config()\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,160 +0,0 @@\n-import unittest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras.layers import core\n-\n-\n-class TestLayerBase(unittest.TestCase):\n-    def test_input_output(self):\n-        nb_samples = 10\n-        input_dim = 5\n-        layer = core.Layer()\n-\n-        # Once an input is provided, it should be reachable through the\n-        # appropriate getters\n-        input = np.ones((nb_samples, input_dim))\n-        layer.input = K.variable(input)\n-        for train in [True, False]:\n-            assert_allclose(K.eval(layer.get_input(train)), input)\n-            assert_allclose(K.eval(layer.get_output(train)), input)\n-\n-    def test_connections(self):\n-        nb_samples = 10\n-        input_dim = 5\n-        layer1 = core.Layer()\n-        layer2 = core.Layer()\n-\n-        input = np.ones((nb_samples, input_dim))\n-        layer1.input = K.variable(input)\n-\n-        # After connecting, input of layer1 should be passed through\n-        layer2.set_previous(layer1)\n-        for train in [True, False]:\n-            assert_allclose(K.eval(layer2.get_input(train)), input)\n-            assert_allclose(K.eval(layer2.get_output(train)), input)\n-\n-\n-class TestConfigParams(unittest.TestCase):\n-    \"\"\"\n-    Test the constructor, config and params functions of all layers in core.\n-    \"\"\"\n-\n-    def _runner(self, layer):\n-        conf = layer.get_config()\n-        assert (type(conf) == dict)\n-\n-        param = layer.get_params()\n-        # Typically a list or a tuple, but may be any iterable\n-        assert hasattr(param, '__iter__')\n-\n-    def test_base(self):\n-        layer = core.Layer()\n-        self._runner(layer)\n-\n-    def test_masked(self):\n-        layer = core.MaskedLayer()\n-        self._runner(layer)\n-\n-    def test_merge(self):\n-        layer_1 = core.Layer()\n-        layer_2 = core.Layer()\n-        layer_1.set_input_shape((None,))\n-        layer_2.set_input_shape((None,))\n-        layer = core.Merge([layer_1, layer_2])\n-        self._runner(layer)\n-\n-    def test_dropout(self):\n-        layer = core.Dropout(0.5)\n-        self._runner(layer)\n-\n-    def test_activation(self):\n-        layer = core.Activation('linear')\n-        self._runner(layer)\n-\n-    def test_reshape(self):\n-        layer = core.Reshape(dims=(10, 10))\n-        self._runner(layer)\n-\n-    def test_flatten(self):\n-        layer = core.Flatten()\n-        self._runner(layer)\n-\n-    def test_repeat_vector(self):\n-        layer = core.RepeatVector(10)\n-        self._runner(layer)\n-\n-    def test_dense(self):\n-        layer = core.Dense(10, input_shape=(10,))\n-        self._runner(layer)\n-\n-    def test_act_reg(self):\n-        layer = core.ActivityRegularization(0.5, 0.5)\n-        self._runner(layer)\n-\n-    def test_time_dist_dense(self):\n-        layer = core.TimeDistributedDense(10, input_shape=(None, 10))\n-        self._runner(layer)\n-\n-    def test_time_dist_merge(self):\n-        layer = core.TimeDistributedMerge()\n-        self._runner(layer)\n-\n-    def test_autoencoder(self):\n-        layer_1 = core.Layer()\n-        layer_2 = core.Layer()\n-\n-        layer = core.AutoEncoder(layer_1, layer_2)\n-        self._runner(layer)\n-\n-    def test_maxout_dense(self):\n-        layer = core.MaxoutDense(10, 10)\n-        self._runner(layer)\n-\n-\n-class TestMasking(unittest.TestCase):\n-    \"\"\"Test the Masking class\"\"\"\n-\n-    def test_sequences(self):\n-        \"\"\"Test masking sequences with zeroes as padding\"\"\"\n-        # integer inputs, one per timestep, like embeddings\n-        layer = core.Masking()\n-        func = K.function([layer.input], [layer.get_output_mask()])\n-        input_data = np.array([[[1], [2], [3], [0]],\n-                              [[0], [4], [5], [0]]], dtype=np.int32)\n-\n-        # This is the expected output mask, one dimension less\n-        expected = np.array([[1, 1, 1, 0], [0, 1, 1, 0]])\n-\n-        # get mask for this input\n-        output = func([input_data])[0]\n-        self.assertTrue(np.all(output == expected))\n-\n-    def test_non_zero(self):\n-        \"\"\"Test masking with non-zero mask value\"\"\"\n-        layer = core.Masking(5)\n-        func = K.function([layer.input], [layer.get_output_mask()])\n-        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n-                              dtype=np.int32)\n-        output = func([input_data])[0]\n-        expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n-        self.assertTrue(np.all(output == expected))\n-\n-    def test_non_zero_output(self):\n-        \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n-        layer = core.Masking(5)\n-        func = K.function([layer.input], [layer.get_output()])\n-\n-        input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n-                              [[1, 5], [5, 0], [0, 0], [0, 0]]],\n-                              dtype=np.int32)\n-        output = func([input_data])[0]\n-        expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n-                            [[1, 5], [5, 0], [0, 0], [0, 0]]])\n-        self.assertTrue(np.all(output == expected))\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,56 +0,0 @@\n-import unittest\n-import numpy as np\n-\n-from keras.layers import recurrent\n-from keras import backend as K\n-\n-nb_samples, timesteps, input_dim, output_dim = 3, 3, 10, 5\n-\n-\n-def _runner(layer_class):\n-    \"\"\"\n-    All the recurrent layers share the same interface,\n-    so we can run through them with a single function.\n-    \"\"\"\n-    for ret_seq in [True, False]:\n-        layer = layer_class(output_dim, return_sequences=ret_seq,\n-                            weights=None, input_shape=(None, input_dim))\n-        layer.input = K.variable(np.ones((nb_samples, timesteps, input_dim)))\n-        config = layer.get_config()\n-\n-        for train in [True, False]:\n-            out = K.eval(layer.get_output(train))\n-            # Make sure the output has the desired shape\n-            if ret_seq:\n-                assert(out.shape == (nb_samples, timesteps, output_dim))\n-            else:\n-                assert(out.shape == (nb_samples, output_dim))\n-\n-            mask = layer.get_output_mask(train)\n-\n-\n-class TestRNNS(unittest.TestCase):\n-    \"\"\"\n-    Test all the RNNs using a generic test runner function defined above.\n-    \"\"\"\n-    def test_simple(self):\n-        _runner(recurrent.SimpleRNN)\n-\n-    def test_gru(self):\n-        _runner(recurrent.GRU)\n-\n-    def test_lstm(self):\n-        _runner(recurrent.LSTM)\n-\n-    # def test_jzs1(self):\n-    #     _runner(recurrent.JZS1)\n-\n-    # def test_jzs2(self):\n-    #     _runner(recurrent.JZS2)\n-\n-    # def test_jzs3(self):\n-    #     _runner(recurrent.JZS3)\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,76 +0,0 @@\n-import unittest\n-from keras import backend as K\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-\n-def get_standard_values():\n-    '''\n-    These are just a set of floats used for testing the activation\n-    functions, and are useful in multiple tests.\n-    '''\n-    return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n-\n-\n-class TestActivations(unittest.TestCase):\n-\n-    def test_softmax(self):\n-        from keras.activations import softmax as s\n-\n-        # Test using a reference implementation of softmax\n-        def softmax(values):\n-            m = np.max(values)\n-            e = np.exp(values - m)\n-            return e / np.sum(e)\n-\n-        x = K.placeholder(ndim=2)\n-        exp = s(x)\n-        f = K.function([x], [exp])\n-        test_values = get_standard_values()\n-\n-        result = f([test_values])[0]\n-        expected = softmax(test_values)\n-        assert_allclose(result, expected, rtol=1e-05)\n-\n-    def test_relu(self):\n-        '''\n-        Relu implementation doesn't depend on the value being\n-        a theano variable. Testing ints, floats and theano tensors.\n-        '''\n-        from keras.activations import relu as r\n-\n-        x = K.placeholder(ndim=2)\n-        exp = r(x)\n-        f = K.function([x], [exp])\n-\n-        test_values = get_standard_values()\n-        result = f([test_values])[0]\n-\n-        # because no negatives in test values\n-        assert_allclose(result, test_values, rtol=1e-05)\n-\n-    def test_tanh(self):\n-        from keras.activations import tanh as t\n-        test_values = get_standard_values()\n-\n-        x = K.placeholder(ndim=2)\n-        exp = t(x)\n-        f = K.function([x], [exp])\n-\n-        result = f([test_values])[0]\n-        expected = np.tanh(test_values)\n-        assert_allclose(result, expected, rtol=1e-05)\n-\n-    def test_linear(self):\n-        '''\n-        This function does no input validation, it just returns the thing\n-        that was passed in.\n-        '''\n-        from keras.activations import linear as l\n-\n-        xs = [1, 5, True, None, 'foo']\n-        for x in xs:\n-            assert x == l(x)\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,72 +0,0 @@\n-import unittest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from keras import backend as K\n-\n-\n-class TestConstraints(unittest.TestCase):\n-    def setUp(self):\n-        self.some_values = [0.1, 0.5, 3, 8, 1e-7]\n-        np.random.seed(3537)\n-        self.example_array = np.random.random((100, 100)) * 100. - 50.\n-        self.example_array[0, 0] = 0.  # 0 could possibly cause trouble\n-\n-    def test_maxnorm(self):\n-        from keras.constraints import maxnorm\n-\n-        for m in self.some_values:\n-            norm_instance = maxnorm(m)\n-            normed = norm_instance(K.variable(self.example_array))\n-            assert (np.all(K.eval(normed) < m))\n-\n-        # a more explicit example\n-        norm_instance = maxnorm(2.0)\n-        x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n-        x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n-                                    [2.0, 0, 0],\n-                                    [2. / np.sqrt(3), 2. / np.sqrt(3), 2. / np.sqrt(3)]]).T\n-        x_normed_actual = K.eval(norm_instance(K.variable(x)))\n-        assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n-\n-    def test_nonneg(self):\n-        from keras.constraints import nonneg\n-\n-        nonneg_instance = nonneg()\n-\n-        normed = nonneg_instance(K.variable(self.example_array))\n-        assert (np.all(np.min(K.eval(normed), axis=1) == 0.))\n-\n-    def test_identity(self):\n-        from keras.constraints import identity\n-\n-        identity_instance = identity()\n-\n-        normed = identity_instance(self.example_array)\n-        assert (np.all(normed == self.example_array))\n-\n-    def test_identity_oddballs(self):\n-        \"\"\"\n-        test the identity constraint on some more exotic input.\n-        this does not need to pass for the desired real life behaviour,\n-        but it should in the current implementation.\n-        \"\"\"\n-        from keras.constraints import identity\n-        identity_instance = identity()\n-\n-        oddball_examples = [\"Hello\", [1], -1, None]\n-        assert(oddball_examples == identity_instance(oddball_examples))\n-\n-    def test_unitnorm(self):\n-        from keras.constraints import unitnorm\n-        unitnorm_instance = unitnorm()\n-\n-        normalized = unitnorm_instance(K.variable(self.example_array))\n-\n-        norm_of_normalized = np.sqrt(np.sum(K.eval(normalized)**2, axis=1))\n-        # in the unit norm constraint, it should be equal to 1.\n-        difference = norm_of_normalized - 1.\n-        largest_difference = np.max(np.abs(difference))\n-        assert np.abs(largest_difference) < 10e-5\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,102 +0,0 @@\n-import unittest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from keras.layers import normalization\n-from keras.models import Sequential\n-from keras import backend as K\n-\n-\n-class TestBatchNormalization(unittest.TestCase):\n-    def setUp(self):\n-        self.input_1 = np.arange(10)\n-        self.input_2 = np.zeros(10)\n-        self.input_3 = np.ones((10))\n-\n-        self.input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n-\n-    def test_setup(self):\n-        norm_m0 = normalization.BatchNormalization(input_shape=(10, 10))\n-        norm_m1 = normalization.BatchNormalization(input_shape=(10, 10), mode=1)\n-\n-        # mode 3 does not exist\n-        self.assertRaises(Exception,\n-                          normalization.BatchNormalization(input_shape=(10, 10), mode=3))\n-\n-    def test_mode_0(self):\n-        model = Sequential()\n-        norm_m0 = normalization.BatchNormalization(input_shape=(10,))\n-        model.add(norm_m0)\n-        model.compile(loss='mse', optimizer='sgd')\n-\n-        # centered on 5.0, variance 10.0\n-        X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n-        model.fit(X, X, nb_epoch=5, verbose=0)\n-        norm_m0.input = K.variable(X)\n-        out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n-\n-        self.assertAlmostEqual(K.eval(K.mean(out)), 0.0, places=1)\n-        self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=1)\n-\n-    def test_mode_1(self):\n-        norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n-\n-        for inp in [self.input_1, self.input_2, self.input_3]:\n-            norm_m1.input = K.variable(inp)\n-            out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n-            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n-            if inp.std() > 0.:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n-            else:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n-\n-    def test_shapes(self):\n-        \"\"\"\n-        Test batch normalization with various input shapes\n-        \"\"\"\n-        for inp in self.input_shapes:\n-            norm_m0 = normalization.BatchNormalization(input_shape=inp.shape, mode=0)\n-            norm_m0.input = K.variable(inp)\n-            out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n-\n-            norm_m1 = normalization.BatchNormalization(input_shape=inp.shape, mode=1)\n-            norm_m1.input = K.variable(inp)\n-            out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n-\n-    def test_weight_init(self):\n-        \"\"\"\n-        Test weight initialization\n-        \"\"\"\n-        norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1,\n-                                                   weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n-\n-        for inp in [self.input_1, self.input_2, self.input_3]:\n-            norm_m1.input = K.variable(inp)\n-            out = (norm_m1.get_output(train=True) - np.ones(10)) / 1.\n-            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n-            if inp.std() > 0.:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n-            else:\n-                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n-\n-        assert_allclose(K.eval(norm_m1.gamma), np.ones(10))\n-        assert_allclose(K.eval(norm_m1.beta), np.ones(10))\n-\n-    def test_config(self):\n-        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n-                                                epsilon=0.1, momentum=0.9)\n-        conf = norm.get_config()\n-        conf_target = {\"input_shape\": (10, 10),\n-                       \"name\": normalization.BatchNormalization.__name__,\n-                       \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n-        self.assertDictEqual(conf, conf_target)\n-\n-    def test_save_weights(self):\n-        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n-                                                epsilon=0.1)\n-        weights = norm.get_weights()\n-        assert(len(weights) == 4)\n-        norm.set_weights(weights)\n-\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,48 +0,0 @@\n-from __future__ import print_function\n-import unittest\n-from keras.datasets import cifar10, cifar100, reuters, imdb, mnist\n-\n-\n-class TestDatasets(unittest.TestCase):\n-    def test_cifar(self):\n-        print('cifar10')\n-        (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n-\n-        print('cifar100 fine')\n-        (X_train, y_train), (X_test, y_test) = cifar100.load_data('fine')\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n-\n-        print('cifar100 coarse')\n-        (X_train, y_train), (X_test, y_test) = cifar100.load_data('coarse')\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n-\n-    def test_reuters(self):\n-        print('reuters')\n-        (X_train, y_train), (X_test, y_test) = reuters.load_data()\n-\n-    def test_mnist(self):\n-        print('mnist')\n-        (X_train, y_train), (X_test, y_test) = mnist.load_data()\n-        print(X_train.shape)\n-        print(X_test.shape)\n-        print(y_train.shape)\n-        print(y_test.shape)\n-\n-    def test_imdb(self):\n-        print('imdb')\n-        (X_train, y_train), (X_test, y_test) = imdb.load_data()\n-\n-\n-if __name__ == '__main__':\n-    print('Test datasets')\n-    unittest.main()\n\n@@ -1,30 +0,0 @@\n-import unittest\n-import numpy as np\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Flatten\n-from keras.layers.embeddings import Embedding\n-from keras.constraints import unitnorm\n-from keras import backend as K\n-\n-\n-class TestEmbedding(unittest.TestCase):\n-    def setUp(self):\n-        self.X1 = np.array([[1], [2]], dtype='int32')\n-        self.W1 = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], dtype='float32')\n-\n-    def test_unitnorm_constraint(self):\n-        lookup = Sequential()\n-        lookup.add(Embedding(3, 2, weights=[self.W1],\n-                             W_constraint=unitnorm(),\n-                             input_length=1))\n-        lookup.add(Flatten())\n-        lookup.add(Dense(1))\n-        lookup.add(Activation('sigmoid'))\n-        lookup.compile(loss='binary_crossentropy', optimizer='sgd',\n-                       class_mode='binary')\n-        lookup.train_on_batch(self.X1, np.array([[1], [0]], dtype='int32'))\n-        norm = np.linalg.norm(K.get_value(lookup.params[0]), axis=1)\n-        self.assertTrue(np.allclose(norm, np.ones_like(norm).astype('float32')))\n-\n-if __name__ == '__main__':\n-    unittest.main()\n\n@@ -1,270 +0,0 @@\n-from __future__ import print_function\n-import unittest\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras.models import Graph, Sequential\n-from keras.layers import containers\n-from keras.layers.core import Dense, Activation\n-from keras.utils.test_utils import get_test_data\n-\n-X = np.random.random((100, 32))\n-X2 = np.random.random((100, 32))\n-y = np.random.random((100, 4))\n-y2 = np.random.random((100,))\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000,\n-                                                     nb_test=200,\n-                                                     input_shape=(32,),\n-                                                     classification=False,\n-                                                     output_shape=(4,))\n-(X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000,\n-                                                         nb_test=200,\n-                                                         input_shape=(32,),\n-                                                         classification=False,\n-                                                         output_shape=(1,))\n-\n-\n-class TestGraph(unittest.TestCase):\n-    def test_1o_1i(self):\n-        print('test a non-sequential graph with 1 input and 1 output')\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input1')\n-        graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-        graph.add_output(name='output1',\n-                         inputs=['dense2', 'dense3'],\n-                         merge_mode='sum')\n-        graph.compile('rmsprop', {'output1': 'mse'})\n-\n-        history = graph.fit({'input1': X_train, 'output1': y_train},\n-                            nb_epoch=10)\n-        out = graph.predict({'input1': X_test})\n-        assert(type(out == dict))\n-        assert(len(out) == 1)\n-        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n-        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n-        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-        print(loss)\n-        assert(loss < 2.5)\n-\n-        # test validation split\n-        history = graph.fit({'input1': X_train, 'output1': y_train},\n-                            validation_split=0.2, nb_epoch=1)\n-        # test validation data\n-        history = graph.fit({'input1': X_train, 'output1': y_train},\n-                            validation_data={'input1': X_train, 'output1': y_train},\n-                            nb_epoch=1)\n-\n-    def test_1o_1i_2(self):\n-        print('test a more complex non-sequential graph with 1 input and 1 output')\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2-0', input='input1')\n-        graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n-\n-        graph.add_node(Dense(16), name='dense3', input='dense2')\n-        graph.add_node(Dense(4), name='dense4', inputs=['dense1', 'dense3'],\n-                       merge_mode='sum')\n-\n-        graph.add_output(name='output1', inputs=['dense2', 'dense4'],\n-                         merge_mode='sum')\n-        graph.compile('rmsprop', {'output1': 'mse'})\n-\n-        history = graph.fit({'input1': X_train, 'output1': y_train},\n-                            nb_epoch=10)\n-        out = graph.predict({'input1': X_train})\n-        assert(type(out == dict))\n-        assert(len(out) == 1)\n-        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n-        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n-        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-        print(loss)\n-        assert(loss < 2.5)\n-        graph.get_config(verbose=1)\n-\n-    def test_1o_2i(self):\n-        print('test a non-sequential graph with 2 inputs and 1 output')\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-        graph.add_input(name='input2', input_shape=(32,))\n-\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input2')\n-        graph.add_node(Dense(4), name='dense3', input='dense1')\n-\n-        graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                         merge_mode='sum')\n-        graph.compile('rmsprop', {'output1': 'mse'})\n-\n-        history = graph.fit({'input1': X_train, 'input2': X2_train, 'output1': y_train},\n-                            nb_epoch=10)\n-        out = graph.predict({'input1': X_test, 'input2': X2_test})\n-        assert(type(out == dict))\n-        assert(len(out) == 1)\n-        loss = graph.test_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-        loss = graph.train_on_batch({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-        loss = graph.evaluate({'input1': X_test, 'input2': X2_test, 'output1': y_test})\n-        print(loss)\n-        assert(loss < 3.0)\n-        graph.get_config(verbose=1)\n-\n-    def test_2o_1i_weights(self):\n-        print('test a non-sequential graph with 1 input and 2 outputs')\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input1')\n-        graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-        graph.add_output(name='output1', input='dense2')\n-        graph.add_output(name='output2', input='dense3')\n-        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                            nb_epoch=10)\n-        out = graph.predict({'input1': X_test})\n-        assert(type(out == dict))\n-        assert(len(out) == 2)\n-        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-        loss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-        print(loss)\n-        assert(loss < 4.)\n-\n-        print('test weight saving')\n-        graph.save_weights('temp.h5', overwrite=True)\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input1')\n-        graph.add_node(Dense(1), name='dense3', input='dense1')\n-        graph.add_output(name='output1', input='dense2')\n-        graph.add_output(name='output2', input='dense3')\n-        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-        graph.load_weights('temp.h5')\n-        nloss = graph.evaluate({'input1': X_test, 'output1': y_test, 'output2': y2_test})\n-        print(nloss)\n-        assert(loss == nloss)\n-\n-    def test_2o_1i_sample_weights(self):\n-        print('test a non-sequential graph with 1 input and 2 outputs with sample weights')\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input1')\n-        graph.add_node(Dense(1), name='dense3', input='dense1')\n-\n-        graph.add_output(name='output1', input='dense2')\n-        graph.add_output(name='output2', input='dense3')\n-\n-        weights1 = np.random.uniform(size=y_train.shape[0])\n-        weights2 = np.random.uniform(size=y2_train.shape[0])\n-        weights1_test = np.random.uniform(size=y_test.shape[0])\n-        weights2_test = np.random.uniform(size=y2_test.shape[0])\n-\n-        graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n-\n-        history = graph.fit({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                            nb_epoch=10,\n-                            sample_weight={'output1': weights1, 'output2': weights2})\n-        out = graph.predict({'input1': X_test})\n-        assert(type(out == dict))\n-        assert(len(out) == 2)\n-        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test, 'output2': y2_test},\n-                                   sample_weight={'output1': weights1_test, 'output2': weights2_test})\n-        loss = graph.train_on_batch({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                                    sample_weight={'output1': weights1, 'output2': weights2})\n-        loss = graph.evaluate({'input1': X_train, 'output1': y_train, 'output2': y2_train},\n-                              sample_weight={'output1': weights1, 'output2': weights2})\n-        print(loss)\n-\n-    def test_recursive(self):\n-        print('test layer-like API')\n-\n-        graph = containers.Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input1')\n-        graph.add_node(Dense(4), name='dense3', input='dense1')\n-        graph.add_output(name='output1', inputs=['dense2', 'dense3'],\n-                         merge_mode='sum')\n-\n-        seq = Sequential()\n-        seq.add(Dense(32, input_shape=(32,)))\n-        seq.add(graph)\n-        seq.add(Dense(4))\n-\n-        seq.compile('rmsprop', 'mse')\n-\n-        history = seq.fit(X_train, y_train, batch_size=10, nb_epoch=10)\n-        loss = seq.evaluate(X_test, y_test)\n-        print(loss)\n-        assert(loss < 2.5)\n-\n-        loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n-        pred = seq.predict(X_test)\n-        seq.get_config(verbose=1)\n-\n-    def test_create_output(self):\n-        print('test create_output argument')\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-\n-        graph.add_node(Dense(16), name='dense1', input='input1')\n-        graph.add_node(Dense(4), name='dense2', input='input1')\n-        graph.add_node(Dense(4), name='dense3', input='dense1')\n-        graph.add_node(Dense(4), name='output1', inputs=['dense2', 'dense3'],\n-                       merge_mode='sum', create_output=True)\n-        graph.compile('rmsprop', {'output1': 'mse'})\n-\n-        history = graph.fit({'input1': X_train, 'output1': y_train},\n-                            nb_epoch=10)\n-        out = graph.predict({'input1': X_test})\n-        assert(type(out == dict))\n-        assert(len(out) == 1)\n-        loss = graph.test_on_batch({'input1': X_test, 'output1': y_test})\n-        loss = graph.train_on_batch({'input1': X_test, 'output1': y_test})\n-        loss = graph.evaluate({'input1': X_test, 'output1': y_test})\n-        print(loss)\n-        assert(loss < 2.5)\n-\n-    def test_count_params(self):\n-        print('test count params')\n-\n-        nb_units = 100\n-        nb_classes = 2\n-\n-        graph = Graph()\n-        graph.add_input(name='input1', input_shape=(32,))\n-        graph.add_input(name='input2', input_shape=(32,))\n-        graph.add_node(Dense(nb_units),\n-                       name='dense1', input='input1')\n-        graph.add_node(Dense(nb_classes),\n-                       name='dense2', input='input2')\n-        graph.add_node(Dense(nb_classes),\n-                       name='dense3', input='dense1')\n-        graph.add_output(name='output', inputs=['dense2', 'dense3'],\n-                         merge_mode='sum')\n-\n-        n = 32 * nb_units + nb_units\n-        n += 32 * nb_classes + nb_classes\n-        n += nb_units * nb_classes + nb_classes\n-\n-        self.assertEqual(n, graph.count_params())\n-\n-        graph.compile('rmsprop', {'output': 'binary_crossentropy'})\n-\n-        self.assertEqual(n, graph.count_params())\n-\n-\n-if __name__ == '__main__':\n-    print('Test graph model')\n-    unittest.main()\n\n@@ -1,44 +0,0 @@\n-import numpy as np\n-np.random.seed(1337)\n-\n-import unittest\n-from keras.models import Sequential, weighted_objective\n-from keras.layers.core import TimeDistributedDense, Masking\n-from keras import objectives\n-from keras import backend as K\n-\n-\n-class TestLossMasking(unittest.TestCase):\n-    def test_loss_masking(self):\n-        X = np.array(\n-            [[[1, 1], [2, 1], [3, 1], [5, 5]],\n-             [[1, 5], [5, 0], [0, 0], [0, 0]]], dtype=np.int32)\n-        model = Sequential()\n-        model.add(Masking(mask_value=0, input_shape=(None, 2)))\n-        model.add(TimeDistributedDense(1, init='one'))\n-        model.compile(loss='mse', optimizer='sgd')\n-        y = model.predict(X)\n-        history = model.fit(X, 4 * y, nb_epoch=1, batch_size=2, verbose=1)\n-        assert history.history['loss'][0] == 285.\n-\n-    def test_loss_masking_time(self):\n-        weighted_loss = weighted_objective(objectives.get('mae'))\n-        shape = (3, 4, 2)\n-        X = np.arange(24).reshape(shape)\n-        Y = 2 * X\n-\n-        # Normally the trailing 1 is added by standardize_weights\n-        weights = np.ones((3,))\n-        mask = np.ones((3, 4))\n-        mask[1, 0] = 0\n-\n-        out = K.eval(weighted_loss(K.variable(X),\n-                                   K.variable(Y),\n-                                   K.variable(weights),\n-                                   K.variable(mask)))\n-        print(out)\n-\n-\n-if __name__ == '__main__':\n-    print('Test loss masking')\n-    unittest.main()\n\n@@ -1,159 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-\n-import numpy as np\n-np.random.seed(1336)  # for reproducibility\n-\n-from keras.datasets import mnist\n-from keras.models import Sequential, Graph\n-from keras.layers.core import Dense, Activation\n-from keras.utils import np_utils\n-import unittest\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 10\n-weighted_class = 9\n-standard_weight = 1\n-high_weight = 5\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\") / 255\n-X_test = X_test.astype(\"float32\") / 255\n-\n-# convert class vectors to binary class matrices\n-y_train = y_train[:max_train_samples]\n-y_test = y_test[:max_test_samples]\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n-test_ids = np.where(y_test == np.array(weighted_class))[0]\n-\n-class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n-class_weight[weighted_class] = high_weight\n-\n-sample_weight = np.ones((y_train.shape[0])) * standard_weight\n-sample_weight[y_train == weighted_class] = high_weight\n-\n-\n-def create_sequential_model():\n-    model = Sequential()\n-    model.add(Dense(50, input_shape=(784,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(10))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-def create_graph_model():\n-    model = Graph()\n-    model.add_input(name='input', input_shape=(784,))\n-    model.add_node(Dense(50, activation='relu'), name='d1', input='input')\n-    model.add_node(Dense(10, activation='softmax'), name='d2', input='d1')\n-    model.add_output(name='output', input='d2')\n-    return model\n-\n-\n-def _test_weights_sequential(model, class_weight=None, sample_weight=None):\n-    if sample_weight is not None:\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight)\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight,\n-                  validation_split=0.1)\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 3, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight,\n-                  validation_data=(X_train, Y_train, sample_weight))\n-    else:\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 2, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight)\n-        model.fit(X_train, Y_train, batch_size=batch_size,\n-                  nb_epoch=nb_epoch // 2, verbose=0,\n-                  class_weight=class_weight, sample_weight=sample_weight,\n-                  validation_split=0.1)\n-\n-    model.train_on_batch(X_train[:32], Y_train[:32],\n-                         class_weight=class_weight,\n-                         sample_weight=sample_weight[:32] if sample_weight is not None else None)\n-    model.test_on_batch(X_train[:32], Y_train[:32],\n-                        sample_weight=sample_weight[:32] if sample_weight is not None else None)\n-    score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-    return score\n-\n-\n-def _test_weights_graph(model, class_weight=None, sample_weight=None):\n-    model.fit({'input': X_train, 'output': Y_train},\n-              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-              class_weight={'output': class_weight},\n-              sample_weight={'output': sample_weight})\n-    model.fit({'input': X_train, 'output': Y_train},\n-              batch_size=batch_size, nb_epoch=nb_epoch // 2, verbose=0,\n-              class_weight={'output': class_weight},\n-              sample_weight={'output': sample_weight}, validation_split=0.1)\n-\n-    model.train_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n-                         class_weight={'output': class_weight},\n-                         sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n-    model.test_on_batch({'input': X_train[:32], 'output': Y_train[:32]},\n-                        sample_weight={'output': sample_weight[:32] if sample_weight is not None else None})\n-    score = model.evaluate({'input': X_test[test_ids, :],\n-                            'output': Y_test[test_ids, :]},\n-                           verbose=0)\n-    return score\n-\n-\n-class TestLossWeighting(unittest.TestCase):\n-    def test_sequential(self):\n-        for loss in ['mae', 'mse']:\n-            print('loss:', loss)\n-            print('sequential')\n-            # no weights: reference point\n-            model = create_sequential_model()\n-            model.compile(loss=loss, optimizer='rmsprop')\n-            standard_score = _test_weights_sequential(model)\n-            # test class_weight\n-            model = create_sequential_model()\n-            model.compile(loss=loss, optimizer='rmsprop')\n-            score = _test_weights_sequential(model, class_weight=class_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n-            # test sample_weight\n-            model = create_sequential_model()\n-            model.compile(loss=loss, optimizer='rmsprop')\n-            score = _test_weights_sequential(model, sample_weight=sample_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n-\n-    def test_graph(self):\n-        for loss in ['mae', 'mse']:\n-            print('loss:', loss)\n-            print('graph')\n-            # no weights: reference point\n-            model = create_graph_model()\n-            model.compile(loss={'output': loss}, optimizer='rmsprop')\n-            standard_score = _test_weights_graph(model)\n-            # test class_weight\n-            model = create_graph_model()\n-            model.compile(loss={'output': loss}, optimizer='rmsprop')\n-            score = _test_weights_graph(model, class_weight=class_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n-            # test sample_weight\n-            model = create_graph_model()\n-            model.compile(loss={'output': loss}, optimizer='rmsprop')\n-            score = _test_weights_graph(model, sample_weight=sample_weight)\n-            print('score:', score, ' vs.', standard_score)\n-            self.assertTrue(score < standard_score)\n-\n-\n-if __name__ == '__main__':\n-    print('Test class_weight and sample_weight')\n-    unittest.main()\n\n@@ -1,59 +0,0 @@\n-from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n-from keras.utils.np_utils import to_categorical\n-import unittest\n-\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,),\n-                                                     classification=True, nb_class=2)\n-y_train = to_categorical(y_train)\n-y_test = to_categorical(y_test)\n-\n-\n-def get_model(input_dim, nb_hidden, output_dim):\n-    model = Sequential()\n-    model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(output_dim))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-def _test_optimizer(optimizer, target=0.9):\n-    model = get_model(X_train.shape[1], 10, y_train.shape[1])\n-    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n-    history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-    return history.history['val_acc'][-1] > target\n-\n-\n-class TestOptimizers(unittest.TestCase):\n-    def test_sgd(self):\n-        print('test SGD')\n-        sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n-        self.assertTrue(_test_optimizer(sgd))\n-\n-    def test_rmsprop(self):\n-        print('test RMSprop')\n-        self.assertTrue(_test_optimizer(RMSprop()))\n-\n-    def test_adagrad(self):\n-        print('test Adagrad')\n-        self.assertTrue(_test_optimizer(Adagrad()))\n-\n-    def test_adadelta(self):\n-        print('test Adadelta')\n-        self.assertTrue(_test_optimizer(Adadelta()))\n-\n-    def test_adam(self):\n-        print('test Adam')\n-        self.assertTrue(_test_optimizer(Adam()))\n-\n-if __name__ == '__main__':\n-    print('Test optimizers')\n-    unittest.main()\n\n@@ -1,62 +0,0 @@\n-import unittest\n-import numpy as np\n-np.random.seed(1337) # for reproducibility\n-\n-from keras.models import Sequential\n-from keras.layers.core import Merge, Dense, Activation, Flatten, ActivityRegularization\n-from keras.layers.embeddings import Embedding\n-from keras.datasets import mnist\n-from keras.utils import np_utils\n-from keras import regularizers\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 5\n-weighted_class = 9\n-standard_weight = 1\n-high_weight = 5\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\") / 255\n-X_test = X_test.astype(\"float32\") / 255\n-\n-# convert class vectors to binary class matrices\n-y_train = y_train[:max_train_samples]\n-y_test = y_test[:max_test_samples]\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n-test_ids = np.where(y_test == np.array(weighted_class))[0]\n-\n-\n-def create_model(weight_reg=None, activity_reg=None):\n-    model = Sequential()\n-    model.add(Dense(50, input_shape=(784,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(10, W_regularizer=weight_reg, activity_regularizer=activity_reg))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-class TestRegularizers(unittest.TestCase):\n-    def test_W_reg(self):\n-        for reg in [regularizers.identity(), regularizers.l1(), regularizers.l2(), regularizers.l1l2()]:\n-            model = create_model(weight_reg=reg)\n-            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-            model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-\n-    def test_A_reg(self):\n-        for reg in [regularizers.activity_l1(), regularizers.activity_l2()]:\n-            model = create_model(activity_reg=reg)\n-            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-            model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n-\n-if __name__ == '__main__':\n-    print('Test weight and activity regularizers')\n-    unittest.main()\n\n@@ -1,407 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import unittest\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras import backend as K\n-from keras.models import Sequential, model_from_json, model_from_yaml\n-from keras.layers.core import Dense, Activation, Merge, Lambda, LambdaMerge\n-from keras.utils import np_utils\n-from keras.utils.test_utils import get_test_data\n-import pickle\n-import sys\n-\n-input_dim = 32\n-nb_hidden = 16\n-nb_class = 4\n-batch_size = 64\n-nb_epoch = 1\n-\n-train_samples = 5000\n-test_samples = 1000\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n-                                                     nb_test=test_samples,\n-                                                     input_shape=(input_dim,),\n-                                                     classification=True,\n-                                                     nb_class=4)\n-y_test = np_utils.to_categorical(y_test)\n-y_train = np_utils.to_categorical(y_train)\n-print(X_train.shape)\n-print(y_train.shape)\n-\n-\n-class TestSequential(unittest.TestCase):\n-    def test_sequential(self):\n-        print('Test sequential')\n-        model = Sequential()\n-        model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        model.add(Activation('relu'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n-\n-        model.train_on_batch(X_train[:32], y_train[:32])\n-\n-        loss = model.evaluate(X_train, y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n-        model.predict(X_test, verbose=0)\n-        model.predict_classes(X_test, verbose=0)\n-        model.predict_proba(X_test, verbose=0)\n-        model.get_config(verbose=1)\n-\n-        print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n-        model = Sequential()\n-        model.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        model.add(Activation('relu'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        model.load_weights('temp.h5')\n-\n-        nloss = model.evaluate(X_train, y_train, verbose=0)\n-        assert(loss == nloss)\n-\n-        # test json serialization\n-        json_data = model.to_json()\n-        model = model_from_json(json_data)\n-\n-        # test yaml serialization\n-        yaml_data = model.to_yaml()\n-        model = model_from_yaml(yaml_data)\n-\n-    def test_merge_sum(self):\n-        print('Test merge: sum')\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([left, right], mode='sum'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n-        model.predict([X_test, X_test], verbose=0)\n-        model.predict_classes([X_test, X_test], verbose=0)\n-        model.predict_proba([X_test, X_test], verbose=0)\n-        model.get_config(verbose=1)\n-\n-        print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-        model = Sequential()\n-        model.add(Merge([left, right], mode='sum'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.load_weights('temp.h5')\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print(nloss)\n-        assert(loss == nloss)\n-\n-    def test_merge_dot1(self):\n-        print('Test merge: dot')\n-        left = Sequential()\n-        left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-        right.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([left, right], mode='dot', dot_axes=1))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    def test_merge_dot2(self):\n-        print('Test merge: dot')\n-        left = Sequential()\n-        left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n-        right.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([left, right], mode='dot', dot_axes=([1], [1])))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    def test_merge_concat(self):\n-        print('Test merge: concat')\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([left, right], mode='concat'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n-        model.predict([X_test, X_test], verbose=0)\n-        model.predict_classes([X_test, X_test], verbose=0)\n-        model.predict_proba([X_test, X_test], verbose=0)\n-        model.get_config(verbose=1)\n-\n-        print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([left, right], mode='concat'))\n-\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        model.load_weights('temp.h5')\n-\n-        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        assert(loss == nloss)\n-\n-    def test_merge_recursivity(self):\n-        print('Test merge recursivity')\n-\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-\n-        righter = Sequential()\n-        righter.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        righter.add(Activation('relu'))\n-\n-        intermediate = Sequential()\n-        intermediate.add(Merge([left, right], mode='sum'))\n-        intermediate.add(Dense(nb_hidden))\n-        intermediate.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([intermediate, righter], mode='sum'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n-        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n-        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-        loss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n-        model.predict([X_test, X_test, X_test], verbose=0)\n-        model.predict_classes([X_test, X_test, X_test], verbose=0)\n-        model.predict_proba([X_test, X_test, X_test], verbose=0)\n-        model.get_config(verbose=1)\n-\n-        model.save_weights('temp.h5', overwrite=True)\n-        model.load_weights('temp.h5')\n-\n-        nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n-        print(nloss)\n-        assert(loss == nloss)\n-\n-    def test_merge_overlap(self):\n-        print('Test merge overlap')\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(Merge([left, left], mode='sum'))\n-        model.add(Dense(nb_class))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n-\n-        model.train_on_batch(X_train[:32], y_train[:32])\n-\n-        loss = model.evaluate(X_train, y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n-        model.predict(X_test, verbose=0)\n-        model.predict_classes(X_test, verbose=0)\n-        model.predict_proba(X_test, verbose=0)\n-        model.get_config(verbose=1)\n-\n-        model.save_weights('temp.h5', overwrite=True)\n-        model.load_weights('temp.h5')\n-\n-        nloss = model.evaluate(X_train, y_train, verbose=0)\n-        print(nloss)\n-        assert(loss == nloss)\n-\n-    def test_lambda(self):\n-        print('Test lambda: sum')\n-\n-        def func(X):\n-            s = X[0]\n-            for i in range(1, len(X)):\n-                s += X[i]\n-            return s\n-\n-        def activation(X):\n-            return K.softmax(X)\n-\n-        def output_shape(input_shapes):\n-            return input_shapes[0]\n-\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-\n-        model = Sequential()\n-        model.add(LambdaMerge([left, right], function=func,\n-                              output_shape=output_shape))\n-        model.add(Dense(nb_class))\n-        model.add(Lambda(activation))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        print('loss:', loss)\n-        if loss > 0.7:\n-            raise Exception('Score too low, learning issue.')\n-        model.predict([X_test, X_test], verbose=0)\n-        model.predict_classes([X_test, X_test], verbose=0)\n-        model.predict_proba([X_test, X_test], verbose=0)\n-        model.get_config(verbose=1)\n-\n-        print('test weight saving')\n-        model.save_weights('temp.h5', overwrite=True)\n-        left = Sequential()\n-        left.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        left.add(Activation('relu'))\n-        right = Sequential()\n-        right.add(Dense(nb_hidden, input_shape=(input_dim,)))\n-        right.add(Activation('relu'))\n-        model = Sequential()\n-        model.add(LambdaMerge([left, right], function=func,\n-                              output_shape=output_shape))\n-        model.add(Dense(nb_class))\n-        model.add(Lambda(activation))\n-        model.load_weights('temp.h5')\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        assert(loss == nloss)\n-\n-        print('test serializing')\n-        del func, activation  # Make sure that the model has the function code, not just the function name.\n-        sys.setrecursionlimit(50000)\n-        model_str = pickle.dumps(model)\n-        model = pickle.loads(model_str)\n-        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n-        assert(loss == nloss)\n-\n-    def test_count_params(self):\n-        print('test count params')\n-        input_dim = 20\n-        nb_units = 10\n-        nb_classes = 2\n-\n-        n = input_dim * nb_units + nb_units\n-        n += nb_units * nb_units + nb_units\n-        n += nb_units * nb_classes + nb_classes\n-\n-        model = Sequential()\n-        model.add(Dense(nb_units, input_shape=(input_dim,)))\n-        model.add(Dense(nb_units))\n-        model.add(Dense(nb_classes))\n-        model.add(Activation('softmax'))\n-\n-        self.assertEqual(n, model.count_params())\n-\n-        model.compile('sgd', 'binary_crossentropy')\n-\n-        self.assertEqual(n, model.count_params())\n-\n-\n-if __name__ == '__main__':\n-    print('Test Sequential model')\n-    unittest.main()\n\n@@ -1,140 +0,0 @@\n-import unittest\n-import numpy as np\n-\n-from keras import backend as K\n-from keras.layers.core import *\n-from keras.layers.convolutional import *\n-from keras.layers.recurrent import SimpleRNN\n-\n-\n-def check_layer_output_shape(layer, input_data):\n-    ndim = len(input_data.shape)\n-    layer.input = K.placeholder(ndim=ndim)\n-    layer.set_input_shape(input_data.shape[1:])\n-    expected_output_shape = layer.output_shape[1:]\n-\n-    function = K.function([layer.input], [layer.get_output()])\n-    output = function([input_data])[0]\n-    assert output.shape[1:] == expected_output_shape\n-\n-\n-class TestShapeInference(unittest.TestCase):\n-    # ########\n-    # # Core #\n-    # ########\n-    def test_Reshape(self):\n-        layer = Reshape(dims=(2, 3))\n-        input_data = np.random.random((2, 6))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_Permute(self):\n-        layer = Permute(dims=(1, 3, 2))\n-        input_data = np.random.random((2, 2, 4, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_Flatten(self):\n-        layer = Flatten()\n-        input_data = np.random.random((2, 2, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_RepeatVector(self):\n-        layer = RepeatVector(2)\n-        input_data = np.random.random((2, 2))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_Dense(self):\n-        layer = Dense(3)\n-        input_data = np.random.random((2, 2))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_TimeDistributedDense(self):\n-        layer = TimeDistributedDense(2)\n-        input_data = np.random.random((2, 2, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-    #################\n-    # Convolutional #\n-    #################\n-    def test_Convolution1D(self):\n-        for border_mode in ['same', 'valid']:\n-            for filter_length in [2, 3]:\n-                for subsample_length in [1, 2]:\n-                    if subsample_length > 1 and border_mode == 'same':\n-                        continue\n-                    for input_data_shape in [(2, 3, 2), (2, 4, 2)]:\n-                        layer = Convolution1D(nb_filter=1,\n-                                              filter_length=filter_length,\n-                                              border_mode=border_mode,\n-                                              subsample_length=subsample_length)\n-                        input_data = np.random.random(input_data_shape)\n-                        check_layer_output_shape(layer, input_data)\n-\n-    def test_Convolution2D(self):\n-        for border_mode in ['same', 'valid']:\n-            for nb_row, nb_col in [(2, 1), (3, 2)]:\n-                for subsample in [(1, 1), (2, 2)]:\n-                    if (subsample[0] > 1 or subsample[1] > 1) and border_mode == 'same':\n-                        continue\n-                    for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4)]:\n-                        layer = Convolution2D(nb_filter=1, nb_row=nb_row,\n-                                              nb_col=nb_row,\n-                                              border_mode=border_mode,\n-                                              subsample=subsample)\n-                        input_data = np.random.random(input_data_shape)\n-                        check_layer_output_shape(layer, input_data)\n-\n-    def test_MaxPooling1D(self):\n-        for ignore_border in [True, False]:\n-            for stride in [1, 2]:\n-                for pool_length in [1, 2]:\n-                    for input_data_shape in [(2, 1, 3), (2, 1, 4)]:\n-                        layer = MaxPooling1D(pool_length=pool_length,\n-                                             stride=stride,\n-                                             border_mode='valid')\n-                        input_data = np.random.random(input_data_shape)\n-                        check_layer_output_shape(layer, input_data)\n-\n-    def test_MaxPooling2D(self):\n-        for ignore_border in [True, False]:\n-            for strides in [(1, 1), (2, 2)]:\n-                for pool_size in [(2, 2), (3, 3), (4, 4)]:\n-                    for input_data_shape in [(2, 1, 3, 3), (2, 1, 4, 4), (2, 1, 5, 5), (2, 1, 6, 6)]:\n-                        layer = MaxPooling2D(pool_size=pool_size,\n-                                             strides=strides,\n-                                             border_mode='valid')\n-                        input_data = np.random.random(input_data_shape)\n-                        check_layer_output_shape(layer, input_data)\n-\n-    def test_UpSample1D(self):\n-        layer = UpSample1D(length=2)\n-        input_data = np.random.random((2, 2, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_UpSample2D(self):\n-        layer = UpSample2D(size=(2, 2))\n-        input_data = np.random.random((2, 1, 2, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_ZeroPadding1D(self):\n-        layer = ZeroPadding1D(1)\n-        input_data = np.random.random((2, 2, 1))\n-        check_layer_output_shape(layer, input_data)\n-\n-    def test_ZeroPadding2D(self):\n-        layer = ZeroPadding2D((1, 2))\n-        input_data = np.random.random((2, 1, 2, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-    # #############\n-    # # Recurrent #\n-    # #############\n-    def test_SimpleRNN(self):\n-        # all recurrent layers inherit output_shape\n-        # from the same base recurrent layer\n-        layer = SimpleRNN(2)\n-        input_data = np.random.random((2, 2, 3))\n-        check_layer_output_shape(layer, input_data)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,132 +0,0 @@\n-from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, TimeDistributedDense, Flatten\n-from keras.layers.recurrent import GRU\n-from keras.layers.convolutional import Convolution2D\n-from keras.utils.np_utils import to_categorical\n-import unittest\n-\n-\n-class TestTasks(unittest.TestCase):\n-    def test_vector_clf(self):\n-        nb_hidden = 10\n-\n-        print('vector classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,),\n-                                                             classification=True, nb_class=2)\n-        print('X_train:', X_train.shape)\n-        print('X_test:', X_test.shape)\n-        print('y_train:', y_train.shape)\n-        print('y_test:', y_test.shape)\n-\n-        y_train = to_categorical(y_train)\n-        y_test = to_categorical(y_test)\n-\n-        model = Sequential()\n-        model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n-        model.add(Activation('relu'))\n-        model.add(Dense(y_train.shape[-1]))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-        print(history.history)\n-        self.assertTrue(history.history['val_acc'][-1] > 0.8)\n-\n-    def test_vector_reg(self):\n-        nb_hidden = 10\n-        print('vector regression data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(10,), output_shape=(2,),\n-                                                             classification=False)\n-        print('X_train:', X_train.shape)\n-        print('X_test:', X_test.shape)\n-        print('y_train:', y_train.shape)\n-        print('y_test:', y_test.shape)\n-\n-        model = Sequential()\n-        model.add(Dense(nb_hidden, input_shape=(X_train.shape[-1],)))\n-        model.add(Activation('tanh'))\n-        model.add(Dense(y_train.shape[-1]))\n-        model.compile(loss='hinge', optimizer='adagrad')\n-        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n-        self.assertTrue(history.history['val_loss'][-1] < 0.9)\n-\n-    def test_temporal_clf(self):\n-        print('temporal classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 5),\n-                                                             classification=True, nb_class=2)\n-        print('X_train:', X_train.shape)\n-        print('X_test:', X_test.shape)\n-        print('y_train:', y_train.shape)\n-        print('y_test:', y_test.shape)\n-\n-        y_train = to_categorical(y_train)\n-        y_test = to_categorical(y_test)\n-\n-        model = Sequential()\n-        model.add(GRU(y_train.shape[-1], input_shape=(None, X_train.shape[-1])))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n-        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n-\n-    def test_temporal_reg(self):\n-        print('temporal regression data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 5), output_shape=(2,),\n-                                                             classification=False)\n-        print('X_train:', X_train.shape)\n-        print('X_test:', X_test.shape)\n-        print('y_train:', y_train.shape)\n-        print('y_test:', y_test.shape)\n-\n-        model = Sequential()\n-        model.add(GRU(y_train.shape[-1], input_shape=(None, X_train.shape[-1])))\n-        model.compile(loss='hinge', optimizer='adam')\n-        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n-        self.assertTrue(history.history['val_loss'][-1] < 0.8)\n-\n-    def test_seq_to_seq(self):\n-        print('sequence to sequence data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 5), output_shape=(3, 5),\n-                                                             classification=False)\n-        print('X_train:', X_train.shape)\n-        print('X_test:', X_test.shape)\n-        print('y_train:', y_train.shape)\n-        print('y_test:', y_test.shape)\n-\n-        model = Sequential()\n-        model.add(TimeDistributedDense(y_train.shape[-1], input_shape=(None, X_train.shape[-1])))\n-        model.compile(loss='hinge', optimizer='rmsprop')\n-        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n-        self.assertTrue(history.history['val_loss'][-1] < 0.8)\n-\n-    def test_img_clf(self):\n-        print('image classification data:')\n-        (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(3, 8, 8),\n-                                                             classification=True, nb_class=2)\n-        print('X_train:', X_train.shape)\n-        print('X_test:', X_test.shape)\n-        print('y_train:', y_train.shape)\n-        print('y_test:', y_test.shape)\n-\n-        y_train = to_categorical(y_train)\n-        y_test = to_categorical(y_test)\n-\n-        model = Sequential()\n-        model.add(Convolution2D(8, 8, 8, input_shape=(3, 8, 8)))\n-        model.add(Activation('sigmoid'))\n-        model.add(Flatten())\n-        model.add(Dense(y_test.shape[-1]))\n-        model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-        history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-        print(history.history['val_acc'][-1])\n-        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n-\n-\n-if __name__ == '__main__':\n-    print('Test different types of classification and regression tasks')\n-    unittest.main()\n\n\n@@ -1,134 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import mnist\n-from keras.models import Sequential, model_from_config\n-from keras.layers.core import AutoEncoder, Dense, Activation, TimeDistributedDense, Flatten\n-from keras.layers.recurrent import LSTM\n-from keras.layers.embeddings import Embedding\n-from keras.layers.core import Layer\n-from keras.layers import containers\n-from keras.utils import np_utils\n-import numpy as np\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 5\n-activation = 'linear'\n-\n-input_dim = 784\n-hidden_dim = 392\n-\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(60000, input_dim)[:max_train_samples]\n-X_test = X_test.reshape(10000, input_dim)[:max_test_samples]\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-print(\"X_train: \", X_train.shape)\n-print(\"X_test: \", X_test.shape)\n-\n-\n-##########################\n-# dense model test       #\n-##########################\n-\n-print(\"Training classical fully connected layer for classification\")\n-model_classical = Sequential()\n-model_classical.add(Dense(input_dim, 10, activation=activation))\n-model_classical.add(Activation('softmax'))\n-model_classical.get_config(verbose=1)\n-model_classical.compile(loss='categorical_crossentropy', optimizer='adam')\n-model_classical.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n-classical_score = model_classical.evaluate(X_test, Y_test, verbose=0, show_accuracy=True)\n-print('\\nclassical_score:', classical_score)\n-\n-##########################\n-# autoencoder model test #\n-##########################\n-\n-\n-def build_lstm_autoencoder(autoencoder, X_train, X_test):\n-    X_train = X_train[:, np.newaxis, :]\n-    X_test = X_test[:, np.newaxis, :]\n-    print(\"Modified X_train: \", X_train.shape)\n-    print(\"Modified X_test: \", X_test.shape)\n-\n-    # The TimeDistributedDense isn't really necessary, however you need a lot of GPU memory to do 784x394-394x784\n-    autoencoder.add(TimeDistributedDense(input_dim, 16))\n-    autoencoder.add(AutoEncoder(encoder=LSTM(16, 8, activation=activation, return_sequences=True),\n-                                decoder=LSTM(8, input_dim, activation=activation, return_sequences=True),\n-                                output_reconstruction=False))\n-    return autoencoder, X_train, X_test\n-\n-\n-def build_deep_classical_autoencoder(autoencoder):\n-    encoder = containers.Sequential([Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)])\n-    decoder = containers.Sequential([Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)])\n-    autoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))\n-    return autoencoder\n-\n-# Try different things here: 'lstm' or 'classical' or 'denoising'\n-# or 'deep_denoising'\n-\n-for autoencoder_type in ['classical', 'lstm']:\n-    print(autoencoder_type)\n-    print('-'*40)\n-    # Build our autoencoder model\n-    autoencoder = Sequential()\n-    if autoencoder_type == 'lstm':\n-        print(\"Training LSTM AutoEncoder\")\n-        autoencoder, X_train, X_test = build_lstm_autoencoder(autoencoder, X_train, X_test)\n-    elif autoencoder_type == 'classical':\n-        print(\"Training Classical AutoEncoder\")\n-        autoencoder = build_deep_classical_autoencoder(autoencoder)\n-    else:\n-        print(\"Error: unknown autoencoder type!\")\n-        exit(-1)\n-\n-    autoencoder.compile(loss='mean_squared_error', optimizer='adam')\n-    # Do NOT use validation data with return output_reconstruction=True\n-    autoencoder.fit(X_train, X_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1)\n-\n-    # Do an inference pass\n-    prefilter_train = autoencoder.predict(X_train, verbose=0)\n-    prefilter_test = autoencoder.predict(X_test, verbose=0)\n-    print(\"prefilter_train: \", prefilter_train.shape)\n-    print(\"prefilter_test: \", prefilter_test.shape)\n-\n-    # Classify results from Autoencoder\n-    print(\"Building classical fully connected layer for classification\")\n-    model = Sequential()\n-    if autoencoder_type == 'lstm':\n-        model.add(TimeDistributedDense(8, nb_classes, activation=activation))\n-        model.add(Flatten())\n-    elif autoencoder_type == 'classical':\n-        model.add(Dense(prefilter_train.shape[1], nb_classes, activation=activation))\n-    else:\n-        model.add(Dense(prefilter_train.shape[1], nb_classes, activation=activation))\n-\n-    model.add(Activation('softmax'))\n-\n-    model.get_config(verbose=1)\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(prefilter_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(prefilter_test, Y_test))\n-\n-    score = model.evaluate(prefilter_test, Y_test, verbose=0, show_accuracy=True)\n-    print('\\nscore:', score)\n-\n-    print('Loss change:', (score[0] - classical_score[0])/classical_score[0], '%')\n-    print('Accuracy change:', (score[1] - classical_score[1])/classical_score[1], '%')\n-\n-    # check serialization\n-    config = autoencoder.get_config(verbose=1)\n-    autoencoder = model_from_config(config)\n\n@@ -1,236 +0,0 @@\n-import numpy as np\n-import random\n-import theano\n-\n-from keras.models import Sequential\n-from keras.callbacks import Callback\n-from keras.layers.core import Dense, Dropout, Activation, Flatten\n-from keras.regularizers import l2\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n-from keras.utils import np_utils\n-from keras.datasets import mnist\n-import keras.callbacks as cbks\n-\n-from matplotlib import pyplot as plt\n-from matplotlib import animation\n-\n-##############################\n-# model DrawActivations test #\n-##############################\n-\n-print('Running DrawActivations test')\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 10\n-\n-max_train_samples = 512\n-max_test_samples = 1\n-\n-np.random.seed(1337)\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(-1,1,28,28)[:max_train_samples]\n-X_train = X_train.astype(\"float32\")\n-X_train /= 255\n-\n-X_test = X_test.reshape(-1,1,28,28)[:max_test_samples]\n-X_test = X_test.astype(\"float32\")\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-\n-class Frames(object):\n-    def __init__(self, n_plots=16):\n-        self._n_frames = 0\n-        self._framedata = []\n-        self._titles = []\n-        for i in range(n_plots):\n-            self._framedata.append([])\n-\n-    def add_frame(self, i, frame):\n-        self._framedata[i].append(frame)\n-\n-    def set_title(self, title):\n-        self._titles.append(title)\n-\n-class SubplotTimedAnimation(animation.TimedAnimation):\n-\n-    def __init__(self, fig, frames, grid=(4, 4), interval=10, blit=False, **kwargs):\n-        self.n_plots = grid[0] * grid[1]\n-        self.axes = [fig.add_subplot(grid[0], grid[1], i + 1) for i in range(self.n_plots)]\n-        for axis in self.axes:\n-            axis.get_xaxis().set_ticks([])\n-            axis.get_yaxis().set_ticks([])\n-        self.frames = frames\n-        self.imgs = [self.axes[i].imshow(frames._framedata[i][0], interpolation='nearest', cmap='bone') for i in range(self.n_plots)]\n-        self.title = fig.suptitle('')\n-        super(SubplotTimedAnimation, self).__init__(fig, interval=interval, blit=blit, **kwargs)\n-\n-    def _draw_frame(self, j):\n-        for i in range(self.n_plots):\n-            self.imgs[i].set_data(self.frames._framedata[i][j])\n-        if len(self.frames._titles) > j:\n-            self.title.set_text(self.frames._titles[j])\n-        self._drawn_artists = self.imgs\n-\n-    def new_frame_seq(self):\n-        return iter(range(len(self.frames._framedata[0])))\n-\n-    def _init_draw(self):\n-        for img in self.imgs:\n-            img.set_data([[]])\n-\n-def combine_imgs(imgs, grid=(1,1)):\n-    n_imgs, img_h, img_w = imgs.shape\n-    if n_imgs != grid[0] * grid[1]:\n-        raise ValueError()\n-    combined = np.zeros((grid[0] * img_h, grid[1] * img_w))\n-    for i in range(grid[0]):\n-        for j in range(grid[1]):\n-            combined[img_h*i:img_h*(i+1),img_w*j:img_w*(j+1)] = imgs[grid[0] * i + j]\n-    return combined\n-\n-class DrawActivations(Callback):\n-    def __init__(self, figsize):\n-        self.fig = plt.figure(figsize=figsize)\n-\n-    def on_train_begin(self, logs={}):\n-        self.imgs = Frames(n_plots=5)\n-\n-        layers_0_ids = np.random.choice(32, 16, replace=False)\n-        self.test_layer0 = theano.function([self.model.get_input()], self.model.layers[1].get_output(train=False)[0, layers_0_ids])\n-\n-        layers_1_ids = np.random.choice(64, 36, replace=False)\n-        self.test_layer1 = theano.function([self.model.get_input()], self.model.layers[5].get_output(train=False)[0, layers_1_ids])\n-\n-        self.test_layer2 = theano.function([self.model.get_input()], self.model.layers[10].get_output(train=False)[0])\n-\n-    def on_epoch_begin(self, epoch, logs={}):\n-        self.epoch = epoch\n-\n-    def on_batch_end(self, batch, logs={}):\n-        if batch % 5 == 0:\n-            self.imgs.add_frame(0, X_test[0,0])\n-            self.imgs.add_frame(1, combine_imgs(self.test_layer0(X_test), grid=(4, 4)))\n-            self.imgs.add_frame(2, combine_imgs(self.test_layer1(X_test), grid=(6, 6)))\n-            self.imgs.add_frame(3, self.test_layer2(X_test).reshape((16,16)))\n-            self.imgs.add_frame(4, self.model._predict(X_test)[0].reshape((1,10)))\n-            self.imgs.set_title('Epoch #%d - Batch #%d' % (self.epoch, batch))\n-\n-    def on_train_end(self, logs={}):\n-        anim = SubplotTimedAnimation(self.fig, self.imgs, grid=(1,5), interval=10, blit=False, repeat_delay=1000)\n-        # anim.save('test_gif.gif', fps=15, writer='imagemagick')\n-        plt.show()\n-\n-# model = Sequential()\n-# model.add(Dense(784, 50))\n-# model.add(Activation('relu'))\n-# model.add(Dense(50, 10))\n-# model.add(Activation('softmax'))\n-\n-model = Sequential()\n-model.add(Convolution2D(32, 1, 3, 3, border_mode='full'))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-\n-model.add(Convolution2D(64, 32, 3, 3, border_mode='full'))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-\n-model.add(Flatten())\n-model.add(Dense(64*8*8, 256))\n-model.add(Activation('relu'))\n-model.add(Dropout(0.5))\n-\n-model.add(Dense(256, 10, W_regularizer = l2(0.1)))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-# Fit the model\n-draw_weights = DrawActivations(figsize=(5.4, 1.35))\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, callbacks=[draw_weights])\n-\n-\n-##########################\n-# model checkpoint tests #\n-##########################\n-\n-print('Running ModelCheckpoint test')\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 20\n-\n-# small sample size to overfit on training data\n-max_train_samples = 50\n-max_test_samples = 1000\n-\n-np.random.seed(1337) # for reproducibility\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(60000,784)[:max_train_samples]\n-X_test = X_test.reshape(10000,784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-\n-# Create a slightly larger network than required to test best validation save only\n-model = Sequential()\n-model.add(Dense(784, 500))\n-model.add(Activation('relu'))\n-model.add(Dense(500, 10))\n-model.add(Activation('softmax'))\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-# test file location\n-path = \"/tmp\"\n-filename = \"model_weights.hdf5\"\n-import os\n-f = os.path.join(path, filename)\n-\n-print(\"Test model checkpointer\")\n-# only store best validation model in checkpointer\n-checkpointer = cbks.ModelCheckpoint(filepath=f, verbose=1, save_best_only=True)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), callbacks =[checkpointer])\n-\n-if not os.path.isfile(f):\n-    raise Exception(\"Model weights were not saved to %s\" % (f))\n-\n-print(\"Test model checkpointer without validation data\")\n-import warnings\n-warnings.filterwarnings('error')\n-try:\n-    passed = False\n-    # this should issue a warning\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, callbacks =[checkpointer])\n-except:\n-    passed = True\n-if not passed:\n-    raise Exception(\"Modelcheckpoint tests did not pass\")\n-\n-print(\"Test model checkpointer with pattern\")\n-filename = \"model_weights.{epoch:04d}.hdf5\"\n-f = os.path.join(path, filename)\n-nb_epoch = 3\n-checkpointer = cbks.ModelCheckpoint(f)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, callbacks=[checkpointer])\n-for i in range(nb_epoch):\n-    if not os.path.isfile(f.format(epoch=i)):\n-        raise Exception(\"Model weights were not saved separately for each epoch\")\n-\n-print(\"Tests passed\")\n\n@@ -1,100 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import keras\n-from keras.datasets import mnist\n-import keras.models\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.regularizers import l2, l1\n-from keras.constraints import maxnorm, nonneg\n-from keras.optimizers import SGD, Adam, RMSprop\n-from keras.utils import np_utils, generic_utils\n-import theano\n-import theano.tensor as T\n-import numpy as np\n-import scipy\n-\n-batch_size = 100\n-nb_classes = 10\n-nb_epoch = 10\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-X_train=X_train.reshape(60000,784)\n-X_test=X_test.reshape(10000,784)\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n-\n-model = Sequential()\n-model.add(Dense(784, 20, W_constraint=maxnorm(1)))\n-model.add(Activation('relu'))\n-model.add(Dropout(0.1))\n-model.add(Dense(20, 20, W_constraint=nonneg()))\n-model.add(Activation('relu'))\n-model.add(Dropout(0.1))\n-model.add(Dense(20, 10, W_constraint=maxnorm(1)))\n-model.add(Activation('softmax'))\n-\n-\n-rms = RMSprop()\n-model.compile(loss='categorical_crossentropy', optimizer=rms)\n-\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0)\n-\n-a=model.params[0].eval()\n-if np.isclose(np.max(np.sqrt(np.sum(a**2, axis=0))),1):\n-\tprint('Maxnorm test passed')\n-else:\n-\traise ValueError('Maxnorm test failed!')\n-\t\t\n-b=model.params[2].eval()\n-if np.min(b)==0 and np.min(a)!=0:\n-\tprint('Nonneg test passed')\n-else:\n-\traise ValueError('Nonneg test failed!')\n-\t\n-\n-model = Sequential()\n-model.add(Dense(784, 20))\n-model.add(Activation('relu'))\n-model.add(Dense(20, 20, W_regularizer=l1(.01)))\n-model.add(Activation('relu'))\n-model.add(Dense(20, 10))\n-model.add(Activation('softmax'))\n-\n-\n-rms = RMSprop()\n-model.compile(loss='categorical_crossentropy', optimizer=rms)\n-\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, show_accuracy=True, verbose=0)\n-\n-a=model.params[2].eval().reshape(400)\n-(D, p1) = scipy.stats.kurtosistest(a)\n-\n-model = Sequential()\n-model.add(Dense(784, 20))\n-model.add(Activation('relu'))\n-model.add(Dense(20, 20, W_regularizer=l2(.01)))\n-model.add(Activation('relu'))\n-model.add(Dense(20, 10))\n-model.add(Activation('softmax'))\n-\n-\n-rms = RMSprop()\n-model.compile(loss='categorical_crossentropy', optimizer=rms)\n-\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=20, show_accuracy=True, verbose=0)\n-\n-a=model.params[2].eval().reshape(400)\n-(D, p2) = scipy.stats.kurtosistest(a)\n-\n-if p1<.01 and p2>.01:\n-\tprint('L1 and L2 regularization tests passed')\n-else:\n-\traise ValueError('L1 and L2 regularization tests failed!')\n\\ No newline at end of file\n\n@@ -1,130 +0,0 @@\n-# Dummy test data as input to RNN. This input is 3 timesteps long where the third timestep always matches the\n-# first. Without masking it should be able to learn it, with masking it should fail.\n-\n-import numpy as np\n-from keras.utils.theano_utils import sharedX\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge, Dropout, TimeDistributedDense\n-from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN, LSTM, GRU\n-import theano\n-\n-theano.config.exception_verbosity = 'high' \n-\n-# (nb_samples, timesteps, dimensions)\n-X = np.random.random_integers(1, 4, size=(500000, 15))\n-\n-print(\"About to compile the first model\")\n-model = Sequential()\n-model.add(Embedding(5, 4, mask_zero=True))\n-model.add(TimeDistributedDense(4, 4)) # obviously this is redundant. Just testing.\n-model.add(SimpleRNN(4, 4, activation='relu', return_sequences=True))\n-model.add(Dropout(0.5))\n-model.add(SimpleDeepRNN(4, 4, depth=2, activation='relu')) \n-model.add(Dropout(0.5))\n-model.add(Dense(4, 4, activation='softmax'))\n-model.compile(loss='categorical_crossentropy',\n-        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n-print(\"Compiled model\")\n-\n-W = model.get_weights() # We'll save these so we can reset it later\n-\n-X[:, : 10] = 0\n-Xmask0 = X.copy()\n-Xmask0[:, 10] = 0\n-\n-Xmask12 = X.copy()\n-Xmask12[:, 11] = 0\n-Xmask12[:, 12] = 0\n-\n-X0_onehot = np.zeros((X.shape[0], 4))\n-X1_onehot = np.zeros((X.shape[0], 4))\n-for i, row in enumerate(X):\n-    X0_onehot[i, row[10] - 1] = 1\n-    X1_onehot[i, row[11] - 1] = 1\n-\n-# Uniform score: 4 options = ln(4) nats (2 bits)\n-# we should not do better than this when we mask out the part of the input\n-# that gives us the correct answer\n-uniform_score = np.log(4)\n-batch_size=512\n-\n-# Train it to guess 0th dim\n-model.fit(X, X0_onehot, nb_epoch=1, batch_size=batch_size)\n-score = model.evaluate(X, X0_onehot, batch_size=batch_size)\n-if score > uniform_score * 0.9:\n-    raise Exception('Failed to learn to copy timestep 0, score %f' % score)\n-    \n-\n-model.set_weights(W)\n-\n-# Train without showing it the 0th dim to learn 1st dim\n-model.fit(X[: , 1:], X1_onehot, nb_epoch=1, batch_size=batch_size)\n-score = model.evaluate(X[:, 1:], X1_onehot, batch_size=batch_size)\n-if score > uniform_score * 0.9:\n-    raise Exception('Failed to learn to copy timestep 1, score %f' % score)\n-\n-model.set_weights(W)\n-\n-# Train to guess 0th dim when 0th dim has been masked (should fail)\n-model.fit(Xmask0, X0_onehot, nb_epoch=1, batch_size=batch_size)\n-score = model.evaluate(Xmask0, X0_onehot, batch_size=batch_size)\n-if score < uniform_score * 0.9:\n-   raise Exception('Somehow learned to copy timestep 0 despite mask, score %f' % score)\n-\n-model.set_weights(W)\n-\n-# Train to guess 1st dim when 0th dim has been masked (should succeed)\n-model.fit(Xmask0, X1_onehot, nb_epoch=1, batch_size=batch_size)\n-score = model.evaluate(Xmask0, X1_onehot, batch_size=batch_size)\n-if score > uniform_score * 0.9:\n-    raise Exception('Failed to learn to copy timestep 1 in masked model, score %f' % score)\n-\n-model.set_weights(W)\n-\n-# Finally, make sure the mask is actually blocking input, mask out timesteps 1 and 2, and see if\n-# it can learn timestep 0 (should fail)\n-model.fit(Xmask12, X0_onehot, nb_epoch=1, batch_size=batch_size)\n-\n-score = model.evaluate(Xmask12, X0_onehot, batch_size=batch_size)\n-if score < uniform_score * 0.9:\n-    raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n-\n-# Another testing approach, just initialize models and make sure that prepending zeros doesn't affect\n-# their output\n-print(\"About to compile the second model\")\n-model2 = Sequential()\n-model2.add(Embedding(5, 4, mask_zero=True))\n-model2.add(TimeDistributedDense(4, 4))\n-model2.add(Activation('time_distributed_softmax'))\n-model2.add(LSTM(4, 4, return_sequences=True))\n-model2.add(Activation('tanh'))\n-model2.add(GRU(4, 4, activation='softmax', return_sequences=True))\n-model2.add(SimpleDeepRNN(4, 4, depth=2, activation='relu', return_sequences=True)) \n-model2.add(SimpleRNN(4, 4, activation='relu', return_sequences=True))\n-model2.compile(loss='categorical_crossentropy',\n-        optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n-print(\"Compiled model2\")\n-\n-X2 = np.random.random_integers(1, 4, size=(2, 5))\n-y2 = np.random.random((X2.shape[0], X2.shape[1], 4))\n-\n-ref = model2.predict(X2)\n-ref_eval = model2.evaluate(X2, y2)\n-mask = np.ones((y2.shape[0], y2.shape[1], 1))\n-\n-for pre_zeros in range(1, 10):\n-    padded_X2 = np.concatenate((np.zeros((X2.shape[0], pre_zeros)), X2), axis=1)\n-    padded_mask = np.concatenate((np.zeros((mask.shape[0], pre_zeros, mask.shape[2])), mask), axis=1)\n-    padded_y2 = np.concatenate((np.zeros((y2.shape[0], pre_zeros, y2.shape[2])), y2), axis=1)\n-\n-    pred = model2.predict(padded_X2)\n-    if not np.allclose(ref[:, -1, :], pred[:, -1, :]):\n-        raise Exception(\"Different result after left-padding %d zeros. Ref: %s, Pred: %s\" % (pre_zeros, ref, pred))\n-\n-    pad_eval = model2.evaluate(padded_X2, padded_y2, weights=padded_mask)\n-    if not np.allclose([pad_eval], [ref_eval]):\n-        raise Exception(\"Got dissimilar categorical_crossentropy after left-padding %d zeros. Ref: %f, Pred %f\" %\\\n-                (pref_eval, pred_val))\n-\n-        \n\n@@ -1,44 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.models import Sequential, Graph\n-from keras.layers.core import Layer, Activation, Dense, Flatten, Reshape, Merge\n-from keras.layers.convolutional import Convolution2D, MaxPooling2D\n-import keras.utils.layer_utils as layer_utils\n-\n-print('-- Sequential model')\n-left = Sequential()\n-left.add(Convolution2D(32, 1, 3, 3, border_mode='valid'))\n-left.add(MaxPooling2D(pool_size=(2, 2)))\n-left.add(Flatten())\n-left.add(Dense(32 * 13 * 13, 50))\n-left.add(Activation('relu'))\n-\n-right = Sequential()\n-right.add(Dense(784, 30))\n-right.add(Activation('relu'))\n-\n-model = Sequential()\n-model.add(Merge([left, right], mode='concat'))\n-\n-model.add(Dense(80, 10))\n-model.add(Activation('softmax'))\n-\n-layer_utils.print_layer_shapes(model, [(1, 1, 28, 28), (1, 784)])\n-\n-print('-- Graph model')\n-graph = Graph()\n-graph.add_input(name='input1', ndim=2)\n-graph.add_input(name='input2', ndim=4)\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-\n-graph.add_node(Convolution2D(32, 1, 3, 3), name='conv1', input='input2')\n-graph.add_node(Flatten(), name='flatten1', input='conv1')\n-graph.add_node(Dense(32 * 13 * 13, 10), name='dense4', input='flatten1')\n-\n-graph.add_output(name='output1', inputs=['dense1', 'dense3'], merge_mode='sum')\n-graph.add_output(name='output2', inputs=['dense1', 'dense4'], merge_mode='concat')\n-\n-layer_utils.print_layer_shapes(graph, {'input1': (1, 32), 'input2': (1, 1, 28, 28)})\n-\n-print('Test script complete')\n\n@@ -1,40 +0,0 @@\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Dropout, Activation\n-from keras.optimizers import SGD\n-\n-import sys\n-sys.setrecursionlimit(10000) # to be able to pickle Theano compiled functions\n-\n-import pickle, numpy\n-\n-def create_model():\n-    model = Sequential()\n-    model.add(Dense(256, 2048, init='uniform', activation='relu'))\n-    model.add(Dropout(0.5))\n-    model.add(Dense(2048, 2048, init='uniform', activation='relu'))\n-    model.add(Dropout(0.5))\n-    model.add(Dense(2048, 2048, init='uniform', activation='relu'))\n-    model.add(Dropout(0.5))\n-    model.add(Dense(2048, 2048, init='uniform', activation='relu'))\n-    model.add(Dropout(0.5))\n-    model.add(Dense(2048, 256, init='uniform', activation='linear'))\n-    return model\n-\n-model = create_model()\n-sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n-model.compile(loss='mse', optimizer=sgd)\n-\n-pickle.dump(model, open('/tmp/model.pkl', 'wb'))\n-model.save_weights('/tmp/model_weights.hdf5')\n-\n-model_loaded = create_model()\n-model_loaded.load_weights('/tmp/model_weights.hdf5')\n-\n-for k in range(len(model.layers)):\n-    weights_orig = model.layers[k].get_weights()\n-    weights_loaded = model_loaded.layers[k].get_weights()\n-    for x, y in zip(weights_orig, weights_loaded):\n-        if numpy.any(x != y):\n-            raise ValueError('Loaded weights are different from pickled weights!')\n-\n-\n\n@@ -1,126 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n-from keras.utils import np_utils\n-from keras.wrappers.scikit_learn import *\n-import numpy as np\n-\n-batch_size = 128\n-nb_epoch = 1\n-\n-nb_classes = 10\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-np.random.seed(1337) # for reproducibility\n-\n-############################################\n-# scikit-learn classification wrapper test #\n-############################################\n-print('Beginning scikit-learn classification wrapper test')\n-\n-print('Loading data')\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(60000, 784)[:max_train_samples]\n-X_test = X_test.reshape(10000, 784)[:max_test_samples]\n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n-\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-print('Defining model')\n-model = Sequential()\n-model.add(Dense(784, 50))\n-model.add(Activation('relu'))\n-model.add(Dense(50, 10))\n-model.add(Activation('softmax'))\n-\n-print('Creating wrapper')\n-classifier = KerasClassifier(model, train_batch_size=batch_size, nb_epoch=nb_epoch)\n-\n-print('Fitting model')\n-classifier.fit(X_train, Y_train)\n-\n-print('Testing score function')\n-score = classifier.score(X_train, Y_train)\n-print('Score: ', score)\n-\n-print('Testing predict function')\n-preds = classifier.predict(X_test)\n-print('Preds.shape: ', preds.shape)\n-\n-print('Testing predict proba function')\n-proba = classifier.predict_proba(X_test)\n-print('Proba.shape: ', proba.shape)\n-\n-print('Testing get params')\n-print(classifier.get_params())\n-\n-print('Testing set params')\n-classifier.set_params(optimizer='sgd', loss='binary_crossentropy')\n-print(classifier.get_params())\n-\n-print('Testing attributes')\n-print('Classes')\n-print(classifier.classes_)\n-print('Config')\n-print(classifier.config_)\n-print('Weights')\n-print(classifier.weights_)\n-print('Compiled model')\n-print(classifier.compiled_model_)\n-\n-########################################\n-# scikit-learn regression wrapper test #\n-########################################\n-print('Beginning scikit-learn regression wrapper test')\n-\n-print('Generating data')\n-X_train = np.random.random((5000, 100))\n-X_test = np.random.random((1000, 100))\n-y_train = np.random.random(5000)\n-y_test = np.random.random(1000)\n-\n-print('Defining model')\n-model = Sequential()\n-model.add(Dense(100, 50))\n-model.add(Activation('relu'))\n-model.add(Dense(50, 1))\n-model.add(Activation('linear'))\n-\n-print('Creating wrapper')\n-regressor = KerasRegressor(model, train_batch_size=batch_size, nb_epoch=nb_epoch)\n-\n-print('Fitting model')\n-regressor.fit(X_train, y_train)\n-\n-print('Testing score function')\n-score = regressor.score(X_train, y_train)\n-print('Score: ', score)\n-\n-print('Testing predict function')\n-preds = regressor.predict(X_test)\n-print('Preds.shape: ', preds.shape)\n-\n-print('Testing get params')\n-print(regressor.get_params())\n-\n-print('Testing set params')\n-regressor.set_params(optimizer='sgd', loss='mean_absolute_error')\n-print(regressor.get_params())\n-\n-print('Testing attributes')\n-print('Config')\n-print(regressor.config_)\n-print('Weights')\n-print(regressor.weights_)\n-print('Compiled model')\n-print(regressor.compiled_model_)\n-\n-print('Test script complete.')\n\n@@ -1,101 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import numpy as np\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.preprocessing import sequence\n-from keras.optimizers import SGD, RMSprop, Adagrad\n-from keras.utils import np_utils\n-from keras.models import Sequential, Graph\n-from keras.layers.core import Dense, Dropout, Activation, Merge\n-from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import LSTM, GRU\n-from keras.datasets import imdb\n-from keras.models import model_from_yaml\n-\n-'''\n-This is essentially the IMDB test. Deserialized models should yield\n-the same config as the original one.\n-'''\n-\n-max_features = 10000\n-maxlen = 100\n-batch_size = 32\n-\n-(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n-\n-X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n-X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n-\n-model = Sequential()\n-model.add(Embedding(max_features, 128))\n-model.add(LSTM(128, 128))\n-model.add(Dropout(0.5))\n-model.add(Dense(128, 1, W_regularizer='identity', b_constraint='maxnorm'))\n-model.add(Activation('sigmoid'))\n-\n-model.get_config(verbose=1)\n-\n-#####################################\n-# save model w/o parameters to yaml #\n-#####################################\n-\n-yaml_no_params = model.to_yaml()\n-\n-no_param_model = model_from_yaml(yaml_no_params)\n-no_param_model.get_config(verbose=1)\n-\n-######################################\n-# save multi-branch sequential model #\n-######################################\n-\n-seq = Sequential()\n-seq.add(Merge([model, model], mode='sum'))\n-seq.get_config(verbose=1)\n-merge_yaml = seq.to_yaml()\n-merge_model = model_from_yaml(merge_yaml)\n-\n-large_model = Sequential()\n-large_model.add(Merge([seq,model], mode='concat'))\n-large_model.get_config(verbose=1)\n-large_model.to_yaml()\n-\n-####################\n-# save graph model #\n-####################\n-\n-X = np.random.random((100, 32))\n-X2 = np.random.random((100, 32))\n-y = np.random.random((100, 4))\n-y2 = np.random.random((100,))\n-\n-(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n-                                                     classification=False, output_shape=(4,))\n-\n-graph = Graph()\n-\n-graph.add_input(name='input1', ndim=2)\n-\n-graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n-\n-graph.add_output(name='output1', inputs=['dense2', 'dense3'], merge_mode='sum')\n-graph.compile('rmsprop', {'output1': 'mse'})\n-\n-graph.get_config(verbose=1)\n-\n-history = graph.fit({'input1': X_train, 'output1': y_train}, nb_epoch=10)\n-original_pred = graph.predict({'input1': X_test})\n-\n-graph_yaml = graph.to_yaml()\n-graph.save_weights('temp.h5', overwrite=True)\n-\n-reloaded_graph = model_from_yaml(graph_yaml)\n-reloaded_graph.load_weights('temp.h5')\n-reloaded_graph.get_config(verbose=1)\n-\n-reloaded_graph.compile('rmsprop', {'output1': 'mse'})\n-new_pred = reloaded_graph.predict({'input1': X_test})\n-\n-assert(np.sum(new_pred['output1'] - original_pred['output1']) == 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
