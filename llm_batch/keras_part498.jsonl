{"custom_id": "keras#3e6b24597fcc0a83b3de067b55caee2a636c8bf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 29 | Files Changed: 3 | Hunks: 21 | Methods Changed: 13 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 108 | Churn Cumulative: 4346 | Contributors (this commit): 14 | Commits (past 90d): 14 | Contributors (cumulative): 28 | DMM Complexity: 0.7380952380952381\n\nDIFF:\n@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"The V2 implementation of Normalization layers.\"\"\"\n \n+import warnings\n+\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n@@ -112,7 +114,8 @@ class BatchNormalizationBase(Layer):\n         the faster implementation if possible. If False, do not used the fused\n         implementation. Note that in TensorFlow 1.x, the meaning of\n         `fused=True` is different: if `False`, the layer uses the\n-        system-recommended implementation.\n+        system-recommended implementation. You cannot use `fused=True` if a\n+        mask is passed in the `call()` method.\n       trainable: Boolean, if `True` the variables will be marked as trainable.\n       virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n         which means batch normalization is performed across the whole batch.\n@@ -146,6 +149,8 @@ class BatchNormalizationBase(Layer):\n           and variance of the current batch of inputs.\n         - `training=False`: The layer will normalize its inputs using the mean\n           and variance of its moving statistics, learned during training.\n+      mask: Binary tensor of shape broadcastable to `inputs` tensor, indicating\n+        the positions for which the mean and variance should be computed.\n \n     Input shape: Arbitrary. Use the keyword argument `input_shape` (tuple of\n       integers, does not include the samples axis) when using this layer as the\n@@ -586,8 +591,17 @@ class BatchNormalizationBase(Layer):\n                 with tf.compat.v1.colocate_with(variable):\n                     return tf.compat.v1.assign(variable, value, name=scope)\n \n-    def _fused_batch_norm(self, inputs, training):\n+    def _fused_batch_norm(self, inputs, mask, training):\n         \"\"\"Returns the output of fused batch norm.\"\"\"\n+        if mask is not None:\n+            warnings.warn(\n+                \"Masking is not supported with `fused=True`. \"\n+                \"You should either turn off fusing \"\n+                \"(`fused=False`) or you should not pass a `mask` \"\n+                \"argument when calling the layer. \"\n+                \"For the moment `mask` will be ignored for the \"\n+                \"normalization.\"\n+            )\n         if self.center:\n             beta = self.beta\n         else:\n@@ -802,16 +816,32 @@ class BatchNormalizationBase(Layer):\n \n         return (r, d, out_mean, out_variance)\n \n-    def _calculate_mean_and_var(self, inputs, reduction_axes, keep_dims):\n+    def _calculate_mean_and_var(\n+        self, inputs, reduction_axes, keep_dims, mask=None\n+    ):\n         if self.synchronized:\n             return self._sync_calculate_mean_and_var(\n-                inputs, reduction_axes, keep_dims\n+                inputs, reduction_axes, keep_dims, mask=mask\n             )\n+        if mask is None:\n             return tf.nn.moments(inputs, reduction_axes, keepdims=keep_dims)\n+        else:\n+            mask_weights = tf.cast(\n+                mask, self.compute_dtype, name=\"mask_weights\"\n+            )\n+            mask_weights = tf.expand_dims(\n+                mask_weights, axis=-1, name=\"mask_weights_broadcasted\"\n+            )\n+            return tf.nn.weighted_moments(\n+                inputs,\n+                axes=reduction_axes,\n+                frequency_weights=mask_weights,\n+                keepdims=keep_dims,\n+            )\n \n-    def _moments(self, inputs, reduction_axes, keep_dims):\n+    def _moments(self, inputs, reduction_axes, keep_dims, mask=None):\n         mean, variance = self._calculate_mean_and_var(\n-            inputs, reduction_axes, keep_dims\n+            inputs, reduction_axes, keep_dims, mask=mask\n         )\n         # TODO(b/129279393): Support zero batch input in non\n         # DistributionStrategy code as well.\n@@ -837,7 +867,7 @@ class BatchNormalizationBase(Layer):\n                 training = False\n         return training\n \n-    def call(self, inputs, training=None):\n+    def call(self, inputs, mask=None, training=None):\n         inputs = tf.cast(inputs, self.compute_dtype)\n         training = self._get_training_value(training)\n         # Determine a boolean value for `training`: could be True, False, or\n@@ -882,7 +912,9 @@ class BatchNormalizationBase(Layer):\n                 return outputs\n \n         if self.fused:\n-            outputs = self._fused_batch_norm(inputs, training=training)\n+            outputs = self._fused_batch_norm(\n+                inputs, mask=mask, training=training\n+            )\n             if self.virtual_batch_size is not None:\n                 # Currently never reaches here since fused_batch_norm does not\n                 # support virtual batching\n@@ -954,6 +986,7 @@ class BatchNormalizationBase(Layer):\n                 tf.cast(inputs, self._param_dtype),\n                 reduction_axes,\n                 keep_dims=keep_dims,\n+                mask=mask,\n             )\n \n             moving_mean = self.moving_mean\n@@ -1118,7 +1151,7 @@ class BatchNormalizationBase(Layer):\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n-    def _sync_calculate_mean_and_var(self, x, axes, keep_dims):\n+    def _sync_calculate_mean_and_var(self, x, axes, keep_dims, mask=None):\n         with backend.name_scope(\"moments\"):\n             # The dynamic range of fp16 is too limited to support the collection\n             # of sufficient statistics. As a workaround we simply perform the\n@@ -1126,11 +1159,24 @@ class BatchNormalizationBase(Layer):\n             # variance back to fp16\n             y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n             replica_ctx = tf.distribute.get_replica_context()\n-            if replica_ctx:\n+\n+            if not replica_ctx:\n+                return super()._calculate_mean_and_var(\n+                    x, axes, keep_dims, mask=mask\n+                )\n+\n+            if mask is not None:\n+                mask_weights = tf.cast(mask, tf.float32, name=\"mask_weights\")\n+                mask_weights = tf.expand_dims(\n+                    mask_weights, axis=-1, name=\"mask_weights_broadcasted\"\n+                )\n+                y *= mask_weights\n+\n             local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n             local_squared_sum = tf.reduce_sum(\n                 tf.square(y), axis=axes, keepdims=True\n             )\n+\n             batch_size = tf.cast(tf.shape(y)[axes[0]], tf.float32)\n             # TODO(b/163099951): batch the all-reduces once we sort out the\n             # ordering issue for NCCL. We don't have a mechanism to launch\n@@ -1146,9 +1192,7 @@ class BatchNormalizationBase(Layer):\n                 tf.distribute.ReduceOp.SUM, batch_size\n             )\n \n-                axes_vals = [\n-                    (tf.shape(y))[axes[i]] for i in range(1, len(axes))\n-                ]\n+            axes_vals = [(tf.shape(y))[axes[i]] for i in range(1, len(axes))]\n             multiplier = tf.cast(tf.reduce_prod(axes_vals), tf.float32)\n             multiplier = multiplier * global_batch_size\n \n@@ -1156,20 +1200,6 @@ class BatchNormalizationBase(Layer):\n             y_squared_mean = y_squared_sum / multiplier\n             # var = E(x^2) - E(x)^2\n             variance = y_squared_mean - tf.square(mean)\n-            else:\n-                # Compute true mean while keeping the dims for proper\n-                # broadcasting.\n-                mean = tf.reduce_mean(y, axes, keepdims=True, name=\"mean\")\n-                # sample variance, not unbiased variance\n-                # Note: stop_gradient does not change the gradient that gets\n-                # backpropagated to the mean from the variance calculation,\n-                # because that gradient is zero\n-                variance = tf.reduce_mean(\n-                    tf.math.squared_difference(y, tf.stop_gradient(mean)),\n-                    axes,\n-                    keepdims=True,\n-                    name=\"variance\",\n-                )\n             if not keep_dims:\n                 mean = tf.squeeze(mean, axes)\n                 variance = tf.squeeze(variance, axes)\n\n@@ -247,6 +247,26 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         train_loss = model.train_on_batch(test_data, test_targets)\n         self.assertAlmostEqual(test_loss, train_loss)\n \n+    @test_combinations.run_all_keras_modes\n+    def test_batchnorm_ignore_masked_values(self):\n+        padded_data = np.array(\n+            [[[1, 5], [2, 5], [0, 0], [0, 0]] for _ in range(10)],\n+            dtype=\"float32\",\n+        )  # Pad value of 0\n+\n+        inputs = keras.layers.Input((None, 2))\n+        masked = keras.layers.Masking()(inputs)\n+        normed = keras.layers.BatchNormalization(momentum=0.0)(masked)\n+        model = keras.models.Model(inputs, normed)\n+        model.compile(\n+            \"rmsprop\", \"mse\", run_eagerly=test_utils.should_run_eagerly()\n+        )\n+\n+        model.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\n+\n+        self.assertAllEqual(model.layers[2].moving_mean, [1.5, 5.0])\n+        self.assertAllEqual(model.layers[2].moving_variance, [0.25, 0.0])\n+\n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_eager_batchnorm_in_custom_model_call_with_tf_function(self):\n         class MyModel(keras.Model):\n\n@@ -233,8 +233,8 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n             **kwargs\n         )\n \n-    def call(self, inputs, training=False):\n-        return super().call(inputs, training=training)\n+    def call(self, inputs, mask=None, training=False):\n+        return super().call(inputs, mask=mask, training=training)\n \n \n @keras_export(v1=[\"keras.__internal__.legacy.layers.batch_normalization\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31a85b25a3371ae4efe8241a0d053789520e364c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 6221 | Contributors (this commit): 31 | Commits (past 90d): 13 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,11 @@ from keras import backend\n from keras.engine import base_layer\n from keras.utils import control_flow_util\n \n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n \n+\n+@keras_export(\"keras.__internal__.layers.BaseDenseAttention\", v1=[])\n class BaseDenseAttention(base_layer.BaseRandomLayer):\n     \"\"\"Base Attention class for Dense networks.\n \n\n@@ -222,6 +222,7 @@ class Loss:\n         return self.reduction\n \n \n+@keras_export(\"keras.__internal__.losses.LossFunctionWrapper\", v1=[])\n class LossFunctionWrapper(Loss):\n     \"\"\"Wraps a loss function in the `Loss` class.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f59b1b7c2a2d352ec2de14fd52ac425959bad79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 2746 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -823,6 +823,13 @@ class BatchNormalizationBase(Layer):\n             return self._sync_calculate_mean_and_var(\n                 inputs, reduction_axes, keep_dims, mask=mask\n             )\n+        return self._no_sync_calculate_mean_and_var(\n+            inputs, reduction_axes, keep_dims, mask=mask\n+        )\n+\n+    def _no_sync_calculate_mean_and_var(\n+        self, inputs, reduction_axes, keep_dims, mask=None\n+    ):\n         if mask is None:\n             return tf.nn.moments(inputs, reduction_axes, keepdims=keep_dims)\n         else:\n@@ -1161,7 +1168,7 @@ class BatchNormalizationBase(Layer):\n             replica_ctx = tf.distribute.get_replica_context()\n \n             if not replica_ctx:\n-                return super()._calculate_mean_and_var(\n+                return self._no_sync_calculate_mean_and_var(\n                     x, axes, keep_dims, mask=mask\n                 )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5ef06f96f1612eb8ded3d00967d26cb29c953d45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3443 | Contributors (this commit): 14 | Commits (past 90d): 10 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -874,7 +874,7 @@ class BatchNormalizationBase(Layer):\n                 training = False\n         return training\n \n-    def call(self, inputs, mask=None, training=None):\n+    def call(self, inputs, training=None, mask=None):\n         inputs = tf.cast(inputs, self.compute_dtype)\n         training = self._get_training_value(training)\n         # Determine a boolean value for `training`: could be True, False, or\n\n@@ -233,8 +233,8 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n             **kwargs\n         )\n \n-    def call(self, inputs, mask=None, training=False):\n-        return super().call(inputs, mask=mask, training=training)\n+    def call(self, inputs, training=False, mask=None):\n+        return super().call(inputs, training=training, mask=mask)\n \n \n @keras_export(v1=[\"keras.__internal__.legacy.layers.batch_normalization\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8467f50815a1773be1a14ce89716c3d12c500f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 26 | Churn Cumulative: 148 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,6 +17,9 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import glob\n+import os\n+\n import tensorflow.compat.v2 as tf\n \n ds_combinations = tf.__internal__.distribute.combinations\n@@ -73,22 +76,41 @@ class DistributedTrainingTest(tf.test.TestCase):\n         with strategy.scope():\n             model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n             optimizer = tf.keras.optimizers.SGD()\n-            model.compile(optimizer, loss=\"mse\", steps_per_execution=10)\n+            model.compile(optimizer, loss=\"mse\", steps_per_execution=5)\n \n         x = tf.keras.utils.experimental.DatasetCreator(dataset_fn)\n \n+        logdir = os.path.join(self.get_temp_dir(), \"logdir\")\n         model.fit(\n             x,\n             epochs=2,\n-            steps_per_epoch=10,\n+            steps_per_epoch=20,\n             callbacks=[\n                 tf.keras.callbacks.TensorBoard(\n+                    logdir,\n                     update_freq=5,\n                     write_steps_per_second=True,\n                 )\n             ],\n         )\n \n+        events = []\n+        for event_file in glob.glob(logdir + \"/train/events.out.*\"):\n+            for event in tf.compat.v1.train.summary_iterator(event_file):\n+                if not event.summary:\n+                    continue\n+                for value in event.summary.value:\n+                    if value.tag != \"batch_loss\":\n+                        continue\n+                    events += [event.step]\n+        events.sort()\n+\n+        if not isinstance(\n+            strategy, tf.distribute.experimental.ParameterServerStrategy\n+        ):\n+            # total steps = epochs * steps_per_epoch\n+            self.assertEqual(events, [5, 10, 15, 20, 25, 30, 35, 40])\n+\n \n if __name__ == \"__main__\":\n     tf.__internal__.distribute.multi_process_runner.test_main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#902bd2ab98d32ef33c60ec9d5eab1e2383bc193f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 11 | Complexity Δ (Sum/Max): 13/10 | Churn Δ: 104 | Churn Cumulative: 29446 | Contributors (this commit): 143 | Commits (past 90d): 40 | Contributors (cumulative): 160 | DMM Complexity: 0.6842105263157895\n\nDIFF:\n@@ -47,6 +47,7 @@ from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n+from keras.utils import tf_inspect\n from keras.utils import tf_utils\n from keras.utils import traceback_utils\n from keras.utils import version_utils\n@@ -2972,6 +2973,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         }\n         return model_config\n \n+    @generic_utils.default\n     def get_config(self):\n         \"\"\"Returns the config of the `Model`.\n \n@@ -2987,18 +2989,33 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Developers of subclassed `Model` are advised to override this method,\n         and continue to update the dict from `super(MyModel, self).get_config()`\n         to provide the proper configuration of this `Model`. The default config\n-        is an empty dict. Optionally, raise `NotImplementedError` to allow Keras\n-        to attempt a default serialization.\n+        will return config dict for init parameters if they are basic types.\n+        Raises `NotImplementedError` when in cases where a custom\n+        `get_config()` implementation is required for the subclassed model.\n \n         Returns:\n             Python dictionary containing the configuration of this `Model`.\n         \"\"\"\n-        # Return an empty dict here because otherwise Model\n-        # subclass developers may see\n-        # their model's `__init__()` fed with unexpected keyword arguments,\n-        # if their `__init__()` takes no argument for example, and they\n-        # don't override `from_config()`, which would use `cls(**config)`\n-        # as a result.\n+        # If sublcass doesn't implement `get_config()` parse from init args\n+        # otherwise default to empty dict\n+        if generic_utils.is_default(self.get_config):\n+            try:\n+                config = super().get_config()\n+            except NotImplementedError:\n+                config = {}\n+                logging.warning(\n+                    \"Model's `__init__()` arguments contain non-serializable \"\n+                    \"objects. Please implement a `get_config()` method in the \"\n+                    \"subclassed Model for proper saving and loading. \"\n+                    \"Defaulting to empty config.\"\n+                )\n+            # `super.get_config` adds additional keys, keep them if they\n+            # are explicitly specified in `__init__`\n+            init_args = tf_inspect.getfullargspec(self.__init__).args[1:]\n+            xtra_args = set([\"name\", \"trainable\", \"dtype\", \"batch_input_shape\"])\n+            for key in xtra_args - xtra_args.intersection(init_args):\n+                config.pop(key, None)\n+        else:\n             config = {}\n         if saving_lib.saving_v3_enabled():\n             if self._is_compiled and hasattr(self, \"_compile_config\"):\n\n@@ -992,6 +992,77 @@ class TrainingTest(test_combinations.TestCase):\n         model = MyModel()\n         self.assertIn('{\"a\": {}}', model.to_json())\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_get_config_default(self):\n+        class MyModel(training_module.Model):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.units = units\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+        # Test default config with named args\n+        model = MyModel(units=10)\n+        config = model.get_config()\n+        self.assertLen(config, 1)\n+        self.assertEqual(config[\"units\"], 10)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n+        # Test default config with positinal args\n+        model = MyModel(10)\n+        config = model.get_config()\n+        self.assertLen(config, 1)\n+        self.assertEqual(config[\"units\"], 10)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n+        # Test non-serializable\n+        model = MyModel(units=np.int32(10))\n+        config = model.get_config()\n+        self.assertNotIn(\"units\", config)\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_get_config_kwargs(self):\n+        class MyModel(training_module.Model):\n+            def __init__(self, units, **kwargs):\n+                super().__init__()\n+                self.units = units\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+        model = MyModel(10, extra=1)\n+        config = model.get_config()\n+        self.assertLen(config, 2)\n+        self.assertEqual(config[\"units\"], 10)\n+        self.assertEqual(config[\"extra\"], 1)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_get_config_override(self):\n+        class MyModel(training_module.Model):\n+            def __init__(self, units):\n+                super().__init__()\n+                self.units = units\n+\n+            def call(self, inputs):\n+                return inputs\n+\n+            def get_config(self):\n+                config = {\"units\": int(self.units)}\n+                config.update(super().get_config())\n+                return config\n+\n+        model = MyModel(units=np.int32(10))\n+        config = model.get_config()\n+        self.assertLen(config, 1)\n+        self.assertEqual(config[\"units\"], 10)\n+        model = model.from_config(config)\n+        self.assertDictEqual(model.get_config(), config)\n+\n     def test_training_on_sparse_data_with_dense_placeholders_v1(self):\n         with tf.Graph().as_default():\n             if scipy_sparse is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4142bfbcf343b2d945bab74044de0dd95381a7f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 9490 | Contributors (this commit): 115 | Commits (past 90d): 13 | Contributors (cumulative): 115 | DMM Complexity: None\n\nDIFF:\n@@ -2354,8 +2354,9 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n           same applies for `'epoch'`. If using an integer, let's say `1000`, the\n           callback will write the metrics and losses to TensorBoard every 1000\n           batches. Note that writing too frequently to TensorBoard can slow down\n-          your training. May not work when doing distributed training, as\n-          currently only a subset of `tf.distribute.Strategy`s are supported.\n+          your training, especially when used with `tf.distribute.Strategy` as\n+          it will incur additional synchronization overhead.\n+          Use with `ParameterServerStrategy` is not supported.\n         profile_batch: Profile the batch(es) to sample compute characteristics.\n           profile_batch must be a non-negative integer or a tuple of integers.\n           A pair of positive integers signify a range of batches to profile.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#856467375583c681ebba0089110380960770c738", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 805 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,11 @@ INT = utils.INT\n ONE_HOT = utils.ONE_HOT\n \n \n-@keras_export(\"keras.layers.experimental.preprocessing.HashedCrossing\")\n+@keras_export(\n+    \"keras.layers.HashedCrossing\",\n+    \"keras.layers.experimental.preprocessing.HashedCrossing\",\n+    v1=[],\n+)\n class HashedCrossing(base_layer.Layer):\n     \"\"\"A preprocessing layer which crosses features using the \"hashing trick\".\n \n@@ -62,7 +66,7 @@ class HashedCrossing(base_layer.Layer):\n \n     **Crossing two scalar features.**\n \n-    >>> layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n+    >>> layer = tf.keras.layers.HashedCrossing(\n     ...     num_bins=5)\n     >>> feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])\n     >>> feat2 = tf.constant([101, 101, 101, 102, 102])\n@@ -71,7 +75,7 @@ class HashedCrossing(base_layer.Layer):\n \n     **Crossing and one-hotting two scalar features.**\n \n-    >>> layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n+    >>> layer = tf.keras.layers.HashedCrossing(\n     ...     num_bins=5, output_mode='one_hot')\n     >>> feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])\n     >>> feat2 = tf.constant([101, 101, 101, 102, 102])\n\n@@ -122,7 +122,7 @@ class FunctionalPreprocessingStage(\n \n     >>> inputs = {'x2': tf.keras.Input(shape=(5,)),\n     ...           'x1': tf.keras.Input(shape=(1,))}\n-    >>> norm_layer = tf.keras.layers.experimental.preprocessing.Normalization()\n+    >>> norm_layer = tf.keras.layers.Normalization()\n     >>> y = norm_layer(inputs['x2'])\n     >>> y, z = tf.keras.layers.Lambda(lambda x: (x, x))(inputs['x1'])\n     >>> outputs = [inputs['x1'], [y, z]]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6246e9cc5a3f34259cdf8f21b1c05edaa4d75164", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 93 | Churn Cumulative: 3679 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -676,8 +676,9 @@ def update_confusion_matrix_variables(\n     if top_k is not None:\n         y_pred = _filter_top_k(y_pred, top_k)\n     if class_id is not None:\n-        y_true = y_true[..., class_id]\n-        y_pred = y_pred[..., class_id]\n+        # Preserve dimension to match with sample_weight\n+        y_true = y_true[..., class_id, None]\n+        y_pred = y_pred[..., class_id, None]\n \n     if thresholds_distributed_evenly:\n         return _update_confusion_matrix_variables_optimized(\n\n@@ -20,6 +20,7 @@ from absl.testing import parameterized\n \n from keras import backend\n from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import metrics_utils\n \n \n@@ -456,5 +457,92 @@ class MatchesMethodsTest(tf.test.TestCase, parameterized.TestCase):\n         )\n \n \n+@test_utils.run_v2_only\n+class UpdateConfusionMatrixVarTest(tf.test.TestCase, parameterized.TestCase):\n+    def setUp(self):\n+        self.tp = metrics_utils.ConfusionMatrix.TRUE_POSITIVES\n+        self.tn = metrics_utils.ConfusionMatrix.TRUE_NEGATIVES\n+        self.fp = metrics_utils.ConfusionMatrix.FALSE_POSITIVES\n+        self.fn = metrics_utils.ConfusionMatrix.FALSE_NEGATIVES\n+        self.variables_to_update = {\n+            self.tp: tf.Variable([0], dtype=tf.float32),\n+            self.tn: tf.Variable([0], dtype=tf.float32),\n+            self.fp: tf.Variable([0], dtype=tf.float32),\n+            self.fn: tf.Variable([0], dtype=tf.float32),\n+        }\n+\n+    def test_without_sample_weight(self):\n+        y_true = tf.constant([[1, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant([[0.8, 0.7, 0.1], [0.1, 0.6, 0.4]])\n+        thresholds = [0.5]\n+\n+        metrics_utils.update_confusion_matrix_variables(\n+            variables_to_update=self.variables_to_update,\n+            y_true=y_true,\n+            y_pred=y_pred,\n+            thresholds=thresholds,\n+        )\n+        self.assertEqual(self.variables_to_update[self.tp].numpy()[0], 2)\n+        self.assertEqual(self.variables_to_update[self.tn].numpy()[0], 2)\n+        self.assertEqual(self.variables_to_update[self.fp].numpy()[0], 1)\n+        self.assertEqual(self.variables_to_update[self.fn].numpy()[0], 1)\n+\n+    def test_with_sample_weight(self):\n+        y_true = tf.constant([[1, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant([[0.8, 0.7, 0.1], [0.1, 0.6, 0.4]])\n+        thresholds = [0.5]\n+        sample_weight = [2, 1]\n+\n+        metrics_utils.update_confusion_matrix_variables(\n+            variables_to_update=self.variables_to_update,\n+            y_true=y_true,\n+            y_pred=y_pred,\n+            thresholds=thresholds,\n+            sample_weight=sample_weight,\n+        )\n+        self.assertEqual(self.variables_to_update[self.tp].numpy()[0], 4)\n+        self.assertEqual(self.variables_to_update[self.tn].numpy()[0], 3)\n+        self.assertEqual(self.variables_to_update[self.fp].numpy()[0], 1)\n+        self.assertEqual(self.variables_to_update[self.fn].numpy()[0], 1)\n+\n+    def test_with_class_id(self):\n+        y_true = tf.constant([[1, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant([[0.8, 0.7, 0.1], [0.1, 0.6, 0.4]])\n+        thresholds = [0.5]\n+        class_id = 2\n+\n+        metrics_utils.update_confusion_matrix_variables(\n+            variables_to_update=self.variables_to_update,\n+            y_true=y_true,\n+            y_pred=y_pred,\n+            thresholds=thresholds,\n+            class_id=class_id,\n+        )\n+        self.assertEqual(self.variables_to_update[self.tp].numpy()[0], 0)\n+        self.assertEqual(self.variables_to_update[self.tn].numpy()[0], 1)\n+        self.assertEqual(self.variables_to_update[self.fp].numpy()[0], 0)\n+        self.assertEqual(self.variables_to_update[self.fn].numpy()[0], 1)\n+\n+    def test_with_sample_weight_and_classid(self):\n+        y_true = tf.constant([[1, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant([[0.8, 0.7, 0.1], [0.1, 0.6, 0.4]])\n+        thresholds = [0.5]\n+        sample_weight = [2, 1]\n+        class_id = 2\n+\n+        metrics_utils.update_confusion_matrix_variables(\n+            variables_to_update=self.variables_to_update,\n+            y_true=y_true,\n+            y_pred=y_pred,\n+            thresholds=thresholds,\n+            sample_weight=sample_weight,\n+            class_id=class_id,\n+        )\n+        self.assertEqual(self.variables_to_update[self.tp].numpy()[0], 0)\n+        self.assertEqual(self.variables_to_update[self.tn].numpy()[0], 2)\n+        self.assertEqual(self.variables_to_update[self.fp].numpy()[0], 0)\n+        self.assertEqual(self.variables_to_update[self.fn].numpy()[0], 1)\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bd5215ed91bee997f38454625da13d66e3c0e404", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 131 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -89,7 +89,7 @@ class SGD(optimizer.Optimizer):\n \n     Reference:\n         - For `nesterov=True`, See [Sutskever et al., 2013](\n-          http://jmlr.org/proceedings/papers/v28/sutskever13.pdf).\n+          http://proceedings.mlr.press/v28/sutskever13.pdf).\n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca1fb58752a30787f4500914f517c2d190885d0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 12498 | Contributors (this commit): 27 | Commits (past 90d): 13 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2293,9 +2293,13 @@ class TrainingTest(test_combinations.TestCase):\n         )\n \n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n-    def test_ema_overwrite(self):\n+    @parameterized.named_parameters(\n+        (\"mixed_float16\", \"mixed_float16\"), (\"float32\", \"float32\")\n+    )\n+    def test_ema_overwrite(self, test_policy):\n         if not tf.__internal__.tf2.enabled():\n             self.skipTest(\"EMA optimizer is only available in TF2.\")\n+        policy.set_global_policy(test_policy)\n         model = sequential.Sequential()\n         model.add(input_layer.Input(shape=(4,)))\n         model.add(layers_module.Dense(1, activation=\"relu\"))\n@@ -2309,6 +2313,7 @@ class TrainingTest(test_combinations.TestCase):\n         history = model.fit(dataset, epochs=2, steps_per_epoch=10)\n         self.assertLen(history.history[\"loss\"], 2)\n         self.assertAllClose(initial_value, model.trainable_variables[0])\n+        policy.set_global_policy(\"float32\")\n \n     @test_combinations.run_all_keras_modes(always_skip_v1=True)\n     def test_get_verbosity(self):\n\n@@ -1398,6 +1398,9 @@ class LossScaleOptimizerV3(\n     def ema_momentum(self, ema_momentum):\n         self._optimizer.ema_momentum = ema_momentum\n \n+    def finalize_variable_values(self, var_list):\n+        self._optimizer.finalize_variable_values(var_list)\n+\n \n class FakeOptimizerForRestoration(tf.__internal__.tracking.Trackable):\n     \"\"\"A fake optimizer used to support restoring TensorFlow 2.2 checkpoints.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8fcafb8292171fe9e0a0e25ac57644c985b6df7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 35 | Churn Cumulative: 1360 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -549,24 +549,16 @@ class MultiHeadAttention(Layer):\n         training=None,\n         use_causal_mask=False,\n     ):\n-        attention_mask = self._compute_attention_mask(\n-            query,\n-            value,\n-            key=key,\n-            attention_mask=attention_mask,\n-            use_causal_mask=use_causal_mask,\n-        )\n-\n         if not self._built_from_signature:\n             self._build_from_signature(query=query, value=value, key=key)\n         if key is None:\n             key = value\n \n+        # Convert RaggedTensor to Tensor.\n         query_is_ragged = isinstance(query, tf.RaggedTensor)\n         if query_is_ragged:\n             query_lengths = query.nested_row_lengths()\n             query = query.to_tensor()\n-\n         key_is_ragged = isinstance(key, tf.RaggedTensor)\n         value_is_ragged = isinstance(value, tf.RaggedTensor)\n         if key_is_ragged and value_is_ragged:\n@@ -581,6 +573,14 @@ class MultiHeadAttention(Layer):\n         elif value_is_ragged:\n             value = value.to_tensor(shape=tf.shape(key))\n \n+        attention_mask = self._compute_attention_mask(\n+            query,\n+            value,\n+            key=key,\n+            attention_mask=attention_mask,\n+            use_causal_mask=use_causal_mask,\n+        )\n+\n         #   N = `num_attention_heads`\n         #   H = `size_per_head`\n         # `query` = [B, T, N ,H]\n\n@@ -329,6 +329,23 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         results = test_layer(query, value, key)\n         self.assertAllEqual(results.shape.as_list(), query.shape.as_list())\n \n+    def test_ragged_tensor_with_causal_mask_no_error(self):\n+        ragged_tensor = tf.ragged.constant(\n+            [\n+                [[3.0, 1.0], [4.0, 1.0]],\n+                [[5.0, 9.0], [2.0, 6.0], [3.0, 1.0]],\n+                [[1.0, 2.0]],\n+            ],\n+            inner_shape=(2,),\n+        )\n+        test_layer = keras.layers.MultiHeadAttention(num_heads=5, key_dim=2)\n+        results = test_layer(\n+            ragged_tensor, ragged_tensor, ragged_tensor, use_causal_mask=True\n+        )\n+        self.assertAllEqual(\n+            results.shape.as_list(), ragged_tensor.shape.as_list()\n+        )\n+\n     def test_query_mask_progagation(self):\n         \"\"\"Test automatic propagation of the query's mask.\"\"\"\n         test_layer = keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc1b762f61900130501cf0336b1acaa9d75aa643", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 9686 | Contributors (this commit): 115 | Commits (past 90d): 18 | Contributors (cumulative): 119 | DMM Complexity: None\n\nDIFF:\n@@ -2349,11 +2349,13 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         write_steps_per_second: whether to log the training steps per second\n           into Tensorboard. This supports both epoch and batch frequency\n           logging.\n-        update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n-          writes the losses and metrics to TensorBoard after each batch. The\n-          same applies for `'epoch'`. If using an integer, let's say `1000`, the\n-          callback will write the metrics and losses to TensorBoard every 1000\n-          batches. Note that writing too frequently to TensorBoard can slow down\n+        update_freq: `'batch'` or `'epoch'` or integer. When using `'epoch'`,\n+          writes the losses and metrics to TensorBoard after every epoch.\n+          If using an integer, let's say `1000`, all metrics and losses\n+          (including custom ones added by `Model.compile`) will be logged to\n+          TensorBoard every 1000 batches. `'batch'` is a synonym for `1`,\n+          meaning that they will be written every batch.\n+          Note however that writing too frequently to TensorBoard can slow down\n           your training, especially when used with `tf.distribute.Strategy` as\n           it will incur additional synchronization overhead.\n           Use with `ParameterServerStrategy` is not supported.\n@@ -2777,9 +2779,10 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                 step=self._train_step,\n             )\n \n-        # `logs` is a `tf.distribute.experimental.coordinator.RemoteValue` when\n-        # using asynchronous strategies, for now we just disable `update_freq`\n-        # entirely in those cases.\n+        # `logs` isn't necessarily always a dict. For example, when using\n+        # `tf.distribute.experimental.ParameterServerStrategy`, a\n+        # `tf.distribute.experimental.coordinator.RemoteValue` will be passed.\n+        # For now, we just disable `update_freq` in those cases.\n         if isinstance(logs, dict):\n             for name, value in logs.items():\n                 tf.summary.scalar(\"batch_\" + name, value, step=self._train_step)\n\n@@ -117,8 +117,9 @@ class DistributedTrainingTest(tf.test.TestCase):\n             strategy.cluster_resolver\n             and strategy.cluster_resolver.task_type == \"worker\"\n         ):\n-            # Workaround for an issue with\n-            # `tf.distribute.MultiWorkerMirroredStrategy`\n+            # The below assertion is run by both chief and workers when using\n+            # `tf.distribute.MultiWorkerMirroredStrategy`, but only the chief\n+            # will log events.\n             events_expected = []\n \n         self.assertEqual(events_got, events_expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85db5d07db54b853484bfd358c3894d948c36baf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1163 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -448,6 +448,12 @@ class Conv3DTest(test_combinations.TestCase):\n             layer = keras.layers.Conv3D(**kwargs)\n             layer.build((None, 5, 5, 5, 2))\n \n+    def test_conv3d_zero_dim_output(self):\n+        conv = keras.layers.Convolution3DTranspose(2, [3, 3, 3], padding=\"same\")\n+        x = tf.random.uniform([1, 32, 32, 0, 3], dtype=tf.float32)\n+        # The layer doesn't crash with 0 dim input\n+        _ = conv(x)\n+\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class GroupedConvTest(test_combinations.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d768c16530ddc4548fe45a8b6b09a218eef255f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 12 | Churn Cumulative: 5531 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1802,8 +1802,8 @@ def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n         # For single x-input, we do no tuple wrapping since in this case\n         # there is no ambiguity. This also makes NumPy and Dataset\n         # consistent in that the user does not have to wrap their Dataset\n-        # data in an unnecessary tuple\n-        if not tf.nest.is_nested(x):\n+        # data in an unnecessary tuple.\n+        if not isinstance(x, tuple or list):\n             return x\n         else:\n             return (x,)\n\n@@ -1482,6 +1482,14 @@ class TestDataAdapterUtils(DataAdapterTestBase):\n         )\n         self.assertEqual(tuple_version, list_version)\n \n+    def test_unpack_pack_dict(self):\n+        # A dictionary can be unambiguously represented without a tuple.\n+        x = {\"key\": self.tensor_input}\n+        packed_x = data_adapter.pack_x_y_sample_weight(x)\n+        self.assertEqual(packed_x, x)\n+        unpacked_x, _, _ = data_adapter.unpack_x_y_sample_weight(x)\n+        self.assertEqual(unpacked_x, x)\n+\n \n if __name__ == \"__main__\":\n     tf.compat.v1.enable_eager_execution()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7bc1f6b8f514f17a4da78c174a8f1bafbf66da68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 291 | Files Changed: 12 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 291 | Churn Cumulative: 1161 | Contributors (this commit): 12 | Commits (past 90d): 31 | Contributors (cumulative): 44 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,15 +23,6 @@ import tensorflow.compat.v2 as tf\n \n # Imports needed for deserialization.\n from keras import backend\n-from keras.optimizers.legacy import adadelta as adadelta_legacy\n-from keras.optimizers.legacy import adagrad as adagrad_legacy\n-from keras.optimizers.legacy import adam as adam_legacy\n-from keras.optimizers.legacy import adamax as adamax_legacy\n-from keras.optimizers.legacy import ftrl as ftrl_legacy\n-from keras.optimizers.legacy import nadam as nadam_legacy\n-from keras.optimizers.legacy import optimizer as optimizer_legacy\n-from keras.optimizers.legacy import rmsprop as rmsprop_legacy\n-from keras.optimizers.legacy import sgd as sgd_legacy\n from keras.optimizers.optimizer_experimental import (\n     adadelta as adadelta_experimental,\n )\n\n@@ -1,24 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy optimizer package.\"\"\"\n-\n-from keras.optimizers.legacy.adadelta import Adadelta\n-from keras.optimizers.legacy.adagrad import Adagrad\n-from keras.optimizers.legacy.adam import Adam\n-from keras.optimizers.legacy.adamax import Adamax\n-from keras.optimizers.legacy.ftrl import Ftrl\n-from keras.optimizers.legacy.nadam import Nadam\n-from keras.optimizers.legacy.rmsprop import RMSprop\n-from keras.optimizers.legacy.sgd import SGD\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Adadelta optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import adadelta\n-\n-\n-class Adadelta(adadelta.Adadelta):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Adagrad optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import adagrad\n-\n-\n-class Adagrad(adagrad.Adagrad):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Adam optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import adam\n-\n-\n-class Adam(adam.Adam):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Adamax optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import adamax\n-\n-\n-class Adamax(adamax.Adamax):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Ftrl optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import ftrl\n-\n-\n-class Ftrl(ftrl.Ftrl):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Nadam optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import nadam\n-\n-\n-class Nadam(nadam.Nadam):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy Adam optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import optimizer_v2\n-\n-\n-class Optimizer(optimizer_v2.OptimizerV2):\n-    pass\n\n@@ -1,69 +0,0 @@\n-\"\"\"Tests for optimizer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-from absl.testing import parameterized\n-\n-import keras\n-from keras.optimizers.legacy import adadelta\n-from keras.optimizers.legacy import adagrad\n-from keras.optimizers.legacy import adam\n-from keras.optimizers.legacy import adamax\n-from keras.optimizers.legacy import ftrl\n-from keras.optimizers.legacy import nadam\n-from keras.optimizers.legacy import rmsprop\n-from keras.optimizers.legacy import sgd\n-\n-adadelta_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"adadelta\", lambda: adadelta.Adadelta(0.002)\n-)\n-adagrad_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"adagrad\", lambda: adagrad.Adagrad(0.002)\n-)\n-adam_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"adam\", lambda: adam.Adam(0.002)\n-)\n-adamax_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"adamax\", lambda: adamax.Adamax(0.002)\n-)\n-ftrl_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"ftrl\", lambda: ftrl.Ftrl(0.002)\n-)\n-gradient_descent_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"sgd\", lambda: sgd.SGD(0.002)\n-)\n-nadam_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"nadam\", lambda: nadam.Nadam(0.002)\n-)\n-rmsprop_fn = tf.__internal__.test.combinations.NamedObject(\n-    \"rmsprop\", lambda: rmsprop.RMSprop(0.002)\n-)\n-\n-OPTIMIZER_FN = [\n-    adadelta_fn,\n-    adagrad_fn,\n-    adam_fn,\n-    adamax_fn,\n-    ftrl_fn,\n-    gradient_descent_fn,\n-    nadam_fn,\n-    rmsprop_fn,\n-]\n-\n-\n-class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n-    \"\"\"Test the functionality of optimizer.\"\"\"\n-\n-    @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n-    def testModelFit(self, optimizer_fn):\n-        model = keras.Sequential(\n-            [keras.layers.Input(shape=(1,)), keras.layers.Dense(1)]\n-        )\n-        optimizer = optimizer_fn()\n-        x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n-        y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n-        model.compile(loss=\"mse\", optimizer=optimizer)\n-        model.fit(x, y, epochs=1, steps_per_epoch=5)\n-\n-\n-if __name__ == \"__main__\":\n-    tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy RMSprop optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import rmsprop\n-\n-\n-class RMSprop(rmsprop.RMSprop):\n-    pass\n\n@@ -1,21 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Legacy SGD optimizer implementation.\"\"\"\n-\n-from keras.optimizers.optimizer_v2 import gradient_descent\n-\n-\n-class SGD(gradient_descent.SGD):\n-    pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
