{"custom_id": "scrapy#84fb234cae1bffa944d9f5870c2fc87c75d35261", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 1128 | Contributors (this commit): 30 | Commits (past 90d): 4 | Contributors (cumulative): 30 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -36,13 +36,17 @@ from scrapy.utils.request import referer_str\n logger = logging.getLogger(__name__)\n \n \n+def _to_string(path: Union[str, os.PathLike]):\n+    return str(path)  # convert a Path object to string\n+\n+\n class FileException(Exception):\n     \"\"\"General media error exception\"\"\"\n \n \n class FSFilesStore:\n     def __init__(self, basedir: Union[str, os.PathLike]):\n-        basedir = str(basedir)  # support Path object\n+        basedir = _to_string(basedir)\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n         self.basedir = basedir\n@@ -67,7 +71,7 @@ class FSFilesStore:\n         return {'last_modified': last_modified, 'checksum': checksum}\n \n     def _get_filesystem_path(self, path: Union[str, os.PathLike]) -> Path:\n-        path_comps = str(path).split('/')\n+        path_comps = _to_string(path).split('/')\n         return Path(self.basedir, *path_comps)\n \n     def _mkdir(self, dirname: Path, domain: Optional[str] = None):\n@@ -323,12 +327,12 @@ class FilesPipeline(MediaPipeline):\n     DEFAULT_FILES_RESULT_FIELD = 'files'\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n+        store_uri = _to_string(store_uri)\n         if not store_uri:\n             raise NotConfigured\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n-\n         cls_name = \"FilesPipeline\"\n         self.store = self._get_store(store_uri)\n         resolve = functools.partial(self._key_for_pipe,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44512bebc8bc74164c73be55a8fd5debafb48f10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1134 | Contributors (this commit): 30 | Commits (past 90d): 5 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ from scrapy.utils.request import referer_str\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, os.PathLike]):\n+def _to_string(path: Union[str, os.PathLike[str]]):\n     return str(path)  # convert a Path object to string\n \n \n@@ -45,7 +45,7 @@ class FileException(Exception):\n \n \n class FSFilesStore:\n-    def __init__(self, basedir: Union[str, os.PathLike]):\n+    def __init__(self, basedir: Union[str, os.PathLike[str]]):\n         basedir = _to_string(basedir)\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n@@ -70,7 +70,7 @@ class FSFilesStore:\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, path: Union[str, os.PathLike]) -> Path:\n+    def _get_filesystem_path(self, path: Union[str, os.PathLike[str]]) -> Path:\n         path_comps = _to_string(path).split('/')\n         return Path(self.basedir, *path_comps)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c753adbe532f1a027d1d285a7c1e4b59fd9e5a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1140 | Contributors (this commit): 30 | Commits (past 90d): 6 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ from scrapy.utils.request import referer_str\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, os.PathLike[str]]):\n+def _to_string(path: Union[str, os.PathLike]):\n     return str(path)  # convert a Path object to string\n \n \n@@ -45,7 +45,7 @@ class FileException(Exception):\n \n \n class FSFilesStore:\n-    def __init__(self, basedir: Union[str, os.PathLike[str]]):\n+    def __init__(self, basedir: Union[str, os.PathLike]):\n         basedir = _to_string(basedir)\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n@@ -70,7 +70,7 @@ class FSFilesStore:\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, path: Union[str, os.PathLike[str]]) -> Path:\n+    def _get_filesystem_path(self, path: Union[str, os.PathLike]) -> Path:\n         path_comps = _to_string(path).split('/')\n         return Path(self.basedir, *path_comps)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42e8d5a6157a9d2495bd7a2934efe776eee2a7d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1147 | Contributors (this commit): 30 | Commits (past 90d): 7 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,7 @@ import logging\n import mimetypes\n import os\n import time\n+from os import PathLike\n from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n@@ -36,7 +37,7 @@ from scrapy.utils.request import referer_str\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, os.PathLike]):\n+def _to_string(path: Union[str, PathLike]):\n     return str(path)  # convert a Path object to string\n \n \n@@ -45,7 +46,7 @@ class FileException(Exception):\n \n \n class FSFilesStore:\n-    def __init__(self, basedir: Union[str, os.PathLike]):\n+    def __init__(self, basedir: Union[str, PathLike]):\n         basedir = _to_string(basedir)\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n@@ -70,7 +71,7 @@ class FSFilesStore:\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, path: Union[str, os.PathLike]) -> Path:\n+    def _get_filesystem_path(self, path: Union[str, PathLike]) -> Path:\n         path_comps = _to_string(path).split('/')\n         return Path(self.basedir, *path_comps)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c883a13006ad98f9f529fe130f7d717ad91f7afb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 591 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -106,10 +106,10 @@ class Request(object_ref):\n         self.flags = [] if flags is None else list(flags)\n \n     def _set_xback(self, name: str, value: Optional[Callable]) -> None:\n-        if (\n-            value is not None\n-            and (name != \"callback\" or value is not NO_CALLBACK)\n-            and not callable(value)\n+        if not (\n+            callable(value)\n+            or value is None\n+            or (name == \"callback\" and value is NO_CALLBACK)\n         ):\n             raise TypeError(f\"{name} must be a callable, got {type(value).__name__}\")\n         setattr(self, name, value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f3e42897a6697f799d04a4a4320a7c16cbcd939", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 597 | Contributors (this commit): 17 | Commits (past 90d): 9 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n \n # https://github.com/python/typing/issues/689#issuecomment-561425237\n-class _NoCallback(Enum):\n+class NoCallbackType(Enum):\n     NO_CALLBACK = 0\n \n \n@@ -37,7 +37,7 @@ class _NoCallback(Enum):\n #: :meth:`scrapy.core.engine.ExecutionEngine.download`, so that download\n #: middlewares handling such requests can treat them differently from requests\n #: intended for the :meth:`~scrapy.Spider.parse` callback.\n-NO_CALLBACK: Final = _NoCallback.NO_CALLBACK\n+NO_CALLBACK: Final = NoCallbackType.NO_CALLBACK\n \n \n class Request(object_ref):\n@@ -67,7 +67,7 @@ class Request(object_ref):\n     Currently used by :meth:`Request.replace`, :meth:`Request.to_dict` and\n     :func:`~scrapy.utils.request.request_from_dict`.\n     \"\"\"\n-    callback: Union[None, _NoCallback, Callable]\n+    callback: Union[None, NoCallbackType, Callable]\n     errback: Optional[Callable]\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0ea7fd4fd2726494adf6a84a9fd6b6d7fcf116f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1633 | Contributors (this commit): 26 | Commits (past 90d): 9 | Contributors (cumulative): 44 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,3 @@ class ScrapyUtilsTest(unittest.TestCase):\n         pinned_twisted_version_string = match[1]\n \n         self.assertEqual(twisted_version.short(), pinned_twisted_version_string)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -528,7 +528,3 @@ class TestHelper(unittest.TestCase):\n             type(a) is type(b), f\"Got {type(a)}, expected {type(b)} for { obj!r}\"\n         )\n         self.assertEqual(a, b)\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -258,7 +258,3 @@ class UtilsPythonTestCase(unittest.TestCase):\n             without_none_values({\"one\": 1, \"none\": None, \"three\": 3, \"four\": 4}),\n             {\"one\": 1, \"three\": 3, \"four\": 4},\n         )\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e33fb812b227b8f5ded8be9913467c31929a87b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 1907 | Contributors (this commit): 26 | Commits (past 90d): 14 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -37,11 +37,11 @@ if __name__ == \"__main__\":\n \n     root = Root()\n     factory = Site(root)\n-    httpPort = reactor.listenTCP(8998, Site(root))\n+    httpPort = reactor.listenTCP(8998, Site(root))  # type: ignore[attr-defined]\n \n     def _print_listening():\n         httpHost = httpPort.getHost()\n         print(f\"Bench server at http://{httpHost.host}:{httpHost.port}\")\n \n-    reactor.callWhenRunning(_print_listening)\n-    reactor.run()\n+    reactor.callWhenRunning(_print_listening)  # type: ignore[attr-defined]\n+    reactor.run()  # type: ignore[attr-defined]\n\n@@ -41,10 +41,10 @@ if __name__ == \"__main__\":\n         url = f\"http://not.a.real.domain:{port}/echo\"\n \n         servers = [(mock_dns_server.host, mock_dns_server.port)]\n-        reactor.installResolver(createResolver(servers=servers))\n+        reactor.installResolver(createResolver(servers=servers))  # type: ignore[attr-defined]\n \n         configure_logging()\n         runner = CrawlerRunner()\n         d = runner.crawl(LocalhostSpider, url=url)\n-        d.addBoth(lambda _: reactor.stop())\n-        reactor.run()\n+        d.addBoth(lambda _: reactor.stop())  # type: ignore[attr-defined]\n+        reactor.run()  # type: ignore[attr-defined]\n\n@@ -10,6 +10,7 @@ from urllib.parse import urlencode\n \n from OpenSSL import SSL\n from twisted.internet import defer, reactor, ssl\n+from twisted.internet.protocol import ServerFactory\n from twisted.internet.task import deferLater\n from twisted.names import dns, error\n from twisted.names.server import DNSServerFactory\n@@ -369,12 +370,14 @@ if __name__ == \"__main__\":\n     )\n     args = parser.parse_args()\n \n+    factory: ServerFactory\n+\n     if args.type == \"http\":\n         root = Root()\n         factory = Site(root)\n-        httpPort = reactor.listenTCP(0, factory)\n+        httpPort = reactor.listenTCP(0, factory)  # type: ignore[attr-defined]\n         contextFactory = ssl_context_factory()\n-        httpsPort = reactor.listenSSL(0, factory, contextFactory)\n+        httpsPort = reactor.listenSSL(0, factory, contextFactory)  # type: ignore[attr-defined]\n \n         def print_listening():\n             httpHost = httpPort.getHost()\n@@ -388,11 +391,11 @@ if __name__ == \"__main__\":\n         clients = [MockDNSResolver()]\n         factory = DNSServerFactory(clients=clients)\n         protocol = dns.DNSDatagramProtocol(controller=factory)\n-        listener = reactor.listenUDP(0, protocol)\n+        listener = reactor.listenUDP(0, protocol)  # type: ignore[attr-defined]\n \n         def print_listening():\n             host = listener.getHost()\n             print(f\"{host.host}:{host.port}\")\n \n-    reactor.callWhenRunning(print_listening)\n-    reactor.run()\n+    reactor.callWhenRunning(print_listening)  # type: ignore[attr-defined]\n+    reactor.run()  # type: ignore[attr-defined]\n\n@@ -557,4 +557,4 @@ class EngineTest(unittest.TestCase):\n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == \"runserver\":\n         start_test_site(debug=True)\n-        reactor.run()\n+        reactor.run()  # type: ignore[attr-defined]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#afd5d8532093c25a6ed4453e3a10b275335c94bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4269 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,7 @@ spider = Spider(\"foo\")\n \n class TestHttpProxyMiddleware(TestCase):\n \n-    failureException = AssertionError\n+    failureException = AssertionError  # type: ignore[assignment]\n \n     def setUp(self):\n         self._oldenv = os.environ.copy()\n\n@@ -5,6 +5,7 @@ import shutil\n import string\n from ipaddress import IPv4Address\n from pathlib import Path\n+from typing import Dict\n from unittest import mock, skipIf\n from urllib.parse import urlencode\n \n@@ -86,6 +87,7 @@ class GetDataHtmlLarge(LeafResource):\n class PostDataJsonMixin:\n     @staticmethod\n     def make_response(request: TxRequest, extra_data: str):\n+        assert request.content is not None\n         response = {\n             \"request-headers\": {},\n             \"request-body\": json.loads(request.content.read()),\n@@ -144,7 +146,8 @@ class QueryParams(LeafResource):\n         request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n         request.setHeader(\"Content-Encoding\", \"UTF-8\")\n \n-        query_params = {}\n+        query_params: Dict[str, str] = {}\n+        assert request.args is not None\n         for k, v in request.args.items():\n             query_params[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n \n\n@@ -112,6 +112,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n     Should work for process_spider_output and, when it's supported, process_start_requests.\n     \"\"\"\n \n+    ITEM_TYPE: type\n     RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects\n \n     @staticmethod\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#764a9d47bb95e87519a4fc2fe20ddf0c97b11739", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4093 | Contributors (this commit): 33 | Commits (past 90d): 5 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ For more information see docs/topics/architecture.rst\n import logging\n import warnings\n from time import time\n-from typing import Callable, Iterable, Iterator, Optional, Set, Union\n+from typing import Any, Callable, Generator, Iterable, Iterator, Optional, Set, Union\n \n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n from twisted.internet.task import LoopingCall\n@@ -96,7 +96,7 @@ class ExecutionEngine:\n         return scheduler_cls\n \n     @inlineCallbacks\n-    def start(self) -> Deferred:\n+    def start(self) -> Generator[Deferred, Any, None]:\n         if self.running:\n             raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n@@ -109,7 +109,7 @@ class ExecutionEngine:\n         \"\"\"Gracefully stop the execution engine\"\"\"\n \n         @inlineCallbacks\n-        def _finish_stopping_engine(_) -> Deferred:\n+        def _finish_stopping_engine(_) -> Generator[Deferred, Any, None]:\n             yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n             self._closewait.callback(None)\n \n\n@@ -182,7 +182,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         spider: Spider,\n         result: Union[Iterable, AsyncIterable],\n         start_index: int = 0,\n-    ) -> Deferred:\n+    ) -> Generator[Deferred, Any, Union[MutableChain, MutableAsyncChain]]:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n         recovered: Union[MutableChain, MutableAsyncChain]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5c1ef82762c6c0910abea00c0a6249c40005e44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 428 | Lines Deleted: 484 | Files Changed: 202 | Hunks: 544 | Methods Changed: 5 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 912 | Churn Cumulative: 111643 | Contributors (this commit): 285 | Commits (past 90d): 628 | Contributors (cumulative): 2657 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,6 @@ import pytest\n from twisted.web.http import H2_ENABLED\n \n from scrapy.utils.reactor import install_reactor\n-\n from tests.keys import generate_keys\n \n \n\n@@ -1,7 +1,8 @@\n from operator import itemgetter\n-from docutils.parsers.rst.roles import set_classes\n+\n from docutils import nodes\n from docutils.parsers.rst import Directive\n+from docutils.parsers.rst.roles import set_classes\n from sphinx.util.nodes import make_refnode\n \n \n\n@@ -1,9 +1,10 @@\n #!/usr/bin/env python\n-from time import time\n from collections import deque\n-from twisted.web.server import Site, NOT_DONE_YET\n-from twisted.web.resource import Resource\n+from time import time\n+\n from twisted.internet import reactor\n+from twisted.web.resource import Resource\n+from twisted.web.server import NOT_DONE_YET, Site\n \n \n class Root(Resource):\n\n@@ -8,8 +8,8 @@ usage:\n \n \"\"\"\n \n-from scrapy.spiders import Spider\n from scrapy.http import Request\n+from scrapy.spiders import Spider\n \n \n class QPSSpider(Spider):\n\n@@ -8,12 +8,12 @@ import warnings\n \n from twisted import version as _txv\n \n+from scrapy.http import FormRequest, Request\n+from scrapy.item import Field, Item\n+from scrapy.selector import Selector\n+\n # Declare top-level shortcuts\n from scrapy.spiders import Spider\n-from scrapy.http import Request, FormRequest\n-from scrapy.selector import Selector\n-from scrapy.item import Item, Field\n-\n \n __all__ = [\n     \"__version__\",\n\n@@ -1,16 +1,17 @@\n-import sys\n-import os\n import argparse\n import cProfile\n import inspect\n+import os\n+import sys\n+\n import pkg_resources\n \n import scrapy\n+from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n from scrapy.crawler import CrawlerProcess\n-from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n-from scrapy.utils.project import inside_project, get_project_settings\n+from scrapy.utils.project import get_project_settings, inside_project\n from scrapy.utils.python import garbage_collect\n \n \n\n@@ -1,16 +1,16 @@\n \"\"\"\n Base class for Scrapy commands\n \"\"\"\n-import os\n import argparse\n+import os\n from pathlib import Path\n from typing import Any, Dict, Optional\n \n from twisted.python import failure\n-from scrapy.crawler import CrawlerProcess\n \n-from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n+from scrapy.crawler import CrawlerProcess\n from scrapy.exceptions import UsageError\n+from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n \n \n class ScrapyCommand:\n\n@@ -1,6 +1,6 @@\n+import subprocess\n import sys\n import time\n-import subprocess\n from urllib.parse import urlencode\n \n import scrapy\n\n@@ -1,11 +1,12 @@\n import time\n from collections import defaultdict\n-from unittest import TextTestRunner, TextTestResult as _TextTestResult\n+from unittest import TextTestResult as _TextTestResult\n+from unittest import TextTestRunner\n \n from scrapy.commands import ScrapyCommand\n from scrapy.contracts import ContractsManager\n-from scrapy.utils.misc import load_object, set_environ\n from scrapy.utils.conf import build_component_list\n+from scrapy.utils.misc import load_object, set_environ\n \n \n class TextTestResult(_TextTestResult):\n\n@@ -1,5 +1,5 @@\n-import sys\n import os\n+import sys\n \n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n\n@@ -1,11 +1,12 @@\n import sys\n+\n from w3lib.url import is_url\n \n from scrapy.commands import ScrapyCommand\n-from scrapy.http import Request\n from scrapy.exceptions import UsageError\n+from scrapy.http import Request\n from scrapy.utils.datatypes import SequenceExclude\n-from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n+from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n \n \n class Command(ScrapyCommand):\n\n@@ -1,16 +1,15 @@\n import os\n import shutil\n import string\n-\n-from pathlib import Path\n from importlib import import_module\n+from pathlib import Path\n from typing import Optional, cast\n from urllib.parse import urlparse\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n+from scrapy.utils.template import render_templatefile, string_camelcase\n \n \n def sanitize_module_name(module_name):\n\n@@ -2,15 +2,15 @@ import json\n import logging\n from typing import Dict\n \n-from itemadapter import is_item, ItemAdapter\n+from itemadapter import ItemAdapter, is_item\n+from twisted.internet.defer import maybeDeferred\n from w3lib.url import is_url\n \n-from twisted.internet.defer import maybeDeferred\n from scrapy.commands import BaseRunSpiderCommand\n+from scrapy.exceptions import UsageError\n from scrapy.http import Request\n from scrapy.utils import display\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n-from scrapy.exceptions import UsageError\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,13 +1,13 @@\n import sys\n+from importlib import import_module\n from os import PathLike\n from pathlib import Path\n-from importlib import import_module\n from types import ModuleType\n from typing import Union\n \n-from scrapy.utils.spider import iter_spider_classes\n-from scrapy.exceptions import UsageError\n from scrapy.commands import BaseRunSpiderCommand\n+from scrapy.exceptions import UsageError\n+from scrapy.utils.spider import iter_spider_classes\n \n \n def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n\n@@ -8,7 +8,7 @@ from threading import Thread\n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.shell import Shell\n-from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n+from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n from scrapy.utils.url import guess_scheme\n \n \n\n@@ -1,16 +1,15 @@\n-import re\n import os\n+import re\n import string\n from importlib.util import find_spec\n from pathlib import Path\n-from shutil import ignore_patterns, move, copy2, copystat\n+from shutil import copy2, copystat, ignore_patterns, move\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n-\n+from scrapy.utils.template import render_templatefile, string_camelcase\n \n TEMPLATES_TO_RENDER = (\n     (\"scrapy.cfg\",),\n\n@@ -1,4 +1,5 @@\n import argparse\n+\n from scrapy.commands import fetch\n from scrapy.utils.response import open_in_browser\n \n\n@@ -1,6 +1,6 @@\n import json\n \n-from itemadapter import is_item, ItemAdapter\n+from itemadapter import ItemAdapter, is_item\n \n from scrapy.contracts import Contract\n from scrapy.exceptions import ContractFail\n\n@@ -1,16 +1,16 @@\n import random\n-from time import time\n-from datetime import datetime\n from collections import deque\n+from datetime import datetime\n+from time import time\n \n from twisted.internet import defer, task\n \n+from scrapy import signals\n+from scrapy.core.downloader.handlers import DownloadHandlers\n+from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.resolver import dnscache\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.resolver import dnscache\n-from scrapy import signals\n-from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy.core.downloader.handlers import DownloadHandlers\n \n \n class Slot:\n\n@@ -3,10 +3,10 @@ import warnings\n from OpenSSL import SSL\n from twisted.internet._sslverify import _setAcceptableProtocols\n from twisted.internet.ssl import (\n-    optionsForClientTLS,\n-    CertificateOptions,\n-    platformTrust,\n     AcceptableCiphers,\n+    CertificateOptions,\n+    optionsForClientTLS,\n+    platformTrust,\n )\n from twisted.web.client import BrowserLikePolicyForHTTPS\n from twisted.web.iweb import IPolicyForHTTPS\n@@ -15,8 +15,8 @@ from zope.interface.verify import verifyObject\n \n from scrapy.core.downloader.tls import (\n     DEFAULT_CIPHERS,\n-    openssl_methods,\n     ScrapyClientTLSOptions,\n+    openssl_methods,\n )\n from scrapy.utils.misc import create_instance, load_object\n \n\n@@ -10,7 +10,6 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -13,15 +13,15 @@ from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.python.failure import Failure\n from twisted.web.client import (\n+    URI,\n     Agent,\n     HTTPConnectionPool,\n     ResponseDone,\n     ResponseFailed,\n-    URI,\n )\n-from twisted.web.http import _DataLoss, PotentialDataLoss\n+from twisted.web.http import PotentialDataLoss, _DataLoss\n from twisted.web.http_headers import Headers as TxHeaders\n-from twisted.web.iweb import IBodyProducer, UNKNOWN_LENGTH\n+from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer\n from zope.interface import implementer\n \n from scrapy import signals\n\n@@ -16,7 +16,6 @@ from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n \n-\n H2DownloadHandlerOrSubclass = TypeVar(\n     \"H2DownloadHandlerOrSubclass\", bound=\"H2DownloadHandler\"\n )\n\n@@ -12,8 +12,8 @@ from scrapy import Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.middleware import MiddlewareManager\n-from scrapy.utils.defer import mustbe_deferred, deferred_from_coro\n from scrapy.utils.conf import build_component_list\n+from scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n \n \n class DownloaderMiddlewareManager(MiddlewareManager):\n\n@@ -4,12 +4,12 @@ from OpenSSL import SSL\n from service_identity.exceptions import CertificateError\n from twisted.internet._sslverify import (\n     ClientTLSOptions,\n-    verifyHostname,\n     VerificationError,\n+    verifyHostname,\n )\n from twisted.internet.ssl import AcceptableCiphers\n \n-from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n+from scrapy.utils.ssl import get_temp_key_info, x509name_to_string\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,15 +1,15 @@\n import re\n from time import time\n-from urllib.parse import urlparse, urlunparse, urldefrag\n-from twisted.web.http import HTTPClient\n+from urllib.parse import urldefrag, urlparse, urlunparse\n \n from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n+from twisted.web.http import HTTPClient\n \n from scrapy.http import Headers\n+from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy.responsetypes import responsetypes\n \n \n def _parsed_url_args(parsed):\n\n@@ -15,19 +15,14 @@ from twisted.python.failure import Failure\n \n from scrapy import signals\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import (\n-    CloseSpider,\n-    DontCloseSpider,\n-    ScrapyDeprecationWarning,\n-)\n-from scrapy.http import Response, Request\n+from scrapy.exceptions import CloseSpider, DontCloseSpider, ScrapyDeprecationWarning\n+from scrapy.http import Request, Response\n from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n-from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n+from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -10,7 +10,7 @@ from twisted.web.client import URI, BrowserLikePolicyForHTTPS, _StandardEndpoint\n from twisted.web.error import SchemeNotSupported\n \n from scrapy.core.downloader.contextfactory import AcceptableProtocolsContextFactory\n-from scrapy.core.http2.protocol import H2ClientProtocol, H2ClientFactory\n+from scrapy.core.http2.protocol import H2ClientFactory, H2ClientProtocol\n from scrapy.http.request import Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n\n@@ -9,9 +9,9 @@ from h2.config import H2Configuration\n from h2.connection import H2Connection\n from h2.errors import ErrorCodes\n from h2.events import (\n-    Event,\n     ConnectionTerminated,\n     DataReceived,\n+    Event,\n     ResponseReceived,\n     SettingsAcknowledged,\n     StreamEnded,\n@@ -23,7 +23,7 @@ from h2.exceptions import FrameTooLargeError, H2Error\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n from twisted.internet.interfaces import IHandshakeListener, IProtocolNegotiationFactory\n-from twisted.internet.protocol import connectionDone, Factory, Protocol\n+from twisted.internet.protocol import Factory, Protocol, connectionDone\n from twisted.internet.ssl import Certificate\n from twisted.protocols.policies import TimeoutMixin\n from twisted.python.failure import Failure\n@@ -35,7 +35,6 @@ from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,13 +1,13 @@\n import logging\n from enum import Enum\n from io import BytesIO\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple\n from urllib.parse import urlparse\n-from typing import Dict, List, Optional, Tuple, TYPE_CHECKING\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n from hpack import HeaderTuple\n-from twisted.internet.defer import Deferred, CancelledError\n+from twisted.internet.defer import CancelledError, Deferred\n from twisted.internet.error import ConnectionClosed\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n\n@@ -12,7 +12,6 @@ from scrapy.spiders import Spider\n from scrapy.utils.job import job_dir\n from scrapy.utils.misc import create_instance, load_object\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -5,6 +5,7 @@ from __future__ import annotations\n import logging\n from collections import deque\n from typing import (\n+    TYPE_CHECKING,\n     Any,\n     AsyncGenerator,\n     AsyncIterable,\n@@ -13,7 +14,6 @@ from typing import (\n     Iterable,\n     Optional,\n     Set,\n-    TYPE_CHECKING,\n     Tuple,\n     Union,\n )\n@@ -22,7 +22,7 @@ from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n \n-from scrapy import signals, Spider\n+from scrapy import Spider, signals\n from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy.http import Request, Response\n@@ -34,12 +34,10 @@ from scrapy.utils.defer import (\n     parallel,\n     parallel_async,\n )\n-\n from scrapy.utils.log import failure_to_exc_info, logformatter_adapter\n from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n \n-\n if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n\n@@ -28,14 +28,13 @@ from scrapy.middleware import MiddlewareManager\n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import (\n-    mustbe_deferred,\n-    deferred_from_coro,\n     deferred_f_from_coro_f,\n+    deferred_from_coro,\n     maybe_deferred_to_future,\n+    mustbe_deferred,\n )\n from scrapy.utils.python import MutableAsyncChain, MutableChain\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -17,20 +17,20 @@ except ImportError:\n \n from zope.interface.verify import verifyClass\n \n-from scrapy import signals, Spider\n+from scrapy import Spider, signals\n from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.extension import ExtensionManager\n from scrapy.interfaces import ISpiderLoader\n-from scrapy.settings import overridden_settings, Settings\n+from scrapy.settings import Settings, overridden_settings\n from scrapy.signalmanager import SignalManager\n from scrapy.utils.log import (\n+    LogCounterHandler,\n     configure_logging,\n     get_scrapy_root_handler,\n     install_scrapy_root_handler,\n     log_reactor_info,\n     log_scrapy_info,\n-    LogCounterHandler,\n )\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n\n@@ -1,12 +1,11 @@\n-import re\n import logging\n+import re\n \n from w3lib import html\n \n from scrapy.exceptions import NotConfigured\n from scrapy.http import HtmlResponse\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -14,7 +14,6 @@ from warnings import warn\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.responsetypes import responsetypes\n \n-\n warn(\n     \"scrapy.downloadermiddlewares.decompression is deprecated\",\n     ScrapyDeprecationWarning,\n\n@@ -23,7 +23,6 @@ from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.misc import load_object\n \n-\n HttpCacheMiddlewareTV = TypeVar(\"HttpCacheMiddlewareTV\", bound=\"HttpCacheMiddleware\")\n \n \n\n@@ -1,6 +1,6 @@\n import base64\n from urllib.parse import unquote, urlunparse\n-from urllib.request import getproxies, proxy_bypass, _parse_proxy\n+from urllib.request import _parse_proxy, getproxies, proxy_bypass\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n\n@@ -3,10 +3,10 @@ from urllib.parse import urljoin, urlparse\n \n from w3lib.url import safe_url_string\n \n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import HtmlResponse\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.response import get_meta_refresh\n-from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n logger = logging.getLogger(__name__)\n \n\n@@ -9,7 +9,7 @@ RETRY_HTTP_CODES - which HTTP response codes to retry\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n-from logging import getLogger, Logger\n+from logging import Logger, getLogger\n from typing import Optional, Union\n \n from twisted.internet import defer\n@@ -31,7 +31,6 @@ from scrapy.spiders import Spider\n from scrapy.utils.python import global_object_name\n from scrapy.utils.response import response_status_message\n \n-\n retry_logger = getLogger(__name__)\n \n \n\n@@ -7,7 +7,8 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n import logging\n \n from twisted.internet.defer import Deferred, maybeDeferred\n-from scrapy.exceptions import NotConfigured, IgnoreRequest\n+\n+from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n\n@@ -10,8 +10,7 @@ from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n-from scrapy.utils.request import referer_str, RequestFingerprinter\n-\n+from scrapy.utils.request import RequestFingerprinter, referer_str\n \n BaseDupeFilterTV = TypeVar(\"BaseDupeFilterTV\", bound=\"BaseDupeFilter\")\n \n\n@@ -11,14 +11,13 @@ import warnings\n from collections.abc import Mapping\n from xml.sax.saxutils import XMLGenerator\n \n-from itemadapter import is_item, ItemAdapter\n+from itemadapter import ItemAdapter, is_item\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import Item\n from scrapy.utils.python import is_listlike, to_bytes, to_unicode\n from scrapy.utils.serialize import ScrapyJSONEncoder\n \n-\n __all__ = [\n     \"BaseItemExporter\",\n     \"PprintItemExporter\",\n\n@@ -4,11 +4,11 @@ Extensions for debugging Scrapy\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n-import sys\n-import signal\n import logging\n-import traceback\n+import signal\n+import sys\n import threading\n+import traceback\n from pdb import Pdb\n \n from scrapy.utils.engine import format_engine_status\n\n@@ -16,9 +16,9 @@ from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n-from zope.interface import implementer, Interface\n+from zope.interface import Interface, implementer\n \n-from scrapy import signals, Spider\n+from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.extensions.postprocessing import PostProcessingManager\n from scrapy.utils.boto import is_botocore_available\n@@ -28,7 +28,6 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import get_func_args, without_none_values\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -7,7 +7,7 @@ from pathlib import Path\n from time import time\n from weakref import WeakKeyDictionary\n \n-from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n+from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n \n from scrapy.http import Headers, Response\n from scrapy.http.request import Request\n\n@@ -2,8 +2,8 @@ import logging\n \n from twisted.internet import task\n \n-from scrapy.exceptions import NotConfigured\n from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n \n logger = logging.getLogger(__name__)\n \n\n@@ -3,11 +3,11 @@ MemoryUsage extension\n \n See documentation in docs/topics/extensions.rst\n \"\"\"\n-import sys\n-import socket\n import logging\n-from pprint import pformat\n+import socket\n+import sys\n from importlib import import_module\n+from pprint import pformat\n \n from twisted.internet import task\n \n\n@@ -5,8 +5,8 @@ Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \"\"\"\n \n from scrapy import signals\n-from scrapy.mail import MailSender\n from scrapy.exceptions import NotConfigured\n+from scrapy.mail import MailSender\n \n \n class StatsMailer:\n\n@@ -4,11 +4,11 @@ Scrapy Telnet Console extension\n See documentation in docs/topics/telnetconsole.rst\n \"\"\"\n \n-import pprint\n-import logging\n-import traceback\n import binascii\n+import logging\n import os\n+import pprint\n+import traceback\n \n from twisted.internet import protocol\n \n@@ -21,12 +21,12 @@ except (ImportError, SyntaxError):\n     _TWISTED_CONCH_TRACEBACK = traceback.format_exc()\n     TWISTED_CONCH_AVAILABLE = False\n \n-from scrapy.exceptions import NotConfigured\n from scrapy import signals\n-from scrapy.utils.trackref import print_live_refs\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.decorators import defers\n from scrapy.utils.engine import print_engine_status\n from scrapy.utils.reactor import listen_tcp\n-from scrapy.utils.decorators import defers\n+from scrapy.utils.trackref import print_live_refs\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,7 +1,7 @@\n import logging\n \n-from scrapy.exceptions import NotConfigured\n from scrapy import signals\n+from scrapy.exceptions import NotConfigured\n \n logger = logging.getLogger(__name__)\n \n\n@@ -6,13 +6,11 @@ Request and Response outside this module.\n \"\"\"\n \n from scrapy.http.headers import Headers\n-\n from scrapy.http.request import Request\n from scrapy.http.request.form import FormRequest\n-from scrapy.http.request.rpc import XmlRpcRequest\n from scrapy.http.request.json_request import JsonRequest\n-\n+from scrapy.http.request.rpc import XmlRpcRequest\n from scrapy.http.response import Response\n from scrapy.http.response.html import HtmlResponse\n-from scrapy.http.response.xml import XmlResponse\n from scrapy.http.response.text import TextResponse\n+from scrapy.http.response.xml import XmlResponse\n\n@@ -1,11 +1,11 @@\n import re\n import time\n-from http.cookiejar import CookieJar as _CookieJar, DefaultCookiePolicy\n+from http.cookiejar import CookieJar as _CookieJar\n+from http.cookiejar import DefaultCookiePolicy\n \n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n-\n # Defined in the http.cookiejar module, but undocumented:\n # https://github.com/python/cpython/blob/v3.9.0/Lib/http/cookiejar.py#L527\n IPV4_RE = re.compile(r\"\\.\\d+$\", re.ASCII)\n\n@@ -1,6 +1,7 @@\n from collections.abc import Mapping\n \n from w3lib.http import headers_dict_to_raw\n+\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.python import to_unicode\n \n\n@@ -17,7 +17,6 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n \n-\n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n \n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union\n-from urllib.parse import urljoin, urlencode, urlsplit, urlunsplit\n+from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n from lxml.html import FormElement, HtmlElement, HTMLParser, SelectElement\n from parsel.selector import create_root_node\n@@ -14,10 +14,9 @@ from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.request import Request\n from scrapy.http.response.text import TextResponse\n-from scrapy.utils.python import to_bytes, is_listlike\n+from scrapy.utils.python import is_listlike, to_bytes\n from scrapy.utils.response import get_base_url\n \n-\n FormRequestTypeVar = TypeVar(\"FormRequestTypeVar\", bound=\"FormRequest\")\n \n FormdataType = Optional[Union[dict, List[Tuple[str, str]]]]\n\n@@ -10,7 +10,6 @@ from typing import Optional\n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n \n-\n DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n \n\n@@ -15,8 +15,8 @@ from w3lib.encoding import (\n     html_body_declared_encoding,\n     html_to_unicode,\n     http_content_type_encoding,\n-    resolve_encoding,\n     read_bom,\n+    resolve_encoding,\n )\n from w3lib.html import strip_html5_whitespace\n \n\n@@ -7,7 +7,6 @@ from itemloaders import processors\n \n from scrapy.utils.deprecate import create_deprecated_class\n \n-\n MapCompose = create_deprecated_class(\"MapCompose\", processors.MapCompose)\n \n Compose = create_deprecated_class(\"Compose\", processors.Compose)\n\n@@ -1,5 +1,5 @@\n-import os\n import logging\n+import os\n \n from twisted.python.failure import Failure\n \n\n@@ -12,14 +12,13 @@ from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n \n-from twisted.python.versions import Version\n-from twisted.internet import defer, ssl\n from twisted import version as twisted_version\n+from twisted.internet import defer, ssl\n+from twisted.python.versions import Version\n \n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -8,8 +8,8 @@ from twisted.internet.defer import Deferred\n from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n from scrapy.settings import Settings\n+from scrapy.utils.defer import process_chain, process_parallel\n from scrapy.utils.misc import create_instance, load_object\n-from scrapy.utils.defer import process_parallel, process_chain\n \n logger = logging.getLogger(__name__)\n \n\n@@ -32,7 +32,6 @@ from scrapy.utils.misc import md5sum\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -9,10 +9,10 @@ from twisted.python.failure import Failure\n \n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n-from scrapy.utils.defer import mustbe_deferred, defer_result\n+from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.misc import arg_to_iter\n \n logger = logging.getLogger(__name__)\n \n\n@@ -3,7 +3,6 @@ import logging\n \n from scrapy.utils.misc import create_instance\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,8 +1,8 @@\n from twisted.internet import defer\n from twisted.internet.base import ThreadedResolver\n from twisted.internet.interfaces import (\n-    IHostResolution,\n     IHostnameResolver,\n+    IHostResolution,\n     IResolutionReceiver,\n     IResolverSimple,\n )\n@@ -10,7 +10,6 @@ from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n \n-\n # TODO: cache misses\n dnscache = LocalCache(10000)\n \n\n@@ -2,9 +2,9 @@\n This module implements a class which returns the appropriate Response class\n based on different criteria.\n \"\"\"\n+from io import StringIO\n from mimetypes import MimeTypes\n from pkgutil import get_data\n-from io import StringIO\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n\n@@ -1,5 +1,5 @@\n-import sys\n import logging\n+import sys\n from abc import ABCMeta, abstractmethod\n \n from scrapy.utils.python import to_unicode\n\n@@ -3,10 +3,10 @@ XPath selectors based on lxml\n \"\"\"\n \n from parsel import Selector as _ParselSelector\n-from scrapy.utils.trackref import object_ref\n-from scrapy.utils.python import to_bytes\n-from scrapy.http import HtmlResponse, XmlResponse\n \n+from scrapy.http import HtmlResponse, XmlResponse\n+from scrapy.utils.python import to_bytes\n+from scrapy.utils.trackref import object_ref\n \n __all__ = [\"Selector\", \"SelectorList\"]\n \n\n@@ -1,12 +1,11 @@\n-import json\n import copy\n+import json\n from collections.abc import MutableMapping\n from importlib import import_module\n from pprint import pformat\n \n from scrapy.settings import default_settings\n \n-\n SETTINGS_PRIORITIES = {\n     \"default\": 0,\n     \"command\": 10,\n\n@@ -7,7 +7,7 @@ import os\n import signal\n \n from itemadapter import is_item\n-from twisted.internet import threads, defer\n+from twisted.internet import defer, threads\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n@@ -20,11 +20,8 @@ from scrapy.utils.conf import get_config\n from scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.misc import load_object\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\n from scrapy.utils.response import open_in_browser\n-from scrapy.utils.reactor import (\n-    is_asyncio_reactor_installed,\n-    set_asyncio_event_loop,\n-)\n \n \n class Shell:\n\n@@ -1,4 +1,5 @@\n from pydispatch import dispatcher\n+\n from scrapy.utils import signal as _signal\n \n \n\n@@ -3,8 +3,8 @@ Offsite Spider Middleware\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n-import re\n import logging\n+import re\n import warnings\n \n from scrapy import signals\n\n@@ -15,7 +15,6 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n \n-\n LOCAL_SCHEMES = (\n     \"about\",\n     \"blob\",\n\n@@ -6,8 +6,8 @@ See documentation in docs/topics/spider-middleware.rst\n \n import logging\n \n-from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n+from scrapy.http import Request\n \n logger = logging.getLogger(__name__)\n \n\n@@ -97,5 +97,5 @@ class Spider(object_ref):\n \n # Top-level imports\n from scrapy.spiders.crawl import CrawlSpider, Rule\n-from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider\n+from scrapy.spiders.feed import CSVFeedSpider, XMLFeedSpider\n from scrapy.spiders.sitemap import SitemapSpider\n\n@@ -8,7 +8,7 @@ See documentation in docs/topics/spiders.rst\n import copy\n from typing import AsyncIterable, Awaitable, Sequence\n \n-from scrapy.http import Request, Response, HtmlResponse\n+from scrapy.http import HtmlResponse, Request, Response\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n from scrapy.utils.asyncgen import collect_asyncgen\n\n@@ -4,11 +4,11 @@ for scraping from an XML feed.\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n-from scrapy.spiders import Spider\n-from scrapy.utils.iterators import xmliter, csviter\n-from scrapy.utils.spider import iterate_spider_output\n-from scrapy.selector import Selector\n from scrapy.exceptions import NotConfigured, NotSupported\n+from scrapy.selector import Selector\n+from scrapy.spiders import Spider\n+from scrapy.utils.iterators import csviter, xmliter\n+from scrapy.utils.spider import iterate_spider_output\n \n \n class XMLFeedSpider(Spider):\n\n@@ -1,11 +1,10 @@\n-import re\n import logging\n+import re\n \n-from scrapy.spiders import Spider\n from scrapy.http import Request, XmlResponse\n-from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n+from scrapy.spiders import Spider\n from scrapy.utils.gz import gunzip, gzip_magic_number\n-\n+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,8 +1,8 @@\n \"\"\"\n Scrapy extension for collecting scraping stats\n \"\"\"\n-import pprint\n import logging\n+import pprint\n \n logger = logging.getLogger(__name__)\n \n\n@@ -1,8 +1,8 @@\n import random\n from urllib.parse import urlencode\n \n-from twisted.web.server import Site\n from twisted.web.resource import Resource\n+from twisted.web.server import Site\n \n \n class Root(Resource):\n\n@@ -8,7 +8,6 @@ from pathlib import Path\n from typing import Any, Dict, List, Optional, Union\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n-\n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n\n@@ -1,7 +1,7 @@\n import argparse\n import warnings\n-from shlex import split\n from http.cookies import SimpleCookie\n+from shlex import split\n from urllib.parse import urlparse\n \n from w3lib.http import basic_auth_header\n\n@@ -26,7 +26,7 @@ from twisted.python import failure\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import is_asyncio_reactor_installed, _get_asyncio_event_loop\n+from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n\n@@ -1,8 +1,9 @@\n \"\"\"Some helpers for deprecation messages\"\"\"\n \n-import warnings\n import inspect\n+import warnings\n from typing import List, Tuple\n+\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n\n@@ -6,6 +6,7 @@ import ctypes\n import platform\n import sys\n from pprint import pformat as pformat_\n+\n from packaging.version import Version as parse_version\n \n \n\n@@ -1,5 +1,5 @@\n import posixpath\n-from ftplib import error_perm, FTP\n+from ftplib import FTP, error_perm\n from posixpath import dirname\n \n \n\n@@ -1,12 +1,11 @@\n \"\"\"Helper functions for scrapy.http objects (Request, Response)\"\"\"\n \n from typing import Union\n-from urllib.parse import urlparse, ParseResult\n+from urllib.parse import ParseResult, urlparse\n from weakref import WeakKeyDictionary\n \n from scrapy.http import Request, Response\n \n-\n _urlparse_cache: \"WeakKeyDictionary[Union[Request, Response], ParseResult]\" = (\n     WeakKeyDictionary()\n )\n\n@@ -3,11 +3,10 @@ import logging\n import re\n from io import StringIO\n \n-from scrapy.http import TextResponse, Response\n+from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -11,7 +11,6 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.settings import Settings\n from scrapy.utils.versions import scrapy_components_versions\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,23 +1,22 @@\n \"\"\"Helper functions which don't fit anywhere else\"\"\"\n import ast\n+import hashlib\n import inspect\n import os\n import re\n-import hashlib\n import warnings\n from collections import deque\n from contextlib import contextmanager\n+from functools import partial\n from importlib import import_module\n from pkgutil import iter_modules\n-from functools import partial\n \n from w3lib.html import replace_entities\n \n-from scrapy.utils.datatypes import LocalWeakReferencedCache\n-from scrapy.utils.python import flatten, to_unicode\n from scrapy.item import Item\n+from scrapy.utils.datatypes import LocalWeakReferencedCache\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-\n+from scrapy.utils.python import flatten, to_unicode\n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n \n\n@@ -1,6 +1,5 @@\n import signal\n \n-\n signal_names = {}\n for signame in dir(signal):\n     if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n\n@@ -1,13 +1,11 @@\n import os\n import warnings\n-\n from importlib import import_module\n from pathlib import Path\n \n-from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n-from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n-\n+from scrapy.settings import Settings\n+from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n \n ENVVAR = \"SCRAPY_SETTINGS_MODULE\"\n DATADIR_CFG_SECTION = \"datadir\"\n\n@@ -5,7 +5,6 @@ import scrapy\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.request import request_from_dict as _from_dict\n \n-\n warnings.warn(\n     (\n         \"Module scrapy.utils.reqser is deprecated, please use request.to_dict method\"\n\n@@ -19,7 +19,6 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n-\n _deprecated_fingerprint_cache: \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\"\n _deprecated_fingerprint_cache = WeakKeyDictionary()\n \n\n@@ -11,12 +11,11 @@ from weakref import WeakKeyDictionary\n \n from twisted.web import http\n from w3lib import html\n+\n import scrapy\n from scrapy.http.response import Response\n-\n-\n-from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.utils.decorators import deprecated\n+from scrapy.utils.python import to_bytes, to_unicode\n \n _baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()\n \n\n@@ -1,8 +1,8 @@\n-import json\n import datetime\n import decimal\n+import json\n \n-from itemadapter import is_item, ItemAdapter\n+from itemadapter import ItemAdapter, is_item\n from twisted.internet import defer\n \n from scrapy.http import Request, Response\n\n@@ -2,9 +2,6 @@\n import collections.abc\n import logging\n \n-from twisted.internet.defer import DeferredList, Deferred\n-from twisted.python.failure import Failure\n-\n from pydispatch.dispatcher import (\n     Anonymous,\n     Any,\n@@ -13,12 +10,13 @@ from pydispatch.dispatcher import (\n     liveReceivers,\n )\n from pydispatch.robustapply import robustApply\n+from twisted.internet.defer import Deferred, DeferredList\n+from twisted.python.failure import Failure\n \n from scrapy.exceptions import StopDownload\n from scrapy.utils.defer import maybeDeferred_coro\n from scrapy.utils.log import failure_to_exc_info\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -5,7 +5,6 @@ from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n \n-\n logger = logging.getLogger(__name__)\n \n \n\n@@ -1,5 +1,5 @@\n-import OpenSSL.SSL\n import OpenSSL._util as pyOpenSSLutil\n+import OpenSSL.SSL\n \n from scrapy.utils.python import to_unicode\n \n\n@@ -1,8 +1,8 @@\n \"\"\"Helper functions for working with templates\"\"\"\n \n-from os import PathLike\n import re\n import string\n+from os import PathLike\n from pathlib import Path\n from typing import Union\n \n\n@@ -4,11 +4,11 @@ This module contains some assorted functions used in tests\n \n import asyncio\n import os\n+from importlib import import_module\n from pathlib import Path\n from posixpath import split\n from unittest import mock\n \n-from importlib import import_module\n from twisted.trial.unittest import SkipTest\n \n from scrapy.utils.boto import is_botocore_available\n@@ -109,7 +109,7 @@ def mock_google_cloud_storage():\n     \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n     classes and set their proper return values.\n     \"\"\"\n-    from google.cloud.storage import Client, Bucket, Blob\n+    from google.cloud.storage import Blob, Bucket, Client\n \n     client_mock = mock.create_autospec(Client)\n \n\n@@ -1,5 +1,5 @@\n-import sys\n import os\n+import sys\n \n from twisted.internet import defer, protocol\n \n\n@@ -1,6 +1,6 @@\n from urllib.parse import urljoin\n \n-from twisted.web import server, resource, static, util\n+from twisted.web import resource, server, static, util\n \n \n class SiteTest:\n\n@@ -15,7 +15,6 @@ from time import time\n from typing import DefaultDict\n from weakref import WeakKeyDictionary\n \n-\n NoneType = type(None)\n live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n \n\n@@ -12,6 +12,7 @@ from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n # move doesn't break old code\n from w3lib.url import *\n from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n+\n from scrapy.utils.python import to_unicode\n \n \n\n@@ -1,7 +1,8 @@\n from pathlib import Path\n-from pkg_resources import parse_version\n-from setuptools import setup, find_packages, __version__ as setuptools_version\n \n+from pkg_resources import parse_version\n+from setuptools import __version__ as setuptools_version\n+from setuptools import find_packages, setup\n \n version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n \n\n@@ -1,9 +1,8 @@\n import asyncio\n import sys\n \n-from uvloop import Loop\n-\n from twisted.internet import asyncioreactor\n+from uvloop import Loop\n \n if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n\n@@ -1,5 +1,6 @@\n from twisted.internet.main import installReactor\n from twisted.internet.selectreactor import SelectReactor\n+\n import scrapy\n from scrapy.crawler import CrawlerProcess\n \n\n@@ -1,14 +1,16 @@\n from urllib.parse import urlparse\n \n from twisted.internet import reactor\n-from twisted.names import cache, hosts as hostsModule, resolve\n+from twisted.names import cache\n+from twisted.names import hosts as hostsModule\n+from twisted.names import resolve\n from twisted.names.client import Resolver\n from twisted.python.runtime import platform\n \n-from scrapy import Spider, Request\n+from scrapy import Request, Spider\n from scrapy.crawler import CrawlerRunner\n from scrapy.utils.log import configure_logging\n-from tests.mockserver import MockServer, MockDNSServer\n+from tests.mockserver import MockDNSServer, MockServer\n \n \n # https://stackoverflow.com/a/32784190\n\n@@ -14,8 +14,8 @@ from cryptography.x509 import (\n     DNSName,\n     Name,\n     NameAttribute,\n-    random_serial_number,\n     SubjectAlternativeName,\n+    random_serial_number,\n )\n from cryptography.x509.oid import NameOID\n \n\n@@ -4,7 +4,7 @@ import random\n import sys\n from pathlib import Path\n from shutil import rmtree\n-from subprocess import Popen, PIPE\n+from subprocess import PIPE, Popen\n from tempfile import mkdtemp\n from urllib.parse import urlencode\n \n@@ -14,7 +14,7 @@ from twisted.internet.task import deferLater\n from twisted.names import dns, error\n from twisted.names.server import DNSServerFactory\n from twisted.web import resource, server\n-from twisted.web.server import GzipEncoderFactory, NOT_DONE_YET, Site\n+from twisted.web.server import NOT_DONE_YET, GzipEncoderFactory, Site\n from twisted.web.static import File\n from twisted.web.util import redirectTo\n \n\n@@ -1,8 +1,9 @@\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n+\n from scrapy.utils.test import get_crawler\n-from tests.spiders import FollowAllSpider, ItemSpider, ErrorSpider\n from tests.mockserver import MockServer\n+from tests.spiders import ErrorSpider, FollowAllSpider, ItemSpider\n \n \n class TestCloseSpider(TestCase):\n\n@@ -6,7 +6,7 @@ import tempfile\n import unittest\n from io import StringIO\n from pathlib import Path\n-from subprocess import Popen, PIPE\n+from subprocess import PIPE, Popen\n \n from scrapy.utils.test import get_testenv\n \n\n@@ -1,7 +1,7 @@\n import sys\n import unittest\n from pathlib import Path\n-from subprocess import Popen, PIPE\n+from subprocess import PIPE, Popen\n \n \n class CmdlineCrawlPipelineTest(unittest.TestCase):\n\n@@ -1,8 +1,8 @@\n-from twisted.trial import unittest\n from twisted.internet import defer\n+from twisted.trial import unittest\n \n-from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n+from scrapy.utils.testsite import SiteTest\n \n \n class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n\n@@ -1,13 +1,14 @@\n-import os\n import argparse\n+import os\n from pathlib import Path\n \n from twisted.internet import defer\n+\n from scrapy.commands import parse\n from scrapy.settings import Settings\n-from scrapy.utils.testsite import SiteTest\n-from scrapy.utils.testproc import ProcessTest\n from scrapy.utils.python import to_unicode\n+from scrapy.utils.testproc import ProcessTest\n+from scrapy.utils.testsite import SiteTest\n from tests.test_commands import CommandTest\n \n \n\n@@ -1,12 +1,11 @@\n from pathlib import Path\n \n-from twisted.trial import unittest\n from twisted.internet import defer\n+from twisted.trial import unittest\n \n-from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n-\n-from tests import tests_datadir, NON_EXISTING_RESOLVABLE\n+from scrapy.utils.testsite import SiteTest\n+from tests import NON_EXISTING_RESOLVABLE, tests_datadir\n \n \n class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n\n@@ -1,6 +1,7 @@\n import sys\n-from twisted.trial import unittest\n+\n from twisted.internet import defer\n+from twisted.trial import unittest\n \n import scrapy\n from scrapy.utils.testproc import ProcessTest\n\n@@ -1,6 +1,6 @@\n+import argparse\n import inspect\n import json\n-import argparse\n import os\n import platform\n import re\n@@ -10,7 +10,7 @@ import tempfile\n from contextlib import contextmanager\n from itertools import chain\n from pathlib import Path\n-from shutil import rmtree, copytree\n+from shutil import copytree, rmtree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import mkdtemp\n from threading import Timer\n@@ -23,12 +23,11 @@ from twisted.python.versions import Version\n from twisted.trial import unittest\n \n import scrapy\n-from scrapy.commands import view, ScrapyCommand, ScrapyHelpFormatter\n+from scrapy.commands import ScrapyCommand, ScrapyHelpFormatter, view\n from scrapy.commands.startproject import IGNORE\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n-\n from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n\n@@ -5,18 +5,18 @@ from twisted.python import failure\n from twisted.trial import unittest\n \n from scrapy import FormRequest\n-from scrapy.spidermiddlewares.httperror import HttpError\n-from scrapy.spiders import Spider\n-from scrapy.http import Request\n-from scrapy.item import Item, Field\n-from scrapy.utils.test import get_crawler\n-from scrapy.contracts import ContractsManager, Contract\n+from scrapy.contracts import Contract, ContractsManager\n from scrapy.contracts.default import (\n-    UrlContract,\n     CallbackKeywordArgumentsContract,\n     ReturnsContract,\n     ScrapesContract,\n+    UrlContract,\n )\n+from scrapy.http import Request\n+from scrapy.item import Field, Item\n+from scrapy.spidermiddlewares.httperror import HttpError\n+from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n\n@@ -1,9 +1,9 @@\n import json\n import logging\n+import unittest\n from ipaddress import IPv4Address\n from socket import gethostbyname\n from urllib.parse import urlparse\n-import unittest\n \n from pytest import mark\n from testfixtures import LogCapture\n\n@@ -5,28 +5,25 @@ import sys\n import warnings\n from pathlib import Path\n \n-from pytest import raises, mark\n+from pkg_resources import parse_version\n+from pytest import mark, raises\n from twisted import version as twisted_version\n from twisted.internet import defer\n from twisted.python.versions import Version\n from twisted.trial import unittest\n-\n-from pkg_resources import parse_version\n from w3lib import __version__ as w3lib_version\n \n import scrapy\n-from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n+from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.extensions import telnet\n+from scrapy.extensions.throttle import AutoThrottle\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n-from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n-from scrapy.utils.test import get_crawler\n-from scrapy.extensions.throttle import AutoThrottle\n-from scrapy.extensions import telnet\n-from scrapy.utils.test import get_testenv\n-\n+from scrapy.utils.spider import DefaultSpider\n+from scrapy.utils.test import get_crawler, get_testenv\n from tests.mockserver import MockServer\n \n \n\n@@ -5,7 +5,7 @@ import sys\n import tempfile\n from pathlib import Path\n from typing import Optional, Type\n-from unittest import mock, SkipTest\n+from unittest import SkipTest, mock\n \n from testfixtures import LogCapture\n from twisted.cred import checkers, credentials, portal\n@@ -1041,7 +1041,8 @@ class BaseFTPTestCase(unittest.TestCase):\n     )\n \n     def setUp(self):\n-        from twisted.protocols.ftp import FTPRealm, FTPFactory\n+        from twisted.protocols.ftp import FTPFactory, FTPRealm\n+\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n         # setup dirs and test file\n@@ -1190,7 +1191,8 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n     req_meta = {}\n \n     def setUp(self):\n-        from twisted.protocols.ftp import FTPRealm, FTPFactory\n+        from twisted.protocols.ftp import FTPFactory, FTPRealm\n+\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n         # setup dir and test file\n\n@@ -15,10 +15,10 @@ from scrapy.utils.misc import create_instance\n from scrapy.utils.test import get_crawler\n from tests.mockserver import ssl_context_factory\n from tests.test_downloader_handlers import (\n-    Https11TestCase,\n-    Https11CustomCiphers,\n     Http11MockServerTestCase,\n     Http11ProxyTestCase,\n+    Https11CustomCiphers,\n+    Https11TestCase,\n     UriResource,\n )\n \n\n@@ -4,15 +4,15 @@ from unittest import mock\n from pytest import mark\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n-from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n+from twisted.trial.unittest import TestCase\n \n+from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n+from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n-from scrapy.exceptions import _InvalidOutput\n-from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n-from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n from scrapy.utils.python import to_bytes\n+from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n \n \n class ManagerTestCase(TestCase):\n\n@@ -1,11 +1,10 @@\n import unittest\n \n from scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware\n+from scrapy.http import HtmlResponse, Request, Response\n from scrapy.spiders import Spider\n-from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n-\n __doctests__ = [\"scrapy.downloadermiddlewares.ajaxcrawl\"]\n \n \n\n@@ -1,14 +1,14 @@\n import logging\n from unittest import TestCase\n-from testfixtures import LogCapture\n \n import pytest\n+from testfixtures import LogCapture\n \n from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response, Request\n+from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n\n@@ -1,6 +1,7 @@\n from unittest import TestCase, main\n-from scrapy.http import Response, XmlResponse\n+\n from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n+from scrapy.http import Response, XmlResponse\n from scrapy.spiders import Spider\n from scrapy.utils.test import assert_samelines\n from tests import get_testdata\n\n@@ -3,8 +3,8 @@ from unittest import TestCase\n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n from scrapy.spiders import Spider\n-from scrapy.utils.test import get_crawler\n from scrapy.utils.python import to_bytes\n+from scrapy.utils.test import get_crawler\n \n \n class TestDefaultHeadersMiddleware(TestCase):\n\n@@ -1,8 +1,8 @@\n import unittest\n \n from scrapy.downloadermiddlewares.downloadtimeout import DownloadTimeoutMiddleware\n-from scrapy.spiders import Spider\n from scrapy.http import Request\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -3,9 +3,9 @@ import unittest\n import pytest\n from w3lib.http import basic_auth_header\n \n+from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n-from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n from scrapy.spiders import Spider\n \n \n\n@@ -1,16 +1,16 @@\n-import time\n-import tempfile\n-import shutil\n-import unittest\n import email.utils\n+import shutil\n+import tempfile\n+import time\n+import unittest\n from contextlib import contextmanager\n \n-from scrapy.http import Response, HtmlResponse, Request\n-from scrapy.spiders import Spider\n-from scrapy.settings import Settings\n-from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.test import get_crawler\n from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\n+from scrapy.exceptions import IgnoreRequest\n+from scrapy.http import HtmlResponse, Request, Response\n+from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n \n \n class _BaseTest(unittest.TestCase):\n\n@@ -1,18 +1,19 @@\n from gzip import GzipFile\n from io import BytesIO\n from pathlib import Path\n-from unittest import TestCase, SkipTest\n+from unittest import SkipTest, TestCase\n from warnings import catch_warnings\n \n from w3lib.encoding import resolve_encoding\n-from scrapy.spiders import Spider\n-from scrapy.http import Response, Request, HtmlResponse\n+\n from scrapy.downloadermiddlewares.httpcompression import (\n-    HttpCompressionMiddleware,\n     ACCEPTED_ENCODINGS,\n+    HttpCompressionMiddleware,\n )\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.http import HtmlResponse, Request, Response\n from scrapy.responsetypes import responsetypes\n+from scrapy.spiders import Spider\n from scrapy.utils.gz import gunzip\n from scrapy.utils.test import get_crawler\n from tests import tests_datadir\n\n@@ -1,12 +1,12 @@\n import unittest\n \n from scrapy.downloadermiddlewares.redirect import (\n-    RedirectMiddleware,\n     MetaRefreshMiddleware,\n+    RedirectMiddleware,\n )\n-from scrapy.spiders import Spider\n from scrapy.exceptions import IgnoreRequest\n-from scrapy.http import Request, Response, HtmlResponse\n+from scrapy.http import HtmlResponse, Request, Response\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -12,7 +12,7 @@ from twisted.internet.error import (\n )\n from twisted.web.client import ResponseFailed\n \n-from scrapy.downloadermiddlewares.retry import get_retry_request, RetryMiddleware\n+from scrapy.downloadermiddlewares.retry import RetryMiddleware, get_retry_request\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n\n@@ -1,17 +1,16 @@\n from unittest import mock\n \n-from twisted.internet import reactor, error\n+from twisted.internet import error, reactor\n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n from twisted.python import failure\n from twisted.trial import unittest\n-from scrapy.downloadermiddlewares.robotstxt import (\n-    RobotsTxtMiddleware,\n-    logger as mw_module_logger,\n-)\n+\n+from scrapy.downloadermiddlewares.robotstxt import RobotsTxtMiddleware\n+from scrapy.downloadermiddlewares.robotstxt import logger as mw_module_logger\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request, Response, TextResponse\n from scrapy.settings import Settings\n-from tests.test_robotstxt_interface import rerp_available, reppy_available\n+from tests.test_robotstxt_interface import reppy_available, rerp_available\n \n \n class RobotsTxtMiddlewareTest(unittest.TestCase):\n\n@@ -1,8 +1,8 @@\n from unittest import TestCase\n \n-from scrapy.spiders import Spider\n-from scrapy.http import Request\n from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n+from scrapy.http import Request\n+from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -1,14 +1,15 @@\n import hashlib\n-import tempfile\n-import unittest\n import shutil\n import sys\n+import tempfile\n+import unittest\n from pathlib import Path\n+\n from testfixtures import LogCapture\n \n+from scrapy.core.scheduler import Scheduler\n from scrapy.dupefilters import RFPDupeFilter\n from scrapy.http import Request\n-from scrapy.core.scheduler import Scheduler\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n from tests.spiders import SimpleSpider\n\n@@ -14,13 +14,13 @@ import re\n import subprocess\n import sys\n from collections import defaultdict\n+from dataclasses import dataclass\n from pathlib import Path\n from threading import Timer\n from urllib.parse import urlparse\n-from dataclasses import dataclass\n \n-import pytest\n import attr\n+import pytest\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n from twisted.internet import defer, reactor\n@@ -31,12 +31,11 @@ from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n from scrapy.exceptions import CloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Request\n-from scrapy.item import Item, Field\n+from scrapy.item import Field, Item\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n-\n from tests import get_testdata, tests_datadir\n \n \n\n@@ -2,14 +2,13 @@ from testfixtures import LogCapture\n from twisted.internet import defer\n \n from scrapy.exceptions import StopDownload\n-\n from tests.test_engine import (\n     AttrsItemsSpider,\n+    CrawlerRun,\n     DataClassItemsSpider,\n     DictItemsSpider,\n-    TestSpider,\n-    CrawlerRun,\n     EngineTest,\n+    TestSpider,\n )\n \n \n\n@@ -2,14 +2,13 @@ from testfixtures import LogCapture\n from twisted.internet import defer\n \n from scrapy.exceptions import StopDownload\n-\n from tests.test_engine import (\n     AttrsItemsSpider,\n+    CrawlerRun,\n     DataClassItemsSpider,\n     DictItemsSpider,\n-    TestSpider,\n-    CrawlerRun,\n     EngineTest,\n+    TestSpider,\n )\n \n \n\n@@ -1,31 +1,31 @@\n-import re\n+import dataclasses\n import json\n import marshal\n import pickle\n+import re\n import tempfile\n import unittest\n-import dataclasses\n-from io import BytesIO\n from datetime import datetime\n+from io import BytesIO\n from warnings import catch_warnings, filterwarnings\n \n import lxml.etree\n from itemadapter import ItemAdapter\n \n-from scrapy.item import Item, Field\n-from scrapy.utils.python import to_unicode\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.exporters import (\n     BaseItemExporter,\n-    PprintItemExporter,\n-    PickleItemExporter,\n     CsvItemExporter,\n-    XmlItemExporter,\n-    JsonLinesItemExporter,\n     JsonItemExporter,\n-    PythonItemExporter,\n+    JsonLinesItemExporter,\n     MarshalItemExporter,\n+    PickleItemExporter,\n+    PprintItemExporter,\n+    PythonItemExporter,\n+    XmlItemExporter,\n )\n+from scrapy.item import Field, Item\n+from scrapy.utils.python import to_unicode\n \n \n def custom_serializer(value):\n\n@@ -1,7 +1,7 @@\n-from twisted.trial import unittest\n from twisted.conch.telnet import ITelnetProtocol\n from twisted.cred import credentials\n from twisted.internet import defer\n+from twisted.trial import unittest\n \n from scrapy.extensions.telnet import TelnetConsole\n from scrapy.utils.test import get_crawler\n\n@@ -19,7 +19,7 @@ from pathlib import Path\n from string import ascii_letters, digits\n from typing import Union\n from unittest import mock\n-from urllib.parse import urljoin, quote\n+from urllib.parse import quote, urljoin\n from urllib.request import pathname2url\n \n import lxml.etree\n@@ -35,7 +35,6 @@ import scrapy\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n-    _FeedSlot,\n     BlockingFeedStorage,\n     FeedExporter,\n     FileFeedStorage,\n@@ -44,15 +43,11 @@ from scrapy.extensions.feedexport import (\n     IFeedStorage,\n     S3FeedStorage,\n     StdoutFeedStorage,\n+    _FeedSlot,\n )\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n-from scrapy.utils.test import (\n-    get_crawler,\n-    mock_google_cloud_storage,\n-    skip_if_no_boto,\n-)\n-\n+from scrapy.utils.test import get_crawler, mock_google_cloud_storage, skip_if_no_boto\n from tests.mockserver import MockFTPServer, MockServer\n from tests.spiders import ItemSpider\n \n\n@@ -17,18 +17,19 @@ from twisted.internet.defer import (\n )\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n from twisted.internet.error import TimeoutError\n-from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n+from twisted.internet.ssl import Certificate, PrivateCertificate, optionsForClientTLS\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n-from twisted.web.client import ResponseFailed, URI\n-from twisted.web.http import H2_ENABLED, Request as TxRequest\n-from twisted.web.server import Site, NOT_DONE_YET\n+from twisted.web.client import URI, ResponseFailed\n+from twisted.web.http import H2_ENABLED\n+from twisted.web.http import Request as TxRequest\n+from twisted.web.server import NOT_DONE_YET, Site\n from twisted.web.static import File\n \n-from scrapy.http import Request, Response, JsonRequest\n+from scrapy.http import JsonRequest, Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n-from tests.mockserver import ssl_context_factory, LeafResource, Status\n+from tests.mockserver import LeafResource, Status, ssl_context_factory\n \n \n def generate_random_string(size):\n\n@@ -1,5 +1,5 @@\n-from urllib.parse import urlparse\n from unittest import TestCase\n+from urllib.parse import urlparse\n \n from scrapy.http import Request, Response\n from scrapy.http.cookies import WrappedRequest, WrappedResponse\n\n@@ -1,5 +1,5 @@\n-import unittest\n import copy\n+import unittest\n \n from scrapy.http import Headers\n \n\n@@ -1,18 +1,18 @@\n-import unittest\n-import re\n import json\n-import xmlrpc.client\n+import re\n+import unittest\n import warnings\n+import xmlrpc.client\n from unittest import mock\n from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n \n from scrapy.http import (\n-    Request,\n     FormRequest,\n-    XmlRpcRequest,\n-    JsonRequest,\n     Headers,\n     HtmlResponse,\n+    JsonRequest,\n+    Request,\n+    XmlRpcRequest,\n )\n from scrapy.utils.python import to_bytes, to_unicode\n \n\n@@ -1,12 +1,12 @@\n-import unittest\n import dataclasses\n+import unittest\n \n import attr\n from itemadapter import ItemAdapter\n from itemloaders.processors import Compose, Identity, MapCompose, TakeFirst\n \n from scrapy.http import HtmlResponse, Response\n-from scrapy.item import Item, Field\n+from scrapy.item import Field, Item\n from scrapy.loader import ItemLoader\n from scrapy.selector import Selector\n \n\n@@ -16,7 +16,7 @@ from itemloaders.processors import (\n     TakeFirst,\n )\n \n-from scrapy.item import Item, Field\n+from scrapy.item import Field, Item\n from scrapy.loader import ItemLoader\n from scrapy.loader.common import wrap_loader_context\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n\n@@ -6,11 +6,11 @@ from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase as TwistedTestCase\n \n from scrapy.exceptions import DropItem\n-from scrapy.utils.test import get_crawler\n from scrapy.http import Request, Response\n-from scrapy.item import Item, Field\n+from scrapy.item import Field, Item\n from scrapy.logformatter import LogFormatter\n from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n from tests.spiders import ItemSpider\n \n\n@@ -1,14 +1,15 @@\n # coding=utf-8\n \n import unittest\n-from io import BytesIO\n from email.charset import Charset\n+from io import BytesIO\n \n+from twisted import version as twisted_version\n+from twisted.internet import defer\n from twisted.internet._sslverify import ClientTLSOptions\n from twisted.internet.ssl import ClientContextFactory\n from twisted.python.versions import Version\n-from twisted.internet import defer\n-from twisted import version as twisted_version\n+\n from scrapy.mail import MailSender\n \n \n\n@@ -1,8 +1,8 @@\n from twisted.trial import unittest\n \n-from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n+from scrapy.settings import Settings\n \n \n class M1:\n\n@@ -6,8 +6,8 @@ from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from w3lib.url import add_or_replace_parameter\n \n-from scrapy.crawler import CrawlerRunner\n from scrapy import signals\n+from scrapy.crawler import CrawlerRunner\n from tests.mockserver import MockServer\n from tests.spiders import SimpleSpider\n \n\n@@ -1,3 +1,4 @@\n+import dataclasses\n import os\n import random\n import time\n@@ -8,7 +9,6 @@ from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import mock\n from urllib.parse import urlparse\n-import dataclasses\n \n import attr\n from itemadapter import ItemAdapter\n\n@@ -18,7 +18,6 @@ from scrapy.pipelines.images import ImageException, ImagesPipeline, NoimagesDrop\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n-\n try:\n     from PIL import Image\n except ImportError:\n\n@@ -1,25 +1,24 @@\n-from typing import Optional\n import io\n+from typing import Optional\n \n from testfixtures import LogCapture\n-from twisted.trial import unittest\n-from twisted.python.failure import Failure\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n+from twisted.python.failure import Failure\n+from twisted.trial import unittest\n \n from scrapy import signals\n from scrapy.http import Request, Response\n-from scrapy.settings import Settings\n-from scrapy.spiders import Spider\n from scrapy.pipelines.files import FileException\n from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n+from scrapy.settings import Settings\n+from scrapy.spiders import Spider\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n \n-\n try:\n     from PIL import Image  # noqa: imported just to check for the import error\n except ImportError:\n\n@@ -5,10 +5,9 @@ from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n \n-from scrapy import Spider, signals, Request\n-from scrapy.utils.defer import maybe_deferred_to_future, deferred_to_future\n+from scrapy import Request, Spider, signals\n+from scrapy.utils.defer import deferred_to_future, maybe_deferred_to_future\n from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n-\n from tests.mockserver import MockServer\n \n \n\n@@ -4,11 +4,10 @@ import unittest\n import queuelib\n \n from scrapy.http.request import Request\n-from scrapy.pqueues import ScrapyPriorityQueue, DownloaderAwarePriorityQueue\n+from scrapy.pqueues import DownloaderAwarePriorityQueue, ScrapyPriorityQueue\n from scrapy.spiders import Spider\n from scrapy.squeues import FifoMemoryQueue\n from scrapy.utils.test import get_crawler\n-\n from tests.test_scheduler import MockDownloader, MockEngine\n \n \n\n@@ -3,15 +3,15 @@ import os\n import re\n import sys\n from pathlib import Path\n-from subprocess import Popen, PIPE\n+from subprocess import PIPE, Popen\n from urllib.parse import urlsplit, urlunsplit\n+\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n from scrapy.utils.test import get_crawler\n-\n from tests.mockserver import MockServer\n from tests.spiders import SimpleSpider, SingleRequestSpider\n \n\n@@ -1,16 +1,13 @@\n+from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n-from testfixtures import LogCapture\n-\n from scrapy import Request, signals\n from scrapy.http.response import Response\n from scrapy.utils.test import get_crawler\n-\n from tests.mockserver import MockServer\n from tests.spiders import SingleRequestSpider\n \n-\n OVERRIDDEN_URL = \"https://example.org\"\n \n \n\n@@ -4,8 +4,8 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n from scrapy.utils.test import get_crawler\n-from tests.spiders import MockServerSpider\n from tests.mockserver import MockServer\n+from tests.spiders import MockServerSpider\n \n \n class InjectArgumentsDownloaderMiddleware:\n\n@@ -3,7 +3,7 @@ import unittest\n import warnings\n from contextlib import suppress\n \n-from scrapy import Spider, Request\n+from scrapy import Request, Spider\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import FormRequest, JsonRequest\n from scrapy.utils.request import request_from_dict\n@@ -171,10 +171,8 @@ class DeprecatedMethodsRequestSerializationTest(RequestSerializationTest):\n                     \"scrapy.utils.reqser\"\n                 ]  # delete module to reset the deprecation warning\n \n-            from scrapy.utils.reqser import (\n-                request_from_dict as _from_dict,\n-                request_to_dict as _to_dict,\n-            )\n+            from scrapy.utils.reqser import request_from_dict as _from_dict\n+            from scrapy.utils.reqser import request_to_dict as _to_dict\n \n             request_copy = _from_dict(_to_dict(request, spider), spider)\n             self._assert_same_request(request, request_copy)\n\n@@ -1,5 +1,6 @@\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n+\n from scrapy.signals import request_left_downloader\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n\n@@ -1,7 +1,7 @@\n import unittest\n-from scrapy.responsetypes import responsetypes\n \n-from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse, Headers\n+from scrapy.http import Headers, HtmlResponse, Response, TextResponse, XmlResponse\n+from scrapy.responsetypes import responsetypes\n \n \n class ResponseTypesTest(unittest.TestCase):\n\n@@ -1,21 +1,20 @@\n+import collections\n import shutil\n import tempfile\n import unittest\n-import collections\n \n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n \n-from scrapy.crawler import Crawler\n from scrapy.core.downloader import Downloader\n from scrapy.core.scheduler import Scheduler\n+from scrapy.crawler import Crawler\n from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n-\n MockEngine = collections.namedtuple(\"MockEngine\", [\"downloader\"])\n MockSlot = collections.namedtuple(\"MockSlot\", [\"active\"])\n \n\n@@ -2,7 +2,7 @@ import weakref\n \n from twisted.trial import unittest\n \n-from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n+from scrapy.http import HtmlResponse, TextResponse, XmlResponse\n from scrapy.selector import Selector\n \n \n\n@@ -2,12 +2,13 @@ import unittest\n from unittest import mock\n \n from scrapy.settings import (\n+    SETTINGS_PRIORITIES,\n     BaseSettings,\n     Settings,\n     SettingsAttribute,\n-    SETTINGS_PRIORITIES,\n     get_settings_priority,\n )\n+\n from . import default_settings\n \n \n\n@@ -2,9 +2,8 @@ from pytest import mark\n from twisted.internet import defer\n from twisted.trial import unittest\n \n-from scrapy import signals, Request, Spider\n+from scrapy import Request, Spider, signals\n from scrapy.utils.test import get_crawler, get_from_asyncio_queue\n-\n from tests.mockserver import MockServer\n \n \n\n@@ -6,21 +6,21 @@ from unittest import mock\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n-\n from w3lib.url import safe_url_string\n+\n from scrapy import signals\n+from scrapy.http import HtmlResponse, Request, Response, TextResponse, XmlResponse\n+from scrapy.linkextractors import LinkExtractor\n from scrapy.settings import Settings\n-from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n-from scrapy.spiders.init import InitSpider\n from scrapy.spiders import (\n-    CSVFeedSpider,\n     CrawlSpider,\n+    CSVFeedSpider,\n     Rule,\n     SitemapSpider,\n     Spider,\n     XMLFeedSpider,\n )\n-from scrapy.linkextractors import LinkExtractor\n+from scrapy.spiders.init import InitSpider\n from scrapy.utils.test import get_crawler\n from tests import get_testdata\n \n\n@@ -1,21 +1,20 @@\n-import sys\n import shutil\n+import sys\n+import tempfile\n import warnings\n from pathlib import Path\n \n-import tempfile\n-from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n-\n+from zope.interface.verify import verifyObject\n \n # ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n # alone\n import scrapy\n-from scrapy.interfaces import ISpiderLoader\n-from scrapy.spiderloader import SpiderLoader\n-from scrapy.settings import Settings\n-from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n+from scrapy.http import Request\n+from scrapy.interfaces import ISpiderLoader\n+from scrapy.settings import Settings\n+from scrapy.spiderloader import SpiderLoader\n \n module_dir = Path(__file__).resolve().parent\n \n\n@@ -4,16 +4,16 @@ from unittest import mock\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n+from twisted.trial.unittest import TestCase\n \n-from scrapy.spiders import Spider\n-from scrapy.http import Request, Response\n+from scrapy.core.spidermw import SpiderMiddlewareManager\n from scrapy.exceptions import _InvalidOutput\n+from scrapy.http import Request, Response\n+from scrapy.spiders import Spider\n from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.defer import deferred_from_coro, maybe_deferred_to_future\n from scrapy.utils.test import get_crawler\n-from scrapy.core.spidermw import SpiderMiddlewareManager\n \n \n class SpiderMiddlewareTestCase(TestCase):\n\n@@ -1,7 +1,7 @@\n from unittest import TestCase\n \n+from scrapy.http import Request, Response\n from scrapy.spidermiddlewares.depth import DepthMiddleware\n-from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector\n from scrapy.utils.test import get_crawler\n\n@@ -2,14 +2,14 @@ import logging\n from unittest import TestCase\n \n from testfixtures import LogCapture\n-from twisted.trial.unittest import TestCase as TrialTestCase\n from twisted.internet import defer\n+from twisted.trial.unittest import TestCase as TrialTestCase\n \n-from scrapy.utils.test import get_crawler\n-from scrapy.http import Response, Request\n-from scrapy.spiders import Spider\n-from scrapy.spidermiddlewares.httperror import HttpErrorMiddleware, HttpError\n+from scrapy.http import Request, Response\n from scrapy.settings import Settings\n+from scrapy.spidermiddlewares.httperror import HttpError, HttpErrorMiddleware\n+from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n from tests.spiders import MockServerSpider\n \n\n@@ -1,10 +1,10 @@\n+import warnings\n from unittest import TestCase\n from urllib.parse import urlparse\n-import warnings\n \n-from scrapy.http import Response, Request\n+from scrapy.http import Request, Response\n+from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, PortWarning, URLWarning\n from scrapy.spiders import Spider\n-from scrapy.spidermiddlewares.offsite import OffsiteMiddleware, URLWarning, PortWarning\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -1,17 +1,11 @@\n-from urllib.parse import urlparse\n-from unittest import TestCase\n import warnings\n-from scrapy.http import Response, Request\n+from unittest import TestCase\n+from urllib.parse import urlparse\n \n-from scrapy.settings import Settings\n-from scrapy.spiders import Spider\n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n+from scrapy.http import Request, Response\n+from scrapy.settings import Settings\n from scrapy.spidermiddlewares.referer import (\n-    DefaultReferrerPolicy,\n-    NoReferrerPolicy,\n-    NoReferrerWhenDowngradePolicy,\n-    OriginPolicy,\n-    OriginWhenCrossOriginPolicy,\n     POLICY_NO_REFERRER,\n     POLICY_NO_REFERRER_WHEN_DOWNGRADE,\n     POLICY_ORIGIN,\n@@ -21,6 +15,11 @@ from scrapy.spidermiddlewares.referer import (\n     POLICY_STRICT_ORIGIN,\n     POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n     POLICY_UNSAFE_URL,\n+    DefaultReferrerPolicy,\n+    NoReferrerPolicy,\n+    NoReferrerWhenDowngradePolicy,\n+    OriginPolicy,\n+    OriginWhenCrossOriginPolicy,\n     RefererMiddleware,\n     ReferrerPolicy,\n     SameOriginPolicy,\n@@ -28,6 +27,7 @@ from scrapy.spidermiddlewares.referer import (\n     StrictOriginWhenCrossOriginPolicy,\n     UnsafeUrlPolicy,\n )\n+from scrapy.spiders import Spider\n \n \n class TestRefererMiddleware(TestCase):\n\n@@ -2,11 +2,11 @@ from unittest import TestCase\n \n from testfixtures import LogCapture\n \n+from scrapy.http import Request, Response\n+from scrapy.settings import Settings\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n-from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n-from scrapy.settings import Settings\n \n \n class TestUrlLengthMiddleware(TestCase):\n\n@@ -1,11 +1,12 @@\n+import shutil\n from datetime import datetime\n from pathlib import Path\n-import shutil\n+\n from twisted.trial import unittest\n \n+from scrapy.exceptions import NotConfigured\n from scrapy.extensions.spiderstate import SpiderState\n from scrapy.spiders import Spider\n-from scrapy.exceptions import NotConfigured\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -2,16 +2,17 @@ import pickle\n import sys\n \n from queuelib.tests import test_queue as t\n+\n+from scrapy.http import Request\n+from scrapy.item import Field, Item\n+from scrapy.loader import ItemLoader\n+from scrapy.selector import Selector\n from scrapy.squeues import (\n     _MarshalFifoSerializationDiskQueue,\n     _MarshalLifoSerializationDiskQueue,\n     _PickleFifoSerializationDiskQueue,\n     _PickleLifoSerializationDiskQueue,\n )\n-from scrapy.item import Item, Field\n-from scrapy.http import Request\n-from scrapy.loader import ItemLoader\n-from scrapy.selector import Selector\n \n \n class TestItem(Item):\n\n@@ -4,16 +4,16 @@ import unittest\n \n import queuelib\n \n-from scrapy.squeues import (\n-    PickleFifoDiskQueue,\n-    PickleLifoDiskQueue,\n-    MarshalFifoDiskQueue,\n-    MarshalLifoDiskQueue,\n-    FifoMemoryQueue,\n-    LifoMemoryQueue,\n-)\n from scrapy.http import Request\n from scrapy.spiders import Spider\n+from scrapy.squeues import (\n+    FifoMemoryQueue,\n+    LifoMemoryQueue,\n+    MarshalFifoDiskQueue,\n+    MarshalLifoDiskQueue,\n+    PickleFifoDiskQueue,\n+    PickleLifoDiskQueue,\n+)\n from scrapy.utils.test import get_crawler\n \n \"\"\"\n\n@@ -1,10 +1,10 @@\n-from datetime import datetime\n import unittest\n+from datetime import datetime\n from unittest import mock\n \n from scrapy.extensions.corestats import CoreStats\n from scrapy.spiders import Spider\n-from scrapy.statscollectors import StatsCollector, DummyStatsCollector\n+from scrapy.statscollectors import DummyStatsCollector, StatsCollector\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -11,7 +11,7 @@ class ToplevelTestCase(TestCase):\n         self.assertIs(type(scrapy.version_info), tuple)\n \n     def test_request_shortcut(self):\n-        from scrapy.http import Request, FormRequest\n+        from scrapy.http import FormRequest, Request\n \n         self.assertIs(scrapy.Request, Request)\n         self.assertIs(scrapy.FormRequest, FormRequest)\n@@ -27,7 +27,7 @@ class ToplevelTestCase(TestCase):\n         self.assertIs(scrapy.Selector, Selector)\n \n     def test_item_shortcut(self):\n-        from scrapy.item import Item, Field\n+        from scrapy.item import Field, Item\n \n         self.assertIs(scrapy.Item, Item)\n         self.assertIs(scrapy.Field, Field)\n\n@@ -1,5 +1,5 @@\n-from urllib.parse import urlparse\n import unittest\n+from urllib.parse import urlparse\n \n \n class UrlparseTestCase(unittest.TestCase):\n\n@@ -3,7 +3,7 @@ from unittest import TestCase\n \n from pytest import mark\n \n-from scrapy.utils.reactor import is_asyncio_reactor_installed, install_reactor\n+from scrapy.utils.reactor import install_reactor, is_asyncio_reactor_installed\n \n \n @mark.usefixtures(\"reactor_pytest\")\n\n@@ -1,7 +1,7 @@\n import unittest\n import warnings\n \n-from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n+from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.conf import (\n     arglist_to_dict,\n\n@@ -11,7 +11,6 @@ from scrapy.utils.datatypes import (\n )\n from scrapy.utils.python import garbage_collect\n \n-\n __doctests__ = [\"scrapy.utils.datatypes\"]\n \n \n\n@@ -1,11 +1,11 @@\n import random\n \n from pytest import mark\n-from twisted.trial import unittest\n-from twisted.internet import reactor, defer\n+from twisted.internet import defer, reactor\n from twisted.python.failure import Failure\n+from twisted.trial import unittest\n \n-from scrapy.utils.asyncgen import collect_asyncgen, as_async_generator\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.defer import (\n     aiter_errback,\n     deferred_f_from_coro_f,\n\n@@ -1,7 +1,7 @@\n import inspect\n import unittest\n-from unittest import mock\n import warnings\n+from unittest import mock\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n\n@@ -1,6 +1,5 @@\n from io import StringIO\n-\n-from unittest import mock, TestCase\n+from unittest import TestCase, mock\n \n from scrapy.utils.display import pformat, pprint\n \n\n@@ -3,11 +3,10 @@ from pathlib import Path\n \n from w3lib.encoding import html_to_unicode\n \n-from scrapy.utils.gz import gunzip, gzip_magic_number\n from scrapy.http import Response\n+from scrapy.utils.gz import gunzip, gzip_magic_number\n from tests import tests_datadir\n \n-\n SAMPLEDIR = Path(tests_datadir, \"compressed\")\n \n \n\n@@ -1,8 +1,8 @@\n from pytest import mark\n from twisted.trial import unittest\n \n-from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml\n-from scrapy.http import XmlResponse, TextResponse, Response\n+from scrapy.http import Response, TextResponse, XmlResponse\n+from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml\n from tests import get_testdata\n \n \n\n@@ -1,18 +1,18 @@\n-import sys\n import logging\n+import sys\n import unittest\n \n from testfixtures import LogCapture\n from twisted.python.failure import Failure\n \n+from scrapy.extensions import telnet\n from scrapy.utils.log import (\n-    failure_to_exc_info,\n-    TopLevelFormatter,\n     LogCounterHandler,\n     StreamLogger,\n+    TopLevelFormatter,\n+    failure_to_exc_info,\n )\n from scrapy.utils.test import get_crawler\n-from scrapy.extensions import telnet\n \n \n class FailureToExcInfoTest(unittest.TestCase):\n\n@@ -1,10 +1,10 @@\n-import sys\n import os\n+import sys\n import unittest\n from pathlib import Path\n from unittest import mock\n \n-from scrapy.item import Item, Field\n+from scrapy.item import Field, Item\n from scrapy.utils.misc import (\n     arg_to_iter,\n     create_instance,\n@@ -14,7 +14,6 @@ from scrapy.utils.misc import (\n     walk_modules,\n )\n \n-\n __doctests__ = [\"scrapy.utils.misc\"]\n \n \n\n@@ -1,8 +1,8 @@\n-import unittest\n-import os\n-import tempfile\n-import shutil\n import contextlib\n+import os\n+import shutil\n+import tempfile\n+import unittest\n import warnings\n from pathlib import Path\n \n\n@@ -5,20 +5,19 @@ import platform\n from twisted.trial import unittest\n \n from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n-from scrapy.utils.defer import deferred_f_from_coro_f, aiter_errback\n+from scrapy.utils.defer import aiter_errback, deferred_f_from_coro_f\n from scrapy.utils.python import (\n-    memoizemethod_noargs,\n+    MutableAsyncChain,\n+    MutableChain,\n     binary_is_text,\n     equal_attributes,\n     get_func_args,\n+    memoizemethod_noargs,\n     to_bytes,\n     to_unicode,\n     without_none_values,\n-    MutableChain,\n-    MutableAsyncChain,\n )\n \n-\n __doctests__ = [\"scrapy.utils.python\"]\n \n \n\n@@ -4,17 +4,16 @@ from pathlib import Path\n from urllib.parse import urlparse\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.http import Response, TextResponse, HtmlResponse\n+from scrapy.http import HtmlResponse, Response, TextResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (\n-    response_httprepr,\n-    open_in_browser,\n-    get_meta_refresh,\n     get_base_url,\n+    get_meta_refresh,\n+    open_in_browser,\n+    response_httprepr,\n     response_status_message,\n )\n \n-\n __doctests__ = [\"scrapy.utils.response\"]\n \n \n\n@@ -1,7 +1,7 @@\n+import dataclasses\n import datetime\n import json\n import unittest\n-import dataclasses\n from decimal import Decimal\n \n import attr\n\n@@ -3,7 +3,7 @@ import unittest\n from scrapy import Spider\n from scrapy.http import Request\n from scrapy.item import Item\n-from scrapy.utils.spider import iterate_spider_output, iter_spider_classes\n+from scrapy.utils.spider import iter_spider_classes, iterate_spider_output\n \n \n class MySpider1(Spider):\n\n@@ -1,9 +1,9 @@\n+import unittest\n from pathlib import Path\n from shutil import rmtree\n from tempfile import mkdtemp\n-import unittest\n-from scrapy.utils.template import render_templatefile\n \n+from scrapy.utils.template import render_templatefile\n \n __doctests__ = [\"scrapy.utils.template\"]\n \n\n@@ -4,13 +4,13 @@ from scrapy.linkextractors import IGNORED_EXTENSIONS\n from scrapy.spiders import Spider\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import (\n+    _is_filesystem_path,\n     add_http_if_no_scheme,\n     guess_scheme,\n-    _is_filesystem_path,\n     strip_url,\n+    url_has_any_extension,\n     url_is_from_any_domain,\n     url_is_from_spider,\n-    url_has_any_extension,\n )\n \n __doctests__ = [\"scrapy.utils.url\"]\n\n@@ -6,9 +6,9 @@ import shutil\n from pathlib import Path\n \n import OpenSSL.SSL\n+from twisted.internet import defer, reactor\n from twisted.trial import unittest\n-from twisted.web import server, static, util, resource\n-from twisted.internet import reactor, defer\n+from twisted.web import resource, server, static, util\n \n try:\n     from twisted.internet.testing import StringTransport\n@@ -16,12 +16,13 @@ except ImportError:\n     # deprecated in Twisted 19.7.0\n     # (remove once we bump our requirement past that version)\n     from twisted.test.proto_helpers import StringTransport\n-from twisted.protocols.policies import WrappingFactory\n+\n from twisted.internet.defer import inlineCallbacks\n+from twisted.protocols.policies import WrappingFactory\n \n from scrapy.core.downloader import webclient as client\n from scrapy.core.downloader.contextfactory import ScrapyClientContextFactory\n-from scrapy.http import Request, Headers\n+from scrapy.http import Headers, Request\n from scrapy.settings import Settings\n from scrapy.utils.misc import create_instance\n from scrapy.utils.python import to_bytes, to_unicode\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
