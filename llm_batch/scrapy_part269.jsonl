{"custom_id": "scrapy#54a786b102a3862c94fcdb123a26b54a39892697", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 28 | Files Changed: 16 | Hunks: 24 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 36 | Churn Cumulative: 6627 | Contributors (this commit): 64 | Commits (past 90d): 74 | Contributors (cumulative): 165 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,9 +1,8 @@\n-import six\n-import signal\n import logging\n+import signal\n+import sys\n import warnings\n \n-import sys\n from twisted.internet import reactor, defer\n from zope.interface.verify import verifyClass, DoesNotImplement\n \n@@ -22,6 +21,7 @@ from scrapy.utils.log import (\n     get_scrapy_root_handler, install_scrapy_root_handler)\n from scrapy import signals\n \n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -11,8 +11,6 @@ import warnings\n import pickle\n from xml.sax.saxutils import XMLGenerator\n \n-import six\n-\n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.utils.python import to_bytes, to_unicode, is_listlike\n from scrapy.item import BaseItem\n\n@@ -1,4 +1,3 @@\n-import six\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.python import to_unicode\n@@ -68,9 +67,6 @@ class Headers(CaselessDict):\n         self[key] = lst\n \n     def items(self):\n-        return list(self.iteritems())\n-\n-    def iteritems(self):\n         return ((k, self.getlist(k)) for k in self.keys())\n \n     def values(self):\n\n@@ -4,7 +4,6 @@ requests in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-import six\n from w3lib.url import safe_url_string\n \n from scrapy.http.headers import Headers\n\n@@ -8,7 +8,6 @@ See documentation in docs/topics/request-response.rst\n from urllib.parse import urljoin\n \n import parsel\n-import six\n from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from w3lib.html import strip_html5_whitespace\n\n@@ -5,7 +5,6 @@ import warnings\n from html.parser import HTMLParser\n from urllib.parse import urljoin\n \n-import six\n from w3lib.url import safe_url_string\n from w3lib.html import strip_html5_whitespace\n \n\n@@ -3,7 +3,6 @@ Link extractor based on lxml.html\n \"\"\"\n from urllib.parse import urljoin\n \n-import six\n import lxml.etree as etree\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url\n\n@@ -5,7 +5,6 @@ import warnings\n from urllib.parse import urljoin\n from sgmllib import SGMLParser\n \n-import six\n from w3lib.url import safe_url_string, canonicalize_url\n from w3lib.html import strip_html5_whitespace\n \n\n@@ -1,4 +1,3 @@\n-import six\n import json\n import copy\n from collections.abc import MutableMapping\n\n@@ -8,8 +8,6 @@ See documentation in docs/topics/spiders.rst\n import copy\n import warnings\n \n-import six\n-\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, HtmlResponse\n from scrapy.linkextractors import LinkExtractor\n\n@@ -1,6 +1,5 @@\n import re\n import logging\n-import six\n \n from scrapy.spiders import Spider\n from scrapy.http import Request, XmlResponse\n\n@@ -4,7 +4,6 @@ from shlex import split\n from http.cookies import SimpleCookie\n from urllib.parse import urlparse\n \n-from six import string_types, iteritems\n from w3lib.http import basic_auth_header\n \n \n@@ -76,7 +75,7 @@ def curl_to_request_kwargs(curl_command, ignore_unknown_options=True):\n         name = name.strip()\n         val = val.strip()\n         if name.title() == 'Cookie':\n-            for name, morsel in iteritems(SimpleCookie(val)):\n+            for name, morsel in SimpleCookie(val).items():\n                 cookies[name] = morsel.value\n         else:\n             headers.append((name, val))\n\n@@ -1,13 +1,13 @@\n-import re\n import csv\n-from io import StringIO\n import logging\n-import six\n+import re\n+from io import StringIO\n \n from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n \n+\n logger = logging.getLogger(__name__)\n \n \n\n@@ -6,7 +6,6 @@ from contextlib import contextmanager\n from importlib import import_module\n from pkgutil import iter_modules\n \n-import six\n from w3lib.html import replace_entities\n \n from scrapy.utils.python import flatten, to_unicode\n\n@@ -85,9 +85,6 @@ class HeadersTest(unittest.TestCase):\n         self.assertSortedEqual(h.items(),\n                                [(b'X-Forwarded-For', [b'ip1', b'ip2']),\n                                 (b'Content-Type', [b'text/html'])])\n-        self.assertSortedEqual(h.iteritems(),\n-                               [(b'X-Forwarded-For', [b'ip1', b'ip2']),\n-                                (b'Content-Type', [b'text/html'])])\n         self.assertSortedEqual(h.values(), [b'ip2', b'text/html'])\n \n     def test_update(self):\n\n@@ -62,7 +62,7 @@ class RequestTest(unittest.TestCase):\n         # headers must not be unicode\n         h = Headers({'key1': u'val1', u'key2': 'val2'})\n         h[u'newkey'] = u'newval'\n-        for k, v in h.iteritems():\n+        for k, v in h.items():\n             self.assertIsInstance(k, bytes)\n             for s in v:\n                 self.assertIsInstance(s, bytes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac62524824c590e9dd2d323bf418f71a683ecbf4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 777 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = yield method(request=request, spider=spider)\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput('Middleware %s.process_request must return None, Response or Request, got %s' % \\\n-                                         (six.get_method_self(method).__class__.__name__, response.__class__.__name__))\n+                                         (method.__self__.__class__.__name__, response.__class__.__name__))\n                 if response:\n                     defer.returnValue(response)\n             defer.returnValue((yield download_func(request=request, spider=spider)))\n@@ -53,7 +53,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = yield method(request=request, response=response, spider=spider)\n                 if not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput('Middleware %s.process_response must return Response or Request, got %s' % \\\n-                                         (six.get_method_self(method).__class__.__name__, type(response)))\n+                                         (method.__self__.__class__.__name__, type(response)))\n                 if isinstance(response, Request):\n                     defer.returnValue(response)\n             defer.returnValue(response)\n@@ -65,7 +65,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n                 response = yield method(request=request, exception=exception, spider=spider)\n                 if response is not None and not isinstance(response, (Response, Request)):\n                     raise _InvalidOutput('Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n-                                         (six.get_method_self(method).__class__.__name__, type(response)))\n+                                         (method.__self__.__class__.__name__, type(response)))\n                 if response:\n                     defer.returnValue(response)\n             defer.returnValue(_failure)\n\n@@ -37,8 +37,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     def scrape_response(self, scrape_func, response, request, spider):\n         fname = lambda f:'%s.%s' % (\n-                six.get_method_self(f).__class__.__name__,\n-                six.get_method_function(f).__name__)\n+                f.__self__.__class__.__name__,\n+                f.__func__.__name__)\n \n         def process_spider_input(response):\n             for method in self.methods['process_spider_input']:\n\n@@ -87,12 +87,12 @@ def _mangle_private_name(obj, func, name):\n def _find_method(obj, func):\n     if obj:\n         try:\n-            func_self = six.get_method_self(func)\n+            func_self = func.__self__\n         except AttributeError:  # func has no __self__\n             pass\n         else:\n             if func_self is obj:\n-                name = six.get_method_function(func).__name__\n+                name = func.__func__.__name__\n                 if _is_private_method(name):\n                     return _mangle_private_name(obj, func, name)\n                 return name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5d8abdde59e7501e8bda25b27c8926fda66b9af1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 24 | Files Changed: 7 | Hunks: 21 | Methods Changed: 16 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 48 | Churn Cumulative: 2510 | Contributors (this commit): 28 | Commits (past 90d): 18 | Contributors (cumulative): 60 | DMM Complexity: None\n\nDIFF:\n@@ -79,7 +79,7 @@ class BaseItemExporterTest(unittest.TestCase):\n \n         ie = self._get_exporter(fields_to_export=['name'], encoding='latin-1')\n         _, name = list(ie._get_serialized_fields(self.i))[0]\n-        assert isinstance(name, six.text_type)\n+        assert isinstance(name, str)\n         self.assertEqual(name, u'John\\xa3')\n \n     def test_field_custom_serializer(self):\n\n@@ -102,7 +102,7 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertEqual(r4.flags, [])\n \n     def _assert_response_values(self, response, encoding, body):\n-        if isinstance(body, six.text_type):\n+        if isinstance(body, str):\n             body_unicode = body\n             body_bytes = body.encode(encoding)\n         else:\n@@ -110,7 +110,7 @@ class BaseResponseTest(unittest.TestCase):\n             body_bytes = body\n \n         assert isinstance(response.body, bytes)\n-        assert isinstance(response.text, six.text_type)\n+        assert isinstance(response.text, str)\n         self._assert_response_encoding(response, encoding)\n         self.assertEqual(response.body, body_bytes)\n         self.assertEqual(response.body_as_unicode(), body_unicode)\n@@ -220,11 +220,11 @@ class TextResponseTest(BaseResponseTest):\n         r1 = self.response_class('http://www.example.com', body=original_string, encoding='cp1251')\n \n         # check body_as_unicode\n-        self.assertTrue(isinstance(r1.body_as_unicode(), six.text_type))\n+        self.assertTrue(isinstance(r1.body_as_unicode(), str))\n         self.assertEqual(r1.body_as_unicode(), unicode_string)\n \n         # check response.text\n-        self.assertTrue(isinstance(r1.text, six.text_type))\n+        self.assertTrue(isinstance(r1.text, str))\n         self.assertEqual(r1.text, unicode_string)\n \n     def test_encoding(self):\n\n@@ -157,7 +157,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_get_value(self):\n         il = NameItemLoader()\n-        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), six.text_type.upper))\n+        self.assertEqual(u'FOO', il.get_value([u'foo', u'bar'], TakeFirst(), str.upper))\n         self.assertEqual([u'foo', u'bar'], il.get_value([u'name:foo', u'name:bar'], re=u'name:(.*)$'))\n         self.assertEqual(u'foo', il.get_value([u'name:foo', u'name:bar'], TakeFirst(), re=u'name:(.*)$'))\n \n@@ -258,7 +258,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_extend_custom_input_processors(self):\n         class ChildItemLoader(TestItemLoader):\n-            name_in = MapCompose(TestItemLoader.name_in, six.text_type.swapcase)\n+            name_in = MapCompose(TestItemLoader.name_in, str.swapcase)\n \n         il = ChildItemLoader()\n         il.add_value('name', u'marta')\n@@ -266,7 +266,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_extend_default_input_processors(self):\n         class ChildDefaultedItemLoader(DefaultedItemLoader):\n-            name_in = MapCompose(DefaultedItemLoader.default_input_processor, six.text_type.swapcase)\n+            name_in = MapCompose(DefaultedItemLoader.default_input_processor, str.swapcase)\n \n         il = ChildDefaultedItemLoader()\n         il.add_value('name', u'marta')\n@@ -689,7 +689,7 @@ class ProcessorsTest(unittest.TestCase):\n         self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])\n         self.assertEqual(proc(['', 'hello', 'world']), u' hello world')\n         self.assertEqual(proc(['hello', 'world']), u'hello world')\n-        self.assertIsInstance(proc(['hello', 'world']), six.text_type)\n+        self.assertIsInstance(proc(['hello', 'world']), str)\n \n     def test_compose(self):\n         proc = Compose(lambda v: v[0], str.upper)\n@@ -704,12 +704,12 @@ class ProcessorsTest(unittest.TestCase):\n     def test_mapcompose(self):\n         def filter_world(x):\n             return None if x == 'world' else x\n-        proc = MapCompose(filter_world, six.text_type.upper)\n+        proc = MapCompose(filter_world, str.upper)\n         self.assertEqual(proc([u'hello', u'world', u'this', u'is', u'scrapy']),\n                          [u'HELLO', u'THIS', u'IS', u'SCRAPY'])\n-        proc = MapCompose(filter_world, six.text_type.upper)\n+        proc = MapCompose(filter_world, str.upper)\n         self.assertEqual(proc(None), [])\n-        proc = MapCompose(filter_world, six.text_type.upper)\n+        proc = MapCompose(filter_world, str.upper)\n         self.assertRaises(ValueError, proc, [1])\n         proc = MapCompose(filter_world, lambda x: x + 1)\n         self.assertRaises(ValueError, proc, 'hello')\n\n@@ -59,7 +59,7 @@ class LoggingContribTest(unittest.TestCase):\n         logkws = self.formatter.dropped(item, exception, response, self.spider)\n         logline = logkws['msg'] % logkws['args']\n         lines = logline.splitlines()\n-        assert all(isinstance(x, six.text_type) for x in lines)\n+        assert all(isinstance(x, str) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n \n     def test_scraped(self):\n@@ -69,7 +69,7 @@ class LoggingContribTest(unittest.TestCase):\n         logkws = self.formatter.scraped(item, response, self.spider)\n         logline = logkws['msg'] % logkws['args']\n         lines = logline.splitlines()\n-        assert all(isinstance(x, six.text_type) for x in lines)\n+        assert all(isinstance(x, str) for x in lines)\n         self.assertEqual(lines, [u\"Scraped from <200 http://www.example.com>\", u'name: \\xa3'])\n \n \n\n@@ -6,7 +6,7 @@ import scrapy\n class ToplevelTestCase(TestCase):\n \n     def test_version(self):\n-        self.assertIs(type(scrapy.__version__), six.text_type)\n+        self.assertIs(type(scrapy.__version__), str)\n \n     def test_version_info(self):\n         self.assertIs(type(scrapy.version_info), tuple)\n\n@@ -255,8 +255,8 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         # explicit type check cuz' we no like stinkin' autocasting! yarrr\n         for result_row in result:\n-            self.assertTrue(all((isinstance(k, six.text_type) for k in result_row.keys())))\n-            self.assertTrue(all((isinstance(v, six.text_type) for v in result_row.values())))\n+            self.assertTrue(all((isinstance(k, str) for k in result_row.keys())))\n+            self.assertTrue(all((isinstance(v, str) for v in result_row.values())))\n \n     def test_csviter_delimiter(self):\n         body = get_testdata('feeds', 'feed-sample3.csv').replace(b',', b'\\t')\n\n@@ -169,8 +169,8 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d2 = stringify_dict(d, keys_only=False)\n         self.assertEqual(d, d2)\n         self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n+        self.assertFalse(any(isinstance(x, str) for x in d2.keys()))\n+        self.assertFalse(any(isinstance(x, str) for x in d2.values()))\n \n     @unittest.skipUnless(six.PY2, \"deprecated function\")\n     def test_stringify_dict_tuples(self):\n@@ -179,8 +179,8 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d2 = stringify_dict(tuples, keys_only=False)\n         self.assertEqual(d, d2)\n         self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()), d2.keys())\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n+        self.assertFalse(any(isinstance(x, str) for x in d2.keys()), d2.keys())\n+        self.assertFalse(any(isinstance(x, str) for x in d2.values()))\n \n     @unittest.skipUnless(six.PY2, \"deprecated function\")\n     def test_stringify_dict_keys_only(self):\n@@ -188,7 +188,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d2 = stringify_dict(d)\n         self.assertEqual(d, d2)\n         self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n+        self.assertFalse(any(isinstance(x, str) for x in d2.keys()))\n \n     def test_get_func_args(self):\n         def f1(a, b, c):\n@@ -227,12 +227,12 @@ class UtilsPythonTestCase(unittest.TestCase):\n \n         if platform.python_implementation() == 'CPython':\n             # TODO: how do we fix this to return the actual argument names?\n-            self.assertEqual(get_func_args(six.text_type.split), [])\n+            self.assertEqual(get_func_args(str.split), [])\n             self.assertEqual(get_func_args(\" \".join), [])\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         else:\n             self.assertEqual(\n-                get_func_args(six.text_type.split, True), ['sep', 'maxsplit'])\n+                get_func_args(str.split, True), ['sep', 'maxsplit'])\n             self.assertEqual(get_func_args(\" \".join, True), ['list'])\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), True), ['obj'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d72444b9c8db94dd5567e763ee53b2c70a423c9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 17 | Files Changed: 12 | Hunks: 13 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 4508 | Contributors (this commit): 37 | Commits (past 90d): 37 | Contributors (cumulative): 94 | DMM Complexity: None\n\nDIFF:\n@@ -3,8 +3,6 @@ Downloader Middleware manager\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-import six\n-\n from twisted.internet import defer\n \n from scrapy.exceptions import _InvalidOutput\n\n@@ -5,7 +5,6 @@ See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n from itertools import chain, islice\n \n-import six\n from twisted.python.failure import Failure\n from scrapy.exceptions import _InvalidOutput\n from scrapy.middleware import MiddlewareManager\n\n@@ -1,8 +1,6 @@\n \"\"\"\n Helper functions for serializing (and deserializing) requests.\n \"\"\"\n-import six\n-\n from scrapy.http import Request\n from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n\n@@ -8,7 +8,6 @@ from io import BytesIO\n from datetime import datetime\n \n import lxml.etree\n-import six\n \n from scrapy.item import Item, Field\n from scrapy.utils.python import to_unicode\n\n@@ -1,7 +1,6 @@\n # -*- coding: utf-8 -*-\n import unittest\n \n-import six\n from w3lib.encoding import resolve_encoding\n \n from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n\n@@ -1,8 +1,6 @@\n from functools import partial\n import unittest\n \n-import six\n-\n from scrapy.http import HtmlResponse\n from scrapy.item import Item, Field\n from scrapy.loader import ItemLoader\n\n@@ -3,7 +3,6 @@ import unittest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase as TwistedTestCase\n-import six\n \n from scrapy.crawler import CrawlerRunner\n from scrapy.exceptions import DropItem\n\n@@ -1,5 +1,5 @@\n from unittest import TestCase\n-import six\n+\n import scrapy\n \n \n\n@@ -1,12 +1,13 @@\n # -*- coding: utf-8 -*-\n import os\n-import six\n+\n from twisted.trial import unittest\n \n from scrapy.utils.iterators import csviter, xmliter, _body_or_str, xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse, Response\n from tests import get_testdata\n \n+\n FOOBAR_NL = u\"foo\\nbar\"\n \n \n\n@@ -2,8 +2,6 @@\n import unittest\n import sys\n \n-import six\n-\n from scrapy.http import Request, FormRequest\n from scrapy.spiders import Spider\n from scrapy.utils.reqser import request_to_dict, request_from_dict, _is_private_method, _mangle_private_name\n\n@@ -2,8 +2,6 @@\n import unittest\n from urllib.parse import urlparse\n \n-import six\n-\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n                               add_http_if_no_scheme, guess_scheme,\n\n@@ -3,7 +3,6 @@ from twisted.internet import defer\n Tests borrowed from the twisted.web.client tests.\n \"\"\"\n import os\n-import six\n import shutil\n \n import OpenSSL.SSL\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f3cb05d8e20048eb3e37c5e83bcf495d28c6064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 510 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -10,8 +10,6 @@ from copy import deepcopy\n from pprint import pformat\n from warnings import warn\n \n-import six\n-\n from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n@@ -130,6 +128,5 @@ class DictItem(MutableMapping, BaseItem):\n         return deepcopy(self)\n \n \n-@six.add_metaclass(ItemMeta)\n-class Item(DictItem):\n+class Item(DictItem, metaclass=ItemMeta):\n     pass\n\n@@ -3,8 +3,6 @@ import unittest\n from unittest import mock\n from warnings import catch_warnings\n \n-import six\n-\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n \n@@ -302,7 +300,7 @@ class ItemMetaTest(unittest.TestCase):\n class ItemMetaClassCellRegression(unittest.TestCase):\n \n     def test_item_meta_classcell_regression(self):\n-        class MyItem(six.with_metaclass(ItemMeta, Item)):\n+        class MyItem(Item, metaclass=ItemMeta):\n             def __init__(self, *args, **kwargs):\n                 # This call to super() trigger the __classcell__ propagation\n                 # requirement. When not done properly raises an error:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#586b25d27e1641433d505338901fa9a1409ccdd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 820 | Contributors (this commit): 20 | Commits (past 90d): 11 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@\n import re\n import logging\n \n-import six\n from w3lib import html\n \n from scrapy.exceptions import NotConfigured\n@@ -66,7 +65,7 @@ class AjaxCrawlMiddleware(object):\n \n \n # XXX: move it to w3lib?\n-_ajax_crawlable_re = re.compile(six.u(r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'))\n+_ajax_crawlable_re = re.compile(r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>')\n def _has_ajaxcrawlable_meta(text):\n     \"\"\"\n     >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n\n@@ -7,7 +7,6 @@ import re\n import inspect\n import weakref\n import errno\n-import six\n from functools import partial, wraps\n from itertools import chain\n import sys\n@@ -162,7 +161,7 @@ def memoizemethod_noargs(method):\n     return new_method\n \n \n-_BINARYCHARS = {six.b(chr(i)) for i in range(32)} - {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n+_BINARYCHARS = {to_bytes(chr(i)) for i in range(32)} - {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n _BINARYCHARS |= {ord(ch) for ch in _BINARYCHARS}\n \n @deprecated(\"scrapy.utils.python.binary_is_text\")\n\n@@ -1,6 +1,7 @@\n-import six\n import unittest\n+from io import StringIO\n from unittest import mock\n+\n from scrapy.utils import trackref\n \n \n@@ -38,12 +39,12 @@ Live References\n Bar                                 1   oldest: 0s ago\n ''')\n \n-    @mock.patch('sys.stdout', new_callable=six.StringIO)\n+    @mock.patch('sys.stdout', new_callable=StringIO)\n     def test_print_live_refs_empty(self, stdout):\n         trackref.print_live_refs()\n         self.assertEqual(stdout.getvalue(), 'Live References\\n\\n\\n')\n \n-    @mock.patch('sys.stdout', new_callable=six.StringIO)\n+    @mock.patch('sys.stdout', new_callable=StringIO)\n     def test_print_live_refs_with_objects(self, stdout):\n         o1 = Foo()  # NOQA\n         trackref.print_live_refs()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5797aefd4c561666c2c047494b7424d77eebb469", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 571 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +1,12 @@\n-from io import StringIO\n import json\n import os\n import pstats\n import shutil\n-import six\n-from subprocess import Popen, PIPE\n import sys\n import tempfile\n import unittest\n+from io import StringIO\n+from subprocess import Popen, PIPE\n \n from scrapy.utils.test import get_testenv\n \n@@ -65,5 +64,5 @@ class CmdlineTest(unittest.TestCase):\n         for char in (\"'\", \"<\", \">\", 'u\"'):\n             settingsstr = settingsstr.replace(char, '\"')\n         settingsdict = json.loads(settingsstr)\n-        six.assertCountEqual(self, settingsdict.keys(), EXTENSIONS.keys())\n+        self.assertCountEqual(settingsdict.keys(), EXTENSIONS.keys())\n         self.assertEqual(200, settingsdict[EXT_PATH])\n\n@@ -1,4 +1,3 @@\n-import six\n import unittest\n from unittest import mock\n \n@@ -43,14 +42,14 @@ class SettingsAttributeTest(unittest.TestCase):\n         new_dict = {'three': 11, 'four': 21}\n         attribute.set(new_dict, 10)\n         self.assertIsInstance(attribute.value, BaseSettings)\n-        six.assertCountEqual(self, attribute.value, new_dict)\n-        six.assertCountEqual(self, original_settings, original_dict)\n+        self.assertCountEqual(attribute.value, new_dict)\n+        self.assertCountEqual(original_settings, original_dict)\n \n         new_settings = BaseSettings({'five': 12}, 0)\n         attribute.set(new_settings, 0)  # Insufficient priority\n-        six.assertCountEqual(self, attribute.value, new_dict)\n+        self.assertCountEqual(attribute.value, new_dict)\n         attribute.set(new_settings, 10)\n-        six.assertCountEqual(self, attribute.value, new_settings)\n+        self.assertCountEqual(attribute.value, new_settings)\n \n     def test_repr(self):\n         self.assertEqual(repr(self.attribute),\n@@ -276,9 +275,8 @@ class BaseSettingsTest(unittest.TestCase):\n                           'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n                           'HASNOBASE': BaseSettings({3: 3000}, 'default')})\n         s['TEST'].set(2, 200, 'cmdline')\n-        six.assertCountEqual(self, s.getwithbase('TEST'),\n-                             {1: 1, 2: 200, 3: 30})\n-        six.assertCountEqual(self, s.getwithbase('HASNOBASE'), s['HASNOBASE'])\n+        self.assertCountEqual(s.getwithbase('TEST'), {1: 1, 2: 200, 3: 30})\n+        self.assertCountEqual(s.getwithbase('HASNOBASE'), s['HASNOBASE'])\n         self.assertEqual(s.getwithbase('NONEXISTENT'), {})\n \n     def test_maxpriority(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00b793dc59a32d13c89ac5c0d54985677b228801", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1208 | Contributors (this commit): 17 | Commits (past 90d): 14 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,12 +1,13 @@\n import sys\n import logging\n from abc import ABCMeta, abstractmethod\n-from six import with_metaclass\n \n from scrapy.utils.python import to_unicode\n \n+\n logger = logging.getLogger(__name__)\n \n+\n def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n     try:\n         if to_native_str_type:\n@@ -23,7 +24,8 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n         robotstxt_body = ''\n     return robotstxt_body\n \n-class RobotParser(with_metaclass(ABCMeta)):\n+\n+class RobotParser(metaclass=ABCMeta):\n     @classmethod\n     @abstractmethod\n     def from_crawler(cls, crawler, robotstxt_body):\n\n@@ -1,6 +1,5 @@\n from unittest import TextTestResult\n \n-from six import get_unbound_function\n from twisted.internet import defer\n from twisted.python import failure\n from twisted.trial import unittest\n@@ -395,8 +394,8 @@ class ContractsManagerTest(unittest.TestCase):\n         with MockServer() as mockserver:\n             contract_doc = '@url {}'.format(mockserver.url('/status?n=200'))\n \n-            get_unbound_function(TestSameUrlSpider.parse_first).__doc__ = contract_doc\n-            get_unbound_function(TestSameUrlSpider.parse_second).__doc__ = contract_doc\n+            TestSameUrlSpider.parse_first.__doc__ = contract_doc\n+            TestSameUrlSpider.parse_second.__doc__ = contract_doc\n \n             crawler = CrawlerRunner().create_crawler(TestSameUrlSpider)\n             yield crawler.crawl()\n\n@@ -1,12 +1,12 @@\n import os\n import random\n import time\n+from io import BytesIO\n from tempfile import mkdtemp\n from shutil import rmtree\n from unittest import mock\n from urllib.parse import urlparse\n \n-from six import BytesIO\n from twisted.trial import unittest\n from twisted.internet import defer\n \n\n@@ -1,7 +1,6 @@\n import unittest\n import warnings\n \n-from six import assertRaisesRegex\n from w3lib.http import basic_auth_header\n \n from scrapy import Request\n@@ -177,8 +176,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n         self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n \n     def test_too_few_arguments_error(self):\n-        assertRaisesRegex(\n-            self,\n+        self.assertRaisesRegex(\n             ValueError,\n             r\"too few arguments|the following arguments are required:\\s*url\",\n             lambda: curl_to_request_kwargs(\"curl\"),\n@@ -194,8 +192,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n             self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n \n         # case 2: ignore_unknown_options=False (raise exception):\n-        assertRaisesRegex(\n-            self,\n+        self.assertRaisesRegex(\n             ValueError,\n             \"Unrecognized options:.*--bar.*--baz\",\n             lambda: curl_to_request_kwargs(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c4e5b68ea0e077033fde5da28c6dc4fdd859d92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 507 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -72,7 +72,6 @@ setup(\n         'pyOpenSSL>=16.2.0',\n         'queuelib>=1.4.2',\n         'service_identity>=16.0.0',\n-        'six>=1.10.0',\n         'w3lib>=1.17.0',\n         'zope.interface>=4.1.3',\n         'protego>=0.1.15',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#439a3e59b8e858441f8d97dbc32f398db392330d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 30 | Churn Cumulative: 472 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -315,6 +315,7 @@ class LocalCache(collections.OrderedDict):\n         self.limit = limit\n \n     def __setitem__(self, key, value):\n+        if self.limit:\n             while len(self) >= self.limit:\n                 self.popitem(last=False)\n         super(LocalCache, self).__setitem__(key, value)\n\n@@ -7,7 +7,7 @@ if six.PY2:\n else:\n     from collections.abc import Mapping, MutableMapping\n \n-from scrapy.utils.datatypes import CaselessDict, SequenceExclude\n+from scrapy.utils.datatypes import CaselessDict, SequenceExclude, LocalCache\n \n \n __doctests__ = ['scrapy.utils.datatypes']\n@@ -242,6 +242,31 @@ class SequenceExcludeTest(unittest.TestCase):\n         for v in [-3, \"test\", 1.1]:\n             self.assertNotIn(v, d)\n \n+\n+class LocalCacheTest(unittest.TestCase):\n+\n+    def test_cache_with_limit(self):\n+        cache = LocalCache(limit=2)\n+        cache['a'] = 1\n+        cache['b'] = 2\n+        cache['c'] = 3\n+        self.assertEqual(len(cache), 2)\n+        self.assertNotIn('a', cache)\n+        self.assertIn('b', cache)\n+        self.assertIn('c', cache)\n+        self.assertEqual(cache['b'], 2)\n+        self.assertEqual(cache['c'], 3)\n+\n+    def test_cache_without_limit(self):\n+        max = 10**4\n+        cache = LocalCache()\n+        for x in range(max):\n+            cache[str(x)] = x\n+        self.assertEqual(len(cache), max)\n+        for x in range(max):\n+            self.assertIn(str(x), cache)\n+            self.assertEqual(cache[str(x)], x)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fed9fbe62d54175d70660498f2fba6b5b7f68a92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 182 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ if six.PY2:\n else:\n     from collections.abc import Mapping, MutableMapping\n \n-from scrapy.utils.datatypes import CaselessDict, SequenceExclude, LocalCache\n+from scrapy.utils.datatypes import CaselessDict, LocalCache, SequenceExclude\n \n \n __doctests__ = ['scrapy.utils.datatypes']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#613c66a034cebc885b90c8ba2a76aea2109ef72e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 190 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -258,12 +258,12 @@ class LocalCacheTest(unittest.TestCase):\n         self.assertEqual(cache['c'], 3)\n \n     def test_cache_without_limit(self):\n-        max = 10**4\n+        maximum = 10**4\n         cache = LocalCache()\n-        for x in range(max):\n+        for x in range(maximum):\n             cache[str(x)] = x\n-        self.assertEqual(len(cache), max)\n-        for x in range(max):\n+        self.assertEqual(len(cache), maximum)\n+        for x in range(maximum):\n             self.assertIn(str(x), cache)\n             self.assertEqual(cache[str(x)], x)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe31695ba0266deaa94222fc01885b7270af4294", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1258 | Contributors (this commit): 23 | Commits (past 90d): 9 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import json\n import xmlrpc.client as xmlrpclib\n import warnings\n from unittest import mock\n-from urllib.parse import parse_qs, unquote, unquote_to_bytes, urlparse\n+from urllib.parse import parse_qs, unquote_to_bytes, urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_unicode\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98caf055b5a343ff663f2c1150b301a3f4546a16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 484 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -136,7 +136,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     \"\"\"\n     if settings is None:\n         if crawler is None:\n-            raise ValueError(\"Specifiy at least one of settings and crawler.\")\n+            raise ValueError(\"Specify at least one of settings and crawler.\")\n         settings = crawler.settings\n     if crawler and hasattr(objcls, 'from_crawler'):\n         return objcls.from_crawler(crawler, *args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8b1e46e85fbcdf22408d320f3cc61b6e802c5e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 10 | Churn Cumulative: 132 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,3 +19,13 @@ if six.PY3:\n def chdir(tmpdir):\n     \"\"\"Change to pytest-provided temporary directory\"\"\"\n     tmpdir.chdir()\n+\n+\n+def pytest_collection_modifyitems(session, config, items):\n+    # Avoid executing tests when executing `--flake8` flag (pytest-flake8)\n+    try:\n+        from pytest_flake8 import Flake8Item\n+        if config.getoption('--flake8'):\n+            items[:] = [item for item in items if isinstance(item, Flake8Item)]\n+    except ImportError:\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c377c14e3263ce3c2bffa446cd0965006e845664", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 23 | Files Changed: 20 | Hunks: 20 | Methods Changed: 0 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 23 | Churn Cumulative: 2838 | Contributors (this commit): 41 | Commits (past 90d): 33 | Contributors (cumulative): 122 | DMM Complexity: None\n\nDIFF:\n@@ -96,4 +96,3 @@ class Command(ScrapyCommand):\n             result.printErrors()\n             result.printSummary(start, stop)\n             self.exitcode = int(not result.wasSuccessful())\n-\n\n@@ -119,4 +119,3 @@ class Command(ScrapyCommand):\n         _templates_base_dir = self.settings['TEMPLATES_DIR'] or \\\n             join(scrapy.__path__[0], 'templates')\n         return join(_templates_base_dir, 'project')\n-\n\n@@ -30,4 +30,3 @@ class Command(ScrapyCommand):\n                 print(patt % (name, version))\n         else:\n             print(\"Scrapy %s\" % scrapy.__version__)\n-\n\n@@ -112,4 +112,3 @@ class FTPDownloadHandler(object):\n                 httpcode = self.CODE_MAPPING.get(ftpcode, self.CODE_MAPPING[\"default\"])\n                 return Response(url=request.url, status=httpcode, body=to_bytes(message))\n         raise result.type(result.value)\n-\n\n@@ -157,4 +157,3 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     def gotHeaders(self, headers):\n         self.headers_time = time()\n         self.response_headers = headers\n-\n\n@@ -244,4 +244,3 @@ class Scraper(object):\n             return self.signals.send_catch_log_deferred(\n                 signal=signals.item_scraped, item=output, response=response,\n                 spider=spider)\n-\n\n@@ -91,5 +91,3 @@ class Headers(CaselessDict):\n     def __copy__(self):\n         return self.__class__(self)\n     copy = __copy__\n-\n-\n\n@@ -15,4 +15,3 @@ class ISpiderLoader(Interface):\n \n     def find_by_request(request):\n         \"\"\"Return the list of spiders names that can handle the given request\"\"\"\n-\n\n@@ -39,4 +39,3 @@ class Link(object):\n     def __repr__(self):\n         return 'Link(url=%r, text=%r, fragment=%r, nofollow=%r)' % \\\n             (self.url, self.text, self.fragment, self.nofollow)\n-\n\n@@ -133,4 +133,3 @@ class CSVFeedSpider(Spider):\n             raise NotConfigured('You must define parse_row method in order to scrape this CSV feed')\n         response = self.adapt_response(response)\n         return self.parse_rows(response)\n-\n\n@@ -29,4 +29,3 @@ class InitSpider(Spider):\n         spider\n         \"\"\"\n         return self.initialized()\n-\n\n@@ -80,5 +80,3 @@ class DummyStatsCollector(StatsCollector):\n \n     def min_value(self, key, value, spider=None):\n         pass\n-\n-\n\n@@ -34,4 +34,3 @@ def decode_chunked_transfer(chunked_body):\n         body += t[:size]\n         t = t[size+2:]\n     return body\n-\n\n@@ -12,4 +12,3 @@ class TestExtension(object):\n \n class DummyExtension(object):\n     pass\n-\n\n@@ -2,4 +2,3 @@\n TEST_DEFAULT = 'defvalue'\n \n TEST_DICT = {'key': 'val'}\n-\n\n@@ -40,4 +40,3 @@ class TestDepthMiddleware(TestCase):\n \n     def tearDown(self):\n         self.stats.close_spider(self.spider, '')\n-\n\n@@ -18,4 +18,3 @@ class TestUrlLengthMiddleware(TestCase):\n         spider = Spider('foo')\n         out = list(mw.process_spider_output(res, reqs, spider))\n         self.assertEqual(out, [short_url_req])\n-\n\n@@ -244,4 +244,3 @@ class SequenceExcludeTest(unittest.TestCase):\n \n if __name__ == \"__main__\":\n     unittest.main()\n-\n\n@@ -16,5 +16,3 @@ class ChunkedTest(unittest.TestCase):\n             \"This is the data in the first chunk\\r\\n\" +\n             \"and this is the second one\\r\\n\" +\n             \"consequence\")\n-\n-\n\n@@ -34,4 +34,3 @@ class UtilsSpidersTestCase(unittest.TestCase):\n \n if __name__ == \"__main__\":\n     unittest.main()\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1df5755699eac5a98239ae73dfb82908706bf03b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,16 @@\n+from doctest import ELLIPSIS\n+\n+from sybil import Sybil\n+from sybil.parsers.codeblock import CodeBlockParser\n+from sybil.parsers.doctest import DocTestParser\n+from sybil.parsers.skip import skip\n+\n+\n+pytest_collect_file = Sybil(\n+    parsers=[\n+        DocTestParser(optionflags=ELLIPSIS),\n+        CodeBlockParser(future_imports=['print_function']),\n+        skip,\n+    ],\n+    pattern='*.rst',\n+).pytest()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6cde428af43a5c8268208c6e4e239ab5ce507af4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 59 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): -18/0 | Churn Δ: 59 | Churn Cumulative: 351 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -235,65 +235,6 @@ class CaselessDict(dict):\n         return dict.pop(self, self.normkey(key), *args)\n \n \n-class MergeDict(object):\n-    \"\"\"\n-    A simple class for creating new \"virtual\" dictionaries that actually look\n-    up values in more than one dictionary, passed in the constructor.\n-\n-    If a key appears in more than one of the given dictionaries, only the\n-    first occurrence will be used.\n-    \"\"\"\n-    def __init__(self, *dicts):\n-        warnings.warn(\n-            \"scrapy.utils.datatypes.MergeDict is deprecated in favor \"\n-            \"of collections.ChainMap (introduced in Python 3.3)\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2,\n-        )\n-        self.dicts = dicts\n-\n-    def __getitem__(self, key):\n-        for dict_ in self.dicts:\n-            try:\n-                return dict_[key]\n-            except KeyError:\n-                pass\n-        raise KeyError\n-\n-    def __copy__(self):\n-        return self.__class__(*self.dicts)\n-\n-    def get(self, key, default=None):\n-        try:\n-            return self[key]\n-        except KeyError:\n-            return default\n-\n-    def getlist(self, key):\n-        for dict_ in self.dicts:\n-            if key in dict_.keys():\n-                return dict_.getlist(key)\n-        return []\n-\n-    def items(self):\n-        item_list = []\n-        for dict_ in self.dicts:\n-            item_list.extend(dict_.items())\n-        return item_list\n-\n-    def has_key(self, key):\n-        for dict_ in self.dicts:\n-            if key in dict_:\n-                return True\n-        return False\n-\n-    __contains__ = has_key\n-\n-    def copy(self):\n-        \"\"\"Returns a copy of this object.\"\"\"\n-        return self.__copy__()\n-\n-\n class LocalCache(collections.OrderedDict):\n     \"\"\"Dictionary with a finite number of keys.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6bbb2819707a2202c87abd6b3dba6af13a7cc85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 21 | Files Changed: 6 | Hunks: 19 | Methods Changed: 7 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 47 | Churn Cumulative: 3006 | Contributors (this commit): 33 | Commits (past 90d): 30 | Contributors (cumulative): 63 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,5 @@\n import logging\n import signal\n-import sys\n import warnings\n \n from twisted.internet import reactor, defer\n\n@@ -4,7 +4,6 @@ Item Exporters are used to export/serialize items into different formats.\n \n import csv\n import io\n-import sys\n import pprint\n import marshal\n import warnings\n\n@@ -4,6 +4,8 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n+\n+\n class Link(object):\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n \n\n@@ -1,5 +1,3 @@\n-import sys\n-\n from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n\n@@ -28,9 +28,14 @@ from mitmproxy.tools.main import mitmdump\n sys.argv[0] = \"mitmdump\"\n sys.exit(mitmdump())\n         \"\"\"\n-        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n-            'keys', 'mitmproxy-ca.pem')\n-        self.proc = Popen([sys.executable,\n+        cert_path = os.path.join(\n+            os.path.abspath(os.path.dirname(__file__)),\n+            'keys',\n+            'mitmproxy-ca.pem'\n+        )\n+        self.proc = Popen(\n+            [\n+                sys.executable,\n                 '-c', script,\n                 '--listen-host', '127.0.0.1',\n                 '--listen-port', '0',\n@@ -38,7 +43,9 @@ sys.exit(mitmdump())\n                 '--certs', cert_path,\n                 '--ssl-insecure',\n             ],\n-                           stdout=PIPE, env=get_testenv())\n+            stdout=PIPE,\n+            env=get_testenv()\n+        )\n         line = self.proc.stdout.readline().decode('utf-8')\n         host_port = re.search(r'listening at http://([^:]+:\\d+)', line).group(1)\n         address = 'http://%s:%s@%s' % (self.auth_user, self.auth_pass, host_port)\n@@ -75,9 +82,9 @@ class ProxyConnectTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel(self):\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as logs:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n+        self._assert_got_response_code(200, logs)\n \n     @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n     @defer.inlineCallbacks\n@@ -85,35 +92,35 @@ class ProxyConnectTestCase(TestCase):\n         proxy = os.environ['https_proxy']\n         os.environ['https_proxy'] = proxy + '?noconnect'\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as logs:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n+        self._assert_got_response_code(200, logs)\n \n     @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info.minor >= 6)\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as logs:\n             yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n-        self._assert_got_tunnel_error(l)\n+        self._assert_got_tunnel_error(logs)\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n         os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as logs:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         # The proxy returns a 407 error code but it does not reach the client;\n         # he just sees a TunnelError.\n-        self._assert_got_tunnel_error(l)\n+        self._assert_got_tunnel_error(logs)\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_without_leak_proxy_authorization_header(self):\n         request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n         crawler = get_crawler(SingleRequestSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as logs:\n             yield crawler.crawl(seed=request)\n-        self._assert_got_response_code(200, l)\n+        self._assert_got_response_code(200, logs)\n         echo = json.loads(crawler.spider.meta['responses'][0].text)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n@@ -122,9 +129,9 @@ class ProxyConnectTestCase(TestCase):\n     def test_https_noconnect_auth_error(self):\n         os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n         crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n+        with LogCapture() as logs:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(407, l)\n+        self._assert_got_response_code(407, logs)\n \n     def _assert_got_response_code(self, code, log):\n         print(log)\n\n@@ -7,7 +7,7 @@ from itertools import count\n \n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n-    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode,\n+    WeakKeyCache, get_func_args, to_bytes, to_unicode,\n     without_none_values, MutableChain)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
