{"custom_id": "keras#598cbe2b0b7c98265a6240a9b9dbf7e1e1461d0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 121 | Lines Deleted: 92 | Files Changed: 13 | Hunks: 45 | Methods Changed: 25 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 213 | Churn Cumulative: 13362 | Contributors (this commit): 12 | Commits (past 90d): 144 | Contributors (cumulative): 55 | DMM Complexity: 1.0\n\nDIFF:\n@@ -162,13 +162,13 @@ def softmax(x, axis=-1):\n \n \n @keras_core_export(\"keras_core.activations.elu\")\n-def elu(x):\n+def elu(x, alpha=1.0):\n     \"\"\"Exponential Linear Unit.\n \n     The exponential linear unit (ELU) with `alpha > 0` is define as:\n \n     - `x` if `x > 0`\n-    - `exp(x) - 1` if `x < 0`\n+    - alpha * `exp(x) - 1` if `x < 0`\n \n     ELUs have negative values which pushes the mean of the activations\n     closer to zero.\n@@ -186,7 +186,7 @@ def elu(x):\n \n     - [Clevert et al., 2016](https://arxiv.org/abs/1511.07289)\n     \"\"\"\n-    return ops.elu(x)\n+    return ops.elu(x, alpha=alpha)\n \n \n @keras_core_export(\"keras_core.activations.selu\")\n\n@@ -56,8 +56,8 @@ def hard_sigmoid(x):\n     return jnn.hard_sigmoid(x)\n \n \n-def elu(x):\n-    return jnn.elu(x)\n+def elu(x, alpha=1.0):\n+    return jnn.elu(x, alpha=alpha)\n \n \n def selu(x):\n\n@@ -57,8 +57,12 @@ def hard_sigmoid(x):\n     return tf.clip_by_value(x, 0.0, 1.0)\n \n \n-def elu(x):\n-    return tf.nn.elu(x)\n+def elu(x, alpha=1.0):\n+    res = tf.nn.elu(x)\n+    if alpha == 1:\n+        return res\n+    else:\n+        return tf.where(x > 0, res, alpha * res)\n \n \n def selu(x):\n\n@@ -47,6 +47,10 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the uniform distribution.\n \n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n     Args:\n         shape: The shape of the random values to generate.\n         minval: Floats, defaults to 0. Lower bound of the range of\n@@ -77,6 +81,10 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     \"\"\"Produce random number based on the truncated normal distribution.\n \n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n+\n     Args:\n         shape: The shape of the random values to generate.\n         mean: Floats, defaults to 0. Mean of the random values to generate.\n@@ -95,14 +103,15 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             across multiple calls, use as seed an instance\n             of `keras_core.backend.SeedGenerator`.\n     \"\"\"\n-    x = torch.empty(shape)\n-    # TODO: setting seed globally via `manual_seed` might create side effects.\n-    if seed is not None:\n-        seed_val, _ = draw_seed(seed)\n-        torch.manual_seed(int(seed_val))\n-    return torch.nn.init.trunc_normal_(\n-        x, mean=mean, std=stddev, a=-stddev * 2, b=stddev * 2\n-    )\n+    # Take a larger standard normal dist, discard values outside 2 * stddev\n+    # Offset by mean and stddev\n+    x = normal(shape + (4,), mean=0, stddev=1, dtype=dtype, seed=seed)\n+    valid = (x > -2) & (x < 2)\n+    indexes = valid.max(-1, keepdim=True)[1]\n+    trunc_x = torch.empty(shape)\n+    trunc_x.data.copy_(x.gather(-1, indexes).squeeze(-1))\n+    trunc_x.data.mul_(stddev).add_(mean)\n+    return trunc_x\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n\n@@ -8,11 +8,9 @@ class Activation(Layer):\n     \"\"\"Applies an activation function to an output.\n \n     Args:\n-        activation: Activation function. It could be\n-            a callable, or the name of an activation\n-            from the `keras_core.activations` namespace.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        activation: Activation function. It could be a callable, or the name of\n+            an activation from the `keras_core.activations` namespace.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Example:\n \n\n@@ -10,21 +10,22 @@ class ELU(Layer):\n     Formula:\n \n     ```\n-    f(x) = (exp(x) - 1.) for x < 0\n+    f(x) = alpha * (exp(x) - 1.) for x < 0\n     f(x) = x for x >= 0\n     ```\n \n     Args:\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        alpha: float, slope of negative section. Defaults to 1.0.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, **kwargs):\n+    def __init__(self, alpha=1.0, **kwargs):\n         super().__init__(**kwargs)\n+        self.alpha = alpha\n         self.supports_masking = True\n \n     def call(self, inputs):\n-        return activations.elu(inputs)\n+        return activations.elu(inputs, alpha=self.alpha)\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core import testing\n from keras_core.layers.activations import elu\n+import tensorflow as tf\n \n \n class ELUTest(testing.TestCase):\n@@ -17,7 +18,12 @@ class ELUTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-        x = np.random.random((2, 5))\n+    def test_correctness(self):\n+        x = np.random.random((2, 2, 5))\n         elu_layer = elu.ELU()\n-        result = elu_layer(x[np.newaxis, :])[0]\n-        self.assertAllClose(result, x, rtol=1e-05)\n+        tf_elu_layer = tf.keras.layers.ELU()\n+        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n+\n+        elu_layer = elu.ELU(alpha=0.7)\n+        tf_elu_layer = tf.keras.layers.ELU(alpha=0.7)\n+        self.assertAllClose(elu_layer(x), tf_elu_layer(x))\n\n@@ -13,46 +13,37 @@ class PReLU(Layer):\n \n     Formula:\n     ``` python\n-    f(x) = negative_slope * x for x < 0\n+    f(x) = alpha * x for x < 0\n     f(x) = x for x >= 0\n     ```\n-    where `negative_slope` is a learned array with the same shape as x.\n+    where `alpha` is a learned array with the same shape as x.\n \n     Args:\n-        negative_slope_initializer: Initializer function for the weights.\n-        negative_slope_regularizer: Regularizer for the weights.\n-        negative_slope_constraint: Constraint for the weights.\n-        shared_axes: The axes along which to share learnable\n-            parameters for the activation function.\n-            For example, if the incoming feature maps\n-            are from a 2D convolution\n-            with output shape `(batch, height, width, channels)`,\n-            and you wish to share parameters across space\n-            so that each filter only has one set of parameters,\n+        alpha_initializer: Initializer function for the weights.\n+        alpha_regularizer: Regularizer for the weights.\n+        alpha_constraint: Constraint for the weights.\n+        shared_axes: The axes along which to share learnable parameters for the\n+            activation function. For example, if the incoming feature maps are\n+            from a 2D convolution with output shape\n+            `(batch, height, width, channels)`, and you wish to share parameters\n+            across space so that each filter only has one set of parameters,\n             set `shared_axes=[1, 2]`.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n     def __init__(\n         self,\n-        negative_slope_initializer=\"Zeros\",\n-        negative_slope_regularizer=None,\n-        negative_slope_constraint=None,\n+        alpha_initializer=\"Zeros\",\n+        alpha_regularizer=None,\n+        alpha_constraint=None,\n         shared_axes=None,\n         **kwargs\n     ):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n-        self.negative_slope_initializer = initializers.get(\n-            negative_slope_initializer\n-        )\n-        self.negative_slope_regularizer = regularizers.get(\n-            negative_slope_regularizer\n-        )\n-        self.negative_slope_constraint = constraints.get(\n-            negative_slope_constraint\n-        )\n+        self.alpha_initializer = initializers.get(alpha_initializer)\n+        self.alpha_regularizer = regularizers.get(alpha_regularizer)\n+        self.alpha_constraint = constraints.get(alpha_constraint)\n         if shared_axes is None:\n             self.shared_axes = None\n         elif not isinstance(shared_axes, (list, tuple)):\n@@ -65,12 +56,12 @@ class PReLU(Layer):\n         if self.shared_axes is not None:\n             for i in self.shared_axes:\n                 param_shape[i - 1] = 1\n-        self.negative_slope = self.add_weight(\n+        self.alpha = self.add_weight(\n             shape=param_shape,\n-            name=\"negative_slope\",\n-            initializer=self.negative_slope_initializer,\n-            regularizer=self.negative_slope_regularizer,\n-            constraint=self.negative_slope_constraint,\n+            name=\"alpha\",\n+            initializer=self.alpha_initializer,\n+            regularizer=self.alpha_regularizer,\n+            constraint=self.alpha_constraint,\n         )\n         # Set input spec\n         axes = {}\n@@ -83,21 +74,21 @@ class PReLU(Layer):\n \n     def call(self, inputs):\n         pos = activations.relu(inputs)\n-        neg = -self.negative_slope * activations.relu(-inputs)\n+        neg = -self.alpha * activations.relu(-inputs)\n         return pos + neg\n \n     def get_config(self):\n         config = super().get_config()\n         config.update(\n             {\n-                \"negative_slope_initializer\": initializers.serialize(\n-                    self.negative_slope_initializer\n+                \"alpha_initializer\": initializers.serialize(\n+                    self.alpha_initializer\n                 ),\n-                \"negative_slope_regularizer\": regularizers.serialize(\n-                    self.negative_slope_regularizer\n+                \"alpha_regularizer\": regularizers.serialize(\n+                    self.alpha_regularizer\n                 ),\n-                \"negative_slope_constraint\": constraints.serialize(\n-                    self.negative_slope_constraint\n+                \"alpha_constraint\": constraints.serialize(\n+                    self.alpha_constraint\n                 ),\n                 \"shared_axes\": self.shared_axes,\n             }\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras_core import testing\n from keras_core.layers.activations import prelu\n+import tensorflow as tf\n \n \n class PReLUTest(testing.TestCase):\n@@ -9,9 +10,9 @@ class PReLUTest(testing.TestCase):\n         self.run_layer_test(\n             prelu.PReLU,\n             init_kwargs={\n-                \"negative_slope_initializer\": \"zeros\",\n-                \"negative_slope_regularizer\": \"L1\",\n-                \"negative_slope_constraint\": \"MaxNorm\",\n+                \"alpha_initializer\": \"zeros\",\n+                \"alpha_regularizer\": \"L1\",\n+                \"alpha_constraint\": \"MaxNorm\",\n                 \"shared_axes\": 1,\n             },\n             input_shape=(2, 3, 4),\n@@ -19,15 +20,25 @@ class PReLUTest(testing.TestCase):\n         )\n \n     def test_prelu_correctness(self):\n+        inputs = np.random.randn(2, 10, 5, 3)\n         prelu_layer = prelu.PReLU(\n-            negative_slope_initializer=\"glorot_uniform\",\n-            negative_slope_regularizer=\"l1\",\n-            negative_slope_constraint=\"non_neg\",\n-            shared_axes=None,\n+            alpha_initializer=\"glorot_uniform\",\n+            alpha_regularizer=\"l1\",\n+            alpha_constraint=\"non_neg\",\n+            shared_axes=(1, 2),\n         )\n-        test_input = np.random.randn(10, 5)\n-        result = prelu_layer(test_input)\n-        expected_output = np.maximum(\n-            0, test_input\n-        ) + prelu_layer.negative_slope.numpy() * np.minimum(0, test_input)\n-        self.assertAllClose(result, expected_output)\n+        tf_prelu_layer = tf.keras.layers.PReLU(\n+            alpha_initializer=\"glorot_uniform\",\n+            alpha_regularizer=\"l1\",\n+            alpha_constraint=\"non_neg\",\n+            shared_axes=(1, 2),\n+        )\n+\n+        prelu_layer.build(inputs.shape)\n+        tf_prelu_layer.build(inputs.shape)\n+\n+        weights = np.random.random((1, 1, 3))\n+        prelu_layer.alpha.assign(weights)\n+        tf_prelu_layer.alpha.assign(weights)\n+\n+        self.assertAllClose(prelu_layer(inputs), tf_prelu_layer(inputs))\n\n@@ -17,7 +17,11 @@ class ReLU(Layer):\n \n     Example:\n     ``` python\n-    relu_layer = relu.ReLU(max_value=10, negative_slope=0.5, threshold=0)\n+    relu_layer = keras_core.layers.activations.ReLU(\n+        max_value=10,\n+        negative_slope=0.5,\n+        threshold=0,\n+    )\n     input = np.array([-10, -5, 0.0, 5, 10])\n     result = relu_layer(input)\n     # result = [-5. , -2.5,  0. ,  5. , 10.]\n@@ -26,12 +30,10 @@ class ReLU(Layer):\n     Args:\n         max_value: Float >= 0. Maximum activation value. None means unlimited.\n             Defaults to `None`.\n-        negative_slope: Float >= 0. Negative slope coefficient.\n-            Defaults to 0.0.\n+        negative_slope: Float >= 0. Negative slope coefficient. Defaults to 0.0.\n         threshold: Float >= 0. Threshold value for thresholded activation.\n             Defaults to 0.0.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n     \"\"\"\n \n     def __init__(\n\n@@ -15,7 +15,7 @@ class Softmax(Layer):\n     ```\n \n     Example:\n-    >>>softmax_layer = Softmax()\n+    >>>softmax_layer = keras_core.layers.activations.Softmax()\n     >>>input = np.array([1.0, 2.0, 1.0])\n     >>>result = softmax_layer(input)\n     [0.21194157, 0.5761169, 0.21194157]\n@@ -24,11 +24,10 @@ class Softmax(Layer):\n     Args:\n         axis: Integer, or list of Integers, axis along which the softmax\n             normalization is applied.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n+        **kwargs: Base layer keyword arguments, such as `name` and `dtype`.\n \n     Call arguments:\n-        inputs: The inputs, or logits to the softmax layer.\n+        inputs: The inputs (logits) to the softmax layer.\n         mask: A boolean mask of the same shape as `inputs`. The mask\n             specifies 1 to keep and 0 to mask. Defaults to `None`.\n \n\n@@ -239,8 +239,12 @@ def hard_sigmoid(x):\n \n \n class Elu(Operation):\n+    def __init__(self, alpha=1.0):\n+        super().__init__()\n+        self.alpha = alpha\n+\n     def call(self, x):\n-        return backend.nn.elu(x)\n+        return backend.nn.elu(x, alpha=self.alpha)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(x.shape, dtype=x.dtype)\n@@ -249,10 +253,10 @@ class Elu(Operation):\n @keras_core_export(\n     [\"keras_core.operations.elu\", \"keras_core.operations.nn.elu\"]\n )\n-def elu(x):\n+def elu(x, alpha=1.0):\n     if any_symbolic_tensors((x,)):\n-        return Elu().symbolic_call(x)\n-    return backend.nn.elu(x)\n+        return Elu(alpha).symbolic_call(x)\n+    return backend.nn.elu(x, alpha=alpha)\n \n \n class Selu(Operation):\n\n@@ -637,6 +637,10 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             knn.elu(x),\n             [-0.63212055, 0, 1, 2, 3],\n         )\n+        self.assertAllClose(\n+            knn.elu(x, alpha=0.5),\n+            [-0.31606027, 0, 1, 2, 3],\n+        )\n \n     def test_selu(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2cae421d476a980e516a19ba5d429825e8765e22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 166 | Lines Deleted: 69 | Files Changed: 8 | Hunks: 22 | Methods Changed: 17 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 235 | Churn Cumulative: 1673 | Contributors (this commit): 1 | Commits (past 90d): 18 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,7 +93,7 @@ class ConvLSTMCell(Layer, DropoutRNNCell):\n         data_format=None,\n         dilation_rate=1,\n         activation=\"tanh\",\n-        recurrent_activation=\"hard_sigmoid\",\n+        recurrent_activation=\"sigmoid\",\n         use_bias=True,\n         kernel_initializer=\"glorot_uniform\",\n         recurrent_initializer=\"orthogonal\",\n@@ -153,30 +153,30 @@ class ConvLSTMCell(Layer, DropoutRNNCell):\n         self.input_spec = InputSpec(ndim=rank + 2)\n         self.state_size = -1  # Custom, defined in methods\n \n-    def build(self, input_shape):\n+    def build(self, inputs_shape, states_shape=None):\n         if self.data_format == \"channels_first\":\n             channel_axis = 1\n-            self.spatial_dims = input_shape[2:]\n+            self.spatial_dims = inputs_shape[2:]\n         else:\n             channel_axis = -1\n-            self.spatial_dims = input_shape[1:-1]\n+            self.spatial_dims = inputs_shape[1:-1]\n         if None in self.spatial_dims:\n             raise ValueError(\n                 \"ConvLSTM layers only support static \"\n                 \"input shapes for the spatial dimension. \"\n-                f\"Received invalid input shape: input_shape={input_shape}\"\n+                f\"Received invalid input shape: input_shape={inputs_shape}\"\n             )\n-        if input_shape[channel_axis] is None:\n+        if inputs_shape[channel_axis] is None:\n             raise ValueError(\n                 \"The channel dimension of the inputs (last axis) should be \"\n                 \"defined. Found None. Full input shape received: \"\n-                f\"input_shape={input_shape}\"\n+                f\"input_shape={inputs_shape}\"\n             )\n         self.input_spec = InputSpec(\n-            ndim=self.rank + 3, shape=(None,) + input_shape[1:]\n+            ndim=self.rank + 3, shape=(None,) + inputs_shape[1:]\n         )\n \n-        input_dim = input_shape[channel_axis]\n+        input_dim = inputs_shape[channel_axis]\n         self.input_dim = input_dim\n         self.kernel_shape = self.kernel_size + (input_dim, self.filters * 4)\n         recurrent_kernel_shape = self.kernel_size + (\n@@ -234,13 +234,13 @@ class ConvLSTMCell(Layer, DropoutRNNCell):\n         h_tm1 = states[0]  # previous memory state\n         c_tm1 = states[1]  # previous carry state\n \n-        dp_mask = self.get_dropout_mask(inputs)\n-        rec_dp_mask = self.get_recurrent_dropout_mask(h_tm1)\n+        # dp_mask = self.get_dropout_mask(inputs)\n+        # rec_dp_mask = self.get_recurrent_dropout_mask(h_tm1)\n \n-        if training and 0.0 < self.dropout < 1.0:\n-            inputs *= dp_mask\n-        if training and 0.0 < self.recurrent_dropout < 1.0:\n-            h_tm1 *= rec_dp_mask\n+        # if training and 0.0 < self.dropout < 1.0:\n+        #     inputs *= dp_mask\n+        # if training and 0.0 < self.recurrent_dropout < 1.0:\n+        #     h_tm1 *= rec_dp_mask\n \n         inputs_i = inputs\n         inputs_f = inputs\n@@ -466,7 +466,7 @@ class ConvLSTM(RNN):\n         data_format=None,\n         dilation_rate=1,\n         activation=\"tanh\",\n-        recurrent_activation=\"hard_sigmoid\",\n+        recurrent_activation=\"sigmoid\",\n         use_bias=True,\n         kernel_initializer=\"glorot_uniform\",\n         recurrent_initializer=\"orthogonal\",\n\n@@ -129,7 +129,7 @@ class ConvLSTM1D(ConvLSTM):\n         data_format=None,\n         dilation_rate=1,\n         activation=\"tanh\",\n-        recurrent_activation=\"hard_sigmoid\",\n+        recurrent_activation=\"sigmoid\",\n         use_bias=True,\n         kernel_initializer=\"glorot_uniform\",\n         recurrent_initializer=\"orthogonal\",\n\n@@ -1,3 +1,6 @@\n+import numpy as np\n+\n+from keras_core import initializers\n from keras_core import layers\n from keras_core import testing\n \n@@ -43,25 +46,22 @@ class ConvLSTM1DTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    # TODO: correctness testing\n-\n-    # def test_correctness(self):\n-    #     sequence = np.arange(120).reshape((2, 3, 4, 5)).astype(\"float32\")\n-    #     layer = layers.ConvLSTM1D(\n-    #         filters=2,\n-    #         kernel_size=3,\n-    #         kernel_initializer=initializers.Constant(0.001),\n-    #         recurrent_initializer=initializers.Constant(0.0),\n-    #         bias_initializer=initializers.Constant(0.3),\n-    #         use_bias=False,\n-    #     )\n-    #     output = layer(sequence)\n-    #     self.assertAllClose(\n-    #         np.array(\n-    #             [\n-    #                 [[0.49877906, 0.49877906], [0.5447451, 0.5447451]],\n-    #                 [[0.94260275, 0.94260275], [0.95974874, 0.95974874]],\n-    #             ]\n-    #         ),\n-    #         output,\n-    #     )\n+    def test_correctness(self):\n+        sequence = np.arange(120).reshape((2, 3, 4, 5)).astype(\"float32\") / 10\n+        layer = layers.ConvLSTM1D(\n+            filters=2,\n+            kernel_size=3,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [[0.40807986, 0.40807986], [0.46421072, 0.46421072]],\n+                    [[0.80933154, 0.80933154], [0.8233646, 0.8233646]],\n+                ]\n+            ),\n+            output,\n+        )\n\n@@ -129,7 +129,7 @@ class ConvLSTM2D(ConvLSTM):\n         data_format=None,\n         dilation_rate=1,\n         activation=\"tanh\",\n-        recurrent_activation=\"hard_sigmoid\",\n+        recurrent_activation=\"sigmoid\",\n         use_bias=True,\n         kernel_initializer=\"glorot_uniform\",\n         recurrent_initializer=\"orthogonal\",\n\n@@ -1,3 +1,6 @@\n+import numpy as np\n+\n+from keras_core import initializers\n from keras_core import layers\n from keras_core import testing\n \n@@ -43,30 +46,30 @@ class ConvLSTM2DTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    # TODO: correctness testing\n-\n-    # def test_correctness(self):\n-    #     sequence = np.arange(480).reshape((2, 3, 4, 4, 5)).astype(\"float32\")\n-    #     layer = layers.ConvLSTM2D(\n-    #         filters=2,\n-    #         kernel_size=3,\n-    #         kernel_initializer=initializers.Constant(0.0001),\n-    #         recurrent_initializer=initializers.Constant(0.01),\n-    #         bias_initializer=initializers.Constant(0.01),\n-    #     )\n-    #     output = layer(sequence)\n-    #     self.assertAllClose(\n-    #         np.array(\n-    #             [\n-    #                 [\n-    #                     [[0.4320268, 0.4320268], [0.4475501, 0.4475501]],\n-    #                     [[0.49229687, 0.49229687], [0.50656533, 0.50656533]],\n-    #                 ],\n-    #                 [\n-    #                     [[0.8781725, 0.8781725], [0.88340145, 0.88340145]],\n-    #                     [[0.8988858, 0.8988858], [0.9039862, 0.9039862]],\n-    #                 ],\n-    #             ]\n-    #         ),\n-    #         output,\n-    #     )\n+    def test_correctness(self):\n+        sequence = (\n+            np.arange(480).reshape((2, 3, 4, 4, 5)).astype(\"float32\") / 100\n+        )\n+        layer = layers.ConvLSTM2D(\n+            filters=2,\n+            kernel_size=3,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [\n+                        [[0.48694518, 0.48694518], [0.50237733, 0.50237733]],\n+                        [[0.5461202, 0.5461202], [0.5598283, 0.5598283]],\n+                    ],\n+                    [\n+                        [[0.8661607, 0.8661607], [0.86909103, 0.86909103]],\n+                        [[0.8774414, 0.8774414], [0.8800861, 0.8800861]],\n+                    ],\n+                ]\n+            ),\n+            output,\n+        )\n\n@@ -128,7 +128,7 @@ class ConvLSTM3D(ConvLSTM):\n         data_format=None,\n         dilation_rate=1,\n         activation=\"tanh\",\n-        recurrent_activation=\"hard_sigmoid\",\n+        recurrent_activation=\"sigmoid\",\n         use_bias=True,\n         kernel_initializer=\"glorot_uniform\",\n         recurrent_initializer=\"orthogonal\",\n\n@@ -1,3 +1,6 @@\n+import numpy as np\n+\n+from keras_core import initializers\n from keras_core import layers\n from keras_core import testing\n \n@@ -43,4 +46,48 @@ class ConvLSTM1DTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    # TODO: correctness testing\n+    def test_correctness(self):\n+        sequence = (\n+            np.arange(1920).reshape((2, 3, 4, 4, 4, 5)).astype(\"float32\") / 100\n+        )\n+        layer = layers.ConvLSTM3D(\n+            filters=2,\n+            kernel_size=3,\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+            bias_initializer=initializers.Constant(0.03),\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [\n+                        [\n+                            [\n+                                [0.99149036, 0.99149036],\n+                                [0.99180907, 0.99180907],\n+                            ],\n+                            [[0.99258363, 0.99258363], [0.9927925, 0.9927925]],\n+                        ],\n+                        [\n+                            [\n+                                [0.99413764, 0.99413764],\n+                                [0.99420583, 0.99420583],\n+                            ],\n+                            [[0.9943788, 0.9943788], [0.9944278, 0.9944278]],\n+                        ],\n+                    ],\n+                    [\n+                        [\n+                            [[0.9950547, 0.9950547], [0.9950547, 0.9950547]],\n+                            [[0.9950547, 0.9950547], [0.9950547, 0.9950547]],\n+                        ],\n+                        [\n+                            [[0.9950547, 0.9950547], [0.9950547, 0.9950547]],\n+                            [[0.9950547, 0.9950547], [0.9950547, 0.9950547]],\n+                        ],\n+                    ],\n+                ]\n+            ),\n+            output,\n+        )\n\n@@ -0,0 +1,47 @@\n+import numpy as np\n+\n+from keras_core import initializers\n+from keras_core import testing\n+from keras_core.layers.rnn.conv_lstm import ConvLSTM\n+from keras_core.layers.rnn.conv_lstm import ConvLSTMCell\n+\n+\n+class ConvLSTMCellTest(testing.TestCase):\n+    def test_correctness(self):\n+        x = np.arange(150).reshape((2, 5, 5, 3)).astype(\"float32\") / 10\n+        s1 = np.arange(200).reshape((2, 5, 5, 4)).astype(\"float32\") / 10\n+        s2 = np.arange(200).reshape((2, 5, 5, 4)).astype(\"float32\") / 10\n+\n+        layer = ConvLSTMCell(\n+            rank=2,\n+            filters=4,\n+            kernel_size=3,\n+            padding=\"same\",\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+        )\n+        output = layer(x, [s1, s2])\n+        checksum_0 = np.sum(output[0])\n+        self.assertAllClose(checksum_0, 188.89502)\n+        checksum_1 = np.sum(output[1][0])\n+        self.assertAllClose(checksum_1, 188.89502)\n+        checksum_2 = np.sum(output[1][1])\n+        self.assertAllClose(checksum_2, 2170.444)\n+\n+\n+class ConvLSTMTest(testing.TestCase):\n+    def test_correctness(self):\n+        x = np.arange(450).reshape((2, 3, 5, 5, 3)).astype(\"float32\") / 100\n+        s1 = np.arange(200).reshape((2, 5, 5, 4)).astype(\"float32\") / 100\n+        s2 = np.arange(200).reshape((2, 5, 5, 4)).astype(\"float32\") / 100\n+\n+        layer = ConvLSTM(\n+            rank=2,\n+            filters=4,\n+            kernel_size=3,\n+            padding=\"same\",\n+            kernel_initializer=initializers.Constant(0.01),\n+            recurrent_initializer=initializers.Constant(0.02),\n+        )\n+        output = layer(x, initial_state=[s1, s2])\n+        self.assertAllClose(np.sum(output), 119.812454)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f92f7ce22283e40ecdddf7bce3faef651927590", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1298 | Contributors (this commit): 7 | Commits (past 90d): 19 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -493,7 +493,7 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n \n \n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n-    target = jnp.array(target, dtype=\"int64\")\n+    target = jnp.array(target, dtype=\"int32\")\n     output = jnp.array(output)\n     if len(target.shape) == len(output.shape) and target.shape[-1] == 1:\n         target = jnp.squeeze(target, axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7cdf8394977aded88c2316527695b9e34648724c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 10010 | Contributors (this commit): 35 | Commits (past 90d): 19 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -1004,6 +1004,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     model.compile(optimizer='adam',\n                   loss=tf.keras.losses.CategoricalFocalCrossentropy())\n     ```\n+    \n     Args:\n         alpha: A weight balancing factor for all classes, default is `0.25` as\n             mentioned in the reference. It can be a list of floats or a scalar.\n@@ -1032,6 +1033,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n             for more details.\n         name: Optional name for the instance.\n             Defaults to 'categorical_focal_crossentropy'.\n+    \n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#427562e42f931d2310e488b037f23c605e7b3074", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 44 | Churn Cumulative: 44 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,44 @@\n+import unittest\n+\n+from keras_core import backend\n+from keras_core.backend.common.keras_tensor import KerasTensor\n+\n+\n+def single_arg_test_fn(x):\n+    return backend.numpy.concatenate([(x + 1) ** 2, x], axis=-1)\n+\n+\n+def three_args_2_kwarg_test_fn(x1, x2, x3=None):\n+    x1 = backend.numpy.max(x1, axis=1)\n+    x2 = backend.numpy.max(x2, axis=1)\n+    if x3 is not None:\n+        x1 += backend.numpy.max(x3, axis=1)\n+    return x1 + x2\n+\n+\n+class ComputeOutputSpecTest(unittest.TestCase):\n+    def test_dynamic_batch_size(self):\n+        x = KerasTensor(shape=(None, 3, 5))\n+        y = backend.compute_output_spec(single_arg_test_fn, x)\n+        self.assertEqual(y.shape, (None, 3, 10))\n+\n+        x1 = KerasTensor(shape=(None, 3, 5))\n+        x2 = KerasTensor(shape=(None, 3, 5))\n+        x3 = KerasTensor(shape=(None, 3, 5))\n+        y = backend.compute_output_spec(\n+            three_args_2_kwarg_test_fn, x1, x2, x3=x3\n+        )\n+        self.assertEqual(y.shape, (None, 5))\n+\n+    def test_dynamic_everything(self):\n+        x = KerasTensor(shape=(2, None, 3))\n+        y = backend.compute_output_spec(single_arg_test_fn, x)\n+        self.assertEqual(y.shape, (2, None, 6))\n+\n+        x1 = KerasTensor(shape=(None, None, 5))\n+        x2 = KerasTensor(shape=(None, None, 5))\n+        x3 = KerasTensor(shape=(None, None, 5))\n+        y = backend.compute_output_spec(\n+            three_args_2_kwarg_test_fn, x1, x2, x3=x3\n+        )\n+        self.assertEqual(y.shape, (None, 5))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a7a4e14eead8682a7e7cd4e40663b58476e36c14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 292 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 24/14 | Churn Δ: 292 | Churn Cumulative: 453 | Contributors (this commit): 11 | Commits (past 90d): 86 | Contributors (cumulative): 13 | DMM Complexity: 0.8520710059171598\n\nDIFF:\n@@ -104,6 +104,7 @@ from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras_core.layers.reshaping.up_sampling2d import UpSampling2D\n from keras_core.layers.reshaping.up_sampling3d import UpSampling3D\n from keras_core.layers.reshaping.zero_padding2d import ZeroPadding2D\n from keras_core.layers.reshaping.zero_padding3d import ZeroPadding3D\n\n@@ -0,0 +1,159 @@\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n+\n+\n+class UpSampling2D(Layer):\n+    \"\"\"Upsampling layer for 2D inputs.\n+\n+    The implementation uses interpolative resizing, given the resize method\n+    (specified by the `interpolation` argument). Use `interpolation=nearest`\n+    to repeat the rows and columns of the data.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 2, 1, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> print(x)\n+    [[[[ 0  1  2]]\n+      [[ 3  4  5]]]\n+     [[[ 6  7  8]]\n+      [[ 9 10 11]]]]\n+    >>> y = keras_core.layers.UpSampling2D(size=(1, 2))(x)\n+    >>> print(y)\n+    [[[[ 0  1  2]\n+       [ 0  1  2]]\n+      [[ 3  4  5]\n+       [ 3  4  5]]]\n+     [[[ 6  7  8]\n+       [ 6  7  8]]\n+      [[ 9 10 11]\n+       [ 9 10 11]]]]\n+\n+    Args:\n+        size: Int, or tuple of 2 integers.\n+            The upsampling factors for rows and columns.\n+        data_format: A string,\n+            one of `\"channels_last\"` (default) or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch_size, height, width, channels)` while `\"channels_first\"`\n+            corresponds to inputs with shape\n+            `(batch_size, channels, height, width)`.\n+            When unspecified, uses\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json` (if exists) else `\"channels_last\"`.\n+            Defaults to `\"channels_last\"`.\n+        interpolation: A string, one of `\"bicubic\"`, `\"bilinear\"`, `\"lanczos3\"`,\n+            `\"lanczos5\"`, `\"nearest\"`.\n+\n+    Input shape:\n+        4D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+            `(batch_size, rows, cols, channels)`\n+        - If `data_format` is `\"channels_first\"`:\n+            `(batch_size, channels, rows, cols)`\n+\n+    Output shape:\n+        4D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+            `(batch_size, upsampled_rows, upsampled_cols, channels)`\n+        - If `data_format` is `\"channels_first\"`:\n+            `(batch_size, channels, upsampled_rows, upsampled_cols)`\n+    \"\"\"\n+\n+    def __init__(\n+        self, size=(2, 2), data_format=None, interpolation=\"nearest\", **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self.data_format = backend.config.standardize_data_format(data_format)\n+        self.size = argument_validation.standardize_tuple(size, 2, \"size\")\n+        self.interpolation = interpolation.lower()\n+        self.input_spec = InputSpec(ndim=4)\n+\n+    def compute_output_shape(self, input_shape):\n+        if self.data_format == \"channels_first\":\n+            height = (\n+                self.size[0] * input_shape[2]\n+                if input_shape[2] is not None\n+                else None\n+            )\n+            width = (\n+                self.size[1] * input_shape[3]\n+                if input_shape[3] is not None\n+                else None\n+            )\n+            return (input_shape[0], input_shape[1], height, width)\n+        else:\n+            height = (\n+                self.size[0] * input_shape[1]\n+                if input_shape[1] is not None\n+                else None\n+            )\n+            width = (\n+                self.size[1] * input_shape[2]\n+                if input_shape[2] is not None\n+                else None\n+            )\n+            return (input_shape[0], height, width, input_shape[3])\n+\n+    def call(self, inputs):\n+        return self._resize_images(\n+            inputs,\n+            self.size[0],\n+            self.size[1],\n+            self.data_format,\n+            interpolation=self.interpolation,\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"size\": self.size,\n+            \"data_format\": self.data_format,\n+            \"interpolation\": self.interpolation,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    def _resize_images(\n+        self,\n+        x,\n+        height_factor,\n+        width_factor,\n+        data_format,\n+        interpolation=\"nearest\",\n+    ):\n+        \"\"\"Resizes the images contained in a 4D tensor.\n+\n+        Args:\n+            x: Tensor or variable to resize.\n+            height_factor: Positive integer.\n+            width_factor: Positive integer.\n+            data_format: One of `\"channels_first\"`, `\"channels_last\"`.\n+            interpolation: A string, one of `\"bicubic\"`, `\"bilinear\"`,\n+            `\"lanczos3\"`, `\"lanczos5\"`, or `\"nearest\"`.\n+\n+        Returns:\n+            A tensor.\n+        \"\"\"\n+        if data_format == \"channels_first\":\n+            rows, cols = 2, 3\n+        elif data_format == \"channels_last\":\n+            rows, cols = 1, 2\n+        else:\n+            raise ValueError(f\"Invalid `data_format` argument: {data_format}\")\n+\n+        new_shape = x.shape[rows : cols + 1]\n+        new_shape *= np.array([height_factor, width_factor])\n+\n+        if data_format == \"channels_first\":\n+            x = ops.transpose(x, [0, 2, 3, 1])\n+        x = ops.image.resize(x, new_shape, method=interpolation)\n+        if data_format == \"channels_first\":\n+            x = ops.transpose(x, [0, 3, 1, 2])\n+\n+        return x\n\n@@ -0,0 +1,132 @@\n+# flake8: noqa\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class UpSampling2dTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.product(\n+        data_format=[\"channels_first\", \"channels_last\"],\n+        length_row=[2],\n+        length_col=[2, 3],\n+    )\n+    def test_upsampling_2d(self, data_format, length_row, length_col):\n+        num_samples = 2\n+        stack_size = 2\n+        input_num_row = 11\n+        input_num_col = 12\n+\n+        if data_format == \"channels_first\":\n+            inputs = np.random.rand(\n+                num_samples, stack_size, input_num_row, input_num_col\n+            )\n+        else:\n+            inputs = np.random.rand(\n+                num_samples, input_num_row, input_num_col, stack_size\n+            )\n+\n+        # basic test\n+        self.run_layer_test(\n+            layers.UpSampling2D,\n+            init_kwargs={\"size\": (2, 2), \"data_format\": data_format},\n+            input_shape=inputs.shape,\n+        )\n+\n+        layer = layers.UpSampling2D(\n+            size=(length_row, length_col),\n+            data_format=data_format,\n+        )\n+        layer.build(inputs.shape)\n+        np_output = layer(inputs=backend.Variable(inputs))\n+        if data_format == \"channels_first\":\n+            assert np_output.shape[2] == length_row * input_num_row\n+            assert np_output.shape[3] == length_col * input_num_col\n+        else:\n+            assert np_output.shape[1] == length_row * input_num_row\n+            assert np_output.shape[2] == length_col * input_num_col\n+\n+        # compare with numpy\n+        if data_format == \"channels_first\":\n+            expected_out = np.repeat(inputs, length_row, axis=2)\n+            expected_out = np.repeat(expected_out, length_col, axis=3)\n+        else:\n+            expected_out = np.repeat(inputs, length_row, axis=1)\n+            expected_out = np.repeat(expected_out, length_col, axis=2)\n+\n+        np.testing.assert_allclose(np_output, expected_out)\n+\n+    @parameterized.product(\n+        data_format=[\"channels_first\", \"channels_last\"],\n+        length_row=[2],\n+        length_col=[2, 3],\n+    )\n+    def test_upsampling_2d_bilinear(self, data_format, length_row, length_col):\n+        num_samples = 2\n+        stack_size = 2\n+        input_num_row = 11\n+        input_num_col = 12\n+        if data_format == \"channels_first\":\n+            inputs = np.random.rand(\n+                num_samples, stack_size, input_num_row, input_num_col\n+            )\n+        else:\n+            inputs = np.random.rand(\n+                num_samples, input_num_row, input_num_col, stack_size\n+            )\n+\n+        self.run_layer_test(\n+            layers.UpSampling2D,\n+            init_kwargs={\n+                \"size\": (2, 2),\n+                \"data_format\": data_format,\n+                \"interpolation\": \"bilinear\",\n+            },\n+            input_shape=inputs.shape,\n+        )\n+\n+        layer = layers.UpSampling2D(\n+            size=(length_row, length_col),\n+            data_format=data_format,\n+        )\n+        layer.build(inputs.shape)\n+        np_output = layer(inputs=backend.Variable(inputs))\n+        if data_format == \"channels_first\":\n+            self.assertEqual(np_output.shape[2], length_row * input_num_row)\n+            self.assertEqual(np_output.shape[3], length_col * input_num_col)\n+        else:\n+            self.assertEqual(np_output.shape[1], length_row * input_num_row)\n+            self.assertEqual(np_output.shape[2], length_col * input_num_col)\n+\n+    def test_upsampling_2d_correctness(self):\n+        input_shape = (2, 2, 1, 3)\n+        x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+        np.testing.assert_array_equal(\n+            layers.UpSampling2D(size=(1, 2))(x),\n+            # fmt: off\n+            np.array(\n+                [[[[ 0.,  1.,  2.],\n+                   [ 0.,  1.,  2.]],\n+                  [[ 3.,  4.,  5.],\n+                   [ 3.,  4.,  5.]]],\n+                 [[[ 6.,  7.,  8.],\n+                   [ 6.,  7.,  8.]],\n+                  [[ 9., 10., 11.],\n+                   [ 9., 10., 11.]]]]\n+            ),\n+            # fmt: on\n+        )\n+\n+    def test_upsampling_2d_various_interpolation_methods(self):\n+        input_shape = (2, 2, 1, 3)\n+        x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+        for interpolation in [\n+            \"bicubic\",\n+            \"bilinear\",\n+            \"lanczos3\",\n+            \"lanczos5\",\n+            \"nearest\",\n+        ]:\n+            layers.UpSampling2D(size=(1, 2), interpolation=interpolation)(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0c1af1d8907fb349be34cd77d18ed62b78e2b317", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1302 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -532,7 +532,9 @@ def binary_crossentropy(target, output, from_logits=False):\n         )\n \n     if from_logits:\n-        output = jnn.sigmoid(output)\n+        log_logits = jax.nn.log_sigmoid(output)\n+        log_neg_logits = jax.nn.log_sigmoid(-output)\n+        return -1.0 * target * log_logits - (1.0 - target) * log_neg_logits\n \n     output = jnp.clip(output, epsilon(), 1.0 - epsilon())\n     bce = target * jnp.log(output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0def266cd4b8ceda888432c8dc07f140bda432e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 21 | Methods Changed: 5 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 65 | Churn Cumulative: 3034 | Contributors (this commit): 7 | Commits (past 90d): 29 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -78,14 +78,14 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n             knn.max_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 3)\n         )\n \n-        x = KerasTensor([None, 8, 8, 3])\n-        self.assertEqual(knn.max_pool(x, 2, 1).shape, (None, 7, 7, 3))\n+        x = KerasTensor([None, 8, None, 3])\n+        self.assertEqual(knn.max_pool(x, 2, 1).shape, (None, 7, None, 3))\n         self.assertEqual(\n-            knn.max_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 4, 3)\n+            knn.max_pool(x, 2, 2, padding=\"same\").shape, (None, 4, None, 3)\n         )\n         self.assertEqual(\n             knn.max_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n-            (None, 4, 4, 3),\n+            (None, 4, None, 3),\n         )\n \n     def test_average_pool(self):\n@@ -95,14 +95,14 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n             knn.average_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 3)\n         )\n \n-        x = KerasTensor([None, 8, 8, 3])\n-        self.assertEqual(knn.average_pool(x, 2, 1).shape, (None, 7, 7, 3))\n+        x = KerasTensor([None, 8, None, 3])\n+        self.assertEqual(knn.average_pool(x, 2, 1).shape, (None, 7, None, 3))\n         self.assertEqual(\n-            knn.average_pool(x, 2, 2, padding=\"same\").shape, (None, 4, 4, 3)\n+            knn.average_pool(x, 2, 2, padding=\"same\").shape, (None, 4, None, 3)\n         )\n         self.assertEqual(\n             knn.average_pool(x, (2, 2), (2, 2), padding=\"same\").shape,\n-            (None, 4, 4, 3),\n+            (None, 4, None, 3),\n         )\n \n     def test_conv(self):\n@@ -121,37 +121,37 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         )\n \n         # Test 2D conv.\n-        inputs_2d = KerasTensor([None, 10, 10, 3])\n+        inputs_2d = KerasTensor([None, 10, None, 3])\n         kernel = KerasTensor([2, 2, 3, 2])\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, 1, padding=\"valid\").shape,\n-            (None, 9, 9, 2),\n+            (None, 9, None, 2),\n         )\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, 1, padding=\"same\").shape,\n-            (None, 10, 10, 2),\n+            (None, 10, None, 2),\n         )\n         self.assertEqual(\n             knn.conv(inputs_2d, kernel, (2, 1), dilation_rate=(2, 1)).shape,\n-            (None, 4, 9, 2),\n+            (None, 4, None, 2),\n         )\n \n         # Test 3D conv.\n-        inputs_3d = KerasTensor([None, 8, 8, 8, 3])\n+        inputs_3d = KerasTensor([None, 8, None, 8, 3])\n         kernel = KerasTensor([3, 3, 3, 3, 2])\n         self.assertEqual(\n             knn.conv(inputs_3d, kernel, 1, padding=\"valid\").shape,\n-            (None, 6, 6, 6, 2),\n+            (None, 6, None, 6, 2),\n         )\n         self.assertEqual(\n             knn.conv(inputs_3d, kernel, (2, 1, 2), padding=\"same\").shape,\n-            (None, 4, 8, 4, 2),\n+            (None, 4, None, 4, 2),\n         )\n         self.assertEqual(\n             knn.conv(\n                 inputs_3d, kernel, 1, padding=\"valid\", dilation_rate=(1, 2, 2)\n             ).shape,\n-            (None, 6, 4, 4, 2),\n+            (None, 6, None, 4, 2),\n         )\n \n     def test_depthwise_conv(self):\n\n@@ -18,13 +18,20 @@ def compute_pooling_output_shape(\n         spatial_shape = input_shape[1:-1]\n     else:\n         spatial_shape = input_shape[2:]\n+    none_dims = []\n+    for i in range(len(spatial_shape)):\n+        if spatial_shape[i] is None:\n+            # Set `None` shape to a manual value so that we can run numpy\n+            # computation on `spatial_shape`.\n+            spatial_shape[i] = -1\n+            none_dims.append(i)\n     pool_size = np.array(pool_size)\n     if padding == \"valid\":\n         output_spatial_shape = (\n             np.floor((spatial_shape - pool_size) / strides) + 1\n         )\n-        negative_in_shape = np.all(output_spatial_shape < 0)\n-        if negative_in_shape:\n+        for i in range(len(output_spatial_shape)):\n+            if i not in none_dims and output_spatial_shape[i] < 0:\n                 raise ValueError(\n                     \"Computed output size would be negative. Received: \"\n                     f\"`inputs.shape={input_shape}` and `pool_size={pool_size}`.\"\n@@ -36,7 +43,10 @@ def compute_pooling_output_shape(\n             \"`padding` must be either `'valid'` or `'same'`. Received \"\n             f\"{padding}.\"\n         )\n-    output_spatial_shape = tuple([int(i) for i in output_spatial_shape])\n+    output_spatial_shape = [int(i) for i in output_spatial_shape]\n+    for i in none_dims:\n+        output_spatial_shape[i] = None\n+    output_spatial_shape = tuple(output_spatial_shape)\n     if data_format == \"channels_last\":\n         output_shape = (\n             (input_shape_origin[0],)\n@@ -84,7 +94,15 @@ def compute_conv_output_shape(\n             f\"`dilation_rate={dilation_rate}` and \"\n             f\"input of shape {input_shape}.\"\n         )\n+    none_dims = []\n     spatial_shape = np.array(spatial_shape)\n+    for i in range(len(spatial_shape)):\n+        if spatial_shape[i] is None:\n+            # Set `None` shape to a manual value so that we can run numpy\n+            # computation on `spatial_shape`.\n+            spatial_shape[i] = -1\n+            none_dims.append(i)\n+\n     kernel_spatial_shape = np.array(kernel_shape[:-2])\n     dilation_rate = np.array(dilation_rate)\n     if padding == \"valid\":\n@@ -95,8 +113,8 @@ def compute_conv_output_shape(\n             )\n             + 1\n         )\n-        negative_in_shape = np.all(output_spatial_shape < 0)\n-        if negative_in_shape:\n+        for i in range(len(output_spatial_shape)):\n+            if i not in none_dims and output_spatial_shape[i] < 0:\n                 raise ValueError(\n                     \"Computed output size would be negative. Received \"\n                     f\"`inputs shape={input_shape}`, \"\n@@ -105,7 +123,10 @@ def compute_conv_output_shape(\n                 )\n     elif padding == \"same\" or padding == \"causal\":\n         output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n-    output_spatial_shape = tuple([int(i) for i in output_spatial_shape])\n+    output_spatial_shape = [int(i) for i in output_spatial_shape]\n+    for i in none_dims:\n+        output_spatial_shape[i] = None\n+    output_spatial_shape = tuple(output_spatial_shape)\n     if data_format == \"channels_last\":\n         output_shape = (\n             (input_shape[0],) + output_spatial_shape + (kernel_shape[-1],)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
