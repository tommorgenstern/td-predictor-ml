{"custom_id": "scrapy#a2bf340bab796704ba6846f8ed755d3ffe37bb0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 66 | Files Changed: 34 | Hunks: 56 | Methods Changed: 13 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 107 | Churn Cumulative: 14593 | Contributors (this commit): 91 | Commits (past 90d): 119 | Contributors (cumulative): 309 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,4 @@\n-from __future__ import print_function\n import time\n-import sys\n from collections import defaultdict\n from unittest import TextTestRunner, TextTestResult as _TextTestResult\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.commands import fetch, ScrapyCommand\n+from scrapy.commands import fetch\n from scrapy.utils.response import open_in_browser\n \n \n\n@@ -1,6 +1,4 @@\n-from __future__ import absolute_import\n import random\n-import warnings\n from time import time\n from datetime import datetime\n from collections import deque\n\n@@ -1,3 +1,2 @@\n-from __future__ import absolute_import\n from .http10 import HTTP10DownloadHandler\n from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n\n@@ -21,7 +21,7 @@ def _get_boto_connection():\n             return http_request.headers\n \n     try:\n-        import boto.auth\n+        import boto.auth  # noqa: F401\n     except ImportError:\n         _S3Connection = _v19_S3Connection\n     else:\n\n@@ -6,18 +6,16 @@ import os\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n from time import time\n-from warnings import warn\n from weakref import WeakKeyDictionary\n \n from six.moves import cPickle as pickle\n from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n-from scrapy.utils.python import to_bytes, to_unicode, garbage_collect\n+from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.utils.request import request_fingerprint\n \n \n\n@@ -118,4 +118,4 @@ class FilteringLinkExtractor(object):\n \n \n # Top-level imports\n-from .lxmlhtml import LxmlLinkExtractor as LinkExtractor\n+from .lxmlhtml import LxmlLinkExtractor as LinkExtractor  # noqa: F401\n\n@@ -1,4 +1,4 @@\n \"\"\"\n Selectors\n \"\"\"\n-from scrapy.selector.unified import *\n+from scrapy.selector.unified import *  # noqa: F401\n\n@@ -2,12 +2,10 @@\n XPath selectors based on lxml\n \"\"\"\n \n-import warnings\n from parsel import Selector as _ParselSelector\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.python import to_bytes\n from scrapy.http import HtmlResponse, XmlResponse\n-from scrapy.utils.decorators import deprecated\n \n \n __all__ = ['Selector', 'SelectorList']\n\n@@ -10,7 +10,6 @@ from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import method_is_overridden\n \n \n@@ -100,6 +99,6 @@ class Spider(object_ref):\n \n \n # Top-level imports\n-from scrapy.spiders.crawl import CrawlSpider, Rule\n-from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider\n-from scrapy.spiders.sitemap import SitemapSpider\n+from scrapy.spiders.crawl import CrawlSpider, Rule  # noqa: F401\n+from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider  # noqa: F401\n+from scrapy.spiders.sitemap import SitemapSpider  # noqa: F401\n\n@@ -7,7 +7,7 @@ from scrapy.exceptions import NotConfigured\n \n def is_botocore():\n     try:\n-        import botocore\n+        import botocore  # noqa: F401\n         return True\n     except ImportError:\n         raise NotConfigured('missing botocore library')\n\n@@ -52,7 +52,7 @@ def _embed_standard_shell(namespace={}, banner=''):\n     except ImportError:\n         pass\n     else:\n-        import rlcompleter\n+        import rlcompleter  # noqa: F401\n         readline.parse_and_bind(\"tab:complete\")\n     @wraps(_embed_standard_shell)\n     def wrapper(namespace=namespace, banner=''):\n\n@@ -4,7 +4,7 @@ from shlex import split\n \n from six.moves.http_cookies import SimpleCookie\n from six.moves.urllib.parse import urlparse\n-from six import string_types, iteritems\n+from six import iteritems\n from w3lib.http import basic_auth_header\n \n \n\n@@ -1,7 +1,7 @@\n \"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n \n-from __future__ import print_function\n-from time import time # used in global tests code\n+# used in global tests code\n+from time import time  # noqa: F401\n \n \n def get_engine_status(engine):\n\n@@ -8,7 +8,7 @@ import warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.decorators import deprecated\n-from w3lib.http import *\n+from w3lib.http import *  # noqa: F401\n \n \n warnings.warn(\"Module `scrapy.utils.http` is deprecated, \"\n\n@@ -6,7 +6,7 @@ For new code, always import from w3lib.html instead of this module\n import warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from w3lib.html import *\n+from w3lib.html import *  # noqa: F401\n \n \n warnings.warn(\"Module `scrapy.utils.markup` is deprecated. \"\n\n@@ -6,7 +6,7 @@ For new code, always import from w3lib.form instead of this module\n import warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from w3lib.form import *\n+from w3lib.form import *  # noqa: F401\n \n \n warnings.warn(\"Module `scrapy.utils.multipart` is deprecated. \"\n\n@@ -12,7 +12,6 @@ from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse, urlunparse\n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n from w3lib.url import *\n-from w3lib.url import _safe_chars, _unquotepath\n from scrapy.utils.python import to_unicode\n \n \n\n@@ -1,9 +1,11 @@\n-from __future__ import print_function\n-import sys, time, random, os, json\n-from six.moves.urllib.parse import urlencode\n+import json\n+import os\n+import random\n+import sys\n from subprocess import Popen, PIPE\n \n from OpenSSL import SSL\n+from six.moves.urllib.parse import urlencode\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.web.static import File\n\n@@ -543,7 +543,7 @@ class Https11InvalidDNSPattern(Https11TestCase):\n \n     def setUp(self):\n         try:\n-            from service_identity.exceptions import CertificateError\n+            from service_identity.exceptions import CertificateError  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest(\"cryptography lib is too old\")\n         self.tls_log_message = 'SSL connection certificate: issuer \"/C=IE/O=Scrapy/CN=127.0.0.1\", subject \"/C=IE/O=Scrapy/CN=127.0.0.1\"'\n@@ -778,7 +778,7 @@ class S3TestCase(unittest.TestCase):\n     @contextlib.contextmanager\n     def _mocked_date(self, date):\n         try:\n-            import botocore.auth\n+            import botocore.auth  # noqa: F401\n         except ImportError:\n             yield\n         else:\n@@ -843,8 +843,10 @@ class S3TestCase(unittest.TestCase):\n                 b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n \n     def test_request_signing5(self):\n-        try: import botocore\n-        except ImportError: pass\n+        try:\n+            import botocore  # noqa: F401\n+        except ImportError:\n+            pass\n         else:\n             raise unittest.SkipTest(\n                 'botocore does not support overriding date with x-amz-date')\n\n@@ -1,12 +1,9 @@\n-from __future__ import print_function\n import time\n import tempfile\n import shutil\n import unittest\n import email.utils\n from contextlib import contextmanager\n-import pytest\n-import sys\n \n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spiders import Spider\n\n@@ -70,7 +70,7 @@ class HttpCompressionTest(TestCase):\n \n     def test_process_response_br(self):\n         try:\n-            import brotli\n+            import brotli  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         response = self._getresponse('br')\n\n@@ -1,11 +1,10 @@\n import os\n-import sys\n from functools import partial\n-from twisted.trial.unittest import TestCase, SkipTest\n+from twisted.trial.unittest import TestCase\n \n from scrapy.downloadermiddlewares.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n-from scrapy.http import Response, Request\n+from scrapy.http import Request\n from scrapy.spiders import Spider\n from scrapy.crawler import Crawler\n from scrapy.settings import Settings\n\n@@ -3,7 +3,7 @@ from twisted.conch.telnet import ITelnetProtocol\n from twisted.cred import credentials\n from twisted.internet import defer\n \n-from scrapy.extensions.telnet import TelnetConsole, logger\n+from scrapy.extensions.telnet import TelnetConsole\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -167,7 +167,7 @@ class S3FeedStorageTest(unittest.TestCase):\n                 create=True)\n     def test_parse_credentials(self):\n         try:\n-            import boto\n+            import boto  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest(\"S3FeedStorage requires boto\")\n         aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',\n@@ -268,7 +268,7 @@ class S3FeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_store_botocore_without_acl(self):\n         try:\n-            import botocore\n+            import botocore  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest('botocore is required')\n \n@@ -288,7 +288,7 @@ class S3FeedStorageTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_store_botocore_with_acl(self):\n         try:\n-            import botocore\n+            import botocore  # noqa: F401\n         except ImportError:\n             raise unittest.SkipTest('botocore is required')\n \n\n@@ -1,5 +1,3 @@\n-# -*- coding: utf-8 -*-\n-import cgi\n import unittest\n import re\n import json\n@@ -8,7 +6,7 @@ from urllib.parse import unquote_to_bytes\n import warnings\n \n from six.moves import xmlrpc_client as xmlrpclib\n-from six.moves.urllib.parse import urlparse, parse_qs, unquote\n+from six.moves.urllib.parse import urlparse, parse_qs\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n from scrapy.utils.python import to_bytes, to_unicode\n\n@@ -1,7 +1,6 @@\n import io\n import hashlib\n import random\n-import warnings\n from tempfile import mkdtemp\n from shutil import rmtree\n \n\n@@ -5,7 +5,7 @@ from twisted.trial import unittest\n def reppy_available():\n     # check if reppy parser is installed\n     try:\n-        from reppy.robots import Robots\n+        from reppy.robots import Robots  # noqa: F401\n     except ImportError:\n         return False\n     return True\n@@ -14,7 +14,7 @@ def reppy_available():\n def rerp_available():\n     # check if robotexclusionrulesparser is installed\n     try:\n-        from robotexclusionrulesparser import RobotExclusionRulesParser\n+        from robotexclusionrulesparser import RobotExclusionRulesParser  # noqa: F401\n     except ImportError:\n         return False\n     return True\n@@ -23,7 +23,7 @@ def rerp_available():\n def protego_available():\n     # check if protego parser is installed\n     try:\n-        from protego import Protego\n+        from protego import Protego  # noqa: F401\n     except ImportError:\n         return False\n     return True\n\n@@ -1,9 +1,9 @@\n-import warnings\n import weakref\n+\n from twisted.trial import unittest\n+\n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.selector import Selector\n-from lxml import etree\n \n \n class SelectorTestCase(unittest.TestCase):\n\n@@ -15,7 +15,6 @@ from scrapy.spiders import Spider, CrawlSpider, Rule, XMLFeedSpider, \\\n     CSVFeedSpider, SitemapSpider\n from scrapy.linkextractors import LinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.trackref import object_ref\n from scrapy.utils.test import get_crawler\n \n \n\n@@ -6,7 +6,6 @@ from twisted.internet import defer\n from scrapy import Spider, Request\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n-from tests.spiders import MockServerSpider\n \n \n class LogExceptionMiddleware:\n\n@@ -2,7 +2,6 @@ from six.moves.urllib.parse import urlparse\n from unittest import TestCase\n import warnings\n \n-from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n\n@@ -1,8 +1,4 @@\n-# -*- coding: utf-8 -*-\n import unittest\n-import sys\n-\n-import six\n \n from scrapy.http import Request, FormRequest\n from scrapy.spiders import Spider\n\n@@ -1,13 +1,9 @@\n # -*- coding: utf-8 -*-\n import unittest\n \n-import six\n-from six.moves.urllib.parse import urlparse\n-\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              add_http_if_no_scheme, guess_scheme,\n-                              parse_url, strip_url)\n+                              add_http_if_no_scheme, guess_scheme, strip_url)\n \n __doctests__ = ['scrapy.utils.url']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b23288135633f9e800b575d84df0a109665a840e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1040 | Contributors (this commit): 19 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,7 @@ from six.moves.urllib.parse import (ParseResult, urldefrag, urlparse, urlunparse\n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n from w3lib.url import *\n+from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n from scrapy.utils.python import to_unicode\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1718e450ef9549a4fc71b01dba1e6faf7a63238a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 12 | Churn Cumulative: 691 | Contributors (this commit): 16 | Commits (past 90d): 9 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -68,6 +68,11 @@ class Spider(object_ref):\n \n     def start_requests(self):\n         cls = self.__class__\n+        if not self.start_urls and hasattr(self, 'start_url'):\n+            raise AttributeError(\n+                \"Crawling could not start: 'start_urls' not found \"\n+                \"or empty (but found 'start_url' attribute instead, \"\n+                \"did you miss an 's'?)\")\n         if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n             warnings.warn(\n                 \"Spider.make_requests_from_url method is deprecated; it \"\n\n@@ -391,6 +391,13 @@ class CrawlSpiderTest(SpiderTest):\n         self.assertTrue(hasattr(spider, '_follow_links'))\n         self.assertFalse(spider._follow_links)\n \n+    def test_start_url(self):\n+        spider = self.spider_class(\"example.com\")\n+        spider.start_url = 'https://www.example.com'\n+\n+        with self.assertRaisesRegex(AttributeError,\n+                                    r'^Crawling could not start.*$'):\n+            list(spider.start_requests())\n \n class SitemapSpiderTest(SpiderTest):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40b5cfc0a4adbc51fa35018b09902255228e360c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 70 | Churn Cumulative: 833 | Contributors (this commit): 13 | Commits (past 90d): 24 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,8 +4,7 @@ Item Loader\n See documentation in docs/topics/loaders.rst\n \"\"\"\n from collections import defaultdict\n-\n-import six\n+from contextlib import suppress\n \n from scrapy.item import Item\n from scrapy.loader.common import wrap_loader_context\n@@ -15,6 +14,17 @@ from scrapy.utils.misc import arg_to_iter, extract_regex\n from scrapy.utils.python import flatten\n \n \n+def unbound_method(method):\n+    \"\"\"\n+    Allow to use single-argument functions as input or output processors\n+    (no need to define an unused first 'self' argument)\n+    \"\"\"\n+    with suppress(AttributeError):\n+        if '.' not in method.__qualname__:\n+            return method.__func__\n+    return method\n+\n+\n class ItemLoader(object):\n \n     default_item_class = Item\n@@ -72,7 +82,7 @@ class ItemLoader(object):\n         if value is None:\n             return\n         if not field_name:\n-            for k, v in six.iteritems(value):\n+            for k, v in value.items():\n                 self._add_value(k, v)\n         else:\n             self._add_value(field_name, value)\n@@ -82,7 +92,7 @@ class ItemLoader(object):\n         if value is None:\n             return\n         if not field_name:\n-            for k, v in six.iteritems(value):\n+            for k, v in value.items():\n                 self._replace_value(k, v)\n         else:\n             self._replace_value(field_name, value)\n@@ -142,14 +152,14 @@ class ItemLoader(object):\n         if not proc:\n             proc = self._get_item_field_attr(field_name, 'input_processor',\n                                              self.default_input_processor)\n-        return proc\n+        return unbound_method(proc)\n \n     def get_output_processor(self, field_name):\n         proc = getattr(self, '%s_out' % field_name, None)\n         if not proc:\n             proc = self._get_item_field_attr(field_name, 'output_processor',\n                                              self.default_output_processor)\n-        return proc\n+        return unbound_method(proc)\n \n     def _process_input_value(self, field_name, value):\n         proc = self.get_input_processor(field_name)\n\n@@ -994,5 +994,53 @@ class SelectJmesTestCase(unittest.TestCase):\n             )\n \n \n+# Functions as processors\n+\n+def function_processor_strip(iterable):\n+    return [x.strip() for x in iterable]\n+\n+\n+def function_processor_upper(iterable):\n+    return [x.upper() for x in iterable]\n+\n+\n+class FunctionProcessorItem(Item):\n+    foo = Field(\n+        input_processor=function_processor_strip,\n+        output_processor=function_processor_upper,\n+    )\n+\n+\n+class FunctionProcessorItemLoader(ItemLoader):\n+    default_item_class = FunctionProcessorItem\n+\n+\n+class FunctionProcessorDictLoader(ItemLoader):\n+    default_item_class = dict\n+    foo_in = function_processor_strip\n+    foo_out = function_processor_upper\n+\n+\n+class FunctionProcessorTestCase(unittest.TestCase):\n+\n+    def test_processor_defined_in_item(self):\n+        lo = FunctionProcessorItemLoader()\n+        lo.add_value('foo', '  bar  ')\n+        lo.add_value('foo', ['  asdf  ', '  qwerty  '])\n+        self.assertEqual(\n+            dict(lo.load_item()),\n+            {'foo': ['BAR', 'ASDF', 'QWERTY']}\n+        )\n+\n+    def test_processor_defined_in_item_loader(self):\n+        lo = FunctionProcessorDictLoader()\n+        lo.add_value('foo', '  bar  ')\n+        lo.add_value('foo', ['  asdf  ', '  qwerty  '])\n+        self.assertEqual(\n+            dict(lo.load_item()),\n+            {'foo': ['BAR', 'ASDF', 'QWERTY']}\n+        )\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a7d13b1122dac397ee0bb8edd4e6fd61665e232", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 12 | Churn Cumulative: 1446 | Contributors (this commit): 21 | Commits (past 90d): 15 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -231,7 +231,7 @@ class Scraper(object):\n                     signal=signals.item_dropped, item=item, response=response,\n                     spider=spider, exception=output.value)\n             else:\n-                logkws = self.logformatter.error(item, ex, response, spider)\n+                logkws = self.logformatter.item_error(item, ex, response, spider)\n                 logger.log(*logformatter_adapter(logkws), extra={'spider': spider},\n                            exc_info=failure_to_exc_info(output))\n                 return self.signals.send_catch_log_deferred(\n\n@@ -8,7 +8,7 @@ from scrapy.utils.request import referer_str\n SCRAPEDMSG = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDMSG = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n-ERRORMSG = u\"'Error processing %(item)s'\"\n+ITEMERRORMSG = u\"'Error processing %(item)s'\"\n \n \n class LogFormatter(object):\n@@ -93,11 +93,11 @@ class LogFormatter(object):\n             }\n         }\n \n-    def error(self, item, exception, response, spider):\n+    def item_error(self, item, exception, response, spider):\n         \"\"\"Logs a message when an item causes an error while it is passing through the item pipeline.\"\"\"\n         return {\n             'level': logging.ERROR,\n-            'msg': ERRORMSG,\n+            'msg': ITEMERRORMSG,\n             'args': {\n                 'item': item,\n             }\n\n@@ -63,13 +63,13 @@ class LogFormatterTestCase(unittest.TestCase):\n         assert all(isinstance(x, six.text_type) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n \n-    def test_error(self):\n+    def test_item_error(self):\n         # In practice, the complete traceback is shown by passing the\n         # 'exc_info' argument to the logging function\n         item = {'key': 'value'}\n         exception = Exception()\n         response = Response(\"http://www.example.com\")\n-        logkws = self.formatter.error(item, exception, response, self.spider)\n+        logkws = self.formatter.item_error(item, exception, response, self.spider)\n         logline = logkws['msg'] % logkws['args']\n         self.assertEqual(logline, u\"'Error processing {'key': 'value'}'\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#facb9265421ead8afb839323af2e18f81dda560b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 456 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -5,10 +5,10 @@ from twisted.python.failure import Failure\n \n from scrapy.utils.request import referer_str\n \n-SCRAPEDMSG = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n-DROPPEDMSG = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n-CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n-ITEMERRORMSG = u\"'Error processing %(item)s'\"\n+SCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n+DROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n+CRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n+ITEMERRORMSG = \"Error processing %(item)s\"\n \n \n class LogFormatter(object):\n\n@@ -71,7 +71,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.item_error(item, exception, response, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline, u\"'Error processing {'key': 'value'}'\")\n+        self.assertEqual(logline, u\"Error processing {'key': 'value'}\")\n \n     def test_scraped(self):\n         item = CustomItem()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4756e7c587880997a54d9abf94b9a4c0b5bab71c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 34 | Churn Cumulative: 1490 | Contributors (this commit): 21 | Commits (past 90d): 20 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n-from scrapy.utils.request import referer_str\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -152,9 +152,9 @@ class Scraper(object):\n         if isinstance(exc, CloseSpider):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n-        logger.error(\n-            \"Spider error processing %(request)s (referer: %(referer)s)\",\n-            {'request': request, 'referer': referer_str(request)},\n+        logkws = self.logformatter.spider_error(_failure, request, response, spider)\n+        logger.log(\n+            *logformatter_adapter(logkws),\n             exc_info=failure_to_exc_info(_failure),\n             extra={'spider': spider}\n         )\n\n@@ -9,6 +9,7 @@ SCRAPEDMSG = \"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n CRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n ITEMERRORMSG = \"Error processing %(item)s\"\n+SPIDERERRORMSG = \"Spider error processing %(request)s (referer: %(referer)s)\"\n \n \n class LogFormatter(object):\n@@ -103,6 +104,17 @@ class LogFormatter(object):\n             }\n         }\n \n+    def spider_error(self, failure, request, response, spider):\n+        \"\"\"Logs an error message from a spider.\"\"\"\n+        return {\n+            'level': logging.ERROR,\n+            'msg': SPIDERERRORMSG,\n+            'args': {\n+                'request': request,\n+                'referer': referer_str(request),\n+            }\n+        }\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls()\n\n@@ -2,6 +2,7 @@ import unittest\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n+from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase as TwistedTestCase\n import six\n \n@@ -73,6 +74,19 @@ class LogFormatterTestCase(unittest.TestCase):\n         logline = logkws['msg'] % logkws['args']\n         self.assertEqual(logline, u\"Error processing {'key': 'value'}\")\n \n+    def test_spider_error(self):\n+        # In practice, the complete traceback is shown by passing the\n+        # 'exc_info' argument to the logging function\n+        failure = Failure(Exception())\n+        request = Request(\"http://www.example.com\", headers={'Referer': 'http://example.org'})\n+        response = Response(\"http://www.example.com\", request=request)\n+        logkws = self.formatter.spider_error(failure, request, response, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(\n+            logline,\n+            \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\"\n+        )\n+\n     def test_scraped(self):\n         item = CustomItem()\n         item['name'] = u'\\xa3'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03af8885ff475dc47a3de89517b1a5d627bd49c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 54 | Churn Cumulative: 1544 | Contributors (this commit): 21 | Commits (past 90d): 23 | Contributors (cumulative): 36 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -200,19 +200,23 @@ class Scraper(object):\n         \"\"\"Log and silence errors that come from the engine (typically download\n         errors that got propagated thru here)\n         \"\"\"\n-        if (isinstance(download_failure, Failure) and\n-                not download_failure.check(IgnoreRequest)):\n+        if isinstance(download_failure, Failure) and not download_failure.check(IgnoreRequest):\n             if download_failure.frames:\n-                logger.error('Error downloading %(request)s',\n-                             {'request': request},\n+                logkws = self.logformatter.download_error(download_failure, request, spider)\n+                logger.log(\n+                    *logformatter_adapter(logkws),\n+                    extra={'spider': spider},\n                     exc_info=failure_to_exc_info(download_failure),\n-                             extra={'spider': spider})\n+                )\n             else:\n                 errmsg = download_failure.getErrorMessage()\n                 if errmsg:\n-                    logger.error('Error downloading %(request)s: %(errmsg)s',\n-                                 {'request': request, 'errmsg': errmsg},\n-                                 extra={'spider': spider})\n+                    logkws = self.logformatter.download_error(\n+                        download_failure, request, spider, errmsg)\n+                    logger.log(\n+                        *logformatter_adapter(logkws),\n+                        extra={'spider': spider},\n+                    )\n \n         if spider_failure is not download_failure:\n             return spider_failure\n\n@@ -10,6 +10,8 @@ DROPPEDMSG = \"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n CRAWLEDMSG = \"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n ITEMERRORMSG = \"Error processing %(item)s\"\n SPIDERERRORMSG = \"Spider error processing %(request)s (referer: %(referer)s)\"\n+DOWNLOADERRORMSG_SHORT = \"Error downloading %(request)s\"\n+DOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n \n \n class LogFormatter(object):\n@@ -115,6 +117,20 @@ class LogFormatter(object):\n             }\n         }\n \n+    def download_error(self, failure, request, spider, errmsg=None):\n+        \"\"\"Logs a download error message from a spider (typically coming from the engine).\"\"\"\n+        args = {'request': request}\n+        if errmsg:\n+            msg = DOWNLOADERRORMSG_LONG\n+            args['errmsg'] = errmsg\n+        else:\n+            msg = DOWNLOADERRORMSG_SHORT\n+        return {\n+            'level': logging.ERROR,\n+            'msg': msg,\n+            'args': args,\n+        }\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls()\n\n@@ -87,6 +87,24 @@ class LogFormatterTestCase(unittest.TestCase):\n             \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\"\n         )\n \n+    def test_download_error_short(self):\n+        # In practice, the complete traceback is shown by passing the\n+        # 'exc_info' argument to the logging function\n+        failure = Failure(Exception())\n+        request = Request(\"http://www.example.com\")\n+        logkws = self.formatter.download_error(failure, request, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline, \"Error downloading <GET http://www.example.com>\")\n+\n+    def test_download_error_long(self):\n+        # In practice, the complete traceback is shown by passing the\n+        # 'exc_info' argument to the logging function\n+        failure = Failure(Exception())\n+        request = Request(\"http://www.example.com\")\n+        logkws = self.formatter.download_error(failure, request, self.spider, \"Some message\")\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline, \"Error downloading <GET http://www.example.com>: Some message\")\n+\n     def test_scraped(self):\n         item = CustomItem()\n         item['name'] = u'\\xa3'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ed1e577610b77e92f9833ca96755b9f0727085f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 383 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,8 +24,9 @@ __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n \n class BaseItemExporter(object):\n \n-    def __init__(self, **kwargs):\n-        self._configure(kwargs)\n+    def __init__(self, dont_fail=False, **kwargs):\n+        self._kwargs = kwargs\n+        self._configure(kwargs, dont_fail=dont_fail)\n \n     def _configure(self, options, dont_fail=False):\n         \"\"\"Configure the exporter by poping options from the ``options`` dict.\n@@ -82,10 +83,10 @@ class BaseItemExporter(object):\n class JsonLinesItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n-        self._configure(kwargs, dont_fail=True)\n+        super().__init__(dont_fail=True, **kwargs)\n         self.file = file\n-        kwargs.setdefault('ensure_ascii', not self.encoding)\n-        self.encoder = ScrapyJSONEncoder(**kwargs)\n+        self._kwargs.setdefault('ensure_ascii', not self.encoding)\n+        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n \n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n@@ -96,15 +97,15 @@ class JsonLinesItemExporter(BaseItemExporter):\n class JsonItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n-        self._configure(kwargs, dont_fail=True)\n+        super().__init__(dont_fail=True, **kwargs)\n         self.file = file\n         # there is a small difference between the behaviour or JsonItemExporter.indent\n         # and ScrapyJSONEncoder.indent. ScrapyJSONEncoder.indent=None is needed to prevent\n         # the addition of newlines everywhere\n         json_indent = self.indent if self.indent is not None and self.indent > 0 else None\n-        kwargs.setdefault('indent', json_indent)\n-        kwargs.setdefault('ensure_ascii', not self.encoding)\n-        self.encoder = ScrapyJSONEncoder(**kwargs)\n+        self._kwargs.setdefault('indent', json_indent)\n+        self._kwargs.setdefault('ensure_ascii', not self.encoding)\n+        self.encoder = ScrapyJSONEncoder(**self._kwargs)\n         self.first_item = True\n \n     def _beautify_newline(self):\n@@ -135,7 +136,7 @@ class XmlItemExporter(BaseItemExporter):\n     def __init__(self, file, **kwargs):\n         self.item_element = kwargs.pop('item_element', 'item')\n         self.root_element = kwargs.pop('root_element', 'items')\n-        self._configure(kwargs)\n+        super().__init__(**kwargs)\n         if not self.encoding:\n             self.encoding = 'utf-8'\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n@@ -191,7 +192,7 @@ class XmlItemExporter(BaseItemExporter):\n class CsvItemExporter(BaseItemExporter):\n \n     def __init__(self, file, include_headers_line=True, join_multivalued=',', **kwargs):\n-        self._configure(kwargs, dont_fail=True)\n+        super().__init__(dont_fail=True, **kwargs)\n         if not self.encoding:\n             self.encoding = 'utf-8'\n         self.include_headers_line = include_headers_line\n@@ -202,7 +203,7 @@ class CsvItemExporter(BaseItemExporter):\n             encoding=self.encoding,\n             newline='' # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n         )\n-        self.csv_writer = csv.writer(self.stream, **kwargs)\n+        self.csv_writer = csv.writer(self.stream, **self._kwargs)\n         self._headers_not_written = True\n         self._join_multivalued = join_multivalued\n \n@@ -251,7 +252,7 @@ class CsvItemExporter(BaseItemExporter):\n class PickleItemExporter(BaseItemExporter):\n \n     def __init__(self, file, protocol=2, **kwargs):\n-        self._configure(kwargs)\n+        super().__init__(**kwargs)\n         self.file = file\n         self.protocol = protocol\n \n@@ -270,7 +271,7 @@ class MarshalItemExporter(BaseItemExporter):\n     \"\"\"\n \n     def __init__(self, file, **kwargs):\n-        self._configure(kwargs)\n+        super().__init__(**kwargs)\n         self.file = file\n \n     def export_item(self, item):\n@@ -280,7 +281,7 @@ class MarshalItemExporter(BaseItemExporter):\n class PprintItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n-        self._configure(kwargs)\n+        super().__init__(**kwargs)\n         self.file = file\n \n     def export_item(self, item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b73fc99b60ed83be403e9570e84f5267d35dcc9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 182 | Contributors (this commit): 12 | Commits (past 90d): 12 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -275,6 +275,7 @@ coverage_ignore_pyobjects = [\n # -------------------------------------\n \n intersphinx_mapping = {\n+    'coverage': ('https://coverage.readthedocs.io/en/stable', None),\n     'pytest': ('https://docs.pytest.org/en/latest', None),\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#63546cbf3e02380819732441ad55d95725dca7c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 36 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 43 | Churn Cumulative: 3800 | Contributors (this commit): 29 | Commits (past 90d): 25 | Contributors (cumulative): 47 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,6 +16,7 @@ from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.client import Agent, ResponseDone, HTTPConnectionPool, ResponseFailed, URI\n from twisted.internet.endpoints import TCP4ClientEndpoint\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Headers\n from scrapy.responsetypes import responsetypes\n from scrapy.core.downloader.webclient import _parse\n@@ -285,6 +286,12 @@ class ScrapyAgent(object):\n             scheme = _parse(request.url)[0]\n             proxyHost = to_unicode(proxyHost)\n             omitConnectTunnel = b'noconnect' in proxyParams\n+            if omitConnectTunnel:\n+                warnings.warn(\"Using HTTPS proxies in the noconnect mode is deprecated. \"\n+                              \"If you use Crawlera, it doesn't require this mode anymore, \"\n+                              \"so you should update scrapy-crawlera to 1.3.0+ \"\n+                              \"and remove '?noconnect' from the Crawlera URL.\",\n+                              ScrapyDeprecationWarning)\n             if scheme == b'https' and not omitConnectTunnel:\n                 proxyAuth = request.headers.get(b'Proxy-Authorization', None)\n                 proxyConf = (proxyHost, proxyPort, proxyAuth)\n\n@@ -687,16 +687,6 @@ class HttpProxyTestCase(unittest.TestCase):\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n-    def test_download_with_proxy_https_noconnect(self):\n-        def _test(response):\n-            self.assertEqual(response.status, 200)\n-            self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'https://example.com')\n-\n-        http_proxy = '%s?noconnect' % self.getURL('')\n-        request = Request('https://example.com', meta={'proxy': http_proxy})\n-        return self.download_request(request, Spider('foo')).addCallback(_test)\n-\n     def test_download_without_proxy(self):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n\n@@ -108,32 +108,6 @@ class ProxyConnectTestCase(TestCase):\n         echo = json.loads(crawler.spider.meta['responses'][0].text)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n-    # The noconnect mode isn't supported by the current mitmproxy, it returns\n-    # \"Invalid request scheme: https\" as it doesn't seem to support full URLs in GET at all,\n-    # and it's not clear what behavior is intended by Scrapy and by mitmproxy here.\n-    # https://github.com/mitmproxy/mitmproxy/issues/848 may be related.\n-    # The Scrapy noconnect mode was required, at least in the past, to work with Crawlera,\n-    # and https://github.com/scrapy-plugins/scrapy-crawlera/pull/44 seems to be related.\n-\n-    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n-    @defer.inlineCallbacks\n-    def test_https_noconnect(self):\n-        proxy = os.environ['https_proxy']\n-        os.environ['https_proxy'] = proxy + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n-\n-    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n-    @defer.inlineCallbacks\n-    def test_https_noconnect_auth_error(self):\n-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(407, l)\n-\n     def _assert_got_response_code(self, code, log):\n         print(log)\n         self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17e648182332a1d231383c7416e08e89280cb1d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 184 | Contributors (this commit): 13 | Commits (past 90d): 13 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -279,7 +279,7 @@ intersphinx_mapping = {\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n     'tox': ('https://tox.readthedocs.io/en/latest', None),\n-    'twisted': ('https://twistedmatrix.com/documents/current', None),\n+    'twisted': ('https://twistedmatrix.com/documents/current/api', None),\n }\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#048cd74ae594f449ba97d07c927d7640f32a6770", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 187 | Contributors (this commit): 13 | Commits (past 90d): 14 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -279,7 +279,8 @@ intersphinx_mapping = {\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n     'tox': ('https://tox.readthedocs.io/en/latest', None),\n-    'twisted': ('https://twistedmatrix.com/documents/current/api', None),\n+    'twisted': ('https://twistedmatrix.com/documents/current', None),\n+    'twistedapi': ('https://twistedmatrix.com/documents/current/api', None),\n }\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1cdfb47013330b0391a8db3b6b812697ee64b6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1604 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import pprint\n import six\n import signal\n import logging\n@@ -45,7 +46,8 @@ class Crawler(object):\n         logging.root.addHandler(handler)\n \n         d = dict(overridden_settings(self.settings))\n-        logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n+        logger.info(\"Overridden settings:\\n%(settings)s\",\n+                    {'settings': pprint.pformat(d)})\n \n         if get_scrapy_root_handler() is not None:\n             # scrapy root handler already installed: update it with new settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d77f74e4089c1a9700fb9e2bc62fc196176250c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 43 | Files Changed: 8 | Hunks: 36 | Methods Changed: 21 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 125 | Churn Cumulative: 3906 | Contributors (this commit): 32 | Commits (past 90d): 30 | Contributors (cumulative): 81 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from twisted.internet import defer\n import six\n from scrapy.exceptions import NotSupported, NotConfigured\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.python import without_none_values\n from scrapy import signals\n \n@@ -48,7 +48,11 @@ class DownloadHandlers(object):\n             dhcls = load_object(path)\n             if skip_lazy and getattr(dhcls, 'lazy', True):\n                 return None\n-            dh = dhcls(self._crawler.settings)\n+            dh = create_instance(\n+                dhcls,\n+                self._crawler.settings,\n+                self._crawler,\n+            )\n         except NotConfigured as ex:\n             self._notconfigured[scheme] = str(ex)\n             return None\n\n@@ -8,9 +8,6 @@ from scrapy.utils.decorators import defers\n class DataURIDownloadHandler(object):\n     lazy = False\n \n-    def __init__(self, settings):\n-        super(DataURIDownloadHandler, self).__init__()\n-\n     @defers\n     def download_request(self, request, spider):\n         uri = parse_data_uri(request.url)\n\n@@ -1,4 +1,5 @@\n from w3lib.url import file_uri_to_path\n+\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n@@ -6,9 +7,6 @@ from scrapy.utils.decorators import defers\n class FileDownloadHandler(object):\n     lazy = False\n \n-    def __init__(self, settings):\n-        pass\n-\n     @defers\n     def download_request(self, request, spider):\n         filepath = file_uri_to_path(request.url)\n\n@@ -59,6 +59,7 @@ class ReceivedDataProtocol(Protocol):\n     def close(self):\n         self.body.close() if self.filename else self.body.seek(0)\n \n+\n _CODE_RE = re.compile(r\"\\d+\")\n \n \n@@ -70,10 +71,14 @@ class FTPDownloadHandler(object):\n         \"default\": 503,\n     }\n \n-    def __init__(self, settings):\n-        self.default_user = settings['FTP_USER']\n-        self.default_password = settings['FTP_PASSWORD']\n-        self.passive_mode = settings['FTP_PASSIVE_MODE']\n+    def __init__(self, crawler):\n+        self.default_user = crawler.settings['FTP_USER']\n+        self.default_password = crawler.settings['FTP_PASSWORD']\n+        self.passive_mode = crawler.settings['FTP_PASSIVE_MODE']\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n \n     def download_request(self, request, spider):\n         parsed_url = urlparse_cached(request)\n\n@@ -1,6 +1,7 @@\n \"\"\"Download handlers for http and https schemes\n \"\"\"\n from twisted.internet import reactor\n+\n from scrapy.utils.misc import load_object, create_instance\n from scrapy.utils.python import to_unicode\n \n@@ -8,10 +9,15 @@ from scrapy.utils.python import to_unicode\n class HTTP10DownloadHandler(object):\n     lazy = False\n \n-    def __init__(self, settings):\n-        self.HTTPClientFactory = load_object(settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n-        self.ClientContextFactory = load_object(settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n-        self._settings = settings\n+    def __init__(self, crawler):\n+        self.HTTPClientFactory = load_object(crawler.settings['DOWNLOADER_HTTPCLIENTFACTORY'])\n+        self.ClientContextFactory = load_object(crawler.settings['DOWNLOADER_CLIENTCONTEXTFACTORY'])\n+        self._crawler = crawler\n+        self._settings = crawler.settings\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n \n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n@@ -22,7 +28,11 @@ class HTTP10DownloadHandler(object):\n     def _connect(self, factory):\n         host, port = to_unicode(factory.host), factory.port\n         if factory.scheme == b'https':\n-            client_context_factory = create_instance(self.ClientContextFactory, settings=self._settings, crawler=None)\n+            client_context_factory = create_instance(\n+                self.ClientContextFactory,\n+                settings=self._settings,\n+                crawler=self._crawler,\n+            )\n             return reactor.connectSSL(host, port, factory, client_context_factory)\n         else:\n             return reactor.connectTCP(host, port, factory)\n\n@@ -30,7 +30,9 @@ logger = logging.getLogger(__name__)\n class HTTP11DownloadHandler(object):\n     lazy = False\n \n-    def __init__(self, settings):\n+    def __init__(self, crawler):\n+        settings = crawler.settings\n+\n         self._pool = HTTPConnectionPool(reactor, persistent=True)\n         self._pool.maxPersistentPerHost = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n         self._pool._factory.noisy = False\n@@ -42,7 +44,7 @@ class HTTP11DownloadHandler(object):\n             self._contextFactory = create_instance(\n                 self._contextFactoryClass,\n                 settings=settings,\n-                crawler=None,\n+                crawler=crawler,\n                 method=self._sslMethod,\n             )\n         except TypeError:\n@@ -50,7 +52,7 @@ class HTTP11DownloadHandler(object):\n             self._contextFactory = create_instance(\n                 self._contextFactoryClass,\n                 settings=settings,\n-                crawler=None,\n+                crawler=crawler,\n             )\n             msg = \"\"\"\n  '%s' does not accept `method` argument (type OpenSSL.SSL method,\\\n@@ -63,6 +65,10 @@ class HTTP11DownloadHandler(object):\n         self._fail_on_dataloss = settings.getbool('DOWNLOAD_FAIL_ON_DATALOSS')\n         self._disconnect_timeout = 1\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def download_request(self, request, spider):\n         \"\"\"Return a deferred for the HTTP download\"\"\"\n         agent = ScrapyAgent(\n\n@@ -32,13 +32,12 @@ def _get_boto_connection():\n \n class S3DownloadHandler(object):\n \n-    def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n+    def __init__(self, crawler, aws_access_key_id=None, aws_secret_access_key=None,\n                  httpdownloadhandler=HTTPDownloadHandler, **kw):\n-\n         if not aws_access_key_id:\n-            aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n+            aws_access_key_id = crawler.settings['AWS_ACCESS_KEY_ID']\n         if not aws_secret_access_key:\n-            aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n+            aws_secret_access_key = crawler.settings['AWS_SECRET_ACCESS_KEY']\n \n         # If no credentials could be found anywhere,\n         # consider this an anonymous connection request by default;\n@@ -67,7 +66,11 @@ class S3DownloadHandler(object):\n             except Exception as ex:\n                 raise NotConfigured(str(ex))\n \n-        self._download_http = httpdownloadhandler(settings).download_request\n+        self._download_http = httpdownloadhandler(crawler).download_request\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n \n     def download_request(self, request, spider):\n         p = urlparse_cached(request)\n\n@@ -30,7 +30,6 @@ from scrapy.spiders import Spider\n from scrapy.http import Headers, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler, skip_if_no_boto\n from scrapy.utils.python import to_bytes\n from scrapy.exceptions import NotConfigured\n@@ -45,6 +44,10 @@ class DummyDH(object):\n     def __init__(self, crawler):\n         pass\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n \n class DummyLazyDH(object):\n     # Default is lazy for backward compatibility\n@@ -52,6 +55,10 @@ class DummyLazyDH(object):\n     def __init__(self, crawler):\n         pass\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n \n class OffDH(object):\n     lazy = False\n@@ -59,6 +66,10 @@ class OffDH(object):\n     def __init__(self, crawler):\n         raise NotConfigured\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n \n class LoadTestCase(unittest.TestCase):\n \n@@ -106,7 +117,7 @@ class FileTestCase(unittest.TestCase):\n         self.tmpname = self.mktemp()\n         with open(self.tmpname + '^', 'w') as f:\n             f.write('0123456789')\n-        self.download_request = FileDownloadHandler(Settings()).download_request\n+        self.download_request = FileDownloadHandler().download_request\n \n     def tearDown(self):\n         os.unlink(self.tmpname + '^')\n@@ -239,7 +250,7 @@ class HttpTestCase(unittest.TestCase):\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls(Settings())\n+        self.download_handler = self.download_handler_cls(get_crawler())\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -479,9 +490,9 @@ class Http11TestCase(HttpTestCase):\n         return self.test_download_broken_content_allow_data_loss('broken-chunked')\n \n     def test_download_broken_content_allow_data_loss_via_setting(self, url='broken'):\n-        download_handler = self.download_handler_cls(Settings({\n-            'DOWNLOAD_FAIL_ON_DATALOSS': False,\n-        }))\n+        download_handler = self.download_handler_cls(\n+            get_crawler(settings_dict={'DOWNLOAD_FAIL_ON_DATALOSS': False})\n+        )\n         request = Request(self.getURL(url))\n         d = download_handler.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.flags)\n@@ -499,9 +510,9 @@ class Https11TestCase(Http11TestCase):\n \n     @defer.inlineCallbacks\n     def test_tls_logging(self):\n-        download_handler = self.download_handler_cls(Settings({\n-            'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True,\n-        }))\n+        download_handler = self.download_handler_cls(\n+            get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING': True})\n+        )\n         try:\n             with LogCapture() as log_capture:\n                 request = Request(self.getURL('file'))\n@@ -569,7 +580,8 @@ class Https11CustomCiphers(unittest.TestCase):\n             interface=self.host)\n         self.portno = self.port.getHost().port\n         self.download_handler = self.download_handler_cls(\n-            Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'}))\n+            get_crawler(settings_dict={'DOWNLOADER_CLIENT_TLS_CIPHERS': 'CAMELLIA256-SHA'})\n+        )\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -665,7 +677,7 @@ class HttpProxyTestCase(unittest.TestCase):\n         wrapper = WrappingFactory(site)\n         self.port = reactor.listenTCP(0, wrapper, interface='127.0.0.1')\n         self.portno = self.port.getHost().port\n-        self.download_handler = self.download_handler_cls(Settings())\n+        self.download_handler = self.download_handler_cls(get_crawler())\n         self.download_request = self.download_handler.download_request\n \n     @defer.inlineCallbacks\n@@ -738,7 +750,8 @@ class S3AnonTestCase(unittest.TestCase):\n \n     def setUp(self):\n         skip_if_no_boto()\n-        self.s3reqh = S3DownloadHandler(Settings(),\n+        self.s3reqh = S3DownloadHandler(\n+            crawler=get_crawler(),\n             httpdownloadhandler=HttpDownloadHandlerMock,\n             #anon=True, # is implicit\n         )\n@@ -766,9 +779,12 @@ class S3TestCase(unittest.TestCase):\n \n     def setUp(self):\n         skip_if_no_boto()\n-        s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID,\n+        s3reqh = S3DownloadHandler(\n+            get_crawler(),\n+            self.AWS_ACCESS_KEY_ID,\n             self.AWS_SECRET_ACCESS_KEY,\n-                httpdownloadhandler=HttpDownloadHandlerMock)\n+            httpdownloadhandler=HttpDownloadHandlerMock,\n+        )\n         self.download_request = s3reqh.download_request\n         self.spider = Spider('foo')\n \n@@ -788,7 +804,7 @@ class S3TestCase(unittest.TestCase):\n \n     def test_extra_kw(self):\n         try:\n-            S3DownloadHandler(Settings(), extra_kw=True)\n+            S3DownloadHandler(get_crawler(), extra_kw=True)\n         except Exception as e:\n             self.assertIsInstance(e, (TypeError, NotConfigured))\n         else:\n@@ -928,7 +944,7 @@ class BaseFTPTestCase(unittest.TestCase):\n         self.factory = FTPFactory(portal=p)\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n-        self.download_handler = FTPDownloadHandler(Settings())\n+        self.download_handler = FTPDownloadHandler(get_crawler())\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1042,7 +1058,7 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n                                   userAnonymous=self.username)\n         self.port = reactor.listenTCP(0, self.factory, interface=\"127.0.0.1\")\n         self.portNum = self.port.getHost().port\n-        self.download_handler = FTPDownloadHandler(Settings())\n+        self.download_handler = FTPDownloadHandler(get_crawler())\n         self.addCleanup(self.port.stopListening)\n \n     def tearDown(self):\n@@ -1052,7 +1068,7 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n class DataURITestCase(unittest.TestCase):\n \n     def setUp(self):\n-        self.download_handler = DataURIDownloadHandler(Settings())\n+        self.download_handler = DataURIDownloadHandler()\n         self.download_request = self.download_handler.download_request\n         self.spider = Spider('foo')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
