{"custom_id": "scrapy#f85bf77da3c8943f0791dcae893e8294c4d118d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 279 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class CachingThreadedResolver(ThreadedResolver):\n             cache_size = 0\n         return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n \n-    def install_on_reactor(self):\n+    def install_on_reactor(self,):\n         self.reactor.installResolver(self)\n \n     def getHostByName(self, name, timeout=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#889b4718520220d1a81e702ff754ec210a7d3c79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 8 | Churn Cumulative: 2719 | Contributors (this commit): 26 | Commits (past 90d): 21 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,11 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n+import ipaddress\n import logging\n import re\n import warnings\n from contextlib import suppress\n from io import BytesIO\n-from ipaddress import ip_address\n from time import time\n from urllib.parse import urldefrag\n \n@@ -468,7 +468,7 @@ class _ResponseReader(protocol.Protocol):\n                 self._certificate = ssl.Certificate(self.transport._producer.getPeerCertificate())\n \n         if self._ip_address is None:\n-            self._ip_address = ip_address(self.transport._producer.getPeer().host)\n+            self._ip_address = ipaddress.ip_address(self.transport._producer.getPeer().host)\n \n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered data.\n\n@@ -441,13 +441,15 @@ with multiples lines\n         self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n \n     @defer.inlineCallbacks\n-    def test_dns_server_ip_address(self):\n+    def test_dns_server_ip_address_none(self):\n         crawler = self.runner.create_crawler(SingleRequestSpider)\n         url = self.mockserver.url('/status?n=200')\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         ip_address = crawler.spider.meta['responses'][0].ip_address\n         self.assertIsNone(ip_address)\n \n+    @defer.inlineCallbacks\n+    def test_dns_server_ip_address(self):\n         crawler = self.runner.create_crawler(SingleRequestSpider)\n         url = self.mockserver.url('/echo?body=test')\n         expected_netloc, _ = urlparse(url).netloc.split(':')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#31f35c9c002178de20a3e124be4aab98c0a0f892", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 281 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class CachingThreadedResolver(ThreadedResolver):\n             cache_size = 0\n         return cls(reactor, cache_size, crawler.settings.getfloat('DNS_TIMEOUT'))\n \n-    def install_on_reactor(self,):\n+    def install_on_reactor(self):\n         self.reactor.installResolver(self)\n \n     def getHostByName(self, name, timeout=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7417a9871c489686b7d3ac1b85b964eca253c979", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 385 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter',\n \n class BaseItemExporter(object):\n \n-    def __init__(self, dont_fail=False, **kwargs):\n+    def __init__(self, *, dont_fail=False, **kwargs):\n         self._kwargs = kwargs\n         self._configure(kwargs, dont_fail=dont_fail)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a34c366fa4f226cb107a19561ca64fac0d1dbdd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 199 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -281,6 +281,7 @@ coverage_ignore_pyobjects = [\n \n intersphinx_mapping = {\n     'coverage': ('https://coverage.readthedocs.io/en/stable', None),\n+    'cssselect': ('https://cssselect.readthedocs.io/en/latest', None),\n     'pytest': ('https://docs.pytest.org/en/latest', None),\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#034e2c31c7d55333c3de208f80dcee1bf45ef9b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 2 | Files Changed: 8 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 13/4 | Churn Δ: 75 | Churn Cumulative: 294 | Contributors (this commit): 10 | Commits (past 90d): 11 | Contributors (cumulative): 17 | DMM Complexity: 0.46153846153846156\n\nDIFF:\n@@ -54,8 +54,13 @@ class Command(ScrapyCommand):\n             raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n         spname = args[0]\n \n-        self.crawler_process.crawl(spname, **opts.spargs)\n+        crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n+\n+        if getattr(crawl_defer, 'result', None) is not None and issubclass(crawl_defer.result.type, Exception):\n+            self.exitcode = 1\n+        else:\n             self.crawler_process.start()\n \n-        if self.crawler_process.bootstrap_failed:\n+            if self.crawler_process.bootstrap_failed or \\\n+                    (hasattr(self.crawler_process, 'has_exception') and self.crawler_process.has_exception):\n                 self.exitcode = 1\n\n@@ -0,0 +1,20 @@\n+import os\n+import sys\n+import unittest\n+from subprocess import Popen, PIPE\n+\n+\n+class CmdlineCrawlPipelineTest(unittest.TestCase):\n+\n+    def _execute(self, spname):\n+        args = (sys.executable, '-m', 'scrapy.cmdline', 'crawl', spname)\n+        cwd = os.path.dirname(os.path.abspath(__file__))\n+        proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)\n+        proc.communicate()\n+        return proc.returncode\n+\n+    def test_open_spider_normally_in_pipeline(self):\n+        self.assertEqual(self._execute('normal'), 0)\n+\n+    def test_exception_at_open_spider_in_pipeline(self):\n+        self.assertEqual(self._execute('exception'), 1)\n\n\n@@ -0,0 +1,16 @@\n+class TestSpiderPipeline(object):\n+\n+    def open_spider(self, spider):\n+        pass\n+\n+    def process_item(self, item, spider):\n+        return item\n+\n+\n+class TestSpiderExceptionPipeline(object):\n+\n+    def open_spider(self, spider):\n+        raise Exception('exception')\n+\n+    def process_item(self, item, spider):\n+        return item\n\n@@ -0,0 +1,2 @@\n+BOT_NAME = 'test_spider'\n+SPIDER_MODULES = ['test_spider.spiders']\n\n\n@@ -0,0 +1,14 @@\n+import scrapy\n+\n+\n+class ExceptionSpider(scrapy.Spider):\n+    name = 'exception'\n+\n+    custom_settings = {\n+        'ITEM_PIPELINES': {\n+            'test_spider.pipelines.TestSpiderExceptionPipeline': 300\n+        }\n+    }\n+\n+    def parse(self, response):\n+        pass\n\n@@ -0,0 +1,14 @@\n+import scrapy\n+\n+\n+class NormalSpider(scrapy.Spider):\n+    name = 'normal'\n+\n+    custom_settings = {\n+        'ITEM_PIPELINES': {\n+            'test_spider.pipelines.TestSpiderPipeline': 300\n+        }\n+    }\n+\n+    def parse(self, response):\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a9d7d8f064fe3086227d4737d07e3ca4a296b4b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 508 | Contributors (this commit): 22 | Commits (past 90d): 1 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -62,6 +62,7 @@ setup(\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n     python_requires='>=3.5',\n+    # Keep in sync with docs/requirements.txt\n     install_requires=[\n         'Twisted>=17.9.0',\n         'cryptography>=2.0',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#778813717df0d5fcd4359266f6785068feea0785", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 509 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,6 @@ setup(\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n     python_requires='>=3.5',\n-    # Keep in sync with docs/requirements.txt\n     install_requires=[\n         'Twisted>=17.9.0',\n         'cryptography>=2.0',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6109ad9aacd5897f67842685b2405627b4af4ad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 49 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -9,10 +9,8 @@ DEPRECATED_SETTINGS = [\n     ('ENCODING_ALIASES', 'no longer needed (encoding discovery uses w3lib now)'),\n     ('STATS_ENABLED', 'no longer supported (change STATS_CLASS instead)'),\n     ('SQLITE_DB', 'no longer supported'),\n-    ('SELECTORS_BACKEND', 'use SCRAPY_SELECTORS_BACKEND environment variable instead'),\n     ('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY', 'use DOWNLOAD_DELAY instead'),\n     ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n-    ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n     ('REDIRECT_MAX_METAREFRESH_DELAY', 'use METAREFRESH_MAXDELAY instead'),\n     ('LOG_UNSERIALIZABLE_REQUESTS', 'use SCHEDULER_DEBUG instead'),\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2acaa86231e8a743333928907c2933feadf40cd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 274 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -75,9 +75,24 @@ def get_project_settings():\n                       \"is deprecated.\", ScrapyDeprecationWarning)\n         settings.setdict(pickle.loads(pickled_settings), priority='project')\n \n-    env_overrides = {k[7:]: v for k, v in os.environ.items() if\n+    scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n                       k.startswith('SCRAPY_')}\n-    if env_overrides:\n-        warnings.warn(\"Use of 'SCRAPY_'-prefixed environment variables to override settings is deprecated.\", ScrapyDeprecationWarning)\n-        settings.setdict(env_overrides, priority='project')\n+    valid_envvars = {\n+        'SCRAPY_CHECK',\n+        'SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE',\n+        'SCRAPY_PROJECT',\n+        'SCRAPY_PYTHON_SHELL',\n+        'SCRAPY_SETTINGS_MODULE',\n+    }\n+    setting_envvars = {k for k in scrapy_envvars if k not in valid_envvars}\n+    if setting_envvars:\n+        setting_envvar_list = ', '.join(sorted(setting_envvars))\n+        warnings.warn(\n+            'Use of environment variables prefixed with SCRAPY_ to override '\n+            'settings is deprecated. The following environment variables are '\n+            'currently defined: {}'.format(setting_envvar_list),\n+            ScrapyDeprecationWarning\n+        )\n+    settings.setdict(scrapy_envvars, priority='project')\n+\n     return settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9aae4c0be7b42e27daa2750b6f01eb497edcd98a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 66 | Churn Cumulative: 453 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 0.7317073170731707\n\nDIFF:\n@@ -78,11 +78,11 @@ def get_project_settings():\n     scrapy_envvars = {k[7:]: v for k, v in os.environ.items() if\n                       k.startswith('SCRAPY_')}\n     valid_envvars = {\n-        'SCRAPY_CHECK',\n-        'SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE',\n-        'SCRAPY_PROJECT',\n-        'SCRAPY_PYTHON_SHELL',\n-        'SCRAPY_SETTINGS_MODULE',\n+        'CHECK',\n+        'PICKLED_SETTINGS_TO_OVERRIDE',\n+        'PROJECT',\n+        'PYTHON_SHELL',\n+        'SETTINGS_MODULE',\n     }\n     setting_envvars = {k for k in scrapy_envvars if k not in valid_envvars}\n     if setting_envvars:\n\n@@ -3,7 +3,11 @@ import os\n import tempfile\n import shutil\n import contextlib\n-from scrapy.utils.project import data_path\n+\n+from pytest import warns\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.project import data_path, get_project_settings\n \n \n @contextlib.contextmanager\n@@ -41,3 +45,53 @@ class ProjectUtilsTest(unittest.TestCase):\n             )\n             abspath = os.path.join(os.path.sep, 'absolute', 'path')\n             self.assertEqual(abspath, data_path(abspath))\n+\n+\n+@contextlib.contextmanager\n+def set_env(**update):\n+    modified = set(update.keys()) & set(os.environ.keys())\n+    update_after = {k: os.environ[k] for k in modified}\n+    remove_after = frozenset(k for k in update if k not in os.environ)\n+    try:\n+        os.environ.update(update)\n+        yield\n+    finally:\n+        os.environ.update(update_after)\n+        for k in remove_after:\n+            os.environ.pop(k)\n+\n+\n+class GetProjectSettingsTestCase(unittest.TestCase):\n+\n+    def test_valid_envvar(self):\n+        value = 'tests.test_cmdline.settings'\n+        envvars = {\n+            'SCRAPY_SETTINGS_MODULE': value,\n+        }\n+        with set_env(**envvars), warns(None) as warnings:\n+            settings = get_project_settings()\n+        assert not warnings\n+        assert settings.get('SETTINGS_MODULE') == value\n+\n+    def test_invalid_envvar(self):\n+        envvars = {\n+            'SCRAPY_FOO': 'bar',\n+        }\n+        with set_env(**envvars), warns(None) as warnings:\n+            get_project_settings()\n+        assert len(warnings) == 1\n+        assert warnings[0].category == ScrapyDeprecationWarning\n+        assert str(warnings[0].message).endswith(': FOO')\n+\n+    def test_valid_and_invalid_envvars(self):\n+        value = 'tests.test_cmdline.settings'\n+        envvars = {\n+            'SCRAPY_FOO': 'bar',\n+            'SCRAPY_SETTINGS_MODULE': value,\n+        }\n+        with set_env(**envvars), warns(None) as warnings:\n+            settings = get_project_settings()\n+        assert len(warnings) == 1\n+        assert warnings[0].category == ScrapyDeprecationWarning\n+        assert str(warnings[0].message).endswith(': FOO')\n+        assert settings.get('SETTINGS_MODULE') == value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c411a51f42a5e6d241d69349b228f1584fdbd31b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 2 | Churn Cumulative: 769 | Contributors (this commit): 14 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_fixed_delay(self):\n-        yield self._test_delay(total=3, delay=0.1)\n+        yield self._test_delay(total=3, delay=0.2)\n \n     @defer.inlineCallbacks\n     def test_randomized_delay(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef00f8eb8eb4f5727409fd40c5826661db2bb665", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class MemoryUsage(object):\n     def get_virtual_size(self):\n         size = self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss\n         if sys.platform != 'darwin':\n-            # on Mac OS X ru_maxrss is in bytes, on Linux it is in KB\n+            # on macOS ru_maxrss is in bytes, on Linux it is in KB\n             size *= 1024\n         return size\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6aa0ba45532a4fd8e868bb4ea15bf002e430e67f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 1579 | Contributors (this commit): 26 | Commits (past 90d): 20 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -129,6 +129,9 @@ class Response(object_ref):\n         :class:`~.TextResponse` provides a :meth:`~.TextResponse.follow`\n         method which supports selectors in addition to absolute/relative URLs\n         and Link objects.\n+\n+        .. versionadded:: 2.0\n+           The *flags* parameter.\n         \"\"\"\n         if isinstance(url, Link):\n             url = url.url\n@@ -157,6 +160,8 @@ class Response(object_ref):\n                    dont_filter=False, errback=None, cb_kwargs=None, flags=None):\n         # type: (...) -> Generator[Request, None, None]\n         \"\"\"\n+        .. versionadded:: 2.0\n+\n         Return an iterable of :class:`~.Request` instances to follow all links\n         in ``urls``. It accepts the same arguments as ``Request.__init__`` method,\n         but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects,\n\n@@ -97,7 +97,11 @@ class LogFormatter(object):\n         }\n \n     def item_error(self, item, exception, response, spider):\n-        \"\"\"Logs a message when an item causes an error while it is passing through the item pipeline.\"\"\"\n+        \"\"\"Logs a message when an item causes an error while it is passing\n+        through the item pipeline.\n+\n+        .. versionadded:: 2.0\n+        \"\"\"\n         return {\n             'level': logging.ERROR,\n             'msg': ITEMERRORMSG,\n@@ -107,7 +111,10 @@ class LogFormatter(object):\n         }\n \n     def spider_error(self, failure, request, response, spider):\n-        \"\"\"Logs an error message from a spider.\"\"\"\n+        \"\"\"Logs an error message from a spider.\n+\n+        .. versionadded:: 2.0\n+        \"\"\"\n         return {\n             'level': logging.ERROR,\n             'msg': SPIDERERRORMSG,\n@@ -118,7 +125,11 @@ class LogFormatter(object):\n         }\n \n     def download_error(self, failure, request, spider, errmsg=None):\n-        \"\"\"Logs a download error message from a spider (typically coming from the engine).\"\"\"\n+        \"\"\"Logs a download error message from a spider (typically coming from\n+        the engine).\n+\n+        .. versionadded:: 2.0\n+        \"\"\"\n         args = {'request': request}\n         if errmsg:\n             msg = DOWNLOADERRORMSG_LONG\n\n@@ -50,6 +50,8 @@ class CallLaterOnce(object):\n \n \n def install_reactor(reactor_path):\n+    \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n+    import path.\"\"\"\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n@@ -63,6 +65,9 @@ def install_reactor(reactor_path):\n \n \n def verify_installed_reactor(reactor_path):\n+    \"\"\"Raises :exc:`Exception` if the installed\n+    :mod:`~twisted.internet.reactor` does not match the specified import\n+    path.\"\"\"\n     from twisted.internet import reactor\n     reactor_class = load_object(reactor_path)\n     if not isinstance(reactor, reactor_class):\n\n@@ -325,7 +325,7 @@ with multiples lines\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n-        runner = CrawlerRunner({\"ASYNCIO_REACTOR\": True})\n+        runner = CrawlerRunner({\"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"})\n         runner.crawl(AsyncDefAsyncioSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         with LogCapture() as log:\n             yield runner.join()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b591ff061f2b38bf328e1d2a4acd9643d45ad80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1436 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -66,8 +66,7 @@ class Scheduler(object):\n \n         dqclass = load_object(settings['SCHEDULER_DISK_QUEUE'])\n         mqclass = load_object(settings['SCHEDULER_MEMORY_QUEUE'])\n-        logunser = settings.getbool('LOG_UNSERIALIZABLE_REQUESTS',\n-                                    settings.getbool('SCHEDULER_DEBUG'))\n+        logunser = settings.getbool('SCHEDULER_DEBUG')\n         return cls(dupefilter, jobdir=job_dir(settings), logunser=logunser,\n                    stats=crawler.stats, pqclass=pqclass, dqclass=dqclass,\n                    mqclass=mqclass, crawler=crawler)\n\n@@ -93,8 +93,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n     def __init__(self, settings):\n         super(MetaRefreshMiddleware, self).__init__(settings)\n         self._ignore_tags = settings.getlist('METAREFRESH_IGNORE_TAGS')\n-        self._maxdelay = settings.getint('REDIRECT_MAX_METAREFRESH_DELAY',\n-                                         settings.getint('METAREFRESH_MAXDELAY'))\n+        self._maxdelay = settings.getint('METAREFRESH_MAXDELAY')\n \n     def process_response(self, request, response, spider):\n         if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \\\n\n@@ -11,8 +11,6 @@ DEPRECATED_SETTINGS = [\n     ('SQLITE_DB', 'no longer supported'),\n     ('AUTOTHROTTLE_MIN_DOWNLOAD_DELAY', 'use DOWNLOAD_DELAY instead'),\n     ('AUTOTHROTTLE_MAX_CONCURRENCY', 'use CONCURRENT_REQUESTS_PER_DOMAIN instead'),\n-    ('REDIRECT_MAX_METAREFRESH_DELAY', 'use METAREFRESH_MAXDELAY instead'),\n-    ('LOG_UNSERIALIZABLE_REQUESTS', 'use SCHEDULER_DEBUG instead'),\n ]\n \n \n\n@@ -46,7 +46,7 @@ class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n \n         settings = dict(\n-                LOG_UNSERIALIZABLE_REQUESTS=False,\n+                SCHEDULER_DEBUG=False,\n                 SCHEDULER_DISK_QUEUE='scrapy.squeues.PickleLifoDiskQueue',\n                 SCHEDULER_MEMORY_QUEUE='scrapy.squeues.LifoMemoryQueue',\n                 SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c35baae25517ed942c72d14d93363a389e5a9d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 174 | Files Changed: 1 | Hunks: 2 | Methods Changed: 21 | Complexity Δ (Sum/Max): -42/0 | Churn Δ: 174 | Churn Cumulative: 564 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 0.14655172413793102\n\nDIFF:\n@@ -6,183 +6,9 @@ This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n import collections\n-import copy\n-import warnings\n import weakref\n from collections.abc import Mapping\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-\n-class MultiValueDictKeyError(KeyError):\n-    def __init__(self, *args, **kwargs):\n-        warnings.warn(\n-            \"scrapy.utils.datatypes.MultiValueDictKeyError is deprecated \"\n-            \"and will be removed in future releases.\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2\n-        )\n-        super(MultiValueDictKeyError, self).__init__(*args, **kwargs)\n-\n-\n-class MultiValueDict(dict):\n-    \"\"\"\n-    A subclass of dictionary customized to handle multiple values for the same key.\n-\n-    >>> d = MultiValueDict({'name': ['Adrian', 'Simon'], 'position': ['Developer']})\n-    >>> d['name']\n-    'Simon'\n-    >>> d.getlist('name')\n-    ['Adrian', 'Simon']\n-    >>> d.get('lastname', 'nonexistent')\n-    'nonexistent'\n-    >>> d.setlist('lastname', ['Holovaty', 'Willison'])\n-\n-    This class exists to solve the irritating problem raised by cgi.parse_qs,\n-    which returns a list for every key, even though most Web forms submit\n-    single name-value pairs.\n-    \"\"\"\n-    def __init__(self, key_to_list_mapping=()):\n-        warnings.warn(\"scrapy.utils.datatypes.MultiValueDict is deprecated \"\n-                      \"and will be removed in future releases.\",\n-                      category=ScrapyDeprecationWarning,\n-                      stacklevel=2)\n-        dict.__init__(self, key_to_list_mapping)\n-\n-    def __repr__(self):\n-        return \"<%s: %s>\" % (self.__class__.__name__, dict.__repr__(self))\n-\n-    def __getitem__(self, key):\n-        \"\"\"\n-        Returns the last data value for this key, or [] if it's an empty list;\n-        raises KeyError if not found.\n-        \"\"\"\n-        try:\n-            list_ = dict.__getitem__(self, key)\n-        except KeyError:\n-            raise MultiValueDictKeyError(\"Key %r not found in %r\" % (key, self))\n-        try:\n-            return list_[-1]\n-        except IndexError:\n-            return []\n-\n-    def __setitem__(self, key, value):\n-        dict.__setitem__(self, key, [value])\n-\n-    def __copy__(self):\n-        return self.__class__(dict.items(self))\n-\n-    def __deepcopy__(self, memo=None):\n-        if memo is None:\n-            memo = {}\n-        result = self.__class__()\n-        memo[id(self)] = result\n-        for key, value in dict.items(self):\n-            dict.__setitem__(result, copy.deepcopy(key, memo), copy.deepcopy(value, memo))\n-        return result\n-\n-    def get(self, key, default=None):\n-        \"Returns the default value if the requested data doesn't exist\"\n-        try:\n-            val = self[key]\n-        except KeyError:\n-            return default\n-        if val == []:\n-            return default\n-        return val\n-\n-    def getlist(self, key):\n-        \"Returns an empty list if the requested data doesn't exist\"\n-        try:\n-            return dict.__getitem__(self, key)\n-        except KeyError:\n-            return []\n-\n-    def setlist(self, key, list_):\n-        dict.__setitem__(self, key, list_)\n-\n-    def setdefault(self, key, default=None):\n-        if key not in self:\n-            self[key] = default\n-        return self[key]\n-\n-    def setlistdefault(self, key, default_list=()):\n-        if key not in self:\n-            self.setlist(key, default_list)\n-        return self.getlist(key)\n-\n-    def appendlist(self, key, value):\n-        \"Appends an item to the internal list associated with key\"\n-        self.setlistdefault(key, [])\n-        dict.__setitem__(self, key, self.getlist(key) + [value])\n-\n-    def items(self):\n-        \"\"\"\n-        Returns a list of (key, value) pairs, where value is the last item in\n-        the list associated with the key.\n-        \"\"\"\n-        return [(key, self[key]) for key in self.keys()]\n-\n-    def lists(self):\n-        \"Returns a list of (key, list) pairs.\"\n-        return dict.items(self)\n-\n-    def values(self):\n-        \"Returns a list of the last value on every key list.\"\n-        return [self[key] for key in self.keys()]\n-\n-    def copy(self):\n-        \"Returns a copy of this object.\"\n-        return self.__deepcopy__()\n-\n-    def update(self, *args, **kwargs):\n-        \"update() extends rather than replaces existing key lists. Also accepts keyword args.\"\n-        if len(args) > 1:\n-            raise TypeError(\"update expected at most 1 arguments, got %d\" % len(args))\n-        if args:\n-            other_dict = args[0]\n-            if isinstance(other_dict, MultiValueDict):\n-                for key, value_list in other_dict.lists():\n-                    self.setlistdefault(key, []).extend(value_list)\n-            else:\n-                try:\n-                    for key, value in other_dict.items():\n-                        self.setlistdefault(key, []).append(value)\n-                except TypeError:\n-                    raise ValueError(\"MultiValueDict.update() takes either a MultiValueDict or dictionary\")\n-        for key, value in kwargs.items():\n-            self.setlistdefault(key, []).append(value)\n-\n-\n-class SiteNode(object):\n-    \"\"\"Class to represent a site node (page, image or any other file)\"\"\"\n-\n-    def __init__(self, url):\n-        warnings.warn(\n-            \"scrapy.utils.datatypes.SiteNode is deprecated \"\n-            \"and will be removed in future releases.\",\n-            category=ScrapyDeprecationWarning,\n-            stacklevel=2\n-        )\n-\n-        self.url = url\n-        self.itemnames = []\n-        self.children = []\n-        self.parent = None\n-\n-    def add_child(self, node):\n-        self.children.append(node)\n-        node.parent = self\n-\n-    def to_string(self, level=0):\n-        s = \"%s%s\\n\" % ('  ' * level, self.url)\n-        if self.itemnames:\n-            for n in self.itemnames:\n-                s += \"%sScraped: %s\\n\" % ('  ' * (level + 1), n)\n-        for node in self.children:\n-            s += node.to_string(level + 1)\n-        return s\n-\n \n class CaselessDict(dict):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1566a696217244d2d506ce8962418943d0b1edc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 60 | Churn Cumulative: 2492 | Contributors (this commit): 24 | Commits (past 90d): 26 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -68,17 +68,6 @@ class Crawler:\n         self.spider = None\n         self.engine = None\n \n-    @property\n-    def spiders(self):\n-        if not hasattr(self, '_spiders'):\n-            warnings.warn(\"Crawler.spiders is deprecated, use \"\n-                          \"CrawlerRunner.spider_loader or instantiate \"\n-                          \"scrapy.spiderloader.SpiderLoader with your \"\n-                          \"settings.\",\n-                          category=ScrapyDeprecationWarning, stacklevel=2)\n-            self._spiders = _get_spider_loader(self.settings.frozencopy())\n-        return self._spiders\n-\n     @defer.inlineCallbacks\n     def crawl(self, *args, **kwargs):\n         assert not self.crawling, \"Crawling already taking place\"\n@@ -130,11 +119,27 @@ class CrawlerRunner:\n             \":meth:`crawl` and managed by this class.\"\n     )\n \n+    @staticmethod\n+    def _get_spider_loader(settings):\n+        \"\"\" Get SpiderLoader instance from settings \"\"\"\n+        cls_path = settings.get('SPIDER_LOADER_CLASS')\n+        loader_cls = load_object(cls_path)\n+        try:\n+            verifyClass(ISpiderLoader, loader_cls)\n+        except DoesNotImplement:\n+            warnings.warn(\n+                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n+                'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n+                'Please add all missing methods to avoid unexpected runtime errors.',\n+                category=ScrapyDeprecationWarning, stacklevel=2\n+            )\n+        return loader_cls.from_settings(settings.frozencopy())\n+\n     def __init__(self, settings=None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings = settings\n-        self.spider_loader = _get_spider_loader(settings)\n+        self.spider_loader = self._get_spider_loader(settings)\n         self._crawlers = set()\n         self._active = set()\n         self.bootstrap_failed = False\n@@ -327,19 +332,3 @@ class CrawlerProcess(CrawlerRunner):\n         if self.settings.get(\"TWISTED_REACTOR\"):\n             install_reactor(self.settings[\"TWISTED_REACTOR\"])\n         super()._handle_twisted_reactor()\n-\n-\n-def _get_spider_loader(settings):\n-    \"\"\" Get SpiderLoader instance from settings \"\"\"\n-    cls_path = settings.get('SPIDER_LOADER_CLASS')\n-    loader_cls = load_object(cls_path)\n-    try:\n-        verifyClass(ISpiderLoader, loader_cls)\n-    except DoesNotImplement:\n-        warnings.warn(\n-            'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n-            'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n-            'Please add all missing methods to avoid unexpected runtime errors.',\n-            category=ScrapyDeprecationWarning, stacklevel=2\n-        )\n-    return loader_cls.from_settings(settings.frozencopy())\n\n@@ -33,21 +33,6 @@ class CrawlerTestCase(BaseCrawlerTest):\n     def setUp(self):\n         self.crawler = Crawler(DefaultSpider, Settings())\n \n-    def test_deprecated_attribute_spiders(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            spiders = self.crawler.spiders\n-            self.assertEqual(len(w), 1)\n-            self.assertIn(\"Crawler.spiders\", str(w[0].message))\n-            sl_cls = load_object(self.crawler.settings['SPIDER_LOADER_CLASS'])\n-            self.assertIsInstance(spiders, sl_cls)\n-\n-            self.crawler.spiders\n-            is_one_warning = len(w) == 1\n-            if not is_one_warning:\n-                for warning in w:\n-                    print(warning)\n-            self.assertTrue(is_one_warning, \"Warn deprecated access only once\")\n-\n     def test_populate_spidercls_settings(self):\n         spider_settings = {'TEST1': 'spider', 'TEST2': 'spider'}\n         project_settings = {'TEST1': 'project', 'TEST3': 'project'}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ada37c5409047291ee5852fb2220e8e256424402", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 429 | Lines Deleted: 177 | Files Changed: 8 | Hunks: 97 | Methods Changed: 50 | Complexity Δ (Sum/Max): 35/16 | Churn Δ: 606 | Churn Cumulative: 4404 | Contributors (this commit): 71 | Commits (past 90d): 90 | Contributors (cumulative): 145 | DMM Complexity: 0.7619047619047619\n\nDIFF:\n@@ -1,7 +1,5 @@\n-import os\n from scrapy.commands import ScrapyCommand\n-from scrapy.utils.conf import arglist_to_dict\n-from scrapy.utils.python import without_none_values\n+from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n from scrapy.exceptions import UsageError\n \n \n@@ -19,7 +17,7 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n         parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                           help=\"format to use for dumping items with -o\")\n@@ -31,21 +29,8 @@ class Command(ScrapyCommand):\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n         if opts.output:\n-            if opts.output == '-':\n-                self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n-            else:\n-                self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            feed_exporters = without_none_values(\n-                self.settings.getwithbase('FEED_EXPORTERS'))\n-            valid_output_formats = feed_exporters.keys()\n-            if not opts.output_format:\n-                opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n-            if opts.output_format not in valid_output_formats:\n-                raise UsageError(\"Unrecognized output format '%s', set one\"\n-                                 \" using the '-t' switch or as a file extension\"\n-                                 \" from the supported list %s\" % (opts.output_format,\n-                                                                  tuple(valid_output_formats)))\n-            self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')\n+            feeds = feed_process_params_from_cli(self.settings, opts.output, opts.output_format)\n+            self.settings.set('FEEDS', feeds, priority='cmdline')\n \n     def run(self, args, opts):\n         if len(args) < 1:\n\n@@ -5,8 +5,7 @@ from importlib import import_module\n from scrapy.utils.spider import iter_spider_classes\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.conf import arglist_to_dict\n-from scrapy.utils.python import without_none_values\n+from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n \n \n def _import_file(filepath):\n@@ -43,7 +42,7 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                           help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n                           help=\"dump scraped items into FILE (use - for stdout)\")\n         parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                           help=\"format to use for dumping items with -o\")\n@@ -55,19 +54,8 @@ class Command(ScrapyCommand):\n         except ValueError:\n             raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n         if opts.output:\n-            if opts.output == '-':\n-                self.settings.set('FEED_URI', 'stdout:', priority='cmdline')\n-            else:\n-                self.settings.set('FEED_URI', opts.output, priority='cmdline')\n-            feed_exporters = without_none_values(self.settings.getwithbase('FEED_EXPORTERS'))\n-            if not opts.output_format:\n-                opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n-            if opts.output_format not in feed_exporters:\n-                raise UsageError(\"Unrecognized output format '%s', set one\"\n-                                 \" using the '-t' switch or as a file extension\"\n-                                 \" from the supported list %s\" % (opts.output_format,\n-                                                                  tuple(feed_exporters)))\n-            self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')\n+            feeds = feed_process_params_from_cli(self.settings, opts.output, opts.output_format)\n+            self.settings.set('FEEDS', feeds, priority='cmdline')\n \n     def run(self, args, opts):\n         if len(args) != 1:\n\n@@ -4,24 +4,27 @@ Feed Exports extension\n See documentation in docs/topics/feed-exports.rst\n \"\"\"\n \n+import logging\n import os\n import sys\n-import logging\n-from tempfile import NamedTemporaryFile\n+import warnings\n from datetime import datetime\n-from urllib.parse import urlparse, unquote\n+from tempfile import NamedTemporaryFile\n+from urllib.parse import unquote, urlparse\n \n-from zope.interface import Interface, implementer\n from twisted.internet import defer, threads\n from w3lib.url import file_uri_to_path\n+from zope.interface import implementer, Interface\n \n from scrapy import signals\n-from scrapy.utils.ftp import ftp_store_file\n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.misc import create_instance, load_object\n-from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import without_none_values\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore\n+from scrapy.utils.conf import feed_complete_default_values_from_settings\n+from scrapy.utils.ftp import ftp_store_file\n+from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.misc import create_instance, load_object\n+from scrapy.utils.python import without_none_values\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -98,8 +101,6 @@ class S3FeedStorage(BlockingFeedStorage):\n             from scrapy.utils.project import get_project_settings\n             settings = get_project_settings()\n             if 'AWS_ACCESS_KEY_ID' in settings or 'AWS_SECRET_ACCESS_KEY' in settings:\n-                import warnings\n-                from scrapy.exceptions import ScrapyDeprecationWarning\n                 warnings.warn(\n                     \"Initialising `scrapy.extensions.feedexport.S3FeedStorage` \"\n                     \"without AWS keys is deprecated. Please supply credentials or \"\n@@ -178,70 +179,101 @@ class FTPFeedStorage(BlockingFeedStorage):\n         )\n \n \n-class SpiderSlot(object):\n-    def __init__(self, file, exporter, storage, uri):\n+class _FeedSlot(object):\n+    def __init__(self, file, exporter, storage, uri, format, store_empty):\n         self.file = file\n         self.exporter = exporter\n         self.storage = storage\n+        # feed params\n         self.uri = uri\n+        self.format = format\n+        self.store_empty = store_empty\n+        # flags\n         self.itemcount = 0\n+        self._exporting = False\n+\n+    def start_exporting(self):\n+        if not self._exporting:\n+            self.exporter.start_exporting()\n+            self._exporting = True\n+\n+    def finish_exporting(self):\n+        if self._exporting:\n+            self.exporter.finish_exporting()\n+            self._exporting = False\n \n \n class FeedExporter(object):\n \n-    def __init__(self, settings):\n-        self.settings = settings\n-        if not settings['FEED_URI']:\n-            raise NotConfigured\n-        self.urifmt = str(settings['FEED_URI'])\n-        self.format = settings['FEED_FORMAT'].lower()\n-        self.export_encoding = settings['FEED_EXPORT_ENCODING']\n-        self.storages = self._load_components('FEED_STORAGES')\n-        self.exporters = self._load_components('FEED_EXPORTERS')\n-        if not self._storage_supported(self.urifmt):\n-            raise NotConfigured\n-        if not self._exporter_supported(self.format):\n-            raise NotConfigured\n-        self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n-        self._exporting = False\n-        self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n-        self.indent = None\n-        if settings.get('FEED_EXPORT_INDENT') is not None:\n-            self.indent = settings.getint('FEED_EXPORT_INDENT')\n-        uripar = settings['FEED_URI_PARAMS']\n-        self._uripar = load_object(uripar) if uripar else lambda x, y: None\n-\n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls(crawler.settings)\n-        o.crawler = crawler\n-        crawler.signals.connect(o.open_spider, signals.spider_opened)\n-        crawler.signals.connect(o.close_spider, signals.spider_closed)\n-        crawler.signals.connect(o.item_scraped, signals.item_scraped)\n-        return o\n+        exporter = cls(crawler)\n+        crawler.signals.connect(exporter.open_spider, signals.spider_opened)\n+        crawler.signals.connect(exporter.close_spider, signals.spider_closed)\n+        crawler.signals.connect(exporter.item_scraped, signals.item_scraped)\n+        return exporter\n+\n+    def __init__(self, crawler):\n+        self.crawler = crawler\n+        self.settings = crawler.settings\n+        self.feeds = {}\n+        self.slots = []\n+\n+        if not self.settings['FEEDS'] and not self.settings['FEED_URI']:\n+            raise NotConfigured\n+\n+        # Begin: Backward compatibility for FEED_URI and FEED_FORMAT settings\n+        if self.settings['FEED_URI']:\n+            warnings.warn(\n+                'The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of '\n+                'the `FEEDS` setting. Please see the `FEEDS` setting docs for more details',\n+                category=ScrapyDeprecationWarning, stacklevel=2,\n+            )\n+            uri = str(self.settings['FEED_URI'])  # handle pathlib.Path objects\n+            feed = {'format': self.settings.get('FEED_FORMAT', 'jsonlines')}\n+            self.feeds[uri] = feed_complete_default_values_from_settings(feed, self.settings)\n+        # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n+\n+        # 'FEEDS' setting takes precedence over 'FEED_URI'\n+        for uri, feed in self.settings.getdict('FEEDS').items():\n+            uri = str(uri)  # handle pathlib.Path objects\n+            self.feeds[uri] = feed_complete_default_values_from_settings(feed, self.settings)\n+\n+        self.storages = self._load_components('FEED_STORAGES')\n+        self.exporters = self._load_components('FEED_EXPORTERS')\n+        for uri, feed in self.feeds.items():\n+            if not self._storage_supported(uri):\n+                raise NotConfigured\n+            if not self._exporter_supported(feed['format']):\n+                raise NotConfigured\n \n     def open_spider(self, spider):\n-        uri = self.urifmt % self._get_uri_params(spider)\n+        for uri, feed in self.feeds.items():\n+            uri = uri % self._get_uri_params(spider, feed['uri_params'])\n             storage = self._get_storage(uri)\n             file = storage.open(spider)\n-        exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n-            encoding=self.export_encoding, indent=self.indent)\n-        if self.store_empty:\n-            exporter.start_exporting()\n-            self._exporting = True\n-        self.slot = SpiderSlot(file, exporter, storage, uri)\n+            exporter = self._get_exporter(\n+                file=file,\n+                format=feed['format'],\n+                fields_to_export=feed['fields'],\n+                encoding=feed['encoding'],\n+                indent=feed['indent'],\n+            )\n+            slot = _FeedSlot(file, exporter, storage, uri, feed['format'], feed['store_empty'])\n+            self.slots.append(slot)\n+            if slot.store_empty:\n+                slot.start_exporting()\n \n     def close_spider(self, spider):\n-        slot = self.slot\n-        if not slot.itemcount and not self.store_empty:\n+        deferred_list = []\n+        for slot in self.slots:\n+            if not slot.itemcount and not slot.store_empty:\n                 # We need to call slot.storage.store nonetheless to get the file\n                 # properly closed.\n                 return defer.maybeDeferred(slot.storage.store, slot.file)\n-        if self._exporting:\n-            slot.exporter.finish_exporting()\n-            self._exporting = False\n+            slot.finish_exporting()\n             logfmt = \"%s %%(format)s feed (%%(itemcount)d items) in: %%(uri)s\"\n-        log_args = {'format': self.format,\n+            log_args = {'format': slot.format,\n                         'itemcount': slot.itemcount,\n                         'uri': slot.uri}\n             d = defer.maybeDeferred(slot.storage.store, slot.file)\n@@ -250,16 +282,14 @@ class FeedExporter(object):\n             d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                                 exc_info=failure_to_exc_info(f),\n                                                 extra={'spider': spider}))\n-        return d\n+            deferred_list.append(d)\n+        return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def item_scraped(self, item, spider):\n-        slot = self.slot\n-        if not self._exporting:\n-            slot.exporter.start_exporting()\n-            self._exporting = True\n+        for slot in self.slots:\n+            slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n-        return item\n \n     def _load_components(self, setting_prefix):\n         conf = without_none_values(self.settings.getwithbase(setting_prefix))\n@@ -295,17 +325,18 @@ class FeedExporter(object):\n             objcls, self.settings, getattr(self, 'crawler', None),\n             *args, **kwargs)\n \n-    def _get_exporter(self, *args, **kwargs):\n-        return self._get_instance(self.exporters[self.format], *args, **kwargs)\n+    def _get_exporter(self, file, format, *args, **kwargs):\n+        return self._get_instance(self.exporters[format], file, *args, **kwargs)\n \n     def _get_storage(self, uri):\n         return self._get_instance(self.storages[urlparse(uri).scheme], uri)\n \n-    def _get_uri_params(self, spider):\n+    def _get_uri_params(self, spider, uri_params):\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n         ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n         params['time'] = ts\n-        self._uripar(params, spider)\n+        uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n+        uripar_function(params, spider)\n         return params\n\n@@ -133,9 +133,8 @@ EXTENSIONS_BASE = {\n }\n \n FEED_TEMPDIR = None\n-FEED_URI = None\n+FEEDS = {}\n FEED_URI_PARAMS = None  # a function to extend uri arguments\n-FEED_FORMAT = 'jsonlines'\n FEED_STORE_EMPTY = False\n FEED_EXPORT_ENCODING = None\n FEED_EXPORT_FIELDS = None\n\n@@ -1,9 +1,12 @@\n+import numbers\n import os\n import sys\n-import numbers\n+import warnings\n from configparser import ConfigParser\n from operator import itemgetter\n \n+from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n+\n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n@@ -106,3 +109,66 @@ def get_sources(use_closest=True):\n     if use_closest:\n         sources.append(closest_scrapy_cfg())\n     return sources\n+\n+\n+def feed_complete_default_values_from_settings(feed, settings):\n+    out = feed.copy()\n+    if 'encoding' not in out:\n+        out['encoding'] = settings['FEED_EXPORT_ENCODING']\n+    if 'fields' not in out:\n+        out['fields'] = settings.getlist('FEED_EXPORT_FIELDS') or None\n+    if 'indent' not in out:\n+        out['indent'] = None if settings['FEED_EXPORT_INDENT'] is None else settings.getint('FEED_EXPORT_INDENT')\n+    if 'store_empty' not in out:\n+        out['store_empty'] = settings.getbool('FEED_STORE_EMPTY')\n+    if 'uri_params' not in out:\n+        out['uri_params'] = settings['FEED_URI_PARAMS']\n+    return out\n+\n+\n+def feed_process_params_from_cli(settings, output, output_format=None):\n+    \"\"\"\n+    Receives feed export params (from the 'crawl' or 'runspider' commands),\n+    checks for inconsistencies in their quantities and returns a dictionary\n+    suitable to be used as the FEEDS setting.\n+    \"\"\"\n+    valid_output_formats = without_none_values(\n+        settings.getwithbase('FEED_EXPORTERS')\n+    ).keys()\n+\n+    def check_valid_format(output_format):\n+        if output_format not in valid_output_formats:\n+            raise UsageError(\"Unrecognized output format '%s', set one after a\"\n+                             \" colon using the -o option (i.e. -o <URI>:<FORMAT>)\"\n+                             \" or as a file extension, from the supported list %s\" %\n+                             (output_format, tuple(valid_output_formats)))\n+\n+    if output_format:\n+        if len(output) == 1:\n+            check_valid_format(output_format)\n+            warnings.warn('The -t command line option is deprecated in favor'\n+                          ' of specifying the output format within the -o'\n+                          ' option, please check the -o option docs for more details',\n+                          category=ScrapyDeprecationWarning, stacklevel=2)\n+            return {output[0]: {'format': output_format}}\n+        else:\n+            raise UsageError('The -t command line option cannot be used if multiple'\n+                             ' output files are specified with the -o option')\n+\n+    result = {}\n+    for element in output:\n+        try:\n+            feed_uri, feed_format = element.rsplit(':', 1)\n+        except ValueError:\n+            feed_uri = element\n+            feed_format = os.path.splitext(element)[1].replace('.', '')\n+        else:\n+            if feed_uri == '-':\n+                feed_uri = 'stdout:'\n+        check_valid_format(feed_format)\n+        result[feed_uri] = {'format': feed_format}\n+\n+    # FEEDS setting should take precedence over the -o and -t CLI options\n+    result.update(settings.getdict('FEEDS'))\n+\n+    return result\n\n@@ -1,22 +1,46 @@\n import inspect\n+import json\n+import optparse\n import os\n-import sys\n import subprocess\n+import sys\n import tempfile\n+from contextlib import contextmanager\n from os.path import exists, join, abspath\n from shutil import rmtree, copytree\n from tempfile import mkdtemp\n-from contextlib import contextmanager\n from threading import Timer\n \n from twisted.trial import unittest\n \n import scrapy\n+from scrapy.commands import ScrapyCommand\n+from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n+\n from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n+class CommandSettings(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.command = ScrapyCommand()\n+        self.command.settings = Settings()\n+        self.parser = optparse.OptionParser(\n+            formatter=optparse.TitledHelpFormatter(),\n+            conflict_handler='resolve',\n+        )\n+        self.command.add_options(self.parser)\n+\n+    def test_settings_json_string(self):\n+        feeds_json = '{\"data.json\": {\"format\": \"json\"}, \"data.xml\": {\"format\": \"xml\"}}'\n+        opts, args = self.parser.parse_args(args=['-s', 'FEEDS={}'.format(feeds_json), 'spider.py'])\n+        self.command.process_options(args, opts)\n+        self.assertIsInstance(self.command.settings['FEEDS'], scrapy.settings.BaseSettings)\n+        self.assertEqual(dict(self.command.settings['FEEDS']), json.loads(feeds_json))\n+\n+\n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n \n\n@@ -1,32 +1,34 @@\n-import os\n import csv\n import json\n-import warnings\n-import tempfile\n+import os\n+import random\n import shutil\n import string\n+import tempfile\n+import warnings\n from io import BytesIO\n from pathlib import Path\n+from string import ascii_letters, digits\n from unittest import mock\n from urllib.parse import urljoin, urlparse, quote\n from urllib.request import pathname2url\n \n-from zope.interface.verify import verifyObject\n-from twisted.trial import unittest\n+import lxml.etree\n from twisted.internet import defer\n-from scrapy.crawler import CrawlerRunner\n-from scrapy.settings import Settings\n-from tests.mockserver import MockServer\n+from twisted.trial import unittest\n from w3lib.url import path_to_file_uri\n+from zope.interface.verify import verifyObject\n \n import scrapy\n+from scrapy.crawler import CrawlerRunner\n from scrapy.exporters import CsvItemExporter\n-from scrapy.extensions.feedexport import (\n-    IFeedStorage, FileFeedStorage, FTPFeedStorage,\n-    S3FeedStorage, StdoutFeedStorage,\n-    BlockingFeedStorage)\n-from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n+from scrapy.extensions.feedexport import (BlockingFeedStorage, FileFeedStorage, FTPFeedStorage,\n+                                          IFeedStorage, S3FeedStorage, StdoutFeedStorage)\n+from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n+from scrapy.utils.test import assert_aws_environ, get_crawler, get_s3_content_and_delete\n+\n+from tests.mockserver import MockServer\n \n \n class FileFeedStorageTest(unittest.TestCase):\n@@ -395,29 +397,41 @@ class FeedExportTest(unittest.TestCase):\n         egg = scrapy.Field()\n         baz = scrapy.Field()\n \n+    def setUp(self):\n+        self.temp_dir = tempfile.mkdtemp()\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.temp_dir, ignore_errors=True)\n+\n+    def _random_temp_filename(self):\n+        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n+        filename = ''.join(chars)\n+        return os.path.join(self.temp_dir, filename)\n+\n     @defer.inlineCallbacks\n-    def run_and_export(self, spider_cls, settings=None):\n+    def run_and_export(self, spider_cls, settings):\n         \"\"\" Run spider with specified settings; return exported data. \"\"\"\n-        tmpdir = tempfile.mkdtemp()\n-        res_path = os.path.join(tmpdir, 'res')\n-        res_uri = urljoin('file:', pathname2url(res_path))\n-        defaults = {\n-            'FEED_URI': res_uri,\n-            'FEED_FORMAT': 'csv',\n-            'FEED_PATH': res_path\n+\n+        FEEDS = settings.get('FEEDS') or {}\n+        settings['FEEDS'] = {\n+            urljoin('file:', pathname2url(str(file_path))): feed\n+            for file_path, feed in FEEDS.items()\n         }\n-        defaults.update(settings or {})\n+\n+        content = {}\n         try:\n             with MockServer() as s:\n-                runner = CrawlerRunner(Settings(defaults))\n+                runner = CrawlerRunner(Settings(settings))\n                 spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n-            with open(str(defaults['FEED_PATH']), 'rb') as f:\n-                content = f.read()\n+            for file_path, feed in FEEDS.items():\n+                with open(str(file_path), 'rb') as f:\n+                    content[feed['format']] = f.read()\n \n         finally:\n-            shutil.rmtree(tmpdir)\n+            for file_path in FEEDS.keys():\n+                os.remove(str(file_path))\n \n         defer.returnValue(content)\n \n@@ -453,10 +467,14 @@ class FeedExportTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n         settings = settings or {}\n-        settings.update({'FEED_FORMAT': 'csv'})\n+        settings.update({\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'csv'},\n+            },\n+        })\n         data = yield self.exported_data(items, settings)\n \n-        reader = csv.DictReader(to_unicode(data).splitlines())\n+        reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n         got_rows = list(reader)\n         if ordered:\n             self.assertEqual(reader.fieldnames, header)\n@@ -468,29 +486,57 @@ class FeedExportTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({'FEED_FORMAT': 'jl'})\n+        settings.update({\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'jl'},\n+            },\n+        })\n         data = yield self.exported_data(items, settings)\n-        parsed = [json.loads(to_unicode(line)) for line in data.splitlines()]\n+        parsed = [json.loads(to_unicode(line)) for line in data['jl'].splitlines()]\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         self.assertEqual(rows, parsed)\n \n     @defer.inlineCallbacks\n     def assertExportedXml(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({'FEED_FORMAT': 'xml'})\n+        settings.update({\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'xml'},\n+            },\n+        })\n         data = yield self.exported_data(items, settings)\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n-        import lxml.etree\n-        root = lxml.etree.fromstring(data)\n+        root = lxml.etree.fromstring(data['xml'])\n         got_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]\n         self.assertEqual(rows, got_rows)\n \n+    @defer.inlineCallbacks\n+    def assertExportedMultiple(self, items, rows, settings=None):\n+        settings = settings or {}\n+        settings.update({\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'xml'},\n+                self._random_temp_filename(): {'format': 'json'},\n+            },\n+        })\n+        data = yield self.exported_data(items, settings)\n+        rows = [{k: v for k, v in row.items() if v} for row in rows]\n+        # XML\n+        root = lxml.etree.fromstring(data['xml'])\n+        xml_rows = [{e.tag: e.text for e in it} for it in root.findall('item')]\n+        self.assertEqual(rows, xml_rows)\n+        # JSON\n+        json_rows = json.loads(to_unicode(data['json']))\n+        self.assertEqual(rows, json_rows)\n+\n     def _load_until_eof(self, data, load_func):\n-        bytes_output = BytesIO(data)\n         result = []\n+        with tempfile.TemporaryFile() as temp:\n+            temp.write(data)\n+            temp.seek(0)\n             while True:\n                 try:\n-                result.append(load_func(bytes_output))\n+                    result.append(load_func(temp))\n                 except EOFError:\n                     break\n         return result\n@@ -498,21 +544,29 @@ class FeedExportTest(unittest.TestCase):\n     @defer.inlineCallbacks\n     def assertExportedPickle(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({'FEED_FORMAT': 'pickle'})\n+        settings.update({\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'pickle'},\n+            },\n+        })\n         data = yield self.exported_data(items, settings)\n         expected = [{k: v for k, v in row.items() if v} for row in rows]\n         import pickle\n-        result = self._load_until_eof(data, load_func=pickle.load)\n+        result = self._load_until_eof(data['pickle'], load_func=pickle.load)\n         self.assertEqual(expected, result)\n \n     @defer.inlineCallbacks\n     def assertExportedMarshal(self, items, rows, settings=None):\n         settings = settings or {}\n-        settings.update({'FEED_FORMAT': 'marshal'})\n+        settings.update({\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'marshal'},\n+            },\n+        })\n         data = yield self.exported_data(items, settings)\n         expected = [{k: v for k, v in row.items() if v} for row in rows]\n         import marshal\n-        result = self._load_until_eof(data, load_func=marshal.load)\n+        result = self._load_until_eof(data['marshal'], load_func=marshal.load)\n         self.assertEqual(expected, result)\n \n     @defer.inlineCallbacks\n@@ -521,6 +575,8 @@ class FeedExportTest(unittest.TestCase):\n         yield self.assertExportedJsonLines(items, rows, settings)\n         yield self.assertExportedXml(items, rows, settings)\n         yield self.assertExportedPickle(items, rows, settings)\n+        yield self.assertExportedMarshal(items, rows, settings)\n+        yield self.assertExportedMultiple(items, rows, settings)\n \n     @defer.inlineCallbacks\n     def test_export_items(self):\n@@ -538,15 +594,14 @@ class FeedExportTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n-        formats = ('json',\n-                   'jsonlines',\n-                   'xml',\n-                   'csv',)\n-\n-        for fmt in formats:\n-            settings = {'FEED_FORMAT': fmt}\n+        for fmt in ('json', 'jsonlines', 'xml', 'csv'):\n+            settings = {\n+                'FEEDS': {\n+                    self._random_temp_filename(): {'format': fmt},\n+                },\n+            }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(data, b'')\n+            self.assertEqual(data[fmt], b'')\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n@@ -558,9 +613,15 @@ class FeedExportTest(unittest.TestCase):\n         )\n \n         for fmt, expctd in formats:\n-            settings = {'FEED_FORMAT': fmt, 'FEED_STORE_EMPTY': True, 'FEED_EXPORT_INDENT': None}\n+            settings = {\n+                'FEEDS': {\n+                    self._random_temp_filename(): {'format': fmt},\n+                },\n+                'FEED_STORE_EMPTY': True,\n+                'FEED_EXPORT_INDENT': None,\n+            }\n             data = yield self.exported_no_data(settings)\n-            self.assertEqual(data, expctd)\n+            self.assertEqual(data[fmt], expctd)\n \n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n@@ -663,10 +724,15 @@ class FeedExportTest(unittest.TestCase):\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n         }\n \n-        for format, expected in formats.items():\n-            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_INDENT': None}\n+        for fmt, expected in formats.items():\n+            settings = {\n+                'FEEDS': {\n+                    self._random_temp_filename(): {'format': fmt},\n+                },\n+                'FEED_EXPORT_INDENT': None,\n+            }\n             data = yield self.exported_data(items, settings)\n-            self.assertEqual(expected, data)\n+            self.assertEqual(expected, data[fmt])\n \n         formats = {\n             'json': u'[{\"foo\": \"Test\\xd6\"}]'.encode('latin-1'),\n@@ -675,11 +741,53 @@ class FeedExportTest(unittest.TestCase):\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n         }\n \n-        settings = {'FEED_EXPORT_INDENT': None, 'FEED_EXPORT_ENCODING': 'latin-1'}\n-        for format, expected in formats.items():\n-            settings['FEED_FORMAT'] = format\n+        for fmt, expected in formats.items():\n+            settings = {\n+                'FEEDS': {\n+                    self._random_temp_filename(): {'format': fmt},\n+                },\n+                'FEED_EXPORT_INDENT': None,\n+                'FEED_EXPORT_ENCODING': 'latin-1',\n+            }\n             data = yield self.exported_data(items, settings)\n-            self.assertEqual(expected, data)\n+            self.assertEqual(expected, data[fmt])\n+\n+    @defer.inlineCallbacks\n+    def test_export_multiple_configs(self):\n+        items = [dict({'foo': u'FOO', 'bar': u'BAR'})]\n+\n+        formats = {\n+            'json': u'[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode('latin-1'),\n+            'csv': u'bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {\n+                    'format': 'json',\n+                    'indent': 0,\n+                    'fields': ['bar'],\n+                    'encoding': 'utf-8',\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'xml',\n+                    'indent': 2,\n+                    'fields': ['foo'],\n+                    'encoding': 'latin-1',\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'csv',\n+                    'indent': None,\n+                    'fields': ['bar', 'foo'],\n+                    'encoding': 'utf-8',\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(items, settings)\n+        for fmt, expected in formats.items():\n+            self.assertEqual(expected, data[fmt])\n \n     @defer.inlineCallbacks\n     def test_export_indentation(self):\n@@ -827,33 +935,38 @@ class FeedExportTest(unittest.TestCase):\n         ]\n \n         for row in test_cases:\n-            settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT': row['indent']}\n+            settings = {\n+                'FEEDS': {\n+                    self._random_temp_filename(): {\n+                        'format': row['format'],\n+                        'indent': row['indent'],\n+                    },\n+                },\n+            }\n             data = yield self.exported_data(items, settings)\n-            print(row['format'], row['indent'])\n-            self.assertEqual(row['expected'], data)\n+            self.assertEqual(row['expected'], data[row['format']])\n \n     @defer.inlineCallbacks\n     def test_init_exporters_storages_with_crawler(self):\n         settings = {\n-            'FEED_EXPORTERS': {'csv': 'tests.test_feedexport.'\n-                                      'FromCrawlerCsvItemExporter'},\n-            'FEED_STORAGES': {'file': 'tests.test_feedexport.'\n-                                      'FromCrawlerFileFeedStorage'},\n+            'FEED_EXPORTERS': {'csv': 'tests.test_feedexport.FromCrawlerCsvItemExporter'},\n+            'FEED_STORAGES': {'file': 'tests.test_feedexport.FromCrawlerFileFeedStorage'},\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'csv'},\n+            },\n         }\n-        yield self.exported_data({}, settings)\n+        yield self.exported_data(items=[], settings=settings)\n         self.assertTrue(FromCrawlerCsvItemExporter.init_with_crawler)\n         self.assertTrue(FromCrawlerFileFeedStorage.init_with_crawler)\n \n     @defer.inlineCallbacks\n     def test_pathlib_uri(self):\n-        tmpdir = tempfile.mkdtemp()\n-        feed_uri = Path(tmpdir) / 'res'\n+        feed_path = Path(self._random_temp_filename())\n         settings = {\n-            'FEED_FORMAT': 'csv',\n             'FEED_STORE_EMPTY': True,\n-            'FEED_URI': feed_uri,\n-            'FEED_PATH': feed_uri\n+            'FEEDS': {\n+                feed_path: {'format': 'csv'}\n+            },\n         }\n         data = yield self.exported_no_data(settings)\n-        self.assertEqual(data, b'')\n-        shutil.rmtree(tmpdir, ignore_errors=True)\n+        self.assertEqual(data['csv'], b'')\n\n@@ -1,7 +1,9 @@\n import unittest\n+import warnings\n \n-from scrapy.settings import BaseSettings\n-from scrapy.utils.conf import build_component_list, arglist_to_dict\n+from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n+from scrapy.settings import BaseSettings, Settings\n+from scrapy.utils.conf import build_component_list, arglist_to_dict, feed_process_params_from_cli\n \n \n class BuildComponentListTest(unittest.TestCase):\n@@ -90,5 +92,49 @@ class UtilsConfTestCase(unittest.TestCase):\n             {'arg1': 'val1', 'arg2': 'val2'})\n \n \n+class FeedExportConfigTestCase(unittest.TestCase):\n+\n+    def test_feed_export_config_invalid_format(self):\n+        settings = Settings()\n+        self.assertRaises(UsageError, feed_process_params_from_cli, settings, ['items.dat'], 'noformat')\n+\n+    def test_feed_export_config_mismatch(self):\n+        settings = Settings()\n+        self.assertRaises(\n+            UsageError,\n+            feed_process_params_from_cli, settings, ['items1.dat', 'items2.dat'], 'noformat'\n+        )\n+\n+    def test_feed_export_config_backward_compatible(self):\n+        with warnings.catch_warnings(record=True) as cw:\n+            settings = Settings()\n+            self.assertEqual(\n+                {'items.dat': {'format': 'csv'}},\n+                feed_process_params_from_cli(settings, ['items.dat'], 'csv')\n+            )\n+            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n+\n+    def test_feed_export_config_explicit_formats(self):\n+        settings = Settings()\n+        self.assertEqual(\n+            {'items_1.dat': {'format': 'json'}, 'items_2.dat': {'format': 'xml'}, 'items_3.dat': {'format': 'csv'}},\n+            feed_process_params_from_cli(settings, ['items_1.dat:json', 'items_2.dat:xml', 'items_3.dat:csv'])\n+        )\n+\n+    def test_feed_export_config_implicit_formats(self):\n+        settings = Settings()\n+        self.assertEqual(\n+            {'items_1.json': {'format': 'json'}, 'items_2.xml': {'format': 'xml'}, 'items_3.csv': {'format': 'csv'}},\n+            feed_process_params_from_cli(settings, ['items_1.json', 'items_2.xml', 'items_3.csv'])\n+        )\n+\n+    def test_feed_export_config_stdout(self):\n+        settings = Settings()\n+        self.assertEqual(\n+            {'stdout:': {'format': 'pickle'}},\n+            feed_process_params_from_cli(settings, ['-:pickle'])\n+        )\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
