{"custom_id": "keras#12e0052f43eface40882f2d4086daf8a2ba9a599", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 37 | Files Changed: 1 | Hunks: 15 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 76 | Churn Cumulative: 6279 | Contributors (this commit): 32 | Commits (past 90d): 96 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -386,22 +386,24 @@ class Merge(Layer):\n     '''\n     def __init__(self, layers, mode='sum', concat_axis=-1, dot_axes=-1):\n         if len(layers) < 2:\n-            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n+            raise Exception('Please specify two or more input layers '\n+                            '(or containers) to merge')\n \n         if mode not in {'sum', 'mul', 'concat', 'ave', 'join', 'cos', 'dot'}:\n-            raise Exception(\"Invalid merge mode: \" + str(mode))\n+            raise Exception('Invalid merge mode: ' + str(mode))\n \n         if mode in {'sum', 'mul', 'ave', 'cos'}:\n             input_shapes = set([l.output_shape for l in layers])\n             if len(input_shapes) > 1:\n-                raise Exception(\"Only layers of same output shape can be merged using \" + mode + \" mode. \" +\n-                                \"Layer shapes: %s\" % ([l.output_shape for l in layers]))\n+                raise Exception('Only layers of same output shape can '\n+                                'be merged using ' + mode + ' mode. ' +\n+                                'Layer shapes: %s' % ([l.output_shape for l in layers]))\n         if mode in {'cos', 'dot'}:\n             if K._BACKEND != 'theano':\n                 raise Exception('\"' + mode + '\" merge mode will only work with Theano.')\n \n             if len(layers) > 2:\n-                raise Exception(mode + \" merge takes exactly 2 layers\")\n+                raise Exception(mode + ' merge takes exactly 2 layers')\n             shape1 = layers[0].output_shape\n             shape2 = layers[1].output_shape\n             n1 = len(shape1)\n@@ -413,16 +415,16 @@ class Merge(Layer):\n                     else:\n                         dot_axes = [range(n1 - dot_axes, n2), range(1, dot_axes + 1)]\n                 if type(dot_axes) not in [list, tuple]:\n-                    raise Exception(\"Invalid type for dot_axes - should be a list.\")\n+                    raise Exception('Invalid type for dot_axes - should be a list.')\n                 if len(dot_axes) != 2:\n-                    raise Exception(\"Invalid format for dot_axes - should contain two elements.\")\n+                    raise Exception('Invalid format for dot_axes - should contain two elements.')\n                 if type(dot_axes[0]) not in [list, tuple, range] or type(dot_axes[1]) not in [list, tuple, range]:\n-                    raise Exception(\"Invalid format for dot_axes - list elements should have type 'list' or 'tuple'.\")\n+                    raise Exception('Invalid format for dot_axes - list elements should have type \"list\" or \"tuple\".')\n                 for i in range(len(dot_axes[0])):\n                     if shape1[dot_axes[0][i]] != shape2[dot_axes[1][i]]:\n-                        raise Exception(\"Dimension incompatibility using dot mode: \" +\n-                                        \"%s != %s. \" % (shape1[dot_axes[0][i]], shape2[dot_axes[1][i]]) +\n-                                        \"Layer shapes: %s, %s\" % (shape1, shape2))\n+                        raise Exception('Dimension incompatibility using dot mode: ' +\n+                                        '%s != %s. ' % (shape1[dot_axes[0][i]], shape2[dot_axes[1][i]]) +\n+                                        'Layer shapes: %s, %s' % (shape1, shape2))\n         elif mode == 'concat':\n             input_shapes = set()\n             for l in layers:\n@@ -431,9 +433,9 @@ class Merge(Layer):\n                 oshape = tuple(oshape)\n                 input_shapes.add(oshape)\n             if len(input_shapes) > 1:\n-                raise Exception(\"'concat' mode can only merge layers with matching \" +\n-                                \"output shapes except for the concat axis. \" +\n-                                \"Layer shapes: %s\" % ([l.output_shape for l in layers]))\n+                raise Exception('\"concat\" mode can only merge layers with matching ' +\n+                                'output shapes except for the concat axis. ' +\n+                                'Layer shapes: %s' % ([l.output_shape for l in layers]))\n         self.mode = mode\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n@@ -567,11 +569,11 @@ class Merge(Layer):\n             weights = weights[nb_param:]\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"layers\": [l.get_config() for l in self.layers],\n-                  \"mode\": self.mode,\n-                  \"concat_axis\": self.concat_axis,\n-                  \"dot_axes\": self.dot_axes}\n+        config = {'name': self.__class__.__name__,\n+                  'layers': [l.get_config() for l in self.layers],\n+                  'mode': self.mode,\n+                  'concat_axis': self.concat_axis,\n+                  'dot_axes': self.dot_axes}\n         base_config = super(Merge, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -599,8 +601,8 @@ class Dropout(MaskedLayer):\n         return X\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"p\": self.p}\n+        config = {'name': self.__class__.__name__,\n+                  'p': self.p}\n         base_config = super(Dropout, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -630,8 +632,8 @@ class Activation(MaskedLayer):\n         return self.activation(X)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"activation\": self.activation.__name__}\n+        config = {'name': self.__class__.__name__,\n+                  'activation': self.activation.__name__}\n         base_config = super(Activation, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -665,8 +667,8 @@ class Reshape(Layer):\n         return K.reshape(X, (-1,) + self.dims)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"dims\": self.dims}\n+        config = {'name': self.__class__.__name__,\n+                  'dims': self.dims}\n         base_config = super(Reshape, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -709,8 +711,8 @@ class Permute(Layer):\n         return K.permute_dimensions(X, (0,) + self.dims)\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"dims\": self.dims}\n+        config = {'name': self.__class__.__name__,\n+                  'dims': self.dims}\n         base_config = super(Permute, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -869,16 +871,16 @@ class Dense(Layer):\n         return output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"output_dim\": self.output_dim,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n-                  \"input_dim\": self.input_dim}\n+        config = {'name': self.__class__.__name__,\n+                  'output_dim': self.output_dim,\n+                  'init': self.init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'input_dim': self.input_dim}\n         base_config = super(Dense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#656681e9d8a2a10a7d9666270a3b2e5157eabbc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 16 | Churn Cumulative: 6295 | Contributors (this commit): 32 | Commits (past 90d): 97 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -155,8 +155,7 @@ class Layer(object):\n         return False\n \n     def get_output_mask(self, train=None):\n-        '''\n-        For some models (such as RNNs) you want a way of being able to mark\n+        '''For some models (such as RNNs) you want a way of being able to mark\n         some output data-points as \"masked\",\n         so they are not used in future calculations.\n         In such a model, get_output_mask() should return a mask\n@@ -182,7 +181,6 @@ class Layer(object):\n     def set_weights(self, weights):\n         '''Set the weights of the layer.\n \n-        # Arguments\n         weights: a list of numpy arrays. The number\n             of arrays and their shape must match\n             number of the dimensions of the weights\n@@ -197,9 +195,8 @@ class Layer(object):\n             K.set_value(p, w)\n \n     def get_weights(self):\n-        '''Return the weights of the layer.\n-\n-        # Returns: list of numpy arrays.\n+        '''Return the weights of the layer,\n+        as a list of numpy arrays.\n         '''\n         weights = []\n         for p in self.params:\n@@ -207,10 +204,7 @@ class Layer(object):\n         return weights\n \n     def get_config(self):\n-        '''Return the parameters of the layer.\n-\n-        # Returns:\n-            A dictionary mapping parameter names to their values.\n+        '''Return the parameters of the layer, as a dictionary.\n         '''\n         config = {\"name\": self.__class__.__name__}\n         if hasattr(self, '_input_shape'):\n@@ -512,7 +506,7 @@ class Merge(Layer):\n             for i in range(len(self.layers)):\n                 X = self.layers[i].get_output(train)\n                 if X.name is None:\n-                    raise ValueError('merge_mode='join' only works with named inputs')\n+                    raise ValueError('merge_mode=\"join\" only works with named inputs')\n                 else:\n                     inputs[X.name] = X\n             return inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#671751fce40a7ffa28dd2c78f54640ab46884bf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 190 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n-import unittest\n+import pytest\n from numpy.testing import assert_allclose\n import numpy as np\n from keras import backend as K\n-import pytest\n+\n \n def get_standard_values():\n     '''\n@@ -15,8 +15,7 @@ def get_standard_values():\n     return np.array([[0, 0.1, 0.5, 0.9, 1.0, 10, 1e2, 0.01]], dtype=K.floatx())\n \n \n-class TestAdvancedActivations(unittest.TestCase):\n-    def test_leaky_relu(self):\n+def test_leaky_relu():\n     np.random.seed(1337)\n     from keras.layers.advanced_activations import LeakyReLU\n     inp = get_standard_values()\n@@ -35,7 +34,8 @@ class TestAdvancedActivations(unittest.TestCase):\n         config = layer.get_config()\n         assert config['alpha'] == alpha\n \n-    def test_prelu(self):\n+\n+def test_prelu():\n     from keras.layers.advanced_activations import PReLU\n     np.random.seed(1337)\n     inp = get_standard_values()\n@@ -69,7 +69,8 @@ class TestAdvancedActivations(unittest.TestCase):\n \n         layer.get_config()\n \n-    def test_elu(self):\n+\n+def test_elu():\n     from keras.layers.advanced_activations import ELU\n     np.random.seed(1337)\n     inp = get_standard_values()\n@@ -88,8 +89,9 @@ class TestAdvancedActivations(unittest.TestCase):\n         config = layer.get_config()\n         assert config['alpha'] == alpha\n \n+\n @pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n-    def test_parametric_softplus(self):\n+def test_parametric_softplus():\n     from keras.layers.advanced_activations import ParametricSoftplus\n     np.random.seed(1337)\n     inp = np.vstack((get_standard_values(), -get_standard_values()))\n@@ -111,8 +113,9 @@ class TestAdvancedActivations(unittest.TestCase):\n             assert config['alpha_init'] == alpha\n             assert config['beta_init'] == beta\n \n+\n @pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n-    def test_thresholded_linear(self):\n+def test_thresholded_linear():\n     from keras.layers.advanced_activations import ThresholdedLinear\n     np.random.seed(1337)\n     inp = get_standard_values()\n@@ -131,8 +134,9 @@ class TestAdvancedActivations(unittest.TestCase):\n         config = layer.get_config()\n         assert config['theta'] == theta\n \n+\n @pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n-    def test_thresholded_relu(self):\n+def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n     np.random.seed(1337)\n     inp = get_standard_values()\n@@ -153,4 +157,4 @@ class TestAdvancedActivations(unittest.TestCase):\n \n \n if __name__ == '__main__':\n-    unittest.main()\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#444976552e1e152e9dd02bbdbc5f9b8acb687d84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 671 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -293,7 +293,7 @@ def test_merge_overlap():\n     model.train_on_batch(X_train[:32], y_train[:32])\n \n     loss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.9)\n     model.predict(X_test, verbose=0)\n     model.predict_classes(X_test, verbose=0)\n     model.predict_proba(X_test, verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d3493fe6c84213d400305d6b8cf119753a7f466a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 432 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -174,3 +174,4 @@ def _runner(layer):\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#039d15bb55b03984c6515f1ada736bd02edd1347", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 240 | Lines Deleted: 129 | Files Changed: 4 | Hunks: 4 | Methods Changed: 13 | Complexity Δ (Sum/Max): 7/10 | Churn Δ: 369 | Churn Cumulative: 694 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 0.24390243902439024\n\nDIFF:\n@@ -0,0 +1,46 @@\n+from __future__ import print_function\n+import numpy as np\n+import pytest\n+\n+from keras.utils.test_utils import get_test_data\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Flatten, Activation\n+from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.utils.np_utils import to_categorical\n+\n+\n+def test_image_classification():\n+    '''\n+    Classify random 16x16 color images into several classes using logistic regression\n+    with convolutional hidden layer.\n+    '''\n+    np.random.seed(1337)\n+    input_shape = (3, 16, 16)\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n+                                                         nb_test=200,\n+                                                         input_shape=input_shape,\n+                                                         classification=True,\n+                                                         nb_class=4)\n+    y_train = to_categorical(y_train)\n+    y_test = to_categorical(y_test)\n+    # convolution kernel size\n+    nb_conv = 3\n+    # size of pooling area for max pooling\n+    nb_pool = 2\n+\n+    model = Sequential([\n+        Convolution2D(nb_filter=8, nb_row=nb_conv, nb_col=nb_conv, input_shape=input_shape),\n+        MaxPooling2D(pool_size=(nb_pool, nb_pool)),\n+        Flatten(),\n+        Activation('relu'),\n+        Dense(y_test.shape[-1], activation='softmax')\n+    ])\n+    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+    history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16,\n+                        validation_data=(X_test, y_test),\n+                        show_accuracy=True, verbose=0)\n+    assert(history.history['val_acc'][-1] > 0.9)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -0,0 +1,131 @@\n+from __future__ import print_function\n+import numpy as np\n+import pytest\n+import string\n+\n+from keras.utils.test_utils import get_test_data\n+from keras.models import Sequential\n+from keras.layers.core import TimeDistributedDense, Dropout, Dense\n+from keras.layers.recurrent import GRU, LSTM\n+from keras.utils.np_utils import to_categorical\n+\n+\n+def test_temporal_classification():\n+    '''\n+    Classify temporal sequences of float numbers of length 3 into 2 classes using\n+    single layer of GRU units and softmax applied to the last activations of the units\n+    '''\n+    np.random.seed(1337)\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n+                                                         nb_test=200,\n+                                                         input_shape=(3, 5),\n+                                                         classification=True,\n+                                                         nb_class=2)\n+    y_train = to_categorical(y_train)\n+    y_test = to_categorical(y_test)\n+\n+    model = Sequential()\n+    model.add(GRU(y_train.shape[-1],\n+                  input_shape=(X_train.shape[1], X_train.shape[2]),\n+                  activation='softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n+    history = model.fit(X_train, y_train, nb_epoch=5, batch_size=16,\n+                        validation_data=(X_test, y_test),\n+                        show_accuracy=True, verbose=0)\n+    assert(history.history['val_acc'][-1] > 0.9)\n+\n+\n+def test_temporal_regression():\n+    '''\n+    Predict float numbers (regression) based on sequences of float numbers of length 3 using\n+    single layer of GRU units\n+    '''\n+    np.random.seed(1337)\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n+                                                         nb_test=200,\n+                                                         input_shape=(3, 5),\n+                                                         output_shape=(2,),\n+                                                         classification=False)\n+    model = Sequential()\n+    model.add(GRU(y_train.shape[-1],\n+              input_shape=(X_train.shape[1], X_train.shape[2])))\n+    model.compile(loss='hinge', optimizer='adam')\n+    history = model.fit(X_train, y_train, nb_epoch=5, batch_size=16,\n+                        validation_data=(X_test, y_test), verbose=0)\n+    assert(history.history['val_loss'][-1] < 0.75)\n+\n+\n+def test_sequence_to_sequence():\n+    '''\n+    Apply a same Dense layer for each element of time dimension of the input\n+    and make predictions of the output sequence elements.\n+    This does not make use of the temporal structure of the sequence\n+    (see TimeDistributedDense for more details)\n+    '''\n+    np.random.seed(1337)\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n+                                                         nb_test=200,\n+                                                         input_shape=(3, 5),\n+                                                         output_shape=(3, 5),\n+                                                         classification=False)\n+\n+    model = Sequential()\n+    model.add(TimeDistributedDense(y_train.shape[-1],\n+              input_shape=(X_train.shape[1], X_train.shape[2])))\n+    model.compile(loss='hinge', optimizer='rmsprop')\n+    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n+                        validation_data=(X_test, y_test), verbose=0)\n+    assert(history.history['val_loss'][-1] < 0.8)\n+\n+\n+def test_stacked_lstm_char_prediction():\n+    '''\n+    Learn alphabetical char sequence with stacked LSTM.\n+    Predict the whole alphabet based on the first two letters ('ab' -> 'ab...z')\n+    See non-toy example in examples/lstm_text_generation.py\n+    '''\n+    np.random.seed(1336)\n+    # generate alphabet: http://stackoverflow.com/questions/16060899/alphabet-range-python\n+    alphabet = string.ascii_lowercase\n+    number_of_chars = len(alphabet)\n+\n+    # generate char sequences of length 'sequence_length' out of alphabet and store the next char as label (e.g. 'ab'->'c')\n+    sequence_length = 2\n+    sentences = [alphabet[i: i + sequence_length] for i in range(len(alphabet) - sequence_length)]\n+    next_chars = [alphabet[i + sequence_length] for i in range(len(alphabet) - sequence_length)]\n+\n+    # Transform sequences and labels into 'one-hot' encoding\n+    X = np.zeros((len(sentences), sequence_length, number_of_chars), dtype=np.bool)\n+    y = np.zeros((len(sentences), number_of_chars), dtype=np.bool)\n+    for i, sentence in enumerate(sentences):\n+        for t, char in enumerate(sentence):\n+            X[i, t, ord(char)-ord('a')] = 1\n+        y[i, ord(next_chars[i])-ord('a')] = 1\n+\n+    # learn the alphabet with stacked LSTM\n+    model = Sequential([\n+        LSTM(16, return_sequences=True, input_shape=(sequence_length, number_of_chars)),\n+        LSTM(16, return_sequences=False),\n+        Dense(number_of_chars, activation='softmax')\n+    ])\n+    model.compile(loss='categorical_crossentropy', optimizer='adam')\n+    model.fit(X, y, batch_size=1, nb_epoch=60, verbose=1)\n+\n+    # prime the model with 'ab' sequence and let it generate the learned alphabet\n+    sentence = alphabet[:sequence_length]\n+    generated = sentence\n+    for iteration in range(number_of_chars-sequence_length):\n+        x = np.zeros((1, sequence_length, number_of_chars))\n+        for t, char in enumerate(sentence):\n+            x[0, t, ord(char) - ord('a')] = 1.\n+        preds = model.predict(x, verbose=0)[0]\n+        next_char = chr(np.argmax(preds) + ord('a'))\n+        generated += next_char\n+        sentence = sentence[1:] + next_char\n+\n+    # check that it did generate the alphabet correctly\n+    assert(generated == alphabet)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -0,0 +1,63 @@\n+from __future__ import print_function\n+import numpy as np\n+import pytest\n+\n+from keras.utils.test_utils import get_test_data\n+from keras.models import Sequential\n+from keras.layers.core import Dense\n+from keras.utils.np_utils import to_categorical\n+\n+\n+def test_vector_classification():\n+    '''\n+    Classify random float vectors into 2 classes with logistic regression\n+    using 2 layer neural network with ReLU hidden units.\n+    '''\n+    np.random.seed(1337)\n+    nb_hidden = 10\n+\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n+                                                         nb_test=200,\n+                                                         input_shape=(20,),\n+                                                         classification=True,\n+                                                         nb_class=2)\n+    y_train = to_categorical(y_train)\n+    y_test = to_categorical(y_test)\n+\n+    model = Sequential([\n+        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='relu'),\n+        Dense(y_train.shape[-1], activation='softmax')\n+    ])\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16,\n+                        validation_data=(X_test, y_test),\n+                        show_accuracy=True, verbose=0)\n+    assert(history.history['val_acc'][-1] > 0.8)\n+\n+\n+def test_vector_regression():\n+    '''\n+    Perform float data prediction (regression) using 2 layer MLP\n+    with tanh and sigmoid activations.\n+    '''\n+    np.random.seed(1337)\n+    nb_hidden = 10\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n+                                                         nb_test=200,\n+                                                         input_shape=(20,),\n+                                                         output_shape=(2,),\n+                                                         classification=False)\n+\n+    model = Sequential([\n+        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='tanh'),\n+        Dense(y_train.shape[-1])\n+    ])\n+\n+    model.compile(loss='hinge', optimizer='adagrad')\n+    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n+                        validation_data=(X_test, y_test), verbose=0)\n+    assert (history.history['val_loss'][-1] < 0.9)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n@@ -1,129 +0,0 @@\n-from __future__ import print_function\n-import numpy as np\n-import pytest\n-np.random.seed(1337)\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.models import Sequential\n-from keras.layers.core import Dense, TimeDistributedDense, Flatten\n-from keras.layers.recurrent import GRU\n-from keras.layers.convolutional import Convolution2D\n-from keras.utils.np_utils import to_categorical\n-\n-\n-def test_vector_classification():\n-    nb_hidden = 10\n-\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n-                                                         input_shape=(20,),\n-                                                         classification=True,\n-                                                         nb_class=2)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    model = Sequential([\n-        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='relu'),\n-        Dense(y_train.shape[-1], activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    history = model.fit(X_train, y_train, nb_epoch=15, batch_size=16,\n-                        validation_data=(X_test, y_test),\n-                        show_accuracy=True, verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.8)\n-\n-\n-def test_vector_regression():\n-    nb_hidden = 10\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n-                                                         input_shape=(20,),\n-                                                         output_shape=(2,),\n-                                                         classification=False)\n-\n-    model = Sequential([\n-        Dense(nb_hidden, input_shape=(X_train.shape[-1],), activation='tanh'),\n-        Dense(y_train.shape[-1])\n-    ])\n-\n-    model.compile(loss='hinge', optimizer='adagrad')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=0)\n-    assert (history.history['val_loss'][-1] < 0.9)\n-\n-\n-def test_temporal_classification():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n-                                                         input_shape=(3, 5),\n-                                                         classification=True,\n-                                                         nb_class=2)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    model = Sequential()\n-    model.add(GRU(y_train.shape[-1],\n-                  input_shape=(X_train.shape[1], X_train.shape[2]),\n-                  activation='softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test),\n-                        show_accuracy=True, verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.9)\n-\n-\n-def test_temporal_regression():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n-                                                         input_shape=(3, 5),\n-                                                         output_shape=(2,),\n-                                                         classification=False)\n-    model = Sequential()\n-    model.add(GRU(y_train.shape[-1],\n-              input_shape=(X_train.shape[1], X_train.shape[2])))\n-    model.compile(loss='hinge', optimizer='adam')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=0)\n-    assert(history.history['val_loss'][-1] < 0.8)\n-\n-\n-def test_sequence_to_sequence():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n-                                                         input_shape=(3, 5),\n-                                                         output_shape=(3, 5),\n-                                                         classification=False)\n-\n-    model = Sequential()\n-    model.add(TimeDistributedDense(y_train.shape[-1],\n-              input_shape=(X_train.shape[1], X_train.shape[2])))\n-    model.compile(loss='hinge', optimizer='rmsprop')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test), verbose=0)\n-    assert(history.history['val_loss'][-1] < 0.8)\n-\n-\n-def test_image_classification():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=500,\n-                                                         nb_test=200,\n-                                                         input_shape=(3, 8, 8),\n-                                                         classification=True,\n-                                                         nb_class=2)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    model = Sequential([\n-        Convolution2D(8, 8, 8, input_shape=(3, 8, 8), activation='sigmoid'),\n-        Flatten(),\n-        Dense(y_test.shape[-1], activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-    history = model.fit(X_train, y_train, nb_epoch=20, batch_size=16,\n-                        validation_data=(X_test, y_test),\n-                        show_accuracy=True, verbose=0)\n-    assert(history.history['val_acc'][-1] > 0.9)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a93fc51cf50bb557c30678ba1c75826860a1425", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 470 | Lines Deleted: 131 | Files Changed: 4 | Hunks: 88 | Methods Changed: 35 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 601 | Churn Cumulative: 11707 | Contributors (this commit): 52 | Commits (past 90d): 127 | Contributors (cumulative): 81 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -69,7 +69,30 @@ class CallbackList(object):\n \n \n class Callback(object):\n+    '''Abstract base class used to build new callbacks.\n \n+    # Properties\n+        params: dict. Training parameters\n+            (eg. verbosity, batch size, number of epochs...).\n+        model: instance of `keras.models.Model`.\n+            Reference of the model being trained.\n+\n+    The `logs` dictionary that callback methods\n+    take as argument will contain keys for quantities relevant to\n+    the current batch or epoch.\n+\n+    Currently, the `.fit()` method of the `Sequential` model class\n+    will include the following quantities in the `logs` that\n+    it passes to its callbacks:\n+\n+        on_epoch_end: logs optionally include `val_loss`\n+            (if validation is enabled in `fit`), and `val_acc`\n+            (if validation and accuracy monitoring are enabled).\n+        on_batch_begin: logs include `size`,\n+            the number of samples in the current batch.\n+        on_batch_end: logs include `loss`, and optionally `acc`\n+            (if accuracy monitoring is enabled).\n+    '''\n     def __init__(self):\n         pass\n \n@@ -99,6 +122,12 @@ class Callback(object):\n \n \n class BaseLogger(Callback):\n+    '''Callback that prints events to the standard output.\n+\n+    This callback is automatically applied to\n+    every Keras model (it is the basis of the verbosity modes\n+    in models).\n+    '''\n     def on_train_begin(self, logs={}):\n         self.verbose = self.params['verbose']\n         self.nb_epoch = self.params['nb_epoch']\n@@ -128,7 +157,8 @@ class BaseLogger(Callback):\n             if k in logs:\n                 self.log_values.append((k, logs[k]))\n \n-        # skip progbar update for the last batch; will be handled by on_epoch_end\n+        # skip progbar update for the last batch;\n+        # will be handled by on_epoch_end\n         if self.verbose and self.seen < self.params['nb_sample']:\n             self.progbar.update(self.seen, self.log_values)\n \n@@ -143,7 +173,13 @@ class BaseLogger(Callback):\n \n \n class History(Callback):\n+    '''Callback that records events\n+    into a `History` object.\n \n+    This callback is automatically applied to\n+    every Keras model. The `History` object\n+    gets returned by the `fit` method of models.\n+    '''\n     def on_train_begin(self, logs={}):\n         self.epoch = []\n         self.history = {}\n@@ -175,7 +211,35 @@ class History(Callback):\n \n \n class ModelCheckpoint(Callback):\n-    def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False, mode='auto'):\n+    '''Save the model after every epoch.\n+\n+    `filepath` can contain named formatting options,\n+    which will be filled the value of `epoch` and\n+    keys in `logs` (passed in `on_epoch_end`).\n+\n+    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n+    then multiple files will be save with the epoch number and\n+    the validation loss.\n+\n+    # Arguments\n+        filepath: string, path to save the model file.\n+        monitor: quantity to monitor.\n+        verbose: verbosity mode, 0 or 1.\n+        save_best_only: if `save_best_only=True`,\n+            the latest best model according to\n+            the validation loss will not be overwritten.\n+        mode: one of {auto, min, max}.\n+            If `save_best_only=True`, the decision\n+            to overwrite the current save file is made\n+            based on either the maximization or the\n+            minization of the monitored. For `val_acc`,\n+            this should be `max`, for `val_loss` this should\n+            be `min`, etc. In `auto` mode, the direction is\n+            automatically inferred from the name of the monitored quantity.\n+\n+    '''\n+    def __init__(self, filepath, monitor='val_loss', verbose=0,\n+                 save_best_only=False, mode='auto'):\n \n         super(Callback, self).__init__()\n         self.monitor = monitor\n@@ -184,17 +248,18 @@ class ModelCheckpoint(Callback):\n         self.save_best_only = save_best_only\n \n         if mode not in ['auto', 'min', 'max']:\n-            warnings.warn(\"ModelCheckpoint mode %s is unknown, fallback to auto mode\" % (self.mode), RuntimeWarning)\n+            warnings.warn('ModelCheckpoint mode %s is unknown, '\n+                          'fallback to auto mode' % (self.mode), RuntimeWarning)\n             mode = 'auto'\n \n-        if mode == \"min\":\n+        if mode == 'min':\n             self.monitor_op = np.less\n             self.best = np.Inf\n-        elif mode == \"max\":\n+        elif mode == 'max':\n             self.monitor_op = np.greater\n             self.best = -np.Inf\n         else:\n-            if \"acc\" in self.monitor:\n+            if 'acc' in self.monitor:\n                 self.monitor_op = np.greater\n                 self.best = -np.Inf\n             else:\n@@ -206,24 +271,36 @@ class ModelCheckpoint(Callback):\n         if self.save_best_only:\n             current = logs.get(self.monitor)\n             if current is None:\n-                warnings.warn(\"Can save best model only with %s available, skipping.\" % (self.monitor), RuntimeWarning)\n+                warnings.warn('Can save best model only with %s available, '\n+                              'skipping.' % (self.monitor), RuntimeWarning)\n             else:\n                 if self.monitor_op(current, self.best):\n                     if self.verbose > 0:\n-                        print(\"Epoch %05d: %s improved from %0.5f to %0.5f, saving model to %s\"\n-                              % (epoch, self.monitor, self.best, current, filepath))\n+                        print('Epoch %05d: %s improved from %0.5f to %0.5f, ' +\n+                              'saving model to %s'\n+                              % (epoch, self.monitor, self.best,\n+                                 current, filepath))\n                     self.best = current\n                     self.model.save_weights(filepath, overwrite=True)\n                 else:\n                     if self.verbose > 0:\n-                        print(\"Epoch %05d: %s did not improve\" % (epoch, self.monitor))\n+                        print('Epoch %05d: %s did not improve' %\n+                              (epoch, self.monitor))\n         else:\n             if self.verbose > 0:\n-                print(\"Epoch %05d: saving model to %s\" % (epoch, filepath))\n+                print('Epoch %05d: saving model to %s' % (epoch, filepath))\n             self.model.save_weights(filepath, overwrite=True)\n \n \n class EarlyStopping(Callback):\n+    '''Stop training when a monitored quantity has stopped improving.\n+\n+    # Arguments\n+        monitor: quantity to be monitored.\n+        patience: number of epochs with no improvement\n+            after which training will be stopped.\n+        verbose: verbosity mode.\n+    '''\n     def __init__(self, monitor='val_loss', patience=0, verbose=0):\n         super(Callback, self).__init__()\n \n@@ -236,7 +313,8 @@ class EarlyStopping(Callback):\n     def on_epoch_end(self, epoch, logs={}):\n         current = logs.get(self.monitor)\n         if current is None:\n-            warnings.warn(\"Early stopping requires %s available!\" % (self.monitor), RuntimeWarning)\n+            warnings.warn('Early stopping requires %s available!' %\n+                          (self.monitor), RuntimeWarning)\n \n         if current < self.best:\n             self.best = current\n@@ -244,12 +322,16 @@ class EarlyStopping(Callback):\n         else:\n             if self.wait >= self.patience:\n                 if self.verbose > 0:\n-                    print(\"Epoch %05d: early stopping\" % (epoch))\n+                    print('Epoch %05d: early stopping' % (epoch))\n                 self.model.stop_training = True\n             self.wait += 1\n \n \n class RemoteMonitor(Callback):\n+    '''Experimental callback used to stream events to a server.\n+\n+    Requires the `requests` library.\n+    '''\n     def __init__(self, root='http://localhost:9000'):\n         self.root = root\n \n@@ -277,14 +359,19 @@ class RemoteMonitor(Callback):\n             send[k] = v\n \n         try:\n-            r = requests.post(self.root + '/publish/epoch/end/', {'data': json.dumps(send)})\n+            requests.post(self.root + '/publish/epoch/end/',\n+                          {'data': json.dumps(send)})\n         except:\n-            print('Warning: could not reach RemoteMonitor root server at ' + str(self.root))\n+            print('Warning: could not reach RemoteMonitor '\n+                  'root server at ' + str(self.root))\n \n \n class LearningRateScheduler(Callback):\n-    '''LearningRateScheduler\n-    schedule is a function that gets an epoch number as input and returns a new\n+    '''Learning rate scheduler.\n+\n+    # Arguments\n+        schedule: a function that gets an epoch index as input\n+            (integer, indexed from 0) and returns a new\n             learning rate as output.\n     '''\n     def __init__(self, schedule):\n\n@@ -21,6 +21,7 @@ class Layer(object):\n     '''Abstract base layer class.\n \n     All Keras layers accept certain keyword arguments:\n+\n         trainable: boolean. Set to \"False\" before model compilation\n             to freeze layer weights (they won't be updated further\n             during training).\n\n@@ -2,8 +2,6 @@ from __future__ import absolute_import\n from __future__ import print_function\n import numpy as np\n import warnings\n-import time\n-import copy\n import pprint\n from six.moves import range\n import six\n@@ -11,11 +9,10 @@ import six\n from . import backend as K\n from . import optimizers\n from . import objectives\n-from . import regularizers\n-from . import constraints\n from . import callbacks as cbks\n-from .utils.layer_utils import container_from_config, model_summary\n-from .utils.generic_utils import Progbar, printv\n+from .utils.layer_utils import container_from_config\n+from .utils.layer_utils import model_summary\n+from .utils.generic_utils import Progbar\n from .layers import containers\n \n \n@@ -52,6 +49,8 @@ def standardize_X(X):\n \n \n def slice_X(X, start=None, stop=None):\n+    '''\n+    '''\n     if type(X) == list:\n         if hasattr(start, '__len__'):\n             # hdf5 dataset only support list object as indices\n@@ -71,9 +70,7 @@ def slice_X(X, start=None, stop=None):\n \n def weighted_objective(fn):\n     def weighted(y_true, y_pred, weights, mask=None):\n-        '''To be called only with non-zero weights.\n-\n-        mask: binary\n+        '''\n         '''\n         # score_array has ndim >= 2\n         score_array = fn(y_true, y_pred)\n@@ -96,12 +93,15 @@ def weighted_objective(fn):\n \n \n def standardize_weights(y, sample_weight=None, class_weight=None):\n+    '''\n+    '''\n     if sample_weight is not None:\n         assert len(sample_weight) == len(y)\n         return sample_weight.flatten()\n     elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n-            raise Exception('class_weight not supported for 3+ dimensional targets.')\n+            raise Exception('class_weight not supported for '\n+                            '3+ dimensional targets.')\n         if y.shape[1] > 1:\n             y_classes = y.argmax(axis=1)\n         elif y.shape[1] == 1:\n@@ -132,6 +132,8 @@ def model_from_json(json_string, custom_objects={}):\n \n \n def model_from_config(config, custom_objects={}):\n+    '''\n+    '''\n     model_name = config.get('name')\n     if model_name not in {'Graph', 'Sequential'}:\n         raise Exception('Unrecognized model:', model_name)\n@@ -147,7 +149,6 @@ def model_from_config(config, custom_objects={}):\n         # if it has an optimizer, the model is assumed to be compiled\n         loss = config.get('loss')\n         class_mode = config.get('class_mode')\n-        theano_mode = config.get('theano_mode')\n \n         optimizer_params = dict([(k, v) for k, v in config.get('optimizer').items()])\n         optimizer_name = optimizer_params.pop('name')\n@@ -155,10 +156,9 @@ def model_from_config(config, custom_objects={}):\n \n         if model_name == 'Sequential':\n             model.compile(loss=loss, optimizer=optimizer,\n-                          class_mode=class_mode, theano_mode=theano_mode)\n+                          class_mode=class_mode)\n         elif model_name == 'Graph':\n-            model.compile(loss=loss, optimizer=optimizer,\n-                          theano_mode=theano_mode)\n+            model.compile(loss=loss, optimizer=optimizer)\n     return model\n \n \n@@ -170,6 +170,8 @@ def get_function_name(o):\n \n \n class Model(object):\n+    '''Abstract base model class.\n+    '''\n     def _fit(self, f, ins, out_labels=[], batch_size=128,\n              nb_epoch=100, verbose=1, callbacks=[],\n              val_f=None, val_ins=None, shuffle=True, metrics=[]):\n@@ -181,7 +183,8 @@ class Model(object):\n         if val_f and val_ins:\n             do_validation = True\n             if verbose:\n-                print(\"Train on %d samples, validate on %d samples\" % (len(ins[0]), len(val_ins[0])))\n+                print('Train on %d samples, validate on %d samples' %\n+                      (len(ins[0]), len(val_ins[0])))\n \n         nb_train_sample = len(ins[0])\n         index_array = np.arange(nb_train_sample)\n@@ -218,9 +221,9 @@ class Model(object):\n                 try:\n                     ins_batch = slice_X(ins, batch_ids)\n                 except TypeError:\n-                    raise Exception('TypeError while preparing batch. \\\n-                        If using HDF5 input data, pass shuffle=\"batch\".\\n')\n-\n+                    raise Exception('TypeError while preparing batch. '\n+                                    'If using HDF5 input data, '\n+                                    'pass shuffle=\"batch\".')\n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = len(batch_ids)\n@@ -255,8 +258,7 @@ class Model(object):\n         return history\n \n     def _predict_loop(self, f, ins, batch_size=128, verbose=0):\n-        '''\n-            Abstract method to loop over some data in batches.\n+        '''Abstract method to loop over some data in batches.\n         '''\n         nb_sample = len(ins[0])\n         outs = []\n@@ -283,8 +285,7 @@ class Model(object):\n         return outs\n \n     def _test_loop(self, f, ins, batch_size=128, verbose=0):\n-        '''\n-            Abstract method to loop over some data in batches.\n+        '''Abstract method to loop over some data in batches.\n         '''\n         nb_sample = len(ins[0])\n         outs = []\n@@ -315,8 +316,14 @@ class Model(object):\n         return outs\n \n     def get_config(self, verbose=0):\n+        '''Return the configuration of the model\n+        as a dictionary.\n+\n+        To load a model from its configuration, use\n+        `keras.models.model_from_config(config, custom_objects={})`.\n+        '''\n         config = super(Model, self).get_config()\n-        for p in ['class_mode', 'theano_mode']:\n+        for p in ['class_mode']:\n             if hasattr(self, p):\n                 config[p] = getattr(self, p)\n         if hasattr(self, 'optimizer'):\n@@ -333,38 +340,54 @@ class Model(object):\n         return config\n \n     def to_yaml(self, **kwargs):\n-        # dump model configuration to yaml string\n+        '''Return a yaml string containing the model configuration.\n+\n+        To load a model from a yaml save file, use\n+        `keras.models.from_yaml(yaml_string, custom_objects={})`.\n+\n+        `custom_objects` should be a dictionary mapping\n+        the names of custom losses / layers / etc to the corresponding\n+        functions / classes.\n+        '''\n         import yaml\n         config = self.get_config()\n         return yaml.dump(config, **kwargs)\n \n     def to_json(self, **kwargs):\n-        # dump model configuration to json string\n+        '''Return a JSON string containing the model configuration.\n+\n+        To load a model from a JSON save file, use\n+        `keras.models.from_json(json_string, custom_objects={})`.\n+        '''\n         import json\n         config = self.get_config()\n         return json.dumps(config, **kwargs)\n \n     def summary(self):\n+        '''Print out a summary of the model architecture,\n+        include parameter count information.\n+        '''\n         model_summary(self)\n \n \n class Sequential(Model, containers.Sequential):\n-    '''\n-        Inherits from Model the following methods:\n-            - _fit\n-            - _predict\n-            - _evaluate\n-        Inherits from containers.Sequential the following methods:\n-            - __init__\n-            - add\n-            - get_output\n-            - get_input\n-            - get_weights\n-            - set_weights\n-    '''\n+    '''Linear stack of layers.\n \n+    Inherits from containers.Sequential.\n+    '''\n     def compile(self, optimizer, loss,\n-                class_mode=\"categorical\", theano_mode=None):\n+                class_mode=\"categorical\"):\n+        '''Configure the learning process.\n+\n+        # Arguments\n+            optimizer: str (name of optimizer) or optimizer object.\n+                See [optimizers](optimizers.md).\n+            loss: str (name of objective function) or objective function.\n+                See [objectives](objectives.md).\n+            class_mode: one of \"categorical\", \"binary\".\n+                This is only used for computing classification accuracy or\n+                using the predict_classes method.\n+        '''\n         self.optimizer = optimizers.get(optimizer)\n \n         self.loss = objectives.get(loss)\n@@ -401,7 +424,6 @@ class Sequential(Model, containers.Sequential):\n         else:\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n-        self.theano_mode = theano_mode\n \n         for r in self.regularizers:\n             train_loss = r(train_loss)\n@@ -426,37 +448,48 @@ class Sequential(Model, containers.Sequential):\n         self._test = K.function(test_ins, [test_loss])\n         self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy])\n \n-    def train_on_batch(self, X, y, accuracy=False,\n-                       class_weight=None, sample_weight=None):\n-        X = standardize_X(X)\n-        y = standardize_y(y)\n-        sample_weight = standardize_weights(y, class_weight=class_weight,\n-                                            sample_weight=sample_weight)\n-        ins = X + [y, sample_weight]\n-        if accuracy:\n-            return self._train_with_acc(ins)\n-        else:\n-            return self._train(ins)\n-\n-    def test_on_batch(self, X, y, accuracy=False, sample_weight=None):\n-        X = standardize_X(X)\n-        y = standardize_y(y)\n-        sample_weight = standardize_weights(y, sample_weight=sample_weight)\n-\n-        ins = X + [y, sample_weight]\n-        if accuracy:\n-            return self._test_with_acc(ins)\n-        else:\n-            return self._test(ins)\n-\n-    def predict_on_batch(self, X):\n-        ins = standardize_X(X)\n-        return self._predict(ins)\n-\n     def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True,\n             show_accuracy=False, class_weight=None, sample_weight=None):\n+        '''Train the model for a fixed number of epochs.\n \n+        Returns a history object. It `history` attribute is a record of\n+        training loss values at successive epochs,\n+        as well as validation loss values (if applicable).\n+\n+        # Arguments\n+            X: data, as a numpy array.\n+            y: labels, as a numpy array.\n+            batch_size: int. Number of samples per gradient update.\n+            nb_epoch: int.\n+            verbose: 0 for no logging to stdout,\n+                1 for progress bar logging, 2 for one log line per epoch.\n+            callbacks: `keras.callbacks.Callback` list.\n+                List of callbacks to apply during training.\n+                See [callbacks](callbacks.md).\n+            validation_split: float (0. < x < 1).\n+                Fraction of the data to use as held-out validation data.\n+            validation_data: tuple (X, y) to be used as held-out\n+                validation data. Will override validation_split.\n+            shuffle: boolean or str (for 'batch').\n+                Whether to shuffle the samples at each epoch.\n+                'batch' is a special option for dealing with the\n+                limitations of HDF5 data; it shuffles in batch-sized chunks.\n+            show_accuracy: boolean. Whether to display\n+                class accuracy in the logs to stdout at each epoch.\n+            class_weight: dictionary mapping classes to a weight value,\n+                used for scaling the loss function (during training only).\n+            sample_weight: list or numpy array with 1:1 mapping to\n+                the training samples, used for scaling the loss function\n+                (during training only). For time-distributed data,\n+                there is one weight per sample *per timestep*,\n+                i.e. if your output data is shaped\n+                `(nb_samples, timesteps, output_dim)`,\n+                your mask should be of shape `(nb_samples, timesteps, 1)`.\n+                This allows you to mask out or reweight individual\n+                output timesteps, which is useful\n+                in sequence to sequence learning.\n+        '''\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n@@ -480,8 +513,11 @@ class Sequential(Model, containers.Sequential):\n                 sample_weight_val = standardize_weights(y_val,\n                                                         sample_weight=sample_weight_val)\n             else:\n-                raise Exception(\"Invalid format for validation data; provide a tuple (X_val, y_val) or (X_val, y_val, sample_weight). \\\n-                    X_val may be a numpy array or a list of numpy arrays depending on your model input.\")\n+                raise Exception('Invalid format for validation data; '\n+                                'provide a tuple (X_val, y_val) or '\n+                                '(X_val, y_val, sample_weight). '\n+                                'X_val may be a numpy array or a list of '\n+                                'numpy arrays depending on your model input.')\n             val_ins = X_val + [y_val, sample_weight_val]\n \n         elif 0 < validation_split < 1:\n@@ -490,7 +526,8 @@ class Sequential(Model, containers.Sequential):\n             y, y_val = (slice_X(y, 0, split_at), slice_X(y, split_at))\n             if sample_weight is not None:\n                 sample_weight, sample_weight_val = (slice_X(sample_weight, 0, split_at), slice_X(sample_weight, split_at))\n-                sample_weight_val = standardize_weights(y_val, sample_weight=sample_weight_val)\n+                sample_weight_val = standardize_weights(y_val,\n+                                                        sample_weight=sample_weight_val)\n             else:\n                 sample_weight_val = standardize_weights(y_val)\n             val_ins = X_val + [y_val, sample_weight_val]\n@@ -502,7 +539,8 @@ class Sequential(Model, containers.Sequential):\n             f = self._train\n             out_labels = ['loss']\n \n-        sample_weight = standardize_weights(y, class_weight=class_weight, sample_weight=sample_weight)\n+        sample_weight = standardize_weights(y, class_weight=class_weight,\n+                                            sample_weight=sample_weight)\n         ins = X + [y, sample_weight]\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n         return self._fit(f, ins, out_labels=out_labels,\n@@ -512,24 +550,67 @@ class Sequential(Model, containers.Sequential):\n                          shuffle=shuffle, metrics=metrics)\n \n     def predict(self, X, batch_size=128, verbose=0):\n+        '''Generate output predictions for the input samples\n+        batch by batch.\n+\n+        # Arguments\n+            X: the input data, as a numpy array.\n+            batch_size: integer.\n+            verbose: verbosity mode, 0 or 1.\n+\n+        # Returns\n+            A numpy array of predictions.\n+        '''\n         X = standardize_X(X)\n         return self._predict_loop(self._predict, X, batch_size, verbose)[0]\n \n     def predict_proba(self, X, batch_size=128, verbose=1):\n+        '''Generate class probability predictions for the input samples\n+        batch by batch.\n+\n+        # Arguments\n+            X: the input data, as a numpy array.\n+            batch_size: integer.\n+            verbose: verbosity mode, 0 or 1.\n+\n+        # Returns\n+            A numpy array of probability predictions.\n+        '''\n         preds = self.predict(X, batch_size, verbose)\n         if preds.min() < 0 or preds.max() > 1:\n-            warnings.warn(\"Network returning invalid probability values.\")\n+            warnings.warn('Network returning invalid probability values.')\n         return preds\n \n     def predict_classes(self, X, batch_size=128, verbose=1):\n+        '''Generate class predictions for the input samples\n+        batch by batch.\n+\n+        # Arguments\n+            X: the input data, as a numpy array.\n+            batch_size: integer.\n+            verbose: verbosity mode, 0 or 1.\n+\n+        # Returns\n+            A numpy array of class predictions.\n+        '''\n         proba = self.predict(X, batch_size=batch_size, verbose=verbose)\n-        if self.class_mode == \"categorical\":\n+        if self.class_mode == 'categorical':\n             return proba.argmax(axis=-1)\n         else:\n             return (proba > 0.5).astype('int32')\n \n     def evaluate(self, X, y, batch_size=128, show_accuracy=False,\n                  verbose=1, sample_weight=None):\n+        '''Compute the loss on some input data, batch by batch.\n+\n+        # Arguments\n+            X: input data, as a numpy array.\n+            y: labels, as a numpy array.\n+            batch_size: integer.\n+            show_accuracy: boolean.\n+            verbose: verbosity mode, 0 or 1.\n+            sample_weight: sample weights, as a numpy array.\n+        '''\n         X = standardize_X(X)\n         y = standardize_y(y)\n         sample_weight = standardize_weights(y, sample_weight=sample_weight)\n@@ -545,8 +626,50 @@ class Sequential(Model, containers.Sequential):\n         else:\n             return outs[0]\n \n+    def train_on_batch(self, X, y, accuracy=False,\n+                       class_weight=None, sample_weight=None):\n+        '''Single gradient update over one batch of samples.\n+\n+        Returns the loss over the data,\n+        or a tuple `(loss, accuracy)` if `accuracy=True`.\n+\n+        Arguments: see `fit` method.\n+        '''\n+        X = standardize_X(X)\n+        y = standardize_y(y)\n+        sample_weight = standardize_weights(y, class_weight=class_weight,\n+                                            sample_weight=sample_weight)\n+        ins = X + [y, sample_weight]\n+        if accuracy:\n+            return self._train_with_acc(ins)\n+        else:\n+            return self._train(ins)\n+\n+    def test_on_batch(self, X, y, accuracy=False, sample_weight=None):\n+        '''Returns the loss over a single batch of samples,\n+        or a tuple `(loss, accuracy)` if `accuracy=True`.\n+\n+        Arguments: see `fit` method.\n+        '''\n+        X = standardize_X(X)\n+        y = standardize_y(y)\n+        sample_weight = standardize_weights(y, sample_weight=sample_weight)\n+\n+        ins = X + [y, sample_weight]\n+        if accuracy:\n+            return self._test_with_acc(ins)\n+        else:\n+            return self._test(ins)\n+\n+    def predict_on_batch(self, X):\n+        '''Returns predictions for a single batch of samples.\n+        '''\n+        ins = standardize_X(X)\n+        return self._predict(ins)\n+\n     def save_weights(self, filepath, overwrite=False):\n-        # Save weights from all layers to HDF5\n+        '''Dump all layer weights to a HDF5 file.\n+        '''\n         import h5py\n         import os.path\n         # if file exists and should not be overwritten\n@@ -555,7 +678,8 @@ class Sequential(Model, containers.Sequential):\n             get_input = input\n             if sys.version_info[:2] <= (2, 7):\n                 get_input = raw_input\n-            overwrite = get_input('[WARNING] %s already exists - overwrite? [y/n]' % (filepath))\n+            overwrite = get_input('[WARNING] %s already exists - overwrite? '\n+                                  '[y/n]' % (filepath))\n             while overwrite not in ['y', 'n']:\n                 overwrite = get_input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n             if overwrite == 'n':\n@@ -570,20 +694,20 @@ class Sequential(Model, containers.Sequential):\n             g.attrs['nb_params'] = len(weights)\n             for n, param in enumerate(weights):\n                 param_name = 'param_{}'.format(n)\n-                param_dset = g.create_dataset(param_name, param.shape, dtype=param.dtype)\n+                param_dset = g.create_dataset(param_name, param.shape,\n+                                              dtype=param.dtype)\n                 param_dset[:] = param\n         f.flush()\n         f.close()\n \n     def load_weights(self, filepath):\n+        '''Load all layer weights from a HDF5 save file.\n         '''\n-            This method does not make use of Sequential.set_weights()\n-            for backwards compatibility.\n-        '''\n-        # Loads weights from HDF5 file\n         import h5py\n         f = h5py.File(filepath)\n         for k in range(f.attrs['nb_layers']):\n+            # This method does not make use of Sequential.set_weights()\n+            # for backwards compatibility.\n             g = f['layer_{}'.format(k)]\n             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n             self.layers[k].set_weights(weights)\n@@ -591,8 +715,24 @@ class Sequential(Model, containers.Sequential):\n \n \n class Graph(Model, containers.Graph):\n-    def compile(self, optimizer, loss, theano_mode=None):\n-        # loss is a dictionary mapping output name to loss functions\n+    '''Arbitrary connection graph.\n+    It can have any number of inputs and outputs,\n+    with each output trained with its own loss function.\n+    The quantity being optimized by a Graph model is\n+    the sum of all loss functions over the different outputs.\n+\n+    Inherits from `containers.Graph`.\n+    '''\n+    def compile(self, optimizer, loss):\n+        '''Configure the learning process.\n+\n+        # Arguments\n+            optimizer: str (name of optimizer) or optimizer object.\n+                See [optimizers](optimizers.md).\n+            loss: dictionary mapping the name(s) of the output(s) to\n+                a loss function (string name of objective function or\n+                objective function. See [objectives](objectives.md)).\n+        '''\n         ys = []\n         ys_train = []\n         ys_test = []\n@@ -627,38 +767,49 @@ class Graph(Model, containers.Graph):\n         for r in self.regularizers:\n             train_loss = r(train_loss)\n         self.optimizer = optimizers.get(optimizer)\n-        updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n+        updates = self.optimizer.get_updates(self.params,\n+                                             self.constraints,\n+                                             train_loss)\n         updates += self.updates\n-        self.theano_mode = theano_mode\n         self.loss = loss\n \n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._test = K.function(test_ins, [test_loss])\n-        self._predict = K.function(inputs=ins, outputs=ys_test, updates=self.state_updates)\n-\n-    def train_on_batch(self, data, class_weight={}, sample_weight={}):\n-        # data is a dictionary mapping output and input names to arrays\n-        sample_weight = [standardize_weights(data[name],\n-                                             sample_weight=sample_weight.get(name),\n-                                             class_weight=class_weight.get(name)) for name in self.output_order]\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n-        return self._train(ins)\n-\n-    def test_on_batch(self, data, sample_weight={}):\n-        # data is a dictionary mapping input names to arrays\n-        sample_weight = [standardize_weights(data[name],\n-                                             sample_weight=sample_weight.get(name)) for name in self.output_order]\n-        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n-        return self._test(ins)\n-\n-    def predict_on_batch(self, data):\n-        # data is a dictionary mapping input names to arrays\n-        ins = [data[name] for name in self.input_order]\n-        return self._predict(ins)\n+        self._predict = K.function(inputs=ins, outputs=ys_test,\n+                                   updates=self.state_updates)\n \n     def fit(self, data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True,\n             class_weight={}, sample_weight={}):\n+        '''Train the model for a fixed number of epochs.\n+\n+        Returns a history object. It `history` attribute is a record of\n+        training loss values at successive epochs,\n+        as well as validation loss values (if applicable).\n+\n+        # Arguments\n+            data: dictionary mapping input names and outputs names to\n+                appropriate numpy arrays. All arrays should contain\n+                the same number of samples.\n+            batch_size: int. Number of samples per gradient update.\n+            nb_epoch: int.\n+            verbose: 0 for no logging to stdout,\n+                1 for progress bar logging, 2 for one log line per epoch.\n+            callbacks: `keras.callbacks.Callback` list. List of callbacks\n+                to apply during training. See [callbacks](callbacks.md).\n+            validation_split: float (0. < x < 1). Fraction of the data to\n+                use as held-out validation data.\n+            validation_data: dictionary mapping input names and outputs names\n+                to appropriate numpy arrays to be used as\n+                held-out validation data.\n+                All arrays should contain the same number of samples.\n+                Will override validation_split.\n+            shuffle: boolean. Whether to shuffle the samples at each epoch.\n+            class_weight: dictionary mapping output names to\n+                class weight dictionaries.\n+            sample_weight: dictionary mapping output names to\n+                numpy arrays of sample weights.\n+        '''\n         X = [data[name] for name in self.input_order]\n         y = [standardize_y(data[name]) for name in self.output_order]\n \n@@ -691,13 +842,18 @@ class Graph(Model, containers.Graph):\n                                                   sample_weight=sample_weight_list[i],\n                                                   class_weight=class_weight_list[i]) for i in range(len(self.output_order))]\n         ins = X + y + sample_weight_list\n-        history = self._fit(f, ins, out_labels=out_labels, batch_size=batch_size, nb_epoch=nb_epoch,\n+        history = self._fit(f, ins, out_labels=out_labels,\n+                            batch_size=batch_size, nb_epoch=nb_epoch,\n                             verbose=verbose, callbacks=callbacks,\n                             val_f=val_f, val_ins=val_ins,\n                             shuffle=shuffle, metrics=metrics)\n         return history\n \n     def evaluate(self, data, batch_size=128, verbose=0, sample_weight={}):\n+        '''Compute the loss on some input data, batch by batch.\n+\n+        Arguments: see `fit` method.\n+        '''\n         sample_weight = [standardize_weights(data[name],\n                                              sample_weight=sample_weight.get(name)) for name in self.output_order]\n \n@@ -706,12 +862,45 @@ class Graph(Model, containers.Graph):\n         return outs[0]\n \n     def predict(self, data, batch_size=128, verbose=0):\n+        '''Generate output predictions for the input samples\n+        batch by batch.\n+\n+        Arguments: see `fit` method.\n+        '''\n         ins = [data[name] for name in self.input_order]\n         outs = self._predict_loop(self._predict, ins, batch_size, verbose)\n         return dict(zip(self.output_order, outs))\n \n+    def train_on_batch(self, data, class_weight={}, sample_weight={}):\n+        '''Single gradient update on a batch of samples.\n+\n+        Arguments: see `fit` method.\n+        '''\n+        sample_weight = [standardize_weights(data[name],\n+                                             sample_weight=sample_weight.get(name),\n+                                             class_weight=class_weight.get(name)) for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n+        return self._train(ins)\n+\n+    def test_on_batch(self, data, sample_weight={}):\n+        '''Compute the loss on a single batch of samples.\n+\n+        Arguments: see `fit` method.\n+        '''\n+        sample_weight = [standardize_weights(data[name],\n+                                             sample_weight=sample_weight.get(name)) for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order] + sample_weight\n+        return self._test(ins)\n+\n+    def predict_on_batch(self, data):\n+        '''Generate predictions for a single batch of samples.\n+        '''\n+        ins = [data[name] for name in self.input_order]\n+        return self._predict(ins)\n+\n     def save_weights(self, filepath, overwrite=False):\n-        # Save weights from all layers to HDF5\n+        '''Save weights from all layers to a HDF5 files.\n+        '''\n         import h5py\n         import os.path\n         # if file exists and should not be overwritten\n@@ -720,7 +909,8 @@ class Graph(Model, containers.Graph):\n             get_input = input\n             if sys.version_info[:2] <= (2, 7):\n                 get_input = raw_input\n-            overwrite = get_input('[WARNING] %s already exists - overwrite? [y/n]' % (filepath))\n+            overwrite = get_input('[WARNING] %s already exists - overwrite? '\n+                                  '[y/n]' % (filepath))\n             while overwrite not in ['y', 'n']:\n                 overwrite = get_input('Enter \"y\" (overwrite) or \"n\" (cancel).')\n             if overwrite == 'n':\n@@ -733,13 +923,15 @@ class Graph(Model, containers.Graph):\n         g.attrs['nb_params'] = len(weights)\n         for n, param in enumerate(weights):\n             param_name = 'param_{}'.format(n)\n-            param_dset = g.create_dataset(param_name, param.shape, dtype=param.dtype)\n+            param_dset = g.create_dataset(param_name, param.shape,\n+                                          dtype=param.dtype)\n             param_dset[:] = param\n         f.flush()\n         f.close()\n \n     def load_weights(self, filepath):\n-        # Loads weights from HDF5 file\n+        '''Load weights from a HDF5 file.\n+        '''\n         import h5py\n         f = h5py.File(filepath)\n         g = f['graph']\n\n@@ -16,6 +16,18 @@ def kl_divergence(p, p_hat):\n \n \n class Optimizer(object):\n+    '''Abstract optimizer base class.\n+\n+    Note: this is the parent class of all optimizers, not an actual optimizer\n+    that can be used for training models.\n+\n+    All Keras optimizers support the following keyword arguments:\n+\n+        clipnorm: float >= 0. Gradients will be clipped\n+            when their L2 norm exceeds this value.\n+        clipvalue: float >= 0. Gradients will be clipped\n+            when their absolute value exceeds this value.\n+    '''\n     def __init__(self, **kwargs):\n         self.__dict__.update(kwargs)\n         self.updates = []\n@@ -45,7 +57,15 @@ class Optimizer(object):\n \n \n class SGD(Optimizer):\n+    '''Stochastic gradient descent, with support for momentum,\n+    decay, and Nesterov momentum.\n \n+    # Arguments\n+        lr: float >= 0. Learning rate.\n+        momentum: float >= 0. Parameter updates momentum.\n+        decay: float >= 0. Learning rate decay over each update.\n+        nesterov: boolean. Whether to apply Nesterov momentum.\n+    '''\n     def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False,\n                  *args, **kwargs):\n         super(SGD, self).__init__(**kwargs)\n@@ -82,6 +102,19 @@ class SGD(Optimizer):\n \n \n class RMSprop(Optimizer):\n+    '''RMSProp optimizer.\n+\n+    It is recommended to leave the parameters of this optimizer\n+    at their default values.\n+\n+    This optimizer is usually a good choice for recurrent\n+    neural networks.\n+\n+    # Arguments\n+        lr: float >= 0. Learning rate.\n+        rho: float >= 0.\n+        epsilon: float >= 0. Fuzz factor.\n+    '''\n     def __init__(self, lr=0.001, rho=0.9, epsilon=1e-6, *args, **kwargs):\n         super(RMSprop, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n@@ -110,6 +143,15 @@ class RMSprop(Optimizer):\n \n \n class Adagrad(Optimizer):\n+    '''Adagrad optimizer.\n+\n+    It is recommended to leave the parameters of this optimizer\n+    at their default values.\n+\n+    # Arguments\n+        lr: float >= 0. Learning rate.\n+        epsilon: float >= 0.\n+    '''\n     def __init__(self, lr=0.01, epsilon=1e-6, *args, **kwargs):\n         super(Adagrad, self).__init__(**kwargs)\n         self.__dict__.update(locals())\n@@ -134,8 +176,18 @@ class Adagrad(Optimizer):\n \n \n class Adadelta(Optimizer):\n-    '''\n-        Reference: http://arxiv.org/abs/1212.5701\n+    '''Adadelta optimizer.\n+\n+    It is recommended to leave the parameters of this optimizer\n+    at their default values.\n+\n+    # Arguments\n+        lr: float >= 0. Learning rate. It is recommended to leave it at the default value.\n+        rho: float >= 0.\n+        epsilon: float >= 0. Fuzz factor.\n+\n+    # References\n+        - [Adadelta - an adaptive learning rate method](http://arxiv.org/abs/1212.5701)\n     '''\n     def __init__(self, lr=1.0, rho=0.95, epsilon=1e-6, *args, **kwargs):\n         super(Adadelta, self).__init__(**kwargs)\n@@ -173,10 +225,17 @@ class Adadelta(Optimizer):\n \n \n class Adam(Optimizer):\n-    '''\n-        Reference: http://arxiv.org/abs/1412.6980v8\n+    '''Adam optimizer.\n \n     Default parameters follow those provided in the original paper.\n+\n+    # Arguments\n+        lr: float >= 0. Learning rate.\n+        beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n+        epsilon: float >= 0. Fuzz factor.\n+\n+    # References\n+        - [Adam - A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980v8)\n     '''\n     def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8,\n                  *args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
