{"custom_id": "keras#b6eb310f75fb9783951911b7525d7f71c20de4ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#419973ee15ecd0e2d085e077399ce3bd5437df15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 11 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 117 | Churn Cumulative: 1595 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import numpy as np\n \n from keras import layers\n+from keras import ops\n from keras import testing\n \n \n@@ -205,6 +206,7 @@ class AttentionTest(testing.TestCase):\n         valid_mask = np.array([True, False, True])\n         mask = [valid_mask, np.array([False, True, False])]\n         computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n         self.assertIsNotNone(\n             computed_mask,\n             \"compute_mask should not return None with a valid mask\",\n@@ -221,7 +223,122 @@ class AttentionTest(testing.TestCase):\n         valid_mask = np.array([True, False, True])\n         mask = [valid_mask, np.array([False, True, False])]\n         computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n         self.assertTrue(\n             np.array_equal(computed_mask, valid_mask),\n             \"compute_mask did not return the correct mask tensor\",\n         )\n+\n+    def test_attention_compute_mask_returns_correct_tensor_with_all_true_mask(\n+        self,\n+    ):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([True, True, True])\n+        mask = [valid_mask, np.array([True, True, True])]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        expected_mask = np.array([True, True, True])\n+        self.assertTrue(\n+            np.array_equal(computed_mask, expected_mask),\n+            \"compute_mask did not return the correct mask tensor\",\n+        )\n+\n+    def test_attention_compute_mask_returns_correct_tensor_with_all_false_mask(\n+        self,\n+    ):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([False, False, False])\n+        mask = [valid_mask, np.array([False, False, False])]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        expected_mask = np.array([False, False, False])\n+        self.assertTrue(\n+            np.array_equal(computed_mask, expected_mask),\n+            \"compute_mask did not return the correct mask tensor\",\n+        )\n+\n+    def test_attention_compute_mask_with_tolerance_1e_3(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([1.0, 0.0, 1.0], dtype=float)\n+        mask = [valid_mask, np.array([0.0, 1.0, 0.0], dtype=float)]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        expected_mask = valid_mask\n+        self.assertTrue(\n+            np.allclose(computed_mask, expected_mask, atol=1e-3),\n+            \"Incorrect mask tensor within tolerance 1e-3\",\n+        )\n+\n+    def test_attention_compute_mask_with_tolerance_1e_5(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([1.0, 0.0, 1.0], dtype=float)\n+        mask = [valid_mask, np.array([0.0, 1.0, 0.0], dtype=float)]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        expected_mask = valid_mask\n+        self.assertTrue(\n+            np.allclose(computed_mask, expected_mask, atol=1e-5),\n+            \"Incorrect mask tensor within tolerance 1e-5\",\n+        )\n+\n+    def test_attention_compute_mask_with_tolerance_1e_7(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([1.0, 0.0, 1.0], dtype=float)\n+        mask = [valid_mask, np.array([0.0, 1.0, 0.0], dtype=float)]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        expected_mask = valid_mask\n+        self.assertTrue(\n+            np.allclose(computed_mask, expected_mask, atol=1e-7),\n+            \"Incorrect mask tensor within tolerance 1e-7 \",\n+        )\n+\n+    def test_attention_compute_mask_with_single_element_masks(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([True])\n+        mask = [valid_mask, np.array([False])]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        expected_shape = (1,)\n+        self.assertEqual(computed_mask.shape, expected_shape)\n+\n+    def test_attention_compute_mask_with_non_boolean_masks(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        valid_mask = np.array([1, 0, 1])\n+        mask = [valid_mask, np.array([0, 1, 0])]\n+        computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+        computed_mask = ops.convert_to_numpy(computed_mask)\n+        self.assertTrue(np.array_equal(computed_mask, valid_mask))\n+\n+    def test_attention_compute_mask_with_edge_case_masks(self):\n+        layer = layers.Attention()\n+        dummy_inputs = [np.ones((2, 3, 4)), np.ones((2, 4, 4))]\n+        edge_case_masks = [\n+            np.array([True, True, True]),\n+            np.array([False, False, False]),\n+            np.array([True, False, True]),\n+        ]\n+        for mask in edge_case_masks:\n+            computed_mask = layer.compute_mask(\n+                inputs=dummy_inputs, mask=[mask, mask]\n+            )\n+            computed_mask = ops.convert_to_numpy(computed_mask)\n+            self.assertTrue(np.array_equal(computed_mask, mask))\n+\n+    def test_attention_compute_mask_with_different_input_shapes(self):\n+        layer = layers.Attention()\n+        input_shapes = [(2, 3, 4), (3, 2, 5), (4, 1, 6)]\n+        valid_mask = np.array([True, False, True])\n+        for shape in input_shapes:\n+            dummy_inputs = [np.ones(shape), np.ones(shape)]\n+            mask = [valid_mask, np.array([False, True, False])]\n+            computed_mask = layer.compute_mask(inputs=dummy_inputs, mask=mask)\n+            computed_mask = ops.convert_to_numpy(computed_mask)\n+            self.assertTrue(np.array_equal(computed_mask, valid_mask))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4766e80b9a614378e0bdc44a0a38eadb3266b095", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 31 | Files Changed: 6 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 44/21 | Churn Δ: 65 | Churn Cumulative: 1182 | Contributors (this commit): 12 | Commits (past 90d): 18 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ import keras\n from keras import backend\n from keras import testing\n from keras.applications import imagenet_utils as utils\n-from keras.layers.dtype_policy import set_dtype_policy\n+from keras.dtype_policies.dtype_policy import set_dtype_policy\n \n \n class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n\n@@ -0,0 +1,21 @@\n+from keras import backend\n+from keras.dtype_policies import dtype_policy\n+from keras.saving import serialization_lib\n+\n+\n+def get(identifier):\n+    if identifier is None:\n+        return dtype_policy.dtype_policy()\n+    if isinstance(identifier, dtype_policy.DTypePolicy):\n+        return identifier\n+    if isinstance(identifier, dict):\n+        return serialization_lib.deserialize_keras_object(identifier)\n+    if isinstance(identifier, str):\n+        return dtype_policy.DTypePolicy(identifier)\n+    try:\n+        return dtype_policy.DTypePolicy(backend.standardize_dtype(identifier))\n+    except:\n+        raise ValueError(\n+            \"Cannot interpret `dtype` argument. Expected a string \"\n+            f\"or an instance of DTypePolicy. Received: dtype={identifier}\"\n+        )\n\n@@ -1,14 +1,14 @@\n from keras import backend\n from keras.api_export import keras_export\n from keras.backend.common import global_state\n-from keras.saving import serialization_lib\n \n \n @keras_export(\n     [\n         \"keras.DTypePolicy\",\n-        \"keras.mixed_precision.DTypePolicy\",\n-        \"keras.mixed_precision.Policy\",\n+        \"keras.dtype_policies.DTypePolicy\",\n+        \"keras.mixed_precision.DTypePolicy\",  # Legacy\n+        \"keras.mixed_precision.Policy\",  # Legacy\n     ]\n )\n class DTypePolicy:\n@@ -145,8 +145,8 @@ class DTypePolicy:\n @keras_export(\n     [\n         \"keras.config.set_dtype_policy\",\n-        \"keras.mixed_precision.set_dtype_policy\",\n-        \"keras.mixed_precision.set_global_policy\",\n+        \"keras.mixed_precision.set_dtype_policy\",  # Legacy\n+        \"keras.mixed_precision.set_global_policy\",  # Legacy\n     ]\n )\n def set_dtype_policy(policy):\n@@ -173,8 +173,8 @@ def set_dtype_policy(policy):\n @keras_export(\n     [\n         \"keras.config.dtype_policy\",\n-        \"keras.mixed_precision.dtype_policy\",\n-        \"keras.mixed_precision.global_policy\",\n+        \"keras.mixed_precision.dtype_policy\",  # Legacy\n+        \"keras.mixed_precision.global_policy\",  # Legacy\n     ]\n )\n def dtype_policy():\n@@ -184,21 +184,3 @@ def dtype_policy():\n         policy = DTypePolicy(backend.floatx())\n         set_dtype_policy(policy)\n     return policy\n-\n-\n-def get(identifier):\n-    if identifier is None:\n-        return dtype_policy()\n-    if isinstance(identifier, DTypePolicy):\n-        return identifier\n-    if isinstance(identifier, dict):\n-        return serialization_lib.deserialize_keras_object(identifier)\n-    if isinstance(identifier, str):\n-        return DTypePolicy(identifier)\n-    try:\n-        return DTypePolicy(backend.standardize_dtype(identifier))\n-    except:\n-        raise ValueError(\n-            \"Cannot interpret `dtype` argument. Expected a string \"\n-            f\"or an instance of DTypePolicy. Received: dtype={identifier}\"\n-        )\n\n@@ -1,6 +1,6 @@\n-from keras.layers.dtype_policy import DTypePolicy\n-from keras.layers.dtype_policy import dtype_policy\n-from keras.layers.dtype_policy import set_dtype_policy\n+from keras.dtype_policies.dtype_policy import DTypePolicy\n+from keras.dtype_policies.dtype_policy import dtype_policy\n+from keras.dtype_policies.dtype_policy import set_dtype_policy\n from keras.testing import test_case\n \n \n\n@@ -25,6 +25,7 @@ import tree\n \n from keras import backend\n from keras import constraints\n+from keras import dtype_policies\n from keras import initializers\n from keras import ops\n from keras import regularizers\n@@ -34,7 +35,6 @@ from keras.backend import KerasTensor\n from keras.backend.common import global_state\n from keras.backend.common.name_scope import current_path\n from keras.distribution import distribution_lib\n-from keras.layers import dtype_policy\n from keras.layers import input_spec\n from keras.metrics.metric import Metric\n from keras.ops.operation import Operation\n@@ -269,7 +269,7 @@ class Layer(BackendLayer, Operation):\n             )\n \n         self.built = False\n-        self.dtype_policy = dtype_policy.get(dtype)\n+        self.dtype_policy = dtype_policies.get(dtype)\n         self.autocast = autocast\n         self._input_spec = None\n         self._called = False\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71a601b857bca160b122690647b2829299edc817", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1052 | Lines Deleted: 405 | Files Changed: 18 | Hunks: 201 | Methods Changed: 98 | Complexity Δ (Sum/Max): 141/49 | Churn Δ: 1457 | Churn Cumulative: 13607 | Contributors (this commit): 21 | Commits (past 90d): 134 | Contributors (cumulative): 90 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import types\n \n import jax\n+import jax.experimental.sparse as jax_sparse\n import jax.numpy as jnp\n import ml_dtypes\n import numpy as np\n@@ -13,9 +14,8 @@ from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.stateless_scope import StatelessScope\n from keras.backend.jax import distribution_lib\n-from keras.utils.nest import pack_sequence_as\n \n-SUPPORTS_SPARSE_TENSORS = False\n+SUPPORTS_SPARSE_TENSORS = True\n \n \n class Variable(KerasVariable):\n@@ -40,16 +40,14 @@ class Variable(KerasVariable):\n         self._value = value\n \n     def _convert_to_tensor(self, value, dtype=None):\n-        return convert_to_tensor(value, dtype=dtype)\n+        return convert_to_tensor(value, dtype=dtype, sparse=False)\n \n     # Overload native accessor.\n     def __jax_array__(self):\n         return self.value\n \n \n-def convert_to_tensor(x, dtype=None, sparse=None):\n-    if sparse:\n-        raise ValueError(\"`sparse=True` is not supported with jax backend\")\n+def convert_to_tensor(x, dtype=None, sparse=True):\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if isinstance(x, (jnp.ndarray, jax.Array)) and dtype == x.dtype:\n@@ -62,6 +60,15 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n         if dtype and dtype != x.dtype:\n             return x.value.astype(dtype)\n         return x.value\n+\n+    if isinstance(x, jax_sparse.JAXSparse):\n+        if sparse is not None and not sparse:\n+            x = x.todense()\n+        elif dtype and dtype != x.dtype:\n+            return x.astype(dtype)\n+        else:\n+            return x\n+\n     if not is_tensor(x) and standardize_dtype(dtype) == \"bfloat16\":\n         # Can't create bfloat16 arrays on the fly (e.g. from a h5 Dataset).\n         # Instead we convert \"as is\" (to stored dtype) and cast.\n@@ -70,13 +77,15 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n \n \n def convert_to_numpy(x):\n+    if isinstance(x, jax_sparse.JAXSparse):\n+        x = x.todense()\n     if is_tensor(x) and x.dtype == \"bfloat16\":\n         return np.asarray(x, ml_dtypes.bfloat16)\n     return np.asarray(x)\n \n \n def is_tensor(x):\n-    if isinstance(x, jnp.ndarray):\n+    if isinstance(x, (jnp.ndarray, jax_sparse.JAXSparse)):\n         return True\n     return False\n \n@@ -91,10 +100,9 @@ def cast(x, dtype):\n     return convert_to_tensor(x, dtype=dtype)\n \n \n-# Shape / dtype inference util\n+# Shape / dtype / sparseness inference util\n def compute_output_spec(fn, *args, **kwargs):\n     with StatelessScope():\n-        all_input_ktensors = []\n         built_in_types = (type(None), int, float, str, bool, complex, bytes)\n \n         # First, separate symbolic args from other args\n@@ -109,27 +117,18 @@ def compute_output_spec(fn, *args, **kwargs):\n                 static_args.append(arg)\n             else:\n                 maybe_symbolic_args.append(arg)\n+        maybe_symbolic_args = tuple(maybe_symbolic_args)\n         for k, v in kwargs.items():\n             if isinstance(v, built_in_types):\n                 static_kwargs[k] = v\n             else:\n                 maybe_symbolic_kwargs[k] = v\n \n-        # Second, identify all ktensors\n-        def index_all_ktensors(x):\n-            if isinstance(x, KerasTensor):\n-                all_input_ktensors.append(x)\n-            return x\n-\n-        # Third, find out if there are dynamic shapes\n-        maybe_symbolic_args, maybe_symbolic_kwargs = tree.map_structure(\n-            index_all_ktensors, (maybe_symbolic_args, maybe_symbolic_kwargs)\n-        )\n-        none_count = 0\n-        for x in all_input_ktensors:\n-            for d in x.shape:\n-                if d is None:\n-                    none_count += 1\n+        # Second, find out if there are dynamic shapes\n+        has_none = False\n+        for x in tree.flatten((maybe_symbolic_args, maybe_symbolic_kwargs)):\n+            if isinstance(x, KerasTensor) and any(d is None for d in x.shape):\n+                has_none = True\n \n         def convert_keras_tensor_to_jax(x, fill_value=None):\n             if isinstance(x, KerasTensor):\n@@ -163,6 +162,22 @@ def compute_output_spec(fn, *args, **kwargs):\n             return x\n \n         def wrapped_fn(*args, **kwargs):\n+\n+            # Turn inputs that are sparse to BCOO tensors\n+            def to_bcoo_if_sparse(x, maybe_symbolic_x):\n+                if (\n+                    isinstance(maybe_symbolic_x, KerasTensor)\n+                    and maybe_symbolic_x.sparse\n+                ):\n+                    return jax_sparse.BCOO.fromdense(x, nse=1)\n+                return x\n+\n+            args, kwargs = tree.map_structure(\n+                to_bcoo_if_sparse,\n+                (args, kwargs),\n+                (maybe_symbolic_args, maybe_symbolic_kwargs),\n+            )\n+\n             rec_args = []\n             idx_static = 0\n             idx_sym = 0\n@@ -179,8 +194,8 @@ def compute_output_spec(fn, *args, **kwargs):\n             with StatelessScope():\n                 return fn(*rec_args, **kwargs, **static_kwargs)\n \n-        jax_out = None\n-        if none_count:\n+        output_spec = None\n+        if has_none:\n             try:\n                 ms_args_1, ms_kwargs_1 = tree.map_structure(\n                     lambda x: convert_keras_tensor_to_jax(x, fill_value=83),\n@@ -198,24 +213,35 @@ def compute_output_spec(fn, *args, **kwargs):\n                     *ms_args_2, **ms_kwargs_2\n                 )\n \n-                flat_out_1 = tree.flatten(jax_out_1)\n-                flat_out_2 = tree.flatten(jax_out_2)\n+                def merge_shapes(shape1, shape2):\n+                    return tuple(\n+                        [\n+                            d1 if d1 == d2 else None\n+                            for d1, d2 in zip(shape1, shape2)\n+                        ]\n+                    )\n \n-                flat_out = []\n-                for x1, x2 in zip(flat_out_1, flat_out_2):\n+                def convert_jax_specs_to_keras_tensor(x1, x2):\n                     if isinstance(x1, jax.ShapeDtypeStruct):\n                         if not isinstance(x2, jax.ShapeDtypeStruct):\n                             raise ValueError(\"Indeterministic output ordering.\")\n-                        shape = list(x1.shape)\n-                        for i, e in enumerate(x2.shape):\n-                            if e != shape[i]:\n-                                shape[i] = None\n-                        flat_out.append(\n-                            jax.ShapeDtypeStruct(shape, dtype=x1.dtype)\n+                        return KerasTensor(\n+                            merge_shapes(x1.shape, x2.shape), dtype=x1.dtype\n+                        )\n+                    elif isinstance(x1, jax_sparse.BCOO):\n+                        if not isinstance(x2, jax_sparse.BCOO):\n+                            raise ValueError(\"Indeterministic output ordering.\")\n+                        return KerasTensor(\n+                            merge_shapes(x1.shape, x2.shape),\n+                            dtype=x1.dtype,\n+                            sparse=True,\n                         )\n                     else:\n-                        flat_out.append(x1)\n-                jax_out = pack_sequence_as(jax_out_1, flat_out)\n+                        return x1\n+\n+                output_spec = tree.map_structure(\n+                    convert_jax_specs_to_keras_tensor, jax_out_1, jax_out_2\n+                )\n             except Exception as e:\n                 if \"[JAX RNG]\" in str(e):\n                     raise e\n@@ -230,7 +256,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 # The error message will be much easier to understand.\n                 pass\n \n-        if jax_out is None:\n+        if output_spec is None:\n             maybe_symbolic_args, maybe_symbolic_kwargs = tree.map_structure(\n                 convert_keras_tensor_to_jax,\n                 (maybe_symbolic_args, maybe_symbolic_kwargs),\n@@ -242,12 +268,15 @@ def compute_output_spec(fn, *args, **kwargs):\n             def convert_jax_spec_to_keras_tensor(x):\n                 if isinstance(x, jax.ShapeDtypeStruct):\n                     return KerasTensor(x.shape, x.dtype)\n+                elif isinstance(x, jax_sparse.BCOO):\n+                    return KerasTensor(x.shape, x.dtype, sparse=True)\n                 return x\n \n-        output_shape = tree.map_structure(\n+            output_spec = tree.map_structure(\n                 convert_jax_spec_to_keras_tensor, jax_out\n             )\n-    return output_shape\n+\n+    return output_spec\n \n \n def cond(pred, true_fn, false_fn):\n\n@@ -1,12 +1,18 @@\n+import builtins\n+import math\n+\n+import jax.experimental.sparse as jax_sparse\n import jax.numpy as jnp\n \n from keras.backend import config\n from keras.backend.common import dtypes\n from keras.backend.common.variables import standardize_dtype\n+from keras.backend.jax import sparse\n from keras.backend.jax.core import cast\n from keras.backend.jax.core import convert_to_tensor\n \n \n+@sparse.elementwise_binary_union(linear=True, use_sparsify=True)\n def add(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n@@ -39,6 +45,7 @@ def einsum(subscripts, *operands, **kwargs):\n     return jnp.einsum(subscripts, *operands, **kwargs)\n \n \n+@sparse.elementwise_binary_union(linear=True, use_sparsify=True)\n def subtract(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n@@ -48,12 +55,62 @@ def subtract(x1, x2):\n def matmul(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n+    if isinstance(x1, jax_sparse.JAXSparse) or isinstance(\n+        x2, jax_sparse.JAXSparse\n+    ):\n+        if not hasattr(matmul, \"sparse_matmul\"):\n+            matmul.sparse_matmul = jax_sparse.sparsify(jnp.matmul)\n+        if isinstance(x1, jax_sparse.BCOO):\n+            x1 = jax_sparse.bcoo_update_layout(\n+                x1, n_batch=len(x1.shape) - 2, on_inefficient=\"warn\"\n+            )\n+        if isinstance(x2, jax_sparse.BCOO):\n+            x2 = jax_sparse.bcoo_update_layout(\n+                x2, n_batch=len(x2.shape) - 2, on_inefficient=\"warn\"\n+            )\n+        return matmul.sparse_matmul(x1, x2)\n+\n     return jnp.matmul(x1, x2)\n \n \n def multiply(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n+    if isinstance(x1, jax_sparse.BCOO):\n+        if isinstance(x2, jax_sparse.BCOO):\n+            # x1 is sparse, x2 is sparse.\n+            if x1.indices is x2.indices:\n+                # `bcoo_multiply_sparse`` will not detect that the indices are\n+                # the same, optimize this case here.\n+                if not x1.unique_indices:\n+                    x1 = jax_sparse.bcoo_sum_duplicates(x1)\n+                    x2 = jax_sparse.bcoo_sum_duplicates(x2)\n+                return jax_sparse.BCOO(\n+                    (jnp.multiply(x1.data, x2.data), x1.indices),\n+                    shape=x1.shape,\n+                    indices_sorted=True,\n+                    unique_indices=True,\n+                )\n+            else:\n+                return jax_sparse.bcoo_multiply_sparse(x1, x2)\n+        else:\n+            # x1 is sparse, x2 is dense.\n+            out_data = jax_sparse.bcoo_multiply_dense(x1, x2)\n+            return jax_sparse.BCOO(\n+                (out_data, x1.indices),\n+                shape=x1.shape,\n+                indices_sorted=x1.indices_sorted,\n+                unique_indices=x1.unique_indices,\n+            )\n+    elif isinstance(x2, jax_sparse.BCOO):\n+        # x1 is dense, x2 is sparse.\n+        out_data = jax_sparse.bcoo_multiply_dense(x2, x1)\n+        return jax_sparse.BCOO(\n+            (out_data, x2.indices),\n+            shape=x2.shape,\n+            indices_sorted=x2.indices_sorted,\n+            unique_indices=x2.unique_indices,\n+        )\n     return jnp.multiply(x1, x2)\n \n \n@@ -67,8 +124,34 @@ def mean(x, axis=None, keepdims=False):\n         result_dtype = compute_dtype\n     else:\n         result_dtype = ori_dtype\n-    outputs = jnp.mean(x, axis=axis, keepdims=keepdims, dtype=compute_dtype)\n-    return cast(outputs, result_dtype)\n+    if isinstance(x, jax_sparse.BCOO):\n+        if axis is None:\n+            axis = tuple(range(len(x.shape)))\n+        (\n+            canonical_axis,\n+            keep_dims_shape,\n+            broadcast_dimensions,\n+        ) = sparse.axis_shape_dims_for_broadcast_in_dim(\n+            axis, x.shape, insert_dims=False\n+        )\n+        divisor = math.prod(x.shape[i] for i in canonical_axis)\n+        output = jax_sparse.bcoo_reduce_sum(x, axes=canonical_axis)\n+        output = jax_sparse.BCOO(\n+            (output.data.astype(result_dtype) / divisor, output.indices),\n+            shape=output.shape,\n+        )\n+        if keepdims:\n+            # `bcoo_reduce_sum` does not support keepdims, neither does\n+            # sparsify(jnp.sum), so we recreate the empty dimensions.\n+            output = jax_sparse.bcoo_broadcast_in_dim(\n+                output,\n+                shape=keep_dims_shape,\n+                broadcast_dimensions=broadcast_dimensions,\n+            )\n+        return output\n+    else:\n+        output = jnp.mean(x, axis=axis, keepdims=keepdims, dtype=compute_dtype)\n+        return cast(output, result_dtype)\n \n \n def max(x, axis=None, keepdims=False, initial=None):\n@@ -86,12 +169,14 @@ def zeros(shape, dtype=None):\n     return jnp.zeros(shape, dtype=dtype)\n \n \n+@sparse.elementwise_unary(linear=False)\n def absolute(x):\n     return jnp.absolute(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def abs(x):\n-    return absolute(x)\n+    return jnp.absolute(x)\n \n \n def all(x, axis=None, keepdims=False):\n@@ -129,6 +214,7 @@ def arange(start, stop=None, step=1, dtype=None):\n     return jnp.arange(start, stop, step=step, dtype=dtype)\n \n \n+@sparse.densifying_unary\n def arccos(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -139,6 +225,7 @@ def arccos(x):\n     return jnp.arccos(x)\n \n \n+@sparse.densifying_unary\n def arccosh(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -149,6 +236,7 @@ def arccosh(x):\n     return jnp.arccosh(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def arcsin(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -159,6 +247,7 @@ def arcsin(x):\n     return jnp.arcsin(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def arcsinh(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -169,6 +258,7 @@ def arcsinh(x):\n     return jnp.arcsinh(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def arctan(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -188,6 +278,7 @@ def arctan2(x1, x2):\n     return jnp.arctan2(x1, x2)\n \n \n+@sparse.elementwise_unary(linear=False)\n def arctanh(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -234,6 +325,7 @@ def broadcast_to(x, shape):\n     return jnp.broadcast_to(x, shape)\n \n \n+@sparse.elementwise_unary(linear=False)\n def ceil(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -251,21 +343,42 @@ def clip(x, x_min, x_max):\n \n \n def concatenate(xs, axis=0):\n+    bcoo_count = builtins.sum(isinstance(x, jax_sparse.BCOO) for x in xs)\n+    if bcoo_count:\n+        if bcoo_count == len(xs):\n+            ndim = len(xs[0].shape)\n+            if not -ndim <= axis < ndim:\n+                raise ValueError(\n+                    f\"In `axis`, axis {axis} is out of bounds for array \"\n+                    f\"of dimension {ndim}\"\n+                )\n+            if axis < 0:\n+                axis = axis + ndim\n+            return jax_sparse.bcoo_concatenate(xs, dimension=axis)\n+        else:\n+            xs = [\n+                x.todense() if isinstance(x, jax_sparse.JAXSparse) else x\n+                for x in xs\n+            ]\n     return jnp.concatenate(xs, axis=axis)\n \n \n+@sparse.elementwise_unary(linear=True)\n def conjugate(x):\n     return jnp.conjugate(x)\n \n \n+@sparse.elementwise_unary(linear=True)\n def conj(x):\n-    return conjugate(x)\n+    return jnp.conjugate(x)\n \n \n+@sparse.elementwise_unary(linear=True)\n def copy(x):\n     return jnp.copy(x)\n \n \n+@sparse.densifying_unary\n def cos(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -276,6 +389,7 @@ def cos(x):\n     return jnp.cos(x)\n \n \n+@sparse.densifying_unary\n def cosh(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -348,6 +462,7 @@ def equal(x1, x2):\n     return jnp.equal(x1, x2)\n \n \n+@sparse.densifying_unary\n def exp(x):\n     x = convert_to_tensor(x)\n     ori_dtype = standardize_dtype(x.dtype)\n@@ -357,9 +472,21 @@ def exp(x):\n \n \n def expand_dims(x, axis):\n+    if isinstance(x, jax_sparse.BCOO):\n+        (\n+            _,\n+            result_shape,\n+            broadcast_dimensions,\n+        ) = sparse.axis_shape_dims_for_broadcast_in_dim(\n+            axis, x.shape, insert_dims=True\n+        )\n+        return jax_sparse.bcoo_broadcast_in_dim(\n+            x, shape=result_shape, broadcast_dimensions=broadcast_dimensions\n+        )\n     return jnp.expand_dims(x, axis)\n \n \n+@sparse.elementwise_unary(linear=False)\n def expm1(x):\n     x = convert_to_tensor(x)\n     ori_dtype = standardize_dtype(x.dtype)\n@@ -372,6 +499,7 @@ def flip(x, axis=None):\n     return jnp.flip(x, axis=axis)\n \n \n+@sparse.elementwise_unary(linear=False)\n def floor(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -409,6 +537,7 @@ def identity(n, dtype=None):\n     return jnp.identity(n, dtype=dtype)\n \n \n+@sparse.elementwise_unary(linear=True)\n def imag(x):\n     return jnp.imag(x)\n \n@@ -419,14 +548,17 @@ def isclose(x1, x2):\n     return jnp.isclose(x1, x2)\n \n \n+@sparse.densifying_unary\n def isfinite(x):\n     return jnp.isfinite(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def isinf(x):\n     return jnp.isinf(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def isnan(x):\n     return jnp.isnan(x)\n \n@@ -457,6 +589,7 @@ def linspace(\n     )\n \n \n+@sparse.densifying_unary\n def log(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -464,6 +597,7 @@ def log(x):\n     return jnp.log(x)\n \n \n+@sparse.densifying_unary\n def log10(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -471,6 +605,7 @@ def log10(x):\n     return jnp.log10(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def log1p(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -478,6 +613,7 @@ def log1p(x):\n     return jnp.log1p(x)\n \n \n+@sparse.densifying_unary\n def log2(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -522,6 +658,7 @@ def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n     )\n \n \n+@sparse.elementwise_binary_union(linear=False, use_sparsify=False)\n def maximum(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n@@ -553,6 +690,7 @@ def min(x, axis=None, keepdims=False, initial=None):\n     return jnp.min(x, axis=axis, keepdims=keepdims, initial=initial)\n \n \n+@sparse.elementwise_binary_union(linear=False, use_sparsify=False)\n def minimum(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n@@ -635,10 +773,12 @@ def ravel(x):\n     return jnp.ravel(x)\n \n \n+@sparse.elementwise_unary(linear=True)\n def real(x):\n     return jnp.real(x)\n \n \n+@sparse.densifying_unary\n def reciprocal(x):\n     return jnp.reciprocal(x)\n \n@@ -648,6 +788,16 @@ def repeat(x, repeats, axis=None):\n \n \n def reshape(x, newshape):\n+    if isinstance(x, jax_sparse.BCOO):\n+        from keras.ops import operation_utils\n+\n+        # Resolve the -1 in `new_shape` if applicable and possible\n+        output_shape = operation_utils.compute_reshape_output_shape(\n+            x.shape, newshape, \"new_shape\"\n+        )\n+        if None not in output_shape:\n+            newshape = output_shape\n+        return jax_sparse.bcoo_reshape(x, new_sizes=newshape)\n     return jnp.reshape(x, newshape)\n \n \n@@ -655,10 +805,12 @@ def roll(x, shift, axis=None):\n     return jnp.roll(x, shift, axis=axis)\n \n \n+@sparse.elementwise_unary(linear=False)\n def sign(x):\n     return jnp.sign(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def sin(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -669,6 +821,7 @@ def sin(x):\n     return jnp.sin(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def sinh(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -708,7 +861,7 @@ def swapaxes(x, axis1, axis2):\n \n def take(x, indices, axis=None):\n     x = convert_to_tensor(x)\n-    indices = convert_to_tensor(indices)\n+    indices = convert_to_tensor(indices, sparse=False)\n     return jnp.take(x, indices, axis=axis)\n \n \n@@ -716,6 +869,7 @@ def take_along_axis(x, indices, axis=None):\n     return jnp.take_along_axis(x, indices, axis=axis)\n \n \n+@sparse.elementwise_unary(linear=False)\n def tan(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -726,6 +880,7 @@ def tan(x):\n     return jnp.tan(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def tanh(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -742,6 +897,7 @@ def tensordot(x1, x2, axes=2):\n     return jnp.tensordot(x1, x2, axes=axes)\n \n \n+@sparse.elementwise_unary(linear=False)\n def round(x, decimals=0):\n     return jnp.round(x, decimals=decimals)\n \n@@ -783,12 +939,14 @@ def where(condition, x1, x2):\n     return jnp.where(condition, x1, x2)\n \n \n+@sparse.elementwise_division\n def divide(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n     return jnp.divide(x1, x2)\n \n \n+@sparse.elementwise_division\n def true_divide(x1, x2):\n     return divide(x1, x2)\n \n@@ -799,14 +957,17 @@ def power(x1, x2):\n     return jnp.power(x1, x2)\n \n \n+@sparse.elementwise_unary(linear=True)\n def negative(x):\n     return jnp.negative(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def square(x):\n     return jnp.square(x)\n \n \n+@sparse.elementwise_unary(linear=False)\n def sqrt(x):\n     x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n@@ -815,11 +976,30 @@ def sqrt(x):\n \n \n def squeeze(x, axis=None):\n+    if isinstance(x, jax_sparse.BCOO):\n+        if axis is None:\n+            axis = tuple(i for i, d in enumerate(x.shape) if d == 1)\n+        elif isinstance(axis, int):\n+            axis = (axis,)\n+        return jax_sparse.bcoo_squeeze(x, dimensions=axis)\n     return jnp.squeeze(x, axis=axis)\n \n \n def transpose(x, axes=None):\n     x = convert_to_tensor(x)\n+    if isinstance(x, jax_sparse.BCOO):\n+        num_dims = len(x.shape)\n+        if axes is None:\n+            permutation = tuple(range(num_dims)[::-1])\n+        else:\n+            permutation = []\n+            for a in axes:\n+                if not -num_dims <= a < num_dims:\n+                    raise ValueError(\n+                        f\"axis {a} out of bounds for tensor of rank {num_dims}\"\n+                    )\n+                permutation.append(a if a >= 0 else a + num_dims)\n+        return jax_sparse.bcoo_transpose(x, permutation=permutation)\n     return jnp.transpose(x, axes=axes)\n \n \n\n@@ -0,0 +1,328 @@\n+import functools\n+\n+import jax.experimental.sparse as jax_sparse\n+import jax.numpy as jnp\n+\n+from keras.utils import jax_utils\n+\n+\n+def axis_shape_dims_for_broadcast_in_dim(axis, input_shape, insert_dims):\n+    \"\"\"Turn the `axis` argument to the arguments needed by `broadcast_in_dim`.\n+\n+    Args:\n+        axis: single int or a tuple of ints for the axis argument. The list of\n+          dimensions to reduce or insert.\n+        input_shape: the shape of the input as a tuple ints.\n+        insert_dims: `False` turns dimensions in `axis` to 1s (use case:\n+          reduction along `axis` with `keep_dims=True`). `True`, inserts 1s\n+          according to `axis` (use case: `expand_dims`).\n+    Returns:\n+        A tuple of three lists\n+        - The canonical value for `axis`: always a list, negative values have\n+          been resolved and values are sorted in ascending order.\n+        - The output shape: `input_shape` with 1s at the indices in `axis`, for\n+          use as the `shape` argument of `broadcast_in_dim`.\n+        - The broadcast dimensions: list of dimensions not in `axis`, for use as\n+          the `broadcast_dimensions` argument of `broadcast_in_dim`.\n+    \"\"\"\n+    if axis is None:\n+        raise ValueError(\"Received `None` value for `axis`\")\n+    if isinstance(axis, int):\n+        axis = (axis,)\n+    # Check uniqueness.\n+    if len(set(axis)) != len(axis):\n+        raise ValueError(f\"Repeated axis in `axis`: {axis}\")\n+    result_dims = len(input_shape)\n+    if insert_dims:\n+        result_dims += len(axis)\n+\n+    # Resolve negative values.\n+    canonical_axis = []\n+    for a in axis:\n+        if not -result_dims <= a < result_dims:\n+            raise ValueError(\n+                f\"In `axis`, axis {a} is out of bounds for array \"\n+                f\"of dimension {result_dims}\"\n+            )\n+        if a < 0:\n+            a = a + result_dims\n+        canonical_axis.append(a)\n+\n+    # Check uniqueness again after resolving negative values.\n+    if len(set(canonical_axis)) != len(canonical_axis):\n+        raise ValueError(f\"Repeated axis in `axis`: {canonical_axis}\")\n+    canonical_axis = sorted(canonical_axis)\n+\n+    # Compute output shape.\n+    output_shape = list(input_shape)\n+    for i in canonical_axis:\n+        if insert_dims:\n+            output_shape.insert(i, 1)\n+        else:\n+            output_shape[i] = 1\n+    broadcast_dims = [i for i in range(result_dims) if i not in canonical_axis]\n+    return canonical_axis, output_shape, broadcast_dims\n+\n+\n+def bcoo_add_indices(x1, x2, sum_duplicates):\n+    \"\"\"Add the indices of `x2` to `x1` with zero values.\n+\n+    Args:\n+        x1: `BCOO` tensor to add indices to.\n+        x2: `BCOO` tensor to take the indices to add to x1.\n+        sum_duplicates: if `True` calls `bcoo_sum_duplicates` on the output.\n+    Returns:\n+        a `BCOO` tensor equal to `x1` but with extra zeros at indices in `x2`\n+        that were missing in `x1`.\n+    \"\"\"\n+    x2_zeros = jnp.zeros(x2.data.shape, x1.data.dtype)\n+    concat_axis = len(x1.indices.shape) - 2\n+    output_indices = jnp.concatenate([x1.indices, x2.indices], axis=concat_axis)\n+    output_data = jnp.concatenate([x1.data, x2_zeros], axis=concat_axis)\n+    output = jax_sparse.BCOO((output_data, output_indices), shape=x1.shape)\n+    if sum_duplicates:\n+        output = jax_sparse.bcoo_sum_duplicates(output)\n+    return output\n+\n+\n+def densifying_unary(func):\n+    \"\"\"Decorator to add support for `JAXSparse` tensors (including `BCOO`) to a\n+    non-zero-preserving element-wise unary operator.\n+\n+    There are requirements on the operator for this decorator to work correctly:\n+\n+    - The operator must be element-wise\n+    - The operator must be unary (one input tensor and one output tensor)\n+    - The operator must return a tensor of the same shape.\n+\n+    Additional arguments to the function (besides the input tensor) are\n+    supported. The returned result is a dense tensor.\n+\n+    Args:\n+        func: The unary operator to wrap.\n+    Returns:\n+        Wrapped function that supports `JAXSparse` tensors.\n+    \"\"\"\n+\n+    @functools.wraps(func)\n+    def sparse_wrapper(x, *args, **kwargs):\n+        if isinstance(x, jax_sparse.JAXSparse):\n+            x = x.todense()\n+        return func(x, *args, **kwargs)\n+\n+    return sparse_wrapper\n+\n+\n+def elementwise_unary(linear):\n+    \"\"\"Decorator to add support for `BCOO` sparse tensors to a zero-preserving\n+    element-wise unary operator.\n+\n+    There are requirements on the operator for this decorator to work correctly:\n+\n+    - The operator must be element-wise\n+    - The operator must be unary (one input tensor and one output tensor)\n+    - The operator must return a tensor of the same shape, and if it is a\n+      `BCOO` tensor, the indices of the result must be the same. Therefore:\n+        - Reduction operations are not supported (e.g. `mean`).\n+        - Operations for which the result may be dense (e.g. `reciprocal`), or\n+          the sparse indices depend on the inputs are not supported (e.g.\n+          `clip`). This implies that `func(0)` must be 0.\n+\n+    Additional arguments to the function (besides the input tensor) are\n+    supported as long as they cannot change the indices of the result. For\n+    instance,`round` is supported, but `clip` is not supported as\n+    `clip(x, 1.0, 2.0)` would always return a dense tensor.\n+\n+    Note that if an input sparse tensor contains zero values, the indices and\n+    the zero values are preserved.\n+\n+    Args:\n+        linear: if `True`, means that the operation is such that\n+            `op(a + b) == op(a) + op(b)`.\n+    Returns:\n+        Wrapped function that supports `BCOO` sparse tensors.\n+    \"\"\"\n+\n+    def wrap_elementwise_unary(func):\n+        @functools.wraps(func)\n+        def sparse_wrapper(x, *args, **kwargs):\n+            if isinstance(x, jax_sparse.BCOO):\n+                if not linear and not x.unique_indices:\n+                    x = jax_sparse.bcoo_sum_duplicates(x)\n+                return jax_sparse.BCOO(\n+                    (func(x.data, *args, **kwargs), x.indices), shape=x.shape\n+                )\n+            else:\n+                return func(x, *args, **kwargs)\n+\n+        return sparse_wrapper\n+\n+    return wrap_elementwise_unary\n+\n+\n+def elementwise_binary_union(linear, use_sparsify):\n+    \"\"\"Decorator to add support for `JAXSparse` tensors (including `BCOO`) to an\n+    element-wise binary operator such that the indices present in the result are\n+    are the union of the indices in the two operand.\n+\n+    The primary use case for this is the `add` and `subtract` operators.\n+\n+    There are requirements on the operator for this decorator to work correctly:\n+\n+    - The operator must be element-wise.\n+    - The operator must be binary (two input tensors and one output tensor).\n+    - Both inputs must be of the same shape or one input must be a scalar.\n+    - The output must be of the same shape as the (non scalar) inputs.\n+    - The indices of the output must be the union of the indices of the inputs.\n+      This implies that func(0, 0) must be 0. As a result, if one operand is\n+      dense or a scalar, then the result will be dense.\n+\n+    Additional arguments to the function (besides the input tensors) are not\n+    supported.\n+\n+    Note that if the result of the operation is zero at some indices, including\n+    because the operands were zero at these indices, the zeros and indices are\n+    preserved.\n+\n+    The `BCOO` format is the only supported one in all cases. Other formats are\n+    not supported when `use_sparsify` is `False`.\n+\n+    Args:\n+        use_sparsify: indicates that the JAX `sparsify` transform supports this\n+            operation.\n+        linear: if `True`, mean that the operation is such that\n+            `op(a + b, c) == op(a, c) + op(b, c)` and\n+            `op(a, c + d) == op(a, c) + op(a, d)`.\n+    Returns:\n+        Wrapped function that supports `JAXSparse`.\n+    \"\"\"\n+\n+    def wrap_elementwise_binary_union(func):\n+        sparse_func = jax_sparse.sparsify(func) if use_sparsify else None\n+\n+        @functools.wraps(func)\n+        def sparse_wrapper(x1, x2):\n+            if isinstance(x1, jax_sparse.JAXSparse):\n+                if isinstance(x2, jax_sparse.JAXSparse):\n+                    # x1 and x2 are sparse.\n+                    # The way we use `sparsify` it cannot know that the indices\n+                    # are the same, so we optimize this case here.\n+                    if (\n+                        x1.indices is x2.indices\n+                        and isinstance(x1, jax_sparse.BCOO)\n+                        and isinstance(x2, jax_sparse.BCOO)\n+                    ):\n+                        if not linear and not x1.unique_indices:\n+                            x1 = jax_sparse.bcoo_sum_duplicates(x1)\n+                            x2 = jax_sparse.bcoo_sum_duplicates(x2)\n+                        return jax_sparse.BCOO(\n+                            (func(x1.data, x2.data), x1.indices),\n+                            shape=x1.shape,\n+                            indices_sorted=x1.indices_sorted,\n+                            unique_indices=x1.unique_indices,\n+                        )\n+                    elif use_sparsify:\n+                        return sparse_func(x1, x2)\n+                    elif isinstance(x1, jax_sparse.BCOO) and isinstance(\n+                        x2, jax_sparse.BCOO\n+                    ):\n+                        x1 = bcoo_add_indices(x1, x2, sum_duplicates=not linear)\n+                        x2 = bcoo_add_indices(x2, x1, sum_duplicates=not linear)\n+                        return jax_sparse.BCOO(\n+                            (func(x1.data, x2.data), x1.indices),\n+                            shape=x1.shape,\n+                            indices_sorted=True,\n+                            unique_indices=True,\n+                        )\n+                    else:\n+                        ValueError(\n+                            \"Unsupported sparse format: \"\n+                            f\"{x1.__class__} and {x2.__class__}\"\n+                        )\n+                else:\n+                    # x1 is sparse, x2 is dense, densify x2.\n+                    x1 = x1.todense()\n+            elif isinstance(x2, jax_sparse.JAXSparse):\n+                # x1 is dense, x2 is sparse, densify x2.\n+                x2 = x2.todense()\n+            return func(x1, x2)\n+\n+        return sparse_wrapper\n+\n+    return wrap_elementwise_binary_union\n+\n+\n+def elementwise_division(func):\n+    \"\"\"Decorator to add support for `BCOO` sparse tensors to element-wise binary\n+    division and related operators.\n+\n+    This decorator is designed for operations related to the division of two\n+    two operands (e.g. `divide`). It accepts `BCOO` tensors for both the\n+    dividend and the divisor, but handles them differently based on whether they\n+    are the dividend or the divisor.\n+\n+    - If the divisor is sparse, it is densified and the result is dense because\n+      the result contains Inf or Nan outside of the indices of the dividend.\n+    - If the dividend is sparse and the divisor is dense, it finds occurrences\n+      of zeros and NaNs in the divisor. The result may therefore have more\n+      indices than there were in the dividend to return correct values where the\n+      divisor was zero or NaN.\n+    - If the dividend is sparse and the divisor is a scalar, it does the\n+      division element-wise. Note that the result is incorrectly sparse if the\n+      scalar divisor is zero.\n+\n+    Args:\n+        func: The function to wrap.\n+    Returns:\n+        Wrapped function that supports `BCOO` sparse tensors.\n+    \"\"\"\n+    sparse_func = jax_sparse.sparsify(func)\n+\n+    @functools.wraps(func)\n+    def sparse_wrapper(x1, x2):\n+        if isinstance(x1, jax_sparse.JAXSparse):\n+            if isinstance(x2, jax_sparse.JAXSparse):\n+                # x1 is sparse and x2 is sparse.\n+                # Divisor is sparse, meaning we're doing divisions by zero\n+                # outside of x2.indices, so the result is dense. Densify both.\n+                x1 = x1.todense()\n+                x2 = x2.todense()\n+            elif isinstance(x1, jax_sparse.BCOO):\n+                if not hasattr(x2, \"shape\") or len(x2.shape) == 0:\n+                    # x1 is sparse BCOO, x2 is scalar, apply func element-wise.\n+                    return jax_sparse.BCOO(\n+                        (func(x1.data, x2), x1.indices),\n+                        shape=x1.shape,\n+                        indices_sorted=x1.indices_sorted,\n+                        unique_indices=x1.unique_indices,\n+                    )\n+                else:\n+                    # x1 is sparse BCOO, x2 is dense.\n+                    if not jax_utils.is_in_jax_tracing_scope(x2):\n+                        # Find zeros and nans in x2 and add indices to x1.\n+                        # 1. Create a dense mask for zeros and nans.\n+                        x2_zeros_and_nans = jnp.equal(x2, 0)\n+                        if not jnp.issubdtype(x2.dtype, jnp.integer):\n+                            x2_zeros_and_nans = jnp.logical_or(\n+                                x2_zeros_and_nans, jnp.isnan(x2)\n+                            )\n+                        # 2. Make it a BCOO of True values.\n+                        x2_zeros_and_nans = jax_sparse.bcoo_fromdense(\n+                            x2_zeros_and_nans,\n+                            n_batch=x1.n_batch,\n+                            n_dense=x1.n_dense,\n+                            index_dtype=x1.indices.dtype,\n+                        )\n+                        # 3. Add the indices to x1.\n+                        x1 = bcoo_add_indices(\n+                            x1, x2_zeros_and_nans, sum_duplicates=True\n+                        )\n+                    return sparse_func(x1, x2)\n+            else:\n+                raise ValueError(f\"Unsupported sparse format: {x1.__class__}\")\n+        elif isinstance(x2, jax_sparse.JAXSparse):\n+            # x1 is dense, x2 is sparse, densify x2\n+            x2 = x2.todense()\n+        return func(x1, x2)\n+\n+    return sparse_wrapper\n\n@@ -1716,11 +1716,14 @@ def sqrt(x):\n def squeeze(x, axis=None):\n     if isinstance(x, tf.SparseTensor):\n         static_shape = x.shape.as_list()\n-        if axis is not None and static_shape[axis] != 1:\n+        if axis is not None:\n+            if static_shape[axis] != 1:\n                 raise ValueError(\n                     f\"Cannot squeeze axis {axis}, because the \"\n                     \"dimension is not 1.\"\n                 )\n+            if axis < 0:\n+                axis += len(static_shape)\n         dynamic_shape = tf.shape(x)\n         new_shape = []\n         gather_indices = []\n\n@@ -63,7 +63,7 @@ class ComputeOutputSpecTest(unittest.TestCase):\n             self.assertEqual(y.shape, (None, 3, 3))\n             self.assertTrue(y.sparse)\n \n-        def three_args_2_kwarg_sparse_fn(x1, x2, x3=None):\n+        def three_args_sparse_fn(x1, x2, x3=None):\n             y0 = ops.add(x1, x2)  # sparse, sparse\n             y1 = ops.concatenate([x1, x2], axis=0)  # sparse, sparse\n             y2 = ops.divide(x1, x3)  # sparse, dense\n@@ -75,9 +75,7 @@ class ComputeOutputSpecTest(unittest.TestCase):\n         x1 = KerasTensor(shape=(None, 3, 3), sparse=True)\n         x2 = KerasTensor(shape=(None, 3, 3), sparse=True)\n         x3 = KerasTensor(shape=(None, 3, 3), sparse=False)\n-        ys = backend.compute_output_spec(\n-            three_args_2_kwarg_sparse_fn, x1, x2, x3=x3\n-        )\n+        ys = backend.compute_output_spec(three_args_sparse_fn, x1, x2, x3=x3)\n         for y in ys:\n             self.assertEqual(y.shape, (None, 3, 3))\n             self.assertTrue(y.sparse)\n@@ -97,23 +95,20 @@ class ComputeOutputSpecTest(unittest.TestCase):\n             self.assertEqual(y.shape, (None, 3, 3))\n             self.assertFalse(y.sparse)\n \n-        def three_args_2_kwarg_dense_fn(x1, x2, x3=None):\n-            y0 = ops.add(x1, x3)  # sparse, dense\n-            y1 = ops.add(x3, x1)  # dense, sparse\n-            y2 = ops.concatenate([x1, x3], axis=0)  # sparse, dense\n-            y3 = ops.matmul(x1, x3)  # sparse, dense\n-            y4 = ops.matmul(x3, x1)  # dense, sparse\n-            indices = KerasTensor((3,), \"int64\", sparse=True)\n-            y5 = ops.take(x3, indices=indices, axis=1)  # dense, sparse\n-            y6 = ops.divide(x1, x2)\n+        def three_args_dense_fn(x1, x2, x3=None):\n+            y0 = ops.add(x1, x2)  # sparse, dense\n+            y1 = ops.add(x2, x1)  # dense, sparse\n+            y2 = ops.concatenate([x1, x2], axis=0)  # sparse, dense\n+            y3 = ops.matmul(x1, x2)  # sparse, dense\n+            y4 = ops.matmul(x2, x1)  # dense, sparse\n+            y5 = ops.take(x2, indices=x3, axis=1)  # dense, sparse\n+            y6 = ops.divide(x1, x1)  # sparse, sparse\n             return (y0, y1, y2, y3, y4, y5, y6)\n \n         x1 = KerasTensor(shape=(None, 3, 3), sparse=True)\n-        x2 = KerasTensor(shape=(None, 3, 3), sparse=True)\n-        x3 = KerasTensor(shape=(None, 3, 3), sparse=False)\n-        ys = backend.compute_output_spec(\n-            three_args_2_kwarg_dense_fn, x1, x2, x3=x3\n-        )\n+        x2 = KerasTensor(shape=(None, 3, 3), sparse=False)\n+        x3 = KerasTensor(shape=(3,), dtype=\"int64\", sparse=True)\n+        ys = backend.compute_output_spec(three_args_dense_fn, x1, x2, x3=x3)\n         for y in ys:\n             self.assertEqual(y.shape, (None, 3, 3))\n             self.assertFalse(y.sparse)\n\n@@ -7,6 +7,7 @@ from keras import backend\n from keras import constraints\n from keras import layers\n from keras import models\n+from keras import ops\n from keras import saving\n from keras import testing\n from keras.backend.common import keras_tensor\n@@ -110,17 +111,36 @@ class DenseTest(testing.TestCase):\n         inputs = 4 * backend.random.uniform((10, 10))\n         inputs = tf.sparse.from_dense(tf.nn.dropout(inputs, 0.8))\n \n-        layer = layers.Dense(units=5)\n+        inputs = np.random.random((10, 10)).astype(\"float32\")\n+        inputs = np.multiply(inputs, inputs >= 0.8)\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            inputs = tf.sparse.from_dense(inputs)\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            inputs = jax_sparse.BCOO.fromdense(inputs)\n+        else:\n+            self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n+\n+        layer = layers.Dense(units=10)\n         outputs = layer(inputs)\n \n         # Verify the computation is the same as if it had been a dense tensor\n-        expected_outputs = tf.add(\n-            tf.matmul(tf.sparse.to_dense(inputs), layer.kernel),\n+        expected_outputs = ops.add(\n+            ops.matmul(\n+                backend.convert_to_tensor(inputs, sparse=False), layer.kernel\n+            ),\n             layer.bias,\n         )\n         self.assertAllClose(outputs, expected_outputs)\n \n         # Verify the gradient is sparse\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n             with tf.GradientTape() as g:\n                 outputs = layer(inputs)\n \n\n@@ -70,14 +70,21 @@ class EmbeddingTest(test_case.TestCase):\n         reason=\"Backend does not support sparse tensors.\",\n     )\n     def test_correctness_sparse(self):\n-        import tensorflow as tf\n-\n         layer = layers.Embedding(input_dim=3, output_dim=2)\n         layer.build()\n         layer.embeddings.assign(np.array([[0.0, 0.0], [2.0, 2.0], [3.0, 3.0]]))\n-        x = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]], values=[2, 1], dense_shape=(2, 3)\n-        )\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            x = tf.SparseTensor([[0, 0], [1, 2]], [2, 1], (2, 3))\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            x = jax_sparse.BCOO(([2, 1], [[0, 0], [1, 2]]), shape=(2, 3))\n+        else:\n+            self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n+\n         self.assertAllClose(\n             layer(x),\n             np.array(\n\n@@ -1,4 +1,7 @@\n+import tree\n+\n from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n from keras.layers.layer import Layer\n \n \n@@ -16,3 +19,12 @@ class Identity(Layer):\n \n     def call(self, inputs):\n         return inputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def compute_output_spec(self, inputs):\n+        return tree.map_structure(\n+            lambda x: KerasTensor(x.shape, dtype=x.dtype, sparse=x.sparse),\n+            inputs,\n+        )\n\n@@ -85,12 +85,12 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n         init_kwargs={},\n         input_shape=(2, 4, 5),\n         expected_output_shape=(2, 4, 5),\n-        **kwargs\n+        **kwargs,\n     ):\n         self.run_layer_test(\n             layer_class,\n             init_kwargs=init_kwargs,\n-            input_shape=[input_shape, input_shape],\n+            input_shape=(input_shape, input_shape),\n             expected_output_shape=expected_output_shape,\n             expected_num_trainable_weights=0,\n             expected_num_non_trainable_weights=0,\n@@ -181,7 +181,7 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n         init_kwargs={},\n         input_shape=(2, 4, 5),\n         skip_mask_test=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         if skip_mask_test:\n             pytest.skip(\"Masking not supported\")\n@@ -246,10 +246,8 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n         init_kwargs={},\n         input_shape=(2, 4, 5),\n         expected_output_shape=(2, 4, 5),\n-        **kwargs\n+        **kwargs,\n     ):\n-        import tensorflow as tf\n-\n         self.run_layer_test(\n             layer_class,\n             init_kwargs=init_kwargs,\n@@ -270,19 +268,29 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n \n         # Merging a sparse tensor with a dense tensor, or a dense tensor with a\n         # sparse tensor produces a dense tensor\n-        x1 = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n-        )\n-        x1_np = tf.sparse.to_dense(x1).numpy()\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            x1 = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n+            x3 = tf.SparseTensor([[0, 0], [1, 1]], [4.0, 5.0], (2, 3))\n+            sparse_class = tf.SparseTensor\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            # Use n_batch of 1 to be compatible with all ops.\n+            x1 = jax_sparse.BCOO(([[1.0, 2.0]], [[[0], [2]]]), shape=(2, 3))\n+            x3 = jax_sparse.BCOO(([[4.0, 5.0]], [[[0], [1]]]), shape=(2, 3))\n+            sparse_class = jax_sparse.JAXSparse\n+        else:\n+            self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n+\n+        x1_np = backend.convert_to_numpy(x1)\n         x2 = np.random.rand(2, 3)\n         self.assertAllClose(layer([x1, x2]), np_op(x1_np, x2, **init_kwargs))\n         self.assertAllClose(layer([x2, x1]), np_op(x2, x1_np, **init_kwargs))\n \n         # Merging a sparse tensor with a sparse tensor produces a sparse tensor\n-        x3 = tf.SparseTensor(\n-            indices=[[0, 0], [1, 1]], values=[4.0, 5.0], dense_shape=(2, 3)\n-        )\n-        x3_np = tf.sparse.to_dense(x3).numpy()\n+        x3_np = backend.convert_to_numpy(x3)\n \n-        self.assertIsInstance(layer([x1, x3]), tf.SparseTensor)\n+        self.assertIsInstance(layer([x1, x3]), sparse_class)\n         self.assertAllClose(layer([x1, x3]), np_op(x1_np, x3_np, **init_kwargs))\n\n@@ -30,13 +30,23 @@ class FlattenTest(testing.TestCase, parameterized.TestCase):\n             np.reshape(np.transpose(inputs, (0, 2, 3, 1)), (-1, 5 * 5 * 3))\n         )\n         if sparse:\n+            if backend.backend() == \"tensorflow\":\n                 import tensorflow as tf\n \n-            inputs = tf.sparse.from_dense(inputs)\n-            expected_output_channels_last = tf.sparse.from_dense(\n+                dense_to_sparse = tf.sparse.from_dense\n+            elif backend.backend() == \"jax\":\n+                import jax.experimental.sparse as jax_sparse\n+\n+                dense_to_sparse = jax_sparse.BCOO.fromdense\n+            else:\n+                self.fail(\n+                    f\"Sparse is unsupported with backend {backend.backend()}\"\n+                )\n+            inputs = dense_to_sparse(inputs)\n+            expected_output_channels_last = dense_to_sparse(\n                 expected_output_channels_last\n             )\n-            expected_output_channels_first = tf.sparse.from_dense(\n+            expected_output_channels_first = dense_to_sparse(\n                 expected_output_channels_first\n             )\n \n\n@@ -27,10 +27,20 @@ class PermuteTest(testing.TestCase, parameterized.TestCase):\n             np.transpose(inputs, axes=(0, 3, 1, 2))\n         )\n         if sparse:\n+            if backend.backend() == \"tensorflow\":\n                 import tensorflow as tf\n \n                 inputs = tf.sparse.from_dense(inputs)\n                 expected_output = tf.sparse.from_dense(expected_output)\n+            elif backend.backend() == \"jax\":\n+                import jax.experimental.sparse as jax_sparse\n+\n+                inputs = jax_sparse.BCOO.fromdense(inputs)\n+                expected_output = jax_sparse.BCOO.fromdense(expected_output)\n+            else:\n+                self.fail(\n+                    f\"Backend {backend.backend()} does not support sparse\"\n+                )\n \n         self.run_layer_test(\n             layers.Permute,\n\n@@ -286,11 +286,17 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         reason=\"Backend does not support sparse tensors.\",\n     )\n     def test_shape_sparse(self):\n+        if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n-        x = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n-        )\n+            x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            x = jax_sparse.BCOO(([1.0, 2.0], [[0, 0], [1, 2]]), shape=(2, 3))\n+        else:\n+            self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n+\n         self.assertAllEqual(core.shape(x), (2, 3))\n \n     def test_convert_to_tensor(self):\n@@ -319,20 +325,27 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         reason=\"Backend does not support sparse tensors.\",\n     )\n     def test_convert_to_tensor_sparse(self):\n+        if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n-        x = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n-        )\n+            x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n+            sparse_class = tf.SparseTensor\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            x = jax_sparse.BCOO(([1.0, 2.0], [[0, 0], [1, 2]]), shape=(2, 3))\n+            sparse_class = jax_sparse.JAXSparse\n+        else:\n+            self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n \n         x_default = ops.convert_to_tensor(x)\n-        self.assertIsInstance(x_default, tf.SparseTensor)\n+        self.assertIsInstance(x_default, sparse_class)\n         self.assertAllClose(x, x_default)\n         x_sparse = ops.convert_to_tensor(x, sparse=True)\n-        self.assertIsInstance(x_sparse, tf.SparseTensor)\n+        self.assertIsInstance(x_sparse, sparse_class)\n         self.assertAllClose(x, x_sparse)\n         x_dense = ops.convert_to_tensor(x, sparse=False)\n-        self.assertNotIsInstance(x_dense, tf.SparseTensor)\n+        self.assertNotIsInstance(x_dense, sparse_class)\n         self.assertAllClose(x, x_dense)\n \n         x_numpy = ops.convert_to_numpy(x)\n\n@@ -5776,9 +5776,11 @@ class Mean(Operation):\n             result_dtype = compute_dtype\n         else:\n             result_dtype = ori_dtype\n+        sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(\n             reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n             dtype=result_dtype,\n+            sparse=sparse,\n         )\n \n \n\n@@ -1,4 +1,5 @@\n import contextlib\n+import functools\n import itertools\n import math\n \n@@ -1299,13 +1300,6 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n         self.assertEqual(knp.reshape(x, (3, -1)).shape, (3, None))\n \n-    def test_reshape_sparse(self):\n-        x = KerasTensor((None, 3), sparse=True)\n-        self.assertTrue(knp.reshape(x, (3, 2)).sparse)\n-        self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n-        self.assertTrue(knp.reshape(x, (3, -1)).sparse)\n-        self.assertEqual(knp.reshape(x, (3, -1)).shape, (3, None))\n-\n     def test_roll(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.roll(x, 1).shape, (None, 3))\n@@ -1467,29 +1461,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             knp.squeeze(x, axis=0)\n \n-    def test_squeeze_sparse(self):\n-        x = KerasTensor((2, 3), sparse=True)\n-        self.assertTrue(knp.squeeze(x).sparse)\n-        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n-\n-        x = KerasTensor((2, 1, 3), sparse=True)\n-        self.assertTrue(knp.squeeze(x).sparse)\n-        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n-        self.assertTrue(knp.squeeze(x, axis=1).sparse)\n-        self.assertEqual(knp.squeeze(x, axis=1).shape, (2, 3))\n-        self.assertTrue(knp.squeeze(x, axis=-2).sparse)\n-        self.assertEqual(knp.squeeze(x, axis=-2).shape, (2, 3))\n-\n     def test_transpose(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.transpose(x).shape, (3, 2))\n \n-    def test_transpose_sparse(self):\n-        x = KerasTensor((2, 3), sparse=True)\n-        result = knp.transpose(x)\n-        self.assertEqual(result.shape, (3, 2))\n-        self.assertTrue(result.sparse)\n-\n     def test_arccos(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.arccos(x).shape, (2, 3))\n@@ -1657,15 +1632,6 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knp.expand_dims(x, 1).shape, (2, 1, 3, 4))\n         self.assertEqual(knp.expand_dims(x, -2).shape, (2, 3, 1, 4))\n \n-    def test_expand_dims_sparse(self):\n-        x = KerasTensor((2, 3, 4), sparse=True)\n-        self.assertTrue(knp.expand_dims(x, 0).sparse)\n-        self.assertEqual(knp.expand_dims(x, 0).shape, (1, 2, 3, 4))\n-        self.assertTrue(knp.expand_dims(x, 1).sparse)\n-        self.assertEqual(knp.expand_dims(x, 1).shape, (2, 1, 3, 4))\n-        self.assertTrue(knp.expand_dims(x, -2).sparse)\n-        self.assertEqual(knp.expand_dims(x, -2).shape, (2, 3, 1, 4))\n-\n     def test_expm1(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.expm1(x).shape, (2, 3))\n@@ -1825,25 +1791,6 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knp.reshape(x, (6,)).shape, (6,))\n         self.assertEqual(knp.reshape(x, (-1,)).shape, (6,))\n \n-    def test_reshape_sparse(self):\n-        x = KerasTensor((2, 3), sparse=True)\n-\n-        result = knp.reshape(x, (3, 2))\n-        self.assertEqual(result.shape, (3, 2))\n-        self.assertTrue(result.sparse)\n-\n-        result = knp.reshape(x, (3, -1))\n-        self.assertEqual(result.shape, (3, 2))\n-        self.assertTrue(result.sparse)\n-\n-        result = knp.reshape(x, (6,))\n-        self.assertEqual(result.shape, (6,))\n-        self.assertTrue(result.sparse)\n-\n-        result = knp.reshape(x, (-1,))\n-        self.assertEqual(result.shape, (6,))\n-        self.assertTrue(result.sparse)\n-\n     def test_roll(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.roll(x, 1).shape, (2, 3))\n@@ -2022,30 +1969,41 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         reason=\"Backend does not support sparse tensors.\",\n     )\n     def test_matmul_sparse(self, dtype, x_shape, y_shape, x_sparse, y_sparse):\n+        if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n             if x_sparse and y_sparse and dtype in (\"float16\", \"int32\"):\n-            pytest.skip(f\"Sparse sparse matmul unsupported for {dtype}\")\n+                pytest.skip(\n+                    f\"Sparse sparse matmul unsupported for {dtype}\"\n+                    \" with TensorFlow backend\"\n+                )\n+\n+            dense_to_sparse = tf.sparse.from_dense\n+            sparse_class = tf.SparseTensor\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            dense_to_sparse = functools.partial(\n+                jax_sparse.BCOO.fromdense, n_batch=len(x_shape) - 2\n+            )\n+            sparse_class = jax_sparse.JAXSparse\n \n         rng = np.random.default_rng(0)\n-        if x_sparse:\n-            x_np = (4 * rng.standard_normal(x_shape)).astype(dtype)\n-            x_np = np.multiply(x_np, rng.random(x_shape) < 0.7)\n-            x = tf.sparse.from_dense(x_np)\n-        else:\n+\n         x = x_np = (4 * rng.standard_normal(x_shape)).astype(dtype)\n+        if x_sparse:\n+            x_np = np.multiply(x_np, rng.random(x_shape) < 0.7)\n+            x = dense_to_sparse(x_np)\n+\n         y = y_np = (4 * rng.standard_normal(y_shape)).astype(dtype)\n         if y_sparse:\n-            y_np = (4 * rng.standard_normal(y_shape)).astype(dtype)\n             y_np = np.multiply(y_np, rng.random(y_shape) < 0.7)\n-            y = tf.sparse.from_dense(y_np)\n-        else:\n-            y = y_np = (4 * rng.standard_normal(y_shape)).astype(dtype)\n+            y = dense_to_sparse(y_np)\n \n         atol = 0.1 if dtype == \"float16\" else 1e-4\n         self.assertAllClose(knp.matmul(x, y), np.matmul(x_np, y_np), atol=atol)\n         if x_sparse and y_sparse:\n-            self.assertIsInstance(knp.matmul(x, y), tf.SparseTensor)\n+            self.assertIsInstance(knp.matmul(x, y), sparse_class)\n \n     def test_power(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n@@ -2507,16 +2465,21 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         reason=\"Backend does not support sparse tensors.\",\n     )\n     def test_take_sparse(self, dtype, axis):\n-        import tensorflow as tf\n-\n         rng = np.random.default_rng(0)\n         x = (4 * rng.standard_normal((3, 4, 5))).astype(dtype)\n-        indices = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=(2, 3)\n-        )\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            indices = tf.SparseTensor([[0, 0], [1, 2]], [1, 2], (2, 3))\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            indices = jax_sparse.BCOO(([1, 2], [[0, 0], [1, 2]]), shape=(2, 3))\n+\n         self.assertAllClose(\n             knp.take(x, indices, axis=axis),\n-            np.take(x, tf.sparse.to_dense(indices).numpy(), axis=axis),\n+            np.take(x, backend.convert_to_numpy(indices), axis=axis),\n         )\n \n     def test_take_along_axis(self):\n@@ -2642,36 +2605,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.array([65504, 65504, 65504], dtype=\"float16\")\n         self.assertAllClose(knp.mean(x), np.mean(x))\n \n-    @parameterized.product(\n-        axis=[None, (), 0, 1, 2, -1, -2, -3, (0, 1), (1, 2), (0, 2), (0, 1, 2)],\n-        keepdims=[True, False],\n-    )\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"IndexedSlices are only supported with TensorFlow backend.\",\n-    )\n-    def test_mean_indexed_slices(self, axis, keepdims):\n-        import tensorflow as tf\n-\n-        x = tf.IndexedSlices(\n-            [\n-                [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],\n-                [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],\n-            ],\n-            (0, 2),\n-            (4, 2, 3),\n-        )\n-        x_np = tf.convert_to_tensor(x).numpy()\n-        self.assertAllClose(\n-            knp.mean(x, axis=axis, keepdims=keepdims),\n-            np.mean(x_np, axis=axis, keepdims=keepdims),\n-        )\n-\n-        self.assertAllClose(\n-            knp.Mean(axis=axis, keepdims=keepdims)(x),\n-            np.mean(x_np, axis=axis, keepdims=keepdims),\n-        )\n-\n     def test_all(self):\n         x = np.array([[True, False, True], [True, True, True]])\n         self.assertAllClose(knp.all(x), np.all(x))\n@@ -2812,25 +2745,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Squeeze()(x), np.squeeze(x))\n         self.assertAllClose(knp.Squeeze(axis=0)(x), np.squeeze(x, axis=0))\n \n-    @pytest.mark.skipif(\n-        not backend.SUPPORTS_SPARSE_TENSORS,\n-        reason=\"Backend does not support sparse tensors.\",\n-    )\n-    def test_squeeze_sparse(self):\n-        import tensorflow as tf\n-\n-        x = tf.SparseTensor(\n-            indices=[[0, 0, 0, 0], [0, 2, 0, 4]],\n-            values=[1, 2],\n-            dense_shape=(1, 3, 1, 5),\n-        )\n-        x_np = tf.sparse.to_dense(x).numpy()\n-        self.assertAllClose(knp.squeeze(x), np.squeeze(x_np))\n-        self.assertAllClose(knp.squeeze(x, axis=0), np.squeeze(x_np, axis=0))\n-\n-        self.assertAllClose(knp.Squeeze()(x), np.squeeze(x_np))\n-        self.assertAllClose(knp.Squeeze(axis=0)(x), np.squeeze(x_np, axis=0))\n-\n     def test_transpose(self):\n         x = np.ones([1, 2, 3, 4, 5])\n         self.assertAllClose(knp.transpose(x), np.transpose(x))\n@@ -2845,40 +2759,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.transpose(x, axes=(1, 0, 3, 2, 4)),\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.SUPPORTS_SPARSE_TENSORS,\n-        reason=\"Backend does not support sparse tensors.\",\n-    )\n-    def test_transpose_sparse(self):\n-        import tensorflow as tf\n-\n-        x = tf.SparseTensor(\n-            indices=[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4]],\n-            values=[1, 2],\n-            dense_shape=(1, 2, 3, 4, 5),\n-        )\n-        x_np = tf.sparse.to_dense(x).numpy()\n-\n-        self.assertIsInstance(knp.transpose(x), tf.SparseTensor)\n-        self.assertAllClose(knp.transpose(x), np.transpose(x_np))\n-        self.assertIsInstance(\n-            knp.transpose(x, axes=(1, 0, 3, 2, 4)), tf.SparseTensor\n-        )\n-        self.assertAllClose(\n-            knp.transpose(x, axes=(1, 0, 3, 2, 4)),\n-            np.transpose(x_np, axes=(1, 0, 3, 2, 4)),\n-        )\n-\n-        self.assertIsInstance(knp.Transpose()(x), tf.SparseTensor)\n-        self.assertAllClose(knp.Transpose()(x), np.transpose(x_np))\n-        self.assertIsInstance(\n-            knp.Transpose(axes=(1, 0, 3, 2, 4))(x), tf.SparseTensor\n-        )\n-        self.assertAllClose(\n-            knp.Transpose(axes=(1, 0, 3, 2, 4))(x),\n-            np.transpose(x_np, axes=(1, 0, 3, 2, 4)),\n-        )\n-\n     def test_arccos(self):\n         x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         self.assertAllClose(knp.arccos(x), np.arccos(x))\n@@ -3093,17 +2973,21 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         reason=\"Backend does not support sparse tensors.\",\n     )\n     def test_concatenate_sparse(self, axis):\n+        if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n-        x = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 3)\n-        )\n-        x_np = tf.sparse.to_dense(x).numpy()\n+            x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n+            y = tf.SparseTensor([[0, 0], [1, 1]], [4.0, 5.0], (2, 3))\n+            sparse_class = tf.SparseTensor\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n \n-        y = tf.SparseTensor(\n-            indices=[[0, 0], [1, 1]], values=[4.0, 5.0], dense_shape=(2, 3)\n-        )\n-        y_np = tf.sparse.to_dense(y).numpy()\n+            x = jax_sparse.BCOO(([1.0, 2.0], [[0, 0], [1, 2]]), shape=(2, 3))\n+            y = jax_sparse.BCOO(([4.0, 5.0], [[0, 0], [1, 1]]), shape=(2, 3))\n+            sparse_class = jax_sparse.JAXSparse\n+\n+        x_np = backend.convert_to_numpy(x)\n+        y_np = backend.convert_to_numpy(y)\n         z = np.random.rand(2, 3).astype(\"float32\")\n \n         self.assertAllClose(\n@@ -3132,12 +3016,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.concatenate([x_np, y_np], axis=axis),\n         )\n \n-        self.assertIsInstance(\n-            knp.concatenate([x, y], axis=axis), tf.SparseTensor\n-        )\n-        self.assertIsInstance(\n-            knp.Concatenate(axis=axis)([x, y]), tf.SparseTensor\n-        )\n+        self.assertIsInstance(knp.concatenate([x, y], axis=axis), sparse_class)\n+        self.assertIsInstance(knp.Concatenate(axis=axis)([x, y]), sparse_class)\n \n     def test_conjugate(self):\n         x = np.array([[1 + 2j, 2 + 3j], [3 + 4j, 4 + 5j]])\n@@ -3304,27 +3184,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.ExpandDims(1)(x), np.expand_dims(x, 1))\n         self.assertAllClose(knp.ExpandDims(-2)(x), np.expand_dims(x, -2))\n \n-    @pytest.mark.skipif(\n-        not backend.SUPPORTS_SPARSE_TENSORS,\n-        reason=\"Backend does not support sparse tensors.\",\n-    )\n-    def test_expand_dims_sparse(self):\n-        import tensorflow as tf\n-\n-        x = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]],\n-            values=[1, 2],\n-            dense_shape=(2, 3),\n-        )\n-        x_np = tf.sparse.to_dense(x).numpy()\n-        self.assertAllClose(knp.expand_dims(x, 0), np.expand_dims(x_np, 0))\n-        self.assertAllClose(knp.expand_dims(x, 1), np.expand_dims(x_np, 1))\n-        self.assertAllClose(knp.expand_dims(x, -2), np.expand_dims(x_np, -2))\n-\n-        self.assertAllClose(knp.ExpandDims(0)(x), np.expand_dims(x_np, 0))\n-        self.assertAllClose(knp.ExpandDims(1)(x), np.expand_dims(x_np, 1))\n-        self.assertAllClose(knp.ExpandDims(-2)(x), np.expand_dims(x_np, -2))\n-\n     def test_expm1(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.expm1(x), np.expm1(x))\n@@ -3680,24 +3539,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.Reshape([3, 2])(x), np.reshape(x, [3, 2]))\n         self.assertAllClose(knp.Reshape(-1)(x), np.reshape(x, -1))\n \n-    @pytest.mark.skipif(\n-        not backend.SUPPORTS_SPARSE_TENSORS,\n-        reason=\"Backend does not support sparse tensors.\",\n-    )\n-    def test_reshape_sparse(self):\n-        import tensorflow as tf\n-\n-        x = tf.SparseTensor(\n-            indices=[[0, 0], [1, 2]],\n-            values=[1, 2],\n-            dense_shape=(2, 3),\n-        )\n-        x_np = tf.sparse.to_dense(x).numpy()\n-        self.assertIsInstance(knp.reshape(x, [3, 2]), tf.SparseTensor)\n-        self.assertAllClose(knp.reshape(x, [3, 2]), np.reshape(x_np, [3, 2]))\n-        self.assertIsInstance(knp.Reshape([3, 2])(x), tf.SparseTensor)\n-        self.assertAllClose(knp.Reshape([3, 2])(x), np.reshape(x_np, [3, 2]))\n-\n     def test_roll(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.roll(x, 1), np.roll(x, 1))\n@@ -4052,6 +3893,12 @@ def create_sparse_tensor(x, indices_from=None, start=0, delta=2):\n         import tensorflow as tf\n \n         return tf.SparseTensor(indices, tf.gather_nd(x, indices), x.shape)\n+    elif backend.backend() == \"jax\":\n+        import jax\n+        import jax.experimental.sparse as jax_sparse\n+\n+        values = x[tuple(jax.numpy.moveaxis(indices, -1, 0))]\n+        return jax_sparse.BCOO((values, indices), shape=x.shape)\n \n \n def create_indexed_slices(x, indices_from=None, start=0, delta=2):\n@@ -4063,17 +3910,27 @@ def create_indexed_slices(x, indices_from=None, start=0, delta=2):\n         if indices_from is not None:\n             indices = indices_from.indices\n         return tf.IndexedSlices(tf.gather(x, indices), indices, x.shape)\n+    elif backend.backend() == \"jax\":\n+        import jax\n+        import jax.experimental.sparse as jax_sparse\n+\n+        if indices_from is not None:\n+            indices = indices_from.indices\n+        else:\n+            indices = jax.numpy.expand_dims(indices, axis=1)\n+        values = jax.numpy.take(x, jax.numpy.squeeze(indices, axis=1), axis=0)\n+        return jax_sparse.BCOO((values, indices), shape=x.shape)\n \n \n-def get_sparseness_combinations(sparsify_fn):\n+def get_sparseness_combinations(dense_to_sparse_fn):\n     x = np.array([[1, 2, 3], [3, 2, 1]])\n     y = np.array([[4, 5, 6], [3, 2, 1]])\n     scalar = backend.convert_to_tensor(2)\n-    x_sp = sparsify_fn(x)\n-    y_sp = sparsify_fn(y, indices_from=x_sp)\n-    x_sp_sup = sparsify_fn(x, start=0, delta=1)\n-    y_sp_dis = sparsify_fn(y, start=1)\n-    y_sp_sup = sparsify_fn(y, start=0, delta=1)\n+    x_sp = dense_to_sparse_fn(x)\n+    y_sp = dense_to_sparse_fn(y, indices_from=x_sp)\n+    x_sp_sup = dense_to_sparse_fn(x, start=0, delta=1)\n+    y_sp_dis = dense_to_sparse_fn(y, start=1)\n+    y_sp_sup = dense_to_sparse_fn(y, start=0, delta=1)\n     x = backend.convert_to_tensor(x)\n     y = backend.convert_to_tensor(y)\n     return [\n@@ -4091,6 +3948,11 @@ def get_sparseness_combinations(sparsify_fn):\n def sparseness(x):\n     if isinstance(x, KerasTensor):\n         return \"sparse\" if x.sparse else \"dense\"\n+    elif x.__class__.__name__ == \"BCOO\":\n+        if x.n_dense > 0:\n+            return \"slices\"\n+        else:\n+            return \"sparse\"\n     elif x.__class__.__name__ == \"SparseTensor\":\n         return \"sparse\"\n     elif x.__class__.__name__ == \"IndexedSlices\":\n@@ -4133,6 +3995,10 @@ def division_sparseness(x1, x2):\n     return \"dense\" if x1_sparseness == \"scalar\" else x1_sparseness\n \n \n+def snake_to_pascal_case(name):\n+    return \"\".join(w.capitalize() for w in name.split(\"_\"))\n+\n+\n @pytest.mark.skipif(\n     not backend.SUPPORTS_SPARSE_TENSORS,\n     reason=\"Backend does not support sparse tensors.\",\n@@ -4160,7 +4026,7 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         }\n         for op in DENSIFYING_UNARY_OPS\n     ]\n-    UNARY_OPS = [\n+    ELEMENTWISE_UNARY_OPS = [\n         \"abs\",\n         \"absolute\",\n         \"arcsin\",\n@@ -4186,14 +4052,52 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         \"tan\",\n         \"tanh\",\n     ]\n-    UNARY_OPS_TESTS = [\n+    ELEMENTWISE_UNARY_OPS_TESTS = [\n         {\n             \"testcase_name\": op,\n             \"op_function\": getattr(knp, op),\n-            \"op_class\": getattr(knp, op.capitalize()),\n+            \"op_class\": getattr(knp, snake_to_pascal_case(op)),\n             \"np_op\": getattr(np, op),\n         }\n-        for op in UNARY_OPS\n+        for op in ELEMENTWISE_UNARY_OPS\n+    ]\n+    OTHER_UNARY_OPS_TESTS = [\n+        {\n+            \"testcase_name\": \"_\".join([op, testcase_name]),\n+            \"op_function\": getattr(knp, op),\n+            \"op_class\": getattr(knp, snake_to_pascal_case(op)),\n+            \"np_op\": getattr(np, op),\n+            \"op_kwargs\": op_kwargs,\n+            \"input_shape\": input_shape,\n+        }\n+        for op, testcase_name, op_kwargs, input_shape in [\n+            (\"mean\", \"none\", {\"axis\": None}, (4, 2, 3)),\n+            (\"mean\", \"none_k\", {\"axis\": None, \"keepdims\": True}, (4, 2, 3)),\n+            (\"mean\", \"empty\", {\"axis\": ()}, (4, 2, 3)),\n+            (\"mean\", \"empty_k\", {\"axis\": (), \"keepdims\": True}, (4, 2, 3)),\n+            (\"mean\", \"0\", {\"axis\": 0}, (4, 2, 3)),\n+            (\"mean\", \"0_k\", {\"axis\": 0, \"keepdims\": True}, (4, 2, 3)),\n+            (\"mean\", \"1\", {\"axis\": 1}, (4, 2, 3)),\n+            (\"mean\", \"1_k\", {\"axis\": 1, \"keepdims\": True}, (4, 2, 3)),\n+            (\"mean\", \"01\", {\"axis\": (0, 1)}, (4, 2, 3)),\n+            (\"mean\", \"01_k\", {\"axis\": (0, 1), \"keepdims\": True}, (4, 2, 3)),\n+            (\"mean\", \"02\", {\"axis\": (1, 2)}, (4, 2, 3)),\n+            (\"mean\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, (4, 2, 3)),\n+            (\"mean\", \"all\", {\"axis\": (0, 1, 2)}, (4, 2, 3)),\n+            (\"mean\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, (4, 2, 3)),\n+            (\"expand_dims\", \"zero\", {\"axis\": 0}, (2, 3)),\n+            (\"expand_dims\", \"one\", {\"axis\": 1}, (2, 3)),\n+            (\"expand_dims\", \"minus_two\", {\"axis\": -2}, (2, 3)),\n+            (\"reshape\", \"basic\", {\"newshape\": (4, 3, 2)}, (4, 2, 3)),\n+            (\"reshape\", \"minus_one\", {\"newshape\": (4, 3, -1)}, (4, 2, 3)),\n+            (\"reshape\", \"fewer_dims\", {\"newshape\": (4, 6)}, (4, 2, 3)),\n+            (\"squeeze\", \"no_axis_no_op\", {}, (2, 3)),\n+            (\"squeeze\", \"one\", {\"axis\": 1}, (2, 1, 3)),\n+            (\"squeeze\", \"minus_two\", {\"axis\": -2}, (2, 1, 3)),\n+            (\"squeeze\", \"no_axis\", {}, (2, 1, 3)),\n+            (\"transpose\", \"no_axes\", {}, (1, 2, 3, 4)),\n+            (\"transpose\", \"axes\", {\"axes\": (0, 3, 2, 1)}, (1, 2, 3, 4)),\n+        ]\n     ]\n \n     BINARY_OPS = [\n@@ -4209,9 +4113,7 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         {\n             \"testcase_name\": op,\n             \"op_function\": getattr(knp, op),\n-            \"op_class\": getattr(\n-                knp, \"\".join(w.capitalize() for w in op.split(\"_\"))\n-            ),\n+            \"op_class\": getattr(knp, snake_to_pascal_case(op)),\n             \"np_op\": getattr(np, op),\n             \"op_sparseness\": op_sparseness,\n         }\n@@ -4224,22 +4126,53 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n     def assertSparseness(self, x, expected_sparseness):\n         self.assertEquals(sparseness(x), expected_sparseness)\n \n-    @parameterized.named_parameters(UNARY_OPS_TESTS)\n-    def test_unary_symbolic_static_shape(self, op_function, op_class, np_op):\n+    @parameterized.named_parameters(ELEMENTWISE_UNARY_OPS_TESTS)\n+    def test_elementwise_unary_symbolic_static_shape(\n+        self, op_function, op_class, np_op\n+    ):\n         x = KerasTensor([2, 3], sparse=True)\n         self.assertEqual(op_function(x).shape, (2, 3))\n         self.assertTrue(op_function(x).sparse)\n         self.assertEqual(op_class()(x).shape, (2, 3))\n         self.assertTrue(op_class()(x).sparse)\n \n-    @parameterized.named_parameters(UNARY_OPS_TESTS)\n-    def test_unary_symbolic_dynamic_shape(self, op_function, op_class, np_op):\n+    @parameterized.named_parameters(ELEMENTWISE_UNARY_OPS_TESTS)\n+    def test_elementwise_unary_symbolic_dynamic_shape(\n+        self, op_function, op_class, np_op\n+    ):\n         x = KerasTensor([None, 3], sparse=True)\n         self.assertEqual(op_function(x).shape, (None, 3))\n         self.assertTrue(op_function(x).sparse)\n         self.assertEqual(op_class()(x).shape, (None, 3))\n         self.assertTrue(op_class()(x).sparse)\n \n+    @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n+    def test_other_unary_symbolic_static_shape(\n+        self, op_function, op_class, np_op, op_kwargs, input_shape\n+    ):\n+        expected_shape = op_function(\n+            KerasTensor(input_shape), **op_kwargs\n+        ).shape\n+        x = KerasTensor(input_shape, sparse=True)\n+        self.assertEqual(op_function(x, **op_kwargs).shape, expected_shape)\n+        self.assertTrue(op_function(x, **op_kwargs).sparse)\n+        self.assertEqual(op_class(**op_kwargs)(x).shape, expected_shape)\n+        self.assertTrue(op_class(**op_kwargs)(x).sparse)\n+\n+    @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n+    def test_other_unary_symbolic_dynamic_shape(\n+        self, op_function, op_class, np_op, op_kwargs, input_shape\n+    ):\n+        input_shape = (None,) + input_shape[1:]\n+        expected_shape = op_function(\n+            KerasTensor(input_shape), **op_kwargs\n+        ).shape\n+        x = KerasTensor(input_shape, sparse=True)\n+        self.assertEqual(op_function(x, **op_kwargs).shape, expected_shape)\n+        self.assertTrue(op_function(x, **op_kwargs).sparse)\n+        self.assertEqual(op_class(**op_kwargs)(x).shape, expected_shape)\n+        self.assertTrue(op_class(**op_kwargs)(x).sparse)\n+\n     @parameterized.named_parameters(DENSIFYING_UNARY_OPS_TESTS)\n     def test_densifying_unary_sparse_correctness(\n         self, op_function, op_class, np_op\n@@ -4262,8 +4195,10 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(op_function(x), np_op(x_np))\n         self.assertAllClose(op_class()(x), np_op(x_np))\n \n-    @parameterized.named_parameters(UNARY_OPS_TESTS)\n-    def test_unary_sparse_correctness(self, op_function, op_class, np_op):\n+    @parameterized.named_parameters(ELEMENTWISE_UNARY_OPS_TESTS)\n+    def test_elementwise_unary_sparse_correctness(\n+        self, op_function, op_class, np_op\n+    ):\n         if op_function.__name__ in (\"conj\", \"conjugate\", \"imag\", \"real\"):\n             x = np.array([[1 + 1j, 2 + 2j, 3 + 3j], [3 + 3j, 2 + 2j, 1 + 1j]])\n         else:\n@@ -4276,8 +4211,8 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(op_class()(x), np_op(x_np))\n         self.assertSameSparseness(op_class()(x), x)\n \n-    @parameterized.named_parameters(UNARY_OPS_TESTS)\n-    def test_unary_indexed_slices_correctness(\n+    @parameterized.named_parameters(ELEMENTWISE_UNARY_OPS_TESTS)\n+    def test_elementwise_unary_indexed_slices_correctness(\n         self, op_function, op_class, np_op\n     ):\n         if op_function.__name__ in (\"conj\", \"conjugate\", \"imag\", \"real\"):\n@@ -4292,6 +4227,27 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(op_class()(x), np_op(x_np))\n         self.assertSameSparseness(op_class()(x), x)\n \n+    @parameterized.named_parameters(OTHER_UNARY_OPS_TESTS)\n+    def test_other_unary_symbolic_sparse_correctness(\n+        self, op_function, op_class, np_op, op_kwargs, input_shape\n+    ):\n+        x = np.random.random(input_shape)\n+        if op_function is knp.mean:\n+            x = create_indexed_slices(x)\n+        else:\n+            x = create_sparse_tensor(x)\n+        x_np = backend.convert_to_numpy(x)\n+\n+        self.assertAllClose(\n+            op_function(x, **op_kwargs), np_op(x_np, **op_kwargs)\n+        )\n+        self.assertAllClose(op_class(**op_kwargs)(x), np_op(x_np, **op_kwargs))\n+        # Reduction operations have complex and backend dependent rules about\n+        # when the result is sparse and it is dense.\n+        if op_function is not knp.mean:\n+            self.assertSameSparseness(op_function(x, **op_kwargs), x)\n+            self.assertSameSparseness(op_class(**op_kwargs)(x), x)\n+\n     @parameterized.named_parameters(\n         named_product(\n             BINARY_OPS_TESTS, x_sparse=[True, False], y_sparse=[True, False]\n\n@@ -1,10 +1,10 @@\n from unittest import mock\n \n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras import backend\n+from keras import ops\n from keras import optimizers\n from keras import testing\n \n@@ -101,8 +101,8 @@ TEST_CASES = [\n \n \n @pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\",\n-    reason=\"The TensorFlow sparse test can only run with TensorFlow backend.\",\n+    not backend.SUPPORTS_SPARSE_TENSORS,\n+    reason=\"Backend does not support sparse tensors.\",\n )\n class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(TEST_CASES)\n@@ -116,7 +116,7 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n         # This test verifies that:\n         # - Optimizers use Keras ops everywhere instead of native operators\n         #   (e.g. `ops.add()` instead of `+`) where sparse gradients are handled\n-        # - The used ops handle sparse gradients (`tf.IndexedSlices`)\n+        # - The used ops handle sparse gradients\n         # - Optimizers use `self.assign/assign_add/assign_sub` instead of\n         #   calling the method on the variable directly. Otherwise, the sparse\n         #   updates are densified before being applied.\n@@ -124,12 +124,28 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n         #   variable update as per `expect_model_sparse_variable_updates` and\n         #   `expect_optimizer_sparse_variable_updates`\n \n-        model_variable = backend.Variable(initializer=tf.ones, shape=(5, 10))\n+        model_variable = backend.Variable(initializer=\"ones\", shape=(5, 10))\n         optimizer = optimizer_class(**init_kwargs)\n \n         # Mocking \"tensorflow.Variable\" won't work as it gets substituted with\n         # the resource variable class.\n-        tf_variable_class = model_variable._value.__class__\n+\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            grad = tf.IndexedSlices(0.5 * ops.ones((3, 10)), (0, 2, 4), (5, 10))\n+            sparse_class = tf.IndexedSlices\n+            variable_class = model_variable._value.__class__\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            grad = jax_sparse.BCOO(\n+                (0.5 * ops.ones((3, 10)), ((0,), (2,), (4,))), shape=(5, 10)\n+            )\n+            sparse_class = jax_sparse.JAXSparse\n+            variable_class = model_variable.__class__\n+        else:\n+            self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n \n         optimizer_to_patch = (\n             optimizer.inner_optimizer\n@@ -145,7 +161,7 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n             nonlocal optimizer_sparse_variable_updates\n             if isinstance(variable, backend.Variable):\n                 variable = variable._value\n-            if isinstance(value, tf.IndexedSlices):\n+            if isinstance(value, sparse_class):\n                 if variable is model_variable._value:\n                     model_sparse_variable_updates = True\n                 elif any(variable is v._value for v in optimizer.variables):\n@@ -173,11 +189,11 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n         ) as optimizer_assign_add, mock.patch.object(\n             optimizer_to_patch, \"assign_sub\", autospec=True\n         ) as optimizer_assign_sub, mock.patch.object(\n-            tf_variable_class, \"assign\", autospec=True\n+            variable_class, \"assign\", autospec=True\n         ) as variable_assign, mock.patch.object(\n-            tf_variable_class, \"assign_add\", autospec=True\n+            variable_class, \"assign_add\", autospec=True\n         ) as variable_assign_add, mock.patch.object(\n-            tf_variable_class, \"assign_sub\", autospec=True\n+            variable_class, \"assign_sub\", autospec=True\n         ) as variable_assign_sub:\n             optimizer_assign.side_effect = mock_optimizer_assign\n             optimizer_assign_add.side_effect = mock_optimizer_assign\n@@ -186,10 +202,7 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n             variable_assign_add.side_effect = mock_variable_assign\n             variable_assign_sub.side_effect = mock_variable_assign\n \n-            grad = tf.IndexedSlices(\n-                values=tf.ones((3, 10)), indices=(0, 2, 4), dense_shape=(5, 10)\n-            )\n-            optimizer.apply_gradients(zip([grad], [model_variable]))\n+            optimizer.apply([grad], [model_variable])\n \n         self.assertEqual(\n             model_sparse_variable_updates, expect_model_sparse_variable_updates\n@@ -208,16 +221,56 @@ class OptimizerSparseTest(testing.TestCase, parameterized.TestCase):\n \n         optimizer_sparse = optimizer_class(**init_kwargs)\n         optimizer_dense = optimizer_class(**init_kwargs)\n-        var_sparse = backend.Variable(initializer=tf.ones, shape=(5, 3, 2))\n-        var_dense = backend.Variable(initializer=tf.ones, shape=(5, 3, 2))\n+        var_sparse = backend.Variable(initializer=\"ones\", shape=(5, 3, 2))\n+        var_dense = backend.Variable(initializer=\"ones\", shape=(5, 3, 2))\n+        stateless = backend.backend() == \"jax\"\n+        if stateless:\n+            optimizer_sparse.build([var_sparse])\n+            optimizer_dense.build([var_dense])\n+\n+        optimizer_sparse_vars = optimizer_sparse.variables\n+        optimizer_dense_vars = optimizer_dense.variables\n+        var_sparse_values = [var_sparse.value]\n+        var_dense_values = [var_dense.value]\n \n         for i in range(5):\n-            grads_sparse = tf.IndexedSlices(\n-                values=tf.ones((3, 3, 2)) * (10.0 - i),\n+            if backend.backend() == \"tensorflow\":\n+                import tensorflow as tf\n+\n+                grad_sparse = tf.IndexedSlices(\n+                    values=ops.ones((3, 3, 2)) * (10.0 - i),\n                     indices=(0, 2, 4),\n                     dense_shape=(5, 3, 2),\n                 )\n-            grads_dense = tf.convert_to_tensor(grads_sparse)\n-            optimizer_sparse.apply_gradients(zip([grads_sparse], [var_sparse]))\n-            optimizer_dense.apply_gradients(zip([grads_dense], [var_dense]))\n-            self.assertAllClose(var_sparse, var_dense)\n+            elif backend.backend() == \"jax\":\n+                import jax.experimental.sparse as jax_sparse\n+\n+                grad_sparse = jax_sparse.BCOO(\n+                    (ops.ones((3, 3, 2)) * (10.0 - i), ((0,), (2,), (4,))),\n+                    shape=(5, 3, 2),\n+                )\n+            else:\n+                self.fail(\n+                    f\"Sparse is unsupported with backend {backend.backend()}\"\n+                )\n+\n+            grad_dense = ops.convert_to_tensor(grad_sparse, sparse=False)\n+            if stateless:\n+                (\n+                    var_sparse_values,\n+                    optimizer_sparse_vars,\n+                ) = optimizer_sparse.stateless_apply(\n+                    optimizer_sparse_vars, [grad_sparse], var_sparse_values\n+                )\n+                (\n+                    var_dense_values,\n+                    optimizer_dense_vars,\n+                ) = optimizer_dense.stateless_apply(\n+                    optimizer_dense_vars, [grad_dense], var_dense_values\n+                )\n+                self.assertAllClose(var_sparse_values[0], var_dense_values[0])\n+\n+            else:\n+                optimizer_sparse.apply([grad_sparse], [var_sparse])\n+                optimizer_dense.apply([grad_dense], [var_dense])\n+                self.assertAllClose(var_sparse.value, var_dense.value)\n\n@@ -15,6 +15,7 @@ from keras.backend.common.global_state import clear_session\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.models import Model\n from keras.utils import traceback_utils\n+from keras.utils.shape_utils import map_shape_structure\n \n \n class TestCase(unittest.TestCase):\n@@ -285,13 +286,22 @@ class TestCase(unittest.TestCase):\n                     msg=\"Unexpected output dtype\",\n                 )\n             if expected_output_sparse:\n-                import tensorflow as tf\n-\n                 for x in tree.flatten(output):\n                     if isinstance(x, KerasTensor):\n                         self.assertTrue(x.sparse)\n-                    else:\n+                    elif backend.backend() == \"tensorflow\":\n+                        import tensorflow as tf\n+\n                         self.assertIsInstance(x, tf.SparseTensor)\n+                    elif backend.backend() == \"jax\":\n+                        import jax.experimental.sparse as jax_sparse\n+\n+                        self.assertIsInstance(x, jax_sparse.JAXSparse)\n+                    else:\n+                        self.fail(\n+                            \"Sparse is unsupported with \"\n+                            f\"backend {backend.backend()}\"\n+                        )\n             if eager:\n                 if expected_output is not None:\n                     self.assertEqual(type(expected_output), type(output))\n@@ -315,29 +325,16 @@ class TestCase(unittest.TestCase):\n \n             model = TestModel(layer)\n \n-            if input_sparse:\n-                import tensorflow as tf\n-\n+            data = (input_data, output_data)\n             if backend.backend() == \"torch\":\n-                    # Bring output Torch tensors to CPU before using `tf.data`\n-                    dataset = tf.data.Dataset.from_tensors(\n-                        (input_data, backend.convert_to_numpy(output_data))\n-                    )\n-                else:\n-                    dataset = tf.data.Dataset.from_tensors(\n-                        (input_data, output_data)\n-                    )\n-                model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=False)\n-                model.fit(dataset, verbose=0)\n-            else:\n-                input_data = tree.map_structure(\n-                    lambda x: backend.convert_to_numpy(x), input_data\n-                )\n-                output_data = tree.map_structure(\n-                    lambda x: backend.convert_to_numpy(x), output_data\n-                )\n+                data = tree.map_structure(backend.convert_to_numpy, data)\n+\n+            def data_generator():\n+                while True:\n+                    yield data\n+\n             # test the \"default\" path for each backend by setting\n-                # jit_compile=\"auto.\n+            # jit_compile=\"auto\".\n             # for tensorflow and jax backends auto is jitted\n             # for torch backend auto is eager\n             #\n@@ -349,7 +346,7 @@ class TestCase(unittest.TestCase):\n             #    TORCHDYNAMO_VERBOSE=1\n             #    TORCHDYNAMO_REPORT_GUARD_FAILURES=1\n             model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=\"auto\")\n-                model.fit(input_data, output_data, verbose=0)\n+            model.fit(data_generator(), steps_per_epoch=1, verbose=0)\n \n         # Build test.\n         if input_data is not None or input_shape is not None:\n@@ -431,10 +428,6 @@ class TestCase(unittest.TestCase):\n \n \n def create_keras_tensors(input_shape, dtype, sparse):\n-    if isinstance(input_shape, tuple):\n-        return KerasTensor(input_shape, dtype=dtype, sparse=sparse)\n-    if isinstance(input_shape, list):\n-        return [KerasTensor(s, dtype=dtype, sparse=sparse) for s in input_shape]\n     if isinstance(input_shape, dict):\n         return {\n             utils.removesuffix(k, \"_shape\"): KerasTensor(\n@@ -442,7 +435,10 @@ def create_keras_tensors(input_shape, dtype, sparse):\n             )\n             for k, v in input_shape.items()\n         }\n-    raise ValueError(f\"Unsupported type for `input_shape`: {type(input_shape)}\")\n+    return map_shape_structure(\n+        lambda shape: KerasTensor(shape, dtype=dtype, sparse=sparse),\n+        input_shape,\n+    )\n \n \n def create_eager_tensors(input_shape, dtype, sparse):\n@@ -462,28 +458,39 @@ def create_eager_tensors(input_shape, dtype, sparse):\n         )\n \n     if sparse:\n+        if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n-        def create_fn(shape, dtype):\n-            min_dim = min(dim for dim in shape if dim > 1)\n-            x = random.uniform(shape, dtype=\"float32\") * 3 / min_dim\n-            x = tf.nn.dropout(x, 1.0 / min_dim)\n-            x = tf.cast(x, dtype=dtype)\n+            def create_fn(shape):\n+                rng = np.random.default_rng(0)\n+                x = (4 * rng.standard_normal(shape)).astype(dtype)\n+                x = np.multiply(x, rng.random(shape) < 0.7)\n                 return tf.sparse.from_dense(x)\n \n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            def create_fn(shape):\n+                rng = np.random.default_rng(0)\n+                x = (4 * rng.standard_normal(shape)).astype(dtype)\n+                x = np.multiply(x, rng.random(shape) < 0.7)\n+                return jax_sparse.BCOO.fromdense(x, n_batch=1)\n+\n+        else:\n+            raise ValueError(\n+                f\"Sparse is unsupported with backend {backend.backend()}\"\n+            )\n+\n     else:\n \n-        def create_fn(shape, dtype):\n+        def create_fn(shape):\n             return ops.cast(\n                 random.uniform(shape, dtype=\"float32\") * 3, dtype=dtype\n             )\n \n-    if isinstance(input_shape, tuple):\n-        return create_fn(input_shape, dtype=dtype)\n-    if isinstance(input_shape, list):\n-        return [create_fn(s, dtype=dtype) for s in input_shape]\n     if isinstance(input_shape, dict):\n         return {\n-            utils.removesuffix(k, \"_shape\"): create_fn(v, dtype=dtype)\n+            utils.removesuffix(k, \"_shape\"): create_fn(v)\n             for k, v in input_shape.items()\n         }\n+    return map_shape_structure(create_fn, input_shape)\n\n@@ -94,7 +94,15 @@ class TrainingTestingLayer(Trainer, layers.Layer):\n         return x * 0\n \n \n-def tf_sparse_generator():\n+def sparse_generator(generator_type):\n+    if generator_type == \"scipy\":\n+        import scipy\n+\n+        for i in range(4):\n+            x = scipy.sparse.random(2, 4, density=0.25, dtype=\"float32\")\n+            y = np.random.rand(2, 3).astype(\"float32\")\n+            yield x, y\n+    elif generator_type == \"tensorflow\":\n         import tensorflow as tf\n \n         for i in range(4):\n@@ -102,15 +110,18 @@ def tf_sparse_generator():\n             x = tf.sparse.from_dense(tf.nn.dropout(x, 0.25))\n             y = tf.random.uniform((2, 3), dtype=\"float32\")\n             yield x, y\n-\n-\n-def scipy_sparse_generator():\n-    import scipy\n+    elif generator_type == \"jax\":\n+        import jax\n \n         for i in range(4):\n-        x = scipy.sparse.random(2, 4, density=0.25, dtype=\"float32\")\n-        y = np.random.rand(2, 3).astype(\"float32\")\n+            seed = jax.random.PRNGKey(0)\n+            x = jax.experimental.sparse.random_bcoo(\n+                seed, (2, 4), dtype=\"float32\", nse=0.25\n+            )\n+            y = jax.random.uniform(seed, (2, 3), dtype=\"float32\")\n             yield x, y\n+    else:\n+        raise ValueError(f\"Invalid generator type {generator_type}\")\n \n \n class TestTrainer(testing.TestCase, parameterized.TestCase):\n@@ -303,9 +314,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             jit_compile=False,\n         )\n         dataset = (\n-            scipy_sparse_generator()\n+            sparse_generator(\"scipy\")\n             if use_scipy_sparse\n-            else tf_sparse_generator()\n+            else sparse_generator(backend.backend())\n         )\n \n         sparse_variable_updates = False\n@@ -321,6 +332,8 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             optimizer_assign_sub.side_effect = mock_optimizer_assign\n             model.fit(dataset)\n \n+        # JAX does not produce sparse gradients the way we use it.\n+        if backend.backend() != \"jax\":\n             # Verify tensors did not get densified along the way.\n             self.assertTrue(sparse_variable_updates)\n \n@@ -374,9 +387,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             jit_compile=False,\n         )\n         dataset = (\n-            scipy_sparse_generator()\n+            sparse_generator(\"scipy\")\n             if use_scipy_sparse\n-            else tf_sparse_generator()\n+            else sparse_generator(backend.backend())\n         )\n         model.evaluate(dataset)\n \n@@ -444,9 +457,9 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             jit_compile=False,\n         )\n         dataset = (\n-            scipy_sparse_generator()\n+            sparse_generator(\"scipy\")\n             if use_scipy_sparse\n-            else tf_sparse_generator()\n+            else sparse_generator(backend.backend())\n         )\n         model.predict(dataset)\n \n\n@@ -1,8 +1,9 @@\n from keras import backend\n \n \n-def is_in_jax_tracing_scope():\n+def is_in_jax_tracing_scope(x=None):\n     if backend.backend() == \"jax\":\n+        if x is None:\n             x = backend.numpy.ones(())\n         if x.__class__.__name__ == \"DynamicJaxprTracer\":\n             return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
