{"custom_id": "keras#944c31adb5cf3f7ab1a599e33885e350eb831d7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 41 | Churn Cumulative: 307 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,7 @@ DYNAMIC_SHAPES_OK = True\n class Variable(\n     KerasVariable,\n     tf.__internal__.types.Tensor,\n-    tf.__internal__.tracking.AutoTrackable,\n+    tf.__internal__.tracking.Trackable,\n ):\n     _should_act_as_resource_variable = True\n \n@@ -43,10 +43,28 @@ class Variable(\n     def __tf_tensor__(self, dtype=None, name=None):\n         return tf.convert_to_tensor(self.value, dtype=dtype, name=name)\n \n-    # For SavedModel\n+    # Methods below are for SavedModel support\n     def _write_object_proto(self, *args, **kwargs):\n         return self.value._write_object_proto(*args, **kwargs)\n \n+    @property\n+    def _shared_name(self):\n+        return self.value._shared_name\n+\n+    def _serialize_to_tensors(self):\n+        return self.value._serialize_to_tensors()\n+\n+    def _restore_from_tensors(self, restored_tensors):\n+        return self.value._restore_from_tensors(restored_tensors)\n+\n+    def _export_to_saved_model_graph(self, object_map, tensor_map, options, **kwargs):\n+        resource_list = self.value._export_to_saved_model_graph(object_map, tensor_map, options, **kwargs)\n+        object_map[self] = tf.Variable(object_map[self.value])\n+        return resource_list\n+\n+    def _write_object_proto(self, proto, options):\n+        return self.value._write_object_proto(proto, options)\n+\n \n def convert_to_tensor(x, dtype=None):\n     if dtype is not None:\n\n@@ -2,4 +2,21 @@ import tensorflow as tf\n \n \n class TFLayer(tf.__internal__.tracking.AutoTrackable):\n-    pass\n+    \n+    @property\n+    def _default_save_signature(self):\n+        \"\"\"For SavedModel support: returns the default serving signature.\"\"\"\n+\n+        shapes_dict = self._build_shapes_dict\n+        if len(shapes_dict) == 1:\n+            input_shape = tuple(shapes_dict.values())[0]\n+            input_signature = [tf.TensorSpec(input_shape, self.compute_dtype)]\n+        else:\n+            input_signature = [tf.nest.map_structure(lambda x: tf.TensorSpec(x.shape, self.compute_dtype), shapes_dict)]\n+\n+        @tf.function(input_signature=input_signature)\n+        def serving_default(inputs):\n+            return self(inputs)\n+        return serving_default\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00841c00ed9e277ae8bbc92433864d2e3afb8b21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 192 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,12 @@ from keras_core.api_export import keras_core_export\n from keras_core.backend.common import global_state\n \n \n-@keras_core_export(\"keras_core.config.enable_interactive_logging\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.enable_interactive_logging\",\n+        \"keras_core.utils.enable_interactive_logging\",\n+    ]\n+)\n def enable_interactive_logging():\n     \"\"\"Turn on interactive logging.\n \n@@ -17,7 +22,12 @@ def enable_interactive_logging():\n     global_state.set_global_setting(\"interactive_logging\", True)\n \n \n-@keras_core_export(\"keras_core.config.disable_interactive_logging\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.disable_interactive_logging\",\n+        \"keras_core.utils.disable_interactive_logging\",\n+    ]\n+)\n def disable_interactive_logging():\n     \"\"\"Turn off interactive logging.\n \n@@ -28,7 +38,12 @@ def disable_interactive_logging():\n     global_state.set_global_setting(\"interactive_logging\", False)\n \n \n-@keras_core_export(\"keras_core.config.is_interactive_logging_enabled\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.is_interactive_logging_enabled\",\n+        \"keras_core.utils.is_interactive_logging_enabled\",\n+    ]\n+)\n def is_interactive_logging_enabled():\n     \"\"\"Check if interactive logging is enabled.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7267c4e32cfb31bc7c1451bb5f06b37f9e680ef8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 22 | Churn Cumulative: 329 | Contributors (this commit): 6 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,9 +44,6 @@ class Variable(\n         return tf.convert_to_tensor(self.value, dtype=dtype, name=name)\n \n     # Methods below are for SavedModel support\n-    def _write_object_proto(self, *args, **kwargs):\n-        return self.value._write_object_proto(*args, **kwargs)\n-\n     @property\n     def _shared_name(self):\n         return self.value._shared_name\n@@ -57,8 +54,12 @@ class Variable(\n     def _restore_from_tensors(self, restored_tensors):\n         return self.value._restore_from_tensors(restored_tensors)\n \n-    def _export_to_saved_model_graph(self, object_map, tensor_map, options, **kwargs):\n-        resource_list = self.value._export_to_saved_model_graph(object_map, tensor_map, options, **kwargs)\n+    def _export_to_saved_model_graph(\n+        self, object_map, tensor_map, options, **kwargs\n+    ):\n+        resource_list = self.value._export_to_saved_model_graph(\n+            object_map, tensor_map, options, **kwargs\n+        )\n         object_map[self] = tf.Variable(object_map[self.value])\n         return resource_list\n \n\n@@ -2,7 +2,6 @@ import tensorflow as tf\n \n \n class TFLayer(tf.__internal__.tracking.AutoTrackable):\n-    \n     @property\n     def _default_save_signature(self):\n         \"\"\"For SavedModel support: returns the default serving signature.\"\"\"\n@@ -12,11 +11,15 @@ class TFLayer(tf.__internal__.tracking.AutoTrackable):\n             input_shape = tuple(shapes_dict.values())[0]\n             input_signature = [tf.TensorSpec(input_shape, self.compute_dtype)]\n         else:\n-            input_signature = [tf.nest.map_structure(lambda x: tf.TensorSpec(x.shape, self.compute_dtype), shapes_dict)]\n+            input_signature = [\n+                tf.nest.map_structure(\n+                    lambda x: tf.TensorSpec(x.shape, self.compute_dtype),\n+                    shapes_dict,\n+                )\n+            ]\n \n         @tf.function(input_signature=input_signature)\n         def serving_default(inputs):\n             return self(inputs)\n+\n         return serving_default\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3a37294580d4f914b467006040eee7be659cbe8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3452 | Contributors (this commit): 17 | Commits (past 90d): 6 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -192,7 +192,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n     **One-hot output**\n \n     Configure the layer with `output_mode='one_hot'`. Note that the first\n-    `num_oov_indices` dimensions in the ont_hot encoding represent OOV values.\n+    `num_oov_indices` dimensions in the one_hot encoding represent OOV values.\n \n     >>> vocab = [12, 36, 1138, 42]\n     >>> data = tf.constant([12, 36, 1138, 42, 7]) # Note OOV tokens\n\n@@ -188,7 +188,7 @@ class StringLookup(index_lookup.IndexLookup):\n     **One-hot output**\n \n     Configure the layer with `output_mode='one_hot'`. Note that the first\n-    `num_oov_indices` dimensions in the ont_hot encoding represent OOV values.\n+    `num_oov_indices` dimensions in the one_hot encoding represent OOV values.\n \n     >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n     >>> data = tf.constant([\"a\", \"b\", \"c\", \"d\", \"z\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1eb1bf05d9183b124970b971178c524a6f7e5148", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 1153 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -125,7 +125,7 @@ def print_summary(\n     from keras_core.models import Functional\n     from keras_core.models import Sequential\n \n-    if print_fn is None:\n+    if not print_fn and not io_utils.is_interactive_logging_enabled():\n         print_fn = io_utils.print_msg\n \n     if isinstance(model, Sequential):\n@@ -292,13 +292,13 @@ def print_summary(\n     total_memory_size = trainable_memory_size + non_trainable_memory_size\n \n     # Create a rich console for printing. Capture for non-interactive logging.\n-    if io_utils.is_interactive_logging_enabled():\n-        console = rich.console.Console(highlight=False)\n-    else:\n+    if print_fn:\n         console = rich.console.Console(\n             highlight=False, force_terminal=False, color_system=None\n         )\n         console.begin_capture()\n+    else:\n+        console = rich.console.Console(highlight=False)\n \n     # Print the to the console.\n     console.print(bold_text(f'Model: \"{rich.markup.escape(model.name)}\"'))\n@@ -320,8 +320,8 @@ def print_summary(\n     )\n \n     # Output captured summary for non-interactive logging.\n-    if not io_utils.is_interactive_logging_enabled():\n-        io_utils.print_msg(console.end_capture(), line_break=False)\n+    if print_fn:\n+        print_fn(console.end_capture(), line_break=False)\n \n \n def get_layer_index_bound_by_layer_name(layers, layer_range=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b903c7eca7b709c15910f102cbaf55d6195d785", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 651 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ from keras_core.layers.layer import Layer\n from keras_core.layers.regularization.dropout import Dropout\n \n \n-@keras_core_export(\"keras_core.layers.Attention\")\n+@keras_core_export(\"keras_core.layers.MultiHeadAttention\")\n class MultiHeadAttention(Layer):\n     \"\"\"MultiHeadAttention layer.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ebe9ee5a1a2105ca3a41028873999846654456ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 596 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 3 | Methods Changed: 21 | Complexity Δ (Sum/Max): 37/22 | Churn Δ: 596 | Churn Cumulative: 596 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n\n\n@@ -0,0 +1,197 @@\n+import time\n+\n+import numpy as np\n+import tensorflow as tf\n+from absl import flags\n+\n+import keras_core\n+\n+FLAGS = flags.FLAGS\n+\n+flags.DEFINE_string(\n+    \"benchmark_name\",\n+    None,\n+    \"The name of benchmark to run.\",\n+)\n+\n+flags.DEFINE_integer(\n+    \"num_samples\",\n+    1000,\n+    \"Number of input data samples.\",\n+)\n+\n+flags.DEFINE_integer(\n+    \"batch_size\",\n+    20,\n+    \"Batch size of data.\",\n+)\n+\n+flags.DEFINE_bool(\n+    \"jit_compile\",\n+    True,\n+    \"If True, the benchmark will run with XLA compilation.\",\n+)\n+\n+\n+class BenchmarkMetricsCallback:\n+    def __init__(self, start_batch=2, stop_batch=None):\n+        self.start_batch = start_batch\n+        self.stop_batch = stop_batch\n+\n+        self.state = {}\n+\n+    def on_train_batch_begin(self, batch, logs=None):\n+        if batch == self.start_batch:\n+            self.state[\"benchmark_begin\"] = time.time()\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        if batch == self.stop_batch:\n+            self.state[\"benchmark_end\"] = time.time()\n+            throughput = (self.stop_batch - self.start_batch) / (\n+                self.state[\"benchmark_end\"] - self.state[\"benchmark_begin\"]\n+            )\n+            self.state[\"throughput\"] = throughput\n+\n+    def on_predict_batch_begin(self, batch, logs=None):\n+        if batch == self.start_batch:\n+            self.state[\"benchmark_begin\"] = time.time()\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        if batch == self.stop_batch:\n+            self.state[\"benchmark_end\"] = time.time()\n+            throughput = (self.stop_batch - self.start_batch) / (\n+                self.state[\"benchmark_end\"] - self.state[\"benchmark_begin\"]\n+            )\n+            self.state[\"throughput\"] = throughput\n+\n+\n+class KerasCoreBenchmarkMetricsCallback(keras_core.callbacks.Callback):\n+    def __init__(self, start_batch=2, stop_batch=None):\n+        self._callback = BenchmarkMetricsCallback(start_batch, stop_batch)\n+\n+    def on_train_batch_begin(self, batch, logs=None):\n+        self._callback.on_train_batch_begin(batch, logs)\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        self._callback.on_train_batch_end(batch, logs)\n+\n+    def on_predict_batch_begin(self, batch, logs=None):\n+        self._callback.on_predict_batch_begin(batch, logs)\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        self._callback.on_predict_batch_end(batch, logs)\n+\n+\n+class TFKerasBenchmarkMetricsCallback(tf.keras.callbacks.Callback):\n+    def __init__(self, start_batch=2, stop_batch=None):\n+        self._callback = BenchmarkMetricsCallback(start_batch, stop_batch)\n+\n+    def on_train_batch_begin(self, batch, logs=None):\n+        self._callback.on_train_batch_begin(batch, logs)\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        self._callback.on_train_batch_end(batch, logs)\n+\n+    def on_predict_batch_begin(self, batch, logs=None):\n+        self._callback.on_predict_batch_begin(batch, logs)\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        self._callback.on_predict_batch_end(batch, logs)\n+\n+\n+class LayerBenchmark:\n+    def __init__(\n+        self,\n+        layer_name,\n+        init_args,\n+        input_shape,\n+        jit_compile=True,\n+    ):\n+        self.layer_name = layer_name\n+        self.input_shape = input_shape\n+        _keras_core_layer_class = getattr(keras_core.layers, layer_name)\n+        _tf_keras_layer_class = getattr(tf.keras.layers, layer_name)\n+\n+        self._keras_core_layer = _keras_core_layer_class(**init_args)\n+        self._tf_keras_layer = _tf_keras_layer_class(**init_args)\n+\n+        self._keras_core_model = keras_core.Sequential([self._keras_core_layer])\n+        self._tf_keras_model = tf.keras.Sequential([self._tf_keras_layer])\n+\n+        self._keras_core_model.compile(\n+            loss=\"mse\", optimizer=\"sgd\", jit_compile=jit_compile\n+        )\n+        self._tf_keras_model.compile(\n+            loss=\"mse\", optimizer=\"sgd\", jit_compile=jit_compile\n+        )\n+\n+        self.jit_compile = jit_compile\n+        self.input_shape = input_shape\n+\n+    def benchmark_predict(self, num_samples, batch_size):\n+        data_shape = [num_samples] + list(self.input_shape)\n+        data = np.random.normal(size=data_shape)\n+        num_iterations = num_samples // batch_size - 1\n+        callback = KerasCoreBenchmarkMetricsCallback(stop_batch=num_iterations)\n+        tf_keras_callback = TFKerasBenchmarkMetricsCallback(\n+            stop_batch=num_iterations\n+        )\n+\n+        self._keras_core_model.predict(\n+            data,\n+            batch_size=batch_size,\n+            callbacks=[callback],\n+        )\n+\n+        self._tf_keras_model.predict(\n+            data,\n+            batch_size=batch_size,\n+            callbacks=[tf_keras_callback],\n+        )\n+\n+        keras_core_throughput = callback._callback.state[\"throughput\"]\n+        tf_keras_throughput = tf_keras_callback._callback.state[\"throughput\"]\n+        print(\n+            f\"Keras Core throughput of forward pass of {self.layer_name}: \"\n+            f\"{keras_core_throughput} samples/sec.\"\n+        )\n+        print(\n+            f\"TF Keras throughput of forward pass of {self.layer_name}: \"\n+            f\"{tf_keras_throughput} samples/sec.\"\n+        )\n+\n+    def benchmark_train(self, num_samples, batch_size):\n+        data_shape = [num_samples] + list(self.input_shape)\n+        data = np.random.normal(size=data_shape)\n+        # Scale by a small factor to avoid zero gradients.\n+        label = np.array(self._keras_core_layer(data)) * 1.001\n+\n+        num_iterations = num_samples // batch_size - 1\n+        callback = KerasCoreBenchmarkMetricsCallback(stop_batch=num_iterations)\n+        tf_keras_callback = TFKerasBenchmarkMetricsCallback(\n+            stop_batch=num_iterations\n+        )\n+\n+        self._keras_core_model.fit(\n+            data,\n+            label,\n+            batch_size=batch_size,\n+            callbacks=[callback],\n+        )\n+        self._tf_keras_model.fit(\n+            data,\n+            label,\n+            batch_size=batch_size,\n+            callbacks=[tf_keras_callback],\n+        )\n+\n+        keras_core_throughput = callback._callback.state[\"throughput\"]\n+        tf_keras_throughput = tf_keras_callback._callback.state[\"throughput\"]\n+        print(\n+            f\"Keras Core throughput of forward & backward pass of \"\n+            f\"{self.layer_name}: {keras_core_throughput} samples/sec.\"\n+        )\n+        print(\n+            f\"TF Keras  throughput of forward & backward pass of \"\n+            f\"{self.layer_name}: {tf_keras_throughput} samples/sec.\"\n+        )\n\n@@ -0,0 +1,276 @@\n+\"\"\" Benchmark conv layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.conv_benchmark \\\n+    --benchmark_name=benchmark_conv2D \\\n+    --num_samples=1000 \\\n+    --batch_size=20 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_conv1D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Conv1D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv2D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Conv2D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv3D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Conv3D\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_depthwise_conv1D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"DepthwiseConv1D\"\n+    init_args = {\n+        \"kernel_size\": 16,\n+        \"depth_multiplier\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_depthwise_conv2D(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"DepthwiseConv2D\"\n+    init_args = {\n+        \"kernel_size\": 16,\n+        \"depth_multiplier\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv1D_transpose(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Conv1DTranspose\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv2D_transpose(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Conv2DTranspose\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_conv3D_transpose(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"Conv3DTranspose\"\n+    init_args = {\n+        \"filters\": 16,\n+        \"kernel_size\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_conv1D\": benchmark_conv1D,\n+    \"benchmark_conv2D\": benchmark_conv2D,\n+    \"benchmark_conv3D\": benchmark_conv3D,\n+    \"benchmark_depthwise_conv1D\": benchmark_depthwise_conv1D,\n+    \"benchmark_depthwise_conv2D\": benchmark_depthwise_conv2D,\n+    \"benchmark_conv1D_transpose\": benchmark_conv1D_transpose,\n+    \"benchmark_conv2D_transpose\": benchmark_conv2D_transpose,\n+    \"benchmark_conv3D_transpose\": benchmark_conv3D_transpose,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n@@ -0,0 +1,123 @@\n+\"\"\" Benchmark normalization layers.\n+\n+To run benchmarks, see the following command for an example, please change the\n+flag to your custom value:\n+\n+```\n+python3 -m benchmarks.layer_benchmark.normalization_benchmark \\\n+    --benchmark_name=benchmark_batch_normalization \\\n+    --num_samples=1000 \\\n+    --batch_size=20 \\\n+    --jit_compile=True\n+```\n+\"\"\"\n+\n+from absl import app\n+from absl import flags\n+\n+from benchmarks.layer_benchmark.base_benchmark import LayerBenchmark\n+\n+FLAGS = flags.FLAGS\n+\n+\n+def benchmark_batch_normalization(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"BatchNormalization\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_group_normalization(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"GroupNormalization\"\n+    init_args = {\n+        \"groups\": 2,\n+    }\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+def benchmark_layer_normalization(\n+    num_samples,\n+    batch_size,\n+    jit_compile=True,\n+):\n+    layer_name = \"LayerNormalization\"\n+    init_args = {}\n+    benchmark = LayerBenchmark(\n+        layer_name,\n+        init_args,\n+        input_shape=[32, 32, 4],\n+        jit_compile=jit_compile,\n+    )\n+\n+    benchmark.benchmark_predict(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+    benchmark.benchmark_train(\n+        num_samples=num_samples,\n+        batch_size=batch_size,\n+    )\n+\n+\n+BENCHMARK_NAMES = {\n+    \"benchmark_batch_normalization\": benchmark_batch_normalization,\n+    \"benchmark_group_normalization\": benchmark_group_normalization,\n+    \"benchmark_layer_normalization\": benchmark_layer_normalization,\n+}\n+\n+\n+def main(_):\n+    benchmark_name = FLAGS.benchmark_name\n+    num_samples = FLAGS.num_samples\n+    batch_size = FLAGS.batch_size\n+    jit_compile = FLAGS.jit_compile\n+\n+    if benchmark_name not in BENCHMARK_NAMES:\n+        raise ValueError(\n+            f\"Invalid benchmark name: {benchmark_name}, `benchmark_name` must \"\n+            f\"be one of {BENCHMARK_NAMES.keys()}\"\n+        )\n+    benchmark_fn = BENCHMARK_NAMES[benchmark_name]\n+    benchmark_fn(num_samples, batch_size, jit_compile)\n+\n+\n+if __name__ == \"__main__\":\n+    app.run(main)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4410ab69494236bee52f691fa89af9c3a7e0633e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 12 | Churn Cumulative: 947 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -305,17 +305,11 @@ class LayerNormalization(Layer):\n             outputs = tf.cast(outputs, input_dtype)\n         else:\n             # Collapse dims before self.axis, and dims in self.axis\n-            pre_dim, in_dim = (1, 1)\n+\n             axis = sorted(self.axis)\n             tensor_shape = tf.shape(inputs)\n-            for dim in range(0, ndims):\n-                dim_tensor = tensor_shape[dim]\n-                if dim < axis[0]:\n-                    pre_dim = pre_dim * dim_tensor\n-                else:\n-                    assert dim in axis\n-                    in_dim = in_dim * dim_tensor\n-\n+            pre_dim = tf.reduce_prod(tensor_shape[: axis[0]])\n+            in_dim = tf.reduce_prod(tensor_shape[axis[0] :])\n             squeezed_shape = [1, pre_dim, in_dim, 1]\n             # This fused operation requires reshaped inputs to be NCHW.\n             data_format = \"NCHW\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9799021d99fb7f51eeef9eec468ce4055f1fdfae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6718 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -973,7 +973,7 @@ class Layer(base_layer.Layer):\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def input_spec(self, value):\n         for v in tf.nest.flatten(value):\n-            if v is not None and not isinstance(v, input_spec.InputSpec):\n+            if v is not None and \"InputSpec\" not in v.__class__.__name__:\n                 raise TypeError(\n                     \"Layer input_spec must be an instance of InputSpec. \"\n                     \"Got: {}\".format(v)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3929a22896a40c4a950d89e9f00c34bec660ff4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 652 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -134,7 +134,6 @@ class MultiHeadAttention(Layer):\n                 f\"Received: attention_axes={attention_axes}\"\n             )\n         self._attention_axes = attention_axes\n-        self._built_from_signature = False\n \n     def get_config(self):\n         base_config = super().get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43071a78553b36a4074c929cf98bb07afe2a9ea7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 8 | Methods Changed: 12 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 25 | Churn Cumulative: 8683 | Contributors (this commit): 9 | Commits (past 90d): 74 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -90,7 +90,7 @@ def append(\n     return jnp.append(x1, x2, axis=axis)\n \n \n-def arange(start, stop=None, step=None, dtype=None):\n+def arange(start, stop=None, step=1, dtype=None):\n     return jnp.arange(start, stop, step=step, dtype=dtype)\n \n \n\n@@ -94,7 +94,7 @@ def append(\n     return tfnp.append(x1, x2, axis=axis)\n \n \n-def arange(start, stop=None, step=None, dtype=None):\n+def arange(start, stop=None, step=1, dtype=None):\n     return tfnp.arange(start, stop, step=step, dtype=dtype)\n \n \n\n@@ -128,11 +128,8 @@ def append(\n     return torch.cat((x1, x2), dim=axis)\n \n \n-def arange(start, stop=None, step=None, dtype=None):\n+def arange(start, stop=None, step=1, dtype=None):\n     dtype = to_torch_dtype(dtype)\n-    if stop is None:\n-        return torch.arange(start, step=step, dtype=dtype)\n-    step = step or 1\n     return torch.arange(start, stop, step=step, dtype=dtype)\n \n \n\n@@ -427,18 +427,12 @@ def append(\n \n \n class Arange(Operation):\n-    def call(self, start, stop=None, step=None, dtype=None):\n-        if stop is None:\n-            start, stop = 0, start\n-        if step is None:\n-            step = 1\n+    def call(self, start, stop=None, step=1, dtype=None):\n         return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n \n-    def compute_output_spec(self, start, stop=None, step=None, dtype=None):\n+    def compute_output_spec(self, start, stop=None, step=1, dtype=None):\n         if stop is None:\n             start, stop = 0, start\n-        if step is None:\n-            step = 1\n         output_shape = [np.ceil((stop - start) / step).astype(int)]\n         return KerasTensor(output_shape, dtype=dtype)\n \n@@ -446,11 +440,7 @@ class Arange(Operation):\n @keras_core_export(\n     [\"keras_core.operations.arange\", \"keras_core.operations.numpy.arange\"]\n )\n-def arange(start, stop=None, step=None, dtype=None):\n-    if stop is None:\n-        start, stop = 0, start\n-    if step is None:\n-        step = 1\n+def arange(start, stop=None, step=1, dtype=None):\n     return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7978e71b1a57338d99a37e12616fe2d6dbfed5a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 48 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,8 @@ import jax\n import numpy as np\n import tensorflow as tf\n \n+from keras_core.api_export import keras_core_export\n+\n try:\n     import pandas\n except ImportError:\n@@ -21,6 +23,7 @@ if pandas:\n # TODO: support torch tensors?\n \n \n+@keras_core_export(\"keras_core.utils.unpack_x_y_sample_weight\")\n def unpack_x_y_sample_weight(data):\n     \"\"\"Unpacks user-provided data tuple.\n \n@@ -63,6 +66,7 @@ def unpack_x_y_sample_weight(data):\n     raise ValueError(error_msg)\n \n \n+@keras_core_export(\"keras_core.utils.pack_x_y_sample_weight\")\n def pack_x_y_sample_weight(x, y=None, sample_weight=None):\n     \"\"\"Packs user-provided data into a tuple.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4364c82f3c52e6f7090801859464b23ffe06441e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 416 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 4 | Methods Changed: 12 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 419 | Churn Cumulative: 705 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,413 @@\n+\"\"\"\n+Title: Image similarity estimation using a Siamese Network with a contrastive loss\n+Author: Mehdi\n+Date created: 2021/05/06\n+Last modified: 2022/09/10\n+Description: Similarity learning using a siamese network trained with a contrastive loss.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+[Siamese Networks](https://en.wikipedia.org/wiki/Siamese_neural_network)\n+are neural networks which share weights between two or more sister networks,\n+each producing embedding vectors of its respective inputs.\n+\n+In supervised similarity learning, the networks are then trained to maximize the\n+contrast (distance) between embeddings of inputs of different classes, while minimizing the distance between\n+embeddings of similar classes, resulting in embedding spaces that reflect\n+the class segmentation of the training inputs.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import random\n+import numpy as np\n+import keras_core as keras\n+from keras_core import operations as ops\n+import matplotlib.pyplot as plt\n+\n+\"\"\"\n+## Hyperparameters\n+\"\"\"\n+\n+epochs = 10\n+batch_size = 16\n+margin = 1  # Margin for contrastive loss.\n+\n+\"\"\"\n+## Load the MNIST dataset\n+\"\"\"\n+(x_train_val, y_train_val), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+# Change the data type to a floating point format\n+x_train_val = x_train_val.astype(\"float32\")\n+x_test = x_test.astype(\"float32\")\n+\n+\n+\"\"\"\n+## Define training and validation sets\n+\"\"\"\n+\n+# Keep 50% of train_val  in validation set\n+x_train, x_val = x_train_val[:30000], x_train_val[30000:]\n+y_train, y_val = y_train_val[:30000], y_train_val[30000:]\n+del x_train_val, y_train_val\n+\n+\n+\"\"\"\n+## Create pairs of images\n+\n+We will train the model to differentiate between digits of different classes. For\n+example, digit `0` needs to be differentiated from the rest of the\n+digits (`1` through `9`), digit `1` - from `0` and `2` through `9`, and so on.\n+To carry this out, we will select N random images from class A (for example,\n+for digit `0`) and pair them with N random images from another class B\n+(for example, for digit `1`). Then, we can repeat this process for all classes\n+of digits (until digit `9`). Once we have paired digit `0` with other digits,\n+we can repeat this process for the remaining classes for the rest of the digits\n+(from `1` until `9`).\n+\"\"\"\n+\n+\n+def make_pairs(x, y):\n+    \"\"\"Creates a tuple containing image pairs with corresponding label.\n+\n+    Arguments:\n+        x: List containing images, each index in this list corresponds to one image.\n+        y: List containing labels, each label with datatype of `int`.\n+\n+    Returns:\n+        Tuple containing two numpy arrays as (pairs_of_samples, labels),\n+        where pairs_of_samples' shape is (2len(x), 2,n_features_dims) and\n+        labels are a binary array of shape (2len(x)).\n+    \"\"\"\n+\n+    num_classes = max(y) + 1\n+    digit_indices = [np.where(y == i)[0] for i in range(num_classes)]\n+\n+    pairs = []\n+    labels = []\n+\n+    for idx1 in range(len(x)):\n+        # add a matching example\n+        x1 = x[idx1]\n+        label1 = y[idx1]\n+        idx2 = random.choice(digit_indices[label1])\n+        x2 = x[idx2]\n+\n+        pairs += [[x1, x2]]\n+        labels += [0]\n+\n+        # add a non-matching example\n+        label2 = random.randint(0, num_classes - 1)\n+        while label2 == label1:\n+            label2 = random.randint(0, num_classes - 1)\n+\n+        idx2 = random.choice(digit_indices[label2])\n+        x2 = x[idx2]\n+\n+        pairs += [[x1, x2]]\n+        labels += [1]\n+\n+    return np.array(pairs), np.array(labels).astype(\"float32\")\n+\n+\n+# make train pairs\n+pairs_train, labels_train = make_pairs(x_train, y_train)\n+\n+# make validation pairs\n+pairs_val, labels_val = make_pairs(x_val, y_val)\n+\n+# make test pairs\n+pairs_test, labels_test = make_pairs(x_test, y_test)\n+\n+\"\"\"\n+We get:\n+\n+**pairs_train.shape = (60000, 2, 28, 28)**\n+\n+- We have 60,000 pairs\n+- Each pair contains 2 images\n+- Each image has shape `(28, 28)`\n+\"\"\"\n+\n+\"\"\"\n+Split the training pairs\n+\"\"\"\n+\n+x_train_1 = pairs_train[:, 0]  # x_train_1.shape is (60000, 28, 28)\n+x_train_2 = pairs_train[:, 1]\n+\n+\"\"\"\n+Split the validation pairs\n+\"\"\"\n+\n+x_val_1 = pairs_val[:, 0]  # x_val_1.shape = (60000, 28, 28)\n+x_val_2 = pairs_val[:, 1]\n+\n+\"\"\"\n+Split the test pairs\n+\"\"\"\n+\n+x_test_1 = pairs_test[:, 0]  # x_test_1.shape = (20000, 28, 28)\n+x_test_2 = pairs_test[:, 1]\n+\n+\n+\"\"\"\n+## Visualize pairs and their labels\n+\"\"\"\n+\n+\n+def visualize(pairs, labels, to_show=6, num_col=3, predictions=None, test=False):\n+    \"\"\"Creates a plot of pairs and labels, and prediction if it's test dataset.\n+\n+    Arguments:\n+        pairs: Numpy Array, of pairs to visualize, having shape\n+               (Number of pairs, 2, 28, 28).\n+        to_show: Int, number of examples to visualize (default is 6)\n+                `to_show` must be an integral multiple of `num_col`.\n+                 Otherwise it will be trimmed if it is greater than num_col,\n+                 and incremented if if it is less then num_col.\n+        num_col: Int, number of images in one row - (default is 3)\n+                 For test and train respectively, it should not exceed 3 and 7.\n+        predictions: Numpy Array of predictions with shape (to_show, 1) -\n+                     (default is None)\n+                     Must be passed when test=True.\n+        test: Boolean telling whether the dataset being visualized is\n+              train dataset or test dataset - (default False).\n+\n+    Returns:\n+        None.\n+    \"\"\"\n+\n+    # Define num_row\n+    # If to_show % num_col != 0\n+    #    trim to_show,\n+    #       to trim to_show limit num_row to the point where\n+    #       to_show % num_col == 0\n+    #\n+    # If to_show//num_col == 0\n+    #    then it means num_col is greater then to_show\n+    #    increment to_show\n+    #       to increment to_show set num_row to 1\n+    num_row = to_show // num_col if to_show // num_col != 0 else 1\n+\n+    # `to_show` must be an integral multiple of `num_col`\n+    #  we found num_row and we have num_col\n+    #  to increment or decrement to_show\n+    #  to make it integral multiple of `num_col`\n+    #  simply set it equal to num_row * num_col\n+    to_show = num_row * num_col\n+\n+    # Plot the images\n+    fig, axes = plt.subplots(num_row, num_col, figsize=(5, 5))\n+    for i in range(to_show):\n+        # If the number of rows is 1, the axes array is one-dimensional\n+        if num_row == 1:\n+            ax = axes[i % num_col]\n+        else:\n+            ax = axes[i // num_col, i % num_col]\n+\n+        ax.imshow(ops.concatenate([pairs[i][0], pairs[i][1]], axis=1), cmap=\"gray\")\n+        ax.set_axis_off()\n+        if test:\n+            ax.set_title(\"True: {} | Pred: {:.5f}\".format(labels[i], predictions[i][0]))\n+        else:\n+            ax.set_title(\"Label: {}\".format(labels[i]))\n+    if test:\n+        plt.tight_layout(rect=(0, 0, 1.9, 1.9), w_pad=0.0)\n+    else:\n+        plt.tight_layout(rect=(0, 0, 1.5, 1.5))\n+    plt.show()\n+\n+\n+\"\"\"\n+Inspect training pairs\n+\"\"\"\n+\n+visualize(pairs_train[:-1], labels_train[:-1], to_show=4, num_col=4)\n+\n+\"\"\"\n+Inspect validation pairs\n+\"\"\"\n+\n+visualize(pairs_val[:-1], labels_val[:-1], to_show=4, num_col=4)\n+\n+\"\"\"\n+Inspect test pairs\n+\"\"\"\n+\n+visualize(pairs_test[:-1], labels_test[:-1], to_show=4, num_col=4)\n+\n+\"\"\"\n+## Define the model\n+\n+There are two input layers, each leading to its own network, which\n+produces embeddings. A `Lambda` layer then merges them using an\n+[Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) and the\n+merged output is fed to the final network.\n+\"\"\"\n+\n+\n+# Provided two tensors t1 and t2\n+# Euclidean distance = sqrt(sum(square(t1-t2)))\n+def euclidean_distance(vects):\n+    \"\"\"Find the Euclidean distance between two vectors.\n+\n+    Arguments:\n+        vects: List containing two tensors of same length.\n+\n+    Returns:\n+        Tensor containing euclidean distance\n+        (as floating point value) between vectors.\n+    \"\"\"\n+\n+    x, y = vects\n+    sum_square = ops.sum(ops.square(x - y), axis=1, keepdims=True)\n+    return ops.sqrt(ops.maximum(sum_square, keras.backend.epsilon()))\n+\n+\n+input = keras.layers.Input((28, 28, 1))\n+x = keras.layers.BatchNormalization()(input)\n+x = keras.layers.Conv2D(4, (5, 5), activation=\"tanh\")(x)\n+x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n+x = keras.layers.Conv2D(16, (5, 5), activation=\"tanh\")(x)\n+x = keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n+x = keras.layers.Flatten()(x)\n+\n+x = keras.layers.BatchNormalization()(x)\n+x = keras.layers.Dense(10, activation=\"tanh\")(x)\n+embedding_network = keras.Model(input, x)\n+\n+\n+input_1 = keras.layers.Input((28, 28, 1))\n+input_2 = keras.layers.Input((28, 28, 1))\n+\n+# As mentioned above, Siamese Network share weights between\n+# tower networks (sister networks). To allow this, we will use\n+# same embedding network for both tower networks.\n+tower_1 = embedding_network(input_1)\n+tower_2 = embedding_network(input_2)\n+\n+merge_layer = keras.layers.Lambda(euclidean_distance, output_shape=(1,))([tower_1, tower_2])\n+normal_layer = keras.layers.BatchNormalization()(merge_layer)\n+output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(normal_layer)\n+siamese = keras.Model(inputs=[input_1, input_2], outputs=output_layer)\n+\n+\n+\"\"\"\n+## Define the contrastive Loss\n+\"\"\"\n+\n+\n+def loss(margin=1):\n+    \"\"\"Provides 'contrastive_loss' an enclosing scope with variable 'margin'.\n+\n+    Arguments:\n+        margin: Integer, defines the baseline for distance for which pairs\n+                should be classified as dissimilar. - (default is 1).\n+\n+    Returns:\n+        'contrastive_loss' function with data ('margin') attached.\n+    \"\"\"\n+\n+    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n+    #                         true_value * square( max(margin-prediction, 0) ))\n+    def contrastive_loss(y_true, y_pred):\n+        \"\"\"Calculates the contrastive loss.\n+\n+        Arguments:\n+            y_true: List of labels, each label is of type float32.\n+            y_pred: List of predictions of same length as of y_true,\n+                    each label is of type float32.\n+\n+        Returns:\n+            A tensor containing contrastive loss as floating point value.\n+        \"\"\"\n+\n+        square_pred = ops.square(y_pred)\n+        margin_square = ops.square(ops.maximum(margin - (y_pred), 0))\n+        return ops.mean(\n+            (1 - y_true) * square_pred + (y_true) * margin_square\n+        )\n+\n+    return contrastive_loss\n+\n+\n+\"\"\"\n+## Compile the model with the contrastive loss\n+\"\"\"\n+\n+siamese.compile(loss=loss(margin=margin), optimizer=\"RMSprop\", metrics=[\"accuracy\"])\n+siamese.summary()\n+\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+history = siamese.fit(\n+    [x_train_1, x_train_2],\n+    labels_train,\n+    validation_data=([x_val_1, x_val_2], labels_val),\n+    batch_size=batch_size,\n+    epochs=epochs,\n+)\n+\n+\"\"\"\n+## Visualize results\n+\"\"\"\n+\n+\n+def plt_metric(history, metric, title, has_valid=True):\n+    \"\"\"Plots the given 'metric' from 'history'.\n+\n+    Arguments:\n+        history: history attribute of History object returned from Model.fit.\n+        metric: Metric to plot, a string value present as key in 'history'.\n+        title: A string to be used as title of plot.\n+        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n+\n+    Returns:\n+        None.\n+    \"\"\"\n+    plt.plot(history[metric])\n+    if has_valid:\n+        plt.plot(history[\"val_\" + metric])\n+        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n+    plt.title(title)\n+    plt.ylabel(metric)\n+    plt.xlabel(\"epoch\")\n+    plt.show()\n+\n+\n+# Plot the accuracy\n+plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n+\n+# Plot the contrastive loss\n+plt_metric(history=history.history, metric=\"loss\", title=\"Contrastive Loss\")\n+\n+\"\"\"\n+## Evaluate the model\n+\"\"\"\n+\n+results = siamese.evaluate([x_test_1, x_test_2], labels_test)\n+print(\"test loss, test acc:\", results)\n+\n+\"\"\"\n+## Visualize the predictions\n+\"\"\"\n+\n+predictions = siamese.predict([x_test_1, x_test_2])\n+visualize(pairs_test, labels_test, to_show=3, predictions=predictions, test=True)\n+\n+\"\"\"\n+**Example available on HuggingFace**\n+| Trained Model | Demo |\n+| :--: | :--: |\n+| [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-Siamese%20Network-black.svg)](https://huggingface.co/keras-io/siamese-contrastive) | [![Generic badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-Siamese%20Network-black.svg)](https://huggingface.co/spaces/keras-io/siamese-contrastive) |\n+\"\"\"\n\n@@ -74,7 +74,7 @@ class AveragePooling1D(BasePooling):\n     def __init__(\n         self,\n         pool_size,\n-        strides,\n+        strides=None,\n         padding=\"valid\",\n         data_format=None,\n         name=None,\n\n@@ -91,7 +91,7 @@ class AveragePooling2D(BasePooling):\n     def __init__(\n         self,\n         pool_size,\n-        strides,\n+        strides=None,\n         padding=\"valid\",\n         data_format=\"channels_last\",\n         name=None,\n\n@@ -67,7 +67,7 @@ class AveragePooling3D(BasePooling):\n     def __init__(\n         self,\n         pool_size,\n-        strides,\n+        strides=None,\n         padding=\"valid\",\n         data_format=\"channels_last\",\n         name=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
