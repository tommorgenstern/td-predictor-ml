{"custom_id": "scrapy#3ca7877781fdae5415c57a9a0f348b1797795209", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 3 | Churn Cumulative: 4626 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -2285,6 +2285,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 self.assertEqual(expected_batch, got_batch)\n \n+    @pytest.mark.skipif(sys.platform == 'win32', reason='Odd behaviour on file creation/output')\n     @defer.inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n@@ -2305,7 +2306,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n         }\n         data = yield self.exported_data(items, settings)\n-        self.assertEqual(len(items) + 1, len(data['json']))\n+        self.assertEqual(len(items), len([_ for _ in data['json'] if _]))\n \n     @defer.inlineCallbacks\n     def test_stats_batch_file_success(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#116d9a97481f5ee4028a1f3f8e72ca34b35a0be9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 93 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ pprint and pformat wrappers with colorization support\n import ctypes\n import platform\n import sys\n-from distutils.version import LooseVersion as parse_version\n+from packaging.version import Version as parse_version\n from pprint import pformat as pformat_\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#759ad5dee4120dc2197ea6fa1559c92f77472abf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 636 | Contributors (this commit): 31 | Commits (past 90d): 2 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -32,6 +32,7 @@ install_requires = [\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'setuptools',\n+    'packaging',\n     'tldextract',\n     'lxml>=4.3.0',\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80194f1c0374b2aa429de4c7ea70a683c946db95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 14/7 | Churn Δ: 29 | Churn Cumulative: 1967 | Contributors (this commit): 20 | Commits (past 90d): 5 | Contributors (cumulative): 37 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -6,7 +6,7 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n import copy\n-from typing import Sequence\n+from typing import Awaitable, Sequence\n \n from scrapy.http import Request, HtmlResponse\n from scrapy.linkextractors import LinkExtractor\n@@ -109,9 +109,11 @@ class CrawlSpider(Spider):\n         rule = self._rules[failure.request.meta['rule']]\n         return self._handle_failure(failure, rule.errback)\n \n-    def _parse_response(self, response, callback, cb_kwargs, follow=True):\n+    async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n+            if isinstance(cb_res, Awaitable):\n+                cb_res = await cb_res\n             cb_res = self.process_results(response, cb_res)\n             for request_or_item in iterate_spider_output(cb_res):\n                 yield request_or_item\n\n@@ -369,6 +369,18 @@ class CrawlSpiderWithParseMethod(MockServerSpider, CrawlSpider):\n         yield Request(self.mockserver.url(\"/status?n=202\"), self.parse, cb_kwargs={\"foo\": \"bar\"})\n \n \n+class CrawlSpiderWithAsyncCallback(CrawlSpiderWithParseMethod):\n+    \"\"\"A CrawlSpider with an async def callback\"\"\"\n+    name = 'crawl_spider_with_async_callback'\n+    rules = (\n+        Rule(LinkExtractor(), callback='parse_async', follow=True),\n+    )\n+\n+    async def parse_async(self, response, foo=None):\n+        self.logger.info('[parse_async] status %i (foo: %s)', response.status, foo)\n+        return Request(self.mockserver.url(\"/status?n=202\"), self.parse_async, cb_kwargs={\"foo\": \"bar\"})\n+\n+\n class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n     name = 'crawl_spider_with_errback'\n     rules = (\n\n@@ -37,6 +37,7 @@ from tests.spiders import (\n     BrokenStartRequestsSpider,\n     BytesReceivedCallbackSpider,\n     BytesReceivedErrbackSpider,\n+    CrawlSpiderWithAsyncCallback,\n     CrawlSpiderWithErrback,\n     CrawlSpiderWithParseMethod,\n     DelaySpider,\n@@ -391,6 +392,16 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIn(\"[parse] status 201 (foo: None)\", str(log))\n         self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n \n+    @defer.inlineCallbacks\n+    def test_crawlspider_with_async_callback(self):\n+        crawler = get_crawler(CrawlSpiderWithAsyncCallback)\n+        with LogCapture() as log:\n+            yield crawler.crawl(mockserver=self.mockserver)\n+\n+        self.assertIn(\"[parse_async] status 200 (foo: None)\", str(log))\n+        self.assertIn(\"[parse_async] status 201 (foo: None)\", str(log))\n+        self.assertIn(\"[parse_async] status 202 (foo: bar)\", str(log))\n+\n     @defer.inlineCallbacks\n     def test_crawlspider_with_errback(self):\n         crawler = get_crawler(CrawlSpiderWithErrback)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#da8f915091a769b3c3aca0f60d2370c2240213a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 36 | Churn Cumulative: 2139 | Contributors (this commit): 20 | Commits (past 90d): 9 | Contributors (cumulative): 39 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -6,11 +6,12 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n import copy\n-from typing import Awaitable, Sequence\n+from typing import AsyncIterable, Awaitable, Sequence\n \n-from scrapy.http import Request, HtmlResponse\n+from scrapy.http import Request, Response, HtmlResponse\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n+from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.spider import iterate_spider_output\n \n \n@@ -78,7 +79,7 @@ class CrawlSpider(Spider):\n     def parse_start_url(self, response, **kwargs):\n         return []\n \n-    def process_results(self, response, results):\n+    def process_results(self, response: Response, results: list):\n         return results\n \n     def _build_request(self, rule_index, link):\n@@ -112,7 +113,9 @@ class CrawlSpider(Spider):\n     async def _parse_response(self, response, callback, cb_kwargs, follow=True):\n         if callback:\n             cb_res = callback(response, **cb_kwargs) or ()\n-            if isinstance(cb_res, Awaitable):\n+            if isinstance(cb_res, AsyncIterable):\n+                cb_res = await collect_asyncgen(cb_res)\n+            elif isinstance(cb_res, Awaitable):\n                 cb_res = await cb_res\n             cb_res = self.process_results(response, cb_res)\n             for request_or_item in iterate_spider_output(cb_res):\n\n@@ -1,7 +1,7 @@\n from typing import AsyncGenerator, AsyncIterable, Iterable, Union\n \n \n-async def collect_asyncgen(result: AsyncIterable):\n+async def collect_asyncgen(result: AsyncIterable) -> list:\n     results = []\n     async for x in result:\n         results.append(x)\n\n@@ -381,6 +381,18 @@ class CrawlSpiderWithAsyncCallback(CrawlSpiderWithParseMethod):\n         return Request(self.mockserver.url(\"/status?n=202\"), self.parse_async, cb_kwargs={\"foo\": \"bar\"})\n \n \n+class CrawlSpiderWithAsyncGeneratorCallback(CrawlSpiderWithParseMethod):\n+    \"\"\"A CrawlSpider with an async generator callback\"\"\"\n+    name = 'crawl_spider_with_async_generator_callback'\n+    rules = (\n+        Rule(LinkExtractor(), callback='parse_async_gen', follow=True),\n+    )\n+\n+    async def parse_async_gen(self, response, foo=None):\n+        self.logger.info('[parse_async_gen] status %i (foo: %s)', response.status, foo)\n+        yield Request(self.mockserver.url(\"/status?n=202\"), self.parse_async_gen, cb_kwargs={\"foo\": \"bar\"})\n+\n+\n class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n     name = 'crawl_spider_with_errback'\n     rules = (\n\n@@ -38,6 +38,7 @@ from tests.spiders import (\n     BytesReceivedCallbackSpider,\n     BytesReceivedErrbackSpider,\n     CrawlSpiderWithAsyncCallback,\n+    CrawlSpiderWithAsyncGeneratorCallback,\n     CrawlSpiderWithErrback,\n     CrawlSpiderWithParseMethod,\n     DelaySpider,\n@@ -402,6 +403,16 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIn(\"[parse_async] status 201 (foo: None)\", str(log))\n         self.assertIn(\"[parse_async] status 202 (foo: bar)\", str(log))\n \n+    @defer.inlineCallbacks\n+    def test_crawlspider_with_async_generator_callback(self):\n+        crawler = get_crawler(CrawlSpiderWithAsyncGeneratorCallback)\n+        with LogCapture() as log:\n+            yield crawler.crawl(mockserver=self.mockserver)\n+\n+        self.assertIn(\"[parse_async_gen] status 200 (foo: None)\", str(log))\n+        self.assertIn(\"[parse_async_gen] status 201 (foo: None)\", str(log))\n+        self.assertIn(\"[parse_async_gen] status 202 (foo: bar)\", str(log))\n+\n     @defer.inlineCallbacks\n     def test_crawlspider_with_errback(self):\n         crawler = get_crawler(CrawlSpiderWithErrback)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fa58ab21e4670db1b11b6097d809cd18dd2ca667", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 21 | Churn Cumulative: 952 | Contributors (this commit): 25 | Commits (past 90d): 1 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -180,23 +180,6 @@ def binary_is_text(data):\n     return all(c not in _BINARYCHARS for c in data)\n \n \n-def _getargspec_py23(func):\n-    \"\"\"_getargspec_py23(function) -> named tuple ArgSpec(args, varargs, keywords,\n-                                                        defaults)\n-\n-    Was identical to inspect.getargspec() in python2, but uses\n-    inspect.getfullargspec() for python3 behind the scenes to avoid\n-    DeprecationWarning.\n-\n-    >>> def f(a, b=2, *ar, **kw):\n-    ...     pass\n-\n-    >>> _getargspec_py23(f)\n-    ArgSpec(args=['a', 'b'], varargs='ar', keywords='kw', defaults=(2,))\n-    \"\"\"\n-    return inspect.ArgSpec(*inspect.getfullargspec(func)[:4])\n-\n-\n def get_func_args(func, stripself=False):\n     \"\"\"Return the argument name list of a callable\"\"\"\n     if inspect.isfunction(func):\n@@ -248,9 +231,9 @@ def get_spec(func):\n     \"\"\"\n \n     if inspect.isfunction(func) or inspect.ismethod(func):\n-        spec = _getargspec_py23(func)\n+        spec = inspect.getfullargspec(func)\n     elif hasattr(func, '__call__'):\n-        spec = _getargspec_py23(func.__call__)\n+        spec = inspect.getfullargspec(func.__call__)\n     else:\n         raise TypeError(f'{type(func)} is not callable')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#92f2d75ed39e4d75f6cca23b8d3aaf934677f4f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 637 | Contributors (this commit): 31 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -83,6 +83,7 @@ setup(\n         'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: 3.9',\n         'Programming Language :: Python :: 3.10',\n+        'Programming Language :: Python :: 3.11',\n         'Programming Language :: Python :: Implementation :: CPython',\n         'Programming Language :: Python :: Implementation :: PyPy',\n         'Topic :: Internet :: WWW/HTTP',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5fa613b419f0d94b7dca8ae4ba2782ca268a9d7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 10 | Churn Cumulative: 228 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,16 +42,6 @@ def chdir(tmpdir):\n     tmpdir.chdir()\n \n \n-def pytest_collection_modifyitems(session, config, items):\n-    # Avoid executing tests when executing `--flake8` flag (pytest-flake8)\n-    try:\n-        from pytest_flake8 import Flake8Item\n-        if config.getoption('--flake8'):\n-            items[:] = [item for item in items if isinstance(item, Flake8Item)]\n-    except ImportError:\n-        pass\n-\n-\n def pytest_addoption(parser):\n     parser.addoption(\n         \"--reactor\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eeb199adda2d7b56c78879df79c7294148ca15f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 17 | Files Changed: 7 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 216 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,6 @@ from typing import Optional\n from scrapy import Spider\n from scrapy.crawler import CrawlerProcess\n from scrapy.utils.defer import deferred_from_coro\n-from twisted.internet.defer import Deferred\n \n \n class UppercasePipeline:\n\n@@ -6,8 +6,8 @@ if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n     asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncioreactor.install(asyncio.get_event_loop())\n \n-import scrapy\n-from scrapy.crawler import CrawlerProcess\n+import scrapy  # noqa: E402\n+from scrapy.crawler import CrawlerProcess  # noqa: E402\n \n \n class NoRequestsSpider(scrapy.Spider):\n\n@@ -1,6 +1,6 @@\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from twisted.internet import reactor\n+from twisted.internet import reactor  # noqa: F401\n \n \n class NoRequestsSpider(scrapy.Spider):\n@@ -14,4 +14,3 @@ process = CrawlerProcess(settings={})\n \n process.crawl(NoRequestsSpider)\n process.start()\n-\n\n@@ -1,6 +1,6 @@\n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from twisted.internet import reactor\n+from twisted.internet import reactor  # noqa: F401\n \n \n class NoRequestsSpider(scrapy.Spider):\n@@ -16,5 +16,3 @@ process = CrawlerProcess(settings={\n \n process.crawl(NoRequestsSpider)\n process.start()\n-\n-\n\n@@ -15,5 +15,3 @@ process = CrawlerProcess(settings={})\n \n process.crawl(NoRequestsSpider)\n process.start()\n-\n-\n\n@@ -25,7 +25,3 @@ process = CrawlerProcess(settings={\n \n process.crawl(NoRequestsSpider)\n process.start()\n-\n-\n-\n-\n\n@@ -17,6 +17,3 @@ process = CrawlerProcess(settings={\n \n process.crawl(NoRequestsSpider)\n process.start()\n-\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5bf42606792c69268cc34da287293e9406e25883", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1090 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -705,7 +705,8 @@ class HtmlResponseTest(TextResponseTest):\n \n     def test_html_encoding(self):\n \n-        body = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n+        body = b\"\"\"<html><head><title>Some page</title>\n+        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\">\n         </head><body>Price: \\xa3100</body></html>'\n         \"\"\"\n         r1 = self.response_class(\"http://www.example.com\", body=body)\n@@ -719,7 +720,8 @@ class HtmlResponseTest(TextResponseTest):\n         self._assert_response_values(r2, 'iso-8859-1', body)\n \n         # for conflicting declarations headers must take precedence\n-        body = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n+        body = b\"\"\"<html><head><title>Some page</title>\n+        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\n         </head><body>Price: \\xa3100</body></html>'\n         \"\"\"\n         r3 = self.response_class(\"http://www.example.com\", body=body,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#715c05d504d22e87935ae42cee55ee35b12c2ebd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2304 | Contributors (this commit): 34 | Commits (past 90d): 1 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -384,8 +384,7 @@ class ScrapyAgent:\n                 logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                              {\"request\": request, \"handler\": handler.__qualname__})\n                 txresponse._transport.stopProducing()\n-                with suppress(AttributeError):\n-                    txresponse._transport._producer.loseConnection()\n+                txresponse._transport.loseConnection()\n                 return {\n                     \"txresponse\": txresponse,\n                     \"body\": b\"\",\n@@ -417,7 +416,7 @@ class ScrapyAgent:\n \n             logger.warning(warning_msg, warning_args)\n \n-            txresponse._transport._producer.loseConnection()\n+            txresponse._transport.loseConnection()\n             raise defer.CancelledError(warning_msg % warning_args)\n \n         if warnsize and expected_size > warnsize:\n@@ -543,7 +542,7 @@ class _ResponseReader(protocol.Protocol):\n                 logger.debug(\"Download stopped for %(request)s from signal handler %(handler)s\",\n                              {\"request\": self._request, \"handler\": handler.__qualname__})\n                 self.transport.stopProducing()\n-                self.transport._producer.loseConnection()\n+                self.transport.loseConnection()\n                 failure = result if result.value.fail else None\n                 self._finish_response(flags=[\"download_stopped\"], failure=failure)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#043575123c57db9f055c3668a3d80485989df0b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 70 | Churn Cumulative: 1194 | Contributors (this commit): 23 | Commits (past 90d): 2 | Contributors (cumulative): 33 | DMM Complexity: 0.7619047619047619\n\nDIFF:\n@@ -5,6 +5,8 @@ from typing import Dict\n from itemadapter import is_item, ItemAdapter\n from w3lib.url import is_url\n \n+from twisted.internet.defer import maybeDeferred\n+\n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.http import Request\n from scrapy.utils import display\n@@ -110,16 +112,19 @@ class Command(BaseRunSpiderCommand):\n             if not opts.nolinks:\n                 self.print_requests(colour=colour)\n \n-    def run_callback(self, response, callback, cb_kwargs=None):\n-        cb_kwargs = cb_kwargs or {}\n+    def _get_items_and_requests(self, spider_output, opts, depth, spider, callback):\n         items, requests = [], []\n-\n-        for x in iterate_spider_output(callback(response, **cb_kwargs)):\n+        for x in spider_output:\n             if is_item(x):\n                 items.append(x)\n             elif isinstance(x, Request):\n                 requests.append(x)\n-        return items, requests\n+        return items, requests, opts, depth, spider, callback\n+\n+    def run_callback(self, response, callback, cb_kwargs=None):\n+        cb_kwargs = cb_kwargs or {}\n+        d = maybeDeferred(iterate_spider_output, callback(response, **cb_kwargs))\n+        return d\n \n     def get_callback_from_rules(self, spider, response):\n         if getattr(spider, 'rules', None):\n@@ -158,6 +163,25 @@ class Command(BaseRunSpiderCommand):\n             logger.error('No response downloaded for: %(url)s',\n                          {'url': url})\n \n+    def scraped_data(self, args):\n+        items, requests, opts, depth, spider, callback = args\n+        if opts.pipelines:\n+            itemproc = self.pcrawler.engine.scraper.itemproc\n+            for item in items:\n+                itemproc.process_item(item, spider)\n+        self.add_items(depth, items)\n+        self.add_requests(depth, requests)\n+\n+        scraped_data = items if opts.output else []\n+        if depth < opts.depth:\n+            for req in requests:\n+                req.meta['_depth'] = depth + 1\n+                req.meta['_callback'] = req.callback\n+                req.callback = callback\n+            scraped_data += requests\n+\n+        return scraped_data\n+\n     def prepare_request(self, spider, request, opts):\n         def callback(response, **cb_kwargs):\n             # memorize first request\n@@ -191,23 +215,10 @@ class Command(BaseRunSpiderCommand):\n             # parse items and requests\n             depth = response.meta['_depth']\n \n-            items, requests = self.run_callback(response, cb, cb_kwargs)\n-            if opts.pipelines:\n-                itemproc = self.pcrawler.engine.scraper.itemproc\n-                for item in items:\n-                    itemproc.process_item(item, spider)\n-            self.add_items(depth, items)\n-            self.add_requests(depth, requests)\n-\n-            scraped_data = items if opts.output else []\n-            if depth < opts.depth:\n-                for req in requests:\n-                    req.meta['_depth'] = depth + 1\n-                    req.meta['_callback'] = req.callback\n-                    req.callback = callback\n-                scraped_data += requests\n-\n-            return scraped_data\n+            d = self.run_callback(response, cb, cb_kwargs)\n+            d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)\n+            d.addCallback(self.scraped_data)\n+            return d\n \n         # update request meta if any extra meta was passed through the --meta/-m opts.\n         if opts.meta:\n\n@@ -29,7 +29,15 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n import scrapy\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import CrawlSpider, Rule\n+from scrapy.utils.test import get_from_asyncio_queue\n \n+class AsyncDefAsyncioSpider(scrapy.Spider):\n+\n+    name = 'asyncdef{self.spider_name}'\n+\n+    async def parse(self, response):\n+        status = await get_from_asyncio_queue(response.status)\n+        return [scrapy.Item(), dict(foo='bar')]\n \n class MySpider(scrapy.Spider):\n     name = '{self.spider_name}'\n@@ -160,6 +168,13 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n                                            self.url('/html')])\n         self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n \n+    @defer.inlineCallbacks\n+    def test_asyncio_parse_items(self):\n+        status, out, stderr = yield self.execute(\n+            ['--spider', 'asyncdef' + self.spider_name, '-c', 'parse', self.url('/html')]\n+        )\n+        self.assertIn(\"\"\"[{}, {'foo': 'bar'}]\"\"\", _textmode(out))\n+\n     @defer.inlineCallbacks\n     def test_parse_items(self):\n         status, out, stderr = yield self.execute(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#75bb516edbc39f9a657e71e75f05f0fd6a33d60d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 1548 | Contributors (this commit): 24 | Commits (past 90d): 1 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -689,8 +689,15 @@ class MySpider(scrapy.Spider):\n         ])\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    def test_asyncio_enabled_false(self):\n+    def test_asyncio_enabled_default(self):\n         log = self.get_log(self.debug_log_spider, args=[])\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+\n+    def test_asyncio_enabled_false(self):\n+        log = self.get_log(self.debug_log_spider, args=[\n+            '-s', 'TWISTED_REACTOR=twisted.internet.selectreactor.SelectReactor'\n+        ])\n+        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#960a7f68f6939744b39d528e28f8f925a2e12ad0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 3 | Files Changed: 5 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 8/2 | Churn Δ: 108 | Churn Cumulative: 3161 | Contributors (this commit): 36 | Commits (past 90d): 9 | Contributors (cumulative): 58 | DMM Complexity: 0.9210526315789473\n\nDIFF:\n@@ -31,7 +31,12 @@ from scrapy.utils.log import (\n )\n from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n-from scrapy.utils.reactor import install_reactor, verify_installed_reactor\n+from scrapy.utils.reactor import (\n+    install_reactor,\n+    is_asyncio_reactor_installed,\n+    verify_installed_asyncio_event_loop,\n+    verify_installed_reactor,\n+)\n \n \n logger = logging.getLogger(__name__)\n@@ -78,17 +83,20 @@ class Crawler:\n             crawler=self,\n         )\n \n-        reactor_class = self.settings.get(\"TWISTED_REACTOR\")\n+        reactor_class = self.settings[\"TWISTED_REACTOR\"]\n+        event_loop = self.settings[\"ASYNCIO_EVENT_LOOP\"]\n         if init_reactor:\n             # this needs to be done after the spider settings are merged,\n             # but before something imports twisted.internet.reactor\n             if reactor_class:\n-                install_reactor(reactor_class, self.settings[\"ASYNCIO_EVENT_LOOP\"])\n+                install_reactor(reactor_class, event_loop)\n             else:\n                 from twisted.internet import reactor  # noqa: F401\n             log_reactor_info()\n         if reactor_class:\n             verify_installed_reactor(reactor_class)\n+            if is_asyncio_reactor_installed() and event_loop:\n+                verify_installed_asyncio_event_loop(event_loop)\n \n         self.extensions = ExtensionManager.from_crawler(self)\n \n\n@@ -90,6 +90,24 @@ def verify_installed_reactor(reactor_path):\n         raise Exception(msg)\n \n \n+def verify_installed_asyncio_event_loop(loop_path):\n+    from twisted.internet import reactor\n+    loop_class = load_object(loop_path)\n+    if isinstance(reactor._asyncioEventloop, loop_class):\n+        return\n+    installed = (\n+        f\"{reactor._asyncioEventloop.__class__.__module__}\"\n+        f\".{reactor._asyncioEventloop.__class__.__qualname__}\"\n+    )\n+    specified = f\"{loop_class.__module__}.{loop_class.__qualname__}\"\n+    raise Exception(\n+        \"Scrapy found an asyncio Twisted reactor already \"\n+        f\"installed, and its event loop class ({installed}) does \"\n+        \"not match the one specified in the ASYNCIO_EVENT_LOOP \"\n+        f\"setting ({specified})\"\n+    )\n+\n+\n def is_asyncio_reactor_installed():\n     from twisted.internet import reactor\n     return isinstance(reactor, asyncioreactor.AsyncioSelectorReactor)\n\n@@ -0,0 +1,25 @@\n+import asyncio\n+import sys\n+\n+from twisted.internet import asyncioreactor\n+if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+asyncioreactor.install(asyncio.get_event_loop())\n+\n+import scrapy  # noqa: E402\n+from scrapy.crawler import CrawlerProcess  # noqa: E402\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+})\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -0,0 +1,28 @@\n+import asyncio\n+import sys\n+\n+from uvloop import Loop\n+\n+from twisted.internet import asyncioreactor\n+if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+asyncio.set_event_loop(Loop())\n+asyncioreactor.install(asyncio.get_event_loop())\n+\n+import scrapy  # noqa: E402\n+from scrapy.crawler import CrawlerProcess  # noqa: E402\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n+    \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n+})\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -454,6 +454,29 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n+    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n+    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n+    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    def test_asyncio_enabled_reactor_same_loop(self):\n+        log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n+        self.assertIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n+\n+    @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n+    @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n+    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n+    def test_asyncio_enabled_reactor_different_loop(self):\n+        log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n+        self.assertNotIn(\"Spider closed (finished)\", log)\n+        self.assertIn(\n+            (\n+                \"does not match the one specified in the ASYNCIO_EVENT_LOOP \"\n+                \"setting (uvloop.Loop)\"\n+            ),\n+            log,\n+        )\n+\n     def test_default_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#06c8f673afe9af08784e62d67930a8cbc9887ced", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 25 | Files Changed: 11 | Hunks: 23 | Methods Changed: 18 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 50 | Churn Cumulative: 6467 | Contributors (this commit): 79 | Commits (past 90d): 25 | Contributors (cumulative): 170 | DMM Complexity: None\n\nDIFF:\n@@ -15,7 +15,7 @@ class SettingsListDirective(Directive):\n \n \n def is_setting_index(node):\n-    if node.tagname == 'index':\n+    if node.tagname == 'index' and node['entries']:\n         # index entries for setting directives look like:\n         # [('pair', 'SETTING_NAME; setting', 'std:setting-SETTING_NAME', '')]\n         entry_type, info, refid = node['entries'][0][:3]\n\n@@ -248,7 +248,7 @@ REFERER_ENABLED = True\n REFERRER_POLICY = 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'\n \n REQUEST_FINGERPRINTER_CLASS = 'scrapy.utils.request.RequestFingerprinter'\n-REQUEST_FINGERPRINTER_IMPLEMENTATION = 'PREVIOUS_VERSION'\n+REQUEST_FINGERPRINTER_IMPLEMENTATION = '2.6'\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n\n@@ -236,10 +236,10 @@ class RequestFingerprinter:\n                 'REQUEST_FINGERPRINTER_IMPLEMENTATION'\n             )\n         else:\n-            implementation = 'PREVIOUS_VERSION'\n-        if implementation == 'PREVIOUS_VERSION':\n+            implementation = '2.6'\n+        if implementation == '2.6':\n             message = (\n-                '\\'PREVIOUS_VERSION\\' is a deprecated value for the '\n+                '\\'2.6\\' is a deprecated value for the '\n                 '\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\' setting.\\n'\n                 '\\n'\n                 'It is also the default value. In other words, it is normal '\n@@ -254,14 +254,14 @@ class RequestFingerprinter:\n             )\n             warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             self._fingerprint = _request_fingerprint_as_bytes\n-        elif implementation == 'VERSION':\n+        elif implementation == '2.7':\n             self._fingerprint = fingerprint\n         else:\n             raise ValueError(\n                 f'Got an invalid value on setting '\n                 f'\\'REQUEST_FINGERPRINTER_IMPLEMENTATION\\': '\n-                f'{implementation!r}. Valid values are \\'PREVIOUS_VERSION\\' (deprecated) '\n-                f'and \\'VERSION\\'.'\n+                f'{implementation!r}. Valid values are \\'2.6\\' (deprecated) '\n+                f'and \\'2.7\\'.'\n             )\n \n     def fingerprint(self, request):\n\n@@ -65,7 +65,7 @@ def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):\n     # Set by default settings that prevent deprecation warnings.\n     settings = {}\n     if prevent_warnings:\n-        settings['REQUEST_FINGERPRINTER_IMPLEMENTATION'] = 'VERSION'\n+        settings['REQUEST_FINGERPRINTER_IMPLEMENTATION'] = '2.7'\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     return runner.create_crawler(spidercls or Spider)\n\n@@ -350,7 +350,7 @@ with multiples lines\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n-        runner = CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'})\n+        runner = CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'})\n         runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n \n\n@@ -110,7 +110,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n                 'LOG_LEVEL': 'INFO',\n                 'LOG_FILE': log_file,\n                 # settings to avoid extra warnings\n-                'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n+                'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n                 'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n             }\n \n@@ -235,7 +235,7 @@ class NoRequestsSpider(scrapy.Spider):\n class CrawlerRunnerHasSpider(unittest.TestCase):\n \n     def _runner(self):\n-        return CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'})\n+        return CrawlerRunner({'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'})\n \n     @defer.inlineCallbacks\n     def test_crawler_runner_bootstrap_successful(self):\n@@ -283,14 +283,14 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n         if self.reactor_pytest == 'asyncio':\n             CrawlerRunner(settings={\n                 \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"VERSION\",\n+                \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n             })\n         else:\n             msg = r\"The installed reactor \\(.*?\\) does not match the requested one \\(.*?\\)\"\n             with self.assertRaisesRegex(Exception, msg):\n                 runner = CrawlerRunner(settings={\n                     \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n-                    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"VERSION\",\n+                    \"REQUEST_FINGERPRINTER_IMPLEMENTATION\": \"2.7\",\n                 })\n                 yield runner.crawl(NoRequestsSpider)\n \n\n@@ -51,7 +51,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_df_from_crawler_scheduler(self):\n         settings = {'DUPEFILTER_DEBUG': True,\n                     'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n+                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n@@ -60,7 +60,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_df_from_settings_scheduler(self):\n         settings = {'DUPEFILTER_DEBUG': True,\n                     'DUPEFILTER_CLASS': FromSettingsRFPDupeFilter,\n-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n+                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertTrue(scheduler.df.debug)\n@@ -68,7 +68,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n \n     def test_df_direct_scheduler(self):\n         settings = {'DUPEFILTER_CLASS': DirectDupeFilter,\n-                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n+                    'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n         crawler = get_crawler(settings_dict=settings)\n         scheduler = Scheduler.from_crawler(crawler)\n         self.assertEqual(scheduler.df.method, 'n/a')\n@@ -172,7 +172,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': False,\n                         'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n+                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n@@ -199,7 +199,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': True,\n                         'DUPEFILTER_CLASS': FromCrawlerRFPDupeFilter,\n-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n+                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n@@ -233,7 +233,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n     def test_log_debug_default_dupefilter(self):\n         with LogCapture() as log:\n             settings = {'DUPEFILTER_DEBUG': True,\n-                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION'}\n+                        'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7'}\n             crawler = get_crawler(SimpleSpider, settings_dict=settings)\n             spider = SimpleSpider.from_crawler(crawler)\n             dupefilter = _get_dupefilter(crawler=crawler)\n\n@@ -64,7 +64,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.tmpmediastore = self.mktemp()\n         os.mkdir(self.tmpmediastore)\n         self.settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n             'ITEM_PIPELINES': {self.pipeline_class: 1},\n             self.store_setting_key: self.tmpmediastore,\n         }\n\n@@ -52,7 +52,7 @@ class MockCrawler(Crawler):\n             SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n             JOBDIR=jobdir,\n             DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter',\n-            REQUEST_FINGERPRINTER_IMPLEMENTATION='VERSION',\n+            REQUEST_FINGERPRINTER_IMPLEMENTATION='2.7',\n         )\n         super().__init__(Spider, settings)\n         self.engine = MockEngine(downloader=MockDownloader())\n\n@@ -98,7 +98,7 @@ class SpiderLoaderTest(unittest.TestCase):\n         module = 'tests.test_spiderloader.test_spiders.spider1'\n         runner = CrawlerRunner({\n             'SPIDER_MODULES': [module],\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n         })\n \n         self.assertRaisesRegex(KeyError, 'Spider not found',\n\n@@ -505,7 +505,7 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n \n     def test_deprecated_implementation(self):\n         settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'PREVIOUS_VERSION',\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.6',\n         }\n         with warnings.catch_warnings(record=True) as logged_warnings:\n             crawler = get_crawler(settings_dict=settings)\n@@ -518,7 +518,7 @@ class RequestFingerprinterTestCase(unittest.TestCase):\n \n     def test_recommended_implementation(self):\n         settings = {\n-            'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n+            'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n         }\n         with warnings.catch_warnings(record=True) as logged_warnings:\n             crawler = get_crawler(settings_dict=settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40d9ca3bdd6fd50438295f91cff275fcac57e0fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 564 | Lines Deleted: 606 | Files Changed: 56 | Hunks: 405 | Methods Changed: 199 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 1170 | Churn Cumulative: 29919 | Contributors (this commit): 177 | Commits (past 90d): 77 | Contributors (cumulative): 788 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,7 +21,7 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-with open('tests/ignores.txt') as reader:\n+with Path('tests/ignores.txt').open() as reader:\n     for line in reader:\n         file_path = line.strip()\n         if file_path and file_path[0] != '#':\n\n@@ -11,13 +11,12 @@\n \n import sys\n from datetime import datetime\n-from os import path\n+from pathlib import Path\n \n # If your extensions are in another directory, add it here. If the directory\n-# is relative to the documentation root, use os.path.abspath to make it\n-# absolute, like shown here.\n-sys.path.append(path.join(path.dirname(__file__), \"_ext\"))\n-sys.path.insert(0, path.dirname(path.dirname(__file__)))\n+# is relative to the documentation root, use Path.absolute to make it absolute.\n+sys.path.append(str(Path(__file__).parent / \"_ext\"))\n+sys.path.insert(0, str(Path(__file__).parent.parent))\n \n \n # General configuration\n\n@@ -1,5 +1,5 @@\n-import os\n from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\n+from pathlib import Path\n \n from scrapy.http.response.html import HtmlResponse\n from sybil import Sybil\n@@ -12,10 +12,9 @@ from sybil.parsers.doctest import DocTestParser\n from sybil.parsers.skip import skip\n \n \n-def load_response(url, filename):\n-    input_path = os.path.join(os.path.dirname(__file__), '_tests', filename)\n-    with open(input_path, 'rb') as input_file:\n-        return HtmlResponse(url, body=input_file.read())\n+def load_response(url: str, filename: str) -> HtmlResponse:\n+    input_path = Path(__file__).parent / '_tests' / filename\n+    return HtmlResponse(url, body=input_path.read_bytes())\n \n \n def setup(namespace):\n\n@@ -13,6 +13,7 @@ Author: dufferzafar\n \"\"\"\n \n import re\n+from pathlib import Path\n \n \n def main():\n@@ -27,7 +28,7 @@ def main():\n \n     # Read lines from the linkcheck output file\n     try:\n-        with open(\"build/linkcheck/output.txt\") as out:\n+        with Path(\"build/linkcheck/output.txt\").open() as out:\n             output_lines = out.readlines()\n     except IOError:\n         print(\"linkcheck output not found; please run linkcheck first.\")\n@@ -51,14 +52,12 @@ def main():\n \n                     # Update the previous file\n                     if _filename:\n-                        with open(_filename, \"w\") as _file:\n-                            _file.write(_contents)\n+                        Path(_filename).write_text(_contents)\n \n                     _filename = newfilename\n \n                     # Read the new file to memory\n-                    with open(_filename) as _file:\n-                        _contents = _file.read()\n+                    _contents = Path(_filename).read_text()\n \n                 _contents = _contents.replace(match.group(3), match.group(4))\n         else:\n\n@@ -3,6 +3,7 @@ Base class for Scrapy commands\n \"\"\"\n import os\n import argparse\n+from pathlib import Path\n from typing import Any, Dict\n \n from twisted.python import failure\n@@ -93,8 +94,7 @@ class ScrapyCommand:\n             self.settings.set('LOG_ENABLED', False, priority='cmdline')\n \n         if opts.pidfile:\n-            with open(opts.pidfile, \"w\") as f:\n-                f.write(str(os.getpid()) + os.linesep)\n+            Path(opts.pidfile).write_text(str(os.getpid()) + os.linesep)\n \n         if opts.pdb:\n             failure.startDebugMode()\n\n@@ -2,8 +2,9 @@ import os\n import shutil\n import string\n \n+from pathlib import Path\n from importlib import import_module\n-from os.path import join, dirname, abspath, exists, splitext\n+from typing import Optional\n from urllib.parse import urlparse\n \n import scrapy\n@@ -62,8 +63,7 @@ class Command(ScrapyCommand):\n         if opts.dump:\n             template_file = self._find_template(opts.dump)\n             if template_file:\n-                with open(template_file, \"r\") as f:\n-                    print(f.read())\n+                print(template_file.read_text())\n             return\n         if len(args) != 2:\n             raise UsageError()\n@@ -98,11 +98,11 @@ class Command(ScrapyCommand):\n         }\n         if self.settings.get('NEWSPIDER_MODULE'):\n             spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n-            spiders_dir = abspath(dirname(spiders_module.__file__))\n+            spiders_dir = Path(spiders_module.__file__).parent.resolve()\n         else:\n             spiders_module = None\n-            spiders_dir = \".\"\n-        spider_file = f\"{join(spiders_dir, module)}.py\"\n+            spiders_dir = Path(\".\")\n+        spider_file = f\"{spiders_dir / module}.py\"\n         shutil.copyfile(template_file, spider_file)\n         render_templatefile(spider_file, **tvars)\n         print(f\"Created spider {name!r} using template {template_name!r} \",\n@@ -110,24 +110,25 @@ class Command(ScrapyCommand):\n         if spiders_module:\n             print(f\"in module:\\n  {spiders_module.__name__}.{module}\")\n \n-    def _find_template(self, template):\n-        template_file = join(self.templates_dir, f'{template}.tmpl')\n-        if exists(template_file):\n+    def _find_template(self, template: str) -> Optional[Path]:\n+        template_file = Path(self.templates_dir, f'{template}.tmpl')\n+        if template_file.exists():\n             return template_file\n         print(f\"Unable to find template: {template}\\n\")\n         print('Use \"scrapy genspider --list\" to see all available templates.')\n \n     def _list_templates(self):\n         print(\"Available templates:\")\n-        for filename in sorted(os.listdir(self.templates_dir)):\n-            if filename.endswith('.tmpl'):\n-                print(f\"  {splitext(filename)[0]}\")\n+        for file in sorted(Path(self.templates_dir).iterdir()):\n+            if file.suffix == '.tmpl':\n+                print(f\"  {file.stem}\")\n \n-    def _spider_exists(self, name):\n+    def _spider_exists(self, name: str) -> bool:\n         if not self.settings.get('NEWSPIDER_MODULE'):\n             # if run as a standalone command and file with same filename already exists\n-            if exists(name + \".py\"):\n-                print(f\"{abspath(name + '.py')} already exists\")\n+            path = Path(name + \".py\")\n+            if path.exists():\n+                print(f\"{path.resolve()} already exists\")\n                 return True\n             return False\n \n@@ -143,17 +144,18 @@ class Command(ScrapyCommand):\n \n         # a file with the same name exists in the target directory\n         spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n-        spiders_dir = dirname(spiders_module.__file__)\n-        spiders_dir_abs = abspath(spiders_dir)\n-        if exists(join(spiders_dir_abs, name + \".py\")):\n-            print(f\"{join(spiders_dir_abs, (name + '.py'))} already exists\")\n+        spiders_dir = Path(spiders_module.__file__).parent\n+        spiders_dir_abs = spiders_dir.resolve()\n+        path = spiders_dir_abs / (name + \".py\")\n+        if path.exists():\n+            print(f\"{path} already exists\")\n             return True\n \n         return False\n \n     @property\n-    def templates_dir(self):\n-        return join(\n-            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n+    def templates_dir(self) -> str:\n+        return str(Path(\n+            self.settings['TEMPLATES_DIR'] or Path(scrapy.__path__[0], 'templates'),\n             'spiders'\n-        )\n+        ))\n\n@@ -1,22 +1,23 @@\n import sys\n-import os\n+from os import PathLike\n+from pathlib import Path\n from importlib import import_module\n+from types import ModuleType\n \n from scrapy.utils.spider import iter_spider_classes\n from scrapy.exceptions import UsageError\n from scrapy.commands import BaseRunSpiderCommand\n \n \n-def _import_file(filepath):\n-    abspath = os.path.abspath(filepath)\n-    dirname, file = os.path.split(abspath)\n-    fname, fext = os.path.splitext(file)\n-    if fext not in ('.py', '.pyw'):\n+def _import_file(filepath: str | PathLike[str]) -> ModuleType:\n+    abspath = Path(filepath).resolve()\n+    dirname = str(abspath.parent)\n+    if abspath.suffix not in ('.py', '.pyw'):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n     if dirname:\n         sys.path = [dirname] + sys.path\n     try:\n-        module = import_module(fname)\n+        module = import_module(abspath.stem)\n     finally:\n         if dirname:\n             sys.path.pop(0)\n@@ -40,13 +41,13 @@ class Command(BaseRunSpiderCommand):\n     def run(self, args, opts):\n         if len(args) != 1:\n             raise UsageError()\n-        filename = args[0]\n-        if not os.path.exists(filename):\n+        filename = Path(args[0])\n+        if not filename.exists():\n             raise UsageError(f\"File not found: {filename}\\n\")\n         try:\n             module = _import_file(filename)\n         except (ImportError, ValueError) as e:\n-            raise UsageError(f\"Unable to load {filename!r}: {e}\\n\")\n+            raise UsageError(f\"Unable to load {str(filename)!r}: {e}\\n\")\n         spclasses = list(iter_spider_classes(module))\n         if not spclasses:\n             raise UsageError(f\"No spider found in file: {filename}\\n\")\n\n@@ -2,7 +2,7 @@ import re\n import os\n import string\n from importlib.util import find_spec\n-from os.path import join, exists, abspath\n+from pathlib import Path\n from shutil import ignore_patterns, move, copy2, copystat\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n \n@@ -54,7 +54,7 @@ class Command(ScrapyCommand):\n             return True\n         return False\n \n-    def _copytree(self, src, dst):\n+    def _copytree(self, src: Path, dst: Path):\n         \"\"\"\n         Since the original function always creates the directory, to resolve\n         the issue a new function had to be created. It's a simple copy and\n@@ -64,19 +64,19 @@ class Command(ScrapyCommand):\n         https://github.com/scrapy/scrapy/pull/2005\n         \"\"\"\n         ignore = IGNORE\n-        names = os.listdir(src)\n+        names = [x.name for x in src.iterdir()]\n         ignored_names = ignore(src, names)\n \n-        if not os.path.exists(dst):\n-            os.makedirs(dst)\n+        if not dst.exists():\n+            dst.mkdir(parents=True)\n \n         for name in names:\n             if name in ignored_names:\n                 continue\n \n-            srcname = os.path.join(src, name)\n-            dstname = os.path.join(dst, name)\n-            if os.path.isdir(srcname):\n+            srcname = src / name\n+            dstname = dst / name\n+            if srcname.is_dir():\n                 self._copytree(srcname, dstname)\n             else:\n                 copy2(srcname, dstname)\n@@ -90,36 +90,36 @@ class Command(ScrapyCommand):\n             raise UsageError()\n \n         project_name = args[0]\n-        project_dir = args[0]\n \n         if len(args) == 2:\n-            project_dir = args[1]\n+            project_dir = Path(args[1])\n+        else:\n+            project_dir = Path(args[0])\n \n-        if exists(join(project_dir, 'scrapy.cfg')):\n+        if (project_dir / 'scrapy.cfg').exists():\n             self.exitcode = 1\n-            print(f'Error: scrapy.cfg already exists in {abspath(project_dir)}')\n+            print(f'Error: scrapy.cfg already exists in {project_dir.resolve()}')\n             return\n \n         if not self._is_valid_name(project_name):\n             self.exitcode = 1\n             return\n \n-        self._copytree(self.templates_dir, abspath(project_dir))\n-        move(join(project_dir, 'module'), join(project_dir, project_name))\n+        self._copytree(Path(self.templates_dir), project_dir.resolve())\n+        move(project_dir / 'module', project_dir / project_name)\n         for paths in TEMPLATES_TO_RENDER:\n-            path = join(*paths)\n-            tplfile = join(project_dir, string.Template(path).substitute(project_name=project_name))\n-            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n+            tplfile = Path(project_dir, *(string.Template(s).substitute(project_name=project_name) for s in paths))\n+            render_templatefile(str(tplfile), project_name=project_name, ProjectName=string_camelcase(project_name))\n         print(f\"New Scrapy project '{project_name}', using template directory \"\n               f\"'{self.templates_dir}', created in:\")\n-        print(f\"    {abspath(project_dir)}\\n\")\n+        print(f\"    {project_dir.resolve()}\\n\")\n         print(\"You can start your first spider with:\")\n         print(f\"    cd {project_dir}\")\n         print(\"    scrapy genspider example example.com\")\n \n     @property\n-    def templates_dir(self):\n-        return join(\n-            self.settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], 'templates'),\n+    def templates_dir(self) -> str:\n+        return str(Path(\n+            self.settings['TEMPLATES_DIR'] or Path(scrapy.__path__[0], 'templates'),\n             'project'\n-        )\n+        ))\n\n@@ -1,3 +1,5 @@\n+from pathlib import Path\n+\n from w3lib.url import file_uri_to_path\n \n from scrapy.responsetypes import responsetypes\n@@ -10,7 +12,6 @@ class FileDownloadHandler:\n     @defers\n     def download_request(self, request, spider):\n         filepath = file_uri_to_path(request.url)\n-        with open(filepath, 'rb') as fo:\n-            body = fo.read()\n+        body = Path(filepath).read_bytes()\n         respcls = responsetypes.from_args(filename=filepath, body=body)\n         return respcls(url=request.url, body=body)\n\n@@ -1,8 +1,7 @@\n import json\n import logging\n-import os\n from abc import abstractmethod\n-from os.path import exists, join\n+from pathlib import Path\n from typing import Optional, Type, TypeVar\n \n from twisted.internet.defer import Deferred\n@@ -324,19 +323,19 @@ class Scheduler(BaseScheduler):\n     def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n         \"\"\" Return a folder name to keep disk queue state at \"\"\"\n         if jobdir is not None:\n-            dqdir = join(jobdir, 'requests.queue')\n-            if not exists(dqdir):\n-                os.makedirs(dqdir)\n-            return dqdir\n+            dqdir = Path(jobdir, 'requests.queue')\n+            if not dqdir.exists():\n+                dqdir.mkdir(parents=True)\n+            return str(dqdir)\n         return None\n \n     def _read_dqs_state(self, dqdir: str) -> list:\n-        path = join(dqdir, 'active.json')\n-        if not exists(path):\n+        path = Path(dqdir, 'active.json')\n+        if not path.exists():\n             return []\n-        with open(path) as f:\n+        with path.open() as f:\n             return json.load(f)\n \n     def _write_dqs_state(self, dqdir: str, state: list) -> None:\n-        with open(join(dqdir, 'active.json'), 'w') as f:\n+        with Path(dqdir, 'active.json').open('w') as f:\n             json.dump(state, f)\n\n@@ -1,7 +1,10 @@\n+from __future__ import annotations\n+\n import logging\n import pprint\n import signal\n import warnings\n+from typing import TYPE_CHECKING\n \n from twisted.internet import defer\n from zope.interface.exceptions import DoesNotImplement\n@@ -33,6 +36,9 @@ from scrapy.utils.misc import create_instance, load_object\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.reactor import install_reactor, verify_installed_reactor\n \n+if TYPE_CHECKING:\n+    from scrapy.utils.request import RequestFingerprinter\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -72,7 +78,7 @@ class Crawler:\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n \n-        self.request_fingerprinter = create_instance(\n+        self.request_fingerprinter: RequestFingerprinter = create_instance(\n             load_object(self.settings['REQUEST_FINGERPRINTER_CLASS']),\n             settings=self.settings,\n             crawler=self,\n\n@@ -1,5 +1,5 @@\n import logging\n-import os\n+from pathlib import Path\n from typing import Optional, Set, Type, TypeVar\n from warnings import warn\n \n@@ -55,7 +55,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         self.debug = debug\n         self.logger = logging.getLogger(__name__)\n         if path:\n-            self.file = open(os.path.join(path, 'requests.seen'), 'a+')\n+            self.file = Path(path, 'requests.seen').open('a+')\n             self.file.seek(0)\n             self.fingerprints.update(x.rstrip() for x in self.file)\n \n\n@@ -5,13 +5,13 @@ See documentation in docs/topics/feed-exports.rst\n \"\"\"\n \n import logging\n-import os\n import re\n import sys\n import warnings\n from datetime import datetime\n+from pathlib import Path\n from tempfile import NamedTemporaryFile\n-from typing import Any, Callable, Optional, Tuple, Union\n+from typing import IO, Any, Callable, Optional, Tuple, Union\n from urllib.parse import unquote, urlparse\n \n from twisted.internet import defer, threads\n@@ -101,7 +101,7 @@ class BlockingFeedStorage:\n \n     def open(self, spider):\n         path = spider.crawler.settings['FEED_TEMPDIR']\n-        if path and not os.path.isdir(path):\n+        if path and not Path(path).is_dir():\n             raise OSError('Not a Directory: ' + str(path))\n \n         return NamedTemporaryFile(prefix='feed-', dir=path)\n@@ -141,11 +141,11 @@ class FileFeedStorage:\n         feed_options = feed_options or {}\n         self.write_mode = 'wb' if feed_options.get('overwrite', False) else 'ab'\n \n-    def open(self, spider):\n-        dirname = os.path.dirname(self.path)\n-        if dirname and not os.path.exists(dirname):\n-            os.makedirs(dirname)\n-        return open(self.path, self.write_mode)\n+    def open(self, spider) -> IO[Any]:\n+        dirname = Path(self.path).parent\n+        if dirname and not dirname.exists():\n+            dirname.mkdir(parents=True)\n+        return Path(self.path).open(self.write_mode)\n \n     def store(self, file):\n         file.close()\n\n@@ -1,16 +1,18 @@\n import gzip\n import logging\n-import os\n import pickle\n from email.utils import mktime_tz, parsedate_tz\n from importlib import import_module\n+from pathlib import Path\n from time import time\n from weakref import WeakKeyDictionary\n \n from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n \n from scrapy.http import Headers, Response\n+from scrapy.http.request import Request\n from scrapy.responsetypes import responsetypes\n+from scrapy.spiders import Spider\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.project import data_path\n from scrapy.utils.python import to_bytes, to_unicode\n@@ -221,9 +223,9 @@ class DbmCacheStorage:\n         self.dbmodule = import_module(settings['HTTPCACHE_DBM_MODULE'])\n         self.db = None\n \n-    def open_spider(self, spider):\n-        dbpath = os.path.join(self.cachedir, f'{spider.name}.db')\n-        self.db = self.dbmodule.open(dbpath, 'c')\n+    def open_spider(self, spider: Spider):\n+        dbpath = Path(self.cachedir, f'{spider.name}.db')\n+        self.db = self.dbmodule.open(str(dbpath), 'c')\n \n         logger.debug(\"Using DBM cache storage in %(cachepath)s\", {'cachepath': dbpath}, extra={'spider': spider})\n \n@@ -277,7 +279,7 @@ class FilesystemCacheStorage:\n         self.use_gzip = settings.getbool('HTTPCACHE_GZIP')\n         self._open = gzip.open if self.use_gzip else open\n \n-    def open_spider(self, spider):\n+    def open_spider(self, spider: Spider):\n         logger.debug(\"Using filesystem cache storage in %(cachedir)s\", {'cachedir': self.cachedir},\n                      extra={'spider': spider})\n \n@@ -286,15 +288,15 @@ class FilesystemCacheStorage:\n     def close_spider(self, spider):\n         pass\n \n-    def retrieve_response(self, spider, request):\n+    def retrieve_response(self, spider: Spider, request: Request):\n         \"\"\"Return response if present in cache, or None otherwise.\"\"\"\n         metadata = self._read_meta(spider, request)\n         if metadata is None:\n             return  # not cached\n         rpath = self._get_request_path(spider, request)\n-        with self._open(os.path.join(rpath, 'response_body'), 'rb') as f:\n+        with self._open(rpath / 'response_body', 'rb') as f:\n             body = f.read()\n-        with self._open(os.path.join(rpath, 'response_headers'), 'rb') as f:\n+        with self._open(rpath / 'response_headers', 'rb') as f:\n             rawheaders = f.read()\n         url = metadata.get('response_url')\n         status = metadata['status']\n@@ -303,11 +305,11 @@ class FilesystemCacheStorage:\n         response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n \n-    def store_response(self, spider, request, response):\n+    def store_response(self, spider: Spider, request: Request, response):\n         \"\"\"Store the given response in the cache.\"\"\"\n         rpath = self._get_request_path(spider, request)\n-        if not os.path.exists(rpath):\n-            os.makedirs(rpath)\n+        if not rpath.exists():\n+            rpath.mkdir(parents=True)\n         metadata = {\n             'url': request.url,\n             'method': request.method,\n@@ -315,29 +317,29 @@ class FilesystemCacheStorage:\n             'response_url': response.url,\n             'timestamp': time(),\n         }\n-        with self._open(os.path.join(rpath, 'meta'), 'wb') as f:\n+        with self._open(rpath / 'meta', 'wb') as f:\n             f.write(to_bytes(repr(metadata)))\n-        with self._open(os.path.join(rpath, 'pickled_meta'), 'wb') as f:\n+        with self._open(rpath / 'pickled_meta', 'wb') as f:\n             pickle.dump(metadata, f, protocol=4)\n-        with self._open(os.path.join(rpath, 'response_headers'), 'wb') as f:\n+        with self._open(rpath / 'response_headers', 'wb') as f:\n             f.write(headers_dict_to_raw(response.headers))\n-        with self._open(os.path.join(rpath, 'response_body'), 'wb') as f:\n+        with self._open(rpath / 'response_body', 'wb') as f:\n             f.write(response.body)\n-        with self._open(os.path.join(rpath, 'request_headers'), 'wb') as f:\n+        with self._open(rpath / 'request_headers', 'wb') as f:\n             f.write(headers_dict_to_raw(request.headers))\n-        with self._open(os.path.join(rpath, 'request_body'), 'wb') as f:\n+        with self._open(rpath / 'request_body', 'wb') as f:\n             f.write(request.body)\n \n-    def _get_request_path(self, spider, request):\n+    def _get_request_path(self, spider: Spider, request: Request) -> Path:\n         key = self._fingerprinter.fingerprint(request).hex()\n-        return os.path.join(self.cachedir, spider.name, key[0:2], key)\n+        return Path(self.cachedir, spider.name, key[0:2], key)\n \n-    def _read_meta(self, spider, request):\n+    def _read_meta(self, spider: Spider, request: Request):\n         rpath = self._get_request_path(spider, request)\n-        metapath = os.path.join(rpath, 'pickled_meta')\n-        if not os.path.exists(metapath):\n+        metapath = rpath / 'pickled_meta'\n+        if not metapath.exists():\n             return  # not found\n-        mtime = os.stat(metapath).st_mtime\n+        mtime = metapath.stat().st_mtime\n         if 0 < self.expiration_secs < time() - mtime:\n             return  # expired\n         with self._open(metapath, 'rb') as f:\n\n@@ -1,5 +1,5 @@\n-import os\n import pickle\n+from pathlib import Path\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n@@ -25,16 +25,16 @@ class SpiderState:\n \n     def spider_closed(self, spider):\n         if self.jobdir:\n-            with open(self.statefn, 'wb') as f:\n+            with Path(self.statefn).open('wb') as f:\n                 pickle.dump(spider.state, f, protocol=4)\n \n     def spider_opened(self, spider):\n-        if self.jobdir and os.path.exists(self.statefn):\n-            with open(self.statefn, 'rb') as f:\n+        if self.jobdir and Path(self.statefn).exists():\n+            with Path(self.statefn).open('rb') as f:\n                 spider.state = pickle.load(f)\n         else:\n             spider.state = {}\n \n     @property\n-    def statefn(self):\n-        return os.path.join(self.jobdir, 'spider.state')\n+    def statefn(self) -> str:\n+        return str(Path(self.jobdir, 'spider.state'))\n\n@@ -32,7 +32,7 @@ class Response(object_ref):\n \n     def __init__(\n         self,\n-        url,\n+        url: str,\n         status=200,\n         headers=None,\n         body=b\"\",\n@@ -75,7 +75,7 @@ class Response(object_ref):\n     def _get_url(self):\n         return self._url\n \n-    def _set_url(self, url):\n+    def _set_url(self, url: str):\n         if isinstance(url, str):\n             self._url = url\n         else:\n\n@@ -13,6 +13,8 @@ from collections import defaultdict\n from contextlib import suppress\n from ftplib import FTP\n from io import BytesIO\n+from pathlib import Path\n+from typing import Optional\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n@@ -39,41 +41,40 @@ class FileException(Exception):\n \n \n class FSFilesStore:\n-    def __init__(self, basedir):\n+    def __init__(self, basedir: str):\n         if '://' in basedir:\n             basedir = basedir.split('://', 1)[1]\n         self.basedir = basedir\n-        self._mkdir(self.basedir)\n-        self.created_directories = defaultdict(set)\n+        self._mkdir(Path(self.basedir))\n+        self.created_directories: defaultdict[str, set[str]] = defaultdict(set)\n \n-    def persist_file(self, path, buf, info, meta=None, headers=None):\n+    def persist_file(self, path: str, buf, info, meta=None, headers=None):\n         absolute_path = self._get_filesystem_path(path)\n-        self._mkdir(os.path.dirname(absolute_path), info)\n-        with open(absolute_path, 'wb') as f:\n-            f.write(buf.getvalue())\n+        self._mkdir(absolute_path.parent, info)\n+        absolute_path.write_bytes(buf.getvalue())\n \n-    def stat_file(self, path, info):\n+    def stat_file(self, path: str, info):\n         absolute_path = self._get_filesystem_path(path)\n         try:\n-            last_modified = os.path.getmtime(absolute_path)\n+            last_modified = absolute_path.stat().st_mtime\n         except os.error:\n             return {}\n \n-        with open(absolute_path, 'rb') as f:\n+        with absolute_path.open('rb') as f:\n             checksum = md5sum(f)\n \n         return {'last_modified': last_modified, 'checksum': checksum}\n \n-    def _get_filesystem_path(self, path):\n+    def _get_filesystem_path(self, path: str) -> Path:\n         path_comps = path.split('/')\n-        return os.path.join(self.basedir, *path_comps)\n+        return Path(self.basedir, *path_comps)\n \n-    def _mkdir(self, dirname, domain=None):\n+    def _mkdir(self, dirname: Path, domain: Optional[str] = None):\n         seen = self.created_directories[domain] if domain else set()\n-        if dirname not in seen:\n-            if not os.path.exists(dirname):\n-                os.makedirs(dirname)\n-            seen.add(dirname)\n+        if str(dirname) not in seen:\n+            if not dirname.exists():\n+                dirname.mkdir(parents=True)\n+            seen.add(str(dirname))\n \n \n class S3FilesStore:\n@@ -374,8 +375,8 @@ class FilesPipeline(MediaPipeline):\n         store_uri = settings['FILES_STORE']\n         return cls(store_uri, settings=settings)\n \n-    def _get_store(self, uri):\n-        if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n+    def _get_store(self, uri: str):\n+        if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = 'file'\n         else:\n             scheme = urlparse(uri).scheme\n@@ -510,7 +511,7 @@ class FilesPipeline(MediaPipeline):\n \n     def file_path(self, request, response=None, info=None, *, item=None):\n         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n-        media_ext = os.path.splitext(request.url)[1]\n+        media_ext = Path(request.url).suffix\n         # Handles empty and wild extensions by trying to guess the\n         # mime type then extension or default to empty string otherwise\n         if media_ext not in mimetypes.types_map:\n\n@@ -15,7 +15,7 @@ Scrapy developers, if you add a setting here remember to:\n \n import sys\n from importlib import import_module\n-from os.path import join, abspath, dirname\n+from pathlib import Path\n \n AJAXCRAWL_ENABLED = False\n \n@@ -288,7 +288,7 @@ STATS_DUMP = True\n \n STATSMAILER_RCPTS = []\n \n-TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n+TEMPLATES_DIR = str((Path(__file__).parent / '..' / 'templates').resolve())\n \n URLLENGTH_LIMIT = 2083\n \n\n@@ -3,14 +3,19 @@ Base class for Scrapy spiders\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n+from __future__ import annotations\n+\n import logging\n-from typing import Optional\n+from typing import TYPE_CHECKING, Optional\n \n from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n+if TYPE_CHECKING:\n+    from scrapy.crawler import Crawler\n+\n \n class Spider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n@@ -49,7 +54,7 @@ class Spider(object_ref):\n         spider._set_crawler(crawler)\n         return spider\n \n-    def _set_crawler(self, crawler):\n+    def _set_crawler(self, crawler: Crawler):\n         self.crawler = crawler\n         self.settings = crawler.settings\n         crawler.signals.connect(self.close, signals.spider_closed)\n\n@@ -3,8 +3,9 @@ Scheduler queues\n \"\"\"\n \n import marshal\n-import os\n import pickle\n+from os import PathLike\n+from pathlib import Path\n \n from queuelib import queue\n \n@@ -16,10 +17,10 @@ def _with_mkdir(queue_class):\n \n     class DirectoriesCreated(queue_class):\n \n-        def __init__(self, path, *args, **kwargs):\n-            dirname = os.path.dirname(path)\n-            if not os.path.exists(dirname):\n-                os.makedirs(dirname, exist_ok=True)\n+        def __init__(self, path: str | PathLike[str], *args, **kwargs):\n+            dirname = Path(path).parent\n+            if not dirname.exists():\n+                dirname.mkdir(parents=True, exist_ok=True)\n             super().__init__(path, *args, **kwargs)\n \n     return DirectoriesCreated\n\n@@ -4,6 +4,8 @@ import sys\n import warnings\n from configparser import ConfigParser\n from operator import itemgetter\n+from pathlib import Path\n+from typing import Optional\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n \n@@ -65,17 +67,17 @@ def arglist_to_dict(arglist):\n     return dict(x.split('=', 1) for x in arglist)\n \n \n-def closest_scrapy_cfg(path='.', prevpath=None):\n+def closest_scrapy_cfg(path: str | os.PathLike[str] = '.', prevpath: Optional[str | os.PathLike] = None) -> str:\n     \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n     directory and its parents\n     \"\"\"\n-    if path == prevpath:\n+    if prevpath is not None and str(path) == str(prevpath):\n         return ''\n-    path = os.path.abspath(path)\n-    cfgfile = os.path.join(path, 'scrapy.cfg')\n-    if os.path.exists(cfgfile):\n-        return cfgfile\n-    return closest_scrapy_cfg(os.path.dirname(path), path)\n+    path = Path(path).resolve()\n+    cfgfile = path / 'scrapy.cfg'\n+    if cfgfile.exists():\n+        return str(cfgfile)\n+    return closest_scrapy_cfg(path.parent, path)\n \n \n def init_env(project='default', set_syspath=True):\n@@ -88,7 +90,7 @@ def init_env(project='default', set_syspath=True):\n         os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)\n     closest = closest_scrapy_cfg()\n     if closest:\n-        projdir = os.path.dirname(closest)\n+        projdir = str(Path(closest).parent)\n         if set_syspath and projdir not in sys.path:\n             sys.path.append(projdir)\n \n@@ -101,13 +103,13 @@ def get_config(use_closest=True):\n     return cfg\n \n \n-def get_sources(use_closest=True):\n-    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or os.path.expanduser('~/.config')\n+def get_sources(use_closest=True) -> list[str]:\n+    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or Path('~/.config').expanduser()\n     sources = [\n         '/etc/scrapy.cfg',\n         r'c:\\scrapy\\scrapy.cfg',\n-        xdg_config_home + '/scrapy.cfg',\n-        os.path.expanduser('~/.scrapy.cfg'),\n+        str(Path(xdg_config_home) / 'scrapy.cfg'),\n+        str(Path('~/.scrapy.cfg').expanduser()),\n     ]\n     if use_closest:\n         sources.append(closest_scrapy_cfg())\n@@ -129,8 +131,8 @@ def feed_complete_default_values_from_settings(feed, settings):\n     return out\n \n \n-def feed_process_params_from_cli(settings, output, output_format=None,\n-                                 overwrite_output=None):\n+def feed_process_params_from_cli(settings, output: list[str], output_format=None,\n+                                 overwrite_output: Optional[list[str]] = None):\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n     checks for inconsistencies in their quantities and returns a dictionary\n@@ -180,7 +182,7 @@ def feed_process_params_from_cli(settings, output, output_format=None,\n             feed_uri, feed_format = element.rsplit(':', 1)\n         except ValueError:\n             feed_uri = element\n-            feed_format = os.path.splitext(element)[1].replace('.', '')\n+            feed_format = Path(element).suffix.replace('.', '')\n         else:\n             if feed_uri == '-':\n                 feed_uri = 'stdout:'\n\n@@ -1,4 +1,4 @@\n-import os\n+from pathlib import Path\n from typing import Optional\n \n from scrapy.settings import BaseSettings\n@@ -6,6 +6,6 @@ from scrapy.settings import BaseSettings\n \n def job_dir(settings: BaseSettings) -> Optional[str]:\n     path = settings['JOBDIR']\n-    if path and not os.path.exists(path):\n-        os.makedirs(path)\n+    if path and not Path(path).exists():\n+        Path(path).mkdir(parents=True)\n     return path\n\n@@ -2,7 +2,7 @@ import os\n import warnings\n \n from importlib import import_module\n-from os.path import join, dirname, abspath, isabs, exists\n+from pathlib import Path\n \n from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n from scrapy.settings import Settings\n@@ -25,36 +25,37 @@ def inside_project():\n     return bool(closest_scrapy_cfg())\n \n \n-def project_data_dir(project='default'):\n+def project_data_dir(project='default') -> str:\n     \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n     if not inside_project():\n         raise NotConfigured(\"Not inside a project\")\n     cfg = get_config()\n     if cfg.has_option(DATADIR_CFG_SECTION, project):\n-        d = cfg.get(DATADIR_CFG_SECTION, project)\n+        d = Path(cfg.get(DATADIR_CFG_SECTION, project))\n     else:\n         scrapy_cfg = closest_scrapy_cfg()\n         if not scrapy_cfg:\n             raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n-        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n-    if not exists(d):\n-        os.makedirs(d)\n-    return d\n+        d = (Path(scrapy_cfg).parent / '.scrapy').resolve()\n+    if not d.exists():\n+        d.mkdir(parents=True)\n+    return str(d)\n \n \n-def data_path(path, createdir=False):\n+def data_path(path: str, createdir=False) -> str:\n     \"\"\"\n     Return the given path joined with the .scrapy data directory.\n     If given an absolute path, return it unmodified.\n     \"\"\"\n-    if not isabs(path):\n+    path_obj = Path(path)\n+    if not path_obj.is_absolute():\n         if inside_project():\n-            path = join(project_data_dir(), path)\n+            path_obj = Path(project_data_dir(), path)\n         else:\n-            path = join('.scrapy', path)\n-    if createdir and not exists(path):\n-        os.makedirs(path)\n-    return path\n+            path_obj = Path('.scrapy', path)\n+    if createdir and not path_obj.exists():\n+        path_obj.mkdir(parents=True)\n+    return str(path_obj)\n \n \n def get_project_settings():\n\n@@ -264,7 +264,7 @@ class RequestFingerprinter:\n                 f'and \\'VERSION\\'.'\n             )\n \n-    def fingerprint(self, request):\n+    def fingerprint(self, request: Request):\n         return self._fingerprint(request)\n \n \n\n@@ -1,23 +1,21 @@\n \"\"\"Helper functions for working with templates\"\"\"\n \n-import os\n import re\n import string\n+from pathlib import Path\n \n \n-def render_templatefile(path, **kwargs):\n-    with open(path, 'rb') as fp:\n-        raw = fp.read().decode('utf8')\n+def render_templatefile(path: str, **kwargs):\n+    raw = Path(path).read_text('utf8')\n \n     content = string.Template(raw).substitute(**kwargs)\n \n     render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n \n     if path.endswith('.tmpl'):\n-        os.rename(path, render_path)\n+        Path(path).rename(render_path)\n \n-    with open(render_path, 'wb') as fp:\n-        fp.write(content.encode('utf8'))\n+    Path(render_path).write_text(content, 'utf8')\n \n \n CAMELCASE_INVALID_CHARS = re.compile(r'[^a-zA-Z\\d]')\n\n@@ -4,6 +4,7 @@ This module contains some assorted functions used in tests\n \n import asyncio\n import os\n+from pathlib import Path\n from posixpath import split\n from unittest import mock\n \n@@ -71,11 +72,11 @@ def get_crawler(spidercls=None, settings_dict=None, prevent_warnings=True):\n     return runner.create_crawler(spidercls or Spider)\n \n \n-def get_pythonpath():\n+def get_pythonpath() -> str:\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n     installation of Scrapy\"\"\"\n     scrapy_path = import_module('scrapy').__path__[0]\n-    return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')\n+    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get('PYTHONPATH', '')\n \n \n def get_testenv():\n\n@@ -1,10 +1,9 @@\n-from os.path import dirname, join\n+from pathlib import Path\n from pkg_resources import parse_version\n from setuptools import setup, find_packages, __version__ as setuptools_version\n \n \n-with open(join(dirname(__file__), 'scrapy/VERSION'), 'rb') as f:\n-    version = f.read().decode('ascii').strip()\n+version = (Path(__file__).parent / 'scrapy/VERSION').read_text('ascii').strip()\n \n \n def has_environment_marker_platform_impl_support():\n\n@@ -6,6 +6,7 @@ see https://docs.scrapy.org/en/latest/contributing.html#running-tests\n \n import os\n import socket\n+from pathlib import Path\n \n # ignore system-wide proxies for tests\n # which would send requests to a totally unsuspecting server\n@@ -16,14 +17,12 @@ os.environ['ftp_proxy'] = ''\n \n # Absolutize paths to coverage config and output file because tests that\n # spawn subprocesses also changes current working directory.\n-_sourceroot = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+_sourceroot = Path(__file__).resolve().parent.parent\n if 'COV_CORE_CONFIG' in os.environ:\n-    os.environ['COVERAGE_FILE'] = os.path.join(_sourceroot, '.coverage')\n-    os.environ['COV_CORE_CONFIG'] = os.path.join(_sourceroot,\n-                                                 os.environ['COV_CORE_CONFIG'])\n+    os.environ['COVERAGE_FILE'] = str(_sourceroot / '.coverage')\n+    os.environ['COV_CORE_CONFIG'] = str(_sourceroot / os.environ['COV_CORE_CONFIG'])\n \n-tests_datadir = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n-                             'sample_data')\n+tests_datadir = str(Path(__file__).parent.resolve() / 'sample_data')\n \n \n # In some environments accessing a non-existing host doesn't raise an\n@@ -35,8 +34,6 @@ except socket.gaierror:\n     NON_EXISTING_RESOLVABLE = False\n \n \n-def get_testdata(*paths):\n+def get_testdata(*paths: str) -> bytes:\n     \"\"\"Return test data\"\"\"\n-    path = os.path.join(tests_datadir, *paths)\n-    with open(path, 'rb') as f:\n-        return f.read()\n+    return Path(tests_datadir, *paths).read_bytes()\n\n@@ -1,5 +1,5 @@\n-import os\n from datetime import datetime, timedelta\n+from pathlib import Path\n \n from cryptography.hazmat.backends import default_backend\n from cryptography.hazmat.primitives.asymmetric import rsa\n@@ -22,20 +22,19 @@ from cryptography.x509.oid import NameOID\n \n # https://cryptography.io/en/latest/x509/tutorial/#creating-a-self-signed-certificate\n def generate_keys():\n-    folder = os.path.dirname(__file__)\n+    folder = Path(__file__).parent\n \n     key = rsa.generate_private_key(\n         public_exponent=65537,\n         key_size=2048,\n         backend=default_backend(),\n     )\n-    with open(os.path.join(folder, 'localhost.key'), \"wb\") as f:\n-        f.write(\n+    (folder / 'localhost.key').write_bytes(\n         key.private_bytes(\n             encoding=Encoding.PEM,\n             format=PrivateFormat.TraditionalOpenSSL,\n             encryption_algorithm=NoEncryption(),\n-            )\n+        ),\n     )\n \n     subject = issuer = Name(\n@@ -59,5 +58,4 @@ def generate_keys():\n         )\n         .sign(key, SHA256(), default_backend())\n     )\n-    with open(os.path.join(folder, 'localhost.crt'), \"wb\") as f:\n-        f.write(cert.public_bytes(Encoding.PEM))\n+    (folder / 'localhost.crt').write_bytes(cert.public_bytes(Encoding.PEM))\n\n@@ -1,6 +1,5 @@\n import argparse\n import json\n-import os\n import random\n import sys\n from pathlib import Path\n@@ -253,7 +252,7 @@ class Root(resource.Resource):\n         self.putChild(b\"alpayload\", ArbitraryLengthPayloadResource())\n         try:\n             from tests import tests_datadir\n-            self.putChild(b\"files\", File(os.path.join(tests_datadir, 'test_site/files/')))\n+            self.putChild(b\"files\", File(str(Path(tests_datadir, 'test_site/files/'))))\n         except Exception:\n             pass\n         self.putChild(b\"redirect-to\", RedirectTo())\n@@ -346,8 +345,8 @@ class MockFTPServer:\n \n def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt', cipher_string=None):\n     factory = ssl.DefaultOpenSSLContextFactory(\n-        os.path.join(os.path.dirname(__file__), keyfile),\n-        os.path.join(os.path.dirname(__file__), certfile),\n+        str(Path(__file__).parent / keyfile),\n+        str(Path(__file__).parent / certfile),\n     )\n     if cipher_string:\n         ctx = factory.getContext()\n\n@@ -1,11 +1,11 @@\n import json\n-import os\n import pstats\n import shutil\n import sys\n import tempfile\n import unittest\n from io import StringIO\n+from pathlib import Path\n from subprocess import Popen, PIPE\n \n from scrapy.utils.test import get_testenv\n@@ -36,17 +36,17 @@ class CmdlineTest(unittest.TestCase):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), 'override')\n \n     def test_profiling(self):\n-        path = tempfile.mkdtemp()\n-        filename = os.path.join(path, 'res.prof')\n+        path = Path(tempfile.mkdtemp())\n+        filename = path / 'res.prof'\n         try:\n-            self._execute('version', '--profile', filename)\n-            self.assertTrue(os.path.exists(filename))\n+            self._execute('version', '--profile', str(filename))\n+            self.assertTrue(filename.exists())\n             out = StringIO()\n-            stats = pstats.Stats(filename, stream=out)\n+            stats = pstats.Stats(str(filename), stream=out)\n             stats.print_stats()\n             out.seek(0)\n             stats = out.read()\n-            self.assertIn(os.path.join('scrapy', 'commands', 'version.py'),\n+            self.assertIn(str(Path('scrapy', 'commands', 'version.py')),\n                           stats)\n             self.assertIn('tottime', stats)\n         finally:\n\n@@ -1,6 +1,6 @@\n-import os\n import sys\n import unittest\n+from pathlib import Path\n from subprocess import Popen, PIPE\n \n \n@@ -8,7 +8,7 @@ class CmdlineCrawlPipelineTest(unittest.TestCase):\n \n     def _execute(self, spname):\n         args = (sys.executable, '-m', 'scrapy.cmdline', 'crawl', spname)\n-        cwd = os.path.dirname(os.path.abspath(__file__))\n+        cwd = Path(__file__).resolve().parent\n         proc = Popen(args, stdout=PIPE, stderr=PIPE, cwd=cwd)\n         proc.communicate()\n         return proc.returncode\n\n@@ -1,5 +1,3 @@\n-from os.path import join, abspath\n-\n from tests.test_commands import CommandTest\n \n \n@@ -10,11 +8,10 @@ class CheckCommandTest(CommandTest):\n     def setUp(self):\n         super(CheckCommandTest, self).setUp()\n         self.spider_name = 'check_spider'\n-        self.spider = abspath(join(self.proj_mod_path, 'spiders', 'checkspider.py'))\n+        self.spider = (self.proj_mod_path / 'spiders' / 'checkspider.py').resolve()\n \n     def _write_contract(self, contracts, parse_def):\n-        with open(self.spider, 'w') as file:\n-            file.write(f\"\"\"\n+        self.spider.write_text(f\"\"\"\n import scrapy\n \n class CheckSpider(scrapy.Spider):\n\n@@ -1,6 +1,6 @@\n import os\n import argparse\n-from os.path import join, abspath, isfile, exists\n+from pathlib import Path\n \n from twisted.internet import defer\n from scrapy.commands import parse\n@@ -23,9 +23,7 @@ class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n     def setUp(self):\n         super().setUp()\n         self.spider_name = 'parse_spider'\n-        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n-        with open(fname, 'w') as f:\n-            f.write(f\"\"\"\n+        (self.proj_mod_path / 'spiders' / 'myspider.py').write_text(f\"\"\"\n import scrapy\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import CrawlSpider, Rule\n@@ -88,9 +86,7 @@ class MyBadCrawlSpider(CrawlSpider):\n         return [scrapy.Item(), dict(foo='bar')]\n \"\"\")\n \n-        fname = abspath(join(self.proj_mod_path, 'pipelines.py'))\n-        with open(fname, 'w') as f:\n-            f.write(\"\"\"\n+        (self.proj_mod_path / 'pipelines.py').write_text(\"\"\"\n import logging\n \n class MyPipeline:\n@@ -101,8 +97,7 @@ class MyPipeline:\n         return item\n \"\"\")\n \n-        fname = abspath(join(self.proj_mod_path, 'settings.py'))\n-        with open(fname, 'a') as f:\n+        with (self.proj_mod_path / 'settings.py').open(\"a\") as f:\n             f.write(f\"\"\"\n ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n \"\"\")\n@@ -234,7 +229,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         correct format containing correct data in it.\n         \"\"\"\n         file_name = 'data.json'\n-        file_path = join(self.proj_path, file_name)\n+        file_path = Path(self.proj_path, file_name)\n         yield self.execute([\n             '--spider', self.spider_name,\n             '-c', 'parse',\n@@ -242,12 +237,11 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             self.url('/html')\n         ])\n \n-        self.assertTrue(exists(file_path))\n-        self.assertTrue(isfile(file_path))\n+        self.assertTrue(file_path.exists())\n+        self.assertTrue(file_path.is_file())\n \n         content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n-        with open(file_path, 'r') as f:\n-            self.assertEqual(f.read(), content)\n+        self.assertEqual(file_path.read_text(), content)\n \n     def test_parse_add_options(self):\n         command = parse.Command()\n\n@@ -1,4 +1,4 @@\n-from os.path import join\n+from pathlib import Path\n \n from twisted.trial import unittest\n from twisted.internet import defer\n@@ -96,8 +96,8 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_local_file(self):\n-        filepath = join(tests_datadir, 'test_site', 'index.html')\n-        _, out, _ = yield self.execute([filepath, '-c', 'item'])\n+        filepath = Path(tests_datadir, 'test_site', 'index.html')\n+        _, out, _ = yield self.execute([str(filepath), '-c', 'item'])\n         assert b'{}' in out\n \n     @defer.inlineCallbacks\n\n@@ -9,12 +9,12 @@ import sys\n import tempfile\n from contextlib import contextmanager\n from itertools import chain\n-from os.path import exists, join, abspath, getmtime\n from pathlib import Path\n from shutil import rmtree, copytree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import mkdtemp\n from threading import Timer\n+from typing import Generator, Optional\n from unittest import skipIf\n \n from pytest import mark\n@@ -66,8 +66,8 @@ class ProjectTest(unittest.TestCase):\n     def setUp(self):\n         self.temp_path = mkdtemp()\n         self.cwd = self.temp_path\n-        self.proj_path = join(self.temp_path, self.project_name)\n-        self.proj_mod_path = join(self.proj_path, self.project_name)\n+        self.proj_path = Path(self.temp_path, self.project_name)\n+        self.proj_mod_path = self.proj_path / self.project_name\n         self.env = get_testenv()\n \n     def tearDown(self):\n@@ -104,10 +104,10 @@ class ProjectTest(unittest.TestCase):\n \n         return p, to_unicode(stdout), to_unicode(stderr)\n \n-    def find_in_file(self, filename, regex):\n+    def find_in_file(self, filename: str | os.PathLike[str], regex) -> Optional[re.Match]:\n         \"\"\"Find first pattern occurrence in file\"\"\"\n         pattern = re.compile(regex)\n-        with open(filename, \"r\") as f:\n+        with Path(filename).open(\"r\") as f:\n             for line in f:\n                 match = pattern.search(line)\n                 if match is not None:\n@@ -122,13 +122,13 @@ class StartprojectTest(ProjectTest):\n         print(err, file=sys.stderr)\n         self.assertEqual(p.returncode, 0)\n \n-        assert exists(join(self.proj_path, 'scrapy.cfg'))\n-        assert exists(join(self.proj_path, 'testproject'))\n-        assert exists(join(self.proj_mod_path, '__init__.py'))\n-        assert exists(join(self.proj_mod_path, 'items.py'))\n-        assert exists(join(self.proj_mod_path, 'pipelines.py'))\n-        assert exists(join(self.proj_mod_path, 'settings.py'))\n-        assert exists(join(self.proj_mod_path, 'spiders', '__init__.py'))\n+        assert Path(self.proj_path, 'scrapy.cfg').exists()\n+        assert Path(self.proj_path, 'testproject').exists()\n+        assert Path(self.proj_mod_path, '__init__.py').exists()\n+        assert Path(self.proj_mod_path, 'items.py').exists()\n+        assert Path(self.proj_mod_path, 'pipelines.py').exists()\n+        assert Path(self.proj_mod_path, 'settings.py').exists()\n+        assert Path(self.proj_mod_path, 'spiders', '__init__.py').exists()\n \n         self.assertEqual(1, self.call('startproject', self.project_name))\n         self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n@@ -138,13 +138,13 @@ class StartprojectTest(ProjectTest):\n         project_dir = mkdtemp()\n         self.assertEqual(0, self.call('startproject', self.project_name, project_dir))\n \n-        assert exists(join(abspath(project_dir), 'scrapy.cfg'))\n-        assert exists(join(abspath(project_dir), 'testproject'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), '__init__.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'items.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'pipelines.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'settings.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'spiders', '__init__.py'))\n+        assert Path(project_dir, 'scrapy.cfg').exists()\n+        assert Path(project_dir, 'testproject').exists()\n+        assert Path(project_dir, self.project_name, '__init__.py').exists()\n+        assert Path(project_dir, self.project_name, 'items.py').exists()\n+        assert Path(project_dir, self.project_name, 'pipelines.py').exists()\n+        assert Path(project_dir, self.project_name, 'settings.py').exists()\n+        assert Path(project_dir, self.project_name, 'spiders', '__init__.py').exists()\n \n         self.assertEqual(0, self.call('startproject', self.project_name, project_dir + '2'))\n \n@@ -158,40 +158,42 @@ class StartprojectTest(ProjectTest):\n     def test_existing_project_dir(self):\n         project_dir = mkdtemp()\n         project_name = self.project_name + '_existing'\n-        project_path = os.path.join(project_dir, project_name)\n-        os.mkdir(project_path)\n+        project_path = Path(project_dir, project_name)\n+        project_path.mkdir()\n \n         p, out, err = self.proc('startproject', project_name, cwd=project_dir)\n         print(out)\n         print(err, file=sys.stderr)\n         self.assertEqual(p.returncode, 0)\n \n-        assert exists(join(abspath(project_path), 'scrapy.cfg'))\n-        assert exists(join(abspath(project_path), project_name))\n-        assert exists(join(join(abspath(project_path), project_name), '__init__.py'))\n-        assert exists(join(join(abspath(project_path), project_name), 'items.py'))\n-        assert exists(join(join(abspath(project_path), project_name), 'pipelines.py'))\n-        assert exists(join(join(abspath(project_path), project_name), 'settings.py'))\n-        assert exists(join(join(abspath(project_path), project_name), 'spiders', '__init__.py'))\n+        assert Path(project_path, 'scrapy.cfg').exists()\n+        assert Path(project_path, project_name).exists()\n+        assert Path(project_path, project_name, '__init__.py').exists()\n+        assert Path(project_path, project_name, 'items.py').exists()\n+        assert Path(project_path, project_name, 'pipelines.py').exists()\n+        assert Path(project_path, project_name, 'settings.py').exists()\n+        assert Path(project_path, project_name, 'spiders', '__init__.py').exists()\n \n \n-def get_permissions_dict(path, renamings=None, ignore=None):\n+def get_permissions_dict(path: str | os.PathLike[str], renamings=None, ignore=None) -> dict[str, str]:\n \n-    def get_permissions(path):\n-        return oct(os.stat(path).st_mode)\n+    def get_permissions(path: Path) -> str:\n+        return oct(path.stat().st_mode)\n+\n+    path_obj = Path(path)\n \n     renamings = renamings or tuple()\n     permissions_dict = {\n-        '.': get_permissions(path),\n+        '.': get_permissions(path_obj),\n     }\n-    for root, dirs, files in os.walk(path):\n+    for root, dirs, files in os.walk(path_obj):\n         nodes = list(chain(dirs, files))\n         if ignore:\n             ignored_names = ignore(root, nodes)\n             nodes = [node for node in nodes if node not in ignored_names]\n         for node in nodes:\n-            absolute_path = os.path.join(root, node)\n-            relative_path = os.path.relpath(absolute_path, path)\n+            absolute_path = Path(root, node)\n+            relative_path = str(absolute_path.relative_to(path))\n             for search_string, replacement in renamings:\n                 relative_path = relative_path.replace(\n                     search_string,\n@@ -208,28 +210,27 @@ class StartprojectTemplatesTest(ProjectTest):\n \n     def setUp(self):\n         super().setUp()\n-        self.tmpl = join(self.temp_path, 'templates')\n-        self.tmpl_proj = join(self.tmpl, 'project')\n+        self.tmpl = str(Path(self.temp_path, 'templates'))\n+        self.tmpl_proj = str(Path(self.tmpl, 'project'))\n \n     def test_startproject_template_override(self):\n-        copytree(join(scrapy.__path__[0], 'templates'), self.tmpl)\n-        with open(join(self.tmpl_proj, 'root_template'), 'w'):\n-            pass\n-        assert exists(join(self.tmpl_proj, 'root_template'))\n+        copytree(Path(scrapy.__path__[0], 'templates'), self.tmpl)\n+        Path(self.tmpl_proj, 'root_template').write_bytes(b\"\")\n+        assert Path(self.tmpl_proj, 'root_template').exists()\n \n         args = ['--set', f'TEMPLATES_DIR={self.tmpl}']\n         p, out, err = self.proc('startproject', self.project_name, *args)\n         self.assertIn(f\"New Scrapy project '{self.project_name}', \"\n                       \"using template directory\", out)\n         self.assertIn(self.tmpl_proj, out)\n-        assert exists(join(self.proj_path, 'root_template'))\n+        assert Path(self.proj_path, 'root_template').exists()\n \n     def test_startproject_permissions_from_writable(self):\n         \"\"\"Check that generated files have the right permissions when the\n         template folder has the same permissions as in the project, i.e.\n         everything is writable.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        project_template = os.path.join(scrapy_path, 'templates', 'project')\n+        project_template = Path(scrapy_path, 'templates', 'project')\n         project_name = 'startproject1'\n         renamings = (\n             ('module', project_name),\n@@ -255,7 +256,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         )\n         process.wait()\n \n-        project_dir = os.path.join(destination, project_name)\n+        project_dir = Path(destination, project_name)\n         actual_permissions = get_permissions_dict(project_dir)\n \n         self.assertEqual(actual_permissions, expected_permissions)\n@@ -268,8 +269,8 @@ class StartprojectTemplatesTest(ProjectTest):\n         See https://github.com/scrapy/scrapy/pull/4604\n         \"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        templates_dir = os.path.join(scrapy_path, 'templates')\n-        project_template = os.path.join(templates_dir, 'project')\n+        templates_dir = Path(scrapy_path, 'templates')\n+        project_template = Path(templates_dir, 'project')\n         project_name = 'startproject2'\n         renamings = (\n             ('module', project_name),\n@@ -281,16 +282,16 @@ class StartprojectTemplatesTest(ProjectTest):\n             IGNORE,\n         )\n \n-        def _make_read_only(path):\n-            current_permissions = os.stat(path).st_mode\n-            os.chmod(path, current_permissions & ~ANYONE_WRITE_PERMISSION)\n+        def _make_read_only(path: Path):\n+            current_permissions = path.stat().st_mode\n+            path.chmod(current_permissions & ~ANYONE_WRITE_PERMISSION)\n \n         read_only_templates_dir = str(Path(mkdtemp()) / 'templates')\n         copytree(templates_dir, read_only_templates_dir)\n \n         for root, dirs, files in os.walk(read_only_templates_dir):\n             for node in chain(dirs, files):\n-                _make_read_only(os.path.join(root, node))\n+                _make_read_only(Path(root, node))\n \n         destination = mkdtemp()\n         process = subprocess.Popen(\n@@ -308,7 +309,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         )\n         process.wait()\n \n-        project_dir = os.path.join(destination, project_name)\n+        project_dir = Path(destination, project_name)\n         actual_permissions = get_permissions_dict(project_dir)\n \n         self.assertEqual(actual_permissions, expected_permissions)\n@@ -317,7 +318,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         \"\"\"Check that pre-existing folders and files in the destination folder\n         do not see their permissions modified.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n-        project_template = os.path.join(scrapy_path, 'templates', 'project')\n+        project_template = Path(scrapy_path, 'templates', 'project')\n         project_name = 'startproject3'\n         renamings = (\n             ('module', project_name),\n@@ -330,7 +331,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         )\n \n         destination = mkdtemp()\n-        project_dir = os.path.join(destination, project_name)\n+        project_dir = Path(destination, project_name)\n \n         existing_nodes = {\n             oct(permissions)[2:] + extension: permissions\n@@ -339,10 +340,9 @@ class StartprojectTemplatesTest(ProjectTest):\n                 0o444, 0o555, 0o644, 0o666, 0o755, 0o777,\n             )\n         }\n-        os.mkdir(project_dir)\n-        project_dir_path = Path(project_dir)\n+        project_dir.mkdir()\n         for node, permissions in existing_nodes.items():\n-            path = project_dir_path / node\n+            path = project_dir / node\n             if node.endswith('.d'):\n                 path.mkdir(mode=permissions)\n             else:\n@@ -378,7 +378,7 @@ class StartprojectTemplatesTest(ProjectTest):\n             os.umask(cur_mask)\n \n         scrapy_path = scrapy.__path__[0]\n-        project_template = os.path.join(\n+        project_template = Path(\n             scrapy_path,\n             'templates',\n             'project'\n@@ -409,7 +409,7 @@ class StartprojectTemplatesTest(ProjectTest):\n             )\n             process.wait()\n \n-            project_dir = os.path.join(destination, project_name)\n+            project_dir = Path(destination, project_name)\n             actual_permissions = get_permissions_dict(project_dir)\n \n             self.assertEqual(actual_permissions, expected_permissions)\n@@ -420,7 +420,7 @@ class CommandTest(ProjectTest):\n     def setUp(self):\n         super().setUp()\n         self.call('startproject', self.project_name)\n-        self.cwd = join(self.temp_path, self.project_name)\n+        self.cwd = Path(self.temp_path, self.project_name)\n         self.env['SCRAPY_SETTINGS_MODULE'] = f'{self.project_name}.settings'\n \n \n@@ -429,10 +429,10 @@ class GenspiderCommandTest(CommandTest):\n     def test_arguments(self):\n         # only pass one argument. spider script shouldn't be created\n         self.assertEqual(2, self.call('genspider', 'test_name'))\n-        assert not exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))\n+        assert not Path(self.proj_mod_path, 'spiders', 'test_name.py').exists()\n         # pass two arguments <name> <domain>. spider script should be created\n         self.assertEqual(0, self.call('genspider', 'test_name', 'test.com'))\n-        assert exists(join(self.proj_mod_path, 'spiders', 'test_name.py'))\n+        assert Path(self.proj_mod_path, 'spiders', 'test_name.py').exists()\n \n     def test_template(self, tplname='crawl'):\n         args = [f'--template={tplname}'] if tplname else []\n@@ -440,11 +440,11 @@ class GenspiderCommandTest(CommandTest):\n         spmodule = f\"{self.project_name}.spiders.{spname}\"\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(f\"Created spider {spname!r} using template {tplname!r} in module:{os.linesep}  {spmodule}\", out)\n-        self.assertTrue(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n-        modify_time_before = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n+        self.assertTrue(Path(self.proj_mod_path, 'spiders', 'test_spider.py').exists())\n+        modify_time_before = Path(self.proj_mod_path, 'spiders', 'test_spider.py').stat().st_mtime\n         p, out, err = self.proc('genspider', spname, 'test.com', *args)\n         self.assertIn(f\"Spider {spname!r} already exists in module\", out)\n-        modify_time_after = getmtime(join(self.proj_mod_path, 'spiders', 'test_spider.py'))\n+        modify_time_after = Path(self.proj_mod_path, 'spiders', 'test_spider.py').stat().st_mtime\n         self.assertEqual(modify_time_after, modify_time_before)\n \n     def test_template_basic(self):\n@@ -465,37 +465,37 @@ class GenspiderCommandTest(CommandTest):\n \n     def test_same_name_as_project(self):\n         self.assertEqual(2, self.call('genspider', self.project_name))\n-        assert not exists(join(self.proj_mod_path, 'spiders', f'{self.project_name}.py'))\n+        assert not Path(self.proj_mod_path, 'spiders', f'{self.project_name}.py').exists()\n \n     def test_same_filename_as_existing_spider(self, force=False):\n         file_name = 'example'\n-        file_path = join(self.proj_mod_path, 'spiders', f'{file_name}.py')\n+        file_path = Path(self.proj_mod_path, 'spiders', f'{file_name}.py')\n         self.assertEqual(0, self.call('genspider', file_name, 'example.com'))\n-        assert exists(file_path)\n+        assert file_path.exists()\n \n         # change name of spider but not its file name\n-        with open(file_path, 'r+') as spider_file:\n+        with file_path.open('r+') as spider_file:\n             file_data = spider_file.read()\n             file_data = file_data.replace(\"name = \\'example\\'\", \"name = \\'renamed\\'\")\n             spider_file.seek(0)\n             spider_file.write(file_data)\n             spider_file.truncate()\n-        modify_time_before = getmtime(file_path)\n+        modify_time_before = file_path.stat().st_mtime\n         file_contents_before = file_data\n \n         if force:\n             p, out, err = self.proc('genspider', '--force', file_name, 'example.com')\n             self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' in module\", out)\n-            modify_time_after = getmtime(file_path)\n+            modify_time_after = file_path.stat().st_mtime\n             self.assertNotEqual(modify_time_after, modify_time_before)\n-            file_contents_after = open(file_path, 'r').read()\n+            file_contents_after = file_path.read_text()\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n             self.assertIn(f\"{file_path} already exists\", out)\n-            modify_time_after = getmtime(file_path)\n+            modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n-            file_contents_after = open(file_path, 'r').read()\n+            file_contents_after = file_path.read_text()\n             self.assertEqual(file_contents_after, file_contents_before)\n \n     def test_same_filename_as_existing_spider_force(self):\n@@ -504,11 +504,11 @@ class GenspiderCommandTest(CommandTest):\n     def test_url(self, url='test.com', domain=\"test.com\"):\n         self.assertEqual(0, self.call('genspider', '--force', 'test_name', url))\n         self.assertEqual(domain,\n-                         self.find_in_file(join(self.proj_mod_path,\n+                         self.find_in_file(Path(self.proj_mod_path,\n                                                 'spiders', 'test_name.py'),\n                                            r'allowed_domains\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n         self.assertEqual(f'http://{domain}/',\n-                         self.find_in_file(join(self.proj_mod_path,\n+                         self.find_in_file(Path(self.proj_mod_path,\n                                                 'spiders', 'test_name.py'),\n                                            r'start_urls\\s*=\\s*\\[\\'(.+)\\'\\]').group(1))\n \n@@ -526,31 +526,31 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n \n     def test_generate_standalone_spider(self):\n         self.call('genspider', 'example', 'example.com')\n-        assert exists(join(self.temp_path, 'example.py'))\n+        assert Path(self.temp_path, 'example.py').exists()\n \n     def test_same_name_as_existing_file(self, force=False):\n         file_name = 'example'\n-        file_path = join(self.temp_path, file_name + '.py')\n+        file_path = Path(self.temp_path, file_name + '.py')\n         p, out, err = self.proc('genspider', file_name, 'example.com')\n         self.assertIn(f\"Created spider {file_name!r} using template \\'basic\\' \", out)\n-        assert exists(file_path)\n-        modify_time_before = getmtime(file_path)\n-        file_contents_before = open(file_path, 'r').read()\n+        assert file_path.exists()\n+        modify_time_before = file_path.stat().st_mtime\n+        file_contents_before = file_path.read_text()\n \n         if force:\n             # use different template to ensure contents were changed\n             p, out, err = self.proc('genspider', '--force', '-t', 'crawl', file_name, 'example.com')\n             self.assertIn(f\"Created spider {file_name!r} using template \\'crawl\\' \", out)\n-            modify_time_after = getmtime(file_path)\n+            modify_time_after = file_path.stat().st_mtime\n             self.assertNotEqual(modify_time_after, modify_time_before)\n-            file_contents_after = open(file_path, 'r').read()\n+            file_contents_after = file_path.read_text()\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n-            self.assertIn(f\"{join(self.temp_path, file_name + '.py')} already exists\", out)\n-            modify_time_after = getmtime(file_path)\n+            self.assertIn(f\"{Path(self.temp_path, file_name + '.py')} already exists\", out)\n+            modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n-            file_contents_after = open(file_path, 'r').read()\n+            file_contents_after = file_path.read_text()\n             self.assertEqual(file_contents_after, file_contents_before)\n \n     def test_same_name_as_existing_file_force(self):\n@@ -588,17 +588,16 @@ class BadSpider(scrapy.Spider):\n         \"\"\"\n \n     @contextmanager\n-    def _create_file(self, content, name=None):\n-        tmpdir = self.mktemp()\n-        os.mkdir(tmpdir)\n+    def _create_file(self, content, name=None) -> Generator[str, None, None]:\n+        tmpdir = Path(self.mktemp())\n+        tmpdir.mkdir()\n         if name:\n-            fname = abspath(join(tmpdir, name))\n+            fname = (tmpdir / name).resolve()\n         else:\n-            fname = abspath(join(tmpdir, self.spider_filename))\n-        with open(fname, 'w') as f:\n-            f.write(content)\n+            fname = (tmpdir / self.spider_filename).resolve()\n+        fname.write_text(content)\n         try:\n-            yield fname\n+            yield str(fname)\n         finally:\n             rmtree(tmpdir)\n \n@@ -747,12 +746,11 @@ class MySpider(scrapy.Spider):\n         )\n         return []\n \"\"\"\n-        with open(os.path.join(self.cwd, \"example.json\"), \"w\") as f1:\n-            f1.write(\"not empty\")\n+        Path(self.cwd, \"example.json\").write_text(\"not empty\")\n         args = ['-O', 'example.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n-        with open(os.path.join(self.cwd, \"example.json\")) as f2:\n+        with Path(self.cwd, \"example.json\").open() as f2:\n             first_line = f2.readline()\n         self.assertNotEqual(first_line, \"not empty\")\n \n@@ -854,9 +852,7 @@ class ViewCommandTest(CommandTest):\n class CrawlCommandTest(CommandTest):\n \n     def crawl(self, code, args=()):\n-        fname = abspath(join(self.proj_mod_path, 'spiders', 'myspider.py'))\n-        with open(fname, 'w') as f:\n-            f.write(code)\n+        Path(self.proj_mod_path, 'spiders', 'myspider.py').write_text(code)\n         return self.proc('crawl', 'myspider', *args)\n \n     def get_log(self, code, args=()):\n@@ -908,12 +904,11 @@ class MySpider(scrapy.Spider):\n         )\n         return []\n \"\"\"\n-        with open(os.path.join(self.cwd, \"example.json\"), \"w\") as f1:\n-            f1.write(\"not empty\")\n+        Path(self.cwd, \"example.json\").write_text(\"not empty\")\n         args = ['-O', 'example.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn('[myspider] DEBUG: FEEDS: {\"example.json\": {\"format\": \"json\", \"overwrite\": true}}', log)\n-        with open(os.path.join(self.cwd, \"example.json\")) as f2:\n+        with Path(self.cwd, \"example.json\").open() as f2:\n             first_line = f2.readline()\n         self.assertNotEqual(first_line, \"not empty\")\n \n\n@@ -1,9 +1,9 @@\n import logging\n-import os\n import platform\n import subprocess\n import sys\n import warnings\n+from pathlib import Path\n \n from pytest import raises, mark\n from twisted import version as twisted_version\n@@ -100,15 +100,14 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         assert get_scrapy_root_handler() is None\n \n     def test_spider_custom_settings_log_level(self):\n-        log_file = self.mktemp()\n-        with open(log_file, 'wb') as fo:\n-            fo.write('previous message\\n'.encode('utf-8'))\n+        log_file = Path(self.mktemp())\n+        log_file.write_text('previous message\\n', encoding='utf-8')\n \n         class MySpider(scrapy.Spider):\n             name = 'spider'\n             custom_settings = {\n                 'LOG_LEVEL': 'INFO',\n-                'LOG_FILE': log_file,\n+                'LOG_FILE': str(log_file),\n                 # settings to avoid extra warnings\n                 'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n                 'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n@@ -124,8 +123,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         logging.warning('warning message')\n         logging.error('error message')\n \n-        with open(log_file, 'rb') as fo:\n-            logged = fo.read().decode('utf-8')\n+        logged = log_file.read_text(encoding='utf-8')\n \n         self.assertIn('previous message', logged)\n         self.assertNotIn('debug message', logged)\n@@ -139,14 +137,13 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)\n \n     def test_spider_custom_settings_log_append(self):\n-        log_file = self.mktemp()\n-        with open(log_file, 'wb') as fo:\n-            fo.write('previous message\\n'.encode('utf-8'))\n+        log_file = Path(self.mktemp())\n+        log_file.write_text('previous message\\n', encoding='utf-8')\n \n         class MySpider(scrapy.Spider):\n             name = 'spider'\n             custom_settings = {\n-                'LOG_FILE': log_file,\n+                'LOG_FILE': str(log_file),\n                 'LOG_FILE_APPEND': False,\n                 # disable telnet if not available to avoid an extra warning\n                 'TELNETCONSOLE_ENABLED': telnet.TWISTED_CONCH_AVAILABLE,\n@@ -156,8 +153,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n         get_crawler(MySpider)\n         logging.debug('debug message')\n \n-        with open(log_file, 'rb') as fo:\n-            logged = fo.read().decode('utf-8')\n+        logged = log_file.read_text(encoding='utf-8')\n \n         self.assertNotIn('previous message', logged)\n         self.assertIn('debug message', logged)\n@@ -296,9 +292,9 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n \n class ScriptRunnerMixin:\n-    def run_script(self, script_name, *script_args):\n-        script_path = os.path.join(self.script_dir, script_name)\n-        args = [sys.executable, script_path] + list(script_args)\n+    def run_script(self, script_name: str, *script_args):\n+        script_path = self.script_dir / script_name\n+        args = [sys.executable, str(script_path)] + list(script_args)\n         p = subprocess.Popen(args, env=get_testenv(),\n                              stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n         stdout, stderr = p.communicate()\n@@ -306,7 +302,7 @@ class ScriptRunnerMixin:\n \n \n class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n-    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerProcess')\n+    script_dir = Path(__file__).parent.resolve() / 'CrawlerProcess'\n \n     def test_simple(self):\n         log = self.run_script('simple.py')\n@@ -463,7 +459,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n \n class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n-    script_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'CrawlerRunner')\n+    script_dir = Path(__file__).parent.resolve() / 'CrawlerRunner'\n \n     def test_response_ip_address(self):\n         log = self.run_script(\"ip_address.py\")\n\n@@ -2,6 +2,7 @@ import os\n import re\n from configparser import ConfigParser\n from importlib import import_module\n+from pathlib import Path\n \n from twisted import version as twisted_version\n from twisted.trial import unittest\n@@ -29,11 +30,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n         if not os.environ.get('_SCRAPY_PINNED', None):\n             self.skipTest('Not in a pinned environment')\n \n-        tox_config_file_path = os.path.join(\n-            os.path.dirname(__file__),\n-            '..',\n-            'tox.ini',\n-        )\n+        tox_config_file_path = Path(__file__) / '..' / 'tox.ini'\n         config_parser = ConfigParser()\n         config_parser.read(tox_config_file_path)\n         pattern = r'Twisted\\[http2\\]==([\\d.]+)'\n\n@@ -3,6 +3,7 @@ import os\n import shutil\n import sys\n import tempfile\n+from pathlib import Path\n from typing import Optional, Type\n from unittest import mock, SkipTest\n \n@@ -10,7 +11,6 @@ from testfixtures import LogCapture\n from twisted.cred import checkers, credentials, portal\n from twisted.internet import defer, error, reactor\n from twisted.protocols.policies import WrappingFactory\n-from twisted.python.filepath import FilePath\n from twisted.trial import unittest\n from twisted.web import resource, server, static, util\n from twisted.web._newclient import ResponseFailed\n@@ -108,14 +108,13 @@ class LoadTestCase(unittest.TestCase):\n class FileTestCase(unittest.TestCase):\n \n     def setUp(self):\n-        self.tmpname = self.mktemp()\n-        with open(self.tmpname + '^', 'w') as f:\n-            f.write('0123456789')\n+        self.tmpname = Path(self.mktemp() + '^')\n+        Path(self.tmpname).write_text('0123456789')\n         handler = create_instance(FileDownloadHandler, None, get_crawler())\n         self.download_request = handler.download_request\n \n     def tearDown(self):\n-        os.unlink(self.tmpname + '^')\n+        self.tmpname.unlink()\n \n     def test_download(self):\n         def _test(response):\n@@ -124,7 +123,7 @@ class FileTestCase(unittest.TestCase):\n             self.assertEqual(response.body, b'0123456789')\n             self.assertEqual(response.protocol, None)\n \n-        request = Request(path_to_file_uri(self.tmpname + '^'))\n+        request = Request(path_to_file_uri(str(self.tmpname)))\n         assert request.url.upper().endswith('%5E')\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n@@ -223,10 +222,10 @@ class HttpTestCase(unittest.TestCase):\n     certfile = 'keys/localhost.crt'\n \n     def setUp(self):\n-        self.tmpname = self.mktemp()\n-        os.mkdir(self.tmpname)\n-        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n-        r = static.File(self.tmpname)\n+        self.tmpname = Path(self.mktemp())\n+        self.tmpname.mkdir()\n+        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n+        r = static.File(str(self.tmpname))\n         r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n         r.putChild(b\"wait\", ForeverTakingResource())\n         r.putChild(b\"hang-after-headers\", ForeverTakingResource(write=True))\n@@ -626,10 +625,10 @@ class Https11CustomCiphers(unittest.TestCase):\n     certfile = 'keys/localhost.crt'\n \n     def setUp(self):\n-        self.tmpname = self.mktemp()\n-        os.mkdir(self.tmpname)\n-        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n-        r = static.File(self.tmpname)\n+        self.tmpname = Path(self.mktemp())\n+        self.tmpname.mkdir()\n+        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n+        r = static.File(str(self.tmpname))\n         self.site = server.Site(r, timeout=None)\n         self.host = 'localhost'\n         self.port = reactor.listenSSL(\n@@ -1002,16 +1001,15 @@ class BaseFTPTestCase(unittest.TestCase):\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n         # setup dirs and test file\n-        self.directory = self.mktemp()\n-        os.mkdir(self.directory)\n-        userdir = os.path.join(self.directory, self.username)\n-        os.mkdir(userdir)\n-        fp = FilePath(userdir)\n+        self.directory = Path(self.mktemp())\n+        self.directory.mkdir()\n+        userdir = self.directory / self.username\n+        userdir.mkdir()\n         for filename, content in self.test_files:\n-            fp.child(filename).setContent(content)\n+            (userdir / filename).write_bytes(content)\n \n         # setup server\n-        realm = FTPRealm(anonymousRoot=self.directory, userHome=self.directory)\n+        realm = FTPRealm(anonymousRoot=str(self.directory), userHome=str(self.directory))\n         p = portal.Portal(realm)\n         users_checker = checkers.InMemoryUsernamePasswordDatabaseDontUse()\n         users_checker.addUser(self.username, self.password)\n@@ -1076,28 +1074,28 @@ class BaseFTPTestCase(unittest.TestCase):\n \n     def test_ftp_local_filename(self):\n         f, local_fname = tempfile.mkstemp()\n-        local_fname = to_bytes(local_fname)\n+        fname_bytes = to_bytes(local_fname)\n+        local_fname = Path(local_fname)\n         os.close(f)\n-        meta = {\"ftp_local_filename\": local_fname}\n+        meta = {\"ftp_local_filename\": fname_bytes}\n         meta.update(self.req_meta)\n         request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/file.txt\",\n                           meta=meta)\n         d = self.download_handler.download_request(request, None)\n \n         def _test(r):\n-            self.assertEqual(r.body, local_fname)\n-            self.assertEqual(r.headers, {b'Local Filename': [local_fname],\n+            self.assertEqual(r.body, fname_bytes)\n+            self.assertEqual(r.headers, {b'Local Filename': [fname_bytes],\n                                          b'Size': [b'17']})\n-            self.assertTrue(os.path.exists(local_fname))\n-            with open(local_fname, \"rb\") as f:\n-                self.assertEqual(f.read(), b\"I have the power!\")\n-            os.remove(local_fname)\n+            self.assertTrue(local_fname.exists())\n+            self.assertEqual(local_fname.read_bytes(), b\"I have the power!\")\n+            local_fname.unlink()\n \n         return self._add_test_callbacks(d, _test)\n \n     def _test_response_class(self, filename, response_class):\n         f, local_fname = tempfile.mkstemp()\n-        local_fname = to_bytes(local_fname)\n+        local_fname = Path(local_fname)\n         os.close(f)\n         meta = {}\n         meta.update(self.req_meta)\n@@ -1107,7 +1105,7 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         def _test(r):\n             self.assertEqual(type(r), response_class)\n-            os.remove(local_fname)\n+            local_fname.unlink()\n         return self._add_test_callbacks(d, _test)\n \n     def test_response_class_from_url(self):\n@@ -1147,15 +1145,14 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n         # setup dir and test file\n-        self.directory = self.mktemp()\n-        os.mkdir(self.directory)\n+        self.directory = Path(self.mktemp())\n+        self.directory.mkdir()\n \n-        fp = FilePath(self.directory)\n         for filename, content in self.test_files:\n-            fp.child(filename).setContent(content)\n+            (self.directory / filename).write_bytes(content)\n \n         # setup server for anonymous access\n-        realm = FTPRealm(anonymousRoot=self.directory)\n+        realm = FTPRealm(anonymousRoot=str(self.directory))\n         p = portal.Portal(realm)\n         p.registerChecker(checkers.AllowAnonymousAccess(),\n                           credentials.IAnonymous)\n\n@@ -1,6 +1,6 @@\n from gzip import GzipFile\n from io import BytesIO\n-from os.path import join\n+from pathlib import Path\n from unittest import TestCase, SkipTest\n from warnings import catch_warnings\n \n@@ -15,7 +15,7 @@ from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n \n-SAMPLEDIR = join(tests_datadir, 'compressed')\n+SAMPLEDIR = Path(tests_datadir, 'compressed')\n \n FORMAT = {\n     'gzip': ('html-gzip.bin', 'gzip'),\n@@ -46,8 +46,7 @@ class HttpCompressionTest(TestCase):\n \n         samplefile, contentencoding = FORMAT[coding]\n \n-        with open(join(SAMPLEDIR, samplefile), 'rb') as sample:\n-            body = sample.read()\n+        body = (SAMPLEDIR / samplefile).read_bytes()\n \n         headers = {\n             'Server': 'Yaws/1.49 Yet Another Web Server',\n\n@@ -2,8 +2,8 @@ import hashlib\n import tempfile\n import unittest\n import shutil\n-import os\n import sys\n+from pathlib import Path\n from testfixtures import LogCapture\n \n from scrapy.dupefilters import RFPDupeFilter\n@@ -157,7 +157,7 @@ class RFPDupeFilterTest(unittest.TestCase):\n             df.request_seen(r1)\n             df.close('finished')\n \n-            with open(os.path.join(path, 'requests.seen'), 'rb') as seen_file:\n+            with Path(path, 'requests.seen').open('rb') as seen_file:\n                 line = next(seen_file).decode()\n                 assert not line.endswith('\\r\\r\\n')\n                 if sys.platform == 'win32':\n\n@@ -10,11 +10,11 @@ module with the ``runserver`` argument::\n     python test_engine.py runserver\n \"\"\"\n \n-import os\n import re\n import subprocess\n import sys\n from collections import defaultdict\n+from pathlib import Path\n from threading import Timer\n from urllib.parse import urlparse\n from dataclasses import dataclass\n@@ -127,8 +127,8 @@ class ChangeCloseReasonSpider(TestSpider):\n \n \n def start_test_site(debug=False):\n-    root_dir = os.path.join(tests_datadir, \"test_site\")\n-    r = static.File(root_dir)\n+    root_dir = Path(tests_datadir, \"test_site\")\n+    r = static.File(str(root_dir))\n     r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n     r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n     numbers = [str(x).encode(\"utf8\") for x in range(2**18)]\n\n@@ -3,7 +3,6 @@ import csv\n import gzip\n import json\n import lzma\n-import os\n import random\n import shutil\n import string\n@@ -15,6 +14,7 @@ from collections import defaultdict\n from contextlib import ExitStack\n from io import BytesIO\n from logging import getLogger\n+from os import PathLike\n from pathlib import Path\n from string import ascii_letters, digits\n from unittest import mock\n@@ -63,41 +63,41 @@ def printf_escape(string):\n     return string.replace('%', '%%')\n \n \n-def build_url(path):\n-    if path[0] != '/':\n-        path = '/' + path\n-    return urljoin('file:', path)\n+def build_url(path: str | PathLike[str]) -> str:\n+    path_str = str(path)\n+    if path_str[0] != '/':\n+        path_str = '/' + path_str\n+    return urljoin('file:', path_str)\n \n \n class FileFeedStorageTest(unittest.TestCase):\n \n     def test_store_file_uri(self):\n-        path = os.path.abspath(self.mktemp())\n-        uri = path_to_file_uri(path)\n+        path = Path(self.mktemp()).resolve()\n+        uri = path_to_file_uri(str(path))\n         return self._assert_stores(FileFeedStorage(uri), path)\n \n     def test_store_file_uri_makedirs(self):\n-        path = os.path.abspath(self.mktemp())\n-        path = os.path.join(path, 'more', 'paths', 'file.txt')\n-        uri = path_to_file_uri(path)\n+        path = Path(self.mktemp()).resolve() / 'more' / 'paths' / 'file.txt'\n+        uri = path_to_file_uri(str(path))\n         return self._assert_stores(FileFeedStorage(uri), path)\n \n     def test_store_direct_path(self):\n-        path = os.path.abspath(self.mktemp())\n-        return self._assert_stores(FileFeedStorage(path), path)\n+        path = Path(self.mktemp()).resolve()\n+        return self._assert_stores(FileFeedStorage(str(path)), path)\n \n     def test_store_direct_path_relative(self):\n-        path = self.mktemp()\n-        return self._assert_stores(FileFeedStorage(path), path)\n+        path = Path(self.mktemp())\n+        return self._assert_stores(FileFeedStorage(str(path)), path)\n \n     def test_interface(self):\n         path = self.mktemp()\n         st = FileFeedStorage(path)\n         verifyObject(IFeedStorage, st)\n \n-    def _store(self, feed_options=None):\n-        path = os.path.abspath(self.mktemp())\n-        storage = FileFeedStorage(path, feed_options=feed_options)\n+    def _store(self, feed_options=None) -> Path:\n+        path = Path(self.mktemp()).resolve()\n+        storage = FileFeedStorage(str(path), feed_options=feed_options)\n         spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n         file.write(b\"content\")\n@@ -106,27 +106,26 @@ class FileFeedStorageTest(unittest.TestCase):\n \n     def test_append(self):\n         path = self._store()\n-        return self._assert_stores(FileFeedStorage(path), path, b\"contentcontent\")\n+        return self._assert_stores(FileFeedStorage(str(path)), path, b\"contentcontent\")\n \n     def test_overwrite(self):\n         path = self._store({\"overwrite\": True})\n         return self._assert_stores(\n-            FileFeedStorage(path, feed_options={\"overwrite\": True}),\n+            FileFeedStorage(str(path), feed_options={\"overwrite\": True}),\n             path\n         )\n \n     @defer.inlineCallbacks\n-    def _assert_stores(self, storage, path, expected_content=b\"content\"):\n+    def _assert_stores(self, storage, path: Path, expected_content=b\"content\"):\n         spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n         file.write(b\"content\")\n         yield storage.store(file)\n-        self.assertTrue(os.path.exists(path))\n+        self.assertTrue(path.exists())\n         try:\n-            with open(path, 'rb') as fp:\n-                self.assertEqual(fp.read(), expected_content)\n+            self.assertEqual(path.read_bytes(), expected_content)\n         finally:\n-            os.unlink(path)\n+            path.unlink()\n \n \n class FTPFeedStorageTest(unittest.TestCase):\n@@ -152,13 +151,12 @@ class FTPFeedStorageTest(unittest.TestCase):\n         file.write(content)\n         return storage.store(file)\n \n-    def _assert_stored(self, path, content):\n+    def _assert_stored(self, path: Path, content):\n         self.assertTrue(path.exists())\n         try:\n-            with path.open('rb') as fp:\n-                self.assertEqual(fp.read(), content)\n+            self.assertEqual(path.read_bytes(), content)\n         finally:\n-            os.unlink(str(path))\n+            path.unlink()\n \n     @defer.inlineCallbacks\n     def test_append(self):\n@@ -221,24 +219,24 @@ class BlockingFeedStorageTest(unittest.TestCase):\n         b = BlockingFeedStorage()\n \n         tmp = b.open(self.get_test_spider())\n-        tmp_path = os.path.dirname(tmp.name)\n-        self.assertEqual(tmp_path, tempfile.gettempdir())\n+        tmp_path = Path(tmp.name).parent\n+        self.assertEqual(str(tmp_path), tempfile.gettempdir())\n \n     def test_temp_file(self):\n         b = BlockingFeedStorage()\n \n-        tests_path = os.path.dirname(os.path.abspath(__file__))\n-        spider = self.get_test_spider({'FEED_TEMPDIR': tests_path})\n+        tests_path = Path(__file__).resolve().parent\n+        spider = self.get_test_spider({'FEED_TEMPDIR': str(tests_path)})\n         tmp = b.open(spider)\n-        tmp_path = os.path.dirname(tmp.name)\n+        tmp_path = Path(tmp.name).parent\n         self.assertEqual(tmp_path, tests_path)\n \n     def test_invalid_folder(self):\n         b = BlockingFeedStorage()\n \n-        tests_path = os.path.dirname(os.path.abspath(__file__))\n-        invalid_path = os.path.join(tests_path, 'invalid_path')\n-        spider = self.get_test_spider({'FEED_TEMPDIR': invalid_path})\n+        tests_path = Path(__file__).resolve().parent\n+        invalid_path = tests_path / 'invalid_path'\n+        spider = self.get_test_spider({'FEED_TEMPDIR': str(invalid_path)})\n \n         self.assertRaises(OSError, b.open, spider=spider)\n \n@@ -564,13 +562,13 @@ class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n class DummyBlockingFeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri, *args, feed_options=None):\n-        self.path = file_uri_to_path(uri)\n+        self.path = Path(file_uri_to_path(uri))\n \n     def _store_in_thread(self, file):\n-        dirname = os.path.dirname(self.path)\n-        if dirname and not os.path.exists(dirname):\n-            os.makedirs(dirname)\n-        with open(self.path, 'ab') as output_file:\n+        dirname = self.path.parent\n+        if dirname and not dirname.exists():\n+            dirname.mkdir(parents=True)\n+        with self.path.open('ab') as output_file:\n             output_file.write(file.read())\n \n         file.close()\n@@ -613,10 +611,10 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         foo = scrapy.Field()\n         hello = scrapy.Field()\n \n-    def _random_temp_filename(self, inter_dir=''):\n+    def _random_temp_filename(self, inter_dir='') -> Path:\n         chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n         filename = ''.join(chars)\n-        return os.path.join(self.temp_dir, inter_dir, filename)\n+        return Path(self.temp_dir, inter_dir, filename)\n \n     def setUp(self):\n         self.temp_dir = tempfile.mkdtemp()\n@@ -702,18 +700,17 @@ class FeedExportTest(FeedExportTestBase):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not os.path.exists(str(file_path)):\n+                if not Path(file_path).exists():\n                     continue\n \n-                with open(str(file_path), 'rb') as f:\n-                    content[feed_options['format']] = f.read()\n+                content[feed_options['format']] = Path(file_path).read_bytes()\n \n         finally:\n             for file_path in FEEDS.keys():\n-                if not os.path.exists(str(file_path)):\n+                if not Path(file_path).exists():\n                     continue\n \n-                os.remove(str(file_path))\n+                Path(file_path).unlink()\n \n         return content\n \n@@ -808,7 +805,7 @@ class FeedExportTest(FeedExportTestBase):\n     def test_stats_file_success(self):\n         settings = {\n             \"FEEDS\": {\n-                printf_escape(path_to_url(self._random_temp_filename())): {\n+                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n                 }\n             },\n@@ -823,7 +820,7 @@ class FeedExportTest(FeedExportTestBase):\n     def test_stats_file_failed(self):\n         settings = {\n             \"FEEDS\": {\n-                printf_escape(path_to_url(self._random_temp_filename())): {\n+                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n                 }\n             },\n@@ -846,7 +843,7 @@ class FeedExportTest(FeedExportTestBase):\n             'AWS_ACCESS_KEY_ID': 'access_key',\n             'AWS_SECRET_ACCESS_KEY': 'secret_key',\n             \"FEEDS\": {\n-                printf_escape(path_to_url(self._random_temp_filename())): {\n+                printf_escape(path_to_url(str(self._random_temp_filename()))): {\n                     \"format\": \"json\",\n                 },\n                 \"s3://bucket/key/foo.csv\": {\n@@ -1427,12 +1424,11 @@ class FeedExportTest(FeedExportTestBase):\n         self.assertTrue(FromCrawlerFileFeedStorage.init_with_crawler)\n \n     @defer.inlineCallbacks\n-    def test_pathlib_uri(self):\n-        feed_path = Path(self._random_temp_filename())\n+    def test_str_uri(self):\n         settings = {\n             'FEED_STORE_EMPTY': True,\n             'FEEDS': {\n-                feed_path: {'format': 'csv'}\n+                str(self._random_temp_filename()): {'format': 'csv'}\n             },\n         }\n         data = yield self.exported_no_data(settings)\n@@ -1538,8 +1534,8 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n         def close(self):\n             self.file.close()\n \n-    def _named_tempfile(self, name):\n-        return os.path.join(self.temp_dir, name)\n+    def _named_tempfile(self, name) -> str:\n+        return str(Path(self.temp_dir, name))\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n@@ -1559,18 +1555,17 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n                 yield crawler.crawl()\n \n             for file_path, feed_options in FEEDS.items():\n-                if not os.path.exists(str(file_path)):\n+                if not Path(file_path).exists():\n                     continue\n \n-                with open(str(file_path), 'rb') as f:\n-                    content[str(file_path)] = f.read()\n+                content[str(file_path)] = Path(file_path).read_bytes()\n \n         finally:\n             for file_path in FEEDS.keys():\n-                if not os.path.exists(str(file_path)):\n+                if not Path(file_path).exists():\n                     continue\n \n-                os.remove(str(file_path))\n+                Path(file_path).unlink()\n \n         return content\n \n@@ -2031,11 +2026,9 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 yield crawler.crawl()\n \n             for path, feed in FEEDS.items():\n-                dir_name = os.path.dirname(path)\n-                for file in sorted(os.listdir(dir_name)):\n-                    with open(os.path.join(dir_name, file), 'rb') as f:\n-                        data = f.read()\n-                        content[feed['format']].append(data)\n+                dir_name = Path(path).parent\n+                for file in sorted(dir_name.iterdir()):\n+                    content[feed['format']].append(file.read_bytes())\n         finally:\n             self.tearDown()\n         defer.returnValue(content)\n@@ -2045,7 +2038,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'jl', self._file_mark): {'format': 'jl'},\n+                self._random_temp_filename() / 'jl' / self._file_mark: {'format': 'jl'},\n             },\n         })\n         batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n@@ -2061,7 +2054,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {'format': 'csv'},\n+                self._random_temp_filename() / 'csv' / self._file_mark: {'format': 'csv'},\n             },\n         })\n         batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n@@ -2077,7 +2070,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n+                self._random_temp_filename() / 'xml' / self._file_mark: {'format': 'xml'},\n             },\n         })\n         batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n@@ -2094,8 +2087,8 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {'format': 'xml'},\n-                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {'format': 'json'},\n+                self._random_temp_filename() / 'xml' / self._file_mark: {'format': 'xml'},\n+                self._random_temp_filename() / 'json' / self._file_mark: {'format': 'json'},\n             },\n         })\n         batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n@@ -2120,7 +2113,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'pickle', self._file_mark): {'format': 'pickle'},\n+                self._random_temp_filename() / 'pickle' / self._file_mark: {'format': 'pickle'},\n             },\n         })\n         batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n@@ -2137,7 +2130,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'marshal', self._file_mark): {'format': 'marshal'},\n+                self._random_temp_filename() / 'marshal' / self._file_mark: {'format': 'marshal'},\n             },\n         })\n         batch_size = Settings(settings).getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n@@ -2184,7 +2177,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         for fmt in ('json', 'jsonlines', 'xml', 'csv'):\n             settings = {\n                 'FEEDS': {\n-                    os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},\n+                    self._random_temp_filename() / fmt / self._file_mark: {'format': fmt},\n                 },\n                 'FEED_EXPORT_BATCH_ITEM_COUNT': 1\n             }\n@@ -2204,7 +2197,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         for fmt, expctd in formats:\n             settings = {\n                 'FEEDS': {\n-                    os.path.join(self._random_temp_filename(), fmt, self._file_mark): {'format': fmt},\n+                    self._random_temp_filename() / fmt / self._file_mark: {'format': fmt},\n                 },\n                 'FEED_STORE_EMPTY': True,\n                 'FEED_EXPORT_INDENT': None,\n@@ -2237,19 +2230,19 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n         settings = {\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {\n+                self._random_temp_filename() / 'json' / self._file_mark: {\n                     'format': 'json',\n                     'indent': 0,\n                     'fields': ['bar'],\n                     'encoding': 'utf-8',\n                 },\n-                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {\n+                self._random_temp_filename() / 'xml' / self._file_mark: {\n                     'format': 'xml',\n                     'indent': 2,\n                     'fields': ['foo'],\n                     'encoding': 'latin-1',\n                 },\n-                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {\n+                self._random_temp_filename() / 'csv' / self._file_mark: {\n                     'format': 'csv',\n                     'indent': None,\n                     'fields': ['foo', 'bar'],\n@@ -2272,7 +2265,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         }\n         settings = {\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {\n+                self._random_temp_filename() / 'json' / self._file_mark: {\n                     'format': 'json',\n                     'indent': None,\n                     'encoding': 'utf-8',\n@@ -2299,7 +2292,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n         ]\n         settings = {\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), '%(batch_time)s'): {\n+                self._random_temp_filename() / '%(batch_time)s': {\n                     'format': 'json',\n                 },\n             },\n@@ -2312,7 +2305,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n     def test_stats_batch_file_success(self):\n         settings = {\n             \"FEEDS\": {\n-                build_url(os.path.join(self._random_temp_filename(), \"json\", self._file_mark)): {\n+                build_url(str(self._random_temp_filename() / \"json\" / self._file_mark)): {\n                     \"format\": \"json\",\n                 }\n             },\n\n@@ -1,10 +1,10 @@\n import json\n-import os\n import random\n import re\n import shutil\n import string\n from ipaddress import IPv4Address\n+from pathlib import Path\n from unittest import mock, skipIf\n from urllib.parse import urlencode\n \n@@ -163,9 +163,8 @@ class RequestHeaders(LeafResource):\n         return bytes(json.dumps(headers), 'utf-8')\n \n \n-def get_client_certificate(key_file, certificate_file) -> PrivateCertificate:\n-    with open(key_file, 'r') as key, open(certificate_file, 'r') as certificate:\n-        pem = ''.join(key.readlines()) + ''.join(certificate.readlines())\n+def get_client_certificate(key_file: Path, certificate_file: Path) -> PrivateCertificate:\n+    pem = key_file.read_text() + certificate_file.read_text()\n \n     return PrivateCertificate.loadPEM(pem)\n \n@@ -173,12 +172,12 @@ def get_client_certificate(key_file, certificate_file) -> PrivateCertificate:\n @skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n class Https2ClientProtocolTestCase(TestCase):\n     scheme = 'https'\n-    key_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.key')\n-    certificate_file = os.path.join(os.path.dirname(__file__), 'keys', 'localhost.crt')\n+    key_file = Path(__file__).parent / 'keys' / 'localhost.key'\n+    certificate_file = Path(__file__).parent / 'keys' / 'localhost.crt'\n \n     def _init_resource(self):\n         self.temp_directory = self.mktemp()\n-        os.mkdir(self.temp_directory)\n+        Path(self.temp_directory).mkdir()\n         r = File(self.temp_directory)\n         r.putChild(b'get-data-html-small', GetDataHtmlSmall())\n         r.putChild(b'get-data-html-large', GetDataHtmlLarge())\n@@ -202,7 +201,7 @@ class Https2ClientProtocolTestCase(TestCase):\n \n         # Start server for testing\n         self.hostname = 'localhost'\n-        context_factory = ssl_context_factory(self.key_file, self.certificate_file)\n+        context_factory = ssl_context_factory(str(self.key_file), str(self.certificate_file))\n \n         server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)\n         self.server = yield server_endpoint.listen(self.site)\n\n@@ -1,5 +1,5 @@\n-import os\n import shutil\n+from pathlib import Path\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -61,12 +61,12 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.mockserver.__enter__()\n \n         # prepare a directory for storing files\n-        self.tmpmediastore = self.mktemp()\n-        os.mkdir(self.tmpmediastore)\n+        self.tmpmediastore = Path(self.mktemp())\n+        self.tmpmediastore.mkdir()\n         self.settings = {\n             'REQUEST_FINGERPRINTER_IMPLEMENTATION': 'VERSION',\n             'ITEM_PIPELINES': {self.pipeline_class: 1},\n-            self.store_setting_key: self.tmpmediastore,\n+            self.store_setting_key: str(self.tmpmediastore),\n         }\n         self.runner = CrawlerRunner(self.settings)\n         self.items = []\n@@ -111,9 +111,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         # check that the image files where actually written to the media store\n         for item in items:\n             for i in item[self.media_key]:\n-                self.assertTrue(\n-                    os.path.exists(\n-                        os.path.join(self.tmpmediastore, i['path'])))\n+                self.assertTrue((self.tmpmediastore / i['path']).exists())\n \n     def _assert_files_download_failure(self, crawler, items, code, logs):\n \n@@ -133,7 +131,7 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertEqual(logs.count(file_dl_failure), 3)\n \n         # check that no files were written to the media store\n-        self.assertEqual(os.listdir(self.tmpmediastore), [])\n+        self.assertEqual([x for x in self.tmpmediastore.iterdir()], [])\n \n     @defer.inlineCallbacks\n     def test_download_media(self):\n\n@@ -3,6 +3,7 @@ import random\n import time\n from datetime import datetime\n from io import BytesIO\n+from pathlib import Path\n from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import mock\n@@ -89,7 +90,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n \n         path = 'some/image/key.jpg'\n-        fullpath = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n+        fullpath = Path(self.tempdir, 'some', 'image', 'key.jpg')\n         self.assertEqual(self.pipeline.store._get_filesystem_path(path), fullpath)\n \n     @defer.inlineCallbacks\n\n@@ -2,6 +2,7 @@ import json\n import os\n import re\n import sys\n+from pathlib import Path\n from subprocess import Popen, PIPE\n from urllib.parse import urlsplit, urlunsplit\n from testfixtures import LogCapture\n@@ -27,14 +28,13 @@ from mitmproxy.tools.main import mitmdump\n sys.argv[0] = \"mitmdump\"\n sys.exit(mitmdump())\n         \"\"\"\n-        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n-                                 'keys', 'mitmproxy-ca.pem')\n+        cert_path = Path(__file__).parent.resolve() / 'keys' / 'mitmproxy-ca.pem'\n         self.proc = Popen([sys.executable,\n                            '-c', script,\n                            '--listen-host', '127.0.0.1',\n                            '--listen-port', '0',\n                            '--proxyauth', f'{self.auth_user}:{self.auth_pass}',\n-                           '--certs', cert_path,\n+                           '--certs', str(cert_path),\n                            '--ssl-insecure',\n                            ],\n                           stdout=PIPE, env=get_testenv())\n\n@@ -1,7 +1,7 @@\n import sys\n-import os\n import shutil\n import warnings\n+from pathlib import Path\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n@@ -17,10 +17,10 @@ from scrapy.settings import Settings\n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n \n-module_dir = os.path.dirname(os.path.abspath(__file__))\n+module_dir = Path(__file__).resolve().parent\n \n \n-def _copytree(source, target):\n+def _copytree(source: Path, target: Path):\n     try:\n         shutil.copytree(source, target)\n     except shutil.Error:\n@@ -30,18 +30,18 @@ def _copytree(source, target):\n class SpiderLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n-        orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n-        self.tmpdir = tempfile.mkdtemp()\n-        self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n+        orig_spiders_dir = module_dir / 'test_spiders'\n+        self.tmpdir = Path(tempfile.mkdtemp())\n+        self.spiders_dir = self.tmpdir / 'test_spiders_xxx'\n         _copytree(orig_spiders_dir, self.spiders_dir)\n-        sys.path.append(self.tmpdir)\n+        sys.path.append(str(self.tmpdir))\n         settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n         self.spider_loader = SpiderLoader.from_settings(settings)\n \n     def tearDown(self):\n         del self.spider_loader\n         del sys.modules['test_spiders_xxx']\n-        sys.path.remove(self.tmpdir)\n+        sys.path.remove(str(self.tmpdir))\n \n     def test_interface(self):\n         verifyObject(ISpiderLoader, self.spider_loader)\n@@ -135,22 +135,22 @@ class SpiderLoaderTest(unittest.TestCase):\n class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n-        orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n-        self.tmpdir = self.mktemp()\n-        os.mkdir(self.tmpdir)\n-        self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n+        orig_spiders_dir = module_dir / 'test_spiders'\n+        self.tmpdir = Path(self.mktemp())\n+        self.tmpdir.mkdir()\n+        self.spiders_dir = self.tmpdir / 'test_spiders_xxx'\n         _copytree(orig_spiders_dir, self.spiders_dir)\n-        sys.path.append(self.tmpdir)\n+        sys.path.append(str(self.tmpdir))\n         self.settings = Settings({'SPIDER_MODULES': ['test_spiders_xxx']})\n \n     def tearDown(self):\n         del sys.modules['test_spiders_xxx']\n-        sys.path.remove(self.tmpdir)\n+        sys.path.remove(str(self.tmpdir))\n \n     def test_dupename_warning(self):\n         # copy 1 spider module so as to have duplicate spider name\n-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider3.py'),\n-                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider3dupe.py'))\n+        shutil.copyfile(self.tmpdir / 'test_spiders_xxx' / 'spider3.py',\n+                        self.tmpdir / 'test_spiders_xxx' / 'spider3dupe.py')\n \n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n@@ -171,10 +171,10 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n     def test_multiple_dupename_warning(self):\n         # copy 2 spider modules so as to have duplicate spider name\n         # This should issue 2 warning, 1 for each duplicate spider name\n-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider1.py'),\n-                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider1dupe.py'))\n-        shutil.copyfile(os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider2.py'),\n-                        os.path.join(self.tmpdir, 'test_spiders_xxx', 'spider2dupe.py'))\n+        shutil.copyfile(self.tmpdir / 'test_spiders_xxx' / 'spider1.py',\n+                        self.tmpdir / 'test_spiders_xxx' / 'spider1dupe.py')\n+        shutil.copyfile(self.tmpdir / 'test_spiders_xxx' / 'spider2.py',\n+                        self.tmpdir / 'test_spiders_xxx' / 'spider2dupe.py')\n \n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n\n@@ -1,5 +1,5 @@\n-import os\n from datetime import datetime\n+from pathlib import Path\n import shutil\n from twisted.trial import unittest\n \n@@ -13,7 +13,7 @@ class SpiderStateTest(unittest.TestCase):\n \n     def test_store_load(self):\n         jobdir = self.mktemp()\n-        os.mkdir(jobdir)\n+        Path(jobdir).mkdir()\n         try:\n             spider = Spider(name='default')\n             dt = datetime.now()\n\n@@ -1,5 +1,5 @@\n import unittest\n-from os.path import join\n+from pathlib import Path\n \n from w3lib.encoding import html_to_unicode\n \n@@ -8,14 +8,13 @@ from scrapy.http import Response\n from tests import tests_datadir\n \n \n-SAMPLEDIR = join(tests_datadir, 'compressed')\n+SAMPLEDIR = Path(tests_datadir, 'compressed')\n \n \n class GunzipTest(unittest.TestCase):\n \n     def test_gunzip_basic(self):\n-        with open(join(SAMPLEDIR, 'feed-sample1.xml.gz'), 'rb') as f:\n-            r1 = Response(\"http://www.example.com\", body=f.read())\n+        r1 = Response(\"http://www.example.com\", body=(SAMPLEDIR / 'feed-sample1.xml.gz').read_bytes())\n         self.assertTrue(gzip_magic_number(r1))\n \n         r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n@@ -23,17 +22,14 @@ class GunzipTest(unittest.TestCase):\n         self.assertEqual(len(r2.body), 9950)\n \n     def test_gunzip_truncated(self):\n-        with open(join(SAMPLEDIR, 'truncated-crc-error.gz'), 'rb') as f:\n-            text = gunzip(f.read())\n+        text = gunzip((SAMPLEDIR / 'truncated-crc-error.gz').read_bytes())\n         assert text.endswith(b'</html')\n \n     def test_gunzip_no_gzip_file_raises(self):\n-        with open(join(SAMPLEDIR, 'feed-sample1.xml'), 'rb') as f:\n-            self.assertRaises(IOError, gunzip, f.read())\n+        self.assertRaises(IOError, gunzip, (SAMPLEDIR / 'feed-sample1.xml').read_bytes())\n \n     def test_gunzip_truncated_short(self):\n-        with open(join(SAMPLEDIR, 'truncated-crc-error-short.gz'), 'rb') as f:\n-            r1 = Response(\"http://www.example.com\", body=f.read())\n+        r1 = Response(\"http://www.example.com\", body=(SAMPLEDIR / 'truncated-crc-error-short.gz').read_bytes())\n         self.assertTrue(gzip_magic_number(r1))\n \n         r2 = Response(\"http://www.example.com\", body=gunzip(r1.body))\n@@ -45,9 +41,7 @@ class GunzipTest(unittest.TestCase):\n         self.assertFalse(gzip_magic_number(r1))\n \n     def test_gunzip_illegal_eof(self):\n-        with open(join(SAMPLEDIR, 'unexpected-eof.gz'), 'rb') as f:\n-            text = html_to_unicode('charset=cp1252', gunzip(f.read()))[1]\n-            with open(join(SAMPLEDIR, 'unexpected-eof-output.txt'), 'rb') as o:\n-                expected_text = o.read().decode(\"utf-8\")\n+        text = html_to_unicode('charset=cp1252', gunzip((SAMPLEDIR / 'unexpected-eof.gz').read_bytes()))[1]\n+        expected_text = (SAMPLEDIR / 'unexpected-eof-output.txt').read_text(encoding=\"utf-8\")\n         self.assertEqual(len(text), len(expected_text))\n         self.assertEqual(text, expected_text)\n\n@@ -1,5 +1,3 @@\n-import os\n-\n from pytest import mark\n from twisted.trial import unittest\n \n@@ -303,11 +301,6 @@ class LxmlXmliterTestCase(XmliterTestCase):\n \n \n class UtilsCsvTestCase(unittest.TestCase):\n-    sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')\n-    sample_feed_path = os.path.join(sample_feeds_dir, 'feed-sample3.csv')\n-    sample_feed2_path = os.path.join(sample_feeds_dir, 'feed-sample4.csv')\n-    sample_feed3_path = os.path.join(sample_feeds_dir, 'feed-sample5.csv')\n-\n     def test_csviter_defaults(self):\n         body = get_testdata('feeds', 'feed-sample3.csv')\n         response = TextResponse(url=\"http://example.com/\", body=body)\n\n@@ -1,6 +1,7 @@\n import sys\n import os\n import unittest\n+from pathlib import Path\n from unittest import mock\n \n from scrapy.item import Item, Field\n@@ -55,7 +56,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertRaises(ImportError, walk_modules, 'nomodule999')\n \n     def test_walk_modules_egg(self):\n-        egg = os.path.join(os.path.dirname(__file__), 'test.egg')\n+        egg = str(Path(__file__).parent / 'test.egg')\n         sys.path.append(egg)\n         try:\n             mods = walk_modules('testegg')\n\n@@ -4,6 +4,7 @@ import tempfile\n import shutil\n import contextlib\n import warnings\n+from pathlib import Path\n \n from pytest import warns\n \n@@ -18,9 +19,7 @@ def inside_a_project():\n \n     try:\n         os.chdir(project_dir)\n-        with open('scrapy.cfg', 'w') as f:\n-            # create an empty scrapy.cfg\n-            f.close()\n+        Path('scrapy.cfg').touch()\n \n         yield project_dir\n     finally:\n@@ -31,20 +30,20 @@ def inside_a_project():\n class ProjectUtilsTest(unittest.TestCase):\n     def test_data_path_outside_project(self):\n         self.assertEqual(\n-            os.path.join('.scrapy', 'somepath'),\n+            str(Path('.scrapy', 'somepath')),\n             data_path('somepath')\n         )\n-        abspath = os.path.join(os.path.sep, 'absolute', 'path')\n+        abspath = str(Path(os.path.sep, 'absolute', 'path'))\n         self.assertEqual(abspath, data_path(abspath))\n \n     def test_data_path_inside_project(self):\n         with inside_a_project() as proj_path:\n-            expected = os.path.join(proj_path, '.scrapy', 'somepath')\n+            expected = Path(proj_path, '.scrapy', 'somepath')\n             self.assertEqual(\n-                os.path.realpath(expected),\n-                os.path.realpath(data_path('somepath'))\n+                expected.resolve(),\n+                Path(data_path('somepath')).resolve()\n             )\n-            abspath = os.path.join(os.path.sep, 'absolute', 'path')\n+            abspath = str(Path(os.path.sep, 'absolute', 'path').resolve())\n             self.assertEqual(abspath, data_path(abspath))\n \n \n\n@@ -1,6 +1,6 @@\n-import os\n import unittest\n import warnings\n+from pathlib import Path\n from urllib.parse import urlparse\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -39,10 +39,9 @@ class ResponseUtilsTest(unittest.TestCase):\n \n         def browser_open(burl):\n             path = urlparse(burl).path\n-            if not os.path.exists(path):\n+            if not path or not Path(path).exists():\n                 path = burl.replace('file://', '')\n-            with open(path, \"rb\") as f:\n-                bbody = f.read()\n+            bbody = Path(path).read_bytes()\n             self.assertIn(b'<base href=\"' + to_bytes(url) + b'\">', bbody)\n             return True\n         response = HtmlResponse(url, body=body)\n@@ -98,10 +97,9 @@ class ResponseUtilsTest(unittest.TestCase):\n \n         def check_base_url(burl):\n             path = urlparse(burl).path\n-            if not os.path.exists(path):\n+            if not path or not Path(path).exists():\n                 path = burl.replace('file://', '')\n-            with open(path, \"rb\") as f:\n-                bbody = f.read()\n+            bbody = Path(path).read_bytes()\n             self.assertEqual(bbody.count(b'<base href=\"' + to_bytes(url) + b'\">'), 1)\n             return True\n \n\n@@ -1,4 +1,4 @@\n-import os\n+from pathlib import Path\n from shutil import rmtree\n from tempfile import mkdtemp\n import unittest\n@@ -22,21 +22,19 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n         template = 'from ${project_name}.spiders.${name} import ${classname}'\n         rendered = 'from proj.spiders.spi import TheSpider'\n \n-        template_path = os.path.join(self.tmp_path, 'templ.py.tmpl')\n-        render_path = os.path.join(self.tmp_path, 'templ.py')\n+        template_path = Path(self.tmp_path, 'templ.py.tmpl')\n+        render_path = Path(self.tmp_path, 'templ.py')\n \n-        with open(template_path, 'wb') as tmpl_file:\n-            tmpl_file.write(template.encode('utf8'))\n-        assert os.path.isfile(template_path)  # Failure of test itself\n+        template_path.write_text(template, encoding='utf8')\n+        assert template_path.is_file()  # Failure of test itself\n \n-        render_templatefile(template_path, **context)\n+        render_templatefile(str(template_path), **context)\n \n-        self.assertFalse(os.path.exists(template_path))\n-        with open(render_path, 'rb') as result:\n-            self.assertEqual(result.read().decode('utf8'), rendered)\n+        self.assertFalse(template_path.exists())\n+        self.assertEqual(render_path.read_text(encoding='utf8'), rendered)\n \n-        os.remove(render_path)\n-        assert not os.path.exists(render_path)  # Failure of test itself\n+        render_path.unlink()\n+        assert not render_path.exists()  # Failure of test itself\n \n \n if '__main__' == __name__:\n\n@@ -2,8 +2,8 @@\n from twisted.internet import defer\n Tests borrowed from the twisted.web.client tests.\n \"\"\"\n-import os\n import shutil\n+from pathlib import Path\n \n import OpenSSL.SSL\n from twisted.trial import unittest\n@@ -15,7 +15,6 @@ except ImportError:\n     # deprecated in Twisted 19.7.0\n     # (remove once we bump our requirement past that version)\n     from twisted.test.proto_helpers import StringTransport\n-from twisted.python.filepath import FilePath\n from twisted.protocols.policies import WrappingFactory\n from twisted.internet.defer import inlineCallbacks\n \n@@ -230,10 +229,10 @@ class WebClientTestCase(unittest.TestCase):\n         return reactor.listenTCP(0, site, interface=\"127.0.0.1\")\n \n     def setUp(self):\n-        self.tmpname = self.mktemp()\n-        os.mkdir(self.tmpname)\n-        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n-        r = static.File(self.tmpname)\n+        self.tmpname = Path(self.mktemp())\n+        self.tmpname.mkdir()\n+        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n+        r = static.File(str(self.tmpname))\n         r.putChild(b\"redirect\", util.Redirect(b\"/file\"))\n         r.putChild(b\"wait\", ForeverTakingResource())\n         r.putChild(b\"error\", ErrorResource())\n@@ -379,10 +378,10 @@ class WebClientSSLTestCase(unittest.TestCase):\n         return f\"https://127.0.0.1:{self.portno}/{path}\"\n \n     def setUp(self):\n-        self.tmpname = self.mktemp()\n-        os.mkdir(self.tmpname)\n-        FilePath(self.tmpname).child(\"file\").setContent(b\"0123456789\")\n-        r = static.File(self.tmpname)\n+        self.tmpname = Path(self.mktemp())\n+        self.tmpname.mkdir()\n+        (self.tmpname / \"file\").write_bytes(b\"0123456789\")\n+        r = static.File(str(self.tmpname))\n         r.putChild(b\"payload\", PayloadResource())\n         self.site = server.Site(r, timeout=None)\n         self.wrapper = WrappingFactory(self.site)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
