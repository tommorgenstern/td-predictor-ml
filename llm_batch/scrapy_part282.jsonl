{"custom_id": "scrapy#182445f9d96130b1041ece8c4b2a9e9891107c73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 10 | Churn Cumulative: 1653 | Contributors (this commit): 32 | Commits (past 90d): 6 | Contributors (cumulative): 61 | DMM Complexity: None\n\nDIFF:\n@@ -173,7 +173,7 @@ def _request_deferred(request):\n \n     This returns a Deferred whose first pair of callbacks are the request\n     callback and errback. The Deferred also triggers when the request\n-    callback/errback is executed (ie. when the request is downloaded)\n+    callback/errback is executed (i.e. when the request is downloaded)\n \n     WARNING: Do not call request.replace() until after the deferred is called.\n     \"\"\"\n\n@@ -37,8 +37,8 @@ def arg_to_iter(arg):\n def load_object(path):\n     \"\"\"Load an object given its absolute object path, and return it.\n \n-    object can be a class, function, variable or an instance.\n-    path ie: 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n+    object can be the import path of a class, function, variable or an\n+    instance, e.g. 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware'\n     \"\"\"\n \n     try:\n\n@@ -28,7 +28,7 @@ def request_fingerprint(request, include_headers=None, keep_fragments=False):\n     http://www.example.com/query?cat=222&id=111\n \n     Even though those are two different URLs both point to the same resource\n-    and are equivalent (ie. they should return the same response).\n+    and are equivalent (i.e. they should return the same response).\n \n     Another example are cookies used to store session ids. Suppose the\n     following page is only accessible to authenticated users:\n\n@@ -15,7 +15,7 @@ def iterate_spider_output(result):\n \n def iter_spider_classes(module):\n     \"\"\"Return an iterator over all spider classes defined in the given module\n-    that can be instantiated (ie. which have name)\n+    that can be instantiated (i.e. which have name)\n     \"\"\"\n     # this needs to be imported here until get rid of the spider manager\n     # singleton in scrapy.spider.spiders\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb21dae5240d2b66feb72940cdd141dba31ecd7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 13 | Churn Cumulative: 823 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -126,7 +126,6 @@ class Shell(object):\n         self.vars['spider'] = spider\n         self.vars['request'] = request\n         self.vars['response'] = response\n-        self.vars['sel'] = _SelectorProxy(response)\n         if self.inthread:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n@@ -192,15 +191,3 @@ def _request_deferred(request):\n \n     request.callback, request.errback = d.callback, d.errback\n     return d\n-\n-\n-class _SelectorProxy(object):\n-\n-    def __init__(self, response):\n-        self._proxiedresponse = response\n-\n-    def __getattr__(self, name):\n-        warnings.warn('\"sel\" shortcut is deprecated. Use \"response.xpath()\", '\n-                      '\"response.css()\" or \"response.selector\" instead',\n-                      category=ScrapyDeprecationWarning, stacklevel=2)\n-        return getattr(self._proxiedresponse.selector, name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6972a197073af11bcb582cc03f6286fceda5ca6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 826 | Contributors (this commit): 19 | Commits (past 90d): 4 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -5,14 +5,13 @@ See documentation in docs/topics/shell.rst\n \"\"\"\n import os\n import signal\n-import warnings\n \n from twisted.internet import threads, defer\n from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n from scrapy.crawler import Crawler\n-from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n+from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.settings import Settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f78a591f8796686dc65854c250d6ef5324024aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1052 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,6 @@ from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n \n \n-\n logger = logging.getLogger(__name__)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91bbc70bc10cf326940eaf53294149444f43fb9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 10 | Files Changed: 28 | Hunks: 36 | Methods Changed: 14 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 36 | Churn Cumulative: 9846 | Contributors (this commit): 72 | Commits (past 90d): 48 | Contributors (cumulative): 258 | DMM Complexity: None\n\nDIFF:\n@@ -89,4 +89,5 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n                     'from host \"{}\" (exception: {})'.format(\n                         self._hostnameASCII, repr(e)))\n \n+\n DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString('DEFAULT')\n\n@@ -230,6 +230,7 @@ class ExecutionEngine(object):\n     def _download(self, request, spider):\n         slot = self.slot\n         slot.add_request(request)\n+\n         def _on_success(response):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n\n@@ -116,4 +116,5 @@ class ResponseTypes(object):\n             cls = self.from_body(body)\n         return cls\n \n+\n responsetypes = ResponseTypes()\n\n@@ -54,6 +54,7 @@ def _embed_standard_shell(namespace={}, banner=''):\n     else:\n         import rlcompleter  # noqa: F401\n         readline.parse_and_bind(\"tab:complete\")\n+\n     @wraps(_embed_standard_shell)\n     def wrapper(namespace=namespace, banner=''):\n         code.interact(banner=banner, local=namespace)\n\n@@ -42,6 +42,7 @@ def gunzip(data):\n                 raise\n     return b''.join(output_list)\n \n+\n _is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search\n _is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search\n \n\n@@ -147,7 +147,6 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n-\n     @defer.inlineCallbacks\n     def test_pipelines(self):\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n\n@@ -107,6 +107,7 @@ class CrawlerLoggingTestCase(unittest.TestCase):\n \n     def test_spider_custom_settings_log_level(self):\n         log_file = self.mktemp()\n+\n         class MySpider(scrapy.Spider):\n             name = 'spider'\n             custom_settings = {\n\n@@ -13,5 +13,6 @@ class ScrapyUtilsTest(unittest.TestCase):\n             installed_version = [int(x) for x in module.__version__.split('.')[:2]]\n             assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -145,7 +145,6 @@ class CookiesMiddlewareTest(TestCase):\n                 {'name': 'C3', 'value': 'value3', 'path': '/foo', 'domain': 'scrapytest.org'},\n                 {'name': 'C4', 'value': 'value4', 'path': '/foo', 'domain': 'scrapy.org'}]\n \n-\n         req = Request('http://scrapytest.org/', cookies=cookies)\n         self.mw.process_request(req, self.spider)\n \n\n@@ -501,5 +501,6 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 self.assertEqualResponse(res1, res2)\n                 assert 'cached' in res2.flags\n \n+\n if __name__ == '__main__':\n     unittest.main()\n\n@@ -68,7 +68,6 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(r, Response)\n         assert r is rsp\n \n-\n     def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n@@ -122,7 +121,6 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         del rsp.headers['Location']\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n-\n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n         req = Request('http://scrapytest.org/302')\n@@ -178,6 +176,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n     def test_request_meta_handling(self):\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n+\n         def _test_passthrough(req):\n             rsp = Response(url, headers={'Location': url2}, status=301, request=req)\n             r = self.mw.process_response(req, rsp, self.spider)\n@@ -316,5 +315,6 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         response = mw.process_response(req, rsp, self.spider)\n         assert isinstance(response, Response)\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -312,6 +312,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n                         for child in children]\n             else:\n                 return [(elem.tag, [(elem.text, ())])]\n+\n         def xmlsplit(xmlcontent):\n             doc = lxml.etree.fromstring(xmlcontent)\n             return xmltuple(doc)\n\n@@ -182,6 +182,7 @@ class BaseResponseTest(unittest.TestCase):\n     def test_follow_whitespace_link(self):\n         self._assert_followed_url(Link('http://example.com/foo '),\n                                   'http://example.com/foo%20')\n+\n     def test_follow_flags(self):\n         res = self.response_class('http://example.com/')\n         fol = res.follow('http://example.com/', flags=['cached', 'allowed'])\n\n@@ -259,6 +259,7 @@ class ItemTest(unittest.TestCase):\n         with catch_warnings(record=True) as warnings:\n             item = Item()\n             self.assertEqual(len(warnings), 0)\n+\n             class SubclassedItem(Item):\n                 pass\n             subclassed_item = SubclassedItem()\n\n@@ -121,5 +121,6 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(text.get_charset(), Charset('utf-8'))\n         self.assertEqual(attach.get_payload(decode=True).decode('utf-8'), body)\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -286,7 +286,6 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n         self.assertEqual(pipeline.files_result_field, \"this\")\n         self.assertEqual(pipeline.files_urls_field, \"that\")\n \n-\n     def test_user_defined_subclass_default_key_names(self):\n         \"\"\"Test situation when user defines subclass of FilesPipeline,\n         but uses attribute names for default pipeline (without prefixing\n\n@@ -177,7 +177,6 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         IMAGES_RESULT_FIELD='images'\n     )\n \n-\n     def setUp(self):\n         self.tempdir = mkdtemp()\n \n\n@@ -304,6 +304,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n             return response\n \n         rsp1 = Response('http://url')\n+\n         def rsp1_func():\n             dfd = Deferred().addCallback(_check_downloading)\n             reactor.callLater(.1, dfd.callback, rsp1)\n\n@@ -90,5 +90,6 @@ class ResponseTypesTest(unittest.TestCase):\n         # check that mime.types files shipped with scrapy are loaded\n         self.assertEqual(responsetypes.mimetypes.guess_type('x.scrapytest')[0], 'x-scrapy/test')\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -83,7 +83,6 @@ class BuildComponentListTest(unittest.TestCase):\n         self.assertRaises(ValueError, build_component_list, {}, d, convert=lambda x: x)\n \n \n-\n class UtilsConfTestCase(unittest.TestCase):\n \n     def test_arglist_to_dict(self):\n\n@@ -9,6 +9,7 @@ from scrapy.utils.defer import mustbe_deferred, process_chain, \\\n class MustbeDeferredTest(unittest.TestCase):\n     def test_success_function(self):\n         steps = []\n+\n         def _append(v):\n             steps.append(v)\n             return steps\n@@ -20,6 +21,7 @@ class MustbeDeferredTest(unittest.TestCase):\n \n     def test_unfired_deferred(self):\n         steps = []\n+\n         def _append(v):\n             steps.append(v)\n             dfd = defer.Deferred()\n\n@@ -110,6 +110,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         # ignore subclassing warnings\n         with warnings.catch_warnings():\n             warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n+\n             class UserClass(Deprecated):\n                 pass\n \n@@ -233,6 +234,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n         with warnings.catch_warnings(record=True) as w:\n             AlsoDeprecated()\n+\n             class UserClass(AlsoDeprecated):\n                 pass\n \n@@ -247,6 +249,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         with mock.patch('inspect.stack', side_effect=IndexError):\n             with warnings.catch_warnings(record=True) as w:\n                 DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n+\n                 class SubClass(DeprecatedName):\n                     pass\n \n\n@@ -387,7 +387,6 @@ class TestHelper(unittest.TestCase):\n             self.assertTrue(type(r1) is type(r2))\n             self.assertTrue(type(r1) is not type(r3))\n \n-\n     def _assert_type_and_value(self, a, b, obj):\n         self.assertTrue(type(a) is type(b),\n                         'Got {}, expected {} for {!r}'.format(type(a), type(b), obj))\n\n@@ -104,7 +104,6 @@ class BinaryIsTextTest(unittest.TestCase):\n         assert not binary_is_text(b\"\\x02\\xa3\")\n \n \n-\n class UtilsPythonTestCase(unittest.TestCase):\n \n     def test_equal_attributes(self):\n@@ -215,7 +214,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n             self.assertEqual(\n                 get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n \n-\n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n         self.assertEqual(without_none_values((1, None, 3, 4)), (1, 3, 4))\n@@ -223,5 +221,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n             without_none_values({'one': 1, 'none': None, 'three': 3, 'four': 4}),\n             {'one': 1, 'three': 3, 'four': 4})\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -83,5 +83,6 @@ class UtilsRequestTest(unittest.TestCase):\n         request_httprepr(Request(\"file:///tmp/foo.txt\"))\n         request_httprepr(Request(\"ftp://localhost/tmp/foo.txt\"))\n \n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -38,5 +38,6 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n         os.remove(render_path)\n         assert not os.path.exists(render_path)  # Failure of test iself\n \n+\n if '__main__' == __name__:\n     unittest.main()\n\n@@ -201,6 +201,7 @@ def create_skipped_scheme_t(args):\n         assert url.startswith(args[1])\n     return do_expected\n \n+\n for k, args in enumerate ([\n             ('/index',                              'file://'),\n             ('/index.html',                         'file://'),\n\n@@ -294,6 +294,7 @@ class WebClientTestCase(unittest.TestCase):\n         finished = self.assertFailure(\n             getPage(self.getURL(\"wait\"), timeout=0.000001),\n             defer.TimeoutError)\n+\n         def cleanup(passthrough):\n             # Clean up the server which is hanging around not doing\n             # anything.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03ed9e17867b8c7533d08ef28108a67305050e9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 44 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 44 | Churn Cumulative: 805 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,6 @@ This module contains essential stuff that should've come with Python itself ;)\n import errno\n import gc\n import inspect\n-import os\n import re\n import sys\n import weakref\n@@ -165,14 +164,6 @@ _BINARYCHARS = {to_bytes(chr(i)) for i in range(32)} - {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\\n _BINARYCHARS |= {ord(ch) for ch in _BINARYCHARS}\n \n \n-@deprecated(\"scrapy.utils.python.binary_is_text\")\n-def isbinarytext(text):\n-    \"\"\" This function is deprecated.\n-    Please use scrapy.utils.python.binary_is_text, which was created to be more\n-    clear about the functions behavior: it is behaving inverted to this one. \"\"\"\n-    return not binary_is_text(text)\n-\n-\n def binary_is_text(data):\n     \"\"\" Returns ``True`` if the given ``data`` argument (a ``bytes`` object)\n     does not contain unprintable control characters.\n@@ -293,41 +284,6 @@ class WeakKeyCache(object):\n         return self._weakdict[key]\n \n \n-@deprecated\n-def stringify_dict(dct_or_tuples, encoding='utf-8', keys_only=True):\n-    \"\"\"Return a (new) dict with unicode keys (and values when \"keys_only\" is\n-    False) of the given dict converted to strings. ``dct_or_tuples`` can be a\n-    dict or a list of tuples, like any dict ``__init__`` method supports.\n-    \"\"\"\n-    d = {}\n-    for k, v in dict(dct_or_tuples).items():\n-        k = k.encode(encoding) if isinstance(k, str) else k\n-        if not keys_only:\n-            v = v.encode(encoding) if isinstance(v, str) else v\n-        d[k] = v\n-    return d\n-\n-\n-@deprecated\n-def is_writable(path):\n-    \"\"\"Return True if the given path can be written (if it exists) or created\n-    (if it doesn't exist)\n-    \"\"\"\n-    if os.path.exists(path):\n-        return os.access(path, os.W_OK)\n-    else:\n-        return os.access(os.path.dirname(path), os.W_OK)\n-\n-\n-@deprecated\n-def setattr_default(obj, name, value):\n-    \"\"\"Set attribute value, but only if it's not already set. Similar to\n-    setdefault() for dicts.\n-    \"\"\"\n-    if not hasattr(obj, name):\n-        setattr(obj, name, value)\n-\n-\n def retry_on_eintr(function, *args, **kw):\n     \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n     while True:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b49ece0b8781c1d53cdec77b96445826a089afc1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 1049 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 42 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -101,8 +101,10 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n     lines = StringIO(_body_or_str(obj, unicode=True))\n \n     kwargs = {}\n-    if delimiter: kwargs[\"delimiter\"] = delimiter\n-    if quotechar: kwargs[\"quotechar\"] = quotechar\n+    if delimiter:\n+        kwargs[\"delimiter\"] = delimiter\n+    if quotechar:\n+        kwargs[\"quotechar\"] = quotechar\n     csv_r = csv.reader(lines, **kwargs)\n \n     if not headers:\n\n@@ -149,13 +149,15 @@ class ItemTest(unittest.TestCase):\n             fields = {'load': Field(default='A')}\n             save = Field(default='A')\n \n-        class B(A): pass\n+        class B(A):\n+            pass\n \n         class C(Item):\n             fields = {'load': Field(default='C')}\n             save = Field(default='C')\n \n-        class D(B, C): pass\n+        class D(B, C):\n+            pass\n \n         item = D(save='X', load='Y')\n         self.assertEqual(item['save'], 'X')\n@@ -164,7 +166,8 @@ class ItemTest(unittest.TestCase):\n             'save': {'default': 'A'}})\n \n         # D class inverted\n-        class E(C, B): pass\n+        class E(C, B):\n+            pass\n \n         self.assertEqual(E(save='X')['save'], 'X')\n         self.assertEqual(E(load='X')['load'], 'X')\n@@ -177,7 +180,8 @@ class ItemTest(unittest.TestCase):\n             save = Field(default='A')\n             load = Field(default='A')\n \n-        class B(A): pass\n+        class B(A):\n+            pass\n \n         class C(A):\n             fields = {'update': Field(default='C')}\n@@ -206,14 +210,16 @@ class ItemTest(unittest.TestCase):\n             fields = {'load': Field(default='A')}\n             save = Field(default='A')\n \n-        class B(A): pass\n+        class B(A):\n+            pass\n \n         class C(object):\n             fields = {'load': Field(default='C')}\n             not_allowed = Field(default='not_allowed')\n             save = Field(default='C')\n \n-        class D(B, C): pass\n+        class D(B, C):\n+            pass\n \n         self.assertRaises(KeyError, D, not_allowed='value')\n         self.assertEqual(D(save='X')['save'], 'X')\n@@ -221,7 +227,8 @@ class ItemTest(unittest.TestCase):\n             'load': {'default': 'A'}})\n \n         # D class inverted\n-        class E(C, B): pass\n+        class E(C, B):\n+            pass\n \n         self.assertRaises(KeyError, E, not_allowed='value')\n         self.assertEqual(E(save='X')['save'], 'X')\n\n@@ -31,7 +31,9 @@ def nonserializable_object_test(self):\n         self.assertRaises(ValueError, q.push, lambda x: x)\n     else:\n         # Use a different unpickleable object\n-        class A(object): pass\n+        class A(object):\n+            pass\n+\n         a = A()\n         a.__reduce__ = a.__reduce_ex__ = None\n         self.assertRaises(ValueError, q.push, a)\n\n@@ -153,7 +153,9 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.assertFalse(equal_attributes(a, b, [compare_z, 'x']))\n \n     def test_weakkeycache(self):\n-        class _Weakme(object): pass\n+        class _Weakme(object):\n+            pass\n+\n         _values = count()\n         wk = WeakKeyCache(lambda k: next(_values))\n         k = _Weakme()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9ad10bb6f727a3f1c5c59d490f444ebb32de97c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 262 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -26,10 +26,9 @@ class MediaDownloadSpider(SimpleSpider):\n             self.media_key: [],\n             self.media_urls_key: [\n                 self._process_url(response.urljoin(href))\n-                    for href in response.xpath('''\n-                        //table[thead/tr/th=\"Filename\"]\n-                            /tbody//a/@href\n-                        ''').getall()],\n+                for href in response.xpath(\n+                    '//table[thead/tr/th=\"Filename\"]/tbody//a/@href'\n+                ).getall()],\n         }\n         yield item\n \n@@ -100,7 +99,8 @@ class FileDownloadCrawlTestCase(TestCase):\n             checksums = set(\n                 i['checksum']\n                 for item in items\n-                        for i in item[self.media_key])\n+                for i in item[self.media_key]\n+            )\n             self.assertEqual(checksums, self.expected_checksums)\n \n         # check that the image files where actually written to the media store\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#67ee0b097fe15aefa787bce64f6fa085d38e69d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 214 | Lines Deleted: 124 | Files Changed: 4 | Hunks: 34 | Methods Changed: 38 | Complexity Δ (Sum/Max): 14/11 | Churn Δ: 338 | Churn Cumulative: 1796 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 29 | DMM Complexity: 0.8035714285714286\n\nDIFF:\n@@ -119,7 +119,7 @@ class Scheduler(object):\n         if self.dqs is None:\n             return\n         try:\n-            self.dqs.push(request, -request.priority)\n+            self.dqs.push(request)\n         except ValueError as e:  # non serializable request\n             if self.logunser:\n                 msg = (\"Unable to serialize request: %(request)s - reason:\"\n@@ -135,35 +135,29 @@ class Scheduler(object):\n             return True\n \n     def _mqpush(self, request):\n-        self.mqs.push(request, -request.priority)\n+        self.mqs.push(request)\n \n     def _dqpop(self):\n         if self.dqs:\n             return self.dqs.pop()\n \n-    def _newmq(self, priority):\n-        \"\"\" Factory for creating memory queues. \"\"\"\n-        return self.mqclass()\n-\n-    def _newdq(self, priority):\n-        \"\"\" Factory for creating disk queues. \"\"\"\n-        path = join(self.dqdir, 'p%s' % (priority, ))\n-        return self.dqclass(path)\n-\n     def _mq(self):\n         \"\"\" Create a new priority queue instance, with in-memory storage \"\"\"\n-        return create_instance(self.pqclass, None, self.crawler, self._newmq,\n-                               serialize=False)\n+        return create_instance(self.pqclass,\n+                               settings=None,\n+                               crawler=self.crawler,\n+                               downstream_queue_cls=self.mqclass,\n+                               key='')\n \n     def _dq(self):\n         \"\"\" Create a new priority queue instance, with disk storage \"\"\"\n         state = self._read_dqs_state(self.dqdir)\n         q = create_instance(self.pqclass,\n-                            None,\n-                            self.crawler,\n-                            self._newdq,\n-                            state,\n-                            serialize=True)\n+                            settings=None,\n+                            crawler=self.crawler,\n+                            downstream_queue_cls=self.dqclass,\n+                            key=self.dqdir,\n+                            startprios=state)\n         if q:\n             logger.info(\"Resuming crawl (%(queuesize)d requests scheduled)\",\n                         {'queuesize': len(q)}, extra={'spider': self.spider})\n\n@@ -1,11 +1,7 @@\n import hashlib\n import logging\n-from collections import namedtuple\n-\n-from queuelib import PriorityQueue\n-\n-from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n+from scrapy.utils.misc import create_instance\n \n logger = logging.getLogger(__name__)\n \n@@ -29,88 +25,89 @@ def _path_safe(text):\n     return '-'.join([pathable_slot, unique_slot])\n \n \n-class _Priority(namedtuple(\"_Priority\", [\"priority\", \"slot\"])):\n-    \"\"\" Slot-specific priority. It is a hack - ``(priority, slot)`` tuple\n-    which can be used instead of int priorities in queues:\n+class ScrapyPriorityQueue:\n+    \"\"\"A priority queue implemented using multiple internal queues (typically,\n+    FIFO queues). It uses one internal queue for each priority value. The internal\n+    queue must implement the following methods:\n+\n+        * push(obj)\n+        * pop()\n+        * close()\n+        * __len__()\n+\n+    ``__init__`` method of ScrapyPriorityQueue receives a downstream_queue_cls\n+    argument, which is a class used to instantiate a new (internal) queue when\n+    a new priority is allocated.\n+\n+    Only integer priorities should be used. Lower numbers are higher\n+    priorities.\n+\n+    startprios is a sequence of priorities to start with. If the queue was\n+    previously closed leaving some priority buckets non-empty, those priorities\n+    should be passed in startprios.\n \n-    * they are ordered in the same way - order is still by priority value,\n-      min(prios) works;\n-    * str(p) representation is guaranteed to be different when slots\n-      are different - this is important because str(p) is used to create\n-      queue files on disk;\n-    * they have readable str(p) representation which is safe\n-      to use as a file name.\n     \"\"\"\n-    __slots__ = ()\n \n-    def __str__(self):\n-        return '%s_%s' % (self.priority, _path_safe(str(self.slot)))\n+    @classmethod\n+    def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n+        return cls(crawler, downstream_queue_cls, key, startprios)\n \n+    def __init__(self, crawler, downstream_queue_cls, key, startprios=()):\n+        self.crawler = crawler\n+        self.downstream_queue_cls = downstream_queue_cls\n+        self.key = key\n+        self.queues = {}\n+        self.curprio = None\n+        self.init_prios(startprios)\n \n-class _SlotPriorityQueues(object):\n-    \"\"\" Container for multiple priority queues. \"\"\"\n-    def __init__(self, pqfactory, slot_startprios=None):\n-        \"\"\"\n-        ``pqfactory`` is a factory for creating new PriorityQueues.\n-        It must be a function which accepts a single optional ``startprios``\n-        argument, with a list of priorities to create queues for.\n+    def init_prios(self, startprios):\n+        if not startprios:\n+            return\n \n-        ``slot_startprios`` is a ``{slot: startprios}`` dict.\n-        \"\"\"\n-        self.pqfactory = pqfactory\n-        self.pqueues = {}  # slot -> priority queue\n-        for slot, startprios in (slot_startprios or {}).items():\n-            self.pqueues[slot] = self.pqfactory(startprios)\n+        for priority in startprios:\n+            self.queues[priority] = self.qfactory(priority)\n \n-    def pop_slot(self, slot):\n-        \"\"\" Pop an object from a priority queue for this slot \"\"\"\n-        queue = self.pqueues[slot]\n-        request = queue.pop()\n-        if len(queue) == 0:\n-            del self.pqueues[slot]\n-        return request\n+        self.curprio = min(startprios)\n \n-    def push_slot(self, slot, obj, priority):\n-        \"\"\" Push an object to a priority queue for this slot \"\"\"\n-        if slot not in self.pqueues:\n-            self.pqueues[slot] = self.pqfactory()\n-        queue = self.pqueues[slot]\n-        queue.push(obj, priority)\n+    def qfactory(self, key):\n+        return create_instance(self.downstream_queue_cls,\n+                               None,\n+                               self.crawler,\n+                               self.key + '/' + str(key))\n+\n+    def priority(self, request):\n+        return -request.priority\n+\n+    def push(self, request):\n+        priority = self.priority(request)\n+        if priority not in self.queues:\n+            self.queues[priority] = self.qfactory(priority)\n+        q = self.queues[priority]\n+        q.push(request)  # this may fail (eg. serialization error)\n+        if self.curprio is None or priority < self.curprio:\n+            self.curprio = priority\n+\n+    def pop(self):\n+        if self.curprio is None:\n+            return\n+        q = self.queues[self.curprio]\n+        m = q.pop()\n+        if not q:\n+            del self.queues[self.curprio]\n+            q.close()\n+            prios = [p for p, q in self.queues.items() if q]\n+            self.curprio = min(prios) if prios else None\n+        return m\n \n     def close(self):\n-        active = {slot: queue.close()\n-                  for slot, queue in self.pqueues.items()}\n-        self.pqueues.clear()\n+        active = []\n+        for p, q in self.queues.items():\n+            active.append(p)\n+            q.close()\n         return active\n \n     def __len__(self):\n-        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n-\n-\n-class ScrapyPriorityQueue(PriorityQueue):\n-    \"\"\"\n-    PriorityQueue which works with scrapy.Request instances and\n-    can optionally convert them to/from dicts before/after putting to a queue.\n-    \"\"\"\n-    def __init__(self, crawler, qfactory, startprios=(), serialize=False):\n-        super(ScrapyPriorityQueue, self).__init__(qfactory, startprios)\n-        self.serialize = serialize\n-        self.spider = crawler.spider\n-\n-    @classmethod\n-    def from_crawler(cls, crawler, qfactory, startprios=(), serialize=False):\n-        return cls(crawler, qfactory, startprios, serialize)\n-\n-    def push(self, request, priority=0):\n-        if self.serialize:\n-            request = request_to_dict(request, self.spider)\n-        super(ScrapyPriorityQueue, self).push(request, priority)\n-\n-    def pop(self):\n-        request = super(ScrapyPriorityQueue, self).pop()\n-        if request and self.serialize:\n-            request = request_from_dict(request, self.spider)\n-        return request\n+        return sum(len(x) for x in self.queues.values()) if self.queues else 0\n \n \n class DownloaderInterface(object):\n@@ -133,16 +130,16 @@ class DownloaderInterface(object):\n \n \n class DownloaderAwarePriorityQueue(object):\n-    \"\"\" PriorityQueue which takes Downlaoder activity in account:\n+    \"\"\" PriorityQueue which takes Downloader activity in account:\n     domains (slots) with the least amount of active downloads are dequeued\n     first.\n     \"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler, qfactory, slot_startprios=None, serialize=False):\n-        return cls(crawler, qfactory, slot_startprios, serialize)\n+    def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n+        return cls(crawler, downstream_queue_cls, key, startprios)\n \n-    def __init__(self, crawler, qfactory, slot_startprios=None, serialize=False):\n+    def __init__(self, crawler, downstream_queue_cls, key, slot_startprios=()):\n         if crawler.settings.getint('CONCURRENT_REQUESTS_PER_IP') != 0:\n             raise ValueError('\"%s\" does not support CONCURRENT_REQUESTS_PER_IP'\n                              % (self.__class__,))\n@@ -156,35 +153,49 @@ class DownloaderAwarePriorityQueue(object):\n                              \"queue class can be resumed.\" %\n                              slot_startprios.__class__)\n \n-        slot_startprios = {\n-            slot: [_Priority(p, slot) for p in startprios]\n-            for slot, startprios in (slot_startprios or {}).items()}\n-\n-        def pqfactory(startprios=()):\n-            return ScrapyPriorityQueue(crawler, qfactory, startprios, serialize)\n-        self._slot_pqueues = _SlotPriorityQueues(pqfactory, slot_startprios)\n-        self.serialize = serialize\n         self._downloader_interface = DownloaderInterface(crawler)\n+        self.downstream_queue_cls = downstream_queue_cls\n+        self.key = key\n+        self.crawler = crawler\n+\n+        self.pqueues = {}  # slot -> priority queue\n+        for slot, startprios in (slot_startprios or {}).items():\n+            self.pqueues[slot] = self.pqfactory(slot, startprios)\n+\n+    def pqfactory(self, slot, startprios=()):\n+        return ScrapyPriorityQueue(self.crawler,\n+                                   self.downstream_queue_cls,\n+                                   self.key + '/' + _path_safe(slot),\n+                                   startprios)\n \n     def pop(self):\n-        stats = self._downloader_interface.stats(self._slot_pqueues.pqueues)\n+        stats = self._downloader_interface.stats(self.pqueues)\n \n         if not stats:\n             return\n \n         slot = min(stats)[1]\n-        request = self._slot_pqueues.pop_slot(slot)\n+        queue = self.pqueues[slot]\n+        request = queue.pop()\n+        if len(queue) == 0:\n+            del self.pqueues[slot]\n         return request\n \n-    def push(self, request, priority):\n+    def push(self, request):\n         slot = self._downloader_interface.get_slot_key(request)\n-        priority_slot = _Priority(priority=priority, slot=slot)\n-        self._slot_pqueues.push_slot(slot, request, priority_slot)\n+        if slot not in self.pqueues:\n+            self.pqueues[slot] = self.pqfactory(slot)\n+        queue = self.pqueues[slot]\n+        queue.push(request)\n \n     def close(self):\n-        active = self._slot_pqueues.close()\n-        return {slot: [p.priority for p in startprios]\n-                for slot, startprios in active.items()}\n+        active = {slot: queue.close()\n+                  for slot, queue in self.pqueues.items()}\n+        self.pqueues.clear()\n+        return active\n \n     def __len__(self):\n-        return len(self._slot_pqueues)\n+        return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n+\n+    def __contains__(self, slot):\n+        return slot in self.pqueues\n\n@@ -3,10 +3,27 @@ Scheduler queues\n \"\"\"\n \n import marshal\n+import os\n import pickle\n \n from queuelib import queue\n \n+from scrapy.utils.reqser import request_to_dict, request_from_dict\n+\n+\n+def _with_mkdir(queue_class):\n+\n+    class DirectoriesCreated(queue_class):\n+\n+        def __init__(self, path, *args, **kwargs):\n+            dirname = os.path.dirname(path)\n+            if not os.path.exists(dirname):\n+                os.makedirs(dirname, exist_ok=True)\n+\n+            super(DirectoriesCreated, self).__init__(path, *args, **kwargs)\n+\n+    return DirectoriesCreated\n+\n \n def _serializable_queue(queue_class, serialize, deserialize):\n \n@@ -24,6 +41,44 @@ def _serializable_queue(queue_class, serialize, deserialize):\n     return SerializableQueue\n \n \n+def _scrapy_serialization_queue(queue_class):\n+\n+    class ScrapyRequestQueue(queue_class):\n+\n+        def __init__(self, crawler, key):\n+            self.spider = crawler.spider\n+            super(ScrapyRequestQueue, self).__init__(key)\n+\n+        @classmethod\n+        def from_crawler(cls, crawler, key, *args, **kwargs):\n+            return cls(crawler, key)\n+\n+        def push(self, request):\n+            request = request_to_dict(request, self.spider)\n+            return super(ScrapyRequestQueue, self).push(request)\n+\n+        def pop(self):\n+            request = super(ScrapyRequestQueue, self).pop()\n+\n+            if not request:\n+                return None\n+\n+            request = request_from_dict(request, self.spider)\n+            return request\n+\n+    return ScrapyRequestQueue\n+\n+\n+def _scrapy_non_serialization_queue(queue_class):\n+\n+    class ScrapyRequestQueue(queue_class):\n+        @classmethod\n+        def from_crawler(cls, crawler, *args, **kwargs):\n+            return cls()\n+\n+    return ScrapyRequestQueue\n+\n+\n def _pickle_serialize(obj):\n     try:\n         return pickle.dumps(obj, protocol=2)\n@@ -34,13 +89,38 @@ def _pickle_serialize(obj):\n         raise ValueError(str(e))\n \n \n-PickleFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue,\n-    _pickle_serialize, pickle.loads)\n-PickleLifoDiskQueue = _serializable_queue(queue.LifoDiskQueue,\n-    _pickle_serialize, pickle.loads)\n-MarshalFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue,\n-    marshal.dumps, marshal.loads)\n-MarshalLifoDiskQueue = _serializable_queue(queue.LifoDiskQueue,\n-    marshal.dumps, marshal.loads)\n-FifoMemoryQueue = queue.FifoMemoryQueue\n-LifoMemoryQueue = queue.LifoMemoryQueue\n+PickleFifoDiskQueueNonRequest = _serializable_queue(\n+    _with_mkdir(queue.FifoDiskQueue),\n+    _pickle_serialize,\n+    pickle.loads\n+)\n+PickleLifoDiskQueueNonRequest = _serializable_queue(\n+    _with_mkdir(queue.LifoDiskQueue),\n+    _pickle_serialize,\n+    pickle.loads\n+)\n+MarshalFifoDiskQueueNonRequest = _serializable_queue(\n+    _with_mkdir(queue.FifoDiskQueue),\n+    marshal.dumps,\n+    marshal.loads\n+)\n+MarshalLifoDiskQueueNonRequest = _serializable_queue(\n+    _with_mkdir(queue.LifoDiskQueue),\n+    marshal.dumps,\n+    marshal.loads\n+)\n+\n+PickleFifoDiskQueue = _scrapy_serialization_queue(\n+    PickleFifoDiskQueueNonRequest\n+)\n+PickleLifoDiskQueue = _scrapy_serialization_queue(\n+    PickleLifoDiskQueueNonRequest\n+)\n+MarshalFifoDiskQueue = _scrapy_serialization_queue(\n+    MarshalFifoDiskQueueNonRequest\n+)\n+MarshalLifoDiskQueue = _scrapy_serialization_queue(\n+    MarshalLifoDiskQueueNonRequest\n+)\n+FifoMemoryQueue = _scrapy_non_serialization_queue(queue.FifoMemoryQueue)\n+LifoMemoryQueue = _scrapy_non_serialization_queue(queue.LifoMemoryQueue)\n\n@@ -1,7 +1,12 @@\n import pickle\n \n from queuelib.tests import test_queue as t\n-from scrapy.squeues import MarshalFifoDiskQueue, MarshalLifoDiskQueue, PickleFifoDiskQueue, PickleLifoDiskQueue\n+from scrapy.squeues import (\n+    MarshalFifoDiskQueueNonRequest as MarshalFifoDiskQueue,\n+    MarshalLifoDiskQueueNonRequest as MarshalLifoDiskQueue,\n+    PickleFifoDiskQueueNonRequest as PickleFifoDiskQueue,\n+    PickleLifoDiskQueueNonRequest as PickleLifoDiskQueue\n+)\n from scrapy.item import Item, Field\n from scrapy.http import Request\n from scrapy.loader import ItemLoader\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d983c1b9962a018686111e20e42d25bbffb579e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 8/3 | Churn Δ: 56 | Churn Cumulative: 3092 | Contributors (this commit): 29 | Commits (past 90d): 22 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,11 +3,12 @@\n import logging\n import re\n import warnings\n+from contextlib import suppress\n from io import BytesIO\n from time import time\n from urllib.parse import urldefrag\n \n-from twisted.internet import defer, protocol, reactor\n+from twisted.internet import defer, protocol, reactor, ssl\n from twisted.internet.endpoints import TCP4ClientEndpoint\n from twisted.internet.error import TimeoutError\n from twisted.web.client import Agent, HTTPConnectionPool, ResponseDone, ResponseFailed, URI\n@@ -382,7 +383,7 @@ class ScrapyAgent(object):\n     def _cb_bodyready(self, txresponse, request):\n         # deliverBody hangs for responses without body\n         if txresponse.length == 0:\n-            return txresponse, b'', None\n+            return txresponse, b'', None, None\n \n         maxsize = request.meta.get('download_maxsize', self._maxsize)\n         warnsize = request.meta.get('download_warnsize', self._warnsize)\n@@ -418,11 +419,12 @@ class ScrapyAgent(object):\n         return d\n \n     def _cb_bodydone(self, result, request, url):\n-        txresponse, body, flags = result\n+        txresponse, body, flags, certificate = result\n         status = int(txresponse.code)\n         headers = Headers(txresponse.headers.getAllRawHeaders())\n         respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n-        return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n+        return respcls(url=url, status=status, headers=headers, body=body,\n+                       flags=flags, certificate=certificate)\n \n \n @implementer(IBodyProducer)\n@@ -456,6 +458,12 @@ class _ResponseReader(protocol.Protocol):\n         self._fail_on_dataloss_warned = False\n         self._reached_warnsize = False\n         self._bytes_received = 0\n+        self._certificate = None\n+\n+    def connectionMade(self):\n+        if self._certificate is None:\n+            with suppress(AttributeError):\n+                self._certificate = ssl.Certificate(self.transport._producer.getPeerCertificate())\n \n     def dataReceived(self, bodyBytes):\n         # This maybe called several times after cancel was called with buffered data.\n@@ -488,16 +496,16 @@ class _ResponseReader(protocol.Protocol):\n \n         body = self._bodybuf.getvalue()\n         if reason.check(ResponseDone):\n-            self._finished.callback((self._txresponse, body, None))\n+            self._finished.callback((self._txresponse, body, None, self._certificate))\n             return\n \n         if reason.check(PotentialDataLoss):\n-            self._finished.callback((self._txresponse, body, ['partial']))\n+            self._finished.callback((self._txresponse, body, ['partial'], self._certificate))\n             return\n \n         if reason.check(ResponseFailed) and any(r.check(_DataLoss) for r in reason.value.reasons):\n             if not self._fail_on_dataloss:\n-                self._finished.callback((self._txresponse, body, ['dataloss']))\n+                self._finished.callback((self._txresponse, body, ['dataloss'], self._certificate))\n                 return\n \n             elif not self._fail_on_dataloss_warned:\n\n@@ -17,13 +17,14 @@ from scrapy.utils.trackref import object_ref\n \n class Response(object_ref):\n \n-    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None):\n+    def __init__(self, url, status=200, headers=None, body=b'', flags=None, request=None, certificate=None):\n         self.headers = Headers(headers or {})\n         self.status = int(status)\n         self._set_body(body)\n         self._set_url(url)\n         self.request = request\n         self.flags = [] if flags is None else list(flags)\n+        self.certificate = certificate\n \n     @property\n     def cb_kwargs(self):\n@@ -86,7 +87,7 @@ class Response(object_ref):\n         \"\"\"Create a new Response with the same attributes except for those\n         given new values.\n         \"\"\"\n-        for x in ['url', 'status', 'headers', 'body', 'request', 'flags']:\n+        for x in ['url', 'status', 'headers', 'body', 'request', 'flags', 'certificate']:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop('cls', self.__class__)\n         return cls(*args, **kwargs)\n\n@@ -5,6 +5,7 @@ import sys\n from pytest import mark\n from testfixtures import LogCapture\n from twisted.internet import defer\n+from twisted.internet.ssl import Certificate\n from twisted.trial.unittest import TestCase\n \n from scrapy import signals\n@@ -407,3 +408,31 @@ with multiples lines\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         for req_id in range(3):\n             self.assertIn(\"Got response 200, req_id %d\" % req_id, str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_response_ssl_certificate_none(self):\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        self.assertIsNone(crawler.spider.meta['responses'][0].certificate)\n+\n+    @defer.inlineCallbacks\n+    def test_response_ssl_certificate(self):\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        cert = crawler.spider.meta['responses'][0].certificate\n+        self.assertIsInstance(cert, Certificate)\n+        self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n+        self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n+\n+    @mark.xfail(reason=\"Responses with no body return early and contain no certificate\")\n+    @defer.inlineCallbacks\n+    def test_response_ssl_certificate_empty_response(self):\n+        crawler = self.runner.create_crawler(SingleRequestSpider)\n+        url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n+        yield crawler.crawl(seed=url, mockserver=self.mockserver)\n+        cert = crawler.spider.meta['responses'][0].certificate\n+        self.assertIsInstance(cert, Certificate)\n+        self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n+        self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
