{"custom_id": "keras#1e73e1bc54a051db4a8215243a6caae61584b9b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 726 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -164,10 +164,14 @@ class History(Callback):\n     def on_epoch_end(self, epoch, logs={}):\n         self.epoch.append(epoch)\n         for k, v in self.totals.items():\n-            self.history[k] = v / self.seen\n+            if k not in self.history:\n+                self.history[k] = []\n+            self.history[k].append(v / self.seen)\n \n         for k, v in logs.items():\n-            self.history[k] = v\n+            if k not in self.history:\n+                self.history[k] = []\n+            self.history[k].append(v)\n \n \n class ModelCheckpoint(Callback):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc05a25b1b1b44f153eb47b0dbe5d213482d4dfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 17 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 442 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,7 +111,7 @@ class TestGraph(unittest.TestCase):\n         loss = graph.train_on_batch({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n         loss = graph.evaluate({'input1':X_test, 'output1':y_test, 'output2':y2_test})\n         print(loss)\n-        assert(loss < 2.5)\n+        assert(loss < 2.7)\n \n         print('test weight saving')\n         graph.save_weights('temp.h5', overwrite=True)\n\n@@ -40,7 +40,7 @@ def create_model():\n     model.add(Activation('softmax'))\n     return model\n \n-def test_weights(model, class_weight=None, sample_weight=None):\n+def _test_weights(model, class_weight=None, sample_weight=None):\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, \\\n         class_weight=class_weight, sample_weight=sample_weight)\n     score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n@@ -60,17 +60,17 @@ class TestConcatenation(unittest.TestCase):\n             # no weights: reference point\n             model = create_model()\n             model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-            standard_score = test_weights(model)\n+            standard_score = _test_weights(model)\n             # test class_weight\n             model = create_model()\n             model.compile(loss=loss, optimizer='rmsprop')\n-            score = test_weights(model, class_weight=class_weight)\n+            score = _test_weights(model, class_weight=class_weight)\n             print('score:', score, ' vs.', standard_score)\n             self.assertTrue(score < standard_score)\n             # test sample_weight\n             model = create_model()\n             model.compile(loss=loss, optimizer='rmsprop')\n-            score = test_weights(model, sample_weight=sample_weight)\n+            score = _test_weights(model, sample_weight=sample_weight)\n             print('score:', score, ' vs.', standard_score)\n             self.assertTrue(score < standard_score)\n \n\n@@ -31,7 +31,8 @@ class TestRegularizers(unittest.TestCase):\n         model.add(Activation('softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-        self.assertTrue(history.validation_accuracy[-1] > 0.9)\n+        print(history.history)\n+        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n \n     def test_vector_reg(self):\n         nb_hidden = 10\n@@ -45,11 +46,11 @@ class TestRegularizers(unittest.TestCase):\n \n         model = Sequential()\n         model.add(Dense(X_train.shape[-1], nb_hidden))\n-        model.add(Activation('relu'))\n+        model.add(Activation('tanh'))\n         model.add(Dense(nb_hidden, y_train.shape[-1]))\n-        model.compile(loss='hinge', optimizer='rmsprop')\n+        model.compile(loss='hinge', optimizer='adagrad')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n-        self.assertTrue(history.validation_loss[-1] < 0.75)\n+        self.assertTrue(history.history['val_loss'][-1] < 0.9)\n \n     def test_temporal_clf(self):\n         print('temporal classification data:')\n@@ -66,9 +67,9 @@ class TestRegularizers(unittest.TestCase):\n         model = Sequential()\n         model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n         model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-        self.assertTrue(history.validation_accuracy[-1] > 0.9)\n+        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n \n     def test_temporal_reg(self):\n         print('temporal regression data:')\n@@ -81,9 +82,9 @@ class TestRegularizers(unittest.TestCase):\n \n         model = Sequential()\n         model.add(GRU(X_train.shape[-1], y_train.shape[-1]))\n-        model.compile(loss='hinge', optimizer='rmsprop')\n+        model.compile(loss='hinge', optimizer='adam')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n-        self.assertTrue(history.validation_loss[-1] < 0.75)\n+        self.assertTrue(history.history['val_loss'][-1] < 0.75)\n \n \n     def test_seq_to_seq(self):\n@@ -99,7 +100,7 @@ class TestRegularizers(unittest.TestCase):\n         model.add(TimeDistributedDense(X_train.shape[-1], y_train.shape[-1]))\n         model.compile(loss='hinge', optimizer='rmsprop')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n-        self.assertTrue(history.validation_loss[-1] < 0.75)\n+        self.assertTrue(history.history['val_loss'][-1] < 0.75)\n \n \n     def test_img_clf(self):\n@@ -116,13 +117,13 @@ class TestRegularizers(unittest.TestCase):\n \n         model = Sequential()\n         model.add(Convolution2D(32, 3, 32, 32))\n-        model.add(Activation('relu'))\n+        model.add(Activation('sigmoid'))\n         model.add(Flatten())\n         model.add(Dense(32, y_test.shape[-1]))\n         model.add(Activation('softmax'))\n-        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n         history = model.fit(X_train, y_train, nb_epoch=12, batch_size=16, validation_data=(X_test, y_test), show_accuracy=True, verbose=2)\n-        self.assertTrue(history.validation_accuracy[-1] > 0.9)\n+        self.assertTrue(history.history['val_acc'][-1] > 0.9)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1bfbb33c0daf24e5f8315f39b3c5605fb17f4228", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 50 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ class GaussianNoise(MaskedLayer):\n         return {\"name\":self.__class__.__name__,\n             \"sigma\":self.sigma}\n \n-class GaussianDropout(Layer):\n+class GaussianDropout(MaskedLayer):\n     '''\n         Multiplicative Gaussian Noise\n         Reference: \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0aedaf4e85fe8246e57cc1ab1d65faec8bd8d38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 52 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -38,7 +38,7 @@ class GaussianDropout(MaskedLayer):\n         X = self.get_input(train)\n         if train:\n             # self.p refers to drop probability rather than retain probability (as in paper) to match Dropout layer syntax\n-            X *= srng.normal(size=X.shape, avg=1.0, std=T.sqrt(self.p / (1.0 - self.p)), dtype=theano.config.floatX)\n+            X *= srng.normal(size=X.shape, avg=1.0, std=theano.tensor.sqrt(self.p / (1.0 - self.p)), dtype=theano.config.floatX)\n         return X\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cbe62c248f4616c133185e02802327a54f2fc4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1955 | Contributors (this commit): 18 | Commits (past 90d): 82 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -268,6 +268,7 @@ class Sequential(Model, containers.Sequential):\n         else:\n             raise Exception(\"Invalid class mode:\" + str(class_mode))\n         self.class_mode = class_mode\n+        self.theano_mode = theano_mode\n \n         for r in self.regularizers:\n             train_loss = r(train_loss)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4c62ffbab4a2e1ac5b65e5a1de0e70bac9697cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 48 | Churn Cumulative: 942 | Contributors (this commit): 7 | Commits (past 90d): 39 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,9 @@ class Constraint(object):\n     def __call__(self, p):\n         return p\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__}\n+\n class MaxNorm(Constraint):\n     def __init__(self, m=2):\n         self.m = m\n@@ -17,6 +20,10 @@ class MaxNorm(Constraint):\n         p = p * (desired / (1e-7 + norms))\n         return p\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"m\":self.m}\n+\n class NonNeg(Constraint):\n     def __call__(self, p):\n         p *= T.ge(p, 0)\n\n@@ -29,6 +29,8 @@ class Optimizer(object):\n \n         return grads\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__}\n \n class SGD(Optimizer):\n \n@@ -55,6 +57,13 @@ class SGD(Optimizer):\n             updates.append((p, c(new_p))) # apply constraints\n         return updates\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"lr\":self.lr,\n+            \"momentum\":self.momentum,\n+            \"decay\":self.decay,\n+            \"nesterov\":self.nesterov}\n+\n \n class RMSprop(Optimizer):\n \n@@ -76,6 +85,11 @@ class RMSprop(Optimizer):\n             \n         return updates\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"lr\":self.lr,\n+            \"rho\":self.rho,\n+            \"epsilon\":self.epsilon}\n \n class Adagrad(Optimizer):\n \n@@ -96,6 +110,10 @@ class Adagrad(Optimizer):\n             updates.append((p, c(new_p))) # apply constraints\n         return updates\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"lr\":self.lr,\n+            \"epsilon\":self.epsilon}\n \n class Adadelta(Optimizer):\n     '''\n@@ -126,6 +144,11 @@ class Adadelta(Optimizer):\n             updates.append((d_a, new_d_a))\n         return updates\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"lr\":self.lr,\n+            \"rho\":self.rho,\n+            \"epsilon\":self.epsilon}\n \n class Adam(Optimizer):\n     '''\n@@ -167,6 +190,14 @@ class Adam(Optimizer):\n             updates.append((p, c(p_t))) # apply constraints\n         return updates\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"lr\":self.lr,\n+            \"beta_1\":self.beta_1,\n+            \"beta_2\":self.beta_2,\n+            \"epsilon\":self.epsilon,\n+            \"kappa\":self.kappa}\n+\n # aliases\n sgd = SGD\n rmsprop = RMSprop\n\n@@ -11,6 +11,8 @@ class Regularizer(object):\n     def __call__(self, loss):\n         return loss\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__}\n \n class WeightRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n@@ -25,6 +27,10 @@ class WeightRegularizer(Regularizer):\n         loss += T.sum(self.p ** 2) * self.l2\n         return loss\n \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"l1\":self.l1,\n+            \"l2\":self.l2}\n \n class ActivityRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n@@ -39,6 +45,10 @@ class ActivityRegularizer(Regularizer):\n         loss += self.l2 * T.sum(T.mean(self.layer.get_output(True) ** 2, axis=0))\n         return loss\n     \n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"l1\":self.l1,\n+            \"l2\":self.l2}\n \n def l1(l=0.01):\n     return WeightRegularizer(l1=l)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a649d2b27c11323183da27e099bb032567624eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 55 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n from .core import srng, MaskedLayer\n import theano\n+import theano.tensor as T\n \n class GaussianNoise(MaskedLayer):\n     '''\n@@ -38,7 +39,7 @@ class GaussianDropout(MaskedLayer):\n         X = self.get_input(train)\n         if train:\n             # self.p refers to drop probability rather than retain probability (as in paper) to match Dropout layer syntax\n-            X *= srng.normal(size=X.shape, avg=1.0, std=theano.tensor.sqrt(self.p / (1.0 - self.p)), dtype=theano.config.floatX)\n+            X *= srng.normal(size=X.shape, avg=1.0, std=T.sqrt(self.p / (1.0 - self.p)), dtype=theano.config.floatX)\n         return X\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5bf271858092e85164ecc309cd943d7eabdab9db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 218 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ def categorical_crossentropy(y_true, y_pred):\n \n def binary_crossentropy(y_true, y_pred):\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n-    bce = T.nnet.binary_crossentropy(y_pred, y_true)\n+    bce = T.nnet.binary_crossentropy(y_pred, y_true).mean(axis=-1)\n     return bce\n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46f591d4bbb25eb3f6e52a44a19db6536417c6f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 65 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,3 +37,7 @@ identity = Constraint\n maxnorm = MaxNorm\n nonneg = NonNeg\n unitnorm = UnitNorm\n+\n+from .utils.generic_utils import get_from_module\n+def get(identifier):\n+    return get_from_module(identifier, globals(), 'constraint', instantiate=True)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3909e783b9c1ec39a6b51a731762ab3405029ea3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1957 | Contributors (this commit): 18 | Commits (past 90d): 83 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -237,6 +237,8 @@ class Sequential(Model, containers.Sequential):\n \n     def compile(self, optimizer, loss, class_mode=\"categorical\", theano_mode=None):\n         self.optimizer = optimizers.get(optimizer)\n+\n+        self.unweighted_loss = objectives.get(loss)\n         self.loss = weighted_objective(objectives.get(loss))\n \n         # input of model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca22bb7db527bc72110c27cd5ccc2c3ac95ed598", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 413 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,3 +69,7 @@ def activity_l1l2(l1=0.01, l2=0.01):\n     return ActivityRegularizer(l1=l1, l2=l2)\n \n identity = Regularizer\n+\n+from .utils.generic_utils import get_from_module\n+def get(identifier):\n+    return get_from_module(identifier, globals(), 'regularizer', instantiate=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f142d34ffc43b6eb7f57b21d2fa742585e1dbe60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 80 | Churn Cumulative: 2037 | Contributors (this commit): 18 | Commits (past 90d): 84 | Contributors (cumulative): 18 | DMM Complexity: 0.29411764705882354\n\nDIFF:\n@@ -3,7 +3,19 @@ from __future__ import print_function\n import theano\n import theano.tensor as T\n import numpy as np\n-import warnings, time, copy\n+import warnings, time, copy, yaml\n+\n+from .layers.convolutional import *\n+from .layers.core import *\n+from .layers.embeddings import *\n+from .layers.noise import *\n+from .layers.normalization import *\n+from .layers.recurrent import *\n+\n+from .optimizers import *\n+from .objectives import *\n+from .regularizers import *\n+from .constraints import *\n \n from . import optimizers\n from . import objectives\n@@ -76,6 +88,50 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n     else:\n         return np.ones(y.shape[:-1] + (1,))\n \n+def sequential_from_yaml(pathToYaml):\n+    '''\n+        Returns a compiled Sequential model generated from a local yaml file,\n+        which is either created by hand or from Sequential.to_yaml\n+    '''\n+    stream = open(pathToYaml, 'r')\n+    modelYaml = yaml.load(stream)\n+    model = Sequential()\n+\n+    class_mode = modelYaml.get('class_mode')\n+    theano_mode = modelYaml.get('theano_mode')\n+    loss = globals()[modelYaml.get('loss')]\n+\n+    optim = modelYaml.get('optimizer')\n+    optimName = optim.get('name')\n+    optim.pop('name')\n+    optimizer = globals()[optimName](**optim)\n+\n+    layers = modelYaml.get('layers')\n+    for layer in layers:\n+        name = layer.get('name')\n+        layer.pop('name')\n+        hasParams = False\n+        if layer.has_key('parameters'):\n+            params = layer.get('parameters')\n+            layer.pop('parameters')\n+            hasParams = True\n+        for k, v in layer.iteritems():\n+            if isinstance(v, dict):\n+                vname = v.get('name')\n+                v.pop('name')\n+                layer[k] = globals()[vname](**v)\n+        initLayer = globals()[name](**layer)\n+        if hasParams:\n+            shapedParams = []\n+            for param in params:\n+                data = np.asarray(param.get('data'))\n+                shape = tuple(param.get('shape'))\n+                shapedParams.append(data.reshape(shape))\n+            initLayer.set_weights(shapedParams)\n+        model.add(initLayer)\n+\n+    model.compile(loss=loss, optimizer=optimizer, class_mode=class_mode, theano_mode=theano_mode)\n+    return model\n \n class Model(object):\n     def _fit(self, f, ins, out_labels=[], batch_size=128, nb_epoch=100, verbose=1, callbacks=[], \\\n@@ -467,6 +523,28 @@ class Sequential(Model, containers.Sequential):\n         f.close()\n \n \n+    def to_yaml(self, fileName, storeParams=True):\n+        '''\n+            Stores compiled Sequential model to local file, optionally storing all learnable parameters\n+        '''\n+        modelDict = {}\n+        modelDict['class_mode'] = self.class_mode\n+        modelDict['theano_mode'] = self.theano_mode\n+        modelDict['loss'] = self.unweighted_loss.__name__\n+        modelDict['optimizer'] = self.optimizer.get_config()\n+\n+        layers = []\n+        for layer in self.layers:\n+            layerConf = layer.get_config()\n+            if storeParams:\n+                layerConf['parameters'] = [{'shape':list(param.get_value().shape), 'data':param.get_value().tolist()} for param in layer.params]\n+            layers.append(layerConf)\n+        modelDict['layers'] = layers\n+\n+        with open(fileName, 'w') as outfile:\n+            outfile.write(yaml.dump(modelDict, default_flow_style=True))\n+\n+\n class Graph(Model, containers.Graph):\n     def compile(self, optimizer, loss, theano_mode=None):\n         # loss is a dictionary mapping output name to loss functions\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d77175fb2e99524a99ec8930679b1370f360471e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 58 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n from .core import srng, MaskedLayer\n import theano\n+import theano.tensor as T\n \n class GaussianNoise(MaskedLayer):\n     '''\n@@ -22,7 +23,7 @@ class GaussianNoise(MaskedLayer):\n         return {\"name\":self.__class__.__name__,\n             \"sigma\":self.sigma}\n \n-class GaussianDropout(Layer):\n+class GaussianDropout(MaskedLayer):\n     '''\n         Multiplicative Gaussian Noise\n         Reference: \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94d857f3cbbc668fbf9eb641895454612c5f124a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 131 | Lines Deleted: 75 | Files Changed: 3 | Hunks: 29 | Methods Changed: 14 | Complexity Δ (Sum/Max): -17/0 | Churn Δ: 206 | Churn Cumulative: 2017 | Contributors (this commit): 15 | Commits (past 90d): 97 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,16 +5,15 @@ import theano\n import theano.tensor as T\n from theano.tensor.signal import downsample\n \n-from .. import activations, initializations\n+from .. import activations, initializations, regularizers, constraints\n from ..utils.theano_utils import shared_zeros\n from ..layers.core import Layer\n \n-\n class Convolution1D(Layer):\n     def __init__(self, input_dim, nb_filter, filter_length,\n         init='uniform', activation='linear', weights=None,\n         border_mode='valid', subsample_length=1,\n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n \n         super(Convolution1D,self).__init__()\n \n@@ -35,17 +34,22 @@ class Convolution1D(Layer):\n         self.params = [self.W, self.b]\n \n         self.regularizers = []\n-        if W_regularizer:\n-            W_regularizer.set_param(self.W)\n-            self.regularizers.append(W_regularizer)\n-        if b_regularizer:\n-            b_regularizer.set_param(self.b)\n-            self.regularizers.append(b_regularizer)\n-        if activity_regularizer:\n-            activity_regularizer.set_layer(self)\n-            self.regularizers.append(activity_regularizer)\n \n-        self.constraints = [W_constraint, b_constraint]\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.W_regularizer.set_param(self.W)\n+        self.regularizers.append(self.W_regularizer)\n+\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.b_regularizer.set_param(self.b)\n+        self.regularizers.append(self.b_regularizer)\n+\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.activity_regularizer.set_layer(self)\n+        self.regularizers.append(self.activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -65,7 +69,12 @@ class Convolution1D(Layer):\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,\n             \"border_mode\":self.border_mode,\n-            \"subsample_length\":self.subsample_length}\n+            \"subsample_length\":self.subsample_length,\n+            \"W_regularizer\":self.W_regularizer.get_config(),\n+            \"b_regularizer\":self.b_regularizer.get_config(),\n+            \"activity_regularizer\":self.activity_regularizer.get_config(),\n+            \"W_constraint\":self.W_constraint.get_config(),\n+            \"b_constraint\":self.b_constraint.get_config()}\n \n \n class MaxPooling1D(Layer):\n@@ -102,7 +111,7 @@ class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col,\n         init='glorot_uniform', activation='linear', weights=None,\n         border_mode='valid', subsample=(1, 1),\n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n \n         super(Convolution2D,self).__init__()\n         self.init = initializations.get(init)\n@@ -123,17 +132,22 @@ class Convolution2D(Layer):\n         self.params = [self.W, self.b]\n \n         self.regularizers = []\n-        if W_regularizer:\n-            W_regularizer.set_param(self.W)\n-            self.regularizers.append(W_regularizer)\n-        if b_regularizer:\n-            b_regularizer.set_param(self.b)\n-            self.regularizers.append(b_regularizer)\n-        if activity_regularizer:\n-            activity_regularizer.set_layer(self)\n-            self.regularizers.append(activity_regularizer)\n \n-        self.constraints = [W_constraint, b_constraint]\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.W_regularizer.set_param(self.W)\n+        self.regularizers.append(self.W_regularizer)\n+\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.b_regularizer.set_param(self.b)\n+        self.regularizers.append(self.b_regularizer)\n+\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.activity_regularizer.set_layer(self)\n+        self.regularizers.append(self.activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -154,7 +168,12 @@ class Convolution2D(Layer):\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,\n             \"border_mode\":self.border_mode,\n-            \"subsample\":self.subsample}\n+            \"subsample\":self.subsample,\n+            \"W_regularizer\":self.W_regularizer.get_config(),\n+            \"b_regularizer\":self.b_regularizer.get_config(),\n+            \"activity_regularizer\":self.activity_regularizer.get_config(),\n+            \"W_constraint\":self.W_constraint.get_config(),\n+            \"b_constraint\":self.b_constraint.get_config()}\n \n \n class MaxPooling2D(Layer):\n\n@@ -5,11 +5,11 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n-from .. import activations, initializations\n+from .. import activations, initializations, regularizers, constraints\n from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n-from ..regularizers import ActivityRegularizer\n-from .. import constraints\n+from ..regularizers import ActivityRegularizer, Regularizer\n+\n \n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n@@ -293,7 +293,7 @@ class Dense(Layer):\n         Just your regular fully connected NN layer.\n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, name=None,\n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n \n         super(Dense, self).__init__()\n         self.init = initializations.get(init)\n@@ -308,17 +308,22 @@ class Dense(Layer):\n         self.params = [self.W, self.b]\n \n         self.regularizers = []\n-        if W_regularizer:\n-            W_regularizer.set_param(self.W)\n-            self.regularizers.append(W_regularizer)\n-        if b_regularizer:\n-            b_regularizer.set_param(self.b)\n-            self.regularizers.append(b_regularizer)\n-        if activity_regularizer:\n-            activity_regularizer.set_layer(self)\n-            self.regularizers.append(activity_regularizer)\n         \n-        self.constraints = [W_constraint, b_constraint]\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.W_regularizer.set_param(self.W)\n+        self.regularizers.append(self.W_regularizer)\n+\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.b_regularizer.set_param(self.b)\n+        self.regularizers.append(self.b_regularizer)\n+\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.activity_regularizer.set_layer(self)\n+        self.regularizers.append(self.activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -340,7 +345,12 @@ class Dense(Layer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"activation\":self.activation.__name__}\n+            \"activation\":self.activation.__name__,\n+            \"W_regularizer\":self.W_regularizer.get_config(),\n+            \"b_regularizer\":self.b_regularizer.get_config(),\n+            \"activity_regularizer\":self.activity_regularizer.get_config(),\n+            \"W_constraint\":self.W_constraint.get_config(),\n+            \"b_constraint\":self.b_constraint.get_config()}\n \n \n class ActivityRegularization(Layer):\n@@ -375,7 +385,7 @@ class TimeDistributedDense(MaskedLayer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n \n         super(TimeDistributedDense, self).__init__()\n         self.init = initializations.get(init)\n@@ -390,17 +400,22 @@ class TimeDistributedDense(MaskedLayer):\n         self.params = [self.W, self.b]\n \n         self.regularizers = []\n-        if W_regularizer:\n-            W_regularizer.set_param(self.W)\n-            self.regularizers.append(W_regularizer)\n-        if b_regularizer:\n-            b_regularizer.set_param(self.b)\n-            self.regularizers.append(b_regularizer)\n-        if activity_regularizer:\n-            activity_regularizer.set_layer(self)\n-            self.regularizers.append(activity_regularizer)\n \n-        self.constraints = [W_constraint, b_constraint]\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.W_regularizer.set_param(self.W)\n+        self.regularizers.append(self.W_regularizer)\n+\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.b_regularizer.set_param(self.b)\n+        self.regularizers.append(self.b_regularizer)\n+\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.activity_regularizer.set_layer(self)\n+        self.regularizers.append(self.activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -416,7 +431,12 @@ class TimeDistributedDense(MaskedLayer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"activation\":self.activation.__name__}\n+            \"activation\":self.activation.__name__,            \n+            \"W_regularizer\":self.W_regularizer.get_config(),\n+            \"b_regularizer\":self.b_regularizer.get_config(),\n+            \"activity_regularizer\":self.activity_regularizer.get_config(),\n+            \"W_constraint\":self.W_constraint.get_config(),\n+            \"b_constraint\":self.b_constraint.get_config()}\n \n \n class AutoEncoder(Layer):\n@@ -503,7 +523,7 @@ class MaxoutDense(Layer):\n         Refer to http://arxiv.org/pdf/1302.4389.pdf\n     '''\n     def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None, \n-        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n+        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n \n         super(MaxoutDense, self).__init__()\n         self.init = initializations.get(init)\n@@ -518,17 +538,22 @@ class MaxoutDense(Layer):\n         self.params = [self.W, self.b]\n \n         self.regularizers = []\n-        if W_regularizer:\n-            W_regularizer.set_param(self.W)\n-            self.regularizers.append(W_regularizer)\n-        if b_regularizer:\n-            b_regularizer.set_param(self.b)\n-            self.regularizers.append(b_regularizer)\n-        if activity_regularizer:\n-            activity_regularizer.set_layer(self)\n-            self.regularizers.append(activity_regularizer)\n \n-        self.constraints = [W_constraint, b_constraint]\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.W_regularizer.set_param(self.W)\n+        self.regularizers.append(self.W_regularizer)\n+\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.b_regularizer.set_param(self.b)\n+        self.regularizers.append(self.b_regularizer)\n+\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.activity_regularizer.set_layer(self)\n+        self.regularizers.append(self.activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -544,4 +569,9 @@ class MaxoutDense(Layer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"nb_feature\" : self.nb_feature}\n+            \"nb_feature\" : self.nb_feature,\n+            \"W_regularizer\":self.W_regularizer.get_config(),\n+            \"b_regularizer\":self.b_regularizer.get_config(),\n+            \"activity_regularizer\":self.activity_regularizer.get_config(),\n+            \"W_constraint\":self.W_constraint.get_config(),\n+            \"b_constraint\":self.b_constraint.get_config()}\n\n@@ -2,7 +2,7 @@ from __future__ import absolute_import\n import theano\n import theano.tensor as T\n \n-from .. import activations, initializations\n+from .. import activations, initializations, regularizers, constraints\n from ..layers.core import Layer, MaskedLayer\n from ..utils.theano_utils import sharedX\n \n@@ -18,7 +18,7 @@ class Embedding(Layer):\n         @out_dim: size of dense representation\n     '''\n     def __init__(self, input_dim, output_dim, init='uniform',\n-        W_regularizer=None, activity_regularizer=None, W_constraint=None,\n+        W_regularizer='identity', activity_regularizer='identity', W_constraint='identity',\n         mask_zero=False, weights=None):\n \n         super(Embedding,self).__init__()\n@@ -31,15 +31,19 @@ class Embedding(Layer):\n         self.mask_zero = mask_zero\n \n         self.params = [self.W]\n-        self.constraints = [W_constraint]\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.constraints = [self.W_constraint]\n \n         self.regularizers = []\n-        if W_regularizer:\n-            W_regularizer.set_param(self.W)\n-            self.regularizers.append(W_regularizer)\n-        if activity_regularizer:\n-            activity_regularizer.set_layer(self)\n-            self.regularizers.append(activity_regularizer)\n+\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.W_regularizer.set_param(self.W)\n+        self.regularizers.append(self.W_regularizer)\n+\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+        self.activity_regularizer.set_layer(self)\n+        self.regularizers.append(self.activity_regularizer)\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -60,7 +64,10 @@ class Embedding(Layer):\n         return {\"name\":self.__class__.__name__,\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n-            \"init\":self.init.__name__}\n+            \"init\":self.init.__name__,\n+            \"activity_regularizer\":self.activity_regularizer.get_config(),\n+            \"W_regularizer\":self.W_regularizer.get_config(),\n+            \"W_constraint\":self.W_constraint.get_config()}\n \n \n class WordContextProduct(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#afc5adbfe1c3100012b84aa621bc827e5645039d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 71 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ def lecun_uniform(shape):\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n     '''\n     fan_in, fan_out = get_fans(shape)\n-    scale = 1./np.sqrt(fan_in)\n+    scale = np.sqrt(3. / fan_in)\n     return uniform(shape, scale)\n \n def glorot_normal(shape):\n@@ -34,7 +34,7 @@ def glorot_normal(shape):\n \n def glorot_uniform(shape):\n     fan_in, fan_out = get_fans(shape)\n-    s = np.sqrt(2. / (fan_in + fan_out))\n+    s = np.sqrt(6. / (fan_in + fan_out))\n     return uniform(shape, s)\n     \n def he_normal(shape):\n@@ -46,7 +46,7 @@ def he_normal(shape):\n \n def he_uniform(shape):\n     fan_in, fan_out = get_fans(shape)\n-    s = np.sqrt(2. / fan_in)\n+    s = np.sqrt(6. / fan_in)\n     return uniform(shape, s)\n \n def orthogonal(shape, scale=1.1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9f71949d6dd5db4c826a81d5e3996e09236bab06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 221 | Contributors (this commit): 5 | Commits (past 90d): 24 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,10 @@ import theano.tensor as T\n import numpy as np\n from six.moves import range\n \n+if theano.config.floatX == 'float64':\n     epsilon = 1.0e-9\n+else:\n+    epsilon = 1.0e-7\n \n def mean_squared_error(y_true, y_pred):\n     return T.sqr(y_pred - y_true).mean(axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#03e512a3f4dfbd3c241c5f7824c3f052005d4e6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 2 | Churn Cumulative: 2039 | Contributors (this commit): 18 | Commits (past 90d): 85 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -145,7 +145,7 @@ class Model(object):\n                     epoch_logs = {}\n                     if do_validation:\n                         # replace with self._evaluate\n-                        val_outs = val_f(*val_ins)\n+                        val_outs = self._test_loop(val_f, val_ins, batch_size=batch_size, verbose=0)\n                         if type(val_outs) != list:\n                             val_outs = [val_outs]\n                         # same labels assumed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#493e6d2852d75fc582b385909df63e7703d96cf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 12 | Churn Cumulative: 2051 | Contributors (this commit): 18 | Commits (past 90d): 86 | Contributors (cumulative): 18 | DMM Complexity: 0.5\n\nDIFF:\n@@ -88,13 +88,12 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n     else:\n         return np.ones(y.shape[:-1] + (1,))\n \n-def sequential_from_yaml(pathToYaml):\n+def sequential_from_yaml(yamlString):\n     '''\n         Returns a compiled Sequential model generated from a local yaml file,\n         which is either created by hand or from Sequential.to_yaml\n     '''\n-    stream = open(pathToYaml, 'r')\n-    modelYaml = yaml.load(stream)\n+    modelYaml = yaml.load(yamlString)\n     model = Sequential()\n \n     class_mode = modelYaml.get('class_mode')\n@@ -523,9 +522,9 @@ class Sequential(Model, containers.Sequential):\n         f.close()\n \n \n-    def to_yaml(self, fileName, storeParams=True):\n+    def to_yaml(self, storeParams=True):\n         '''\n-            Stores compiled Sequential model to local file, optionally storing all learnable parameters\n+            Stores compiled Sequential model to yaml string, optionally storing all learnable parameters\n         '''\n         modelDict = {}\n         modelDict['class_mode'] = self.class_mode\n@@ -541,8 +540,7 @@ class Sequential(Model, containers.Sequential):\n             layers.append(layerConf)\n         modelDict['layers'] = layers\n \n-        with open(fileName, 'w') as outfile:\n-            outfile.write(yaml.dump(modelDict, default_flow_style=True))\n+        return yaml.dump(modelDict)\n \n \n class Graph(Model, containers.Graph):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38c4cb2ac241e278d2dcd8e07918b75cbfe4fd0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 55 | Churn Cumulative: 55 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,55 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import numpy as np\n+\n+from keras.preprocessing import sequence\n+from keras.optimizers import SGD, RMSprop, Adagrad\n+from keras.utils import np_utils\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation\n+from keras.layers.embeddings import Embedding\n+from keras.layers.recurrent import LSTM, GRU\n+from keras.datasets import imdb\n+from keras.models import sequential_from_yaml\n+\n+'''\n+This is essentially the IMDB test. Deserialized models should yield \n+the same config as the original one.\n+'''\n+\n+max_features = 10000\n+maxlen = 100 \n+batch_size = 32\n+\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features, test_split=0.2)\n+\n+X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n+X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n+\n+model = Sequential()\n+model.add(Embedding(max_features, 128))\n+model.add(LSTM(128, 128)) \n+model.add(Dropout(0.5))\n+model.add(Dense(128, 1))\n+model.add(Activation('sigmoid'))\n+\n+model.compile(loss='binary_crossentropy', optimizer='adam', class_mode=\"binary\")\n+\n+model.get_config(verbose=1)\n+\n+######################\n+# save model to yaml #\n+######################\n+yamlString = model.to_yaml()\n+\n+recovered_model = sequential_from_yaml(yamlString)\n+recovered_model.get_config(verbose=1)\n+\n+#####################################\n+# save model w/o parameters to yaml #\n+#####################################\n+\n+yaml_no_params = model.to_yaml(storeParams=False)\n+\n+no_param_model = sequential_from_yaml(yaml_no_params)\n+no_param_model.get_config(verbose=1)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
