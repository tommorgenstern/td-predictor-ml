{"custom_id": "keras#d2c913e2f5bd20d90473ebbd58e4b31c3ec38769", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 354 | Files Changed: 11 | Hunks: 79 | Methods Changed: 67 | Complexity Δ (Sum/Max): -63/4 | Churn Δ: 398 | Churn Cumulative: 11585 | Contributors (this commit): 21 | Commits (past 90d): 163 | Contributors (cumulative): 67 | DMM Complexity: 0.0\n\nDIFF:\n@@ -58,9 +58,9 @@ def get_model():\n     # Make a simple convnet with batch normalization and dropout.\n     inputs = keras.Input(shape=(28, 28, 1))\n     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n-    x = keras.layers.Conv2D(\n-        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n-    )(x)\n+    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n+        x\n+    )\n     x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n     x = keras.layers.ReLU()(x)\n     x = keras.layers.Conv2D(\n@@ -187,11 +187,7 @@ compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)\n # Training step, Keras provides a pure functional optimizer.stateless_apply\n @jax.jit\n def train_step(train_state, x, y):\n-    (\n-        trainable_variables,\n-        non_trainable_variables,\n-        optimizer_variables,\n-    ) = train_state\n+    trainable_variables, non_trainable_variables, optimizer_variables = train_state\n     (loss_value, non_trainable_variables), grads = compute_gradients(\n         trainable_variables, non_trainable_variables, x, y\n     )\n@@ -215,9 +211,7 @@ def get_replicated_train_state(devices):\n     var_replication = NamedSharding(var_mesh, P())\n \n     # Apply the distribution settings to the model variables\n-    trainable_variables = jax.device_put(\n-        model.trainable_variables, var_replication\n-    )\n+    trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n     non_trainable_variables = jax.device_put(\n         model.non_trainable_variables, var_replication\n     )\n@@ -261,9 +255,7 @@ for epoch in range(num_epochs):\n trainable_variables, non_trainable_variables, optimizer_variables = train_state\n for variable, value in zip(model.trainable_variables, trainable_variables):\n     variable.assign(value)\n-for variable, value in zip(\n-    model.non_trainable_variables, non_trainable_variables\n-):\n+for variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n     variable.assign(value)\n \n \"\"\"\n\n@@ -53,9 +53,9 @@ def get_model():\n     # Make a simple convnet with batch normalization and dropout.\n     inputs = keras.Input(shape=(28, 28, 1))\n     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n-    x = keras.layers.Conv2D(\n-        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n-    )(x)\n+    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n+        x\n+    )\n     x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n     x = keras.layers.ReLU()(x)\n     x = keras.layers.Conv2D(\n@@ -231,9 +231,7 @@ def per_device_launch_fn(current_gpu_index, num_gpu):\n     model = get_model()\n \n     # prepare the dataloader\n-    dataloader = prepare_dataloader(\n-        dataset, current_gpu_index, num_gpu, batch_size\n-    )\n+    dataloader = prepare_dataloader(dataset, current_gpu_index, num_gpu, batch_size)\n \n     # Instantiate the torch optimizer\n     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n@@ -14,66 +14,82 @@ from keras_core.backend.jax.core import convert_to_tensor\n \n \n def relu(x):\n+    x = convert_to_tensor(x)\n     return jnn.relu(x)\n \n \n def relu6(x):\n+    x = convert_to_tensor(x)\n     return jnn.relu6(x)\n \n \n def sigmoid(x):\n+    x = convert_to_tensor(x)\n     return jnn.sigmoid(x)\n \n \n def tanh(x):\n+    x = convert_to_tensor(x)\n     return jnn.tanh(x)\n \n \n def softplus(x):\n+    x = convert_to_tensor(x)\n     return jnn.softplus(x)\n \n \n def softsign(x):\n+    x = convert_to_tensor(x)\n     return jnn.soft_sign(x)\n \n \n def silu(x):\n+    x = convert_to_tensor(x)\n     return jnn.silu(x)\n \n \n def swish(x):\n+    x = convert_to_tensor(x)\n     return jnn.swish(x)\n \n \n def log_sigmoid(x):\n+    x = convert_to_tensor(x)\n     return jnn.log_sigmoid(x)\n \n \n def leaky_relu(x, negative_slope=0.2):\n+    x = convert_to_tensor(x)\n     return jnn.leaky_relu(x, negative_slope=negative_slope)\n \n \n def hard_sigmoid(x):\n+    x = convert_to_tensor(x)\n     return jnn.hard_sigmoid(x)\n \n \n def elu(x, alpha=1.0):\n+    x = convert_to_tensor(x)\n     return jnn.elu(x, alpha=alpha)\n \n \n def selu(x):\n+    x = convert_to_tensor(x)\n     return jnn.selu(x)\n \n \n def gelu(x, approximate=True):\n+    x = convert_to_tensor(x)\n     return jnn.gelu(x, approximate)\n \n \n def softmax(x, axis=None):\n+    x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n \n \n def log_softmax(x, axis=-1):\n+    x = convert_to_tensor(x)\n     return jnn.log_softmax(x, axis=axis)\n \n \n@@ -389,6 +405,7 @@ def conv_transpose(\n \n \n def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     return jnn.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n\n@@ -107,18 +107,10 @@ def arccos(x):\n     return jnp.arccos(x)\n \n \n-def arccosh(x):\n-    return jnp.arccosh(x)\n-\n-\n def arcsin(x):\n     return jnp.arcsin(x)\n \n \n-def arcsinh(x):\n-    return jnp.arcsinh(x)\n-\n-\n def arctan(x):\n     return jnp.arctan(x)\n \n@@ -127,10 +119,6 @@ def arctan2(x1, x2):\n     return jnp.arctan2(x1, x2)\n \n \n-def arctanh(x):\n-    return jnp.arctanh(x)\n-\n-\n def argmax(x, axis=None):\n     return jnp.argmax(x, axis=axis)\n \n@@ -183,10 +171,6 @@ def cos(x):\n     return jnp.cos(x)\n \n \n-def cosh(x):\n-    return jnp.cosh(x)\n-\n-\n def count_nonzero(x, axis=None):\n     return jnp.count_nonzero(x, axis=axis)\n \n@@ -457,10 +441,6 @@ def sin(x):\n     return jnp.sin(x)\n \n \n-def sinh(x):\n-    return jnp.sinh(x)\n-\n-\n def size(x):\n     return jnp.size(x)\n \n@@ -499,10 +479,6 @@ def tan(x):\n     return jnp.tan(x)\n \n \n-def tanh(x):\n-    return jnp.tanh(x)\n-\n-\n def tensordot(x1, x2, axes=2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n\n@@ -84,18 +84,10 @@ def arccos(x):\n     return np.arccos(x)\n \n \n-def arccosh(x):\n-    return np.arccosh(x)\n-\n-\n def arcsin(x):\n     return np.arcsin(x)\n \n \n-def arcsinh(x):\n-    return np.arcsinh(x)\n-\n-\n def arctan(x):\n     return np.arctan(x)\n \n@@ -104,10 +96,6 @@ def arctan2(x1, x2):\n     return np.arctan2(x1, x2)\n \n \n-def arctanh(x):\n-    return np.arctanh(x)\n-\n-\n def argmax(x, axis=None):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n     return np.argmax(x, axis=axis)\n@@ -169,10 +157,6 @@ def cos(x):\n     return np.cos(x)\n \n \n-def cosh(x):\n-    return np.cosh(x)\n-\n-\n def count_nonzero(x, axis=None):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n     return np.count_nonzero(x, axis=axis)\n@@ -454,10 +438,6 @@ def sin(x):\n     return np.sin(x)\n \n \n-def sinh(x):\n-    return np.sinh(x)\n-\n-\n def size(x):\n     return np.size(x)\n \n@@ -500,10 +480,6 @@ def tan(x):\n     return np.tan(x)\n \n \n-def tanh(x):\n-    return np.tanh(x)\n-\n-\n def tensordot(x1, x2, axes=2):\n     axes = tuple(axes) if isinstance(axes, list) else axes\n     return np.tensordot(x1, x2, axes=axes)\n\n@@ -105,18 +105,10 @@ def arccos(x):\n     return tfnp.arccos(x)\n \n \n-def arccosh(x):\n-    return tfnp.arccosh(x)\n-\n-\n def arcsin(x):\n     return tfnp.arcsin(x)\n \n \n-def arcsinh(x):\n-    return tfnp.arcsinh(x)\n-\n-\n def arctan(x):\n     return tfnp.arctan(x)\n \n@@ -125,10 +117,6 @@ def arctan2(x1, x2):\n     return tfnp.arctan2(x1, x2)\n \n \n-def arctanh(x):\n-    return tfnp.arctanh(x)\n-\n-\n def argmax(x, axis=None):\n     return tfnp.argmax(x, axis=axis)\n \n@@ -186,10 +174,6 @@ def cos(x):\n     return tfnp.cos(x)\n \n \n-def cosh(x):\n-    return tfnp.cosh(x)\n-\n-\n def count_nonzero(x, axis=None):\n     return tfnp.count_nonzero(x, axis=axis)\n \n@@ -488,10 +472,6 @@ def sin(x):\n     return tfnp.sin(x)\n \n \n-def sinh(x):\n-    return tfnp.sinh(x)\n-\n-\n def size(x):\n     return tfnp.size(x)\n \n@@ -528,10 +508,6 @@ def tan(x):\n     return tfnp.tan(x)\n \n \n-def tanh(x):\n-    return tfnp.tanh(x)\n-\n-\n def tensordot(x1, x2, axes=2):\n     return tfnp.tensordot(x1, x2, axes=axes)\n \n\n@@ -173,21 +173,11 @@ def arccos(x):\n     return torch.arccos(x)\n \n \n-def arccosh(x):\n-    x = convert_to_tensor(x)\n-    return torch.arccosh(x)\n-\n-\n def arcsin(x):\n     x = convert_to_tensor(x)\n     return torch.arcsin(x)\n \n \n-def arcsinh(x):\n-    x = convert_to_tensor(x)\n-    return torch.arcsinh(x)\n-\n-\n def arctan(x):\n     x = convert_to_tensor(x)\n     return torch.arctan(x)\n@@ -198,11 +188,6 @@ def arctan2(x1, x2):\n     return torch.arctan2(x1, x2)\n \n \n-def arctanh(x):\n-    x = convert_to_tensor(x)\n-    return torch.arctanh(x)\n-\n-\n def argmax(x, axis=None):\n     x = convert_to_tensor(x)\n     return torch.argmax(x, dim=axis)\n@@ -292,11 +277,6 @@ def cos(x):\n     return torch.cos(x)\n \n \n-def cosh(x):\n-    x = convert_to_tensor(x)\n-    return torch.cosh(x)\n-\n-\n def count_nonzero(x, axis=None):\n     x = convert_to_tensor(x)\n     if axis == () or axis == []:\n@@ -749,11 +729,6 @@ def sin(x):\n     return torch.sin(x)\n \n \n-def sinh(x):\n-    x = convert_to_tensor(x)\n-    return torch.sinh(x)\n-\n-\n def size(x):\n     x_shape = convert_to_tensor(tuple(x.shape))\n     return torch.prod(x_shape)\n@@ -831,11 +806,6 @@ def tan(x):\n     return torch.tan(x)\n \n \n-def tanh(x):\n-    x = convert_to_tensor(x)\n-    return torch.tanh(x)\n-\n-\n def tensordot(x1, x2, axes=2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n     # Conversion to long necessary for `torch.tensordot`\n\n@@ -57,6 +57,21 @@ def sigmoid(x):\n     return backend.nn.sigmoid(x)\n \n \n+class Tanh(Operation):\n+    def call(self, x):\n+        return backend.nn.tanh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export([\"keras_core.ops.tanh\", \"keras_core.ops.nn.tanh\"])\n+def tanh(x):\n+    if any_symbolic_tensors((x,)):\n+        return Tanh().symbolic_call(x)\n+    return backend.nn.tanh(x)\n+\n+\n class Softplus(Operation):\n     def call(self, x):\n         return backend.nn.softplus(x)\n\n@@ -10,12 +10,9 @@ amin\n append\n arange\n arccos\n-arccosh\n arcsin\n-arcsinh\n arctan\n arctan2\n-arctanh\n argmax\n argmin\n argsort\n@@ -30,7 +27,6 @@ conj\n conjugate\n copy\n cos\n-cosh\n count_nonzero\n cross\n cumprod\n@@ -106,7 +102,6 @@ roll\n round\n sign\n sin\n-sinh\n size\n sort\n split\n@@ -121,7 +116,6 @@ swapaxes\n take\n take_along_axis\n tan\n-tanh\n tensordot\n tile\n trace\n@@ -719,28 +713,6 @@ def arccos(x):\n     return backend.numpy.arccos(x)\n \n \n-class Arccosh(Operation):\n-    def call(self, x):\n-        return backend.numpy.arccosh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-def arccosh(x):\n-    \"\"\"Inverse hyperbolic cosine, element-wise.\n-\n-    Arguments:\n-        x: Input tensor.\n-\n-    Returns:\n-        Output tensor of same shape as x.\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Arccosh().symbolic_call(x)\n-    return backend.numpy.arccosh(x)\n-\n-\n class Arcsin(Operation):\n     def call(self, x):\n         return backend.numpy.arcsin(x)\n@@ -770,29 +742,6 @@ def arcsin(x):\n     return backend.numpy.arcsin(x)\n \n \n-class Arcsinh(Operation):\n-    def call(self, x):\n-        return backend.numpy.arcsinh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.arcsinh\", \"keras_core.ops.numpy.arcsinh\"])\n-def arcsinh(x):\n-    \"\"\"Inverse hyperbolic sine, element-wise.\n-\n-    Arguments:\n-        x: Input tensor.\n-\n-    Returns:\n-        Output tensor of same shape as x.\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Arcsinh().symbolic_call(x)\n-    return backend.numpy.arcsinh(x)\n-\n-\n class Arctan(Operation):\n     def call(self, x):\n         return backend.numpy.arctan(x)\n@@ -877,29 +826,6 @@ def arctan2(x1, x2):\n     return backend.numpy.arctan2(x1, x2)\n \n \n-class Arctanh(Operation):\n-    def call(self, x):\n-        return backend.numpy.arctanh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.arctanh\", \"keras_core.ops.numpy.arctanh\"])\n-def arctanh(x):\n-    \"\"\"Inverse hyperbolic tangent, element-wise.\n-\n-    Arguments:\n-        x: Input tensor.\n-\n-    Returns:\n-        Output tensor of same shape as x.\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Arctanh().symbolic_call(x)\n-    return backend.numpy.arctanh(x)\n-\n-\n class Argmax(Operation):\n     def __init__(self, axis=None):\n         super().__init__()\n@@ -1363,29 +1289,6 @@ def cos(x):\n     return backend.numpy.cos(x)\n \n \n-class Cosh(Operation):\n-    def call(self, x):\n-        return backend.numpy.cosh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.cosh\", \"keras_core.ops.numpy.cosh\"])\n-def cosh(x):\n-    \"\"\"Hyperbolic cosine, element-wise.\n-\n-    Arguments:\n-        x: Input tensor.\n-\n-    Returns:\n-        Output tensor of same shape as x.\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Cosh().symbolic_call(x)\n-    return backend.numpy.cosh(x)\n-\n-\n class CountNonzero(Operation):\n     def __init__(self, axis=None):\n         super().__init__()\n@@ -3212,29 +3115,6 @@ def sin(x):\n     return backend.numpy.sin(x)\n \n \n-class Sinh(Operation):\n-    def call(self, x):\n-        return backend.numpy.sinh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.sinh\", \"keras_core.ops.numpy.sinh\"])\n-def sinh(x):\n-    \"\"\"Hyperbolic sine, element-wise.\n-\n-    Arguments:\n-        x: Input tensor.\n-\n-    Returns:\n-        Output tensor of same shape as x.\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Sinh().symbolic_call(x)\n-    return backend.numpy.sinh(x)\n-\n-\n class Size(Operation):\n     def call(self, x):\n         return backend.numpy.size(x)\n@@ -3496,29 +3376,6 @@ def tan(x):\n     return backend.numpy.tan(x)\n \n \n-class Tanh(Operation):\n-    def call(self, x):\n-        return backend.numpy.tanh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.tanh\", \"keras_core.ops.numpy.tanh\"])\n-def tanh(x):\n-    \"\"\"Hyperbolic tangent, element-wise.\n-\n-    Arguments:\n-        x: Input tensor.\n-\n-    Returns:\n-        Output tensor of same shape as x.\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Tanh().symbolic_call(x)\n-    return backend.numpy.tanh(x)\n-\n-\n class Tensordot(Operation):\n     def __init__(self, axes=2):\n         super().__init__()\n\n@@ -754,26 +754,14 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arccos(x).shape, (None, 3))\n \n-    def test_arccosh(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.arccosh(x).shape, (None, 3))\n-\n     def test_arcsin(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arcsin(x).shape, (None, 3))\n \n-    def test_arcsinh(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.arcsinh(x).shape, (None, 3))\n-\n     def test_arctan(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arctan(x).shape, (None, 3))\n \n-    def test_arctanh(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.arctanh(x).shape, (None, 3))\n-\n     def test_argmax(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.argmax(x).shape, ())\n@@ -867,10 +855,6 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.cos(x).shape, (None, 3))\n \n-    def test_cosh(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.cosh(x).shape, (None, 3))\n-\n     def test_count_nonzero(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.count_nonzero(x).shape, ())\n@@ -1111,10 +1095,6 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.sin(x).shape, (None, 3))\n \n-    def test_sinh(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.sinh(x).shape, (None, 3))\n-\n     def test_size(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.size(x).shape, ())\n@@ -1157,10 +1137,6 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.tan(x).shape, (None, 3))\n \n-    def test_tanh(self):\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.tanh(x).shape, (None, 3))\n-\n     def test_tile(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.tile(x, [2]).shape, (None, 6))\n@@ -1251,26 +1227,14 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arccos(x).shape, (2, 3))\n \n-    def test_arccosh(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.arccosh(x).shape, (2, 3))\n-\n     def test_arcsin(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arcsin(x).shape, (2, 3))\n \n-    def test_arcsinh(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.arcsinh(x).shape, (2, 3))\n-\n     def test_arctan(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arctan(x).shape, (2, 3))\n \n-    def test_arctanh(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.arctanh(x).shape, (2, 3))\n-\n     def test_argmax(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.argmax(x).shape, ())\n@@ -1333,10 +1297,6 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.cos(x).shape, (2, 3))\n \n-    def test_cosh(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.cosh(x).shape, (2, 3))\n-\n     def test_count_nonzero(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.count_nonzero(x).shape, ())\n@@ -1572,10 +1532,6 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sin(x).shape, (2, 3))\n \n-    def test_sinh(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.sinh(x).shape, (2, 3))\n-\n     def test_size(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.size(x).shape, ())\n@@ -1623,10 +1579,6 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.tan(x).shape, (2, 3))\n \n-    def test_tanh(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.tanh(x).shape, (2, 3))\n-\n     def test_tile(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.tile(x, [2]).shape, (2, 6))\n@@ -2314,42 +2266,18 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.transpose(x, axes=(1, 0, 3, 2, 4)),\n         )\n \n-    def test_arccos(self):\n+    def test_arcos(self):\n         x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         self.assertAllClose(knp.arccos(x), np.arccos(x))\n \n         self.assertAllClose(knp.Arccos()(x), np.arccos(x))\n \n-    def test_arccosh(self):\n-        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n-        self.assertAllClose(knp.arccosh(x), np.arccosh(x))\n-\n-        self.assertAllClose(knp.Arccosh()(x), np.arccosh(x))\n-\n     def test_arcsin(self):\n         x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         self.assertAllClose(knp.arcsin(x), np.arcsin(x))\n \n         self.assertAllClose(knp.Arcsin()(x), np.arcsin(x))\n \n-    def test_arcsinh(self):\n-        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n-        self.assertAllClose(knp.arcsinh(x), np.arcsinh(x))\n-\n-        self.assertAllClose(knp.Arcsinh()(x), np.arcsinh(x))\n-\n-    def test_arctan(self):\n-        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n-        self.assertAllClose(knp.arctan(x), np.arctan(x))\n-\n-        self.assertAllClose(knp.Arctan()(x), np.arctan(x))\n-\n-    def test_arctanh(self):\n-        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n-        self.assertAllClose(knp.arctanh(x), np.arctanh(x))\n-\n-        self.assertAllClose(knp.Arctanh()(x), np.arctanh(x))\n-\n     def test_argmax(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.argmax(x), np.argmax(x))\n@@ -2505,11 +2433,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.cos(x), np.cos(x))\n         self.assertAllClose(knp.Cos()(x), np.cos(x))\n \n-    def test_cosh(self):\n-        x = np.array([[1, 2, 3], [3, 2, 1]])\n-        self.assertAllClose(knp.cosh(x), np.cosh(x))\n-        self.assertAllClose(knp.Cosh()(x), np.cosh(x))\n-\n     def test_count_nonzero(self):\n         x = np.array([[0, 2, 3], [3, 2, 0]])\n         self.assertAllClose(knp.count_nonzero(x), np.count_nonzero(x))\n@@ -2976,11 +2899,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.sin(x), np.sin(x))\n         self.assertAllClose(knp.Sin()(x), np.sin(x))\n \n-    def test_sinh(self):\n-        x = np.array([[1, -2, 3], [-3, 2, -1]])\n-        self.assertAllClose(knp.sinh(x), np.sinh(x))\n-        self.assertAllClose(knp.Sinh()(x), np.sinh(x))\n-\n     def test_size(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.size(x), np.size(x))\n@@ -3075,11 +2993,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.tan(x), np.tan(x))\n         self.assertAllClose(knp.Tan()(x), np.tan(x))\n \n-    def test_tanh(self):\n-        x = np.array([[1, -2, 3], [-3, 2, -1]])\n-        self.assertAllClose(knp.tanh(x), np.tanh(x))\n-        self.assertAllClose(knp.Tanh()(x), np.tanh(x))\n-\n     def test_tile(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.tile(x, [2, 3]), np.tile(x, [2, 3]))\n\n@@ -218,7 +218,7 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n         if is_batched(dataset):\n             dataset = dataset.unbatch()\n         return iter(dataset)\n-    # torch dataset iterator might be required to change\n+\n     elif is_torch_dataset(dataset):\n         return iter(dataset)\n     elif dataset_type_spec == np.ndarray:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4135b933b4a8a1dd6266be1bc9bd093eb7795fb0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 353 | Lines Deleted: 26 | Files Changed: 9 | Hunks: 61 | Methods Changed: 49 | Complexity Δ (Sum/Max): 63/24 | Churn Δ: 379 | Churn Cumulative: 8554 | Contributors (this commit): 17 | Commits (past 90d): 147 | Contributors (cumulative): 55 | DMM Complexity: 1.0\n\nDIFF:\n@@ -58,9 +58,9 @@ def get_model():\n     # Make a simple convnet with batch normalization and dropout.\n     inputs = keras.Input(shape=(28, 28, 1))\n     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n-    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n-        x\n-    )\n+    x = keras.layers.Conv2D(\n+        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n+    )(x)\n     x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n     x = keras.layers.ReLU()(x)\n     x = keras.layers.Conv2D(\n@@ -187,7 +187,11 @@ compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)\n # Training step, Keras provides a pure functional optimizer.stateless_apply\n @jax.jit\n def train_step(train_state, x, y):\n-    trainable_variables, non_trainable_variables, optimizer_variables = train_state\n+    (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+    ) = train_state\n     (loss_value, non_trainable_variables), grads = compute_gradients(\n         trainable_variables, non_trainable_variables, x, y\n     )\n@@ -211,7 +215,9 @@ def get_replicated_train_state(devices):\n     var_replication = NamedSharding(var_mesh, P())\n \n     # Apply the distribution settings to the model variables\n-    trainable_variables = jax.device_put(model.trainable_variables, var_replication)\n+    trainable_variables = jax.device_put(\n+        model.trainable_variables, var_replication\n+    )\n     non_trainable_variables = jax.device_put(\n         model.non_trainable_variables, var_replication\n     )\n@@ -255,7 +261,9 @@ for epoch in range(num_epochs):\n trainable_variables, non_trainable_variables, optimizer_variables = train_state\n for variable, value in zip(model.trainable_variables, trainable_variables):\n     variable.assign(value)\n-for variable, value in zip(model.non_trainable_variables, non_trainable_variables):\n+for variable, value in zip(\n+    model.non_trainable_variables, non_trainable_variables\n+):\n     variable.assign(value)\n \n \"\"\"\n\n@@ -53,9 +53,9 @@ def get_model():\n     # Make a simple convnet with batch normalization and dropout.\n     inputs = keras.Input(shape=(28, 28, 1))\n     x = keras.layers.Rescaling(1.0 / 255.0)(inputs)\n-    x = keras.layers.Conv2D(filters=12, kernel_size=3, padding=\"same\", use_bias=False)(\n-        x\n-    )\n+    x = keras.layers.Conv2D(\n+        filters=12, kernel_size=3, padding=\"same\", use_bias=False\n+    )(x)\n     x = keras.layers.BatchNormalization(scale=False, center=True)(x)\n     x = keras.layers.ReLU()(x)\n     x = keras.layers.Conv2D(\n@@ -231,7 +231,9 @@ def per_device_launch_fn(current_gpu_index, num_gpu):\n     model = get_model()\n \n     # prepare the dataloader\n-    dataloader = prepare_dataloader(dataset, current_gpu_index, num_gpu, batch_size)\n+    dataloader = prepare_dataloader(\n+        dataset, current_gpu_index, num_gpu, batch_size\n+    )\n \n     # Instantiate the torch optimizer\n     optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n@@ -107,10 +107,18 @@ def arccos(x):\n     return jnp.arccos(x)\n \n \n+def arccosh(x):\n+    return jnp.arccosh(x)\n+\n+\n def arcsin(x):\n     return jnp.arcsin(x)\n \n \n+def arcsinh(x):\n+    return jnp.arcsinh(x)\n+\n+\n def arctan(x):\n     return jnp.arctan(x)\n \n@@ -119,6 +127,10 @@ def arctan2(x1, x2):\n     return jnp.arctan2(x1, x2)\n \n \n+def arctanh(x):\n+    return jnp.arctanh(x)\n+\n+\n def argmax(x, axis=None):\n     return jnp.argmax(x, axis=axis)\n \n@@ -171,6 +183,10 @@ def cos(x):\n     return jnp.cos(x)\n \n \n+def cosh(x):\n+    return jnp.cosh(x)\n+\n+\n def count_nonzero(x, axis=None):\n     return jnp.count_nonzero(x, axis=axis)\n \n@@ -441,6 +457,10 @@ def sin(x):\n     return jnp.sin(x)\n \n \n+def sinh(x):\n+    return jnp.sinh(x)\n+\n+\n def size(x):\n     return jnp.size(x)\n \n@@ -479,6 +499,10 @@ def tan(x):\n     return jnp.tan(x)\n \n \n+def tanh(x):\n+    return jnp.tanh(x)\n+\n+\n def tensordot(x1, x2, axes=2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n\n@@ -84,10 +84,18 @@ def arccos(x):\n     return np.arccos(x)\n \n \n+def arccosh(x):\n+    return np.arccosh(x)\n+\n+\n def arcsin(x):\n     return np.arcsin(x)\n \n \n+def arcsinh(x):\n+    return np.arcsinh(x)\n+\n+\n def arctan(x):\n     return np.arctan(x)\n \n@@ -96,6 +104,10 @@ def arctan2(x1, x2):\n     return np.arctan2(x1, x2)\n \n \n+def arctanh(x):\n+    return np.arctanh(x)\n+\n+\n def argmax(x, axis=None):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n     return np.argmax(x, axis=axis)\n@@ -157,6 +169,10 @@ def cos(x):\n     return np.cos(x)\n \n \n+def cosh(x):\n+    return np.cosh(x)\n+\n+\n def count_nonzero(x, axis=None):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n     return np.count_nonzero(x, axis=axis)\n@@ -438,6 +454,10 @@ def sin(x):\n     return np.sin(x)\n \n \n+def sinh(x):\n+    return np.sinh(x)\n+\n+\n def size(x):\n     return np.size(x)\n \n@@ -480,6 +500,10 @@ def tan(x):\n     return np.tan(x)\n \n \n+def tanh(x):\n+    return np.tanh(x)\n+\n+\n def tensordot(x1, x2, axes=2):\n     axes = tuple(axes) if isinstance(axes, list) else axes\n     return np.tensordot(x1, x2, axes=axes)\n\n@@ -105,10 +105,18 @@ def arccos(x):\n     return tfnp.arccos(x)\n \n \n+def arccosh(x):\n+    return tfnp.arccosh(x)\n+\n+\n def arcsin(x):\n     return tfnp.arcsin(x)\n \n \n+def arcsinh(x):\n+    return tfnp.arcsinh(x)\n+\n+\n def arctan(x):\n     return tfnp.arctan(x)\n \n@@ -117,6 +125,10 @@ def arctan2(x1, x2):\n     return tfnp.arctan2(x1, x2)\n \n \n+def arctanh(x):\n+    return tfnp.arctanh(x)\n+\n+\n def argmax(x, axis=None):\n     return tfnp.argmax(x, axis=axis)\n \n@@ -174,6 +186,10 @@ def cos(x):\n     return tfnp.cos(x)\n \n \n+def cosh(x):\n+    return tfnp.cosh(x)\n+\n+\n def count_nonzero(x, axis=None):\n     return tfnp.count_nonzero(x, axis=axis)\n \n@@ -472,6 +488,10 @@ def sin(x):\n     return tfnp.sin(x)\n \n \n+def sinh(x):\n+    return tfnp.sinh(x)\n+\n+\n def size(x):\n     return tfnp.size(x)\n \n@@ -508,6 +528,10 @@ def tan(x):\n     return tfnp.tan(x)\n \n \n+def tanh(x):\n+    return tfnp.tanh(x)\n+\n+\n def tensordot(x1, x2, axes=2):\n     return tfnp.tensordot(x1, x2, axes=axes)\n \n\n@@ -173,11 +173,21 @@ def arccos(x):\n     return torch.arccos(x)\n \n \n+def arccosh(x):\n+    x = convert_to_tensor(x)\n+    return torch.arccosh(x)\n+\n+\n def arcsin(x):\n     x = convert_to_tensor(x)\n     return torch.arcsin(x)\n \n \n+def arcsinh(x):\n+    x = convert_to_tensor(x)\n+    return torch.arcsinh(x)\n+\n+\n def arctan(x):\n     x = convert_to_tensor(x)\n     return torch.arctan(x)\n@@ -188,6 +198,11 @@ def arctan2(x1, x2):\n     return torch.arctan2(x1, x2)\n \n \n+def arctanh(x):\n+    x = convert_to_tensor(x)\n+    return torch.arctanh(x)\n+\n+\n def argmax(x, axis=None):\n     x = convert_to_tensor(x)\n     return torch.argmax(x, dim=axis)\n@@ -277,6 +292,11 @@ def cos(x):\n     return torch.cos(x)\n \n \n+def cosh(x):\n+    x = convert_to_tensor(x)\n+    return torch.cosh(x)\n+\n+\n def count_nonzero(x, axis=None):\n     x = convert_to_tensor(x)\n     if axis == () or axis == []:\n@@ -729,6 +749,11 @@ def sin(x):\n     return torch.sin(x)\n \n \n+def sinh(x):\n+    x = convert_to_tensor(x)\n+    return torch.sinh(x)\n+\n+\n def size(x):\n     x_shape = convert_to_tensor(tuple(x.shape))\n     return torch.prod(x_shape)\n@@ -806,6 +831,11 @@ def tan(x):\n     return torch.tan(x)\n \n \n+def tanh(x):\n+    x = convert_to_tensor(x)\n+    return torch.tanh(x)\n+\n+\n def tensordot(x1, x2, axes=2):\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n     # Conversion to long necessary for `torch.tensordot`\n\n@@ -57,21 +57,6 @@ def sigmoid(x):\n     return backend.nn.sigmoid(x)\n \n \n-class Tanh(Operation):\n-    def call(self, x):\n-        return backend.nn.tanh(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.tanh\", \"keras_core.ops.nn.tanh\"])\n-def tanh(x):\n-    if any_symbolic_tensors((x,)):\n-        return Tanh().symbolic_call(x)\n-    return backend.nn.tanh(x)\n-\n-\n class Softplus(Operation):\n     def call(self, x):\n         return backend.nn.softplus(x)\n\n@@ -10,9 +10,12 @@ amin\n append\n arange\n arccos\n+arccosh\n arcsin\n+arcsinh\n arctan\n arctan2\n+arctanh\n argmax\n argmin\n argsort\n@@ -27,6 +30,7 @@ conj\n conjugate\n copy\n cos\n+cosh\n count_nonzero\n cross\n cumprod\n@@ -102,6 +106,7 @@ roll\n round\n sign\n sin\n+sinh\n size\n sort\n split\n@@ -116,6 +121,7 @@ swapaxes\n take\n take_along_axis\n tan\n+tanh\n tensordot\n tile\n trace\n@@ -713,6 +719,28 @@ def arccos(x):\n     return backend.numpy.arccos(x)\n \n \n+class Arccosh(Operation):\n+    def call(self, x):\n+        return backend.numpy.arccosh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+def arccosh(x):\n+    \"\"\"Inverse hyperbolic cosine, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as x.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Arccosh().symbolic_call(x)\n+    return backend.numpy.arccosh(x)\n+\n+\n class Arcsin(Operation):\n     def call(self, x):\n         return backend.numpy.arcsin(x)\n@@ -742,6 +770,29 @@ def arcsin(x):\n     return backend.numpy.arcsin(x)\n \n \n+class Arcsinh(Operation):\n+    def call(self, x):\n+        return backend.numpy.arcsinh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export([\"keras_core.ops.arcsinh\", \"keras_core.ops.numpy.arcsinh\"])\n+def arcsinh(x):\n+    \"\"\"Inverse hyperbolic sine, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as x.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Arcsinh().symbolic_call(x)\n+    return backend.numpy.arcsinh(x)\n+\n+\n class Arctan(Operation):\n     def call(self, x):\n         return backend.numpy.arctan(x)\n@@ -826,6 +877,29 @@ def arctan2(x1, x2):\n     return backend.numpy.arctan2(x1, x2)\n \n \n+class Arctanh(Operation):\n+    def call(self, x):\n+        return backend.numpy.arctanh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export([\"keras_core.ops.arctanh\", \"keras_core.ops.numpy.arctanh\"])\n+def arctanh(x):\n+    \"\"\"Inverse hyperbolic tangent, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as x.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Arctanh().symbolic_call(x)\n+    return backend.numpy.arctanh(x)\n+\n+\n class Argmax(Operation):\n     def __init__(self, axis=None):\n         super().__init__()\n@@ -1289,6 +1363,29 @@ def cos(x):\n     return backend.numpy.cos(x)\n \n \n+class Cosh(Operation):\n+    def call(self, x):\n+        return backend.numpy.cosh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export([\"keras_core.ops.cosh\", \"keras_core.ops.numpy.cosh\"])\n+def cosh(x):\n+    \"\"\"Hyperbolic cosine, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as x.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Cosh().symbolic_call(x)\n+    return backend.numpy.cosh(x)\n+\n+\n class CountNonzero(Operation):\n     def __init__(self, axis=None):\n         super().__init__()\n@@ -3115,6 +3212,29 @@ def sin(x):\n     return backend.numpy.sin(x)\n \n \n+class Sinh(Operation):\n+    def call(self, x):\n+        return backend.numpy.sinh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export([\"keras_core.ops.sinh\", \"keras_core.ops.numpy.sinh\"])\n+def sinh(x):\n+    \"\"\"Hyperbolic sine, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as x.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Sinh().symbolic_call(x)\n+    return backend.numpy.sinh(x)\n+\n+\n class Size(Operation):\n     def call(self, x):\n         return backend.numpy.size(x)\n@@ -3376,6 +3496,29 @@ def tan(x):\n     return backend.numpy.tan(x)\n \n \n+class Tanh(Operation):\n+    def call(self, x):\n+        return backend.numpy.tanh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_core_export([\"keras_core.ops.tanh\", \"keras_core.ops.numpy.tanh\"])\n+def tanh(x):\n+    \"\"\"Hyperbolic tangent, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as x.\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Tanh().symbolic_call(x)\n+    return backend.numpy.tanh(x)\n+\n+\n class Tensordot(Operation):\n     def __init__(self, axes=2):\n         super().__init__()\n\n@@ -754,14 +754,26 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arccos(x).shape, (None, 3))\n \n+    def test_arccosh(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.arccosh(x).shape, (None, 3))\n+\n     def test_arcsin(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arcsin(x).shape, (None, 3))\n \n+    def test_arcsinh(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.arcsinh(x).shape, (None, 3))\n+\n     def test_arctan(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arctan(x).shape, (None, 3))\n \n+    def test_arctanh(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.arctanh(x).shape, (None, 3))\n+\n     def test_argmax(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.argmax(x).shape, ())\n@@ -855,6 +867,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.cos(x).shape, (None, 3))\n \n+    def test_cosh(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.cosh(x).shape, (None, 3))\n+\n     def test_count_nonzero(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.count_nonzero(x).shape, ())\n@@ -1095,6 +1111,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.sin(x).shape, (None, 3))\n \n+    def test_sinh(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sinh(x).shape, (None, 3))\n+\n     def test_size(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.size(x).shape, ())\n@@ -1137,6 +1157,10 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.tan(x).shape, (None, 3))\n \n+    def test_tanh(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.tanh(x).shape, (None, 3))\n+\n     def test_tile(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.tile(x, [2]).shape, (None, 6))\n@@ -1227,14 +1251,26 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arccos(x).shape, (2, 3))\n \n+    def test_arccosh(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.arccosh(x).shape, (2, 3))\n+\n     def test_arcsin(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arcsin(x).shape, (2, 3))\n \n+    def test_arcsinh(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.arcsinh(x).shape, (2, 3))\n+\n     def test_arctan(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arctan(x).shape, (2, 3))\n \n+    def test_arctanh(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.arctanh(x).shape, (2, 3))\n+\n     def test_argmax(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.argmax(x).shape, ())\n@@ -1297,6 +1333,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.cos(x).shape, (2, 3))\n \n+    def test_cosh(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.cosh(x).shape, (2, 3))\n+\n     def test_count_nonzero(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.count_nonzero(x).shape, ())\n@@ -1532,6 +1572,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sin(x).shape, (2, 3))\n \n+    def test_sinh(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.sinh(x).shape, (2, 3))\n+\n     def test_size(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.size(x).shape, ())\n@@ -1579,6 +1623,10 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.tan(x).shape, (2, 3))\n \n+    def test_tanh(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.tanh(x).shape, (2, 3))\n+\n     def test_tile(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.tile(x, [2]).shape, (2, 6))\n@@ -2266,18 +2314,42 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.transpose(x, axes=(1, 0, 3, 2, 4)),\n         )\n \n-    def test_arcos(self):\n+    def test_arccos(self):\n         x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         self.assertAllClose(knp.arccos(x), np.arccos(x))\n \n         self.assertAllClose(knp.Arccos()(x), np.arccos(x))\n \n+    def test_arccosh(self):\n+        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n+        self.assertAllClose(knp.arccosh(x), np.arccosh(x))\n+\n+        self.assertAllClose(knp.Arccosh()(x), np.arccosh(x))\n+\n     def test_arcsin(self):\n         x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n         self.assertAllClose(knp.arcsin(x), np.arcsin(x))\n \n         self.assertAllClose(knp.Arcsin()(x), np.arcsin(x))\n \n+    def test_arcsinh(self):\n+        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n+        self.assertAllClose(knp.arcsinh(x), np.arcsinh(x))\n+\n+        self.assertAllClose(knp.Arcsinh()(x), np.arcsinh(x))\n+\n+    def test_arctan(self):\n+        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n+        self.assertAllClose(knp.arctan(x), np.arctan(x))\n+\n+        self.assertAllClose(knp.Arctan()(x), np.arctan(x))\n+\n+    def test_arctanh(self):\n+        x = np.array([[1, 0.5, -0.7], [0.9, 0.2, -1]])\n+        self.assertAllClose(knp.arctanh(x), np.arctanh(x))\n+\n+        self.assertAllClose(knp.Arctanh()(x), np.arctanh(x))\n+\n     def test_argmax(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.argmax(x), np.argmax(x))\n@@ -2433,6 +2505,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.cos(x), np.cos(x))\n         self.assertAllClose(knp.Cos()(x), np.cos(x))\n \n+    def test_cosh(self):\n+        x = np.array([[1, 2, 3], [3, 2, 1]])\n+        self.assertAllClose(knp.cosh(x), np.cosh(x))\n+        self.assertAllClose(knp.Cosh()(x), np.cosh(x))\n+\n     def test_count_nonzero(self):\n         x = np.array([[0, 2, 3], [3, 2, 0]])\n         self.assertAllClose(knp.count_nonzero(x), np.count_nonzero(x))\n@@ -2899,6 +2976,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.sin(x), np.sin(x))\n         self.assertAllClose(knp.Sin()(x), np.sin(x))\n \n+    def test_sinh(self):\n+        x = np.array([[1, -2, 3], [-3, 2, -1]])\n+        self.assertAllClose(knp.sinh(x), np.sinh(x))\n+        self.assertAllClose(knp.Sinh()(x), np.sinh(x))\n+\n     def test_size(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.size(x), np.size(x))\n@@ -2993,6 +3075,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.tan(x), np.tan(x))\n         self.assertAllClose(knp.Tan()(x), np.tan(x))\n \n+    def test_tanh(self):\n+        x = np.array([[1, -2, 3], [-3, 2, -1]])\n+        self.assertAllClose(knp.tanh(x), np.tanh(x))\n+        self.assertAllClose(knp.Tanh()(x), np.tanh(x))\n+\n     def test_tile(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.tile(x, [2, 3]), np.tile(x, [2, 3]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#974e7c742eab71eaedb919f5bf5b7223962d8905", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,8 @@ class LazyModule:\n             )\n \n     def __getattr__(self, name):\n+        if name == \"_api_export_path\":\n+            raise AttributeError\n         if self.module is None:\n             self.initialize()\n         return getattr(self.module, name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc4d170b99c1f8516ba362012c1dc5a5f8b9ba46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 209 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 209 | Churn Cumulative: 209 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,209 @@\n+\"\"\"\n+Title: Visualizing what convnets learn\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/05/29\n+Last modified: 2020/05/29\n+Description: Displaying the visual patterns that convnet filters respond to.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+In this example, we look into what sort of visual patterns image classification models\n+learn. We'll be using the `ResNet50V2` model, trained on the ImageNet dataset.\n+\n+Our process is simple: we will create input images that maximize the activation of\n+specific filters in a target layer (picked somewhere in the middle of the model: layer\n+`conv3_block4_out`). Such images represent a visualization of the\n+pattern that the filter responds to.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import keras_core as keras\n+\n+\n+import numpy as np\n+import tensorflow as tf\n+\n+# The dimensions of our input image\n+img_width = 180\n+img_height = 180\n+# Our target layer: we will visualize the filters from this layer.\n+# See `model.summary()` for list of layer names, if you want to change this.\n+layer_name = \"conv3_block4_out\"\n+\n+\"\"\"\n+## Build a feature extraction model\n+\"\"\"\n+\n+# Build a ResNet50V2 model loaded with pre-trained ImageNet weights\n+model = keras.applications.ResNet50V2(weights=\"imagenet\", include_top=False)\n+\n+# Set up a model that returns the activation values for our target layer\n+layer = model.get_layer(name=layer_name)\n+feature_extractor = keras.Model(inputs=model.inputs, outputs=layer.output)\n+\n+\"\"\"\n+## Set up the gradient ascent process\n+\n+The \"loss\" we will maximize is simply the mean of the activation of a specific filter in\n+our target layer. To avoid border effects, we exclude border pixels.\n+\"\"\"\n+\n+\n+def compute_loss(input_image, filter_index):\n+    activation = feature_extractor(input_image)\n+    # We avoid border artifacts by only involving non-border pixels in the loss.\n+    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n+    return tf.reduce_mean(filter_activation)\n+\n+\n+\"\"\"\n+Our gradient ascent function simply computes the gradients of the loss above\n+with regard to the input image, and update the update image so as to move it\n+towards a state that will activate the target filter more strongly.\n+\"\"\"\n+\n+\n+@tf.function\n+def gradient_ascent_step(img, filter_index, learning_rate):\n+    with tf.GradientTape() as tape:\n+        tape.watch(img)\n+        loss = compute_loss(img, filter_index)\n+    # Compute gradients.\n+    grads = tape.gradient(loss, img)\n+    # Normalize gradients.\n+    grads = tf.math.l2_normalize(grads)\n+    img += learning_rate * grads\n+    return loss, img\n+\n+\n+\"\"\"\n+## Set up the end-to-end filter visualization loop\n+\n+Our process is as follow:\n+\n+- Start from a random image that is close to \"all gray\" (i.e. visually netural)\n+- Repeatedly apply the gradient ascent step function defined above\n+- Convert the resulting input image back to a displayable form, by normalizing it,\n+center-cropping it, and restricting it to the [0, 255] range.\n+\"\"\"\n+\n+\n+def initialize_image():\n+    # We start from a gray image with some random noise\n+    img = tf.random.uniform((1, img_width, img_height, 3))\n+    # ResNet50V2 expects inputs in the range [-1, +1].\n+    # Here we scale our random inputs to [-0.125, +0.125]\n+    return (img - 0.5) * 0.25\n+\n+\n+def visualize_filter(filter_index):\n+    # We run gradient ascent for 20 steps\n+    iterations = 30\n+    learning_rate = 10.0\n+    img = initialize_image()\n+    for iteration in range(iterations):\n+        loss, img = gradient_ascent_step(img, filter_index, learning_rate)\n+\n+    # Decode the resulting input image\n+    img = deprocess_image(img[0].numpy())\n+    return loss, img\n+\n+\n+def deprocess_image(img):\n+    # Normalize array: center on 0., ensure variance is 0.15\n+    img -= img.mean()\n+    img /= img.std() + 1e-5\n+    img *= 0.15\n+\n+    # Center crop\n+    img = img[25:-25, 25:-25, :]\n+\n+    # Clip to [0, 1]\n+    img += 0.5\n+    img = np.clip(img, 0, 1)\n+\n+    # Convert to RGB array\n+    img *= 255\n+    img = np.clip(img, 0, 255).astype(\"uint8\")\n+    return img\n+\n+\n+\"\"\"\n+Let's try it out with filter 0 in the target layer:\n+\"\"\"\n+\n+from IPython.display import Image, display\n+\n+loss, img = visualize_filter(0)\n+keras.utils.save_img(\"0.png\", img)\n+\n+\"\"\"\n+This is what an input that maximizes the response of filter 0 in the target layer would\n+look like:\n+\"\"\"\n+\n+display(Image(\"0.png\"))\n+\n+\"\"\"\n+## Visualize the first 64 filters in the target layer\n+\n+Now, let's make a 8x8 grid of the first 64 filters\n+in the target layer to get of feel for the range\n+of different visual patterns that the model has learned.\n+\"\"\"\n+\n+# Compute image inputs that maximize per-filter activations\n+# for the first 64 filters of our target layer\n+all_imgs = []\n+for filter_index in range(64):\n+    print(\"Processing filter %d\" % (filter_index,))\n+    loss, img = visualize_filter(filter_index)\n+    all_imgs.append(img)\n+\n+# Build a black picture with enough space for\n+# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n+margin = 5\n+n = 8\n+cropped_width = img_width - 25 * 2\n+cropped_height = img_height - 25 * 2\n+width = n * cropped_width + (n - 1) * margin\n+height = n * cropped_height + (n - 1) * margin\n+stitched_filters = np.zeros((width, height, 3))\n+\n+# Fill the picture with our saved filters\n+for i in range(n):\n+    for j in range(n):\n+        img = all_imgs[i * n + j]\n+        stitched_filters[\n+            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n+            (cropped_height + margin) * j : (cropped_height + margin) * j\n+            + cropped_height,\n+            :,\n+        ] = img\n+keras.utils.save_img(\"stiched_filters.png\", stitched_filters)\n+\n+from IPython.display import Image, display\n+\n+display(Image(\"stiched_filters.png\"))\n+\n+\"\"\"\n+Image classification models see the world by decomposing their inputs over a \"vector\n+basis\" of texture filters such as these.\n+\n+See also\n+[this old blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n+for analysis and interpretation.\n+\n+Example available on HuggingFace.\n+\n+[![Generic badge](https://img.shields.io/badge/🤗%20Spaces-What%20Convnets%20Learn-black.svg)](https://huggingface.co/spaces/keras-io/what-convnets-learn)\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
