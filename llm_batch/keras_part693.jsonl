{"custom_id": "keras#21c25fd38023a3783950c5577383ffe51a62f650", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 650 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,7 +89,11 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         optimizer = adam.Adam(mesh=self.mesh)\n \n         variable_init_value = tf.ones(shape=(), dtype=tf.float32)\n-        model_variable = dtensor.DVariable(variable_init_value, trainable=True)\n+        model_variable = dtensor.DVariable(\n+            variable_init_value,\n+            trainable=True,\n+            layout=dtensor.Layout.replicated(self.mesh, rank=0),\n+        )\n         grads = tf.ones_like(variable_init_value)\n \n         grad_and_var = zip([grads], [model_variable])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#24fa8f2e46578d1dc43d5500827c22e5e53e93e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2600 | Contributors (this commit): 17 | Commits (past 90d): 75 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -1353,7 +1353,7 @@ def get_shapes_dict(call_spec):\n     \"\"\"\n     shapes_dict = {}\n     for k, v in call_spec.tensor_arguments_dict.items():\n-        if k == \"mask\" or k.startswith(\"mask_\"):\n+        if k == \"mask\" or k.endswith(\"_mask\"):\n             # Do not include mask tensors in shapes dict\n             continue\n         if k == \"kwargs\" or k == \"args\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ca2b045cc150f5957c83502eae274e86c551c44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 495 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -20,6 +20,9 @@ class ArrayLike:\n         return np.array(self.values)\n \n \n+@pytest.mark.skipif(\n+    backend.backend() == \"numpy\", reason=\"Broken with NumPy backend.\"\n+)\n class HashingTest(testing.TestCase, parameterized.TestCase):\n     def test_config(self):\n         layer = layers.Hashing(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96c9a4d8c346cd3652fd66921a3728a74292b8ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 815 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@\n import datetime\n import io\n import json\n-import os\n import tempfile\n import warnings\n import zipfile\n@@ -85,8 +84,8 @@ def save_model(model, filepath, weights_format=\"h5\"):\n         }\n     )\n     if file_utils.is_remote_path(filepath):\n-        # Remote path. Zip to local drive and copy to remote\n-        zip_filepath = os.path.join(get_temp_dir(), \"tmp_model.keras\")\n+        # Remote path. Zip to local memory byte io and copy to remote\n+        zip_filepath = io.BytesIO()\n     else:\n         zip_filepath = filepath\n \n@@ -122,10 +121,8 @@ def save_model(model, filepath, weights_format=\"h5\"):\n         asset_store.close()\n \n     if file_utils.is_remote_path(filepath):\n-        # Using gfile context manager doesn't close zip file when\n-        # writing to GCS. Hence writing to local and copying to filepath.\n-        file_utils.copy(zip_filepath, filepath, overwrite=True)\n-        os.remove(zip_filepath)\n+        with file_utils.File(filepath, \"wb\") as f:\n+            f.write(zip_filepath.getvalue())\n \n \n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a49bb48e02044f8bb51ff664d947067e5053a1c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 823 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 11 | Methods Changed: 18 | Complexity Δ (Sum/Max): 134/103 | Churn Δ: 841 | Churn Cumulative: 1450 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.4241935483870968\n\nDIFF:\n@@ -1,3 +1,9 @@\n+import random\n+import time\n+import warnings\n+\n+import numpy as np\n+\n from keras_core.api_export import keras_core_export\n from keras_core.utils.module_utils import tensorflow as tf\n \n@@ -6,45 +12,510 @@ from keras_core.utils.module_utils import tensorflow as tf\n def split_dataset(\n     dataset, left_size=None, right_size=None, shuffle=False, seed=None\n ):\n-    \"\"\"Splits a dataset into a left half and a right half (e.g. train / test).\n+    \"\"\"Split a dataset into a left half and a right half (e.g. train / test).\n \n     Args:\n-        dataset: A `tf.data.Dataset` object, or a list/tuple of arrays with the\n-            same length.\n+        dataset:\n+            A `tf.data.Dataset or torch.utils.data.Dataset`  object,\n+            or a list/tuple of arrays with the same length.\n         left_size: If float (in the range `[0, 1]`), it signifies\n-            the fraction of the data to pack in the left dataset.\n-            If integer, it signifies the number of samples to pack\n-            in the left dataset. If `None`, it defaults to the complement\n-            to `right_size`.\n+            the fraction of the data to pack in the left dataset. If integer, it\n+            signifies the number of samples to pack in the left dataset. If\n+            `None`, it uses the complement to `right_size`. Defaults to `None`.\n         right_size: If float (in the range `[0, 1]`), it signifies\n             the fraction of the data to pack in the right dataset.\n             If integer, it signifies the number of samples to pack\n-            in the right dataset. If `None`, it defaults to the complement\n-            to `left_size`.\n+            in the right dataset.\n+            If `None`, it uses the complement to `left_size`.\n+            Defaults to `None`.\n         shuffle: Boolean, whether to shuffle the data before splitting it.\n         seed: A random seed for shuffling.\n \n     Returns:\n-        A tuple of two `tf.data.Dataset` objects: the left and right splits.\n+        A tuple of two `tf.data.Dataset` objects:\n+        the left and right splits.\n \n     Example:\n \n     >>> data = np.random.random(size=(1000, 4))\n-    >>> left_ds, right_ds = split_dataset(data, left_size=0.8)\n+    >>> left_ds, right_ds = keras_core.utils.split_dataset(data, left_size=0.8)\n     >>> int(left_ds.cardinality())\n     800\n     >>> int(right_ds.cardinality())\n     200\n+\n     \"\"\"\n-    # TODO: long-term, port implementation.\n-    return tf.keras.utils.split_dataset(\n-        dataset,\n-        left_size=left_size,\n-        right_size=right_size,\n-        shuffle=shuffle,\n-        seed=seed,\n+    dataset_type_spec = _get_type_spec(dataset)\n+\n+    if dataset_type_spec is None:\n+        raise TypeError(\n+            \"The `dataset` argument must be either\"\n+            \"a `tf.data.Dataset` or `torch.utils.data.Dataset`\"\n+            \"object or a list/tuple of arrays. \"\n+            f\"Received: dataset={dataset} of type {type(dataset)}\"\n         )\n \n+    if right_size is None and left_size is None:\n+        raise ValueError(\n+            \"At least one of the `left_size` or `right_size` \"\n+            \"must be specified. Received: left_size=None and \"\n+            \"right_size=None\"\n+        )\n+\n+    dataset_as_list = _convert_dataset_to_list(dataset, dataset_type_spec)\n+\n+    if shuffle:\n+        if seed is None:\n+            seed = random.randint(0, int(1e6))\n+        random.seed(seed)\n+        random.shuffle(dataset_as_list)\n+\n+    total_length = len(dataset_as_list)\n+\n+    left_size, right_size = _rescale_dataset_split_sizes(\n+        left_size, right_size, total_length\n+    )\n+    left_split = list(dataset_as_list[:left_size])\n+    right_split = list(dataset_as_list[-right_size:])\n+\n+    left_split = _restore_dataset_from_list(\n+        left_split, dataset_type_spec, dataset\n+    )\n+    right_split = _restore_dataset_from_list(\n+        right_split, dataset_type_spec, dataset\n+    )\n+\n+    left_split = tf.data.Dataset.from_tensor_slices(left_split)\n+    right_split = tf.data.Dataset.from_tensor_slices(right_split)\n+\n+    # apply batching to the splits if the dataset is batched\n+    if dataset_type_spec is tf.data.Dataset and is_batched(dataset):\n+        batch_size = get_batch_size(dataset)\n+        if batch_size is not None:\n+            left_split = left_split.batch(batch_size)\n+            right_split = right_split.batch(batch_size)\n+\n+    left_split = left_split.prefetch(tf.data.AUTOTUNE)\n+    right_split = right_split.prefetch(tf.data.AUTOTUNE)\n+    return left_split, right_split\n+\n+\n+def _convert_dataset_to_list(\n+    dataset,\n+    dataset_type_spec,\n+    data_size_warning_flag=True,\n+    ensure_shape_similarity=True,\n+):\n+    \"\"\"Convert `tf.data.Dataset` or `torch.utils.data.Dataset` object\n+        or list/tuple of NumPy arrays to a list.\n+\n+    Args:\n+        dataset :\n+            A `tf.data.Dataset` or `torch.utils.data.Dataset` object\n+            or a list/tuple of arrays.\n+        dataset_type_spec : the type of the dataset\n+        data_size_warning_flag (bool, optional): If set to True, a warning will\n+            be issued if the dataset takes longer than 10 seconds to iterate.\n+            Defaults to `True`.\n+        ensure_shape_similarity (bool, optional): If set to True, the shape of\n+            the first sample will be used to validate the shape of rest of the\n+            samples. Defaults to `True`.\n+\n+    Returns:\n+        List: A list of tuples/NumPy arrays.\n+    \"\"\"\n+    dataset_iterator = _get_data_iterator_from_dataset(\n+        dataset, dataset_type_spec\n+    )\n+    dataset_as_list = []\n+\n+    start_time = time.time()\n+    for sample in _get_next_sample(\n+        dataset_iterator,\n+        ensure_shape_similarity,\n+        data_size_warning_flag,\n+        start_time,\n+    ):\n+        if dataset_type_spec in [tuple, list]:\n+            # The try-except here is for NumPy 1.24 compatibility, see:\n+            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n+            try:\n+                arr = np.array(sample)\n+            except ValueError:\n+                arr = np.array(sample, dtype=object)\n+            dataset_as_list.append(arr)\n+        else:\n+            dataset_as_list.append(sample)\n+\n+    return dataset_as_list\n+\n+\n+def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n+    \"\"\"Get the iterator from a dataset.\n+\n+    Args:\n+        dataset :\n+            A `tf.data.Dataset` or `torch.utils.data.Dataset` object\n+            or a list/tuple of arrays.\n+        dataset_type_spec :\n+             the type of the dataset\n+\n+    Raises:\n+        ValueError:\n+            - If the dataset is empty.\n+            - If the dataset is not a `tf.data.Dataset` object\n+            or a list/tuple of arrays.\n+            - If the dataset is a list/tuple of arrays and the\n+            length of the list/tuple is not equal to the number\n+\n+    Returns:\n+        iterator: An `iterator` object.\n+    \"\"\"\n+    if dataset_type_spec == list:\n+        if len(dataset) == 0:\n+            raise ValueError(\n+                \"Received an empty list dataset. \"\n+                \"Please provide a non-empty list of arrays.\"\n+            )\n+\n+        if _get_type_spec(dataset[0]) is np.ndarray:\n+            expected_shape = dataset[0].shape\n+            for i, element in enumerate(dataset):\n+                if np.array(element).shape[0] != expected_shape[0]:\n+                    raise ValueError(\n+                        \"Received a list of NumPy arrays with different \"\n+                        f\"lengths. Mismatch found at index {i}, \"\n+                        f\"Expected shape={expected_shape} \"\n+                        f\"Received shape={np.array(element).shape}.\"\n+                        \"Please provide a list of NumPy arrays with \"\n+                        \"the same length.\"\n+                    )\n+        else:\n+            raise ValueError(\n+                \"Expected a list of `numpy.ndarray` objects,\"\n+                f\"Received: {type(dataset[0])}\"\n+            )\n+\n+        return iter(zip(*dataset))\n+    elif dataset_type_spec == tuple:\n+        if len(dataset) == 0:\n+            raise ValueError(\n+                \"Received an empty list dataset.\"\n+                \"Please provide a non-empty tuple of arrays.\"\n+            )\n+\n+        if _get_type_spec(dataset[0]) is np.ndarray:\n+            expected_shape = dataset[0].shape\n+            for i, element in enumerate(dataset):\n+                if np.array(element).shape[0] != expected_shape[0]:\n+                    raise ValueError(\n+                        \"Received a tuple of NumPy arrays with different \"\n+                        f\"lengths. Mismatch found at index {i}, \"\n+                        f\"Expected shape={expected_shape} \"\n+                        f\"Received shape={np.array(element).shape}.\"\n+                        \"Please provide a tuple of NumPy arrays with \"\n+                        \"the same length.\"\n+                    )\n+        else:\n+            raise ValueError(\n+                \"Expected a tuple of `numpy.ndarray` objects, \"\n+                f\"Received: {type(dataset[0])}\"\n+            )\n+\n+        return iter(zip(*dataset))\n+    elif dataset_type_spec == tf.data.Dataset:\n+        if is_batched(dataset):\n+            dataset = dataset.unbatch()\n+        return iter(dataset)\n+\n+    # torch dataset iterator might be required to change\n+    elif is_torch_dataset(dataset):\n+        return iter(dataset)\n+\n+    elif dataset_type_spec == np.ndarray:\n+        return iter(dataset)\n+\n+\n+def _get_next_sample(\n+    dataset_iterator,\n+    ensure_shape_similarity,\n+    data_size_warning_flag,\n+    start_time,\n+):\n+    \"\"\" \"Yield data samples from the `dataset_iterator`.\n+\n+    Args:\n+        dataset_iterator : An `iterator` object.\n+        ensure_shape_similarity (bool, optional): If set to True, the shape of\n+            the first sample will be used to validate the shape of rest of the\n+            samples. Defaults to `True`.\n+        data_size_warning_flag (bool, optional): If set to True, a warning will\n+            be issued if the dataset takes longer than 10 seconds to iterate.\n+            Defaults to `True`.\n+        start_time (float): the start time of the dataset iteration. this is\n+            used only if `data_size_warning_flag` is set to true.\n+\n+    Raises:\n+        ValueError:\n+            - If the dataset is empty.\n+            - If `ensure_shape_similarity` is set to True and the\n+            shape of the first sample is not equal to the shape of\n+            atleast one of the rest of the samples.\n+\n+    Yields:\n+        data_sample: A tuple/list of numpy arrays.\n+    \"\"\"\n+    try:\n+        dataset_iterator = iter(dataset_iterator)\n+        first_sample = next(dataset_iterator)\n+        if isinstance(first_sample, (tf.Tensor, np.ndarray)) or is_torch_tensor(\n+            first_sample\n+        ):\n+            first_sample_shape = np.array(first_sample).shape\n+        else:\n+            first_sample_shape = None\n+            ensure_shape_similarity = False\n+        yield first_sample\n+    except StopIteration:\n+        raise ValueError(\n+            \"Received an empty Dataset. `dataset` must \"\n+            \"be a non-empty list/tuple of `numpy.ndarray` objects \"\n+            \"or `tf.data.Dataset` objects.\"\n+        )\n+\n+    for i, sample in enumerate(dataset_iterator):\n+        if ensure_shape_similarity:\n+            if first_sample_shape != np.array(sample).shape:\n+                raise ValueError(\n+                    \"All `dataset` samples must have same shape, \"\n+                    f\"Expected shape: {np.array(first_sample).shape} \"\n+                    f\"Received shape: {np.array(sample).shape} at index \"\n+                    f\"{i}.\"\n+                )\n+        if data_size_warning_flag:\n+            if i % 10 == 0:\n+                cur_time = time.time()\n+                # warns user if the dataset is too large to iterate within 10s\n+                if int(cur_time - start_time) > 10 and data_size_warning_flag:\n+                    warnings.warn(\n+                        \"The dataset is taking longer than 10 seconds to \"\n+                        \"iterate over. This may be due to the size of the \"\n+                        \"dataset. Keep in mind that the `split_dataset` \"\n+                        \"utility is only for small in-memory dataset \"\n+                        \"(e.g. < 10,000 samples).\",\n+                        category=ResourceWarning,\n+                        source=\"split_dataset\",\n+                    )\n+                    data_size_warning_flag = False\n+        yield sample\n+\n+\n+def is_torch_tensor(value):\n+    if hasattr(value, \"__class__\"):\n+        for parent in value.__class__.__mro__:\n+            if parent.__name__ == \"Tensor\" and str(parent.__module__).endswith(\n+                \"torch\"\n+            ):\n+                return True\n+    return False\n+\n+\n+def is_torch_dataset(dataset):\n+    if hasattr(dataset, \"__class__\"):\n+        for parent in dataset.__class__.__mro__:\n+            if parent.__name__ == \"Dataset\" and str(\n+                parent.__module__\n+            ).startswith(\"torch.utils.data\"):\n+                return True\n+    return False\n+\n+\n+def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n+    \"\"\"Rescale the dataset split sizes.\n+\n+    We want to ensure that the sum of\n+    the split sizes is equal to the total length of the dataset.\n+\n+    Args:\n+        left_size : The size of the left dataset split.\n+        right_size : The size of the right dataset split.\n+        total_length : The total length of the dataset.\n+\n+    Raises:\n+        TypeError: - If `left_size` or `right_size` is not an integer or float.\n+        ValueError: - If `left_size` or `right_size` is negative or greater\n+                    than 1 or greater than `total_length`.\n+\n+    Returns:\n+        tuple: A tuple of rescaled left_size and right_size\n+    \"\"\"\n+    left_size_type = type(left_size)\n+    right_size_type = type(right_size)\n+\n+    # check both left_size and right_size are integers or floats\n+    if (left_size is not None and left_size_type not in [int, float]) and (\n+        right_size is not None and right_size_type not in [int, float]\n+    ):\n+        raise TypeError(\n+            \"Invalid `left_size` and `right_size` Types. Expected: \"\n+            \"integer or float or None, Received: type(left_size)=\"\n+            f\"{left_size_type} and type(right_size)={right_size_type}\"\n+        )\n+\n+    # check left_size is a integer or float\n+    if left_size is not None and left_size_type not in [int, float]:\n+        raise TypeError(\n+            \"Invalid `left_size` Type. Expected: int or float or None, \"\n+            f\"Received: type(left_size)={left_size_type}.  \"\n+        )\n+\n+    # check right_size is a integer or float\n+    if right_size is not None and right_size_type not in [int, float]:\n+        raise TypeError(\n+            \"Invalid `right_size` Type. \"\n+            \"Expected: int or float or None,\"\n+            f\"Received: type(right_size)={right_size_type}.\"\n+        )\n+\n+    # check left_size and right_size are non-zero\n+    if left_size == 0 and right_size == 0:\n+        raise ValueError(\n+            \"Both `left_size` and `right_size` are zero. \"\n+            \"At least one of the split sizes must be non-zero.\"\n+        )\n+\n+    # check left_size is non-negative and less than 1 and less than total_length\n+    if (\n+        left_size_type == int\n+        and (left_size <= 0 or left_size >= total_length)\n+        or left_size_type == float\n+        and (left_size <= 0 or left_size >= 1)\n+    ):\n+        raise ValueError(\n+            \"`left_size` should be either a positive integer \"\n+            f\"smaller than {total_length}, or a float \"\n+            \"within the range `[0, 1]`. Received: left_size=\"\n+            f\"{left_size}\"\n+        )\n+\n+    # check right_size is non-negative and less than 1 and less than\n+    # total_length\n+    if (\n+        right_size_type == int\n+        and (right_size <= 0 or right_size >= total_length)\n+        or right_size_type == float\n+        and (right_size <= 0 or right_size >= 1)\n+    ):\n+        raise ValueError(\n+            \"`right_size` should be either a positive integer \"\n+            f\"and smaller than {total_length} or a float \"\n+            \"within the range `[0, 1]`. Received: right_size=\"\n+            f\"{right_size}\"\n+        )\n+\n+    # check sum of left_size and right_size is less than or equal to\n+    # total_length\n+    if (\n+        right_size_type == left_size_type == float\n+        and right_size + left_size > 1\n+    ):\n+        raise ValueError(\n+            \"The sum of `left_size` and `right_size` is greater \"\n+            \"than 1. It must be less than or equal to 1.\"\n+        )\n+\n+    if left_size_type == float:\n+        left_size = round(left_size * total_length)\n+    elif left_size_type == int:\n+        left_size = float(left_size)\n+\n+    if right_size_type == float:\n+        right_size = round(right_size * total_length)\n+    elif right_size_type == int:\n+        right_size = float(right_size)\n+\n+    if left_size is None:\n+        left_size = total_length - right_size\n+    elif right_size is None:\n+        right_size = total_length - left_size\n+\n+    if left_size + right_size > total_length:\n+        raise ValueError(\n+            \"The sum of `left_size` and `right_size` should \"\n+            \"be smaller than the {total_length}. \"\n+            f\"Received: left_size + right_size = {left_size+right_size}\"\n+            f\"and total_length = {total_length}\"\n+        )\n+\n+    for split, side in [(left_size, \"left\"), (right_size, \"right\")]:\n+        if split == 0:\n+            raise ValueError(\n+                f\"With `dataset` of length={total_length}, `left_size`=\"\n+                f\"{left_size} and `right_size`={right_size}.\"\n+                f\"Resulting {side} side dataset split will be empty. \"\n+                \"Adjust any of the aforementioned parameters\"\n+            )\n+\n+    left_size, right_size = int(left_size), int(right_size)\n+    return left_size, right_size\n+\n+\n+def _restore_dataset_from_list(\n+    dataset_as_list, dataset_type_spec, original_dataset\n+):\n+    \"\"\"Restore the dataset from the list of arrays.\"\"\"\n+    if dataset_type_spec in [tuple, list]:\n+        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n+    elif dataset_type_spec == tf.data.Dataset:\n+        if isinstance(original_dataset.element_spec, dict):\n+            restored_dataset = {}\n+            for d in dataset_as_list:\n+                for k, v in d.items():\n+                    if k not in restored_dataset:\n+                        restored_dataset[k] = [v]\n+                    else:\n+                        restored_dataset[k].append(v)\n+            return restored_dataset\n+        else:\n+            return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n+\n+    elif is_torch_dataset(original_dataset):\n+        return tuple(np.array(sample) for sample in zip(*dataset_as_list))\n+    return dataset_as_list\n+\n+\n+def is_batched(dataset):\n+    \"\"\" \"Check if the `tf.data.Dataset` is batched.\"\"\"\n+    return hasattr(dataset, \"_batch_size\")\n+\n+\n+def get_batch_size(dataset):\n+    \"\"\"Get the batch size of the dataset.\"\"\"\n+    if is_batched(dataset):\n+        return dataset._batch_size\n+    else:\n+        return None\n+\n+\n+def _get_type_spec(dataset):\n+    \"\"\"Get the type spec of the dataset.\"\"\"\n+    if isinstance(dataset, tuple):\n+        return tuple\n+    elif isinstance(dataset, list):\n+        return list\n+    elif isinstance(dataset, np.ndarray):\n+        return np.ndarray\n+    elif isinstance(dataset, dict):\n+        return dict\n+    elif isinstance(dataset, tf.data.Dataset):\n+        return tf.data.Dataset\n+    elif is_torch_dataset(dataset):\n+        from torch.utils.data import Dataset as torchDataset\n+\n+        return torchDataset\n+    else:\n+        return None\n+\n \n @keras_core_export(\n     [\n\n@@ -0,0 +1,334 @@\n+import numpy as np\n+\n+from keras_core.testing import test_case\n+from keras_core.utils.dataset_utils import split_dataset\n+from keras_core.utils.module_utils import tensorflow as tf\n+\n+\n+class DatasetUtilsTest(test_case.TestCase):\n+    def test_split_dataset_list(self):\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = [\n+            np.random.sample((n_sample, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        ]\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = [\n+            np.random.sample((n_sample, 100, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        ]\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = [\n+            np.random.sample((n_sample, 10, 10, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        ]\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = [\n+            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        ]\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape,\n+            (100, 10, 30, n_cols),\n+        )\n+\n+    def test_split_dataset_tuple(self):\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = (\n+            np.random.sample((n_sample, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = (\n+            np.random.sample((n_sample, 100, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = (\n+            np.random.sample((n_sample, 10, 10, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        dataset = (\n+            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        dataset_left, dataset_right = split_dataset(\n+            dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape,\n+            (100, 10, 30, n_cols),\n+        )\n+\n+    def test_split_dataset_tensorflow(self):\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        dataset_left, dataset_right = split_dataset(\n+            tf_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, 100, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        dataset_left, dataset_right = split_dataset(\n+            tf_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, 10, 10, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        dataset_left, dataset_right = split_dataset(\n+            tf_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        tf_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n+        dataset_left, dataset_right = split_dataset(\n+            tf_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            int(dataset_left.cardinality()), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            int(dataset_right.cardinality()), int(n_sample * right_size)\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape,\n+            (100, 10, 30, n_cols),\n+        )\n+\n+    def test_split_dataset_torch(self):\n+        # sample torch dataset class\n+        from torch.utils.data import Dataset as torchDataset\n+\n+        class Dataset(torchDataset):\n+            \"Characterizes a dataset for PyTorch\"\n+\n+            def __init__(self, x, y):\n+                \"Initialization\"\n+                self.x = x\n+                self.y = y\n+\n+            def __len__(self):\n+                \"Denotes the total number of samples\"\n+                return len(self.x)\n+\n+            def __getitem__(self, index):\n+                \"Generates one sample of data\"\n+                return self.x[index], self.y[index]\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        torch_dataset = Dataset(features, labels)\n+        dataset_left, dataset_right = split_dataset(\n+            torch_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_left]), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_right]),\n+            int(n_sample * right_size),\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (n_cols,)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, 100, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        torch_dataset = Dataset(features, labels)\n+        dataset_left, dataset_right = split_dataset(\n+            torch_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_left]), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_right]),\n+            int(n_sample * right_size),\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (100, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, 10, 10, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        torch_dataset = Dataset(features, labels)\n+        dataset_left, dataset_right = split_dataset(\n+            torch_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_left]), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_right]),\n+            int(n_sample * right_size),\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape, (10, 10, n_cols)\n+        )\n+\n+        n_sample, n_cols, n_pred, left_size, right_size = 100, 2, 1, 0.2, 0.8\n+        features, labels = (\n+            np.random.sample((n_sample, 100, 10, 30, n_cols)),\n+            np.random.sample((n_sample, n_pred)),\n+        )\n+        torch_dataset = Dataset(features, labels)\n+        dataset_left, dataset_right = split_dataset(\n+            torch_dataset, left_size=left_size, right_size=right_size\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_left]), int(n_sample * left_size)\n+        )\n+        self.assertEqual(\n+            len([sample for sample in dataset_right]),\n+            int(n_sample * right_size),\n+        )\n+        self.assertEqual(\n+            [sample for sample in dataset_right][0][0].shape,\n+            (100, 10, 30, n_cols),\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c428eea924ac412e6bdf7f433720ab23e69b299", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 62 | Files Changed: 2 | Hunks: 31 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 99 | Churn Cumulative: 1308 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,19 +10,19 @@ import keras_core as keras\n keras.config.disable_traceback_filtering()\n \n inputs = layers.Input((100,))\n-x = layers.Dense(1024, activation=\"relu\")(inputs)\n+x = layers.Dense(512, activation=\"relu\")(inputs)\n residual = x\n-x = layers.Dense(1024, activation=\"relu\")(x)\n-x = layers.Dense(1024, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n x += residual\n-x = layers.Dense(1024, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n residual = x\n-x = layers.Dense(1024, activation=\"relu\")(x)\n-x = layers.Dense(1024, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n x += residual\n residual = x\n-x = layers.Dense(1024, activation=\"relu\")(x)\n-x = layers.Dense(1024, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n+x = layers.Dense(512, activation=\"relu\")(x)\n x += residual\n outputs = layers.Dense(16)(x)\n model = Model(inputs, outputs)\n@@ -43,12 +43,12 @@ model.compile(\n     ],\n )\n \n-print(\"\\nTrain model\")\n-history = model.fit(\n-    x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n-)\n-print(\"\\nHistory:\")\n-print(history.history)\n+# print(\"\\nTrain model\")\n+# history = model.fit(\n+#     x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n+# )\n+# print(\"\\nHistory:\")\n+# print(history.history)\n \n print(\"\\nEvaluate model\")\n scores = model.evaluate(x, y, return_dict=True)\n\n@@ -12,21 +12,22 @@ from keras_core.utils.module_utils import tensorflow as tf\n def split_dataset(\n     dataset, left_size=None, right_size=None, shuffle=False, seed=None\n ):\n-    \"\"\"Split a dataset into a left half and a right half (e.g. train / test).\n+    \"\"\"Splits a dataset into a left half and a right half (e.g. train / test).\n \n     Args:\n         dataset:\n-            A `tf.data.Dataset or torch.utils.data.Dataset`  object,\n+            A `tf.data.Dataset`, a `torch.utils.data.Dataset` object,\n             or a list/tuple of arrays with the same length.\n         left_size: If float (in the range `[0, 1]`), it signifies\n             the fraction of the data to pack in the left dataset. If integer, it\n             signifies the number of samples to pack in the left dataset. If\n-            `None`, it uses the complement to `right_size`. Defaults to `None`.\n+            `None`, defaults to the complement to `right_size`.\n+            Defaults to `None`.\n         right_size: If float (in the range `[0, 1]`), it signifies\n             the fraction of the data to pack in the right dataset.\n             If integer, it signifies the number of samples to pack\n             in the right dataset.\n-            If `None`, it uses the complement to `left_size`.\n+            If `None`, defaults to the complement to `left_size`.\n             Defaults to `None`.\n         shuffle: Boolean, whether to shuffle the data before splitting it.\n         seed: A random seed for shuffling.\n@@ -43,15 +44,14 @@ def split_dataset(\n     800\n     >>> int(right_ds.cardinality())\n     200\n-\n     \"\"\"\n     dataset_type_spec = _get_type_spec(dataset)\n \n     if dataset_type_spec is None:\n         raise TypeError(\n             \"The `dataset` argument must be either\"\n-            \"a `tf.data.Dataset` or `torch.utils.data.Dataset`\"\n-            \"object or a list/tuple of arrays. \"\n+            \"a `tf.data.Dataset`, a `torch.utils.data.Dataset`\"\n+            \"object, or a list/tuple of arrays. \"\n             f\"Received: dataset={dataset} of type {type(dataset)}\"\n         )\n \n@@ -106,23 +106,21 @@ def _convert_dataset_to_list(\n     data_size_warning_flag=True,\n     ensure_shape_similarity=True,\n ):\n-    \"\"\"Convert `tf.data.Dataset` or `torch.utils.data.Dataset` object\n-        or list/tuple of NumPy arrays to a list.\n+    \"\"\"Convert `dataset` object to a list of samples.\n \n     Args:\n-        dataset :\n-            A `tf.data.Dataset` or `torch.utils.data.Dataset` object\n+        dataset: A `tf.data.Dataset`, a `torch.utils.data.Dataset` object,\n             or a list/tuple of arrays.\n-        dataset_type_spec : the type of the dataset\n-        data_size_warning_flag (bool, optional): If set to True, a warning will\n+        dataset_type_spec: the type of the dataset.\n+        data_size_warning_flag: If set to `True`, a warning will\n             be issued if the dataset takes longer than 10 seconds to iterate.\n             Defaults to `True`.\n-        ensure_shape_similarity (bool, optional): If set to True, the shape of\n+        ensure_shape_similarity: If set to `True`, the shape of\n             the first sample will be used to validate the shape of rest of the\n             samples. Defaults to `True`.\n \n     Returns:\n-        List: A list of tuples/NumPy arrays.\n+        List: A list of samples.\n     \"\"\"\n     dataset_iterator = _get_data_iterator_from_dataset(\n         dataset, dataset_type_spec\n@@ -154,19 +152,9 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n     \"\"\"Get the iterator from a dataset.\n \n     Args:\n-        dataset :\n-            A `tf.data.Dataset` or `torch.utils.data.Dataset` object\n+        dataset: A `tf.data.Dataset`, a `torch.utils.data.Dataset` object,\n             or a list/tuple of arrays.\n-        dataset_type_spec :\n-             the type of the dataset\n-\n-    Raises:\n-        ValueError:\n-            - If the dataset is empty.\n-            - If the dataset is not a `tf.data.Dataset` object\n-            or a list/tuple of arrays.\n-            - If the dataset is a list/tuple of arrays and the\n-            length of the list/tuple is not equal to the number\n+        dataset_type_spec: The type of the dataset.\n \n     Returns:\n         iterator: An `iterator` object.\n@@ -227,13 +215,12 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n         if is_batched(dataset):\n             dataset = dataset.unbatch()\n         return iter(dataset)\n-\n     # torch dataset iterator might be required to change\n     elif is_torch_dataset(dataset):\n         return iter(dataset)\n-\n     elif dataset_type_spec == np.ndarray:\n         return iter(dataset)\n+    raise ValueError(f\"Invalid dataset_type_spec: {dataset_type_spec}\")\n \n \n def _get_next_sample(\n@@ -242,28 +229,21 @@ def _get_next_sample(\n     data_size_warning_flag,\n     start_time,\n ):\n-    \"\"\" \"Yield data samples from the `dataset_iterator`.\n+    \"\"\"Yield data samples from the `dataset_iterator`.\n \n     Args:\n         dataset_iterator: An `iterator` object.\n-        ensure_shape_similarity (bool, optional): If set to True, the shape of\n+        ensure_shape_similarity: If set to `True`, the shape of\n             the first sample will be used to validate the shape of rest of the\n             samples. Defaults to `True`.\n-        data_size_warning_flag (bool, optional): If set to True, a warning will\n+        data_size_warning_flag: If set to `True`, a warning will\n             be issued if the dataset takes longer than 10 seconds to iterate.\n             Defaults to `True`.\n         start_time (float): the start time of the dataset iteration. this is\n             used only if `data_size_warning_flag` is set to true.\n \n-    Raises:\n-        ValueError:\n-            - If the dataset is empty.\n-            - If `ensure_shape_similarity` is set to True and the\n-            shape of the first sample is not equal to the shape of\n-            atleast one of the rest of the samples.\n-\n     Yields:\n-        data_sample: A tuple/list of numpy arrays.\n+        data_sample: The next sample.\n     \"\"\"\n     try:\n         dataset_iterator = iter(dataset_iterator)\n@@ -278,7 +258,7 @@ def _get_next_sample(\n         yield first_sample\n     except StopIteration:\n         raise ValueError(\n-            \"Received an empty Dataset. `dataset` must \"\n+            \"Received an empty dataset. Argument `dataset` must \"\n             \"be a non-empty list/tuple of `numpy.ndarray` objects \"\n             \"or `tf.data.Dataset` objects.\"\n         )\n@@ -341,13 +321,8 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n         right_size: The size of the right dataset split.\n         total_length: The total length of the dataset.\n \n-    Raises:\n-        TypeError: - If `left_size` or `right_size` is not an integer or float.\n-        ValueError: - If `left_size` or `right_size` is negative or greater\n-                    than 1 or greater than `total_length`.\n-\n     Returns:\n-        tuple: A tuple of rescaled left_size and right_size\n+        tuple: A tuple of rescaled `left_size` and `right_size` integers.\n     \"\"\"\n     left_size_type = type(left_size)\n     right_size_type = type(right_size)\n@@ -485,7 +460,7 @@ def _restore_dataset_from_list(\n \n \n def is_batched(dataset):\n-    \"\"\" \"Check if the `tf.data.Dataset` is batched.\"\"\"\n+    \"\"\"Check if the `tf.data.Dataset` is batched.\"\"\"\n     return hasattr(dataset, \"_batch_size\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#010f4a40f7e1dd8128af3fdc68f57cfcbbe901ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 553 | Lines Deleted: 543 | Files Changed: 6 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 1096 | Churn Cumulative: 2307 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,7 @@\n-from keras_core.utils.dataset_utils import audio_dataset_from_directory\n-from keras_core.utils.dataset_utils import image_dataset_from_directory\n+from keras_core.utils.audio_dataset_utils import audio_dataset_from_directory\n from keras_core.utils.dataset_utils import split_dataset\n-from keras_core.utils.dataset_utils import text_dataset_from_directory\n-from keras_core.utils.dataset_utils import timeseries_dataset_from_array\n from keras_core.utils.file_utils import get_file\n+from keras_core.utils.image_dataset_utils import image_dataset_from_directory\n from keras_core.utils.image_utils import array_to_img\n from keras_core.utils.image_utils import img_to_array\n from keras_core.utils.image_utils import load_img\n@@ -22,3 +20,7 @@ from keras_core.utils.python_utils import removeprefix\n from keras_core.utils.python_utils import removesuffix\n from keras_core.utils.rng_utils import set_random_seed\n from keras_core.utils.sequence_utils import pad_sequences\n+from keras_core.utils.text_dataset_utils import text_dataset_from_directory\n+from keras_core.utils.timeseries_dataset_utils import (\n+    timeseries_dataset_from_array,\n+)\n\n@@ -0,0 +1,124 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.module_utils import tensorflow as tf\n+\n+\n+@keras_core_export(\"keras_core.utils.audio_dataset_from_directory\")\n+def audio_dataset_from_directory(\n+    directory,\n+    labels=\"inferred\",\n+    label_mode=\"int\",\n+    class_names=None,\n+    batch_size=32,\n+    sampling_rate=None,\n+    output_sequence_length=None,\n+    ragged=False,\n+    shuffle=True,\n+    seed=None,\n+    validation_split=None,\n+    subset=None,\n+    follow_links=False,\n+):\n+    \"\"\"Generates a `tf.data.Dataset` from audio files in a directory.\n+\n+    If your directory structure is:\n+\n+    ```\n+    main_directory/\n+    ...class_a/\n+    ......a_audio_1.wav\n+    ......a_audio_2.wav\n+    ...class_b/\n+    ......b_audio_1.wav\n+    ......b_audio_2.wav\n+    ```\n+\n+    Then calling `audio_dataset_from_directory(main_directory,\n+    labels='inferred')`\n+    will return a `tf.data.Dataset` that yields batches of audio files from\n+    the subdirectories `class_a` and `class_b`, together with labels\n+    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n+\n+    Only `.wav` files are supported at this time.\n+\n+    Args:\n+        directory: Directory where the data is located.\n+            If `labels` is `\"inferred\"`, it should contain subdirectories,\n+            each containing audio files for a class. Otherwise, the directory\n+            structure is ignored.\n+        labels: Either \"inferred\" (labels are generated from the directory\n+            structure), `None` (no labels), or a list/tuple of integer labels\n+            of the same size as the number of audio files found in\n+            the directory. Labels should be sorted according to the\n+            alphanumeric order of the audio file paths\n+            (obtained via `os.walk(directory)` in Python).\n+        label_mode: String describing the encoding of `labels`. Options are:\n+            - `\"int\"`: means that the labels are encoded as integers (e.g. for\n+              `sparse_categorical_crossentropy` loss).\n+            - `\"categorical\"` means that the labels are encoded as a categorical\n+              vector (e.g. for `categorical_crossentropy` loss)\n+            - `\"binary\"` means that the labels (there can be only 2)\n+              are encoded as `float32` scalars with values 0\n+              or 1 (e.g. for `binary_crossentropy`).\n+            - `None` (no labels).\n+        class_names: Only valid if \"labels\" is `\"inferred\"`.\n+            This is the explicit list of class names\n+            (must match names of subdirectories). Used to control the order\n+            of the classes (otherwise alphanumerical order is used).\n+        batch_size: Size of the batches of data. Default: 32. If `None`,\n+            the data will not be batched\n+            (the dataset will yield individual samples).\n+        sampling_rate: Audio sampling rate (in samples per second).\n+        output_sequence_length: Maximum length of an audio sequence. Audio files\n+            longer than this will be truncated to `output_sequence_length`.\n+            If set to `None`, then all sequences in the same batch will\n+            be padded to the\n+            length of the longest sequence in the batch.\n+        ragged: Whether to return a Ragged dataset (where each sequence has its\n+            own length). Defaults to `False`.\n+        shuffle: Whether to shuffle the data. Defaults to `True`.\n+            If set to `False`, sorts the data in alphanumeric order.\n+        seed: Optional random seed for shuffling and transformations.\n+        validation_split: Optional float between 0 and 1, fraction of data to\n+            reserve for validation.\n+        subset: Subset of the data to return. One of `\"training\"`,\n+            `\"validation\"` or `\"both\"`. Only used if `validation_split` is set.\n+        follow_links: Whether to visits subdirectories pointed to by symlinks.\n+            Defaults to `False`.\n+\n+    Returns:\n+\n+    A `tf.data.Dataset` object.\n+\n+    - If `label_mode` is `None`, it yields `string` tensors of shape\n+      `(batch_size,)`, containing the contents of a batch of audio files.\n+    - Otherwise, it yields a tuple `(audio, labels)`, where `audio`\n+      has shape `(batch_size, sequence_length, num_channels)` and `labels`\n+      follows the format described\n+      below.\n+\n+    Rules regarding labels format:\n+\n+    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n+      `(batch_size,)`.\n+    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n+      1s and 0s of shape `(batch_size, 1)`.\n+    - if `label_mode` is `categorical`, the labels are a `float32` tensor\n+      of shape `(batch_size, num_classes)`, representing a one-hot\n+      encoding of the class index.\n+    \"\"\"\n+    # TODO: long-term, port implementation.\n+    return tf.keras.utils.audio_dataset_from_directory(\n+        directory,\n+        labels=labels,\n+        label_mode=label_mode,\n+        class_names=class_names,\n+        batch_size=batch_size,\n+        sampling_rate=sampling_rate,\n+        output_sequence_length=output_sequence_length,\n+        ragged=ragged,\n+        shuffle=shuffle,\n+        seed=seed,\n+        validation_split=validation_split,\n+        subset=subset,\n+        follow_links=follow_links,\n+    )\n\n@@ -490,542 +490,3 @@ def _get_type_spec(dataset):\n         return torchDataset\n     else:\n         return None\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.utils.image_dataset_from_directory\",\n-        \"keras_core.preprocessing.image_dataset_from_directory\",\n-    ]\n-)\n-def image_dataset_from_directory(\n-    directory,\n-    labels=\"inferred\",\n-    label_mode=\"int\",\n-    class_names=None,\n-    color_mode=\"rgb\",\n-    batch_size=32,\n-    image_size=(256, 256),\n-    shuffle=True,\n-    seed=None,\n-    validation_split=None,\n-    subset=None,\n-    interpolation=\"bilinear\",\n-    follow_links=False,\n-    crop_to_aspect_ratio=False,\n-):\n-    \"\"\"Generates a `tf.data.Dataset` from image files in a directory.\n-\n-    If your directory structure is:\n-\n-    ```\n-    main_directory/\n-    ...class_a/\n-    ......a_image_1.jpg\n-    ......a_image_2.jpg\n-    ...class_b/\n-    ......b_image_1.jpg\n-    ......b_image_2.jpg\n-    ```\n-\n-    Then calling `image_dataset_from_directory(main_directory,\n-    labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n-    images from the subdirectories `class_a` and `class_b`, together with labels\n-    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n-\n-    Supported image formats: `.jpeg`, `.jpg`, `.png`, `.bmp`, `.gif`.\n-    Animated gifs are truncated to the first frame.\n-\n-    Args:\n-        directory: Directory where the data is located.\n-            If `labels` is `\"inferred\"`, it should contain\n-            subdirectories, each containing images for a class.\n-            Otherwise, the directory structure is ignored.\n-        labels: Either `\"inferred\"`\n-            (labels are generated from the directory structure),\n-            `None` (no labels),\n-            or a list/tuple of integer labels of the same size as the number of\n-            image files found in the directory. Labels should be sorted\n-            according to the alphanumeric order of the image file paths\n-            (obtained via `os.walk(directory)` in Python).\n-        label_mode: String describing the encoding of `labels`. Options are:\n-            - `\"int\"`: means that the labels are encoded as integers\n-                (e.g. for `sparse_categorical_crossentropy` loss).\n-            - `\"categorical\"` means that the labels are\n-                encoded as a categorical vector\n-                (e.g. for `categorical_crossentropy` loss).\n-            - `\"binary\"` means that the labels (there can be only 2)\n-                are encoded as `float32` scalars with values 0 or 1\n-                (e.g. for `binary_crossentropy`).\n-            - `None` (no labels).\n-        class_names: Only valid if `labels` is `\"inferred\"`.\n-            This is the explicit list of class names\n-            (must match names of subdirectories). Used to control the order\n-            of the classes (otherwise alphanumerical order is used).\n-        color_mode: One of `\"grayscale\"`, `\"rgb\"`, `\"rgba\"`.\n-            Defaults to `\"rgb\"`. Whether the images will be converted to\n-            have 1, 3, or 4 channels.\n-        batch_size: Size of the batches of data. Defaults to 32.\n-            If `None`, the data will not be batched\n-            (the dataset will yield individual samples).\n-        image_size: Size to resize images to after they are read from disk,\n-            specified as `(height, width)`. Defaults to `(256, 256)`.\n-            Since the pipeline processes batches of images that must all have\n-            the same size, this must be provided.\n-        shuffle: Whether to shuffle the data. Defaults to `True`.\n-            If set to `False`, sorts the data in alphanumeric order.\n-        seed: Optional random seed for shuffling and transformations.\n-        validation_split: Optional float between 0 and 1,\n-            fraction of data to reserve for validation.\n-        subset: Subset of the data to return.\n-            One of `\"training\"`, `\"validation\"`, or `\"both\"`.\n-            Only used if `validation_split` is set.\n-            When `subset=\"both\"`, the utility returns a tuple of two datasets\n-            (the training and validation datasets respectively).\n-        interpolation: String, the interpolation method used when\n-            resizing images. Defaults to `\"bilinear\"`.\n-            Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n-            `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n-        follow_links: Whether to visit subdirectories pointed to by symlinks.\n-            Defaults to `False`.\n-        crop_to_aspect_ratio: If `True`, resize the images without aspect\n-            ratio distortion. When the original aspect ratio differs from the\n-            target aspect ratio, the output image will be cropped so as to\n-            return the largest possible window in the image\n-            (of size `image_size`) that matches the target aspect ratio. By\n-            default (`crop_to_aspect_ratio=False`), aspect ratio may not be\n-            preserved.\n-\n-    Returns:\n-\n-    A `tf.data.Dataset` object.\n-\n-    - If `label_mode` is `None`, it yields `float32` tensors of shape\n-        `(batch_size, image_size[0], image_size[1], num_channels)`,\n-        encoding images (see below for rules regarding `num_channels`).\n-    - Otherwise, it yields a tuple `(images, labels)`, where `images` has\n-        shape `(batch_size, image_size[0], image_size[1], num_channels)`,\n-        and `labels` follows the format described below.\n-\n-    Rules regarding labels format:\n-\n-    - if `label_mode` is `\"int\"`, the labels are an `int32` tensor of shape\n-        `(batch_size,)`.\n-    - if `label_mode` is `\"binary\"`, the labels are a `float32` tensor of\n-        1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `\"categorical\"`, the labels are a `float32` tensor\n-        of shape `(batch_size, num_classes)`, representing a one-hot\n-        encoding of the class index.\n-\n-    Rules regarding number of channels in the yielded images:\n-\n-    - if `color_mode` is `\"grayscale\"`,\n-        there's 1 channel in the image tensors.\n-    - if `color_mode` is `\"rgb\"`,\n-        there are 3 channels in the image tensors.\n-    - if `color_mode` is `\"rgba\"`,\n-        there are 4 channels in the image tensors.\n-    \"\"\"\n-    # TODO: long-term, port implementation.\n-    return tf.keras.utils.image_dataset_from_directory(\n-        directory,\n-        labels=labels,\n-        label_mode=label_mode,\n-        class_names=class_names,\n-        color_mode=color_mode,\n-        batch_size=batch_size,\n-        image_size=image_size,\n-        shuffle=shuffle,\n-        seed=seed,\n-        validation_split=validation_split,\n-        subset=subset,\n-        interpolation=interpolation,\n-        follow_links=follow_links,\n-        crop_to_aspect_ratio=crop_to_aspect_ratio,\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.utils.timeseries_dataset_from_array\",\n-        \"keras_core.preprocessing.timeseries_dataset_from_array\",\n-    ]\n-)\n-def timeseries_dataset_from_array(\n-    data,\n-    targets,\n-    sequence_length,\n-    sequence_stride=1,\n-    sampling_rate=1,\n-    batch_size=128,\n-    shuffle=False,\n-    seed=None,\n-    start_index=None,\n-    end_index=None,\n-):\n-    \"\"\"Creates a dataset of sliding windows over a timeseries provided as array.\n-\n-    This function takes in a sequence of data-points gathered at\n-    equal intervals, along with time series parameters such as\n-    length of the sequences/windows, spacing between two sequence/windows, etc.,\n-    to produce batches of timeseries inputs and targets.\n-\n-    Args:\n-        data: Numpy array or eager tensor\n-            containing consecutive data points (timesteps).\n-            Axis 0 is expected to be the time dimension.\n-        targets: Targets corresponding to timesteps in `data`.\n-            `targets[i]` should be the target\n-            corresponding to the window that starts at index `i`\n-            (see example 2 below).\n-            Pass `None` if you don't have target data (in this case the dataset\n-            will only yield the input data).\n-        sequence_length: Length of the output sequences\n-            (in number of timesteps).\n-        sequence_stride: Period between successive output sequences.\n-            For stride `s`, output samples would\n-            start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.\n-        sampling_rate: Period between successive individual timesteps\n-            within sequences. For rate `r`, timesteps\n-            `data[i], data[i + r], ... data[i + sequence_length]`\n-            are used for creating a sample sequence.\n-        batch_size: Number of timeseries samples in each batch\n-            (except maybe the last one). If `None`, the data will not be batched\n-            (the dataset will yield individual samples).\n-        shuffle: Whether to shuffle output samples,\n-            or instead draw them in chronological order.\n-        seed: Optional int; random seed for shuffling.\n-        start_index: Optional int; data points earlier (exclusive)\n-            than `start_index` will not be used\n-            in the output sequences. This is useful to reserve part of the\n-            data for test or validation.\n-        end_index: Optional int; data points later (exclusive) than `end_index`\n-            will not be used in the output sequences.\n-            This is useful to reserve part of the data for test or validation.\n-\n-    Returns:\n-\n-    A `tf.data.Dataset` instance. If `targets` was passed, the dataset yields\n-    tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields\n-    only `batch_of_sequences`.\n-\n-    Example 1:\n-\n-    Consider indices `[0, 1, ... 98]`.\n-    With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,\n-    `shuffle=False`, the dataset will yield batches of sequences\n-    composed of the following indices:\n-\n-    ```\n-    First sequence:  [0  2  4  6  8 10 12 14 16 18]\n-    Second sequence: [3  5  7  9 11 13 15 17 19 21]\n-    Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n-    ...\n-    Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n-    ```\n-\n-    In this case the last 2 data points are discarded since no full sequence\n-    can be generated to include them (the next sequence would have started\n-    at index 81, and thus its last step would have gone over 98).\n-\n-    Example 2: Temporal regression.\n-\n-    Consider an array `data` of scalar values, of shape `(steps,)`.\n-    To generate a dataset that uses the past 10\n-    timesteps to predict the next timestep, you would use:\n-\n-    ```python\n-    input_data = data[:-10]\n-    targets = data[10:]\n-    dataset = timeseries_dataset_from_array(\n-        input_data, targets, sequence_length=10)\n-    for batch in dataset:\n-      inputs, targets = batch\n-      assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]\n-      # Corresponding target: step 10\n-      assert np.array_equal(targets[0], data[10])\n-      break\n-    ```\n-\n-    Example 3: Temporal regression for many-to-many architectures.\n-\n-    Consider two arrays of scalar values `X` and `Y`,\n-    both of shape `(100,)`. The resulting dataset should consist samples with\n-    20 timestamps each. The samples should not overlap.\n-    To generate a dataset that uses the current timestamp\n-    to predict the corresponding target timestep, you would use:\n-\n-    ```python\n-    X = np.arange(100)\n-    Y = X*2\n-\n-    sample_length = 20\n-    input_dataset = timeseries_dataset_from_array(\n-        X, None, sequence_length=sample_length, sequence_stride=sample_length)\n-    target_dataset = timeseries_dataset_from_array(\n-        Y, None, sequence_length=sample_length, sequence_stride=sample_length)\n-\n-    for batch in zip(input_dataset, target_dataset):\n-        inputs, targets = batch\n-        assert np.array_equal(inputs[0], X[:sample_length])\n-\n-        # second sample equals output timestamps 20-40\n-        assert np.array_equal(targets[1], Y[sample_length:2*sample_length])\n-        break\n-    ```\n-    \"\"\"\n-    # TODO: long-term, port implementation.\n-    return tf.keras.utils.timeseries_dataset_from_array(\n-        data,\n-        targets,\n-        sequence_length,\n-        sequence_stride=sequence_stride,\n-        sampling_rate=sampling_rate,\n-        batch_size=batch_size,\n-        shuffle=shuffle,\n-        seed=seed,\n-        start_index=start_index,\n-        end_index=end_index,\n-    )\n-\n-\n-@keras_core_export(\n-    [\n-        \"keras_core.utils.text_dataset_from_directory\",\n-        \"keras_core.preprocessing.text_dataset_from_directory\",\n-    ]\n-)\n-def text_dataset_from_directory(\n-    directory,\n-    labels=\"inferred\",\n-    label_mode=\"int\",\n-    class_names=None,\n-    batch_size=32,\n-    max_length=None,\n-    shuffle=True,\n-    seed=None,\n-    validation_split=None,\n-    subset=None,\n-    follow_links=False,\n-):\n-    \"\"\"Generates a `tf.data.Dataset` from text files in a directory.\n-\n-    If your directory structure is:\n-\n-    ```\n-    main_directory/\n-    ...class_a/\n-    ......a_text_1.txt\n-    ......a_text_2.txt\n-    ...class_b/\n-    ......b_text_1.txt\n-    ......b_text_2.txt\n-    ```\n-\n-    Then calling `text_dataset_from_directory(main_directory,\n-    labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n-    texts from the subdirectories `class_a` and `class_b`, together with labels\n-    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n-\n-    Only `.txt` files are supported at this time.\n-\n-    Args:\n-        directory: Directory where the data is located.\n-            If `labels` is `\"inferred\"`, it should contain\n-            subdirectories, each containing text files for a class.\n-            Otherwise, the directory structure is ignored.\n-        labels: Either `\"inferred\"`\n-            (labels are generated from the directory structure),\n-            `None` (no labels),\n-            or a list/tuple of integer labels of the same size as the number of\n-            text files found in the directory. Labels should be sorted according\n-            to the alphanumeric order of the text file paths\n-            (obtained via `os.walk(directory)` in Python).\n-        label_mode: String describing the encoding of `labels`. Options are:\n-            - `\"int\"`: means that the labels are encoded as integers\n-                (e.g. for `sparse_categorical_crossentropy` loss).\n-            - `\"categorical\"` means that the labels are\n-                encoded as a categorical vector\n-                (e.g. for `categorical_crossentropy` loss).\n-            - `\"binary\"` means that the labels (there can be only 2)\n-                are encoded as `float32` scalars with values 0 or 1\n-                (e.g. for `binary_crossentropy`).\n-            - `None` (no labels).\n-        class_names: Only valid if `\"labels\"` is `\"inferred\"`.\n-            This is the explicit list of class names\n-            (must match names of subdirectories). Used to control the order\n-            of the classes (otherwise alphanumerical order is used).\n-        batch_size: Size of the batches of data. Defaults to 32.\n-            If `None`, the data will not be batched\n-            (the dataset will yield individual samples).\n-        max_length: Maximum size of a text string. Texts longer than this will\n-            be truncated to `max_length`.\n-        shuffle: Whether to shuffle the data. Defaults to `True`.\n-            If set to `False`, sorts the data in alphanumeric order.\n-        seed: Optional random seed for shuffling and transformations.\n-        validation_split: Optional float between 0 and 1,\n-            fraction of data to reserve for validation.\n-        subset: Subset of the data to return.\n-            One of `\"training\"`, `\"validation\"` or `\"both\"`.\n-            Only used if `validation_split` is set.\n-            When `subset=\"both\"`, the utility returns a tuple of two datasets\n-            (the training and validation datasets respectively).\n-        follow_links: Whether to visits subdirectories pointed to by symlinks.\n-            Defaults to `False`.\n-\n-    Returns:\n-\n-    A `tf.data.Dataset` object.\n-\n-    - If `label_mode` is `None`, it yields `string` tensors of shape\n-        `(batch_size,)`, containing the contents of a batch of text files.\n-    - Otherwise, it yields a tuple `(texts, labels)`, where `texts`\n-        has shape `(batch_size,)` and `labels` follows the format described\n-        below.\n-\n-    Rules regarding labels format:\n-\n-    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n-        `(batch_size,)`.\n-    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n-        1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `categorical`, the labels are a `float32` tensor\n-        of shape `(batch_size, num_classes)`, representing a one-hot\n-        encoding of the class index.\n-    \"\"\"\n-    # TODO: long-term, port implementation.\n-    return tf.keras.utils.text_dataset_from_directory(\n-        directory,\n-        labels=labels,\n-        label_mode=label_mode,\n-        class_names=class_names,\n-        batch_size=batch_size,\n-        max_length=max_length,\n-        shuffle=shuffle,\n-        seed=seed,\n-        validation_split=validation_split,\n-        subset=subset,\n-        follow_links=follow_links,\n-    )\n-\n-\n-@keras_core_export(\"keras_core.utils.audio_dataset_from_directory\")\n-def audio_dataset_from_directory(\n-    directory,\n-    labels=\"inferred\",\n-    label_mode=\"int\",\n-    class_names=None,\n-    batch_size=32,\n-    sampling_rate=None,\n-    output_sequence_length=None,\n-    ragged=False,\n-    shuffle=True,\n-    seed=None,\n-    validation_split=None,\n-    subset=None,\n-    follow_links=False,\n-):\n-    \"\"\"Generates a `tf.data.Dataset` from audio files in a directory.\n-\n-    If your directory structure is:\n-\n-    ```\n-    main_directory/\n-    ...class_a/\n-    ......a_audio_1.wav\n-    ......a_audio_2.wav\n-    ...class_b/\n-    ......b_audio_1.wav\n-    ......b_audio_2.wav\n-    ```\n-\n-    Then calling `audio_dataset_from_directory(main_directory,\n-    labels='inferred')`\n-    will return a `tf.data.Dataset` that yields batches of audio files from\n-    the subdirectories `class_a` and `class_b`, together with labels\n-    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n-\n-    Only `.wav` files are supported at this time.\n-\n-    Args:\n-        directory: Directory where the data is located.\n-            If `labels` is `\"inferred\"`, it should contain subdirectories,\n-            each containing audio files for a class. Otherwise, the directory\n-            structure is ignored.\n-        labels: Either \"inferred\" (labels are generated from the directory\n-            structure), `None` (no labels), or a list/tuple of integer labels\n-            of the same size as the number of audio files found in\n-            the directory. Labels should be sorted according to the\n-            alphanumeric order of the audio file paths\n-            (obtained via `os.walk(directory)` in Python).\n-        label_mode: String describing the encoding of `labels`. Options are:\n-            - `\"int\"`: means that the labels are encoded as integers (e.g. for\n-              `sparse_categorical_crossentropy` loss).\n-            - `\"categorical\"` means that the labels are encoded as a categorical\n-              vector (e.g. for `categorical_crossentropy` loss)\n-            - `\"binary\"` means that the labels (there can be only 2)\n-              are encoded as `float32` scalars with values 0\n-              or 1 (e.g. for `binary_crossentropy`).\n-            - `None` (no labels).\n-        class_names: Only valid if \"labels\" is `\"inferred\"`.\n-            This is the explicit list of class names\n-            (must match names of subdirectories). Used to control the order\n-            of the classes (otherwise alphanumerical order is used).\n-        batch_size: Size of the batches of data. Default: 32. If `None`,\n-            the data will not be batched\n-            (the dataset will yield individual samples).\n-        sampling_rate: Audio sampling rate (in samples per second).\n-        output_sequence_length: Maximum length of an audio sequence. Audio files\n-            longer than this will be truncated to `output_sequence_length`.\n-            If set to `None`, then all sequences in the same batch will\n-            be padded to the\n-            length of the longest sequence in the batch.\n-        ragged: Whether to return a Ragged dataset (where each sequence has its\n-            own length). Defaults to `False`.\n-        shuffle: Whether to shuffle the data. Defaults to `True`.\n-            If set to `False`, sorts the data in alphanumeric order.\n-        seed: Optional random seed for shuffling and transformations.\n-        validation_split: Optional float between 0 and 1, fraction of data to\n-            reserve for validation.\n-        subset: Subset of the data to return. One of `\"training\"`,\n-            `\"validation\"` or `\"both\"`. Only used if `validation_split` is set.\n-        follow_links: Whether to visits subdirectories pointed to by symlinks.\n-            Defaults to `False`.\n-\n-    Returns:\n-\n-    A `tf.data.Dataset` object.\n-\n-    - If `label_mode` is `None`, it yields `string` tensors of shape\n-      `(batch_size,)`, containing the contents of a batch of audio files.\n-    - Otherwise, it yields a tuple `(audio, labels)`, where `audio`\n-      has shape `(batch_size, sequence_length, num_channels)` and `labels`\n-      follows the format described\n-      below.\n-\n-    Rules regarding labels format:\n-\n-    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n-      `(batch_size,)`.\n-    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n-      1s and 0s of shape `(batch_size, 1)`.\n-    - if `label_mode` is `categorical`, the labels are a `float32` tensor\n-      of shape `(batch_size, num_classes)`, representing a one-hot\n-      encoding of the class index.\n-    \"\"\"\n-    # TODO: long-term, port implementation.\n-    return tf.keras.utils.audio_dataset_from_directory(\n-        directory,\n-        labels=labels,\n-        label_mode=label_mode,\n-        class_names=class_names,\n-        batch_size=batch_size,\n-        sampling_rate=sampling_rate,\n-        output_sequence_length=output_sequence_length,\n-        ragged=ragged,\n-        shuffle=shuffle,\n-        seed=seed,\n-        validation_split=validation_split,\n-        subset=subset,\n-        follow_links=follow_links,\n-    )\n\n@@ -0,0 +1,155 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.module_utils import tensorflow as tf\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.utils.image_dataset_from_directory\",\n+        \"keras_core.preprocessing.image_dataset_from_directory\",\n+    ]\n+)\n+def image_dataset_from_directory(\n+    directory,\n+    labels=\"inferred\",\n+    label_mode=\"int\",\n+    class_names=None,\n+    color_mode=\"rgb\",\n+    batch_size=32,\n+    image_size=(256, 256),\n+    shuffle=True,\n+    seed=None,\n+    validation_split=None,\n+    subset=None,\n+    interpolation=\"bilinear\",\n+    follow_links=False,\n+    crop_to_aspect_ratio=False,\n+):\n+    \"\"\"Generates a `tf.data.Dataset` from image files in a directory.\n+\n+    If your directory structure is:\n+\n+    ```\n+    main_directory/\n+    ...class_a/\n+    ......a_image_1.jpg\n+    ......a_image_2.jpg\n+    ...class_b/\n+    ......b_image_1.jpg\n+    ......b_image_2.jpg\n+    ```\n+\n+    Then calling `image_dataset_from_directory(main_directory,\n+    labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n+    images from the subdirectories `class_a` and `class_b`, together with labels\n+    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n+\n+    Supported image formats: `.jpeg`, `.jpg`, `.png`, `.bmp`, `.gif`.\n+    Animated gifs are truncated to the first frame.\n+\n+    Args:\n+        directory: Directory where the data is located.\n+            If `labels` is `\"inferred\"`, it should contain\n+            subdirectories, each containing images for a class.\n+            Otherwise, the directory structure is ignored.\n+        labels: Either `\"inferred\"`\n+            (labels are generated from the directory structure),\n+            `None` (no labels),\n+            or a list/tuple of integer labels of the same size as the number of\n+            image files found in the directory. Labels should be sorted\n+            according to the alphanumeric order of the image file paths\n+            (obtained via `os.walk(directory)` in Python).\n+        label_mode: String describing the encoding of `labels`. Options are:\n+            - `\"int\"`: means that the labels are encoded as integers\n+                (e.g. for `sparse_categorical_crossentropy` loss).\n+            - `\"categorical\"` means that the labels are\n+                encoded as a categorical vector\n+                (e.g. for `categorical_crossentropy` loss).\n+            - `\"binary\"` means that the labels (there can be only 2)\n+                are encoded as `float32` scalars with values 0 or 1\n+                (e.g. for `binary_crossentropy`).\n+            - `None` (no labels).\n+        class_names: Only valid if `labels` is `\"inferred\"`.\n+            This is the explicit list of class names\n+            (must match names of subdirectories). Used to control the order\n+            of the classes (otherwise alphanumerical order is used).\n+        color_mode: One of `\"grayscale\"`, `\"rgb\"`, `\"rgba\"`.\n+            Defaults to `\"rgb\"`. Whether the images will be converted to\n+            have 1, 3, or 4 channels.\n+        batch_size: Size of the batches of data. Defaults to 32.\n+            If `None`, the data will not be batched\n+            (the dataset will yield individual samples).\n+        image_size: Size to resize images to after they are read from disk,\n+            specified as `(height, width)`. Defaults to `(256, 256)`.\n+            Since the pipeline processes batches of images that must all have\n+            the same size, this must be provided.\n+        shuffle: Whether to shuffle the data. Defaults to `True`.\n+            If set to `False`, sorts the data in alphanumeric order.\n+        seed: Optional random seed for shuffling and transformations.\n+        validation_split: Optional float between 0 and 1,\n+            fraction of data to reserve for validation.\n+        subset: Subset of the data to return.\n+            One of `\"training\"`, `\"validation\"`, or `\"both\"`.\n+            Only used if `validation_split` is set.\n+            When `subset=\"both\"`, the utility returns a tuple of two datasets\n+            (the training and validation datasets respectively).\n+        interpolation: String, the interpolation method used when\n+            resizing images. Defaults to `\"bilinear\"`.\n+            Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n+            `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+        follow_links: Whether to visit subdirectories pointed to by symlinks.\n+            Defaults to `False`.\n+        crop_to_aspect_ratio: If `True`, resize the images without aspect\n+            ratio distortion. When the original aspect ratio differs from the\n+            target aspect ratio, the output image will be cropped so as to\n+            return the largest possible window in the image\n+            (of size `image_size`) that matches the target aspect ratio. By\n+            default (`crop_to_aspect_ratio=False`), aspect ratio may not be\n+            preserved.\n+\n+    Returns:\n+\n+    A `tf.data.Dataset` object.\n+\n+    - If `label_mode` is `None`, it yields `float32` tensors of shape\n+        `(batch_size, image_size[0], image_size[1], num_channels)`,\n+        encoding images (see below for rules regarding `num_channels`).\n+    - Otherwise, it yields a tuple `(images, labels)`, where `images` has\n+        shape `(batch_size, image_size[0], image_size[1], num_channels)`,\n+        and `labels` follows the format described below.\n+\n+    Rules regarding labels format:\n+\n+    - if `label_mode` is `\"int\"`, the labels are an `int32` tensor of shape\n+        `(batch_size,)`.\n+    - if `label_mode` is `\"binary\"`, the labels are a `float32` tensor of\n+        1s and 0s of shape `(batch_size, 1)`.\n+    - if `label_mode` is `\"categorical\"`, the labels are a `float32` tensor\n+        of shape `(batch_size, num_classes)`, representing a one-hot\n+        encoding of the class index.\n+\n+    Rules regarding number of channels in the yielded images:\n+\n+    - if `color_mode` is `\"grayscale\"`,\n+        there's 1 channel in the image tensors.\n+    - if `color_mode` is `\"rgb\"`,\n+        there are 3 channels in the image tensors.\n+    - if `color_mode` is `\"rgba\"`,\n+        there are 4 channels in the image tensors.\n+    \"\"\"\n+    # TODO: long-term, port implementation.\n+    return tf.keras.utils.image_dataset_from_directory(\n+        directory,\n+        labels=labels,\n+        label_mode=label_mode,\n+        class_names=class_names,\n+        color_mode=color_mode,\n+        batch_size=batch_size,\n+        image_size=image_size,\n+        shuffle=shuffle,\n+        seed=seed,\n+        validation_split=validation_split,\n+        subset=subset,\n+        interpolation=interpolation,\n+        follow_links=follow_links,\n+        crop_to_aspect_ratio=crop_to_aspect_ratio,\n+    )\n\n@@ -0,0 +1,122 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.module_utils import tensorflow as tf\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.utils.text_dataset_from_directory\",\n+        \"keras_core.preprocessing.text_dataset_from_directory\",\n+    ]\n+)\n+def text_dataset_from_directory(\n+    directory,\n+    labels=\"inferred\",\n+    label_mode=\"int\",\n+    class_names=None,\n+    batch_size=32,\n+    max_length=None,\n+    shuffle=True,\n+    seed=None,\n+    validation_split=None,\n+    subset=None,\n+    follow_links=False,\n+):\n+    \"\"\"Generates a `tf.data.Dataset` from text files in a directory.\n+\n+    If your directory structure is:\n+\n+    ```\n+    main_directory/\n+    ...class_a/\n+    ......a_text_1.txt\n+    ......a_text_2.txt\n+    ...class_b/\n+    ......b_text_1.txt\n+    ......b_text_2.txt\n+    ```\n+\n+    Then calling `text_dataset_from_directory(main_directory,\n+    labels='inferred')` will return a `tf.data.Dataset` that yields batches of\n+    texts from the subdirectories `class_a` and `class_b`, together with labels\n+    0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n+\n+    Only `.txt` files are supported at this time.\n+\n+    Args:\n+        directory: Directory where the data is located.\n+            If `labels` is `\"inferred\"`, it should contain\n+            subdirectories, each containing text files for a class.\n+            Otherwise, the directory structure is ignored.\n+        labels: Either `\"inferred\"`\n+            (labels are generated from the directory structure),\n+            `None` (no labels),\n+            or a list/tuple of integer labels of the same size as the number of\n+            text files found in the directory. Labels should be sorted according\n+            to the alphanumeric order of the text file paths\n+            (obtained via `os.walk(directory)` in Python).\n+        label_mode: String describing the encoding of `labels`. Options are:\n+            - `\"int\"`: means that the labels are encoded as integers\n+                (e.g. for `sparse_categorical_crossentropy` loss).\n+            - `\"categorical\"` means that the labels are\n+                encoded as a categorical vector\n+                (e.g. for `categorical_crossentropy` loss).\n+            - `\"binary\"` means that the labels (there can be only 2)\n+                are encoded as `float32` scalars with values 0 or 1\n+                (e.g. for `binary_crossentropy`).\n+            - `None` (no labels).\n+        class_names: Only valid if `\"labels\"` is `\"inferred\"`.\n+            This is the explicit list of class names\n+            (must match names of subdirectories). Used to control the order\n+            of the classes (otherwise alphanumerical order is used).\n+        batch_size: Size of the batches of data. Defaults to 32.\n+            If `None`, the data will not be batched\n+            (the dataset will yield individual samples).\n+        max_length: Maximum size of a text string. Texts longer than this will\n+            be truncated to `max_length`.\n+        shuffle: Whether to shuffle the data. Defaults to `True`.\n+            If set to `False`, sorts the data in alphanumeric order.\n+        seed: Optional random seed for shuffling and transformations.\n+        validation_split: Optional float between 0 and 1,\n+            fraction of data to reserve for validation.\n+        subset: Subset of the data to return.\n+            One of `\"training\"`, `\"validation\"` or `\"both\"`.\n+            Only used if `validation_split` is set.\n+            When `subset=\"both\"`, the utility returns a tuple of two datasets\n+            (the training and validation datasets respectively).\n+        follow_links: Whether to visits subdirectories pointed to by symlinks.\n+            Defaults to `False`.\n+\n+    Returns:\n+\n+    A `tf.data.Dataset` object.\n+\n+    - If `label_mode` is `None`, it yields `string` tensors of shape\n+        `(batch_size,)`, containing the contents of a batch of text files.\n+    - Otherwise, it yields a tuple `(texts, labels)`, where `texts`\n+        has shape `(batch_size,)` and `labels` follows the format described\n+        below.\n+\n+    Rules regarding labels format:\n+\n+    - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n+        `(batch_size,)`.\n+    - if `label_mode` is `binary`, the labels are a `float32` tensor of\n+        1s and 0s of shape `(batch_size, 1)`.\n+    - if `label_mode` is `categorical`, the labels are a `float32` tensor\n+        of shape `(batch_size, num_classes)`, representing a one-hot\n+        encoding of the class index.\n+    \"\"\"\n+    # TODO: long-term, port implementation.\n+    return tf.keras.utils.text_dataset_from_directory(\n+        directory,\n+        labels=labels,\n+        label_mode=label_mode,\n+        class_names=class_names,\n+        batch_size=batch_size,\n+        max_length=max_length,\n+        shuffle=shuffle,\n+        seed=seed,\n+        validation_split=validation_split,\n+        subset=subset,\n+        follow_links=follow_links,\n+    )\n\n@@ -0,0 +1,146 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.utils.module_utils import tensorflow as tf\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.utils.timeseries_dataset_from_array\",\n+        \"keras_core.preprocessing.timeseries_dataset_from_array\",\n+    ]\n+)\n+def timeseries_dataset_from_array(\n+    data,\n+    targets,\n+    sequence_length,\n+    sequence_stride=1,\n+    sampling_rate=1,\n+    batch_size=128,\n+    shuffle=False,\n+    seed=None,\n+    start_index=None,\n+    end_index=None,\n+):\n+    \"\"\"Creates a dataset of sliding windows over a timeseries provided as array.\n+\n+    This function takes in a sequence of data-points gathered at\n+    equal intervals, along with time series parameters such as\n+    length of the sequences/windows, spacing between two sequence/windows, etc.,\n+    to produce batches of timeseries inputs and targets.\n+\n+    Args:\n+        data: Numpy array or eager tensor\n+            containing consecutive data points (timesteps).\n+            Axis 0 is expected to be the time dimension.\n+        targets: Targets corresponding to timesteps in `data`.\n+            `targets[i]` should be the target\n+            corresponding to the window that starts at index `i`\n+            (see example 2 below).\n+            Pass `None` if you don't have target data (in this case the dataset\n+            will only yield the input data).\n+        sequence_length: Length of the output sequences\n+            (in number of timesteps).\n+        sequence_stride: Period between successive output sequences.\n+            For stride `s`, output samples would\n+            start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.\n+        sampling_rate: Period between successive individual timesteps\n+            within sequences. For rate `r`, timesteps\n+            `data[i], data[i + r], ... data[i + sequence_length]`\n+            are used for creating a sample sequence.\n+        batch_size: Number of timeseries samples in each batch\n+            (except maybe the last one). If `None`, the data will not be batched\n+            (the dataset will yield individual samples).\n+        shuffle: Whether to shuffle output samples,\n+            or instead draw them in chronological order.\n+        seed: Optional int; random seed for shuffling.\n+        start_index: Optional int; data points earlier (exclusive)\n+            than `start_index` will not be used\n+            in the output sequences. This is useful to reserve part of the\n+            data for test or validation.\n+        end_index: Optional int; data points later (exclusive) than `end_index`\n+            will not be used in the output sequences.\n+            This is useful to reserve part of the data for test or validation.\n+\n+    Returns:\n+\n+    A `tf.data.Dataset` instance. If `targets` was passed, the dataset yields\n+    tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields\n+    only `batch_of_sequences`.\n+\n+    Example 1:\n+\n+    Consider indices `[0, 1, ... 98]`.\n+    With `sequence_length=10,  sampling_rate=2, sequence_stride=3`,\n+    `shuffle=False`, the dataset will yield batches of sequences\n+    composed of the following indices:\n+\n+    ```\n+    First sequence:  [0  2  4  6  8 10 12 14 16 18]\n+    Second sequence: [3  5  7  9 11 13 15 17 19 21]\n+    Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n+    ...\n+    Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n+    ```\n+\n+    In this case the last 2 data points are discarded since no full sequence\n+    can be generated to include them (the next sequence would have started\n+    at index 81, and thus its last step would have gone over 98).\n+\n+    Example 2: Temporal regression.\n+\n+    Consider an array `data` of scalar values, of shape `(steps,)`.\n+    To generate a dataset that uses the past 10\n+    timesteps to predict the next timestep, you would use:\n+\n+    ```python\n+    input_data = data[:-10]\n+    targets = data[10:]\n+    dataset = timeseries_dataset_from_array(\n+        input_data, targets, sequence_length=10)\n+    for batch in dataset:\n+      inputs, targets = batch\n+      assert np.array_equal(inputs[0], data[:10])  # First sequence: steps [0-9]\n+      # Corresponding target: step 10\n+      assert np.array_equal(targets[0], data[10])\n+      break\n+    ```\n+\n+    Example 3: Temporal regression for many-to-many architectures.\n+\n+    Consider two arrays of scalar values `X` and `Y`,\n+    both of shape `(100,)`. The resulting dataset should consist samples with\n+    20 timestamps each. The samples should not overlap.\n+    To generate a dataset that uses the current timestamp\n+    to predict the corresponding target timestep, you would use:\n+\n+    ```python\n+    X = np.arange(100)\n+    Y = X*2\n+\n+    sample_length = 20\n+    input_dataset = timeseries_dataset_from_array(\n+        X, None, sequence_length=sample_length, sequence_stride=sample_length)\n+    target_dataset = timeseries_dataset_from_array(\n+        Y, None, sequence_length=sample_length, sequence_stride=sample_length)\n+\n+    for batch in zip(input_dataset, target_dataset):\n+        inputs, targets = batch\n+        assert np.array_equal(inputs[0], X[:sample_length])\n+\n+        # second sample equals output timestamps 20-40\n+        assert np.array_equal(targets[1], Y[sample_length:2*sample_length])\n+        break\n+    ```\n+    \"\"\"\n+    # TODO: long-term, port implementation.\n+    return tf.keras.utils.timeseries_dataset_from_array(\n+        data,\n+        targets,\n+        sequence_length,\n+        sequence_stride=sequence_stride,\n+        sampling_rate=sampling_rate,\n+        batch_size=batch_size,\n+        shuffle=shuffle,\n+        seed=seed,\n+        start_index=start_index,\n+        end_index=end_index,\n+    )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
