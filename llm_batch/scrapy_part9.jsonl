{"custom_id": "scrapy#17dec39c2965b0790b16fe5d4ac99ec19efd2f33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 172 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -33,9 +33,6 @@ class ExtractAdaptor(ExtendedAdaptor):\n     def function(self, item, location, **pipeargs):\n         return self.do(self.extract, location, **pipeargs)\n     \n-    def _extract(self, location, **kwargs):\n-        return self.extract(location, **kwargs)\n-    \n     def extract(self, location, **kwargs):\n         \"\"\"Extract a list of strings from the location passed.\n         Receives a list of XPathSelectors or an XPathSelector,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1c8c73ebfa2c20a204bcd3bdcffaebb78fa1e7ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import os\n+import string\n \n from scrapy.spider import spiders\n from scrapy.command import ScrapyCommand\n@@ -33,6 +34,9 @@ class Command(ScrapyCommand):\n \n         spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n         spidersdir = os.path.abspath(os.path.dirname(spiders_module.__file__))\n+        if name[0] not in string.letters: # must start with a letter, for valid python modules\n+            name = \"a\" + name\n+        name = name.replace('-', '_') # - are replaced by _, for valid python modules\n         self._genfiles('spider.tmpl', '%s/%s.py' % (spidersdir, name), tvars)\n \n     def _genfiles(self, template_name, source_name, tvars):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#643ea99f367beb5d4c86fb5495e59d9a665f741d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 178 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class ExtendedAdaptor(BaseAdaptor):\n         \n class ExtractAdaptor(ExtendedAdaptor):\n \n-    def function(self, item, location, **pipeargs):\n+    def function(self, attrname, location, **pipeargs):\n         return self.do(self.extract, location, **pipeargs)\n     \n     def extract(self, location, **kwargs):\n@@ -75,7 +75,7 @@ _xml_cdata_split_re = re.compile('(<!\\[CDATA\\[.*?\\]\\]>)', re.S)\n \n class HtmlCleanAdaptor(ExtendedAdaptor):\n \n-    def function(self, item, location, **pipeargs):\n+    def function(self, attrname, string, **pipeargs):\n         return self.do(self.clean, string, **pipeargs)\n     \n     def clean(self, string, **kwargs):\n@@ -121,4 +121,6 @@ class XmlCleanAdaptor(HtmlCleanAdaptor):\n                     pieces.append(HtmlCleanAdaptor._remove_tags(self, piece, **kwargs))\n \n             xml = \"\".join(pieces)\n+        else:\n+            xml = HtmlCleanAdaptor._remove_tags(self, xml, **kwargs)\n         return xml\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2bd70ba21af273b97ecb4f35ad6521940bf3cd6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 52 | Files Changed: 2 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/16 | Churn Δ: 115 | Churn Cumulative: 352 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -10,6 +10,7 @@ from scrapy.utils.python import flatten, unique\n from scrapy.xpath import XPathSelector\n from scrapy.utils.misc import unquote_html, location_str\n from scrapy.conf import settings\n+from scrapy.utils.markup import clean_markup\n \n class ExtendedAdaptor(BaseAdaptor):\n \n@@ -30,7 +31,7 @@ class ExtendedAdaptor(BaseAdaptor):\n         \n class ExtractAdaptor(ExtendedAdaptor):\n \n-    def function(self, attrname, location, **pipeargs):\n+    def function(self, location, **pipeargs):\n         return self.do(self.extract, location, **pipeargs)\n     \n     def extract(self, location, **kwargs):\n@@ -66,61 +67,16 @@ class ExtractAdaptor(ExtendedAdaptor):\n \n         return strings\n \n-   \n-_clean_spaces_re = re.compile(\"\\s+\", re.U)\n-_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-_xml_remove_tags_re = re.compile(r'<[a-zA-Z\\/!][^>]*?>')\n-_xml_remove_cdata_re = re.compile('<!\\[CDATA\\[(.*)\\]\\]', re.S)\n-_xml_cdata_split_re = re.compile('(<!\\[CDATA\\[.*?\\]\\]>)', re.S)\n-\n class HtmlCleanAdaptor(ExtendedAdaptor):\n \n-    def function(self, attrname, string, **pipeargs):\n+    def function(self, string, **pipeargs):\n         return self.do(self.clean, string, **pipeargs)\n     \n-    def clean(self, string, **kwargs):\n-        \"\"\"Clean (list of) strings removing newlines, spaces, etc\"\"\"\n-        if isinstance(string, list):\n-            return [self.clean(s, **kwargs) for s in string]\n-        \n-        xml = self._remove_tags(string, **kwargs)\n-\n-        if kwargs.get('remove_root', True) and not kwargs.get('remove_tags', True):\n-            m = _remove_root_re.search(xml)\n-            if m:\n-                xml = m.group(1)\n-\n-        if kwargs.get('remove_spaces', True):\n-            xml = _clean_spaces_re.sub(' ', xml)\n-        if kwargs.get('strip', True):\n-            xml = xml.strip()\n-    \n-        return xml\n-    \n-    def _remove_tags(self, xml, **kwargs):\n-        if kwargs.get('remove_tags', True):\n-            xml = _xml_remove_tags_re.sub(' ', xml)\n-        return xml\n+    def clean(self, string, **pipeargs):\n+        return clean_markup(string, **pipeargs)\n         \n class XmlCleanAdaptor(HtmlCleanAdaptor):\n     \n-    def _remove_tags(self, xml, **kwargs):\n-        #process in pieces the text that contains CDATA. The first check is to avoid unnecesary regex check\n-        if _xml_remove_cdata_re.search(xml):\n-            pieces = []\n-            \n-            for piece in _xml_cdata_split_re.split(xml):\n-                \n-                m = _xml_remove_cdata_re.search(piece)\n-                if m:\n-                    if kwargs.get('remove_cdata', True):#remove cdata special tag\n-                        pieces.append(HtmlCleanAdaptor._remove_tags(self, m.groups()[0], **kwargs))\n-                    else:\n-                        pieces.append(piece)#conserve intact the cdata\n-                else:\n-                    pieces.append(HtmlCleanAdaptor._remove_tags(self, piece, **kwargs))\n-\n-            xml = \"\".join(pieces)\n-        else:\n-            xml = HtmlCleanAdaptor._remove_tags(self, xml, **kwargs)\n-        return xml\n+    def clean(self, string, **pipeargs):\n+        pipeargs[\"xml_doc\"] = True\n+        return clean_markup(string, **pipeargs)\n\n@@ -57,3 +57,58 @@ def replace_tags(text, token=''):\n     Always returns a unicode string.\n     \"\"\"\n     return _tag_re.sub(token, text.decode('utf-8'))\n+\n+_clean_spaces_re = re.compile(\"\\s+\", re.U)\n+_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n+_xml_remove_tags_re = re.compile(r'<[a-zA-Z\\/!][^>]*?>')\n+_xml_remove_cdata_re = re.compile('<!\\[CDATA\\[(.*)\\]\\]', re.S)\n+_xml_cdata_split_re = re.compile('(<!\\[CDATA\\[.*?\\]\\]>)', re.S)\n+\n+def remove_tags(xml, **kwargs):\n+    if kwargs.get('remove_tags', True):\n+        xml = _xml_remove_tags_re.sub(' ', xml)\n+    return xml\n+\n+def xml_remove_tags(xml, **kwargs):\n+    #process in pieces the text that contains CDATA. The first check is to avoid unnecesary regex check\n+    if _xml_remove_cdata_re.search(xml):\n+        pieces = []\n+        \n+        for piece in _xml_cdata_split_re.split(xml):\n+            \n+            m = _xml_remove_cdata_re.search(piece)\n+            if m:\n+                if kwargs.get('remove_cdata', True):#remove cdata special tag\n+                    pieces.append(remove_tags(m.groups()[0], **kwargs))\n+                else:\n+                    pieces.append(piece)#conserve intact the cdata\n+            else:\n+                pieces.append(remove_tags(piece, **kwargs))\n+\n+        xml = \"\".join(pieces)\n+    else:\n+        xml = remove_tags(xml, **kwargs)\n+    return xml\n+\n+\n+def clean_markup(string, **kwargs):\n+    \"\"\"Clean (list of) strings removing newlines, spaces, etc\"\"\"\n+    \n+    _remove_tags = xml_remove_tags if kwargs.get(\"xml_doc\") else remove_tags\n+\n+    if isinstance(string, list):\n+        return [clean_markup(s, **kwargs) for s in string]\n+    \n+    string = _remove_tags(string, **kwargs)\n+\n+    if kwargs.get('remove_root', True) and not kwargs.get('remove_tags', True):\n+        m = _remove_root_re.search(string)\n+        if m:\n+            string = m.group(1)\n+\n+    if kwargs.get('remove_spaces', True):\n+        string = _clean_spaces_re.sub(' ', string)\n+    if kwargs.get('strip', True):\n+        string = string.strip()\n+\n+    return string\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77053113cdd33d31c8939eb0d2403dd8dd36b2dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 70 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -19/0 | Churn Δ: 70 | Churn Cumulative: 422 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: 0.34146341463414637\n\nDIFF:\n@@ -10,7 +10,6 @@ from scrapy.utils.python import flatten, unique\n from scrapy.xpath import XPathSelector\n from scrapy.utils.misc import unquote_html, location_str\n from scrapy.conf import settings\n-from scrapy.utils.markup import clean_markup\n \n class ExtendedAdaptor(BaseAdaptor):\n \n@@ -66,17 +65,3 @@ class ExtractAdaptor(ExtendedAdaptor):\n             raise TypeError, \"unsupported location type: %s\" % type(location)\n \n         return strings\n\\ No newline at end of file\n-\n-class HtmlCleanAdaptor(ExtendedAdaptor):\n-\n-    def function(self, string, **pipeargs):\n-        return self.do(self.clean, string, **pipeargs)\n-    \n-    def clean(self, string, **pipeargs):\n-        return clean_markup(string, **pipeargs)\n-        \n-class XmlCleanAdaptor(HtmlCleanAdaptor):\n-    \n-    def clean(self, string, **pipeargs):\n-        pipeargs[\"xml_doc\"] = True\n-        return clean_markup(string, **pipeargs)\n\n@@ -57,58 +57,3 @@ def replace_tags(text, token=''):\n     Always returns a unicode string.\n     \"\"\"\n     return _tag_re.sub(token, text.decode('utf-8'))\n-\n-_clean_spaces_re = re.compile(\"\\s+\", re.U)\n-_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n-_xml_remove_tags_re = re.compile(r'<[a-zA-Z\\/!][^>]*?>')\n-_xml_remove_cdata_re = re.compile('<!\\[CDATA\\[(.*)\\]\\]', re.S)\n-_xml_cdata_split_re = re.compile('(<!\\[CDATA\\[.*?\\]\\]>)', re.S)\n-\n-def remove_tags(xml, **kwargs):\n-    if kwargs.get('remove_tags', True):\n-        xml = _xml_remove_tags_re.sub(' ', xml)\n-    return xml\n-\n-def xml_remove_tags(xml, **kwargs):\n-    #process in pieces the text that contains CDATA. The first check is to avoid unnecesary regex check\n-    if _xml_remove_cdata_re.search(xml):\n-        pieces = []\n-        \n-        for piece in _xml_cdata_split_re.split(xml):\n-            \n-            m = _xml_remove_cdata_re.search(piece)\n-            if m:\n-                if kwargs.get('remove_cdata', True):#remove cdata special tag\n-                    pieces.append(remove_tags(m.groups()[0], **kwargs))\n-                else:\n-                    pieces.append(piece)#conserve intact the cdata\n-            else:\n-                pieces.append(remove_tags(piece, **kwargs))\n-\n-        xml = \"\".join(pieces)\n-    else:\n-        xml = remove_tags(xml, **kwargs)\n-    return xml\n-\n-\n-def clean_markup(string, **kwargs):\n-    \"\"\"Clean (list of) strings removing newlines, spaces, etc\"\"\"\n-    \n-    _remove_tags = xml_remove_tags if kwargs.get(\"xml_doc\") else remove_tags\n-\n-    if isinstance(string, list):\n-        return [clean_markup(s, **kwargs) for s in string]\n-    \n-    string = _remove_tags(string, **kwargs)\n-\n-    if kwargs.get('remove_root', True) and not kwargs.get('remove_tags', True):\n-        m = _remove_root_re.search(string)\n-        if m:\n-            string = m.group(1)\n-\n-    if kwargs.get('remove_spaces', True):\n-        string = _clean_spaces_re.sub(' ', string)\n-    if kwargs.get('strip', True):\n-        string = string.strip()\n-\n-    return string\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e83dcb588e40eed3def0037729f59ed0db71e28e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 109 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -20,10 +20,11 @@ def load_url(url, response):\n     itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n     item = itemcls()\n     vars['item'] = item\n-    vars['url'] = url\n-    vars['response'] = response\n     vars['xxs'] = XmlXPathSelector(response)\n     vars['hxs'] = HtmlXPathSelector(response)\n+    if url:\n+        vars['url'] = url\n+        vars['response'] = response\n         vars['spider'] = spiders.fromurl(url)\n     vars['get'] = get_url\n     return vars\n@@ -40,7 +41,7 @@ def print_vars(vars):\n \n class Command(ScrapyCommand):\n     def syntax(self):\n-        return \"<url>\"\n+        return \"[url]\"\n \n     def short_desc(self):\n         return \"Interactive scraping console\"\n@@ -53,8 +54,9 @@ class Command(ScrapyCommand):\n         pass\n \n     def run(self, args, opts):\n-        if not args:\n-            return False\n+        \n+        url = None\n+        if args:\n             url = args[0]\n \n         print \"Scrapy %s - Interactive scraping console\\n\" % scrapy.__version__\n@@ -62,6 +64,8 @@ class Command(ScrapyCommand):\n         print \"Enabling Scrapy extensions...\",\n         extensions.load()\n         print \"done\"\n+        response = None\n+        if url:\n             print \"Downloading URL...           \",\n             responses = fetch([url])\n             print \"done\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#397d3ff2476a9eb18796bf03262d90634ca2738e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 47 | Files Changed: 1 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 147 | Churn Cumulative: 256 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,43 +1,44 @@\n+from twisted.internet import reactor\n+\n import scrapy\n from scrapy.command import ScrapyCommand\n-from scrapy.fetcher import fetch\n from scrapy.spider import spiders\n from scrapy.xpath import XmlXPathSelector, HtmlXPathSelector\n from scrapy.utils.misc import load_class\n from scrapy.extension import extensions\n from scrapy.conf import settings\n+from scrapy.core.manager import scrapymanager\n \n-def get_url(url):\n-    from scrapy.http import Request\n-    from scrapy.core.downloader.handlers import download_any\n-    request = Request(url)\n-    spider = spiders.fromurl(url)\n+#This code comes from twisted 8. We define here while\n+#using old twisted version.\n+def blockingCallFromThread(reactor, f, *a, **kw):\n+    \"\"\"\n+    Run a function in the reactor from a thread, and wait for the result\n+    synchronously, i.e. until the callback chain returned by the function\n+    get a result.\n \n-    return download_any(request, spider)\n+    @param reactor: The L{IReactorThreads} provider which will be used to\n+        schedule the function call.\n+    @param f: the callable to run in the reactor thread\n+    @type f: any callable.\n+    @param a: the arguments to pass to C{f}.\n+    @param kw: the keyword arguments to pass to C{f}.\n \n-def load_url(url, response):\n-    vars = {}\n-    itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n-    item = itemcls()\n-    vars['item'] = item\n-    vars['xxs'] = XmlXPathSelector(response)\n-    vars['hxs'] = HtmlXPathSelector(response)\n-    if url:\n-        vars['url'] = url\n-        vars['response'] = response\n-        vars['spider'] = spiders.fromurl(url)\n-    vars['get'] = get_url\n-    return vars\n-\n-def print_vars(vars):\n-    print '-' * 78\n-    print \"Available local variables:\"\n-    for key, val in vars.iteritems():\n-        if isinstance(val, basestring):\n-            print \"   %s: %s\" % (key, val)\n-        else:\n-            print \"   %s: %s\" % (key, val.__class__)\n-    print '-' * 78\n+    @return: the result of the callback chain.\n+    @raise: any error raised during the callback chain.\n+    \"\"\"\n+    import Queue\n+    from twisted.python import failure\n+    from twisted.internet import defer\n+    queue = Queue.Queue()\n+    def _callFromThread():\n+        result = defer.maybeDeferred(f, *a, **kw)\n+        result.addBoth(queue.put)\n+    reactor.callFromThread(_callFromThread)\n+    result = queue.get()\n+    if isinstance(result, failure.Failure):\n+        result.raiseException()\n+    return result\n \n class Command(ScrapyCommand):\n     def syntax(self):\n@@ -49,12 +50,66 @@ class Command(ScrapyCommand):\n     def long_desc(self):\n         return \"Interactive console for scraping the given url. For scraping local files you can use a URL like file://path/to/file.html\"\n \n-    def update_vars(self, vars):\n+    def update_vars(self):\n         \"\"\" You can use this function to update the local variables that will be available in the scrape console \"\"\"\n         pass\n \n-    def run(self, args, opts):\n+    def get_url(self, url):\n+        #def _get_callback(_response):\n+            #print \"done\"\n+            #if not _response:\n+                #if not opts.loglevel:\n+                    #print 'Nothing downloaded, run with -o DEBUG to see why it failed'\n+                #scrapymanager.stop()\n+                #return\n+            #self.generate_vars(url, response)\n \n+        #def _errback(_failure):\n+            #print _failure\n+\n+        print \"Downloading URL...           \",\n+        from scrapy.http import Request, Response\n+        from scrapy.core.downloader.handlers import download_any\n+        from scrapy.fetcher import get_or_create_spider\n+        r = Request(url)\n+        spider = get_or_create_spider(url)\n+        try:\n+            result = blockingCallFromThread(reactor, download_any, r, spider)\n+            if isinstance(result, Response):\n+                print \"OK\"\n+                self.generate_vars(url, result)\n+        except Exception, e:\n+            print \"Error: %s\" % e\n+            \n+    def generate_vars(self, url, response):\n+        itemcls = load_class(settings['DEFAULT_ITEM_CLASS'])\n+        item = itemcls()\n+        self.vars['item'] = item\n+        if url:\n+            self.vars['xxs'] = XmlXPathSelector(response)\n+            self.vars['hxs'] = HtmlXPathSelector(response)\n+            self.vars['url'] = url\n+            self.vars['response'] = response\n+            self.vars['spider'] = spiders.fromurl(url)\n+        self.vars['get'] = self.get_url\n+        self.vars['scrape_help'] = self.print_vars\n+        self.update_vars()\n+        self.user_ns.update(self.vars)\n+        self.print_vars()\n+        \n+    def print_vars(self):\n+        print '-' * 78\n+        print \"Available local variables:\"\n+        for key, val in self.vars.iteritems():\n+            if isinstance(val, basestring):\n+                print \"   %s: %s\" % (key, val)\n+            else:\n+                print \"   %s: %s\" % (key, val.__class__)\n+        print '-' * 78\n+    \n+    def run(self, args, opts):\n+        self.vars = {}\n+        self.user_ns = {}\n         url = None\n         if args:\n             url = args[0]\n@@ -64,25 +119,20 @@ class Command(ScrapyCommand):\n         print \"Enabling Scrapy extensions...\",\n         extensions.load()\n         print \"done\"\n-        response = None\n-        if url:\n-            print \"Downloading URL...           \",\n-            responses = fetch([url])\n-            print \"done\"\n-            if not responses:\n-                if not opts.loglevel:\n-                    print 'Nothing downloaded, run with -o DEBUG to see why it failed'\n-                return\n-            response = responses[0]\n \n-        vars = load_url(url, response)\n-        self.update_vars(vars)\n-        print_vars(vars)\n+        def _console_thread():\n+\n+            if url:\n+                self.get_url(url)\n+            else:\n+                self.generate_vars(url, None)\n+            \n             \n             try: # use IPython if available\n                 import IPython\n-            shell = IPython.Shell.IPShell(argv=[], user_ns=vars)\n+                shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n                 shell.mainloop()\n+                reactor.callFromThread(scrapymanager.stop)\n             except ImportError:\n                 import code\n                 try: # readline module is only available on unix systems\n@@ -92,4 +142,7 @@ class Command(ScrapyCommand):\n                 else:\n                     import rlcompleter\n                     readline.parse_and_bind(\"tab:complete\")\n-            code.interact(local=vars)\n+                code.interact(local=self.vars)\n+\n+        reactor.callInThread(_console_thread)\n+        scrapymanager.start()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eee86b98273958cb46e8da5b3c1826bea7d446a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 2 | Methods Changed: 9 | Complexity Δ (Sum/Max): 19/13 | Churn Δ: 96 | Churn Cumulative: 96 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,73 @@\n+\"\"\"\n+SimpageMiddleware is a middleware for detecting similar page layouts\n+\"\"\"\n+\n+import sys\n+import datetime\n+import pprint\n+\n+from pydispatch import dispatcher\n+\n+from scrapy.core import signals\n+from scrapy.http import Response\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.conf import settings\n+\n+from .metrics import tagdepth\n+\n+class SimpagesMiddleware(object):\n+    \n+    metric = tagdepth\n+    threshold = 0.8\n+\n+    def __init__(self):\n+        if not settings.getbool('SIMPAGES_ENABLED'):\n+            raise NotConfigured\n+        repfilename = settings.get('SIMPAGES_REPORT_FILE')\n+        self.reportfile = open(repfilename, \"a\") if repfilename else None\n+        self.sim_groups = {}\n+        self.last_group = 0\n+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+\n+    def process_response(self, request, response, spider):\n+        if isinstance(response, Response):\n+            group, simrate = self.get_similarity_group(response)\n+            if group:\n+                self.sim_groups[group]['similar_urls'].append((response.url, simrate))\n+            else:\n+                self.create_similarity_group(response)\n+        return response\n+\n+    def get_similarity_group(self, response):\n+        fp = self.metric.simhash(response)\n+        for group, data in self.sim_groups.iteritems():\n+            simrate = self.metric.compare(fp, data['simhash'])\n+            if simrate > self.threshold:\n+                return (group, simrate)\n+        return (None, 0)\n+\n+    def create_similarity_group(self, response):\n+        self.last_group += 1\n+        data = {}\n+        data['simhash'] = self.metric.simhash(response)\n+        data['first_url'] = response.url\n+        data['similar_urls'] = []\n+        self.sim_groups[self.last_group] = data\n+\n+    def get_report(self):\n+        r =  \"Page similarity results\\n\"\n+        r += \"=======================\\n\\n\"\n+        r += \"Datetime : %s\\n\" % datetime.datetime.now()\n+        r += \"Metric   : %s\\n\" % self.metric.__name__\n+        r += \"Threshold: %s\\n\" % self.threshold\n+        r += \"Results  :\\n\"\n+        r += pprint.pformat(self.sim_groups)\n+        r += \"\\n\\n\"\n+        return r\n+\n+    def engine_stopped(self):\n+        rep = self.get_report()\n+        if self.reportfile:\n+            self.reportfile.write(rep)\n+        else:\n+            print rep\n\n\n@@ -0,0 +1,23 @@\n+#!/usr/bin/env python\n+from __future__ import division\n+\n+from BeautifulSoup import BeautifulSoup, Tag\n+\n+relevant_tags = ['div', 'table', 'td', 'tr', 'h1']\n+\n+def get_symbol_dict(node, tags=(), depth=1):\n+    symdict = {}\n+    for tag in node:\n+        if isinstance(tag, Tag) and tag.name in tags:\n+            symbol = str(\"%d%s\" % (depth, tag.name))\n+            symdict[symbol] = symdict.setdefault(symbol, 0) + 1\n+            symdict.update(get_symbol_dict(tag, tags, depth+1))\n+    return symdict\n+\n+def simhash(response):\n+    soup = BeautifulSoup(response.body.to_string())\n+    symdict = get_symbol_dict(soup.find('body'), relevant_tags)\n+    return set(symdict.keys())\n+\n+def compare(fp1, fp2):\n+    return len(fp1 & fp2) / len(fp1 | fp2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bfe6168f3b5ba2c2bb9db7ad7af436ca852062c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 143 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -39,9 +39,9 @@ class SimpagesMiddleware(object):\n         return response\n \n     def get_similarity_group(self, response):\n-        fp = self.metric.simhash(response)\n+        sh = self.metric.simhash(response)\n         for group, data in self.sim_groups.iteritems():\n-            simrate = self.metric.compare(fp, data['simhash'])\n+            simrate = self.metric.compare(sh, data['simhash'])\n             if simrate > self.threshold:\n                 return (group, simrate)\n         return (None, 0)\n\n@@ -0,0 +1,19 @@\n+\"\"\"\n+This module contains several metrics that can be used with the\n+SimpagesMiddleware.\n+\n+A metric must implement two functions:\n+\n+1. simhash(response)\n+\n+Receives a response and returns a simhash of that response. A simhash can be\n+an object of any type and its only purpose is to provide a fast way for\n+comparing the [simhashed] response with another responses (that will also be\n+simhashed).\n+\n+2. compare(simhash1, simhash2)\n+\n+Receives two simhashes and must return a (float) value between 0 and 1,\n+depending on how similar the two simhashes (and, thus, the responses they\n+represent) are. 0 means completely different, 1 means identical.\n+\"\"\"\n\n@@ -1,23 +1,31 @@\n-#!/usr/bin/env python\n+\"\"\"\n+tagdepth metric\n+\n+Compares pages analyzing a predefined set of (relevant)\n+tags and the depth where they appear in the page markup document. \n+\n+Requires ResponseSoup extension enabled.\n+\"\"\"\n+\n from __future__ import division\n \n-from BeautifulSoup import BeautifulSoup, Tag\n+from BeautifulSoup import Tag\n \n-relevant_tags = ['div', 'table', 'td', 'tr', 'h1']\n+relevant_tags = set(['div', 'table', 'td', 'tr', 'h1'])\n \n def get_symbol_dict(node, tags=(), depth=1):\n     symdict = {}\n     for tag in node:\n         if isinstance(tag, Tag) and tag.name in tags:\n-            symbol = str(\"%d%s\" % (depth, tag.name))\n+            symbol = \"%d%s\" % (depth, str(tag.name))\n             symdict[symbol] = symdict.setdefault(symbol, 0) + 1\n             symdict.update(get_symbol_dict(tag, tags, depth+1))\n     return symdict\n \n def simhash(response):\n-    soup = BeautifulSoup(response.body.to_string())\n-    symdict = get_symbol_dict(soup.find('body'), relevant_tags)\n+    soup = response.soup\n+    symdict = get_symbol_dict(response.soup.find('body'), relevant_tags)\n     return set(symdict.keys())\n \n-def compare(fp1, fp2):\n-    return len(fp1 & fp2) / len(fp1 | fp2)\n+def compare(sh1, sh2):\n+    return len(sh1 & sh2) / len(sh1 | sh2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b11b84fff136a4c18849b5d5dad974c93b5c61da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 25/24 | Churn Δ: 25 | Churn Cumulative: 104 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,6 +78,7 @@ class Command(ScrapyCommand):\n             if isinstance(result, Response):\n                 print \"OK\"\n                 self.generate_vars(url, result)\n+                return True\n         except Exception, e:\n             print \"Error: %s\" % e\n             \n@@ -91,8 +92,6 @@ class Command(ScrapyCommand):\n             self.vars['url'] = url\n             self.vars['response'] = response\n             self.vars['spider'] = spiders.fromurl(url)\n-        self.vars['get'] = self.get_url\n-        self.vars['scrape_help'] = self.print_vars\n         self.update_vars()\n         self.user_ns.update(self.vars)\n         self.print_vars()\n@@ -105,6 +104,9 @@ class Command(ScrapyCommand):\n                 print \"   %s: %s\" % (key, val)\n             else:\n                 print \"   %s: %s\" % (key, val.__class__)\n+        print \"Available commands:\"\n+        print \"   get <url>: Fetches an url and updates all variables.\"\n+        print \"   scrapehelp: Prints this help.\"\n         print '-' * 78\n     \n     def run(self, args, opts):\n@@ -122,15 +124,23 @@ class Command(ScrapyCommand):\n \n         def _console_thread():\n             \n-            if url:\n+            def _get_magic(shell, url):\n                 self.get_url(url)\n+            def _help_magic(shell, _):\n+                self.print_vars()\n+                \n+            if url:\n+                result = self.get_url(url)\n+                if not result:\n+                    self.generate_vars(None, None)\n             else:\n-                self.generate_vars(url, None)\n-            \n-            \n+                self.generate_vars(None, None)\n             try: # use IPython if available\n                 import IPython\n                 shell = IPython.Shell.IPShell(argv=[], user_ns=self.user_ns)\n+                ip = shell.IP.getapi()\n+                ip.expose_magic(\"get\", _get_magic)\n+                ip.expose_magic(\"scrapehelp\", _help_magic)\n                 shell.mainloop()\n                 reactor.callFromThread(scrapymanager.stop)\n             except ImportError:\n\n@@ -41,6 +41,9 @@ class LiveStats(object):\n         self.domains[spider.domain_name].scraped += 1\n \n     def response_downloaded(self, response, spider):\n+        #sometimes we download responses without opening/closing domains,\n+        #for example from scrapy shell\n+        if self.domains.get(spider.domain_name):\n             self.domains[spider.domain_name].crawled += 1\n             \n     def webconsole_render(self, wc_request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9164150bed4b40b2aa15fd50c5666bbf1f2f3f68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 191 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,6 +36,7 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--pages\", dest=\"pages\", action=\"store_true\", help=\"update all the pages in the replay file, recording it again.\")\n \n     def process_options(self, args, opts):\n+        if args:\n             ScrapyCommand.process_options(self, args, opts)\n             self.opts = opts\n             self.action = args[1] if len(args) > 1 else 'crawl'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e6562cb478b3574ac03cbd0ef22f91e828c2ae9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 78 | Churn Cumulative: 161 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.4594594594594595\n\nDIFF:\n@@ -7,6 +7,7 @@ import re\n import urlparse\n import urllib\n import posixpath\n+import cgi\n \n def url_is_from_any_domain(url, domains):\n     \"\"\"Return True if the url belongs to the given domain\"\"\"\n@@ -68,7 +69,7 @@ def safe_download_url(url):\n     to be within the document root.\n     \"\"\"\n     safe_url = safe_url_string(url)\n-    scheme, netloc, path, query, fragment = urlparse.urlsplit(safe_url)\n+    scheme, netloc, path, query, _ = urlparse.urlsplit(safe_url)\n     if path:\n         path = _parent_dirs.sub('', posixpath.normpath(path))\n         if url.endswith('/') and not path.endswith('/'):\n@@ -81,3 +82,78 @@ def safe_download_url(url):\n def is_url(text):\n     return text.partition(\"://\")[0] in ('file', 'http', 'https')\n \n+def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n+    \"\"\" Return the given query parameter in the url.\n+    For example:\n+    >>> url_query_parameter(\"product.html?id=200&foo=bar\", \"id\")\n+    '200'\n+    >>> url_query_parameter(\"product.html?id=200&foo=bar\", \"notthere\", \"mydefault\")\n+    'mydefault'\n+    >>> url_query_parameter(\"product.html?id=\", \"id\")\n+    >>> url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1)\n+    ''\n+    \"\"\"\n+    queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)\n+    return queryparams.get(parameter, [default])[0] \n+\n+def url_query_cleaner(url, parameterlist=None, sep='&', kvsep='='):\n+    \"\"\" Return the given url with given query parameters.\n+    >>> url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", 'id')\n+    'product.html?id=200'\n+    >>> url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name'])\n+    'product.html?id=200&name=wired'\n+    \"\"\"\n+    parameterlist = parameterlist or []\n+    if not isinstance(parameterlist, (list, tuple)):\n+        parameterlist = [parameterlist]\n+     \n+    try:\n+        base, query = url.split('?', 1)\n+        parameters = [pair.split(kvsep, 1) for pair in query.split(sep)]\n+    except:\n+        base = url\n+        query = \"\"\n+        parameters = []\n+    \n+    # unique parameters while keeping order\n+    unique = {}\n+    querylist = []\n+    for pair in parameters: \n+        k = pair[0]\n+        if not unique.get(k):\n+            querylist += [pair]\n+            unique[k] = 1\n+    \n+    query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in parameterlist])\n+    return '?'.join([base, query])\n+    \n+def _has_querystring(url):\n+    _, _, _, query, _ = urlparse.urlsplit(url)\n+    return bool(query)\n+\n+def add_or_replace_parameter(url, name, new_value, sep='&'):\n+    \"\"\"\n+    >>> url = 'http://domain/test'\n+    >>> add_or_replace_parameter(url, 'arg', 'v')\n+    'http://domain/test?arg=v'\n+    >>> url = 'http://domain/test?arg1=v1&arg2=v2&arg3=v3'\n+    >>> add_or_replace_parameter(url, 'arg4', 'v4')\n+    'http://domain/test?arg1=v1&arg2=v2&arg3=v3&arg4=v4'\n+    >>> add_or_replace_parameter(url, 'arg3', 'nv3')\n+    'http://domain/test?arg1=v1&arg2=v2&arg3=nv3'\n+    >>> url = 'http://domain/test?arg1=v1'\n+    >>> add_or_replace_parameter(url, 'arg2', 'v2', sep=';')\n+    'http://domain/test?arg1=v1;arg2=v2'\n+    >>> add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20')\n+    'http://domain/moreInfo.asp?prodID=20'\n+    \"\"\"\n+    parameter = url_query_parameter(url, name, keep_blank_values=1)\n+    if parameter is None:\n+        if _has_querystring(url):\n+            next_url = url + sep + name + '=' + new_value\n+        else:\n+            next_url = url + '?' + name + '=' + new_value\n+    else:\n+        next_url = url.replace(name+'='+parameter,\n+                               name+'='+new_value)\n+    return next_url\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f3013bb9adabbe6bc1dc2a7f131e487e9f9f4421", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 7 | Methods Changed: 12 | Complexity Δ (Sum/Max): 14/16 | Churn Δ: 96 | Churn Cumulative: 158 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,73 @@\n+import re\n+\n+class DuplicatedAdaptorName(Exception):\n+    def __init__(self, name):\n+        self.__name = name\n+    def __str__(self):\n+        return name\n+\n+class Adaptor(object):\n+    \"\"\"\n+    Adaptors instances should be instantiated and used only\n+    inside the AdaptorPipe.\n+    \"\"\"\n+    def __init__(self, function, name, attribute_re=None):\n+        self.name = name\n+        self.basefunction = function\n+        if not attribute_re:\n+            attribute_re = \".*\"\n+        self.attribute_re = re.compile(attribute_re)\n+    def function(self, value, **pipeargs):\n+        return self.basefunction(value, **pipeargs)\n+        \n+class AdaptorPipe:\n+    def __init__(self, define_from=None, adaptorclass=None):\n+        \"\"\"\n+        If \"define_from\" is given, constructs pipeline from this.\n+        \"define_from\" is an ordered tuple of triplets, each of which\n+        has the attribute name regex, the adaptor name, and the module\n+        path to the adaptor function. Example:\n+        (\n+          (\"url\", \"remove_entities\", \"scrapy.utils.markup.remove_entities\")\n+          (\".*\", \"replace_tags\", \"scrapy.utils.markup.replace_tags\")\n+          ...\n+        )\n+        \"\"\"\n+        self.__adaptorspipe = []\n+        self.__adaptorclass = adaptorclass or Adaptor\n+    @property\n+    def adaptors_names(self):\n+        _adaptors = []\n+        for a in self.__adaptorspipe:\n+            _adaptors.append(a.name)\n+        return _adaptors\n+    \n+    def insertadaptor(self, function, name, attrs_re=None, after=None, before=None):\n+        \"\"\"\n+        Inserts a \"function\" as an adaptor that will apply for attribute names\n+        which matches \"attrs_re\" regex, after adaptor of name \"after\", or before\n+        adaptor of name \"before\". The \"function\" must always have a **keyword\n+        argument to ignore unused keywords. \"name\" is the name of the adaptor.\n+        \"\"\"\n+        if name in self.adaptors_names:\n+            raise DuplicatedAdaptorName(name)\n+        else:\n+            adaptor = self.__adaptorclass(function, name, attrs_re)\n+            #by default append adaptor at end of pipe\n+            pos = len(self.adaptors_names)\n+            if after:\n+                pos = self.adaptors_names.index(after) + 1\n+            elif before:\n+                pos = self.adaptors_names.index(before)\n+            self.__adaptorspipe.insert(pos, adaptor)\n+            return pos\n+\n+    def execute(self, attrname, value, **pipeargs):\n+        \"\"\"\n+        Execute pipeline for attribute name \"attrname\" and value \"value\".\n+        Pass the given pipeargs to each adaptor function in the pipe.\n+        \"\"\"\n+        for adaptor in self.__adaptorspipe:\n+            if adaptor.attribute_re.search(attrname):\n+                value = adaptor.function(value, **pipeargs)\n+        return value\n\\ No newline at end of file\n\n@@ -1,14 +1,13 @@\n-class BaseAdaptor(object):\n-    def function(self, item, value, **pipeargs):\n-        raise NotImplemented\n+from scrapy.item.adaptors import AdaptorPipe\n \n-#default adaptors\n-class ExtractAdaptor(BaseAdaptor):\n-    def function(self, item, value, **pipeargs):\n+def extract(value):\n     if hasattr(value, 'extract'):\n         value = value.extract()\n     return value\n     \n+standardpipe = AdaptorPipe()\n+standardpipe.insertadaptor(extract, \"extract\")\n+\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n@@ -17,14 +16,12 @@ class ScrapedItem(object):\n     * guid (unique global indentifier)\n     * url (URL where that item was scraped from)\n     \"\"\"\n-    adaptors_pipe = [ExtractAdaptor()]\n+    adaptors_pipe = standardpipe\n     \n     def set_adaptors_pipe(adaptors_pipes):\n         ScrapedItem.adaptors_pipes = adaptors_pipes\n \n-    def attribute(self, name, value, **pipeargs):\n-\n-        for adaptor in ScrapedItem.adaptors_pipe:\n-            value = adaptor.function(self, value, **pipeargs)\n-        if not hasattr(item, name):\n-            setattr(item, name, value)\n\\ No newline at end of file\n+    def attribute(self, attrname, value, **pipeargs):\n+        value =ScrapedItem.adaptors_pipe.execute(attrname, value, **pipeargs)\n+        if not hasattr(self, attrname):\n+            setattr(self, attrname, value)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34355048c3221e6dc314b17c94cdb0a0cb41eef6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 41 | Files Changed: 2 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 80 | Churn Cumulative: 289 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n import unittest\n-from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url\n+from scrapy.utils.url import url_is_from_any_domain, safe_url_string, safe_download_url, url_query_parameter, add_or_replace_parameter, url_query_cleaner\n \n class UrlUtilsTest(unittest.TestCase):\n \n@@ -43,6 +43,37 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertEqual(safe_download_url('http://www.scrapy.org/dir/'),\n                          'http://www.scrapy.org/dir/')\n \n+    def test_url_query_parameter(self):\n+        self.assertEqual(url_query_parameter(\"product.html?id=200&foo=bar\", \"id\"),\n+                         '200')\n+        self.assertEqual(url_query_parameter(\"product.html?id=200&foo=bar\", \"notthere\", \"mydefault\"),\n+                         'mydefault')\n+        self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\"),\n+                         None)\n+        self.assertEqual(url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1),\n+                         '')\n+\n+    def test_add_or_replace_parameter(self):\n+        url = 'http://domain/test'\n+        self.assertEqual(add_or_replace_parameter(url, 'arg', 'v'),\n+                         'http://domain/test?arg=v')\n+        url = 'http://domain/test?arg1=v1&arg2=v2&arg3=v3'\n+        self.assertEqual(add_or_replace_parameter(url, 'arg4', 'v4'),\n+                         'http://domain/test?arg1=v1&arg2=v2&arg3=v3&arg4=v4')\n+        self.assertEqual(add_or_replace_parameter(url, 'arg3', 'nv3'),\n+                         'http://domain/test?arg1=v1&arg2=v2&arg3=nv3')\n+        url = 'http://domain/test?arg1=v1'\n+        self.assertEqual(add_or_replace_parameter(url, 'arg2', 'v2', sep=';'),\n+                         'http://domain/test?arg1=v1;arg2=v2')\n+        self.assertEqual(add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20'),\n+                         'http://domain/moreInfo.asp?prodID=20')\n+\n+    def test_url_query_cleaner(self):\n+        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", 'id'),\n+                         'product.html?id=200')\n+        self.assertEqual(url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name']),\n+                         'product.html?id=200&name=wired')\n+\n if __name__ == \"__main__\":\n     unittest.main()\n \n\n@@ -78,35 +78,16 @@ def safe_download_url(url):\n         path = '/'\n     return urlparse.urlunsplit((scheme, netloc, path, query, ''))\n \n-\n def is_url(text):\n     return text.partition(\"://\")[0] in ('file', 'http', 'https')\n \n def url_query_parameter(url, parameter, default=None, keep_blank_values=0):\n-    \"\"\" Return the given query parameter in the url.\n-    For example:\n-    >>> url_query_parameter(\"product.html?id=200&foo=bar\", \"id\")\n-    '200'\n-    >>> url_query_parameter(\"product.html?id=200&foo=bar\", \"notthere\", \"mydefault\")\n-    'mydefault'\n-    >>> url_query_parameter(\"product.html?id=\", \"id\")\n-    >>> url_query_parameter(\"product.html?id=\", \"id\", keep_blank_values=1)\n-    ''\n-    \"\"\"\n+    \"\"\"Return the value of a url parameter, given the url and parameter name\"\"\"\n     queryparams = cgi.parse_qs(urlparse.urlsplit(str(url))[3], keep_blank_values=keep_blank_values)\n     return queryparams.get(parameter, [default])[0] \n \n-def url_query_cleaner(url, parameterlist=None, sep='&', kvsep='='):\n-    \"\"\" Return the given url with given query parameters.\n-    >>> url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", 'id')\n-    'product.html?id=200'\n-    >>> url_query_cleaner(\"product.html?id=200&foo=bar&name=wired\", ['id', 'name'])\n-    'product.html?id=200&name=wired'\n-    \"\"\"\n-    parameterlist = parameterlist or []\n-    if not isinstance(parameterlist, (list, tuple)):\n-        parameterlist = [parameterlist]\n-     \n+def url_query_cleaner(url, parameterlist=(), sep='&', kvsep='='):\n+    \"\"\"Clean url arguments leaving only those passed in the parameterlist\"\"\"\n     try:\n         base, query = url.split('?', 1)\n         parameters = [pair.split(kvsep, 1) for pair in query.split(sep)]\n@@ -127,29 +108,15 @@ def url_query_cleaner(url, parameterlist=None, sep='&', kvsep='='):\n     query = sep.join([kvsep.join(pair) for pair in querylist if pair[0] in parameterlist])\n     return '?'.join([base, query])\n     \n-def _has_querystring(url):\n+def add_or_replace_parameter(url, name, new_value, sep='&'):\n+    \"\"\"Add or remove a parameter to a given url\"\"\"\n+    def has_querystring(url):\n         _, _, _, query, _ = urlparse.urlsplit(url)\n         return bool(query)\n \n-def add_or_replace_parameter(url, name, new_value, sep='&'):\n-    \"\"\"\n-    >>> url = 'http://domain/test'\n-    >>> add_or_replace_parameter(url, 'arg', 'v')\n-    'http://domain/test?arg=v'\n-    >>> url = 'http://domain/test?arg1=v1&arg2=v2&arg3=v3'\n-    >>> add_or_replace_parameter(url, 'arg4', 'v4')\n-    'http://domain/test?arg1=v1&arg2=v2&arg3=v3&arg4=v4'\n-    >>> add_or_replace_parameter(url, 'arg3', 'nv3')\n-    'http://domain/test?arg1=v1&arg2=v2&arg3=nv3'\n-    >>> url = 'http://domain/test?arg1=v1'\n-    >>> add_or_replace_parameter(url, 'arg2', 'v2', sep=';')\n-    'http://domain/test?arg1=v1;arg2=v2'\n-    >>> add_or_replace_parameter(\"http://domain/moreInfo.asp?prodID=\", 'prodID', '20')\n-    'http://domain/moreInfo.asp?prodID=20'\n-    \"\"\"\n     parameter = url_query_parameter(url, name, keep_blank_values=1)\n     if parameter is None:\n-        if _has_querystring(url):\n+        if has_querystring(url):\n             next_url = url + sep + name + '=' + new_value\n         else:\n             next_url = url + '?' + name + '=' + new_value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
