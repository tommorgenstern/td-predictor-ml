{"custom_id": "keras#1788a0a901efccfffcead3d64a5b8e5876998d32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 149 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -24,16 +24,11 @@ MODEL_LIST = [\n     (applications.MobileNetV2, 1280),\n     (applications.DenseNet121, 1024),\n     (applications.DenseNet169, 1664),\n-    (applications.DenseNet201, 1920)\n+    (applications.DenseNet201, 1920),\n+    # Note that NASNetLarge is too heavy to test on Travis.\n+    (applications.NASNetMobile, 1056)\n ]\n \n-# TODO: enable Nasnet for Theano and CNTK\n-if K.backend() == 'tensorflow':\n-    MODEL_LIST.extend([\n-        (applications.NASNetMobile, 1056),\n-        (applications.NASNetLarge, 4032),\n-    ])\n-\n \n def _get_output_shape(model_fn):\n     if K.backend() == 'cntk':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#57fba26d8cb8434a14140c0ea65c2c71a14636bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 133 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 38 | Methods Changed: 11 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 159 | Churn Cumulative: 3008 | Contributors (this commit): 26 | Commits (past 90d): 10 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,39 +17,51 @@ class _Pooling1D(Layer):\n     \"\"\"\n \n     def __init__(self, pool_size=2, strides=None,\n-                 padding='valid', **kwargs):\n+                 padding='valid', data_format=None, **kwargs):\n         super(_Pooling1D, self).__init__(**kwargs)\n         if strides is None:\n             strides = pool_size\n         self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')\n         self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n+        self.data_format = K.normalize_data_format(data_format)\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n-        length = conv_utils.conv_output_length(input_shape[1],\n+        if self.data_format == 'channels_first':\n+            steps = input_shape[2]\n+            features = input_shape[1]\n+        else:\n+            steps = input_shape[1]\n+            features = input_shape[2]\n+        length = conv_utils.conv_output_length(steps,\n                                                self.pool_size[0],\n                                                self.padding,\n                                                self.strides[0])\n-        return (input_shape[0], length, input_shape[2])\n+        if self.data_format == 'channels_first':\n+            return (input_shape[0], features, length)\n+        else:\n+            return (input_shape[0], length, features)\n \n     def _pooling_function(self, inputs, pool_size, strides,\n                           padding, data_format):\n         raise NotImplementedError\n \n     def call(self, inputs):\n-        inputs = K.expand_dims(inputs, 2)   # add dummy last dimension\n+        dummy_axis = 2 if self.data_format == 'channels_last' else 3\n+        inputs = K.expand_dims(inputs, dummy_axis)   # add dummy last dimension\n         output = self._pooling_function(inputs=inputs,\n                                         pool_size=self.pool_size + (1,),\n                                         strides=self.strides + (1,),\n                                         padding=self.padding,\n-                                        data_format='channels_last')\n-        return K.squeeze(output, 2)  # remove dummy last dimension\n+                                        data_format=self.data_format)\n+        return K.squeeze(output, dummy_axis)  # remove dummy last dimension\n \n     def get_config(self):\n         config = {'strides': self.strides,\n                   'pool_size': self.pool_size,\n-                  'padding': self.padding}\n+                  'padding': self.padding,\n+                  'data_format': self.data_format}\n         base_config = super(_Pooling1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -63,19 +75,40 @@ class MaxPooling1D(_Pooling1D):\n             E.g. 2 will halve the input.\n             If None, it will default to `pool_size`.\n         padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, steps, features)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, features, steps)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n \n     # Input shape\n-        3D tensor with shape: `(batch_size, steps, features)`.\n+        - If `data_format='channels_last'`:\n+            3D tensor with shape:\n+            `(batch_size, steps, features)`\n+        - If `data_format='channels_first'`:\n+            3D tensor with shape:\n+            `(batch_size, features, steps)`\n \n     # Output shape\n-        3D tensor with shape: `(batch_size, downsampled_steps, features)`.\n+        - If `data_format='channels_last'`:\n+            3D tensor with shape:\n+            `(batch_size, downsampled_steps, features)`\n+        - If `data_format='channels_first'`:\n+            3D tensor with shape:\n+            `(batch_size, features, downsampled_steps)`\n     \"\"\"\n \n     @interfaces.legacy_pooling1d_support\n     def __init__(self, pool_size=2, strides=None,\n-                 padding='valid', **kwargs):\n+                 padding='valid', data_format=None, **kwargs):\n         super(MaxPooling1D, self).__init__(pool_size, strides,\n-                                           padding, **kwargs)\n+                                           padding, data_format,\n+                                           **kwargs)\n \n     def _pooling_function(self, inputs, pool_size, strides,\n                           padding, data_format):\n@@ -93,19 +126,40 @@ class AveragePooling1D(_Pooling1D):\n             E.g. 2 will halve the input.\n             If None, it will default to `pool_size`.\n         padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, steps, features)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, features, steps)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n \n     # Input shape\n-        3D tensor with shape: `(batch_size, steps, features)`.\n+        - If `data_format='channels_last'`:\n+            3D tensor with shape:\n+            `(batch_size, steps, features)`\n+        - If `data_format='channels_first'`:\n+            3D tensor with shape:\n+            `(batch_size, features, steps)`\n \n     # Output shape\n-        3D tensor with shape: `(batch_size, downsampled_steps, features)`.\n+        - If `data_format='channels_last'`:\n+            3D tensor with shape:\n+            `(batch_size, downsampled_steps, features)`\n+        - If `data_format='channels_first'`:\n+            3D tensor with shape:\n+            `(batch_size, features, downsampled_steps)`\n     \"\"\"\n \n     @interfaces.legacy_pooling1d_support\n     def __init__(self, pool_size=2, strides=None,\n-                 padding='valid', **kwargs):\n+                 padding='valid', data_format=None, **kwargs):\n         super(AveragePooling1D, self).__init__(pool_size, strides,\n-                                               padding, **kwargs)\n+                                               padding, data_format,\n+                                               **kwargs)\n \n     def _pooling_function(self, inputs, pool_size, strides,\n                           padding, data_format):\n@@ -440,42 +494,70 @@ class _GlobalPooling1D(Layer):\n     \"\"\"Abstract class for different global pooling 1D layers.\n     \"\"\"\n \n-    def __init__(self, **kwargs):\n+    def __init__(self, data_format=None, **kwargs):\n         super(_GlobalPooling1D, self).__init__(**kwargs)\n         self.input_spec = InputSpec(ndim=3)\n+        self.data_format = K.normalize_data_format(data_format)\n \n     def compute_output_shape(self, input_shape):\n+        if self.data_format == 'channels_first':\n+            return (input_shape[0], input_shape[1])\n+        else:\n             return (input_shape[0], input_shape[2])\n \n     def call(self, inputs):\n         raise NotImplementedError\n \n+    def get_config(self):\n+        config = {'data_format': self.data_format}\n+        base_config = super(_GlobalPooling1D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n \n class GlobalAveragePooling1D(_GlobalPooling1D):\n     \"\"\"Global average pooling operation for temporal data.\n \n+    # Arguments\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, steps, features)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, features, steps)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+\n     # Input shape\n-        3D tensor with shape: `(batch_size, steps, features)`.\n+        - If `data_format='channels_last'`:\n+            3D tensor with shape:\n+            `(batch_size, steps, features)`\n+        - If `data_format='channels_first'`:\n+            3D tensor with shape:\n+            `(batch_size, features, steps)`\n \n     # Output shape\n         2D tensor with shape:\n         `(batch_size, features)`\n     \"\"\"\n \n-    def __init__(self, **kwargs):\n-        super(GlobalAveragePooling1D, self).__init__(**kwargs)\n+    def __init__(self, data_format=None, **kwargs):\n+        super(GlobalAveragePooling1D, self).__init__(data_format,\n+                                                     **kwargs)\n         self.supports_masking = True\n \n     def call(self, inputs, mask=None):\n+        steps_axis = 1 if self.data_format == 'channels_last' else 2\n         if mask is not None:\n             mask = K.cast(mask, K.floatx())\n             input_shape = K.int_shape(inputs)\n-            broadcast_shape = [-1, input_shape[1], 1]\n+            broadcast_shape = [-1, input_shape[steps_axis], 1]\n             mask = K.reshape(mask, broadcast_shape)\n             inputs *= mask\n-            return K.sum(inputs, axis=1) / K.sum(mask, axis=1)\n+            return K.sum(inputs, axis=steps_axis) / K.sum(mask, axis=steps_axis)\n         else:\n-            return K.mean(inputs, axis=1)\n+            return K.mean(inputs, axis=steps_axis)\n \n     def compute_mask(self, inputs, mask=None):\n         return None\n@@ -484,8 +566,25 @@ class GlobalAveragePooling1D(_GlobalPooling1D):\n class GlobalMaxPooling1D(_GlobalPooling1D):\n     \"\"\"Global max pooling operation for temporal data.\n \n+    # Arguments\n+        data_format: A string,\n+            one of `channels_last` (default) or `channels_first`.\n+            The ordering of the dimensions in the inputs.\n+            `channels_last` corresponds to inputs with shape\n+            `(batch, steps, features)` while `channels_first`\n+            corresponds to inputs with shape\n+            `(batch, features, steps)`.\n+            It defaults to the `image_data_format` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"channels_last\".\n+\n     # Input shape\n-        3D tensor with shape: `(batch_size, steps, features)`.\n+        - If `data_format='channels_last'`:\n+            3D tensor with shape:\n+            `(batch_size, steps, features)`\n+        - If `data_format='channels_first'`:\n+            3D tensor with shape:\n+            `(batch_size, features, steps)`\n \n     # Output shape\n         2D tensor with shape:\n@@ -493,7 +592,8 @@ class GlobalMaxPooling1D(_GlobalPooling1D):\n     \"\"\"\n \n     def call(self, inputs):\n-        return K.max(inputs, axis=1)\n+        steps_axis = 1 if self.data_format == 'channels_last' else 2\n+        return K.max(inputs, axis=steps_axis)\n \n \n class _GlobalPooling2D(Layer):\n\n@@ -117,9 +117,11 @@ def test_conv_1d():\n def test_maxpooling_1d():\n     for padding in ['valid', 'same']:\n         for stride in [1, 2]:\n+            for data_format in ['channels_first', 'channels_last']:\n                 layer_test(convolutional.MaxPooling1D,\n                            kwargs={'strides': stride,\n-                               'padding': padding},\n+                                   'padding': padding,\n+                                   'data_format': data_format},\n                            input_shape=(3, 5, 4))\n \n \n@@ -127,9 +129,11 @@ def test_maxpooling_1d():\n def test_averagepooling_1d():\n     for padding in ['valid', 'same']:\n         for stride in [1, 2]:\n+            for data_format in ['channels_first', 'channels_last']:\n                 layer_test(convolutional.AveragePooling1D,\n                            kwargs={'strides': stride,\n-                               'padding': padding},\n+                                   'padding': padding,\n+                                   'data_format': data_format},\n                            input_shape=(3, 5, 4))\n \n \n@@ -408,9 +412,12 @@ def test_depthwise_conv_2d():\n \n @keras_test\n def test_globalpooling_1d():\n+    for data_format in ['channels_first', 'channels_last']:\n         layer_test(pooling.GlobalMaxPooling1D,\n+                   kwargs={'data_format': data_format},\n                    input_shape=(3, 4, 5))\n         layer_test(pooling.GlobalAveragePooling1D,\n+                   kwargs={'data_format': data_format},\n                    input_shape=(3, 4, 5))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8796e04f8f8a814eb8ae3206624b5c4b47362f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 4489 | Contributors (this commit): 29 | Commits (past 90d): 9 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -463,11 +463,10 @@ class TestBackend(object):\n         # It doesn't check the functionality (which is checked at the\n         # test_gradient test).\n         val = np.random.random((4, 2))\n-        for k in BACKENDS:\n-            a = k.variable(val)\n-            b = k.square(a)\n-            c, d = k.stop_gradient([a, b])\n-            e = k.stop_gradient(b)\n+        a = K.variable(val)\n+        b = K.square(a)\n+        c, d = K.stop_gradient([a, b])\n+        e = K.stop_gradient(b)\n \n     # cntk currently not support function in this way, so can't test as this\n     def test_function(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb8b70addf6ae58ef17739b4c35cc22b1c4c56d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 159 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 11 | Methods Changed: 32 | Complexity Δ (Sum/Max): 39/36 | Churn Δ: 168 | Churn Cumulative: 4827 | Contributors (this commit): 29 | Commits (past 90d): 12 | Contributors (cumulative): 30 | DMM Complexity: 0.9340659340659341\n\nDIFF:\n@@ -8,7 +8,7 @@ from keras.utils.test_utils import keras_test\n from keras import backend as K\n from keras.backend import floatx, set_floatx, variable\n from keras.utils.conv_utils import convert_kernel\n-import reference_operations\n+import reference_operations as KNP\n \n \n BACKENDS = []  # Holds a list of all available back-ends\n@@ -108,6 +108,16 @@ def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, *\n     if shape_or_val:\n         x_shape, x_val = parse_shape_or_val(x_shape_or_val)\n \n+        # If we can take a NumPy output, it is efficient to compare the outputs\n+        # from a single backend and NumPy.\n+        try:\n+            assert_value_with_ref = getattr(KNP, function_name)(x_val, **kwargs)\n+            # Leave only the designated backend from the test list of backends.\n+            backend_list = [k for k in backend_list\n+                            if K.backend() == k.__name__.split('.')[-1][:-8]]\n+        except:\n+            assert_value_with_ref = None\n+\n     t_list = []\n     z_list = []\n     for k in backend_list:\n@@ -391,7 +401,7 @@ class TestBackend(object):\n \n         check_single_tensor_operation('std', (4, 2), BACKENDS)\n         check_single_tensor_operation('std', (4, 2), BACKENDS, axis=1, keepdims=True)\n-        check_single_tensor_operation('std', (4, 2, 3), BACKENDS, axis=[1, -1])\n+        # check_single_tensor_operation('std', (4, 2, 3), BACKENDS, axis=[1, -1])\n \n         check_single_tensor_operation('prod', (4, 2), BACKENDS)\n         check_single_tensor_operation('prod', (4, 2), BACKENDS, axis=1, keepdims=True)\n@@ -589,7 +599,7 @@ class TestBackend(object):\n         ]\n \n         for (i, kwargs) in enumerate(kwargs_list):\n-            last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n+            last_y1, y1, h1 = KNP.rnn(x, [wi, wh, None], h0, **kwargs)\n             last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n \n             assert len(h2) == 1\n@@ -659,7 +669,7 @@ class TestBackend(object):\n         ]\n \n         for (i, kwargs) in enumerate(kwargs_list):\n-            last_y1, y1, h1 = reference_operations.rnn(x, [wi, wh, None], h0, **kwargs)\n+            last_y1, y1, h1 = KNP.rnn(x, [wi, wh, None], h0, **kwargs)\n             last_y2, y2, h2 = K.rnn(rnn_fn, x_k, h0_k, **kwargs)\n \n             assert len(h2) == 2\n@@ -712,7 +722,7 @@ class TestBackend(object):\n             y_k = K.dot(x_k, wi_k)\n             return y_k, []\n \n-        last_y1, y1, h1 = reference_operations.rnn(x, [wi, None, None], None,\n+        last_y1, y1, h1 = KNP.rnn(x, [wi, None, None], None,\n                                   go_backwards=False, mask=None)\n         last_y2, y2, h2 = K.rnn(rnn_fn, x_k, [],\n                                 go_backwards=False, mask=None)\n@@ -1010,7 +1020,7 @@ class TestBackend(object):\n         k = K.backend()\n         _, x = parse_shape_or_val(input_shape)\n         _, w = parse_shape_or_val(kernel_shape)\n-        y1 = reference_operations.conv(x, w, padding, data_format)\n+        y1 = KNP.conv(x, w, padding, data_format)\n         y2 = check_two_tensor_operation(\n             op, x, w, [KTH if k == 'theano' else KC if k == 'cntk' else KTF],\n             padding=padding, data_format=data_format,\n@@ -1027,7 +1037,7 @@ class TestBackend(object):\n         k = K.backend()\n         _, x = parse_shape_or_val(input_shape)\n         _, w = parse_shape_or_val(kernel_shape)\n-        y1 = reference_operations.depthwise_conv(x, w, padding, data_format)\n+        y1 = KNP.depthwise_conv(x, w, padding, data_format)\n         y2 = check_two_tensor_operation(\n             op, x, w, [KTH if k == 'theano' else KC if k == 'cntk' else KTF],\n             padding=padding, data_format=data_format,\n@@ -1047,7 +1057,7 @@ class TestBackend(object):\n     def test_pool(self, op, input_shape, pool_size, strides, padding, data_format, pool_mode):\n         k = K.backend()\n         _, x = parse_shape_or_val(input_shape)\n-        y1 = reference_operations.pool(x, pool_size, strides, padding, data_format, pool_mode)\n+        y1 = KNP.pool(x, pool_size, strides, padding, data_format, pool_mode)\n         y2 = check_single_tensor_operation(\n             op, x, [KTH if k == 'theano' else KC if k == 'cntk' else KTF],\n             pool_size=pool_size, strides=strides,\n@@ -1114,7 +1124,7 @@ class TestBackend(object):\n         _, x = parse_shape_or_val(input_shape)\n         _, depthwise = parse_shape_or_val(kernel_shape + (input_depth, depth_multiplier))\n         _, pointwise = parse_shape_or_val((1,) * len(kernel_shape) + (input_depth * depth_multiplier, 7))\n-        y1 = reference_operations.separable_conv(x, depthwise, pointwise, padding, data_format)\n+        y1 = KNP.separable_conv(x, depthwise, pointwise, padding, data_format)\n         if K.backend() == 'cntk':\n             y2 = cntk_func_three_tensor(\n                 op, input_shape,\n\n@@ -166,3 +166,143 @@ def rnn(x, w, init, go_backwards=False, mask=None, unroll=False, input_length=No\n         h.append(h_t + h_t1)\n \n     return o[-1], np.stack(o, axis=1), np.stack(h, axis=1)\n+\n+\n+def relu(x, alpha=0., max_value=None):\n+    y = x * (x > 0) + alpha * x * (x < 0)\n+    if max_value is not None:\n+        y = np.minimum(y, max_value)\n+    return y\n+\n+\n+def softplus(x):\n+    return np.log(1. + np.exp(x))\n+\n+\n+def elu(x, alpha=1.):\n+    return x * (x > 0) + alpha * (np.exp(x) - 1.) * (x < 0)\n+\n+\n+def sigmoid(x):\n+    return 1. / (1. + np.exp(-x))\n+\n+\n+def hard_sigmoid(x):\n+    y = 0.2 * x + 0.5\n+    y = np.minimum(y, 1.)\n+    y = np.maximum(y, 0.)\n+    return y\n+\n+\n+def tanh(x):\n+    return np.tanh(x)\n+\n+\n+def softmax(x, axis=-1):\n+    y = np.exp(x - np.max(x, axis, keepdims=True))\n+    return y / np.sum(y, axis, keepdims=True)\n+\n+\n+def l2_normalize(x, axis=-1):\n+    y = np.max(np.sum(x ** 2, axis, keepdims=True), axis, keepdims=True)\n+    return x / np.sqrt(y)\n+\n+\n+def max(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        for a in axis:\n+            x = np.max(x, axis=a, keepdims=keepdims)\n+        return x\n+    else:\n+        return np.max(x, axis=axis, keepdims=keepdims)\n+\n+\n+def min(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        for a in axis:\n+            x = np.min(x, axis=a, keepdims=keepdims)\n+        return x\n+    else:\n+        return np.min(x, axis=axis, keepdims=keepdims)\n+\n+\n+def mean(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        for a in axis:\n+            x = np.mean(x, axis=a, keepdims=keepdims)\n+        return x\n+    else:\n+        return np.mean(x, axis=axis, keepdims=keepdims)\n+\n+\n+def std(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        for a in axis:\n+            x = np.std(x, axis=a, keepdims=keepdims)\n+        return x\n+    else:\n+        return np.std(x, axis=axis, keepdims=keepdims)\n+\n+\n+def sum(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        for a in axis:\n+            x = np.sum(x, axis=a, keepdims=keepdims)\n+        return x\n+    else:\n+        return np.sum(x, axis=axis, keepdims=keepdims)\n+\n+\n+def prod(x, axis=None, keepdims=False):\n+    if isinstance(axis, list):\n+        for a in axis:\n+            x = np.prod(x, axis=a, keepdims=keepdims)\n+        return x\n+    else:\n+        return np.prod(x, axis=axis, keepdims=keepdims)\n+\n+\n+def cumsum(x, axis=0):\n+    return np.cumsum(x, axis=axis)\n+\n+\n+def cumprod(x, axis=0):\n+    return np.cumprod(x, axis=axis)\n+\n+\n+def any(x, axis=None, keepdims=False):\n+    return np.any(x, axis=axis, keepdims=keepdims)\n+\n+\n+def all(x, axis=None, keepdims=False):\n+    return np.all(x, axis=axis, keepdims=keepdims)\n+\n+\n+def argmax(x, axis=-1):\n+    return np.argmax(x, axis=axis)\n+\n+\n+def argmin(x, axis=-1):\n+    return np.argmin(x, axis=axis)\n+\n+\n+def sqrt(x):\n+    y = np.sqrt(x)\n+    y[np.isnan(y)] = 0.\n+    return y\n+\n+\n+def pow(x, a=1.):\n+    return np.power(x, a)\n+\n+\n+def clip(x, min_value, max_value):\n+    return np.clip(x, min_value, max_value)\n+\n+\n+square = np.square\n+abs = np.abs\n+exp = np.exp\n+log = np.log\n+round = np.round\n+sign = np.sign\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d88f2006af35179b986479ac6ad5a20dac8ac9d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 4594 | Contributors (this commit): 34 | Commits (past 90d): 17 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -1515,27 +1515,17 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     x = _preprocess_conv2d_input(x, data_format)\n     kernel = _preprocess_conv2d_kernel(kernel, data_format)\n     padding = _preprocess_border_mode(padding)\n-    if dilation_rate == (1, 1):\n-        strides = (1,) + strides\n-        x = C.convolution(\n-            kernel,\n+\n+    if dev.type() == 0 and dilation_rate != (1, 1):\n+        raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. ' +\n+                         'Please set dilation_rate with (1, 1).')\n+\n+    x = C.convolution(kernel,\n                       x,\n                       strides,\n-            auto_padding=[\n-                False,\n-                padding,\n-                padding])\n-    else:\n-        assert dilation_rate[0] == dilation_rate[1]\n-        assert strides == (1, 1), 'Invalid strides for dilated convolution'\n-        x = C.convolution(\n-            kernel,\n-            x,\n-            strides=dilation_rate[0],\n-            auto_padding=[\n-                False,\n-                padding,\n-                padding])\n+                      auto_padding=[False, padding, padding],\n+                      dilation=dilation_rate)\n+\n     return _postprocess_conv2d_output(x, data_format)\n \n \n\n@@ -139,7 +139,7 @@ def test_averagepooling_1d():\n \n @keras_test\n @pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support dilated conv\")\n+                    reason=\"cntk only supports dilated conv on GPU\")\n def test_convolution_2d():\n     num_samples = 2\n     filters = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23bdd4d417f0fda90465e3ab612c682e1d207ee3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 246 | Lines Deleted: 163 | Files Changed: 5 | Hunks: 46 | Methods Changed: 37 | Complexity Δ (Sum/Max): 2/6 | Churn Δ: 409 | Churn Cumulative: 5848 | Contributors (this commit): 48 | Commits (past 90d): 19 | Contributors (cumulative): 74 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,7 @@ import sys\n import tarfile\n import threading\n import time\n-import traceback\n+import warnings\n import zipfile\n from abc import abstractmethod\n from contextlib import closing\n@@ -582,7 +582,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n                     yield inputs\n         except Exception as e:\n             self.stop()\n-            six.raise_from(StopIteration(e), e)\n+            six.reraise(*sys.exc_info())\n \n     def _send_sequence(self):\n         \"\"\"Send current Sequence to all workers.\"\"\"\n@@ -606,6 +606,37 @@ class OrderedEnqueuer(SequenceEnqueuer):\n         _SHARED_SEQUENCES[self.uid] = None\n \n \n+# Global variables to be shared across processes\n+_SHARED_GENERATOR = {}\n+# We use a Value to provide unique id to different processes.\n+_GENERATOR_COUNTER = None\n+\n+\n+def init_pool_generator(gens, random_seed=None):\n+    global _SHARED_GENERATOR\n+    _SHARED_GENERATOR = gens\n+\n+    if random_seed is not None:\n+        ident = mp.current_process().ident\n+        np.random.seed(random_seed + ident)\n+\n+\n+def next_sample(uid):\n+    \"\"\"Get the next value from the generator `uid`.\n+\n+    To allow multiple generators to be used at the same time, we use `uid` to\n+    get a specific one. A single generator would cause the validation to\n+    overwrite the training generator.\n+\n+    # Arguments\n+        uid: int, generator identifier\n+\n+    # Returns\n+        The next value of generator `uid`.\n+    \"\"\"\n+    return six.next(_SHARED_GENERATOR[uid])\n+\n+\n class GeneratorEnqueuer(SequenceEnqueuer):\n     \"\"\"Builds a queue out of a data generator.\n \n@@ -624,143 +655,75 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n \n     def __init__(self, generator,\n                  use_multiprocessing=False,\n-                 wait_time=0.05,\n-                 seed=None):\n-        self.wait_time = wait_time\n-        self._generator = generator\n-        if os.name is 'nt' and use_multiprocessing is True:\n-            # On Windows, avoid **SYSTEMATIC** error in `multiprocessing`:\n-            # `TypeError: can't pickle generator objects`\n-            # => Suggest multithreading instead of multiprocessing on Windows\n-            raise ValueError('Using a generator with `use_multiprocessing=True`'\n-                             ' is not supported on Windows (no marshalling of'\n-                             ' generators across process boundaries). Instead,'\n-                             ' use single thread/process or multithreading.')\n-        else:\n-            self._use_multiprocessing = use_multiprocessing\n-        self._threads = []\n-        self._stop_event = None\n-        self._manager = None\n-        self.queue = None\n-        self.seed = seed\n+                 wait_time=None,\n+                 random_seed=None):\n+        self.generator = generator\n+        self.use_multiprocessing = use_multiprocessing\n+        self.random_seed = random_seed\n+        if wait_time is not None:\n+            warnings.warn('`wait_time` is not used anymore.',\n+                          DeprecationWarning)\n \n-    def _data_generator_task(self):\n-        if self._use_multiprocessing is False:\n-            while not self._stop_event.is_set():\n-                with self.genlock:\n+        global _GENERATOR_COUNTER\n+        if _GENERATOR_COUNTER is None:\n             try:\n-                        if (self.queue is not None and\n-                                self.queue.qsize() < self.max_queue_size):\n-                            # On all OSes, avoid **SYSTEMATIC** error\n-                            # in multithreading mode:\n-                            # `ValueError: generator already executing`\n-                            # => Serialize calls to\n-                            # infinite iterator/generator's next() function\n-                            generator_output = next(self._generator)\n-                            self.queue.put((True, generator_output))\n+                _GENERATOR_COUNTER = mp.Value('i', 0)\n+            except OSError:\n+                # In this case the OS does not allow us to use\n+                # multiprocessing. We resort to an int\n+                # for enqueuer indexing.\n+                _GENERATOR_COUNTER = 0\n+\n+        if isinstance(_GENERATOR_COUNTER, int):\n+            self.uid = _GENERATOR_COUNTER\n+            _GENERATOR_COUNTER += 1\n         else:\n-                            time.sleep(self.wait_time)\n-                    except StopIteration:\n-                        break\n-                    except Exception as e:\n-                        # Can't pickle tracebacks.\n-                        # As a compromise, print the traceback and\n-                        # pickle None instead.\n-                        if not hasattr(e, '__traceback__'):\n-                            setattr(e, '__traceback__', sys.exc_info()[2])\n-                        self.queue.put((False, e))\n-                        self._stop_event.set()\n-                        break\n-        else:\n-            while not self._stop_event.is_set():\n-                try:\n-                    if (self.queue is not None and\n-                            self.queue.qsize() < self.max_queue_size):\n-                        generator_output = next(self._generator)\n-                        self.queue.put((True, generator_output))\n-                    else:\n-                        time.sleep(self.wait_time)\n-                except StopIteration:\n-                    break\n-                except Exception as e:\n-                    # Can't pickle tracebacks.\n-                    # As a compromise, print the traceback and pickle None instead.\n-                    traceback.print_exc()\n-                    setattr(e, '__traceback__', None)\n-                    self.queue.put((False, e))\n-                    self._stop_event.set()\n-                    break\n+            # Doing Multiprocessing.Value += x is not process-safe.\n+            with _GENERATOR_COUNTER.get_lock():\n+                self.uid = _GENERATOR_COUNTER.value\n+                _GENERATOR_COUNTER.value += 1\n+\n+        self.workers = 0\n+        self.executor_fn = None\n+        self.queue = None\n+        self.run_thread = None\n+        self.stop_signal = None\n+\n+    def is_running(self):\n+        return self.stop_signal is not None and not self.stop_signal.is_set()\n \n     def start(self, workers=1, max_queue_size=10):\n-        \"\"\"Kicks off threads which add data from the generator into the queue.\n+        \"\"\"Start the handler's workers.\n \n         # Arguments\n             workers: number of worker threads\n             max_queue_size: queue size\n-                (when full, threads could block on `put()`)\n+                (when full, workers could block on `put()`)\n         \"\"\"\n-        try:\n-            self.max_queue_size = max_queue_size\n-            if self._use_multiprocessing:\n-                self._manager = mp.Manager()\n-                self.queue = self._manager.Queue(maxsize=max_queue_size)\n-                self._stop_event = mp.Event()\n+        if self.use_multiprocessing:\n+            self.executor_fn = lambda gens: mp.Pool(workers,\n+                                                    initializer=init_pool_generator,\n+                                                    initargs=(gens,\n+                                                              self.random_seed))\n         else:\n-                # On all OSes, avoid **SYSTEMATIC** error in multithreading mode:\n-                # `ValueError: generator already executing`\n-                # => Serialize calls to infinite iterator/generator's next() function\n-                self.genlock = threading.Lock()\n-                self.queue = queue.Queue(maxsize=max_queue_size)\n-                self._stop_event = threading.Event()\n+            # We do not need the init since it's threads.\n+            self.executor_fn = lambda _: ThreadPool(workers)\n+        self.workers = workers\n+        self.queue = queue.Queue(max_queue_size)\n+        self.stop_signal = threading.Event()\n+        self.run_thread = threading.Thread(target=self._run)\n+        self.run_thread.daemon = True\n+        self.run_thread.start()\n \n-            for _ in range(workers):\n-                if self._use_multiprocessing:\n-                    # Reset random seed else all children processes\n-                    # share the same seed\n-                    np.random.seed(self.seed)\n-                    thread = mp.Process(target=self._data_generator_task)\n-                    thread.daemon = True\n-                    if self.seed is not None:\n-                        self.seed += 1\n-                else:\n-                    thread = threading.Thread(target=self._data_generator_task)\n-                self._threads.append(thread)\n-                thread.start()\n-        except:\n-            self.stop()\n-            raise\n-\n-    def is_running(self):\n-        return self._stop_event is not None and not self._stop_event.is_set()\n-\n-    def stop(self, timeout=None):\n-        \"\"\"Stops running threads and wait for them to exit, if necessary.\n-\n-        Should be called by the same thread which called `start()`.\n-\n-        # Arguments\n-            timeout: maximum time to wait on `thread.join()`.\n-        \"\"\"\n-        if self.is_running():\n-            self._stop_event.set()\n-\n-        for thread in self._threads:\n-            if self._use_multiprocessing:\n-                if thread.is_alive():\n-                    thread.terminate()\n-            else:\n-                # The thread.is_alive() test is subject to a race condition:\n-                # the thread could terminate right after the test and before the\n-                # join, rendering this test meaningless -> Call thread.join()\n-                # always, which is ok no matter what the status of the thread.\n-                thread.join(timeout)\n-\n-        if self._manager:\n-            self._manager.shutdown()\n-\n-        self._threads = []\n-        self._stop_event = None\n-        self.queue = None\n+    def _run(self):\n+        \"\"\"Submits request to the executor and queue the `Future` objects.\"\"\"\n+        self._send_generator()  # Share the initial generator\n+        with closing(self.executor_fn(_SHARED_GENERATOR)) as executor:\n+            while True:\n+                if self.stop_signal.is_set():\n+                    return\n+                self.queue.put(\n+                    executor.apply_async(next_sample, (self.uid,)), block=True)\n \n     def get(self):\n         \"\"\"Creates a generator to extract data from the queue.\n@@ -772,25 +735,53 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n             `(inputs, targets)` or\n             `(inputs, targets, sample_weights)`.\n         \"\"\"\n+        try:\n             while self.is_running():\n-            if not self.queue.empty():\n-                success, value = self.queue.get()\n-                # Rethrow any exceptions found in the queue\n-                if not success:\n-                    six.reraise(value.__class__, value, value.__traceback__)\n-                # Yield regular values\n-                if value is not None:\n-                    yield value\n-            else:\n-                all_finished = all([not thread.is_alive()\n-                                    for thread in self._threads])\n-                if all_finished and self.queue.empty():\n-                    raise StopIteration()\n-                else:\n-                    time.sleep(self.wait_time)\n+                inputs = self.queue.get(block=True).get()\n+                self.queue.task_done()\n+                if inputs is not None:\n+                    yield inputs\n+        except StopIteration:\n+            # Special case for finite generators\n+            last_ones = []\n+            while self.queue.qsize() > 0:\n+                last_ones.append(self.queue.get(block=True))\n+            # Wait for them to complete\n+            list(map(lambda f: f.wait(), last_ones))\n+            # Keep the good ones\n+            last_ones = [future.get() for future in last_ones if future.successful()]\n+            for inputs in last_ones:\n+                if inputs is not None:\n+                    yield inputs\n+        except Exception as e:\n+            self.stop()\n+            if 'generator already executing' in str(e):\n+                raise RuntimeError(\n+                    \"Your generator is NOT thread-safe.\"\n+                    \"Keras requires a thread-safe generator when\"\n+                    \"`use_multiprocessing=False, workers > 1`.\"\n+                    \"For more information see issue #1638.\")\n+            six.reraise(*sys.exc_info())\n \n-        # Make sure to rethrow the first exception in the queue, if any\n-        while not self.queue.empty():\n-            success, value = self.queue.get()\n-            if not success:\n-                six.reraise(value.__class__, value, value.__traceback__)\n+    def _send_generator(self):\n+        \"\"\"Send current generator to all workers.\"\"\"\n+        # For new processes that may spawn\n+        _SHARED_GENERATOR[self.uid] = self.generator\n+\n+    def stop(self, timeout=None):\n+        \"\"\"Stops running threads and wait for them to exit, if necessary.\n+\n+        Should be called by the same thread which called `start()`.\n+\n+        # Arguments\n+            timeout: maximum time to wait on `thread.join()`\n+        \"\"\"\n+        self.stop_signal.set()\n+\n+        with self.queue.mutex:\n+            self.queue.queue.clear()\n+            self.queue.unfinished_tasks = 0\n+            self.queue.not_full.notify()\n+\n+        self.run_thread.join(timeout)\n+        _SHARED_GENERATOR[self.uid] = None\n\n@@ -1,3 +1,5 @@\n+import threading\n+\n import pytest\n import numpy as np\n import pandas as pd\n@@ -39,6 +41,36 @@ class RandomSequence(Sequence):\n         pass\n \n \n+class threadsafe_iter:\n+    \"\"\"Takes an iterator/generator and makes it thread-safe by\n+    serializing call to the `next` method of given iterator/generator.\n+    \"\"\"\n+\n+    def __init__(self, it):\n+        self.it = it\n+        self.lock = threading.Lock()\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        return self.next()\n+\n+    def next(self):\n+        with self.lock:\n+            return next(self.it)\n+\n+\n+def threadsafe_generator(f):\n+    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n+    \"\"\"\n+\n+    def g(*a, **kw):\n+        return threadsafe_iter(f(*a, **kw))\n+\n+    return g\n+\n+\n @keras_test\n def test_check_array_length_consistency():\n     training_utils.check_array_length_consistency(None, None, None)\n@@ -264,6 +296,7 @@ def test_model_methods():\n     # test starting from non-zero initial epoch for generator too\n     trained_epochs = []\n \n+    @threadsafe_generator\n     def gen_data(batch_sz):\n         while True:\n             yield ([np.random.random((batch_sz, 3)),\n@@ -309,6 +342,7 @@ def test_model_methods():\n \n     # empty batch\n     with pytest.raises(ValueError):\n+        @threadsafe_generator\n         def gen_data():\n             while True:\n                 yield (np.asarray([]), np.asarray([]))\n@@ -442,6 +476,7 @@ def test_model_methods():\n     # fit_generator will throw an exception\n     # if steps is unspecified for regular generator\n     with pytest.raises(ValueError):\n+        @threadsafe_generator\n         def gen_data():\n             while True:\n                 yield (np.asarray([]), np.asarray([]))\n@@ -453,12 +488,12 @@ def test_model_methods():\n     # Check if generator is only accessed an expected number of times\n     gen_counters = [0, 0]\n \n+    @threadsafe_generator\n     def gen_data(i):\n         while True:\n             gen_counters[i] += 1\n             yield ([np.random.random((1, 3)), np.random.random((1, 3))],\n                    [np.random.random((1, 4)), np.random.random((1, 3))])\n-\n     out = model.fit_generator(generator=gen_data(0), epochs=3,\n                               steps_per_epoch=2,\n                               validation_data=gen_data(1),\n@@ -468,7 +503,9 @@ def test_model_methods():\n \n     # Need range check here as filling\n     # of the queue depends on sleep in the enqueuers\n-    assert 6 <= gen_counters[0] <= 8\n+    max_train = 3 * 2 + 2 * 2\n+    min_train = 2 * 3\n+    assert min_train <= gen_counters[0] <= max_train\n     # 12 = (epoch * workers * validation steps * max_queue_size)\n     assert 3 <= gen_counters[1] <= 12\n \n@@ -545,6 +582,7 @@ def test_warnings():\n     model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                   sample_weight_mode=None)\n \n+    @threadsafe_generator\n     def gen_data(batch_sz):\n         while True:\n             yield ([np.random.random((batch_sz, 3)),\n@@ -926,6 +964,7 @@ def test_model_with_external_loss():\n         out = model.fit(None, None, epochs=1, steps_per_epoch=1)\n \n         # define a generator to produce x=None and y=None\n+        @threadsafe_generator\n         def data_tensors_generator():\n             while True:\n                 yield (None, None)\n\n@@ -202,17 +202,20 @@ def test_stateful_metrics(metrics_mode):\n     history = model.fit_generator(iter(gen), epochs=1, steps_per_epoch=samples,\n                                   validation_data=iter(val_gen),\n                                   validation_steps=val_samples)\n-    outs = model.evaluate_generator(iter(gen), steps=samples)\n-    preds = model.predict_generator(iter(gen), steps=samples)\n+    outs = model.evaluate_generator(iter(gen), steps=samples, workers=0)\n+    preds = model.predict_generator(iter(gen), steps=samples, workers=0)\n \n     # Test correctness of the metric re ref_true_pos()\n-    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds), atol=1e-5)\n+    np.testing.assert_allclose(outs[2], ref_true_pos(y, preds),\n+                               atol=1e-5)\n \n     # Test correctness of the validation metric computation\n-    val_preds = model.predict_generator(iter(val_gen), steps=val_samples)\n-    val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples)\n-    assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds), atol=1e-5)\n-    assert_allclose(val_outs[2], history.history['val_true_positives'][-1],\n+    val_preds = model.predict_generator(iter(val_gen), steps=val_samples, workers=0)\n+    val_outs = model.evaluate_generator(iter(val_gen), steps=val_samples, workers=0)\n+    np.testing.assert_allclose(val_outs[2], ref_true_pos(val_y, val_preds),\n+                               atol=1e-5)\n+    np.testing.assert_allclose(val_outs[2],\n+                               history.history['val_true_positives'][-1],\n                                atol=1e-5)\n \n \n\n@@ -198,6 +198,17 @@ def test_generator_enqueuer_processes():\n     enqueuer.stop()\n \n \n+def test_generator_enqueuer_threadsafe():\n+    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n+        DummySequence([3, 200, 200, 3])), use_multiprocessing=False)\n+    enqueuer.start(3, 10)\n+    gen_output = enqueuer.get()\n+    with pytest.raises(RuntimeError) as e:\n+        [next(gen_output) for _ in range(10)]\n+    assert 'thread-safe' in str(e.value)\n+    enqueuer.stop()\n+\n+\n def test_generator_enqueuer_fail_threads():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_threads(\n         FaultSequence()), use_multiprocessing=False)\n@@ -261,7 +272,7 @@ def test_ordered_enqueuer_fail_threads():\n     enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=False)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n-    with pytest.raises(StopIteration):\n+    with pytest.raises(IndexError):\n         next(gen_output)\n \n \n@@ -333,7 +344,7 @@ def test_ordered_enqueuer_fail_processes():\n     enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n     gen_output = enqueuer.get()\n-    with pytest.raises(StopIteration):\n+    with pytest.raises(IndexError):\n         next(gen_output)\n \n \n\n@@ -26,6 +26,36 @@ class DummySequence(Sequence):\n         return 10\n \n \n+class threadsafe_iter:\n+    \"\"\"Takes an iterator/generator and makes it thread-safe by\n+    serializing call to the `next` method of given iterator/generator.\n+    \"\"\"\n+\n+    def __init__(self, it):\n+        self.it = it\n+        self.lock = threading.Lock()\n+\n+    def __iter__(self):\n+        return self\n+\n+    def __next__(self):\n+        return self.next()\n+\n+    def next(self):\n+        with self.lock:\n+            return next(self.it)\n+\n+\n+def threadsafe_generator(f):\n+    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n+    \"\"\"\n+\n+    def g(*a, **kw):\n+        return threadsafe_iter(f(*a, **kw))\n+\n+    return g\n+\n+\n @pytest.fixture\n def in_tmpdir(tmpdir):\n     \"\"\"Runs a function in a temporary directory.\n@@ -43,6 +73,7 @@ def test_multiprocessing_training():\n     arr_labels = np.random.randint(0, 2, 50)\n     arr_weights = np.random.random(50)\n \n+    @threadsafe_generator\n     def custom_generator(use_weights=False):\n         batch_size = 10\n         n_samples = 50\n@@ -206,6 +237,7 @@ def test_multiprocessing_training():\n                         use_multiprocessing=False)\n \n     # Test invalid use cases\n+    @threadsafe_generator\n     def invalid_generator():\n         while True:\n             yield arr_data[:10], arr_data[:10], arr_labels[:10], arr_labels[:10]\n@@ -250,6 +282,7 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n     arr_labels = np.random.randint(0, 2, 50)\n     np.savez('data.npz', **{'data': arr_data, 'labels': arr_labels})\n \n+    @threadsafe_generator\n     def custom_generator():\n \n         batch_size = 10\n@@ -368,6 +401,7 @@ def test_multiprocessing_training_from_file(in_tmpdir):\n def test_multiprocessing_predicting():\n     arr_data = np.random.randint(0, 256, (50, 2))\n \n+    @threadsafe_generator\n     def custom_generator():\n         batch_size = 10\n         n_samples = 50\n@@ -457,6 +491,7 @@ def test_multiprocessing_evaluating():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     arr_labels = np.random.randint(0, 2, 50)\n \n+    @threadsafe_generator\n     def custom_generator():\n         batch_size = 10\n         n_samples = 50\n@@ -551,6 +586,7 @@ def test_multiprocessing_fit_error():\n     n_samples = 50\n     good_batches = 3\n \n+    @threadsafe_generator\n     def custom_generator(use_weights=False):\n         \"\"\"Raises an exception after a few good batches\"\"\"\n         for i in range(good_batches):\n@@ -575,7 +611,7 @@ def test_multiprocessing_fit_error():\n     #     exception and does not attempt to run the generator.\n     #   - On other platforms, make sure `RuntimeError` exception bubbles up\n     if os.name is 'nt':\n-        with pytest.raises(ValueError):\n+        with pytest.raises(RuntimeError):\n             model.fit_generator(custom_generator(),\n                                 steps_per_epoch=samples,\n                                 validation_steps=None,\n@@ -609,7 +645,7 @@ def test_multiprocessing_fit_error():\n     #     exception and does not attempt to run the generator.\n     #   - On other platforms, make sure `RuntimeError` exception bubbles up\n     if os.name is 'nt':\n-        with pytest.raises(ValueError):\n+        with pytest.raises(RuntimeError):\n             model.fit_generator(custom_generator(),\n                                 steps_per_epoch=samples,\n                                 validation_steps=None,\n@@ -663,6 +699,7 @@ def test_multiprocessing_evaluate_error():\n     n_samples = 50\n     good_batches = 3\n \n+    @threadsafe_generator\n     def custom_generator():\n         \"\"\"Raises an exception after a few good batches\"\"\"\n         for i in range(good_batches):\n@@ -716,7 +753,7 @@ def test_multiprocessing_evaluate_error():\n     #     exception and does not attempt to run the generator.\n     #   - On other platforms, make sure `RuntimeError` exception bubbles up\n     if os.name is 'nt':\n-        with pytest.raises(ValueError):\n+        with pytest.raises(RuntimeError):\n             model.evaluate_generator(custom_generator(),\n                                      steps=good_batches + 1,\n                                      max_queue_size=10,\n@@ -762,6 +799,7 @@ def test_multiprocessing_predict_error():\n     arr_data = np.random.randint(0, 256, (50, 2))\n     good_batches = 3\n \n+    @threadsafe_generator\n     def custom_generator():\n         \"\"\"Raises an exception after a few good batches\"\"\"\n         batch_size = 10\n@@ -786,7 +824,7 @@ def test_multiprocessing_predict_error():\n     #     exception and does not attempt to run the generator.\n     #   - On other platforms, make sure `RuntimeError` exception bubbles up\n     if os.name is 'nt':\n-        with pytest.raises(ValueError):\n+        with pytest.raises(StopIteration):\n             model.predict_generator(custom_generator(),\n                                     steps=good_batches * WORKERS + 1,\n                                     max_queue_size=10,\n@@ -817,7 +855,7 @@ def test_multiprocessing_predict_error():\n     #     exception and does not attempt to run the generator.\n     #   - On other platforms, make sure `RuntimeError` exception bubbles up\n     if os.name is 'nt':\n-        with pytest.raises(ValueError):\n+        with pytest.raises(RuntimeError):\n             model.predict_generator(custom_generator(),\n                                     steps=good_batches + 1,\n                                     max_queue_size=10,\n@@ -857,5 +895,6 @@ def test_multiprocessing_predict_error():\n                                 workers=0,\n                                 use_multiprocessing=False)\n \n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
