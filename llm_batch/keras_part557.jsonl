{"custom_id": "keras#de4ef7bddc2c47ba6fc8f0dccabd8e7ad533f58e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 60 | Churn Cumulative: 723 | Contributors (this commit): 5 | Commits (past 90d): 48 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -685,7 +685,7 @@ class Layer(Operation):\n     def _maybe_reset_call_context(self):\n         global CALL_CTX\n         call_ctx = getattr(CALL_CTX, \"current\", None)\n-        if call_ctx is None and call_ctx.entry_layer == self:\n+        if call_ctx is None or call_ctx.entry_layer == self:\n             CALL_CTX.current = None\n \n     def _get_default_training_value(self):\n\n@@ -2,6 +2,8 @@ import numpy as np\n \n from keras_core import backend\n from keras_core import layers\n+from keras_core import models\n+from keras_core import operations as ops\n from keras_core import testing\n \n \n@@ -133,7 +135,59 @@ class LayerTest(testing.TestCase):\n         self.assertLen(layer.weights, 4)\n \n     def test_activity_regularization(self):\n-        pass\n+        class ActivityRegularizer(layers.Layer):\n+            def call(self, x):\n+                return x\n+\n+        layer = ActivityRegularizer(activity_regularizer=\"l1\")\n+        layer(\n+            np.ones(\n+                1,\n+            )\n+        )\n+        self.assertLen(layer.losses, 1)\n+        self.assertAllClose(layer.losses[0], 0.01)\n+\n+        # losses are reset upon call\n+        layer(\n+            np.ones(\n+                1,\n+            )\n+        )\n+        self.assertLen(layer.losses, 1)\n+        self.assertAllClose(layer.losses[0], 0.01)\n \n     def test_add_loss(self):\n-        pass\n+        class LossLayer(layers.Layer):\n+            def call(self, x):\n+                self.add_loss(ops.sum(x))\n+                return x\n+\n+        layer = LossLayer()\n+        layer(\n+            np.ones(\n+                1,\n+            )\n+        )\n+        self.assertLen(layer.losses, 1)\n+        self.assertAllClose(layer.losses[0], 1.0)\n+\n+        # losses are reset upon call\n+        layer = LossLayer()\n+        layer(\n+            np.ones(\n+                1,\n+            )\n+        )\n+        self.assertLen(layer.losses, 1)\n+        self.assertAllClose(layer.losses[0], 1.0)\n+\n+        # It works inside a model\n+        model = models.Sequential([layer])\n+        model(\n+            np.ones(\n+                1,\n+            )\n+        )\n+        self.assertLen(model.losses, 1)\n+        self.assertAllClose(model.losses[0], 1.0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d03973f1da4760e1b7809f4bde0ec0e6cf39dfa2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 243 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -140,20 +140,12 @@ class LayerTest(testing.TestCase):\n                 return x\n \n         layer = ActivityRegularizer(activity_regularizer=\"l1\")\n-        layer(\n-            np.ones(\n-                1,\n-            )\n-        )\n+        layer(np.ones((1,)))\n         self.assertLen(layer.losses, 1)\n         self.assertAllClose(layer.losses[0], 0.01)\n \n         # losses are reset upon call\n-        layer(\n-            np.ones(\n-                1,\n-            )\n-        )\n+        layer(np.ones((1,)))\n         self.assertLen(layer.losses, 1)\n         self.assertAllClose(layer.losses[0], 0.01)\n \n@@ -164,30 +156,18 @@ class LayerTest(testing.TestCase):\n                 return x\n \n         layer = LossLayer()\n-        layer(\n-            np.ones(\n-                1,\n-            )\n-        )\n+        layer(np.ones((1,)))\n         self.assertLen(layer.losses, 1)\n         self.assertAllClose(layer.losses[0], 1.0)\n \n         # losses are reset upon call\n         layer = LossLayer()\n-        layer(\n-            np.ones(\n-                1,\n-            )\n-        )\n+        layer(np.ones((1,)))\n         self.assertLen(layer.losses, 1)\n         self.assertAllClose(layer.losses[0], 1.0)\n \n         # It works inside a model\n         model = models.Sequential([layer])\n-        model(\n-            np.ones(\n-                1,\n-            )\n-        )\n+        model(np.ones((1,)))\n         self.assertLen(model.losses, 1)\n         self.assertAllClose(model.losses[0], 1.0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ed93c861e593759c1abb50e646912c4146d2ed5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 1 | Files Changed: 5 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 78 | Churn Cumulative: 843 | Contributors (this commit): 5 | Commits (past 90d): 61 | Contributors (cumulative): 11 | DMM Complexity: 0.9473684210526315\n\nDIFF:\n@@ -3,4 +3,7 @@ from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.input_layer import Input\n from keras_core.layers.core.input_layer import InputLayer\n from keras_core.layers.layer import Layer\n+from keras_core.layers.regularization.activity_regularization import (\n+    ActivityRegularization,\n+)\n from keras_core.layers.regularization.dropout import Dropout\n\n@@ -343,7 +343,9 @@ class Layer(Operation):\n             outputs = super().__call__(*args, **kwargs)\n             # Record activity regularizer loss.\n             if self.activity_regularizer is not None:\n-                self.add_loss(self.activity_regularizer(outputs))\n+                for output in nest.flatten(outputs):\n+                    if backend.is_tensor(output):\n+                        self.add_loss(self.activity_regularizer(output))\n \n         # TODO: Set masks on outputs\n         # self._set_mask_metadata(inputs, outputs, previous_mask)\n\n@@ -149,6 +149,11 @@ class LayerTest(testing.TestCase):\n         self.assertLen(layer.losses, 1)\n         self.assertAllClose(layer.losses[0], 0.01)\n \n+        # KerasTensors are no op\n+        layer = ActivityRegularizer(activity_regularizer=\"l1\")\n+        layer(layers.Input((2, 2)))\n+        self.assertLen(layer.losses, 0)\n+\n     def test_add_loss(self):\n         class LossLayer(layers.Layer):\n             def call(self, x):\n\n@@ -0,0 +1,40 @@\n+from keras_core import regularizers\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.ActivityRegularization\")\n+class ActivityRegularization(Layer):\n+    \"\"\"Layer that applies an update to the cost function based input activity.\n+\n+    Args:\n+        l1: L1 regularization factor (positive float).\n+        l2: L2 regularization factor (positive float).\n+\n+    Input shape:\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    Output shape:\n+        Same shape as input.\n+    \"\"\"\n+\n+    def __init__(self, l1=0.0, l2=0.0, **kwargs):\n+        super().__init__(\n+            activity_regularizer=regularizers.L1L2(l1=l1, l2=l2), **kwargs\n+        )\n+        self.supports_masking = True\n+        self.l1 = l1\n+        self.l2 = l2\n+\n+    def call(self, inputs):\n+        return inputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\"l1\": self.l1, \"l2\": self.l2}\n+        return {**base_config, **config}\n\n@@ -0,0 +1,26 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core.testing import test_case\n+\n+\n+class ActivityRegularizationTest(test_case.TestCase):\n+    def test_correctness(self):\n+        layer = layers.ActivityRegularization(l1=0.2, l2=0.3)\n+        layer(2 * np.ones((1,)))\n+        self.assertLen(layer.losses, 1)\n+        self.assertAllClose(layer.losses[0], 4 * 0.3 + 2 * 0.2)\n+\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.ActivityRegularization,\n+            {\"l1\": 0.1, \"l2\": 0.2},\n+            input_shape=(2, 3),\n+            input_dtype=\"float32\",\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=1,\n+            supports_masking=True,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8ce551b0f58f7774ecb70a6bce35351a2dcf7d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 250 | Contributors (this commit): 3 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -151,7 +151,7 @@ class LayerTest(testing.TestCase):\n \n         # KerasTensors are no op\n         layer = ActivityRegularizer(activity_regularizer=\"l1\")\n-        layer(layers.Input((2, 2)))\n+        layer(layers.Input(batch_shape=(2, 2)))\n         self.assertLen(layer.losses, 0)\n \n     def test_add_loss(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d72829af440ed349243ae16e9debe6d96cdda123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 4 | Churn Cumulative: 416 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -84,7 +84,7 @@ def write_dirpath(dirpath, strategy):\n       The writing dir path that should be used to save with distribution.\n     \"\"\"\n     if strategy is None:\n-        # Infer strategy from `distribution_strategy_context` if not given.\n+        # Infer strategy from `tf.distribute` if not given.\n         strategy = tf.distribute.get_strategy()\n     if strategy is None:\n         # If strategy is still not available, this is not in distributed\n@@ -107,7 +107,7 @@ def remove_temp_dirpath(dirpath, strategy):\n       strategy: The tf.distribute strategy object currently used.\n     \"\"\"\n     if strategy is None:\n-        # Infer strategy from `distribution_strategy_context` if not given.\n+        # Infer strategy from `tf.distribute` if not given.\n         strategy = tf.distribute.get_strategy()\n     if strategy is None:\n         # If strategy is still not available, this is not in distributed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3672ba47ca016f15a2599c5039f8cb01fed315cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 132 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 132 | Churn Cumulative: 138 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras_core.callbacks.callback import Callback\n from keras_core.callbacks.callback_list import CallbackList\n from keras_core.callbacks.history import History\n+from keras_core.callbacks.lambda_callback import LambdaCallback\n from keras_core.callbacks.progbar_logger import ProgbarLogger\n\n@@ -0,0 +1,87 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+\n+\n+@keras_core_export(\"keras_core.callbacks.LambdaCallback\")\n+class LambdaCallback(Callback):\n+    \"\"\"Callback for creating simple, custom callbacks on-the-fly.\n+\n+    This callback is constructed with anonymous functions that will be called\n+    at the appropriate time (during `Model.{fit | evaluate | predict}`).\n+    Note that the callbacks expects positional arguments, as:\n+\n+    - `on_epoch_begin` and `on_epoch_end` expect two positional arguments:\n+      `epoch`, `logs`\n+    - `on_train_begin` and `on_train_end` expect one positional argument:\n+      `logs`\n+    - `on_train_batch_begin` and `on_train_batch_end` expect two positional\n+      arguments: `batch`, `logs`\n+    - See `Callback` class definition for the full list of functions and their\n+      expected arguments.\n+\n+    Args:\n+        on_epoch_begin: called at the beginning of every epoch.\n+        on_epoch_end: called at the end of every epoch.\n+        on_train_begin: called at the beginning of model training.\n+        on_train_end: called at the end of model training.\n+        on_train_batch_begin: called at the beginning of every train batch.\n+        on_train_batch_end: called at the end of every train batch.\n+        kwargs: Any function in `Callback` that you want to override by\n+            passing `function_name=function`. For example,\n+            `LambdaCallback(.., on_train_end=train_end_fn)`. The custom function\n+            needs to have same arguments as the ones defined in `Callback`.\n+\n+    Example:\n+\n+    ```python\n+    # Print the batch number at the beginning of every batch.\n+    batch_print_callback = LambdaCallback(\n+        on_train_batch_begin=lambda batch,logs: print(batch))\n+\n+    # Stream the epoch loss to a file in JSON format. The file content\n+    # is not well-formed JSON but rather has a JSON object per line.\n+    import json\n+    json_log = open('loss_log.json', mode='wt', buffering=1)\n+    json_logging_callback = LambdaCallback(\n+        on_epoch_end=lambda epoch, logs: json_log.write(\n+            json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n+        on_train_end=lambda logs: json_log.close()\n+    )\n+\n+    # Terminate some processes after having finished model training.\n+    processes = ...\n+    cleanup_callback = LambdaCallback(\n+        on_train_end=lambda logs: [\n+            p.terminate() for p in processes if p.is_alive()])\n+\n+    model.fit(...,\n+              callbacks=[batch_print_callback,\n+                         json_logging_callback,\n+                         cleanup_callback])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        on_epoch_begin=None,\n+        on_epoch_end=None,\n+        on_train_begin=None,\n+        on_train_end=None,\n+        on_train_batch_begin=None,\n+        on_train_batch_end=None,\n+        **kwargs,\n+    ):\n+        super().__init__()\n+        self.__dict__.update(kwargs)\n+        if on_epoch_begin is not None:\n+            self.on_epoch_begin = on_epoch_begin\n+        if on_epoch_end is not None:\n+            self.on_epoch_end = on_epoch_end\n+        if on_train_begin is not None:\n+            self.on_train_begin = on_train_begin\n+        if on_train_end is not None:\n+            self.on_train_end = on_train_end\n+        if on_train_batch_begin is not None:\n+            self.on_train_batch_begin = on_train_batch_begin\n+        if on_train_batch_end is not None:\n+            self.on_train_batch_end = on_train_batch_end\n\n@@ -0,0 +1,44 @@\n+from absl import logging\n+import numpy as np\n+\n+from keras_core import testing\n+from keras_core import optimizers\n+from keras_core import layers\n+from keras_core import losses\n+from keras_core import callbacks\n+from keras_core.models.sequential import Sequential\n+\n+\n+class LambdaCallbackTest(testing.TestCase):\n+    def test_LambdaCallback(self):\n+        BATCH_SIZE = 4\n+        model = Sequential(\n+            [layers.Input(shape=(2,), batch_size=BATCH_SIZE), layers.Dense(1)]\n+        )\n+        model.compile(\n+            optimizer=optimizers.SGD(), loss=losses.MeanSquaredError()\n+        )\n+        x = np.random.randn(16, 2)\n+        y = np.random.randn(16, 1)\n+        lambda_log_callback = callbacks.LambdaCallback(\n+            on_train_begin=lambda logs: logging.warning(\"on_train_begin\"),\n+            on_epoch_begin=lambda epoch, logs: logging.warning(\n+                \"on_epoch_begin\"\n+            ),\n+            on_epoch_end=lambda epoch, logs: logging.warning(\"on_epoch_end\"),\n+            on_train_end=lambda logs: logging.warning(\"on_train_end\"),\n+        )\n+        with self.assertLogs(level=\"WARNING\") as logs:\n+            model.fit(\n+                x,\n+                y,\n+                batch_size=BATCH_SIZE,\n+                validation_split=0.2,\n+                callbacks=[lambda_log_callback],\n+                epochs=5,\n+                verbose=0,\n+            )\n+            self.assertTrue(any(\"on_train_begin\" in log for log in logs.output))\n+            self.assertTrue(any(\"on_epoch_begin\" in log for log in logs.output))\n+            self.assertTrue(any(\"on_epoch_end\" in log for log in logs.output))\n+            self.assertTrue(any(\"on_train_end\" in log for log in logs.output))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42921c766395f926cd75d5c7ca90e97e50b2e3a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 294 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 40/35 | Churn Δ: 299 | Churn Cumulative: 8258 | Contributors (this commit): 4 | Commits (past 90d): 44 | Contributors (cumulative): 10 | DMM Complexity: 0.5\n\nDIFF:\n@@ -5,6 +5,10 @@ def add(x1, x2):\n     return jnp.add(x1, x2)\n \n \n+def einsum(subscripts, *operands, **kwargs):\n+    return jnp.einsum(subscripts, *operands, **kwargs)\n+\n+\n def subtract(x1, x2):\n     return jnp.subtract(x1, x2)\n \n\n@@ -6,6 +6,10 @@ def add(x1, x2):\n     return tfnp.add(x1, x2)\n \n \n+def einsum(subscripts, *operands, **kwargs):\n+    return tfnp.einsum(subscripts, *operands, **kwargs)\n+\n+\n def subtract(x1, x2):\n     return tfnp.subtract(x1, x2)\n \n\n@@ -131,6 +131,8 @@ zeros_like\n \n \n \"\"\"\n+import re\n+\n import numpy as np\n \n from keras_core import backend\n@@ -148,15 +150,15 @@ def broadcast_shapes(shape1, shape2):\n     origin_shape2 = shape2\n \n     if len(shape1) > len(shape2):\n-        shape2 = [None] * (len(shape1) - len(shape2)) + shape2\n+        shape2 = [1] * (len(shape1) - len(shape2)) + shape2\n     if len(shape1) < len(shape2):\n-        shape1 = [None] * (len(shape2) - len(shape1)) + shape1\n+        shape1 = [1] * (len(shape2) - len(shape1)) + shape1\n     output_shape = list(shape1)\n     for i in range(len(shape1)):\n         if shape1[i] == 1:\n             output_shape[i] = shape2[i]\n         elif shape1[i] is None:\n-            output_shape[i] = shape2[i]\n+            output_shape[i] = None if shape2[i] == 1 else shape2[i]\n         else:\n             if shape2[i] == 1 or shape2[i] is None or shape2[i] == shape1[i]:\n                 output_shape[i] = shape1[i]\n@@ -1012,6 +1014,185 @@ class Dot(Operation):\n         return KerasTensor(x1_shape + x2_shape, dtype=x1.dtype)\n \n \n+class Einsum(Operation):\n+    def __init__(self, subscripts):\n+        super().__init__()\n+        self.subscripts = subscripts\n+\n+    def call(self, *operands):\n+        return backend.numpy.einsum(self.subscripts, *operands)\n+\n+    def compute_output_spec(self, *operands):\n+        \"\"\"Compute the output shape of `einsum`.\n+\n+        The shape computation follows the steps below:\n+        1. Find all letters in the input specs (left part of \"->\"), and\n+            break them into two categories: letters appearing more than once\n+            go to `reduced_dims`, otherwise go to `kept_dims`.\n+        2. Adjust `reduced_dims` and `kept_dims` based on the output spec\n+            (right part of \"->\"). The rule is if the letter appears in the\n+            output spec, then move it to `kept_dims`, otherwise move it to\n+            `reduced_dims`.\n+        3. Compute the target output shape. If no output spec is set, then\n+            the target output shape will be \"...{kept_dims}\", e.g., \"...ijk\",\n+            else it will be the same as output spec. \"...\" is a wildcard that\n+            could map shape of arbitrary length.\n+        4. For each operand in `operands`, map the shape specified in the input\n+            spec to the output target, e.g, if operand is of shape [2,3,4],\n+            input spec is \"i...\" and output target is \"i...jk\", then 2 will go\n+            the index 0. For dims not represented by any letter, insert to the\n+            wildcard part. For each letter in output target not appearing in\n+            input spec, the dim will be 1 for broadcasting. After 4, each\n+            operand should have a target shape containing only number and None.\n+        5. Broadcast all shapes computed from 4, and the result is the output\n+            shape.\n+\n+        Let's take an example to illustrate the steps above. Let's define:\n+        ```python\n+        x = KerasTensor([None, 3, 4])\n+        y = KerasTensor(2, 4, 3)\n+        z = knp.einsum(\"...ij, kji->...k\", x, y)\n+        ```\n+\n+        1. `reduced_dims` is {\"i\", \"j\"}, `kept_dims` is {\"k\"}.\n+        2. `reduced_dims` is still {\"i\", \"j\"}, and `kept_dims` is {\"k\"}.\n+        3. Output target is \"...k\".\n+        4. For `x`, the input spec is \"...ij\", and the output target is \"...k\".\n+            \"i\" and \"j\" do not appear in the output target, so no replacement\n+            happens, and [None] goes to wildcard. Afterwards, \"k\" is replaced\n+            by 1, so we get shape [None, 1]. Applying the same logic to `y`, we\n+            get shape [2].\n+        5. Broadcast [None, 1] and [2], and we get [None, 2], which is the\n+            output shape.\n+        \"\"\"\n+        split_subscripts = self.subscripts.split(\"->\")\n+        if len(split_subscripts) > 2:\n+            raise ValueError(\n+                \"At most one '->' is supported in `einsum` subscripts, but \"\n+                f\"received {self.subscripts}.\"\n+            )\n+        if len(split_subscripts) == 2:\n+            subscripts = split_subscripts[0]\n+            output_spec = split_subscripts[1]\n+        else:\n+            subscripts = self.subscripts\n+            output_spec = None\n+        input_specs = subscripts.split(\",\")\n+        if len(input_specs) != len(operands):\n+            raise ValueError(\n+                f\"Number of operands ({len(operands)}) does not match the \"\n+                f\"number of input specs ({len(input_specs)}) in `einsum`, \"\n+                f\"received subscripts={self.subscripts}.\"\n+            )\n+        reduced_dims = set()\n+        kept_dims = set()\n+        for s in subscripts:\n+            if not s.isalpha():\n+                continue\n+            if s not in reduced_dims and s not in kept_dims:\n+                kept_dims.add(s)\n+            elif s in kept_dims:\n+                kept_dims.remove(s)\n+                reduced_dims.add(s)\n+\n+        if output_spec is not None:\n+            # The output spec changes the rule of kept_dims and reduced_dims.\n+            # In short, dims appearing in the output spec will be kept, and\n+            # dims not appearing in the output spec will be reduced.\n+            kept_dims_copy = kept_dims.copy()\n+            reduced_dims_copy = reduced_dims.copy()\n+            for dim in kept_dims:\n+                if dim not in output_spec:\n+                    kept_dims_copy.remove(dim)\n+                    reduced_dims_copy.add(dim)\n+            for dim in reduced_dims:\n+                if dim in output_spec:\n+                    reduced_dims_copy.remove(dim)\n+                    kept_dims_copy.add(dim)\n+            kept_dims = kept_dims_copy\n+            reduced_dims = reduced_dims_copy\n+\n+        reduced_dims = sorted(reduced_dims)\n+        kept_dims = sorted(kept_dims)\n+\n+        if output_spec is None:\n+            target_broadcast_spec = \"...\" + \"\".join(kept_dims)\n+        else:\n+            target_broadcast_spec = output_spec\n+\n+        expanded_operands_shapes = []\n+        for x, spec in zip(operands, input_specs):\n+            x_shape = getattr(x, \"shape\", [])\n+            x_shape = [-1 if size is None else size for size in x_shape]\n+            split_spec = spec.split(\"...\")\n+            expanded_shape = target_broadcast_spec\n+            if len(split_spec) == 1:\n+                # In this case, the input spec is just a string of letters,\n+                # e.g., \"ijk\".\n+                if len(x_shape) != len(split_spec[0]):\n+                    raise ValueError(\n+                        \"Number of dimensions in the subscript does not \"\n+                        \"match the number of dimensions in the operand, \"\n+                        f\"received subscript `{spec}` and operand of shape \"\n+                        f\"{x_shape}.\"\n+                    )\n+                for size, s in zip(x_shape, split_spec[0]):\n+                    # Replace the letter with the right shape.\n+                    expanded_shape = expanded_shape.replace(s, str(size) + \" \")\n+                expanded_shape = expanded_shape.replace(\"...\", \"\")\n+            else:\n+                # In this case, the input spec has \"...\", e.g., \"i...j\", \"i...\",\n+                # or \"...j\".\n+                for i in range(len(split_spec[0])):\n+                    expanded_shape = expanded_shape.replace(\n+                        split_spec[0][i], str(x_shape[i]) + \" \"\n+                    )\n+                for i in range(len(split_spec[1])):\n+                    expanded_shape = expanded_shape.replace(\n+                        split_spec[1][-i - 1], str(x_shape[-i - 1]) + \" \"\n+                    )\n+                # Shape matched by \"...\" will be inserted to the position of\n+                # \"...\".\n+                wildcard_shape_start_index = len(split_spec[0])\n+                wildcard_shape_end_index = (\n+                    len(x_shape)\n+                    if len(split_spec[1]) == 0\n+                    else -len(split_spec[1])\n+                )\n+                wildcard_shape = x_shape[\n+                    wildcard_shape_start_index:wildcard_shape_end_index\n+                ]\n+                wildcard_shape_str = (\n+                    \" \".join([str(size) for size in wildcard_shape]) + \" \"\n+                )\n+                expanded_shape = expanded_shape.replace(\n+                    \"...\", wildcard_shape_str\n+                )\n+            # Replace all letters not yet handled with \"1\" for broadcasting.\n+            expanded_shape = re.sub(\"[a-z]\", \"1 \", expanded_shape)\n+            expanded_shape = expanded_shape.split()\n+            expanded_shape = [\n+                None if size == \"-1\" else int(size) for size in expanded_shape\n+            ]\n+            expanded_operands_shapes.append(expanded_shape)\n+\n+        output_shape = expanded_operands_shapes[0]\n+        for shape in expanded_operands_shapes[1:]:\n+            output_shape = broadcast_shapes(output_shape, shape)\n+        dtype = None\n+        for x in operands:\n+            if hasattr(x, \"dtype\"):\n+                dtype = x.dtype\n+                break\n+        return KerasTensor(output_shape, dtype=dtype)\n+\n+\n+def einsum(subscripts, *operands):\n+    if any_symbolic_tensors(operands):\n+        return Einsum(subscripts).symbolic_call(*operands)\n+    return backend.numpy.einsum(subscripts, *operands)\n+\n+\n class Empty(Operation):\n     def call(self, shape, dtype=\"float32\"):\n         return backend.execute(\"empty\", shape, dtype=dtype)\n\n@@ -68,6 +68,38 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertEqual(knp.cross(x1, y).shape, (2, 3, 3))\n         self.assertEqual(knp.cross(x2, y).shape, (None, 3))\n \n+    def test_einsum(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([3, 4])\n+        self.assertEqual(knp.einsum(\"ij,jk->ik\", x, y).shape, (None, 4))\n+        self.assertEqual(knp.einsum(\"ij,jk->ikj\", x, y).shape, (None, 4, 3))\n+        self.assertEqual(knp.einsum(\"ii\", x).shape, ())\n+        self.assertEqual(knp.einsum(\",ij\", 5, x).shape, (None, 3))\n+\n+        x = KerasTensor([None, 3, 4])\n+        y = KerasTensor([None, 4, 5])\n+        z = KerasTensor([1, 1, 1, 9])\n+        self.assertEqual(knp.einsum(\"ijk,jkl->li\", x, y).shape, (5, None))\n+        self.assertEqual(knp.einsum(\"ijk,jkl->lij\", x, y).shape, (5, None, 3))\n+        self.assertEqual(\n+            knp.einsum(\"...,...j->...j\", x, y).shape, (None, 3, 4, 5)\n+        )\n+        self.assertEqual(\n+            knp.einsum(\"i...,...j->i...j\", x, y).shape, (None, 3, 4, 5)\n+        )\n+        self.assertEqual(knp.einsum(\"i...,...j\", x, y).shape, (3, 4, None, 5))\n+        self.assertEqual(\n+            knp.einsum(\"i...,...j,...k\", x, y, z).shape, (1, 3, 4, None, 5, 9)\n+        )\n+        self.assertEqual(\n+            knp.einsum(\"mij,ijk,...\", x, y, z).shape, (1, 1, 1, 9, 5, None)\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([None, 3])\n+            y = KerasTensor([3, 4])\n+            knp.einsum(\"ijk,jk->ik\", x, y)\n+\n     def test_full_like(self):\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.full_like(x, KerasTensor([1, 3])).shape, (None, 3))\n@@ -180,12 +212,14 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 3])\n         indices = KerasTensor([1, 3])\n         self.assertEqual(knp.take_along_axis(x, indices, axis=0).shape, (1, 3))\n-        self.assertEqual(knp.take_along_axis(x, indices, axis=1).shape, (1, 3))\n+        self.assertEqual(\n+            knp.take_along_axis(x, indices, axis=1).shape, (None, 3)\n+        )\n \n         x = KerasTensor([None, 3, 3])\n         indices = KerasTensor([1, 3, None])\n         self.assertEqual(\n-            knp.take_along_axis(x, indices, axis=1).shape, (1, 3, 3)\n+            knp.take_along_axis(x, indices, axis=1).shape, (None, 3, 3)\n         )\n \n     def test_tensordot(self):\n@@ -341,6 +375,37 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 3])\n             knp.cross(x, y)\n \n+    def test_einsum(self):\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([3, 4])\n+        self.assertEqual(knp.einsum(\"ij,jk->ik\", x, y).shape, (2, 4))\n+        self.assertEqual(knp.einsum(\"ij,jk->ikj\", x, y).shape, (2, 4, 3))\n+        self.assertEqual(knp.einsum(\"ii\", x).shape, ())\n+        self.assertEqual(knp.einsum(\",ij\", 5, x).shape, (2, 3))\n+\n+        x = KerasTensor([2, 3, 4])\n+        y = KerasTensor([3, 4, 5])\n+        z = KerasTensor([1, 1, 1, 9])\n+        self.assertEqual(knp.einsum(\"ijk,jkl->li\", x, y).shape, (5, 2))\n+        self.assertEqual(knp.einsum(\"ijk,jkl->lij\", x, y).shape, (5, 2, 3))\n+        self.assertEqual(knp.einsum(\"...,...j->...j\", x, y).shape, (2, 3, 4, 5))\n+        self.assertEqual(\n+            knp.einsum(\"i...,...j->i...j\", x, y).shape, (2, 3, 4, 5)\n+        )\n+        self.assertEqual(knp.einsum(\"i...,...j\", x, y).shape, (3, 4, 2, 5))\n+        self.assertEqual(knp.einsum(\"i...,...j\", x, y).shape, (3, 4, 2, 5))\n+        self.assertEqual(\n+            knp.einsum(\"i...,...j,...k\", x, y, z).shape, (1, 3, 4, 2, 5, 9)\n+        )\n+        self.assertEqual(\n+            knp.einsum(\"mij,ijk,...\", x, y, z).shape, (1, 1, 1, 9, 5, 2)\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            y = KerasTensor([3, 4])\n+            knp.einsum(\"ijk,jk->ik\", x, y)\n+\n     def test_full_like(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.full_like(x, 2).shape, (2, 3))\n@@ -1551,6 +1616,41 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.Cross()(x1, y3)), np.cross(x1, y3))\n         self.assertAllClose(np.array(knp.Cross()(x2, y3)), np.cross(x2, y3))\n \n+    def test_einsum(self):\n+        x = np.arange(24).reshape([2, 3, 4])\n+        y = np.arange(24).reshape([2, 4, 3])\n+        self.assertAllClose(\n+            np.array(knp.einsum(\"ijk,lkj->il\", x, y)),\n+            np.einsum(\"ijk,lkj->il\", x, y),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.einsum(\"ijk,ikj->i\", x, y)),\n+            np.einsum(\"ijk,ikj->i\", x, y),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.einsum(\"i...,j...k->...ijk\", x, y)),\n+            np.einsum(\"i..., j...k->...ijk\", x, y),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.einsum(\",ijk\", 5, y)), np.einsum(\",ijk\", 5, y)\n+        )\n+\n+        self.assertAllClose(\n+            np.array(knp.Einsum(\"ijk,lkj->il\")(x, y)),\n+            np.einsum(\"ijk,lkj->il\", x, y),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Einsum(\"ijk,ikj->i\")(x, y)),\n+            np.einsum(\"ijk,ikj->i\", x, y),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Einsum(\"i...,j...k->...ijk\")(x, y)),\n+            np.einsum(\"i...,j...k->...ijk\", x, y),\n+        )\n+        self.assertAllClose(\n+            np.array(knp.Einsum(\",ijk\")(5, y)), np.einsum(\",ijk\", 5, y)\n+        )\n+\n     def test_full_like(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(np.array(knp.full_like(x, 2)), np.full_like(x, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3b523619fb8fb0aef0c94debc763a06ce145ed6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 278 | Lines Deleted: 212 | Files Changed: 13 | Hunks: 102 | Methods Changed: 33 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 490 | Churn Cumulative: 8312 | Contributors (this commit): 7 | Commits (past 90d): 198 | Contributors (cumulative): 53 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,10 +22,10 @@ class RandomNormal(Initializer):\n     >>> layer = Dense(3, kernel_initializer=initializer)\n \n     Args:\n-        mean: A python scalar or a scalar keras tensor. Mean of the random values to\n-            generate.\n-        stddev: A python scalar or a scalar keras tensor. Standard deviation of the\n-           random values to generate.\n+        mean: A python scalar or a scalar keras tensor. Mean of the random\n+            values to generate.\n+        stddev: A python scalar or a scalar keras tensor. Standard deviation of\n+           the random values to generate.\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n@@ -75,10 +75,10 @@ class TruncatedNormal(Initializer):\n     >>> layer = Dense(3, kernel_initializer=initializer)\n \n     Args:\n-        mean: A python scalar or a scalar keras tensor. Mean of the random values to\n-            generate.\n-        stddev: A python scalar or a scalar keras tensor. Standard deviation of the\n-           random values to generate.\n+        mean: A python scalar or a scalar keras tensor. Mean of the random\n+            values to generate.\n+        stddev: A python scalar or a scalar keras tensor. Standard deviation of\n+           the random values to generate.\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n@@ -125,10 +125,10 @@ class RandomUniform(Initializer):\n     >>> layer = Dense(3, kernel_initializer=initializer)\n \n     Args:\n-        minval: A python scalar or a scalar keras tensor. Lower bound of the range of\n-            random values to generate (inclusive).\n-        maxval: A python scalar or a scalar keras tensor. Upper bound of the range of\n-            random values to generate (exclusive).\n+        minval: A python scalar or a scalar keras tensor. Lower bound of the\n+            range of random values to generate (inclusive).\n+        maxval: A python scalar or a scalar keras tensor. Upper bound of the\n+            range of random values to generate (exclusive).\n         seed: A Python integer or instance of\n             `keras_core.backend.SeedGenerator`.\n             Used to make the behavior of the initializer\n@@ -177,11 +177,13 @@ class VarianceScaling(Initializer):\n     Examples:\n \n     >>> # Standalone usage:\n-    >>> initializer = VarianceScaling(scale=0.1, mode='fan_in', distribution='uniform')\n+    >>> initializer = VarianceScaling(\n+        scale=0.1, mode='fan_in', distribution='uniform')\n     >>> values = initializer(shape=(2, 2))\n \n     >>> # Usage in a Keras layer:\n-    >>> initializer = VarianceScaling(scale=0.1, mode='fan_in', distribution='uniform')\n+    >>> initializer = VarianceScaling(\n+        scale=0.1, mode='fan_in', distribution='uniform')\n     >>> layer = Dense(3, kernel_initializer=initializer)\n \n     Args:\n@@ -241,9 +243,9 @@ class VarianceScaling(Initializer):\n         Args:\n             shape: Shape of the tensor.\n             dtype: Optional dtype of the tensor. Only floating point types are\n-                supported. If not specified, `tf.keras.backend.floatx()` is used,\n-                which default to `float32` unless you configured it otherwise (via\n-                `tf.keras.backend.set_floatx(float_dtype)`)\n+                supported. If not specified, `tf.keras.backend.floatx()` is\n+                used, which default to `float32` unless you configured it\n+                otherwise (via `tf.keras.backend.set_floatx(float_dtype)`)\n         \"\"\"\n         scale = self.scale\n         fan_in, fan_out = compute_fans(shape)\n\n@@ -36,7 +36,8 @@ from keras_core.operations.operation import Operation\n from keras_core.utils import summary_utils\n from keras_core.utils.tracking import Tracker\n \n-# TODO: cache all call signature processing. See layer_utils.CallFunctionSpec() in Keras.\n+# TODO: cache all call signature processing.\n+# See layer_utils.CallFunctionSpec() in Keras.\n \n \n @keras_core_export([\"keras_core.Layer\", \"keras_core.layers.Layer\"])\n@@ -240,8 +241,8 @@ class Layer(Operation):\n         if len(layer_weights) != len(weights):\n             raise ValueError(\n                 f\"You called `set_weights(weights)` on layer '{self.name}' \"\n-                f\"with a weight list of length {len(weights)}, but the layer was \"\n-                f\"expecting {len(layer_weights)} weights.\"\n+                f\"with a weight list of length {len(weights)}, but the layer \"\n+                f\"was expecting {len(layer_weights)} weights.\"\n             )\n         for variable, value in zip(layer_weights, weights):\n             if variable.shape != value.shape:\n@@ -320,9 +321,9 @@ class Layer(Operation):\n \n         # Infer training value\n         # Training phase for `Layer.call` is set via (in order of priority):\n-        # (1) The `training` argument passed to this `Layer.call`, if it is not None\n+        # (1) The `training` argument passed to this `Layer.call`, if not None\n         # (2) The training argument of an outer `Layer.call`.\n-        # (4) Any non-None default value for `training` specified in the call signature\n+        # (4) Any non-None default value for `training` in the call signature\n         # (5) False (treating the layer as if it's in inference)\n         arguments_dict = get_arguments_dict(self.call, *args, **kwargs)\n         training = arguments_dict.get(\"training\", None)\n@@ -366,23 +367,25 @@ class Layer(Operation):\n \n         if not self.built:\n             raise ValueError(\n-                \"To call stateless_call, {self.__class__.__name__} must be built \"\n-                \"(i.e. its variables must have been already created). \"\n+                \"To call stateless_call, {self.__class__.__name__} must be \"\n+                \"built (i.e. its variables must have been already created). \"\n                 \"You can build it by calling it on some data.\"\n             )\n         if len(trainable_variables) != len(self.trainable_variables):\n             raise ValueError(\n                 \"Argument `trainable_variables` must be a list of tensors \"\n-                f\"corresponding 1:1 to {self.__class__.__name__}().trainable_variables. \"\n-                f\"Received list with length {len(trainable_variables)}, but expected \"\n-                f\"{len(self.trainable_variables)} variables.\"\n+                \"corresponding 1:1 to \"\n+                f\"{self.__class__.__name__}().trainable_variables. \"\n+                f\"Received list with length {len(trainable_variables)}, \"\n+                f\"but expected {len(self.trainable_variables)} variables.\"\n             )\n         if len(non_trainable_variables) != len(self.non_trainable_variables):\n             raise ValueError(\n                 \"Argument `non_trainable_variables` must be a list of tensors \"\n-                f\"corresponding 1:1 to {self.__class__.__name__}().non_trainable_variables. \"\n-                f\"Received list with length {len(non_trainable_variables)}, but expected \"\n-                f\"{len(self.non_trainable_variables)} variables.\"\n+                \"corresponding 1:1 to \"\n+                f\"{self.__class__.__name__}().non_trainable_variables. \"\n+                f\"Received list with length {len(non_trainable_variables)}, \"\n+                f\"but expected {len(self.non_trainable_variables)} variables.\"\n             )\n \n         # Gather variable mapping\n@@ -440,8 +443,8 @@ class Layer(Operation):\n         for x in losses:\n             if not backend.is_tensor(x):\n                 raise ValueError(\n-                    \"`add_loss()` can only be called from inside `build()` or `call()`, \"\n-                    f\"on a tensor input. Received invalid value: {x}\"\n+                    \"`add_loss()` can only be called from inside `build()` or \"\n+                    f\"`call()`, on a tensor input. Received invalid value: {x}\"\n                 )\n         if backend.in_stateless_scope():\n             scope = backend.get_stateless_scope()\n@@ -760,7 +763,8 @@ def get_shapes_dict(arguments_dict):\n     Example:\n \n     ```\n-    >>> get_shapes_dict({\"input_a\": KerasTensor(shape=(2, 3)), \"training\": False})\n+    >>> get_shapes_dict(\n+        {\"input_a\": KerasTensor(shape=(2, 3)), \"training\": False})\n     {\"input_a_shape\": (2, 3)}\n     ```\n     \"\"\"\n@@ -778,8 +782,8 @@ def get_shapes_dict(arguments_dict):\n                     for x in flat\n                 ):\n                     raise ValueError(\n-                        \"You cannot mix tensors and non-tensors in a nested argument. \"\n-                        f\"Invalid argument: {k}={v}\"\n+                        \"You cannot mix tensors and non-tensors in a nested \"\n+                        f\"argument. Invalid argument: {k}={v}\"\n                     )\n             shapes_dict[f\"{k}_shape\"] = nest.map_structure(\n                 lambda x: backend.standardize_shape(x.shape), v\n@@ -788,7 +792,7 @@ def get_shapes_dict(arguments_dict):\n \n \n def check_build_signature(build_fn, shapes_dict):\n-    \"\"\"Asserts that the argument names in build_fn match the entries in shapes_dict.\n+    \"\"\"Asserts that the argument names in build_fn match entries in shapes_dict.\n \n     For instance if call() has the signature `def call(self, a, b)`\n     then we'll see `shapes_dict == {\"a_shape\": (...), \"b_shape\": (...)}\n\n@@ -34,49 +34,55 @@ class Functional(Function, Model):\n             for k, v in inputs.items():\n                 if not isinstance(v, backend.KerasTensor):\n                     raise ValueError(\n-                        \"When providing `inputs` as a dict, all values in the dict \"\n-                        f\"must be KerasTensors. Received: inputs={inputs} including \"\n-                        f\"invalid value {v} of type {type(v)}\"\n+                        \"When providing `inputs` as a dict, all values in the \"\n+                        f\"dict must be KerasTensors. Received: inputs={inputs} \"\n+                        f\"including invalid value {v} of type {type(v)}\"\n                     )\n                 if k != v.name:\n                     # TODO: maybe make this a warning\n                     raise ValueError(\n-                        \"When providing `inputs` as a dict, all keys in the dict \"\n-                        \"must match the names of the corresponding tensors. \"\n-                        f\"Received key '{k}' mapping to value {v} which has name '{v.name}'. \"\n-                        f\"Change the tensor name to '{k}' (via `Input(..., name='{k}')`)\"\n+                        \"When providing `inputs` as a dict, all keys in the \"\n+                        \"dict must match the names of the corresponding \"\n+                        f\"tensors. Received key '{k}' mapping to value {v} \"\n+                        f\"which has name '{v.name}'. Change the tensor name to \"\n+                        f\"'{k}' (via `Input(..., name='{k}')`)\"\n                     )\n         elif isinstance(inputs, (list, tuple)):\n             for x in inputs:\n                 if not isinstance(x, backend.KerasTensor):\n                     raise ValueError(\n-                        \"When providing `inputs` as a list/tuple, all values in the list/tuple \"\n-                        f\"must be KerasTensors. Received: inputs={inputs} including \"\n-                        f\"invalid value {x} of type {type(x)}\"\n+                        \"When providing `inputs` as a list/tuple, all values \"\n+                        f\"in the list/tuple must be KerasTensors. Received: \"\n+                        f\"inputs={inputs} including invalid value {x} of type \"\n+                        f\"{type(x)}\"\n                     )\n         elif not isinstance(inputs, backend.KerasTensor):\n             raise ValueError(\n-                f\"Unrecognized type for `inputs`: {inputs} (of type {type(inputs)})\"\n+                f\"Unrecognized type for `inputs`: {inputs} \"\n+                f\"(of type {type(inputs)})\"\n             )\n         if isinstance(outputs, dict):\n             for k, v in outputs.items():\n                 if not isinstance(v, backend.KerasTensor):\n                     raise ValueError(\n-                        \"When providing `outputs` as a dict, all values in the dict \"\n-                        f\"must be KerasTensors. Received: outputs={outputs} including \"\n-                        f\"invalid value {v} of type {type(v)}\"\n+                        \"When providing `outputs` as a dict, all values in the \"\n+                        f\"dict must be KerasTensors. Received: \"\n+                        f\"outputs={outputs} including invalid value {v} of \"\n+                        f\"type {type(v)}\"\n                     )\n         elif isinstance(outputs, (list, tuple)):\n             for x in outputs:\n                 if not isinstance(x, backend.KerasTensor):\n                     raise ValueError(\n-                        \"When providing `outputs` as a list/tuple, all values in the list/tuple \"\n-                        f\"must be KerasTensors. Received: outputs={outputs} including \"\n-                        f\"invalid value {x} of type {type(x)}\"\n+                        \"When providing `outputs` as a list/tuple, all values \"\n+                        f\"in the list/tuple must be KerasTensors. Received: \"\n+                        f\"outputs={outputs} including invalid value {x} of \"\n+                        f\"type {type(x)}\"\n                     )\n         elif not isinstance(outputs, backend.KerasTensor):\n             raise ValueError(\n-                f\"Unrecognized type for `outputs`: {outputs} (of type {type(outputs)})\"\n+                f\"Unrecognized type for `outputs`: {outputs} \"\n+                f\"(of type {type(outputs)})\"\n             )\n \n         Function.__init__(self, inputs, outputs, name=name, **kwargs)\n@@ -498,7 +504,8 @@ def deserialize_node(node_data, created_layers):\n                 raise IndexError(\n                     \"Layer node index out of bounds.\\n\"\n                     f\"inbound_layer = {inbound_layer}\\n\"\n-                    f\"inbound_layer._inbound_nodes = {inbound_layer._inbound_nodes}\\n\"\n+                    \"inbound_layer._inbound_nodes = \"\n+                    f\"{inbound_layer._inbound_nodes}\\n\"\n                     f\"inbound_node_index = {inbound_node_index}\"\n                 )\n             inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n\n@@ -42,9 +42,9 @@ class Sequential(Model):\n             and isinstance(self._layers[0], InputLayer)\n         ):\n             raise ValueError(\n-                f\"Sequential model '{self.name}' has already been configured to \"\n-                f\"use input shape {self._layers[0].batch_shape}. You cannot add \"\n-                f\"a different Input layer to it.\"\n+                f\"Sequential model '{self.name}' has already been configured \"\n+                f\"to use input shape {self._layers[0].batch_shape}. You cannot \"\n+                f\"add a different Input layer to it.\"\n             )\n \n         self._layers.append(layer)\n@@ -73,23 +73,26 @@ class Sequential(Model):\n \n     def build(self, input_shape=None):\n         if not isinstance(input_shape, (tuple, list)):\n-            # Do not attempt to build if the model does not have a single input tensor.\n+            # Do not attempt to build if the model does not have a single\n+            # input tensor.\n             return\n         if input_shape and not (\n             isinstance(input_shape[0], int) or input_shape[0] is None\n         ):\n-            # Do not attempt to build if the model does not have a single input tensor.\n+            # Do not attempt to build if the model does not have a single\n+            # input tensor.\n             return\n         if not self._layers:\n             raise ValueError(\n-                f\"Sequential model {self.name} cannot be built because it has no layers. \"\n-                \"Call `model.add(layer)`.\"\n+                f\"Sequential model {self.name} cannot be built because it has \"\n+                \"no layers. Call `model.add(layer)`.\"\n             )\n         if isinstance(self._layers[0], InputLayer):\n             if self._layers[0].batch_shape != input_shape:\n                 raise ValueError(\n-                    f\"Sequential model '{self.name}' has already been configured to \"\n-                    f\"use input shape {self._layers[0].batch_shape}. You cannot build it \"\n+                    f\"Sequential model '{self.name}' has already been \"\n+                    \"configured to use input shape \"\n+                    f\"{self._layers[0].batch_shape}. You cannot build it \"\n                     f\"with input_shape {input_shape}\"\n                 )\n         else:\n@@ -103,7 +106,7 @@ class Sequential(Model):\n                 x = layer(x)\n             except NotImplementedError:\n                 # Can happen if shape inference is not implemented.\n-                # TODO: consider reverting inbound nodes on layers processed so far.\n+                # TODO: consider reverting inbound nodes on layers processed.\n                 return\n         outputs = x\n         self._functional = Functional(inputs=inputs, outputs=outputs)\n\n@@ -124,17 +124,19 @@ class Function(Operation):\n         for x, x_ref in zip(nest.flatten(inputs), self._inputs):\n             if len(x.shape) != len(x_ref.shape):\n                 raise ValueError(\n-                    f\"{self.__class__.__name__} was passed incompatible inputs. \"\n-                    f\"For input '{x_ref.name}', expected shape {x_ref.shape}, \"\n-                    f\"but received instead a tensor with shape {x.shape}.\"\n+                    f\"{self.__class__.__name__} was passed \"\n+                    f\"incompatible inputs. For input '{x_ref.name}', \"\n+                    f\"expected shape {x_ref.shape}, but received \"\n+                    f\"instead a tensor with shape {x.shape}.\"\n                 )\n             for dim, ref_dim in zip(x.shape, x_ref.shape):\n                 if ref_dim is not None and dim is not None:\n                     if dim != ref_dim:\n                         raise ValueError(\n-                            f\"{self.__class__.__name__} was passed incompatible inputs. \"\n-                            f\"For input '{x_ref.name}', expected shape {x_ref.shape}, \"\n-                            f\"but received instead a tensor with shape {x.shape}.\"\n+                            f\"{self.__class__.__name__} was passed \"\n+                            f\"incompatible inputs. For input '{x_ref.name}', \"\n+                            f\"expected shape {x_ref.shape}, but received \"\n+                            f\"instead a tensor with shape {x.shape}.\"\n                         )\n \n \n@@ -154,7 +156,8 @@ def map_graph(inputs, outputs):\n         - nodes: list of Node instances.\n         - nodes_by_depth: dict mapping ints (depth) to lists of node instances.\n         - operations: list of Operation instances.\n-        - operations_by_depth: dict mapping ints (depth) to lists of Operation instances.\n+        - operations_by_depth: dict mapping ints (depth) to lists of Operation\n+            instances.\n     \"\"\"\n     # \"depth\" is number of operations between output Node and the Node.\n     # Nodes are ordered from inputs -> outputs.\n@@ -274,14 +277,15 @@ def _build_map(outputs):\n \n     Returns:\n         A tuple like (ordered_nodes, operation_to_first_traversal_index)\n-        ordered_nodes: list of nodes appearing in the keras history, topologically\n-            sorted from original inputs to the `outputs`.\n-            (If outputs have different sets of ancestors, the inputs to one output\n-            may appear after a different output).\n+        ordered_nodes: list of nodes appearing in the keras history,\n+            topologically sorted from original inputs to the `outputs`.\n+            (If outputs have different sets of ancestors, the inputs to one\n+            output may appear after a different output).\n         operation_to_first_traversal_index:\n-            A dict mapping operation to the traversal index in the DFS where it is\n-            seen. Note: if a operation is shared by several nodes, the dict will only\n-            store the index corresponding to the *first* time the operation seen.\n+            A dict mapping operation to the traversal index in the DFS where it\n+            is seen. Note: if a operation is shared by several nodes, the dict\n+            will onlystore the index corresponding to the *first* time the\n+            operation seen.\n     \"\"\"\n     finished_nodes = set()\n     nodes_in_progress = set()\n@@ -323,7 +327,8 @@ def _build_map_helper(\n     # Prevent cycles.\n     if node in nodes_in_progress:\n         raise ValueError(\n-            f'Tensor {tensor} from operation \"{operation.name}\" is part of a cycle.'\n+            f\"Tensor {tensor} from operation '{operation.name}' is part of a \"\n+            \"cycle.\"\n         )\n \n     # Store the traversal order for operation sorting.\n\n@@ -45,8 +45,8 @@ class Optimizer:\n                 or ema_overwrite_frequency < 1\n             ):\n                 raise ValueError(\n-                    \"`ema_overwrite_frequency` must be an integer >= 1 or None. \"\n-                    \"Received: ema_overwrite_frequency=\"\n+                    \"`ema_overwrite_frequency` must be an integer >= 1 or \"\n+                    \"None. Received: ema_overwrite_frequency=\"\n                     f\"{ema_overwrite_frequency}\"\n                 )\n         self.ema_momentum = ema_momentum\n@@ -75,11 +75,11 @@ class Optimizer:\n         else:\n             if not isinstance(learning_rate, float):\n                 raise ValueError(\n-                    \"Argument `learning_rate` should be float, or an instance of \"\n-                    \"LearningRateSchedule, or a callable \"\n+                    \"Argument `learning_rate` should be float, or an instance \"\n+                    \"of LearningRateSchedule, or a callable \"\n                     \"(that takes in the current iteration value \"\n-                    \"and returns the corresponding learning rate value). Received instead: \"\n-                    f\"learning_rate={learning_rate}\"\n+                    \"and returns the corresponding learning rate value). \"\n+                    f\"Received instead: learning_rate={learning_rate}\"\n                 )\n             self._learning_rate = backend.Variable(\n                 learning_rate,\n@@ -134,7 +134,9 @@ class Optimizer:\n         return variable\n \n     def add_variable_from_reference(self, reference_variable, name=None):\n-        \"\"\"Add an all-zeros variable with the shape and dtype of a reference variable.\"\"\"\n+        \"\"\"Add an all-zeros variable with the shape and dtype of a reference\n+        variable.\n+        \"\"\"\n         initializer = initializers.Zeros()\n         name = name or auto_name(self.__class__.__name__)\n         return self.add_variable(\n@@ -150,8 +152,8 @@ class Optimizer:\n                 raise ValueError(\n                     f\"Unknown variable: {v}. This optimizer can only \"\n                     \"be called for the variables it was originally built with. \"\n-                    \"When working with a new set of variables, you should recreate \"\n-                    \"a new optimizer instance.\"\n+                    \"When working with a new set of variables, you should \"\n+                    \"recreate a new optimizer instance.\"\n                 )\n \n     def update_step(self, gradient, variable, learning_rate):\n@@ -178,13 +180,15 @@ class Optimizer:\n             if not self.built:\n                 raise ValueError(\n                     \"When passing `grads` without `variables`, the optimizer \"\n-                    \"must already be built on a list of variables. Call `optimizer.build(trainable_variables)` first. \"\n+                    \"must already be built on a list of variables. \"\n+                    \"Call `optimizer.build(trainable_variables)` first. \"\n                 )\n             if len(grads) != len(self._trainable_variables_indices):\n                 raise ValueError(\n-                    \"When passing `grads` as a list of gradient tensors, the gradients must \"\n-                    f\"match `optimizer.variables` one-to-on. Received a list of {len(grads)} \"\n-                    f\"gradients, but the optimizer is tracking {len(self._trainable_variables)} \"\n+                    \"When passing `grads` as a list of gradient tensors, the \"\n+                    f\"gradients must match `optimizer.variables` one-to-on. \"\n+                    f\"Received a list of {len(grads)} gradients, but the \"\n+                    f\"optimizer is tracking {len(self._trainable_variables)} \"\n                     \"trainable variables.\"\n                 )\n             trainable_variables = self._trainable_variables\n@@ -227,22 +231,22 @@ class Optimizer:\n         if not self.built:\n             raise ValueError(\n                 \"To call stateless_apply_gradients, {self.__class__.__name__} \"\n-                \"must be built (i.e. its variables must have been already created). \"\n+                \"must be built (i.e. its variables must have been created). \"\n                 \"You can build it via `optimizer.build(trainable_variables)`.\"\n             )\n         if len(optimizer_variables) != len(self.variables):\n             raise ValueError(\n                 \"Argument `optimizer_variables` must be a list of tensors \"\n                 f\"corresponding 1:1 to {self.__class__.__name__}().variables. \"\n-                f\"Received list with length {len(optimizer_variables)}, but expected \"\n-                f\"{len(self.variables)} variables.\"\n+                f\"Received list with length {len(optimizer_variables)}, but \"\n+                f\"expected {len(self.variables)} variables.\"\n             )\n         if len(trainable_variables) != len(self._trainable_variables):\n             raise ValueError(\n                 \"Argument `optimizer_variables` must be a list of tensors \"\n                 \"corresponding 1:1 to the trainable variables list that \"\n-                \"the optimizer was built with. \"\n-                f\"Received len(trainable_variables) == {len(trainable_variables)} \"\n+                \"the optimizer was built with. Received \"\n+                f\"len(trainable_variables) == {len(trainable_variables)} \"\n                 \"whereas the optimizer was built with \"\n                 f\"{len(self._trainable_variables)} variables.\"\n             )\n\n@@ -22,7 +22,8 @@ class Regularizer:\n \n     - `kernel_regularizer`: Regularizer to apply a penalty on the layer's kernel\n     - `bias_regularizer`: Regularizer to apply a penalty on the layer's bias\n-    - `activity_regularizer`: Regularizer to apply a penalty on the layer's output\n+    - `activity_regularizer`: Regularizer to apply a penalty on the layer's\n+        output\n \n     All layers (including custom layers) expose `activity_regularizer` as a\n     settable property, whether or not it is in the constructor arguments.\n@@ -278,15 +279,15 @@ class OrthogonalRegularizer(Regularizer):\n     (i.e. the basis of the output space) orthogonal to each other.\n \n     Arguments:\n-        factor: Float. The regularization factor. The regularization penalty will\n-            be proportional to `factor` times the mean of the dot products between\n-            the L2-normalized rows (if `mode=\"rows\"`, or columns if\n+        factor: Float. The regularization factor. The regularization penalty\n+            will be proportional to `factor` times the mean of the dot products\n+            between the L2-normalized rows (if `mode=\"rows\"`, or columns if\n             `mode=\"columns\"`) of the inputs, excluding the product of each\n             row/column with itself.  Defaults to 0.01.\n-        mode: String, one of `{\"rows\", \"columns\"}`. Defaults to `\"rows\"`. In rows\n-            mode, the regularization effect seeks to make the rows of the input\n-            orthogonal to each other. In columns mode, it seeks to make the columns\n-            of the input orthogonal to each other.\n+        mode: String, one of `{\"rows\", \"columns\"}`. Defaults to `\"rows\"`. In\n+            rows mode, the regularization effect seeks to make the rows of the\n+            input orthogonal to each other. In columns mode, it seeks to make\n+            the columns of the input orthogonal to each other.\n \n     Example:\n \n\n@@ -19,9 +19,9 @@ class CustomObjectScope:\n     \"\"\"Exposes custom classes/functions to Keras deserialization internals.\n \n     Under a scope `with custom_object_scope(objects_dict)`, Keras methods such\n-    as `keras_core.models.load_model()` or `keras_core.models.model_from_config()`\n-    will be able to deserialize any custom object referenced by a\n-    saved config (e.g. a custom layer or metric).\n+    as `keras_core.models.load_model()` or\n+    `keras_core.models.model_from_config()` will be able to deserialize any\n+    custom object referenced by a saved config (e.g. a custom layer or metric).\n \n     Example:\n \n@@ -108,18 +108,20 @@ def register_keras_serializable(package=\"Custom\", name=None):\n     class MyDense(keras_core.layers.Dense):\n         pass\n \n-    assert keras_core.saving.get_registered_object('my_package>MyDense') == MyDense\n-    assert keras_core.saving.get_registered_name(MyDense) == 'my_package>MyDense'\n+    assert keras_core.saving.get_registered_object(\n+        'my_package>MyDense') == MyDense\n+    assert keras_core.saving.get_registered_name(\n+        MyDense) == 'my_package>MyDense'\n     ```\n \n     Args:\n         package: The package that this class belongs to. This is used for the\n-            `key` (which is `\"package>name\"`) to idenfify the class. Note that this\n-            is the first argument passed into the decorator.\n+            `key` (which is `\"package>name\"`) to idenfify the class. Note that\n+            this is the first argument passed into the decorator.\n         name: The name to serialize this class under in this package. If not\n-            provided or `None`, the class' name will be used (note that this is the\n-            case when the decorator is used with only one argument, which becomes\n-            the `package`).\n+            provided or `None`, the class' name will be used (note that this is\n+            the case when the decorator is used with only one argument, which\n+            becomes the `package`).\n \n     Returns:\n         A decorator that registers the decorated class with the passed names.\n@@ -187,7 +189,8 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n         custom_objects: A dictionary of custom objects to look the name up in.\n             Generally, custom_objects is provided by the user.\n         module_objects: A dictionary of custom objects to look the name up in.\n-            Generally, module_objects is provided by midlevel library implementers.\n+            Generally, module_objects is provided by midlevel library\n+            implementers.\n \n     Returns:\n         An instantiable class associated with `name`, or `None` if no such class\n\n@@ -377,7 +377,9 @@ class SavingTest(testing.TestCase):\n     #         os.path.join(self.get_temp_dir(), \"my_model.keras\")\n     #     )\n     #     model = CompileOverridingModel()\n-    #     with mock.patch(\"re.match\", autospec=True) as mock_re_match, mock.patch(\n+    #     with mock.patch(\n+    #         \"re.match\", autospec=True\n+    #     ) as mock_re_match, mock.patch(\n     #         \"tensorflow.compat.v2.io.gfile.copy\", autospec=True\n     #     ) as mock_copy:\n     #         # Mock Remote Path check to true to test gfile copy logic\n@@ -389,7 +391,9 @@ class SavingTest(testing.TestCase):\n     #         self.assertIn(str(temp_filepath), mock_copy.call_args.args)\n \n     # def test_load_model_api_endpoint(self):\n-    #     temp_filepath = Path(os.path.join(self.get_temp_dir(), \"mymodel.keras\"))\n+    #     temp_filepath = Path(\n+    #         os.path.join(self.get_temp_dir(), \"mymodel.keras\")\n+    #     )\n     #     model = self._get_functional_model()\n     #     ref_input = np.random.random((10, 32))\n     #     ref_output = model.predict(ref_input)\n@@ -448,7 +452,9 @@ class SavingTest(testing.TestCase):\n     #     with self.assertRaises(EOFError):\n     #         model.save(temp_filepath, overwrite=False)\n \n-    #     temp_filepath = os.path.join(self.get_temp_dir(), \"mymodel.weights.h5\")\n+    #     temp_filepath = os.path.join(\n+    #         self.get_temp_dir(), \"mymodel.weights.h5\"\n+    #     )\n     #     model = self._get_basic_functional_model()\n     #     model.save_weights(temp_filepath)\n     #     model.save_weights(temp_filepath, overwrite=True)\n@@ -550,7 +556,8 @@ class SavingTest(testing.TestCase):\n #             [\n #                 keras_core.Input(shape=(3,)),\n #                 keras_core.layers.Normalization(\n-#                     mean=np.random.random((3,)), variance=np.random.random((3,))\n+#                     mean=np.random.random((3,)),\n+#                     variance=np.random.random((3,)),\n #                 ),\n #             ]\n #         )\n@@ -566,8 +573,12 @@ class SavingTest(testing.TestCase):\n #     def __init__(self, units):\n #         super(CustomRNN, self).__init__()\n #         self.units = units\n-#         self.projection_1 = keras_core.layers.Dense(units=units, activation=\"tanh\")\n-#         self.projection_2 = keras_core.layers.Dense(units=units, activation=\"tanh\")\n+#         self.projection_1 = keras_core.layers.Dense(\n+#             units=units, activation=\"tanh\"\n+#         )\n+#         self.projection_2 = keras_core.layers.Dense(\n+#             units=units, activation=\"tanh\"\n+#         )\n #         self.classifier = keras_core.layers.Dense(1)\n \n #     def call(self, inputs):\n@@ -647,7 +658,9 @@ class SavingTest(testing.TestCase):\n #         input_dim = 5\n #         batch_size = 16\n \n-#         inputs = keras_core.Input(batch_shape=(batch_size, timesteps, input_dim))\n+#         inputs = keras_core.Input(\n+#             batch_shape=(batch_size, timesteps, input_dim)\n+#         )\n #         x = keras_core.layers.Conv1D(32, 3)(inputs)\n #         outputs = CustomRNN(32)(x)\n \n@@ -665,7 +678,9 @@ class SavingTest(testing.TestCase):\n #         )\n \n #         inputs = keras_core.Input(shape=(4, 4))\n-#         outputs = keras_core.layers.Dense(1, activation=GrowthFactor(0.5))(inputs)\n+#         outputs = keras_core.layers.Dense(\n+#             1, activation=GrowthFactor(0.5)\n+#         )(inputs)\n #         model = keras_core.Model(inputs, outputs)\n \n #         model.save(temp_filepath)\n@@ -676,7 +691,9 @@ class SavingTest(testing.TestCase):\n #             _ = keras_core.models.load_model(temp_filepath)\n \n #     def test_complex_model_without_explicit_deserialization(self):\n-#         temp_filepath = os.path.join(self.get_temp_dir(), \"complex_model.keras\")\n+#         temp_filepath = os.path.join(\n+#             self.get_temp_dir(), \"complex_model.keras\"\n+#         )\n \n #         inputs = keras_core.Input((32,))\n #         outputs = ComplexModel(first_layer=FactorLayer(0.5))(inputs)\n\n@@ -124,8 +124,8 @@ class CompileMetrics(metrics_module.Metric):\n             weighted_metrics, (list, tuple, dict)\n         ):\n             raise ValueError(\n-                \"Expected `weighted_metrics` argument to be a list, tuple, or dict. \"\n-                f\"Received instead: weighted_metrics={weighted_metrics} \"\n+                \"Expected `weighted_metrics` argument to be a list, tuple, or \"\n+                f\"dict. Received instead: weighted_metrics={weighted_metrics} \"\n                 f\"of type {type(weighted_metrics)}\"\n             )\n         self._user_metrics = metrics\n@@ -195,14 +195,16 @@ class CompileMetrics(metrics_module.Metric):\n             else:\n                 if not isinstance(metrics, list):\n                     raise ValueError(\n-                        f\"When there is only a single output, the `{argument_name}` argument \"\n-                        \"must be a list of metric objects. \"\n-                        f\"Received instead:\\n{argument_name}={metrics} of type {type(metrics)}\"\n+                        \"When there is only a single output, the \"\n+                        f\"`{argument_name}` argument must be a list of metric \"\n+                        f\"objects. Received instead:\\n\"\n+                        f\"{argument_name}={metrics} of type {type(metrics)}\"\n                     )\n                 if not all(is_function_like(m) for m in metrics):\n                     raise ValueError(\n-                        f\"Expected all entries in the `{argument_name}` list to be \"\n-                        f\"metric objects. Received instead:\\n{argument_name}={metrics}\"\n+                        f\"Expected all entries in the `{argument_name}` list \"\n+                        f\"to be metric objects. Received instead:\\n\"\n+                        f\"{argument_name}={metrics}\"\n                     )\n                 flat_metrics.append(\n                     MetricsList(\n@@ -218,24 +220,27 @@ class CompileMetrics(metrics_module.Metric):\n                 if len(metrics) != len(y_pred):\n                     raise ValueError(\n                         \"For a model with multiple outputs, \"\n-                        f\"when providing the `{argument_name}` argument as a list, \"\n-                        \"it should have as many entries as the model has outputs. \"\n-                        f\"Received:\\n{argument_name}={metrics}\\nof length {len(metrics)} \"\n-                        f\"whereas the model has {len(y_pred)} outputs.\"\n+                        f\"when providing the `{argument_name}` argument as a \"\n+                        \"list, it should have as many entries as the model has \"\n+                        f\"outputs. Received:\\n{argument_name}={metrics}\\nof \"\n+                        f\"length {len(metrics)} whereas the model has \"\n+                        f\"{len(y_pred)} outputs.\"\n                     )\n                 if not all(isinstance(mls, list) for mls in metrics):\n                     raise ValueError(\n                         \"For a model with multiple outputs, \"\n-                        f\"when providing the `{argument_name}` argument as a list, \"\n-                        \"each list entry should itself be a list (the list of metrics \"\n-                        f\"corresponding to that output). Received:\\n{argument_name}={metrics}\"\n+                        f\"when providing the `{argument_name}` argument as a \"\n+                        \"list, each list entry should itself be a list \"\n+                        \"(the list of metrics corresponding to that output). \"\n+                        f\"Received:\\n{argument_name}={metrics}\"\n                     )\n                 for mls, yt, yp in zip(metrics, y_true, y_pred):\n                     if not all(is_function_like(e) for e in mls):\n                         raise ValueError(\n-                            f\"All entries in the sublists of the `{argument_name}` \"\n-                            \"list should be metric objects. \"\n-                            f\"Found the following sublist with unknown types: {mls}\"\n+                            f\"All entries in the sublists of the \"\n+                            f\"`{argument_name}` list should be metric objects. \"\n+                            f\"Found the following sublist with unknown \"\n+                            f\"types: {mls}\"\n                         )\n                     flat_metrics.append(\n                         MetricsList(\n@@ -249,29 +254,30 @@ class CompileMetrics(metrics_module.Metric):\n             elif isinstance(metrics, dict):\n                 if output_names is None:\n                     raise ValueError(\n-                        f\"Argument `{argument_name}` can only be provided as a dict \"\n-                        \"when the model also returns a dict of outputs. Received \"\n-                        f\"{argument_name}={metrics}\"\n+                        f\"Argument `{argument_name}` can only be provided as a \"\n+                        \"dict when the model also returns a dict of outputs. \"\n+                        f\"Received {argument_name}={metrics}\"\n                     )\n                 for name in metrics.keys():\n                     if name not in output_names:\n                         raise ValueError(\n                             f\"In the dict argument `{argument_name}`, key \"\n-                            f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received:\\n{argument_name}={metrics}\"\n+                            f\"'{name}' does not correspond to any model \"\n+                            f\"output. Received:\\n{argument_name}={metrics}\"\n                         )\n                     if not isinstance(metrics[name], list):\n                         raise ValueError(\n                             \"For a model with multiple outputs, \"\n-                            f\"when providing the `{argument_name}` argument as a dict, \"\n-                            \"each dict entry should be a list (the list of metrics \"\n-                            \"corresponding to that output). \"\n-                            f\"At key '{name}', received invalid type:\\n{metrics[name]}\"\n+                            f\"when providing the `{argument_name}` argument as \"\n+                            \"a dict, each dict entry should be a list (the \"\n+                            \"list of metrics corresponding to that output). \"\n+                            f\"At key '{name}', received invalid type:\\n\"\n+                            f\"{metrics[name]}\"\n                         )\n                     if not all(is_function_like(e) for e in metrics[name]):\n                         raise ValueError(\n-                            f\"All entries in the sublists of the `{argument_name}` \"\n-                            \"dict should be metric objects. \"\n+                            f\"All entries in the sublists of the \"\n+                            f\"`{argument_name}` dict should be metric objects. \"\n                             f\"At key '{name}', found the following sublist \"\n                             f\"with unknown types: {metrics[name]}\"\n                         )\n@@ -369,8 +375,8 @@ class CompileLoss(losses_module.Loss):\n     ):\n         if loss_weights and not isinstance(loss_weights, (list, tuple, dict)):\n             raise ValueError(\n-                \"Expected `loss_weights` argument to be a list, tuple, or dict. \"\n-                f\"Received instead: loss_weights={loss_weights} \"\n+                \"Expected `loss_weights` argument to be a list, tuple, or \"\n+                f\"dict. Received instead: loss_weights={loss_weights} \"\n                 f\"of type {type(loss_weights)}\"\n             )\n         self._user_loss = loss\n@@ -400,7 +406,7 @@ class CompileLoss(losses_module.Loss):\n         if num_outputs == 1:\n             if not is_function_like(loss):\n                 raise ValueError(\n-                    f\"When there is only a single output, the `loss` argument \"\n+                    \"When there is only a single output, the `loss` argument \"\n                     \"must be a callable. \"\n                     f\"Received instead:\\nloss={loss} of type {type(loss)}\"\n                 )\n@@ -408,9 +414,11 @@ class CompileLoss(losses_module.Loss):\n             if loss_weights:\n                 if not isinstance(loss_weights, float):\n                     raise ValueError(\n-                        f\"When there is only a single output, the `loss_weights` argument \"\n+                        \"When there is only a single output, the \"\n+                        \"`loss_weights` argument \"\n                         \"must be a Python float. \"\n-                        f\"Received instead: loss_weights={loss_weights} of type {type(loss_weights)}\"\n+                        f\"Received instead: loss_weights={loss_weights} of \"\n+                        f\"type {type(loss_weights)}\"\n                     )\n                 flat_loss_weights.append(loss_weights)\n             else:\n@@ -419,7 +427,7 @@ class CompileLoss(losses_module.Loss):\n             if len(loss) != len(y_pred):\n                 raise ValueError(\n                     \"For a model with multiple outputs, \"\n-                    f\"when providing the `loss` argument as a list, \"\n+                    \"when providing the `loss` argument as a list, \"\n                     \"it should have as many entries as the model has outputs. \"\n                     f\"Received:\\nloss={loss}\\nof length {len(loss)} \"\n                     f\"whereas the model has {len(y_pred)} outputs.\"\n@@ -427,7 +435,7 @@ class CompileLoss(losses_module.Loss):\n             if not all(is_function_like(e) for e in loss):\n                 raise ValueError(\n                     \"For a model with multiple outputs, \"\n-                    f\"when providing the `loss` argument as a list, \"\n+                    \"when providing the `loss` argument as a list, \"\n                     \"each list entry should be a callable (the loss function \"\n                     \"corresponding to that output). \"\n                     f\"Received: loss={loss}\"\n@@ -439,24 +447,26 @@ class CompileLoss(losses_module.Loss):\n                 if not isinstance(loss_weights, (list, tuple)):\n                     raise ValueError(\n                         \"If the `loss` argument is provided as a list/tuple, \"\n-                        \"the `loss_weight` argument should also be provided as a list/tuple, \"\n-                        f\"of equal length. Received: loss_weights={loss_weights}\"\n+                        \"the `loss_weight` argument should also be provided as \"\n+                        \"a list/tuple, of equal length. \"\n+                        f\"Received: loss_weights={loss_weights}\"\n                     )\n                 if len(loss_weights) != len(y_pred):\n                     raise ValueError(\n                         \"For a model with multiple outputs, \"\n-                        f\"when providing the `loss_weights` argument as a list, \"\n-                        \"it should have as many entries as the model has outputs. \"\n-                        f\"Received: loss_weights={loss_weights} of length {len(loss_weights)} \"\n-                        f\"whereas the model has {len(y_pred)} outputs.\"\n+                        \"when providing the `loss_weights` argument as a list, \"\n+                        \"it should have as many entries as the model has \"\n+                        f\"outputs. Received: loss_weights={loss_weights} of \"\n+                        f\"length {len(loss_weights)} whereas the model has \"\n+                        f\"{len(y_pred)} outputs.\"\n                     )\n                 if not all(isinstance(e, float) for e in loss_weights):\n                     raise ValueError(\n                         \"For a model with multiple outputs, \"\n-                        f\"when providing the `loss_weights` argument as a list, \"\n-                        \"each list entry should be a Python float (the weighting coefficient \"\n-                        \"corresponding to the loss for that output). \"\n-                        f\"Received: loss_weights={loss_weights}\"\n+                        \"when providing the `loss_weights` argument as a \"\n+                        \"list, each list entry should be a Python float (the \"\n+                        \"weighting coefficient corresponding to the loss for \"\n+                        f\"that output). Received: loss_weights={loss_weights}\"\n                     )\n                 flat_loss_weights = list(loss_weights)\n             else:\n@@ -464,21 +474,21 @@ class CompileLoss(losses_module.Loss):\n         elif isinstance(loss, dict):\n             if output_names is None:\n                 raise ValueError(\n-                    f\"Argument `loss` can only be provided as a dict \"\n-                    \"when the model also returns a dict of outputs. Received \"\n-                    f\"loss={loss}\"\n+                    \"Argument `loss` can only be provided as a dict \"\n+                    \"when the model also returns a dict of outputs. \"\n+                    f\"Received loss={loss}\"\n                 )\n             for name in loss.keys():\n                 if name not in output_names:\n                     raise ValueError(\n-                        f\"In the dict argument `loss`, key \"\n+                        \"In the dict argument `loss`, key \"\n                         f\"'{name}' does not correspond to any model output. \"\n                         f\"Received:\\nloss={loss}\"\n                     )\n                 if not is_function_like(loss[name]):\n                     raise ValueError(\n                         \"For a model with multiple outputs, \"\n-                        f\"when providing the `loss` argument as a dict, \"\n+                        \"when providing the `loss` argument as a dict, \"\n                         \"each dict entry should be a callable (the loss \"\n                         \"function corresponding to that output). \"\n                         f\"At key '{name}', received invalid type:\\n{loss[name]}\"\n@@ -495,23 +505,24 @@ class CompileLoss(losses_module.Loss):\n                 if not isinstance(loss_weights, dict):\n                     raise ValueError(\n                         \"If the `loss` argument is provided as a dict, \"\n-                        \"the `loss_weight` argument should also be provided as a dict. \"\n-                        f\"Received: loss_weights={loss_weights}\"\n+                        \"the `loss_weight` argument should also be provided as \"\n+                        f\"a dict. Received: loss_weights={loss_weights}\"\n                     )\n                 for name in loss_weights.keys():\n                     if name not in output_names:\n                         raise ValueError(\n-                            f\"In the dict argument `loss_weights`, key \"\n-                            f\"'{name}' does not correspond to any model output. \"\n-                            f\"Received: loss_weights={loss_weights}\"\n+                            \"In the dict argument `loss_weights`, key \"\n+                            f\"'{name}' does not correspond to any model \"\n+                            f\"output. Received: loss_weights={loss_weights}\"\n                         )\n                     if not isinstance(loss_weights[name], float):\n                         raise ValueError(\n                             \"For a model with multiple outputs, \"\n-                            f\"when providing the `loss_weights` argument as a dict, \"\n-                            \"each dict entry should be a Python float (the weighting coefficient \"\n-                            \"corresponding to the loss for that output). \"\n-                            f\"At key '{name}', received invalid type:\\n{loss_weights[name]}\"\n+                            \"when providing the `loss_weights` argument as a \"\n+                            \"dict, each dict entry should be a Python float \"\n+                            \"(the weighting coefficient corresponding to the \"\n+                            f\"loss for that output). At key '{name}', \"\n+                            f\"received invalid type:\\n{loss_weights[name]}\"\n                         )\n                 for name in output_names:\n                     if name in loss_weights:\n\n@@ -15,7 +15,9 @@ except ImportError:\n \n \n class ArrayDataAdapter(DataAdapter):\n-    \"\"\"Adapter that handles array-like objects, e.g. tf.Tensor and NumPy arrays.\"\"\"\n+    \"\"\"Adapter that handles array-like objects,\n+    e.g. tf.Tensor and NumPy arrays.\n+    \"\"\"\n \n     def __init__(\n         self,\n@@ -33,15 +35,16 @@ class ArrayDataAdapter(DataAdapter):\n             issubclass(c, data_adapter_utils.ARRAY_TYPES) for c in flat_types\n         ):\n             raise ValueError(\n-                \"Expected all elements of `x` to be array-like. Received invalid types: \"\n-                f\"x={x}\"\n+                \"Expected all elements of `x` to be array-like. \"\n+                f\"Received invalid types: x={x}\"\n             )\n \n         x, y, sample_weight = convert_to_arrays((x, y, sample_weight))\n         if sample_weight is not None:\n             if class_weight is not None:\n                 raise ValueError(\n-                    \"You cannot `class_weight` and `sample_weight` at the same time.\"\n+                    \"You cannot `class_weight` and `sample_weight` \"\n+                    \"at the same time.\"\n                 )\n             if tf.nest.is_nested(y):\n                 if isinstance(sample_weight, np.ndarray):\n@@ -51,10 +54,12 @@ class ArrayDataAdapter(DataAdapter):\n                     )\n                     if not is_samplewise:\n                         raise ValueError(\n-                            \"For a model with multiple outputs, when providing a single `sample_weight` array, \"\n-                            \"it should only have one scalar score per sample (i.e. shape `(num_samples,)`). \"\n-                            \"If you want to use non-scalar sample weights, pass a `sample_weight` argument with one \"\n-                            \"array per model output.\"\n+                            \"For a model with multiple outputs, when providing \"\n+                            \"a single `sample_weight` array, it should only \"\n+                            \"have one scalar score per sample \"\n+                            \"(i.e. shape `(num_samples,)`). If you want to use \"\n+                            \"non-scalar sample weights, pass a `sample_weight` \"\n+                            \"argument with one array per model output.\"\n                         )\n                     # Replicate the same sample_weight array on all outputs.\n                     sample_weight = tf.nest.map_structure(\n@@ -65,8 +70,8 @@ class ArrayDataAdapter(DataAdapter):\n                         tf.nest.assert_same_structure(y, sample_weight)\n                     except ValueError:\n                         raise ValueError(\n-                            \"You should provide one `sample_weight` array per output in `y`. \"\n-                            \"The two structures did not match:\\n\"\n+                            \"You should provide one `sample_weight` array per \"\n+                            \"output in `y`. The two structures did not match:\\n\"\n                             f\"- y: {y}\\n\"\n                             f\"- sample_weight: {sample_weight}\\n\"\n                         )\n@@ -161,8 +166,8 @@ def convert_to_arrays(arrays, dtype=None):\n             x = x.numpy()\n         if not isinstance(x, np.ndarray):\n             raise ValueError(\n-                \"Expected a NumPy array, tf.Tensor, Pandas Dataframe or Pandas Series. \"\n-                f\"Received invalid input: {x} (of type {type(x)})\"\n+                \"Expected a NumPy array, tf.Tensor, Pandas Dataframe or Pandas \"\n+                f\"Series. Received invalid input: {x} (of type {type(x)})\"\n             )\n         if not str(x.dtype) == str(dtype):\n             x = x.astype(dtype)\n\n@@ -78,19 +78,22 @@ class EpochIterator:\n             # Unsupported args: y, sample_weight, shuffle\n             if y is not None:\n                 raise ValueError(\n-                    \"When providing `x` as a tf.data.Dataset, `y` should not be passed. \"\n-                    \"Instead, the targets should be included as part of the Dataset `x`.\"\n+                    \"When providing `x` as a tf.data.Dataset, `y` should not \"\n+                    \"be passed. Instead, the targets should be included as \"\n+                    \"part of the Dataset `x`.\"\n                 )\n             if sample_weight is not None:\n                 raise ValueError(\n-                    \"When providing `x` as a tf.data.Dataset, `sample_weight` should not be passed. \"\n-                    \"Instead, the sample weights should be included as part of the Dataset `x`.\"\n+                    \"When providing `x` as a tf.data.Dataset, `sample_weight` \"\n+                    \"should not be passed. Instead, the sample weights should \"\n+                    \"be included as part of the Dataset `x`.\"\n                 )\n             # TODO: should we warn or not?\n             # warnings.warn(\n-            #     \"`shuffle=True` was passed, but will be ignored since the data `x` was provided \"\n-            #     \"as a tf.data.Dataset. The Dataset is expected to already \"\n-            #     \"be shuffled (via `.shuffle(tf.data.AUTOTUNE)`)\"\n+            #     \"`shuffle=True` was passed, but will be ignored since the \"\n+            #     \"data `x` was provided as a tf.data.Dataset. The Dataset is \"\n+            #     \"expected to already be shuffled \"\n+            #     \"(via `.shuffle(tf.data.AUTOTUNE)`)\"\n             # )\n         else:\n             # TODO: add support for more types.\n\n@@ -39,8 +39,8 @@ class Trainer:\n         if jit_compile and run_eagerly:\n             jit_compile = False\n             warnings.warn(\n-                \"If `run_eagerly` is True, then `jit_compile` cannot also be True. \"\n-                \"Disabling `jit_compile`.\",\n+                \"If `run_eagerly` is True, then `jit_compile` \"\n+                \"cannot also be True. Disabling `jit_compile`.\",\n                 stacklevel=2,\n             )\n         self.jit_compile = jit_compile\n@@ -174,7 +174,8 @@ class Trainer:\n           def compute_metrics(self, x, y, y_pred, sample_weight):\n             # This super call updates `self.compiled_metrics` and returns\n             # results for all metrics listed in `self.metrics`.\n-            metric_results = super().compute_metrics(x, y, y_pred, sample_weight)\n+            metric_results = super().compute_metrics(\n+                x, y, y_pred, sample_weight)\n \n             # Note that `self.custom_metric` is not listed in `self.metrics`.\n             self.custom_metric.update_state(x, y, y_pred, sample_weight)\n@@ -185,14 +186,14 @@ class Trainer:\n         Args:\n             x: Input data.\n             y: Target data.\n-            y_pred: Predictions returned by the model (output of `model.call(x)`)\n+            y_pred: Predictions returned by the model output of `model.call(x)`.\n             sample_weight: Sample weights for weighting the loss function.\n \n         Returns:\n             A `dict` containing values that will be passed to\n-            `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically, the\n-            values of the metrics listed in `self.metrics` are returned. Example:\n-            `{'loss': 0.2, 'accuracy': 0.7}`.\n+            `tf.keras.callbacks.CallbackList.on_train_batch_end()`. Typically,\n+            the values of the metrics listed in `self.metrics` are returned.\n+            Example: `{'loss': 0.2, 'accuracy': 0.7}`.\n         \"\"\"\n         del x  # The default implementation does not use `x`.\n         if self._compile_metrics is not None:\n@@ -323,7 +324,7 @@ class Trainer:\n         return result\n \n     def _flatten_metrics_in_order(self, logs):\n-        \"\"\"Turns the `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n+        \"\"\"Turns `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n         metric_names = [m.name for m in self.metrics]\n         results = []\n         for name in metric_names:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
