{"custom_id": "keras#ff28c354af4692f6b8cc2b7662f5d010479bd6f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1324 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 81 | Complexity Δ (Sum/Max): 153/82 | Churn Δ: 1324 | Churn Cumulative: 1324 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.7072829131652661\n\nDIFF:\n@@ -0,0 +1,667 @@\n+import inspect\n+\n+import jax\n+import numpy as np\n+\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.layers.layer import Layer\n+from keras.saving import serialization_lib\n+from keras.utils import shape_utils\n+from keras.utils import tracking\n+\n+\n+@keras_export(\"keras.layers.JaxLayer\")\n+class JaxLayer(Layer):\n+    \"\"\"Keras Layer that wraps a JAX model.\n+\n+    This layer enables the use of JAX components within Keras when using JAX as\n+    the backend for Keras.\n+\n+    ## Model function\n+\n+    This layer accepts JAX models in the form of a function, `call_fn`, which\n+    must take the following arguments with these exact names:\n+\n+    - `params`: trainable parameters of the model.\n+    - `state` (*optional*): non-trainable state of the model. Can be omitted if\n+        the model has no non-trainable state.\n+    - `rng` (*optional*): a `jax.random.PRNGKey` instance. Can be omitted if the\n+        model does not need RNGs, neither during training nor during inference.\n+    - `inputs`: inputs to the model, a JAX array or a `PyTree` of arrays.\n+    - `training` (*optional*): an argument specifying if we're in training mode\n+        or inference mode, `True` is passed in training mode. Can be omitted if\n+        the model behaves the same in training mode and inference mode.\n+\n+    The `inputs` argument is mandatory. Inputs to the model must be provided via\n+    a single argument. If the JAX model takes multiple inputs as separate\n+    arguments, they must be combined into a single structure, for instance in a\n+    `tuple` or a `dict`.\n+\n+    ## Model weights initialization\n+\n+    The initialization of the `params` and `state` of the model can be handled\n+    by this layer, in which case the `init_fn` argument must be provided. This\n+    allows the model to be initialized dynamically with the right shape.\n+    Alternatively, and if the shape is known, the `params` argument and\n+    optionally the `state` argument can be used to create an already initialized\n+    model.\n+\n+    The `init_fn` function, if provided, must take the following arguments with\n+    these exact names:\n+\n+    - `rng`: a `jax.random.PRNGKey` instance.\n+    - `inputs`: a JAX array or a `PyTree` of arrays with placeholder values to\n+        provide the shape of the inputs.\n+    - `training` (*optional*): an argument specifying if we're in training mode\n+        or inference mode. `True` is always passed to `init_fn`. Can be omitted\n+        regardless of whether `call_fn` has a `training` argument.\n+\n+    ## Models with non-trainable state\n+\n+    For JAX models that have non-trainable state:\n+\n+    - `call_fn` must have a `state` argument\n+    - `call_fn` must return a `tuple` containing the outputs of the model and\n+        the new non-trainable state of the model\n+    - `init_fn` must return a `tuple` containing the initial trainable params of\n+        the model and the initial non-trainable state of the model.\n+\n+    This code shows a possible combination of `call_fn` and `init_fn` signatures\n+    for a model with non-trainable state. In this example, the model has a\n+    `training` argument and an `rng` argument in `call_fn`.\n+\n+    ```python\n+    def stateful_call(params, state, rng, inputs, training):\n+        outputs = ...\n+        new_state = ...\n+        return outputs, new_state\n+\n+    def stateful_init(rng, inputs):\n+        initial_params = ...\n+        initial_state = ...\n+        return initial_params, initial_state\n+    ```\n+\n+    ## Models without non-trainable state\n+\n+    For JAX models with no non-trainable state:\n+\n+    - `call_fn` must not have a `state` argument\n+    - `call_fn` must return only the outputs of the model\n+    - `init_fn` must return only the initial trainable params of the model.\n+\n+    This code shows a possible combination of `call_fn` and `init_fn` signatures\n+    for a model without non-trainable state. In this example, the model does not\n+    have a `training` argument and does not have an `rng` argument in `call_fn`.\n+\n+    ```python\n+    def stateless_call(params, inputs):\n+        outputs = ...\n+        return outputs\n+\n+    def stateless_init(rng, inputs):\n+        initial_params = ...\n+        return initial_params\n+    ```\n+\n+    ## Conforming to the required signature\n+\n+    If a model has a different signature than the one required by `JaxLayer`,\n+    one can easily write a wrapper method to adapt the arguments. This example\n+    shows a model that has multiple inputs as separate arguments, expects\n+    multiple RNGs in a `dict`, and has a `deterministic` argument with the\n+    opposite meaning of `training`. To conform, the inputs are combined in a\n+    single structure using a `tuple`, the RNG is split and used the populate the\n+    expected `dict`, and the Boolean flag is negated:\n+\n+    ```python\n+    def my_model_fn(params, rngs, input1, input2, deterministic):\n+        ...\n+        if not deterministic:\n+            dropout_rng = rngs[\"dropout\"]\n+            keep = jax.random.bernoulli(dropout_rng, dropout_rate, x.shape)\n+            x = jax.numpy.where(keep, x / dropout_rate, 0)\n+            ...\n+        ...\n+        return outputs\n+\n+    def my_model_wrapper_fn(params, rng, inputs, training):\n+        input1, input2 = inputs\n+        rng1, rng2 = jax.random.split(rng)\n+        rngs = {\"dropout\": rng1, \"preprocessing\": rng2}\n+        deterministic = not training\n+        return my_model_fn(params, rngs, input1, input2, deterministic)\n+\n+    keras_layer = JaxLayer(my_model_wrapper_fn, params=initial_params)\n+    ```\n+\n+    ## Usage with Haiku modules\n+\n+    `JaxLayer` enables the use of [Haiku](https://dm-haiku.readthedocs.io)\n+    components in the form of\n+    [`haiku.Module`](https://dm-haiku.readthedocs.io/en/latest/api.html#module).\n+    This is achieved by transforming the module per the Haiku pattern and then\n+    passing `module.apply` in the `call_fn` parameter and `module.init` in the\n+    `init_fn` parameter if needed.\n+\n+    If the model has non-trainable state, it should be transformed with\n+    [`haiku.transform_with_state`](\n+      https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.transform_with_state).\n+    If the model has no non-trainable state, it should be transformed with\n+    [`haiku.transform`](\n+      https://dm-haiku.readthedocs.io/en/latest/api.html#haiku.transform).\n+    Additionally, and optionally, if the module does not use RNGs in \"apply\", it\n+    can be transformed with\n+    [`haiku.without_apply_rng`](\n+      https://dm-haiku.readthedocs.io/en/latest/api.html#without-apply-rng).\n+\n+    The following example shows how to create a `JaxLayer` from a Haiku module\n+    that uses random number generators via `hk.next_rng_key()` and takes a\n+    training positional argument:\n+\n+    ```python\n+    class MyHaikuModule(hk.Module):\n+        def __call__(self, x, training):\n+            x = hk.Conv2D(32, (3, 3))(x)\n+            x = jax.nn.relu(x)\n+            x = hk.AvgPool((1, 2, 2, 1), (1, 2, 2, 1), \"VALID\")(x)\n+            x = hk.Flatten()(x)\n+            x = hk.Linear(200)(x)\n+            if training:\n+                x = hk.dropout(rng=hk.next_rng_key(), rate=0.3, x=x)\n+            x = jax.nn.relu(x)\n+            x = hk.Linear(10)(x)\n+            x = jax.nn.softmax(x)\n+            return x\n+\n+    def my_haiku_module_fn(inputs, training):\n+        module = MyHaikuModule()\n+        return module(inputs, training)\n+\n+    transformed_module = hk.transform(my_haiku_module_fn)\n+\n+    keras_layer = JaxLayer(\n+        call_fn=transformed_module.apply,\n+        init_fn=transformed_module.init,\n+    )\n+    ```\n+\n+    Args:\n+        call_fn: The function to call the model. See description above for the\n+            list of arguments it takes and the outputs it returns.\n+        init_fn: the function to call to initialize the model. See description\n+            above for the list of arguments it takes and the ouputs it returns.\n+            If `None`, then `params` and/or `state` must be provided.\n+      params: A `PyTree` containing all the model trainable parameters. This\n+            allows passing trained parameters or controlling the initialization.\n+            If both `params` and `state` are `None`, `init_fn` is called at\n+            build time to initialize the trainable parameters of the model.\n+      state: A `PyTree` containing all the model non-trainable state. This\n+            allows passing learned state or controlling the initialization. If\n+            both `params` and `state` are `None`, and `call_fn` takes a `state`\n+            argument, then `init_fn` is called at build time to initialize the\n+            non-trainable state of the model.\n+      seed: Seed for random number generator. Optional.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        call_fn,\n+        init_fn=None,\n+        params=None,\n+        state=None,\n+        seed=None,\n+        **kwargs,\n+    ):\n+        if backend.backend() != \"jax\":\n+            raise ValueError(\n+                \"JaxLayer is only supported with the JAX backend. Current \"\n+                f\"backend: {backend.backend()}\"\n+            )\n+\n+        if init_fn is None and params is None and state is None:\n+            raise ValueError(\n+                \"`init_fn`, `params` and `state` cannot all be `None`.\"\n+            )\n+\n+        super().__init__(**kwargs)\n+        self.call_fn = call_fn\n+        self.init_fn = init_fn\n+        self.tracked_params = self._create_variables(params, trainable=True)\n+        self.tracked_state = self._create_variables(state, trainable=False)\n+        self.seed_generator = backend.random.SeedGenerator(seed)\n+\n+        self.call_fn_arguments = self._validate_signature(\n+            call_fn,\n+            \"call_fn\",\n+            {\"params\", \"state\", \"rng\", \"inputs\", \"training\"},\n+            {\"inputs\"},\n+        )\n+        self.has_state = \"state\" in self.call_fn_arguments\n+\n+        if init_fn:\n+            self.init_fn_arguments = self._validate_signature(\n+                init_fn, \"init_fn\", {\"rng\", \"inputs\", \"training\"}, {\"inputs\"}\n+            )\n+\n+    def _validate_signature(self, fn, fn_name, allowed, required):\n+        fn_parameters = inspect.signature(fn).parameters\n+        for parameter_name in required:\n+            if parameter_name not in fn_parameters:\n+                raise ValueError(\n+                    f\"Missing required argument in `{fn_name}`: \"\n+                    f\"`{parameter_name}`\"\n+                )\n+\n+        parameter_names = []\n+        for parameter in fn_parameters.values():\n+            if parameter.name not in allowed:\n+                raise ValueError(\n+                    f\"Unsupported argument in `{fn_name}`: `{parameter.name}`, \"\n+                    f\"supported arguments are `{'`, `'.join(allowed)}`\"\n+                )\n+            parameter_names.append(parameter.name)\n+\n+        return parameter_names\n+\n+    @tracking.no_automatic_dependency_tracking\n+    def _create_variables(self, values, trainable):\n+        \"\"\"Create a structure of variables from a structure of JAX arrays.\n+\n+        `values` is traversed via JAX's `tree_map`. When a leaf is a JAX array\n+        or a tensor-like object, a corresponding variable is created with it as\n+        the initial value. The resulting structure of variables is assigned to\n+        `self.params` or `self.state` depending on `trainable`. Then, a\n+        flattened version of the variables is returned for tracking.\n+        `self.params` or `self.state` are intentionally not tracked because\n+        structures like `TrackedList` interfere with `jax.tree_utils`.\n+        Note that leaf objects that are not JAX arrays and not tensor-like are\n+        left intact as they are assumed to be configuration used by the model.\n+\n+        Args:\n+            values: the structure of values to traverse.\n+            trainable: whether to create trainable variables.\n+\n+        Returns:\n+            flat list of variables initialized with `values` for tracking.\n+        \"\"\"\n+\n+        def create_variable(value):\n+            if backend.is_tensor(value) or isinstance(value, np.ndarray):\n+                variable = self.add_weight(\n+                    value.shape, initializer=\"zeros\", trainable=trainable\n+                )\n+                variable.assign(value)\n+                return variable\n+            elif isinstance(value, (np.generic, int, float)):\n+                variable = self.add_weight(\n+                    (), initializer=\"zeros\", trainable=trainable\n+                )\n+                variable.assign(value)\n+                return variable\n+            else:\n+                return value\n+\n+        # Use JAX's tree_map as it understands registered classes.\n+        variables = jax.tree_util.tree_map(create_variable, values)\n+\n+        if trainable:\n+            self.params = variables\n+        else:\n+            self.state = variables\n+\n+        return jax.tree_util.tree_flatten(variables)\n+\n+    def _get_init_rng(self):\n+        \"\"\"\n+        Returns a JAX `PRNGKey` or structure of `PRNGKey`s to pass to `init_fn`.\n+\n+        By default, this returns a single `PRNGKey` retrieved by calling\n+        `self.seed_generator.next()`. Override this to return a different\n+        structure.\n+\n+        Returns:\n+            a JAX `PRNGKey` or structure of `PRNGKey`s that will be passed as\n+            the `rng` argument of `init_fn`.\n+        \"\"\"\n+        return self.seed_generator.next()\n+\n+    def _get_call_rng(self, training):\n+        \"\"\"\n+        Returns a JAX `PRNGKey` or structure of `PRNGKey`s to pass to `call_fn`.\n+\n+        By default, this returns a single `PRNGKey` retrieved by calling\n+        `self.seed_generator.next()` when `training` is `True`, and `None` when\n+        `training` is `False`. Override this to return a different structure or\n+        to pass RNGs in inference mode too.\n+\n+        Returns:\n+            a JAX `PRNGKey` or structure of `PRNGKey`s that will be passed as\n+            the `rng` argument of `call_fn`.\n+        \"\"\"\n+        if training:\n+            return self.seed_generator.next()\n+        else:\n+            return None\n+\n+    def build(self, input_shape):\n+        if self.params is not None or self.state is not None:\n+            return\n+\n+        # Initialize `params` and `state` if needed by calling `init_fn`.\n+        def create_input(shape):\n+            shape = [d if d is not None else 1 for d in shape]\n+            return jax.numpy.ones(shape)\n+\n+        init_inputs = shape_utils.map_shape_structure(create_input, input_shape)\n+        init_args = []\n+        for argument_name in self.init_fn_arguments:\n+            if argument_name == \"rng\":\n+                init_args.append(self._get_init_rng())\n+            elif argument_name == \"inputs\":\n+                init_args.append(init_inputs)\n+            elif argument_name == \"training\":\n+                init_args.append(True)\n+\n+        init_result = self.init_fn(*init_args)\n+        if self.has_state:\n+            init_params, init_state = init_result\n+        else:\n+            init_params, init_state = init_result, None\n+\n+        self.tracked_params = self._create_variables(\n+            init_params, trainable=True\n+        )\n+        self.tracked_state = self._create_variables(init_state, trainable=False)\n+\n+    def call(self, inputs, training=False):\n+        def unwrap_variable(variable):\n+            return None if variable is None else variable.value\n+\n+        call_args = []\n+        for argument_name in self.call_fn_arguments:\n+            if argument_name == \"params\":\n+                call_args.append(\n+                    jax.tree_util.tree_map(unwrap_variable, self.params)\n+                )\n+            elif argument_name == \"state\":\n+                call_args.append(\n+                    jax.tree_util.tree_map(unwrap_variable, self.state)\n+                )\n+            elif argument_name == \"rng\":\n+                call_args.append(self._get_call_rng(training))\n+            elif argument_name == \"inputs\":\n+                call_args.append(inputs)\n+            elif argument_name == \"training\":\n+                call_args.append(training)\n+\n+        def assign_state_to_variable(value, variable):\n+            # This exists only to make debugging this error case easier.\n+            if not hasattr(variable, \"assign\"):\n+                raise ValueError(\n+                    \"Structure mismatch: the structure of the state returned \"\n+                    \"by `call` does not match the structure of the state at \"\n+                    \"initialization time.\"\n+                )\n+            variable.assign(value)\n+\n+        if self.has_state:\n+            predictions, new_state = self.call_fn(*call_args)\n+            jax.tree_util.tree_map(\n+                assign_state_to_variable, new_state, self.state\n+            )\n+            return predictions\n+        else:\n+            return self.call_fn(*call_args)\n+\n+    def get_config(self):\n+        config = {\n+            \"call_fn\": serialization_lib.serialize_keras_object(self.call_fn),\n+            \"init_fn\": serialization_lib.serialize_keras_object(self.init_fn),\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        call_fn = serialization_lib.deserialize_keras_object(config[\"call_fn\"])\n+        init_fn = serialization_lib.deserialize_keras_object(config[\"init_fn\"])\n+        config[\"call_fn\"] = call_fn\n+        config[\"init_fn\"] = init_fn\n+        return super().from_config(config)\n+\n+\n+@keras_export(\"keras.layers.FlaxLayer\")\n+class FlaxLayer(JaxLayer):\n+    \"\"\"Keras Layer that wraps a [Flax](https://flax.readthedocs.io) module.\n+\n+    This layer enables the use of Flax components in the form of\n+    [`flax.linen.Module`](\n+            https://flax.readthedocs.io/en/latest/flax.linen.html#module)\n+    instances within Keras when using JAX as the backend for Keras.\n+\n+    The module method to use for the forward pass can be specified via the\n+    `method` argument and is `__call__` by default. This method must take the\n+    following arguments with these exact names:\n+\n+    - `self` if the method is bound to the module, which is the case for the\n+        default of `__call__`, and `module` otherwise to pass the module.\n+    - `inputs`: the inputs to the model, a JAX array or a `PyTree` of arrays.\n+    - `training` *(optional)*: an argument specifying if we're in training mode\n+        or inference mode, `True` is passed in training mode.\n+\n+    `FlaxLayer` handles the non-trainable state of your model and required RNGs\n+    automatically. Note that the `mutable` parameter of\n+    [`flax.linen.Module.apply()`](\n+      https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)\n+    is set to `DenyList([\"params\"])`, therefore making the assumption that all\n+    the variables outside of the \"params\" collection are non-trainable weights.\n+\n+    This example shows how to create a `FlaxLayer` from a Flax `Module` with\n+    the default `__call__` method and no training argument:\n+\n+    ```python\n+    class MyFlaxModule(flax.linen.Module):\n+        @flax.linen.compact\n+        def __call__(self, inputs):\n+            x = inputs\n+            x = flax.linen.Conv(features=32, kernel_size=(3, 3))(x)\n+            x = flax.linen.relu(x)\n+            x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n+            x = x.reshape((x.shape[0], -1))  # flatten\n+            x = flax.linen.Dense(features=200)(x)\n+            x = flax.linen.relu(x)\n+            x = flax.linen.Dense(features=10)(x)\n+            x = flax.linen.softmax(x)\n+            return x\n+\n+    flax_module = MyFlaxModule()\n+    keras_layer = FlaxLayer(flax_module)\n+    ```\n+\n+    This example shows how to wrap the module method to conform to the required\n+    signature. This allows having multiple input arguments and a training\n+    argument that has a different name and values. This additionally shows how\n+    to use a function that is not bound to the module.\n+\n+    ```python\n+    class MyFlaxModule(flax.linen.Module):\n+        @flax.linen.compact\n+        def forward(self, input1, input1, deterministic):\n+            ...\n+            return outputs\n+\n+    def my_flax_module_wrapper(module, inputs, training):\n+        input1, input2 = inputs\n+        return module.forward(input1, input2, not training)\n+\n+    flax_module = MyFlaxModule()\n+    keras_layer = FlaxLayer(flax_module)\n+        module=flax_module,\n+        method=my_flax_module_wrapper,\n+    )\n+    ```\n+\n+    Args:\n+        module: An instance of `flax.linen.Module` or subclass.\n+        method: The method to call the model. This is generally a method in the\n+            `Module`. If not provided, the `__call__` method is used. `method`\n+            can also be a function not defined in the `Module`, in which case it\n+            must take the `Module` as the first argument. It is used for both\n+            `Module.init` and `Module.apply`. Details are documented in the\n+            `method` argument of [`flax.linen.Module.apply()`](\n+              https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply).\n+        variables: A `dict` containing all the variables of the module in the\n+            same format as what is returned by [`flax.linen.Module.init()`](\n+              https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.init).\n+            It should contain a \"params\" key and, if applicable, other keys for\n+            collections of variables for non-trainable state. This allows\n+            passing trained parameters and learned non-trainable state or\n+            controlling the initialization. If `None` is passed, the module's\n+            `init` function is called at build time to initialize the variables\n+            of the model.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        module,\n+        method=None,\n+        variables=None,\n+        **kwargs,\n+    ):\n+        # Late import to only require Flax when this is used.\n+        from flax.core import scope as flax_scope\n+\n+        if backend.backend() != \"jax\":\n+            raise ValueError(\n+                \"FlaxLayer is only supported with the JAX backend. Current \"\n+                f\"backend: {backend.backend()}\"\n+            )\n+\n+        self.module = module\n+        self.method = method\n+\n+        apply_mutable = flax_scope.DenyList([\"params\"])\n+\n+        def apply_with_training(params, state, rng, inputs, training):\n+            return self.module.apply(\n+                self._params_and_state_to_variables(params, state),\n+                inputs,\n+                rngs=rng,\n+                method=self.method,\n+                mutable=apply_mutable,\n+                training=training,\n+            )\n+\n+        def apply_without_training(params, state, rng, inputs):\n+            return self.module.apply(\n+                self._params_and_state_to_variables(params, state),\n+                inputs,\n+                rngs=rng,\n+                method=self.method,\n+                mutable=apply_mutable,\n+            )\n+\n+        def init_with_training(rng, inputs, training):\n+            return self._variables_to_params_and_state(\n+                self.module.init(\n+                    rng,\n+                    inputs,\n+                    method=self.method,\n+                    training=training,\n+                )\n+            )\n+\n+        def init_without_training(rng, inputs):\n+            return self._variables_to_params_and_state(\n+                self.module.init(\n+                    rng,\n+                    inputs,\n+                    method=self.method,\n+                )\n+            )\n+\n+        if (\n+            \"training\"\n+            in inspect.signature(method or module.__call__).parameters\n+        ):\n+            call_fn, init_fn = apply_with_training, init_with_training\n+        else:\n+            call_fn, init_fn = apply_without_training, init_without_training\n+\n+        params, state = self._variables_to_params_and_state(variables)\n+\n+        super().__init__(\n+            call_fn=call_fn,\n+            init_fn=init_fn,\n+            params=params,\n+            state=state,\n+            **kwargs,\n+        )\n+\n+    def _params_and_state_to_variables(self, params, state):\n+        if params:\n+            if state:\n+                return {**params, **state}\n+            else:\n+                return params\n+        elif state:\n+            return state\n+        return {}\n+\n+    def _variables_to_params_and_state(self, variables):\n+        # neither params nor state\n+        if variables is None:\n+            return None, None\n+        # state only\n+        if \"params\" not in variables:\n+            return {}, variables\n+        # params only\n+        if len(variables) == 1:\n+            return variables, {}\n+        # both, we need to split\n+        params = {\"params\": variables[\"params\"]}\n+        state = {k: v for k, v in variables.items() if k != \"params\"}\n+        return params, state\n+\n+    def _get_init_rng(self):\n+        return {\n+            \"params\": self.seed_generator.next(),\n+            \"dropout\": self.seed_generator.next(),\n+        }\n+\n+    def _get_call_rng(self, training):\n+        if training:\n+            return {\"dropout\": self.seed_generator.next()}\n+        else:\n+            return {}\n+\n+    def get_config(self):\n+        config_method = self.method\n+        if (\n+            hasattr(self.method, \"__self__\")\n+            and self.method.__self__ == self.module\n+        ):\n+            # A method bound to the module is serialized by name.\n+            config_method = self.method.__name__\n+        config = {\n+            \"module\": serialization_lib.serialize_keras_object(self.module),\n+            \"method\": serialization_lib.serialize_keras_object(config_method),\n+        }\n+        base_config = super().get_config()\n+        # call_fn and init_fn come from module, do not save them.\n+        base_config.pop(\"call_fn\")\n+        base_config.pop(\"init_fn\")\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        module = serialization_lib.deserialize_keras_object(config[\"module\"])\n+        method = serialization_lib.deserialize_keras_object(config[\"method\"])\n+        if isinstance(config[\"method\"], str):\n+            # Deserialize bound method from the module.\n+            method = getattr(module, method)\n+        config[\"module\"] = module\n+        config[\"method\"] = method\n+        return cls(**config)\n\n@@ -0,0 +1,657 @@\n+import os\n+\n+import flax\n+import jax\n+import jax.numpy as jnp\n+import numpy as np\n+import pytest\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras import backend\n+from keras import layers\n+from keras import metrics\n+from keras import models\n+from keras import saving\n+from keras import testing\n+from keras import utils\n+from keras.export import export_lib\n+from keras.saving import object_registration\n+from keras.utils import tree\n+from keras.utils.jax_layer import FlaxLayer\n+from keras.utils.jax_layer import JaxLayer\n+\n+num_classes = 10\n+input_shape = (28, 28, 1)  # Excluding batch_size\n+\n+\n+@object_registration.register_keras_serializable()\n+def jax_stateless_init(rng, inputs):\n+    layer_sizes = [784, 300, 100, 10]\n+    params = []\n+    w_init = jax.nn.initializers.glorot_normal()\n+    b_init = jax.nn.initializers.normal(0.1)\n+    for m, n in zip(layer_sizes[:-1], layer_sizes[1:]):\n+        rng, w_rng = jax.random.split(rng)\n+        rng, b_rng = jax.random.split(rng)\n+        params.append([w_init(w_rng, (m, n)), b_init(b_rng, (n,))])\n+    return params\n+\n+\n+@object_registration.register_keras_serializable()\n+def jax_stateless_apply(params, inputs):\n+    activations = inputs.reshape((inputs.shape[0], -1))  # flatten\n+    for w, b in params[:-1]:\n+        outputs = jnp.dot(activations, w) + b\n+        activations = jnp.tanh(outputs)\n+\n+    final_w, final_b = params[-1]\n+    logits = jnp.dot(activations, final_w) + final_b\n+    return jax.nn.softmax(logits, axis=-1)\n+\n+\n+@object_registration.register_keras_serializable()\n+def jax_stateful_init(rng, inputs, training):\n+    params = jax_stateless_init(rng, inputs)\n+    state = jnp.zeros([], jnp.int32)\n+    return params, state\n+\n+\n+@object_registration.register_keras_serializable()\n+def jax_stateful_apply(params, state, inputs, training):\n+    outputs = jax_stateless_apply(params, inputs)\n+    if training:\n+        state = state + 1\n+    return outputs, state\n+\n+\n+@object_registration.register_keras_serializable()\n+class FlaxTrainingIndependentModel(flax.linen.Module):\n+    @flax.linen.compact\n+    def forward(self, inputs):\n+        x = inputs\n+        x = flax.linen.Conv(features=32, kernel_size=(3, 3))(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n+        x = flax.linen.Conv(features=64, kernel_size=(3, 3))(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n+        x = x.reshape((x.shape[0], -1))  # flatten\n+        x = flax.linen.Dense(features=200)(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.Dense(features=10)(x)\n+        x = flax.linen.softmax(x)\n+        return x\n+\n+    def get_config(self):\n+        return {}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n+\n+@object_registration.register_keras_serializable()\n+class FlaxDropoutModel(flax.linen.Module):\n+    @flax.linen.compact\n+    def my_apply(self, inputs, training):\n+        x = inputs\n+        x = flax.linen.Conv(features=32, kernel_size=(3, 3))(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n+        x = flax.linen.Conv(features=64, kernel_size=(3, 3))(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n+        x = x.reshape((x.shape[0], -1))  # flatten\n+        x = flax.linen.Dense(features=200)(x)\n+        x = flax.linen.Dropout(rate=0.3, deterministic=not training)(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.Dense(features=10)(x)\n+        x = flax.linen.softmax(x)\n+        return x\n+\n+    def get_config(self):\n+        return {}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n+\n+@object_registration.register_keras_serializable()\n+def flax_dropout_wrapper(module, x, training):\n+    return module.my_apply(x, training)\n+\n+\n+@object_registration.register_keras_serializable()\n+class FlaxBatchNormModel(flax.linen.Module):\n+    @flax.linen.compact\n+    def __call__(self, inputs, training=False):\n+        ura = not training\n+        x = inputs\n+        x = flax.linen.Conv(features=12, kernel_size=(3, 3), use_bias=False)(x)\n+        x = flax.linen.BatchNorm(use_running_average=ura, use_scale=False)(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.Conv(features=24, kernel_size=(6, 6), strides=(2, 2))(x)\n+        x = flax.linen.BatchNorm(use_running_average=ura, use_scale=False)(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.Conv(features=32, kernel_size=(6, 6), strides=(2, 2))(x)\n+        x = flax.linen.BatchNorm(use_running_average=ura, use_scale=False)(x)\n+        x = x.reshape((x.shape[0], -1))  # flatten\n+        x = flax.linen.Dense(features=200, use_bias=True)(x)\n+        x = flax.linen.BatchNorm(use_running_average=ura, use_scale=False)(x)\n+        x = flax.linen.Dropout(rate=0.3, deterministic=not training)(x)\n+        x = flax.linen.relu(x)\n+        x = flax.linen.Dense(features=10)(x)\n+        x = flax.linen.softmax(x)\n+        return x\n+\n+    def get_config(self):\n+        return {}\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        return cls(**config)\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"jax\",\n+    reason=\"JaxLayer and FlaxLayer are only supported with JAX backend\",\n+)\n+class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n+    def _test_layer(\n+        self,\n+        model_name,\n+        layer_class,\n+        layer_init_kwargs,\n+        trainable_weights,\n+        trainable_params,\n+        non_trainable_weights,\n+        non_trainable_params,\n+    ):\n+        # Fake MNIST data\n+        x_train = np.random.uniform(size=(320, 28, 28, 1))\n+        y_train = np.eye(num_classes, dtype=\"int32\")[\n+            (np.random.uniform(size=(320,)) * num_classes).astype(\"int32\")\n+        ]\n+        x_test = np.random.uniform(size=(32, 28, 28, 1))\n+\n+        def _count_params(weights):\n+            count = 0\n+            for weight in weights:\n+                count = count + np.prod(weight.shape)\n+            return count\n+\n+        def verify_weights_and_params(layer):\n+\n+            self.assertEqual(trainable_weights, len(layer.trainable_weights))\n+            self.assertEqual(\n+                trainable_params,\n+                _count_params(layer.trainable_weights),\n+            )\n+            self.assertEqual(\n+                non_trainable_weights, len(layer.non_trainable_weights)\n+            )\n+            self.assertEqual(\n+                non_trainable_params,\n+                _count_params(layer.non_trainable_weights),\n+            )\n+\n+        # functional model\n+        layer1 = layer_class(**layer_init_kwargs)\n+        inputs1 = layers.Input(shape=input_shape)\n+        outputs1 = layer1(inputs1)\n+        model1 = models.Model(\n+            inputs=inputs1, outputs=outputs1, name=model_name + \"1\"\n+        )\n+        model1.summary()\n+\n+        verify_weights_and_params(layer1)\n+\n+        model1.compile(\n+            loss=\"categorical_crossentropy\",\n+            optimizer=\"adam\",\n+            metrics=[metrics.CategoricalAccuracy()],\n+        )\n+\n+        tw1_before_fit = tree.map_structure(\n+            backend.convert_to_numpy, layer1.trainable_weights\n+        )\n+        ntw1_before_fit = tree.map_structure(\n+            backend.convert_to_numpy, layer1.non_trainable_weights\n+        )\n+        model1.fit(x_train, y_train, epochs=1, steps_per_epoch=10)\n+        tw1_after_fit = tree.map_structure(\n+            backend.convert_to_numpy, layer1.trainable_weights\n+        )\n+        ntw1_after_fit = tree.map_structure(\n+            backend.convert_to_numpy, layer1.non_trainable_weights\n+        )\n+\n+        # verify both trainable and non-trainable weights did change after fit\n+        for before, after in zip(tw1_before_fit, tw1_after_fit):\n+            self.assertNotAllClose(before, after)\n+        for before, after in zip(ntw1_before_fit, ntw1_after_fit):\n+            self.assertNotAllClose(before, after)\n+\n+        expected_ouput_shape = (x_test.shape[0], num_classes)\n+        output1 = model1(x_test)\n+        self.assertEqual(output1.shape, expected_ouput_shape)\n+        predict1 = model1.predict(x_test, steps=1)\n+        self.assertEqual(predict1.shape, expected_ouput_shape)\n+\n+        # verify both trainable and non-trainable weights did not change\n+        tw1_after_call = tree.map_structure(\n+            backend.convert_to_numpy, layer1.trainable_weights\n+        )\n+        ntw1_after_call = tree.map_structure(\n+            backend.convert_to_numpy, layer1.non_trainable_weights\n+        )\n+        for after_fit, after_call in zip(tw1_after_fit, tw1_after_call):\n+            self.assertAllClose(after_fit, after_call)\n+        for after_fit, after_call in zip(ntw1_after_fit, ntw1_after_call):\n+            self.assertAllClose(after_fit, after_call)\n+\n+        exported_params = jax.tree_util.tree_map(\n+            backend.convert_to_numpy, layer1.params\n+        )\n+        if layer1.state is not None:\n+            exported_state = jax.tree_util.tree_map(\n+                backend.convert_to_numpy, layer1.state\n+            )\n+        else:\n+            exported_state = None\n+\n+        def verify_identical_model(model):\n+            output = model(x_test)\n+            self.assertAllClose(output1, output)\n+\n+            predict = model.predict(x_test, steps=1)\n+            self.assertAllClose(predict1, predict)\n+\n+        # sequential model to compare results\n+        layer2 = layer_class(\n+            params=exported_params,\n+            state=exported_state,\n+            input_shape=input_shape,\n+            **layer_init_kwargs,\n+        )\n+        model2 = models.Sequential([layer2], name=model_name + \"2\")\n+        model2.summary()\n+        verify_weights_and_params(layer2)\n+        model2.compile(\n+            loss=\"categorical_crossentropy\",\n+            optimizer=\"adam\",\n+            metrics=[metrics.CategoricalAccuracy()],\n+        )\n+        verify_identical_model(model2)\n+\n+        # save, load back and compare results\n+        path = os.path.join(self.get_temp_dir(), \"jax_layer_model.keras\")\n+        model2.save(path)\n+\n+        model3 = saving.load_model(path)\n+        layer3 = model3.layers[0]\n+        model3.summary()\n+        verify_weights_and_params(layer3)\n+        verify_identical_model(model3)\n+\n+        # export, load back and compare results\n+        path = os.path.join(self.get_temp_dir(), \"jax_layer_export\")\n+        # export_archive = export_lib.ExportArchive()\n+        # export_archive.track(model2)\n+        # export_archive.add_endpoint(\n+        #     \"call\",\n+        #     model2.call,\n+        #     input_signature=[\n+        #         tf.TensorSpec(\n+        #             shape=(None,) + input_shape,\n+        #             dtype=tf.float32,\n+        #         )\n+        #     ],\n+        # )\n+        # export_archive.write_out(\n+        #     path,\n+        #     tf.saved_model.SaveOptions(experimental_custom_gradients=False),\n+        # )\n+        export_lib.export_model(model2, path)\n+        model4 = tf.saved_model.load(path)\n+        output4 = model4.serve(x_test)\n+        self.assertAllClose(output1, output4)\n+\n+    @parameterized.named_parameters(\n+        {\n+            \"testcase_name\": \"training_independent\",\n+            \"init_kwargs\": {\n+                \"call_fn\": jax_stateless_apply,\n+                \"init_fn\": jax_stateless_init,\n+            },\n+            \"trainable_weights\": 6,\n+            \"trainable_params\": 266610,\n+            \"non_trainable_weights\": 0,\n+            \"non_trainable_params\": 0,\n+        },\n+        {\n+            \"testcase_name\": \"training_state\",\n+            \"init_kwargs\": {\n+                \"call_fn\": jax_stateful_apply,\n+                \"init_fn\": jax_stateful_init,\n+            },\n+            \"trainable_weights\": 6,\n+            \"trainable_params\": 266610,\n+            \"non_trainable_weights\": 1,\n+            \"non_trainable_params\": 1,\n+        },\n+    )\n+    def test_jax_layer(\n+        self,\n+        init_kwargs,\n+        trainable_weights,\n+        trainable_params,\n+        non_trainable_weights,\n+        non_trainable_params,\n+    ):\n+        self._test_layer(\n+            init_kwargs[\"call_fn\"].__name__,\n+            JaxLayer,\n+            init_kwargs,\n+            trainable_weights,\n+            trainable_params,\n+            non_trainable_weights,\n+            non_trainable_params,\n+        )\n+\n+    @parameterized.named_parameters(\n+        {\n+            \"testcase_name\": \"training_independent_bound_method\",\n+            \"flax_model_class\": FlaxTrainingIndependentModel,\n+            \"flax_model_method\": \"forward\",\n+            \"init_kwargs\": {},\n+            \"trainable_weights\": 8,\n+            \"trainable_params\": 648226,\n+            \"non_trainable_weights\": 0,\n+            \"non_trainable_params\": 0,\n+        },\n+        {\n+            \"testcase_name\": \"training_rng_unbound_method\",\n+            \"flax_model_class\": FlaxDropoutModel,\n+            \"flax_model_method\": None,\n+            \"init_kwargs\": {\n+                \"method\": flax_dropout_wrapper,\n+            },\n+            \"trainable_weights\": 8,\n+            \"trainable_params\": 648226,\n+            \"non_trainable_weights\": 0,\n+            \"non_trainable_params\": 0,\n+        },\n+        {\n+            \"testcase_name\": \"training_rng_state_no_method\",\n+            \"flax_model_class\": FlaxBatchNormModel,\n+            \"flax_model_method\": None,\n+            \"init_kwargs\": {},\n+            \"trainable_weights\": 13,\n+            \"trainable_params\": 354258,\n+            \"non_trainable_weights\": 8,\n+            \"non_trainable_params\": 536,\n+        },\n+    )\n+    def test_flax_layer(\n+        self,\n+        flax_model_class,\n+        flax_model_method,\n+        init_kwargs,\n+        trainable_weights,\n+        trainable_params,\n+        non_trainable_weights,\n+        non_trainable_params,\n+    ):\n+        def create_wrapper(**kwargs):\n+            params = kwargs.pop(\"params\") if \"params\" in kwargs else None\n+            state = kwargs.pop(\"state\") if \"state\" in kwargs else None\n+            if params and state:\n+                variables = {**params, **state}\n+            elif params:\n+                variables = params\n+            elif state:\n+                variables = state\n+            else:\n+                variables = None\n+            kwargs[\"variables\"] = variables\n+            flax_model = flax_model_class()\n+            if flax_model_method:\n+                kwargs[\"method\"] = getattr(flax_model, flax_model_method)\n+            return FlaxLayer(flax_model_class(), **kwargs)\n+\n+        self._test_layer(\n+            flax_model_class.__name__,\n+            create_wrapper,\n+            init_kwargs,\n+            trainable_weights,\n+            trainable_params,\n+            non_trainable_weights,\n+            non_trainable_params,\n+        )\n+\n+    def test_with_no_init_fn_and_no_params(self):\n+        def jax_fn(params, inputs):\n+            return inputs\n+\n+        with self.assertRaises(ValueError):\n+            JaxLayer(jax_fn)\n+\n+    def test_with_training_in_call_fn_but_not_init_fn(self):\n+        def jax_call_fn(params, state, rng, inputs, training):\n+            return inputs, {}\n+\n+        def jax_init_fn(rng, inputs):\n+            return {}, {}\n+\n+        layer = JaxLayer(jax_call_fn, jax_init_fn)\n+        layer(np.ones((1,)))\n+\n+    def test_with_different_argument_order(self):\n+        def jax_call_fn(training, inputs, rng, state, params):\n+            return inputs, {}\n+\n+        def jax_init_fn(training, inputs, rng):\n+            return {}, {}\n+\n+        layer = JaxLayer(jax_call_fn, jax_init_fn)\n+        layer(np.ones((1,)))\n+\n+    def test_with_minimal_arguments(self):\n+        def jax_call_fn(inputs):\n+            return inputs\n+\n+        def jax_init_fn(inputs):\n+            return {}\n+\n+        layer = JaxLayer(jax_call_fn, jax_init_fn)\n+        layer(np.ones((1,)))\n+\n+    def test_with_missing_inputs_in_call_fn(self):\n+        def jax_call_fn(params, rng, training):\n+            return jnp.ones((1,))\n+\n+        def jax_init_fn(rng, inputs):\n+            return {}\n+\n+        with self.assertRaisesRegex(ValueError, \"`call_fn`.*`inputs`\"):\n+            JaxLayer(jax_call_fn, jax_init_fn)\n+\n+    def test_with_missing_inputs_in_init_fn(self):\n+        def jax_call_fn(params, rng, inputs, training):\n+            return jnp.ones((1,))\n+\n+        def jax_init_fn(rng, training):\n+            return {}\n+\n+        with self.assertRaisesRegex(ValueError, \"`init_fn`.*`inputs`\"):\n+            JaxLayer(jax_call_fn, jax_init_fn)\n+\n+    def test_with_unsupported_argument_in_call_fn(self):\n+        def jax_call_fn(params, rng, inputs, mode):\n+            return jnp.ones((1,))\n+\n+        def jax_init_fn(rng, inputs):\n+            return {}\n+\n+        with self.assertRaisesRegex(ValueError, \"`call_fn`.*`mode`\"):\n+            JaxLayer(jax_call_fn, jax_init_fn)\n+\n+    def test_with_unsupported_argument_in_init_fn(self):\n+        def jax_call_fn(params, rng, inputs, training):\n+            return inputs\n+\n+        def jax_init_fn(rng, inputs, mode):\n+            return {}\n+\n+        with self.assertRaisesRegex(ValueError, \"`init_fn`.*`mode`\"):\n+            JaxLayer(jax_call_fn, jax_init_fn)\n+\n+    def test_with_structures_as_inputs_and_outputs(self):\n+        def jax_fn(params, inputs):\n+            a = inputs[\"a\"]\n+            b = inputs[\"b\"]\n+            output1 = jnp.concatenate([a, b], axis=1)\n+            output2 = jnp.concatenate([b, a], axis=1)\n+            return output1, output2\n+\n+        layer = JaxLayer(jax_fn, params={})\n+        inputs = {\n+            \"a\": layers.Input((None, 3)),\n+            \"b\": layers.Input((None, 3)),\n+        }\n+        outputs = layer(inputs)\n+        model = models.Model(inputs, outputs)\n+\n+        test_inputs = {\n+            \"a\": np.ones((2, 6, 3)),\n+            \"b\": np.ones((2, 7, 3)),\n+        }\n+        test_outputs = model(test_inputs)\n+        self.assertAllClose(test_outputs[0], np.ones((2, 13, 3)))\n+        self.assertAllClose(test_outputs[1], np.ones((2, 13, 3)))\n+\n+    def test_with_polymorphic_shape_more_than_26_dimension_names(self):\n+        def jax_fn(params, inputs):\n+            return jnp.concatenate(inputs, axis=1)\n+\n+        layer = JaxLayer(jax_fn, params=())\n+        inputs = [layers.Input((None, 3)) for _ in range(60)]\n+        output = layer(inputs)\n+        model = models.Model(inputs, output)\n+\n+        test_inputs = [np.ones((2, 1, 3))] * 60\n+        test_output = model(test_inputs)\n+        self.assertAllClose(test_output, np.ones((2, 60, 3)))\n+\n+    def test_with_flax_state_no_params(self):\n+        class MyFlaxLayer(flax.linen.Module):\n+            @flax.linen.compact\n+            def __call__(self, x):\n+                def zeros_init(shape):\n+                    return jnp.zeros(shape, jnp.int32)\n+\n+                count = self.variable(\"a\", \"b\", zeros_init, [])\n+                count.value = count.value + 1\n+                return x\n+\n+        layer = FlaxLayer(MyFlaxLayer(), variables={\"a\": {\"b\": 0}})\n+        layer(np.ones((1,)))\n+        self.assertLen(layer.params, 0)\n+        self.assertEqual(layer.state[\"a\"][\"b\"].value, 1)\n+\n+    def test_with_state_none_leaves(self):\n+        def jax_fn(params, state, inputs):\n+            return inputs, state\n+\n+        layer = JaxLayer(jax_fn, state={\"foo\": None})\n+        self.assertIsNone(layer.state[\"foo\"])\n+        layer(np.ones((1,)))\n+\n+    def test_with_state_non_tensor_leaves(self):\n+        def jax_fn(params, state, inputs):\n+            return inputs, state\n+\n+        layer = JaxLayer(jax_fn, state={\"foo\": \"bar\"})\n+        self.assertEqual(layer.state[\"foo\"], \"bar\")\n+        # layer cannot be invoked as jax2tf will fail on strings\n+\n+    def test_with_state_jax_registered_node_class(self):\n+        @jax.tree_util.register_pytree_node_class\n+        class NamedPoint:\n+            def __init__(self, x, y, name):\n+                self.x = x\n+                self.y = y\n+                self.name = name\n+\n+            def tree_flatten(self):\n+                return ((self.x, self.y), self.name)\n+\n+            @classmethod\n+            def tree_unflatten(cls, aux_data, children):\n+                return cls(*children, aux_data)\n+\n+        def jax_fn(params, state, inputs):\n+            return inputs, state\n+\n+        layer = JaxLayer(jax_fn, state=[NamedPoint(1.0, 2.0, \"foo\")])\n+        layer(np.ones((1,)))\n+\n+    @parameterized.named_parameters(\n+        {\n+            \"testcase_name\": \"sequence_instead_of_mapping\",\n+            \"init_state\": [0.0],\n+            \"error_regex\": \"Expected dict, got \",\n+        },\n+        {\n+            \"testcase_name\": \"mapping_instead_of_sequence\",\n+            \"init_state\": {\"state\": {\"foo\": 0.0}},\n+            \"error_regex\": \"Expected list, got \",\n+        },\n+        {\n+            \"testcase_name\": \"sequence_instead_of_variable\",\n+            \"init_state\": {\"state\": [[0.0]]},\n+            \"error_regex\": \"Structure mismatch\",\n+        },\n+        {\n+            \"testcase_name\": \"no_initial_state\",\n+            \"init_state\": None,\n+            \"error_regex\": \"Expected dict, got None\",\n+        },\n+        {\n+            \"testcase_name\": \"missing_dict_key\",\n+            \"init_state\": {\"state\": {}},\n+            \"error_regex\": \"Expected list, got \",\n+        },\n+        {\n+            \"testcase_name\": \"missing_variable_in_list\",\n+            \"init_state\": {\"state\": {\"foo\": [2.0]}},\n+            \"error_regex\": \"Expected list, got \",\n+        },\n+    )\n+    def test_state_mismatch_during_update(self, init_state, error_regex):\n+        def jax_fn(params, state, inputs):\n+            return inputs, {\"state\": [jnp.ones([])]}\n+\n+        layer = JaxLayer(jax_fn, params={}, state=init_state)\n+        with self.assertRaisesRegex(ValueError, error_regex):\n+            layer(np.ones((1,)))\n+\n+    def test_rng_seeding(self):\n+        def jax_init(rng, inputs):\n+            return [jax.nn.initializers.normal(1.0)(rng, inputs.shape)]\n+\n+        def jax_apply(params, inputs):\n+            return jnp.dot(inputs, params[0])\n+\n+        shape = (2, 2)\n+\n+        utils.set_random_seed(0)\n+        layer1 = JaxLayer(jax_apply, jax_init)\n+        layer1.build(shape)\n+        utils.set_random_seed(0)\n+        layer2 = JaxLayer(jax_apply, jax_init)\n+        layer2.build(shape)\n+        self.assertAllClose(layer1.params[0], layer2.params[0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
