{"custom_id": "keras#bc267ba87982e07ff7b1d95b76339c6844e9eac7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2069 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1171,7 +1171,7 @@ class NestedTrackingTest(tf.test.TestCase):\n     self.assertEqual(len(layer.non_trainable_weights), 8)\n     self.assertEqual(\n         {id(v) for v in [layer.dense1, layer.dense2, layer.v1, layer.v2]},\n-        {id(v) for _, v in layer._checkpoint_dependencies})\n+        {id(v) for v in layer._trackable_children().values()})\n \n   def test_nested_layer_updates_losses_tracking(self):\n     # Test that updates and losses from nested sublayers are\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc7efd5f1285725690769adb26413f0ae54035b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1011 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -703,9 +703,8 @@ class TemplateTests(keras_parameterized.TestCase):\n           set([id(v1_save), id(v2_save), id(manual_scope),\n                id(manual_scope_v), id(save_template)]),\n           set(map(id, trackable_utils.list_objects(save_template))))\n-      manual_dep, = manual_scope._checkpoint_dependencies\n-      self.assertEqual(\"in_manual_scope\", manual_dep.name)\n-      self.assertIs(manual_scope_v, manual_dep.ref)\n+      self.assertDictEqual({\"in_manual_scope\": manual_scope_v},\n+                           manual_scope._trackable_children())\n       optimizer = adam.Adam(0.0)\n       save_root = tf.train.Checkpoint(\n           my_template=save_template, optimizer=optimizer)\n@@ -728,11 +727,9 @@ class TemplateTests(keras_parameterized.TestCase):\n       status = load_root.restore(save_path)\n       var, var_plus_one, var2, _, _ = load_template()\n       load_optimizer.minimize(var.read_value, var_list=[var])\n-      self.assertLen(load_template._checkpoint_dependencies, 3)\n-      self.assertEqual(\"v\", load_template._checkpoint_dependencies[0].name)\n-      self.assertEqual(\"v2\", load_template._checkpoint_dependencies[1].name)\n-      self.assertEqual(\"ManualScope\",\n-                       load_template._checkpoint_dependencies[2].name)\n+\n+      children = load_template._trackable_children()\n+      self.assertEqual({\"v\", \"v2\", \"ManualScope\"}, children.keys())\n       status.assert_consumed().run_restore_ops()\n       self.assertAllEqual([12.], self.evaluate(var))\n       self.assertAllEqual([13.], self.evaluate(var_plus_one))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9760e4441036002dd4f176cb41f6d207a054a5aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 33 | Churn Cumulative: 723 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -75,13 +75,13 @@ class ListTests(keras_parameterized.TestCase):\n           len(model.layer_list.layers + model.layers_with_updates))\n       for index in range(10):\n         self.assertEqual(3 + index, model.layer_list.layers[index].units)\n-      self.assertEqual(2, len(model._checkpoint_dependencies))\n-      self.assertIs(model.layer_list, model._checkpoint_dependencies[0].ref)\n+      children = model._trackable_children()\n+      self.assertLen(children, 2)\n+      self.assertIs(model.layer_list, children[\"layer_list\"])\n       self.assertIs(model.layers_with_updates,\n-                    model._checkpoint_dependencies[1].ref)\n-      self.assertEqual(\n-          10,\n-          len(model._checkpoint_dependencies[0].ref._checkpoint_dependencies))\n+                    children[\"layers_with_updates\"])\n+      self.assertLen(\n+          children[\"layer_list\"]._trackable_children(), 10)\n       self.evaluate([v.initializer for v in model.variables])\n       self.evaluate(model.variables[0].assign([[1., 2., 3.], [4., 5., 6.]]))\n       save_path = os.path.join(self.get_temp_dir(), \"ckpt\")\n@@ -258,8 +258,8 @@ class MappingTests(keras_parameterized.TestCase):\n       self.assertAllEqual([32, 7], output.shape.as_list())\n       self.assertEqual(5, len(model.layers))\n       self.assertEqual(len(model.layers), len(model.layer_dict.layers))\n-      self.assertEqual(1, len(model._checkpoint_dependencies))\n-      self.assertIs(model.layer_dict, model._checkpoint_dependencies[0].ref)\n+      self.assertLen(model._trackable_children(), 1)\n+      self.assertIs(model.layer_dict, model._trackable_children()[\"layer_dict\"])\n       self.evaluate([v.initializer for v in model.variables])\n       test_var = model.layer_dict[\"output\"].kernel\n       self.evaluate(test_var.assign(tf.ones([6, 7])))\n@@ -414,12 +414,11 @@ class TupleTests(keras_parameterized.TestCase):\n       self.assertEqual(3, model.layer_list.layers[0].units)\n       self.assertEqual(4, model.layer_list.layers[1].units)\n       self.assertEqual(5, model.layer_list.layers[2].units)\n-      self.assertLen(model._checkpoint_dependencies, 2)\n-      self.assertIs(model.layer_list, model._checkpoint_dependencies[0].ref)\n+      self.assertLen(model._trackable_children(), 2)\n+      self.assertIs(model.layer_list, model._trackable_children()[\"layer_list\"])\n       self.assertIs(model.layers_with_updates,\n-                    model._checkpoint_dependencies[1].ref)\n-      self.assertLen(\n-          model._checkpoint_dependencies[0].ref._checkpoint_dependencies, 3)\n+                    model._trackable_children()[\"layers_with_updates\"])\n+      self.assertLen(model.layer_list._trackable_children(), 3)\n       self.evaluate([v.initializer for v in model.variables])\n       self.evaluate(model.variables[0].assign([[1., 2., 3.], [4., 5., 6.]]))\n       save_path = os.path.join(self.get_temp_dir(), \"ckpt\")\n@@ -536,8 +535,8 @@ class InterfaceTests(keras_parameterized.TestCase):\n     root.hasdep = hasdep\n     nodep = tf.Module()\n     root.nodep = data_structures.NoDependency(nodep)\n-    self.assertEqual(1, len(root._checkpoint_dependencies))\n-    self.assertIs(root._checkpoint_dependencies[0].ref, root.hasdep)\n+    self.assertLen(root._trackable_children(), 1)\n+    self.assertIs(root._trackable_children()[\"hasdep\"], root.hasdep)\n     self.assertIs(root.hasdep, hasdep)\n     self.assertIs(root.nodep, nodep)\n \n@@ -564,9 +563,7 @@ class InterfaceTests(keras_parameterized.TestCase):\n     self.assertIn(b, a_deps)\n     self.assertIn(c, a_deps)\n     self.assertIs(b, a.attribute[\"b\"])\n-    self.assertEqual(\n-        len([\"b\", \"c\"]),\n-        len([dep.name for dep in a.attribute._checkpoint_dependencies]))\n+    self.assertEqual({\"b\", \"c\"}, a.attribute._trackable_children().keys())\n     self.assertEqual([b, c], a.layers)\n     self.assertEqual([b, c], a.attribute.layers)\n     self.assertEqual([c], a.attribute[\"c\"].layers)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1b0a4cddd07cb284d1817307d8723d1c7ca23ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 4 | Churn Cumulative: 387 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -70,7 +70,7 @@ class ShardedVariableTest(tf.test.TestCase):\n                                layer.trainable_variables)\n     self.assert_list_all_equal(layer.weights, layer.variables)\n \n-    checkpoint_deps = set(dep.ref for dep in layer._checkpoint_dependencies)\n+    checkpoint_deps = set(layer._trackable_children().values())\n     self.assertEqual(checkpoint_deps, set([layer.w, layer.b]))\n \n   def test_keras_layer_add_weight(self):\n@@ -103,7 +103,7 @@ class ShardedVariableTest(tf.test.TestCase):\n                                layer.trainable_variables)\n     self.assert_list_all_equal(layer.weights, layer.variables)\n \n-    checkpoint_deps = set(dep.ref for dep in layer._checkpoint_dependencies)\n+    checkpoint_deps = set(layer._trackable_children().values())\n     self.assertEqual(checkpoint_deps, set([layer.w, layer.b]))\n \n   def test_keras_metrics(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35504ff7a13e745d60f0c9ef6d6c97542df40e03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 39 | Churn Cumulative: 1425 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -113,6 +113,32 @@ class _BaseOptimizer(tf.Module):\n     \"\"\"\n     raise NotImplementedError\n \n+  @tf.function(jit_compile=True)\n+  def _update_step_xla(self, gradient, variable, key):\n+    \"\"\"A wrapper of `update_step` to enable XLA acceleration.\n+\n+    Due to `tf.function` tracing mechanism, for (gradient, variable) pairs of\n+    the same shape and dtype, the execution graph always invoke the first\n+    pair it has seen. Thus, we need a `key` argument to make each\n+    (gradient, variable) pair unique. In additions, XLA cannot understand\n+    string input, so the key is an integer.\n+\n+    Args:\n+      gradient: backpropagated gradient of the given variable.\n+      variable: variable whose value needs to be updated.\n+      key (int): a unique key that identifies the variable.\n+\n+    Returns:\n+      An `Operation` that applies the specified gradients.\n+    \"\"\"\n+    return self.update_step(gradient, variable)\n+\n+  def _update_step(self, gradient, variable):\n+    if self._jit_compile:\n+      self._update_step_xla(gradient, variable, id(self._var_key(variable)))\n+    else:\n+      self.update_step(gradient, variable)\n+\n   def compute_gradients(self, loss, var_list, tape=None):\n     \"\"\"Compute gradients of loss on trainable variables.\n \n@@ -391,11 +417,9 @@ class _BaseOptimizer(tf.Module):\n     Args:\n       grads_and_vars: List of (gradient, variable) pairs.\n     \"\"\"\n-    update_step = self.update_step\n-    if self._jit_compile:\n-      update_step = tf.function(update_step, jit_compile=True)\n     for grad, var in grads_and_vars:\n-      update_step(grad, var)\n+      self._update_step(grad, var)\n+\n     self.iterations.assign_add(1)\n \n   def _update_model_variables_moving_average(self, var_list):\n@@ -515,7 +539,7 @@ class Optimizer(_BaseOptimizer):\n         `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n         current_variable_value`.\n     ema_overwrite_frequency: int or None, default to None. Only used if\n-      `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+      `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations, we\n       overwrite the model variable by its moving average. If None, the optimizer\n        does not overwrite model variables in the middle of training, and you\n       need to explicitly overwrite the variables at the end of training\n@@ -798,10 +822,7 @@ class Optimizer(_BaseOptimizer):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n \n     def apply_grad_to_update_var(var, grad):\n-      update_step = self.update_step\n-      if self._jit_compile:\n-        update_step = tf.function(update_step, jit_compile=True)\n-      return update_step(grad, var)\n+      return self._update_step(grad, var)\n \n     for grad, var in grads_and_vars:\n       distribution.extended.update(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#02a79050aee3e3385cda5b6a900df8bf49371606", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 935 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -214,7 +214,8 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     attention_layer = dense_attention.Attention(score_mode='concat')\n     attention_layer.concat_score_weight = 1\n     attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))\n-    actual = attention_layer._calculate_scores(query=q, key=k)\n+    actual = keras.backend.get_value(\n+            attention_layer._calculate_scores(query=q, key=k))\n \n     # pylint:disable=line-too-long\n     # expected000 = tanh(1.+1.5) + tanh(1.1+1.6) + tanh(1.2+1.7) + tanh(1.3+1.8) = 3.96753427840\n@@ -269,7 +270,8 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     attention_layer.concat_score_weight = 1\n     attention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))\n     attention_layer.scale = 2.\n-    actual = attention_layer._calculate_scores(query=q, key=k)\n+    actual = keras.backend.get_value(\n+            attention_layer._calculate_scores(query=q, key=k))\n \n     # Expected tensor of shape [1, 1, 1].\n     # expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#326cf80085a9d7d980b968ea1ca235490e32833b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 937 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -341,7 +341,7 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n         dtype=np.float32)\n     # Value mask tensor of shape [1, 3]\n     v_mask = np.array([[True, True, False]], dtype=np.bool_)\n-    attention_layer = dense_attention.Attention(score='concat')\n+    attention_layer = dense_attention.Attention(score_mode='concat')\n     attention_layer.concat_score_weight = 1\n     actual = attention_layer([q, v, k], mask=[None, v_mask])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#88f10c04703c360c696095a091a81a125c10b54d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5782 | Lines Deleted: 5100 | Files Changed: 35 | Hunks: 50 | Methods Changed: 219 | Complexity Δ (Sum/Max): 21/49 | Churn Δ: 10882 | Churn Cumulative: 32833 | Contributors (this commit): 92 | Commits (past 90d): 47 | Contributors (cumulative): 151 | DMM Complexity: 0.0\n\nDIFF:\n@@ -58,36 +58,37 @@ from keras.layers.advanced_activations import ThresholdedReLU\n from keras.layers.advanced_activations import Softmax\n \n # Convolution layers.\n-from keras.layers.convolutional import Conv1D\n-from keras.layers.convolutional import Conv2D\n-from keras.layers.convolutional import Conv3D\n-from keras.layers.convolutional import Conv1DTranspose\n-from keras.layers.convolutional import Conv2DTranspose\n-from keras.layers.convolutional import Conv3DTranspose\n-from keras.layers.convolutional import SeparableConv1D\n-from keras.layers.convolutional import SeparableConv2D\n+from keras.layers.convolutional.conv1d import Conv1D\n+from keras.layers.convolutional.conv2d import Conv2D\n+from keras.layers.convolutional.conv3d import Conv3D\n+from keras.layers.convolutional.conv1d_transpose import Conv1DTranspose\n+from keras.layers.convolutional.conv2d_transpose import Conv2DTranspose\n+from keras.layers.convolutional.conv3d_transpose import Conv3DTranspose\n+from keras.layers.convolutional.depthwise_conv1d import DepthwiseConv1D\n+from keras.layers.convolutional.depthwise_conv2d import DepthwiseConv2D\n+from keras.layers.convolutional.separable_conv1d import SeparableConv1D\n+from keras.layers.convolutional.separable_conv2d import SeparableConv2D\n \n # Convolution layer aliases.\n-from keras.layers.convolutional import Convolution1D\n-from keras.layers.convolutional import Convolution2D\n-from keras.layers.convolutional import Convolution3D\n-from keras.layers.convolutional import Convolution2DTranspose\n-from keras.layers.convolutional import Convolution3DTranspose\n-from keras.layers.convolutional import SeparableConvolution1D\n-from keras.layers.convolutional import SeparableConvolution2D\n-from keras.layers.convolutional import DepthwiseConv1D\n-from keras.layers.convolutional import DepthwiseConv2D\n+from keras.layers.convolutional.conv1d import Convolution1D\n+from keras.layers.convolutional.conv2d import Convolution2D\n+from keras.layers.convolutional.conv3d import Convolution3D\n+from keras.layers.convolutional.conv1d_transpose import Convolution1DTranspose\n+from keras.layers.convolutional.conv2d_transpose import Convolution2DTranspose\n+from keras.layers.convolutional.conv3d_transpose import Convolution3DTranspose\n+from keras.layers.convolutional.separable_conv1d import SeparableConvolution1D\n+from keras.layers.convolutional.separable_conv2d import SeparableConvolution2D\n \n-# Image processing layers.\n-from keras.layers.convolutional import UpSampling1D\n-from keras.layers.convolutional import UpSampling2D\n-from keras.layers.convolutional import UpSampling3D\n-from keras.layers.convolutional import ZeroPadding1D\n-from keras.layers.convolutional import ZeroPadding2D\n-from keras.layers.convolutional import ZeroPadding3D\n-from keras.layers.convolutional import Cropping1D\n-from keras.layers.convolutional import Cropping2D\n-from keras.layers.convolutional import Cropping3D\n+# Reshaping layers.\n+from keras.layers.reshaping.cropping1d import Cropping1D\n+from keras.layers.reshaping.cropping2d import Cropping2D\n+from keras.layers.reshaping.cropping3d import Cropping3D\n+from keras.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras.layers.reshaping.up_sampling2d import UpSampling2D\n+from keras.layers.reshaping.up_sampling3d import UpSampling3D\n+from keras.layers.reshaping.zero_padding1d import ZeroPadding1D\n+from keras.layers.reshaping.zero_padding2d import ZeroPadding2D\n+from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n \n # Core layers.\n from keras.layers.core.activation import Activation\n\n@@ -1,3778 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Keras convolution layers and image transformation layers.\"\"\"\n-# pylint: disable=g-bad-import-order\n-import tensorflow.compat.v2 as tf\n-\n-from keras import activations\n-from keras import backend\n-from keras import constraints\n-from keras import initializers\n-from keras import regularizers\n-from keras.engine.base_layer import Layer\n-from keras.engine.input_spec import InputSpec\n-# imports for backwards namespace compatibility\n-# pylint: disable=unused-import\n-from keras.layers.pooling import AveragePooling1D\n-from keras.layers.pooling import AveragePooling2D\n-from keras.layers.pooling import AveragePooling3D\n-from keras.layers.pooling import MaxPooling1D\n-from keras.layers.pooling import MaxPooling2D\n-from keras.layers.pooling import MaxPooling3D\n-# pylint: enable=unused-import\n-from keras.utils import conv_utils\n-from keras.utils import tf_utils\n-from tensorflow.python.util.tf_export import keras_export\n-# pylint: disable=g-classes-have-attributes\n-\n-\n-class Conv(Layer):\n-  \"\"\"Abstract N-D convolution layer (private, used as implementation base).\n-\n-  This layer creates a convolution kernel that is convolved\n-  (actually cross-correlated) with the layer input to produce a tensor of\n-  outputs. If `use_bias` is True (and a `bias_initializer` is provided),\n-  a bias vector is created and added to the outputs. Finally, if\n-  `activation` is not `None`, it is applied to the outputs as well.\n-\n-  Note: layer attributes cannot be modified after the layer has been called\n-  once (except the `trainable` attribute).\n-\n-  Args:\n-    rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n-    filters: Integer, the dimensionality of the output space (i.e. the number\n-      of filters in the convolution). Could be \"None\", eg in the case of\n-      depth wise convolution.\n-    kernel_size: An integer or tuple/list of n integers, specifying the\n-      length of the convolution window.\n-    strides: An integer or tuple/list of n integers,\n-      specifying the stride length of the convolution.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"`,  `\"same\"`, or `\"causal\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n-      evenly to the left/right or up/down of the input such that output has the\n-      same height/width dimension as the input. `\"causal\"` results in causal\n-      (dilated) convolutions, e.g. `output[t]` does not depend on `input[t+1:]`.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, ..., channels)` while `channels_first` corresponds to\n-      inputs with shape `(batch_size, channels, ...)`.\n-    dilation_rate: An integer or tuple/list of n integers, specifying\n-      the dilation rate to use for dilated convolution.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any `strides` value != 1.\n-    groups: A positive integer specifying the number of groups in which the\n-      input is split along the channel axis. Each group is convolved\n-      separately with `filters / groups` filters. The output is the\n-      concatenation of all the `groups` results along the channel axis.\n-      Input channels and `filters` must both be divisible by `groups`.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied.\n-    use_bias: Boolean, whether the layer uses a bias.\n-    kernel_initializer: An initializer for the convolution kernel. If None, the\n-      default initializer (glorot_uniform) will be used.\n-    bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer (zeros) will be used.\n-    kernel_regularizer: Optional regularizer for the convolution kernel.\n-    bias_regularizer: Optional regularizer for the bias vector.\n-    activity_regularizer: Optional regularizer function for the output.\n-    kernel_constraint: Optional projection function to be applied to the\n-        kernel after being updated by an `Optimizer` (e.g. used to implement\n-        norm constraints or value constraints for layer weights). The function\n-        must take as input the unprojected variable and must return the\n-        projected variable (which must have the same shape). Constraints are\n-        not safe to use when doing asynchronous distributed training.\n-    bias_constraint: Optional projection function to be applied to the\n-        bias after being updated by an `Optimizer`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               rank,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=1,\n-               groups=1,\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               trainable=True,\n-               name=None,\n-               conv_op=None,\n-               **kwargs):\n-    super(Conv, self).__init__(\n-        trainable=trainable,\n-        name=name,\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        **kwargs)\n-    self.rank = rank\n-\n-    if isinstance(filters, float):\n-      filters = int(filters)\n-    if filters is not None and filters < 0:\n-      raise ValueError(f'Received a negative value for `filters`.'\n-                       f'Was expecting a positive value. Received {filters}.')\n-    self.filters = filters\n-    self.groups = groups or 1\n-    self.kernel_size = conv_utils.normalize_tuple(\n-        kernel_size, rank, 'kernel_size')\n-    self.strides = conv_utils.normalize_tuple(\n-        strides, rank, 'strides', allow_zero=True)\n-    self.padding = conv_utils.normalize_padding(padding)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.dilation_rate = conv_utils.normalize_tuple(\n-        dilation_rate, rank, 'dilation_rate')\n-\n-    self.activation = activations.get(activation)\n-    self.use_bias = use_bias\n-\n-    self.kernel_initializer = initializers.get(kernel_initializer)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n-    self.bias_regularizer = regularizers.get(bias_regularizer)\n-    self.kernel_constraint = constraints.get(kernel_constraint)\n-    self.bias_constraint = constraints.get(bias_constraint)\n-    self.input_spec = InputSpec(min_ndim=self.rank + 2)\n-\n-    self._validate_init()\n-    self._is_causal = self.padding == 'causal'\n-    self._channels_first = self.data_format == 'channels_first'\n-    self._tf_data_format = conv_utils.convert_data_format(\n-        self.data_format, self.rank + 2)\n-\n-  def _validate_init(self):\n-    if self.filters is not None and self.filters % self.groups != 0:\n-      raise ValueError(\n-          'The number of filters must be evenly divisible by the number of '\n-          'groups. Received: groups={}, filters={}'.format(\n-              self.groups, self.filters))\n-\n-    if not all(self.kernel_size):\n-      raise ValueError('The argument `kernel_size` cannot contain 0(s). '\n-                       'Received: %s' % (self.kernel_size,))\n-\n-    if not all(self.strides):\n-      raise ValueError('The argument `strides` cannot contains 0(s). '\n-                       'Received: %s' % (self.strides,))\n-\n-    if (self.padding == 'causal' and not isinstance(self,\n-                                                    (Conv1D, SeparableConv1D))):\n-      raise ValueError('Causal padding is only supported for `Conv1D`'\n-                       'and `SeparableConv1D`.')\n-\n-  def build(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape)\n-    input_channel = self._get_input_channel(input_shape)\n-    if input_channel % self.groups != 0:\n-      raise ValueError(\n-          'The number of input channels must be evenly divisible by the number '\n-          'of groups. Received groups={}, but the input has {} channels '\n-          '(full input shape is {}).'.format(self.groups, input_channel,\n-                                             input_shape))\n-    kernel_shape = self.kernel_size + (input_channel // self.groups,\n-                                       self.filters)\n-\n-    # compute_output_shape contains some validation logic for the input shape,\n-    # and make sure the output shape has all positive dimensions.\n-    self.compute_output_shape(input_shape)\n-\n-    self.kernel = self.add_weight(\n-        name='kernel',\n-        shape=kernel_shape,\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        trainable=True,\n-        dtype=self.dtype)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          name='bias',\n-          shape=(self.filters,),\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          trainable=True,\n-          dtype=self.dtype)\n-    else:\n-      self.bias = None\n-    channel_axis = self._get_channel_axis()\n-    self.input_spec = InputSpec(min_ndim=self.rank + 2,\n-                                axes={channel_axis: input_channel})\n-    self.built = True\n-\n-  def convolution_op(self, inputs, kernel):\n-    if self.padding == 'causal':\n-      tf_padding = 'VALID'  # Causal padding handled in `call`.\n-    elif isinstance(self.padding, str):\n-      tf_padding = self.padding.upper()\n-    else:\n-      tf_padding = self.padding\n-\n-    return tf.nn.convolution(\n-        inputs,\n-        kernel,\n-        strides=list(self.strides),\n-        padding=tf_padding,\n-        dilations=list(self.dilation_rate),\n-        data_format=self._tf_data_format,\n-        name=self.__class__.__name__)\n-\n-  # TODO(b/213173659): remove this when grouped convolutions are fully supported\n-  # on the CPU for compiled functions. For now, we need this as a workaround for\n-  # CPU support.\n-  @tf.function(jit_compile=True)\n-  def _jit_compiled_convolution_op(self, inputs, kernel):\n-    return self.convolution_op(inputs, kernel)\n-\n-  def call(self, inputs):\n-    input_shape = inputs.shape\n-\n-    if self._is_causal:  # Apply causal padding to inputs for Conv1D.\n-      inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n-\n-    if self.groups > 1:\n-      outputs = self._jit_compiled_convolution_op(inputs, self.kernel)\n-    else:\n-      outputs = self.convolution_op(inputs, self.kernel)\n-\n-    if self.use_bias:\n-      output_rank = outputs.shape.rank\n-      if self.rank == 1 and self._channels_first:\n-        # nn.bias_add does not accept a 1D input tensor.\n-        bias = tf.reshape(self.bias, (1, self.filters, 1))\n-        outputs += bias\n-      else:\n-        # Handle multiple batch dimensions.\n-        if output_rank is not None and output_rank > 2 + self.rank:\n-\n-          def _apply_fn(o):\n-            return tf.nn.bias_add(o, self.bias, data_format=self._tf_data_format)\n-\n-          outputs = conv_utils.squeeze_batch_dims(\n-              outputs, _apply_fn, inner_rank=self.rank + 1)\n-        else:\n-          outputs = tf.nn.bias_add(\n-              outputs, self.bias, data_format=self._tf_data_format)\n-\n-    if not tf.executing_eagerly():\n-      # Infer the static output shape:\n-      out_shape = self.compute_output_shape(input_shape)\n-      outputs.set_shape(out_shape)\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-    return outputs\n-\n-  def _spatial_output_shape(self, spatial_input_shape):\n-    return [\n-        conv_utils.conv_output_length(\n-            length,\n-            self.kernel_size[i],\n-            padding=self.padding,\n-            stride=self.strides[i],\n-            dilation=self.dilation_rate[i])\n-        for i, length in enumerate(spatial_input_shape)\n-    ]\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    batch_rank = len(input_shape) - self.rank - 1\n-    try:\n-      if self.data_format == 'channels_last':\n-        return tf.TensorShape(\n-            input_shape[:batch_rank] +\n-            self._spatial_output_shape(input_shape[batch_rank:-1]) +\n-            [self.filters])\n-      else:\n-        return tf.TensorShape(\n-            input_shape[:batch_rank] + [self.filters] +\n-            self._spatial_output_shape(input_shape[batch_rank + 1:]))\n-\n-    except ValueError:\n-      raise ValueError(\n-          f'One of the dimensions in the output is <= 0 '\n-          f'due to downsampling in {self.name}. Consider '\n-          f'increasing the input size. '\n-          f'Received input shape {input_shape} which would produce '\n-          f'output shape with a zero or negative value in a '\n-          f'dimension.')\n-\n-  def _recreate_conv_op(self, inputs):  # pylint: disable=unused-argument\n-    return False\n-\n-  def get_config(self):\n-    config = {\n-        'filters':\n-            self.filters,\n-        'kernel_size':\n-            self.kernel_size,\n-        'strides':\n-            self.strides,\n-        'padding':\n-            self.padding,\n-        'data_format':\n-            self.data_format,\n-        'dilation_rate':\n-            self.dilation_rate,\n-        'groups':\n-            self.groups,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'use_bias':\n-            self.use_bias,\n-        'kernel_initializer':\n-            initializers.serialize(self.kernel_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'kernel_regularizer':\n-            regularizers.serialize(self.kernel_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'kernel_constraint':\n-            constraints.serialize(self.kernel_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint)\n-    }\n-    base_config = super(Conv, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-  def _compute_causal_padding(self, inputs):\n-    \"\"\"Calculates padding for 'causal' option for 1-d conv layers.\"\"\"\n-    left_pad = self.dilation_rate[0] * (self.kernel_size[0] - 1)\n-    if getattr(inputs.shape, 'ndims', None) is None:\n-      batch_rank = 1\n-    else:\n-      batch_rank = len(inputs.shape) - 2\n-    if self.data_format == 'channels_last':\n-      causal_padding = [[0, 0]] * batch_rank + [[left_pad, 0], [0, 0]]\n-    else:\n-      causal_padding = [[0, 0]] * batch_rank + [[0, 0], [left_pad, 0]]\n-    return causal_padding\n-\n-  def _get_channel_axis(self):\n-    if self.data_format == 'channels_first':\n-      return -1 - self.rank\n-    else:\n-      return -1\n-\n-  def _get_input_channel(self, input_shape):\n-    channel_axis = self._get_channel_axis()\n-    if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs should be defined. '\n-                       f'The input_shape received is {input_shape}, '\n-                       f'where axis {channel_axis} (0-based) '\n-                       'is the channel dimension, which found to be `None`.')\n-    return int(input_shape[channel_axis])\n-\n-  def _get_padding_op(self):\n-    if self.padding == 'causal':\n-      op_padding = 'valid'\n-    else:\n-      op_padding = self.padding\n-    if not isinstance(op_padding, (list, tuple)):\n-      op_padding = op_padding.upper()\n-    return op_padding\n-\n-\n-@keras_export('keras.layers.Conv1D', 'keras.layers.Convolution1D')\n-class Conv1D(Conv):\n-  \"\"\"1D convolution layer (e.g. temporal convolution).\n-\n-  This layer creates a convolution kernel that is convolved\n-  with the layer input over a single spatial (or temporal) dimension\n-  to produce a tensor of outputs.\n-  If `use_bias` is True, a bias vector is created and added to the outputs.\n-  Finally, if `activation` is not `None`,\n-  it is applied to the outputs as well.\n-\n-  When using this layer as the first layer in a model,\n-  provide an `input_shape` argument\n-  (tuple of integers or `None`, e.g.\n-  `(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\n-  or `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n-\n-  Examples:\n-\n-  >>> # The inputs are 128-length vectors with 10 timesteps, and the batch size\n-  >>> # is 4.\n-  >>> input_shape = (4, 10, 128)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv1D(\n-  ... 32, 3, activation='relu',input_shape=input_shape[1:])(x)\n-  >>> print(y.shape)\n-  (4, 8, 32)\n-\n-  >>> # With extended batch shape [4, 7] (e.g. weather data where batch\n-  >>> # dimensions correspond to spatial location and the third dimension\n-  >>> # corresponds to time.)\n-  >>> input_shape = (4, 7, 10, 128)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv1D(\n-  ... 32, 3, activation='relu', input_shape=input_shape[2:])(x)\n-  >>> print(y.shape)\n-  (4, 7, 8, 32)\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n-    kernel_size: An integer or tuple/list of a single integer,\n-      specifying the length of the 1D convolution window.\n-    strides: An integer or tuple/list of a single integer,\n-      specifying the stride length of the convolution.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"`, `\"same\"` or `\"causal\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-      `\"causal\"` results in causal (dilated) convolutions, e.g. `output[t]`\n-      does not depend on `input[t+1:]`. Useful when modeling temporal data\n-      where the model should not violate the temporal order.\n-      See [WaveNet: A Generative Model for Raw Audio, section\n-        2.1](https://arxiv.org/abs/1609.03499).\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-    dilation_rate: an integer or tuple/list of a single integer, specifying\n-      the dilation rate to use for dilated convolution.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any `strides` value != 1.\n-    groups: A positive integer specifying the number of groups in which the\n-      input is split along the channel axis. Each group is convolved\n-      separately with `filters / groups` filters. The output is the\n-      concatenation of all the `groups` results along the channel axis.\n-      Input channels and `filters` must both be divisible by `groups`.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix\n-      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n-    bias_initializer: Initializer for the bias vector\n-      (see `keras.initializers`). Defaults to 'zeros'.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector\n-      (see `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\")\n-      (see `keras.regularizers`).\n-    kernel_constraint: Constraint function applied to the kernel matrix\n-      (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector\n-      (see `keras.constraints`).\n-\n-  Input shape:\n-    3+D tensor with shape: `batch_shape + (steps, input_dim)`\n-\n-  Output shape:\n-    3+D tensor with shape: `batch_shape + (new_steps, filters)`\n-      `steps` value might have changed due to padding or strides.\n-\n-  Returns:\n-    A tensor of rank 3 representing\n-    `activation(conv1d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format='channels_last',\n-               dilation_rate=1,\n-               groups=1,\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Conv1D, self).__init__(\n-        rank=1,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        groups=groups,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        kernel_initializer=initializers.get(kernel_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        kernel_regularizer=regularizers.get(kernel_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        kernel_constraint=constraints.get(kernel_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.Conv2D', 'keras.layers.Convolution2D')\n-class Conv2D(Conv):\n-  \"\"\"2D convolution layer (e.g. spatial convolution over images).\n-\n-  This layer creates a convolution kernel that is convolved\n-  with the layer input to produce a tensor of\n-  outputs. If `use_bias` is True,\n-  a bias vector is created and added to the outputs. Finally, if\n-  `activation` is not `None`, it is applied to the outputs as well.\n-\n-  When using this layer as the first layer in a model,\n-  provide the keyword argument `input_shape`\n-  (tuple of integers or `None`, does not include the sample axis),\n-  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n-  in `data_format=\"channels_last\"`. You can use `None` when\n-  a dimension has variable size.\n-\n-  Examples:\n-\n-  >>> # The inputs are 28x28 RGB images with `channels_last` and the batch\n-  >>> # size is 4.\n-  >>> input_shape = (4, 28, 28, 3)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv2D(\n-  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n-  >>> print(y.shape)\n-  (4, 26, 26, 2)\n-\n-  >>> # With `dilation_rate` as 2.\n-  >>> input_shape = (4, 28, 28, 3)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv2D(\n-  ... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\n-  >>> print(y.shape)\n-  (4, 24, 24, 2)\n-\n-  >>> # With `padding` as \"same\".\n-  >>> input_shape = (4, 28, 28, 3)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv2D(\n-  ... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n-  >>> print(y.shape)\n-  (4, 28, 28, 2)\n-\n-  >>> # With extended batch shape [4, 7]:\n-  >>> input_shape = (4, 7, 28, 28, 3)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv2D(\n-  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n-  >>> print(y.shape)\n-  (4, 7, 26, 26, 2)\n-\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space (i.e. the number of\n-      output filters in the convolution).\n-    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n-      and width of the 2D convolution window. Can be a single integer to specify\n-      the same value for all spatial dimensions.\n-    strides: An integer or tuple/list of 2 integers, specifying the strides of\n-      the convolution along the height and width. Can be a single integer to\n-      specify the same value for all spatial dimensions. Specifying any stride\n-      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n-    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input. When `padding=\"same\"` and\n-      `strides=1`, the output has the same size as the input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch_size, height, width, channels)` while\n-      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-      height, width)`. It defaults to the `image_data_format` value found in\n-      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-      it will be `channels_last`.\n-    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n-      dilation rate to use for dilated convolution. Can be a single integer to\n-      specify the same value for all spatial dimensions. Currently, specifying\n-      any `dilation_rate` value != 1 is incompatible with specifying any stride\n-      value != 1.\n-    groups: A positive integer specifying the number of groups in which the\n-      input is split along the channel axis. Each group is convolved separately\n-      with `filters / groups` filters. The output is the concatenation of all\n-      the `groups` results along the channel axis. Input channels and `filters`\n-      must both be divisible by `groups`.\n-    activation: Activation function to use. If you don't specify anything, no\n-      activation is applied (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix (see\n-      `keras.initializers`). Defaults to 'glorot_uniform'.\n-    bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`). Defaults to 'zeros'.\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector (see\n-      `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its \"activation\") (see `keras.regularizers`).\n-    kernel_constraint: Constraint function applied to the kernel matrix (see\n-      `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector (see\n-      `keras.constraints`).\n-\n-  Input shape:\n-    4+D tensor with shape: `batch_shape + (channels, rows, cols)` if\n-      `data_format='channels_first'`\n-    or 4+D tensor with shape: `batch_shape + (rows, cols, channels)` if\n-      `data_format='channels_last'`.\n-\n-  Output shape:\n-    4+D tensor with shape: `batch_shape + (filters, new_rows, new_cols)` if\n-    `data_format='channels_first'` or 4+D tensor with shape: `batch_shape +\n-      (new_rows, new_cols, filters)` if `data_format='channels_last'`.  `rows`\n-      and `cols` values might have changed due to padding.\n-\n-  Returns:\n-    A tensor of rank 4+ representing\n-    `activation(conv2d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is `\"causal\"`.\n-    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1),\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=(1, 1),\n-               groups=1,\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Conv2D, self).__init__(\n-        rank=2,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        groups=groups,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        kernel_initializer=initializers.get(kernel_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        kernel_regularizer=regularizers.get(kernel_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        kernel_constraint=constraints.get(kernel_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.Conv3D', 'keras.layers.Convolution3D')\n-class Conv3D(Conv):\n-  \"\"\"3D convolution layer (e.g. spatial convolution over volumes).\n-\n-  This layer creates a convolution kernel that is convolved\n-  with the layer input to produce a tensor of\n-  outputs. If `use_bias` is True,\n-  a bias vector is created and added to the outputs. Finally, if\n-  `activation` is not `None`, it is applied to the outputs as well.\n-\n-  When using this layer as the first layer in a model,\n-  provide the keyword argument `input_shape`\n-  (tuple of integers or `None`, does not include the sample axis),\n-  e.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\n-  with a single channel,\n-  in `data_format=\"channels_last\"`.\n-\n-  Examples:\n-\n-  >>> # The inputs are 28x28x28 volumes with a single channel, and the\n-  >>> # batch size is 4\n-  >>> input_shape =(4, 28, 28, 28, 1)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv3D(\n-  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n-  >>> print(y.shape)\n-  (4, 26, 26, 26, 2)\n-\n-  >>> # With extended batch shape [4, 7], e.g. a batch of 4 videos of 3D frames,\n-  >>> # with 7 frames per video.\n-  >>> input_shape = (4, 7, 28, 28, 28, 1)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.Conv3D(\n-  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n-  >>> print(y.shape)\n-  (4, 7, 26, 26, 26, 2)\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space (i.e. the number of\n-      output filters in the convolution).\n-    kernel_size: An integer or tuple/list of 3 integers, specifying the depth,\n-      height and width of the 3D convolution window. Can be a single integer to\n-      specify the same value for all spatial dimensions.\n-    strides: An integer or tuple/list of 3 integers, specifying the strides of\n-      the convolution along each spatial dimension. Can be a single integer to\n-      specify the same value for all spatial dimensions. Specifying any stride\n-      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n-    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `batch_shape + (spatial_dim1, spatial_dim2,\n-      spatial_dim3, channels)` while `channels_first` corresponds to inputs with\n-      shape `batch_shape + (channels, spatial_dim1, spatial_dim2,\n-      spatial_dim3)`. It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`. If you never set it, then it\n-      will be \"channels_last\".\n-    dilation_rate: an integer or tuple/list of 3 integers, specifying the\n-      dilation rate to use for dilated convolution. Can be a single integer to\n-      specify the same value for all spatial dimensions. Currently, specifying\n-      any `dilation_rate` value != 1 is incompatible with specifying any stride\n-      value != 1.\n-    groups: A positive integer specifying the number of groups in which the\n-      input is split along the channel axis. Each group is convolved separately\n-      with `filters / groups` filters. The output is the concatenation of all\n-      the `groups` results along the channel axis. Input channels and `filters`\n-      must both be divisible by `groups`.\n-    activation: Activation function to use. If you don't specify anything, no\n-      activation is applied (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix (see\n-      `keras.initializers`). Defaults to 'glorot_uniform'.\n-    bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`). Defaults to 'zeros'.\n-    kernel_regularizer: Regularizer function applied to the `kernel` weights\n-      matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector (see\n-      `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its \"activation\") (see `keras.regularizers`).\n-    kernel_constraint: Constraint function applied to the kernel matrix (see\n-      `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector (see\n-      `keras.constraints`).\n-\n-  Input shape:\n-    5+D tensor with shape: `batch_shape + (channels, conv_dim1, conv_dim2,\n-      conv_dim3)` if data_format='channels_first'\n-    or 5+D tensor with shape: `batch_shape + (conv_dim1, conv_dim2, conv_dim3,\n-      channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    5+D tensor with shape: `batch_shape + (filters, new_conv_dim1,\n-      new_conv_dim2, new_conv_dim3)` if data_format='channels_first'\n-    or 5+D tensor with shape: `batch_shape + (new_conv_dim1, new_conv_dim2,\n-      new_conv_dim3, filters)` if data_format='channels_last'. `new_conv_dim1`,\n-      `new_conv_dim2` and `new_conv_dim3` values might have changed due to\n-      padding.\n-\n-  Returns:\n-    A tensor of rank 5+ representing\n-    `activation(conv3d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1, 1),\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=(1, 1, 1),\n-               groups=1,\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Conv3D, self).__init__(\n-        rank=3,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        groups=groups,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        kernel_initializer=initializers.get(kernel_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        kernel_regularizer=regularizers.get(kernel_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        kernel_constraint=constraints.get(kernel_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.Conv1DTranspose',\n-              'keras.layers.Convolution1DTranspose')\n-class Conv1DTranspose(Conv1D):\n-  \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n-\n-  The need for transposed convolutions generally arises\n-  from the desire to use a transformation going in the opposite direction\n-  of a normal convolution, i.e., from something that has the shape of the\n-  output of some convolution to something that has the shape of its input\n-  while maintaining a connectivity pattern that is compatible with\n-  said convolution.\n-\n-  When using this layer as the first layer in a model,\n-  provide the keyword argument `input_shape`\n-  (tuple of integers or `None`, does not include the sample axis),\n-  e.g. `input_shape=(128, 3)` for data with 128 time steps and 3 channels.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n-    kernel_size: An integer length of the 1D convolution window.\n-    strides: An integer specifying the stride of the convolution along the\n-      time dimension. Specifying a stride value != 1 is incompatible with\n-      specifying a `dilation_rate` value != 1. Defaults to 1.\n-    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    output_padding: An integer specifying the amount of padding along\n-      the time dimension of the output tensor.\n-      The amount of output padding must be lower than the stride.\n-      If set to `None` (default), the output shape is inferred.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, length, channels)` while `channels_first` corresponds to\n-      inputs with shape `(batch_size, channels, length)`.\n-    dilation_rate: an integer, specifying\n-      the dilation rate to use for dilated convolution.\n-      Currently, specifying a `dilation_rate` value != 1 is\n-      incompatible with specifying a stride value != 1.\n-      Also dilation rate larger than 1 is not currently supported.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix\n-      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n-    bias_initializer: Initializer for the bias vector\n-      (see `keras.initializers`). Defaults to 'zeros'.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector\n-      (see `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\") (see `keras.regularizers`).\n-    kernel_constraint: Constraint function applied to the kernel matrix\n-      (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector\n-      (see `keras.constraints`).\n-\n-  Input shape:\n-    3D tensor with shape:\n-    `(batch_size, steps, channels)`\n-\n-  Output shape:\n-    3D tensor with shape:\n-    `(batch_size, new_steps, filters)`\n-    If `output_padding` is specified:\n-    ```\n-    new_timesteps = ((timesteps - 1) * strides + kernel_size -\n-    2 * padding + output_padding)\n-    ```\n-\n-  Returns:\n-    A tensor of rank 3 representing\n-    `activation(conv1dtranspose(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n-\n-  References:\n-    - [A guide to convolution arithmetic for deep learning](\n-      https://arxiv.org/abs/1603.07285v1)\n-    - [Deconvolutional Networks](\n-      https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               output_padding=None,\n-               data_format=None,\n-               dilation_rate=1,\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Conv1DTranspose, self).__init__(\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        kernel_initializer=initializers.get(kernel_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        kernel_regularizer=regularizers.get(kernel_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        kernel_constraint=constraints.get(kernel_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-    self.output_padding = output_padding\n-    if self.output_padding is not None:\n-      self.output_padding = conv_utils.normalize_tuple(\n-          self.output_padding, 1, 'output_padding', allow_zero=True)\n-      for stride, out_pad in zip(self.strides, self.output_padding):\n-        if out_pad >= stride:\n-          raise ValueError('Strides must be greater than output padding. '\n-                           f'Received strides={self.strides}, '\n-                           f'output_padding={self.output_padding}.')\n-\n-  def build(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape)\n-    if len(input_shape) != 3:\n-      raise ValueError('Inputs should have rank 3. '\n-                       f'Received input_shape={input_shape}.')\n-    channel_axis = self._get_channel_axis()\n-    if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs '\n-                       'to `Conv1DTranspose` should be defined. '\n-                       f'The input_shape received is {input_shape}, '\n-                       f'where axis {channel_axis} (0-based) '\n-                       'is the channel dimension, which found to be `None`.')\n-    input_dim = int(input_shape[channel_axis])\n-    self.input_spec = InputSpec(ndim=3, axes={channel_axis: input_dim})\n-    kernel_shape = self.kernel_size + (self.filters, input_dim)\n-\n-    self.kernel = self.add_weight(\n-        name='kernel',\n-        shape=kernel_shape,\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        trainable=True,\n-        dtype=self.dtype)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          name='bias',\n-          shape=(self.filters,),\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          trainable=True,\n-          dtype=self.dtype)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs):\n-    inputs_shape = tf.shape(inputs)\n-    batch_size = inputs_shape[0]\n-    if self.data_format == 'channels_first':\n-      t_axis = 2\n-    else:\n-      t_axis = 1\n-\n-    length = inputs_shape[t_axis]\n-    if self.output_padding is None:\n-      output_padding = None\n-    else:\n-      output_padding = self.output_padding[0]\n-\n-    # Infer the dynamic output shape:\n-    out_length = conv_utils.deconv_output_length(\n-        length, self.kernel_size[0], padding=self.padding,\n-        output_padding=output_padding, stride=self.strides[0],\n-        dilation=self.dilation_rate[0])\n-    if self.data_format == 'channels_first':\n-      output_shape = (batch_size, self.filters, out_length)\n-    else:\n-      output_shape = (batch_size, out_length, self.filters)\n-    data_format = conv_utils.convert_data_format(self.data_format, ndim=3)\n-\n-    output_shape_tensor = tf.stack(output_shape)\n-    outputs = tf.nn.conv1d_transpose(\n-        inputs,\n-        self.kernel,\n-        output_shape_tensor,\n-        strides=self.strides,\n-        padding=self.padding.upper(),\n-        data_format=data_format,\n-        dilations=self.dilation_rate)\n-\n-    if not tf.executing_eagerly():\n-      # Infer the static output shape:\n-      out_shape = self.compute_output_shape(inputs.shape)\n-      outputs.set_shape(out_shape)\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=data_format)\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    output_shape = list(input_shape)\n-    if self.data_format == 'channels_first':\n-      c_axis, t_axis = 1, 2\n-    else:\n-      c_axis, t_axis = 2, 1\n-\n-    if self.output_padding is None:\n-      output_padding = None\n-    else:\n-      output_padding = self.output_padding[0]\n-    output_shape[c_axis] = self.filters\n-    output_shape[t_axis] = conv_utils.deconv_output_length(\n-        output_shape[t_axis],\n-        self.kernel_size[0],\n-        padding=self.padding,\n-        output_padding=output_padding,\n-        stride=self.strides[0],\n-        dilation=self.dilation_rate[0])\n-    return tf.TensorShape(output_shape)\n-\n-  def get_config(self):\n-    config = super(Conv1DTranspose, self).get_config()\n-    config['output_padding'] = self.output_padding\n-    return config\n-\n-\n-@keras_export('keras.layers.Conv2DTranspose',\n-              'keras.layers.Convolution2DTranspose')\n-class Conv2DTranspose(Conv2D):\n-  \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n-\n-  The need for transposed convolutions generally arises\n-  from the desire to use a transformation going in the opposite direction\n-  of a normal convolution, i.e., from something that has the shape of the\n-  output of some convolution to something that has the shape of its input\n-  while maintaining a connectivity pattern that is compatible with\n-  said convolution.\n-\n-  When using this layer as the first layer in a model,\n-  provide the keyword argument `input_shape`\n-  (tuple of integers or `None`, does not include the sample axis),\n-  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n-  in `data_format=\"channels_last\"`.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n-    kernel_size: An integer or tuple/list of 2 integers, specifying the\n-      height and width of the 2D convolution window.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    strides: An integer or tuple/list of 2 integers,\n-      specifying the strides of the convolution along the height and width.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    output_padding: An integer or tuple/list of 2 integers,\n-      specifying the amount of padding along the height and width\n-      of the output tensor.\n-      Can be a single integer to specify the same value for all\n-      spatial dimensions.\n-      The amount of output padding along a given dimension must be\n-      lower than the stride along that same dimension.\n-      If set to `None` (default), the output shape is inferred.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch_size, channels, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    dilation_rate: an integer, specifying the dilation rate for all spatial\n-      dimensions for dilated convolution. Specifying different dilation rates\n-      for different dimensions is not supported.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any stride value != 1.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix\n-      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n-    bias_initializer: Initializer for the bias vector\n-      (see `keras.initializers`). Defaults to 'zeros'.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector\n-      (see `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\") (see `keras.regularizers`).\n-    kernel_constraint: Constraint function applied to the kernel matrix\n-      (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector\n-      (see `keras.constraints`).\n-\n-  Input shape:\n-    4D tensor with shape:\n-    `(batch_size, channels, rows, cols)` if data_format='channels_first'\n-    or 4D tensor with shape:\n-    `(batch_size, rows, cols, channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    4D tensor with shape:\n-    `(batch_size, filters, new_rows, new_cols)` if data_format='channels_first'\n-    or 4D tensor with shape:\n-    `(batch_size, new_rows, new_cols, filters)` if data_format='channels_last'.\n-    `rows` and `cols` values might have changed due to padding.\n-    If `output_padding` is specified:\n-    ```\n-    new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\n-    output_padding[0])\n-    new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\n-    output_padding[1])\n-    ```\n-\n-  Returns:\n-    A tensor of rank 4 representing\n-    `activation(conv2dtranspose(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n-\n-  References:\n-    - [A guide to convolution arithmetic for deep\n-      learning](https://arxiv.org/abs/1603.07285v1)\n-    - [Deconvolutional\n-      Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1),\n-               padding='valid',\n-               output_padding=None,\n-               data_format=None,\n-               dilation_rate=(1, 1),\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Conv2DTranspose, self).__init__(\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        kernel_initializer=initializers.get(kernel_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        kernel_regularizer=regularizers.get(kernel_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        kernel_constraint=constraints.get(kernel_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-    self.output_padding = output_padding\n-    if self.output_padding is not None:\n-      self.output_padding = conv_utils.normalize_tuple(\n-          self.output_padding, 2, 'output_padding', allow_zero=True)\n-      for stride, out_pad in zip(self.strides, self.output_padding):\n-        if out_pad >= stride:\n-          raise ValueError('Strides must be greater than output padding. '\n-                           f'Received strides={self.strides}, '\n-                           f'output_padding={self.output_padding}.')\n-\n-  def build(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape)\n-    if len(input_shape) != 4:\n-      raise ValueError('Inputs should have rank 4. '\n-                       f'Received input_shape={input_shape}.')\n-    channel_axis = self._get_channel_axis()\n-    if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs '\n-                       'to `Conv2DTranspose` should be defined. '\n-                       f'The input_shape received is {input_shape}, '\n-                       f'where axis {channel_axis} (0-based) '\n-                       'is the channel dimension, which found to be `None`.')\n-    input_dim = int(input_shape[channel_axis])\n-    self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n-    kernel_shape = self.kernel_size + (self.filters, input_dim)\n-\n-    self.kernel = self.add_weight(\n-        name='kernel',\n-        shape=kernel_shape,\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        trainable=True,\n-        dtype=self.dtype)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          name='bias',\n-          shape=(self.filters,),\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          trainable=True,\n-          dtype=self.dtype)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs):\n-    inputs_shape = tf.shape(inputs)\n-    batch_size = inputs_shape[0]\n-    if self.data_format == 'channels_first':\n-      h_axis, w_axis = 2, 3\n-    else:\n-      h_axis, w_axis = 1, 2\n-\n-    # Use the constant height and weight when possible.\n-    # TODO(scottzhu): Extract this into a utility function that can be applied\n-    # to all convolutional layers, which currently lost the static shape\n-    # information due to tf.shape().\n-    height, width = None, None\n-    if inputs.shape.rank is not None:\n-      dims = inputs.shape.as_list()\n-      height = dims[h_axis]\n-      width = dims[w_axis]\n-    height = height if height is not None else inputs_shape[h_axis]\n-    width = width if width is not None else inputs_shape[w_axis]\n-\n-    kernel_h, kernel_w = self.kernel_size\n-    stride_h, stride_w = self.strides\n-\n-    if self.output_padding is None:\n-      out_pad_h = out_pad_w = None\n-    else:\n-      out_pad_h, out_pad_w = self.output_padding\n-\n-    # Infer the dynamic output shape:\n-    out_height = conv_utils.deconv_output_length(height,\n-                                                 kernel_h,\n-                                                 padding=self.padding,\n-                                                 output_padding=out_pad_h,\n-                                                 stride=stride_h,\n-                                                 dilation=self.dilation_rate[0])\n-    out_width = conv_utils.deconv_output_length(width,\n-                                                kernel_w,\n-                                                padding=self.padding,\n-                                                output_padding=out_pad_w,\n-                                                stride=stride_w,\n-                                                dilation=self.dilation_rate[1])\n-    if self.data_format == 'channels_first':\n-      output_shape = (batch_size, self.filters, out_height, out_width)\n-    else:\n-      output_shape = (batch_size, out_height, out_width, self.filters)\n-\n-    output_shape_tensor = tf.stack(output_shape)\n-    outputs = backend.conv2d_transpose(\n-        inputs,\n-        self.kernel,\n-        output_shape_tensor,\n-        strides=self.strides,\n-        padding=self.padding,\n-        data_format=self.data_format,\n-        dilation_rate=self.dilation_rate)\n-\n-    if not tf.executing_eagerly():\n-      # Infer the static output shape:\n-      out_shape = self.compute_output_shape(inputs.shape)\n-      outputs.set_shape(out_shape)\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    output_shape = list(input_shape)\n-    if self.data_format == 'channels_first':\n-      c_axis, h_axis, w_axis = 1, 2, 3\n-    else:\n-      c_axis, h_axis, w_axis = 3, 1, 2\n-\n-    kernel_h, kernel_w = self.kernel_size\n-    stride_h, stride_w = self.strides\n-\n-    if self.output_padding is None:\n-      out_pad_h = out_pad_w = None\n-    else:\n-      out_pad_h, out_pad_w = self.output_padding\n-\n-    output_shape[c_axis] = self.filters\n-    output_shape[h_axis] = conv_utils.deconv_output_length(\n-        output_shape[h_axis],\n-        kernel_h,\n-        padding=self.padding,\n-        output_padding=out_pad_h,\n-        stride=stride_h,\n-        dilation=self.dilation_rate[0])\n-    output_shape[w_axis] = conv_utils.deconv_output_length(\n-        output_shape[w_axis],\n-        kernel_w,\n-        padding=self.padding,\n-        output_padding=out_pad_w,\n-        stride=stride_w,\n-        dilation=self.dilation_rate[1])\n-    return tf.TensorShape(output_shape)\n-\n-  def get_config(self):\n-    config = super(Conv2DTranspose, self).get_config()\n-    config['output_padding'] = self.output_padding\n-    return config\n-\n-\n-@keras_export('keras.layers.Conv3DTranspose',\n-              'keras.layers.Convolution3DTranspose')\n-class Conv3DTranspose(Conv3D):\n-  \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n-\n-  The need for transposed convolutions generally arises\n-  from the desire to use a transformation going in the opposite direction\n-  of a normal convolution, i.e., from something that has the shape of the\n-  output of some convolution to something that has the shape of its input\n-  while maintaining a connectivity pattern that is compatible with\n-  said convolution.\n-\n-  When using this layer as the first layer in a model,\n-  provide the keyword argument `input_shape`\n-  (tuple of integers or `None`, does not include the sample axis),\n-  e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\n-  if `data_format=\"channels_last\"`.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n-    kernel_size: An integer or tuple/list of 3 integers, specifying the\n-      depth, height and width of the 3D convolution window.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    strides: An integer or tuple/list of 3 integers,\n-      specifying the strides of the convolution along the depth, height\n-        and width.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    output_padding: An integer or tuple/list of 3 integers,\n-      specifying the amount of padding along the depth, height, and\n-      width.\n-      Can be a single integer to specify the same value for all\n-      spatial dimensions.\n-      The amount of output padding along a given dimension must be\n-      lower than the stride along that same dimension.\n-      If set to `None` (default), the output shape is inferred.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, depth, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch_size, channels, depth, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    dilation_rate: an integer or tuple/list of 3 integers, specifying\n-      the dilation rate to use for dilated convolution.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any stride value != 1.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    kernel_initializer: Initializer for the `kernel` weights matrix\n-      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n-    bias_initializer: Initializer for the bias vector\n-      (see `keras.initializers`). Defaults to 'zeros'.\n-    kernel_regularizer: Regularizer function applied to\n-      the `kernel` weights matrix\n-      (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector\n-      (see `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\")\n-      (see `keras.regularizers`).\n-    kernel_constraint: Constraint function applied to the kernel matrix\n-      (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector\n-      (see `keras.constraints`).\n-\n-  Input shape:\n-    5D tensor with shape:\n-    `(batch_size, channels, depth, rows, cols)` if data_format='channels_first'\n-    or 5D tensor with shape:\n-    `(batch_size, depth, rows, cols, channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    5D tensor with shape:\n-    `(batch_size, filters, new_depth, new_rows, new_cols)` if\n-      data_format='channels_first'\n-    or 5D tensor with shape:\n-    `(batch_size, new_depth, new_rows, new_cols, filters)` if\n-      data_format='channels_last'.\n-    `depth` and `rows` and `cols` values might have changed due to padding.\n-    If `output_padding` is specified::\n-    ```\n-    new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\n-    output_padding[0])\n-    new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\n-    output_padding[1])\n-    new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +\n-    output_padding[2])\n-    ```\n-\n-  Returns:\n-    A tensor of rank 5 representing\n-    `activation(conv3dtranspose(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n-\n-  References:\n-    - [A guide to convolution arithmetic for deep\n-      learning](https://arxiv.org/abs/1603.07285v1)\n-    - [Deconvolutional\n-      Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1, 1),\n-               padding='valid',\n-               output_padding=None,\n-               data_format=None,\n-               dilation_rate=(1, 1, 1),\n-               activation=None,\n-               use_bias=True,\n-               kernel_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               kernel_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               kernel_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(Conv3DTranspose, self).__init__(\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        kernel_initializer=initializers.get(kernel_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        kernel_regularizer=regularizers.get(kernel_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        kernel_constraint=constraints.get(kernel_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-    self.output_padding = output_padding\n-    if self.output_padding is not None:\n-      self.output_padding = conv_utils.normalize_tuple(\n-          self.output_padding, 3, 'output_padding', allow_zero=True)\n-      for stride, out_pad in zip(self.strides, self.output_padding):\n-        if out_pad >= stride:\n-          raise ValueError('Strides must be greater than output padding. '\n-                           f'Received strides={self.strides}, '\n-                           f'output_padding={self.output_padding}.')\n-\n-  def build(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape)\n-    if len(input_shape) != 5:\n-      raise ValueError('Inputs should have rank 5. '\n-                       f'Received input_shape={input_shape}.')\n-    channel_axis = self._get_channel_axis()\n-    if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs '\n-                       'to `Conv3DTranspose` should be defined. '\n-                       f'The input_shape received is {input_shape}, '\n-                       f'where axis {channel_axis} (0-based) '\n-                       'is the channel dimension, which found to be `None`.')\n-    input_dim = int(input_shape[channel_axis])\n-    kernel_shape = self.kernel_size + (self.filters, input_dim)\n-    self.input_spec = InputSpec(ndim=5, axes={channel_axis: input_dim})\n-\n-    self.kernel = self.add_weight(\n-        'kernel',\n-        shape=kernel_shape,\n-        initializer=self.kernel_initializer,\n-        regularizer=self.kernel_regularizer,\n-        constraint=self.kernel_constraint,\n-        trainable=True,\n-        dtype=self.dtype)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          'bias',\n-          shape=(self.filters,),\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          trainable=True,\n-          dtype=self.dtype)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs):\n-    inputs_shape = tf.shape(inputs)\n-    batch_size = inputs_shape[0]\n-    if self.data_format == 'channels_first':\n-      d_axis, h_axis, w_axis = 2, 3, 4\n-    else:\n-      d_axis, h_axis, w_axis = 1, 2, 3\n-\n-    depth = inputs_shape[d_axis]\n-    height = inputs_shape[h_axis]\n-    width = inputs_shape[w_axis]\n-\n-    kernel_d, kernel_h, kernel_w = self.kernel_size\n-    stride_d, stride_h, stride_w = self.strides\n-\n-    if self.output_padding is None:\n-      out_pad_d = out_pad_h = out_pad_w = None\n-    else:\n-      out_pad_d, out_pad_h, out_pad_w = self.output_padding\n-\n-    # Infer the dynamic output shape:\n-    out_depth = conv_utils.deconv_output_length(depth,\n-                                                kernel_d,\n-                                                padding=self.padding,\n-                                                output_padding=out_pad_d,\n-                                                stride=stride_d)\n-    out_height = conv_utils.deconv_output_length(height,\n-                                                 kernel_h,\n-                                                 padding=self.padding,\n-                                                 output_padding=out_pad_h,\n-                                                 stride=stride_h)\n-    out_width = conv_utils.deconv_output_length(width,\n-                                                kernel_w,\n-                                                padding=self.padding,\n-                                                output_padding=out_pad_w,\n-                                                stride=stride_w)\n-    if self.data_format == 'channels_first':\n-      output_shape = (batch_size, self.filters, out_depth, out_height,\n-                      out_width)\n-      strides = (1, 1, stride_d, stride_h, stride_w)\n-    else:\n-      output_shape = (batch_size, out_depth, out_height, out_width,\n-                      self.filters)\n-      strides = (1, stride_d, stride_h, stride_w, 1)\n-\n-    output_shape_tensor = tf.stack(output_shape)\n-    outputs = tf.nn.conv3d_transpose(\n-        inputs,\n-        self.kernel,\n-        output_shape_tensor,\n-        strides,\n-        data_format=conv_utils.convert_data_format(self.data_format, ndim=5),\n-        padding=self.padding.upper())\n-\n-    if not tf.executing_eagerly():\n-      # Infer the static output shape:\n-      out_shape = self.compute_output_shape(inputs.shape)\n-      outputs.set_shape(out_shape)\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    output_shape = list(input_shape)\n-    if self.data_format == 'channels_first':\n-      c_axis, d_axis, h_axis, w_axis = 1, 2, 3, 4\n-    else:\n-      c_axis, d_axis, h_axis, w_axis = 4, 1, 2, 3\n-\n-    kernel_d, kernel_h, kernel_w = self.kernel_size\n-    stride_d, stride_h, stride_w = self.strides\n-\n-    if self.output_padding is None:\n-      out_pad_d = out_pad_h = out_pad_w = None\n-    else:\n-      out_pad_d, out_pad_h, out_pad_w = self.output_padding\n-\n-    output_shape[c_axis] = self.filters\n-    output_shape[d_axis] = conv_utils.deconv_output_length(\n-        output_shape[d_axis],\n-        kernel_d,\n-        padding=self.padding,\n-        output_padding=out_pad_d,\n-        stride=stride_d)\n-    output_shape[h_axis] = conv_utils.deconv_output_length(\n-        output_shape[h_axis],\n-        kernel_h,\n-        padding=self.padding,\n-        output_padding=out_pad_h,\n-        stride=stride_h)\n-    output_shape[w_axis] = conv_utils.deconv_output_length(\n-        output_shape[w_axis],\n-        kernel_w,\n-        padding=self.padding,\n-        output_padding=out_pad_w,\n-        stride=stride_w)\n-    return tf.TensorShape(output_shape)\n-\n-  def get_config(self):\n-    config = super(Conv3DTranspose, self).get_config()\n-    config.pop('dilation_rate')\n-    config['output_padding'] = self.output_padding\n-    return config\n-\n-\n-class SeparableConv(Conv):\n-  \"\"\"Abstract base layer for separable nD convolution.\n-\n-  This layer performs a depthwise convolution that acts separately on\n-  channels, followed by a pointwise convolution that mixes channels.\n-  If `use_bias` is True and a bias initializer is provided,\n-  it adds a bias vector to the output.\n-  It then optionally applies an activation function to produce the final output.\n-\n-  Args:\n-    rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n-    filters: Integer, the dimensionality of the output space (i.e. the number\n-      of filters in the convolution).\n-    kernel_size: A tuple or list of integers specifying the spatial\n-      dimensions of the filters. Can be a single integer to specify the same\n-      value for all spatial dimensions.\n-    strides: A tuple or list of integers specifying the strides\n-      of the convolution. Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-      Specifying any `stride` value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, ..., channels)` while `channels_first` corresponds to\n-      inputs with shape `(batch_size, channels, ...)`.\n-    dilation_rate: An integer or tuple/list of 2 integers, specifying\n-      the dilation rate to use for dilated convolution.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-      Currently, specifying any `dilation_rate` value != 1 is\n-      incompatible with specifying any stride value != 1.\n-    depth_multiplier: The number of depthwise convolution output channels for\n-      each input channel. The total number of depthwise convolution output\n-      channels will be equal to `num_filters_in * depth_multiplier`.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias.\n-    depthwise_initializer: An initializer for the depthwise convolution kernel\n-      (see `keras.initializers`). If None, then the default initializer\n-      ('glorot_uniform') will be used.\n-    pointwise_initializer: An initializer for the pointwise convolution kernel\n-      (see `keras.initializers`). If None, then the default initializer\n-      ('glorot_uniform') will be used.\n-    bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer ('zeros') will be used (see `keras.initializers`).\n-    depthwise_regularizer: Optional regularizer for the depthwise\n-      convolution kernel.\n-    pointwise_regularizer: Optional regularizer for the pointwise\n-      convolution kernel.\n-    bias_regularizer: Optional regularizer for the bias vector.\n-    activity_regularizer: Optional regularizer function for the output.\n-    depthwise_constraint: Optional projection function to be applied to the\n-      depthwise kernel after being updated by an `Optimizer` (e.g. used for\n-      norm constraints or value constraints for layer weights). The function\n-      must take as input the unprojected variable and must return the\n-      projected variable (which must have the same shape). Constraints are\n-      not safe to use when doing asynchronous distributed training.\n-    pointwise_constraint: Optional projection function to be applied to the\n-      pointwise kernel after being updated by an `Optimizer`.\n-    bias_constraint: Optional projection function to be applied to the\n-      bias after being updated by an `Optimizer`.\n-    trainable: Boolean, if `True` the weights of this layer will be marked as\n-      trainable (and listed in `layer.trainable_weights`).\n-  \"\"\"\n-\n-  def __init__(self,\n-               rank,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=1,\n-               depth_multiplier=1,\n-               activation=None,\n-               use_bias=True,\n-               depthwise_initializer='glorot_uniform',\n-               pointwise_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               depthwise_regularizer=None,\n-               pointwise_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               depthwise_constraint=None,\n-               pointwise_constraint=None,\n-               bias_constraint=None,\n-               trainable=True,\n-               name=None,\n-               **kwargs):\n-    super(SeparableConv, self).__init__(\n-        rank=rank,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        bias_initializer=initializers.get(bias_initializer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        bias_constraint=bias_constraint,\n-        trainable=trainable,\n-        name=name,\n-        **kwargs)\n-    self.depth_multiplier = depth_multiplier\n-    self.depthwise_initializer = initializers.get(depthwise_initializer)\n-    self.pointwise_initializer = initializers.get(pointwise_initializer)\n-    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n-    self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n-    self.depthwise_constraint = constraints.get(depthwise_constraint)\n-    self.pointwise_constraint = constraints.get(pointwise_constraint)\n-\n-  def build(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape)\n-    channel_axis = self._get_channel_axis()\n-    if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs should be defined. '\n-                       f'The input_shape received is {input_shape}, '\n-                       f'where axis {channel_axis} (0-based) '\n-                       'is the channel dimension, which found to be `None`.')\n-    input_dim = int(input_shape[channel_axis])\n-    self.input_spec = InputSpec(ndim=self.rank + 2,\n-                                axes={channel_axis: input_dim})\n-    depthwise_kernel_shape = self.kernel_size + (input_dim,\n-                                                 self.depth_multiplier)\n-    pointwise_kernel_shape = (\n-        1,) * self.rank + (self.depth_multiplier * input_dim, self.filters)\n-\n-    self.depthwise_kernel = self.add_weight(\n-        name='depthwise_kernel',\n-        shape=depthwise_kernel_shape,\n-        initializer=self.depthwise_initializer,\n-        regularizer=self.depthwise_regularizer,\n-        constraint=self.depthwise_constraint,\n-        trainable=True,\n-        dtype=self.dtype)\n-    self.pointwise_kernel = self.add_weight(\n-        name='pointwise_kernel',\n-        shape=pointwise_kernel_shape,\n-        initializer=self.pointwise_initializer,\n-        regularizer=self.pointwise_regularizer,\n-        constraint=self.pointwise_constraint,\n-        trainable=True,\n-        dtype=self.dtype)\n-    if self.use_bias:\n-      self.bias = self.add_weight(\n-          name='bias',\n-          shape=(self.filters,),\n-          initializer=self.bias_initializer,\n-          regularizer=self.bias_regularizer,\n-          constraint=self.bias_constraint,\n-          trainable=True,\n-          dtype=self.dtype)\n-    else:\n-      self.bias = None\n-    self.built = True\n-\n-  def call(self, inputs):\n-    raise NotImplementedError\n-\n-  def get_config(self):\n-    config = {\n-        'filters':\n-            self.filters,\n-        'kernel_size':\n-            self.kernel_size,\n-        'strides':\n-            self.strides,\n-        'padding':\n-            self.padding,\n-        'data_format':\n-            self.data_format,\n-        'depth_multiplier':\n-            self.depth_multiplier,\n-        'dilation_rate':\n-            self.dilation_rate,\n-        'activation':\n-            activations.serialize(self.activation),\n-        'use_bias':\n-            self.use_bias,\n-        'depthwise_initializer':\n-            initializers.serialize(self.depthwise_initializer),\n-        'pointwise_initializer':\n-            initializers.serialize(self.pointwise_initializer),\n-        'bias_initializer':\n-            initializers.serialize(self.bias_initializer),\n-        'depthwise_regularizer':\n-            regularizers.serialize(self.depthwise_regularizer),\n-        'pointwise_regularizer':\n-            regularizers.serialize(self.pointwise_regularizer),\n-        'bias_regularizer':\n-            regularizers.serialize(self.bias_regularizer),\n-        'activity_regularizer':\n-            regularizers.serialize(self.activity_regularizer),\n-        'depthwise_constraint':\n-            constraints.serialize(self.depthwise_constraint),\n-        'pointwise_constraint':\n-            constraints.serialize(self.pointwise_constraint),\n-        'bias_constraint':\n-            constraints.serialize(self.bias_constraint)\n-    }\n-    base_config = super(SeparableConv, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.SeparableConv1D',\n-              'keras.layers.SeparableConvolution1D')\n-class SeparableConv1D(SeparableConv):\n-  \"\"\"Depthwise separable 1D convolution.\n-\n-  This layer performs a depthwise convolution that acts separately on\n-  channels, followed by a pointwise convolution that mixes channels.\n-  If `use_bias` is True and a bias initializer is provided,\n-  it adds a bias vector to the output.\n-  It then optionally applies an activation function to produce the final output.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space (i.e. the number\n-      of filters in the convolution).\n-    kernel_size: A single integer specifying the spatial\n-      dimensions of the filters.\n-    strides: A single integer specifying the strides\n-      of the convolution.\n-      Specifying any `stride` value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"`, `\"same\"`, or `\"causal\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input. `\"causal\"` results in causal\n-      (dilated) convolutions, e.g. `output[t]` does not depend on `input[t+1:]`.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, length, channels)` while `channels_first` corresponds to\n-      inputs with shape `(batch_size, channels, length)`.\n-    dilation_rate: A single integer, specifying\n-      the dilation rate to use for dilated convolution.\n-    depth_multiplier: The number of depthwise convolution output channels for\n-      each input channel. The total number of depthwise convolution output\n-      channels will be equal to `num_filters_in * depth_multiplier`.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias.\n-    depthwise_initializer: An initializer for the depthwise convolution kernel\n-      (see `keras.initializers`). If None, then the default initializer\n-      ('glorot_uniform') will be used.\n-    pointwise_initializer: An initializer for the pointwise convolution kernel\n-      (see `keras.initializers`). If None, then the default initializer\n-      ('glorot_uniform') will be used.\n-    bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer ('zeros') will be used (see `keras.initializers`).\n-    depthwise_regularizer: Optional regularizer for the depthwise\n-      convolution kernel (see `keras.regularizers`).\n-    pointwise_regularizer: Optional regularizer for the pointwise\n-      convolution kernel (see `keras.regularizers`).\n-    bias_regularizer: Optional regularizer for the bias vector\n-      (see `keras.regularizers`).\n-    activity_regularizer: Optional regularizer function for the output\n-      (see `keras.regularizers`).\n-    depthwise_constraint: Optional projection function to be applied to the\n-      depthwise kernel after being updated by an `Optimizer` (e.g. used for\n-      norm constraints or value constraints for layer weights). The function\n-      must take as input the unprojected variable and must return the\n-      projected variable (which must have the same shape). Constraints are\n-      not safe to use when doing asynchronous distributed training\n-      (see `keras.constraints`).\n-    pointwise_constraint: Optional projection function to be applied to the\n-      pointwise kernel after being updated by an `Optimizer`\n-      (see `keras.constraints`).\n-    bias_constraint: Optional projection function to be applied to the\n-      bias after being updated by an `Optimizer`\n-      (see `keras.constraints`).\n-    trainable: Boolean, if `True` the weights of this layer will be marked as\n-      trainable (and listed in `layer.trainable_weights`).\n-\n-  Input shape:\n-    3D tensor with shape:\n-    `(batch_size, channels, steps)` if data_format='channels_first'\n-    or 3D tensor with shape:\n-    `(batch_size, steps, channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    3D tensor with shape:\n-    `(batch_size, filters, new_steps)` if data_format='channels_first'\n-    or 3D tensor with shape:\n-    `(batch_size,  new_steps, filters)` if data_format='channels_last'.\n-    `new_steps` value might have changed due to padding or strides.\n-\n-  Returns:\n-    A tensor of rank 3 representing\n-    `activation(separableconv1d(inputs, kernel) + bias)`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=1,\n-               depth_multiplier=1,\n-               activation=None,\n-               use_bias=True,\n-               depthwise_initializer='glorot_uniform',\n-               pointwise_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               depthwise_regularizer=None,\n-               pointwise_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               depthwise_constraint=None,\n-               pointwise_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(SeparableConv1D, self).__init__(\n-        rank=1,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        depth_multiplier=depth_multiplier,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        depthwise_initializer=initializers.get(depthwise_initializer),\n-        pointwise_initializer=initializers.get(pointwise_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        depthwise_regularizer=regularizers.get(depthwise_regularizer),\n-        pointwise_regularizer=regularizers.get(pointwise_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        depthwise_constraint=constraints.get(depthwise_constraint),\n-        pointwise_constraint=constraints.get(pointwise_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-  def call(self, inputs):\n-    if self.padding == 'causal':\n-      inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n-    if self.data_format == 'channels_last':\n-      strides = (1,) + self.strides * 2 + (1,)\n-      spatial_start_dim = 1\n-    else:\n-      strides = (1, 1) + self.strides * 2\n-      spatial_start_dim = 2\n-\n-    # Explicitly broadcast inputs and kernels to 4D.\n-    # TODO(fchollet): refactor when a native separable_conv1d op is available.\n-    inputs = tf.expand_dims(inputs, spatial_start_dim)\n-    depthwise_kernel = tf.expand_dims(self.depthwise_kernel, 0)\n-    pointwise_kernel = tf.expand_dims(self.pointwise_kernel, 0)\n-    dilation_rate = (1,) + self.dilation_rate\n-\n-    if self.padding == 'causal':\n-      op_padding = 'valid'\n-    else:\n-      op_padding = self.padding\n-    outputs = tf.compat.v1.nn.separable_conv2d(\n-        inputs,\n-        depthwise_kernel,\n-        pointwise_kernel,\n-        strides=strides,\n-        padding=op_padding.upper(),\n-        rate=dilation_rate,\n-        data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    outputs = tf.squeeze(outputs, [spatial_start_dim])\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-    return outputs\n-\n-\n-@keras_export('keras.layers.SeparableConv2D',\n-              'keras.layers.SeparableConvolution2D')\n-class SeparableConv2D(SeparableConv):\n-  \"\"\"Depthwise separable 2D convolution.\n-\n-  Separable convolutions consist of first performing\n-  a depthwise spatial convolution\n-  (which acts on each input channel separately)\n-  followed by a pointwise convolution which mixes the resulting\n-  output channels. The `depth_multiplier` argument controls how many\n-  output channels are generated per input channel in the depthwise step.\n-\n-  Intuitively, separable convolutions can be understood as\n-  a way to factorize a convolution kernel into two smaller kernels,\n-  or as an extreme version of an Inception block.\n-\n-  Args:\n-    filters: Integer, the dimensionality of the output space\n-      (i.e. the number of output filters in the convolution).\n-    kernel_size: An integer or tuple/list of 2 integers, specifying the\n-      height and width of the 2D convolution window.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    strides: An integer or tuple/list of 2 integers,\n-      specifying the strides of the convolution along the height and width.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions. Current implementation only supports equal\n-      length strides in the row and column dimensions.\n-      Specifying any stride value != 1 is incompatible with specifying\n-      any `dilation_rate` value != 1.\n-    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n-      to the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch_size, channels, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    dilation_rate: An integer or tuple/list of 2 integers, specifying\n-      the dilation rate to use for dilated convolution.\n-    depth_multiplier: The number of depthwise convolution output channels\n-      for each input channel.\n-      The total number of depthwise convolution output\n-      channels will be equal to `filters_in * depth_multiplier`.\n-    activation: Activation function to use.\n-      If you don't specify anything, no activation is applied\n-      (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    depthwise_initializer: An initializer for the depthwise convolution kernel\n-      (see `keras.initializers`). If None, then the default initializer\n-      ('glorot_uniform') will be used.\n-    pointwise_initializer: An initializer for the pointwise convolution kernel\n-      (see `keras.initializers`). If None, then the default initializer\n-      ('glorot_uniform') will be used.\n-    bias_initializer: An initializer for the bias vector. If None, the default\n-      initializer ('zeros') will be used (see `keras.initializers`).\n-    depthwise_regularizer: Regularizer function applied to\n-      the depthwise kernel matrix (see `keras.regularizers`).\n-    pointwise_regularizer: Regularizer function applied to\n-      the pointwise kernel matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector\n-      (see `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to\n-      the output of the layer (its \"activation\")\n-      (see `keras.regularizers`).\n-    depthwise_constraint: Constraint function applied to\n-      the depthwise kernel matrix\n-      (see `keras.constraints`).\n-    pointwise_constraint: Constraint function applied to\n-      the pointwise kernel matrix\n-      (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector\n-      (see `keras.constraints`).\n-\n-  Input shape:\n-    4D tensor with shape:\n-    `(batch_size, channels, rows, cols)` if data_format='channels_first'\n-    or 4D tensor with shape:\n-    `(batch_size, rows, cols, channels)` if data_format='channels_last'.\n-\n-  Output shape:\n-    4D tensor with shape:\n-    `(batch_size, filters, new_rows, new_cols)` if data_format='channels_first'\n-    or 4D tensor with shape:\n-    `(batch_size, new_rows, new_cols, filters)` if data_format='channels_last'.\n-    `rows` and `cols` values might have changed due to padding.\n-\n-  Returns:\n-    A tensor of rank 4 representing\n-    `activation(separableconv2d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-  \"\"\"\n-\n-  def __init__(self,\n-               filters,\n-               kernel_size,\n-               strides=(1, 1),\n-               padding='valid',\n-               data_format=None,\n-               dilation_rate=(1, 1),\n-               depth_multiplier=1,\n-               activation=None,\n-               use_bias=True,\n-               depthwise_initializer='glorot_uniform',\n-               pointwise_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               depthwise_regularizer=None,\n-               pointwise_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               depthwise_constraint=None,\n-               pointwise_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(SeparableConv2D, self).__init__(\n-        rank=2,\n-        filters=filters,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        depth_multiplier=depth_multiplier,\n-        activation=activations.get(activation),\n-        use_bias=use_bias,\n-        depthwise_initializer=initializers.get(depthwise_initializer),\n-        pointwise_initializer=initializers.get(pointwise_initializer),\n-        bias_initializer=initializers.get(bias_initializer),\n-        depthwise_regularizer=regularizers.get(depthwise_regularizer),\n-        pointwise_regularizer=regularizers.get(pointwise_regularizer),\n-        bias_regularizer=regularizers.get(bias_regularizer),\n-        activity_regularizer=regularizers.get(activity_regularizer),\n-        depthwise_constraint=constraints.get(depthwise_constraint),\n-        pointwise_constraint=constraints.get(pointwise_constraint),\n-        bias_constraint=constraints.get(bias_constraint),\n-        **kwargs)\n-\n-  def call(self, inputs):\n-    # Apply the actual ops.\n-    if self.data_format == 'channels_last':\n-      strides = (1,) + self.strides + (1,)\n-    else:\n-      strides = (1, 1) + self.strides\n-    outputs = tf.compat.v1.nn.separable_conv2d(\n-        inputs,\n-        self.depthwise_kernel,\n-        self.pointwise_kernel,\n-        strides=strides,\n-        padding=self.padding.upper(),\n-        rate=self.dilation_rate,\n-        data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-    return outputs\n-\n-\n-class DepthwiseConv(Conv):\n-  \"\"\"Depthwise convolution.\n-\n-  Depthwise convolution is a type of convolution in which each input channel is\n-  convolved with a different kernel (called a depthwise kernel). You\n-  can understand depthwise convolution as the first step in a depthwise\n-  separable convolution.\n-\n-  It is implemented via the following steps:\n-\n-  - Split the input into individual channels.\n-  - Convolve each channel with an individual depthwise kernel with\n-    `depth_multiplier` output channels.\n-  - Concatenate the convolved outputs along the channels axis.\n-\n-  Unlike a regular convolution, depthwise convolution does not mix\n-  information across different input channels.\n-\n-  The `depth_multiplier` argument determines how many filter are applied to one\n-  input channel. As such, it controls the amount of output channels that are\n-  generated per input channel in the depthwise step.\n-\n-  Args:\n-    kernel_size: A tuple or list of integers specifying the spatial dimensions\n-      of the filters. Can be a single integer to specify the same value for all\n-      spatial dimensions.\n-    strides: A tuple or list of integers specifying the strides of the\n-      convolution. Can be a single integer to specify the same value for all\n-      spatial dimensions. Specifying any `stride` value != 1 is incompatible\n-      with specifying any `dilation_rate` value != 1.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding with zeros evenly to the left/right\n-      or up/down of the input such that output has the same height/width\n-      dimension as the input.\n-    depth_multiplier: The number of depthwise convolution output channels for\n-      each input channel. The total number of depthwise convolution output\n-      channels will be equal to `filters_in * depth_multiplier`.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch_size, height, width, channels)` while\n-      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-      height, width)`. It defaults to the `image_data_format` value found in\n-      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-      it will be 'channels_last'.\n-    dilation_rate: An integer or tuple/list of 2 integers, specifying the\n-      dilation rate to use for dilated convolution. Currently, specifying any\n-      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n-      value != 1.\n-    activation: Activation function to use. If you don't specify anything, no\n-      activation is applied (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    depthwise_initializer: Initializer for the depthwise kernel matrix (see\n-      `keras.initializers`). If None, the default initializer\n-      ('glorot_uniform') will be used.\n-    bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`). If None, the default initializer ('zeros') will be\n-      used.\n-    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n-      matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector (see\n-      `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its 'activation') (see `keras.regularizers`).\n-    depthwise_constraint: Constraint function applied to the depthwise kernel\n-      matrix (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector (see\n-      `keras.constraints`).\n-\n-  Input shape:\n-    4D tensor with shape: `[batch_size, channels, rows, cols]` if\n-      data_format='channels_first'\n-    or 4D tensor with shape: `[batch_size, rows, cols, channels]` if\n-      data_format='channels_last'.\n-\n-  Output shape:\n-    4D tensor with shape: `[batch_size, channels * depth_multiplier, new_rows,\n-      new_cols]` if `data_format='channels_first'`\n-      or 4D tensor with shape: `[batch_size,\n-      new_rows, new_cols, channels * depth_multiplier]` if\n-      `data_format='channels_last'`. `rows` and `cols` values might have changed\n-      due to padding.\n-\n-  Returns:\n-    A tensor of rank 4 representing\n-    `activation(depthwiseconv2d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n-  \"\"\"\n-\n-  def __init__(self,\n-               rank,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               depth_multiplier=1,\n-               data_format=None,\n-               dilation_rate=1,\n-               activation=None,\n-               use_bias=True,\n-               depthwise_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               depthwise_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               depthwise_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(DepthwiseConv, self).__init__(\n-        rank,\n-        filters=None,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        use_bias=use_bias,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        bias_constraint=bias_constraint,\n-        **kwargs)\n-    self.depth_multiplier = depth_multiplier\n-    self.depthwise_initializer = initializers.get(depthwise_initializer)\n-    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n-    self.depthwise_constraint = constraints.get(depthwise_constraint)\n-    self.bias_initializer = initializers.get(bias_initializer)\n-\n-  def build(self, input_shape):\n-    if len(input_shape) != self.rank + 2:\n-      raise ValueError('Inputs to `DepthwiseConv` should have '\n-                       f'rank {self.rank + 2}. '\n-                       f'Received input_shape={input_shape}.')\n-    input_shape = tf.TensorShape(input_shape)\n-    channel_axis = self._get_channel_axis()\n-    if input_shape.dims[channel_axis].value is None:\n-      raise ValueError('The channel dimension of the inputs to `DepthwiseConv` '\n-                       'should be defined. '\n-                       f'The input_shape received is {input_shape}, '\n-                       f'where axis {channel_axis} (0-based) '\n-                       'is the channel dimension, which found to be `None`.')\n-    input_dim = int(input_shape[channel_axis])\n-    depthwise_kernel_shape = self.kernel_size + (input_dim,\n-                                                 self.depth_multiplier)\n-\n-    self.depthwise_kernel = self.add_weight(\n-        shape=depthwise_kernel_shape,\n-        initializer=self.depthwise_initializer,\n-        name='depthwise_kernel',\n-        regularizer=self.depthwise_regularizer,\n-        constraint=self.depthwise_constraint)\n-\n-    if self.use_bias:\n-      self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\n-                                  initializer=self.bias_initializer,\n-                                  name='bias',\n-                                  regularizer=self.bias_regularizer,\n-                                  constraint=self.bias_constraint)\n-    else:\n-      self.bias = None\n-    # Set input spec.\n-    self.input_spec = InputSpec(\n-        min_ndim=self.rank + 2, axes={channel_axis: input_dim})\n-    self.built = True\n-\n-  def call(self, inputs):\n-    raise NotImplementedError\n-\n-  def get_config(self):\n-    config = super(DepthwiseConv, self).get_config()\n-    config.pop('filters')\n-    config.pop('kernel_initializer')\n-    config.pop('kernel_regularizer')\n-    config.pop('kernel_constraint')\n-    config['depth_multiplier'] = self.depth_multiplier\n-    config['depthwise_initializer'] = initializers.serialize(\n-        self.depthwise_initializer)\n-    config['depthwise_regularizer'] = regularizers.serialize(\n-        self.depthwise_regularizer)\n-    config['depthwise_constraint'] = constraints.serialize(\n-        self.depthwise_constraint)\n-    return config\n-\n-\n-@keras_export('keras.layers.DepthwiseConv1D')\n-class DepthwiseConv1D(DepthwiseConv):\n-  \"\"\"Depthwise 1D convolution.\n-\n-  Depthwise convolution is a type of convolution in which each input channel is\n-  convolved with a different kernel (called a depthwise kernel). You\n-  can understand depthwise convolution as the first step in a depthwise\n-  separable convolution.\n-\n-  It is implemented via the following steps:\n-\n-  - Split the input into individual channels.\n-  - Convolve each channel with an individual depthwise kernel with\n-    `depth_multiplier` output channels.\n-  - Concatenate the convolved outputs along the channels axis.\n-\n-  Unlike a regular 1D convolution, depthwise convolution does not mix\n-  information across different input channels.\n-\n-  The `depth_multiplier` argument determines how many filter are applied to one\n-  input channel. As such, it controls the amount of output channels that are\n-  generated per input channel in the depthwise step.\n-\n-  Args:\n-    kernel_size: An integer, specifying the height and width of the 1D\n-      convolution window. Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    strides: An integer, specifying the strides of the convolution along the\n-      height and width. Can be a single integer to specify the same value for\n-      all spatial dimensions. Specifying any stride value != 1 is incompatible\n-      with specifying any `dilation_rate` value != 1.\n-    padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding with zeros evenly to the left/right\n-      or up/down of the input such that output has the same height/width\n-      dimension as the input.\n-    depth_multiplier: The number of depthwise convolution output channels for\n-      each input channel. The total number of depthwise convolution output\n-      channels will be equal to `filters_in * depth_multiplier`.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch_size, height, width, channels)` while\n-      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-      height, width)`. It defaults to the `image_data_format` value found in\n-      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-      it will be 'channels_last'.\n-    dilation_rate: A single integer, specifying the dilation rate to use for\n-      dilated convolution. Currently, specifying any `dilation_rate` value != 1\n-      is incompatible with specifying any stride value != 1.\n-    activation: Activation function to use. If you don't specify anything, no\n-      activation is applied (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    depthwise_initializer: Initializer for the depthwise kernel matrix (see\n-      `keras.initializers`). If None, the default initializer\n-      ('glorot_uniform') will be used.\n-    bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`). If None, the default initializer ('zeros') will be\n-      used.\n-    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n-      matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector (see\n-      `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its 'activation') (see `keras.regularizers`).\n-    depthwise_constraint: Constraint function applied to the depthwise kernel\n-      matrix (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector (see\n-      `keras.constraints`).\n-\n-  Input shape:\n-    4D tensor with shape: `[batch_size, channels, rows, cols]` if\n-      data_format='channels_first'\n-    or 4D tensor with shape: `[batch_size, rows, cols, channels]` if\n-      data_format='channels_last'.\n-\n-  Output shape:\n-    4D tensor with shape: `[batch_size, channels * depth_multiplier, new_rows,\n-      new_cols]` if `data_format='channels_first'`\n-      or 4D tensor with shape: `[batch_size,\n-      new_rows, new_cols, channels * depth_multiplier]` if\n-      `data_format='channels_last'`. `rows` and `cols` values might have changed\n-      due to padding.\n-\n-  Returns:\n-    A tensor of rank 4 representing\n-    `activation(depthwiseconv2d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n-  \"\"\"\n-\n-  def __init__(self,\n-               kernel_size,\n-               strides=1,\n-               padding='valid',\n-               depth_multiplier=1,\n-               data_format=None,\n-               dilation_rate=1,\n-               activation=None,\n-               use_bias=True,\n-               depthwise_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               depthwise_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               depthwise_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(DepthwiseConv1D, self).__init__(\n-        1,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        depth_multiplier=depth_multiplier,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        use_bias=use_bias,\n-        depthwise_initializer=depthwise_initializer,\n-        bias_initializer=bias_initializer,\n-        depthwise_regularizer=depthwise_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        depthwise_constraint=depthwise_constraint,\n-        bias_constraint=bias_constraint,\n-        **kwargs)\n-\n-  def call(self, inputs):\n-    if self.data_format == 'channels_last':\n-      strides = (1,) + self.strides * 2 + (1,)\n-      spatial_start_dim = 1\n-    else:\n-      strides = (1, 1) + self.strides * 2\n-      spatial_start_dim = 2\n-    inputs = tf.expand_dims(inputs, spatial_start_dim)\n-    depthwise_kernel = tf.expand_dims(self.depthwise_kernel, axis=0)\n-    dilation_rate = (1,) + self.dilation_rate\n-\n-    outputs = tf.nn.depthwise_conv2d(\n-        inputs,\n-        depthwise_kernel,\n-        strides=strides,\n-        padding=self.padding.upper(),\n-        dilations=dilation_rate,\n-        data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    if self.use_bias:\n-      outputs = tf.nn.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n-\n-    outputs = tf.squeeze(outputs, [spatial_start_dim])\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-\n-    return outputs\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if self.data_format == 'channels_first':\n-      rows = input_shape[2]\n-      out_filters = input_shape[1] * self.depth_multiplier\n-    elif self.data_format == 'channels_last':\n-      rows = input_shape[1]\n-      out_filters = input_shape[2] * self.depth_multiplier\n-\n-    rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n-                                         self.padding, self.strides[0],\n-                                         self.dilation_rate[0])\n-    if self.data_format == 'channels_first':\n-      return (input_shape[0], out_filters, rows)\n-    elif self.data_format == 'channels_last':\n-      return (input_shape[0], rows, out_filters)\n-\n-\n-@keras_export('keras.layers.DepthwiseConv2D')\n-class DepthwiseConv2D(DepthwiseConv):\n-  \"\"\"Depthwise 2D convolution.\n-\n-  Depthwise convolution is a type of convolution in which each input channel is\n-  convolved with a different kernel (called a depthwise kernel). You\n-  can understand depthwise convolution as the first step in a depthwise\n-  separable convolution.\n-\n-  It is implemented via the following steps:\n-\n-  - Split the input into individual channels.\n-  - Convolve each channel with an individual depthwise kernel with\n-    `depth_multiplier` output channels.\n-  - Concatenate the convolved outputs along the channels axis.\n-\n-  Unlike a regular 2D convolution, depthwise convolution does not mix\n-  information across different input channels.\n-\n-  The `depth_multiplier` argument determines how many filter are applied to one\n-  input channel. As such, it controls the amount of output channels that are\n-  generated per input channel in the depthwise step.\n-\n-  Args:\n-    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n-      and width of the 2D convolution window. Can be a single integer to specify\n-      the same value for all spatial dimensions.\n-    strides: An integer or tuple/list of 2 integers, specifying the strides of\n-      the convolution along the height and width. Can be a single integer to\n-      specify the same value for all spatial dimensions. Specifying any stride\n-      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n-    padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means no\n-      padding. `\"same\"` results in padding with zeros evenly to the left/right\n-      or up/down of the input such that output has the same height/width\n-      dimension as the input.\n-    depth_multiplier: The number of depthwise convolution output channels for\n-      each input channel. The total number of depthwise convolution output\n-      channels will be equal to `filters_in * depth_multiplier`.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs. `channels_last` corresponds\n-      to inputs with shape `(batch_size, height, width, channels)` while\n-      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n-      height, width)`. It defaults to the `image_data_format` value found in\n-      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n-      it will be 'channels_last'.\n-    dilation_rate: An integer or tuple/list of 2 integers, specifying the\n-      dilation rate to use for dilated convolution. Currently, specifying any\n-      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n-      value != 1.\n-    activation: Activation function to use. If you don't specify anything, no\n-      activation is applied (see `keras.activations`).\n-    use_bias: Boolean, whether the layer uses a bias vector.\n-    depthwise_initializer: Initializer for the depthwise kernel matrix (see\n-      `keras.initializers`). If None, the default initializer\n-      ('glorot_uniform') will be used.\n-    bias_initializer: Initializer for the bias vector (see\n-      `keras.initializers`). If None, the default initializer ('zeros') will be\n-      used.\n-    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n-      matrix (see `keras.regularizers`).\n-    bias_regularizer: Regularizer function applied to the bias vector (see\n-      `keras.regularizers`).\n-    activity_regularizer: Regularizer function applied to the output of the\n-      layer (its 'activation') (see `keras.regularizers`).\n-    depthwise_constraint: Constraint function applied to the depthwise kernel\n-      matrix (see `keras.constraints`).\n-    bias_constraint: Constraint function applied to the bias vector (see\n-      `keras.constraints`).\n-\n-  Input shape:\n-    4D tensor with shape: `[batch_size, channels, rows, cols]` if\n-      data_format='channels_first'\n-    or 4D tensor with shape: `[batch_size, rows, cols, channels]` if\n-      data_format='channels_last'.\n-\n-  Output shape:\n-    4D tensor with shape: `[batch_size, channels * depth_multiplier, new_rows,\n-      new_cols]` if `data_format='channels_first'`\n-      or 4D tensor with shape: `[batch_size,\n-      new_rows, new_cols, channels * depth_multiplier]` if\n-      `data_format='channels_last'`. `rows` and `cols` values might have changed\n-      due to padding.\n-\n-  Returns:\n-    A tensor of rank 4 representing\n-    `activation(depthwiseconv2d(inputs, kernel) + bias)`.\n-\n-  Raises:\n-    ValueError: if `padding` is \"causal\".\n-    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n-  \"\"\"\n-\n-  def __init__(self,\n-               kernel_size,\n-               strides=(1, 1),\n-               padding='valid',\n-               depth_multiplier=1,\n-               data_format=None,\n-               dilation_rate=(1, 1),\n-               activation=None,\n-               use_bias=True,\n-               depthwise_initializer='glorot_uniform',\n-               bias_initializer='zeros',\n-               depthwise_regularizer=None,\n-               bias_regularizer=None,\n-               activity_regularizer=None,\n-               depthwise_constraint=None,\n-               bias_constraint=None,\n-               **kwargs):\n-    super(DepthwiseConv2D, self).__init__(\n-        2,\n-        kernel_size=kernel_size,\n-        strides=strides,\n-        padding=padding,\n-        depth_multiplier=depth_multiplier,\n-        data_format=data_format,\n-        dilation_rate=dilation_rate,\n-        activation=activation,\n-        use_bias=use_bias,\n-        depthwise_initializer=depthwise_initializer,\n-        bias_initializer=bias_initializer,\n-        depthwise_regularizer=depthwise_regularizer,\n-        bias_regularizer=bias_regularizer,\n-        activity_regularizer=activity_regularizer,\n-        depthwise_constraint=depthwise_constraint,\n-        bias_constraint=bias_constraint,\n-        **kwargs)\n-\n-  def call(self, inputs):\n-    outputs = backend.depthwise_conv2d(\n-        inputs,\n-        self.depthwise_kernel,\n-        strides=self.strides,\n-        padding=self.padding,\n-        dilation_rate=self.dilation_rate,\n-        data_format=self.data_format)\n-\n-    if self.use_bias:\n-      outputs = backend.bias_add(\n-          outputs,\n-          self.bias,\n-          data_format=self.data_format)\n-\n-    if self.activation is not None:\n-      return self.activation(outputs)\n-\n-    return outputs\n-\n-  @tf_utils.shape_type_conversion\n-  def compute_output_shape(self, input_shape):\n-    if self.data_format == 'channels_first':\n-      rows = input_shape[2]\n-      cols = input_shape[3]\n-      out_filters = input_shape[1] * self.depth_multiplier\n-    elif self.data_format == 'channels_last':\n-      rows = input_shape[1]\n-      cols = input_shape[2]\n-      out_filters = input_shape[3] * self.depth_multiplier\n-\n-    rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n-                                         self.padding,\n-                                         self.strides[0],\n-                                         self.dilation_rate[0])\n-    cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n-                                         self.padding,\n-                                         self.strides[1],\n-                                         self.dilation_rate[1])\n-    if self.data_format == 'channels_first':\n-      return (input_shape[0], out_filters, rows, cols)\n-    elif self.data_format == 'channels_last':\n-      return (input_shape[0], rows, cols, out_filters)\n-\n-\n-@keras_export('keras.layers.UpSampling1D')\n-class UpSampling1D(Layer):\n-  \"\"\"Upsampling layer for 1D inputs.\n-\n-  Repeats each temporal step `size` times along the time axis.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 2, 3)\n-  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-  >>> print(x)\n-  [[[ 0  1  2]\n-    [ 3  4  5]]\n-   [[ 6  7  8]\n-    [ 9 10 11]]]\n-  >>> y = tf.keras.layers.UpSampling1D(size=2)(x)\n-  >>> print(y)\n-  tf.Tensor(\n-    [[[ 0  1  2]\n-      [ 0  1  2]\n-      [ 3  4  5]\n-      [ 3  4  5]]\n-     [[ 6  7  8]\n-      [ 6  7  8]\n-      [ 9 10 11]\n-      [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)\n-\n-  Args:\n-    size: Integer. Upsampling factor.\n-\n-  Input shape:\n-    3D tensor with shape: `(batch_size, steps, features)`.\n-\n-  Output shape:\n-    3D tensor with shape: `(batch_size, upsampled_steps, features)`.\n-  \"\"\"\n-\n-  def __init__(self, size=2, **kwargs):\n-    super(UpSampling1D, self).__init__(**kwargs)\n-    self.size = int(size)\n-    self.input_spec = InputSpec(ndim=3)\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    size = self.size * input_shape[1] if input_shape[1] is not None else None\n-    return tf.TensorShape([input_shape[0], size, input_shape[2]])\n-\n-  def call(self, inputs):\n-    output = backend.repeat_elements(inputs, self.size, axis=1)\n-    return output\n-\n-  def get_config(self):\n-    config = {'size': self.size}\n-    base_config = super(UpSampling1D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.UpSampling2D')\n-class UpSampling2D(Layer):\n-  \"\"\"Upsampling layer for 2D inputs.\n-\n-  Repeats the rows and columns of the data\n-  by `size[0]` and `size[1]` respectively.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 2, 1, 3)\n-  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-  >>> print(x)\n-  [[[[ 0  1  2]]\n-    [[ 3  4  5]]]\n-   [[[ 6  7  8]]\n-    [[ 9 10 11]]]]\n-  >>> y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)\n-  >>> print(y)\n-  tf.Tensor(\n-    [[[[ 0  1  2]\n-       [ 0  1  2]]\n-      [[ 3  4  5]\n-       [ 3  4  5]]]\n-     [[[ 6  7  8]\n-       [ 6  7  8]]\n-      [[ 9 10 11]\n-       [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)\n-\n-  Args:\n-    size: Int, or tuple of 2 integers.\n-      The upsampling factors for rows and columns.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch_size, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch_size, channels, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    interpolation: A string, one of `nearest` or `bilinear`.\n-\n-  Input shape:\n-    4D tensor with shape:\n-    - If `data_format` is `\"channels_last\"`:\n-        `(batch_size, rows, cols, channels)`\n-    - If `data_format` is `\"channels_first\"`:\n-        `(batch_size, channels, rows, cols)`\n-\n-  Output shape:\n-    4D tensor with shape:\n-    - If `data_format` is `\"channels_last\"`:\n-        `(batch_size, upsampled_rows, upsampled_cols, channels)`\n-    - If `data_format` is `\"channels_first\"`:\n-        `(batch_size, channels, upsampled_rows, upsampled_cols)`\n-  \"\"\"\n-\n-  def __init__(self,\n-               size=(2, 2),\n-               data_format=None,\n-               interpolation='nearest',\n-               **kwargs):\n-    super(UpSampling2D, self).__init__(**kwargs)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.size = conv_utils.normalize_tuple(size, 2, 'size')\n-    if interpolation not in {'nearest', 'bilinear'}:\n-      raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\n-                       f'or `\"bilinear\"`. Received: \"{interpolation}\".')\n-    self.interpolation = interpolation\n-    self.input_spec = InputSpec(ndim=4)\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_first':\n-      height = self.size[0] * input_shape[\n-          2] if input_shape[2] is not None else None\n-      width = self.size[1] * input_shape[\n-          3] if input_shape[3] is not None else None\n-      return tf.TensorShape(\n-          [input_shape[0], input_shape[1], height, width])\n-    else:\n-      height = self.size[0] * input_shape[\n-          1] if input_shape[1] is not None else None\n-      width = self.size[1] * input_shape[\n-          2] if input_shape[2] is not None else None\n-      return tf.TensorShape(\n-          [input_shape[0], height, width, input_shape[3]])\n-\n-  def call(self, inputs):\n-    return backend.resize_images(\n-        inputs, self.size[0], self.size[1], self.data_format,\n-        interpolation=self.interpolation)\n-\n-  def get_config(self):\n-    config = {\n-        'size': self.size,\n-        'data_format': self.data_format,\n-        'interpolation': self.interpolation\n-    }\n-    base_config = super(UpSampling2D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.UpSampling3D')\n-class UpSampling3D(Layer):\n-  \"\"\"Upsampling layer for 3D inputs.\n-\n-  Repeats the 1st, 2nd and 3rd dimensions\n-  of the data by `size[0]`, `size[1]` and `size[2]` respectively.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 1, 2, 1, 3)\n-  >>> x = tf.constant(1, shape=input_shape)\n-  >>> y = tf.keras.layers.UpSampling3D(size=2)(x)\n-  >>> print(y.shape)\n-  (2, 2, 4, 2, 3)\n-\n-  Args:\n-    size: Int, or tuple of 3 integers.\n-      The upsampling factors for dim1, dim2 and dim3.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n@@ -0,0 +1,57 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras convolution layers.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+# Convolution layers.\n+from keras.layers.convolutional.conv1d import Conv1D\n+from keras.layers.convolutional.conv2d import Conv2D\n+from keras.layers.convolutional.conv3d import Conv3D\n+from keras.layers.convolutional.conv1d_transpose import Conv1DTranspose\n+from keras.layers.convolutional.conv2d_transpose import Conv2DTranspose\n+from keras.layers.convolutional.conv3d_transpose import Conv3DTranspose\n+from keras.layers.convolutional.depthwise_conv1d import DepthwiseConv1D\n+from keras.layers.convolutional.depthwise_conv2d import DepthwiseConv2D\n+from keras.layers.convolutional.separable_conv1d import SeparableConv1D\n+from keras.layers.convolutional.separable_conv2d import SeparableConv2D\n+\n+# Convolution layer aliases.\n+from keras.layers.convolutional.conv1d import Convolution1D\n+from keras.layers.convolutional.conv2d import Convolution2D\n+from keras.layers.convolutional.conv3d import Convolution3D\n+from keras.layers.convolutional.conv1d_transpose import Convolution1DTranspose\n+from keras.layers.convolutional.conv2d_transpose import Convolution2DTranspose\n+from keras.layers.convolutional.conv3d_transpose import Convolution3DTranspose\n+from keras.layers.convolutional.separable_conv1d import SeparableConvolution1D\n+from keras.layers.convolutional.separable_conv2d import SeparableConvolution2D\n+\n+# Pooling layers imported for backwards namespace compatibility.\n+from keras.layers.pooling import AveragePooling1D\n+from keras.layers.pooling import AveragePooling2D\n+from keras.layers.pooling import AveragePooling3D\n+from keras.layers.pooling import MaxPooling1D\n+from keras.layers.pooling import MaxPooling2D\n+from keras.layers.pooling import MaxPooling3D\n+\n+# Reshaping layers imported for backwards namespace compatibility\n+from keras.layers.reshaping.cropping1d import Cropping1D\n+from keras.layers.reshaping.cropping2d import Cropping2D\n+from keras.layers.reshaping.cropping3d import Cropping3D\n+from keras.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras.layers.reshaping.up_sampling2d import UpSampling2D\n+from keras.layers.reshaping.up_sampling3d import UpSampling3D\n+from keras.layers.reshaping.zero_padding1d import ZeroPadding1D\n+from keras.layers.reshaping.zero_padding2d import ZeroPadding2D\n+from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n\n@@ -0,0 +1,390 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras base class for convolution layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class Conv(Layer):\n+  \"\"\"Abstract N-D convolution layer (private, used as implementation base).\n+\n+  This layer creates a convolution kernel that is convolved\n+  (actually cross-correlated) with the layer input to produce a tensor of\n+  outputs. If `use_bias` is True (and a `bias_initializer` is provided),\n+  a bias vector is created and added to the outputs. Finally, if\n+  `activation` is not `None`, it is applied to the outputs as well.\n+\n+  Note: layer attributes cannot be modified after the layer has been called\n+  once (except the `trainable` attribute).\n+\n+  Args:\n+    rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n+    filters: Integer, the dimensionality of the output space (i.e. the number\n+      of filters in the convolution). Could be \"None\", eg in the case of\n+      depth wise convolution.\n+    kernel_size: An integer or tuple/list of n integers, specifying the\n+      length of the convolution window.\n+    strides: An integer or tuple/list of n integers,\n+      specifying the stride length of the convolution.\n+      Specifying any stride value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"`,  `\"same\"`, or `\"causal\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros\n+      evenly to the left/right or up/down of the input such that output has the\n+      same height/width dimension as the input. `\"causal\"` results in causal\n+      (dilated) convolutions, e.g. `output[t]` does not depend on `input[t+1:]`.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, ..., channels)` while `channels_first` corresponds to\n+      inputs with shape `(batch_size, channels, ...)`.\n+    dilation_rate: An integer or tuple/list of n integers, specifying\n+      the dilation rate to use for dilated convolution.\n+      Currently, specifying any `dilation_rate` value != 1 is\n+      incompatible with specifying any `strides` value != 1.\n+    groups: A positive integer specifying the number of groups in which the\n+      input is split along the channel axis. Each group is convolved\n+      separately with `filters / groups` filters. The output is the\n+      concatenation of all the `groups` results along the channel axis.\n+      Input channels and `filters` must both be divisible by `groups`.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied.\n+    use_bias: Boolean, whether the layer uses a bias.\n+    kernel_initializer: An initializer for the convolution kernel. If None, the\n+      default initializer (glorot_uniform) will be used.\n+    bias_initializer: An initializer for the bias vector. If None, the default\n+      initializer (zeros) will be used.\n+    kernel_regularizer: Optional regularizer for the convolution kernel.\n+    bias_regularizer: Optional regularizer for the bias vector.\n+    activity_regularizer: Optional regularizer function for the output.\n+    kernel_constraint: Optional projection function to be applied to the\n+        kernel after being updated by an `Optimizer` (e.g. used to implement\n+        norm constraints or value constraints for layer weights). The function\n+        must take as input the unprojected variable and must return the\n+        projected variable (which must have the same shape). Constraints are\n+        not safe to use when doing asynchronous distributed training.\n+    bias_constraint: Optional projection function to be applied to the\n+        bias after being updated by an `Optimizer`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               rank,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               groups=1,\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               trainable=True,\n+               name=None,\n+               conv_op=None,\n+               **kwargs):\n+    super(Conv, self).__init__(\n+        trainable=trainable,\n+        name=name,\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        **kwargs)\n+    self.rank = rank\n+\n+    if isinstance(filters, float):\n+      filters = int(filters)\n+    if filters is not None and filters < 0:\n+      raise ValueError(f'Received a negative value for `filters`.'\n+                       f'Was expecting a positive value. Received {filters}.')\n+    self.filters = filters\n+    self.groups = groups or 1\n+    self.kernel_size = conv_utils.normalize_tuple(\n+        kernel_size, rank, 'kernel_size')\n+    self.strides = conv_utils.normalize_tuple(\n+        strides, rank, 'strides', allow_zero=True)\n+    self.padding = conv_utils.normalize_padding(padding)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.dilation_rate = conv_utils.normalize_tuple(\n+        dilation_rate, rank, 'dilation_rate')\n+\n+    self.activation = activations.get(activation)\n+    self.use_bias = use_bias\n+\n+    self.kernel_initializer = initializers.get(kernel_initializer)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+    self.kernel_regularizer = regularizers.get(kernel_regularizer)\n+    self.bias_regularizer = regularizers.get(bias_regularizer)\n+    self.kernel_constraint = constraints.get(kernel_constraint)\n+    self.bias_constraint = constraints.get(bias_constraint)\n+    self.input_spec = InputSpec(min_ndim=self.rank + 2)\n+\n+    self._validate_init()\n+    self._is_causal = self.padding == 'causal'\n+    self._channels_first = self.data_format == 'channels_first'\n+    self._tf_data_format = conv_utils.convert_data_format(\n+        self.data_format, self.rank + 2)\n+\n+  def _validate_init(self):\n+    if self.filters is not None and self.filters % self.groups != 0:\n+      raise ValueError(\n+          'The number of filters must be evenly divisible by the number of '\n+          'groups. Received: groups={}, filters={}'.format(\n+              self.groups, self.filters))\n+\n+    if not all(self.kernel_size):\n+      raise ValueError('The argument `kernel_size` cannot contain 0(s). '\n+                       'Received: %s' % (self.kernel_size,))\n+\n+    if not all(self.strides):\n+      raise ValueError('The argument `strides` cannot contains 0(s). '\n+                       'Received: %s' % (self.strides,))\n+\n+    if self.padding == 'causal':\n+      # pylint: disable=g-import-not-at-top\n+      from keras.layers.convolutional.conv1d import Conv1D\n+      from keras.layers.convolutional.separable_conv1d import SeparableConv1D\n+      # pylint: enable=g-import-not-at-top\n+      if not isinstance(self, (Conv1D, SeparableConv1D)):\n+        raise ValueError('Causal padding is only supported for `Conv1D`'\n+                         'and `SeparableConv1D`.')\n+\n+  def build(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape)\n+    input_channel = self._get_input_channel(input_shape)\n+    if input_channel % self.groups != 0:\n+      raise ValueError(\n+          'The number of input channels must be evenly divisible by the number '\n+          'of groups. Received groups={}, but the input has {} channels '\n+          '(full input shape is {}).'.format(self.groups, input_channel,\n+                                             input_shape))\n+    kernel_shape = self.kernel_size + (input_channel // self.groups,\n+                                       self.filters)\n+\n+    # compute_output_shape contains some validation logic for the input shape,\n+    # and make sure the output shape has all positive dimensions.\n+    self.compute_output_shape(input_shape)\n+\n+    self.kernel = self.add_weight(\n+        name='kernel',\n+        shape=kernel_shape,\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        trainable=True,\n+        dtype=self.dtype)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          name='bias',\n+          shape=(self.filters,),\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          trainable=True,\n+          dtype=self.dtype)\n+    else:\n+      self.bias = None\n+    channel_axis = self._get_channel_axis()\n+    self.input_spec = InputSpec(min_ndim=self.rank + 2,\n+                                axes={channel_axis: input_channel})\n+    self.built = True\n+\n+  def convolution_op(self, inputs, kernel):\n+    if self.padding == 'causal':\n+      tf_padding = 'VALID'  # Causal padding handled in `call`.\n+    elif isinstance(self.padding, str):\n+      tf_padding = self.padding.upper()\n+    else:\n+      tf_padding = self.padding\n+\n+    return tf.nn.convolution(\n+        inputs,\n+        kernel,\n+        strides=list(self.strides),\n+        padding=tf_padding,\n+        dilations=list(self.dilation_rate),\n+        data_format=self._tf_data_format,\n+        name=self.__class__.__name__)\n+\n+  # TODO(b/213173659): remove this when grouped convolutions are fully supported\n+  # on the CPU for compiled functions. For now, we need this as a workaround for\n+  # CPU support.\n+  @tf.function(jit_compile=True)\n+  def _jit_compiled_convolution_op(self, inputs, kernel):\n+    return self.convolution_op(inputs, kernel)\n+\n+  def call(self, inputs):\n+    input_shape = inputs.shape\n+\n+    if self._is_causal:  # Apply causal padding to inputs for Conv1D.\n+      inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n+\n+    if self.groups > 1:\n+      outputs = self._jit_compiled_convolution_op(inputs, self.kernel)\n+    else:\n+      outputs = self.convolution_op(inputs, self.kernel)\n+\n+    if self.use_bias:\n+      output_rank = outputs.shape.rank\n+      if self.rank == 1 and self._channels_first:\n+        # nn.bias_add does not accept a 1D input tensor.\n+        bias = tf.reshape(self.bias, (1, self.filters, 1))\n+        outputs += bias\n+      else:\n+        # Handle multiple batch dimensions.\n+        if output_rank is not None and output_rank > 2 + self.rank:\n+\n+          def _apply_fn(o):\n+            return tf.nn.bias_add(\n+                o, self.bias, data_format=self._tf_data_format)\n+\n+          outputs = conv_utils.squeeze_batch_dims(\n+              outputs, _apply_fn, inner_rank=self.rank + 1)\n+        else:\n+          outputs = tf.nn.bias_add(\n+              outputs, self.bias, data_format=self._tf_data_format)\n+\n+    if not tf.executing_eagerly():\n+      # Infer the static output shape:\n+      out_shape = self.compute_output_shape(input_shape)\n+      outputs.set_shape(out_shape)\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+  def _spatial_output_shape(self, spatial_input_shape):\n+    return [\n+        conv_utils.conv_output_length(  # pylint: disable=g-complex-comprehension\n+            length,\n+            self.kernel_size[i],\n+            padding=self.padding,\n+            stride=self.strides[i],\n+            dilation=self.dilation_rate[i])\n+        for i, length in enumerate(spatial_input_shape)\n+    ]\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    batch_rank = len(input_shape) - self.rank - 1\n+    try:\n+      if self.data_format == 'channels_last':\n+        return tf.TensorShape(\n+            input_shape[:batch_rank] +\n+            self._spatial_output_shape(input_shape[batch_rank:-1]) +\n+            [self.filters])\n+      else:\n+        return tf.TensorShape(\n+            input_shape[:batch_rank] + [self.filters] +\n+            self._spatial_output_shape(input_shape[batch_rank + 1:]))\n+\n+    except ValueError:\n+      raise ValueError(\n+          f'One of the dimensions in the output is <= 0 '\n+          f'due to downsampling in {self.name}. Consider '\n+          f'increasing the input size. '\n+          f'Received input shape {input_shape} which would produce '\n+          f'output shape with a zero or negative value in a '\n+          f'dimension.')\n+\n+  def _recreate_conv_op(self, inputs):  # pylint: disable=unused-argument\n+    return False\n+\n+  def get_config(self):\n+    config = {\n+        'filters':\n+            self.filters,\n+        'kernel_size':\n+            self.kernel_size,\n+        'strides':\n+            self.strides,\n+        'padding':\n+            self.padding,\n+        'data_format':\n+            self.data_format,\n+        'dilation_rate':\n+            self.dilation_rate,\n+        'groups':\n+            self.groups,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'use_bias':\n+            self.use_bias,\n+        'kernel_initializer':\n+            initializers.serialize(self.kernel_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'kernel_regularizer':\n+            regularizers.serialize(self.kernel_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'kernel_constraint':\n+            constraints.serialize(self.kernel_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint)\n+    }\n+    base_config = super(Conv, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def _compute_causal_padding(self, inputs):\n+    \"\"\"Calculates padding for 'causal' option for 1-d conv layers.\"\"\"\n+    left_pad = self.dilation_rate[0] * (self.kernel_size[0] - 1)\n+    if getattr(inputs.shape, 'ndims', None) is None:\n+      batch_rank = 1\n+    else:\n+      batch_rank = len(inputs.shape) - 2\n+    if self.data_format == 'channels_last':\n+      causal_padding = [[0, 0]] * batch_rank + [[left_pad, 0], [0, 0]]\n+    else:\n+      causal_padding = [[0, 0]] * batch_rank + [[0, 0], [left_pad, 0]]\n+    return causal_padding\n+\n+  def _get_channel_axis(self):\n+    if self.data_format == 'channels_first':\n+      return -1 - self.rank\n+    else:\n+      return -1\n+\n+  def _get_input_channel(self, input_shape):\n+    channel_axis = self._get_channel_axis()\n+    if input_shape.dims[channel_axis].value is None:\n+      raise ValueError('The channel dimension of the inputs should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n+    return int(input_shape[channel_axis])\n+\n+  def _get_padding_op(self):\n+    if self.padding == 'causal':\n+      op_padding = 'valid'\n+    else:\n+      op_padding = self.padding\n+    if not isinstance(op_padding, (list, tuple)):\n+      op_padding = op_padding.upper()\n+    return op_padding\n\n@@ -0,0 +1,208 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras abstract base for depthwise convolutions.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.convolutional.base_conv import Conv\n+import tensorflow.compat.v2 as tf\n+\n+\n+class DepthwiseConv(Conv):\n+  \"\"\"Depthwise convolution.\n+\n+  Depthwise convolution is a type of convolution in which each input channel is\n+  convolved with a different kernel (called a depthwise kernel). You\n+  can understand depthwise convolution as the first step in a depthwise\n+  separable convolution.\n+\n+  It is implemented via the following steps:\n+\n+  - Split the input into individual channels.\n+  - Convolve each channel with an individual depthwise kernel with\n+    `depth_multiplier` output channels.\n+  - Concatenate the convolved outputs along the channels axis.\n+\n+  Unlike a regular convolution, depthwise convolution does not mix\n+  information across different input channels.\n+\n+  The `depth_multiplier` argument determines how many filter are applied to one\n+  input channel. As such, it controls the amount of output channels that are\n+  generated per input channel in the depthwise step.\n+\n+  Args:\n+    kernel_size: A tuple or list of integers specifying the spatial dimensions\n+      of the filters. Can be a single integer to specify the same value for all\n+      spatial dimensions.\n+    strides: A tuple or list of integers specifying the strides of the\n+      convolution. Can be a single integer to specify the same value for all\n+      spatial dimensions. Specifying any `stride` value != 1 is incompatible\n+      with specifying any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding with zeros evenly to the left/right\n+      or up/down of the input such that output has the same height/width\n+      dimension as the input.\n+    depth_multiplier: The number of depthwise convolution output channels for\n+      each input channel. The total number of depthwise convolution output\n+      channels will be equal to `filters_in * depth_multiplier`.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch_size, height, width, channels)` while\n+      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n+      height, width)`. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be 'channels_last'.\n+    dilation_rate: An integer or tuple/list of 2 integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    depthwise_initializer: Initializer for the depthwise kernel matrix (see\n+      `keras.initializers`). If None, the default initializer\n+      ('glorot_uniform') will be used.\n+    bias_initializer: Initializer for the bias vector (see\n+      `keras.initializers`). If None, the default initializer ('zeros') will be\n+      used.\n+    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n+      matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector (see\n+      `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its 'activation') (see `keras.regularizers`).\n+    depthwise_constraint: Constraint function applied to the depthwise kernel\n+      matrix (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector (see\n+      `keras.constraints`).\n+\n+  Input shape:\n+    4D tensor with shape: `[batch_size, channels, rows, cols]` if\n+      data_format='channels_first'\n+    or 4D tensor with shape: `[batch_size, rows, cols, channels]` if\n+      data_format='channels_last'.\n+\n+  Output shape:\n+    4D tensor with shape: `[batch_size, channels * depth_multiplier, new_rows,\n+      new_cols]` if `data_format='channels_first'`\n+      or 4D tensor with shape: `[batch_size,\n+      new_rows, new_cols, channels * depth_multiplier]` if\n+      `data_format='channels_last'`. `rows` and `cols` values might have changed\n+      due to padding.\n+\n+  Returns:\n+    A tensor of rank 4 representing\n+    `activation(depthwiseconv2d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n+  \"\"\"\n+\n+  def __init__(self,\n+               rank,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               depth_multiplier=1,\n+               data_format=None,\n+               dilation_rate=1,\n+               activation=None,\n+               use_bias=True,\n+               depthwise_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               depthwise_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               depthwise_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(DepthwiseConv, self).__init__(\n+        rank,\n+        filters=None,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        use_bias=use_bias,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        bias_constraint=bias_constraint,\n+        **kwargs)\n+    self.depth_multiplier = depth_multiplier\n+    self.depthwise_initializer = initializers.get(depthwise_initializer)\n+    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n+    self.depthwise_constraint = constraints.get(depthwise_constraint)\n+    self.bias_initializer = initializers.get(bias_initializer)\n+\n+  def build(self, input_shape):\n+    if len(input_shape) != self.rank + 2:\n+      raise ValueError('Inputs to `DepthwiseConv` should have '\n+                       f'rank {self.rank + 2}. '\n+                       f'Received input_shape={input_shape}.')\n+    input_shape = tf.TensorShape(input_shape)\n+    channel_axis = self._get_channel_axis()\n+    if input_shape.dims[channel_axis].value is None:\n+      raise ValueError('The channel dimension of the inputs to `DepthwiseConv` '\n+                       'should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n+    input_dim = int(input_shape[channel_axis])\n+    depthwise_kernel_shape = self.kernel_size + (input_dim,\n+                                                 self.depth_multiplier)\n+\n+    self.depthwise_kernel = self.add_weight(\n+        shape=depthwise_kernel_shape,\n+        initializer=self.depthwise_initializer,\n+        name='depthwise_kernel',\n+        regularizer=self.depthwise_regularizer,\n+        constraint=self.depthwise_constraint)\n+\n+    if self.use_bias:\n+      self.bias = self.add_weight(shape=(input_dim * self.depth_multiplier,),\n+                                  initializer=self.bias_initializer,\n+                                  name='bias',\n+                                  regularizer=self.bias_regularizer,\n+                                  constraint=self.bias_constraint)\n+    else:\n+      self.bias = None\n+    # Set input spec.\n+    self.input_spec = InputSpec(\n+        min_ndim=self.rank + 2, axes={channel_axis: input_dim})\n+    self.built = True\n+\n+  def call(self, inputs):\n+    raise NotImplementedError\n+\n+  def get_config(self):\n+    config = super(DepthwiseConv, self).get_config()\n+    config.pop('filters')\n+    config.pop('kernel_initializer')\n+    config.pop('kernel_regularizer')\n+    config.pop('kernel_constraint')\n+    config['depth_multiplier'] = self.depth_multiplier\n+    config['depthwise_initializer'] = initializers.serialize(\n+        self.depthwise_initializer)\n+    config['depthwise_regularizer'] = regularizers.serialize(\n+        self.depthwise_regularizer)\n+    config['depthwise_constraint'] = constraints.serialize(\n+        self.depthwise_constraint)\n+    return config\n\n@@ -0,0 +1,237 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras abstract base layer for separable nD convolution.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.convolutional.base_conv import Conv\n+import tensorflow.compat.v2 as tf\n+\n+\n+class SeparableConv(Conv):\n+  \"\"\"Abstract base layer for separable nD convolution.\n+\n+  This layer performs a depthwise convolution that acts separately on\n+  channels, followed by a pointwise convolution that mixes channels.\n+  If `use_bias` is True and a bias initializer is provided,\n+  it adds a bias vector to the output.\n+  It then optionally applies an activation function to produce the final output.\n+\n+  Args:\n+    rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n+    filters: Integer, the dimensionality of the output space (i.e. the number\n+      of filters in the convolution).\n+    kernel_size: A tuple or list of integers specifying the spatial\n+      dimensions of the filters. Can be a single integer to specify the same\n+      value for all spatial dimensions.\n+    strides: A tuple or list of integers specifying the strides\n+      of the convolution. Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+      Specifying any `stride` value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, ..., channels)` while `channels_first` corresponds to\n+      inputs with shape `(batch_size, channels, ...)`.\n+    dilation_rate: An integer or tuple/list of 2 integers, specifying\n+      the dilation rate to use for dilated convolution.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+      Currently, specifying any `dilation_rate` value != 1 is\n+      incompatible with specifying any stride value != 1.\n+    depth_multiplier: The number of depthwise convolution output channels for\n+      each input channel. The total number of depthwise convolution output\n+      channels will be equal to `num_filters_in * depth_multiplier`.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias.\n+    depthwise_initializer: An initializer for the depthwise convolution kernel\n+      (see `keras.initializers`). If None, then the default initializer\n+      ('glorot_uniform') will be used.\n+    pointwise_initializer: An initializer for the pointwise convolution kernel\n+      (see `keras.initializers`). If None, then the default initializer\n+      ('glorot_uniform') will be used.\n+    bias_initializer: An initializer for the bias vector. If None, the default\n+      initializer ('zeros') will be used (see `keras.initializers`).\n+    depthwise_regularizer: Optional regularizer for the depthwise\n+      convolution kernel.\n+    pointwise_regularizer: Optional regularizer for the pointwise\n+      convolution kernel.\n+    bias_regularizer: Optional regularizer for the bias vector.\n+    activity_regularizer: Optional regularizer function for the output.\n+    depthwise_constraint: Optional projection function to be applied to the\n+      depthwise kernel after being updated by an `Optimizer` (e.g. used for\n+      norm constraints or value constraints for layer weights). The function\n+      must take as input the unprojected variable and must return the\n+      projected variable (which must have the same shape). Constraints are\n+      not safe to use when doing asynchronous distributed training.\n+    pointwise_constraint: Optional projection function to be applied to the\n+      pointwise kernel after being updated by an `Optimizer`.\n+    bias_constraint: Optional projection function to be applied to the\n+      bias after being updated by an `Optimizer`.\n+    trainable: Boolean, if `True` the weights of this layer will be marked as\n+      trainable (and listed in `layer.trainable_weights`).\n+  \"\"\"\n+\n+  def __init__(self,\n+               rank,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               depth_multiplier=1,\n+               activation=None,\n+               use_bias=True,\n+               depthwise_initializer='glorot_uniform',\n+               pointwise_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               depthwise_regularizer=None,\n+               pointwise_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               depthwise_constraint=None,\n+               pointwise_constraint=None,\n+               bias_constraint=None,\n+               trainable=True,\n+               name=None,\n+               **kwargs):\n+    super(SeparableConv, self).__init__(\n+        rank=rank,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        bias_initializer=initializers.get(bias_initializer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        bias_constraint=bias_constraint,\n+        trainable=trainable,\n+        name=name,\n+        **kwargs)\n+    self.depth_multiplier = depth_multiplier\n+    self.depthwise_initializer = initializers.get(depthwise_initializer)\n+    self.pointwise_initializer = initializers.get(pointwise_initializer)\n+    self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n+    self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n+    self.depthwise_constraint = constraints.get(depthwise_constraint)\n+    self.pointwise_constraint = constraints.get(pointwise_constraint)\n+\n+  def build(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape)\n+    channel_axis = self._get_channel_axis()\n+    if input_shape.dims[channel_axis].value is None:\n+      raise ValueError('The channel dimension of the inputs should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n+    input_dim = int(input_shape[channel_axis])\n+    self.input_spec = InputSpec(ndim=self.rank + 2,\n+                                axes={channel_axis: input_dim})\n+    depthwise_kernel_shape = self.kernel_size + (input_dim,\n+                                                 self.depth_multiplier)\n+    pointwise_kernel_shape = (\n+        1,) * self.rank + (self.depth_multiplier * input_dim, self.filters)\n+\n+    self.depthwise_kernel = self.add_weight(\n+        name='depthwise_kernel',\n+        shape=depthwise_kernel_shape,\n+        initializer=self.depthwise_initializer,\n+        regularizer=self.depthwise_regularizer,\n+        constraint=self.depthwise_constraint,\n+        trainable=True,\n+        dtype=self.dtype)\n+    self.pointwise_kernel = self.add_weight(\n+        name='pointwise_kernel',\n+        shape=pointwise_kernel_shape,\n+        initializer=self.pointwise_initializer,\n+        regularizer=self.pointwise_regularizer,\n+        constraint=self.pointwise_constraint,\n+        trainable=True,\n+        dtype=self.dtype)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          name='bias',\n+          shape=(self.filters,),\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          trainable=True,\n+          dtype=self.dtype)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs):\n+    raise NotImplementedError\n+\n+  def get_config(self):\n+    config = {\n+        'filters':\n+            self.filters,\n+        'kernel_size':\n+            self.kernel_size,\n+        'strides':\n+            self.strides,\n+        'padding':\n+            self.padding,\n+        'data_format':\n+            self.data_format,\n+        'depth_multiplier':\n+            self.depth_multiplier,\n+        'dilation_rate':\n+            self.dilation_rate,\n+        'activation':\n+            activations.serialize(self.activation),\n+        'use_bias':\n+            self.use_bias,\n+        'depthwise_initializer':\n+            initializers.serialize(self.depthwise_initializer),\n+        'pointwise_initializer':\n+            initializers.serialize(self.pointwise_initializer),\n+        'bias_initializer':\n+            initializers.serialize(self.bias_initializer),\n+        'depthwise_regularizer':\n+            regularizers.serialize(self.depthwise_regularizer),\n+        'pointwise_regularizer':\n+            regularizers.serialize(self.pointwise_regularizer),\n+        'bias_regularizer':\n+            regularizers.serialize(self.bias_regularizer),\n+        'activity_regularizer':\n+            regularizers.serialize(self.activity_regularizer),\n+        'depthwise_constraint':\n+            constraints.serialize(self.depthwise_constraint),\n+        'pointwise_constraint':\n+            constraints.serialize(self.pointwise_constraint),\n+        'bias_constraint':\n+            constraints.serialize(self.bias_constraint)\n+    }\n+    base_config = super(SeparableConv, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,169 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras 1D convolution layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.convolutional.base_conv import Conv\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Conv1D', 'keras.layers.Convolution1D')\n+class Conv1D(Conv):\n+  \"\"\"1D convolution layer (e.g. temporal convolution).\n+\n+  This layer creates a convolution kernel that is convolved\n+  with the layer input over a single spatial (or temporal) dimension\n+  to produce a tensor of outputs.\n+  If `use_bias` is True, a bias vector is created and added to the outputs.\n+  Finally, if `activation` is not `None`,\n+  it is applied to the outputs as well.\n+\n+  When using this layer as the first layer in a model,\n+  provide an `input_shape` argument\n+  (tuple of integers or `None`, e.g.\n+  `(10, 128)` for sequences of 10 vectors of 128-dimensional vectors,\n+  or `(None, 128)` for variable-length sequences of 128-dimensional vectors.\n+\n+  Examples:\n+\n+  >>> # The inputs are 128-length vectors with 10 timesteps, and the batch size\n+  >>> # is 4.\n+  >>> input_shape = (4, 10, 128)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv1D(\n+  ... 32, 3, activation='relu',input_shape=input_shape[1:])(x)\n+  >>> print(y.shape)\n+  (4, 8, 32)\n+\n+  >>> # With extended batch shape [4, 7] (e.g. weather data where batch\n+  >>> # dimensions correspond to spatial location and the third dimension\n+  >>> # corresponds to time.)\n+  >>> input_shape = (4, 7, 10, 128)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv1D(\n+  ... 32, 3, activation='relu', input_shape=input_shape[2:])(x)\n+  >>> print(y.shape)\n+  (4, 7, 8, 32)\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space\n+      (i.e. the number of output filters in the convolution).\n+    kernel_size: An integer or tuple/list of a single integer,\n+      specifying the length of the 1D convolution window.\n+    strides: An integer or tuple/list of a single integer,\n+      specifying the stride length of the convolution.\n+      Specifying any stride value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"`, `\"same\"` or `\"causal\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+      `\"causal\"` results in causal (dilated) convolutions, e.g. `output[t]`\n+      does not depend on `input[t+1:]`. Useful when modeling temporal data\n+      where the model should not violate the temporal order.\n+      See [WaveNet: A Generative Model for Raw Audio, section\n+        2.1](https://arxiv.org/abs/1609.03499).\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+    dilation_rate: an integer or tuple/list of a single integer, specifying\n+      the dilation rate to use for dilated convolution.\n+      Currently, specifying any `dilation_rate` value != 1 is\n+      incompatible with specifying any `strides` value != 1.\n+    groups: A positive integer specifying the number of groups in which the\n+      input is split along the channel axis. Each group is convolved\n+      separately with `filters / groups` filters. The output is the\n+      concatenation of all the `groups` results along the channel axis.\n+      Input channels and `filters` must both be divisible by `groups`.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix\n+      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector\n+      (see `keras.initializers`). Defaults to 'zeros'.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector\n+      (see `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\")\n+      (see `keras.regularizers`).\n+    kernel_constraint: Constraint function applied to the kernel matrix\n+      (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector\n+      (see `keras.constraints`).\n+\n+  Input shape:\n+    3+D tensor with shape: `batch_shape + (steps, input_dim)`\n+\n+  Output shape:\n+    3+D tensor with shape: `batch_shape + (new_steps, filters)`\n+      `steps` value might have changed due to padding or strides.\n+\n+  Returns:\n+    A tensor of rank 3 representing\n+    `activation(conv1d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format='channels_last',\n+               dilation_rate=1,\n+               groups=1,\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Conv1D, self).__init__(\n+        rank=1,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        groups=groups,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        kernel_initializer=initializers.get(kernel_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        kernel_regularizer=regularizers.get(kernel_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        kernel_constraint=constraints.get(kernel_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+# Alias\n+\n+Convolution1D = Conv1D\n\n@@ -0,0 +1,281 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras 1D transposed convolution layer (sometimes called deconvolution).\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.convolutional.conv1d import Conv1D\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Conv1DTranspose',\n+              'keras.layers.Convolution1DTranspose')\n+class Conv1DTranspose(Conv1D):\n+  \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n+\n+  The need for transposed convolutions generally arises\n+  from the desire to use a transformation going in the opposite direction\n+  of a normal convolution, i.e., from something that has the shape of the\n+  output of some convolution to something that has the shape of its input\n+  while maintaining a connectivity pattern that is compatible with\n+  said convolution.\n+\n+  When using this layer as the first layer in a model,\n+  provide the keyword argument `input_shape`\n+  (tuple of integers or `None`, does not include the sample axis),\n+  e.g. `input_shape=(128, 3)` for data with 128 time steps and 3 channels.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space\n+      (i.e. the number of output filters in the convolution).\n+    kernel_size: An integer length of the 1D convolution window.\n+    strides: An integer specifying the stride of the convolution along the\n+      time dimension. Specifying a stride value != 1 is incompatible with\n+      specifying a `dilation_rate` value != 1. Defaults to 1.\n+    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    output_padding: An integer specifying the amount of padding along\n+      the time dimension of the output tensor.\n+      The amount of output padding must be lower than the stride.\n+      If set to `None` (default), the output shape is inferred.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, length, channels)` while `channels_first` corresponds to\n+      inputs with shape `(batch_size, channels, length)`.\n+    dilation_rate: an integer, specifying\n+      the dilation rate to use for dilated convolution.\n+      Currently, specifying a `dilation_rate` value != 1 is\n+      incompatible with specifying a stride value != 1.\n+      Also dilation rate larger than 1 is not currently supported.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix\n+      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector\n+      (see `keras.initializers`). Defaults to 'zeros'.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector\n+      (see `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\") (see `keras.regularizers`).\n+    kernel_constraint: Constraint function applied to the kernel matrix\n+      (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector\n+      (see `keras.constraints`).\n+\n+  Input shape:\n+    3D tensor with shape:\n+    `(batch_size, steps, channels)`\n+\n+  Output shape:\n+    3D tensor with shape:\n+    `(batch_size, new_steps, filters)`\n+    If `output_padding` is specified:\n+    ```\n+    new_timesteps = ((timesteps - 1) * strides + kernel_size -\n+    2 * padding + output_padding)\n+    ```\n+\n+  Returns:\n+    A tensor of rank 3 representing\n+    `activation(conv1dtranspose(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n+\n+  References:\n+    - [A guide to convolution arithmetic for deep learning](\n+      https://arxiv.org/abs/1603.07285v1)\n+    - [Deconvolutional Networks](\n+      https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               output_padding=None,\n+               data_format=None,\n+               dilation_rate=1,\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Conv1DTranspose, self).__init__(\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        kernel_initializer=initializers.get(kernel_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        kernel_regularizer=regularizers.get(kernel_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        kernel_constraint=constraints.get(kernel_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+    self.output_padding = output_padding\n+    if self.output_padding is not None:\n+      self.output_padding = conv_utils.normalize_tuple(\n+          self.output_padding, 1, 'output_padding', allow_zero=True)\n+      for stride, out_pad in zip(self.strides, self.output_padding):\n+        if out_pad >= stride:\n+          raise ValueError('Strides must be greater than output padding. '\n+                           f'Received strides={self.strides}, '\n+                           f'output_padding={self.output_padding}.')\n+\n+  def build(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape)\n+    if len(input_shape) != 3:\n+      raise ValueError('Inputs should have rank 3. '\n+                       f'Received input_shape={input_shape}.')\n+    channel_axis = self._get_channel_axis()\n+    if input_shape.dims[channel_axis].value is None:\n+      raise ValueError('The channel dimension of the inputs '\n+                       'to `Conv1DTranspose` should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n+    input_dim = int(input_shape[channel_axis])\n+    self.input_spec = InputSpec(ndim=3, axes={channel_axis: input_dim})\n+    kernel_shape = self.kernel_size + (self.filters, input_dim)\n+\n+    self.kernel = self.add_weight(\n+        name='kernel',\n+        shape=kernel_shape,\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        trainable=True,\n+        dtype=self.dtype)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          name='bias',\n+          shape=(self.filters,),\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          trainable=True,\n+          dtype=self.dtype)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs):\n+    inputs_shape = tf.shape(inputs)\n+    batch_size = inputs_shape[0]\n+    if self.data_format == 'channels_first':\n+      t_axis = 2\n+    else:\n+      t_axis = 1\n+\n+    length = inputs_shape[t_axis]\n+    if self.output_padding is None:\n+      output_padding = None\n+    else:\n+      output_padding = self.output_padding[0]\n+\n+    # Infer the dynamic output shape:\n+    out_length = conv_utils.deconv_output_length(\n+        length, self.kernel_size[0], padding=self.padding,\n+        output_padding=output_padding, stride=self.strides[0],\n+        dilation=self.dilation_rate[0])\n+    if self.data_format == 'channels_first':\n+      output_shape = (batch_size, self.filters, out_length)\n+    else:\n+      output_shape = (batch_size, out_length, self.filters)\n+    data_format = conv_utils.convert_data_format(self.data_format, ndim=3)\n+\n+    output_shape_tensor = tf.stack(output_shape)\n+    outputs = tf.nn.conv1d_transpose(\n+        inputs,\n+        self.kernel,\n+        output_shape_tensor,\n+        strides=self.strides,\n+        padding=self.padding.upper(),\n+        data_format=data_format,\n+        dilations=self.dilation_rate)\n+\n+    if not tf.executing_eagerly():\n+      # Infer the static output shape:\n+      out_shape = self.compute_output_shape(inputs.shape)\n+      outputs.set_shape(out_shape)\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=data_format)\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    output_shape = list(input_shape)\n+    if self.data_format == 'channels_first':\n+      c_axis, t_axis = 1, 2\n+    else:\n+      c_axis, t_axis = 2, 1\n+\n+    if self.output_padding is None:\n+      output_padding = None\n+    else:\n+      output_padding = self.output_padding[0]\n+    output_shape[c_axis] = self.filters\n+    output_shape[t_axis] = conv_utils.deconv_output_length(\n+        output_shape[t_axis],\n+        self.kernel_size[0],\n+        padding=self.padding,\n+        output_padding=output_padding,\n+        stride=self.strides[0],\n+        dilation=self.dilation_rate[0])\n+    return tf.TensorShape(output_shape)\n+\n+  def get_config(self):\n+    config = super(Conv1DTranspose, self).get_config()\n+    config['output_padding'] = self.output_padding\n+    return config\n+\n+# Alias\n+\n+Convolution1DTranspose = Conv1DTranspose\n\n@@ -0,0 +1,190 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras 2D convolution layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.convolutional.base_conv import Conv\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Conv2D', 'keras.layers.Convolution2D')\n+class Conv2D(Conv):\n+  \"\"\"2D convolution layer (e.g. spatial convolution over images).\n+\n+  This layer creates a convolution kernel that is convolved\n+  with the layer input to produce a tensor of\n+  outputs. If `use_bias` is True,\n+  a bias vector is created and added to the outputs. Finally, if\n+  `activation` is not `None`, it is applied to the outputs as well.\n+\n+  When using this layer as the first layer in a model,\n+  provide the keyword argument `input_shape`\n+  (tuple of integers or `None`, does not include the sample axis),\n+  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n+  in `data_format=\"channels_last\"`. You can use `None` when\n+  a dimension has variable size.\n+\n+  Examples:\n+\n+  >>> # The inputs are 28x28 RGB images with `channels_last` and the batch\n+  >>> # size is 4.\n+  >>> input_shape = (4, 28, 28, 3)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv2D(\n+  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n+  >>> print(y.shape)\n+  (4, 26, 26, 2)\n+\n+  >>> # With `dilation_rate` as 2.\n+  >>> input_shape = (4, 28, 28, 3)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv2D(\n+  ... 2, 3, activation='relu', dilation_rate=2, input_shape=input_shape[1:])(x)\n+  >>> print(y.shape)\n+  (4, 24, 24, 2)\n+\n+  >>> # With `padding` as \"same\".\n+  >>> input_shape = (4, 28, 28, 3)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv2D(\n+  ... 2, 3, activation='relu', padding=\"same\", input_shape=input_shape[1:])(x)\n+  >>> print(y.shape)\n+  (4, 28, 28, 2)\n+\n+  >>> # With extended batch shape [4, 7]:\n+  >>> input_shape = (4, 7, 28, 28, 3)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv2D(\n+  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n+  >>> print(y.shape)\n+  (4, 7, 26, 26, 2)\n+\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n+      and width of the 2D convolution window. Can be a single integer to specify\n+      the same value for all spatial dimensions.\n+    strides: An integer or tuple/list of 2 integers, specifying the strides of\n+      the convolution along the height and width. Can be a single integer to\n+      specify the same value for all spatial dimensions. Specifying any stride\n+      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n+    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input. When `padding=\"same\"` and\n+      `strides=1`, the output has the same size as the input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch_size, height, width, channels)` while\n+      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n+      height, width)`. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be `channels_last`.\n+    dilation_rate: an integer or tuple/list of 2 integers, specifying the\n+      dilation rate to use for dilated convolution. Can be a single integer to\n+      specify the same value for all spatial dimensions. Currently, specifying\n+      any `dilation_rate` value != 1 is incompatible with specifying any stride\n+      value != 1.\n+    groups: A positive integer specifying the number of groups in which the\n+      input is split along the channel axis. Each group is convolved separately\n+      with `filters / groups` filters. The output is the concatenation of all\n+      the `groups` results along the channel axis. Input channels and `filters`\n+      must both be divisible by `groups`.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix (see\n+      `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector (see\n+      `keras.initializers`). Defaults to 'zeros'.\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector (see\n+      `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its \"activation\") (see `keras.regularizers`).\n+    kernel_constraint: Constraint function applied to the kernel matrix (see\n+      `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector (see\n+      `keras.constraints`).\n+\n+  Input shape:\n+    4+D tensor with shape: `batch_shape + (channels, rows, cols)` if\n+      `data_format='channels_first'`\n+    or 4+D tensor with shape: `batch_shape + (rows, cols, channels)` if\n+      `data_format='channels_last'`.\n+\n+  Output shape:\n+    4+D tensor with shape: `batch_shape + (filters, new_rows, new_cols)` if\n+    `data_format='channels_first'` or 4+D tensor with shape: `batch_shape +\n+      (new_rows, new_cols, filters)` if `data_format='channels_last'`.  `rows`\n+      and `cols` values might have changed due to padding.\n+\n+  Returns:\n+    A tensor of rank 4+ representing\n+    `activation(conv2d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is `\"causal\"`.\n+    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1),\n+               groups=1,\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Conv2D, self).__init__(\n+        rank=2,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        groups=groups,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        kernel_initializer=initializers.get(kernel_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        kernel_regularizer=regularizers.get(kernel_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        kernel_constraint=constraints.get(kernel_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+# Alias\n+\n+Convolution2D = Conv2D\n\n@@ -0,0 +1,337 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras 2D transposed convolution layer (sometimes called deconvolution).\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import backend\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.convolutional.conv2d import Conv2D\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Conv2DTranspose',\n+              'keras.layers.Convolution2DTranspose')\n+class Conv2DTranspose(Conv2D):\n+  \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n+\n+  The need for transposed convolutions generally arises\n+  from the desire to use a transformation going in the opposite direction\n+  of a normal convolution, i.e., from something that has the shape of the\n+  output of some convolution to something that has the shape of its input\n+  while maintaining a connectivity pattern that is compatible with\n+  said convolution.\n+\n+  When using this layer as the first layer in a model,\n+  provide the keyword argument `input_shape`\n+  (tuple of integers or `None`, does not include the sample axis),\n+  e.g. `input_shape=(128, 128, 3)` for 128x128 RGB pictures\n+  in `data_format=\"channels_last\"`.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space\n+      (i.e. the number of output filters in the convolution).\n+    kernel_size: An integer or tuple/list of 2 integers, specifying the\n+      height and width of the 2D convolution window.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    strides: An integer or tuple/list of 2 integers,\n+      specifying the strides of the convolution along the height and width.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+      Specifying any stride value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    output_padding: An integer or tuple/list of 2 integers,\n+      specifying the amount of padding along the height and width\n+      of the output tensor.\n+      Can be a single integer to specify the same value for all\n+      spatial dimensions.\n+      The amount of output padding along a given dimension must be\n+      lower than the stride along that same dimension.\n+      If set to `None` (default), the output shape is inferred.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch_size, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    dilation_rate: an integer, specifying the dilation rate for all spatial\n+      dimensions for dilated convolution. Specifying different dilation rates\n+      for different dimensions is not supported.\n+      Currently, specifying any `dilation_rate` value != 1 is\n+      incompatible with specifying any stride value != 1.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix\n+      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector\n+      (see `keras.initializers`). Defaults to 'zeros'.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector\n+      (see `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\") (see `keras.regularizers`).\n+    kernel_constraint: Constraint function applied to the kernel matrix\n+      (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector\n+      (see `keras.constraints`).\n+\n+  Input shape:\n+    4D tensor with shape:\n+    `(batch_size, channels, rows, cols)` if data_format='channels_first'\n+    or 4D tensor with shape:\n+    `(batch_size, rows, cols, channels)` if data_format='channels_last'.\n+\n+  Output shape:\n+    4D tensor with shape:\n+    `(batch_size, filters, new_rows, new_cols)` if data_format='channels_first'\n+    or 4D tensor with shape:\n+    `(batch_size, new_rows, new_cols, filters)` if data_format='channels_last'.\n+    `rows` and `cols` values might have changed due to padding.\n+    If `output_padding` is specified:\n+    ```\n+    new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\n+    output_padding[0])\n+    new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\n+    output_padding[1])\n+    ```\n+\n+  Returns:\n+    A tensor of rank 4 representing\n+    `activation(conv2dtranspose(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n+\n+  References:\n+    - [A guide to convolution arithmetic for deep\n+      learning](https://arxiv.org/abs/1603.07285v1)\n+    - [Deconvolutional\n+      Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1),\n+               padding='valid',\n+               output_padding=None,\n+               data_format=None,\n+               dilation_rate=(1, 1),\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Conv2DTranspose, self).__init__(\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        kernel_initializer=initializers.get(kernel_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        kernel_regularizer=regularizers.get(kernel_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        kernel_constraint=constraints.get(kernel_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+    self.output_padding = output_padding\n+    if self.output_padding is not None:\n+      self.output_padding = conv_utils.normalize_tuple(\n+          self.output_padding, 2, 'output_padding', allow_zero=True)\n+      for stride, out_pad in zip(self.strides, self.output_padding):\n+        if out_pad >= stride:\n+          raise ValueError('Strides must be greater than output padding. '\n+                           f'Received strides={self.strides}, '\n+                           f'output_padding={self.output_padding}.')\n+\n+  def build(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape)\n+    if len(input_shape) != 4:\n+      raise ValueError('Inputs should have rank 4. '\n+                       f'Received input_shape={input_shape}.')\n+    channel_axis = self._get_channel_axis()\n+    if input_shape.dims[channel_axis].value is None:\n+      raise ValueError('The channel dimension of the inputs '\n+                       'to `Conv2DTranspose` should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n+    input_dim = int(input_shape[channel_axis])\n+    self.input_spec = InputSpec(ndim=4, axes={channel_axis: input_dim})\n+    kernel_shape = self.kernel_size + (self.filters, input_dim)\n+\n+    self.kernel = self.add_weight(\n+        name='kernel',\n+        shape=kernel_shape,\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        trainable=True,\n+        dtype=self.dtype)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          name='bias',\n+          shape=(self.filters,),\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          trainable=True,\n+          dtype=self.dtype)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs):\n+    inputs_shape = tf.shape(inputs)\n+    batch_size = inputs_shape[0]\n+    if self.data_format == 'channels_first':\n+      h_axis, w_axis = 2, 3\n+    else:\n+      h_axis, w_axis = 1, 2\n+\n+    # Use the constant height and weight when possible.\n+    # TODO(scottzhu): Extract this into a utility function that can be applied\n+    # to all convolutional layers, which currently lost the static shape\n+    # information due to tf.shape().\n+    height, width = None, None\n+    if inputs.shape.rank is not None:\n+      dims = inputs.shape.as_list()\n+      height = dims[h_axis]\n+      width = dims[w_axis]\n+    height = height if height is not None else inputs_shape[h_axis]\n+    width = width if width is not None else inputs_shape[w_axis]\n+\n+    kernel_h, kernel_w = self.kernel_size\n+    stride_h, stride_w = self.strides\n+\n+    if self.output_padding is None:\n+      out_pad_h = out_pad_w = None\n+    else:\n+      out_pad_h, out_pad_w = self.output_padding\n+\n+    # Infer the dynamic output shape:\n+    out_height = conv_utils.deconv_output_length(height,\n+                                                 kernel_h,\n+                                                 padding=self.padding,\n+                                                 output_padding=out_pad_h,\n+                                                 stride=stride_h,\n+                                                 dilation=self.dilation_rate[0])\n+    out_width = conv_utils.deconv_output_length(width,\n+                                                kernel_w,\n+                                                padding=self.padding,\n+                                                output_padding=out_pad_w,\n+                                                stride=stride_w,\n+                                                dilation=self.dilation_rate[1])\n+    if self.data_format == 'channels_first':\n+      output_shape = (batch_size, self.filters, out_height, out_width)\n+    else:\n+      output_shape = (batch_size, out_height, out_width, self.filters)\n+\n+    output_shape_tensor = tf.stack(output_shape)\n+    outputs = backend.conv2d_transpose(\n+        inputs,\n+        self.kernel,\n+        output_shape_tensor,\n+        strides=self.strides,\n+        padding=self.padding,\n+        data_format=self.data_format,\n+        dilation_rate=self.dilation_rate)\n+\n+    if not tf.executing_eagerly():\n+      # Infer the static output shape:\n+      out_shape = self.compute_output_shape(inputs.shape)\n+      outputs.set_shape(out_shape)\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    output_shape = list(input_shape)\n+    if self.data_format == 'channels_first':\n+      c_axis, h_axis, w_axis = 1, 2, 3\n+    else:\n+      c_axis, h_axis, w_axis = 3, 1, 2\n+\n+    kernel_h, kernel_w = self.kernel_size\n+    stride_h, stride_w = self.strides\n+\n+    if self.output_padding is None:\n+      out_pad_h = out_pad_w = None\n+    else:\n+      out_pad_h, out_pad_w = self.output_padding\n+\n+    output_shape[c_axis] = self.filters\n+    output_shape[h_axis] = conv_utils.deconv_output_length(\n+        output_shape[h_axis],\n+        kernel_h,\n+        padding=self.padding,\n+        output_padding=out_pad_h,\n+        stride=stride_h,\n+        dilation=self.dilation_rate[0])\n+    output_shape[w_axis] = conv_utils.deconv_output_length(\n+        output_shape[w_axis],\n+        kernel_w,\n+        padding=self.padding,\n+        output_padding=out_pad_w,\n+        stride=stride_w,\n+        dilation=self.dilation_rate[1])\n+    return tf.TensorShape(output_shape)\n+\n+  def get_config(self):\n+    config = super(Conv2DTranspose, self).get_config()\n+    config['output_padding'] = self.output_padding\n+    return config\n+\n+# Alias\n+\n+Convolution2DTranspose = Conv2DTranspose\n\n@@ -0,0 +1,177 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras 3D convolution layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.convolutional.base_conv import Conv\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Conv3D', 'keras.layers.Convolution3D')\n+class Conv3D(Conv):\n+  \"\"\"3D convolution layer (e.g. spatial convolution over volumes).\n+\n+  This layer creates a convolution kernel that is convolved\n+  with the layer input to produce a tensor of\n+  outputs. If `use_bias` is True,\n+  a bias vector is created and added to the outputs. Finally, if\n+  `activation` is not `None`, it is applied to the outputs as well.\n+\n+  When using this layer as the first layer in a model,\n+  provide the keyword argument `input_shape`\n+  (tuple of integers or `None`, does not include the sample axis),\n+  e.g. `input_shape=(128, 128, 128, 1)` for 128x128x128 volumes\n+  with a single channel,\n+  in `data_format=\"channels_last\"`.\n+\n+  Examples:\n+\n+  >>> # The inputs are 28x28x28 volumes with a single channel, and the\n+  >>> # batch size is 4\n+  >>> input_shape =(4, 28, 28, 28, 1)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv3D(\n+  ... 2, 3, activation='relu', input_shape=input_shape[1:])(x)\n+  >>> print(y.shape)\n+  (4, 26, 26, 26, 2)\n+\n+  >>> # With extended batch shape [4, 7], e.g. a batch of 4 videos of 3D frames,\n+  >>> # with 7 frames per video.\n+  >>> input_shape = (4, 7, 28, 28, 28, 1)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.Conv3D(\n+  ... 2, 3, activation='relu', input_shape=input_shape[2:])(x)\n+  >>> print(y.shape)\n+  (4, 7, 26, 26, 26, 2)\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number of\n+      output filters in the convolution).\n+    kernel_size: An integer or tuple/list of 3 integers, specifying the depth,\n+      height and width of the 3D convolution window. Can be a single integer to\n+      specify the same value for all spatial dimensions.\n+    strides: An integer or tuple/list of 3 integers, specifying the strides of\n+      the convolution along each spatial dimension. Can be a single integer to\n+      specify the same value for all spatial dimensions. Specifying any stride\n+      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n+    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `batch_shape + (spatial_dim1, spatial_dim2,\n+      spatial_dim3, channels)` while `channels_first` corresponds to inputs with\n+      shape `batch_shape + (channels, spatial_dim1, spatial_dim2,\n+      spatial_dim3)`. It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`. If you never set it, then it\n+      will be \"channels_last\".\n+    dilation_rate: an integer or tuple/list of 3 integers, specifying the\n+      dilation rate to use for dilated convolution. Can be a single integer to\n+      specify the same value for all spatial dimensions. Currently, specifying\n+      any `dilation_rate` value != 1 is incompatible with specifying any stride\n+      value != 1.\n+    groups: A positive integer specifying the number of groups in which the\n+      input is split along the channel axis. Each group is convolved separately\n+      with `filters / groups` filters. The output is the concatenation of all\n+      the `groups` results along the channel axis. Input channels and `filters`\n+      must both be divisible by `groups`.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix (see\n+      `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector (see\n+      `keras.initializers`). Defaults to 'zeros'.\n+    kernel_regularizer: Regularizer function applied to the `kernel` weights\n+      matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector (see\n+      `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its \"activation\") (see `keras.regularizers`).\n+    kernel_constraint: Constraint function applied to the kernel matrix (see\n+      `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector (see\n+      `keras.constraints`).\n+\n+  Input shape:\n+    5+D tensor with shape: `batch_shape + (channels, conv_dim1, conv_dim2,\n+      conv_dim3)` if data_format='channels_first'\n+    or 5+D tensor with shape: `batch_shape + (conv_dim1, conv_dim2, conv_dim3,\n+      channels)` if data_format='channels_last'.\n+\n+  Output shape:\n+    5+D tensor with shape: `batch_shape + (filters, new_conv_dim1,\n+      new_conv_dim2, new_conv_dim3)` if data_format='channels_first'\n+    or 5+D tensor with shape: `batch_shape + (new_conv_dim1, new_conv_dim2,\n+      new_conv_dim3, filters)` if data_format='channels_last'. `new_conv_dim1`,\n+      `new_conv_dim2` and `new_conv_dim3` values might have changed due to\n+      padding.\n+\n+  Returns:\n+    A tensor of rank 5+ representing\n+    `activation(conv3d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides > 1` and `dilation_rate > 1`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1, 1),\n+               groups=1,\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Conv3D, self).__init__(\n+        rank=3,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        groups=groups,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        kernel_initializer=initializers.get(kernel_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        kernel_regularizer=regularizers.get(kernel_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        kernel_constraint=constraints.get(kernel_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+# Alias\n+\n+Convolution3D = Conv3D\n\n@@ -0,0 +1,347 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras 3D transposed convolution layer (sometimes called deconvolution).\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.input_spec import InputSpec\n+from keras.layers.convolutional.conv3d import Conv3D\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Conv3DTranspose',\n+              'keras.layers.Convolution3DTranspose')\n+class Conv3DTranspose(Conv3D):\n+  \"\"\"Transposed convolution layer (sometimes called Deconvolution).\n+\n+  The need for transposed convolutions generally arises\n+  from the desire to use a transformation going in the opposite direction\n+  of a normal convolution, i.e., from something that has the shape of the\n+  output of some convolution to something that has the shape of its input\n+  while maintaining a connectivity pattern that is compatible with\n+  said convolution.\n+\n+  When using this layer as the first layer in a model,\n+  provide the keyword argument `input_shape`\n+  (tuple of integers or `None`, does not include the sample axis),\n+  e.g. `input_shape=(128, 128, 128, 3)` for a 128x128x128 volume with 3 channels\n+  if `data_format=\"channels_last\"`.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space\n+      (i.e. the number of output filters in the convolution).\n+    kernel_size: An integer or tuple/list of 3 integers, specifying the\n+      depth, height and width of the 3D convolution window.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    strides: An integer or tuple/list of 3 integers,\n+      specifying the strides of the convolution along the depth, height\n+        and width.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+      Specifying any stride value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    output_padding: An integer or tuple/list of 3 integers,\n+      specifying the amount of padding along the depth, height, and\n+      width.\n+      Can be a single integer to specify the same value for all\n+      spatial dimensions.\n+      The amount of output padding along a given dimension must be\n+      lower than the stride along that same dimension.\n+      If set to `None` (default), the output shape is inferred.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, depth, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch_size, channels, depth, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    dilation_rate: an integer or tuple/list of 3 integers, specifying\n+      the dilation rate to use for dilated convolution.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+      Currently, specifying any `dilation_rate` value != 1 is\n+      incompatible with specifying any stride value != 1.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    kernel_initializer: Initializer for the `kernel` weights matrix\n+      (see `keras.initializers`). Defaults to 'glorot_uniform'.\n+    bias_initializer: Initializer for the bias vector\n+      (see `keras.initializers`). Defaults to 'zeros'.\n+    kernel_regularizer: Regularizer function applied to\n+      the `kernel` weights matrix\n+      (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector\n+      (see `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\")\n+      (see `keras.regularizers`).\n+    kernel_constraint: Constraint function applied to the kernel matrix\n+      (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector\n+      (see `keras.constraints`).\n+\n+  Input shape:\n+    5D tensor with shape:\n+    `(batch_size, channels, depth, rows, cols)` if data_format='channels_first'\n+    or 5D tensor with shape:\n+    `(batch_size, depth, rows, cols, channels)` if data_format='channels_last'.\n+\n+  Output shape:\n+    5D tensor with shape:\n+    `(batch_size, filters, new_depth, new_rows, new_cols)` if\n+      data_format='channels_first'\n+    or 5D tensor with shape:\n+    `(batch_size, new_depth, new_rows, new_cols, filters)` if\n+      data_format='channels_last'.\n+    `depth` and `rows` and `cols` values might have changed due to padding.\n+    If `output_padding` is specified::\n+    ```\n+    new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +\n+    output_padding[0])\n+    new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +\n+    output_padding[1])\n+    new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +\n+    output_padding[2])\n+    ```\n+\n+  Returns:\n+    A tensor of rank 5 representing\n+    `activation(conv3dtranspose(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n+\n+  References:\n+    - [A guide to convolution arithmetic for deep\n+      learning](https://arxiv.org/abs/1603.07285v1)\n+    - [Deconvolutional\n+      Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1, 1),\n+               padding='valid',\n+               output_padding=None,\n+               data_format=None,\n+               dilation_rate=(1, 1, 1),\n+               activation=None,\n+               use_bias=True,\n+               kernel_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               kernel_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               kernel_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(Conv3DTranspose, self).__init__(\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        kernel_initializer=initializers.get(kernel_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        kernel_regularizer=regularizers.get(kernel_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        kernel_constraint=constraints.get(kernel_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+    self.output_padding = output_padding\n+    if self.output_padding is not None:\n+      self.output_padding = conv_utils.normalize_tuple(\n+          self.output_padding, 3, 'output_padding', allow_zero=True)\n+      for stride, out_pad in zip(self.strides, self.output_padding):\n+        if out_pad >= stride:\n+          raise ValueError('Strides must be greater than output padding. '\n+                           f'Received strides={self.strides}, '\n+                           f'output_padding={self.output_padding}.')\n+\n+  def build(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape)\n+    if len(input_shape) != 5:\n+      raise ValueError('Inputs should have rank 5. '\n+                       f'Received input_shape={input_shape}.')\n+    channel_axis = self._get_channel_axis()\n+    if input_shape.dims[channel_axis].value is None:\n+      raise ValueError('The channel dimension of the inputs '\n+                       'to `Conv3DTranspose` should be defined. '\n+                       f'The input_shape received is {input_shape}, '\n+                       f'where axis {channel_axis} (0-based) '\n+                       'is the channel dimension, which found to be `None`.')\n+    input_dim = int(input_shape[channel_axis])\n+    kernel_shape = self.kernel_size + (self.filters, input_dim)\n+    self.input_spec = InputSpec(ndim=5, axes={channel_axis: input_dim})\n+\n+    self.kernel = self.add_weight(\n+        'kernel',\n+        shape=kernel_shape,\n+        initializer=self.kernel_initializer,\n+        regularizer=self.kernel_regularizer,\n+        constraint=self.kernel_constraint,\n+        trainable=True,\n+        dtype=self.dtype)\n+    if self.use_bias:\n+      self.bias = self.add_weight(\n+          'bias',\n+          shape=(self.filters,),\n+          initializer=self.bias_initializer,\n+          regularizer=self.bias_regularizer,\n+          constraint=self.bias_constraint,\n+          trainable=True,\n+          dtype=self.dtype)\n+    else:\n+      self.bias = None\n+    self.built = True\n+\n+  def call(self, inputs):\n+    inputs_shape = tf.shape(inputs)\n+    batch_size = inputs_shape[0]\n+    if self.data_format == 'channels_first':\n+      d_axis, h_axis, w_axis = 2, 3, 4\n+    else:\n+      d_axis, h_axis, w_axis = 1, 2, 3\n+\n+    depth = inputs_shape[d_axis]\n+    height = inputs_shape[h_axis]\n+    width = inputs_shape[w_axis]\n+\n+    kernel_d, kernel_h, kernel_w = self.kernel_size\n+    stride_d, stride_h, stride_w = self.strides\n+\n+    if self.output_padding is None:\n+      out_pad_d = out_pad_h = out_pad_w = None\n+    else:\n+      out_pad_d, out_pad_h, out_pad_w = self.output_padding\n+\n+    # Infer the dynamic output shape:\n+    out_depth = conv_utils.deconv_output_length(depth,\n+                                                kernel_d,\n+                                                padding=self.padding,\n+                                                output_padding=out_pad_d,\n+                                                stride=stride_d)\n+    out_height = conv_utils.deconv_output_length(height,\n+                                                 kernel_h,\n+                                                 padding=self.padding,\n+                                                 output_padding=out_pad_h,\n+                                                 stride=stride_h)\n+    out_width = conv_utils.deconv_output_length(width,\n+                                                kernel_w,\n+                                                padding=self.padding,\n+                                                output_padding=out_pad_w,\n+                                                stride=stride_w)\n+    if self.data_format == 'channels_first':\n+      output_shape = (batch_size, self.filters, out_depth, out_height,\n+                      out_width)\n+      strides = (1, 1, stride_d, stride_h, stride_w)\n+    else:\n+      output_shape = (batch_size, out_depth, out_height, out_width,\n+                      self.filters)\n+      strides = (1, stride_d, stride_h, stride_w, 1)\n+\n+    output_shape_tensor = tf.stack(output_shape)\n+    outputs = tf.nn.conv3d_transpose(\n+        inputs,\n+        self.kernel,\n+        output_shape_tensor,\n+        strides,\n+        data_format=conv_utils.convert_data_format(self.data_format, ndim=5),\n+        padding=self.padding.upper())\n+\n+    if not tf.executing_eagerly():\n+      # Infer the static output shape:\n+      out_shape = self.compute_output_shape(inputs.shape)\n+      outputs.set_shape(out_shape)\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    output_shape = list(input_shape)\n+    if self.data_format == 'channels_first':\n+      c_axis, d_axis, h_axis, w_axis = 1, 2, 3, 4\n+    else:\n+      c_axis, d_axis, h_axis, w_axis = 4, 1, 2, 3\n+\n+    kernel_d, kernel_h, kernel_w = self.kernel_size\n+    stride_d, stride_h, stride_w = self.strides\n+\n+    if self.output_padding is None:\n+      out_pad_d = out_pad_h = out_pad_w = None\n+    else:\n+      out_pad_d, out_pad_h, out_pad_w = self.output_padding\n+\n+    output_shape[c_axis] = self.filters\n+    output_shape[d_axis] = conv_utils.deconv_output_length(\n+        output_shape[d_axis],\n+        kernel_d,\n+        padding=self.padding,\n+        output_padding=out_pad_d,\n+        stride=stride_d)\n+    output_shape[h_axis] = conv_utils.deconv_output_length(\n+        output_shape[h_axis],\n+        kernel_h,\n+        padding=self.padding,\n+        output_padding=out_pad_h,\n+        stride=stride_h)\n+    output_shape[w_axis] = conv_utils.deconv_output_length(\n+        output_shape[w_axis],\n+        kernel_w,\n+        padding=self.padding,\n+        output_padding=out_pad_w,\n+        stride=stride_w)\n+    return tf.TensorShape(output_shape)\n+\n+  def get_config(self):\n+    config = super(Conv3DTranspose, self).get_config()\n+    config.pop('dilation_rate')\n+    config['output_padding'] = self.output_padding\n+    return config\n+\n+# Alias\n+\n+Convolution3DTranspose = Conv3DTranspose\n\n@@ -0,0 +1,558 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for convolutional layers.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.framework import test_util\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class Conv1DTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, kwargs, expected_output_shape):\n+    num_samples = 2\n+    stack_size = 3\n+    length = 7\n+\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.Conv1D,\n+          kwargs=kwargs,\n+          input_shape=(num_samples, length, stack_size),\n+          expected_output_shape=expected_output_shape)\n+\n+  def _run_test_extra_batch_dim(self, kwargs, expected_output_shape):\n+    batch_shape = (2, 11)\n+    stack_size = 3\n+    length = 7\n+\n+    with self.cached_session():\n+      if expected_output_shape is not None:\n+        expected_output_shape = (None,) + expected_output_shape\n+\n+      testing_utils.layer_test(\n+          keras.layers.Conv1D,\n+          kwargs=kwargs,\n+          input_shape=batch_shape + (length, stack_size),\n+          expected_output_shape=expected_output_shape)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {\n+          'padding': 'valid'\n+      }, (None, 5, 2)),\n+      ('padding_same', {\n+          'padding': 'same'\n+      }, (None, 7, 2)),\n+      ('padding_same_dilation_2', {\n+          'padding': 'same',\n+          'dilation_rate': 2\n+      }, (None, 7, 2)),\n+      ('padding_same_dilation_3', {\n+          'padding': 'same',\n+          'dilation_rate': 3\n+      }, (None, 7, 2)),\n+      ('padding_causal', {\n+          'padding': 'causal'\n+      }, (None, 7, 2)),\n+      ('strides', {\n+          'strides': 2\n+      }, (None, 3, 2)),\n+      ('dilation_rate', {\n+          'dilation_rate': 2\n+      }, (None, 3, 2)),\n+      ('group', {\n+          'groups': 3,\n+          'filters': 6\n+      }, (None, 5, 6)),\n+  )\n+  def test_conv1d(self, kwargs, expected_output_shape):\n+    kwargs['filters'] = kwargs.get('filters', 2)\n+    kwargs['kernel_size'] = 3\n+    self._run_test(kwargs, expected_output_shape)\n+    self._run_test_extra_batch_dim(kwargs, expected_output_shape)\n+\n+  def test_conv1d_regularizers(self):\n+    kwargs = {\n+        'filters': 3,\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'kernel_regularizer': 'l2',\n+        'bias_regularizer': 'l2',\n+        'activity_regularizer': 'l2',\n+        'strides': 1\n+    }\n+    with self.cached_session():\n+      layer = keras.layers.Conv1D(**kwargs)\n+      layer.build((None, 5, 2))\n+      self.assertEqual(len(layer.losses), 2)\n+      layer(keras.backend.variable(np.ones((1, 5, 2))))\n+      self.assertEqual(len(layer.losses), 3)\n+\n+  def test_conv1d_constraints(self):\n+    k_constraint = lambda x: x\n+    b_constraint = lambda x: x\n+\n+    kwargs = {\n+        'filters': 3,\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'kernel_constraint': k_constraint,\n+        'bias_constraint': b_constraint,\n+        'strides': 1\n+    }\n+    with self.cached_session():\n+      layer = keras.layers.Conv1D(**kwargs)\n+      layer.build((None, 5, 2))\n+      self.assertEqual(layer.kernel.constraint, k_constraint)\n+      self.assertEqual(layer.bias.constraint, b_constraint)\n+\n+  def test_conv1d_recreate_conv(self):\n+    with self.cached_session():\n+      layer = keras.layers.Conv1D(filters=1,\n+                                  kernel_size=3,\n+                                  strides=1,\n+                                  dilation_rate=2,\n+                                  padding='causal')\n+      inpt1 = np.random.normal(size=[1, 2, 1])\n+      inpt2 = np.random.normal(size=[1, 1, 1])\n+      outp1_shape = layer(inpt1).shape\n+      _ = layer(inpt2).shape\n+      self.assertEqual(outp1_shape, layer(inpt1).shape)\n+\n+  def test_conv1d_recreate_conv_unknown_dims(self):\n+    with self.cached_session():\n+      layer = keras.layers.Conv1D(filters=1,\n+                                  kernel_size=3,\n+                                  strides=1,\n+                                  dilation_rate=2,\n+                                  padding='causal')\n+\n+      inpt1 = np.random.normal(size=[1, 9, 1]).astype(np.float32)\n+      inpt2 = np.random.normal(size=[1, 2, 1]).astype(np.float32)\n+      outp1_shape = layer(inpt1).shape\n+\n+      @tf.function(input_signature=[\n+          tf.TensorSpec([1, None, 1])])\n+      def fn(inpt):\n+        return layer(inpt)\n+\n+      fn(inpt2)\n+      self.assertEqual(outp1_shape, layer(inpt1).shape)\n+\n+  def test_conv1d_invalid_output_shapes(self):\n+    kwargs = {'filters': 2, 'kernel_size': 20}\n+    with self.assertRaisesRegex(\n+        ValueError, r\"\"\"One of the dimensions in the output is <= 0\"\"\"):\n+      layer = keras.layers.Conv1D(**kwargs)\n+      layer.build((None, 5, 2))\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class Conv2DTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, kwargs, expected_output_shape, spatial_shape=(7, 6)):\n+    num_samples = 2\n+    stack_size = 3\n+    num_row, num_col = spatial_shape\n+    input_data = None\n+    # Generate valid input data.\n+    if None in spatial_shape:\n+      input_data_shape = (num_samples, num_row or 7, num_col or 6, stack_size)\n+      input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n+\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.Conv2D,\n+          kwargs=kwargs,\n+          input_shape=(num_samples, num_row, num_col, stack_size),\n+          input_data=input_data,\n+          expected_output_shape=expected_output_shape)\n+\n+  def _run_test_extra_batch_dim(self,\n+                                kwargs,\n+                                expected_output_shape,\n+                                spatial_shape=(7, 6)):\n+    batch_shape = (2, 11)\n+    stack_size = 3\n+    num_row, num_col = spatial_shape\n+    input_data = None\n+    # Generate valid input data.\n+    if None in spatial_shape:\n+      input_data_shape = batch_shape + (num_row or 7, num_col or 6, stack_size)\n+      input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n+\n+    with self.cached_session():\n+      if expected_output_shape is not None:\n+        expected_output_shape = (None,) + expected_output_shape\n+      testing_utils.layer_test(\n+          keras.layers.Conv2D,\n+          kwargs=kwargs,\n+          input_shape=batch_shape + (num_row, num_col, stack_size),\n+          input_data=input_data,\n+          expected_output_shape=expected_output_shape)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {\n+          'padding': 'valid'\n+      }, (None, 5, 4, 2)),\n+      ('padding_same', {\n+          'padding': 'same'\n+      }, (None, 7, 6, 2)),\n+      ('padding_same_dilation_2', {\n+          'padding': 'same',\n+          'dilation_rate': 2\n+      }, (None, 7, 6, 2)),\n+      ('strides', {\n+          'strides': (2, 2)\n+      }, (None, 3, 2, 2)),\n+      ('dilation_rate', {\n+          'dilation_rate': (2, 2)\n+      }, (None, 3, 2, 2)),\n+      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+      # TODO(b/62340061): Support channels_first on CPU.\n+      ('data_format', {\n+          'data_format': 'channels_first'\n+      }, None, True),\n+      ('group', {\n+          'groups': 3,\n+          'filters': 6\n+      }, (None, 5, 4, 6), False),\n+      ('dilation_2_unknown_width', {\n+          'dilation_rate': (2, 2)\n+      }, (None, None, 2, 2), False, (None, 6)),\n+      ('dilation_2_unknown_height', {\n+          'dilation_rate': (2, 2)\n+      }, (None, 3, None, 2), False, (7, None)),\n+  )\n+  def test_conv2d(self,\n+                  kwargs,\n+                  expected_output_shape=None,\n+                  requires_gpu=False,\n+                  spatial_shape=(7, 6)):\n+    kwargs['filters'] = kwargs.get('filters', 2)\n+    kwargs['kernel_size'] = (3, 3)\n+    if not requires_gpu or tf.test.is_gpu_available(cuda_only=True):\n+      self._run_test(kwargs, expected_output_shape, spatial_shape)\n+      self._run_test_extra_batch_dim(kwargs, expected_output_shape,\n+                                     spatial_shape)\n+\n+  def test_conv2d_regularizers(self):\n+    kwargs = {\n+        'filters': 3,\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'kernel_regularizer': 'l2',\n+        'bias_regularizer': 'l2',\n+        'activity_regularizer': 'l2',\n+        'strides': 1\n+    }\n+    with self.cached_session():\n+      layer = keras.layers.Conv2D(**kwargs)\n+      layer.build((None, 5, 5, 2))\n+      self.assertEqual(len(layer.losses), 2)\n+      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))\n+      self.assertEqual(len(layer.losses), 3)\n+\n+  def test_conv2d_constraints(self):\n+    k_constraint = lambda x: x\n+    b_constraint = lambda x: x\n+\n+    kwargs = {\n+        'filters': 3,\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'kernel_constraint': k_constraint,\n+        'bias_constraint': b_constraint,\n+        'strides': 1\n+    }\n+    with self.cached_session():\n+      layer = keras.layers.Conv2D(**kwargs)\n+      layer.build((None, 5, 5, 2))\n+      self.assertEqual(layer.kernel.constraint, k_constraint)\n+      self.assertEqual(layer.bias.constraint, b_constraint)\n+\n+  def test_conv2d_zero_kernel_size(self):\n+    kwargs = {'filters': 2, 'kernel_size': 0}\n+    with self.assertRaises(ValueError):\n+      keras.layers.Conv2D(**kwargs)\n+\n+  def test_conv2d_invalid_output_shapes(self):\n+    kwargs = {'filters': 2, 'kernel_size': 20}\n+    with self.assertRaisesRegex(\n+        ValueError, r\"\"\"One of the dimensions in the output is <= 0\"\"\"):\n+      layer = keras.layers.Conv2D(**kwargs)\n+      layer.build((None, 5, 5, 2))\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class Conv3DTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, kwargs, expected_output_shape, validate_training=True):\n+    num_samples = 2\n+    stack_size = 3\n+    num_row = 7\n+    num_col = 6\n+    depth = 5\n+\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.Conv3D,\n+          kwargs=kwargs,\n+          input_shape=(num_samples, depth, num_row, num_col, stack_size),\n+          expected_output_shape=expected_output_shape,\n+          validate_training=validate_training)\n+\n+  def _run_test_extra_batch_dim(self,\n+                                kwargs,\n+                                expected_output_shape,\n+                                validate_training=True):\n+    batch_shape = (2, 11)\n+    stack_size = 3\n+    num_row = 7\n+    num_col = 6\n+    depth = 5\n+\n+    with self.cached_session():\n+      if expected_output_shape is not None:\n+        expected_output_shape = (None,) + expected_output_shape\n+\n+      testing_utils.layer_test(\n+          keras.layers.Conv3D,\n+          kwargs=kwargs,\n+          input_shape=batch_shape + (depth, num_row, num_col, stack_size),\n+          expected_output_shape=expected_output_shape,\n+          validate_training=validate_training)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {\n+          'padding': 'valid'\n+      }, (None, 3, 5, 4, 2)),\n+      ('padding_same', {\n+          'padding': 'same'\n+      }, (None, 5, 7, 6, 2)),\n+      ('strides', {\n+          'strides': (2, 2, 2)\n+      }, (None, 2, 3, 2, 2)),\n+      ('dilation_rate', {\n+          'dilation_rate': (2, 2, 2)\n+      }, (None, 1, 3, 2, 2)),\n+      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+      # TODO(b/62340061): Support channels_first on CPU.\n+      ('data_format', {\n+          'data_format': 'channels_first'\n+      }, None, True),\n+      ('group', {\n+          'groups': 3,\n+          'filters': 6\n+      }, (None, 3, 5, 4, 6)),\n+  )\n+  def test_conv3d(self, kwargs, expected_output_shape=None, requires_gpu=False):\n+    kwargs['filters'] = kwargs.get('filters', 2)\n+    kwargs['kernel_size'] = (3, 3, 3)\n+    # train_on_batch currently fails with XLA enabled on GPUs\n+    test_training = 'groups' not in kwargs or not test_util.is_xla_enabled()\n+    if not requires_gpu or tf.test.is_gpu_available(cuda_only=True):\n+      self._run_test(kwargs, expected_output_shape, test_training)\n+      self._run_test_extra_batch_dim(kwargs, expected_output_shape,\n+                                     test_training)\n+\n+  def test_conv3d_regularizers(self):\n+    kwargs = {\n+        'filters': 3,\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'kernel_regularizer': 'l2',\n+        'bias_regularizer': 'l2',\n+        'activity_regularizer': 'l2',\n+        'strides': 1\n+    }\n+    with self.cached_session():\n+      layer = keras.layers.Conv3D(**kwargs)\n+      layer.build((None, 5, 5, 5, 2))\n+      self.assertEqual(len(layer.losses), 2)\n+      self.assertEqual(len(layer.losses), 2)\n+      layer(keras.backend.variable(np.ones((1, 5, 5, 5, 2))))\n+      self.assertEqual(len(layer.losses), 3)\n+\n+  def test_conv3d_constraints(self):\n+    k_constraint = lambda x: x\n+    b_constraint = lambda x: x\n+\n+    kwargs = {\n+        'filters': 3,\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'kernel_constraint': k_constraint,\n+        'bias_constraint': b_constraint,\n+        'strides': 1\n+    }\n+    with self.cached_session():\n+      layer = keras.layers.Conv3D(**kwargs)\n+      layer.build((None, 5, 5, 5, 2))\n+      self.assertEqual(layer.kernel.constraint, k_constraint)\n+      self.assertEqual(layer.bias.constraint, b_constraint)\n+\n+  def test_conv3d_dynamic_shape(self):\n+    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)\n+    with self.cached_session():\n+      # Won't raise error here.\n+      testing_utils.layer_test(\n+          keras.layers.Conv3D,\n+          kwargs={\n+              'data_format': 'channels_last',\n+              'filters': 3,\n+              'kernel_size': 3\n+          },\n+          input_shape=(None, None, None, None, 3),\n+          input_data=input_data)\n+      if tf.test.is_gpu_available(cuda_only=True):\n+        testing_utils.layer_test(\n+            keras.layers.Conv3D,\n+            kwargs={\n+                'data_format': 'channels_first',\n+                'filters': 3,\n+                'kernel_size': 3\n+            },\n+            input_shape=(None, 3, None, None, None),\n+            input_data=input_data)\n+\n+  def test_conv3d_invalid_output_shapes(self):\n+    kwargs = {'filters': 2, 'kernel_size': 20}\n+    with self.assertRaisesRegex(\n+        ValueError, r\"\"\"One of the dimensions in the output is <= 0\"\"\"):\n+      layer = keras.layers.Conv3D(**kwargs)\n+      layer.build((None, 5, 5, 5, 2))\n+\n+\n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+class GroupedConvTest(keras_parameterized.TestCase):\n+\n+  @parameterized.named_parameters(\n+      ('Conv1D', keras.layers.Conv1D),\n+      ('Conv2D', keras.layers.Conv2D),\n+      ('Conv3D', keras.layers.Conv3D),\n+  )\n+  def test_group_conv_incorrect_use(self, layer):\n+    with self.assertRaisesRegex(ValueError, 'The number of filters'):\n+      layer(16, 3, groups=3)\n+    with self.assertRaisesRegex(ValueError, 'The number of input channels'):\n+      layer(16, 3, groups=4).build((32, 12, 12, 3))\n+\n+  @parameterized.named_parameters(\n+      ('Conv1D', keras.layers.Conv1D, (32, 12, 32)),\n+      ('Conv2D', keras.layers.Conv2D, (32, 12, 12, 32)),\n+      ('Conv3D', keras.layers.Conv3D, (32, 12, 12, 12, 32)),\n+  )\n+  def test_group_conv(self, layer_cls, input_shape):\n+    if tf.test.is_gpu_available(cuda_only=True):\n+      with testing_utils.use_gpu():\n+        inputs = tf.random.uniform(shape=input_shape)\n+\n+        layer = layer_cls(16, 3, groups=4, use_bias=False)\n+        layer.build(input_shape)\n+\n+        input_slices = tf.split(inputs, 4, axis=-1)\n+        weight_slices = tf.split(layer.kernel, 4, axis=-1)\n+        expected_outputs = tf.concat([\n+            tf.nn.convolution(inputs, weights)\n+            for inputs, weights in zip(input_slices, weight_slices)\n+        ],\n+                                     axis=-1)\n+        self.assertAllClose(\n+            layer(inputs), expected_outputs, rtol=3e-5, atol=3e-5)\n+\n+  def test_group_conv_depthwise(self):\n+    if tf.test.is_gpu_available(cuda_only=True):\n+      with testing_utils.use_gpu():\n+        inputs = tf.random.uniform(shape=(3, 27, 27, 32))\n+\n+        layer = keras.layers.Conv2D(32, 3, groups=32, use_bias=False)\n+        layer.build((3, 27, 27, 32))\n+\n+        weights_dw = tf.reshape(layer.kernel, [3, 3, 32, 1])\n+        expected_outputs = tf.compat.v1.nn.depthwise_conv2d(\n+            inputs, weights_dw, strides=[1, 1, 1, 1], padding='VALID')\n+\n+        self.assertAllClose(layer(inputs), expected_outputs, rtol=1e-5)\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ConvSequentialTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, conv_layer_cls, kwargs, input_shape1, input_shape2,\n+                expected_output_shape1, expected_output_shape2):\n+    kwargs['filters'] = 1\n+    kwargs['kernel_size'] = 3\n+    kwargs['dilation_rate'] = 2\n+    with self.cached_session():\n+      layer = conv_layer_cls(**kwargs)\n+      output1 = layer(np.zeros(input_shape1))\n+      self.assertEqual(output1.shape, expected_output_shape1)\n+      output2 = layer(np.zeros(input_shape2))\n+      self.assertEqual(output2.shape, expected_output_shape2)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {'padding': 'valid'},\n+       (1, 8, 2), (1, 5, 2), (1, 4, 1), (1, 1, 1)),\n+      ('padding_same', {'padding': 'same'},\n+       (1, 8, 2), (1, 5, 2), (1, 8, 1), (1, 5, 1)),\n+      ('padding_causal', {'padding': 'causal'},\n+       (1, 8, 2), (1, 5, 2), (1, 8, 1), (1, 5, 1)),\n+  )\n+  def test_conv1d(self, kwargs, input_shape1, input_shape2,\n+                  expected_output_shape1, expected_output_shape2):\n+    self._run_test(keras.layers.Conv1D, kwargs, input_shape1, input_shape2,\n+                   expected_output_shape1, expected_output_shape2)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {'padding': 'valid'},\n+       (1, 7, 6, 2), (1, 6, 5, 2), (1, 3, 2, 1), (1, 2, 1, 1)),\n+      ('padding_same', {'padding': 'same'},\n+       (1, 7, 6, 2), (1, 6, 5, 2), (1, 7, 6, 1), (1, 6, 5, 1)),\n+  )\n+  def test_conv2d(self, kwargs, input_shape1, input_shape2,\n+                  expected_output_shape1, expected_output_shape2):\n+    self._run_test(keras.layers.Conv2D, kwargs, input_shape1, input_shape2,\n+                   expected_output_shape1, expected_output_shape2)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {'padding': 'valid'},\n+       (1, 5, 7, 6, 2), (1, 8, 6, 5, 2), (1, 1, 3, 2, 1), (1, 4, 2, 1, 1)),\n+      ('padding_same', {'padding': 'same'},\n+       (1, 5, 7, 6, 2), (1, 8, 6, 5, 2), (1, 5, 7, 6, 1), (1, 8, 6, 5, 1)),\n+  )\n+  def test_conv3d(self, kwargs, input_shape1, input_shape2,\n+                  expected_output_shape1, expected_output_shape2):\n+    self._run_test(keras.layers.Conv3D, kwargs, input_shape1, input_shape2,\n+                   expected_output_shape1, expected_output_shape2)\n+\n+  def test_dynamic_shape(self):\n+    with self.cached_session():\n+      layer = keras.layers.Conv3D(2, 3)\n+      input_shape = (5, None, None, 2)\n+      inputs = keras.Input(shape=input_shape)\n+      x = layer(inputs)\n+      # Won't raise error here with None values in input shape (b/144282043).\n+      layer(x)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -14,14 +14,45 @@\n # ==============================================================================\n \"\"\"Tests for convolutional transpose layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class Conv1DTransposeTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, kwargs, expected_output_shape):\n+    num_samples = 2\n+    stack_size = 3\n+    num_col = 6\n+\n+    with testing_utils.use_gpu():\n+      testing_utils.layer_test(\n+          keras.layers.Conv1DTranspose,\n+          kwargs=kwargs,\n+          input_shape=(num_samples, num_col, stack_size),\n+          expected_output_shape=expected_output_shape)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {'padding': 'valid'}, (None, 8, 2)),\n+      ('padding_same', {'padding': 'same'}, (None, 6, 2)),\n+      ('strides', {'strides': 2}, (None, 13, 2)),\n+      # Only runs on GPU with CUDA, dilation_rate>1 is not supported on CPU.\n+      ('dilation_rate', {'dilation_rate': 2}, (None, 10, 2)),\n+      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+      # TODO(b/62340061): Support channels_first on CPU.\n+      ('data_format', {'data_format': 'channels_first'}),\n+  )\n+  def test_conv1d_transpose(self, kwargs, expected_output_shape=None):\n+    kwargs['filters'] = 2\n+    kwargs['kernel_size'] = 3\n+    if (('data_format' not in kwargs and 'dilation_rate' not in kwargs) or\n+        tf.test.is_gpu_available(cuda_only=True)):\n+      self._run_test(kwargs, expected_output_shape)\n \n \n @keras_parameterized.run_all_keras_modes\n@@ -120,34 +151,48 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n @keras_parameterized.run_all_keras_modes\n class Conv3DTransposeTest(keras_parameterized.TestCase):\n \n-  def _run_test(self, kwargs):\n+  def _run_test(self, kwargs, expected_output_shape):\n     num_samples = 2\n     stack_size = 3\n     num_row = 7\n     num_col = 6\n     depth = 5\n \n-    with self.cached_session():\n+    with testing_utils.use_gpu():\n       testing_utils.layer_test(\n           keras.layers.Conv3DTranspose,\n           kwargs=kwargs,\n-          input_shape=(num_samples, depth, num_row, num_col, stack_size))\n+          input_shape=(num_samples, depth, num_row, num_col, stack_size),\n+          expected_output_shape=expected_output_shape)\n \n   @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'}),\n-      ('padding_same', {'padding': 'same'}),\n-      ('strides', {'strides': (2, 2, 2)}),\n+      ('padding_valid', {\n+          'padding': 'valid'\n+      }, (None, 7, 9, 8, 2)),\n+      ('padding_same', {\n+          'padding': 'same'\n+      }, (None, 5, 7, 6, 2)),\n+      ('strides', {\n+          'strides': (2, 2, 2)\n+      }, (None, 11, 15, 13, 2)),\n+      ('dilation_rate', {\n+          'dilation_rate': (2, 2, 2)\n+      }, (None, 7, 9, 8, 2)),\n       # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n       # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {'data_format': 'channels_first'}),\n-      ('strides_output_padding', {'strides': (2, 2, 2),\n-                                  'output_padding': (1, 1, 1)}),\n+      ('data_format', {\n+          'data_format': 'channels_first'\n+      }),\n+      ('strides_output_padding', {\n+          'strides': (2, 2, 2),\n+          'output_padding': (1, 1, 1)\n+      }, (None, 12, 16, 14, 2)),\n   )\n-  def test_conv3d_transpose(self, kwargs):\n+  def test_conv3d_transpose(self, kwargs, expected_output_shape=None):\n     kwargs['filters'] = 2\n     kwargs['kernel_size'] = (3, 3, 3)\n     if 'data_format' not in kwargs or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs)\n+      self._run_test(kwargs, expected_output_shape)\n \n   def test_conv3d_transpose_regularizers(self):\n     kwargs = {\n\n@@ -0,0 +1,200 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras depthwise 1D convolution.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.layers.convolutional.base_depthwise_conv import DepthwiseConv\n+from keras.utils import conv_utils\n+from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.DepthwiseConv1D')\n+class DepthwiseConv1D(DepthwiseConv):\n+  \"\"\"Depthwise 1D convolution.\n+\n+  Depthwise convolution is a type of convolution in which each input channel is\n+  convolved with a different kernel (called a depthwise kernel). You\n+  can understand depthwise convolution as the first step in a depthwise\n+  separable convolution.\n+\n+  It is implemented via the following steps:\n+\n+  - Split the input into individual channels.\n+  - Convolve each channel with an individual depthwise kernel with\n+    `depth_multiplier` output channels.\n+  - Concatenate the convolved outputs along the channels axis.\n+\n+  Unlike a regular 1D convolution, depthwise convolution does not mix\n+  information across different input channels.\n+\n+  The `depth_multiplier` argument determines how many filter are applied to one\n+  input channel. As such, it controls the amount of output channels that are\n+  generated per input channel in the depthwise step.\n+\n+  Args:\n+    kernel_size: An integer, specifying the height and width of the 1D\n+      convolution window. Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    strides: An integer, specifying the strides of the convolution along the\n+      height and width. Can be a single integer to specify the same value for\n+      all spatial dimensions. Specifying any stride value != 1 is incompatible\n+      with specifying any `dilation_rate` value != 1.\n+    padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding with zeros evenly to the left/right\n+      or up/down of the input such that output has the same height/width\n+      dimension as the input.\n+    depth_multiplier: The number of depthwise convolution output channels for\n+      each input channel. The total number of depthwise convolution output\n+      channels will be equal to `filters_in * depth_multiplier`.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch_size, height, width, channels)` while\n+      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n+      height, width)`. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be 'channels_last'.\n+    dilation_rate: A single integer, specifying the dilation rate to use for\n+      dilated convolution. Currently, specifying any `dilation_rate` value != 1\n+      is incompatible with specifying any stride value != 1.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    depthwise_initializer: Initializer for the depthwise kernel matrix (see\n+      `keras.initializers`). If None, the default initializer\n+      ('glorot_uniform') will be used.\n+    bias_initializer: Initializer for the bias vector (see\n+      `keras.initializers`). If None, the default initializer ('zeros') will be\n+      used.\n+    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n+      matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector (see\n+      `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its 'activation') (see `keras.regularizers`).\n+    depthwise_constraint: Constraint function applied to the depthwise kernel\n+      matrix (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector (see\n+      `keras.constraints`).\n+\n+  Input shape:\n+    4D tensor with shape: `[batch_size, channels, rows, cols]` if\n+      data_format='channels_first'\n+    or 4D tensor with shape: `[batch_size, rows, cols, channels]` if\n+      data_format='channels_last'.\n+\n+  Output shape:\n+    4D tensor with shape: `[batch_size, channels * depth_multiplier, new_rows,\n+      new_cols]` if `data_format='channels_first'`\n+      or 4D tensor with shape: `[batch_size,\n+      new_rows, new_cols, channels * depth_multiplier]` if\n+      `data_format='channels_last'`. `rows` and `cols` values might have changed\n+      due to padding.\n+\n+  Returns:\n+    A tensor of rank 4 representing\n+    `activation(depthwiseconv2d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n+  \"\"\"\n+\n+  def __init__(self,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               depth_multiplier=1,\n+               data_format=None,\n+               dilation_rate=1,\n+               activation=None,\n+               use_bias=True,\n+               depthwise_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               depthwise_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               depthwise_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(DepthwiseConv1D, self).__init__(\n+        1,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        depth_multiplier=depth_multiplier,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        use_bias=use_bias,\n+        depthwise_initializer=depthwise_initializer,\n+        bias_initializer=bias_initializer,\n+        depthwise_regularizer=depthwise_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        depthwise_constraint=depthwise_constraint,\n+        bias_constraint=bias_constraint,\n+        **kwargs)\n+\n+  def call(self, inputs):\n+    if self.data_format == 'channels_last':\n+      strides = (1,) + self.strides * 2 + (1,)\n+      spatial_start_dim = 1\n+    else:\n+      strides = (1, 1) + self.strides * 2\n+      spatial_start_dim = 2\n+    inputs = tf.expand_dims(inputs, spatial_start_dim)\n+    depthwise_kernel = tf.expand_dims(self.depthwise_kernel, axis=0)\n+    dilation_rate = (1,) + self.dilation_rate\n+\n+    outputs = tf.nn.depthwise_conv2d(\n+        inputs,\n+        depthwise_kernel,\n+        strides=strides,\n+        padding=self.padding.upper(),\n+        dilations=dilation_rate,\n+        data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    outputs = tf.squeeze(outputs, [spatial_start_dim])\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+\n+    return outputs\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if self.data_format == 'channels_first':\n+      rows = input_shape[2]\n+      out_filters = input_shape[1] * self.depth_multiplier\n+    elif self.data_format == 'channels_last':\n+      rows = input_shape[1]\n+      out_filters = input_shape[2] * self.depth_multiplier\n+\n+    rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n+                                         self.padding, self.strides[0],\n+                                         self.dilation_rate[0])\n+    if self.data_format == 'channels_first':\n+      return (input_shape[0], out_filters, rows)\n+    elif self.data_format == 'channels_last':\n+      return (input_shape[0], rows, out_filters)\n\n@@ -0,0 +1,196 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras depthwise 2D convolution.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.convolutional.base_depthwise_conv import DepthwiseConv\n+from keras.utils import conv_utils\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.DepthwiseConv2D')\n+class DepthwiseConv2D(DepthwiseConv):\n+  \"\"\"Depthwise 2D convolution.\n+\n+  Depthwise convolution is a type of convolution in which each input channel is\n+  convolved with a different kernel (called a depthwise kernel). You\n+  can understand depthwise convolution as the first step in a depthwise\n+  separable convolution.\n+\n+  It is implemented via the following steps:\n+\n+  - Split the input into individual channels.\n+  - Convolve each channel with an individual depthwise kernel with\n+    `depth_multiplier` output channels.\n+  - Concatenate the convolved outputs along the channels axis.\n+\n+  Unlike a regular 2D convolution, depthwise convolution does not mix\n+  information across different input channels.\n+\n+  The `depth_multiplier` argument determines how many filter are applied to one\n+  input channel. As such, it controls the amount of output channels that are\n+  generated per input channel in the depthwise step.\n+\n+  Args:\n+    kernel_size: An integer or tuple/list of 2 integers, specifying the height\n+      and width of the 2D convolution window. Can be a single integer to specify\n+      the same value for all spatial dimensions.\n+    strides: An integer or tuple/list of 2 integers, specifying the strides of\n+      the convolution along the height and width. Can be a single integer to\n+      specify the same value for all spatial dimensions. Specifying any stride\n+      value != 1 is incompatible with specifying any `dilation_rate` value != 1.\n+    padding: one of `'valid'` or `'same'` (case-insensitive). `\"valid\"` means no\n+      padding. `\"same\"` results in padding with zeros evenly to the left/right\n+      or up/down of the input such that output has the same height/width\n+      dimension as the input.\n+    depth_multiplier: The number of depthwise convolution output channels for\n+      each input channel. The total number of depthwise convolution output\n+      channels will be equal to `filters_in * depth_multiplier`.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs. `channels_last` corresponds\n+      to inputs with shape `(batch_size, height, width, channels)` while\n+      `channels_first` corresponds to inputs with shape `(batch_size, channels,\n+      height, width)`. It defaults to the `image_data_format` value found in\n+      your Keras config file at `~/.keras/keras.json`. If you never set it, then\n+      it will be 'channels_last'.\n+    dilation_rate: An integer or tuple/list of 2 integers, specifying the\n+      dilation rate to use for dilated convolution. Currently, specifying any\n+      `dilation_rate` value != 1 is incompatible with specifying any `strides`\n+      value != 1.\n+    activation: Activation function to use. If you don't specify anything, no\n+      activation is applied (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    depthwise_initializer: Initializer for the depthwise kernel matrix (see\n+      `keras.initializers`). If None, the default initializer\n+      ('glorot_uniform') will be used.\n+    bias_initializer: Initializer for the bias vector (see\n+      `keras.initializers`). If None, the default initializer ('zeros') will be\n+      used.\n+    depthwise_regularizer: Regularizer function applied to the depthwise kernel\n+      matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector (see\n+      `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to the output of the\n+      layer (its 'activation') (see `keras.regularizers`).\n+    depthwise_constraint: Constraint function applied to the depthwise kernel\n+      matrix (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector (see\n+      `keras.constraints`).\n+\n+  Input shape:\n+    4D tensor with shape: `[batch_size, channels, rows, cols]` if\n+      data_format='channels_first'\n+    or 4D tensor with shape: `[batch_size, rows, cols, channels]` if\n+      data_format='channels_last'.\n+\n+  Output shape:\n+    4D tensor with shape: `[batch_size, channels * depth_multiplier, new_rows,\n+      new_cols]` if `data_format='channels_first'`\n+      or 4D tensor with shape: `[batch_size,\n+      new_rows, new_cols, channels * depth_multiplier]` if\n+      `data_format='channels_last'`. `rows` and `cols` values might have changed\n+      due to padding.\n+\n+  Returns:\n+    A tensor of rank 4 representing\n+    `activation(depthwiseconv2d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+    ValueError: when both `strides` > 1 and `dilation_rate` > 1.\n+  \"\"\"\n+\n+  def __init__(self,\n+               kernel_size,\n+               strides=(1, 1),\n+               padding='valid',\n+               depth_multiplier=1,\n+               data_format=None,\n+               dilation_rate=(1, 1),\n+               activation=None,\n+               use_bias=True,\n+               depthwise_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               depthwise_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               depthwise_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(DepthwiseConv2D, self).__init__(\n+        2,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        depth_multiplier=depth_multiplier,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        activation=activation,\n+        use_bias=use_bias,\n+        depthwise_initializer=depthwise_initializer,\n+        bias_initializer=bias_initializer,\n+        depthwise_regularizer=depthwise_regularizer,\n+        bias_regularizer=bias_regularizer,\n+        activity_regularizer=activity_regularizer,\n+        depthwise_constraint=depthwise_constraint,\n+        bias_constraint=bias_constraint,\n+        **kwargs)\n+\n+  def call(self, inputs):\n+    outputs = backend.depthwise_conv2d(\n+        inputs,\n+        self.depthwise_kernel,\n+        strides=self.strides,\n+        padding=self.padding,\n+        dilation_rate=self.dilation_rate,\n+        data_format=self.data_format)\n+\n+    if self.use_bias:\n+      outputs = backend.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=self.data_format)\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+\n+    return outputs\n+\n+  @tf_utils.shape_type_conversion\n+  def compute_output_shape(self, input_shape):\n+    if self.data_format == 'channels_first':\n+      rows = input_shape[2]\n+      cols = input_shape[3]\n+      out_filters = input_shape[1] * self.depth_multiplier\n+    elif self.data_format == 'channels_last':\n+      rows = input_shape[1]\n+      cols = input_shape[2]\n+      out_filters = input_shape[3] * self.depth_multiplier\n+\n+    rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n+                                         self.padding,\n+                                         self.strides[0],\n+                                         self.dilation_rate[0])\n+    cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n+                                         self.padding,\n+                                         self.strides[1],\n+                                         self.dilation_rate[1])\n+    if self.data_format == 'channels_first':\n+      return (input_shape[0], out_filters, rows, cols)\n+    elif self.data_format == 'channels_last':\n+      return (input_shape[0], rows, cols, out_filters)\n\n@@ -0,0 +1,137 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for depthwise convolutional layers.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class DepthwiseConv1DTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, kwargs, expected_output_shape=None):\n+    num_samples = 2\n+    stack_size = 3\n+    num_row = 7\n+\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.DepthwiseConv1D,\n+          kwargs=kwargs,\n+          input_shape=(num_samples, num_row, stack_size),\n+          expected_output_shape=expected_output_shape)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {\n+          'padding': 'valid'\n+      }),\n+      ('padding_same', {\n+          'padding': 'same'\n+      }),\n+      ('strides', {\n+          'strides': 2\n+      }),\n+      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+      # TODO(b/62340061): Support channels_first on CPU.\n+      ('data_format', {\n+          'data_format': 'channels_first'\n+      }),\n+      ('depth_multiplier_1', {\n+          'depth_multiplier': 1\n+      }),\n+      ('depth_multiplier_2', {\n+          'depth_multiplier': 2\n+      }),\n+      ('dilation_rate', {\n+          'dilation_rate': 2\n+      }, (None, 3, 3)),\n+  )\n+  def test_depthwise_conv1d(self, kwargs, expected_output_shape=None):\n+    kwargs['kernel_size'] = 3\n+    if 'data_format' not in kwargs or tf.test.is_gpu_available(cuda_only=True):\n+      self._run_test(kwargs, expected_output_shape)\n+\n+  def test_depthwise_conv1d_full(self):\n+    kwargs = {\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'data_format': 'channels_last',\n+        'dilation_rate': 1,\n+        'activation': None,\n+        'depthwise_regularizer': 'l2',\n+        'bias_regularizer': 'l2',\n+        'activity_regularizer': 'l2',\n+        'depthwise_constraint': 'unit_norm',\n+        'use_bias': True,\n+        'strides': 2,\n+        'depth_multiplier': 1,\n+    }\n+    self._run_test(kwargs)\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class DepthwiseConv2DTest(keras_parameterized.TestCase):\n+\n+  def _run_test(self, kwargs, expected_output_shape=None):\n+    num_samples = 2\n+    stack_size = 3\n+    num_row = 7\n+    num_col = 6\n+\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.DepthwiseConv2D,\n+          kwargs=kwargs,\n+          input_shape=(num_samples, num_row, num_col, stack_size),\n+          expected_output_shape=expected_output_shape)\n+\n+  @parameterized.named_parameters(\n+      ('padding_valid', {'padding': 'valid'}),\n+      ('padding_same', {'padding': 'same'}),\n+      ('strides', {'strides': (2, 2)}),\n+      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+      # TODO(b/62340061): Support channels_first on CPU.\n+      ('data_format', {'data_format': 'channels_first'}),\n+      ('depth_multiplier_1', {'depth_multiplier': 1}),\n+      ('depth_multiplier_2', {'depth_multiplier': 2}),\n+      ('dilation_rate', {'dilation_rate': (2, 2)}, (None, 3, 2, 3)),\n+  )\n+  def test_depthwise_conv2d(self, kwargs, expected_output_shape=None):\n+    kwargs['kernel_size'] = (3, 3)\n+    if 'data_format' not in kwargs or tf.test.is_gpu_available(cuda_only=True):\n+      self._run_test(kwargs, expected_output_shape)\n+\n+  def test_depthwise_conv2d_full(self):\n+    kwargs = {\n+        'kernel_size': 3,\n+        'padding': 'valid',\n+        'data_format': 'channels_last',\n+        'dilation_rate': (1, 1),\n+        'activation': None,\n+        'depthwise_regularizer': 'l2',\n+        'bias_regularizer': 'l2',\n+        'activity_regularizer': 'l2',\n+        'depthwise_constraint': 'unit_norm',\n+        'use_bias': True,\n+        'strides': (2, 2),\n+        'depth_multiplier': 1,\n+    }\n+    self._run_test(kwargs)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,206 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras depthwise separable 1D convolution.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.convolutional.base_separable_conv import SeparableConv\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SeparableConv1D',\n+              'keras.layers.SeparableConvolution1D')\n+class SeparableConv1D(SeparableConv):\n+  \"\"\"Depthwise separable 1D convolution.\n+\n+  This layer performs a depthwise convolution that acts separately on\n+  channels, followed by a pointwise convolution that mixes channels.\n+  If `use_bias` is True and a bias initializer is provided,\n+  it adds a bias vector to the output.\n+  It then optionally applies an activation function to produce the final output.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space (i.e. the number\n+      of filters in the convolution).\n+    kernel_size: A single integer specifying the spatial\n+      dimensions of the filters.\n+    strides: A single integer specifying the strides\n+      of the convolution.\n+      Specifying any `stride` value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: One of `\"valid\"`, `\"same\"`, or `\"causal\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input. `\"causal\"` results in causal\n+      (dilated) convolutions, e.g. `output[t]` does not depend on `input[t+1:]`.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, length, channels)` while `channels_first` corresponds to\n+      inputs with shape `(batch_size, channels, length)`.\n+    dilation_rate: A single integer, specifying\n+      the dilation rate to use for dilated convolution.\n+    depth_multiplier: The number of depthwise convolution output channels for\n+      each input channel. The total number of depthwise convolution output\n+      channels will be equal to `num_filters_in * depth_multiplier`.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias.\n+    depthwise_initializer: An initializer for the depthwise convolution kernel\n+      (see `keras.initializers`). If None, then the default initializer\n+      ('glorot_uniform') will be used.\n+    pointwise_initializer: An initializer for the pointwise convolution kernel\n+      (see `keras.initializers`). If None, then the default initializer\n+      ('glorot_uniform') will be used.\n+    bias_initializer: An initializer for the bias vector. If None, the default\n+      initializer ('zeros') will be used (see `keras.initializers`).\n+    depthwise_regularizer: Optional regularizer for the depthwise\n+      convolution kernel (see `keras.regularizers`).\n+    pointwise_regularizer: Optional regularizer for the pointwise\n+      convolution kernel (see `keras.regularizers`).\n+    bias_regularizer: Optional regularizer for the bias vector\n+      (see `keras.regularizers`).\n+    activity_regularizer: Optional regularizer function for the output\n+      (see `keras.regularizers`).\n+    depthwise_constraint: Optional projection function to be applied to the\n+      depthwise kernel after being updated by an `Optimizer` (e.g. used for\n+      norm constraints or value constraints for layer weights). The function\n+      must take as input the unprojected variable and must return the\n+      projected variable (which must have the same shape). Constraints are\n+      not safe to use when doing asynchronous distributed training\n+      (see `keras.constraints`).\n+    pointwise_constraint: Optional projection function to be applied to the\n+      pointwise kernel after being updated by an `Optimizer`\n+      (see `keras.constraints`).\n+    bias_constraint: Optional projection function to be applied to the\n+      bias after being updated by an `Optimizer`\n+      (see `keras.constraints`).\n+    trainable: Boolean, if `True` the weights of this layer will be marked as\n+      trainable (and listed in `layer.trainable_weights`).\n+\n+  Input shape:\n+    3D tensor with shape:\n+    `(batch_size, channels, steps)` if data_format='channels_first'\n+    or 3D tensor with shape:\n+    `(batch_size, steps, channels)` if data_format='channels_last'.\n+\n+  Output shape:\n+    3D tensor with shape:\n+    `(batch_size, filters, new_steps)` if data_format='channels_first'\n+    or 3D tensor with shape:\n+    `(batch_size,  new_steps, filters)` if data_format='channels_last'.\n+    `new_steps` value might have changed due to padding or strides.\n+\n+  Returns:\n+    A tensor of rank 3 representing\n+    `activation(separableconv1d(inputs, kernel) + bias)`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=1,\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=1,\n+               depth_multiplier=1,\n+               activation=None,\n+               use_bias=True,\n+               depthwise_initializer='glorot_uniform',\n+               pointwise_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               depthwise_regularizer=None,\n+               pointwise_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               depthwise_constraint=None,\n+               pointwise_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(SeparableConv1D, self).__init__(\n+        rank=1,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        depth_multiplier=depth_multiplier,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        depthwise_initializer=initializers.get(depthwise_initializer),\n+        pointwise_initializer=initializers.get(pointwise_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        depthwise_regularizer=regularizers.get(depthwise_regularizer),\n+        pointwise_regularizer=regularizers.get(pointwise_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        depthwise_constraint=constraints.get(depthwise_constraint),\n+        pointwise_constraint=constraints.get(pointwise_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+  def call(self, inputs):\n+    if self.padding == 'causal':\n+      inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n+    if self.data_format == 'channels_last':\n+      strides = (1,) + self.strides * 2 + (1,)\n+      spatial_start_dim = 1\n+    else:\n+      strides = (1, 1) + self.strides * 2\n+      spatial_start_dim = 2\n+\n+    # Explicitly broadcast inputs and kernels to 4D.\n+    # TODO(fchollet): refactor when a native separable_conv1d op is available.\n+    inputs = tf.expand_dims(inputs, spatial_start_dim)\n+    depthwise_kernel = tf.expand_dims(self.depthwise_kernel, 0)\n+    pointwise_kernel = tf.expand_dims(self.pointwise_kernel, 0)\n+    dilation_rate = (1,) + self.dilation_rate\n+\n+    if self.padding == 'causal':\n+      op_padding = 'valid'\n+    else:\n+      op_padding = self.padding\n+    outputs = tf.compat.v1.nn.separable_conv2d(\n+        inputs,\n+        depthwise_kernel,\n+        pointwise_kernel,\n+        strides=strides,\n+        padding=op_padding.upper(),\n+        rate=dilation_rate,\n+        data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    outputs = tf.squeeze(outputs, [spatial_start_dim])\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+# Alias\n+\n+SeparableConvolution1D = SeparableConv1D\n\n@@ -0,0 +1,201 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras depthwise separable 2D convolution.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import activations\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers.convolutional.base_separable_conv import SeparableConv\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.SeparableConv2D',\n+              'keras.layers.SeparableConvolution2D')\n+class SeparableConv2D(SeparableConv):\n+  \"\"\"Depthwise separable 2D convolution.\n+\n+  Separable convolutions consist of first performing\n+  a depthwise spatial convolution\n+  (which acts on each input channel separately)\n+  followed by a pointwise convolution which mixes the resulting\n+  output channels. The `depth_multiplier` argument controls how many\n+  output channels are generated per input channel in the depthwise step.\n+\n+  Intuitively, separable convolutions can be understood as\n+  a way to factorize a convolution kernel into two smaller kernels,\n+  or as an extreme version of an Inception block.\n+\n+  Args:\n+    filters: Integer, the dimensionality of the output space\n+      (i.e. the number of output filters in the convolution).\n+    kernel_size: An integer or tuple/list of 2 integers, specifying the\n+      height and width of the 2D convolution window.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    strides: An integer or tuple/list of 2 integers,\n+      specifying the strides of the convolution along the height and width.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions. Current implementation only supports equal\n+      length strides in the row and column dimensions.\n+      Specifying any stride value != 1 is incompatible with specifying\n+      any `dilation_rate` value != 1.\n+    padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n+      to the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch_size, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    dilation_rate: An integer or tuple/list of 2 integers, specifying\n+      the dilation rate to use for dilated convolution.\n+    depth_multiplier: The number of depthwise convolution output channels\n+      for each input channel.\n+      The total number of depthwise convolution output\n+      channels will be equal to `filters_in * depth_multiplier`.\n+    activation: Activation function to use.\n+      If you don't specify anything, no activation is applied\n+      (see `keras.activations`).\n+    use_bias: Boolean, whether the layer uses a bias vector.\n+    depthwise_initializer: An initializer for the depthwise convolution kernel\n+      (see `keras.initializers`). If None, then the default initializer\n+      ('glorot_uniform') will be used.\n+    pointwise_initializer: An initializer for the pointwise convolution kernel\n+      (see `keras.initializers`). If None, then the default initializer\n+      ('glorot_uniform') will be used.\n+    bias_initializer: An initializer for the bias vector. If None, the default\n+      initializer ('zeros') will be used (see `keras.initializers`).\n+    depthwise_regularizer: Regularizer function applied to\n+      the depthwise kernel matrix (see `keras.regularizers`).\n+    pointwise_regularizer: Regularizer function applied to\n+      the pointwise kernel matrix (see `keras.regularizers`).\n+    bias_regularizer: Regularizer function applied to the bias vector\n+      (see `keras.regularizers`).\n+    activity_regularizer: Regularizer function applied to\n+      the output of the layer (its \"activation\")\n+      (see `keras.regularizers`).\n+    depthwise_constraint: Constraint function applied to\n+      the depthwise kernel matrix\n+      (see `keras.constraints`).\n+    pointwise_constraint: Constraint function applied to\n+      the pointwise kernel matrix\n+      (see `keras.constraints`).\n+    bias_constraint: Constraint function applied to the bias vector\n+      (see `keras.constraints`).\n+\n+  Input shape:\n+    4D tensor with shape:\n+    `(batch_size, channels, rows, cols)` if data_format='channels_first'\n+    or 4D tensor with shape:\n+    `(batch_size, rows, cols, channels)` if data_format='channels_last'.\n+\n+  Output shape:\n+    4D tensor with shape:\n+    `(batch_size, filters, new_rows, new_cols)` if data_format='channels_first'\n+    or 4D tensor with shape:\n+    `(batch_size, new_rows, new_cols, filters)` if data_format='channels_last'.\n+    `rows` and `cols` values might have changed due to padding.\n+\n+  Returns:\n+    A tensor of rank 4 representing\n+    `activation(separableconv2d(inputs, kernel) + bias)`.\n+\n+  Raises:\n+    ValueError: if `padding` is \"causal\".\n+  \"\"\"\n+\n+  def __init__(self,\n+               filters,\n+               kernel_size,\n+               strides=(1, 1),\n+               padding='valid',\n+               data_format=None,\n+               dilation_rate=(1, 1),\n+               depth_multiplier=1,\n+               activation=None,\n+               use_bias=True,\n+               depthwise_initializer='glorot_uniform',\n+               pointwise_initializer='glorot_uniform',\n+               bias_initializer='zeros',\n+               depthwise_regularizer=None,\n+               pointwise_regularizer=None,\n+               bias_regularizer=None,\n+               activity_regularizer=None,\n+               depthwise_constraint=None,\n+               pointwise_constraint=None,\n+               bias_constraint=None,\n+               **kwargs):\n+    super(SeparableConv2D, self).__init__(\n+        rank=2,\n+        filters=filters,\n+        kernel_size=kernel_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        dilation_rate=dilation_rate,\n+        depth_multiplier=depth_multiplier,\n+        activation=activations.get(activation),\n+        use_bias=use_bias,\n+        depthwise_initializer=initializers.get(depthwise_initializer),\n+        pointwise_initializer=initializers.get(pointwise_initializer),\n+        bias_initializer=initializers.get(bias_initializer),\n+        depthwise_regularizer=regularizers.get(depthwise_regularizer),\n+        pointwise_regularizer=regularizers.get(pointwise_regularizer),\n+        bias_regularizer=regularizers.get(bias_regularizer),\n+        activity_regularizer=regularizers.get(activity_regularizer),\n+        depthwise_constraint=constraints.get(depthwise_constraint),\n+        pointwise_constraint=constraints.get(pointwise_constraint),\n+        bias_constraint=constraints.get(bias_constraint),\n+        **kwargs)\n+\n+  def call(self, inputs):\n+    # Apply the actual ops.\n+    if self.data_format == 'channels_last':\n+      strides = (1,) + self.strides + (1,)\n+    else:\n+      strides = (1, 1) + self.strides\n+    outputs = tf.compat.v1.nn.separable_conv2d(\n+        inputs,\n+        self.depthwise_kernel,\n+        self.pointwise_kernel,\n+        strides=strides,\n+        padding=self.padding.upper(),\n+        rate=self.dilation_rate,\n+        data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    if self.use_bias:\n+      outputs = tf.nn.bias_add(\n+          outputs,\n+          self.bias,\n+          data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n+\n+    if self.activation is not None:\n+      return self.activation(outputs)\n+    return outputs\n+\n+# Alias\n+\n+SeparableConvolution2D = SeparableConv2D\n\n@@ -14,14 +14,12 @@\n # ==============================================================================\n \"\"\"Tests for separable convolutional layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n from absl.testing import parameterized\n-import numpy as np\n-\n import keras\n from keras import keras_parameterized\n from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes\n\n@@ -1,1272 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for convolutional layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from absl.testing import parameterized\n-import numpy as np\n-\n-import keras\n-from tensorflow.python.framework import test_util\n-from keras import keras_parameterized\n-from keras import testing_utils\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class Conv1DTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape):\n-    num_samples = 2\n-    stack_size = 3\n-    length = 7\n-\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.Conv1D,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, length, stack_size),\n-          expected_output_shape=expected_output_shape)\n-\n-  def _run_test_extra_batch_dim(self, kwargs, expected_output_shape):\n-    batch_shape = (2, 11)\n-    stack_size = 3\n-    length = 7\n-\n-    with self.cached_session():\n-      if expected_output_shape is not None:\n-        expected_output_shape = (None,) + expected_output_shape\n-\n-      testing_utils.layer_test(\n-          keras.layers.Conv1D,\n-          kwargs=kwargs,\n-          input_shape=batch_shape + (length, stack_size),\n-          expected_output_shape=expected_output_shape)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {\n-          'padding': 'valid'\n-      }, (None, 5, 2)),\n-      ('padding_same', {\n-          'padding': 'same'\n-      }, (None, 7, 2)),\n-      ('padding_same_dilation_2', {\n-          'padding': 'same',\n-          'dilation_rate': 2\n-      }, (None, 7, 2)),\n-      ('padding_same_dilation_3', {\n-          'padding': 'same',\n-          'dilation_rate': 3\n-      }, (None, 7, 2)),\n-      ('padding_causal', {\n-          'padding': 'causal'\n-      }, (None, 7, 2)),\n-      ('strides', {\n-          'strides': 2\n-      }, (None, 3, 2)),\n-      ('dilation_rate', {\n-          'dilation_rate': 2\n-      }, (None, 3, 2)),\n-      ('group', {\n-          'groups': 3,\n-          'filters': 6\n-      }, (None, 5, 6)),\n-  )\n-  def test_conv1d(self, kwargs, expected_output_shape):\n-    kwargs['filters'] = kwargs.get('filters', 2)\n-    kwargs['kernel_size'] = 3\n-    self._run_test(kwargs, expected_output_shape)\n-    self._run_test_extra_batch_dim(kwargs, expected_output_shape)\n-\n-  def test_conv1d_regularizers(self):\n-    kwargs = {\n-        'filters': 3,\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'kernel_regularizer': 'l2',\n-        'bias_regularizer': 'l2',\n-        'activity_regularizer': 'l2',\n-        'strides': 1\n-    }\n-    with self.cached_session():\n-      layer = keras.layers.Conv1D(**kwargs)\n-      layer.build((None, 5, 2))\n-      self.assertEqual(len(layer.losses), 2)\n-      layer(keras.backend.variable(np.ones((1, 5, 2))))\n-      self.assertEqual(len(layer.losses), 3)\n-\n-  def test_conv1d_constraints(self):\n-    k_constraint = lambda x: x\n-    b_constraint = lambda x: x\n-\n-    kwargs = {\n-        'filters': 3,\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'kernel_constraint': k_constraint,\n-        'bias_constraint': b_constraint,\n-        'strides': 1\n-    }\n-    with self.cached_session():\n-      layer = keras.layers.Conv1D(**kwargs)\n-      layer.build((None, 5, 2))\n-      self.assertEqual(layer.kernel.constraint, k_constraint)\n-      self.assertEqual(layer.bias.constraint, b_constraint)\n-\n-  def test_conv1d_recreate_conv(self):\n-    with self.cached_session():\n-      layer = keras.layers.Conv1D(filters=1,\n-                                  kernel_size=3,\n-                                  strides=1,\n-                                  dilation_rate=2,\n-                                  padding='causal')\n-      inpt1 = np.random.normal(size=[1, 2, 1])\n-      inpt2 = np.random.normal(size=[1, 1, 1])\n-      outp1_shape = layer(inpt1).shape\n-      _ = layer(inpt2).shape\n-      self.assertEqual(outp1_shape, layer(inpt1).shape)\n-\n-  def test_conv1d_recreate_conv_unknown_dims(self):\n-    with self.cached_session():\n-      layer = keras.layers.Conv1D(filters=1,\n-                                  kernel_size=3,\n-                                  strides=1,\n-                                  dilation_rate=2,\n-                                  padding='causal')\n-\n-      inpt1 = np.random.normal(size=[1, 9, 1]).astype(np.float32)\n-      inpt2 = np.random.normal(size=[1, 2, 1]).astype(np.float32)\n-      outp1_shape = layer(inpt1).shape\n-\n-      @tf.function(input_signature=[\n-          tf.TensorSpec([1, None, 1])])\n-      def fn(inpt):\n-        return layer(inpt)\n-\n-      fn(inpt2)\n-      self.assertEqual(outp1_shape, layer(inpt1).shape)\n-\n-  def test_conv1d_invalid_output_shapes(self):\n-    kwargs = {'filters': 2, 'kernel_size': 20}\n-    with self.assertRaisesRegex(\n-        ValueError, r\"\"\"One of the dimensions in the output is <= 0\"\"\"):\n-      layer = keras.layers.convolutional.Conv1D(**kwargs)\n-      layer.build((None, 5, 2))\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class Conv2DTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape, spatial_shape=(7, 6)):\n-    num_samples = 2\n-    stack_size = 3\n-    num_row, num_col = spatial_shape\n-    input_data = None\n-    # Generate valid input data.\n-    if None in spatial_shape:\n-      input_data_shape = (num_samples, num_row or 7, num_col or 6, stack_size)\n-      input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n-\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.Conv2D,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, num_row, num_col, stack_size),\n-          input_data=input_data,\n-          expected_output_shape=expected_output_shape)\n-\n-  def _run_test_extra_batch_dim(self,\n-                                kwargs,\n-                                expected_output_shape,\n-                                spatial_shape=(7, 6)):\n-    batch_shape = (2, 11)\n-    stack_size = 3\n-    num_row, num_col = spatial_shape\n-    input_data = None\n-    # Generate valid input data.\n-    if None in spatial_shape:\n-      input_data_shape = batch_shape + (num_row or 7, num_col or 6, stack_size)\n-      input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n-\n-    with self.cached_session():\n-      if expected_output_shape is not None:\n-        expected_output_shape = (None,) + expected_output_shape\n-      testing_utils.layer_test(\n-          keras.layers.Conv2D,\n-          kwargs=kwargs,\n-          input_shape=batch_shape + (num_row, num_col, stack_size),\n-          input_data=input_data,\n-          expected_output_shape=expected_output_shape)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {\n-          'padding': 'valid'\n-      }, (None, 5, 4, 2)),\n-      ('padding_same', {\n-          'padding': 'same'\n-      }, (None, 7, 6, 2)),\n-      ('padding_same_dilation_2', {\n-          'padding': 'same',\n-          'dilation_rate': 2\n-      }, (None, 7, 6, 2)),\n-      ('strides', {\n-          'strides': (2, 2)\n-      }, (None, 3, 2, 2)),\n-      ('dilation_rate', {\n-          'dilation_rate': (2, 2)\n-      }, (None, 3, 2, 2)),\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {\n-          'data_format': 'channels_first'\n-      }, None, True),\n-      ('group', {\n-          'groups': 3,\n-          'filters': 6\n-      }, (None, 5, 4, 6), False),\n-      ('dilation_2_unknown_width', {\n-          'dilation_rate': (2, 2)\n-      }, (None, None, 2, 2), False, (None, 6)),\n-      ('dilation_2_unknown_height', {\n-          'dilation_rate': (2, 2)\n-      }, (None, 3, None, 2), False, (7, None)),\n-  )\n-  def test_conv2d(self,\n-                  kwargs,\n-                  expected_output_shape=None,\n-                  requires_gpu=False,\n-                  spatial_shape=(7, 6)):\n-    kwargs['filters'] = kwargs.get('filters', 2)\n-    kwargs['kernel_size'] = (3, 3)\n-    if not requires_gpu or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs, expected_output_shape, spatial_shape)\n-      self._run_test_extra_batch_dim(kwargs, expected_output_shape,\n-                                     spatial_shape)\n-\n-  def test_conv2d_regularizers(self):\n-    kwargs = {\n-        'filters': 3,\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'kernel_regularizer': 'l2',\n-        'bias_regularizer': 'l2',\n-        'activity_regularizer': 'l2',\n-        'strides': 1\n-    }\n-    with self.cached_session():\n-      layer = keras.layers.Conv2D(**kwargs)\n-      layer.build((None, 5, 5, 2))\n-      self.assertEqual(len(layer.losses), 2)\n-      layer(keras.backend.variable(np.ones((1, 5, 5, 2))))\n-      self.assertEqual(len(layer.losses), 3)\n-\n-  def test_conv2d_constraints(self):\n-    k_constraint = lambda x: x\n-    b_constraint = lambda x: x\n-\n-    kwargs = {\n-        'filters': 3,\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'kernel_constraint': k_constraint,\n-        'bias_constraint': b_constraint,\n-        'strides': 1\n-    }\n-    with self.cached_session():\n-      layer = keras.layers.Conv2D(**kwargs)\n-      layer.build((None, 5, 5, 2))\n-      self.assertEqual(layer.kernel.constraint, k_constraint)\n-      self.assertEqual(layer.bias.constraint, b_constraint)\n-\n-  def test_conv2d_zero_kernel_size(self):\n-    kwargs = {'filters': 2, 'kernel_size': 0}\n-    with self.assertRaises(ValueError):\n-      keras.layers.Conv2D(**kwargs)\n-\n-  def test_conv2d_invalid_output_shapes(self):\n-    kwargs = {'filters': 2, 'kernel_size': 20}\n-    with self.assertRaisesRegex(\n-        ValueError, r\"\"\"One of the dimensions in the output is <= 0\"\"\"):\n-      layer = keras.layers.convolutional.Conv2D(**kwargs)\n-      layer.build((None, 5, 5, 2))\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class Conv3DTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape, validate_training=True):\n-    num_samples = 2\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-    depth = 5\n-\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.Conv3D,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, depth, num_row, num_col, stack_size),\n-          expected_output_shape=expected_output_shape,\n-          validate_training=validate_training)\n-\n-  def _run_test_extra_batch_dim(self,\n-                                kwargs,\n-                                expected_output_shape,\n-                                validate_training=True):\n-    batch_shape = (2, 11)\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-    depth = 5\n-\n-    with self.cached_session():\n-      if expected_output_shape is not None:\n-        expected_output_shape = (None,) + expected_output_shape\n-\n-      testing_utils.layer_test(\n-          keras.layers.Conv3D,\n-          kwargs=kwargs,\n-          input_shape=batch_shape + (depth, num_row, num_col, stack_size),\n-          expected_output_shape=expected_output_shape,\n-          validate_training=validate_training)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {\n-          'padding': 'valid'\n-      }, (None, 3, 5, 4, 2)),\n-      ('padding_same', {\n-          'padding': 'same'\n-      }, (None, 5, 7, 6, 2)),\n-      ('strides', {\n-          'strides': (2, 2, 2)\n-      }, (None, 2, 3, 2, 2)),\n-      ('dilation_rate', {\n-          'dilation_rate': (2, 2, 2)\n-      }, (None, 1, 3, 2, 2)),\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {\n-          'data_format': 'channels_first'\n-      }, None, True),\n-      ('group', {\n-          'groups': 3,\n-          'filters': 6\n-      }, (None, 3, 5, 4, 6)),\n-  )\n-  def test_conv3d(self, kwargs, expected_output_shape=None, requires_gpu=False):\n-    kwargs['filters'] = kwargs.get('filters', 2)\n-    kwargs['kernel_size'] = (3, 3, 3)\n-    # train_on_batch currently fails with XLA enabled on GPUs\n-    test_training = 'groups' not in kwargs or not test_util.is_xla_enabled()\n-    if not requires_gpu or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs, expected_output_shape, test_training)\n-      self._run_test_extra_batch_dim(kwargs, expected_output_shape,\n-                                     test_training)\n-\n-  def test_conv3d_regularizers(self):\n-    kwargs = {\n-        'filters': 3,\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'kernel_regularizer': 'l2',\n-        'bias_regularizer': 'l2',\n-        'activity_regularizer': 'l2',\n-        'strides': 1\n-    }\n-    with self.cached_session():\n-      layer = keras.layers.Conv3D(**kwargs)\n-      layer.build((None, 5, 5, 5, 2))\n-      self.assertEqual(len(layer.losses), 2)\n-      self.assertEqual(len(layer.losses), 2)\n-      layer(keras.backend.variable(np.ones((1, 5, 5, 5, 2))))\n-      self.assertEqual(len(layer.losses), 3)\n-\n-  def test_conv3d_constraints(self):\n-    k_constraint = lambda x: x\n-    b_constraint = lambda x: x\n-\n-    kwargs = {\n-        'filters': 3,\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'kernel_constraint': k_constraint,\n-        'bias_constraint': b_constraint,\n-        'strides': 1\n-    }\n-    with self.cached_session():\n-      layer = keras.layers.Conv3D(**kwargs)\n-      layer.build((None, 5, 5, 5, 2))\n-      self.assertEqual(layer.kernel.constraint, k_constraint)\n-      self.assertEqual(layer.bias.constraint, b_constraint)\n-\n-  def test_conv3d_dynamic_shape(self):\n-    input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)\n-    with self.cached_session():\n-      # Won't raise error here.\n-      testing_utils.layer_test(\n-          keras.layers.Conv3D,\n-          kwargs={\n-              'data_format': 'channels_last',\n-              'filters': 3,\n-              'kernel_size': 3\n-          },\n-          input_shape=(None, None, None, None, 3),\n-          input_data=input_data)\n-      if tf.test.is_gpu_available(cuda_only=True):\n-        testing_utils.layer_test(\n-            keras.layers.Conv3D,\n-            kwargs={\n-                'data_format': 'channels_first',\n-                'filters': 3,\n-                'kernel_size': 3\n-            },\n-            input_shape=(None, 3, None, None, None),\n-            input_data=input_data)\n-\n-  def test_conv3d_invalid_output_shapes(self):\n-    kwargs = {'filters': 2, 'kernel_size': 20}\n-    with self.assertRaisesRegex(\n-        ValueError, r\"\"\"One of the dimensions in the output is <= 0\"\"\"):\n-      layer = keras.layers.convolutional.Conv3D(**kwargs)\n-      layer.build((None, 5, 5, 5, 2))\n-\n-\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class GroupedConvTest(keras_parameterized.TestCase):\n-\n-  @parameterized.named_parameters(\n-      ('Conv1D', keras.layers.Conv1D),\n-      ('Conv2D', keras.layers.Conv2D),\n-      ('Conv3D', keras.layers.Conv3D),\n-  )\n-  def test_group_conv_incorrect_use(self, layer):\n-    with self.assertRaisesRegex(ValueError, 'The number of filters'):\n-      layer(16, 3, groups=3)\n-    with self.assertRaisesRegex(ValueError, 'The number of input channels'):\n-      layer(16, 3, groups=4).build((32, 12, 12, 3))\n-\n-  @parameterized.named_parameters(\n-      ('Conv1D', keras.layers.Conv1D, (32, 12, 32)),\n-      ('Conv2D', keras.layers.Conv2D, (32, 12, 12, 32)),\n-      ('Conv3D', keras.layers.Conv3D, (32, 12, 12, 12, 32)),\n-  )\n-  def test_group_conv(self, layer_cls, input_shape):\n-    if tf.test.is_gpu_available(cuda_only=True):\n-      with testing_utils.use_gpu():\n-        inputs = tf.random.uniform(shape=input_shape)\n-\n-        layer = layer_cls(16, 3, groups=4, use_bias=False)\n-        layer.build(input_shape)\n-\n-        input_slices = tf.split(inputs, 4, axis=-1)\n-        weight_slices = tf.split(layer.kernel, 4, axis=-1)\n-        expected_outputs = tf.concat([\n-            tf.nn.convolution(inputs, weights)\n-            for inputs, weights in zip(input_slices, weight_slices)\n-        ],\n-                                            axis=-1)\n-        self.assertAllClose(\n-            layer(inputs), expected_outputs, rtol=3e-5, atol=3e-5)\n-\n-  def test_group_conv_depthwise(self):\n-    if tf.test.is_gpu_available(cuda_only=True):\n-      with testing_utils.use_gpu():\n-        inputs = tf.random.uniform(shape=(3, 27, 27, 32))\n-\n-        layer = keras.layers.Conv2D(32, 3, groups=32, use_bias=False)\n-        layer.build((3, 27, 27, 32))\n-\n-        weights_dw = tf.reshape(layer.kernel, [3, 3, 32, 1])\n-        expected_outputs = tf.compat.v1.nn.depthwise_conv2d(\n-            inputs, weights_dw, strides=[1, 1, 1, 1], padding='VALID')\n-\n-        self.assertAllClose(layer(inputs), expected_outputs, rtol=1e-5)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class Conv1DTransposeTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape):\n-    num_samples = 2\n-    stack_size = 3\n-    num_col = 6\n-\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n-          keras.layers.Conv1DTranspose,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, num_col, stack_size),\n-          expected_output_shape=expected_output_shape)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'}, (None, 8, 2)),\n-      ('padding_same', {'padding': 'same'}, (None, 6, 2)),\n-      ('strides', {'strides': 2}, (None, 13, 2)),\n-      # Only runs on GPU with CUDA, dilation_rate>1 is not supported on CPU.\n-      ('dilation_rate', {'dilation_rate': 2}, (None, 10, 2)),\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {'data_format': 'channels_first'}),\n-  )\n-  def test_conv1d_transpose(self, kwargs, expected_output_shape=None):\n-    kwargs['filters'] = 2\n-    kwargs['kernel_size'] = 3\n-    if (('data_format' not in kwargs and 'dilation_rate' not in kwargs) or\n-        tf.test.is_gpu_available(cuda_only=True)):\n-      self._run_test(kwargs, expected_output_shape)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class Conv3DTransposeTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape):\n-    num_samples = 2\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-    depth = 5\n-\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n-          keras.layers.Conv3DTranspose,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, depth, num_row, num_col, stack_size),\n-          expected_output_shape=expected_output_shape)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'}, (None, 7, 9, 8, 2)),\n-      ('padding_same', {'padding': 'same'}, (None, 5, 7, 6, 2)),\n-      ('strides', {'strides': (2, 2, 2)}, (None, 11, 15, 13, 2)),\n-      ('dilation_rate', {'dilation_rate': (2, 2, 2)}, (None, 7, 9, 8, 2)),\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {'data_format': 'channels_first'}),\n-  )\n-  def test_conv3d_transpose(self, kwargs, expected_output_shape=None):\n-    kwargs['filters'] = 2\n-    kwargs['kernel_size'] = (3, 3, 3)\n-    if 'data_format' not in kwargs or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs, expected_output_shape)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class ConvSequentialTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, conv_layer_cls, kwargs, input_shape1, input_shape2,\n-                expected_output_shape1, expected_output_shape2):\n-    kwargs['filters'] = 1\n-    kwargs['kernel_size'] = 3\n-    kwargs['dilation_rate'] = 2\n-    with self.cached_session():\n-      layer = conv_layer_cls(**kwargs)\n-      output1 = layer(np.zeros(input_shape1))\n-      self.assertEqual(output1.shape, expected_output_shape1)\n-      output2 = layer(np.zeros(input_shape2))\n-      self.assertEqual(output2.shape, expected_output_shape2)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'},\n-       (1, 8, 2), (1, 5, 2), (1, 4, 1), (1, 1, 1)),\n-      ('padding_same', {'padding': 'same'},\n-       (1, 8, 2), (1, 5, 2), (1, 8, 1), (1, 5, 1)),\n-      ('padding_causal', {'padding': 'causal'},\n-       (1, 8, 2), (1, 5, 2), (1, 8, 1), (1, 5, 1)),\n-  )\n-  def test_conv1d(self, kwargs, input_shape1, input_shape2,\n-                  expected_output_shape1, expected_output_shape2):\n-    self._run_test(keras.layers.Conv1D, kwargs, input_shape1, input_shape2,\n-                   expected_output_shape1, expected_output_shape2)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'},\n-       (1, 7, 6, 2), (1, 6, 5, 2), (1, 3, 2, 1), (1, 2, 1, 1)),\n-      ('padding_same', {'padding': 'same'},\n-       (1, 7, 6, 2), (1, 6, 5, 2), (1, 7, 6, 1), (1, 6, 5, 1)),\n-  )\n-  def test_conv2d(self, kwargs, input_shape1, input_shape2,\n-                  expected_output_shape1, expected_output_shape2):\n-    self._run_test(keras.layers.Conv2D, kwargs, input_shape1, input_shape2,\n-                   expected_output_shape1, expected_output_shape2)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'},\n-       (1, 5, 7, 6, 2), (1, 8, 6, 5, 2), (1, 1, 3, 2, 1), (1, 4, 2, 1, 1)),\n-      ('padding_same', {'padding': 'same'},\n-       (1, 5, 7, 6, 2), (1, 8, 6, 5, 2), (1, 5, 7, 6, 1), (1, 8, 6, 5, 1)),\n-  )\n-  def test_conv3d(self, kwargs, input_shape1, input_shape2,\n-                  expected_output_shape1, expected_output_shape2):\n-    self._run_test(keras.layers.Conv3D, kwargs, input_shape1, input_shape2,\n-                   expected_output_shape1, expected_output_shape2)\n-\n-  def test_dynamic_shape(self):\n-    with self.cached_session():\n-      layer = keras.layers.Conv3D(2, 3)\n-      input_shape = (5, None, None, 2)\n-      inputs = keras.Input(shape=input_shape)\n-      x = layer(inputs)\n-      # Won't raise error here with None values in input shape (b/144282043).\n-      layer(x)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class ZeroPaddingTest(keras_parameterized.TestCase):\n-\n-  def test_zero_padding_1d(self):\n-    num_samples = 2\n-    input_dim = 2\n-    num_steps = 5\n-    shape = (num_samples, num_steps, input_dim)\n-    inputs = np.ones(shape)\n-\n-    with self.cached_session():\n-      # basic test\n-      testing_utils.layer_test(\n-          keras.layers.ZeroPadding1D,\n-          kwargs={'padding': 2},\n-          input_shape=inputs.shape)\n-      testing_utils.layer_test(\n-          keras.layers.ZeroPadding1D,\n-          kwargs={'padding': (1, 2)},\n-          input_shape=inputs.shape)\n-\n-      # correctness test\n-      layer = keras.layers.ZeroPadding1D(padding=2)\n-      layer.build(shape)\n-      output = layer(keras.backend.variable(inputs))\n-      if tf.executing_eagerly():\n-        np_output = output.numpy()\n-      else:\n-        np_output = keras.backend.eval(output)\n-      for offset in [0, 1, -1, -2]:\n-        np.testing.assert_allclose(np_output[:, offset, :], 0.)\n-      np.testing.assert_allclose(np_output[:, 2:-2, :], 1.)\n-\n-      layer = keras.layers.ZeroPadding1D(padding=(1, 2))\n-      layer.build(shape)\n-      output = layer(keras.backend.variable(inputs))\n-      if tf.executing_eagerly():\n-        np_output = output.numpy()\n-      else:\n-        np_output = keras.backend.eval(output)\n-      for left_offset in [0]:\n-        np.testing.assert_allclose(np_output[:, left_offset, :], 0.)\n-      for right_offset in [-1, -2]:\n-        np.testing.assert_allclose(np_output[:, right_offset, :], 0.)\n-      np.testing.assert_allclose(np_output[:, 1:-2, :], 1.)\n-      layer.get_config()\n-\n-    # test incorrect use\n-    with self.assertRaises(ValueError):\n-      keras.layers.ZeroPadding1D(padding=(1, 1, 1))\n-    with self.assertRaises(ValueError):\n-      keras.layers.ZeroPadding1D(padding=None)\n-\n-  @parameterized.named_parameters(('channels_first', 'channels_first'),\n-                                  ('channels_last', 'channels_last'))\n-  def test_zero_padding_2d(self, data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 4\n-    input_num_col = 5\n-    if data_format == 'channels_first':\n-      inputs = np.ones((num_samples, stack_size, input_num_row, input_num_col))\n-    elif data_format == 'channels_last':\n-      inputs = np.ones((num_samples, input_num_row, input_num_col, stack_size))\n-\n-    # basic test\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.ZeroPadding2D,\n-          kwargs={\n-              'padding': (2, 2),\n-              'data_format': data_format\n-          },\n-          input_shape=inputs.shape)\n-      testing_utils.layer_test(\n-          keras.layers.ZeroPadding2D,\n-          kwargs={\n-              'padding': ((1, 2), (3, 4)),\n-              'data_format': data_format\n-          },\n-          input_shape=inputs.shape)\n-\n-    # correctness test\n-    with self.cached_session():\n-      layer = keras.layers.ZeroPadding2D(\n-          padding=(2, 2), data_format=data_format)\n-      layer.build(inputs.shape)\n-      output = layer(keras.backend.variable(inputs))\n-      if tf.executing_eagerly():\n-        np_output = output.numpy()\n-      else:\n-        np_output = keras.backend.eval(output)\n-      if data_format == 'channels_last':\n-        for offset in [0, 1, -1, -2]:\n-          np.testing.assert_allclose(np_output[:, offset, :, :], 0.)\n-          np.testing.assert_allclose(np_output[:, :, offset, :], 0.)\n-        np.testing.assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n-      elif data_format == 'channels_first':\n-        for offset in [0, 1, -1, -2]:\n-          np.testing.assert_allclose(np_output[:, :, offset, :], 0.)\n-          np.testing.assert_allclose(np_output[:, :, :, offset], 0.)\n-        np.testing.assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n-\n-      layer = keras.layers.ZeroPadding2D(\n-          padding=((1, 2), (3, 4)), data_format=data_format)\n-      layer.build(inputs.shape)\n-      output = layer(keras.backend.variable(inputs))\n-      if tf.executing_eagerly():\n-        np_output = output.numpy()\n-      else:\n-        np_output = keras.backend.eval(output)\n-      if data_format == 'channels_last':\n-        for top_offset in [0]:\n-          np.testing.assert_allclose(np_output[:, top_offset, :, :], 0.)\n-        for bottom_offset in [-1, -2]:\n-          np.testing.assert_allclose(np_output[:, bottom_offset, :, :], 0.)\n-        for left_offset in [0, 1, 2]:\n-          np.testing.assert_allclose(np_output[:, :, left_offset, :], 0.)\n-        for right_offset in [-1, -2, -3, -4]:\n-          np.testing.assert_allclose(np_output[:, :, right_offset, :], 0.)\n-        np.testing.assert_allclose(np_output[:, 1:-2, 3:-4, :], 1.)\n-      elif data_format == 'channels_first':\n-        for top_offset in [0]:\n-          np.testing.assert_allclose(np_output[:, :, top_offset, :], 0.)\n-        for bottom_offset in [-1, -2]:\n-          np.testing.assert_allclose(np_output[:, :, bottom_offset, :], 0.)\n-        for left_offset in [0, 1, 2]:\n-          np.testing.assert_allclose(np_output[:, :, :, left_offset], 0.)\n-        for right_offset in [-1, -2, -3, -4]:\n-          np.testing.assert_allclose(np_output[:, :, :, right_offset], 0.)\n-        np.testing.assert_allclose(np_output[:, :, 1:-2, 3:-4], 1.)\n-\n-    # test incorrect use\n-    with self.assertRaises(ValueError):\n-      keras.layers.ZeroPadding2D(padding=(1, 1, 1))\n-    with self.assertRaises(ValueError):\n-      keras.layers.ZeroPadding2D(padding=None)\n-\n-  @parameterized.named_parameters(('channels_first', 'channels_first'),\n-                                  ('channels_last', 'channels_last'))\n-  def test_zero_padding_3d(self, data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 4\n-    input_len_dim2 = 5\n-    input_len_dim3 = 3\n-\n-    if data_format == 'channels_first':\n-      inputs = np.ones((num_samples, stack_size, input_len_dim1, input_len_dim2,\n-                        input_len_dim3))\n-    elif data_format == 'channels_last':\n-      inputs = np.ones((num_samples, input_len_dim1, input_len_dim2,\n-                        input_len_dim3, stack_size))\n-\n-    with self.cached_session():\n-      # basic test\n-      testing_utils.layer_test(\n-          keras.layers.ZeroPadding3D,\n-          kwargs={\n-              'padding': (2, 2, 2),\n-              'data_format': data_format\n-          },\n-          input_shape=inputs.shape)\n-      testing_utils.layer_test(\n-          keras.layers.ZeroPadding3D,\n-          kwargs={\n-              'padding': ((1, 2), (3, 4), (0, 2)),\n-              'data_format': data_format\n-          },\n-          input_shape=inputs.shape)\n-\n-    with self.cached_session():\n-      # correctness test\n-      layer = keras.layers.ZeroPadding3D(\n-          padding=(2, 2, 2), data_format=data_format)\n-      layer.build(inputs.shape)\n-      output = layer(keras.backend.variable(inputs))\n-      if tf.executing_eagerly():\n-        np_output = output.numpy()\n-      else:\n-        np_output = keras.backend.eval(output)\n-      if data_format == 'channels_last':\n-        for offset in [0, 1, -1, -2]:\n-          np.testing.assert_allclose(np_output[:, offset, :, :, :], 0.)\n-          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n-          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n-        np.testing.assert_allclose(np_output[:, 2:-2, 2:-2, 2:-2, :], 1.)\n-      elif data_format == 'channels_first':\n-        for offset in [0, 1, -1, -2]:\n-          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n-          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n-          np.testing.assert_allclose(np_output[:, :, :, :, offset], 0.)\n-        np.testing.assert_allclose(np_output[:, :, 2:-2, 2:-2, 2:-2], 1.)\n-\n-      layer = keras.layers.ZeroPadding3D(\n-          padding=((1, 2), (3, 4), (0, 2)), data_format=data_format)\n-      layer.build(inputs.shape)\n-      output = layer(keras.backend.variable(inputs))\n-      if tf.executing_eagerly():\n-        np_output = output.numpy()\n-      else:\n-        np_output = keras.backend.eval(output)\n-      if data_format == 'channels_last':\n-        for offset in [0]:\n-          np.testing.assert_allclose(np_output[:, offset, :, :, :], 0.)\n-        for offset in [-1, -2]:\n-          np.testing.assert_allclose(np_output[:, offset, :, :, :], 0.)\n-        for offset in [0, 1, 2]:\n-          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n-        for offset in [-1, -2, -3, -4]:\n-          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n-        for offset in [-1, -2]:\n-          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n-        np.testing.assert_allclose(np_output[:, 1:-2, 3:-4, 0:-2, :], 1.)\n-      elif data_format == 'channels_first':\n-        for offset in [0]:\n-          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n-        for offset in [-1, -2]:\n-          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n-        for offset in [0, 1, 2]:\n-          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n-        for offset in [-1, -2, -3, -4]:\n-          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n-        for offset in [-1, -2]:\n-          np.testing.assert_allclose(np_output[:, :, :, :, offset], 0.)\n-        np.testing.assert_allclose(np_output[:, :, 1:-2, 3:-4, 0:-2], 1.)\n-\n-    # test incorrect use\n-    with self.assertRaises(ValueError):\n-      keras.layers.ZeroPadding3D(padding=(1, 1))\n-    with self.assertRaises(ValueError):\n-      keras.layers.ZeroPadding3D(padding=None)\n-\n-\n-@test_util.for_all_test_methods(test_util.disable_xla,\n-                                'align_corners=False not supported by XLA')\n-@keras_parameterized.run_all_keras_modes\n-class UpSamplingTest(keras_parameterized.TestCase):\n-\n-  def test_upsampling_1d(self):\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.UpSampling1D, kwargs={'size': 2}, input_shape=(3, 5, 4))\n-\n-  def test_upsampling_2d(self):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 11\n-    input_num_col = 12\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-      if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_num_row,\n-                                input_num_col)\n-      else:\n-        inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n-                                stack_size)\n-\n-      # basic test\n-      with self.cached_session():\n-        testing_utils.layer_test(\n-            keras.layers.UpSampling2D,\n-            kwargs={'size': (2, 2),\n-                    'data_format': data_format},\n-            input_shape=inputs.shape)\n-\n-        for length_row in [2]:\n-          for length_col in [2, 3]:\n-            layer = keras.layers.UpSampling2D(\n-                size=(length_row, length_col), data_format=data_format)\n-            layer.build(inputs.shape)\n-            output = layer(keras.backend.variable(inputs))\n-            if tf.executing_eagerly():\n-              np_output = output.numpy()\n-            else:\n-              np_output = keras.backend.eval(output)\n-            if data_format == 'channels_first':\n-              assert np_output.shape[2] == length_row * input_num_row\n-              assert np_output.shape[3] == length_col * input_num_col\n-            else:  # tf\n-              assert np_output.shape[1] == length_row * input_num_row\n-              assert np_output.shape[2] == length_col * input_num_col\n-\n-            # compare with numpy\n-            if data_format == 'channels_first':\n-              expected_out = np.repeat(inputs, length_row, axis=2)\n-              expected_out = np.repeat(expected_out, length_col, axis=3)\n-            else:  # tf\n-              expected_out = np.repeat(inputs, length_row, axis=1)\n-              expected_out = np.repeat(expected_out, length_col, axis=2)\n-\n-            np.testing.assert_allclose(np_output, expected_out)\n-\n-  def test_upsampling_2d_bilinear(self):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 11\n-    input_num_col = 12\n-    for data_format in ['channels_first', 'channels_last']:\n-      if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_num_row,\n-                                input_num_col)\n-      else:\n-        inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n-                                stack_size)\n-\n-      testing_utils.layer_test(keras.layers.UpSampling2D,\n-                               kwargs={'size': (2, 2),\n-                                       'data_format': data_format,\n-                                       'interpolation': 'bilinear'},\n-                               input_shape=inputs.shape)\n-\n-      if not tf.executing_eagerly():\n-        for length_row in [2]:\n-          for length_col in [2, 3]:\n-            layer = keras.layers.UpSampling2D(\n-                size=(length_row, length_col),\n-                data_format=data_format)\n-            layer.build(inputs.shape)\n-            outputs = layer(keras.backend.variable(inputs))\n-            np_output = keras.backend.eval(outputs)\n-            if data_format == 'channels_first':\n-              self.assertEqual(np_output.shape[2], length_row * input_num_row)\n-              self.assertEqual(np_output.shape[3], length_col * input_num_col)\n-            else:\n-              self.assertEqual(np_output.shape[1], length_row * input_num_row)\n-              self.assertEqual(np_output.shape[2], length_col * input_num_col)\n-\n-  def test_upsampling_3d(self):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 10\n-    input_len_dim2 = 11\n-    input_len_dim3 = 12\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-      if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n-                                input_len_dim2, input_len_dim3)\n-      else:\n-        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n-                                input_len_dim3, stack_size)\n-\n-      # basic test\n-      with self.cached_session():\n-        testing_utils.layer_test(\n-            keras.layers.UpSampling3D,\n-            kwargs={'size': (2, 2, 2),\n-                    'data_format': data_format},\n-            input_shape=inputs.shape)\n-\n-        for length_dim1 in [2, 3]:\n-          for length_dim2 in [2]:\n-            for length_dim3 in [3]:\n-              layer = keras.layers.UpSampling3D(\n-                  size=(length_dim1, length_dim2, length_dim3),\n-                  data_format=data_format)\n-              layer.build(inputs.shape)\n-              output = layer(keras.backend.variable(inputs))\n-              if tf.executing_eagerly():\n-                np_output = output.numpy()\n-              else:\n-                np_output = keras.backend.eval(output)\n-              if data_format == 'channels_first':\n-                assert np_output.shape[2] == length_dim1 * input_len_dim1\n-                assert np_output.shape[3] == length_dim2 * input_len_dim2\n-                assert np_output.shape[4] == length_dim3 * input_len_dim3\n-              else:  # tf\n-                assert np_output.shape[1] == length_dim1 * input_len_dim1\n-                assert np_output.shape[2] == length_dim2 * input_len_dim2\n-                assert np_output.shape[3] == length_dim3 * input_len_dim3\n-\n-              # compare with numpy\n-              if data_format == 'channels_first':\n-                expected_out = np.repeat(inputs, length_dim1, axis=2)\n-                expected_out = np.repeat(expected_out, length_dim2, axis=3)\n-                expected_out = np.repeat(expected_out, length_dim3, axis=4)\n-              else:  # tf\n-                expected_out = np.repeat(inputs, length_dim1, axis=1)\n-                expected_out = np.repeat(expected_out, length_dim2, axis=2)\n-                expected_out = np.repeat(expected_out, length_dim3, axis=3)\n-\n-              np.testing.assert_allclose(np_output, expected_out)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class CroppingTest(keras_parameterized.TestCase):\n-\n-  def test_cropping_1d(self):\n-    num_samples = 2\n-    time_length = 4\n-    input_len_dim1 = 2\n-    inputs = np.random.rand(num_samples, time_length, input_len_dim1)\n-\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.Cropping1D,\n-          kwargs={'cropping': (1, 1)},\n-          input_shape=inputs.shape)\n-\n-    # test incorrect use\n-    with self.assertRaises(ValueError):\n-      keras.layers.Cropping1D(cropping=(1, 1, 1))\n-    with self.assertRaises(ValueError):\n-      keras.layers.Cropping1D(cropping=None)\n-    with self.assertRaises(ValueError):\n-      input_layer = keras.layers.Input(\n-          shape=(num_samples, time_length, input_len_dim1))\n-      keras.layers.Cropping1D(cropping=(2, 3))(input_layer)\n-\n-  def test_cropping_2d(self):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 9\n-    input_len_dim2 = 9\n-    cropping = ((2, 2), (3, 3))\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-      if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n-                                input_len_dim2)\n-      else:\n-        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n-                                stack_size)\n-      with self.cached_session():\n-        # basic test\n-        testing_utils.layer_test(\n-            keras.layers.Cropping2D,\n-            kwargs={'cropping': cropping,\n-                    'data_format': data_format},\n-            input_shape=inputs.shape)\n-        # correctness test\n-        layer = keras.layers.Cropping2D(\n-            cropping=cropping, data_format=data_format)\n-        layer.build(inputs.shape)\n-        output = layer(keras.backend.variable(inputs))\n-        if tf.executing_eagerly():\n-          np_output = output.numpy()\n-        else:\n-          np_output = keras.backend.eval(output)\n-        # compare with numpy\n-        if data_format == 'channels_first':\n-          expected_out = inputs[:, :, cropping[0][0]:-cropping[0][1], cropping[\n-              1][0]:-cropping[1][1]]\n-        else:\n-          expected_out = inputs[:, cropping[0][0]:-cropping[0][1], cropping[1][\n-              0]:-cropping[1][1], :]\n-        np.testing.assert_allclose(np_output, expected_out)\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-      if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n-                                input_len_dim2)\n-      else:\n-        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n-                                stack_size)\n-      # another correctness test (no cropping)\n-      with self.cached_session():\n-        cropping = ((0, 0), (0, 0))\n-        layer = keras.layers.Cropping2D(\n-            cropping=cropping, data_format=data_format)\n-        layer.build(inputs.shape)\n-        output = layer(keras.backend.variable(inputs))\n-        if tf.executing_eagerly():\n-          np_output = output.numpy()\n-        else:\n-          np_output = keras.backend.eval(output)\n-        # compare with input\n-        np.testing.assert_allclose(np_output, inputs)\n-\n-    # test incorrect use\n-    with self.assertRaises(ValueError):\n-      keras.layers.Cropping2D(cropping=(1, 1, 1))\n-    with self.assertRaises(ValueError):\n-      keras.layers.Cropping2D(cropping=None)\n-    with self.assertRaises(ValueError):\n-      input_layer = keras.layers.Input(\n-          shape=(num_samples, input_len_dim1, input_len_dim2, stack_size))\n-      keras.layers.Cropping2D(cropping=((5, 4), (3, 4)))(input_layer)\n-\n-  def test_cropping_3d(self):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 8\n-    input_len_dim2 = 8\n-    input_len_dim3 = 8\n-    croppings = [((2, 2), (1, 1), (2, 3)), 3, (0, 1, 1)]\n-\n-    for cropping in croppings:\n-      for data_format in ['channels_last', 'channels_first']:\n-        if data_format == 'channels_first':\n-          inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n-                                  input_len_dim2, input_len_dim3)\n-        else:\n-          inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n-                                  input_len_dim3, stack_size)\n-        # basic test\n-        with self.cached_session():\n-          testing_utils.layer_test(\n-              keras.layers.Cropping3D,\n-              kwargs={'cropping': cropping,\n-                      'data_format': data_format},\n-              input_shape=inputs.shape)\n-\n-        if len(croppings) == 3 and len(croppings[0]) == 2:\n-          # correctness test\n-          with self.cached_session():\n-            layer = keras.layers.Cropping3D(\n-                cropping=cropping, data_format=data_format)\n-            layer.build(inputs.shape)\n-            output = layer(keras.backend.variable(inputs))\n-            if tf.executing_eagerly():\n-              np_output = output.numpy()\n-            else:\n-              np_output = keras.backend.eval(output)\n-            # compare with numpy\n-            if data_format == 'channels_first':\n-              expected_out = inputs[:, :,\n-                                    cropping[0][0]:-cropping[0][1],\n-                                    cropping[1][0]:-cropping[1][1],\n-                                    cropping[2][0]:-cropping[2][1]]\n-            else:\n-              expected_out = inputs[:,\n-                                    cropping[0][0]:-cropping[0][1],\n-                                    cropping[1][0]:-cropping[1][1],\n-                                    cropping[2][0]:-cropping[2][1], :]\n-            np.testing.assert_allclose(np_output, expected_out)\n-\n-    # test incorrect use\n-    with self.assertRaises(ValueError):\n-      keras.layers.Cropping3D(cropping=(1, 1))\n-    with self.assertRaises(ValueError):\n-      keras.layers.Cropping3D(cropping=None)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class DepthwiseConv1DTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape=None):\n-    num_samples = 2\n-    stack_size = 3\n-    num_row = 7\n-\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.DepthwiseConv1D,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, num_row, stack_size),\n-          expected_output_shape=expected_output_shape)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {\n-          'padding': 'valid'\n-      }),\n-      ('padding_same', {\n-          'padding': 'same'\n-      }),\n-      ('strides', {\n-          'strides': 2\n-      }),\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {\n-          'data_format': 'channels_first'\n-      }),\n-      ('depth_multiplier_1', {\n-          'depth_multiplier': 1\n-      }),\n-      ('depth_multiplier_2', {\n-          'depth_multiplier': 2\n-      }),\n-      ('dilation_rate', {\n-          'dilation_rate': 2\n-      }, (None, 3, 3)),\n-  )\n-  def test_depthwise_conv1d(self, kwargs, expected_output_shape=None):\n-    kwargs['kernel_size'] = 3\n-    if 'data_format' not in kwargs or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs, expected_output_shape)\n-\n-  def test_depthwise_conv1d_full(self):\n-    kwargs = {\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'data_format': 'channels_last',\n-        'dilation_rate': 1,\n-        'activation': None,\n-        'depthwise_regularizer': 'l2',\n-        'bias_regularizer': 'l2',\n-        'activity_regularizer': 'l2',\n-        'depthwise_constraint': 'unit_norm',\n-        'use_bias': True,\n-        'strides': 2,\n-        'depth_multiplier': 1,\n-    }\n-    self._run_test(kwargs)\n-\n-\n-@keras_parameterized.run_all_keras_modes\n-class DepthwiseConv2DTest(keras_parameterized.TestCase):\n-\n-  def _run_test(self, kwargs, expected_output_shape=None):\n-    num_samples = 2\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-\n-    with self.cached_session():\n-      testing_utils.layer_test(\n-          keras.layers.DepthwiseConv2D,\n-          kwargs=kwargs,\n-          input_shape=(num_samples, num_row, num_col, stack_size),\n-          expected_output_shape=expected_output_shape)\n-\n-  @parameterized.named_parameters(\n-      ('padding_valid', {'padding': 'valid'}),\n-      ('padding_same', {'padding': 'same'}),\n-      ('strides', {'strides': (2, 2)}),\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      ('data_format', {'data_format': 'channels_first'}),\n-      ('depth_multiplier_1', {'depth_multiplier': 1}),\n-      ('depth_multiplier_2', {'depth_multiplier': 2}),\n-      ('dilation_rate', {'dilation_rate': (2, 2)}, (None, 3, 2, 3)),\n-  )\n-  def test_depthwise_conv2d(self, kwargs, expected_output_shape=None):\n-    kwargs['kernel_size'] = (3, 3)\n-    if 'data_format' not in kwargs or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs, expected_output_shape)\n-\n-  def test_depthwise_conv2d_full(self):\n-    kwargs = {\n-        'kernel_size': 3,\n-        'padding': 'valid',\n-        'data_format': 'channels_last',\n-        'dilation_rate': (1, 1),\n-        'activation': None,\n-        'depthwise_regularizer': 'l2',\n-        'bias_regularizer': 'l2',\n-        'activity_regularizer': 'l2',\n-        'depthwise_constraint': 'unit_norm',\n-        'use_bias': True,\n-        'strides': (2, 2),\n-        'depth_multiplier': 1,\n-    }\n-    self._run_test(kwargs)\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n@@ -0,0 +1,25 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras reshaping layers layers.\"\"\"\n+\n+from keras.layers.reshaping.cropping1d import Cropping1D\n+from keras.layers.reshaping.cropping2d import Cropping2D\n+from keras.layers.reshaping.cropping3d import Cropping3D\n+from keras.layers.reshaping.up_sampling1d import UpSampling1D\n+from keras.layers.reshaping.up_sampling2d import UpSampling2D\n+from keras.layers.reshaping.up_sampling3d import UpSampling3D\n+from keras.layers.reshaping.zero_padding1d import ZeroPadding1D\n+from keras.layers.reshaping.zero_padding2d import ZeroPadding2D\n+from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n\n@@ -0,0 +1,89 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras cropping layer for 1D input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Cropping1D')\n+class Cropping1D(Layer):\n+  \"\"\"Cropping layer for 1D input (e.g. temporal sequence).\n+\n+  It crops along the time dimension (axis 1).\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 3, 2)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> print(x)\n+  [[[ 0  1]\n+    [ 2  3]\n+    [ 4  5]]\n+   [[ 6  7]\n+    [ 8  9]\n+    [10 11]]]\n+  >>> y = tf.keras.layers.Cropping1D(cropping=1)(x)\n+  >>> print(y)\n+  tf.Tensor(\n+    [[[2 3]]\n+     [[8 9]]], shape=(2, 1, 2), dtype=int64)\n+\n+  Args:\n+    cropping: Int or tuple of int (length 2)\n+      How many units should be trimmed off at the beginning and end of\n+      the cropping dimension (axis 1).\n+      If a single int is provided, the same value will be used for both.\n+\n+  Input shape:\n+    3D tensor with shape `(batch_size, axis_to_crop, features)`\n+\n+  Output shape:\n+    3D tensor with shape `(batch_size, cropped_axis, features)`\n+  \"\"\"\n+\n+  def __init__(self, cropping=(1, 1), **kwargs):\n+    super(Cropping1D, self).__init__(**kwargs)\n+    self.cropping = conv_utils.normalize_tuple(\n+        cropping, 2, 'cropping', allow_zero=True)\n+    self.input_spec = InputSpec(ndim=3)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if input_shape[1] is not None:\n+      length = input_shape[1] - self.cropping[0] - self.cropping[1]\n+    else:\n+      length = None\n+    return tf.TensorShape([input_shape[0], length, input_shape[2]])\n+\n+  def call(self, inputs):\n+    if inputs.shape[1] is not None and sum(self.cropping) >= inputs.shape[1]:\n+      raise ValueError('cropping parameter of Cropping layer must be '\n+                       'greater than the input shape. Received: inputs.shape='\n+                       f'{inputs.shape}, and cropping={self.cropping}')\n+    if self.cropping[1] == 0:\n+      return inputs[:, self.cropping[0]:, :]\n+    else:\n+      return inputs[:, self.cropping[0]:-self.cropping[1], :]\n+\n+  def get_config(self):\n+    config = {'cropping': self.cropping}\n+    base_config = super(Cropping1D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,164 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras cropping layer for 2D input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Cropping2D')\n+class Cropping2D(Layer):\n+  \"\"\"Cropping layer for 2D input (e.g. picture).\n+\n+  It crops along spatial dimensions, i.e. height and width.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 28, 28, 3)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)\n+  >>> print(y.shape)\n+  (2, 24, 20, 3)\n+\n+  Args:\n+    cropping: Int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n+      - If int: the same symmetric cropping\n+        is applied to height and width.\n+      - If tuple of 2 ints:\n+        interpreted as two different\n+        symmetric cropping values for height and width:\n+        `(symmetric_height_crop, symmetric_width_crop)`.\n+      - If tuple of 2 tuples of 2 ints:\n+        interpreted as\n+        `((top_crop, bottom_crop), (left_crop, right_crop))`\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch_size, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    4D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+      `(batch_size, rows, cols, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+      `(batch_size, channels, rows, cols)`\n+\n+  Output shape:\n+    4D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+      `(batch_size, cropped_rows, cropped_cols, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+      `(batch_size, channels, cropped_rows, cropped_cols)`\n+  \"\"\"\n+\n+  def __init__(self, cropping=((0, 0), (0, 0)), data_format=None, **kwargs):\n+    super(Cropping2D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    if isinstance(cropping, int):\n+      self.cropping = ((cropping, cropping), (cropping, cropping))\n+    elif hasattr(cropping, '__len__'):\n+      if len(cropping) != 2:\n+        raise ValueError('`cropping` should have two elements. '\n+                         f'Received: {cropping}.')\n+      height_cropping = conv_utils.normalize_tuple(\n+          cropping[0], 2, '1st entry of cropping', allow_zero=True)\n+      width_cropping = conv_utils.normalize_tuple(\n+          cropping[1], 2, '2nd entry of cropping', allow_zero=True)\n+      self.cropping = (height_cropping, width_cropping)\n+    else:\n+      raise ValueError('`cropping` should be either an int, '\n+                       'a tuple of 2 ints '\n+                       '(symmetric_height_crop, symmetric_width_crop), '\n+                       'or a tuple of 2 tuples of 2 ints '\n+                       '((top_crop, bottom_crop), (left_crop, right_crop)). '\n+                       f'Received: {cropping}.')\n+    self.input_spec = InputSpec(ndim=4)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    # pylint: disable=invalid-unary-operand-type\n+    if self.data_format == 'channels_first':\n+      return tf.TensorShape([\n+          input_shape[0], input_shape[1],\n+          input_shape[2] - self.cropping[0][0] - self.cropping[0][1]\n+          if input_shape[2] else None,\n+          input_shape[3] - self.cropping[1][0] - self.cropping[1][1]\n+          if input_shape[3] else None\n+      ])\n+    else:\n+      return tf.TensorShape([\n+          input_shape[0],\n+          input_shape[1] - self.cropping[0][0] - self.cropping[0][1]\n+          if input_shape[1] else None,\n+          input_shape[2] - self.cropping[1][0] - self.cropping[1][1]\n+          if input_shape[2] else None, input_shape[3]\n+      ])\n+    # pylint: enable=invalid-unary-operand-type\n+\n+  def call(self, inputs):\n+    # pylint: disable=invalid-unary-operand-type\n+    if self.data_format == 'channels_first':\n+      if ((inputs.shape[2] is not None and\n+           sum(self.cropping[0]) >= inputs.shape[2]) or\n+          (inputs.shape[3] is not None and\n+           sum(self.cropping[1]) >= inputs.shape[3])):\n+        raise ValueError('Argument `cropping` must be '\n+                         'greater than the input shape. Received: inputs.shape='\n+                         f'{inputs.shape}, and cropping={self.cropping}')\n+      if self.cropping[0][1] == self.cropping[1][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:]\n+      elif self.cropping[0][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:\n+                      -self.cropping[1][1]]\n+      elif self.cropping[1][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1],\n+                      self.cropping[1][0]:]\n+      return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1],\n+                    self.cropping[1][0]:-self.cropping[1][1]]\n+    else:\n+      if ((inputs.shape[1] is not None and\n+           sum(self.cropping[0]) >= inputs.shape[1]) or\n+          (inputs.shape[2] is not None and\n+           sum(self.cropping[1]) >= inputs.shape[2])):\n+        raise ValueError('Argument `cropping` must be '\n+                         'greater than the input shape. Received: inputs.shape='\n+                         f'{inputs.shape}, and cropping={self.cropping}')\n+      if self.cropping[0][1] == self.cropping[1][1] == 0:\n+        return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:, :]\n+      elif self.cropping[0][1] == 0:\n+        return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:\n+                      -self.cropping[1][1], :]\n+      elif self.cropping[1][1] == 0:\n+        return inputs[:, self.cropping[0][0]:-self.cropping[0][1],\n+                      self.cropping[1][0]:, :]\n+      return inputs[:, self.cropping[0][0]:-self.cropping[0][1], self.cropping[\n+          1][0]:-self.cropping[1][1], :]  # pylint: disable=invalid-unary-operand-type\n+    # pylint: enable=invalid-unary-operand-type\n+\n+  def get_config(self):\n+    config = {'cropping': self.cropping, 'data_format': self.data_format}\n+    base_config = super(Cropping2D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,205 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras cropping layer for 3D input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.Cropping3D')\n+class Cropping3D(Layer):\n+  \"\"\"Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n+\n+    Examples:\n+\n+  >>> input_shape = (2, 28, 28, 10, 3)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)\n+  >>> print(y.shape)\n+  (2, 24, 20, 6, 3)\n+\n+  Args:\n+    cropping: Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n+      - If int: the same symmetric cropping\n+        is applied to depth, height, and width.\n+      - If tuple of 3 ints: interpreted as two different\n+        symmetric cropping values for depth, height, and width:\n+        `(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop)`.\n+      - If tuple of 3 tuples of 2 ints: interpreted as\n+        `((left_dim1_crop, right_dim1_crop), (left_dim2_crop,\n+          right_dim2_crop), (left_dim3_crop, right_dim3_crop))`\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    5D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+      `(batch_size, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop,\n+        depth)`\n+    - If `data_format` is `\"channels_first\"`:\n+      `(batch_size, depth, first_axis_to_crop, second_axis_to_crop,\n+        third_axis_to_crop)`\n+\n+  Output shape:\n+    5D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+      `(batch_size, first_cropped_axis, second_cropped_axis, third_cropped_axis,\n+        depth)`\n+    - If `data_format` is `\"channels_first\"`:\n+      `(batch_size, depth, first_cropped_axis, second_cropped_axis,\n+        third_cropped_axis)`\n+  \"\"\"\n+\n+  def __init__(self,\n+               cropping=((1, 1), (1, 1), (1, 1)),\n+               data_format=None,\n+               **kwargs):\n+    super(Cropping3D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    if isinstance(cropping, int):\n+      self.cropping = ((cropping, cropping), (cropping, cropping), (cropping,\n+                                                                    cropping))\n+    elif hasattr(cropping, '__len__'):\n+      if len(cropping) != 3:\n+        raise ValueError('`cropping` should have 3 elements. '\n+                         f'Received: {cropping}.')\n+      dim1_cropping = conv_utils.normalize_tuple(\n+          cropping[0], 2, '1st entry of cropping', allow_zero=True)\n+      dim2_cropping = conv_utils.normalize_tuple(\n+          cropping[1], 2, '2nd entry of cropping', allow_zero=True)\n+      dim3_cropping = conv_utils.normalize_tuple(\n+          cropping[2], 2, '3rd entry of cropping', allow_zero=True)\n+      self.cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n+    else:\n+      raise ValueError(\n+          '`cropping` should be either an int, '\n+          'a tuple of 3 ints '\n+          '(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), '\n+          'or a tuple of 3 tuples of 2 ints '\n+          '((left_dim1_crop, right_dim1_crop),'\n+          ' (left_dim2_crop, right_dim2_crop),'\n+          ' (left_dim3_crop, right_dim2_crop)). '\n+          f'Received: {cropping}.')\n+    self.input_spec = InputSpec(ndim=5)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    # pylint: disable=invalid-unary-operand-type\n+    if self.data_format == 'channels_first':\n+      if input_shape[2] is not None:\n+        dim1 = input_shape[2] - self.cropping[0][0] - self.cropping[0][1]\n+      else:\n+        dim1 = None\n+      if input_shape[3] is not None:\n+        dim2 = input_shape[3] - self.cropping[1][0] - self.cropping[1][1]\n+      else:\n+        dim2 = None\n+      if input_shape[4] is not None:\n+        dim3 = input_shape[4] - self.cropping[2][0] - self.cropping[2][1]\n+      else:\n+        dim3 = None\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], dim1, dim2, dim3])\n+    elif self.data_format == 'channels_last':\n+      if input_shape[1] is not None:\n+        dim1 = input_shape[1] - self.cropping[0][0] - self.cropping[0][1]\n+      else:\n+        dim1 = None\n+      if input_shape[2] is not None:\n+        dim2 = input_shape[2] - self.cropping[1][0] - self.cropping[1][1]\n+      else:\n+        dim2 = None\n+      if input_shape[3] is not None:\n+        dim3 = input_shape[3] - self.cropping[2][0] - self.cropping[2][1]\n+      else:\n+        dim3 = None\n+      return tf.TensorShape(\n+          [input_shape[0], dim1, dim2, dim3, input_shape[4]])\n+    # pylint: enable=invalid-unary-operand-type\n+\n+  def call(self, inputs):\n+    # pylint: disable=invalid-unary-operand-type\n+    if self.data_format == 'channels_first':\n+      if self.cropping[0][1] == self.cropping[1][1] == self.cropping[2][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:,\n+                      self.cropping[2][0]:]\n+      elif self.cropping[0][1] == self.cropping[1][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:,\n+                      self.cropping[2][0]:-self.cropping[2][1]]\n+      elif self.cropping[1][1] == self.cropping[2][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1],\n+                      self.cropping[1][0]:, self.cropping[2][0]:]\n+      elif self.cropping[0][1] == self.cropping[2][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:, self.cropping[1][0]:\n+                      -self.cropping[1][1], self.cropping[2][0]:]\n+      elif self.cropping[0][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:, self.cropping[1][\n+            0]:-self.cropping[1][1], self.cropping[2][0]:-self.cropping[2][1]]\n+      elif self.cropping[1][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1], self.\n+                      cropping[1][0]:, self.cropping[2][0]:-self.cropping[2][1]]\n+      elif self.cropping[2][1] == 0:\n+        return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1], self.\n+                      cropping[1][0]:-self.cropping[1][1], self.cropping[2][0]:]\n+      return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1],\n+                    self.cropping[1][0]:-self.cropping[1][1], self.cropping[2][\n+                        0]:-self.cropping[2][1]]\n+    else:\n+      if self.cropping[0][1] == self.cropping[1][1] == self.cropping[2][1] == 0:\n+        return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:,\n+                      self.cropping[2][0]:, :]\n+      elif self.cropping[0][1] == self.cropping[1][1] == 0:\n+        return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:,\n+                      self.cropping[2][0]:-self.cropping[2][1], :]\n+      elif self.cropping[1][1] == self.cropping[2][1] == 0:\n+        return inputs[:, self.cropping[0][0]:-self.cropping[0][1],\n+                      self.cropping[1][0]:, self.cropping[2][0]:, :]\n+      elif self.cropping[0][1] == self.cropping[2][1] == 0:\n+        return inputs[:, self.cropping[0][0]:, self.cropping[1][0]:\n+                      -self.cropping[1][1], self.cropping[2][0]:, :]\n+      elif self.cropping[0][1] == 0:\n+        return inputs[:, self.cropping[0][0]:, self.cropping[1][\n+            0]:-self.cropping[1][1], self.cropping[2][0]:\n+                      -self.cropping[2][1], :]\n+      elif self.cropping[1][1] == 0:\n+        return inputs[:, self.cropping[0][\n+            0]:-self.cropping[0][1], self.cropping[1][0]:, self.cropping[2][0]:\n+                      -self.cropping[2][1], :]\n+      elif self.cropping[2][1] == 0:\n+        return inputs[:, self.cropping[0][0]:-self.cropping[0][1],\n+                      self.cropping[1][0]:-self.cropping[1][1], self.cropping[\n+                          2][0]:, :]\n+      return inputs[:, self.cropping[0][0]:-self.cropping[0][1], self.cropping[\n+          1][0]:-self.cropping[1][1], self.cropping[2][0]:  # pylint: disable=invalid-unary-operand-type\n+                    -self.cropping[2][1], :]  # pylint: disable=invalid-unary-operand-type\n+    # pylint: enable=invalid-unary-operand-type\n+\n+  def get_config(self):\n+    config = {'cropping': self.cropping, 'data_format': self.data_format}\n+    base_config = super(Cropping3D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,174 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for reshaping layers.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class CroppingTest(keras_parameterized.TestCase):\n+\n+  def test_cropping_1d(self):\n+    num_samples = 2\n+    time_length = 4\n+    input_len_dim1 = 2\n+    inputs = np.random.rand(num_samples, time_length, input_len_dim1)\n+\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.Cropping1D,\n+          kwargs={'cropping': (1, 1)},\n+          input_shape=inputs.shape)\n+\n+    # test incorrect use\n+    with self.assertRaises(ValueError):\n+      keras.layers.Cropping1D(cropping=(1, 1, 1))\n+    with self.assertRaises(ValueError):\n+      keras.layers.Cropping1D(cropping=None)\n+    with self.assertRaises(ValueError):\n+      input_layer = keras.layers.Input(\n+          shape=(num_samples, time_length, input_len_dim1))\n+      keras.layers.Cropping1D(cropping=(2, 3))(input_layer)\n+\n+  def test_cropping_2d(self):\n+    num_samples = 2\n+    stack_size = 2\n+    input_len_dim1 = 9\n+    input_len_dim2 = 9\n+    cropping = ((2, 2), (3, 3))\n+\n+    for data_format in ['channels_first', 'channels_last']:\n+      if data_format == 'channels_first':\n+        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n+                                input_len_dim2)\n+      else:\n+        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n+                                stack_size)\n+      with self.cached_session():\n+        # basic test\n+        testing_utils.layer_test(\n+            keras.layers.Cropping2D,\n+            kwargs={'cropping': cropping,\n+                    'data_format': data_format},\n+            input_shape=inputs.shape)\n+        # correctness test\n+        layer = keras.layers.Cropping2D(\n+            cropping=cropping, data_format=data_format)\n+        layer.build(inputs.shape)\n+        output = layer(keras.backend.variable(inputs))\n+        if tf.executing_eagerly():\n+          np_output = output.numpy()\n+        else:\n+          np_output = keras.backend.eval(output)\n+        # compare with numpy\n+        if data_format == 'channels_first':\n+          expected_out = inputs[:, :, cropping[0][0]:-cropping[0][1], cropping[\n+              1][0]:-cropping[1][1]]\n+        else:\n+          expected_out = inputs[:, cropping[0][0]:-cropping[0][1], cropping[1][\n+              0]:-cropping[1][1], :]\n+        np.testing.assert_allclose(np_output, expected_out)\n+\n+    for data_format in ['channels_first', 'channels_last']:\n+      if data_format == 'channels_first':\n+        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n+                                input_len_dim2)\n+      else:\n+        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n+                                stack_size)\n+      # another correctness test (no cropping)\n+      with self.cached_session():\n+        cropping = ((0, 0), (0, 0))\n+        layer = keras.layers.Cropping2D(\n+            cropping=cropping, data_format=data_format)\n+        layer.build(inputs.shape)\n+        output = layer(keras.backend.variable(inputs))\n+        if tf.executing_eagerly():\n+          np_output = output.numpy()\n+        else:\n+          np_output = keras.backend.eval(output)\n+        # compare with input\n+        np.testing.assert_allclose(np_output, inputs)\n+\n+    # test incorrect use\n+    with self.assertRaises(ValueError):\n+      keras.layers.Cropping2D(cropping=(1, 1, 1))\n+    with self.assertRaises(ValueError):\n+      keras.layers.Cropping2D(cropping=None)\n+    with self.assertRaises(ValueError):\n+      input_layer = keras.layers.Input(\n+          shape=(num_samples, input_len_dim1, input_len_dim2, stack_size))\n+      keras.layers.Cropping2D(cropping=((5, 4), (3, 4)))(input_layer)\n+\n+  def test_cropping_3d(self):\n+    num_samples = 2\n+    stack_size = 2\n+    input_len_dim1 = 8\n+    input_len_dim2 = 8\n+    input_len_dim3 = 8\n+    croppings = [((2, 2), (1, 1), (2, 3)), 3, (0, 1, 1)]\n+\n+    for cropping in croppings:\n+      for data_format in ['channels_last', 'channels_first']:\n+        if data_format == 'channels_first':\n+          inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n+                                  input_len_dim2, input_len_dim3)\n+        else:\n+          inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n+                                  input_len_dim3, stack_size)\n+        # basic test\n+        with self.cached_session():\n+          testing_utils.layer_test(\n+              keras.layers.Cropping3D,\n+              kwargs={'cropping': cropping,\n+                      'data_format': data_format},\n+              input_shape=inputs.shape)\n+\n+        if len(croppings) == 3 and len(croppings[0]) == 2:\n+          # correctness test\n+          with self.cached_session():\n+            layer = keras.layers.Cropping3D(\n+                cropping=cropping, data_format=data_format)\n+            layer.build(inputs.shape)\n+            output = layer(keras.backend.variable(inputs))\n+            if tf.executing_eagerly():\n+              np_output = output.numpy()\n+            else:\n+              np_output = keras.backend.eval(output)\n+            # compare with numpy\n+            if data_format == 'channels_first':\n+              expected_out = inputs[:, :,\n+                                    cropping[0][0]:-cropping[0][1],\n+                                    cropping[1][0]:-cropping[1][1],\n+                                    cropping[2][0]:-cropping[2][1]]\n+            else:\n+              expected_out = inputs[:,\n+                                    cropping[0][0]:-cropping[0][1],\n+                                    cropping[1][0]:-cropping[1][1],\n+                                    cropping[2][0]:-cropping[2][1], :]\n+            np.testing.assert_allclose(np_output, expected_out)\n+\n+    # test incorrect use\n+    with self.assertRaises(ValueError):\n+      keras.layers.Cropping3D(cropping=(1, 1))\n+    with self.assertRaises(ValueError):\n+      keras.layers.Cropping3D(cropping=None)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,80 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras upsampling layer for 1D inputs.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.UpSampling1D')\n+class UpSampling1D(Layer):\n+  \"\"\"Upsampling layer for 1D inputs.\n+\n+  Repeats each temporal step `size` times along the time axis.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 2, 3)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> print(x)\n+  [[[ 0  1  2]\n+    [ 3  4  5]]\n+   [[ 6  7  8]\n+    [ 9 10 11]]]\n+  >>> y = tf.keras.layers.UpSampling1D(size=2)(x)\n+  >>> print(y)\n+  tf.Tensor(\n+    [[[ 0  1  2]\n+      [ 0  1  2]\n+      [ 3  4  5]\n+      [ 3  4  5]]\n+     [[ 6  7  8]\n+      [ 6  7  8]\n+      [ 9 10 11]\n+      [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)\n+\n+  Args:\n+    size: Integer. Upsampling factor.\n+\n+  Input shape:\n+    3D tensor with shape: `(batch_size, steps, features)`.\n+\n+  Output shape:\n+    3D tensor with shape: `(batch_size, upsampled_steps, features)`.\n+  \"\"\"\n+\n+  def __init__(self, size=2, **kwargs):\n+    super(UpSampling1D, self).__init__(**kwargs)\n+    self.size = int(size)\n+    self.input_spec = InputSpec(ndim=3)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    size = self.size * input_shape[1] if input_shape[1] is not None else None\n+    return tf.TensorShape([input_shape[0], size, input_shape[2]])\n+\n+  def call(self, inputs):\n+    output = backend.repeat_elements(inputs, self.size, axis=1)\n+    return output\n+\n+  def get_config(self):\n+    config = {'size': self.size}\n+    base_config = super(UpSampling1D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,128 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras upsampling layer for 2D inputs.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.UpSampling2D')\n+class UpSampling2D(Layer):\n+  \"\"\"Upsampling layer for 2D inputs.\n+\n+  Repeats the rows and columns of the data\n+  by `size[0]` and `size[1]` respectively.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 2, 1, 3)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> print(x)\n+  [[[[ 0  1  2]]\n+    [[ 3  4  5]]]\n+   [[[ 6  7  8]]\n+    [[ 9 10 11]]]]\n+  >>> y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)\n+  >>> print(y)\n+  tf.Tensor(\n+    [[[[ 0  1  2]\n+       [ 0  1  2]]\n+      [[ 3  4  5]\n+       [ 3  4  5]]]\n+     [[[ 6  7  8]\n+       [ 6  7  8]]\n+      [[ 9 10 11]\n+       [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)\n+\n+  Args:\n+    size: Int, or tuple of 2 integers.\n+      The upsampling factors for rows and columns.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch_size, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    interpolation: A string, one of `nearest` or `bilinear`.\n+\n+  Input shape:\n+    4D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, rows, cols, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, channels, rows, cols)`\n+\n+  Output shape:\n+    4D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, upsampled_rows, upsampled_cols, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, channels, upsampled_rows, upsampled_cols)`\n+  \"\"\"\n+\n+  def __init__(self,\n+               size=(2, 2),\n+               data_format=None,\n+               interpolation='nearest',\n+               **kwargs):\n+    super(UpSampling2D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.size = conv_utils.normalize_tuple(size, 2, 'size')\n+    if interpolation not in {'nearest', 'bilinear'}:\n+      raise ValueError('`interpolation` argument should be one of `\"nearest\"` '\n+                       f'or `\"bilinear\"`. Received: \"{interpolation}\".')\n+    self.interpolation = interpolation\n+    self.input_spec = InputSpec(ndim=4)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      height = self.size[0] * input_shape[\n+          2] if input_shape[2] is not None else None\n+      width = self.size[1] * input_shape[\n+          3] if input_shape[3] is not None else None\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], height, width])\n+    else:\n+      height = self.size[0] * input_shape[\n+          1] if input_shape[1] is not None else None\n+      width = self.size[1] * input_shape[\n+          2] if input_shape[2] is not None else None\n+      return tf.TensorShape(\n+          [input_shape[0], height, width, input_shape[3]])\n+\n+  def call(self, inputs):\n+    return backend.resize_images(\n+        inputs, self.size[0], self.size[1], self.data_format,\n+        interpolation=self.interpolation)\n+\n+  def get_config(self):\n+    config = {\n+        'size': self.size,\n+        'data_format': self.data_format,\n+        'interpolation': self.interpolation\n+    }\n+    base_config = super(UpSampling2D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,105 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras upsampling layer for 3D inputs.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.UpSampling3D')\n+class UpSampling3D(Layer):\n+  \"\"\"Upsampling layer for 3D inputs.\n+\n+  Repeats the 1st, 2nd and 3rd dimensions\n+  of the data by `size[0]`, `size[1]` and `size[2]` respectively.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 1, 2, 1, 3)\n+  >>> x = tf.constant(1, shape=input_shape)\n+  >>> y = tf.keras.layers.UpSampling3D(size=2)(x)\n+  >>> print(y.shape)\n+  (2, 2, 4, 2, 3)\n+\n+  Args:\n+    size: Int, or tuple of 3 integers.\n+      The upsampling factors for dim1, dim2 and dim3.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    5D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, dim1, dim2, dim3, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, channels, dim1, dim2, dim3)`\n+\n+  Output shape:\n+    5D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)`\n+  \"\"\"\n+\n+  def __init__(self, size=(2, 2, 2), data_format=None, **kwargs):\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.size = conv_utils.normalize_tuple(size, 3, 'size')\n+    self.input_spec = InputSpec(ndim=5)\n+    super(UpSampling3D, self).__init__(**kwargs)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      dim1 = self.size[0] * input_shape[\n+          2] if input_shape[2] is not None else None\n+      dim2 = self.size[1] * input_shape[\n+          3] if input_shape[3] is not None else None\n+      dim3 = self.size[2] * input_shape[\n+          4] if input_shape[4] is not None else None\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], dim1, dim2, dim3])\n+    else:\n+      dim1 = self.size[0] * input_shape[\n+          1] if input_shape[1] is not None else None\n+      dim2 = self.size[1] * input_shape[\n+          2] if input_shape[2] is not None else None\n+      dim3 = self.size[2] * input_shape[\n+          3] if input_shape[3] is not None else None\n+      return tf.TensorShape(\n+          [input_shape[0], dim1, dim2, dim3, input_shape[4]])\n+\n+  def call(self, inputs):\n+    return backend.resize_volumes(\n+        inputs, self.size[0], self.size[1], self.size[2], self.data_format)\n+\n+  def get_config(self):\n+    config = {'size': self.size, 'data_format': self.data_format}\n+    base_config = super(UpSampling3D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,178 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for convolutional layers.\"\"\"\n+# pylint: disable=g-direct-tensorflow-import\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.framework import test_util\n+\n+\n+@test_util.for_all_test_methods(test_util.disable_xla,\n+                                'align_corners=False not supported by XLA')\n+@keras_parameterized.run_all_keras_modes\n+class UpSamplingTest(keras_parameterized.TestCase):\n+\n+  def test_upsampling_1d(self):\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.UpSampling1D, kwargs={'size': 2}, input_shape=(3, 5, 4))\n+\n+  def test_upsampling_2d(self):\n+    num_samples = 2\n+    stack_size = 2\n+    input_num_row = 11\n+    input_num_col = 12\n+\n+    for data_format in ['channels_first', 'channels_last']:\n+      if data_format == 'channels_first':\n+        inputs = np.random.rand(num_samples, stack_size, input_num_row,\n+                                input_num_col)\n+      else:\n+        inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n+                                stack_size)\n+\n+      # basic test\n+      with self.cached_session():\n+        testing_utils.layer_test(\n+            keras.layers.UpSampling2D,\n+            kwargs={'size': (2, 2),\n+                    'data_format': data_format},\n+            input_shape=inputs.shape)\n+\n+        for length_row in [2]:\n+          for length_col in [2, 3]:\n+            layer = keras.layers.UpSampling2D(\n+                size=(length_row, length_col), data_format=data_format)\n+            layer.build(inputs.shape)\n+            output = layer(keras.backend.variable(inputs))\n+            if tf.executing_eagerly():\n+              np_output = output.numpy()\n+            else:\n+              np_output = keras.backend.eval(output)\n+            if data_format == 'channels_first':\n+              assert np_output.shape[2] == length_row * input_num_row\n+              assert np_output.shape[3] == length_col * input_num_col\n+            else:  # tf\n+              assert np_output.shape[1] == length_row * input_num_row\n+              assert np_output.shape[2] == length_col * input_num_col\n+\n+            # compare with numpy\n+            if data_format == 'channels_first':\n+              expected_out = np.repeat(inputs, length_row, axis=2)\n+              expected_out = np.repeat(expected_out, length_col, axis=3)\n+            else:  # tf\n+              expected_out = np.repeat(inputs, length_row, axis=1)\n+              expected_out = np.repeat(expected_out, length_col, axis=2)\n+\n+            np.testing.assert_allclose(np_output, expected_out)\n+\n+  def test_upsampling_2d_bilinear(self):\n+    num_samples = 2\n+    stack_size = 2\n+    input_num_row = 11\n+    input_num_col = 12\n+    for data_format in ['channels_first', 'channels_last']:\n+      if data_format == 'channels_first':\n+        inputs = np.random.rand(num_samples, stack_size, input_num_row,\n+                                input_num_col)\n+      else:\n+        inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n+                                stack_size)\n+\n+      testing_utils.layer_test(keras.layers.UpSampling2D,\n+                               kwargs={'size': (2, 2),\n+                                       'data_format': data_format,\n+                                       'interpolation': 'bilinear'},\n+                               input_shape=inputs.shape)\n+\n+      if not tf.executing_eagerly():\n+        for length_row in [2]:\n+          for length_col in [2, 3]:\n+            layer = keras.layers.UpSampling2D(\n+                size=(length_row, length_col),\n+                data_format=data_format)\n+            layer.build(inputs.shape)\n+            outputs = layer(keras.backend.variable(inputs))\n+            np_output = keras.backend.eval(outputs)\n+            if data_format == 'channels_first':\n+              self.assertEqual(np_output.shape[2], length_row * input_num_row)\n+              self.assertEqual(np_output.shape[3], length_col * input_num_col)\n+            else:\n+              self.assertEqual(np_output.shape[1], length_row * input_num_row)\n+              self.assertEqual(np_output.shape[2], length_col * input_num_col)\n+\n+  def test_upsampling_3d(self):\n+    num_samples = 2\n+    stack_size = 2\n+    input_len_dim1 = 10\n+    input_len_dim2 = 11\n+    input_len_dim3 = 12\n+\n+    for data_format in ['channels_first', 'channels_last']:\n+      if data_format == 'channels_first':\n+        inputs = np.random.rand(num_samples, stack_size, input_len_dim1,\n+                                input_len_dim2, input_len_dim3)\n+      else:\n+        inputs = np.random.rand(num_samples, input_len_dim1, input_len_dim2,\n+                                input_len_dim3, stack_size)\n+\n+      # basic test\n+      with self.cached_session():\n+        testing_utils.layer_test(\n+            keras.layers.UpSampling3D,\n+            kwargs={'size': (2, 2, 2),\n+                    'data_format': data_format},\n+            input_shape=inputs.shape)\n+\n+        for length_dim1 in [2, 3]:\n+          for length_dim2 in [2]:\n+            for length_dim3 in [3]:\n+              layer = keras.layers.UpSampling3D(\n+                  size=(length_dim1, length_dim2, length_dim3),\n+                  data_format=data_format)\n+              layer.build(inputs.shape)\n+              output = layer(keras.backend.variable(inputs))\n+              if tf.executing_eagerly():\n+                np_output = output.numpy()\n+              else:\n+                np_output = keras.backend.eval(output)\n+              if data_format == 'channels_first':\n+                assert np_output.shape[2] == length_dim1 * input_len_dim1\n+                assert np_output.shape[3] == length_dim2 * input_len_dim2\n+                assert np_output.shape[4] == length_dim3 * input_len_dim3\n+              else:  # tf\n+                assert np_output.shape[1] == length_dim1 * input_len_dim1\n+                assert np_output.shape[2] == length_dim2 * input_len_dim2\n+                assert np_output.shape[3] == length_dim3 * input_len_dim3\n+\n+              # compare with numpy\n+              if data_format == 'channels_first':\n+                expected_out = np.repeat(inputs, length_dim1, axis=2)\n+                expected_out = np.repeat(expected_out, length_dim2, axis=3)\n+                expected_out = np.repeat(expected_out, length_dim3, axis=4)\n+              else:  # tf\n+                expected_out = np.repeat(inputs, length_dim1, axis=1)\n+                expected_out = np.repeat(expected_out, length_dim2, axis=2)\n+                expected_out = np.repeat(expected_out, length_dim3, axis=3)\n+\n+              np.testing.assert_allclose(np_output, expected_out)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,91 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras zero-padding layer for 1D input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ZeroPadding1D')\n+class ZeroPadding1D(Layer):\n+  \"\"\"Zero-padding layer for 1D input (e.g. temporal sequence).\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 2, 3)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> print(x)\n+  [[[ 0  1  2]\n+    [ 3  4  5]]\n+   [[ 6  7  8]\n+    [ 9 10 11]]]\n+  >>> y = tf.keras.layers.ZeroPadding1D(padding=2)(x)\n+  >>> print(y)\n+  tf.Tensor(\n+    [[[ 0  0  0]\n+      [ 0  0  0]\n+      [ 0  1  2]\n+      [ 3  4  5]\n+      [ 0  0  0]\n+      [ 0  0  0]]\n+     [[ 0  0  0]\n+      [ 0  0  0]\n+      [ 6  7  8]\n+      [ 9 10 11]\n+      [ 0  0  0]\n+      [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)\n+\n+  Args:\n+      padding: Int, or tuple of int (length 2), or dictionary.\n+          - If int:\n+          How many zeros to add at the beginning and end of\n+          the padding dimension (axis 1).\n+          - If tuple of int (length 2):\n+          How many zeros to add at the beginning and the end of\n+          the padding dimension (`(left_pad, right_pad)`).\n+\n+  Input shape:\n+      3D tensor with shape `(batch_size, axis_to_pad, features)`\n+\n+  Output shape:\n+      3D tensor with shape `(batch_size, padded_axis, features)`\n+  \"\"\"\n+\n+  def __init__(self, padding=1, **kwargs):\n+    super(ZeroPadding1D, self).__init__(**kwargs)\n+    self.padding = conv_utils.normalize_tuple(\n+        padding, 2, 'padding', allow_zero=True)\n+    self.input_spec = InputSpec(ndim=3)\n+\n+  def compute_output_shape(self, input_shape):\n+    if input_shape[1] is not None:\n+      length = input_shape[1] + self.padding[0] + self.padding[1]\n+    else:\n+      length = None\n+    return tf.TensorShape([input_shape[0], length, input_shape[2]])\n+\n+  def call(self, inputs):\n+    return backend.temporal_padding(inputs, padding=self.padding)\n+\n+  def get_config(self):\n+    config = {'padding': self.padding}\n+    base_config = super(ZeroPadding1D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,149 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras zero-padding layer for 2D input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ZeroPadding2D')\n+class ZeroPadding2D(Layer):\n+  \"\"\"Zero-padding layer for 2D input (e.g. picture).\n+\n+  This layer can add rows and columns of zeros\n+  at the top, bottom, left and right side of an image tensor.\n+\n+  Examples:\n+\n+  >>> input_shape = (1, 1, 2, 2)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> print(x)\n+  [[[[0 1]\n+     [2 3]]]]\n+  >>> y = tf.keras.layers.ZeroPadding2D(padding=1)(x)\n+  >>> print(y)\n+  tf.Tensor(\n+    [[[[0 0]\n+       [0 0]\n+       [0 0]\n+       [0 0]]\n+      [[0 0]\n+       [0 1]\n+       [2 3]\n+       [0 0]]\n+      [[0 0]\n+       [0 0]\n+       [0 0]\n+       [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)\n+\n+  Args:\n+    padding: Int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n+      - If int: the same symmetric padding\n+        is applied to height and width.\n+      - If tuple of 2 ints:\n+        interpreted as two different\n+        symmetric padding values for height and width:\n+        `(symmetric_height_pad, symmetric_width_pad)`.\n+      - If tuple of 2 tuples of 2 ints:\n+        interpreted as\n+        `((top_pad, bottom_pad), (left_pad, right_pad))`\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch_size, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    4D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, rows, cols, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, channels, rows, cols)`\n+\n+  Output shape:\n+    4D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, padded_rows, padded_cols, channels)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, channels, padded_rows, padded_cols)`\n+  \"\"\"\n+\n+  def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n+    super(ZeroPadding2D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    if isinstance(padding, int):\n+      self.padding = ((padding, padding), (padding, padding))\n+    elif hasattr(padding, '__len__'):\n+      if len(padding) != 2:\n+        raise ValueError('`padding` should have two elements. '\n+                         f'Received: {padding}.')\n+      height_padding = conv_utils.normalize_tuple(\n+          padding[0], 2, '1st entry of padding', allow_zero=True)\n+      width_padding = conv_utils.normalize_tuple(\n+          padding[1], 2, '2nd entry of padding', allow_zero=True)\n+      self.padding = (height_padding, width_padding)\n+    else:\n+      raise ValueError('`padding` should be either an int, '\n+                       'a tuple of 2 ints '\n+                       '(symmetric_height_pad, symmetric_width_pad), '\n+                       'or a tuple of 2 tuples of 2 ints '\n+                       '((top_pad, bottom_pad), (left_pad, right_pad)). '\n+                       f'Received: {padding}.')\n+    self.input_spec = InputSpec(ndim=4)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      if input_shape[2] is not None:\n+        rows = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n+      else:\n+        rows = None\n+      if input_shape[3] is not None:\n+        cols = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n+      else:\n+        cols = None\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], rows, cols])\n+    elif self.data_format == 'channels_last':\n+      if input_shape[1] is not None:\n+        rows = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n+      else:\n+        rows = None\n+      if input_shape[2] is not None:\n+        cols = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n+      else:\n+        cols = None\n+      return tf.TensorShape(\n+          [input_shape[0], rows, cols, input_shape[3]])\n+\n+  def call(self, inputs):\n+    return backend.spatial_2d_padding(\n+        inputs, padding=self.padding, data_format=self.data_format)\n+\n+  def get_config(self):\n+    config = {'padding': self.padding, 'data_format': self.data_format}\n+    base_config = super(ZeroPadding2D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,150 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras zero-padding layer for 3D input.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.ZeroPadding3D')\n+class ZeroPadding3D(Layer):\n+  \"\"\"Zero-padding layer for 3D data (spatial or spatio-temporal).\n+\n+  Examples:\n+\n+  >>> input_shape = (1, 1, 2, 2, 3)\n+  >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+  >>> y = tf.keras.layers.ZeroPadding3D(padding=2)(x)\n+  >>> print(y.shape)\n+  (1, 5, 6, 6, 3)\n+\n+  Args:\n+    padding: Int, or tuple of 3 ints, or tuple of 3 tuples of 2 ints.\n+      - If int: the same symmetric padding\n+        is applied to height and width.\n+      - If tuple of 3 ints:\n+        interpreted as two different\n+        symmetric padding values for height and width:\n+        `(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)`.\n+      - If tuple of 3 tuples of 2 ints:\n+        interpreted as\n+        `((left_dim1_pad, right_dim1_pad), (left_dim2_pad,\n+          right_dim2_pad), (left_dim3_pad, right_dim3_pad))`\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    5D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad,\n+          depth)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, depth, first_axis_to_pad, second_axis_to_pad,\n+          third_axis_to_pad)`\n+\n+  Output shape:\n+    5D tensor with shape:\n+    - If `data_format` is `\"channels_last\"`:\n+        `(batch_size, first_padded_axis, second_padded_axis, third_axis_to_pad,\n+          depth)`\n+    - If `data_format` is `\"channels_first\"`:\n+        `(batch_size, depth, first_padded_axis, second_padded_axis,\n+          third_axis_to_pad)`\n+  \"\"\"\n+\n+  def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):\n+    super(ZeroPadding3D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    if isinstance(padding, int):\n+      self.padding = ((padding, padding), (padding, padding), (padding,\n+                                                               padding))\n+    elif hasattr(padding, '__len__'):\n+      if len(padding) != 3:\n+        raise ValueError('`padding` should have 3 elements. '\n+                         f'Received: {padding}.')\n+      dim1_padding = conv_utils.normalize_tuple(\n+          padding[0], 2, '1st entry of padding', allow_zero=True)\n+      dim2_padding = conv_utils.normalize_tuple(\n+          padding[1], 2, '2nd entry of padding', allow_zero=True)\n+      dim3_padding = conv_utils.normalize_tuple(\n+          padding[2], 2, '3rd entry of padding', allow_zero=True)\n+      self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+    else:\n+      raise ValueError(\n+          '`padding` should be either an int, '\n+          'a tuple of 3 ints '\n+          '(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), '\n+          'or a tuple of 3 tuples of 2 ints '\n+          '((left_dim1_pad, right_dim1_pad),'\n+          ' (left_dim2_pad, right_dim2_pad),'\n+          ' (left_dim3_pad, right_dim2_pad)). '\n+          f'Received: {padding}.')\n+    self.input_spec = InputSpec(ndim=5)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      if input_shape[2] is not None:\n+        dim1 = input_shape[2] + self.padding[0][0] + self.padding[0][1]\n+      else:\n+        dim1 = None\n+      if input_shape[3] is not None:\n+        dim2 = input_shape[3] + self.padding[1][0] + self.padding[1][1]\n+      else:\n+        dim2 = None\n+      if input_shape[4] is not None:\n+        dim3 = input_shape[4] + self.padding[2][0] + self.padding[2][1]\n+      else:\n+        dim3 = None\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], dim1, dim2, dim3])\n+    elif self.data_format == 'channels_last':\n+      if input_shape[1] is not None:\n+        dim1 = input_shape[1] + self.padding[0][0] + self.padding[0][1]\n+      else:\n+        dim1 = None\n+      if input_shape[2] is not None:\n+        dim2 = input_shape[2] + self.padding[1][0] + self.padding[1][1]\n+      else:\n+        dim2 = None\n+      if input_shape[3] is not None:\n+        dim3 = input_shape[3] + self.padding[2][0] + self.padding[2][1]\n+      else:\n+        dim3 = None\n+      return tf.TensorShape(\n+          [input_shape[0], dim1, dim2, dim3, input_shape[4]])\n+\n+  def call(self, inputs):\n+    return backend.spatial_3d_padding(\n+        inputs, padding=self.padding, data_format=self.data_format)\n+\n+  def get_config(self):\n+    config = {'padding': self.padding, 'data_format': self.data_format}\n+    base_config = super(ZeroPadding3D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,259 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for zero-padding layers.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ZeroPaddingTest(keras_parameterized.TestCase):\n+\n+  def test_zero_padding_1d(self):\n+    num_samples = 2\n+    input_dim = 2\n+    num_steps = 5\n+    shape = (num_samples, num_steps, input_dim)\n+    inputs = np.ones(shape)\n+\n+    with self.cached_session():\n+      # basic test\n+      testing_utils.layer_test(\n+          keras.layers.ZeroPadding1D,\n+          kwargs={'padding': 2},\n+          input_shape=inputs.shape)\n+      testing_utils.layer_test(\n+          keras.layers.ZeroPadding1D,\n+          kwargs={'padding': (1, 2)},\n+          input_shape=inputs.shape)\n+\n+      # correctness test\n+      layer = keras.layers.ZeroPadding1D(padding=2)\n+      layer.build(shape)\n+      output = layer(keras.backend.variable(inputs))\n+      if tf.executing_eagerly():\n+        np_output = output.numpy()\n+      else:\n+        np_output = keras.backend.eval(output)\n+      for offset in [0, 1, -1, -2]:\n+        np.testing.assert_allclose(np_output[:, offset, :], 0.)\n+      np.testing.assert_allclose(np_output[:, 2:-2, :], 1.)\n+\n+      layer = keras.layers.ZeroPadding1D(padding=(1, 2))\n+      layer.build(shape)\n+      output = layer(keras.backend.variable(inputs))\n+      if tf.executing_eagerly():\n+        np_output = output.numpy()\n+      else:\n+        np_output = keras.backend.eval(output)\n+      for left_offset in [0]:\n+        np.testing.assert_allclose(np_output[:, left_offset, :], 0.)\n+      for right_offset in [-1, -2]:\n+        np.testing.assert_allclose(np_output[:, right_offset, :], 0.)\n+      np.testing.assert_allclose(np_output[:, 1:-2, :], 1.)\n+      layer.get_config()\n+\n+    # test incorrect use\n+    with self.assertRaises(ValueError):\n+      keras.layers.ZeroPadding1D(padding=(1, 1, 1))\n+    with self.assertRaises(ValueError):\n+      keras.layers.ZeroPadding1D(padding=None)\n+\n+  @parameterized.named_parameters(('channels_first', 'channels_first'),\n+                                  ('channels_last', 'channels_last'))\n+  def test_zero_padding_2d(self, data_format):\n+    num_samples = 2\n+    stack_size = 2\n+    input_num_row = 4\n+    input_num_col = 5\n+    if data_format == 'channels_first':\n+      inputs = np.ones((num_samples, stack_size, input_num_row, input_num_col))\n+    elif data_format == 'channels_last':\n+      inputs = np.ones((num_samples, input_num_row, input_num_col, stack_size))\n+\n+    # basic test\n+    with self.cached_session():\n+      testing_utils.layer_test(\n+          keras.layers.ZeroPadding2D,\n+          kwargs={\n+              'padding': (2, 2),\n+              'data_format': data_format\n+          },\n+          input_shape=inputs.shape)\n+      testing_utils.layer_test(\n+          keras.layers.ZeroPadding2D,\n+          kwargs={\n+              'padding': ((1, 2), (3, 4)),\n+              'data_format': data_format\n+          },\n+          input_shape=inputs.shape)\n+\n+    # correctness test\n+    with self.cached_session():\n+      layer = keras.layers.ZeroPadding2D(\n+          padding=(2, 2), data_format=data_format)\n+      layer.build(inputs.shape)\n+      output = layer(keras.backend.variable(inputs))\n+      if tf.executing_eagerly():\n+        np_output = output.numpy()\n+      else:\n+        np_output = keras.backend.eval(output)\n+      if data_format == 'channels_last':\n+        for offset in [0, 1, -1, -2]:\n+          np.testing.assert_allclose(np_output[:, offset, :, :], 0.)\n+          np.testing.assert_allclose(np_output[:, :, offset, :], 0.)\n+        np.testing.assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n+      elif data_format == 'channels_first':\n+        for offset in [0, 1, -1, -2]:\n+          np.testing.assert_allclose(np_output[:, :, offset, :], 0.)\n+          np.testing.assert_allclose(np_output[:, :, :, offset], 0.)\n+        np.testing.assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n+\n+      layer = keras.layers.ZeroPadding2D(\n+          padding=((1, 2), (3, 4)), data_format=data_format)\n+      layer.build(inputs.shape)\n+      output = layer(keras.backend.variable(inputs))\n+      if tf.executing_eagerly():\n+        np_output = output.numpy()\n+      else:\n+        np_output = keras.backend.eval(output)\n+      if data_format == 'channels_last':\n+        for top_offset in [0]:\n+          np.testing.assert_allclose(np_output[:, top_offset, :, :], 0.)\n+        for bottom_offset in [-1, -2]:\n+          np.testing.assert_allclose(np_output[:, bottom_offset, :, :], 0.)\n+        for left_offset in [0, 1, 2]:\n+          np.testing.assert_allclose(np_output[:, :, left_offset, :], 0.)\n+        for right_offset in [-1, -2, -3, -4]:\n+          np.testing.assert_allclose(np_output[:, :, right_offset, :], 0.)\n+        np.testing.assert_allclose(np_output[:, 1:-2, 3:-4, :], 1.)\n+      elif data_format == 'channels_first':\n+        for top_offset in [0]:\n+          np.testing.assert_allclose(np_output[:, :, top_offset, :], 0.)\n+        for bottom_offset in [-1, -2]:\n+          np.testing.assert_allclose(np_output[:, :, bottom_offset, :], 0.)\n+        for left_offset in [0, 1, 2]:\n+          np.testing.assert_allclose(np_output[:, :, :, left_offset], 0.)\n+        for right_offset in [-1, -2, -3, -4]:\n+          np.testing.assert_allclose(np_output[:, :, :, right_offset], 0.)\n+        np.testing.assert_allclose(np_output[:, :, 1:-2, 3:-4], 1.)\n+\n+    # test incorrect use\n+    with self.assertRaises(ValueError):\n+      keras.layers.ZeroPadding2D(padding=(1, 1, 1))\n+    with self.assertRaises(ValueError):\n+      keras.layers.ZeroPadding2D(padding=None)\n+\n+  @parameterized.named_parameters(('channels_first', 'channels_first'),\n+                                  ('channels_last', 'channels_last'))\n+  def test_zero_padding_3d(self, data_format):\n+    num_samples = 2\n+    stack_size = 2\n+    input_len_dim1 = 4\n+    input_len_dim2 = 5\n+    input_len_dim3 = 3\n+\n+    if data_format == 'channels_first':\n+      inputs = np.ones((num_samples, stack_size, input_len_dim1, input_len_dim2,\n+                        input_len_dim3))\n+    elif data_format == 'channels_last':\n+      inputs = np.ones((num_samples, input_len_dim1, input_len_dim2,\n+                        input_len_dim3, stack_size))\n+\n+    with self.cached_session():\n+      # basic test\n+      testing_utils.layer_test(\n+          keras.layers.ZeroPadding3D,\n+          kwargs={\n+              'padding': (2, 2, 2),\n+              'data_format': data_format\n+          },\n+          input_shape=inputs.shape)\n+      testing_utils.layer_test(\n+          keras.layers.ZeroPadding3D,\n+          kwargs={\n+              'padding': ((1, 2), (3, 4), (0, 2)),\n+              'data_format': data_format\n+          },\n+          input_shape=inputs.shape)\n+\n+    with self.cached_session():\n+      # correctness test\n+      layer = keras.layers.ZeroPadding3D(\n+          padding=(2, 2, 2), data_format=data_format)\n+      layer.build(inputs.shape)\n+      output = layer(keras.backend.variable(inputs))\n+      if tf.executing_eagerly():\n+        np_output = output.numpy()\n+      else:\n+        np_output = keras.backend.eval(output)\n+      if data_format == 'channels_last':\n+        for offset in [0, 1, -1, -2]:\n+          np.testing.assert_allclose(np_output[:, offset, :, :, :], 0.)\n+          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n+          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n+        np.testing.assert_allclose(np_output[:, 2:-2, 2:-2, 2:-2, :], 1.)\n+      elif data_format == 'channels_first':\n+        for offset in [0, 1, -1, -2]:\n+          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n+          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n+          np.testing.assert_allclose(np_output[:, :, :, :, offset], 0.)\n+        np.testing.assert_allclose(np_output[:, :, 2:-2, 2:-2, 2:-2], 1.)\n+\n+      layer = keras.layers.ZeroPadding3D(\n+          padding=((1, 2), (3, 4), (0, 2)), data_format=data_format)\n+      layer.build(inputs.shape)\n+      output = layer(keras.backend.variable(inputs))\n+      if tf.executing_eagerly():\n+        np_output = output.numpy()\n+      else:\n+        np_output = keras.backend.eval(output)\n+      if data_format == 'channels_last':\n+        for offset in [0]:\n+          np.testing.assert_allclose(np_output[:, offset, :, :, :], 0.)\n+        for offset in [-1, -2]:\n+          np.testing.assert_allclose(np_output[:, offset, :, :, :], 0.)\n+        for offset in [0, 1, 2]:\n+          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n+        for offset in [-1, -2, -3, -4]:\n+          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n+        for offset in [-1, -2]:\n+          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n+        np.testing.assert_allclose(np_output[:, 1:-2, 3:-4, 0:-2, :], 1.)\n+      elif data_format == 'channels_first':\n+        for offset in [0]:\n+          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n+        for offset in [-1, -2]:\n+          np.testing.assert_allclose(np_output[:, :, offset, :, :], 0.)\n+        for offset in [0, 1, 2]:\n+          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n+        for offset in [-1, -2, -3, -4]:\n+          np.testing.assert_allclose(np_output[:, :, :, offset, :], 0.)\n+        for offset in [-1, -2]:\n+          np.testing.assert_allclose(np_output[:, :, :, :, offset], 0.)\n+        np.testing.assert_allclose(np_output[:, :, 1:-2, 3:-4, 0:-2], 1.)\n+\n+    # test incorrect use\n+    with self.assertRaises(ValueError):\n+      keras.layers.ZeroPadding3D(padding=(1, 1))\n+    with self.assertRaises(ValueError):\n+      keras.layers.ZeroPadding3D(padding=None)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -15,8 +15,7 @@\n \"\"\"Layer serialization/deserialization functions.\"\"\"\n \n import tensorflow.compat.v2 as tf\n-# pylint: disable=wildcard-import\n-# pylint: disable=unused-import\n+# pylint: disable=g-bad-import-order,g-direct-tensorflow-import,unused-import,wildcard-import\n \n import threading\n from keras.engine import base_layer\n@@ -37,6 +36,7 @@ from keras.layers import noise\n from keras.layers import pooling\n from keras.layers import recurrent\n from keras.layers import recurrent_v2\n+from keras.layers import reshaping\n from keras.layers import rnn_cell_wrapper_v2\n from keras.layers import wrappers\n from keras.layers.normalization import batch_normalization\n@@ -61,8 +61,8 @@ ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n                embeddings, einsum_dense, local, merge, noise,\n                batch_normalization_v1, layer_normalization, unit_normalization,\n-               pooling, image_preprocessing, recurrent, wrappers, hashing,\n-               hashed_crossing, category_encoding, discretization,\n+               pooling, image_preprocessing, recurrent, reshaping, wrappers,\n+               hashing, hashed_crossing, category_encoding, discretization,\n                multi_head_attention, integer_lookup,\n                preprocessing_normalization, string_lookup, text_vectorization)\n ALL_V2_MODULES = (rnn_cell_wrapper_v2, batch_normalization, layer_normalization,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
