{"custom_id": "keras#ae3102106f4eb4e6ed76a1c8e3c116e3d9f2d01f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2829 | Contributors (this commit): 6 | Commits (past 90d): 36 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,7 +3,7 @@\n To run the benchmark, make sure you are in model_benchmark/ directory, and run\n the command below:\n \n-python3 -m model_benchmark.resnet_image_classification_benchmark \\\n+python3 -m model_benchmark.efficient_net_image_classification_benchmark \\\n     --epochs=2 \\\n     --batch_size=32\n \"\"\"\n\n@@ -661,7 +661,7 @@ def repeat(x, repeats, axis=None):\n \n         x = KerasTensor(x.shape, standardize_dtype(x.dtype))\n         outputs = repeat(x, repeats, axis=axis)\n-        print(outputs)\n+\n         return torch.empty(\n             size=outputs.shape,\n             dtype=to_torch_dtype(outputs.dtype),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaa95600ef3e5c52ebfa82ed742ae02406d22e77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 331 | Contributors (this commit): 8 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class Variable(\n         )\n \n     def _direct_assign(self, value):\n-        self.value.assign(value)\n+        self._value.assign(tf.cast(value, self._value.dtype))\n \n     def _convert_to_tensor(self, value, dtype=None):\n         return convert_to_tensor(value, dtype=dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fff9bbbe38be8775105a39b8899fd19feff7f890", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 30 | Files Changed: 6 | Hunks: 18 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 80 | Churn Cumulative: 2140 | Contributors (this commit): 11 | Commits (past 90d): 73 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,7 @@\n+try:\n+    # When using torch and tensorflow, torch needs to be imported first,\n+    # otherwise it will segfault upon import. This should force the torch\n+    # import to happen first for all tests.\n+    import torch  # noqa: F401\n+except ImportError:\n+    pass\n\n@@ -5,7 +5,6 @@ import inspect\n import types\n import warnings\n \n-import jax\n import numpy as np\n import tensorflow as tf\n \n@@ -162,11 +161,7 @@ def serialize_keras_object(obj):\n         }\n     if isinstance(obj, tf.TensorShape):\n         return obj.as_list() if obj._dims is not None else None\n-    if isinstance(obj, (tf.Tensor, jax.numpy.ndarray)) or hasattr(\n-        obj, \"device\"\n-    ):\n-        # Import torch creates circular dependency, so we use\n-        # `hasattr(obj, \"device\")` to check if obj is a torch tensor.\n+    if backend.is_tensor(obj):\n         return {\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n\n@@ -27,11 +27,7 @@ class ArrayDataAdapter(DataAdapter):\n         shuffle=False,\n         class_weight=None,\n     ):\n-        types_struct = nest.map_structure(lambda x: type(x), x)\n-        flat_types = nest.flatten(types_struct)\n-        if not all(\n-            issubclass(c, data_adapter_utils.ARRAY_TYPES) for c in flat_types\n-        ):\n+        if not can_convert_arrays((x, y, sample_weight)):\n             raise ValueError(\n                 \"Expected all elements of `x` to be array-like. \"\n                 f\"Received invalid types: x={x}\"\n@@ -252,6 +248,28 @@ class ArrayDataAdapter(DataAdapter):\n         return self._partial_batch_size or None\n \n \n+def can_convert_arrays(arrays):\n+    \"\"\"Check if array like-inputs can be handled by `ArrayDataAdapter`\n+\n+    Args:\n+        inputs: Structure of `Tensor`s, NumPy arrays, or tensor-like.\n+\n+    Returns:\n+        `True` if `arrays` can be handled by `ArrayDataAdapter`, `False`\n+        otherwise.\n+    \"\"\"\n+\n+    def can_convert_single_array(x):\n+        is_none = x is None\n+        known_type = isinstance(x, data_adapter_utils.ARRAY_TYPES)\n+        convertable_type = hasattr(x, \"__array__\")\n+        return is_none or known_type or convertable_type\n+\n+    return all(\n+        tf.nest.flatten(tf.nest.map_structure(can_convert_single_array, arrays))\n+    )\n+\n+\n def convert_to_arrays(arrays, dtype=None):\n     \"\"\"Process array-like inputs.\n \n@@ -262,7 +280,7 @@ def convert_to_arrays(arrays, dtype=None):\n     - Converts `list`s to `tuple`s (for `tf.data` support).\n \n     Args:\n-        inputs: Structure of `Tensor`s, `NumPy` arrays, or tensor-like.\n+        inputs: Structure of `Tensor`s, NumPy arrays, or tensor-like.\n \n     Returns:\n         Structure of NumPy `ndarray`s.\n@@ -277,15 +295,16 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.expand_dims(x.to_numpy(dtype=dtype), axis=-1)\n             elif isinstance(x, pandas.DataFrame):\n                 x = x.to_numpy(dtype=dtype)\n-        if isinstance(x, (tf.Tensor, tf.Variable)):\n-            x = x.numpy()\n         if not isinstance(x, np.ndarray):\n+            # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n+            # `torch.Tensor`, as well as any other tensor-like object that has\n+            # added numpy support.\n             if hasattr(x, \"__array__\"):\n                 x = np.array(x, dtype=dtype)\n             else:\n                 raise ValueError(\n-                    \"Expected a NumPy array, tf.Tensor, \"\n-                    \"Pandas Dataframe, or Pandas Series. \"\n+                    \"Expected a NumPy array, tf.Tensor, jax.np.ndarray, \"\n+                    \"torch.Tensor, Pandas Dataframe, or Pandas Series. \"\n                     f\"Received invalid input: {x} (of type {type(x)})\"\n                 )\n         if x.dtype == object:\n\n@@ -2,6 +2,7 @@ import jax\n import numpy as np\n import pandas\n import tensorflow as tf\n+import torch\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -10,7 +11,9 @@ from keras_core.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters([(\"np\",), (\"tf\",), (\"pandas\")])\n+    @parameterized.parameters(\n+        [(\"np\",), (\"tf\",), (\"jax\",), (\"torch\",), (\"pandas\")]\n+    )\n     def test_basic_flow(self, array_type):\n         if array_type == \"np\":\n             x = np.random.random((34, 4))\n@@ -21,6 +24,9 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n         elif array_type == \"jax\":\n             x = jax.numpy.ones((34, 4))\n             y = jax.numpy.ones((34, 2))\n+        elif array_type == \"torch\":\n+            x = torch.ones((34, 4))\n+            y = torch.ones((34, 2))\n         elif array_type == \"pandas\":\n             x = pandas.DataFrame(np.random.random((34, 4)))\n             y = pandas.DataFrame(np.random.random((34, 2)))\n\n@@ -1,6 +1,5 @@\n import math\n \n-import jax\n import numpy as np\n import tensorflow as tf\n \n@@ -12,15 +11,12 @@ except ImportError:\n     pandas = None\n \n \n-ARRAY_TYPES = (tf.Tensor, np.ndarray, jax.numpy.ndarray)\n+# Leave jax, tf, and torch arrays off this list. Instead we will use\n+# `__array__` to detect these types. Doing so allows us to avoid importing a\n+# backend framework we are not currently using just to do type-checking.\n+ARRAY_TYPES = (np.ndarray,)\n if pandas:\n-    ARRAY_TYPES = ARRAY_TYPES + (\n-        tf.Tensor,\n-        np.ndarray,\n-        pandas.Series,\n-        pandas.DataFrame,\n-    )\n-# TODO: support torch tensors?\n+    ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n \n @keras_core_export(\"keras_core.utils.unpack_x_y_sample_weight\")\n\n@@ -42,10 +42,8 @@ import types\n import warnings\n \n import tensorflow as tf\n-from tensorflow import nest\n \n from keras_core.trainers.data_adapters import array_data_adapter\n-from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.data_adapters import generator_data_adapter\n from keras_core.trainers.data_adapters import py_dataset_adapter\n from keras_core.trainers.data_adapters import tf_dataset_adapter\n@@ -69,8 +67,7 @@ class EpochIterator:\n         if steps_per_epoch:\n             self._current_iterator = None\n             self._insufficient_data = False\n-        first_element = next(iter(nest.flatten(x)), None)\n-        if isinstance(first_element, data_adapter_utils.ARRAY_TYPES):\n+        if array_data_adapter.can_convert_arrays((x, y, sample_weight)):\n             self.data_adapter = array_data_adapter.ArrayDataAdapter(\n                 x,\n                 y,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e9ae0f65e3a320a430b661b60a80a85ea09f717", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 1356 | Contributors (this commit): 10 | Commits (past 90d): 57 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -129,9 +129,9 @@ def convert_to_tensor(x, dtype=None):\n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n-    elif len(x) > 0 and isinstance(x[0], torch.Tensor):\n+    elif len(x) > 0 and any(isinstance(x1, torch.Tensor) for x1 in x):\n         # Handle list or tuple of torch tensors\n-        return torch.stack(x)\n+        return torch.stack([convert_to_tensor(x1) for x1 in x])\n     if isinstance(x, np.ndarray) and x.dtype == np.uint32:\n         # Torch backend does not support uint32.\n         x = x.astype(np.int64)\n\n@@ -239,5 +239,9 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllEqual(x, (1, 1))\n         self.assertIsInstance(x, np.ndarray)\n \n+        # Partially converted.\n+        x = ops.convert_to_tensor((1, ops.array(2), 3))\n+        self.assertAllEqual(x, (1, 2, 3))\n+\n         with self.assertRaises(ValueError):\n             ops.convert_to_numpy(KerasTensor((2,)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0454a403867cd4ba23bf2604f1feaa36c357db17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 32 | Churn Cumulative: 1764 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -172,7 +172,6 @@ def save_model(model, filepath, weights_format=\"h5\"):\n         zip_filepath = filepath\n     try:\n         with zipfile.ZipFile(zip_filepath, \"w\") as zf:\n-\n             with zf.open(_METADATA_FILENAME, \"w\") as f:\n                 f.write(metadata_json.encode())\n             with zf.open(_CONFIG_FILENAME, \"w\") as f:\n@@ -233,7 +232,6 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n         with tf.io.gfile.GFile(\n             filepath, mode=\"r+b\"\n         ) as gfile_handle, zipfile.ZipFile(gfile_handle, \"r\") as zf:\n-\n             with zf.open(_CONFIG_FILENAME, \"r\") as f:\n                 config_json = f.read()\n \n@@ -484,6 +482,10 @@ def _save_container_state(\n \n     for trackable in container:\n         if _is_keras_trackable(trackable):\n+            # Keeps layer name indexing in proper order\n+            # when duplicate layers are in container.\n+            if id(trackable) in visited_trackables:\n+                continue\n             # Do NOT address the trackable via `trackable.name`, since\n             # names are usually autogenerated and thus not reproducible\n             # (i.e. they may vary across two instances of the same model).\n@@ -516,6 +518,13 @@ def _load_container_state(\n \n     for trackable in container:\n         if _is_keras_trackable(trackable):\n+            # Keeps layer name indexing in proper order\n+            # when duplicate layers are in container.\n+            if visited_trackables and id(trackable) in visited_trackables:\n+                continue\n+            # Do NOT address the trackable via `trackable.name`, since\n+            # names are usually autogenerated and thus not reproducible\n+            # (i.e. they may vary across two instances of the same model).\n             name = generic_utils.to_snake_case(trackable.__class__.__name__)\n             if name in used_names:\n                 used_names[name] += 1\n\n@@ -19,6 +19,7 @@ import zipfile\n from pathlib import Path\n from unittest import mock\n \n+import h5py\n import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n@@ -734,6 +735,24 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         out = model(data)\n         self.assertAllClose(ref_out, out, atol=1e-6)\n \n+    def test_layer_index_naming(self):\n+        weights_filepath = os.path.join(self.get_temp_dir(), \"model.weights.h5\")\n+        model = keras.Sequential(\n+            [\n+                keras.layers.Dense(10),\n+                keras.layers.Dense(10),\n+                keras.layers.Dense(10),\n+                keras.layers.Dense(10),\n+            ]\n+        )\n+        model.build([1, 20])\n+        model.save_weights(weights_filepath)\n+        with h5py.File(weights_filepath, \"r\") as f:\n+            self.assertAllEqual(\n+                list(f[\"_layer_checkpoint_dependencies\"].keys()),\n+                [\"dense\", \"dense_1\", \"dense_2\", \"dense_3\"],\n+            )\n+\n \n # This custom class lacks custom object registration.\n class CustomRNN(keras.layers.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccbcd63af75a94ef36e01abf808f5116b6134bb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 33 | Files Changed: 9 | Hunks: 22 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 90 | Churn Cumulative: 4007 | Contributors (this commit): 13 | Commits (past 90d): 172 | Contributors (cumulative): 49 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,7 @@\n+try:\n+    # When using torch and tensorflow, torch needs to be imported first,\n+    # otherwise it will segfault upon import. This should force the torch\n+    # import to happen first for all tests.\n+    import torch  # noqa: F401\n+except ImportError:\n+    pass\n\n@@ -28,7 +28,7 @@ class Variable(\n         )\n \n     def _direct_assign(self, value):\n-        self.value.assign(value)\n+        self._value.assign(tf.cast(value, self._value.dtype))\n \n     def _convert_to_tensor(self, value, dtype=None):\n         return convert_to_tensor(value, dtype=dtype)\n\n@@ -129,9 +129,9 @@ def convert_to_tensor(x, dtype=None):\n     # Convert to np in case of any array-like that is not list or tuple.\n     if not isinstance(x, (list, tuple)):\n         x = np.array(x)\n-    elif len(x) > 0 and isinstance(x[0], torch.Tensor):\n+    elif len(x) > 0 and any(isinstance(x1, torch.Tensor) for x1 in x):\n         # Handle list or tuple of torch tensors\n-        return torch.stack(x)\n+        return torch.stack([convert_to_tensor(x1) for x1 in x])\n     if isinstance(x, np.ndarray) and x.dtype == np.uint32:\n         # Torch backend does not support uint32.\n         x = x.astype(np.int64)\n\n@@ -239,5 +239,9 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllEqual(x, (1, 1))\n         self.assertIsInstance(x, np.ndarray)\n \n+        # Partially converted.\n+        x = ops.convert_to_tensor((1, ops.array(2), 3))\n+        self.assertAllEqual(x, (1, 2, 3))\n+\n         with self.assertRaises(ValueError):\n             ops.convert_to_numpy(KerasTensor((2,)))\n\n@@ -5,7 +5,6 @@ import inspect\n import types\n import warnings\n \n-import jax\n import numpy as np\n import tensorflow as tf\n \n@@ -162,11 +161,7 @@ def serialize_keras_object(obj):\n         }\n     if isinstance(obj, tf.TensorShape):\n         return obj.as_list() if obj._dims is not None else None\n-    if isinstance(obj, (tf.Tensor, jax.numpy.ndarray)) or hasattr(\n-        obj, \"device\"\n-    ):\n-        # Import torch creates circular dependency, so we use\n-        # `hasattr(obj, \"device\")` to check if obj is a torch tensor.\n+    if backend.is_tensor(obj):\n         return {\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n\n@@ -27,11 +27,7 @@ class ArrayDataAdapter(DataAdapter):\n         shuffle=False,\n         class_weight=None,\n     ):\n-        types_struct = nest.map_structure(lambda x: type(x), x)\n-        flat_types = nest.flatten(types_struct)\n-        if not all(\n-            issubclass(c, data_adapter_utils.ARRAY_TYPES) for c in flat_types\n-        ):\n+        if not can_convert_arrays((x, y, sample_weight)):\n             raise ValueError(\n                 \"Expected all elements of `x` to be array-like. \"\n                 f\"Received invalid types: x={x}\"\n@@ -252,6 +248,28 @@ class ArrayDataAdapter(DataAdapter):\n         return self._partial_batch_size or None\n \n \n+def can_convert_arrays(arrays):\n+    \"\"\"Check if array like-inputs can be handled by `ArrayDataAdapter`\n+\n+    Args:\n+        inputs: Structure of `Tensor`s, NumPy arrays, or tensor-like.\n+\n+    Returns:\n+        `True` if `arrays` can be handled by `ArrayDataAdapter`, `False`\n+        otherwise.\n+    \"\"\"\n+\n+    def can_convert_single_array(x):\n+        is_none = x is None\n+        known_type = isinstance(x, data_adapter_utils.ARRAY_TYPES)\n+        convertable_type = hasattr(x, \"__array__\")\n+        return is_none or known_type or convertable_type\n+\n+    return all(\n+        tf.nest.flatten(tf.nest.map_structure(can_convert_single_array, arrays))\n+    )\n+\n+\n def convert_to_arrays(arrays, dtype=None):\n     \"\"\"Process array-like inputs.\n \n@@ -262,7 +280,7 @@ def convert_to_arrays(arrays, dtype=None):\n     - Converts `list`s to `tuple`s (for `tf.data` support).\n \n     Args:\n-        inputs: Structure of `Tensor`s, `NumPy` arrays, or tensor-like.\n+        inputs: Structure of `Tensor`s, NumPy arrays, or tensor-like.\n \n     Returns:\n         Structure of NumPy `ndarray`s.\n@@ -277,15 +295,16 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.expand_dims(x.to_numpy(dtype=dtype), axis=-1)\n             elif isinstance(x, pandas.DataFrame):\n                 x = x.to_numpy(dtype=dtype)\n-        if isinstance(x, (tf.Tensor, tf.Variable)):\n-            x = x.numpy()\n         if not isinstance(x, np.ndarray):\n+            # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n+            # `torch.Tensor`, as well as any other tensor-like object that has\n+            # added numpy support.\n             if hasattr(x, \"__array__\"):\n                 x = np.array(x, dtype=dtype)\n             else:\n                 raise ValueError(\n-                    \"Expected a NumPy array, tf.Tensor, \"\n-                    \"Pandas Dataframe, or Pandas Series. \"\n+                    \"Expected a NumPy array, tf.Tensor, jax.np.ndarray, \"\n+                    \"torch.Tensor, Pandas Dataframe, or Pandas Series. \"\n                     f\"Received invalid input: {x} (of type {type(x)})\"\n                 )\n         if x.dtype == object:\n\n@@ -2,6 +2,7 @@ import jax\n import numpy as np\n import pandas\n import tensorflow as tf\n+import torch\n from absl.testing import parameterized\n \n from keras_core import backend\n@@ -10,7 +11,9 @@ from keras_core.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters([(\"np\",), (\"tf\",), (\"pandas\")])\n+    @parameterized.parameters(\n+        [(\"np\",), (\"tf\",), (\"jax\",), (\"torch\",), (\"pandas\")]\n+    )\n     def test_basic_flow(self, array_type):\n         if array_type == \"np\":\n             x = np.random.random((34, 4))\n@@ -21,6 +24,9 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n         elif array_type == \"jax\":\n             x = jax.numpy.ones((34, 4))\n             y = jax.numpy.ones((34, 2))\n+        elif array_type == \"torch\":\n+            x = torch.ones((34, 4))\n+            y = torch.ones((34, 2))\n         elif array_type == \"pandas\":\n             x = pandas.DataFrame(np.random.random((34, 4)))\n             y = pandas.DataFrame(np.random.random((34, 2)))\n\n@@ -1,6 +1,5 @@\n import math\n \n-import jax\n import numpy as np\n import tensorflow as tf\n \n@@ -12,15 +11,12 @@ except ImportError:\n     pandas = None\n \n \n-ARRAY_TYPES = (tf.Tensor, np.ndarray, jax.numpy.ndarray)\n+# Leave jax, tf, and torch arrays off this list. Instead we will use\n+# `__array__` to detect these types. Doing so allows us to avoid importing a\n+# backend framework we are not currently using just to do type-checking.\n+ARRAY_TYPES = (np.ndarray,)\n if pandas:\n-    ARRAY_TYPES = ARRAY_TYPES + (\n-        tf.Tensor,\n-        np.ndarray,\n-        pandas.Series,\n-        pandas.DataFrame,\n-    )\n-# TODO: support torch tensors?\n+    ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n \n @keras_core_export(\"keras_core.utils.unpack_x_y_sample_weight\")\n\n@@ -42,10 +42,8 @@ import types\n import warnings\n \n import tensorflow as tf\n-from tensorflow import nest\n \n from keras_core.trainers.data_adapters import array_data_adapter\n-from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.data_adapters import generator_data_adapter\n from keras_core.trainers.data_adapters import py_dataset_adapter\n from keras_core.trainers.data_adapters import tf_dataset_adapter\n@@ -69,8 +67,7 @@ class EpochIterator:\n         if steps_per_epoch:\n             self._current_iterator = None\n             self._insufficient_data = False\n-        first_element = next(iter(nest.flatten(x)), None)\n-        if isinstance(first_element, data_adapter_utils.ARRAY_TYPES):\n+        if array_data_adapter.can_convert_arrays((x, y, sample_weight)):\n             self.data_adapter = array_data_adapter.ArrayDataAdapter(\n                 x,\n                 y,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a78f714973a83fa3502962624c49a6c9f714b31a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 302/159 | Churn Δ: 52 | Churn Cumulative: 7285 | Contributors (this commit): 20 | Commits (past 90d): 7 | Contributors (cumulative): 27 | DMM Complexity: 0.9705882352941176\n\nDIFF:\n@@ -26,6 +26,7 @@ from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_utils\n from keras.saving import serialization_lib\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.saved_model import model_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n@@ -434,14 +435,15 @@ class Sequential(functional.Functional):\n \n     def get_config(self):\n         layer_configs = []\n+        serialize_obj_fn = serialization_lib.serialize_keras_object\n+        if getattr(self, \"use_legacy_config\", None):\n+            serialize_obj_fn = legacy_serialization.serialize_keras_object\n         for layer in super().layers:\n             # `super().layers` include the InputLayer if available (it is\n             # filtered out of `self.layers`). Note that\n             # `self._self_tracked_trackables` is managed by the tracking\n             # infrastructure and should not be used.\n-            layer_configs.append(\n-                serialization_lib.serialize_keras_object(layer)\n-            )\n+            layer_configs.append(serialize_obj_fn(layer))\n         config = training.Model.get_config(self)\n         config[\"name\"] = self.name\n         config[\"layers\"] = copy.deepcopy(layer_configs)\n\n@@ -81,6 +81,9 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n             \"import h5py.\"\n         )\n \n+    # Ensures that all models saved in HDF5 format follow the old serialization\n+    model.use_legacy_config = True\n+\n     # TODO(psv) Add warning when we save models that contain non-serializable\n     # entities like metrics added using `add_metric` and losses added using\n     # `add_loss.`\n\n@@ -1134,6 +1134,46 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         )\n         self.assertIsInstance(reloaded_model, new_cls)\n \n+    @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n+    def test_custom_sequential_registered_no_scope(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(keras.layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        input_shape = [1]\n+        inputs = keras.Input(shape=input_shape)\n+        custom_layer = MyDense(1)\n+        saved_model_dir = self._save_model_dir()\n+        save_format = test_utils.get_save_format()\n+\n+        model = keras.Sequential(layers=[inputs, custom_layer])\n+        model.save(saved_model_dir, save_format=save_format)\n+        loaded_model = keras.models.load_model(saved_model_dir)\n+\n+        x = tf.constant([5])\n+        self.assertAllEqual(model(x), loaded_model(x))\n+\n+    @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n+    def test_custom_functional_registered_no_scope(self):\n+        @object_registration.register_keras_serializable(package=\"my_package\")\n+        class MyDense(keras.layers.Dense):\n+            def __init__(self, units, **kwargs):\n+                super().__init__(units, **kwargs)\n+\n+        saved_model_dir = self._save_model_dir()\n+        save_format = test_utils.get_save_format()\n+        input_shape = [1]\n+        inputs = keras.Input(shape=input_shape)\n+        outputs = MyDense(1)(inputs)\n+        model = keras.Model(inputs, outputs)\n+\n+        model.save(saved_model_dir, save_format=save_format)\n+        loaded_model = keras.models.load_model(saved_model_dir)\n+\n+        x = tf.constant([5])\n+        self.assertAllEqual(model(x), loaded_model(x))\n+\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_shared_objects(self):\n         class OuterLayer(keras.layers.Layer):\n@@ -1222,7 +1262,6 @@ class TestWholeModelSaving(test_combinations.TestCase):\n         with object_registration.CustomObjectScope(\n             {\"OuterLayer\": OuterLayer, \"InnerLayer\": InnerLayer}\n         ):\n-\n             # Test saving and loading to disk\n             save_format = test_utils.get_save_format()\n             saved_model_dir = self._save_model_dir()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#964db435b24a948662ebc96e806735d56a4f1479", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 128 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 21/10 | Churn Δ: 133 | Churn Cumulative: 3459 | Contributors (this commit): 12 | Commits (past 90d): 109 | Contributors (cumulative): 26 | DMM Complexity: 0.547945205479452\n\nDIFF:\n@@ -487,7 +487,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n-            batch_outputs, np.concatenate, outputs\n+            batch_outputs, potentially_ragged_concat, outputs\n         )\n \n     def train_on_batch(\n@@ -826,3 +826,49 @@ def _is_tpu_strategy_class(clz):\n     if is_tpu_strat(clz):\n         return True\n     return any(map(_is_tpu_strategy_class, clz.__bases__))\n+\n+\n+def potentially_ragged_concat(tensors):\n+    \"\"\"Concats `Tensor`s along their first dimension.\n+\n+    Args:\n+        tensors: List of `Tensor`s.\n+\n+    Returns:\n+        Concatenation of the inputs along the first dimension -- of type\n+        `Tensor` if all input shapes are compatible, or `RaggedTensor`\n+        if not.\n+    \"\"\"\n+    if len(tensors) == 1:\n+        return tensors[0]\n+    if isinstance(tensors[0], tf.SparseTensor):\n+        return tf.sparse.concat(axis=0, sp_inputs=tensors)\n+    elif isinstance(tensors[0], tf.RaggedTensor):\n+        return tf.concat(tensors, axis=0)\n+    elif not tf.__internal__.tf2.enabled():\n+        return tf.concat(tensors, axis=0)\n+\n+    non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n+    constant_dims = tf.math.reduce_all(\n+        non_batch_shapes == non_batch_shapes[:1], axis=0\n+    )\n+    if tf.math.reduce_all(constant_dims).numpy().item():\n+        # All non-batch dims are constant\n+        if _is_scalar(tensors[0]):\n+            return tf.stack(tensors, axis=0)\n+        else:\n+            return tf.concat(tensors, axis=0)\n+\n+    # First, identify constant inner dimensions by finding the\n+    # rightmost dimension that is not constant\n+    constant_inner_dimensions = (\n+        constant_dims.numpy().tolist()[::-1].index(False)\n+    )\n+    # If there are constant inner dimensions, define a constant inner shape\n+    if constant_inner_dimensions == 0:\n+        constant_inner_shape = None\n+    else:\n+        constant_inner_shape = tensors[0].shape[-constant_inner_dimensions:]\n+    return tf.ragged.constant(\n+        [tensor.numpy() for tensor in tensors], inner_shape=constant_inner_shape\n+    ).merge_dims(0, 1)\n\n@@ -295,6 +295,10 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.expand_dims(x.to_numpy(dtype=dtype), axis=-1)\n             elif isinstance(x, pandas.DataFrame):\n                 x = x.to_numpy(dtype=dtype)\n+        if isinstance(x, (tf.Tensor, tf.Variable)):\n+            x = x.numpy()\n+        if isinstance(x, tf.RaggedTensor):\n+            return tf.cast(x, dtype=dtype)\n         if not isinstance(x, np.ndarray):\n             # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n             # `torch.Tensor`, as well as any other tensor-like object that has\n@@ -303,9 +307,10 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.array(x, dtype=dtype)\n             else:\n                 raise ValueError(\n-                    \"Expected a NumPy array, tf.Tensor, jax.np.ndarray, \"\n-                    \"torch.Tensor, Pandas Dataframe, or Pandas Series. \"\n-                    f\"Received invalid input: {x} (of type {type(x)})\"\n+                    \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n+                    \"jax.np.ndarray, torch.Tensor, Pandas Dataframe, or \"\n+                    \"Pandas Series. Received invalid input: \"\n+                    f\"{x} (of type {type(x)})\"\n                 )\n         if x.dtype == object:\n             return x\n\n@@ -14,7 +14,7 @@ except ImportError:\n # Leave jax, tf, and torch arrays off this list. Instead we will use\n # `__array__` to detect these types. Doing so allows us to avoid importing a\n # backend framework we are not currently using just to do type-checking.\n-ARRAY_TYPES = (np.ndarray,)\n+ARRAY_TYPES = (np.ndarray, tf.RaggedTensor)\n if pandas:\n     ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n\n@@ -450,3 +450,75 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         x = np.ones((16, 2))\n         y = np.zeros((16, 1))\n         model.fit(x, y, batch_size=4)\n+\n+    def get_layer(self):\n+        class ExampleLayer(keras_core.Layer):\n+            def call(self, x):\n+                return x * 2\n+\n+        return ExampleLayer\n+\n+    def get_model(self):\n+        class ExampleModel(keras_core.Model):\n+            def call(self, x):\n+                return x * 2\n+\n+        return ExampleModel\n+\n+    def get_functional(self):\n+        ExampleLayer = self.get_layer()\n+\n+        class ExampleFunctional(keras_core.Functional):\n+            def __init__(self, input_shape=(None,)):\n+                inputs = keras_core.Input(input_shape)\n+                outputs = ExampleLayer()(inputs)\n+                super().__init__(inputs=inputs, outputs=outputs)\n+\n+        return ExampleFunctional\n+\n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"model\",\n+                \"model_class\": \"get_model\",\n+            },\n+            {\n+                \"testcase_name\": \"layer\",\n+                \"model_class\": \"get_layer\",\n+            },\n+            {\n+                \"testcase_name\": \"functional\",\n+                \"model_class\": \"get_functional\",\n+            },\n+        ]\n+    )\n+    @pytest.mark.skipif(\n+        keras_core.backend.backend() != \"tensorflow\",\n+        reason=\"Only tensorflow supports raggeds\",\n+    )\n+    def test_trainer_with_raggeds(self, model_class):\n+        import tensorflow as tf\n+\n+        def loss_fn(y, y_pred, sample_weight=None):\n+            return 0\n+\n+        model = getattr(self, model_class)()()\n+        x = tf.ragged.constant([[1], [2, 3]])\n+\n+        # test forward pass\n+        y = model(x)\n+        self.assertEqual(type(y), tf.RaggedTensor)\n+\n+        # test training\n+        if model_class in [\"get_model\", \"get_functional\"]:\n+            model.compile(optimizer=\"adam\", loss=loss_fn)\n+            model.fit(x, x)\n+            y = model.predict(x)\n+            self.assertEqual(type(y), tf.RaggedTensor)\n+\n+        # test if everything works with the sequential model\n+        model = keras_core.Sequential([model])\n+        model.compile(optimizer=\"adam\", loss=loss_fn)\n+        model.fit(x, x)\n+        y = model.predict(x)\n+        self.assertEqual(type(y), tf.RaggedTensor)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#deeb4ad9ab1b2ab4bf2db825bef8b94aba4e60db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 639 | Lines Deleted: 165 | Files Changed: 12 | Hunks: 34 | Methods Changed: 28 | Complexity Δ (Sum/Max): -3/10 | Churn Δ: 804 | Churn Cumulative: 9821 | Contributors (this commit): 14 | Commits (past 90d): 201 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -92,7 +92,7 @@ def train_step(state, data):\n         trainable_variables, non_trainable_variables, x, y\n     )\n     trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        grads, trainable_variables, optimizer_variables\n+        optimizer_variables, grads, trainable_variables\n     )\n     # Return updated state\n     return loss, (\n\n@@ -0,0 +1,527 @@\n+\"\"\"\n+Title: Writing a training loop from scratch in JAX\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2023/06/25\n+Last modified: 2023/06/25\n+Description: Writing low-level training & evaluation loops in JAX.\n+Accelerator: None\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import os\n+\n+# This guide can only be run with the jax backend.\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"\n+\n+import jax\n+\n+# We import TF so we can use tf.data.\n+import tensorflow as tf\n+import keras_core as keras\n+import numpy as np\n+\n+\"\"\"\n+## Introduction\n+\n+Keras provides default training and evaluation loops, `fit()` and `evaluate()`.\n+Their usage is covered in the guide\n+[Training & evaluation with the built-in methods](https://keras.io/guides/training_with_built_in_methods/).\n+\n+If you want to customize the learning algorithm of your model while still leveraging\n+the convenience of `fit()`\n+(for instance, to train a GAN using `fit()`), you can subclass the `Model` class and\n+implement your own `train_step()` method, which\n+is called repeatedly during `fit()`.\n+\n+Now, if you want very low-level control over training & evaluation, you should write\n+your own training & evaluation loops from scratch. This is what this guide is about.\n+\"\"\"\n+\n+\"\"\"\n+## A first end-to-end example\n+\n+To write a custom training loop, we need the following ingredients:\n+\n+- A model to train, of course.\n+- An optimizer. You could either use an optimizer from `keras_core.optimizers`, or\n+one from the `optax` package.\n+- A loss function.\n+- A dataset. The standard in the JAX ecosystem is to load data via `tf.data`,\n+so that's what we'll use.\n+\n+Let's line them up.\n+\n+First, let's get the model and the MNIST dataset:\n+\"\"\"\n+\n+\n+def get_model():\n+    inputs = keras.Input(shape=(784,), name=\"digits\")\n+    x1 = keras.layers.Dense(64, activation=\"relu\")(inputs)\n+    x2 = keras.layers.Dense(64, activation=\"relu\")(x1)\n+    outputs = keras.layers.Dense(10, name=\"predictions\")(x2)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+model = get_model()\n+\n+# Prepare the training dataset.\n+batch_size = 32\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\")\n+x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\")\n+y_train = keras.utils.to_categorical(y_train)\n+y_test = keras.utils.to_categorical(y_test)\n+\n+# Reserve 10,000 samples for validation.\n+x_val = x_train[-10000:]\n+y_val = y_train[-10000:]\n+x_train = x_train[:-10000]\n+y_train = y_train[:-10000]\n+\n+# Prepare the training dataset.\n+train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n+\n+# Prepare the validation dataset.\n+val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n+val_dataset = val_dataset.batch(batch_size)\n+\n+\"\"\"\n+Next, here's the loss function and the optimizer.\n+We'll use a Keras optimizer in this case.\n+\"\"\"\n+\n+# Instantiate a loss function.\n+loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n+\n+# Instantiate an optimizer.\n+optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n+\n+\"\"\"\n+### Getting gradients in JAX \n+\n+Let's train our model using mini-batch gradient with a custom training loop.\n+\n+In JAX, gradients are computed via *metaprogramming*: you call the `jax.grad` (or\n+`jax.value_and_grad` on a function in order to create a gradient-computing function\n+for that first function.\n+\n+So the first thing we need is a function that returns the loss value.\n+That's the function we'll use to generate the gradient function. Something like this:\n+\n+```python\n+def compute_loss(x, y):\n+    ...\n+    return loss\n+```\n+\n+Once you have such a function, you can compute gradients via metaprogramming as such:\n+\n+```python\n+grad_fn = jax.grad(compute_loss)\n+grads = grad_fn(x, y)\n+```\n+\n+Typically, you don't just want to get the gradient values, you also want to get\n+the loss value. You can do this by using `jax.value_and_grad` instead of `jax.grad`:\n+\n+```python\n+grad_fn = jax.value_and_grad(compute_loss)\n+loss, grads = grad_fn(x, y)\n+```\n+\n+### JAX computation is purely stateless\n+\n+In JAX, everything must be a stateless function -- so our loss computation function\n+must be stateless as well. That means that all Keras variables (e.g. weight tensors)\n+must be passed as function inputs, and any variable that has been updated during the\n+forward pass must be returned as function output. The function have no side effect.\n+\n+During the forward pass, the non-trainable variables of a Keras model might get\n+updated. These variables could be, for instance, RNG seed state variables or\n+BatchNormalization statistics. We're going to need to return those. So we need\n+something like this:\n+\n+```python\n+def compute_loss_and_updates(trainable_variables, non_trainable_variables, x, y):\n+    ...\n+    return loss, non_trainable_variables\n+```\n+\n+Once you have such a function, you can get the gradient function by\n+specifying `hax_aux` in `value_and_grad`: it tells JAX that the loss\n+computation function returns more outputs than just the loss. Note that the loss\n+should always be the first output.\n+\n+```python\n+grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n+(loss, non_trainable_variables), grads = grad_fn(\n+    trainable_variables, non_trainable_variables, x, y\n+)\n+```\n+\n+Now that we have established the basics,\n+let's implement this `compute_loss_and_updates` function.\n+Keras models have a `stateless_call` method which will come in handy here.\n+It works just like `model.__call__`, but it requires you to explicitly\n+pass the value of all the variables in the model, and it returns not just\n+the `__call__` outputs but also the (potentially updated) non-trainable\n+variables.\n+\"\"\"\n+\n+\n+def compute_loss_and_updates(\n+    trainable_variables, non_trainable_variables, x, y\n+):\n+    y_pred, non_trainable_variables = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x\n+    )\n+    loss = loss_fn(y, y_pred)\n+    return loss, non_trainable_variables\n+\n+\n+\"\"\"\n+Let's get the gradient function:\n+\"\"\"\n+\n+grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n+\n+\"\"\"\n+### The training step function\n+\n+Next, let's implement the end-to-end training step, the function\n+that will both run the forward pass, compute the loss, compute the gradients,\n+but also use the optimizer to update the trainable variables. This function\n+also needs to be stateless, so it will get as input a `state` tuple that\n+includes every state element we're going to use:\n+\n+- `trainable_variables` and `non_trainable_variables`: the model's variables.\n+- `optimizer_variables`: the optimizer's state variables,\n+such as momentum accumulators.\n+\n+To update the trainable variables, we use the optimizer's stateless method\n+`stateless_apply`. It's equivalent to `optimizer.apply()`, but it requires\n+always passing `trainable_variables` and `optimizer_variables`. It returns\n+both the updated trainable variables and the updated optimizer_variables.\n+\"\"\"\n+\n+\n+def train_step(state, data):\n+    trainable_variables, non_trainable_variables, optimizer_variables = state\n+    x, y = data\n+    (loss, non_trainable_variables), grads = grad_fn(\n+        trainable_variables, non_trainable_variables, x, y\n+    )\n+    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n+        grads, trainable_variables, optimizer_variables\n+    )\n+    # Return updated state\n+    return loss, (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+    )\n+\n+\n+\"\"\"\n+### Make it fast with `jax.jit`\n+\n+By default, JAX operations run eagerly,\n+just like in TensorFlow eager mode and PyTorch eager mode.\n+And just like TensorFlow eager mode and PyTorch eager mode, it's pretty slow\n+-- eager mode is better used as a debugging environment, not as a way to do\n+any actual work. So let's make our `train_step` fast by compiling it.\n+\n+When you have a stateless JAX function, you can compile it to XLA via the \n+`@jax.jit` decorator. It will get traced during its first execution, and in\n+subsequent executions you will be executing the traced graph (this is just\n+like `@tf.function(jit_compile=True)`. Let's try it:\n+\"\"\"\n+\n+\n+@jax.jit\n+def train_step(state, data):\n+    trainable_variables, non_trainable_variables, optimizer_variables = state\n+    x, y = data\n+    (loss, non_trainable_variables), grads = grad_fn(\n+        trainable_variables, non_trainable_variables, x, y\n+    )\n+    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n+        optimizer_variables, grads, trainable_variables\n+    )\n+    # Return updated state\n+    return loss, (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+    )\n+\n+\n+\"\"\"\n+We're now ready to train our model. The training loop itself\n+is trivial: we just repeatedly call `loss, state = train_step(state, data)`.\n+\n+Note:\n+\n+- We convert the TF tensors yielded by the `tf.data.Dataset` to NumPy\n+before passing them to our JAX function.\n+- All variables must be built beforehand:\n+the model must be built and the optimizer must be built. Since we're using a\n+Functional API model, it's already built, but if it were a subclassed model\n+you'd need to call it on a batch of data to build it.\n+\"\"\"\n+\n+# Build optimizer variables.\n+optimizer.build(model.trainable_variables)\n+\n+trainable_variables = model.trainable_variables\n+non_trainable_variables = model.non_trainable_variables\n+optimizer_variables = optimizer.variables\n+state = trainable_variables, non_trainable_variables, optimizer_variables\n+\n+# Training loop\n+for step, data in enumerate(train_dataset):\n+    data = (data[0].numpy(), data[1].numpy())\n+    loss, state = train_step(state, data)\n+    # Log every 100 batches.\n+    if step % 100 == 0:\n+        print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")\n+        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n+\n+\"\"\"\n+A key thing to notice here is that the loop is entirely stateless -- the variables\n+attached to the model (`model.weights`) are never getting updated during the loop.\n+Their new values are only stored in the `state` tuple. That means that at some point,\n+before saving the model, you should be attaching the new variable values back to the model.\n+\n+Just call `variable.assign(new_value)` on each model variable you want to update:\n+\"\"\"\n+\n+trainable_variables, non_trainable_variables, optimizer_variables = state\n+for variable, value in zip(model.trainable_variables, trainable_variables):\n+    variable.assign(value)\n+for variable, value in zip(\n+    model.non_trainable_variables, non_trainable_variables\n+):\n+    variable.assign(value)\n+\n+\"\"\"\n+## Low-level handling of metrics\n+\n+Let's add metrics monitoring to this basic training loop.\n+\n+You can readily reuse built-in Keras metrics (or custom ones you wrote) in such training\n+loops written from scratch. Here's the flow:\n+\n+- Instantiate the metric at the start of the loop\n+- Include `metric_variables` in the `train_step` arguments\n+and `compute_loss_and_updates` arguments.\n+- Call `metric.stateless_update_state()` in the `compute_loss_and_updates` function.\n+It's equivalent to `update_state()` -- only stateless.\n+- When you need to display the current value of the metric, outside the `train_step`\n+(in the eager scope), attach the new metric variable values to the metric object\n+and vall `metric.result()`.\n+- Call `metric.reset_state()` when you need to clear the state of the metric\n+(typically at the end of an epoch)\n+\n+Let's use this knowledge to compute `CategoricalAccuracy` on training and\n+validation data at the end of training:\n+\"\"\"\n+\n+# Get a fresh model\n+model = get_model()\n+\n+# Instantiate an optimizer to train the model.\n+optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n+# Instantiate a loss function.\n+loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n+\n+# Prepare the metrics.\n+train_acc_metric = keras.metrics.CategoricalAccuracy()\n+val_acc_metric = keras.metrics.CategoricalAccuracy()\n+\n+\n+def compute_loss_and_updates(\n+    trainable_variables, non_trainable_variables, metric_variables, x, y\n+):\n+    y_pred, non_trainable_variables = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x\n+    )\n+    loss = loss_fn(y, y_pred)\n+    metric_variables = train_acc_metric.stateless_update_state(\n+        metric_variables, y, y_pred\n+    )\n+    return loss, (non_trainable_variables, metric_variables)\n+\n+\n+grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n+\n+\n+@jax.jit\n+def train_step(state, data):\n+    (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+        metric_variables,\n+    ) = state\n+    x, y = data\n+    (loss, (non_trainable_variables, metric_variables)), grads = grad_fn(\n+        trainable_variables, non_trainable_variables, metric_variables, x, y\n+    )\n+    trainable_variables, optimizer_variables = optimizer.stateless_apply(\n+        optimizer_variables, grads, trainable_variables\n+    )\n+    # Return updated state\n+    return loss, (\n+        trainable_variables,\n+        non_trainable_variables,\n+        optimizer_variables,\n+        metric_variables,\n+    )\n+\n+\n+\"\"\"\n+We'll also prepare an evaluation step function:\n+\"\"\"\n+\n+\n+@jax.jit\n+def eval_step(state, data):\n+    trainable_variables, non_trainable_variables, metric_variables = state\n+    x, y = data\n+    y_pred, non_trainable_variables = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x\n+    )\n+    loss = loss_fn(y, y_pred)\n+    metric_variables = val_acc_metric.stateless_update_state(\n+        metric_variables, y, y_pred\n+    )\n+    return loss, (\n+        trainable_variables,\n+        non_trainable_variables,\n+        metric_variables,\n+    )\n+\n+\n+\"\"\"\n+Here are our loops:\n+\"\"\"\n+\n+# Build optimizer variables.\n+optimizer.build(model.trainable_variables)\n+\n+trainable_variables = model.trainable_variables\n+non_trainable_variables = model.non_trainable_variables\n+optimizer_variables = optimizer.variables\n+metric_variables = train_acc_metric.variables\n+state = (\n+    trainable_variables,\n+    non_trainable_variables,\n+    optimizer_variables,\n+    metric_variables,\n+)\n+\n+# Training loop\n+for step, data in enumerate(train_dataset):\n+    data = (data[0].numpy(), data[1].numpy())\n+    loss, state = train_step(state, data)\n+    # Log every 100 batches.\n+    if step % 100 == 0:\n+        print(f\"Training loss (for 1 batch) at step {step}: {float(loss):.4f}\")\n+        _, _, _, metric_variables = state\n+        for variable, value in zip(\n+            train_acc_metric.variables, metric_variables\n+        ):\n+            variable.assign(value)\n+        print(f\"Training accuracy: {train_acc_metric.result()}\")\n+        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n+\n+metric_variables = val_acc_metric.variables\n+(\n+    trainable_variables,\n+    non_trainable_variables,\n+    optimizer_variables,\n+    metric_variables,\n+) = state\n+state = trainable_variables, non_trainable_variables, metric_variables\n+\n+# Eval loop\n+for step, data in enumerate(val_dataset):\n+    data = (data[0].numpy(), data[1].numpy())\n+    loss, state = eval_step(state, data)\n+    # Log every 100 batches.\n+    if step % 100 == 0:\n+        print(\n+            f\"Validation loss (for 1 batch) at step {step}: {float(loss):.4f}\"\n+        )\n+        _, _, metric_variables = state\n+        for variable, value in zip(val_acc_metric.variables, metric_variables):\n+            variable.assign(value)\n+        print(f\"Validation accuracy: {val_acc_metric.result()}\")\n+        print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n+\n+\"\"\"\n+## Low-level handling of losses tracked by the model\n+\n+Layers & models recursively track any losses created during the forward pass\n+by layers that call `self.add_loss(value)`. The resulting list of scalar loss\n+values are available via the property `model.losses`\n+at the end of the forward pass.\n+\n+If you want to be using these loss components, you should sum them\n+and add them to the main loss in your training step.\n+\n+Consider this layer, that creates an activity regularization loss:\n+\"\"\"\n+\n+\n+class ActivityRegularizationLayer(keras.layers.Layer):\n+    def call(self, inputs):\n+        self.add_loss(1e-2 * jax.numpy.sum(inputs))\n+        return inputs\n+\n+\n+\"\"\"\n+Let's build a really simple model that uses it:\n+\"\"\"\n+\n+inputs = keras.Input(shape=(784,), name=\"digits\")\n+x = keras.layers.Dense(64, activation=\"relu\")(inputs)\n+# Insert activity regularization as a layer\n+x = ActivityRegularizationLayer()(x)\n+x = keras.layers.Dense(64, activation=\"relu\")(x)\n+outputs = keras.layers.Dense(10, name=\"predictions\")(x)\n+\n+model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+\"\"\"\n+Here's what our `compute_loss_and_updates` function should look like now:\n+\n+- Pass `return_losses=True` to `model.stateless_call()`.\n+- Sum the resulting `losses` and add them to the main loss.\n+\"\"\"\n+\n+\n+def compute_loss_and_updates(\n+    trainable_variables, non_trainable_variables, metric_variables, x, y\n+):\n+    y_pred, non_trainable_variables, losses = model.stateless_call(\n+        trainable_variables, non_trainable_variables, x, return_losses=True\n+    )\n+    loss = loss_fn(y, y_pred)\n+    if losses:\n+        loss += jax.numpy.sum(losses)\n+    metric_variables = train_acc_metric.stateless_update_state(\n+        metric_variables, y, y_pred\n+    )\n+    return loss, non_trainable_variables, metric_variables\n+\n+\n+\"\"\"\n+That's it!\n+\"\"\"\n\n@@ -40,11 +40,6 @@ your own training & evaluation loops from scratch. This is what this guide is ab\n \"\"\"\n ## A first end-to-end example\n \n-Calling a model inside a `GradientTape` scope enables you to retrieve the gradients of\n-the trainable weights of the layer with respect to a loss value. Using an optimizer\n-instance, you can use these gradients to update these variables (which you can\n-retrieve using `model.trainable_weights`).\n-\n Let's consider a simple MNIST model:\n \"\"\"\n \n@@ -67,7 +62,7 @@ First, we're going to need an optimizer, a loss function, and a dataset:\n \"\"\"\n \n # Instantiate an optimizer.\n-optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n+optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n # Instantiate a loss function.\n loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n \n@@ -92,6 +87,11 @@ val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n val_dataset = val_dataset.batch(batch_size)\n \n \"\"\"\n+Calling a model inside a `GradientTape` scope enables you to retrieve the gradients of\n+the trainable weights of the layer with respect to a loss value. Using an optimizer\n+instance, you can use these gradients to update these variables (which you can\n+retrieve using `model.trainable_weights`).\n+\n Here's our training loop, step by step:\n \n - We open a `for` loop that iterates over epochs\n@@ -132,8 +132,8 @@ for epoch in range(epochs):\n         # the value of the variables to minimize the loss.\n         optimizer.apply_gradients(zip(grads, model.trainable_weights))\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n             )\n@@ -153,15 +153,15 @@ loops written from scratch. Here's the flow:\n - Call `metric.reset_state()` when you need to clear the state of the metric\n (typically at the end of an epoch)\n \n-Let's use this knowledge to compute `SparseCategoricalAccuracy` on validation data at\n-the end of each epoch:\n+Let's use this knowledge to compute `SparseCategoricalAccuracy` on training and\n+validation data at the end of each epoch:\n \"\"\"\n \n # Get a fresh model\n model = get_model()\n \n # Instantiate an optimizer to train the model.\n-optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n+optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n # Instantiate a loss function.\n loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n \n@@ -189,8 +189,8 @@ for epoch in range(epochs):\n         # Update training metric.\n         train_acc_metric.update_state(y_batch_train, logits)\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n             )\n@@ -265,8 +265,8 @@ for epoch in range(epochs):\n     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n         loss_value = train_step(x_batch_train, y_batch_train)\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n             )\n@@ -509,7 +509,7 @@ for epoch in range(epochs):\n         d_loss, g_loss, generated_images = train_step(real_images)\n \n         # Logging.\n-        if step % 200 == 0:\n+        if step % 100 == 0:\n             # Print metrics\n             print(f\"discriminator loss at step {step}: {d_loss:.2f}\")\n             print(f\"adversarial loss at step {step}: {g_loss:.2f}\")\n\n@@ -39,7 +39,7 @@ your own training & evaluation loops from scratch. This is what this guide is ab\n \"\"\"\n ## A first end-to-end example\n \n-To write a custom training loop, you need the following ingredients:\n+To write a custom training loop, we need the following ingredients:\n \n - A model to train, of course.\n - An optimizer. You could either use a `keras_core.optimizers` optimizer,\n@@ -70,8 +70,8 @@ def get_model():\n # Prepare the training dataset.\n batch_size = 32\n (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n-x_train = np.reshape(x_train, (-1, 784))\n-x_test = np.reshape(x_test, (-1, 784))\n+x_train = np.reshape(x_train, (-1, 784)).astype(\"float32\")\n+x_test = np.reshape(x_test, (-1, 784)).astype(\"float32\")\n y_train = keras.utils.to_categorical(y_train)\n y_test = keras.utils.to_categorical(y_test)\n \n@@ -145,8 +145,8 @@ for epoch in range(epochs):\n         # Optimizer variable updates\n         optimizer.step()\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n             )\n@@ -194,8 +194,8 @@ for epoch in range(epochs):\n         with torch.no_grad():\n             optimizer.apply_gradients(zip(gradients, trainable_weights))\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n             )\n@@ -215,8 +215,8 @@ loops written from scratch. Here's the flow:\n - Call `metric.reset_state()` when you need to clear the state of the metric\n (typically at the end of an epoch)\n \n-Let's use this knowledge to compute `CategoricalAccuracy` on validation data at\n-the end of each epoch:\n+Let's use this knowledge to compute `CategoricalAccuracy` on training and\n+validation data at the end of each epoch:\n \"\"\"\n \n # Get a fresh model\n@@ -258,8 +258,8 @@ for epoch in range(epochs):\n         # Update training metric.\n         train_acc_metric.update_state(targets, logits)\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n             )\n@@ -357,8 +357,8 @@ for epoch in range(epochs):\n         # Update training metric.\n         train_acc_metric.update_state(targets, logits)\n \n-        # Log every 200 batches.\n-        if step % 200 == 0:\n+        # Log every 100 batches.\n+        if step % 100 == 0:\n             print(\n                 f\"Training loss (for 1 batch) at step {step}: {loss.detach().numpy():.4f}\"\n             )\n@@ -379,3 +379,7 @@ for epoch in range(epochs):\n     val_acc = val_acc_metric.result()\n     val_acc_metric.reset_state()\n     print(f\"Validation acc: {float(val_acc):.4f}\")\n+\n+\"\"\"\n+That's it!\n+\"\"\"\n\n@@ -93,7 +93,7 @@ class JAXTrainer(base_trainer.Trainer):\n             trainable_variables,\n             optimizer_variables,\n         ) = self.optimizer.stateless_apply(\n-            grads, trainable_variables, optimizer_variables\n+            optimizer_variables, grads, trainable_variables\n         )\n \n         with backend.StatelessScope(\n\n@@ -487,7 +487,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n-            batch_outputs, potentially_ragged_concat, outputs\n+            batch_outputs, np.concatenate, outputs\n         )\n \n     def train_on_batch(\n@@ -826,49 +826,3 @@ def _is_tpu_strategy_class(clz):\n     if is_tpu_strat(clz):\n         return True\n     return any(map(_is_tpu_strategy_class, clz.__bases__))\n-\n-\n-def potentially_ragged_concat(tensors):\n-    \"\"\"Concats `Tensor`s along their first dimension.\n-\n-    Args:\n-        tensors: List of `Tensor`s.\n-\n-    Returns:\n-        Concatenation of the inputs along the first dimension -- of type\n-        `Tensor` if all input shapes are compatible, or `RaggedTensor`\n-        if not.\n-    \"\"\"\n-    if len(tensors) == 1:\n-        return tensors[0]\n-    if isinstance(tensors[0], tf.SparseTensor):\n-        return tf.sparse.concat(axis=0, sp_inputs=tensors)\n-    elif isinstance(tensors[0], tf.RaggedTensor):\n-        return tf.concat(tensors, axis=0)\n-    elif not tf.__internal__.tf2.enabled():\n-        return tf.concat(tensors, axis=0)\n-\n-    non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n-    constant_dims = tf.math.reduce_all(\n-        non_batch_shapes == non_batch_shapes[:1], axis=0\n-    )\n-    if tf.math.reduce_all(constant_dims).numpy().item():\n-        # All non-batch dims are constant\n-        if _is_scalar(tensors[0]):\n-            return tf.stack(tensors, axis=0)\n-        else:\n-            return tf.concat(tensors, axis=0)\n-\n-    # First, identify constant inner dimensions by finding the\n-    # rightmost dimension that is not constant\n-    constant_inner_dimensions = (\n-        constant_dims.numpy().tolist()[::-1].index(False)\n-    )\n-    # If there are constant inner dimensions, define a constant inner shape\n-    if constant_inner_dimensions == 0:\n-        constant_inner_shape = None\n-    else:\n-        constant_inner_shape = tensors[0].shape[-constant_inner_dimensions:]\n-    return tf.ragged.constant(\n-        [tensor.numpy() for tensor in tensors], inner_shape=constant_inner_shape\n-    ).merge_dims(0, 1)\n\n@@ -109,6 +109,31 @@ class Metric:\n         \"\"\"Accumulate statistics for the metric.\"\"\"\n         raise NotImplementedError\n \n+    def stateless_update_state(self, metric_variables, *args, **kwargs):\n+        if len(metric_variables) != len(self.variables):\n+            raise ValueError(\n+                \"Argument `metric_variables` must be a list of tensors \"\n+                f\"corresponding 1:1 to {self.__class__.__name__}().variables. \"\n+                f\"Received list with length {len(metric_variables)}, but \"\n+                f\"expected {len(self.variables)} variables.\"\n+            )\n+        # Gather variable mapping\n+        mapping = list(zip(self.variables, metric_variables))\n+\n+        # Call in stateless scope\n+        with backend.StatelessScope(state_mapping=mapping) as scope:\n+            self.update_state(*args, **kwargs)\n+\n+        # Gather updated variables\n+        metric_variables = []\n+        for v in self.variables:\n+            new_v = scope.get_current_value(v)\n+            if new_v is not None:\n+                metric_variables.append(new_v)\n+            else:\n+                metric_variables.append(v)\n+        return metric_variables\n+\n     def result(self):\n         \"\"\"Compute the current metric value.\n \n\n@@ -48,7 +48,6 @@ class MetricTest(testing.TestCase):\n         y_pred = np.random.random((num_samples, 3))\n         batch_size = 8\n         for b in range(0, num_samples // batch_size + 1):\n-            print(b * batch_size, (b + 1) * batch_size)\n             y_true_batch = y_true[b * batch_size : (b + 1) * batch_size]\n             y_pred_batch = y_pred[b * batch_size : (b + 1) * batch_size]\n             metric.update_state(y_true_batch, y_pred_batch)\n@@ -61,6 +60,48 @@ class MetricTest(testing.TestCase):\n         metric.reset_state()\n         self.assertEqual(metric.result(), 0.0)\n \n+    def test_stateless_update_state(self):\n+        metric = ExampleMetric(name=\"mse\")\n+        self.assertEqual(len(metric.variables), 2)\n+        original_variable_values = (\n+            metric.variables[0].numpy(),\n+            metric.variables[1].numpy(),\n+        )\n+\n+        num_samples = 20\n+        y_true = np.random.random((num_samples, 3))\n+        y_pred = np.random.random((num_samples, 3))\n+        batch_size = 8\n+        metric_variables = metric.variables\n+        for b in range(0, num_samples // batch_size + 1):\n+            y_true_batch = y_true[b * batch_size : (b + 1) * batch_size]\n+            y_pred_batch = y_pred[b * batch_size : (b + 1) * batch_size]\n+            metric_variables = metric.stateless_update_state(\n+                metric_variables, y_true_batch, y_pred_batch\n+            )\n+\n+        self.assertAllClose(metric.variables[0], original_variable_values[0])\n+        self.assertAllClose(metric.variables[1], original_variable_values[1])\n+        metric.variables[0].assign(metric_variables[0])\n+        metric.variables[1].assign(metric_variables[1])\n+        self.assertAllClose(metric.total, 20)\n+        result = metric.result()\n+        self.assertAllClose(\n+            result, np.sum((y_true - y_pred) ** 2) / num_samples\n+        )\n+\n+        if backend.backend() == \"jax\":\n+            # Check no side effects.\n+            import jax\n+\n+            @jax.jit\n+            def update(metric_variables, y_true_batch, y_pred_batch):\n+                metric_variables = metric.stateless_update_state(\n+                    metric_variables, y_true_batch, y_pred_batch\n+                )\n+\n+            update(metric_variables, y_true_batch, y_pred_batch)\n+\n     def test_variable_tracking(self):\n         # In list\n         metric = ExampleMetric(name=\"mse\")\n\n@@ -178,7 +178,7 @@ class BaseOptimizer:\n         grads, trainable_variables = zip(*grads_and_vars)\n         return self.apply(grads, trainable_variables)\n \n-    def apply(self, grads, variables=None):\n+    def apply(self, grads, trainable_variables=None):\n         \"\"\"\n         `grads` should be a list of gradient tensors\n         with 1:1 mapping to the list of variables the optimizer was built with.\n@@ -191,7 +191,7 @@ class BaseOptimizer:\n             # `apply_gradients` is a no-op.\n             return\n \n-        if variables is None:\n+        if trainable_variables is None:\n             if not self.built:\n                 raise ValueError(\n                     \"When passing `grads` without `variables`, the optimizer \"\n@@ -208,7 +208,7 @@ class BaseOptimizer:\n                 )\n             trainable_variables = self._trainable_variables\n         else:\n-            trainable_variables = list(variables)\n+            trainable_variables = list(trainable_variables)\n             # Optionally build optimizer.\n             if not self.built:\n                 with ops.name_scope(self.name):\n@@ -246,12 +246,12 @@ class BaseOptimizer:\n             self.update_step(grad, var, learning_rate)\n         self.iterations.assign(self.iterations + 1)\n \n-    def stateless_apply(self, grads, trainable_variables, optimizer_variables):\n+    def stateless_apply(self, optimizer_variables, grads, trainable_variables):\n         self._check_super_called()\n \n         if not self.built:\n             raise ValueError(\n-                \"To call stateless_apply_gradients, {self.__class__.__name__} \"\n+                f\"To call `stateless_apply`, {self.__class__.__name__} \"\n                 \"must be built (i.e. its variables must have been created). \"\n                 \"You can build it via `optimizer.build(trainable_variables)`.\"\n             )\n\n@@ -295,10 +295,6 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.expand_dims(x.to_numpy(dtype=dtype), axis=-1)\n             elif isinstance(x, pandas.DataFrame):\n                 x = x.to_numpy(dtype=dtype)\n-        if isinstance(x, (tf.Tensor, tf.Variable)):\n-            x = x.numpy()\n-        if isinstance(x, tf.RaggedTensor):\n-            return tf.cast(x, dtype=dtype)\n         if not isinstance(x, np.ndarray):\n             # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n             # `torch.Tensor`, as well as any other tensor-like object that has\n@@ -307,10 +303,9 @@ def convert_to_arrays(arrays, dtype=None):\n                 x = np.array(x, dtype=dtype)\n             else:\n                 raise ValueError(\n-                    \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n-                    \"jax.np.ndarray, torch.Tensor, Pandas Dataframe, or \"\n-                    \"Pandas Series. Received invalid input: \"\n-                    f\"{x} (of type {type(x)})\"\n+                    \"Expected a NumPy array, tf.Tensor, jax.np.ndarray, \"\n+                    \"torch.Tensor, Pandas Dataframe, or Pandas Series. \"\n+                    f\"Received invalid input: {x} (of type {type(x)})\"\n                 )\n         if x.dtype == object:\n             return x\n\n@@ -14,7 +14,7 @@ except ImportError:\n # Leave jax, tf, and torch arrays off this list. Instead we will use\n # `__array__` to detect these types. Doing so allows us to avoid importing a\n # backend framework we are not currently using just to do type-checking.\n-ARRAY_TYPES = (np.ndarray, tf.RaggedTensor)\n+ARRAY_TYPES = (np.ndarray,)\n if pandas:\n     ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n\n@@ -450,75 +450,3 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         x = np.ones((16, 2))\n         y = np.zeros((16, 1))\n         model.fit(x, y, batch_size=4)\n-\n-    def get_layer(self):\n-        class ExampleLayer(keras_core.Layer):\n-            def call(self, x):\n-                return x * 2\n-\n-        return ExampleLayer\n-\n-    def get_model(self):\n-        class ExampleModel(keras_core.Model):\n-            def call(self, x):\n-                return x * 2\n-\n-        return ExampleModel\n-\n-    def get_functional(self):\n-        ExampleLayer = self.get_layer()\n-\n-        class ExampleFunctional(keras_core.Functional):\n-            def __init__(self, input_shape=(None,)):\n-                inputs = keras_core.Input(input_shape)\n-                outputs = ExampleLayer()(inputs)\n-                super().__init__(inputs=inputs, outputs=outputs)\n-\n-        return ExampleFunctional\n-\n-    @parameterized.named_parameters(\n-        [\n-            {\n-                \"testcase_name\": \"model\",\n-                \"model_class\": \"get_model\",\n-            },\n-            {\n-                \"testcase_name\": \"layer\",\n-                \"model_class\": \"get_layer\",\n-            },\n-            {\n-                \"testcase_name\": \"functional\",\n-                \"model_class\": \"get_functional\",\n-            },\n-        ]\n-    )\n-    @pytest.mark.skipif(\n-        keras_core.backend.backend() != \"tensorflow\",\n-        reason=\"Only tensorflow supports raggeds\",\n-    )\n-    def test_trainer_with_raggeds(self, model_class):\n-        import tensorflow as tf\n-\n-        def loss_fn(y, y_pred, sample_weight=None):\n-            return 0\n-\n-        model = getattr(self, model_class)()()\n-        x = tf.ragged.constant([[1], [2, 3]])\n-\n-        # test forward pass\n-        y = model(x)\n-        self.assertEqual(type(y), tf.RaggedTensor)\n-\n-        # test training\n-        if model_class in [\"get_model\", \"get_functional\"]:\n-            model.compile(optimizer=\"adam\", loss=loss_fn)\n-            model.fit(x, x)\n-            y = model.predict(x)\n-            self.assertEqual(type(y), tf.RaggedTensor)\n-\n-        # test if everything works with the sequential model\n-        model = keras_core.Sequential([model])\n-        model.compile(optimizer=\"adam\", loss=loss_fn)\n-        model.fit(x, x)\n-        y = model.predict(x)\n-        self.assertEqual(type(y), tf.RaggedTensor)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
