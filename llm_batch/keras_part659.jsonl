{"custom_id": "keras#e3761db7adc8231c71be4caee323058bd6e547c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1241 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 19 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 1241 | Churn Cumulative: 1241 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,1241 @@\n+\"\"\"\n+Title: Training & evaluation with the built-in methods\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2019/03/01\n+Last modified: 2023/03/20\n+Description: Complete guide to training & evaluation with `fit()` and `evaluate()`.\n+Accelerator: GPU\n+\"\"\"\n+\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+# We import torch & TF so as to use torch Dataloaders & tf.data.Datasets.\n+import torch\n+import tensorflow as tf\n+\n+import os\n+import numpy as np\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import operations as ops\n+\n+\"\"\"\n+## Introduction\n+\n+This guide covers training, evaluation, and prediction (inference) models\n+when using built-in APIs for training & validation (such as `Model.fit()`,\n+`Model.evaluate()` and `Model.predict()`).\n+\n+If you are interested in leveraging `fit()` while specifying your\n+own training step function, see the\n+[Customizing what happens in `fit()` guide](/guides/customizing_what_happens_in_fit/).\n+\n+If you are interested in writing your own training & evaluation loops from\n+scratch, see the guide\n+[\"writing a training loop from scratch\"](/guides/writing_a_training_loop_from_scratch/).\n+\n+In general, whether you are using built-in loops or writing your own, model training &\n+evaluation works strictly in the same way across every kind of Keras model --\n+Sequential models, models built with the Functional API, and models written from\n+scratch via model subclassing.\n+\n+This guide doesn't cover distributed training, which is covered in our\n+[guide to multi-GPU & distributed training](https://keras.io/guides/distributed_training/).\n+\"\"\"\n+\n+\"\"\"\n+## API overview: a first end-to-end example\n+\n+When passing data to the built-in training loops of a model, you should either use:\n+\n+- NumPy arrays (if your data is small and fits in memory)\n+- Subclasses of `keras_core.utils.PyDataset`\n+- `tf.data.Dataset` objects\n+- PyTorch `DataLoader` instances\n+\n+In the next few paragraphs, we'll use the MNIST dataset as NumPy arrays, in\n+order to demonstrate how to use optimizers, losses, and metrics. Afterwards, we'll\n+take a close look at each of the other options.\n+\n+Let's consider the following model (here, we build in with the Functional API, but it\n+could be a Sequential model or a subclassed model as well):\n+\"\"\"\n+\n+inputs = keras.Input(shape=(784,), name=\"digits\")\n+x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n+x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n+outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n+\n+model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+\"\"\"\n+Here's what the typical end-to-end workflow looks like, consisting of:\n+\n+- Training\n+- Validation on a holdout set generated from the original training data\n+- Evaluation on the test data\n+\n+We'll use MNIST data for this example.\n+\"\"\"\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+# Preprocess the data (these are NumPy arrays)\n+x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n+x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n+\n+y_train = y_train.astype(\"float32\")\n+y_test = y_test.astype(\"float32\")\n+\n+# Reserve 10,000 samples for validation\n+x_val = x_train[-10000:]\n+y_val = y_train[-10000:]\n+x_train = x_train[:-10000]\n+y_train = y_train[:-10000]\n+\n+\"\"\"\n+We specify the training configuration (optimizer, loss, metrics):\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(),  # Optimizer\n+    # Loss function to minimize\n+    loss=keras.losses.SparseCategoricalCrossentropy(),\n+    # List of metrics to monitor\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+)\n+\n+\"\"\"\n+We call `fit()`, which will train the model by slicing the data into \"batches\" of size\n+`batch_size`, and repeatedly iterating over the entire dataset for a given number of\n+`epochs`.\n+\"\"\"\n+\n+print(\"Fit model on training data\")\n+history = model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=64,\n+    epochs=2,\n+    # We pass some validation for\n+    # monitoring validation loss and metrics\n+    # at the end of each epoch\n+    validation_data=(x_val, y_val),\n+)\n+\n+\"\"\"\n+The returned `history` object holds a record of the loss values and metric values\n+during training:\n+\"\"\"\n+\n+history.history\n+\n+\"\"\"\n+We evaluate the model on the test data via `evaluate()`:\n+\"\"\"\n+\n+# Evaluate the model on the test data using `evaluate`\n+print(\"Evaluate on test data\")\n+results = model.evaluate(x_test, y_test, batch_size=128)\n+print(\"test loss, test acc:\", results)\n+\n+# Generate predictions (probabilities -- the output of the last layer)\n+# on new data using `predict`\n+print(\"Generate predictions for 3 samples\")\n+predictions = model.predict(x_test[:3])\n+print(\"predictions shape:\", predictions.shape)\n+\n+\"\"\"\n+Now, let's review each piece of this workflow in detail.\n+\"\"\"\n+\n+\"\"\"\n+## The `compile()` method: specifying a loss, metrics, and an optimizer\n+\n+To train a model with `fit()`, you need to specify a loss function, an optimizer, and\n+optionally, some metrics to monitor.\n+\n+You pass these to the model as arguments to the `compile()` method:\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n+    loss=keras.losses.SparseCategoricalCrossentropy(),\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+)\n+\n+\"\"\"\n+The `metrics` argument should be a list -- your model can have any number of metrics.\n+\n+If your model has multiple outputs, you can specify different losses and metrics for\n+each output, and you can modulate the contribution of each output to the total loss of\n+the model. You will find more details about this in the **Passing data to multi-input,\n+multi-output models** section.\n+\n+Note that if you're satisfied with the default settings, in many cases the optimizer,\n+loss, and metrics can be specified via string identifiers as a shortcut:\n+\"\"\"\n+\n+model.compile(\n+    optimizer=\"rmsprop\",\n+    loss=\"sparse_categorical_crossentropy\",\n+    metrics=[\"sparse_categorical_accuracy\"],\n+)\n+\n+\"\"\"\n+For later reuse, let's put our model definition and compile step in functions; we will\n+call them several times across different examples in this guide.\n+\"\"\"\n+\n+\n+def get_uncompiled_model():\n+    inputs = keras.Input(shape=(784,), name=\"digits\")\n+    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n+    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n+    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+def get_compiled_model():\n+    model = get_uncompiled_model()\n+    model.compile(\n+        optimizer=\"rmsprop\",\n+        loss=\"sparse_categorical_crossentropy\",\n+        metrics=[\"sparse_categorical_accuracy\"],\n+    )\n+    return model\n+\n+\n+\"\"\"\n+### Many built-in optimizers, losses, and metrics are available\n+\n+In general, you won't have to create your own losses, metrics, or optimizers\n+from scratch, because what you need is likely to be already part of the Keras API:\n+\n+Optimizers:\n+\n+- `SGD()` (with or without momentum)\n+- `RMSprop()`\n+- `Adam()`\n+- etc.\n+\n+Losses:\n+\n+- `MeanSquaredError()`\n+- `KLDivergence()`\n+- `CosineSimilarity()`\n+- etc.\n+\n+Metrics:\n+\n+- `AUC()`\n+- `Precision()`\n+- `Recall()`\n+- etc.\n+\"\"\"\n+\n+\"\"\"\n+### Custom losses\n+\n+If you need to create a custom loss, Keras provides three ways to do so.\n+\n+The first method involves creating a function that accepts inputs `y_true` and\n+`y_pred`. The following example shows a loss function that computes the mean squared\n+error between the real data and the predictions:\n+\"\"\"\n+\n+\n+def custom_mean_squared_error(y_true, y_pred):\n+    return ops.mean(ops.square(y_true - y_pred), axis=-1)\n+\n+\n+model = get_uncompiled_model()\n+model.compile(optimizer=keras.optimizers.Adam(), loss=custom_mean_squared_error)\n+\n+# We need to one-hot encode the labels to use MSE\n+y_train_one_hot = ops.one_hot(y_train, num_classes=10)\n+model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\n+\n+\"\"\"\n+If you need a loss function that takes in parameters beside `y_true` and `y_pred`, you\n+can subclass the `keras.losses.Loss` class and implement the following two methods:\n+\n+- `__init__(self)`: accept parameters to pass during the call of your loss function\n+- `call(self, y_true, y_pred)`: use the targets (y_true) and the model predictions\n+(y_pred) to compute the model's loss\n+\n+Let's say you want to use mean squared error, but with an added term that\n+will de-incentivize  prediction values far from 0.5 (we assume that the categorical\n+targets are one-hot encoded and take values between 0 and 1). This\n+creates an incentive for the model not to be too confident, which may help\n+reduce overfitting (we won't know if it works until we try!).\n+\n+Here's how you would do it:\n+\"\"\"\n+\n+\n+class CustomMSE(keras.losses.Loss):\n+    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n+        super().__init__(name=name)\n+        self.regularization_factor = regularization_factor\n+\n+    def call(self, y_true, y_pred):\n+        mse = ops.mean(ops.square(y_true - y_pred), axis=-1)\n+        reg = ops.mean(ops.square(0.5 - y_pred), axis=-1)\n+        return mse + reg * self.regularization_factor\n+\n+\n+model = get_uncompiled_model()\n+model.compile(optimizer=keras.optimizers.Adam(), loss=CustomMSE())\n+\n+y_train_one_hot = ops.one_hot(y_train, num_classes=10)\n+model.fit(x_train, y_train_one_hot, batch_size=64, epochs=1)\n+\n+\n+\"\"\"\n+### Custom metrics\n+\n+If you need a metric that isn't part of the API, you can easily create custom metrics\n+by subclassing the `keras.metrics.Metric` class. You will need to implement 4\n+methods:\n+\n+- `__init__(self)`, in which you will create state variables for your metric.\n+- `update_state(self, y_true, y_pred, sample_weight=None)`, which uses the targets\n+y_true and the model predictions y_pred to update the state variables.\n+- `result(self)`, which uses the state variables to compute the final results.\n+- `reset_state(self)`, which reinitializes the state of the metric.\n+\n+State update and results computation are kept separate (in `update_state()` and\n+`result()`, respectively) because in some cases, the results computation might be very\n+expensive and would only be done periodically.\n+\n+Here's a simple example showing how to implement a `CategoricalTruePositives` metric\n+that counts how many samples were correctly classified as belonging to a given class:\n+\"\"\"\n+\n+\n+class CategoricalTruePositives(keras.metrics.Metric):\n+    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.true_positives = self.add_variable(\n+            shape=(), name=\"ctp\", initializer=\"zeros\"\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_pred = ops.reshape(ops.argmax(y_pred, axis=1), (-1, 1))\n+        values = ops.cast(y_true, \"int32\") == ops.cast(y_pred, \"int32\")\n+        values = ops.cast(values, \"float32\")\n+        if sample_weight is not None:\n+            sample_weight = ops.cast(sample_weight, \"float32\")\n+            values = ops.multiply(values, sample_weight)\n+        self.true_positives.assign_add(ops.sum(values))\n+\n+    def result(self):\n+        return self.true_positives\n+\n+    def reset_state(self):\n+        # The state of the metric will be reset at the start of each epoch.\n+        self.true_positives.assign(0.0)\n+\n+\n+model = get_uncompiled_model()\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n+    loss=keras.losses.SparseCategoricalCrossentropy(),\n+    metrics=[CategoricalTruePositives()],\n+)\n+model.fit(x_train, y_train, batch_size=64, epochs=3)\n+\n+\"\"\"\n+### Handling losses and metrics that don't fit the standard signature\n+\n+The overwhelming majority of losses and metrics can be computed from `y_true` and\n+`y_pred`, where `y_pred` is an output of your model -- but not all of them. For\n+instance, a regularization loss may only require the activation of a layer (there are\n+no targets in this case), and this activation may not be a model output.\n+\n+In such cases, you can call `self.add_loss(loss_value)` from inside the call method of\n+a custom layer. Losses added in this way get added to the \"main\" loss during training\n+(the one passed to `compile()`). Here's a simple example that adds activity\n+regularization (note that activity regularization is built-in in all Keras layers --\n+this layer is just for the sake of providing a concrete example):\n+\"\"\"\n+\n+\n+class ActivityRegularizationLayer(layers.Layer):\n+    def call(self, inputs):\n+        self.add_loss(ops.sum(inputs) * 0.1)\n+        return inputs  # Pass-through layer.\n+\n+\n+inputs = keras.Input(shape=(784,), name=\"digits\")\n+x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n+\n+# Insert activity regularization as a layer\n+x = ActivityRegularizationLayer()(x)\n+\n+x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n+outputs = layers.Dense(10, name=\"predictions\")(x)\n+\n+model = keras.Model(inputs=inputs, outputs=outputs)\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+)\n+\n+# The displayed loss will be much higher than before\n+# due to the regularization component.\n+model.fit(x_train, y_train, batch_size=64, epochs=1)\n+\n+\"\"\"\n+Note that when you pass losses via `add_loss()`, it becomes possible to call\n+`compile()` without a loss function, since the model already has a loss to minimize.\n+\n+Consider the following `LogisticEndpoint` layer: it takes as inputs\n+targets & logits, and it tracks a crossentropy loss via `add_loss()`.\n+\"\"\"\n+\n+\n+class LogisticEndpoint(keras.layers.Layer):\n+    def __init__(self, name=None):\n+        super().__init__(name=name)\n+        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n+\n+    def call(self, targets, logits, sample_weights=None):\n+        # Compute the training-time loss value and add it\n+        # to the layer using `self.add_loss()`.\n+        loss = self.loss_fn(targets, logits, sample_weights)\n+        self.add_loss(loss)\n+\n+        # Return the inference-time prediction tensor (for `.predict()`).\n+        return ops.softmax(logits)\n+\n+\n+\"\"\"\n+You can use it in a model with two inputs (input data & targets), compiled without a\n+`loss` argument, like this:\n+\"\"\"\n+\n+inputs = keras.Input(shape=(3,), name=\"inputs\")\n+targets = keras.Input(shape=(10,), name=\"targets\")\n+logits = keras.layers.Dense(10)(inputs)\n+predictions = LogisticEndpoint(name=\"predictions\")(targets, logits)\n+\n+model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n+model.compile(optimizer=\"adam\")  # No loss argument!\n+\n+data = {\n+    \"inputs\": np.random.random((3, 3)),\n+    \"targets\": np.random.random((3, 10)),\n+}\n+model.fit(data)\n+\n+\"\"\"\n+For more information about training multi-input models, see the section **Passing data\n+to multi-input, multi-output models**.\n+\"\"\"\n+\n+\"\"\"\n+### Automatically setting apart a validation holdout set\n+\n+In the first end-to-end example you saw, we used the `validation_data` argument to pass\n+a tuple of NumPy arrays `(x_val, y_val)` to the model for evaluating a validation loss\n+and validation metrics at the end of each epoch.\n+\n+Here's another option: the argument `validation_split` allows you to automatically\n+reserve part of your training data for validation. The argument value represents the\n+fraction of the data to be reserved for validation, so it should be set to a number\n+higher than 0 and lower than 1. For instance, `validation_split=0.2` means \"use 20% of\n+the data for validation\", and `validation_split=0.6` means \"use 60% of the data for\n+validation\".\n+\n+The way the validation is computed is by taking the last x% samples of the arrays\n+received by the `fit()` call, before any shuffling.\n+\n+Note that you can only use `validation_split` when training with NumPy data.\n+\"\"\"\n+\n+model = get_compiled_model()\n+model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1)\n+\n+\"\"\"\n+## Training & evaluation using `tf.data` Datasets\n+\n+In the past few paragraphs, you've seen how to handle losses, metrics, and optimizers,\n+and you've seen how to use the `validation_data` and `validation_split` arguments in\n+`fit()`, when your data is passed as NumPy arrays.\n+\n+Another option is to use an iterator-like, such as a `tf.data.Dataset`, a\n+PyTorch `DataLoader`, or a Keras `PyDataset`. Let's take look at the former.\n+\n+The `tf.data` API is a set of utilities in TensorFlow 2.0 for loading and preprocessing\n+data in a way that's fast and scalable. For a complete guide about creating `Datasets`,\n+see the [tf.data documentation](https://www.tensorflow.org/guide/data).\n+\n+**You can use `tf.data` to train your Keras\n+models regardless of the backend you're using --\n+whether it's JAX, PyTorch, or TensorFlow.**\n+You can pass a `Dataset` instance directly to the methods `fit()`, `evaluate()`, and\n+`predict()`:\n+\"\"\"\n+\n+model = get_compiled_model()\n+\n+# First, let's create a training Dataset instance.\n+# For the sake of our example, we'll use the same MNIST data as before.\n+train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+# Shuffle and slice the dataset.\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n+\n+# Now we get a test dataset.\n+test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+test_dataset = test_dataset.batch(64)\n+\n+# Since the dataset already takes care of batching,\n+# we don't pass a `batch_size` argument.\n+model.fit(train_dataset, epochs=3)\n+\n+# You can also evaluate or predict on a dataset.\n+print(\"Evaluate\")\n+result = model.evaluate(test_dataset)\n+dict(zip(model.metrics_names, result))\n+\n+\"\"\"\n+Note that the Dataset is reset at the end of each epoch, so it can be reused of the\n+next epoch.\n+\n+If you want to run training only on a specific number of batches from this Dataset, you\n+can pass the `steps_per_epoch` argument, which specifies how many training steps the\n+model should run using this Dataset before moving on to the next epoch.\n+\"\"\"\n+\n+model = get_compiled_model()\n+\n+# Prepare the training dataset\n+train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n+\n+# Only use the 100 batches per epoch (that's 64 * 100 samples)\n+model.fit(train_dataset, epochs=3, steps_per_epoch=100)\n+\n+\"\"\"\n+You can also pass a `Dataset` instance as the `validation_data` argument in `fit()`:\n+\"\"\"\n+\n+model = get_compiled_model()\n+\n+# Prepare the training dataset\n+train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n+\n+# Prepare the validation dataset\n+val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n+val_dataset = val_dataset.batch(64)\n+\n+model.fit(train_dataset, epochs=1, validation_data=val_dataset)\n+\n+\"\"\"\n+At the end of each epoch, the model will iterate over the validation dataset and\n+compute the validation loss and validation metrics.\n+\n+If you want to run validation only on a specific number of batches from this dataset,\n+you can pass the `validation_steps` argument, which specifies how many validation\n+steps the model should run with the validation dataset before interrupting validation\n+and moving on to the next epoch:\n+\"\"\"\n+\n+model = get_compiled_model()\n+\n+# Prepare the training dataset\n+train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n+\n+# Prepare the validation dataset\n+val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n+val_dataset = val_dataset.batch(64)\n+\n+model.fit(\n+    train_dataset,\n+    epochs=1,\n+    # Only run validation using the first 10 batches of the dataset\n+    # using the `validation_steps` argument\n+    validation_data=val_dataset,\n+    validation_steps=10,\n+)\n+\n+\"\"\"\n+Note that the validation dataset will be reset after each use (so that you will always\n+be evaluating on the same samples from epoch to epoch).\n+\n+The argument `validation_split` (generating a holdout set from the training data) is\n+not supported when training from `Dataset` objects, since this feature requires the\n+ability to index the samples of the datasets, which is not possible in general with\n+the `Dataset` API.\n+\"\"\"\n+\n+\"\"\"\n+## Training & evaluation using `PyDataset` instances\n+\n+`keras.utils.PyDataset` is a utility that you can subclass to obtain\n+a Python generator with two important properties:\n+\n+- It works well with multiprocessing.\n+- It can be shuffled (e.g. when passing `shuffle=True` in `fit()`).\n+\n+A `PyDataset` must implement two methods:\n+\n+- `__getitem__`\n+- `__len__`\n+\n+The method `__getitem__` should return a complete batch.\n+If you want to modify your dataset between epochs, you may implement `on_epoch_end`.\n+\n+Here's a quick example:\n+\"\"\"\n+\n+\n+class ExamplePyDataset(keras.utils.PyDataset):\n+    def __init__(self, x, y, batch_size, **kwargs):\n+        super().__init__(**kwargs)\n+        self.x = x\n+        self.y = y\n+        self.batch_size = batch_size\n+\n+    def __len__(self):\n+        return int(np.ceil(len(self.x) / float(self.batch_size)))\n+\n+    def __getitem__(self, idx):\n+        batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size]\n+        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n+        return batch_x, batch_y\n+\n+\n+train_py_dataset = ExamplePyDataset(x_train, y_train, batch_size=32)\n+val_py_dataset = ExamplePyDataset(x_val, y_val, batch_size=32)\n+\n+\"\"\"\n+To fit the model, pass the dataset instead as the `x` argument (no need for a `y`\n+argument since the dataset includes the targets), and pass the validation dataset\n+as the `validation_data` argument. And no need for the `batch_size` argument, since\n+the dataset is already batched!\n+\"\"\"\n+\n+model = get_compiled_model()\n+model.fit(\n+    train_py_dataset, batch_size=64, validation_data=val_py_dataset, epochs=1\n+)\n+\n+\"\"\"\n+Evaluating the model is just as easy:\n+\"\"\"\n+\n+model.evaluate(val_py_dataset)\n+\n+\"\"\"\n+Importantly, `PyDataset` objects support three common constructor arguments\n+that handle the parallel processing configuration:\n+\n+- `workers`: Number of workers to use in multithreading or\n+    multiprocessing. Typically, you'd set it to the number of\n+    cores on your CPU.\n+- `use_multiprocessing`: Whether to use Python multiprocessing for\n+    parallelism. Setting this to `True` means that your\n+    dataset will be replicated in multiple forked processes.\n+    This is necessary to gain compute-level (rather than I/O level)\n+    benefits from parallelism. However it can only be set to\n+    `True` if your dataset can be safely pickled.\n+- `max_queue_size`: Maximum number of batches to keep in the queue\n+    when iterating over the dataset in a multithreaded or\n+    multipricessed setting.\n+    You can reduce this value to reduce the CPU memory consumption of\n+    your dataset. It defaults to 10.\n+\n+By default, multiprocessing is disabled (`use_multiprocessing=False`) and only\n+one thread is used. You should make sure to only turn on `use_multiprocessing` if\n+your code is running inside a Python `if __name__ == \"__main__\":` block in order\n+to avoid issues.\n+\n+Here's a 4-thread, non-multiprocessed example:\n+\"\"\"\n+\n+train_py_dataset = ExamplePyDataset(x_train, y_train, batch_size=32, workers=4)\n+val_py_dataset = ExamplePyDataset(x_val, y_val, batch_size=32, workers=4)\n+\n+model = get_compiled_model()\n+model.fit(\n+    train_py_dataset, batch_size=64, validation_data=val_py_dataset, epochs=1\n+)\n+\n+\"\"\"\n+## Training & evaluation using PyTorch `DataLoader` objects\n+\n+All built-in training and evaluation APIs are also compatible with `torch.utils.data.Dataset` and\n+`torch.utils.data.DataLoader` objects -- regardless of whether you're using the PyTorch backend,\n+or the JAX or TensorFlow backends. Let's take a look at a simple example.\n+\n+Unlike `PyDataset` which are batch-centric, PyTorch `Dataset` objects are sample-centric:\n+the `__len__` method returns the number of samples,\n+and the `__getitem__` method returns a specific sample.\n+\"\"\"\n+\n+\n+class ExampleTorchDataset(torch.utils.data.Dataset):\n+    def __init__(self, x, y):\n+        self.x = x\n+        self.y = y\n+\n+    def __len__(self):\n+        return len(self.x)\n+\n+    def __getitem__(self, idx):\n+        return self.x[idx], self.y[idx]\n+\n+\n+train_torch_dataset = ExampleTorchDataset(x_train, y_train)\n+val_torch_dataset = ExampleTorchDataset(x_val, y_val)\n+\n+\"\"\"\n+To use a PyTorch Dataset, you need to wrap it into a `Dataloader` which takes care\n+of batching and shuffling:\n+\"\"\"\n+\n+train_dataloader = torch.utils.data.DataLoader(\n+    train_torch_dataset, batch_size=32, shuffle=True\n+)\n+val_dataloader = torch.utils.data.DataLoader(\n+    val_torch_dataset, batch_size=32, shuffle=True\n+)\n+\n+\"\"\"\n+Now you can use them in the Keras API just like any other iterator:\n+\"\"\"\n+\n+model = get_compiled_model()\n+model.fit(\n+    train_dataloader, batch_size=64, validation_data=val_dataloader, epochs=1\n+)\n+model.evaluate(val_dataloader)\n+\n+\"\"\"\n+## Using sample weighting and class weighting\n+\n+With the default settings the weight of a sample is decided by its frequency\n+in the dataset. There are two methods to weight the data, independent of\n+sample frequency:\n+\n+* Class weights\n+* Sample weights\n+\"\"\"\n+\n+\"\"\"\n+### Class weights\n+\n+This is set by passing a dictionary to the `class_weight` argument to\n+`Model.fit()`. This dictionary maps class indices to the weight that should\n+be used for samples belonging to this class.\n+\n+This can be used to balance classes without resampling, or to train a\n+model that gives more importance to a particular class.\n+\n+For instance, if class \"0\" is half as represented as class \"1\" in your data,\n+you could use `Model.fit(..., class_weight={0: 1., 1: 0.5})`.\n+\"\"\"\n+\n+\"\"\"\n+Here's a NumPy example where we use class weights or sample weights to\n+give more importance to the correct classification of class #5 (which\n+is the digit \"5\" in the MNIST dataset).\n+\"\"\"\n+\n+class_weight = {\n+    0: 1.0,\n+    1: 1.0,\n+    2: 1.0,\n+    3: 1.0,\n+    4: 1.0,\n+    # Set weight \"2\" for class \"5\",\n+    # making this class 2x more important\n+    5: 2.0,\n+    6: 1.0,\n+    7: 1.0,\n+    8: 1.0,\n+    9: 1.0,\n+}\n+\n+print(\"Fit with class weight\")\n+model = get_compiled_model()\n+model.fit(x_train, y_train, class_weight=class_weight, batch_size=64, epochs=1)\n+\n+\"\"\"\n+### Sample weights\n+\n+For fine grained control, or if you are not building a classifier,\n+you can use \"sample weights\".\n+\n+- When training from NumPy data: Pass the `sample_weight`\n+  argument to `Model.fit()`.\n+- When training from `tf.data` or any other sort of iterator:\n+  Yield `(input_batch, label_batch, sample_weight_batch)` tuples.\n+\n+A \"sample weights\" array is an array of numbers that specify how much weight\n+each sample in a batch should have in computing the total loss. It is commonly\n+used in imbalanced classification problems (the idea being to give more weight\n+to rarely-seen classes).\n+\n+When the weights used are ones and zeros, the array can be used as a *mask* for\n+the loss function (entirely discarding the contribution of certain samples to\n+the total loss).\n+\"\"\"\n+\n+sample_weight = np.ones(shape=(len(y_train),))\n+sample_weight[y_train == 5] = 2.0\n+\n+print(\"Fit with sample weight\")\n+model = get_compiled_model()\n+model.fit(\n+    x_train, y_train, sample_weight=sample_weight, batch_size=64, epochs=1\n+)\n+\n+\"\"\"\n+Here's a matching `Dataset` example:\n+\"\"\"\n+\n+sample_weight = np.ones(shape=(len(y_train),))\n+sample_weight[y_train == 5] = 2.0\n+\n+# Create a Dataset that includes sample weights\n+# (3rd element in the return tuple).\n+train_dataset = tf.data.Dataset.from_tensor_slices(\n+    (x_train, y_train, sample_weight)\n+)\n+\n+# Shuffle and slice the dataset.\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n+\n+model = get_compiled_model()\n+model.fit(train_dataset, epochs=1)\n+\n+\"\"\"\n+## Passing data to multi-input, multi-output models\n+\n+In the previous examples, we were considering a model with a single input (a tensor of\n+shape `(764,)`) and a single output (a prediction tensor of shape `(10,)`). But what\n+about models that have multiple inputs or outputs?\n+\n+Consider the following model, which has an image input of shape `(32, 32, 3)` (that's\n+`(height, width, channels)`) and a time series input of shape `(None, 10)` (that's\n+`(timesteps, features)`). Our model will have two outputs computed from the\n+combination of these inputs: a \"score\" (of shape `(1,)`) and a probability\n+distribution over five classes (of shape `(5,)`).\n+\"\"\"\n+\n+image_input = keras.Input(shape=(32, 32, 3), name=\"img_input\")\n+timeseries_input = keras.Input(shape=(None, 10), name=\"ts_input\")\n+\n+x1 = layers.Conv2D(3, 3)(image_input)\n+x1 = layers.GlobalMaxPooling2D()(x1)\n+\n+x2 = layers.Conv1D(3, 3)(timeseries_input)\n+x2 = layers.GlobalMaxPooling1D()(x2)\n+\n+x = layers.concatenate([x1, x2])\n+\n+score_output = layers.Dense(1, name=\"score_output\")(x)\n+class_output = layers.Dense(5, name=\"class_output\")(x)\n+\n+model = keras.Model(\n+    inputs=[image_input, timeseries_input], outputs=[score_output, class_output]\n+)\n+\n+\"\"\"\n+Let's plot this model, so you can clearly see what we're doing here (note that the\n+shapes shown in the plot are batch shapes, rather than per-sample shapes).\n+\"\"\"\n+\n+keras.utils.plot_model(\n+    model, \"multi_input_and_output_model.png\", show_shapes=True\n+)\n+\n+\"\"\"\n+At compilation time, we can specify different losses to different outputs, by passing\n+the loss functions as a list:\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss=[\n+        keras.losses.MeanSquaredError(),\n+        keras.losses.CategoricalCrossentropy(),\n+    ],\n+)\n+\n+\"\"\"\n+If we only passed a single loss function to the model, the same loss function would be\n+applied to every output (which is not appropriate here).\n+\n+Likewise for metrics:\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss=[\n+        keras.losses.MeanSquaredError(),\n+        keras.losses.CategoricalCrossentropy(),\n+    ],\n+    metrics=[\n+        [\n+            keras.metrics.MeanAbsolutePercentageError(),\n+            keras.metrics.MeanAbsoluteError(),\n+        ],\n+        [keras.metrics.CategoricalAccuracy()],\n+    ],\n+)\n+\n+\"\"\"\n+Since we gave names to our output layers, we could also specify per-output losses and\n+metrics via a dict:\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss={\n+        \"score_output\": keras.losses.MeanSquaredError(),\n+        \"class_output\": keras.losses.CategoricalCrossentropy(),\n+    },\n+    metrics={\n+        \"score_output\": [\n+            keras.metrics.MeanAbsolutePercentageError(),\n+            keras.metrics.MeanAbsoluteError(),\n+        ],\n+        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n+    },\n+)\n+\n+\"\"\"\n+We recommend the use of explicit names and dicts if you have more than 2 outputs.\n+\n+It's possible to give different weights to different output-specific losses (for\n+instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x\n+the importance of the class loss), using the `loss_weights` argument:\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss={\n+        \"score_output\": keras.losses.MeanSquaredError(),\n+        \"class_output\": keras.losses.CategoricalCrossentropy(),\n+    },\n+    metrics={\n+        \"score_output\": [\n+            keras.metrics.MeanAbsolutePercentageError(),\n+            keras.metrics.MeanAbsoluteError(),\n+        ],\n+        \"class_output\": [keras.metrics.CategoricalAccuracy()],\n+    },\n+    loss_weights={\"score_output\": 2.0, \"class_output\": 1.0},\n+)\n+\n+\"\"\"\n+You could also choose not to compute a loss for certain outputs, if these outputs are\n+meant for prediction but not for training:\n+\"\"\"\n+\n+# List loss version\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss=[None, keras.losses.CategoricalCrossentropy()],\n+)\n+\n+# Or dict loss version\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss={\"class_output\": keras.losses.CategoricalCrossentropy()},\n+)\n+\n+\"\"\"\n+Passing data to a multi-input or multi-output model in `fit()` works in a similar way as\n+specifying a loss function in compile: you can pass **lists of NumPy arrays** (with\n+1:1 mapping to the outputs that received a loss function) or **dicts mapping output\n+names to NumPy arrays**.\n+\"\"\"\n+\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(1e-3),\n+    loss=[\n+        keras.losses.MeanSquaredError(),\n+        keras.losses.CategoricalCrossentropy(),\n+    ],\n+)\n+\n+# Generate dummy NumPy data\n+img_data = np.random.random_sample(size=(100, 32, 32, 3))\n+ts_data = np.random.random_sample(size=(100, 20, 10))\n+score_targets = np.random.random_sample(size=(100, 1))\n+class_targets = np.random.random_sample(size=(100, 5))\n+\n+# Fit on lists\n+model.fit(\n+    [img_data, ts_data], [score_targets, class_targets], batch_size=32, epochs=1\n+)\n+\n+# Alternatively, fit on dicts\n+model.fit(\n+    {\"img_input\": img_data, \"ts_input\": ts_data},\n+    {\"score_output\": score_targets, \"class_output\": class_targets},\n+    batch_size=32,\n+    epochs=1,\n+)\n+\n+\"\"\"\n+Here's the `Dataset` use case: similarly as what we did for NumPy arrays, the `Dataset`\n+should return a tuple of dicts.\n+\"\"\"\n+\n+train_dataset = tf.data.Dataset.from_tensor_slices(\n+    (\n+        {\"img_input\": img_data, \"ts_input\": ts_data},\n+        {\"score_output\": score_targets, \"class_output\": class_targets},\n+    )\n+)\n+train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n+\n+model.fit(train_dataset, epochs=1)\n+\n+\"\"\"\n+## Using callbacks\n+\n+Callbacks in Keras are objects that are called at different points during training (at\n+the start of an epoch, at the end of a batch, at the end of an epoch, etc.). They\n+can be used to implement certain behaviors, such as:\n+\n+- Doing validation at different points during training (beyond the built-in per-epoch\n+validation)\n+- Checkpointing the model at regular intervals or when it exceeds a certain accuracy\n+threshold\n+- Changing the learning rate of the model when training seems to be plateauing\n+- Doing fine-tuning of the top layers when training seems to be plateauing\n+- Sending email or instant message notifications when training ends or where a certain\n+performance threshold is exceeded\n+- Etc.\n+\n+Callbacks can be passed as a list to your call to `fit()`:\n+\"\"\"\n+\n+model = get_compiled_model()\n+\n+callbacks = [\n+    keras.callbacks.EarlyStopping(\n+        # Stop training when `val_loss` is no longer improving\n+        monitor=\"val_loss\",\n+        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n+        min_delta=1e-2,\n+        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n+        patience=2,\n+        verbose=1,\n+    )\n+]\n+model.fit(\n+    x_train,\n+    y_train,\n+    epochs=20,\n+    batch_size=64,\n+    callbacks=callbacks,\n+    validation_split=0.2,\n+)\n+\n+\"\"\"\n+### Many built-in callbacks are available\n+\n+There are many built-in callbacks already available in Keras, such as:\n+\n+- `ModelCheckpoint`: Periodically save the model.\n+- `EarlyStopping`: Stop training when training is no longer improving the validation\n+metrics.\n+- `TensorBoard`: periodically write model logs that can be visualized in\n+[TensorBoard](https://www.tensorflow.org/tensorboard) (more details in the section\n+\"Visualization\").\n+- `CSVLogger`: streams loss and metrics data to a CSV file.\n+- etc.\n+\n+See the [callbacks documentation](/api/callbacks/) for the complete list.\n+\n+### Writing your own callback\n+\n+You can create a custom callback by extending the base class\n+`keras.callbacks.Callback`. A callback has access to its associated model through the\n+class property `self.model`.\n+\n+Make sure to read the\n+[complete guide to writing custom callbacks](/guides/writing_your_own_callbacks/).\n+\n+Here's a simple example saving a list of per-batch loss values during training:\n+\"\"\"\n+\n+\n+class LossHistory(keras.callbacks.Callback):\n+    def on_train_begin(self, logs):\n+        self.per_batch_losses = []\n+\n+    def on_batch_end(self, batch, logs):\n+        self.per_batch_losses.append(logs.get(\"loss\"))\n+\n+\n+\"\"\"\n+## Checkpointing models\n+\n+When you're training model on relatively large datasets, it's crucial to save\n+checkpoints of your model at frequent intervals.\n+\n+The easiest way to achieve this is with the `ModelCheckpoint` callback:\n+\"\"\"\n+\n+model = get_compiled_model()\n+\n+callbacks = [\n+    keras.callbacks.ModelCheckpoint(\n+        # Path where to save the model\n+        # The two parameters below mean that we will overwrite\n+        # the current checkpoint if and only if\n+        # the `val_loss` score has improved.\n+        # The saved model name will include the current epoch.\n+        filepath=\"mymodel_{epoch}.keras\",\n+        save_best_only=True,  # Only save a model if `val_loss` has improved.\n+        monitor=\"val_loss\",\n+        verbose=1,\n+    )\n+]\n+model.fit(\n+    x_train,\n+    y_train,\n+    epochs=2,\n+    batch_size=64,\n+    callbacks=callbacks,\n+    validation_split=0.2,\n+)\n+\n+\"\"\"\n+The `ModelCheckpoint` callback can be used to implement fault-tolerance:\n+the ability to restart training from the last saved state of the model in case training\n+gets randomly interrupted. Here's a basic example:\n+\"\"\"\n+\n+# Prepare a directory to store all the checkpoints.\n+checkpoint_dir = \"./ckpt\"\n+if not os.path.exists(checkpoint_dir):\n+    os.makedirs(checkpoint_dir)\n+\n+\n+def make_or_restore_model():\n+    # Either restore the latest model, or create a fresh one\n+    # if there is no checkpoint available.\n+    checkpoints = [\n+        checkpoint_dir + \"/\" + name for name in os.listdir(checkpoint_dir)\n+    ]\n+    if checkpoints:\n+        latest_checkpoint = max(checkpoints, key=os.path.getctime)\n+        print(\"Restoring from\", latest_checkpoint)\n+        return keras.models.load_model(latest_checkpoint)\n+    print(\"Creating a new model\")\n+    return get_compiled_model()\n+\n+\n+model = make_or_restore_model()\n+callbacks = [\n+    # This callback saves the model every 100 batches.\n+    # We include the training loss in the saved model name.\n+    keras.callbacks.ModelCheckpoint(\n+        filepath=checkpoint_dir + \"/model-loss={loss:.2f}.keras\", save_freq=100\n+    )\n+]\n+model.fit(x_train, y_train, epochs=1, callbacks=callbacks)\n+\n+\"\"\"\n+You call also write your own callback for saving and restoring models.\n+\n+For a complete guide on serialization and saving, see the\n+[guide to saving and serializing Models](/guides/serialization_and_saving/).\n+\"\"\"\n+\n+\"\"\"\n+## Using learning rate schedules\n+\n+A common pattern when training deep learning models is to gradually reduce the learning\n+as training progresses. This is generally known as \"learning rate decay\".\n+\n+The learning decay schedule could be static (fixed in advance, as a function of the\n+current epoch or the current batch index), or dynamic (responding to the current\n+behavior of the model, in particular the validation loss).\n+\n+### Passing a schedule to an optimizer\n+\n+You can easily use a static learning rate decay schedule by passing a schedule object\n+as the `learning_rate` argument in your optimizer:\n+\"\"\"\n+\n+initial_learning_rate = 0.1\n+lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n+    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n+)\n+\n+optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n+\n+\"\"\"\n+Several built-in schedules are available: `ExponentialDecay`, `PiecewiseConstantDecay`,\n+`PolynomialDecay`, and `InverseTimeDecay`.\n+\n+### Using callbacks to implement a dynamic learning rate schedule\n+\n+A dynamic learning rate schedule (for instance, decreasing the learning rate when the\n+validation loss is no longer improving) cannot be achieved with these schedule objects,\n+since the optimizer does not have access to validation metrics.\n+\n+However, callbacks do have access to all metrics, including validation metrics! You can\n+thus achieve this pattern by using a callback that modifies the current learning rate\n+on the optimizer. In fact, this is even built-in as the `ReduceLROnPlateau` callback.\n+\"\"\"\n+\n+\"\"\"\n+## Visualizing loss and metrics during training with TensorBoard\n+\n+The best way to keep an eye on your model during training is to use\n+[TensorBoard](https://www.tensorflow.org/tensorboard) -- a browser-based application\n+that you can run locally that provides you with:\n+\n+- Live plots of the loss and metrics for training and evaluation\n+- (optionally) Visualizations of the histograms of your layer activations\n+- (optionally) 3D visualizations of the embedding spaces learned by your `Embedding`\n+layers\n+\n+If you have installed TensorFlow with pip, you should be able to launch TensorBoard\n+from the command line:\n+\n+```\n+tensorboard --logdir=/full_path_to_your_logs\n+```\n+\"\"\"\n+\n+\"\"\"\n+### Using the TensorBoard callback\n+\n+The easiest way to use TensorBoard with a Keras model and the `fit()` method is the\n+`TensorBoard` callback.\n+\n+In the simplest case, just specify where you want the callback to write logs, and\n+you're good to go:\n+\"\"\"\n+\n+keras.callbacks.TensorBoard(\n+    log_dir=\"/full_path_to_your_logs\",\n+    histogram_freq=0,  # How often to log histogram visualizations\n+    embeddings_freq=0,  # How often to log embedding visualizations\n+    update_freq=\"epoch\",\n+)  # How often to write logs (default: once per epoch)\n+\n+\"\"\"\n+For more information, see the\n+[documentation for the `TensorBoard` callback](https://keras.io/api/callbacks/tensorboard/).\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0716889b9e8043b67b10be63631b9d0227a585db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 162 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@\n To run the benchmark, make sure you are in model_benchmark/ directory, and run\n the command below:\n \n-python3 -m model_benchmark.resnet_image_classification_benchmark \\\n+python3 -m model_benchmark.efficient_net_image_classification_benchmark \\\n     --epochs=2 \\\n     --batch_size=32\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f9bf138bab1bb40a35b4a878ad301e1c6960dc2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2659 | Contributors (this commit): 6 | Commits (past 90d): 29 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -661,7 +661,7 @@ def repeat(x, repeats, axis=None):\n \n         x = KerasTensor(x.shape, standardize_dtype(x.dtype))\n         outputs = repeat(x, repeats, axis=axis)\n-        print(outputs)\n+\n         return torch.empty(\n             size=outputs.shape,\n             dtype=to_torch_dtype(outputs.dtype),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4400b55fd81df29a91922b94ae8b5d6c1a9f4cb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 825 | Lines Deleted: 3 | Files Changed: 5 | Hunks: 5 | Methods Changed: 25 | Complexity Δ (Sum/Max): 47/33 | Churn Δ: 828 | Churn Cumulative: 4890 | Contributors (this commit): 6 | Commits (past 90d): 38 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@\n To run the benchmark, make sure you are in model_benchmark/ directory, and run\n the command below:\n \n-python3 -m model_benchmark.efficient_net_image_classification_benchmark \\\n+python3 -m model_benchmark.resnet_image_classification_benchmark \\\n     --epochs=2 \\\n     --batch_size=32\n \"\"\"\n\n@@ -7,7 +7,6 @@ Description: Complete guide to training & evaluation with `fit()` and `evaluate(\n Accelerator: GPU\n \"\"\"\n \n-\n \"\"\"\n ## Setup\n \"\"\"\n\n@@ -0,0 +1,380 @@\n+\"\"\"\n+Title: Understanding masking & padding\n+Authors: Scott Zhu, Francois Chollet\n+Date created: 2019/07/16\n+Last modified: 2020/04/14\n+Description: Complete guide to using mask-aware sequence layers in Keras.\n+Accelerator: None\n+\"\"\"\n+\"\"\"\n+## Setup\n+\"\"\"\n+import numpy as np\n+import keras_core as keras\n+from keras_core import operations as ops\n+from keras_core import layers\n+\n+\"\"\"\n+## Introduction\n+\n+**Masking** is a way to tell sequence-processing layers that certain timesteps\n+in an input are missing, and thus should be skipped when processing the data.\n+\n+**Padding** is a special form of masking where the masked steps are at the start or\n+the end of a sequence. Padding comes from the need to encode sequence data into\n+contiguous batches: in order to make all sequences in a batch fit a given standard\n+length, it is necessary to pad or truncate some sequences.\n+\n+Let's take a close look.\n+\"\"\"\n+\n+\"\"\"\n+## Padding sequence data\n+\n+When processing sequence data, it is very common for individual samples to have\n+different lengths. Consider the following example (text tokenized as words):\n+\n+```\n+[\n+  [\"Hello\", \"world\", \"!\"],\n+  [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n+  [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n+]\n+```\n+\n+After vocabulary lookup, the data might be vectorized as integers, e.g.:\n+\n+```\n+[\n+  [71, 1331, 4231]\n+  [73, 8, 3215, 55, 927],\n+  [83, 91, 1, 645, 1253, 927],\n+]\n+```\n+\n+The data is a nested list where individual samples have length 3, 5, and 6,\n+respectively. Since the input data for a deep learning model must be a single tensor\n+(of shape e.g. `(batch_size, 6, vocab_size)` in this case), samples that are shorter\n+than the longest item need to be padded with some placeholder value (alternatively,\n+one might also truncate long samples before padding short samples).\n+\n+Keras provides a utility function to truncate and pad Python lists to a common length:\n+`keras.utils.pad_sequences`.\n+\"\"\"\n+\n+raw_inputs = [\n+    [711, 632, 71],\n+    [73, 8, 3215, 55, 927],\n+    [83, 91, 1, 645, 1253, 927],\n+]\n+\n+# By default, this will pad using 0s; it is configurable via the\n+# \"value\" parameter.\n+# Note that you could use \"pre\" padding (at the beginning) or\n+# \"post\" padding (at the end).\n+# We recommend using \"post\" padding when working with RNN layers\n+# (in order to be able to use the\n+# CuDNN implementation of the layers).\n+padded_inputs = keras.utils.pad_sequences(raw_inputs, padding=\"post\")\n+print(padded_inputs)\n+\n+\n+\"\"\"\n+## Masking\n+\n+Now that all samples have a uniform length, the model must be informed that some part\n+of the data is actually padding and should be ignored. That mechanism is **masking**.\n+\n+There are three ways to introduce input masks in Keras models:\n+\n+- Add a `keras.layers.Masking` layer.\n+- Configure a `keras.layers.Embedding` layer with `mask_zero=True`.\n+- Pass a `mask` argument manually when calling layers that support this argument (e.g.\n+RNN layers).\n+\"\"\"\n+\n+\"\"\"\n+## Mask-generating layers: `Embedding` and `Masking`\n+\n+Under the hood, these layers will create a mask tensor (2D tensor with shape `(batch,\n+sequence_length)`), and attach it to the tensor output returned by the `Masking` or\n+`Embedding` layer.\n+\"\"\"\n+\n+embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n+masked_output = embedding(padded_inputs)\n+\n+print(masked_output._keras_mask)\n+\n+masking_layer = layers.Masking()\n+# Simulate the embedding lookup by expanding the 2D input to 3D,\n+# with embedding dimension of 10.\n+unmasked_embedding = ops.cast(\n+    ops.tile(ops.expand_dims(padded_inputs, axis=-1), [1, 1, 10]),\n+    dtype=\"float32\",\n+)\n+\n+masked_embedding = masking_layer(unmasked_embedding)\n+print(masked_embedding._keras_mask)\n+\n+\"\"\"\n+As you can see from the printed result, the mask is a 2D boolean tensor with shape\n+`(batch_size, sequence_length)`, where each individual `False` entry indicates that\n+the corresponding timestep should be ignored during processing.\n+\"\"\"\n+\n+\"\"\"\n+## Mask propagation in the Functional API and Sequential API\n+\n+When using the Functional API or the Sequential API, a mask generated by an `Embedding`\n+or `Masking` layer will be propagated through the network for any layer that is\n+capable of using them (for example, RNN layers). Keras will automatically fetch the\n+mask corresponding to an input and pass it to any layer that knows how to use it.\n+\n+For instance, in the following Sequential model, the `LSTM` layer will automatically\n+receive a mask, which means it will ignore padded values:\n+\"\"\"\n+\n+model = keras.Sequential(\n+    [\n+        layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True),\n+        layers.LSTM(32),\n+    ]\n+)\n+\n+\"\"\"\n+This is also the case for the following Functional API model:\n+\"\"\"\n+\n+inputs = keras.Input(shape=(None,), dtype=\"int32\")\n+x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n+outputs = layers.LSTM(32)(x)\n+\n+model = keras.Model(inputs, outputs)\n+\n+\"\"\"\n+## Passing mask tensors directly to layers\n+\"\"\"\n+\n+\"\"\"\n+Layers that can handle masks (such as the `LSTM` layer) have a `mask` argument in their\n+`__call__` method.\n+\n+Meanwhile, layers that produce a mask (e.g. `Embedding`) expose a `compute_mask(input,\n+previous_mask)` method which you can call.\n+\n+Thus, you can pass the output of the `compute_mask()` method of a mask-producing layer\n+to the `__call__` method of a mask-consuming layer, like this:\n+\n+\"\"\"\n+\n+\n+class MyLayer(layers.Layer):\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.embedding = layers.Embedding(\n+            input_dim=5000, output_dim=16, mask_zero=True\n+        )\n+        self.lstm = layers.LSTM(32)\n+\n+    def call(self, inputs):\n+        x = self.embedding(inputs)\n+        # Note that you could also prepare a `mask` tensor manually.\n+        # It only needs to be a boolean tensor\n+        # with the right shape, i.e. (batch_size, timesteps).\n+        mask = self.embedding.compute_mask(inputs)\n+        output = self.lstm(\n+            x, mask=mask\n+        )  # The layer will ignore the masked values\n+        return output\n+\n+\n+layer = MyLayer()\n+x = np.random.random((32, 10)) * 100\n+x = x.astype(\"int32\")\n+layer(x)\n+\n+\"\"\"\n+## Supporting masking in your custom layers\n+\"\"\"\n+\n+\"\"\"\n+Sometimes, you may need to write layers that generate a mask (like `Embedding`), or\n+layers that need to modify the current mask.\n+\n+For instance, any layer that produces a tensor with a different time dimension than its\n+input, such as a `Concatenate` layer that concatenates on the time dimension, will\n+need to modify the current mask so that downstream layers will be able to properly\n+take masked timesteps into account.\n+\n+To do this, your layer should implement the `layer.compute_mask()` method, which\n+produces a new mask given the input and the current mask.\n+\n+Here is an example of a `TemporalSplit` layer that needs to modify the current mask.\n+\"\"\"\n+\n+\n+class TemporalSplit(keras.layers.Layer):\n+    \"\"\"Split the input tensor into 2 tensors along the time dimension.\"\"\"\n+\n+    def call(self, inputs):\n+        # Expect the input to be 3D and mask to be 2D, split the input tensor into 2\n+        # subtensors along the time axis (axis 1).\n+        return ops.split(inputs, 2, axis=1)\n+\n+    def compute_mask(self, inputs, mask=None):\n+        # Also split the mask into 2 if it presents.\n+        if mask is None:\n+            return None\n+        return ops.split(mask, 2, axis=1)\n+\n+\n+first_half, second_half = TemporalSplit()(masked_embedding)\n+print(first_half._keras_mask)\n+print(second_half._keras_mask)\n+\n+\"\"\"\n+Here is another example of a `CustomEmbedding` layer that is capable of generating a\n+mask from input values:\n+\"\"\"\n+\n+\n+class CustomEmbedding(keras.layers.Layer):\n+    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n+        super().__init__(**kwargs)\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+        self.mask_zero = mask_zero\n+\n+    def build(self, input_shape):\n+        self.embeddings = self.add_weight(\n+            shape=(self.input_dim, self.output_dim),\n+            initializer=\"random_normal\",\n+            dtype=\"float32\",\n+        )\n+\n+    def call(self, inputs):\n+        inputs = ops.cast(inputs, \"int32\")\n+        return ops.take(self.embeddings, inputs)\n+\n+    def compute_mask(self, inputs, mask=None):\n+        if not self.mask_zero:\n+            return None\n+        return ops.not_equal(inputs, 0)\n+\n+\n+layer = CustomEmbedding(10, 32, mask_zero=True)\n+x = np.random.random((3, 10)) * 9\n+x = x.astype(\"int32\")\n+\n+y = layer(x)\n+mask = layer.compute_mask(x)\n+\n+print(mask)\n+\n+\"\"\"\n+Note: For more details about format limitations related to masking, see the \n+[serialization guide](/guides/serialization_and_saving).\n+\"\"\"\n+\n+\"\"\"\n+## Opting-in to mask propagation on compatible layers\n+\n+Most layers don't modify the time dimension, so don't need to modify the current mask.\n+However, they may still want to be able to **propagate** the current mask, unchanged,\n+to the next layer. **This is an opt-in behavior.** By default, a custom layer will\n+destroy the current mask (since the framework has no way to tell whether propagating\n+the mask is safe to do).\n+\n+If you have a custom layer that does not modify the time dimension, and if you want it\n+to be able to propagate the current input mask, you should set `self.supports_masking\n+= True` in the layer constructor. In this case, the default behavior of\n+`compute_mask()` is to just pass the current mask through.\n+\n+Here's an example of a layer that is whitelisted for mask propagation:\n+\n+\"\"\"\n+\n+\n+class MyActivation(keras.layers.Layer):\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        # Signal that the layer is safe for mask propagation\n+        self.supports_masking = True\n+\n+    def call(self, inputs):\n+        return ops.relu(inputs)\n+\n+\n+\"\"\"\n+You can now use this custom layer in-between a mask-generating layer (like `Embedding`)\n+and a mask-consuming layer (like `LSTM`), and it will pass the mask along so that it\n+reaches the mask-consuming layer.\n+\"\"\"\n+\n+inputs = keras.Input(shape=(None,), dtype=\"int32\")\n+x = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n+x = MyActivation()(x)  # Will pass the mask along\n+print(\"Mask found:\", x._keras_mask)\n+outputs = layers.LSTM(32)(x)  # Will receive the mask\n+\n+model = keras.Model(inputs, outputs)\n+y = model(np.random.randint(0, 5000, size=(32, 100)))\n+\n+\"\"\"\n+## Writing layers that need mask information\n+\n+Some layers are mask *consumers*: they accept a `mask` argument in `call` and use it to\n+determine whether to skip certain time steps.\n+\n+To write such a layer, you can simply add a `mask=None` argument in your `call`\n+signature. The mask associated with the inputs will be passed to your layer whenever\n+it is available.\n+\n+Here's a simple example below: a layer that computes a softmax over the time dimension\n+(axis 1) of an input sequence, while discarding masked timesteps.\n+\"\"\"\n+\n+\n+class TemporalSoftmax(keras.layers.Layer):\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+\n+    def call(self, inputs, mask=None):\n+        assert mask is not None\n+        broadcast_float_mask = ops.expand_dims(ops.cast(mask, \"float32\"), -1)\n+        inputs_exp = ops.exp(inputs) * broadcast_float_mask\n+        inputs_sum = ops.sum(\n+            inputs_exp * broadcast_float_mask, axis=-1, keepdims=True\n+        )\n+        return inputs_exp / inputs_sum\n+\n+\n+inputs = keras.Input(shape=(None,), dtype=\"int32\")\n+x = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n+x = layers.Dense(1)(x)\n+outputs = TemporalSoftmax()(x)\n+\n+model = keras.Model(inputs, outputs)\n+y = model(np.random.randint(0, 10, size=(32, 100)))\n+\n+\"\"\"\n+## Summary\n+\n+That is all you need to know about padding & masking in Keras. To recap:\n+\n+- \"Masking\" is how layers are able to know when to skip / ignore certain timesteps in\n+sequence inputs.\n+- Some layers are mask-generators: `Embedding` can generate a mask from input values\n+(if `mask_zero=True`), and so can the `Masking` layer.\n+- Some layers are mask-consumers: they expose a `mask` argument in their `__call__`\n+method. This is the case for RNN layers.\n+- In the Functional API and Sequential API, mask information is propagated\n+automatically.\n+- When using layers in a standalone way, you can pass the `mask` arguments to layers\n+manually.\n+- You can easily write layers that modify the current mask, that generate a new mask,\n+or that consume the mask associated with the inputs.\n+\n+\"\"\"\n\n@@ -0,0 +1,443 @@\n+\"\"\"\n+Title: Writing your own callbacks\n+Authors: Rick Chao, Francois Chollet\n+Date created: 2019/03/20\n+Last modified: 2020/07/12\n+Description: Complete guide to writing new Keras callbacks.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+A callback is a powerful tool to customize the behavior of a Keras model during\n+training, evaluation, or inference. Examples include `keras.callbacks.TensorBoard`\n+to visualize training progress and results with TensorBoard, or\n+`keras.callbacks.ModelCheckpoint` to periodically save your model during training.\n+\n+In this guide, you will learn what a Keras callback is, what it can do, and how you can\n+build your own. We provide a few demos of simple callback applications to get you\n+started.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import keras_core as keras\n+\n+\"\"\"\n+## Keras callbacks overview\n+\n+All callbacks subclass the `keras.callbacks.Callback` class, and\n+override a set of methods called at various stages of training, testing, and\n+predicting. Callbacks are useful to get a view on internal states and statistics of\n+the model during training.\n+\n+You can pass a list of callbacks (as the keyword argument `callbacks`) to the following\n+model methods:\n+\n+- `keras.Model.fit()`\n+- `keras.Model.evaluate()`\n+- `keras.Model.predict()`\n+\"\"\"\n+\n+\"\"\"\n+## An overview of callback methods\n+\n+### Global methods\n+\n+#### `on_(train|test|predict)_begin(self, logs=None)`\n+\n+Called at the beginning of `fit`/`evaluate`/`predict`.\n+\n+#### `on_(train|test|predict)_end(self, logs=None)`\n+\n+Called at the end of `fit`/`evaluate`/`predict`.\n+\n+### Batch-level methods for training/testing/predicting\n+\n+#### `on_(train|test|predict)_batch_begin(self, batch, logs=None)`\n+\n+Called right before processing a batch during training/testing/predicting.\n+\n+#### `on_(train|test|predict)_batch_end(self, batch, logs=None)`\n+\n+Called at the end of training/testing/predicting a batch. Within this method, `logs` is\n+a dict containing the metrics results.\n+\n+### Epoch-level methods (training only)\n+\n+#### `on_epoch_begin(self, epoch, logs=None)`\n+\n+Called at the beginning of an epoch during training.\n+\n+#### `on_epoch_end(self, epoch, logs=None)`\n+\n+Called at the end of an epoch during training.\n+\"\"\"\n+\n+\"\"\"\n+## A basic example\n+\n+Let's take a look at a concrete example. To get started, let's import tensorflow and\n+define a simple Sequential Keras model:\n+\"\"\"\n+\n+\n+# Define the Keras model to add callbacks to\n+def get_model():\n+    model = keras.Sequential()\n+    model.add(keras.layers.Dense(1))\n+    model.compile(\n+        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n+        loss=\"mean_squared_error\",\n+        metrics=[\"mean_absolute_error\"],\n+    )\n+    return model\n+\n+\n+\"\"\"\n+Then, load the MNIST data for training and testing from Keras datasets API:\n+\"\"\"\n+\n+# Load example MNIST data and pre-process it\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n+x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n+\n+# Limit the data to 1000 samples\n+x_train = x_train[:1000]\n+y_train = y_train[:1000]\n+x_test = x_test[:1000]\n+y_test = y_test[:1000]\n+\n+\"\"\"\n+Now, define a simple custom callback that logs:\n+\n+- When `fit`/`evaluate`/`predict` starts & ends\n+- When each epoch starts & ends\n+- When each training batch starts & ends\n+- When each evaluation (test) batch starts & ends\n+- When each inference (prediction) batch starts & ends\n+\"\"\"\n+\n+\n+class CustomCallback(keras.callbacks.Callback):\n+    def on_train_begin(self, logs=None):\n+        keys = list(logs.keys())\n+        print(\"Starting training; got log keys: {}\".format(keys))\n+\n+    def on_train_end(self, logs=None):\n+        keys = list(logs.keys())\n+        print(\"Stop training; got log keys: {}\".format(keys))\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"Start epoch {} of training; got log keys: {}\".format(epoch, keys)\n+        )\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        keys = list(logs.keys())\n+        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n+\n+    def on_test_begin(self, logs=None):\n+        keys = list(logs.keys())\n+        print(\"Start testing; got log keys: {}\".format(keys))\n+\n+    def on_test_end(self, logs=None):\n+        keys = list(logs.keys())\n+        print(\"Stop testing; got log keys: {}\".format(keys))\n+\n+    def on_predict_begin(self, logs=None):\n+        keys = list(logs.keys())\n+        print(\"Start predicting; got log keys: {}\".format(keys))\n+\n+    def on_predict_end(self, logs=None):\n+        keys = list(logs.keys())\n+        print(\"Stop predicting; got log keys: {}\".format(keys))\n+\n+    def on_train_batch_begin(self, batch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"...Training: start of batch {}; got log keys: {}\".format(\n+                batch, keys\n+            )\n+        )\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"...Training: end of batch {}; got log keys: {}\".format(batch, keys)\n+        )\n+\n+    def on_test_batch_begin(self, batch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"...Evaluating: start of batch {}; got log keys: {}\".format(\n+                batch, keys\n+            )\n+        )\n+\n+    def on_test_batch_end(self, batch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"...Evaluating: end of batch {}; got log keys: {}\".format(\n+                batch, keys\n+            )\n+        )\n+\n+    def on_predict_batch_begin(self, batch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"...Predicting: start of batch {}; got log keys: {}\".format(\n+                batch, keys\n+            )\n+        )\n+\n+    def on_predict_batch_end(self, batch, logs=None):\n+        keys = list(logs.keys())\n+        print(\n+            \"...Predicting: end of batch {}; got log keys: {}\".format(\n+                batch, keys\n+            )\n+        )\n+\n+\n+\"\"\"\n+Let's try it out:\n+\"\"\"\n+\n+model = get_model()\n+model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=128,\n+    epochs=1,\n+    verbose=0,\n+    validation_split=0.5,\n+    callbacks=[CustomCallback()],\n+)\n+\n+res = model.evaluate(\n+    x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]\n+)\n+\n+res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])\n+\n+\"\"\"\n+### Usage of `logs` dict\n+\n+The `logs` dict contains the loss value, and all the metrics at the end of a batch or\n+epoch. Example includes the loss and mean absolute error.\n+\"\"\"\n+\n+\n+class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n+    def on_train_batch_end(self, batch, logs=None):\n+        print(\n+            \"Up to batch {}, the average loss is {:7.2f}.\".format(\n+                batch, logs[\"loss\"]\n+            )\n+        )\n+\n+    def on_test_batch_end(self, batch, logs=None):\n+        print(\n+            \"Up to batch {}, the average loss is {:7.2f}.\".format(\n+                batch, logs[\"loss\"]\n+            )\n+        )\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        print(\n+            \"The average loss for epoch {} is {:7.2f} \"\n+            \"and mean absolute error is {:7.2f}.\".format(\n+                epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]\n+            )\n+        )\n+\n+\n+model = get_model()\n+model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=128,\n+    epochs=2,\n+    verbose=0,\n+    callbacks=[LossAndErrorPrintingCallback()],\n+)\n+\n+res = model.evaluate(\n+    x_test,\n+    y_test,\n+    batch_size=128,\n+    verbose=0,\n+    callbacks=[LossAndErrorPrintingCallback()],\n+)\n+\n+\"\"\"\n+## Usage of `self.model` attribute\n+\n+In addition to receiving log information when one of their methods is called,\n+callbacks have access to the model associated with the current round of\n+training/evaluation/inference: `self.model`.\n+\n+Here are a few of the things you can do with `self.model` in a callback:\n+\n+- Set `self.model.stop_training = True` to immediately interrupt training.\n+- Mutate hyperparameters of the optimizer (available as `self.model.optimizer`),\n+such as `self.model.optimizer.learning_rate`.\n+- Save the model at period intervals.\n+- Record the output of `model.predict()` on a few test samples at the end of each\n+epoch, to use as a sanity check during training.\n+- Extract visualizations of intermediate features at the end of each epoch, to monitor\n+what the model is learning over time.\n+- etc.\n+\n+Let's see this in action in a couple of examples.\n+\"\"\"\n+\n+\"\"\"\n+## Examples of Keras callback applications\n+\n+### Early stopping at minimum loss\n+\n+This first example shows the creation of a `Callback` that stops training when the\n+minimum of loss has been reached, by setting the attribute `self.model.stop_training`\n+(boolean). Optionally, you can provide an argument `patience` to specify how many\n+epochs we should wait before stopping after having reached a local minimum.\n+\n+`keras.callbacks.EarlyStopping` provides a more complete and general implementation.\n+\"\"\"\n+\n+\n+class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n+    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n+\n+    Arguments:\n+        patience: Number of epochs to wait after min has been hit. After this\n+        number of no improvement, training stops.\n+    \"\"\"\n+\n+    def __init__(self, patience=0):\n+        super().__init__()\n+        self.patience = patience\n+        # best_weights to store the weights at which the minimum loss occurs.\n+        self.best_weights = None\n+\n+    def on_train_begin(self, logs=None):\n+        # The number of epoch it has waited when loss is no longer minimum.\n+        self.wait = 0\n+        # The epoch the training stops at.\n+        self.stopped_epoch = 0\n+        # Initialize the best as infinity.\n+        self.best = np.Inf\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        current = logs.get(\"loss\")\n+        if np.less(current, self.best):\n+            self.best = current\n+            self.wait = 0\n+            # Record the best weights if current results is better (less).\n+            self.best_weights = self.model.get_weights()\n+        else:\n+            self.wait += 1\n+            if self.wait >= self.patience:\n+                self.stopped_epoch = epoch\n+                self.model.stop_training = True\n+                print(\"Restoring model weights from the end of the best epoch.\")\n+                self.model.set_weights(self.best_weights)\n+\n+    def on_train_end(self, logs=None):\n+        if self.stopped_epoch > 0:\n+            print(f\"Epoch {self.stopped_epoch + 1}: early stopping\")\n+\n+\n+model = get_model()\n+model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=64,\n+    epochs=30,\n+    verbose=0,\n+    callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss()],\n+)\n+\n+\"\"\"\n+### Learning rate scheduling\n+\n+In this example, we show how a custom Callback can be used to dynamically change the\n+learning rate of the optimizer during the course of training.\n+\n+See `callbacks.LearningRateScheduler` for a more general implementations.\n+\"\"\"\n+\n+\n+class CustomLearningRateScheduler(keras.callbacks.Callback):\n+    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n+\n+    Arguments:\n+        schedule: a function that takes an epoch index\n+            (integer, indexed from 0) and current learning rate\n+            as inputs and returns a new learning rate as output (float).\n+    \"\"\"\n+\n+    def __init__(self, schedule):\n+        super().__init__()\n+        self.schedule = schedule\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        if not hasattr(self.model.optimizer, \"learning_rate\"):\n+            raise ValueError('Optimizer must have a \"learning_rate\" attribute.')\n+        # Get the current learning rate from model's optimizer.\n+        lr = self.model.optimizer.learning_rate\n+        # Call schedule function to get the scheduled learning rate.\n+        scheduled_lr = self.schedule(epoch, lr)\n+        # Set the value back to the optimizer before this epoch starts\n+        self.model.optimizer.learning_rate = scheduled_lr\n+        print(\n+            f\"\\nEpoch {epoch}: Learning rate is {float(np.array(scheduled_lr))}.\"\n+        )\n+\n+\n+LR_SCHEDULE = [\n+    # (epoch to start, learning rate) tuples\n+    (3, 0.05),\n+    (6, 0.01),\n+    (9, 0.005),\n+    (12, 0.001),\n+]\n+\n+\n+def lr_schedule(epoch, lr):\n+    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n+    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n+        return lr\n+    for i in range(len(LR_SCHEDULE)):\n+        if epoch == LR_SCHEDULE[i][0]:\n+            return LR_SCHEDULE[i][1]\n+    return lr\n+\n+\n+model = get_model()\n+model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=64,\n+    epochs=15,\n+    verbose=0,\n+    callbacks=[\n+        LossAndErrorPrintingCallback(),\n+        CustomLearningRateScheduler(lr_schedule),\n+    ],\n+)\n+\n+\"\"\"\n+### Built-in Keras callbacks\n+\n+Be sure to check out the existing Keras callbacks by\n+reading the [API docs](https://keras.io/api/callbacks/).\n+Applications include logging to CSV, saving\n+the model, visualizing metrics in TensorBoard, and a lot more!\n+\"\"\"\n\n@@ -661,7 +661,7 @@ def repeat(x, repeats, axis=None):\n \n         x = KerasTensor(x.shape, standardize_dtype(x.dtype))\n         outputs = repeat(x, repeats, axis=axis)\n-\n+        print(outputs)\n         return torch.empty(\n             size=outputs.shape,\n             dtype=to_torch_dtype(outputs.dtype),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
