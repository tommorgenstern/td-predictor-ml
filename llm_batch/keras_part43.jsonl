{"custom_id": "keras#e94f29cac4151b1b52f3ee419bf7191018974308", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 2949 | Contributors (this commit): 25 | Commits (past 90d): 14 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -92,7 +92,8 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n         if len(y.shape) > 3:\n             raise Exception('class_weight not supported for 4+ dimensional targets.')\n         yshape = y.shape\n-        y = np.reshape(y, (-1, yshape[-1]))  # for time-distributed data, collapse time and sample\n+        # for time-distributed data, collapse time and sample\n+        y = np.reshape(y, (-1, yshape[-1]))\n         if y.shape[1] > 1:\n             y_classes = y.argmax(axis=1)\n         elif y.shape[1] == 1:\n@@ -658,7 +659,8 @@ class Graph(Model, containers.Graph):\n             val_f = self._test\n         if validation_data:\n             # can't use sample weights with validation data at this point\n-            sample_weight = [standardize_weights(validation_data[name]) for name in self.output_order]\n+            y_val = [standardize_y(data[name]) for name in self.output_order]\n+            sample_weight = [standardize_weights(y_val[i]) for i in range(len(y_val))]\n             val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order] + sample_weight\n \n         elif 0 < validation_split < 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52dac5e4b3e0e3a913f8c709ea4983f52f11d6d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 15 | Methods Changed: 20 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 108 | Churn Cumulative: 1051 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,13 @@ def zeros_like(x, name=None):\n     return tf.zeros_like(x)\n \n \n+def count_params(x):\n+    '''Return number of scalars in a tensor.\n+    '''\n+    shape = x.get_shape()\n+    return np.prod([shape[i]._value for i in range(len(shape))])\n+\n+\n # LINEAR ALGEBRA\n \n def dot(x, y):\n@@ -98,12 +105,20 @@ def sum(x, axis=None, keepdims=False):\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n-def mul(x, axis=None, keepdims=False):\n+def prod(x, axis=None, keepdims=False):\n     '''Multiply the values in a tensor, alongside the specified axis.\n     '''\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n \n \n+def std(x, axis=None, keepdims=False):\n+    m = tf.reduce_mean(x, reduction_indices=axis, keep_dims=keepdims)\n+    devs_squared = tf.square(x - m)\n+    return tf.sqrt(tf.reduce_mean(devs_squared,\n+                                  reduction_indices=axis,\n+                                  keep_dims=keepdims))\n+\n+\n def mean(x, axis=None, keepdims=False):\n     if axis is not None:\n         axis = axis % len(x.get_shape())\n@@ -197,10 +212,19 @@ def permute_dimensions(x, pattern):\n     return tf.transpose(x, perm=pattern)\n \n \n-def repeat(x, n, axis=-1):\n-    tiling_pattern = [1 for _ in range(len(x.get_shape()))]\n-    tiling_pattern[axis] = n\n-    return tf.tile(x, tiling_pattern)\n+def repeat(x, n):\n+    '''Repeat a 2D tensor:\n+\n+    if x has shape (samples, dim) and n=2,\n+    the output will have shape (samples, 2, dim)\n+    '''\n+    tensors = [x] * n\n+    stacked = tf.pack(tensors)\n+    return tf.transpose(stacked, (1, 0, 2))\n+\n+\n+def tile(x, n):\n+    return tf.tile(x, n)\n \n \n def flatten(x):\n@@ -208,6 +232,18 @@ def flatten(x):\n     return x\n \n \n+def expand_dims(x, dim=-1):\n+    '''Add a 1-sized dimension at index \"dim\".\n+    '''\n+    return tf.expand_dims(x, dim)\n+\n+\n+def squeeze(x, axis):\n+    '''Remove a 1-dimension from the tensor at index \"axis\".\n+    '''\n+    return tf.squeeze(x, [axis])\n+\n+\n # VALUE MANIPULATION\n \n def get_value(x):\n@@ -377,7 +413,8 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th'):\n     return x\n \n \n-def maxpool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='th'):\n+def maxpool2d(x, pool_size, strides=(1, 1),\n+              border_mode='valid', dim_ordering='th'):\n     '''\n     pool_size: tuple of 2 integers.\n     strides: tuple of 2 integers.\n@@ -395,7 +432,8 @@ def maxpool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='t\n     pool_size = (1,) + pool_size + (1,)\n \n     if dim_ordering == 'th':\n-        # TF uses the last dimension as channel dimension, instead of the 2nd one.\n+        # TF uses the last dimension as channel dimension,\n+        # instead of the 2nd one.\n         # TH input shape: (samples, input_depth, rows, cols)\n         # TF input shape: (samples, rows, cols, input_depth)\n         # TH kernel shape: (depth, input_depth, rows, cols)\n\n@@ -88,6 +88,14 @@ def zeros_like(x):\n     return T.zeros_like(x)\n \n \n+def count_params(x):\n+    '''Return number of scalars in a tensor.\n+\n+    Return: numpy integer.\n+    '''\n+    return np.prod(x.shape.eval())\n+\n+\n # LINEAR ALGEBRA\n \n '''\n@@ -129,16 +137,20 @@ def sum(x, axis=None, keepdims=False):\n     return T.sum(x, axis=axis, keepdims=keepdims)\n \n \n-def mul(x, axis=None, keepdims=False):\n+def prod(x, axis=None, keepdims=False):\n     '''Multiply the values in a tensor, alongside the specified axis.\n     '''\n-    return T.mul(x, axis=axis, keepdims=keepdims)\n+    return T.prod(x, axis=axis, keepdims=keepdims)\n \n \n def mean(x, axis=None, keepdims=False):\n     return T.mean(x, axis=axis, keepdims=keepdims)\n \n \n+def std(x, axis=None, keepdims=False):\n+    return T.std(x, axis=axis, keepdims=keepdims)\n+\n+\n def any(x, axis=None, keepdims=False):\n     '''Bitwise reduction (logical OR).\n     '''\n@@ -216,8 +228,19 @@ def permute_dimensions(x, pattern):\n     return x.dimshuffle(pattern)\n \n \n-def repeat(x, n, axis=-1):\n-    return T.extra_ops.repeat(x, n, axis=axis)\n+def repeat(x, n):\n+    '''Repeat a 2D tensor:\n+\n+    if x has shape (samples, dim) and n=2,\n+    the output will have shape (samples, 2, dim)\n+    '''\n+    tensors = [x] * n\n+    stacked = T.stack(*tensors)\n+    return stacked.dimshuffle((1, 0, 2))\n+\n+\n+def tile(x, n):\n+    return T.tile(x, n)\n \n \n def flatten(x):\n@@ -225,6 +248,23 @@ def flatten(x):\n     return x\n \n \n+def expand_dims(x, dim=-1):\n+    '''Add a 1-sized dimension at index \"dim\".\n+    '''\n+    pattern = [i for i in range(x.type.ndim)]\n+    if dim < 0:\n+        dim = dim % x.type.ndim + 1\n+    pattern.insert(dim, 'x')\n+    return x.dimshuffle(pattern)\n+\n+\n+def squeeze(x, axis):\n+    '''Remove a 1-dimension from the tensor at index \"axis\".\n+    '''\n+    x = T.addbroadcast(x, axis)\n+    return T.squeeze(x)\n+\n+\n # VALUE MANIPULATION\n \n def get_value(x):\n@@ -393,7 +433,8 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th'):\n     return conv_out\n \n \n-def maxpool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='th'):\n+def maxpool2d(x, pool_size, strides=(1, 1), border_mode='valid',\n+              dim_ordering='th'):\n     if border_mode == 'same':\n         # TODO: add implementation for border_mode=\"same\"\n         raise Exception('border_mode=\"same\" not supported with Theano.')\n@@ -440,9 +481,8 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n '''\n more TODO:\n \n-shape_padright/left\n-tensordot\n-batched_tensordot\n+tensordot -> soon to be introduced in TF\n+batched_tensordot -> reimplement\n \n addbroadcast -> remove usage?\n unbroadcast -> remove usage?\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a744b600e94ae00fbec71ef493afdff48bc3816b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 139 | Files Changed: 4 | Hunks: 70 | Methods Changed: 23 | Complexity Δ (Sum/Max): -14/1 | Churn Δ: 248 | Churn Cumulative: 4271 | Contributors (this commit): 37 | Commits (past 90d): 108 | Contributors (cumulative): 54 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,9 +3,8 @@ from __future__ import absolute_import\n from __future__ import print_function\n \n from collections import OrderedDict\n-import theano.tensor as T\n+from .. import backend as K\n from ..layers.core import Layer, Merge\n-from ..utils.theano_utils import ndim_tensor\n from six.moves import range\n \n \n@@ -76,8 +75,8 @@ class Sequential(Layer):\n     def set_input(self):\n         for l in self.layers:\n             if hasattr(l, 'input'):\n-                ndim = l.input.ndim\n-                self.layers[0].input = ndim_tensor(ndim)\n+                ndim = len(K.get_shape(l.input))\n+                self.layers[0].input = K.placeholder(ndim=ndim)\n                 break\n \n     def get_input(self, train=False):\n@@ -227,10 +226,10 @@ class Graph(Layer):\n         layer.set_input_shape(input_shape)\n         ndim = len(input_shape) + 1\n         if dtype == 'float':\n-            layer.input = ndim_tensor(ndim)\n+            layer.input = K.placeholder(ndim=ndim)\n         else:\n             if ndim == 2:\n-                layer.input = T.imatrix()\n+                layer.input = K.placeholder(ndim=2, dtype='int32')\n             else:\n                 raise Exception('Type \"int\" can only be used with ndim==2 (Embedding).')\n         layer.input.name = name\n@@ -240,9 +239,8 @@ class Graph(Layer):\n                                   'dtype': dtype})\n \n     def add_node(self, layer, name, input=None, inputs=[],\n-                 merge_mode='concat', concat_axis=-1, dot_axes=-1, create_output=False):\n-        if hasattr(layer, 'set_name'):\n-            layer.set_name(name)\n+                 merge_mode='concat', concat_axis=-1, dot_axes=-1,\n+                 create_output=False):\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         if input:\n@@ -261,7 +259,8 @@ class Graph(Layer):\n                     to_merge.append(self.inputs[n])\n                 else:\n                     raise Exception('Unknown identifier: ' + n)\n-            merge = Merge(to_merge, mode=merge_mode, concat_axis=concat_axis, dot_axes=dot_axes)\n+            merge = Merge(to_merge, mode=merge_mode,\n+                          concat_axis=concat_axis, dot_axes=dot_axes)\n             layer.set_previous(merge)\n \n         self.namespace.add(name)\n@@ -294,7 +293,8 @@ class Graph(Layer):\n                 if n not in self.nodes:\n                     raise Exception('Unknown identifier: ' + n)\n                 to_merge.append(self.nodes[n])\n-            merge = Merge(to_merge, mode=merge_mode, concat_axis=concat_axis, dot_axes=dot_axes)\n+            merge = Merge(to_merge, mode=merge_mode,\n+                          concat_axis=concat_axis, dot_axes=dot_axes)\n             self.outputs[name] = merge\n \n         self.output_order.append(name)\n\n@@ -2,21 +2,16 @@\n from __future__ import absolute_import\n from __future__ import division\n \n-import theano\n-import theano.tensor as T\n import numpy as np\n \n from collections import OrderedDict\n import copy\n-\n-from .. import activations, initializations, regularizers, constraints\n-from ..utils.theano_utils import shared_zeros, floatX, ndim_tensor\n-from ..utils.generic_utils import make_tuple\n-from ..regularizers import ActivityRegularizer, Regularizer\n-\n-from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n \n+from .. import backend as K\n+from .. import activations, initializations, regularizers, constraints\n+from ..regularizers import ActivityRegularizer\n+\n \n class Layer(object):\n     def __init__(self, **kwargs):\n@@ -87,7 +82,7 @@ class Layer(object):\n                 raise Exception('Invalid input shape - Layer expects input ndim=' +\n                                 str(self.input_ndim) + ', was provided with input shape ' + str(input_shape))\n         self._input_shape = input_shape\n-        self.input = ndim_tensor(len(self._input_shape))\n+        self.input = K.placeholder(ndim=len(self._input_shape))\n         self.build()\n \n     @property\n@@ -133,14 +128,14 @@ class Layer(object):\n         assert len(self.params) == len(weights), 'Provided weight array does not match layer weights (' + \\\n             str(len(self.params)) + ' layer params vs. ' + str(len(weights)) + ' provided weights)'\n         for p, w in zip(self.params, weights):\n-            if p.eval().shape != w.shape:\n-                raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (p.eval().shape, w.shape))\n-            p.set_value(floatX(w))\n+            if K.get_value(p).shape != w.shape:\n+                raise Exception(\"Layer shape %s not compatible with weight shape %s.\" % (K.get_value(p).shape, w.shape))\n+            K.set_value(p, w)\n \n     def get_weights(self):\n         weights = []\n         for p in self.params:\n-            weights.append(p.get_value())\n+            weights.append(K.get_value(p))\n         return weights\n \n     def get_config(self):\n@@ -176,18 +171,17 @@ class Layer(object):\n \n         return self.params, regularizers, consts, updates\n \n-    def set_name(self, name):\n-        for i in range(len(self.params)):\n-            self.params[i].name = '%s_p%d' % (name, i)\n-\n     def count_params(self):\n-        return sum([np.prod(p.shape.eval()) for p in self.params])\n+        return sum([K.count_params(p) for p in self.params])\n \n \n class MaskedLayer(Layer):\n     '''\n-    If your layer trivially supports masking (by simply copying the input mask to the output), then subclass MaskedLayer\n-    instead of Layer, and make sure that you incorporate the input mask into your calculation of get_output()\n+    If your layer trivially supports masking\n+    (by simply copying the input mask to the output),\n+    then subclass MaskedLayer instead of Layer,\n+    and make sure that you incorporate the input mask\n+    into your calculation of get_output()\n     '''\n     def supports_masked_input(self):\n         return True\n@@ -199,8 +193,9 @@ class MaskedLayer(Layer):\n             return None\n \n     def get_output_mask(self, train=False):\n-        ''' The default output mask is just the input mask unchanged. Override this in your own\n-        implementations if, for instance, you are reshaping the input'''\n+        ''' The default output mask is just the input mask unchanged.\n+        Override this in your own implementations if,\n+        for instance, you are reshaping the input'''\n         return self.get_input_mask(train)\n \n \n@@ -218,15 +213,17 @@ class Masking(MaskedLayer):\n     def __init__(self, mask_value=0., **kwargs):\n         super(Masking, self).__init__(**kwargs)\n         self.mask_value = mask_value\n-        self.input = T.tensor3()\n+        self.input = K.placeholder(ndim=3)\n \n     def get_output_mask(self, train=False):\n         X = self.get_input(train)\n-        return T.any(T.ones_like(X) * (1. - T.eq(X, self.mask_value)), axis=-1)\n+        return K.any(K.ones_like(X) * (1. - K.equal(X, self.mask_value)),\n+                     axis=-1)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        return X * T.shape_padright(T.any((1. - T.eq(X, self.mask_value)), axis=-1))\n+        return X * K.any((1. - K.equal(X, self.mask_value)),\n+                         axis=-1, keepdims=True)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -258,13 +255,13 @@ class TimeDistributedMerge(Layer):\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         if self.mode == 'ave':\n-            s = theano.tensor.mean(X, axis=1)\n+            s = K.mean(X, axis=1)\n             return s\n         if self.mode == 'sum':\n-            s = theano.tensor.sum(X, axis=1)\n+            s = K.sum(X, axis=1)\n             return s\n         elif self.mode == 'mul':\n-            s = theano.tensor.mul(X, axis=1)\n+            s = K.prod(X, axis=1)\n             return s\n         else:\n             raise Exception('Unknown merge mode')\n@@ -293,6 +290,9 @@ class Merge(Layer):\n                 raise Exception(\"Only layers of same output shape can be merged using \" + mode + \" mode. \" +\n                                 \"Layer shapes: %s\" % ([l.output_shape for l in layers]))\n         if mode in {'cos', 'dot'}:\n+            if K._BACKEND != 'theano':\n+                raise Exception('\"' + mode + '\" merge mode will only work with Theano.')\n+\n             if len(layers) > 2:\n                 raise Exception(mode + \" merge takes exactly 2 layers\")\n             shape1 = layers[0].output_shape\n@@ -388,7 +388,7 @@ class Merge(Layer):\n             return s\n         elif self.mode == 'concat':\n             inputs = [self.layers[i].get_output(train) for i in range(len(self.layers))]\n-            return T.concatenate(inputs, axis=self.concat_axis)\n+            return K.concatenate(inputs, axis=self.concat_axis)\n         elif self.mode == 'join':\n             inputs = OrderedDict()\n             for i in range(len(self.layers)):\n@@ -404,6 +404,9 @@ class Merge(Layer):\n                 s *= self.layers[i].get_output(train)\n             return s\n         elif self.mode == 'dot':\n+            if K._BACKEND != 'theano':\n+                raise Exception('\"dot\" merge mode will only work with Theano.')\n+            from theano import tensor as T\n             l1 = self.layers[0].get_output(train)\n             l2 = self.layers[1].get_output(train)\n             output = T.batched_tensordot(l1, l2, self.dot_axes)\n@@ -412,9 +415,12 @@ class Merge(Layer):\n             output = output.reshape(tuple(output_shape))\n             return output\n         elif self.mode == 'cos':\n+            if K._BACKEND != 'theano':\n+                raise Exception('\"dot\" merge mode will only work with Theano.')\n+            import theano\n             l1 = self.layers[0].get_output(train)\n             l2 = self.layers[1].get_output(train)\n-            output, _ = theano.scan(lambda v1, v2: T.dot(v1, v2) / T.sqrt(T.dot(v1, v1) * T.dot(v2, v2)),\n+            output, _ = theano.scan(lambda v1, v2: K.dot(v1, v2) / K.sqrt(K.dot(v1, v1) * K.dot(v2, v2)),\n                                     sequences=[l1, l2],\n                                     outputs_info=None)\n             return output\n@@ -471,14 +477,12 @@ class Dropout(MaskedLayer):\n     def __init__(self, p, **kwargs):\n         super(Dropout, self).__init__(**kwargs)\n         self.p = p\n-        self.srng = RandomStreams(seed=np.random.randint(10e6))\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n         if self.p > 0.:\n-            retain_prob = 1. - self.p\n             if train:\n-                X *= self.srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX) / retain_prob\n+                X = K.dropout(X, level=self.p)\n         return X\n \n     def get_config(self):\n@@ -528,7 +532,7 @@ class Reshape(Layer):\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         new_shape = (X.shape[0],) + self.dims\n-        return theano.tensor.reshape(X, new_shape)\n+        return K.reshape(X, new_shape)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -556,7 +560,7 @@ class Permute(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        return X.dimshuffle((0,) + self.dims)\n+        return K.permute_dimensions(X, (0,) + self.dims)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -580,9 +584,7 @@ class Flatten(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        size = theano.tensor.prod(X.shape) // X.shape[0]\n-        nshape = (X.shape[0], size)\n-        return theano.tensor.reshape(X, nshape)\n+        return K.flatten(X)\n \n \n class RepeatVector(Layer):\n@@ -603,9 +605,7 @@ class RepeatVector(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        tensors = [X]*self.n\n-        stacked = theano.tensor.stack(*tensors)\n-        return stacked.dimshuffle((1, 0, 2))\n+        return K.repeat(X, self.n)\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -645,9 +645,9 @@ class Dense(Layer):\n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.input = T.matrix()\n+        self.input = K.placeholder(ndim=2)\n         self.W = self.init((input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim,))\n+        self.b = K.zeros((self.output_dim,))\n \n         self.params = [self.W, self.b]\n \n@@ -674,7 +674,7 @@ class Dense(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = self.activation(T.dot(X, self.W) + self.b)\n+        output = self.activation(K.dot(X, self.W) + self.b)\n         return output\n \n     def get_config(self):\n@@ -753,9 +753,9 @@ class TimeDistributedDense(MaskedLayer):\n     def build(self):\n         input_dim = self.input_shape[2]\n \n-        self.input = T.tensor3()\n+        self.input = K.placeholder(ndim=3)\n         self.W = self.init((input_dim, self.output_dim))\n-        self.b = shared_zeros((self.output_dim))\n+        self.b = K.zeros((self.output_dim))\n \n         self.params = [self.W, self.b]\n         self.regularizers = []\n@@ -783,8 +783,9 @@ class TimeDistributedDense(MaskedLayer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        output = self.activation(T.dot(X.dimshuffle(1, 0, 2), self.W) + self.b)\n-        return output.dimshuffle(1, 0, 2)\n+        output = self.activation(K.dot(K.permute_dimensions(X, (1, 0, 2)),\n+                                       self.W) + self.b)\n+        return K.permute_dimensions(output, (1, 0, 2))\n \n     def get_config(self):\n         config = {\"name\": self.__class__.__name__,\n@@ -916,9 +917,9 @@ class MaxoutDense(Layer):\n     def build(self):\n         input_dim = self.input_shape[1]\n \n-        self.input = T.matrix()\n+        self.input = K.placeholder(ndim=2)\n         self.W = self.init((self.nb_feature, input_dim, self.output_dim))\n-        self.b = shared_zeros((self.nb_feature, self.output_dim))\n+        self.b = K.zeros((self.nb_feature, self.output_dim))\n \n         self.params = [self.W, self.b]\n         self.regularizers = []\n@@ -946,7 +947,7 @@ class MaxoutDense(Layer):\n     def get_output(self, train=False):\n         X = self.get_input(train)\n         # -- don't need activation since it's just linear.\n-        output = T.max(T.dot(X, self.W) + self.b, axis=1)\n+        output = K.max(K.dot(X, self.W) + self.b, axis=1)\n         return output\n \n     def get_config(self):\n\n@@ -1,22 +1,24 @@\n from ..layers.core import Layer\n-from ..utils.theano_utils import shared_zeros, shared_ones, ndim_tensor, floatX\n from .. import initializations\n-\n-import theano.tensor as T\n+from .. import backend as K\n \n \n class BatchNormalization(Layer):\n     '''\n         Reference:\n-            Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n+            Batch Normalization: Accelerating Deep Network Training\n+            by Reducing Internal Covariate Shift\n                 http://arxiv.org/pdf/1502.03167v3.pdf\n \n             mode: 0 -> featurewise normalization\n-                  1 -> samplewise normalization (may sometimes outperform featurewise mode)\n+                  1 -> samplewise normalization\n+                       (may sometimes outperform featurewise mode)\n \n-            momentum: momentum term in the computation of a running estimate of the mean and std of the data\n+            momentum: momentum term in the computation\n+            of a running estimate of the mean and std of the data\n     '''\n-    def __init__(self, epsilon=1e-6, mode=0, momentum=0.9, weights=None, **kwargs):\n+    def __init__(self, epsilon=1e-6, mode=0, momentum=0.9,\n+                 weights=None, **kwargs):\n         self.init = initializations.get(\"uniform\")\n         self.epsilon = epsilon\n         self.mode = mode\n@@ -27,46 +29,47 @@ class BatchNormalization(Layer):\n     def build(self):\n         input_shape = self.input_shape  # starts with samples axis\n         input_shape = input_shape[1:]\n-        self.input = ndim_tensor(len(input_shape) + 1)\n+        self.input = K.placeholder(ndim=len(input_shape) + 1)\n \n         self.gamma = self.init((input_shape))\n-        self.beta = shared_zeros(input_shape)\n+        self.beta = K.zeros(input_shape)\n \n         self.params = [self.gamma, self.beta]\n-        self.running_mean = shared_zeros(input_shape)\n-        self.running_std = shared_ones((input_shape))\n+        self.running_mean = K.zeros(input_shape)\n+        self.running_std = K.ones((input_shape))\n \n         # initialize self.updates: batch mean/std computation\n         X = self.get_input(train=True)\n-        m = X.mean(axis=0)\n-        std = T.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n+        m = K.mean(X, axis=0)\n+        std = K.mean((X - m) ** 2 + self.epsilon, axis=0) ** 0.5\n         mean_update = self.momentum * self.running_mean + (1-self.momentum) * m\n         std_update = self.momentum * self.running_std + (1-self.momentum) * std\n-        self.updates = [(self.running_mean, mean_update), (self.running_std, std_update)]\n+        self.updates = [(self.running_mean, mean_update),\n+                        (self.running_std, std_update)]\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n \n     def get_weights(self):\n-        return super(BatchNormalization, self).get_weights() + [self.running_mean.get_value(), self.running_std.get_value()]\n+        super_weights = super(BatchNormalization, self).get_weights()\n+        return super_weights + [K.get_value(self.running_mean),\n+                                K.get_value(self.running_std)]\n \n     def set_weights(self, weights):\n-        self.running_mean.set_value(floatX(weights[-2]))\n-        self.running_std.set_value(floatX(weights[-1]))\n+        K.set_value(self.running_mean, weights[-2])\n+        K.set_value(self.running_std, weights[-1])\n         super(BatchNormalization, self).set_weights(weights[:-2])\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-\n         if self.mode == 0:\n-            X_normed = (X - self.running_mean) / (self.running_std + self.epsilon)\n-\n+            X_normed = ((X - self.running_mean) /\n+                        (self.running_std + self.epsilon))\n         elif self.mode == 1:\n-            m = X.mean(axis=-1, keepdims=True)\n-            std = X.std(axis=-1, keepdims=True)\n+            m = K.mean(X, axis=-1, keepdims=True)\n+            std = K.std(X, axis=-1, keepdims=True)\n             X_normed = (X - m) / (std + self.epsilon)\n-\n         out = self.gamma * X_normed + self.beta\n         return out\n \n@@ -96,12 +99,14 @@ class LRN2D(Layer):\n \n     def get_output(self, train):\n         X = self.get_input(train)\n-        b, ch, r, c = X.shape\n+        b, ch, r, c = K.shape(X)\n         half_n = self.n // 2\n-        input_sqr = T.sqr(X)\n-        extra_channels = T.alloc(0., b, ch + 2*half_n, r, c)\n-        # TODO: use concatenate instead\n-        input_sqr = T.set_subtensor(extra_channels[:, half_n:half_n+ch, :, :], input_sqr)\n+        input_sqr = K.square(X)\n+        extra_channels = K.zeros((b, ch + 2*half_n, r, c))\n+        input_sqr = K.concatenate([extra_channels[:, :half_n, :, :],\n+                                   input_sqr,\n+                                   extra_channels[:, half_n+ch:, :, :]],\n+                                  axis=1)\n         scale = self.k\n         for i in range(self.n):\n             scale += self.alpha * input_sqr[:, i:i+ch, :, :]\n\n@@ -1,17 +1,15 @@\n from __future__ import print_function\n import inspect\n import numpy as np\n-import theano\n import copy\n \n from ..layers.advanced_activations import LeakyReLU, PReLU\n-from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer, AutoEncoder, Masking, Permute\n-from ..layers.core import ActivityRegularization, TimeDistributedDense, TimeDistributedMerge, AutoEncoder, MaxoutDense\n-from ..layers.convolutional import Convolution1D, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D\n-from ..layers.embeddings import Embedding, WordContextProduct\n-from ..layers.noise import GaussianNoise, GaussianDropout\n-from ..layers.normalization import BatchNormalization, LRN2D\n-from ..layers.recurrent import SimpleRNN, SimpleDeepRNN, GRU, LSTM, JZS1, JZS2, JZS3\n+from ..layers.core import *\n+from ..layers.convolutional import *\n+from ..layers.embeddings import *\n+from ..layers.noise import *\n+from ..layers.normalization import *\n+from ..layers.recurrent import *\n from ..layers import containers\n from .. import regularizers\n from .. import constraints\n@@ -21,7 +19,8 @@ def container_from_config(original_layer_dict, custom_objects={}):\n     layer_dict = copy.deepcopy(original_layer_dict)\n     name = layer_dict.get('name')\n \n-    # Insert custom layers into globals so they can be accessed by `get_from_module`.\n+    # Insert custom layers into globals so they can\n+    # be accessed by `get_from_module`.\n     for cls_key in custom_objects:\n         globals()[cls_key] = custom_objects[cls_key]\n \n@@ -80,50 +79,15 @@ def container_from_config(original_layer_dict, custom_objects={}):\n                     layer_dict[k] = constraints.get(vname, v)\n                 elif vname in [x for x, y in inspect.getmembers(regularizers, predicate=inspect.isclass)]:\n                     layer_dict[k] = regularizers.get(vname, v)\n-                else: # not a regularizer of constraint, don't touch it\n+                else:\n+                    # not a regularizer of constraint, don't touch it\n                     v['name'] = vname\n \n         base_layer = get_layer(name, layer_dict)\n         return base_layer\n \n \n-def print_layer_shapes(model, input_shapes):\n-    \"\"\"\n-    Utility function to print the shape of the output at each layer of a Model\n-\n-    Arguments:\n-        model: instance of Model / Merge\n-        input_shapes: dict (Graph), list of tuples (Merge) or tuple (Sequential)\n-    \"\"\"\n-    if model.__class__.__name__ in ['Sequential', 'Merge']:\n-        # in this case input_shapes is a tuple, or a list [shape1, shape2]\n-        if not isinstance(input_shapes[0], tuple):\n-            input_shapes = [input_shapes]\n-\n-        inputs = model.get_input(train=False)\n-        if not isinstance(inputs, list):\n-            inputs = [inputs]\n-        input_dummy = [np.zeros(shape, dtype=np.float32)\n-                       for shape in input_shapes]\n-        layers = model.layers\n-\n-    elif model.__class__.__name__ == 'Graph':\n-        # in this case input_shapes is a dictionary\n-        inputs = [model.inputs[name].input\n-                  for name in model.input_order]\n-        input_dummy = [np.zeros(input_shapes[name], dtype=np.float32)\n-                       for name in model.input_order]\n-        layers = [model.nodes[c['name']] for c in model.node_config]\n-\n-    print(\"input shapes : \", input_shapes)\n-    for l in layers:\n-        shape_f = theano.function(inputs, l.get_output(train=False).shape,\n-                                  on_unused_input='ignore')\n-        out_shape = tuple(shape_f(*input_dummy))\n-        config = l.get_config()\n-        print('shape after %s: %s' % (config['name'], out_shape))\n-\n-\n from .generic_utils import get_from_module\n def get_layer(identifier, kwargs=None):\n-    return get_from_module(identifier, globals(), 'layer', instantiate=True, kwargs=kwargs)\n+    return get_from_module(identifier, globals(), 'layer',\n+                           instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ad18ce8f5f88d2244ceec3075c5de0a0880e1aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 78 | Lines Deleted: 77 | Files Changed: 6 | Hunks: 55 | Methods Changed: 23 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 155 | Churn Cumulative: 1415 | Contributors (this commit): 7 | Commits (past 90d): 22 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,17 +61,22 @@ class TestBackend(unittest.TestCase):\n                                       pattern=(2, 0, 1))\n         check_single_tensor_operation('repeat', (4, 1), n=3)\n         check_single_tensor_operation('flatten', (4, 1))\n+        check_single_tensor_operation('expand_dims', (4, 3), dim=-1)\n+        check_single_tensor_operation('expand_dims', (4, 3, 2), dim=1)\n+        check_single_tensor_operation('squeeze', (4, 3, 1), axis=2)\n \n     def test_value_manipulation(self):\n         val = np.random.random((4, 2))\n         xth = KTH.variable(val)\n         xtf = KTF.variable(val)\n \n+        # get_value\n         valth = KTH.get_value(xth)\n         valtf = KTF.get_value(xtf)\n         assert valtf.shape == valth.shape\n         assert_allclose(valth, valtf, atol=1e-06)\n \n+        # set_value\n         val = np.random.random((4, 2))\n         KTH.set_value(xth, val)\n         KTF.set_value(xtf, val)\n@@ -81,6 +86,9 @@ class TestBackend(unittest.TestCase):\n         assert valtf.shape == valth.shape\n         assert_allclose(valth, valtf, atol=1e-06)\n \n+        # count_params\n+        assert KTH.count_params(xth) == KTF.count_params(xtf)\n+\n     def test_elementwise_operations(self):\n         check_single_tensor_operation('max', (4, 2))\n         check_single_tensor_operation('max', (4, 2), axis=1, keepdims=True)\n@@ -91,6 +99,12 @@ class TestBackend(unittest.TestCase):\n         check_single_tensor_operation('mean', (4, 2))\n         check_single_tensor_operation('mean', (4, 2), axis=1, keepdims=True)\n \n+        check_single_tensor_operation('std', (4, 2))\n+        check_single_tensor_operation('std', (4, 2), axis=1, keepdims=True)\n+\n+        check_single_tensor_operation('prod', (4, 2))\n+        check_single_tensor_operation('prod', (4, 2), axis=1, keepdims=True)\n+\n         # does not work yet, wait for bool <-> int casting in TF (coming soon)\n         # check_single_tensor_operation('any', (4, 2))\n         # check_single_tensor_operation('any', (4, 2), axis=1, keepdims=True)\n\n@@ -1,20 +1,8 @@\n+import unittest\n import math\n-\n-import keras\n-import theano\n-import theano.tensor as T\n-\n-import numpy\n-\n-\n-def list_assert_equal(a, b, round_to=7):\n-    '''\n-    This will do a pairwise, rounded equality test across two lists of\n-    numbers.\n-    '''\n-    pairs = zip(a, b)\n-    for i, j in pairs:\n-        assert round(i, round_to) == round(j, round_to)\n+from keras import backend as K\n+import numpy as np\n+from numpy.testing import assert_allclose\n \n \n def get_standard_values():\n@@ -25,39 +13,32 @@ def get_standard_values():\n     return [0, 0.1, 0.5, 0.9, 1.0]\n \n \n-def test_softmax():\n+class TestActivations(unittest.TestCase):\n \n+    def test_softmax(self):\n         from keras.activations import softmax as s\n \n         # Test using a reference implementation of softmax\n         def softmax(values):\n             m = max(values)\n-        values = numpy.array(values)\n-        e = numpy.exp(values - m)\n-        dist = list(e / numpy.sum(e))\n+            values = np.array(values)\n+            e = np.exp(values - m)\n+            return e / np.sum(e)\n \n-        return dist\n-\n-    x = T.vector()\n+        x = K.placeholder(ndim=1)\n         exp = s(x)\n-    f = theano.function([x], exp)\n+        f = K.function([x], exp)\n         test_values = get_standard_values()\n \n         result = f(test_values)\n         expected = softmax(test_values)\n+        assert_allclose(result.flatten(), expected)\n \n-    print(str(result))\n-    print(str(expected))\n-\n-    list_assert_equal(result, expected)\n-\n-\n-def test_relu():\n+    def test_relu(self):\n         '''\n         Relu implementation doesn't depend on the value being\n         a theano variable. Testing ints, floats and theano tensors.\n         '''\n-\n         from keras.activations import relu as r\n \n         assert r(5) == 5\n@@ -65,42 +46,38 @@ def test_relu():\n         assert r(-0.1) == 0\n         assert r(0.1) == 0.1\n \n-    x = T.vector()\n+        x = K.placeholder(ndim=1)\n         exp = r(x)\n-    f = theano.function([x], exp)\n+        f = K.function([x], exp)\n \n         test_values = get_standard_values()\n         result = f(test_values)\n \n-    list_assert_equal(result, test_values)  # because no negatives in test values\n+        # because no negatives in test values\n+        assert_allclose(result.flatten(), test_values)\n \n-\n-def test_tanh():\n+    def test_tanh(self):\n         from keras.activations import tanh as t\n         test_values = get_standard_values()\n \n-    x = T.vector()\n+        x = K.placeholder(ndim=1)\n         exp = t(x)\n-    f = theano.function([x], exp)\n+        f = K.function([x], exp)\n \n         result = f(test_values)\n         expected = [math.tanh(v) for v in test_values]\n+        assert_allclose(result.flatten(), expected)\n \n-    print(result)\n-    print(expected)\n-\n-    list_assert_equal(result, expected)\n-\n-\n-def test_linear():\n+    def test_linear(self):\n         '''\n         This function does no input validation, it just returns the thing\n         that was passed in.\n         '''\n-\n         from keras.activations import linear as l\n \n         xs = [1, 5, True, None, 'foo']\n-\n         for x in xs:\n             assert x == l(x)\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -1,9 +1,9 @@\n import unittest\n import numpy as np\n from numpy.testing import assert_allclose\n-from theano import tensor as T\n from keras.layers import normalization\n from keras.models import Sequential\n+from keras import backend as K\n \n \n class TestBatchNormalization(unittest.TestCase):\n@@ -30,23 +30,23 @@ class TestBatchNormalization(unittest.TestCase):\n         # centered on 5.0, variance 10.0\n         X = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n         model.fit(X, X, nb_epoch=5, verbose=0)\n-        norm_m0.input = X\n+        norm_m0.input = K.variable(X)\n         out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n \n-        self.assertAlmostEqual(out.mean().eval(), 0.0, places=1)\n-        self.assertAlmostEqual(out.std().eval(), 1.0, places=1)\n+        self.assertAlmostEqual(K.eval(K.mean(out)), 0.0, places=1)\n+        self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=1)\n \n     def test_mode_1(self):\n         norm_m1 = normalization.BatchNormalization(input_shape=(10,), mode=1)\n \n         for inp in [self.input_1, self.input_2, self.input_3]:\n-            norm_m1.input = inp\n+            norm_m1.input = K.variable(inp)\n             out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n-            self.assertAlmostEqual(out.mean().eval(), 0.0)\n+            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n             if inp.std() > 0.:\n-                self.assertAlmostEqual(out.std().eval(), 1.0, places=2)\n+                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n             else:\n-                self.assertAlmostEqual(out.std().eval(), 0.0, places=2)\n+                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n \n     def test_shapes(self):\n         \"\"\"\n@@ -54,11 +54,11 @@ class TestBatchNormalization(unittest.TestCase):\n         \"\"\"\n         for inp in self.input_shapes:\n             norm_m0 = normalization.BatchNormalization(input_shape=inp.shape, mode=0)\n-            norm_m0.input = inp\n+            norm_m0.input = K.variable(inp)\n             out = (norm_m0.get_output(train=True) - norm_m0.beta) / norm_m0.gamma\n \n             norm_m1 = normalization.BatchNormalization(input_shape=inp.shape, mode=1)\n-            norm_m1.input = inp\n+            norm_m1.input = K.variable(inp)\n             out = (norm_m1.get_output(train=True) - norm_m1.beta) / norm_m1.gamma\n \n     def test_weight_init(self):\n@@ -69,26 +69,29 @@ class TestBatchNormalization(unittest.TestCase):\n                                                    weights=[np.ones(10), np.ones(10), np.zeros(10), np.zeros(10)])\n \n         for inp in [self.input_1, self.input_2, self.input_3]:\n-            norm_m1.input = inp\n+            norm_m1.input = K.variable(inp)\n             out = (norm_m1.get_output(train=True) - np.ones(10)) / 1.\n-            self.assertAlmostEqual(out.mean().eval(), 0.0)\n+            self.assertAlmostEqual(K.eval(K.mean(out)), 0.0)\n             if inp.std() > 0.:\n-                self.assertAlmostEqual(out.std().eval(), 1.0, places=2)\n+                self.assertAlmostEqual(K.eval(K.std(out)), 1.0, places=2)\n             else:\n-                self.assertAlmostEqual(out.std().eval(), 0.0, places=2)\n+                self.assertAlmostEqual(K.eval(K.std(out)), 0.0, places=2)\n \n         assert_allclose(norm_m1.gamma.eval(), np.ones(10))\n         assert_allclose(norm_m1.beta.eval(), np.ones(10))\n \n     def test_config(self):\n-        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1, epsilon=0.1, momentum=0.9)\n+        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n+                                                epsilon=0.1, momentum=0.9)\n         conf = norm.get_config()\n-        conf_target = {\"input_shape\": (10, 10), \"name\": normalization.BatchNormalization.__name__,\n+        conf_target = {\"input_shape\": (10, 10),\n+                       \"name\": normalization.BatchNormalization.__name__,\n                        \"epsilon\": 0.1, \"mode\": 1, \"momentum\": 0.9}\n         self.assertDictEqual(conf, conf_target)\n \n     def test_save_weights(self):\n-        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1, epsilon=0.1)\n+        norm = normalization.BatchNormalization(input_shape=(10, 10), mode=1,\n+                                                epsilon=0.1)\n         weights = norm.get_weights()\n         assert(len(weights) == 4)\n         norm.set_weights(weights)\n\n@@ -42,6 +42,14 @@ class TestGraph(unittest.TestCase):\n         print(loss)\n         assert(loss < 2.5)\n \n+        # test validation split\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            validation_split=0.2, nb_epoch=1)\n+        # test validation data\n+        history = graph.fit({'input1': X_train, 'output1': y_train},\n+                            validation_data={'input1': X_train, 'output1': y_train},\n+                            nb_epoch=1)\n+\n     def test_1o_1i_2(self):\n         print('test a more complex non-sequential graph with 1 input and 1 output')\n         graph = Graph()\n\n@@ -5,7 +5,7 @@ import unittest\n from keras.models import Sequential, weighted_objective\n from keras.layers.core import TimeDistributedDense, Masking\n from keras import objectives\n-import theano\n+from keras import backend as K\n \n \n class TestLossMasking(unittest.TestCase):\n@@ -22,8 +22,7 @@ class TestLossMasking(unittest.TestCase):\n         assert loss == 285.\n \n     def test_loss_masking_time(self):\n-        theano.config.mode = 'FAST_COMPILE'\n-        weighted_loss = weighted_objective(objectives.get('categorical_crossentropy'))\n+        weighted_loss = weighted_objective(objectives.get('mae'))\n         shape = (3, 4, 2)\n         X = np.arange(24).reshape(shape)\n         Y = 2 * X\n@@ -33,9 +32,9 @@ class TestLossMasking(unittest.TestCase):\n         mask = np.ones((3, 4))\n         mask[1, 0] = 0\n \n-        out = weighted_loss(X, Y, weights, mask).eval()\n+        out = K.eval(weighted_loss(X, Y, weights, mask))\n         weights[0, 0] = 1e-9  # so that nonzero() doesn't remove this weight\n-        out2 = weighted_loss(X, Y, weights, mask).eval()\n+        out2 = K.eval(weighted_loss(X, Y, weights, mask))\n         print(out)\n         print(out2)\n         assert abs(out - out2) < 1e-8\n\n@@ -12,7 +12,7 @@ import unittest\n \n nb_classes = 10\n batch_size = 128\n-nb_epoch = 8\n+nb_epoch = 10\n weighted_class = 9\n standard_weight = 1\n high_weight = 5\n@@ -96,12 +96,12 @@ def _test_weights_graph(model, class_weight=None, sample_weight=None):\n \n class TestLossWeighting(unittest.TestCase):\n     def test_sequential(self):\n-        for loss in ['mae', 'mse', 'categorical_crossentropy']:\n+        for loss in ['mae', 'mse']:\n             print('loss:', loss)\n             print('sequential')\n             # no weights: reference point\n             model = create_sequential_model()\n-            model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+            model.compile(loss=loss, optimizer='rmsprop')\n             standard_score = _test_weights_sequential(model)\n             # test class_weight\n             model = create_sequential_model()\n@@ -117,22 +117,22 @@ class TestLossWeighting(unittest.TestCase):\n             self.assertTrue(score < standard_score)\n \n     def test_graph(self):\n-        for loss in ['mae', 'mse', 'categorical_crossentropy']:\n+        for loss in ['mae', 'mse']:\n             print('loss:', loss)\n             print('graph')\n             # no weights: reference point\n             model = create_graph_model()\n-            model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+            model.compile(loss={'output': loss}, optimizer='rmsprop')\n             standard_score = _test_weights_graph(model)\n             # test class_weight\n             model = create_graph_model()\n-            model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+            model.compile(loss={'output': loss}, optimizer='rmsprop')\n             score = _test_weights_graph(model, class_weight=class_weight)\n             print('score:', score, ' vs.', standard_score)\n             self.assertTrue(score < standard_score)\n             # test sample_weight\n             model = create_graph_model()\n-            model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='rmsprop')\n+            model.compile(loss={'output': loss}, optimizer='rmsprop')\n             score = _test_weights_graph(model, sample_weight=sample_weight)\n             print('score:', score, ' vs.', standard_score)\n             self.assertTrue(score < standard_score)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d444b9190f3d44a6b9412aaac64d6ef44db86772", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 2 | Churn Cumulative: 325 | Contributors (this commit): 10 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,7 @@ import theano\n import copy\n \n from ..layers.advanced_activations import LeakyReLU, PReLU\n-from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer, AutoEncoder, Masking, Permute\n+from ..layers.core import Dense, Merge, Dropout, Activation, Reshape, Flatten, RepeatVector, Layer, AutoEncoder, Masking, Permute, Lambda, MaskedLambda, LambdaMerge\n from ..layers.core import ActivityRegularization, TimeDistributedDense, TimeDistributedMerge, AutoEncoder, MaxoutDense\n from ..layers.convolutional import Convolution1D, Convolution2D, MaxPooling1D, MaxPooling2D, ZeroPadding2D\n from ..layers.embeddings import Embedding, WordContextProduct\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2431764fed27199fe6a0751f44312a0b0c04f889", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n import pydot\n # old pydot will not work with python3, must use one\n # that works with python3 such as pydot2 or pydot\n-\n+from keras.models import Sequential, Graph\n \n def plot(model, to_file='model.png'):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
