{"custom_id": "scrapy#da42e8f124362a5087c50bca7f76dcc573e8194a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 52 | Files Changed: 17 | Hunks: 51 | Methods Changed: 42 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 137 | Churn Cumulative: 16108 | Contributors (this commit): 122 | Commits (past 90d): 34 | Contributors (cumulative): 345 | DMM Complexity: 0.75\n\nDIFF:\n@@ -27,7 +27,10 @@ class DownloadHandlers:\n         self._handlers: Dict[str, Any] = {}  # stores instanced handlers for schemes\n         self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n         handlers: Dict[str, Union[str, Callable]] = without_none_values(\n-            crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\")\n+            cast(\n+                Dict[str, Union[str, Callable]],\n+                crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n+            )\n         )\n         for scheme, clspath in handlers.items():\n             self._schemes[scheme] = clspath\n\n@@ -3,7 +3,7 @@ import itertools\n import logging\n from collections import deque\n from ipaddress import IPv4Address, IPv6Address\n-from typing import Dict, List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -115,7 +115,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n         # Some meta data of this connection\n         # initialized when connection is successfully made\n-        self.metadata: Dict = {\n+        self.metadata: Dict[str, Any] = {\n             # Peer certificate instance\n             \"certificate\": None,\n             # Address of the server we are connected to which\n\n@@ -110,7 +110,7 @@ class Stream:\n \n         # Metadata of an HTTP/2 connection stream\n         # initialized when stream is instantiated\n-        self.metadata: Dict = {\n+        self.metadata: Dict[str, Any] = {\n             \"request_content_length\": (\n                 0 if self._request.body is None else len(self._request.body)\n             ),\n@@ -131,7 +131,7 @@ class Stream:\n         # Private variable used to build the response\n         # this response is then converted to appropriate Response class\n         # passed to the response deferred callback\n-        self._response: Dict = {\n+        self._response: Dict[str, Any] = {\n             # Data received frame by frame from the server is appended\n             # and passed to the response Deferred when completely received.\n             \"body\": BytesIO(),\n\n@@ -694,7 +694,9 @@ class FeedExporter:\n         self.slots = slots\n \n     def _load_components(self, setting_prefix: str) -> Dict[str, Any]:\n-        conf = without_none_values(self.settings.getwithbase(setting_prefix))\n+        conf = without_none_values(\n+            cast(Dict[str, str], self.settings.getwithbase(setting_prefix))\n+        )\n         d = {}\n         for k, v in conf.items():\n             try:\n\n@@ -97,7 +97,7 @@ class Request(object_ref):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[dict, List[dict]]] = None,\n+        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: str = \"utf-8\",\n         priority: int = 0,\n@@ -123,7 +123,7 @@ class Request(object_ref):\n         self.callback: Optional[Callable] = callback\n         self.errback: Optional[Callable] = errback\n \n-        self.cookies: Union[dict, List[dict]] = cookies or {}\n+        self.cookies: Union[Dict[str, str], List[Dict[str, str]]] = cookies or {}\n         self.headers: Headers = Headers(headers or {}, encoding=encoding)\n         self.dont_filter: bool = dont_filter\n \n\n@@ -7,7 +7,17 @@ See documentation in docs/topics/request-response.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n from lxml.html import FormElement  # nosec\n@@ -26,8 +36,9 @@ if TYPE_CHECKING:\n     from typing_extensions import Self\n \n \n-FormdataKVType = Tuple[str, Union[str, Iterable[str]]]\n-FormdataType = Optional[Union[dict, List[FormdataKVType]]]\n+FormdataVType = Union[str, Iterable[str]]\n+FormdataKVType = Tuple[str, FormdataVType]\n+FormdataType = Optional[Union[Dict[str, FormdataVType], List[FormdataKVType]]]\n \n \n class FormRequest(Request):\n@@ -62,7 +73,7 @@ class FormRequest(Request):\n         formid: Optional[str] = None,\n         formnumber: int = 0,\n         formdata: FormdataType = None,\n-        clickdata: Optional[dict] = None,\n+        clickdata: Optional[Dict[str, Union[str, int]]] = None,\n         dont_click: bool = False,\n         formxpath: Optional[str] = None,\n         formcss: Optional[str] = None,\n@@ -156,7 +167,7 @@ def _get_inputs(\n     form: FormElement,\n     formdata: FormdataType,\n     dont_click: bool,\n-    clickdata: Optional[dict],\n+    clickdata: Optional[Dict[str, Union[str, int]]],\n ) -> List[FormdataKVType]:\n     \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n     try:\n@@ -186,10 +197,8 @@ def _get_inputs(\n         if clickable and clickable[0] not in formdata and not clickable[0] is None:\n             values.append(clickable)\n \n-    if isinstance(formdata, dict):\n-        formdata = formdata.items()  # type: ignore[assignment]\n-\n-    values.extend((k, v) for k, v in formdata if v is not None)\n+    formdata_items = formdata.items() if isinstance(formdata, dict) else formdata\n+    values.extend((k, v) for k, v in formdata_items if v is not None)\n     return values\n \n \n@@ -216,7 +225,7 @@ def _select_value(\n \n \n def _get_clickable(\n-    clickdata: Optional[dict], form: FormElement\n+    clickdata: Optional[Dict[str, Union[str, int]]], form: FormElement\n ) -> Optional[Tuple[str, str]]:\n     \"\"\"\n     Returns the clickable element specified in clickdata,\n@@ -243,6 +252,7 @@ def _get_clickable(\n     # because that uniquely identifies the element\n     nr = clickdata.get(\"nr\", None)\n     if nr is not None:\n+        assert isinstance(nr, int)\n         try:\n             el = list(form.inputs)[nr]\n         except IndexError:\n\n@@ -8,7 +8,7 @@ See documentation in docs/topics/request-response.rst\n import copy\n import json\n import warnings\n-from typing import Any, Optional, Tuple\n+from typing import Any, Dict, Optional, Tuple\n \n from scrapy.http.request import Request\n \n@@ -17,15 +17,15 @@ class JsonRequest(Request):\n     attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n \n     def __init__(\n-        self, *args: Any, dumps_kwargs: Optional[dict] = None, **kwargs: Any\n+        self, *args: Any, dumps_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n     ) -> None:\n         dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n         dumps_kwargs.setdefault(\"sort_keys\", True)\n-        self._dumps_kwargs = dumps_kwargs\n+        self._dumps_kwargs: Dict[str, Any] = dumps_kwargs\n \n         body_passed = kwargs.get(\"body\", None) is not None\n-        data = kwargs.pop(\"data\", None)\n-        data_passed = data is not None\n+        data: Any = kwargs.pop(\"data\", None)\n+        data_passed: bool = data is not None\n \n         if body_passed and data_passed:\n             warnings.warn(\"Both body and data passed. data will be ignored\")\n@@ -41,13 +41,13 @@ class JsonRequest(Request):\n         )\n \n     @property\n-    def dumps_kwargs(self) -> dict:\n+    def dumps_kwargs(self) -> Dict[str, Any]:\n         return self._dumps_kwargs\n \n     def replace(self, *args: Any, **kwargs: Any) -> Request:\n         body_passed = kwargs.get(\"body\", None) is not None\n-        data = kwargs.pop(\"data\", None)\n-        data_passed = data is not None\n+        data: Any = kwargs.pop(\"data\", None)\n+        data_passed: bool = data is not None\n \n         if body_passed and data_passed:\n             warnings.warn(\"Both body and data passed. data will be ignored\")\n@@ -56,6 +56,6 @@ class JsonRequest(Request):\n \n         return super().replace(*args, **kwargs)\n \n-    def _dumps(self, data: dict) -> str:\n+    def _dumps(self, data: Any) -> str:\n         \"\"\"Convert to JSON\"\"\"\n         return json.dumps(data, **self._dumps_kwargs)\n\n@@ -181,7 +181,7 @@ class Response(object_ref):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[dict, List[dict]]] = None,\n+        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n@@ -234,7 +234,7 @@ class Response(object_ref):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[dict, List[dict]]] = None,\n+        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n\n@@ -183,7 +183,7 @@ class TextResponse(Response):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[dict, List[dict]]] = None,\n+        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = None,\n         priority: int = 0,\n@@ -236,7 +236,7 @@ class TextResponse(Response):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[dict, List[dict]]] = None,\n+        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = None,\n         priority: int = 0,\n\n@@ -27,7 +27,7 @@ if TYPE_CHECKING:\n     from typing_extensions import Self\n \n \n-class Field(dict):\n+class Field(Dict[str, Any]):\n     \"\"\"Container of field metadata\"\"\"\n \n \n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n import os\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, TypedDict, Union\n \n from twisted.python.failure import Failure\n \n@@ -26,6 +26,12 @@ DOWNLOADERRORMSG_SHORT = \"Error downloading %(request)s\"\n DOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n \n \n+class LogFormatterResult(TypedDict):\n+    level: int\n+    msg: str\n+    args: Union[Dict[str, Any], Tuple[Any, ...]]\n+\n+\n class LogFormatter:\n     \"\"\"Class for generating log messages for different actions.\n \n@@ -64,7 +70,9 @@ class LogFormatter:\n                     }\n     \"\"\"\n \n-    def crawled(self, request: Request, response: Response, spider: Spider) -> dict:\n+    def crawled(\n+        self, request: Request, response: Response, spider: Spider\n+    ) -> LogFormatterResult:\n         \"\"\"Logs a message when the crawler finds a webpage.\"\"\"\n         request_flags = f\" {str(request.flags)}\" if request.flags else \"\"\n         response_flags = f\" {str(response.flags)}\" if response.flags else \"\"\n@@ -84,7 +92,7 @@ class LogFormatter:\n \n     def scraped(\n         self, item: Any, response: Union[Response, Failure], spider: Spider\n-    ) -> dict:\n+    ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is scraped by a spider.\"\"\"\n         src: Any\n         if isinstance(response, Failure):\n@@ -102,7 +110,7 @@ class LogFormatter:\n \n     def dropped(\n         self, item: Any, exception: BaseException, response: Response, spider: Spider\n-    ) -> dict:\n+    ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n         return {\n             \"level\": logging.WARNING,\n@@ -115,7 +123,7 @@ class LogFormatter:\n \n     def item_error(\n         self, item: Any, exception: BaseException, response: Response, spider: Spider\n-    ) -> dict:\n+    ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item causes an error while it is passing\n         through the item pipeline.\n \n@@ -135,7 +143,7 @@ class LogFormatter:\n         request: Request,\n         response: Union[Response, Failure],\n         spider: Spider,\n-    ) -> dict:\n+    ) -> LogFormatterResult:\n         \"\"\"Logs an error message from a spider.\n \n         .. versionadded:: 2.0\n@@ -155,7 +163,7 @@ class LogFormatter:\n         request: Request,\n         spider: Spider,\n         errmsg: Optional[str] = None,\n-    ) -> dict:\n+    ) -> LogFormatterResult:\n         \"\"\"Logs a download error message from a spider (typically coming from\n         the engine).\n \n\n@@ -411,7 +411,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         \"\"\"\n         self._assert_mutability()\n         if isinstance(values, str):\n-            values = cast(dict, json.loads(values))\n+            values = cast(Dict[_SettingsKeyT, Any], json.loads(values))\n         if values is not None:\n             if isinstance(values, BaseSettings):\n                 for name, value in values.items():\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Union, cast\n+from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, Union, cast\n \n from twisted.internet.defer import Deferred\n \n@@ -24,7 +24,7 @@ if TYPE_CHECKING:\n     from typing_extensions import Concatenate, Self\n \n     from scrapy.crawler import Crawler\n-    from scrapy.settings import BaseSettings\n+    from scrapy.settings import BaseSettings, _SettingsKeyT\n     from scrapy.utils.log import SpiderLoggerAdapter\n \n     CallbackT = Callable[Concatenate[Response, ...], Any]\n@@ -36,7 +36,7 @@ class Spider(object_ref):\n     \"\"\"\n \n     name: str\n-    custom_settings: Optional[dict] = None\n+    custom_settings: Optional[Dict[_SettingsKeyT, Any]] = None\n \n     def __init__(self, name: Optional[str] = None, **kwargs: Any):\n         if name is not None:\n\n@@ -16,6 +16,7 @@ from typing import (\n     MutableMapping,\n     Optional,\n     Union,\n+    cast,\n )\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n@@ -173,7 +174,7 @@ def feed_process_params_from_cli(\n     suitable to be used as the FEEDS setting.\n     \"\"\"\n     valid_output_formats: Iterable[str] = without_none_values(\n-        settings.getwithbase(\"FEED_EXPORTERS\")\n+        cast(Dict[str, str], settings.getwithbase(\"FEED_EXPORTERS\"))\n     ).keys()\n \n     def check_valid_format(output_format: str) -> None:\n\n@@ -7,6 +7,7 @@ from types import TracebackType\n from typing import (\n     TYPE_CHECKING,\n     Any,\n+    Dict,\n     List,\n     MutableMapping,\n     Optional,\n@@ -20,7 +21,8 @@ from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n \n import scrapy\n-from scrapy.settings import Settings\n+from scrapy.logformatter import LogFormatterResult\n+from scrapy.settings import Settings, _SettingsKeyT\n from scrapy.utils.versions import scrapy_components_versions\n \n if TYPE_CHECKING:\n@@ -86,7 +88,8 @@ DEFAULT_LOGGING = {\n \n \n def configure_logging(\n-    settings: Union[Settings, dict, None] = None, install_root_handler: bool = True\n+    settings: Union[Settings, Dict[_SettingsKeyT, Any], None] = None,\n+    install_root_handler: bool = True,\n ) -> None:\n     \"\"\"\n     Initialize logging defaults for Scrapy.\n@@ -234,7 +237,9 @@ class LogCounterHandler(logging.Handler):\n         self.crawler.stats.inc_value(sname)\n \n \n-def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n+def logformatter_adapter(\n+    logkws: LogFormatterResult,\n+) -> Tuple[int, str, Union[Dict[str, Any], Tuple[Any, ...]]]:\n     \"\"\"\n     Helper that takes the dictionary output from the methods in LogFormatter\n     and adapts it into a tuple of positional arguments for logger.log calls,\n@@ -245,7 +250,7 @@ def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n     message = logkws.get(\"msg\") or \"\"\n     # NOTE: This also handles 'args' being an empty dict, that case doesn't\n     # play well in logger.log calls\n-    args = logkws if not logkws.get(\"args\") else logkws[\"args\"]\n+    args = cast(Dict[str, Any], logkws) if not logkws.get(\"args\") else logkws[\"args\"]\n \n     return (level, message, args)\n \n\n@@ -42,6 +42,8 @@ if TYPE_CHECKING:\n     _P = ParamSpec(\"_P\")\n \n _T = TypeVar(\"_T\")\n+_KT = TypeVar(\"_KT\")\n+_VT = TypeVar(\"_VT\")\n \n \n def flatten(x: Iterable) -> list:\n@@ -303,14 +305,16 @@ def equal_attributes(\n \n \n @overload\n-def without_none_values(iterable: Mapping) -> dict: ...\n+def without_none_values(iterable: Mapping[_KT, _VT]) -> Dict[_KT, _VT]: ...\n \n \n @overload\n-def without_none_values(iterable: Iterable) -> Iterable: ...\n+def without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n \n \n-def without_none_values(iterable: Union[Mapping, Iterable]) -> Union[dict, Iterable]:\n+def without_none_values(\n+    iterable: Union[Mapping[_KT, _VT], Iterable[_KT]]\n+) -> Union[Dict[_KT, _VT], Iterable[_KT]]:\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n     If ``iterable`` is a mapping, return a dictionary where all pairs that have\n\n@@ -197,7 +197,7 @@ def referer_str(request: Request) -> Optional[str]:\n     return to_unicode(referrer, errors=\"replace\")\n \n \n-def request_from_dict(d: dict, *, spider: Optional[Spider] = None) -> Request:\n+def request_from_dict(d: Dict[str, Any], *, spider: Optional[Spider] = None) -> Request:\n     \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n \n     If a spider is given, it will try to resolve the callbacks looking at the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#98c755e5fbc005083a5fde810476f2de610bf912", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 13 | Methods Changed: 15 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 32 | Churn Cumulative: 2601 | Contributors (this commit): 43 | Commits (past 90d): 10 | Contributors (cumulative): 62 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import json\n import logging\n from abc import abstractmethod\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional, Type, cast\n+from typing import TYPE_CHECKING, Any, List, Optional, Type, cast\n \n from twisted.internet.defer import Deferred\n \n@@ -362,13 +362,13 @@ class Scheduler(BaseScheduler):\n             return str(dqdir)\n         return None\n \n-    def _read_dqs_state(self, dqdir: str) -> list:\n+    def _read_dqs_state(self, dqdir: str) -> List[int]:\n         path = Path(dqdir, \"active.json\")\n         if not path.exists():\n             return []\n         with path.open(encoding=\"utf-8\") as f:\n-            return cast(list, json.load(f))\n+            return cast(List[int], json.load(f))\n \n-    def _write_dqs_state(self, dqdir: str, state: list) -> None:\n+    def _write_dqs_state(self, dqdir: str, state: List[int]) -> None:\n         with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n             json.dump(state, f)\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Dict, Union\n+from typing import TYPE_CHECKING, Dict, List, Tuple, Union\n \n from twisted.web import http\n \n@@ -17,7 +17,9 @@ if TYPE_CHECKING:\n     from typing_extensions import Self\n \n \n-def get_header_size(headers: Dict[str, Union[list, tuple]]) -> int:\n+def get_header_size(\n+    headers: Dict[str, Union[List[Union[str, bytes]], Tuple[Union[str, bytes], ...]]]\n+) -> int:\n     size = 0\n     for key, value in headers.items():\n         if isinstance(value, (list, tuple)):\n\n@@ -1,14 +1,18 @@\n-from typing import AsyncGenerator, AsyncIterable, Iterable, Union\n+from typing import AsyncGenerator, AsyncIterable, Iterable, List, TypeVar, Union\n+\n+_T = TypeVar(\"_T\")\n \n \n-async def collect_asyncgen(result: AsyncIterable) -> list:\n+async def collect_asyncgen(result: AsyncIterable[_T]) -> List[_T]:\n     results = []\n     async for x in result:\n         results.append(x)\n     return results\n \n \n-async def as_async_generator(it: Union[Iterable, AsyncIterable]) -> AsyncGenerator:\n+async def as_async_generator(\n+    it: Union[Iterable[_T], AsyncIterable[_T]]\n+) -> AsyncGenerator[_T, None]:\n     \"\"\"Wraps an iterable (sync or async) into an async generator.\"\"\"\n     if isinstance(it, AsyncIterable):\n         async for r in it:\n\n@@ -46,7 +46,7 @@ _KT = TypeVar(\"_KT\")\n _VT = TypeVar(\"_VT\")\n \n \n-def flatten(x: Iterable) -> list:\n+def flatten(x: Iterable[Any]) -> List[Any]:\n     \"\"\"flatten(sequence) -> list\n \n     Returns a single, flat list which contains all elements retrieved\n@@ -66,7 +66,7 @@ def flatten(x: Iterable) -> list:\n     return list(iflatten(x))\n \n \n-def iflatten(x: Iterable) -> Iterable:\n+def iflatten(x: Iterable[Any]) -> Iterable[Any]:\n     \"\"\"iflatten(sequence) -> iterator\n \n     Similar to ``.flatten()``, but returns iterator instead\"\"\"\n@@ -101,10 +101,10 @@ def is_listlike(x: Any) -> bool:\n     return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n \n \n-def unique(list_: Iterable, key: Callable[[Any], Any] = lambda x: x) -> list:\n+def unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> List[_T]:\n     \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n     seen = set()\n-    result = []\n+    result: List[_T] = []\n     for item in list_:\n         seenkey = key(item)\n         if seenkey in seen:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4164e63725dc19bc8585abbfb0e5009f8eceefcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 381 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,8 @@ from scrapy.http.request import Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n \n+ConnectionKeyT = Tuple[bytes, bytes, int]\n+\n \n class H2ConnectionPool:\n     def __init__(self, reactor: ReactorBase, settings: Settings) -> None:\n@@ -28,13 +30,13 @@ class H2ConnectionPool:\n \n         # Store a dictionary which is used to get the respective\n         # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)\n-        self._connections: Dict[Tuple, H2ClientProtocol] = {}\n+        self._connections: Dict[ConnectionKeyT, H2ClientProtocol] = {}\n \n         # Save all requests that arrive before the connection is established\n-        self._pending_requests: Dict[Tuple, Deque[Deferred]] = {}\n+        self._pending_requests: Dict[ConnectionKeyT, Deque[Deferred]] = {}\n \n     def get_connection(\n-        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint\n+        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n     ) -> Deferred:\n         if key in self._pending_requests:\n             # Received a request while connecting to remote\n@@ -54,7 +56,7 @@ class H2ConnectionPool:\n         return self._new_connection(key, uri, endpoint)\n \n     def _new_connection(\n-        self, key: Tuple, uri: URI, endpoint: HostnameEndpoint\n+        self, key: ConnectionKeyT, uri: URI, endpoint: HostnameEndpoint\n     ) -> Deferred:\n         self._pending_requests[key] = deque()\n \n@@ -69,7 +71,9 @@ class H2ConnectionPool:\n         self._pending_requests[key].append(d)\n         return d\n \n-    def put_connection(self, conn: H2ClientProtocol, key: Tuple) -> H2ClientProtocol:\n+    def put_connection(\n+        self, conn: H2ClientProtocol, key: ConnectionKeyT\n+    ) -> H2ClientProtocol:\n         self._connections[key] = conn\n \n         # Now as we have established a proper HTTP/2 connection\n@@ -81,7 +85,9 @@ class H2ConnectionPool:\n \n         return conn\n \n-    def _remove_connection(self, errors: List[BaseException], key: Tuple) -> None:\n+    def _remove_connection(\n+        self, errors: List[BaseException], key: ConnectionKeyT\n+    ) -> None:\n         self._connections.pop(key)\n \n         # Call the errback of all the pending requests for this connection\n@@ -122,7 +128,7 @@ class H2Agent:\n     def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n         return self.endpoint_factory.endpointForURI(uri)\n \n-    def get_key(self, uri: URI) -> Tuple:\n+    def get_key(self, uri: URI) -> ConnectionKeyT:\n         \"\"\"\n         Arguments:\n             uri - URI obtained directly from request URL\n@@ -164,6 +170,6 @@ class ScrapyProxyH2Agent(H2Agent):\n     def get_endpoint(self, uri: URI) -> HostnameEndpoint:\n         return self.endpoint_factory.endpointForURI(self._proxy_uri)\n \n-    def get_key(self, uri: URI) -> Tuple:\n+    def get_key(self, uri: URI) -> ConnectionKeyT:\n         \"\"\"We use the proxy uri instead of uri obtained from request url\"\"\"\n-        return \"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n+        return b\"http-proxy\", self._proxy_uri.host, self._proxy_uri.port\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70c56faf4847406de6eb3594758c5531610757e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 8 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 3672 | Contributors (this commit): 77 | Commits (past 90d): 13 | Contributors (cumulative): 121 | DMM Complexity: 1.0\n\nDIFF:\n@@ -315,7 +315,9 @@ class FilesystemCacheStorage:\n         self.expiration_secs: int = settings.getint(\"HTTPCACHE_EXPIRATION_SECS\")\n         self.use_gzip: bool = settings.getbool(\"HTTPCACHE_GZIP\")\n         # https://github.com/python/mypy/issues/10740\n-        self._open: Callable[Concatenate[Union[str, os.PathLike], str, ...], IO] = (\n+        self._open: Callable[\n+            Concatenate[Union[str, os.PathLike], str, ...], IO[bytes]\n+        ] = (\n             gzip.open if self.use_gzip else open  # type: ignore[assignment]\n         )\n \n@@ -368,11 +370,12 @@ class FilesystemCacheStorage:\n         with self._open(rpath / \"pickled_meta\", \"wb\") as f:\n             pickle.dump(metadata, f, protocol=4)\n         with self._open(rpath / \"response_headers\", \"wb\") as f:\n-            f.write(headers_dict_to_raw(response.headers))\n+            # headers_dict_to_raw() needs a better type hint\n+            f.write(cast(bytes, headers_dict_to_raw(response.headers)))\n         with self._open(rpath / \"response_body\", \"wb\") as f:\n             f.write(response.body)\n         with self._open(rpath / \"request_headers\", \"wb\") as f:\n-            f.write(headers_dict_to_raw(request.headers))\n+            f.write(cast(bytes, headers_dict_to_raw(request.headers)))\n         with self._open(rpath / \"request_body\", \"wb\") as f:\n             f.write(request.body)\n \n\n@@ -97,7 +97,7 @@ class MailSender:\n         subject: str,\n         body: str,\n         cc: Union[str, List[str], None] = None,\n-        attachs: Sequence[Tuple[str, str, IO]] = (),\n+        attachs: Sequence[Tuple[str, str, IO[Any]]] = (),\n         mimetype: str = \"text/plain\",\n         charset: Optional[str] = None,\n         _callback: Optional[Callable[..., None]] = None,\n@@ -214,7 +214,7 @@ class MailSender:\n         return d\n \n     def _create_sender_factory(\n-        self, to_addrs: List[str], msg: IO, d: Deferred\n+        self, to_addrs: List[str], msg: IO[bytes], d: Deferred\n     ) -> ESMTPSenderFactory:\n         from twisted.mail.smtp import ESMTPSenderFactory\n \n\n@@ -47,7 +47,7 @@ def _to_string(path: Union[str, PathLike]) -> str:\n     return str(path)  # convert a Path object to string\n \n \n-def _md5sum(file: IO) -> str:\n+def _md5sum(file: IO[bytes]) -> str:\n     \"\"\"Calculate the md5 checksum of a file-like object without reading its\n     whole content in memory.\n \n\n@@ -21,7 +21,7 @@ def ftp_makedirs_cwd(ftp: FTP, path: str, first_call: bool = True) -> None:\n def ftp_store_file(\n     *,\n     path: str,\n-    file: IO,\n+    file: IO[bytes],\n     host: str,\n     port: int,\n     username: str,\n\n@@ -111,7 +111,7 @@ def walk_modules(path: str) -> List[ModuleType]:\n     return mods\n \n \n-def md5sum(file: IO) -> str:\n+def md5sum(file: IO[bytes]) -> str:\n     \"\"\"Calculate the md5 checksum of a file-like object without reading its\n     whole content in memory.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#751c91e614b91827dc68cd462b907c4b9d03f071", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 17 | Files Changed: 5 | Hunks: 14 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 7332 | Contributors (this commit): 55 | Commits (past 90d): 15 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,8 @@ For more information see docs/topics/architecture.rst\n \n \"\"\"\n \n+from __future__ import annotations\n+\n import logging\n from time import time\n from typing import (\n@@ -51,15 +53,15 @@ class Slot:\n         self,\n         start_requests: Iterable[Request],\n         close_if_idle: bool,\n-        nextcall: CallLaterOnce,\n-        scheduler: \"BaseScheduler\",\n+        nextcall: CallLaterOnce[None],\n+        scheduler: BaseScheduler,\n     ) -> None:\n         self.closing: Optional[Deferred] = None\n         self.inprogress: Set[Request] = set()\n         self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n         self.close_if_idle: bool = close_if_idle\n-        self.nextcall: CallLaterOnce = nextcall\n-        self.scheduler: \"BaseScheduler\" = scheduler\n+        self.nextcall: CallLaterOnce[None] = nextcall\n+        self.scheduler: BaseScheduler = scheduler\n         self.heartbeat: LoopingCall = LoopingCall(nextcall.schedule)\n \n     def add_request(self, request: Request) -> None:\n@@ -84,8 +86,8 @@ class Slot:\n \n \n class ExecutionEngine:\n-    def __init__(self, crawler: \"Crawler\", spider_closed_callback: Callable) -> None:\n-        self.crawler: \"Crawler\" = crawler\n+    def __init__(self, crawler: Crawler, spider_closed_callback: Callable) -> None:\n+        self.crawler: Crawler = crawler\n         self.settings: Settings = crawler.settings\n         self.signals: SignalManager = crawler.signals\n         assert crawler.logformatter\n@@ -94,7 +96,7 @@ class ExecutionEngine:\n         self.spider: Optional[Spider] = None\n         self.running: bool = False\n         self.paused: bool = False\n-        self.scheduler_cls: Type[\"BaseScheduler\"] = self._get_scheduler_class(\n+        self.scheduler_cls: Type[BaseScheduler] = self._get_scheduler_class(\n             crawler.settings\n         )\n         downloader_cls: Type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n@@ -103,10 +105,10 @@ class ExecutionEngine:\n         self._spider_closed_callback: Callable = spider_closed_callback\n         self.start_time: Optional[float] = None\n \n-    def _get_scheduler_class(self, settings: BaseSettings) -> Type[\"BaseScheduler\"]:\n+    def _get_scheduler_class(self, settings: BaseSettings) -> Type[BaseScheduler]:\n         from scrapy.core.scheduler import BaseScheduler\n \n-        scheduler_cls: Type = load_object(settings[\"SCHEDULER\"])\n+        scheduler_cls: Type[BaseScheduler] = load_object(settings[\"SCHEDULER\"])\n         if not issubclass(scheduler_cls, BaseScheduler):\n             raise TypeError(\n                 f\"The provided scheduler class ({settings['SCHEDULER']})\"\n\n@@ -3,7 +3,7 @@ import itertools\n import logging\n from collections import deque\n from ipaddress import IPv4Address, IPv6Address\n-from typing import Any, Dict, List, Optional, Union\n+from typing import Any, Deque, Dict, List, Optional, Union\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -107,7 +107,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n \n         # If requests are received before connection is made we keep\n         # all requests in a pool and send them as the connection is made\n-        self._pending_request_stream_pool: deque = deque()\n+        self._pending_request_stream_pool: Deque[Stream] = deque()\n \n         # Save an instance of errors raised which lead to losing the connection\n         # We pass these instances to the streams ResponseFailed() failure\n\n@@ -196,8 +196,8 @@ class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n class SequenceExclude:\n     \"\"\"Object to test if an item is NOT within some sequence.\"\"\"\n \n-    def __init__(self, seq: Sequence):\n-        self.seq: Sequence = seq\n+    def __init__(self, seq: Sequence[Any]):\n+        self.seq: Sequence[Any] = seq\n \n     def __contains__(self, item: Any) -> bool:\n         return item not in self.seq\n\n@@ -148,7 +148,7 @@ def to_bytes(\n \n \n def re_rsearch(\n-    pattern: Union[str, Pattern], text: str, chunk_size: int = 1024\n+    pattern: Union[str, Pattern[str]], text: str, chunk_size: int = 1024\n ) -> Optional[Tuple[int, int]]:\n     \"\"\"\n     This function does a reverse search in a text using a regular expression\n\n@@ -7,7 +7,7 @@ import os\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from typing import Any, Coroutine, Dict, List, Optional, Tuple, Type\n+from typing import Any, Awaitable, Dict, List, Optional, Tuple, Type, TypeVar\n from unittest import TestCase, mock\n \n from twisted.internet.defer import Deferred\n@@ -17,6 +17,8 @@ from scrapy import Spider\n from scrapy.crawler import Crawler\n from scrapy.utils.boto import is_botocore_available\n \n+_T = TypeVar(\"_T\")\n+\n \n def assert_gcs_environ() -> None:\n     if \"GCS_PROJECT_ID\" not in os.environ:\n@@ -118,8 +120,8 @@ def assert_samelines(\n     testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n \n \n-def get_from_asyncio_queue(value: Any) -> Coroutine:\n-    q: asyncio.Queue = asyncio.Queue()\n+def get_from_asyncio_queue(value: _T) -> Awaitable[_T]:\n+    q: asyncio.Queue[_T] = asyncio.Queue()\n     getter = q.get()\n     q.put_nowait(value)\n     return getter\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#859a77ee4243f17f338072e45785383f12516308", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 26 | Files Changed: 4 | Hunks: 17 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 57 | Churn Cumulative: 2962 | Contributors (this commit): 34 | Commits (past 90d): 10 | Contributors (cumulative): 71 | DMM Complexity: None\n\nDIFF:\n@@ -3,16 +3,7 @@ from __future__ import annotations\n import logging\n from collections import defaultdict\n from http.cookiejar import Cookie\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    DefaultDict,\n-    Dict,\n-    Iterable,\n-    Optional,\n-    Sequence,\n-    Union,\n-)\n+from typing import TYPE_CHECKING, Any, DefaultDict, Iterable, Optional, Sequence, Union\n \n from tldextract import TLDExtract\n \n@@ -21,6 +12,7 @@ from scrapy.crawler import Crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n+from scrapy.http.request import VerboseCookie\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n@@ -128,7 +120,7 @@ class CookiesMiddleware:\n                 msg = f\"Received cookies from: {response}\\n{cookies}\"\n                 logger.debug(msg, extra={\"spider\": spider})\n \n-    def _format_cookie(self, cookie: Dict[str, Any], request: Request) -> Optional[str]:\n+    def _format_cookie(self, cookie: VerboseCookie, request: Request) -> Optional[str]:\n         \"\"\"\n         Given a dict consisting of cookie components, return its string representation.\n         Decode from bytes if necessary.\n@@ -142,18 +134,19 @@ class CookiesMiddleware:\n                     logger.warning(msg)\n                     return None\n                 continue\n-            if isinstance(cookie[key], (bool, float, int, str)):\n-                decoded[key] = str(cookie[key])\n+            # https://github.com/python/mypy/issues/7178, https://github.com/python/mypy/issues/9168\n+            if isinstance(cookie[key], (bool, float, int, str)):  # type: ignore[literal-required]\n+                decoded[key] = str(cookie[key])  # type: ignore[literal-required]\n             else:\n                 try:\n-                    decoded[key] = cookie[key].decode(\"utf8\")\n+                    decoded[key] = cookie[key].decode(\"utf8\")  # type: ignore[literal-required]\n                 except UnicodeDecodeError:\n                     logger.warning(\n                         \"Non UTF-8 encoded cookie found in request %s: %s\",\n                         request,\n                         cookie,\n                     )\n-                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n+                    decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")  # type: ignore[literal-required]\n         for flag in (\"secure\",):\n             value = cookie.get(flag, _UNSET)\n             if value is _UNSET or not value:\n@@ -174,7 +167,7 @@ class CookiesMiddleware:\n         \"\"\"\n         if not request.cookies:\n             return []\n-        cookies: Iterable[Dict[str, Any]]\n+        cookies: Iterable[VerboseCookie]\n         if isinstance(request.cookies, dict):\n             cookies = tuple({\"name\": k, \"value\": v} for k, v in request.cookies.items())\n         else:\n\n@@ -20,6 +20,7 @@ from typing import (\n     NoReturn,\n     Optional,\n     Tuple,\n+    TypedDict,\n     Union,\n     cast,\n )\n@@ -34,8 +35,19 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n \n if TYPE_CHECKING:\n-    # typing.Self requires Python 3.11\n-    from typing_extensions import Self\n+    # typing.NotRequired and typing.Self require Python 3.11\n+    from typing_extensions import NotRequired, Self\n+\n+\n+class VerboseCookie(TypedDict):\n+    name: str\n+    value: str\n+    domain: NotRequired[str]\n+    path: NotRequired[str]\n+    secure: NotRequired[bool]\n+\n+\n+CookiesT = Union[Dict[str, str], List[VerboseCookie]]\n \n \n def NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n@@ -97,7 +109,7 @@ class Request(object_ref):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n+        cookies: Optional[CookiesT] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: str = \"utf-8\",\n         priority: int = 0,\n@@ -123,7 +135,7 @@ class Request(object_ref):\n         self.callback: Optional[Callable] = callback\n         self.errback: Optional[Callable] = errback\n \n-        self.cookies: Union[Dict[str, str], List[Dict[str, str]]] = cookies or {}\n+        self.cookies: CookiesT = cookies or {}\n         self.headers: Headers = Headers(headers or {}, encoding=encoding)\n         self.dont_filter: bool = dont_filter\n \n\n@@ -29,7 +29,7 @@ from twisted.internet.ssl import Certificate\n \n from scrapy.exceptions import NotSupported\n from scrapy.http.headers import Headers\n-from scrapy.http.request import Request\n+from scrapy.http.request import CookiesT, Request\n from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n \n@@ -181,7 +181,7 @@ class Response(object_ref):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n+        cookies: Optional[CookiesT] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n@@ -234,7 +234,7 @@ class Response(object_ref):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n+        cookies: Optional[CookiesT] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n\n@@ -36,7 +36,7 @@ from w3lib.encoding import (\n )\n from w3lib.html import strip_html5_whitespace\n \n-from scrapy.http import Request\n+from scrapy.http.request import CookiesT, Request\n from scrapy.http.response import Response\n from scrapy.link import Link\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n@@ -183,7 +183,7 @@ class TextResponse(Response):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n+        cookies: Optional[CookiesT] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = None,\n         priority: int = 0,\n@@ -236,7 +236,7 @@ class TextResponse(Response):\n         method: str = \"GET\",\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n-        cookies: Optional[Union[Dict[str, str], List[Dict[str, str]]]] = None,\n+        cookies: Optional[CookiesT] = None,\n         meta: Optional[Dict[str, Any]] = None,\n         encoding: Optional[str] = None,\n         priority: int = 0,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#019f23e3b75a0a481a4fcc22dc93c867ce424b18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 14 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 6621 | Contributors (this commit): 72 | Commits (past 90d): 20 | Contributors (cumulative): 135 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n \"\"\"Download handlers for different schemes\"\"\"\n \n+from __future__ import annotations\n+\n import logging\n from typing import TYPE_CHECKING, Any, Callable, Dict, Generator, Union, cast\n \n@@ -19,16 +21,16 @@ logger = logging.getLogger(__name__)\n \n \n class DownloadHandlers:\n-    def __init__(self, crawler: \"Crawler\"):\n-        self._crawler: \"Crawler\" = crawler\n-        self._schemes: Dict[str, Union[str, Callable]] = (\n+    def __init__(self, crawler: Crawler):\n+        self._crawler: Crawler = crawler\n+        self._schemes: Dict[str, Union[str, Callable[..., Any]]] = (\n             {}\n         )  # stores acceptable schemes on instancing\n         self._handlers: Dict[str, Any] = {}  # stores instanced handlers for schemes\n         self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n-        handlers: Dict[str, Union[str, Callable]] = without_none_values(\n+        handlers: Dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n             cast(\n-                Dict[str, Union[str, Callable]],\n+                Dict[str, Union[str, Callable[..., Any]]],\n                 crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n             )\n         )\n\n@@ -86,7 +86,11 @@ class Slot:\n \n \n class ExecutionEngine:\n-    def __init__(self, crawler: Crawler, spider_closed_callback: Callable) -> None:\n+    def __init__(\n+        self,\n+        crawler: Crawler,\n+        spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]],\n+    ) -> None:\n         self.crawler: Crawler = crawler\n         self.settings: Settings = crawler.settings\n         self.signals: SignalManager = crawler.signals\n@@ -102,7 +106,9 @@ class ExecutionEngine:\n         downloader_cls: Type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n         self.downloader: Downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n-        self._spider_closed_callback: Callable = spider_closed_callback\n+        self._spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]] = (\n+            spider_closed_callback\n+        )\n         self.start_time: Optional[float] = None\n \n     def _get_scheduler_class(self, settings: BaseSettings) -> Type[BaseScheduler]:\n@@ -427,7 +433,7 @@ class ExecutionEngine:\n \n         dfd = self.slot.close()\n \n-        def log_failure(msg: str) -> Callable:\n+        def log_failure(msg: str) -> Callable[[Failure], None]:\n             def errback(failure: Failure) -> None:\n                 logger.error(\n                     msg, exc_info=failure_to_exc_info(failure), extra={\"spider\": spider}\n\n@@ -266,7 +266,7 @@ class Request(object_ref):\n         return d\n \n \n-def _find_method(obj: Any, func: Callable) -> str:\n+def _find_method(obj: Any, func: Callable[..., Any]) -> str:\n     \"\"\"Helper function for Request.to_dict\"\"\"\n     # Only instance methods contain ``__func__``\n     if obj and hasattr(func, \"__func__\"):\n\n@@ -56,7 +56,7 @@ def arg_to_iter(arg: Any) -> Iterable[Any]:\n     return [arg]\n \n \n-def load_object(path: Union[str, Callable]) -> Any:\n+def load_object(path: Union[str, Callable[..., Any]]) -> Any:\n     \"\"\"Load an object given its absolute object path, and return it.\n \n     The object can be the import path of a class, function, variable or an\n@@ -263,7 +263,7 @@ def walk_callable(node: ast.AST) -> Generator[ast.AST, Any, None]:\n _generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n \n \n-def is_generator_with_return_value(callable: Callable) -> bool:\n+def is_generator_with_return_value(callable: Callable[..., Any]) -> bool:\n     \"\"\"\n     Returns True if a callable is a generator function which includes a\n     'return' statement with a value different than None, False otherwise\n@@ -300,7 +300,9 @@ def is_generator_with_return_value(callable: Callable) -> bool:\n     return bool(_generator_callbacks_cache[callable])\n \n \n-def warn_on_generator_with_return_value(spider: Spider, callable: Callable) -> None:\n+def warn_on_generator_with_return_value(\n+    spider: Spider, callable: Callable[..., Any]\n+) -> None:\n     \"\"\"\n     Logs a warning if a callable is a generator function and includes\n     a 'return' statement with a value different than None\n\n@@ -217,7 +217,7 @@ def binary_is_text(data: bytes) -> bool:\n     return all(c not in _BINARYCHARS for c in data)\n \n \n-def get_func_args(func: Callable, stripself: bool = False) -> List[str]:\n+def get_func_args(func: Callable[..., Any], stripself: bool = False) -> List[str]:\n     \"\"\"Return the argument name list of a callable object\"\"\"\n     if not callable(func):\n         raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n@@ -247,7 +247,7 @@ def get_func_args(func: Callable, stripself: bool = False) -> List[str]:\n     return args\n \n \n-def get_spec(func: Callable) -> Tuple[List[str], Dict[str, Any]]:\n+def get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n     \"\"\"Returns (args, kwargs) tuple for a function\n     >>> import re\n     >>> get_spec(re.match)\n@@ -285,7 +285,7 @@ def get_spec(func: Callable) -> Tuple[List[str], Dict[str, Any]]:\n \n \n def equal_attributes(\n-    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable]]]\n+    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable[[Any], Any]]]]\n ) -> bool:\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
