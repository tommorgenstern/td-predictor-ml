{"custom_id": "scrapy#492c3bce9dfc6cccdad8fc7002db4bec49cfcb35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 17 | Churn Cumulative: 1780 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,10 +24,6 @@ if TYPE_CHECKING:\n logger = logging.getLogger(__name__)\n \n \n-def _DUMMY_CALLBACK(response):\n-    return response\n-\n-\n class MediaPipeline(ABC):\n     LOG_FAILED_RESULTS = True\n \n@@ -89,10 +85,6 @@ class MediaPipeline(ABC):\n \n     def _process_request(self, request, info, item):\n         fp = self._fingerprinter.fingerprint(request)\n-        if not request.callback or request.callback is NO_CALLBACK:\n-            cb = _DUMMY_CALLBACK\n-        else:\n-            cb = request.callback\n         eb = request.errback\n         request.callback = NO_CALLBACK\n         request.errback = None\n@@ -100,14 +92,12 @@ class MediaPipeline(ABC):\n         # Return cached result if request was already seen\n         if fp in info.downloaded:\n             d = defer_result(info.downloaded[fp])\n-            d.addCallback(cb)\n             if eb:\n                 d.addErrback(eb)\n             return d\n \n         # Otherwise, wait for result\n         wad = Deferred()\n-        wad.addCallback(cb)\n         if eb:\n             wad.addErrback(eb)\n         info.waiting[fp].append(wad)\n\n@@ -211,10 +211,6 @@ class MockedMediaPipeline(UserDefinedPipeline):\n class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     pipeline_class = MockedMediaPipeline\n \n-    def _callback(self, result):\n-        self.pipe._mockcalled.append(\"request_callback\")\n-        return result\n-\n     def _errback(self, result):\n         self.pipe._mockcalled.append(\"request_errback\")\n         return result\n@@ -225,7 +221,6 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         req = Request(\n             \"http://url1\",\n             meta={\"response\": rsp},\n-            callback=self._callback,\n             errback=self._errback,\n         )\n         item = {\"requests\": req}\n@@ -237,7 +232,6 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n                 \"get_media_requests\",\n                 \"media_to_download\",\n                 \"media_downloaded\",\n-                \"request_callback\",\n                 \"item_completed\",\n             ],\n         )\n@@ -249,7 +243,6 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         req = Request(\n             \"http://url1\",\n             meta={\"response\": fail},\n-            callback=self._callback,\n             errback=self._errback,\n         )\n         item = {\"requests\": req}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e56b425198bfe3e86f2c578e7bc1f2988c7d3ec9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 423 | Lines Deleted: 142 | Files Changed: 4 | Hunks: 123 | Methods Changed: 114 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 565 | Churn Cumulative: 3198 | Contributors (this commit): 50 | Commits (past 90d): 12 | Contributors (cumulative): 84 | DMM Complexity: 0.8426395939086294\n\nDIFF:\n@@ -10,6 +10,7 @@ from twisted.internet.defer import Deferred\n \n from scrapy import Spider\n from scrapy.middleware import MiddlewareManager\n+from scrapy.settings import Settings\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import deferred_f_from_coro_f\n \n@@ -18,7 +19,7 @@ class ItemPipelineManager(MiddlewareManager):\n     component_name = \"item pipeline\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings) -> List[Any]:\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n         return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n \n     def _add_middleware(self, pipe: Any) -> None:\n\n@@ -18,16 +18,35 @@ from ftplib import FTP\n from io import BytesIO\n from os import PathLike\n from pathlib import Path\n-from typing import IO, TYPE_CHECKING, DefaultDict, Optional, Set, Type, Union, cast\n+from typing import (\n+    IO,\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    DefaultDict,\n+    Dict,\n+    List,\n+    NoReturn,\n+    Optional,\n+    Protocol,\n+    Set,\n+    Type,\n+    TypedDict,\n+    Union,\n+    cast,\n+)\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n from twisted.internet import defer, threads\n+from twisted.internet.defer import Deferred\n+from twisted.python.failure import Failure\n \n+from scrapy import Spider\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.media import MediaPipeline\n+from scrapy.pipelines.media import FileInfo, FileInfoOrError, MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n@@ -40,10 +59,11 @@ if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+\n logger = logging.getLogger(__name__)\n \n \n-def _to_string(path: Union[str, PathLike]) -> str:\n+def _to_string(path: Union[str, PathLike[str]]) -> str:\n     return str(path)  # convert a Path object to string\n \n \n@@ -68,23 +88,54 @@ class FileException(Exception):\n     \"\"\"General media error exception\"\"\"\n \n \n+class StatInfo(TypedDict, total=False):\n+    checksum: str\n+    last_modified: float\n+\n+\n+class FilesStoreProtocol(Protocol):\n+    def __init__(self, basedir: str): ...\n+\n+    def persist_file(\n+        self,\n+        path: str,\n+        buf: BytesIO,\n+        info: MediaPipeline.SpiderInfo,\n+        meta: Optional[Dict[str, Any]] = None,\n+        headers: Optional[Dict[str, str]] = None,\n+    ) -> Optional[Deferred[Any]]: ...\n+\n+    def stat_file(\n+        self, path: str, info: MediaPipeline.SpiderInfo\n+    ) -> Union[StatInfo, Deferred[StatInfo]]: ...\n+\n+\n class FSFilesStore:\n-    def __init__(self, basedir: Union[str, PathLike]):\n+    def __init__(self, basedir: Union[str, PathLike[str]]):\n         basedir = _to_string(basedir)\n         if \"://\" in basedir:\n             basedir = basedir.split(\"://\", 1)[1]\n-        self.basedir = basedir\n+        self.basedir: str = basedir\n         self._mkdir(Path(self.basedir))\n-        self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)\n+        self.created_directories: DefaultDict[MediaPipeline.SpiderInfo, Set[str]] = (\n+            defaultdict(set)\n+        )\n \n     def persist_file(\n-        self, path: Union[str, PathLike], buf, info, meta=None, headers=None\n-    ):\n+        self,\n+        path: Union[str, PathLike[str]],\n+        buf: BytesIO,\n+        info: MediaPipeline.SpiderInfo,\n+        meta: Optional[Dict[str, Any]] = None,\n+        headers: Optional[Dict[str, str]] = None,\n+    ) -> None:\n         absolute_path = self._get_filesystem_path(path)\n         self._mkdir(absolute_path.parent, info)\n         absolute_path.write_bytes(buf.getvalue())\n \n-    def stat_file(self, path: Union[str, PathLike], info):\n+    def stat_file(\n+        self, path: Union[str, PathLike[str]], info: MediaPipeline.SpiderInfo\n+    ) -> StatInfo:\n         absolute_path = self._get_filesystem_path(path)\n         try:\n             last_modified = absolute_path.stat().st_mtime\n@@ -96,12 +147,14 @@ class FSFilesStore:\n \n         return {\"last_modified\": last_modified, \"checksum\": checksum}\n \n-    def _get_filesystem_path(self, path: Union[str, PathLike]) -> Path:\n+    def _get_filesystem_path(self, path: Union[str, PathLike[str]]) -> Path:\n         path_comps = _to_string(path).split(\"/\")\n         return Path(self.basedir, *path_comps)\n \n-    def _mkdir(self, dirname: Path, domain: Optional[str] = None):\n-        seen = self.created_directories[domain] if domain else set()\n+    def _mkdir(\n+        self, dirname: Path, domain: Optional[MediaPipeline.SpiderInfo] = None\n+    ) -> None:\n+        seen: Set[str] = self.created_directories[domain] if domain else set()\n         if str(dirname) not in seen:\n             if not dirname.exists():\n                 dirname.mkdir(parents=True)\n@@ -122,7 +175,7 @@ class S3FilesStore:\n         \"Cache-Control\": \"max-age=172800\",\n     }\n \n-    def __init__(self, uri):\n+    def __init__(self, uri: str):\n         if not is_botocore_available():\n             raise NotConfigured(\"missing botocore library\")\n         import botocore.session\n@@ -142,8 +195,10 @@ class S3FilesStore:\n             raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n         self.bucket, self.prefix = uri[5:].split(\"/\", 1)\n \n-    def stat_file(self, path, info):\n-        def _onsuccess(boto_key):\n+    def stat_file(\n+        self, path: str, info: MediaPipeline.SpiderInfo\n+    ) -> Deferred[StatInfo]:\n+        def _onsuccess(boto_key: Dict[str, Any]) -> StatInfo:\n             checksum = boto_key[\"ETag\"].strip('\"')\n             last_modified = boto_key[\"LastModified\"]\n             modified_stamp = time.mktime(last_modified.timetuple())\n@@ -151,13 +206,23 @@ class S3FilesStore:\n \n         return self._get_boto_key(path).addCallback(_onsuccess)\n \n-    def _get_boto_key(self, path):\n+    def _get_boto_key(self, path: str) -> Deferred[Dict[str, Any]]:\n         key_name = f\"{self.prefix}{path}\"\n-        return threads.deferToThread(\n-            self.s3_client.head_object, Bucket=self.bucket, Key=key_name\n+        return cast(\n+            \"Deferred[Dict[str, Any]]\",\n+            threads.deferToThread(\n+                self.s3_client.head_object, Bucket=self.bucket, Key=key_name  # type: ignore[attr-defined]\n+            ),\n         )\n \n-    def persist_file(self, path, buf, info, meta=None, headers=None):\n+    def persist_file(\n+        self,\n+        path: str,\n+        buf: BytesIO,\n+        info: MediaPipeline.SpiderInfo,\n+        meta: Optional[Dict[str, Any]] = None,\n+        headers: Optional[Dict[str, str]] = None,\n+    ) -> Deferred[Any]:\n         \"\"\"Upload file to S3 storage\"\"\"\n         key_name = f\"{self.prefix}{path}\"\n         buf.seek(0)\n@@ -165,7 +230,7 @@ class S3FilesStore:\n         if headers:\n             extra.update(self._headers_to_botocore_kwargs(headers))\n         return threads.deferToThread(\n-            self.s3_client.put_object,\n+            self.s3_client.put_object,  # type: ignore[attr-defined]\n             Bucket=self.bucket,\n             Key=key_name,\n             Body=buf,\n@@ -174,7 +239,7 @@ class S3FilesStore:\n             **extra,\n         )\n \n-    def _headers_to_botocore_kwargs(self, headers):\n+    def _headers_to_botocore_kwargs(self, headers: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n         # This is required while we need to support both boto and botocore.\n         mapping = CaseInsensitiveDict(\n@@ -206,7 +271,7 @@ class S3FilesStore:\n                 \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n             }\n         )\n-        extra = {}\n+        extra: Dict[str, Any] = {}\n         for key, value in headers.items():\n             try:\n                 kwarg = mapping[key]\n@@ -226,13 +291,13 @@ class GCSFilesStore:\n     # Overridden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n     POLICY = None\n \n-    def __init__(self, uri):\n+    def __init__(self, uri: str):\n         from google.cloud import storage\n \n         client = storage.Client(project=self.GCS_PROJECT_ID)\n         bucket, prefix = uri[5:].split(\"/\", 1)\n         self.bucket = client.bucket(bucket)\n-        self.prefix = prefix\n+        self.prefix: str = prefix\n         permissions = self.bucket.test_iam_permissions(\n             [\"storage.objects.get\", \"storage.objects.create\"]\n         )\n@@ -248,8 +313,10 @@ class GCSFilesStore:\n                 {\"bucket\": bucket},\n             )\n \n-    def stat_file(self, path, info):\n-        def _onsuccess(blob):\n+    def stat_file(\n+        self, path: str, info: MediaPipeline.SpiderInfo\n+    ) -> Deferred[StatInfo]:\n+        def _onsuccess(blob) -> StatInfo:\n             if blob:\n                 checksum = base64.b64decode(blob.md5_hash).hex()\n                 last_modified = time.mktime(blob.updated.timetuple())\n@@ -257,19 +324,29 @@ class GCSFilesStore:\n             return {}\n \n         blob_path = self._get_blob_path(path)\n-        return threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n+        return cast(\n+            Deferred[StatInfo],\n+            threads.deferToThread(self.bucket.get_blob, blob_path).addCallback(\n                 _onsuccess\n+            ),\n         )\n \n-    def _get_content_type(self, headers):\n+    def _get_content_type(self, headers: Optional[Dict[str, str]]) -> str:\n         if headers and \"Content-Type\" in headers:\n             return headers[\"Content-Type\"]\n         return \"application/octet-stream\"\n \n-    def _get_blob_path(self, path):\n+    def _get_blob_path(self, path: str) -> str:\n         return self.prefix + path\n \n-    def persist_file(self, path, buf, info, meta=None, headers=None):\n+    def persist_file(\n+        self,\n+        path: str,\n+        buf: BytesIO,\n+        info: MediaPipeline.SpiderInfo,\n+        meta: Optional[Dict[str, Any]] = None,\n+        headers: Optional[Dict[str, str]] = None,\n+    ) -> Deferred[Any]:\n         blob_path = self._get_blob_path(path)\n         blob = self.bucket.blob(blob_path)\n         blob.cache_control = self.CACHE_CONTROL\n@@ -283,22 +360,33 @@ class GCSFilesStore:\n \n \n class FTPFilesStore:\n-    FTP_USERNAME = None\n-    FTP_PASSWORD = None\n-    USE_ACTIVE_MODE = None\n+    FTP_USERNAME: Optional[str] = None\n+    FTP_PASSWORD: Optional[str] = None\n+    USE_ACTIVE_MODE: Optional[bool] = None\n \n-    def __init__(self, uri):\n+    def __init__(self, uri: str):\n         if not uri.startswith(\"ftp://\"):\n             raise ValueError(f\"Incorrect URI scheme in {uri}, expected 'ftp'\")\n         u = urlparse(uri)\n-        self.port = u.port\n-        self.host = u.hostname\n+        assert u.port\n+        assert u.hostname\n+        self.port: int = u.port\n+        self.host: str = u.hostname\n         self.port = int(u.port or 21)\n-        self.username = u.username or self.FTP_USERNAME\n-        self.password = u.password or self.FTP_PASSWORD\n-        self.basedir = u.path.rstrip(\"/\")\n+        assert self.FTP_USERNAME\n+        assert self.FTP_PASSWORD\n+        self.username: str = u.username or self.FTP_USERNAME\n+        self.password: str = u.password or self.FTP_PASSWORD\n+        self.basedir: str = u.path.rstrip(\"/\")\n \n-    def persist_file(self, path, buf, info, meta=None, headers=None):\n+    def persist_file(\n+        self,\n+        path: str,\n+        buf: BytesIO,\n+        info: MediaPipeline.SpiderInfo,\n+        meta: Optional[Dict[str, Any]] = None,\n+        headers: Optional[Dict[str, str]] = None,\n+    ) -> Deferred[Any]:\n         path = f\"{self.basedir}/{path}\"\n         return threads.deferToThread(\n             ftp_store_file,\n@@ -311,8 +399,10 @@ class FTPFilesStore:\n             use_active_mode=self.USE_ACTIVE_MODE,\n         )\n \n-    def stat_file(self, path, info):\n-        def _stat_file(path):\n+    def stat_file(\n+        self, path: str, info: MediaPipeline.SpiderInfo\n+    ) -> Deferred[StatInfo]:\n+        def _stat_file(path: str) -> StatInfo:\n             try:\n                 ftp = FTP()\n                 ftp.connect(self.host, self.port)\n@@ -328,7 +418,7 @@ class FTPFilesStore:\n             except Exception:\n                 return {}\n \n-        return threads.deferToThread(_stat_file, path)\n+        return cast(\"Deferred[StatInfo]\", threads.deferToThread(_stat_file, path))\n \n \n class FilesPipeline(MediaPipeline):\n@@ -350,20 +440,23 @@ class FilesPipeline(MediaPipeline):\n \n     \"\"\"\n \n-    MEDIA_NAME = \"file\"\n-    EXPIRES = 90\n-    STORE_SCHEMES = {\n+    MEDIA_NAME: str = \"file\"\n+    EXPIRES: int = 90\n+    STORE_SCHEMES: Dict[str, Type[FilesStoreProtocol]] = {\n         \"\": FSFilesStore,\n         \"file\": FSFilesStore,\n         \"s3\": S3FilesStore,\n         \"gs\": GCSFilesStore,\n         \"ftp\": FTPFilesStore,\n     }\n-    DEFAULT_FILES_URLS_FIELD = \"file_urls\"\n-    DEFAULT_FILES_RESULT_FIELD = \"files\"\n+    DEFAULT_FILES_URLS_FIELD: str = \"file_urls\"\n+    DEFAULT_FILES_RESULT_FIELD: str = \"files\"\n \n     def __init__(\n-        self, store_uri: Union[str, PathLike], download_func=None, settings=None\n+        self,\n+        store_uri: Union[str, PathLike[str]],\n+        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n+        settings: Union[Settings, Dict[str, Any], None] = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n@@ -372,26 +465,26 @@ class FilesPipeline(MediaPipeline):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         cls_name = \"FilesPipeline\"\n-        self.store = self._get_store(store_uri)\n+        self.store: FilesStoreProtocol = self._get_store(store_uri)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=cls_name, settings=settings\n         )\n-        self.expires = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n+        self.expires: int = settings.getint(resolve(\"FILES_EXPIRES\"), self.EXPIRES)\n         if not hasattr(self, \"FILES_URLS_FIELD\"):\n             self.FILES_URLS_FIELD = self.DEFAULT_FILES_URLS_FIELD\n         if not hasattr(self, \"FILES_RESULT_FIELD\"):\n             self.FILES_RESULT_FIELD = self.DEFAULT_FILES_RESULT_FIELD\n-        self.files_urls_field = settings.get(\n+        self.files_urls_field: str = settings.get(\n             resolve(\"FILES_URLS_FIELD\"), self.FILES_URLS_FIELD\n         )\n-        self.files_result_field = settings.get(\n+        self.files_result_field: str = settings.get(\n             resolve(\"FILES_RESULT_FIELD\"), self.FILES_RESULT_FIELD\n         )\n \n         super().__init__(download_func=download_func, settings=settings)\n \n     @classmethod\n-    def from_settings(cls, settings) -> Self:\n+    def from_settings(cls, settings: Settings) -> Self:\n         s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -418,7 +511,7 @@ class FilesPipeline(MediaPipeline):\n         store_uri = settings[\"FILES_STORE\"]\n         return cls(store_uri, settings=settings)\n \n-    def _get_store(self, uri: str):\n+    def _get_store(self, uri: str) -> FilesStoreProtocol:\n         if Path(uri).is_absolute():  # to support win32 paths like: C:\\\\some\\dir\n             scheme = \"file\"\n         else:\n@@ -426,19 +519,21 @@ class FilesPipeline(MediaPipeline):\n         store_cls = self.STORE_SCHEMES[scheme]\n         return store_cls(uri)\n \n-    def media_to_download(self, request, info, *, item=None):\n-        def _onsuccess(result):\n+    def media_to_download(\n+        self, request: Request, info: MediaPipeline.SpiderInfo, *, item: Any = None\n+    ) -> Deferred[Optional[FileInfo]]:\n+        def _onsuccess(result: StatInfo) -> Optional[FileInfo]:\n             if not result:\n-                return  # returning None force download\n+                return None  # returning None force download\n \n             last_modified = result.get(\"last_modified\", None)\n             if not last_modified:\n-                return  # returning None force download\n+                return None  # returning None force download\n \n             age_seconds = time.time() - last_modified\n             age_days = age_seconds / 60 / 60 / 24\n             if age_days > self.expires:\n-                return  # returning None force download\n+                return None  # returning None force download\n \n             referer = referer_str(request)\n             logger.debug(\n@@ -458,19 +553,22 @@ class FilesPipeline(MediaPipeline):\n             }\n \n         path = self.file_path(request, info=info, item=item)\n-        dfd = defer.maybeDeferred(self.store.stat_file, path, info)\n-        dfd.addCallback(_onsuccess)\n-        dfd.addErrback(lambda _: None)\n-        dfd.addErrback(\n+        # defer.maybeDeferred() overloads don't seem to support a Union[_T, Deferred[_T]] return type\n+        dfd: Deferred[StatInfo] = defer.maybeDeferred(self.store.stat_file, path, info)  # type: ignore[arg-type]\n+        dfd2: Deferred[Optional[FileInfo]] = dfd.addCallback(_onsuccess)\n+        dfd2.addErrback(lambda _: None)\n+        dfd2.addErrback(\n             lambda f: logger.error(\n                 self.__class__.__name__ + \".store.stat_file\",\n                 exc_info=failure_to_exc_info(f),\n                 extra={\"spider\": info.spider},\n             )\n         )\n-        return dfd\n+        return dfd2\n \n-    def media_failed(self, failure, request, info):\n+    def media_failed(\n+        self, failure: Failure, request: Request, info: MediaPipeline.SpiderInfo\n+    ) -> NoReturn:\n         if not isinstance(failure.value, IgnoreRequest):\n             referer = referer_str(request)\n             logger.warning(\n@@ -487,7 +585,14 @@ class FilesPipeline(MediaPipeline):\n \n         raise FileException\n \n-    def media_downloaded(self, response, request, info, *, item=None):\n+    def media_downloaded(\n+        self,\n+        response: Response,\n+        request: Request,\n+        info: MediaPipeline.SpiderInfo,\n+        *,\n+        item: Any = None,\n+    ) -> FileInfo:\n         referer = referer_str(request)\n \n         if response.status != 200:\n@@ -546,16 +651,26 @@ class FilesPipeline(MediaPipeline):\n             \"status\": status,\n         }\n \n-    def inc_stats(self, spider, status):\n+    def inc_stats(self, spider: Spider, status: str) -> None:\n+        assert spider.crawler.stats\n         spider.crawler.stats.inc_value(\"file_count\", spider=spider)\n         spider.crawler.stats.inc_value(f\"file_status_count/{status}\", spider=spider)\n \n     # Overridable Interface\n-    def get_media_requests(self, item, info):\n+    def get_media_requests(\n+        self, item: Any, info: MediaPipeline.SpiderInfo\n+    ) -> List[Request]:\n         urls = ItemAdapter(item).get(self.files_urls_field, [])\n         return [Request(u, callback=NO_CALLBACK) for u in urls]\n \n-    def file_downloaded(self, response, request, info, *, item=None):\n+    def file_downloaded(\n+        self,\n+        response: Response,\n+        request: Request,\n+        info: MediaPipeline.SpiderInfo,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n         path = self.file_path(request, response=response, info=info, item=item)\n         buf = BytesIO(response.body)\n         checksum = _md5sum(buf)\n@@ -563,12 +678,21 @@ class FilesPipeline(MediaPipeline):\n         self.store.persist_file(path, buf, info)\n         return checksum\n \n-    def item_completed(self, results, item, info):\n+    def item_completed(\n+        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n+    ) -> Any:\n         with suppress(KeyError):\n             ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n         return item\n \n-    def file_path(self, request, response=None, info=None, *, item=None):\n+    def file_path(\n+        self,\n+        request: Request,\n+        response: Optional[Response] = None,\n+        info: Optional[MediaPipeline.SpiderInfo] = None,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n         media_ext = Path(request.url).suffix\n         # Handles empty and wild extensions by trying to guess the\n@@ -577,5 +701,5 @@ class FilesPipeline(MediaPipeline):\n             media_ext = \"\"\n             media_type = mimetypes.guess_type(request.url)[0]\n             if media_type:\n-                media_ext = mimetypes.guess_extension(media_type)\n+                media_ext = cast(str, mimetypes.guess_extension(media_type))\n         return f\"full/{media_guid}{media_ext}\"\n\n@@ -12,12 +12,25 @@ import warnings\n from contextlib import suppress\n from io import BytesIO\n from os import PathLike\n-from typing import TYPE_CHECKING, Dict, Tuple, Type, Union, cast\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    Type,\n+    Union,\n+    cast,\n+)\n \n from itemadapter import ItemAdapter\n \n+from scrapy import Spider\n from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.http.request import NO_CALLBACK\n from scrapy.pipelines.files import (\n     FileException,\n@@ -27,20 +40,20 @@ from scrapy.pipelines.files import (\n     S3FilesStore,\n     _md5sum,\n )\n-\n-# TODO: from scrapy.pipelines.media import MediaPipeline\n+from scrapy.pipelines.media import FileInfoOrError, MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.python import get_func_args, to_bytes\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n+    from PIL import Image\n     from typing_extensions import Self\n \n \n class NoimagesDrop(DropItem):\n     \"\"\"Product with no images exception\"\"\"\n \n-    def __init__(self, *args, **kwargs):\n+    def __init__(self, *args: Any, **kwargs: Any):\n         warnings.warn(\n             \"The NoimagesDrop class is deprecated\",\n             category=ScrapyDeprecationWarning,\n@@ -56,19 +69,22 @@ class ImageException(FileException):\n class ImagesPipeline(FilesPipeline):\n     \"\"\"Abstract pipeline that implement the image thumbnail generation logic\"\"\"\n \n-    MEDIA_NAME = \"image\"\n+    MEDIA_NAME: str = \"image\"\n \n     # Uppercase attributes kept for backward compatibility with code that subclasses\n     # ImagesPipeline. They may be overridden by settings.\n-    MIN_WIDTH = 0\n-    MIN_HEIGHT = 0\n-    EXPIRES = 90\n+    MIN_WIDTH: int = 0\n+    MIN_HEIGHT: int = 0\n+    EXPIRES: int = 90\n     THUMBS: Dict[str, Tuple[int, int]] = {}\n     DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n     DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n \n     def __init__(\n-        self, store_uri: Union[str, PathLike], download_func=None, settings=None\n+        self,\n+        store_uri: Union[str, PathLike[str]],\n+        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n+        settings: Union[Settings, Dict[str, Any], None] = None,\n     ):\n         try:\n             from PIL import Image\n@@ -89,27 +105,33 @@ class ImagesPipeline(FilesPipeline):\n             base_class_name=\"ImagesPipeline\",\n             settings=settings,\n         )\n-        self.expires = settings.getint(resolve(\"IMAGES_EXPIRES\"), self.EXPIRES)\n+        self.expires: int = settings.getint(resolve(\"IMAGES_EXPIRES\"), self.EXPIRES)\n \n         if not hasattr(self, \"IMAGES_RESULT_FIELD\"):\n-            self.IMAGES_RESULT_FIELD = self.DEFAULT_IMAGES_RESULT_FIELD\n+            self.IMAGES_RESULT_FIELD: str = self.DEFAULT_IMAGES_RESULT_FIELD\n         if not hasattr(self, \"IMAGES_URLS_FIELD\"):\n-            self.IMAGES_URLS_FIELD = self.DEFAULT_IMAGES_URLS_FIELD\n+            self.IMAGES_URLS_FIELD: str = self.DEFAULT_IMAGES_URLS_FIELD\n \n-        self.images_urls_field = settings.get(\n+        self.images_urls_field: str = settings.get(\n             resolve(\"IMAGES_URLS_FIELD\"), self.IMAGES_URLS_FIELD\n         )\n-        self.images_result_field = settings.get(\n+        self.images_result_field: str = settings.get(\n             resolve(\"IMAGES_RESULT_FIELD\"), self.IMAGES_RESULT_FIELD\n         )\n-        self.min_width = settings.getint(resolve(\"IMAGES_MIN_WIDTH\"), self.MIN_WIDTH)\n-        self.min_height = settings.getint(resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT)\n-        self.thumbs = settings.get(resolve(\"IMAGES_THUMBS\"), self.THUMBS)\n+        self.min_width: int = settings.getint(\n+            resolve(\"IMAGES_MIN_WIDTH\"), self.MIN_WIDTH\n+        )\n+        self.min_height: int = settings.getint(\n+            resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT\n+        )\n+        self.thumbs: Dict[str, Tuple[int, int]] = settings.get(\n+            resolve(\"IMAGES_THUMBS\"), self.THUMBS\n+        )\n \n-        self._deprecated_convert_image = None\n+        self._deprecated_convert_image: Optional[bool] = None\n \n     @classmethod\n-    def from_settings(cls, settings) -> Self:\n+    def from_settings(cls, settings: Settings) -> Self:\n         s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n@@ -136,11 +158,25 @@ class ImagesPipeline(FilesPipeline):\n         store_uri = settings[\"IMAGES_STORE\"]\n         return cls(store_uri, settings=settings)\n \n-    def file_downloaded(self, response, request, info, *, item=None):\n+    def file_downloaded(\n+        self,\n+        response: Response,\n+        request: Request,\n+        info: MediaPipeline.SpiderInfo,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n         return self.image_downloaded(response, request, info, item=item)\n \n-    def image_downloaded(self, response, request, info, *, item=None):\n-        checksum = None\n+    def image_downloaded(\n+        self,\n+        response: Response,\n+        request: Request,\n+        info: MediaPipeline.SpiderInfo,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n+        checksum: Optional[str] = None\n         for path, image, buf in self.get_images(response, request, info, item=item):\n             if checksum is None:\n                 buf.seek(0)\n@@ -153,9 +189,17 @@ class ImagesPipeline(FilesPipeline):\n                 meta={\"width\": width, \"height\": height},\n                 headers={\"Content-Type\": \"image/jpeg\"},\n             )\n+        assert checksum is not None\n         return checksum\n \n-    def get_images(self, response, request, info, *, item=None):\n+    def get_images(\n+        self,\n+        response: Response,\n+        request: Request,\n+        info: MediaPipeline.SpiderInfo,\n+        *,\n+        item: Any = None,\n+    ) -> Iterable[Tuple[str, Image.Image, BytesIO]]:\n         path = self.file_path(request, response=response, info=info, item=item)\n         orig_image = self._Image.open(BytesIO(response.body))\n \n@@ -196,7 +240,12 @@ class ImagesPipeline(FilesPipeline):\n                 thumb_image, thumb_buf = self.convert_image(image, size, buf)\n             yield thumb_path, thumb_image, thumb_buf\n \n-    def convert_image(self, image, size=None, response_body=None):\n+    def convert_image(\n+        self,\n+        image: Image.Image,\n+        size: Optional[Tuple[int, int]] = None,\n+        response_body: Optional[BytesIO] = None,\n+    ) -> Tuple[Image.Image, BytesIO]:\n         if response_body is None:\n             warnings.warn(\n                 f\"{self.__class__.__name__}.convert_image() method called in a deprecated way, \"\n@@ -225,7 +274,7 @@ class ImagesPipeline(FilesPipeline):\n                 # when updating the minimum requirements for Pillow.\n                 resampling_filter = self._Image.Resampling.LANCZOS\n             except AttributeError:\n-                resampling_filter = self._Image.ANTIALIAS\n+                resampling_filter = self._Image.ANTIALIAS  # type: ignore[attr-defined]\n             image.thumbnail(size, resampling_filter)\n         elif response_body is not None and image.format == \"JPEG\":\n             return image, response_body\n@@ -234,19 +283,38 @@ class ImagesPipeline(FilesPipeline):\n         image.save(buf, \"JPEG\")\n         return image, buf\n \n-    def get_media_requests(self, item, info):\n+    def get_media_requests(\n+        self, item: Any, info: MediaPipeline.SpiderInfo\n+    ) -> List[Request]:\n         urls = ItemAdapter(item).get(self.images_urls_field, [])\n         return [Request(u, callback=NO_CALLBACK) for u in urls]\n \n-    def item_completed(self, results, item, info):\n+    def item_completed(\n+        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n+    ) -> Any:\n         with suppress(KeyError):\n             ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n         return item\n \n-    def file_path(self, request, response=None, info=None, *, item=None):\n+    def file_path(\n+        self,\n+        request: Request,\n+        response: Optional[Response] = None,\n+        info: Optional[MediaPipeline.SpiderInfo] = None,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n         image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n         return f\"full/{image_guid}.jpg\"\n \n-    def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\n+    def thumb_path(\n+        self,\n+        request: Request,\n+        thumb_id: str,\n+        response: Optional[Response] = None,\n+        info: Optional[MediaPipeline.SpiderInfo] = None,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n         thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n         return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n\n@@ -4,54 +4,101 @@ import functools\n import logging\n from abc import ABC, abstractmethod\n from collections import defaultdict\n-from typing import TYPE_CHECKING\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    DefaultDict,\n+    Dict,\n+    List,\n+    Literal,\n+    NoReturn,\n+    Optional,\n+    Set,\n+    Tuple,\n+    TypedDict,\n+    TypeVar,\n+    Union,\n+    cast,\n+)\n \n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n-from scrapy.http.request import NO_CALLBACK\n+from scrapy import Spider\n+from scrapy.crawler import Crawler\n+from scrapy.http import Response\n+from scrapy.http.request import NO_CALLBACK, Request\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.request import RequestFingerprinter\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n+_T = TypeVar(\"_T\")\n+\n+\n+class FileInfo(TypedDict):\n+    url: str\n+    path: str\n+    checksum: Optional[str]\n+    status: str\n+\n+\n+FileInfoOrError = Union[Tuple[Literal[True], FileInfo], Tuple[Literal[False], Failure]]\n+\n \n logger = logging.getLogger(__name__)\n \n \n class MediaPipeline(ABC):\n-    LOG_FAILED_RESULTS = True\n+    crawler: Crawler\n+    _fingerprinter: RequestFingerprinter\n+\n+    LOG_FAILED_RESULTS: bool = True\n \n     class SpiderInfo:\n-        def __init__(self, spider):\n-            self.spider = spider\n-            self.downloading = set()\n-            self.downloaded = {}\n-            self.waiting = defaultdict(list)\n+        def __init__(self, spider: Spider):\n+            self.spider: Spider = spider\n+            self.downloading: Set[bytes] = set()\n+            self.downloaded: Dict[bytes, Union[FileInfo, Failure]] = {}\n+            self.waiting: DefaultDict[bytes, List[Deferred[FileInfo]]] = defaultdict(\n+                list\n+            )\n \n-    def __init__(self, download_func=None, settings=None):\n+    def __init__(\n+        self,\n+        download_func: Optional[Callable[[Request, Spider], Response]] = None,\n+        settings: Union[Settings, Dict[str, Any], None] = None,\n+    ):\n         self.download_func = download_func\n-        self._expects_item = {}\n \n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(\n             self._key_for_pipe, base_class_name=\"MediaPipeline\", settings=settings\n         )\n-        self.allow_redirects = settings.getbool(resolve(\"MEDIA_ALLOW_REDIRECTS\"), False)\n+        self.allow_redirects: bool = settings.getbool(\n+            resolve(\"MEDIA_ALLOW_REDIRECTS\"), False\n+        )\n         self._handle_statuses(self.allow_redirects)\n \n-    def _handle_statuses(self, allow_redirects):\n+    def _handle_statuses(self, allow_redirects: bool) -> None:\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n             self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n \n-    def _key_for_pipe(self, key, base_class_name=None, settings=None):\n+    def _key_for_pipe(\n+        self,\n+        key: str,\n+        base_class_name: Optional[str] = None,\n+        settings: Optional[Settings] = None,\n+    ) -> str:\n         class_name = self.__class__.__name__\n         formatted_key = f\"{class_name.upper()}_{key}\"\n         if (\n@@ -64,26 +111,34 @@ class MediaPipeline(ABC):\n         return formatted_key\n \n     @classmethod\n-    def from_crawler(cls, crawler) -> Self:\n+    def from_crawler(cls, crawler: Crawler) -> Self:\n+        pipe: Self\n         try:\n             pipe = cls.from_settings(crawler.settings)  # type: ignore[attr-defined]\n         except AttributeError:\n             pipe = cls()\n         pipe.crawler = crawler\n+        assert crawler.request_fingerprinter\n         pipe._fingerprinter = crawler.request_fingerprinter\n         return pipe\n \n-    def open_spider(self, spider):\n+    def open_spider(self, spider: Spider) -> None:\n         self.spiderinfo = self.SpiderInfo(spider)\n \n-    def process_item(self, item, spider):\n+    def process_item(\n+        self, item: Any, spider: Spider\n+    ) -> Deferred[List[FileInfoOrError]]:\n         info = self.spiderinfo\n         requests = arg_to_iter(self.get_media_requests(item, info))\n         dlist = [self._process_request(r, info, item) for r in requests]\n-        dfd = DeferredList(dlist, consumeErrors=True)\n+        dfd = cast(\n+            \"Deferred[List[FileInfoOrError]]\", DeferredList(dlist, consumeErrors=True)\n+        )\n         return dfd.addCallback(self.item_completed, item, info)\n \n-    def _process_request(self, request, info, item):\n+    def _process_request(\n+        self, request: Request, info: SpiderInfo, item: Any\n+    ) -> Deferred[FileInfo]:\n         fp = self._fingerprinter.fingerprint(request)\n         eb = request.errback\n         request.callback = NO_CALLBACK\n@@ -97,7 +152,7 @@ class MediaPipeline(ABC):\n             return d\n \n         # Otherwise, wait for result\n-        wad = Deferred()\n+        wad: Deferred[FileInfo] = Deferred()\n         if eb:\n             wad.addErrback(eb)\n         info.waiting[fp].append(wad)\n@@ -108,36 +163,48 @@ class MediaPipeline(ABC):\n \n         # Download request checking media_to_download hook output first\n         info.downloading.add(fp)\n-        dfd = mustbe_deferred(self.media_to_download, request, info, item=item)\n-        dfd.addCallback(self._check_media_to_download, request, info, item=item)\n-        dfd.addErrback(self._log_exception)\n-        dfd.addBoth(self._cache_result_and_execute_waiters, fp, info)\n-        return dfd.addBoth(lambda _: wad)  # it must return wad at last\n+        dfd: Deferred[Optional[FileInfo]] = mustbe_deferred(\n+            self.media_to_download, request, info, item=item\n+        )\n+        dfd2: Deferred[FileInfo] = dfd.addCallback(\n+            self._check_media_to_download, request, info, item=item\n+        )\n+        dfd2.addErrback(self._log_exception)\n+        dfd2.addBoth(self._cache_result_and_execute_waiters, fp, info)\n+        return dfd2.addBoth(lambda _: wad)  # it must return wad at last\n \n-    def _log_exception(self, result):\n+    def _log_exception(self, result: Failure) -> Failure:\n         logger.exception(result)\n         return result\n \n-    def _modify_media_request(self, request):\n+    def _modify_media_request(self, request: Request) -> None:\n         if self.handle_httpstatus_list:\n             request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n         else:\n             request.meta[\"handle_httpstatus_all\"] = True\n \n-    def _check_media_to_download(self, result, request, info, item):\n+    def _check_media_to_download(\n+        self, result: Optional[FileInfo], request: Request, info: SpiderInfo, item: Any\n+    ) -> Union[FileInfo, Deferred[FileInfo]]:\n         if result is not None:\n             return result\n+        dfd: Deferred[Response]\n         if self.download_func:\n             # this ugly code was left only to support tests. TODO: remove\n             dfd = mustbe_deferred(self.download_func, request, info.spider)\n         else:\n             self._modify_media_request(request)\n+            assert self.crawler.engine\n             dfd = self.crawler.engine.download(request)\n-        dfd.addCallback(self.media_downloaded, request, info, item=item)\n-        dfd.addErrback(self.media_failed, request, info)\n-        return dfd\n+        dfd2: Deferred[FileInfo] = dfd.addCallback(\n+            self.media_downloaded, request, info, item=item\n+        )\n+        dfd2.addErrback(self.media_failed, request, info)\n+        return dfd2\n \n-    def _cache_result_and_execute_waiters(self, result, fp, info):\n+    def _cache_result_and_execute_waiters(\n+        self, result: Union[FileInfo, Failure], fp: bytes, info: SpiderInfo\n+    ) -> None:\n         if isinstance(result, Failure):\n             # minimize cached information for failure\n             result.cleanFailure()\n@@ -176,30 +243,44 @@ class MediaPipeline(ABC):\n \n     # Overridable Interface\n     @abstractmethod\n-    def media_to_download(self, request, info, *, item=None):\n+    def media_to_download(\n+        self, request: Request, info: SpiderInfo, *, item: Any = None\n+    ) -> Deferred[Optional[FileInfo]]:\n         \"\"\"Check request before starting download\"\"\"\n         raise NotImplementedError()\n \n     @abstractmethod\n-    def get_media_requests(self, item, info):\n+    def get_media_requests(self, item: Any, info: SpiderInfo) -> List[Request]:\n         \"\"\"Returns the media requests to download\"\"\"\n         raise NotImplementedError()\n \n     @abstractmethod\n-    def media_downloaded(self, response, request, info, *, item=None):\n+    def media_downloaded(\n+        self,\n+        response: Response,\n+        request: Request,\n+        info: SpiderInfo,\n+        *,\n+        item: Any = None,\n+    ) -> FileInfo:\n         \"\"\"Handler for success downloads\"\"\"\n         raise NotImplementedError()\n \n     @abstractmethod\n-    def media_failed(self, failure, request, info):\n+    def media_failed(\n+        self, failure: Failure, request: Request, info: SpiderInfo\n+    ) -> NoReturn:\n         \"\"\"Handler for failed downloads\"\"\"\n         raise NotImplementedError()\n \n-    def item_completed(self, results, item, info):\n+    def item_completed(\n+        self, results: List[FileInfoOrError], item: Any, info: SpiderInfo\n+    ) -> Any:\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n         if self.LOG_FAILED_RESULTS:\n             for ok, value in results:\n                 if not ok:\n+                    assert isinstance(value, Failure)\n                     logger.error(\n                         \"%(class)s found errors processing %(item)s\",\n                         {\"class\": self.__class__.__name__, \"item\": item},\n@@ -209,6 +290,13 @@ class MediaPipeline(ABC):\n         return item\n \n     @abstractmethod\n-    def file_path(self, request, response=None, info=None, *, item=None):\n+    def file_path(\n+        self,\n+        request: Request,\n+        response: Optional[Response] = None,\n+        info: Optional[SpiderInfo] = None,\n+        *,\n+        item: Any = None,\n+    ) -> str:\n         \"\"\"Returns the path where downloaded media should be stored\"\"\"\n         raise NotImplementedError()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f76853bd27d84f53ebaaa97cb819e8a29195a89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 31 | Churn Cumulative: 1206 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -120,7 +120,8 @@ class ContractsManager:\n \n             if line.startswith(\"@\"):\n                 m = re.match(r\"@(\\w+)\\s*(.*)\", line)\n-                assert m is not None\n+                if m is None:\n+                    continue\n                 name, args = m.groups()\n                 args = re.split(r\"\\s+\", args)\n \n\n@@ -182,6 +182,19 @@ class TestSpider(Spider):\n         \"\"\"\n         pass\n \n+    def invalid_regex(self, response):\n+        \"\"\"method with invalid regex\n+        @ Scrapy is awsome\n+        \"\"\"\n+        pass\n+\n+    def invalid_regex_with_valid_contract(self, response):\n+        \"\"\"method with invalid regex\n+        @ scrapy is awsome\n+        @url http://scrapy.org\n+        \"\"\"\n+        pass\n+\n \n class CustomContractSuccessSpider(Spider):\n     name = \"custom_contract_success_spider\"\n@@ -385,6 +398,21 @@ class ContractsManagerTest(unittest.TestCase):\n         message = \"ContractFail: Missing fields: name, url\"\n         assert message in self.results.failures[-1][-1]\n \n+    def test_regex(self):\n+        spider = TestSpider()\n+        response = ResponseMock()\n+\n+        # invalid regex\n+        request = self.conman.from_method(spider.invalid_regex, self.results)\n+        self.should_succeed()\n+\n+        # invalid regex with valid contract\n+        request = self.conman.from_method(\n+            spider.invalid_regex_with_valid_contract, self.results\n+        )\n+        self.should_succeed()\n+        request.callback(response)\n+\n     def test_custom_contracts(self):\n         self.conman.from_spider(CustomContractSuccessSpider(), self.results)\n         self.should_succeed()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e214210f6707181a863dbceabf2d34e767396cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 81 | Files Changed: 13 | Hunks: 75 | Methods Changed: 51 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 181 | Churn Cumulative: 16170 | Contributors (this commit): 106 | Commits (past 90d): 43 | Contributors (cumulative): 304 | DMM Complexity: 0.6956521739130435\n\nDIFF:\n@@ -140,13 +140,13 @@ class Command(BaseRunSpiderCommand):\n \n     @overload\n     def iterate_spider_output(\n-        self, result: Union[AsyncGenerator, CoroutineType]\n-    ) -> Deferred: ...\n+        self, result: Union[AsyncGenerator[_T, None], CoroutineType[Any, Any, _T]]\n+    ) -> Deferred[_T]: ...\n \n     @overload\n-    def iterate_spider_output(self, result: _T) -> Iterable: ...\n+    def iterate_spider_output(self, result: _T) -> Iterable[Any]: ...\n \n-    def iterate_spider_output(self, result: Any) -> Union[Iterable, Deferred]:\n+    def iterate_spider_output(self, result: Any) -> Union[Iterable[Any], Deferred]:\n         if inspect.isasyncgen(result):\n             d = deferred_from_coro(\n                 collect_asyncgen(aiter_errback(result, self.handle_exception))\n\n@@ -372,7 +372,10 @@ class ExecutionEngine:\n \n     @inlineCallbacks\n     def open_spider(\n-        self, spider: Spider, start_requests: Iterable = (), close_if_idle: bool = True\n+        self,\n+        spider: Spider,\n+        start_requests: Iterable[Request] = (),\n+        close_if_idle: bool = True,\n     ) -> Generator[Deferred, Any, None]:\n         if self.slot is not None:\n             raise RuntimeError(f\"No free spider slot when opening {spider.name!r}\")\n\n@@ -16,6 +16,7 @@ from typing import (\n     Set,\n     Tuple,\n     Type,\n+    TypeVar,\n     Union,\n     cast,\n )\n@@ -47,6 +48,7 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n \n+_T = TypeVar(\"_T\")\n QueueTuple = Tuple[Union[Response, Failure], Request, Deferred]\n \n \n@@ -256,14 +258,14 @@ class Scraper:\n \n     def handle_spider_output(\n         self,\n-        result: Union[Iterable, AsyncIterable],\n+        result: Union[Iterable[_T], AsyncIterable[_T]],\n         request: Request,\n         response: Response,\n         spider: Spider,\n     ) -> Deferred:\n         if not result:\n             return defer_succeed(None)\n-        it: Union[Iterable, AsyncIterable]\n+        it: Union[Iterable[_T], AsyncIterable[_T]]\n         if isinstance(result, AsyncIterable):\n             it = aiter_errback(\n                 result, self.handle_spider_error, request, response, spider\n\n@@ -9,7 +9,6 @@ from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n from typing import (\n     Any,\n-    AsyncGenerator,\n     AsyncIterable,\n     Callable,\n     Generator,\n@@ -17,6 +16,7 @@ from typing import (\n     List,\n     Optional,\n     Tuple,\n+    TypeVar,\n     Union,\n     cast,\n )\n@@ -42,6 +42,7 @@ from scrapy.utils.python import MutableAsyncChain, MutableChain\n logger = logging.getLogger(__name__)\n \n \n+_T = TypeVar(\"_T\")\n ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]\n \n \n@@ -98,31 +99,39 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        iterable: Union[Iterable, AsyncIterable],\n+        iterable: Union[Iterable[_T], AsyncIterable[_T]],\n         exception_processor_index: int,\n-        recover_to: Union[MutableChain, MutableAsyncChain],\n-    ) -> Union[Generator, AsyncGenerator]:\n-        def process_sync(iterable: Iterable) -> Generator:\n+        recover_to: Union[MutableChain[_T], MutableAsyncChain[_T]],\n+    ) -> Union[Iterable[_T], AsyncIterable[_T]]:\n+        def process_sync(iterable: Iterable[_T]) -> Iterable[_T]:\n             try:\n                 yield from iterable\n             except Exception as ex:\n-                exception_result = self._process_spider_exception(\n+                exception_result = cast(\n+                    Union[Failure, MutableChain[_T]],\n+                    self._process_spider_exception(\n                         response, spider, Failure(ex), exception_processor_index\n+                    ),\n                 )\n                 if isinstance(exception_result, Failure):\n                     raise\n+                assert isinstance(recover_to, MutableChain)\n                 recover_to.extend(exception_result)\n \n-        async def process_async(iterable: AsyncIterable) -> AsyncGenerator:\n+        async def process_async(iterable: AsyncIterable[_T]) -> AsyncIterable[_T]:\n             try:\n                 async for r in iterable:\n                     yield r\n             except Exception as ex:\n-                exception_result = self._process_spider_exception(\n+                exception_result = cast(\n+                    Union[Failure, MutableAsyncChain[_T]],\n+                    self._process_spider_exception(\n                         response, spider, Failure(ex), exception_processor_index\n+                    ),\n                 )\n                 if isinstance(exception_result, Failure):\n                     raise\n+                assert isinstance(recover_to, MutableAsyncChain)\n                 recover_to.extend(exception_result)\n \n         if isinstance(iterable, AsyncIterable):\n@@ -135,7 +144,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         spider: Spider,\n         _failure: Failure,\n         start_index: int = 0,\n-    ) -> Union[Failure, MutableChain]:\n+    ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n         exception = _failure.value\n         # don't handle _InvalidOutput exception\n         if isinstance(exception, _InvalidOutput):\n@@ -151,14 +160,18 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             if _isiterable(result):\n                 # stop exception handling by handing control over to the\n                 # process_spider_output chain if an iterable has been returned\n-                dfd: Deferred = self._process_spider_output(\n+                dfd: Deferred[Union[MutableChain[_T], MutableAsyncChain[_T]]] = (\n+                    self._process_spider_output(\n                         response, spider, result, method_index + 1\n                     )\n+                )\n                 # _process_spider_output() returns a Deferred only because of downgrading so this can be\n                 # simplified when downgrading is removed.\n                 if dfd.called:\n                     # the result is available immediately if _process_spider_output didn't do downgrading\n-                    return cast(MutableChain, dfd.result)\n+                    return cast(\n+                        Union[MutableChain[_T], MutableAsyncChain[_T]], dfd.result\n+                    )\n                 # we forbid waiting here because otherwise we would need to return a deferred from\n                 # _process_spider_exception too, which complicates the architecture\n                 msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n@@ -181,12 +194,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self,\n         response: Response,\n         spider: Spider,\n-        result: Union[Iterable, AsyncIterable],\n+        result: Union[Iterable[_T], AsyncIterable[_T]],\n         start_index: int = 0,\n-    ) -> Generator[Deferred, Any, Union[MutableChain, MutableAsyncChain]]:\n+    ) -> Generator[Deferred[Any], Any, Union[MutableChain[_T], MutableAsyncChain[_T]]]:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n-        recovered: Union[MutableChain, MutableAsyncChain]\n+        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n         last_result_is_async = isinstance(result, AsyncIterable)\n         if last_result_is_async:\n             recovered = MutableAsyncChain()\n@@ -237,7 +250,9 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 # might fail directly if the output value is not a generator\n                 result = method(response=response, result=result, spider=spider)\n             except Exception as ex:\n-                exception_result = self._process_spider_exception(\n+                exception_result: Union[\n+                    Failure, MutableChain[_T], MutableAsyncChain[_T]\n+                ] = self._process_spider_exception(\n                     response, spider, Failure(ex), method_index + 1\n                 )\n                 if isinstance(exception_result, Failure):\n@@ -267,9 +282,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n     async def _process_callback_output(\n-        self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n-    ) -> Union[MutableChain, MutableAsyncChain]:\n-        recovered: Union[MutableChain, MutableAsyncChain]\n+        self,\n+        response: Response,\n+        spider: Spider,\n+        result: Union[Iterable[_T], AsyncIterable[_T]],\n+    ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n+        recovered: Union[MutableChain[_T], MutableAsyncChain[_T]]\n         if isinstance(result, AsyncIterable):\n             recovered = MutableAsyncChain()\n         else:\n@@ -293,14 +311,16 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         spider: Spider,\n     ) -> Deferred:\n         async def process_callback_output(\n-            result: Union[Iterable, AsyncIterable]\n-        ) -> Union[MutableChain, MutableAsyncChain]:\n+            result: Union[Iterable[_T], AsyncIterable[_T]]\n+        ) -> Union[MutableChain[_T], MutableAsyncChain[_T]]:\n             return await self._process_callback_output(response, spider, result)\n \n-        def process_spider_exception(_failure: Failure) -> Union[Failure, MutableChain]:\n+        def process_spider_exception(\n+            _failure: Failure,\n+        ) -> Union[Failure, MutableChain[_T], MutableAsyncChain[_T]]:\n             return self._process_spider_exception(response, spider, _failure)\n \n-        dfd = mustbe_deferred(\n+        dfd: Deferred = mustbe_deferred(\n             self._process_spider_input, scrape_func, response, request, spider\n         )\n         dfd.addCallback(deferred_f_from_coro_f(process_callback_output))\n\n@@ -14,7 +14,6 @@ from typing import (\n     AnyStr,\n     Callable,\n     Dict,\n-    Generator,\n     Iterable,\n     List,\n     Mapping,\n@@ -242,7 +241,7 @@ class Response(object_ref):\n         errback: Optional[Callable] = None,\n         cb_kwargs: Optional[Dict[str, Any]] = None,\n         flags: Optional[List[str]] = None,\n-    ) -> Generator[Request, None, None]:\n+    ) -> Iterable[Request]:\n         \"\"\"\n         .. versionadded:: 2.0\n \n\n@@ -15,7 +15,6 @@ from typing import (\n     AnyStr,\n     Callable,\n     Dict,\n-    Generator,\n     Iterable,\n     List,\n     Mapping,\n@@ -246,7 +245,7 @@ class TextResponse(Response):\n         flags: Optional[List[str]] = None,\n         css: Optional[str] = None,\n         xpath: Optional[str] = None,\n-    ) -> Generator[Request, None, None]:\n+    ) -> Iterable[Request]:\n         \"\"\"\n         A generator that produces :class:`~.Request` instances to follow all\n         links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s\n\n@@ -6,8 +6,7 @@ from typing import (\n     Any,\n     Callable,\n     Dict,\n-    Generator,\n-    Iterable,\n+    Iterator,\n     List,\n     Literal,\n     Optional,\n@@ -22,14 +21,12 @@ from lxml import etree  # nosec\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n-from scrapy.utils.python import re_rsearch, to_unicode\n+from scrapy.utils.python import re_rsearch\n \n logger = logging.getLogger(__name__)\n \n \n-def xmliter(\n-    obj: Union[Response, str, bytes], nodename: str\n-) -> Generator[Selector, Any, None]:\n+def xmliter(obj: Union[Response, str, bytes], nodename: str) -> Iterator[Selector]:\n     \"\"\"Return a iterator of Selector's over all nodes of a XML document,\n        given the name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -90,7 +87,7 @@ def xmliter_lxml(\n     nodename: str,\n     namespace: Optional[str] = None,\n     prefix: str = \"x\",\n-) -> Generator[Selector, Any, None]:\n+) -> Iterator[Selector]:\n     reader = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(\n@@ -168,7 +165,7 @@ def csviter(\n     headers: Optional[List[str]] = None,\n     encoding: Optional[str] = None,\n     quotechar: Optional[str] = None,\n-) -> Generator[Dict[str, str], Any, None]:\n+) -> Iterator[Dict[str, str]]:\n     \"\"\"Returns an iterator of dictionaries from the given csv object\n \n     obj can be:\n@@ -184,10 +181,13 @@ def csviter(\n     quotechar is the character used to enclosure fields on the given obj.\n     \"\"\"\n \n-    encoding = obj.encoding if isinstance(obj, TextResponse) else encoding or \"utf-8\"\n-\n-    def row_to_unicode(row_: Iterable) -> List[str]:\n-        return [to_unicode(field, encoding) for field in row_]\n+    if encoding is not None:\n+        warn(\n+            \"The encoding argument of csviter() is ignored and will be removed\"\n+            \" in a future Scrapy version.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n \n     lines = StringIO(_body_or_str(obj, unicode=True))\n \n@@ -200,13 +200,11 @@ def csviter(\n \n     if not headers:\n         try:\n-            row = next(csv_r)\n+            headers = next(csv_r)\n         except StopIteration:\n             return\n-        headers = row_to_unicode(row)\n \n     for row in csv_r:\n-        row = row_to_unicode(row)\n         if len(row) != len(headers):\n             logger.warning(\n                 \"ignoring row %(csvlnum)d (length: %(csvrow)d, \"\n\n@@ -20,8 +20,8 @@ from typing import (\n     Any,\n     Callable,\n     Deque,\n-    Generator,\n     Iterable,\n+    Iterator,\n     List,\n     Optional,\n     Type,\n@@ -227,7 +227,7 @@ def build_from_settings(\n \n \n @contextmanager\n-def set_environ(**kwargs: str) -> Generator[None, Any, None]:\n+def set_environ(**kwargs: str) -> Iterator[None]:\n     \"\"\"Temporarily set environment variables inside the context manager and\n     fully restore previous environment afterwards\n     \"\"\"\n@@ -244,7 +244,7 @@ def set_environ(**kwargs: str) -> Generator[None, Any, None]:\n                 os.environ[k] = v\n \n \n-def walk_callable(node: ast.AST) -> Generator[ast.AST, Any, None]:\n+def walk_callable(node: ast.AST) -> Iterable[ast.AST]:\n     \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n     functions defined within the node.\n     \"\"\"\n\n@@ -15,12 +15,10 @@ from itertools import chain\n from typing import (\n     TYPE_CHECKING,\n     Any,\n-    AsyncGenerator,\n     AsyncIterable,\n     AsyncIterator,\n     Callable,\n     Dict,\n-    Generator,\n     Iterable,\n     Iterator,\n     List,\n@@ -163,7 +161,7 @@ def re_rsearch(\n     the start position of the match, and the ending (regarding the entire text).\n     \"\"\"\n \n-    def _chunk_iter() -> Generator[Tuple[str, int], Any, None]:\n+    def _chunk_iter() -> Iterable[Tuple[str, int]]:\n         offset = len(text)\n         while True:\n             offset -= chunk_size * 1024\n@@ -351,43 +349,45 @@ else:\n         gc.collect()\n \n \n-class MutableChain(Iterable):\n+class MutableChain(Iterable[_T]):\n     \"\"\"\n     Thin wrapper around itertools.chain, allowing to add iterables \"in-place\"\n     \"\"\"\n \n-    def __init__(self, *args: Iterable):\n-        self.data = chain.from_iterable(args)\n+    def __init__(self, *args: Iterable[_T]):\n+        self.data: Iterator[_T] = chain.from_iterable(args)\n \n-    def extend(self, *iterables: Iterable) -> None:\n+    def extend(self, *iterables: Iterable[_T]) -> None:\n         self.data = chain(self.data, chain.from_iterable(iterables))\n \n-    def __iter__(self) -> Iterator:\n+    def __iter__(self) -> Iterator[_T]:\n         return self\n \n-    def __next__(self) -> Any:\n+    def __next__(self) -> _T:\n         return next(self.data)\n \n \n-async def _async_chain(*iterables: Union[Iterable, AsyncIterable]) -> AsyncGenerator:\n+async def _async_chain(\n+    *iterables: Union[Iterable[_T], AsyncIterable[_T]]\n+) -> AsyncIterator[_T]:\n     for it in iterables:\n         async for o in as_async_generator(it):\n             yield o\n \n \n-class MutableAsyncChain(AsyncIterable):\n+class MutableAsyncChain(AsyncIterable[_T]):\n     \"\"\"\n     Similar to MutableChain but for async iterables\n     \"\"\"\n \n-    def __init__(self, *args: Union[Iterable, AsyncIterable]):\n-        self.data = _async_chain(*args)\n+    def __init__(self, *args: Union[Iterable[_T], AsyncIterable[_T]]):\n+        self.data: AsyncIterator[_T] = _async_chain(*args)\n \n-    def extend(self, *iterables: Union[Iterable, AsyncIterable]) -> None:\n+    def extend(self, *iterables: Union[Iterable[_T], AsyncIterable[_T]]) -> None:\n         self.data = _async_chain(self.data, _async_chain(*iterables))\n \n-    def __aiter__(self) -> AsyncIterator:\n+    def __aiter__(self) -> AsyncIterator[_T]:\n         return self\n \n-    async def __anext__(self) -> Any:\n+    async def __anext__(self) -> _T:\n         return await self.data.__anext__()\n\n@@ -12,7 +12,6 @@ from typing import (\n     TYPE_CHECKING,\n     Any,\n     Dict,\n-    Generator,\n     Iterable,\n     List,\n     Optional,\n@@ -40,9 +39,7 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n \n-def _serialize_headers(\n-    headers: Iterable[bytes], request: Request\n-) -> Generator[bytes, Any, None]:\n+def _serialize_headers(headers: Iterable[bytes], request: Request) -> Iterable[bytes]:\n     for header in headers:\n         if header in request.headers:\n             yield header\n\n@@ -5,7 +5,7 @@ Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n \n-from typing import Any, Dict, Generator, Iterator, Optional, Union\n+from typing import Any, Dict, Iterable, Iterator, Optional, Union\n from urllib.parse import urljoin\n \n import lxml.etree  # nosec\n@@ -42,7 +42,7 @@ class Sitemap:\n \n def sitemap_urls_from_robots(\n     robots_text: str, base_url: Optional[str] = None\n-) -> Generator[str, Any, None]:\n+) -> Iterable[str]:\n     \"\"\"Return an iterator over all sitemap urls contained in the given\n     robots.txt file\n     \"\"\"\n\n@@ -7,7 +7,6 @@ from typing import (\n     TYPE_CHECKING,\n     Any,\n     AsyncGenerator,\n-    Generator,\n     Iterable,\n     Literal,\n     Optional,\n@@ -34,18 +33,20 @@ _T = TypeVar(\"_T\")\n \n # https://stackoverflow.com/questions/60222982\n @overload\n-def iterate_spider_output(result: AsyncGenerator) -> AsyncGenerator: ...  # type: ignore[overload-overlap]\n+def iterate_spider_output(result: AsyncGenerator[_T, None]) -> AsyncGenerator[_T, None]: ...  # type: ignore[overload-overlap]\n \n \n @overload\n-def iterate_spider_output(result: CoroutineType) -> Deferred: ...\n+def iterate_spider_output(result: CoroutineType[Any, Any, _T]) -> Deferred[_T]: ...\n \n \n @overload\n-def iterate_spider_output(result: _T) -> Iterable: ...\n+def iterate_spider_output(result: _T) -> Iterable[Any]: ...\n \n \n-def iterate_spider_output(result: Any) -> Union[Iterable, AsyncGenerator, Deferred]:\n+def iterate_spider_output(\n+    result: Any,\n+) -> Union[Iterable[Any], AsyncGenerator[_T, None], Deferred[_T]]:\n     if inspect.isasyncgen(result):\n         return result\n     if inspect.iscoroutine(result):\n@@ -55,7 +56,7 @@ def iterate_spider_output(result: Any) -> Union[Iterable, AsyncGenerator, Deferr\n     return arg_to_iter(deferred_from_coro(result))\n \n \n-def iter_spider_classes(module: ModuleType) -> Generator[Type[Spider], Any, None]:\n+def iter_spider_classes(module: ModuleType) -> Iterable[Type[Spider]]:\n     \"\"\"Return an iterator over all spider classes defined in the given module\n     that can be instantiated (i.e. which have name)\n     \"\"\"\n\n@@ -13,7 +13,7 @@ from shutil import copytree, rmtree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import TemporaryFile, mkdtemp\n from threading import Timer\n-from typing import Dict, Generator, Optional, Union\n+from typing import Dict, Iterator, Optional, Union\n from unittest import skipIf\n \n from pytest import mark\n@@ -674,7 +674,7 @@ class BadSpider(scrapy.Spider):\n         \"\"\"\n \n     @contextmanager\n-    def _create_file(self, content, name=None) -> Generator[str, None, None]:\n+    def _create_file(self, content, name=None) -> Iterator[str]:\n         tmpdir = Path(self.mktemp())\n         tmpdir.mkdir()\n         if name:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
