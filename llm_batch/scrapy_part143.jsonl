{"custom_id": "scrapy#5fa5c4545b069f3f41f4a8e5f8277344df44c7db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 33 | Churn Cumulative: 1214 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.9047619047619048\n\nDIFF:\n@@ -8,6 +8,7 @@ import marshal\n import json\n import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n+from scrapy.utils.serialize import ScrapyJSONEncoder\n \n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n@@ -79,7 +80,7 @@ class JsonLinesItemExporter(BaseItemExporter):\n     def __init__(self, file, **kwargs):\n         self._configure(kwargs)\n         self.file = file\n-        self.encoder = json.JSONEncoder(**kwargs)\n+        self.encoder = ScrapyJSONEncoder(**kwargs)\n \n     def export_item(self, item):\n         itemdict = dict(self._get_serialized_fields(item))\n@@ -91,7 +92,7 @@ class JsonItemExporter(JsonLinesItemExporter):\n     def __init__(self, file, **kwargs):\n         self._configure(kwargs)\n         self.file = file\n-        self.encoder = json.JSONEncoder(**kwargs)\n+        self.encoder = ScrapyJSONEncoder(**kwargs)\n         self.first_item = True\n \n     def start_exporting(self):\n\n@@ -173,6 +173,8 @@ class XmlItemExporterTest(BaseItemExporterTest):\n \n class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n+    _expected_nested = {'name': u'Jesus', 'age': {'name': 'Maria', 'age': {'name': 'Joseph', 'age': '22'}}}\n+\n     def _get_exporter(self, **kwargs):\n         return JsonLinesItemExporter(self.output, **kwargs)\n \n@@ -180,8 +182,20 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         exported = json.loads(self.output.getvalue().strip())\n         self.assertEqual(exported, dict(self.i))\n \n+    def test_nested_item(self):\n+        i1 = TestItem(name=u'Joseph\\xa3', age='22')\n+        i2 = TestItem(name=u'Maria', age=i1)\n+        i3 = TestItem(name=u'Jesus', age=i2)\n+        self.ie.start_exporting()\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(self.output.getvalue())\n+        self.assertEqual(exported, self._expected_nested)\n+\n class JsonItemExporterTest(JsonLinesItemExporterTest):\n \n+    _expected_nested = [JsonLinesItemExporterTest._expected_nested]\n+\n     def _get_exporter(self, **kwargs):\n         return JsonItemExporter(self.output, **kwargs)\n \n@@ -197,6 +211,17 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         exported = json.loads(self.output.getvalue())\n         self.assertEqual(exported, [dict(self.i), dict(self.i)])\n \n+    def test_nested_item(self):\n+        i1 = TestItem(name=u'Joseph\\xa3', age='22')\n+        i2 = TestItem(name=u'Maria', age=i1)\n+        i3 = TestItem(name=u'Jesus', age=i2)\n+        self.ie.start_exporting()\n+        self.ie.export_item(i3)\n+        self.ie.finish_exporting()\n+        exported = json.loads(self.output.getvalue())\n+        expected = {'name': u'Jesus', 'age': {'name': 'Maria', 'age': dict(i1)}}\n+        self.assertEqual(exported, [expected])\n+\n class CustomItemExporterTest(unittest.TestCase):\n \n     def test_exporter_custom_serializer(self):\n\n@@ -7,6 +7,7 @@ from twisted.internet import defer\n \n from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n+from scrapy.item import BaseItem\n \n \n class SpiderReferencer(object):\n@@ -98,6 +99,8 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n             return str(o)\n         elif isinstance(o, defer.Deferred):\n             return str(o)\n+        elif isinstance(o, BaseItem):\n+            return dict(o)\n         elif isinstance(o, Request):\n             return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n         elif isinstance(o, Response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c95f717ef2833e0cd57599105c24cd7ca088972b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 661 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -183,7 +183,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(exported, dict(self.i))\n \n     def test_nested_item(self):\n-        i1 = TestItem(name=u'Joseph\\xa3', age='22')\n+        i1 = TestItem(name=u'Joseph', age='22')\n         i2 = TestItem(name=u'Maria', age=i1)\n         i3 = TestItem(name=u'Jesus', age=i2)\n         self.ie.start_exporting()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#901987154eebcbaa1ce3e37b7c31892aae353d57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 190 | Lines Deleted: 98 | Files Changed: 7 | Hunks: 21 | Methods Changed: 28 | Complexity Δ (Sum/Max): 17/15 | Churn Δ: 288 | Churn Cumulative: 853 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: 0.7021276595744681\n\nDIFF:\n@@ -1,9 +1,21 @@\n from functools import wraps\n \n+from scrapy.conf import settings\n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n+from scrapy.contracts import ContractsManager\n+from scrapy.utils import display\n+from scrapy.utils.misc import load_object\n+from scrapy.utils.spider import iterate_spider_output\n \n-from scrapy.contracts import Contract\n+def _generate(cb):\n+    \"\"\" create a callback which does not return anything \"\"\"\n+    @wraps(cb)\n+    def wrapper(response):\n+        output = cb(response)\n+        output = list(iterate_spider_output(output))\n+        # display.pprint(output)\n+    return wrapper\n \n class Command(ScrapyCommand):\n     requires_project = True\n@@ -15,6 +27,17 @@ class Command(ScrapyCommand):\n         return \"Check contracts for given spider\"\n \n     def run(self, args, opts):\n+        self.conman = ContractsManager()\n+\n+        # load contracts\n+        contracts = settings['SPIDER_CONTRACTS_BASE'] + \\\n+                settings['SPIDER_CONTRACTS']\n+\n+        for contract in contracts:\n+            concls = load_object(contract)\n+            self.conman.register(concls)\n+\n+        # schedule requests\n         self.crawler.engine.has_capacity = lambda: True\n \n         for spider in args or self.crawler.spiders.list():\n@@ -22,29 +45,19 @@ class Command(ScrapyCommand):\n             requests = self.get_requests(spider)\n             self.crawler.crawl(spider, requests)\n \n+        # start checks\n         self.crawler.start()\n \n     def get_requests(self, spider):\n         requests = []\n \n-        for key, value in vars(type(spider)).iteritems():\n+        for key, value in vars(type(spider)).items():\n             if callable(value) and value.__doc__:\n                 bound_method = value.__get__(spider, type(spider))\n-                request = Request(url='http://scrapy.org', callback=bound_method)\n-\n-                # register contract hooks to the request\n-                contracts = Contract.from_method(value)\n-                for contract in contracts:\n-                    request = contract.prepare_request(request)\n-\n-                # discard anything the request might return\n-                cb = request.callback\n-                @wraps(cb)\n-                def wrapper(response):\n-                    cb(response)\n-\n-                request.callback = wrapper\n+                request = self.conman.from_method(bound_method)\n \n+                if request:\n+                    request.callback = _generate(request.callback)\n                     requests.append(request)\n \n         return requests\n\n@@ -1,2 +1,76 @@\n-from .base import Contract, ContractType\n-from .default import *\n+import re\n+import inspect\n+from functools import wraps\n+\n+from scrapy.http import Request\n+from scrapy.utils.spider import iterate_spider_output\n+from scrapy.utils.misc import get_spec\n+from scrapy.exceptions import ContractFail\n+\n+class ContractsManager(object):\n+    registered = {}\n+\n+    def register(self, contract):\n+        self.registered[contract.name] = contract\n+\n+    def extract_contracts(self, method):\n+        contracts = []\n+        for line in method.__doc__.split('\\n'):\n+            line = line.strip()\n+\n+            if line.startswith('@'):\n+                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n+                args = re.split(r'\\s*\\,\\s*', args)\n+\n+                contracts.append(self.registered[name](method, *args))\n+\n+        return contracts\n+\n+    def from_method(self, method):\n+        contracts = self.extract_contracts(method)\n+        if contracts:\n+            # calculate request args\n+            args = get_spec(Request.__init__)[1]\n+            args['callback'] = method\n+            for contract in contracts:\n+                args = contract.adjust_request_args(args)\n+\n+            # create and prepare request\n+            assert 'url' in args, \"Method '%s' does not have an url contract\" % method.__name__\n+            request = Request(**args)\n+            for contract in contracts:\n+                request = contract.prepare_request(request)\n+\n+            return request\n+\n+class Contract(object):\n+    \"\"\" Abstract class for contracts \"\"\"\n+\n+    def __init__(self, method, *args):\n+        self.method = method\n+        self.args = args\n+\n+    def prepare_request(self, request):\n+        cb = request.callback\n+        @wraps(cb)\n+        def wrapper(response):\n+            self.pre_process(response)\n+            output = list(iterate_spider_output(cb(response)))\n+            self.post_process(output)\n+            return output\n+\n+        request.callback = wrapper\n+        request = self.modify_request(request)\n+        return request\n+\n+    def adjust_request_args(self, args):\n+        return args\n+\n+    def modify_request(self, request):\n+        return request\n+\n+    def pre_process(self, response):\n+        pass\n+\n+    def post_process(self, output):\n+        pass\n\n@@ -1,74 +0,0 @@\n-import re\n-from functools import wraps\n-\n-from scrapy.utils.spider import iterate_spider_output\n-\n-class ContractType(type):\n-    \"\"\" Metaclass for contracts\n-        - automatically registers contracts in the root `Contract` class\n-    \"\"\"\n-\n-    def __new__(meta, name, bases, dct):\n-        # only allow single inheritence\n-        assert len(bases) == 1, 'Multiple inheritance is not allowed'\n-        base = bases[0]\n-\n-        # ascend in inheritence chain\n-        while type(base) not in [type, meta]:\n-            base = type(base)\n-\n-        # register this as a valid contract\n-        cls = type.__new__(meta, name, bases, dct)\n-        if type(base) != type:\n-            base.registered[cls.name] = cls\n-        return cls\n-\n-\n-class Contract(object):\n-    \"\"\" Abstract class for contracts\n-        - keeps a reference of all derived classes in `registered`\n-    \"\"\"\n-\n-    __metaclass__ = ContractType\n-    registered = {}\n-\n-    def __init__(self, method, *args):\n-        self.method = method\n-        self.args = args\n-\n-    @classmethod\n-    def from_method(cls, method):\n-        contracts = []\n-        for line in method.__doc__.split('\\n'):\n-            line = line.strip()\n-\n-            if line.startswith('@'):\n-                name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n-                args = re.split(r'[\\,\\s+]', args)\n-                args = filter(lambda x:x, args)\n-\n-                contracts.append(cls.registered[name](method, *args))\n-\n-        return contracts\n-\n-    def prepare_request(self, request):\n-        cb = request.callback\n-        @wraps(cb)\n-        def wrapper(response):\n-            self.pre_process(response)\n-            output = list(iterate_spider_output(cb(response)))\n-            self.post_process(output)\n-            return output\n-\n-        request.callback = wrapper\n-        request = self.modify_request(request)\n-        return request\n-\n-    def modify_request(self, request):\n-        return request\n-\n-    def pre_process(self, response):\n-        pass\n-\n-    def post_process(self, output):\n-        pass\n\n@@ -1,23 +1,77 @@\n from scrapy.item import BaseItem\n+from scrapy.http import Request\n+from scrapy.exceptions import ContractFail\n \n-from .base import Contract\n+from . import Contract\n \n \n # contracts\n class UrlContract(Contract):\n+    \"\"\" Contract to set the url of the request (mandatory)\n+        @url http://scrapy.org\n+    \"\"\"\n+\n     name = 'url'\n \n-    def modify_request(self, request):\n-        return request.replace(url=self.args[0])\n+    def adjust_request_args(self, args):\n+        args['url'] = self.args[0]\n+        return args\n \n-class ReturnsRequestContract(Contract):\n-    name = 'returns_request'\n+class ReturnsContract(Contract):\n+    \"\"\" Contract to check the output of a callback\n+        @returns items, 1\n+        @returns requests, 1+\n+    \"\"\"\n+\n+    name = 'returns'\n+    objects = {\n+        'requests': Request,\n+        'items': BaseItem,\n+    }\n+\n+    def __init__(self, *args, **kwargs):\n+        super(ReturnsContract, self).__init__(*args, **kwargs)\n+\n+        if len(self.args) != 2:\n+            raise ContractError(\"Returns Contract must have two arguments\")\n+        self.obj_name, self.raw_num = self.args\n+\n+        # validate input\n+        self.obj_type = self.objects[self.obj_name]\n+\n+        self.modifier = self.raw_num[-1]\n+        if self.modifier in ['+', '-']:\n+            self.num = int(self.raw_num[:-1])\n+        else:\n+            self.num = int(self.raw_num)\n+            self.modifier = None\n+\n+    def post_process(self, output):\n+        occurences = 0\n+        for x in output:\n+            if isinstance(x, self.obj_type):\n+                occurences += 1\n+\n+        if self.modifier == '+':\n+            assertion = (occurences >= self.num)\n+        elif self.modifier == '-':\n+            assertion = (occurences <= self.num)\n+        else:\n+            assertion = (occurences == self.num)\n+\n+        if not assertion:\n+            raise ContractFail(\"Returned %s %s, expected %s\" % \\\n+                (occurences, self.obj_name, self.raw_num))\n \n class ScrapesContract(Contract):\n+    \"\"\" Contract to check presence of fields in scraped items\n+        @scrapes page_name, page_body\n+    \"\"\"\n     name = 'scrapes'\n \n     def post_process(self, output):\n         for x in output:\n             if isinstance(x, BaseItem):\n                 for arg in self.args:\n-                    assert arg in x, '%r field is missing' % arg\n+                    if not arg in x:\n+                        raise ContractFail('%r field is missing' % arg)\n\n@@ -50,3 +50,6 @@ class ScrapyDeprecationWarning(Warning):\n     \"\"\"\n     pass\n \n+class ContractFail(Exception):\n+    \"\"\"Error in constructing contracts for a method\"\"\"\n+    pass\n\n@@ -241,3 +241,10 @@ WEBSERVICE_RESOURCES_BASE = {\n     'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n     'scrapy.contrib.webservice.stats.StatsResource': 1,\n }\n+\n+SPIDER_CONTRACTS = []\n+SPIDER_CONTRACTS_BASE = [\n+    'scrapy.contracts.default.UrlContract',\n+    'scrapy.contracts.default.ReturnsContract',\n+    'scrapy.contracts.default.ScrapesContract',\n+]\n\n@@ -1,6 +1,7 @@\n \"\"\"Helper functions which doesn't fit anywhere else\"\"\"\n \n import re\n+import inspect\n import hashlib\n from pkgutil import iter_modules\n \n@@ -104,3 +105,17 @@ def md5sum(file):\n         m.update(d)\n     return m.hexdigest()\n \n+def get_spec(func):\n+    \"\"\"Returns (args, kwargs) touple for a function\n+\n+    >>> import re\n+    >>> get_spec(re.match)\n+    (['pattern', 'string'], {'flags': 0})\n+    \"\"\"\n+    spec = inspect.getargspec(func)\n+    defaults = spec.defaults or []\n+\n+    firstdefault = len(spec.args) - len(defaults)\n+    args = spec.args[:firstdefault]\n+    kwargs = dict(zip(spec.args[firstdefault:], defaults))\n+    return args, kwargs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#babfc6e79bfadab32b9bb4af477a0bf157774e37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 638 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,11 +4,8 @@ Depth Spider Middleware\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n \n-import warnings\n-\n from scrapy import log\n from scrapy.http import Request\n-from scrapy.exceptions import ScrapyDeprecationWarning\n \n class DepthMiddleware(object):\n \n@@ -22,7 +19,6 @@ class DepthMiddleware(object):\n     def from_crawler(cls, crawler):\n         settings = crawler.settings\n         maxdepth = settings.getint('DEPTH_LIMIT')\n-        usestats = settings.getbool('DEPTH_STATS')\n         verbose = settings.getbool('DEPTH_STATS_VERBOSE')\n         prio = settings.getint('DEPTH_PRIORITY')\n         return cls(maxdepth, crawler.stats, verbose, prio)\n\n@@ -216,7 +216,6 @@ SPIDER_MIDDLEWARES_BASE = {\n SPIDER_MODULES = []\n \n STATS_CLASS = 'scrapy.statscol.MemoryStatsCollector'\n-STATS_ENABLED = True\n STATS_DUMP = True\n \n STATSMAILER_RCPTS = []\n\n@@ -90,13 +90,13 @@ class CrawlerRun(object):\n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n                 dispatcher.connect(self.record_signal, signal)\n-        dispatcher.connect(self.item_scraped, signals.item_scraped)\n-        dispatcher.connect(self.request_received, signals.request_received)\n-        dispatcher.connect(self.response_downloaded, signals.response_downloaded)\n \n         self.crawler = get_crawler()\n         self.crawler.install()\n         self.crawler.configure()\n+        self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n+        self.crawler.signals.connect(self.request_received, signals.request_received)\n+        self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n         self.crawler.crawl(self.spider)\n         self.crawler.start()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#52151e8703a1eb803bfb4d8568a3bf6143dae912", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 332 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -151,16 +151,12 @@ class FeedExporter(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        o = cls()\n+        o = cls(crawler.settings)\n         crawler.signals.connect(o.open_spider, signals.spider_opened)\n         crawler.signals.connect(o.close_spider, signals.spider_closed)\n         crawler.signals.connect(o.item_scraped, signals.item_scraped)\n         return o\n \n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        return FeedExporter(crawler.settings)\n-\n     def open_spider(self, spider):\n         uri = self.urifmt % self._get_uri_params(spider)\n         storage = self._get_storage(uri)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70f8e517a19c177e1546157a39a96d00c8c814aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 39 | Churn Cumulative: 204 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,33 +1,6 @@\n-from scrapy.item import Field, Item, ItemMeta\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.contrib_exp.djangoitem` is deprecated, use `scrapy.contrib.djangoitem` instead\",\n+    ScrapyDeprecationWarning, stacklevel=2)\n \n-\n-class DjangoItemMeta(ItemMeta):\n-\n-    def  __new__(mcs, class_name, bases, attrs):\n-        cls = super(DjangoItemMeta, mcs).__new__(mcs, class_name, bases, attrs)\n-        cls.fields = cls.fields.copy()\n-\n-        if cls.django_model:\n-            cls._model_fields = []\n-            cls._model_meta = cls.django_model._meta\n-            for model_field in cls._model_meta.fields:\n-                if model_field.auto_created == False:\n-                    if model_field.name not in cls.fields:\n-                        cls.fields[model_field.name] = Field()\n-                    cls._model_fields.append(model_field.name)\n-        return cls\n-\n-\n-class DjangoItem(Item):\n-\n-    __metaclass__ = DjangoItemMeta\n-\n-    django_model = None\n-\n-    def save(self, commit=True):\n-        modelargs = dict((k, self.get(k)) for k in self._values\n-                         if k in self._model_fields)\n-        model = self.django_model(**modelargs)\n-        if commit:\n-            model.save()\n-        return model\n+from scrapy.contrib.djangoitem import DjangoItem\n\n@@ -1,7 +1,7 @@\n import os\n from twisted.trial import unittest\n \n-from scrapy.contrib_exp.djangoitem import DjangoItem, Field\n+from scrapy.contrib.djangoitem import DjangoItem, Field\n \n os.environ['DJANGO_SETTINGS_MODULE'] = 'scrapy.tests.test_djangoitem.settings'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#280d9d40942ee64feded40f1eeee04cc21f3c434", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 33 | Churn Cumulative: 33 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,33 @@\n+from scrapy.item import Field, Item, ItemMeta\n+\n+\n+class DjangoItemMeta(ItemMeta):\n+\n+    def  __new__(mcs, class_name, bases, attrs):\n+        cls = super(DjangoItemMeta, mcs).__new__(mcs, class_name, bases, attrs)\n+        cls.fields = cls.fields.copy()\n+\n+        if cls.django_model:\n+            cls._model_fields = []\n+            cls._model_meta = cls.django_model._meta\n+            for model_field in cls._model_meta.fields:\n+                if model_field.auto_created == False:\n+                    if model_field.name not in cls.fields:\n+                        cls.fields[model_field.name] = Field()\n+                    cls._model_fields.append(model_field.name)\n+        return cls\n+\n+\n+class DjangoItem(Item):\n+\n+    __metaclass__ = DjangoItemMeta\n+\n+    django_model = None\n+\n+    def save(self, commit=True):\n+        modelargs = dict((k, self.get(k)) for k in self._values\n+                         if k in self._model_fields)\n+        model = self.django_model(**modelargs)\n+        if commit:\n+            model.save()\n+        return model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8aa46a4b5d18ae886379a4d0ee4165c3ed5b94b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 96 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,6 @@ from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n from scrapy.command import ScrapyCommand\n-from scrapy.conf import settings\n from scrapy.utils.template import render_templatefile, string_camelcase\n from scrapy.exceptions import UsageError\n \n@@ -19,14 +18,10 @@ def sanitize_module_name(module_name):\n         module_name = \"a\" + module_name\n     return module_name\n \n-_templates_base_dir = settings['TEMPLATES_DIR'] or join(scrapy.__path__[0], \\\n-    'templates')\n-\n class Command(ScrapyCommand):\n \n     requires_project = True\n     default_settings = {'LOG_ENABLED': False}\n-    templates_dir = join(_templates_base_dir, 'spiders')\n \n     def syntax(self):\n         return \"[options] <name> <domain>\"\n@@ -80,15 +75,15 @@ class Command(ScrapyCommand):\n     def _genspider(self, module, name, domain, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         tvars = {\n-            'project_name': settings.get('BOT_NAME'),\n-            'ProjectName': string_camelcase(settings.get('BOT_NAME')),\n+            'project_name': self.settings.get('BOT_NAME'),\n+            'ProjectName': string_camelcase(self.settings.get('BOT_NAME')),\n             'module': module,\n             'name': name,\n             'domain': domain,\n             'classname': '%sSpider' % ''.join([s.capitalize() \\\n                 for s in module.split('_')])\n         }\n-        spiders_module = __import__(settings['NEWSPIDER_MODULE'], {}, {}, [''])\n+        spiders_module = __import__(self.settings['NEWSPIDER_MODULE'], {}, {}, [''])\n         spiders_dir = abspath(dirname(spiders_module.__file__))\n         spider_file = \"%s.py\" % join(spiders_dir, module)\n         shutil.copyfile(template_file, spider_file)\n@@ -110,3 +105,8 @@ class Command(ScrapyCommand):\n             if filename.endswith('.tmpl'):\n                 print \"  %s\" % splitext(filename)[0]\n \n+    @property\n+    def templates_dir(self):\n+        _templates_base_dir = self.settings['TEMPLATES_DIR'] or \\\n+            join(scrapy.__path__[0], 'templates')\n+        return join(_templates_base_dir, 'spiders')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6217f108cce8a7db0c0ee3682848159b06c9e9d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 17 | Churn Cumulative: 528 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,8 @@ from tempfile import mkdtemp\n \n from twisted.trial import unittest\n \n+from scrapy.utils.python import retry_on_eintr\n+\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -75,11 +77,11 @@ class GenspiderCommandTest(CommandTest):\n         args = ['--template=%s' % tplname] if tplname else []\n         spname = 'test_spider'\n         p = self.proc('genspider', spname, 'test.com', *args)\n-        out = p.stdout.read()\n+        out = retry_on_eintr(p.stdout.read)\n         self.assert_(\"Created spider %r using template %r in module\" % (spname, tplname) in out)\n         self.assert_(exists(join(self.proj_mod_path, 'spiders', 'test_spider.py')))\n         p = self.proc('genspider', spname, 'test.com', *args)\n-        out = p.stdout.read()\n+        out = retry_on_eintr(p.stdout.read)\n         self.assert_(\"Spider %r already exists in module\" % spname in out)\n \n     def test_template_basic(self):\n\n@@ -9,6 +9,7 @@ import os\n import re\n import inspect\n import weakref\n+import errno\n from functools import wraps\n from sgmllib import SGMLParser\n \n@@ -222,3 +223,13 @@ def setattr_default(obj, name, value):\n     \"\"\"\n     if not hasattr(obj, name):\n         setattr(obj, name, value)\n+\n+\n+def retry_on_eintr(function, *args, **kw):\n+    \"\"\"Run a function and retry it while getting EINTR errors\"\"\"\n+    while True:\n+        try:\n+            return function(*args, **kw)\n+        except IOError, e:\n+            if e.errno != errno.EINTR:\n+                raise\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f1a5d8db6adf7e84e506321fb47f6636e4ea8aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 56 | Files Changed: 6 | Hunks: 36 | Methods Changed: 15 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 151 | Churn Cumulative: 857 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from collections import defaultdict\n from functools import wraps\n \n from scrapy.conf import settings\n@@ -7,6 +8,7 @@ from scrapy.contracts import ContractsManager\n from scrapy.utils import display\n from scrapy.utils.misc import load_object\n from scrapy.utils.spider import iterate_spider_output\n+from scrapy.utils.conf import build_component_list\n \n def _generate(cb):\n     \"\"\" create a callback which does not return anything \"\"\"\n@@ -19,6 +21,7 @@ def _generate(cb):\n \n class Command(ScrapyCommand):\n     requires_project = True\n+    default_settings = {'LOG_ENABLED': False}\n \n     def syntax(self):\n         return \"[options] <spider>\"\n@@ -26,26 +29,39 @@ class Command(ScrapyCommand):\n     def short_desc(self):\n         return \"Check contracts for given spider\"\n \n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\", \\\n+            help=\"only list contracts, without checking them\")\n+\n+\n     def run(self, args, opts):\n-        self.conman = ContractsManager()\n-\n         # load contracts\n-        contracts = settings['SPIDER_CONTRACTS_BASE'] + \\\n-                settings['SPIDER_CONTRACTS']\n+        contracts = build_component_list(settings['SPIDER_CONTRACTS_BASE'],\n+                settings['SPIDER_CONTRACTS'])\n+        self.conman = ContractsManager([load_object(c) for c in contracts])\n \n-        for contract in contracts:\n-            concls = load_object(contract)\n-            self.conman.register(concls)\n-\n-        # schedule requests\n+        # contract requests\n+        contract_reqs = defaultdict(list)\n         self.crawler.engine.has_capacity = lambda: True\n \n         for spider in args or self.crawler.spiders.list():\n             spider = self.crawler.spiders.create(spider)\n             requests = self.get_requests(spider)\n+\n+            if opts.list:\n+                for req in requests:\n+                    contract_reqs[spider.name].append(req.callback.__name__)\n+            else:\n                 self.crawler.crawl(spider, requests)\n \n         # start checks\n+        if opts.list:\n+            for spider, methods in sorted(contract_reqs.iteritems()):\n+                print spider\n+                for method in sorted(methods):\n+                    print '  * %s' % method\n+        else:\n             self.crawler.start()\n \n     def get_requests(self, spider):\n\n@@ -8,10 +8,11 @@ from scrapy.utils.misc import get_spec\n from scrapy.exceptions import ContractFail\n \n class ContractsManager(object):\n-    registered = {}\n+    contracts = {}\n \n-    def register(self, contract):\n-        self.registered[contract.name] = contract\n+    def __init__(self, contracts):\n+        for contract in contracts:\n+            self.contracts[contract.name] = contract\n \n     def extract_contracts(self, method):\n         contracts = []\n@@ -20,9 +21,9 @@ class ContractsManager(object):\n \n             if line.startswith('@'):\n                 name, args = re.match(r'@(\\w+)\\s*(.*)', line).groups()\n-                args = re.split(r'\\s*\\,\\s*', args)\n+                args = re.split(r'\\s+', args)\n \n-                contracts.append(self.registered[name](method, *args))\n+                contracts.append(self.contracts[name](method, *args))\n \n         return contracts\n \n@@ -30,16 +31,21 @@ class ContractsManager(object):\n         contracts = self.extract_contracts(method)\n         if contracts:\n             # calculate request args\n-            args = get_spec(Request.__init__)[1]\n-            args['callback'] = method\n+            args, kwargs = get_spec(Request.__init__)\n+            kwargs['callback'] = method\n             for contract in contracts:\n-                args = contract.adjust_request_args(args)\n+                kwargs = contract.adjust_request_args(kwargs)\n \n             # create and prepare request\n-            assert 'url' in args, \"Method '%s' does not have an url contract\" % method.__name__\n-            request = Request(**args)\n+            args.remove('self')\n+            if set(args).issubset(set(kwargs)):\n+                request = Request(**kwargs)\n+\n+                # execute pre and post hooks in order\n+                for contract in reversed(contracts):\n+                    request = contract.add_pre_hook(request)\n                 for contract in contracts:\n-                request = contract.prepare_request(request)\n+                    request = contract.add_post_hook(request)\n \n                 return request\n \n@@ -50,25 +56,30 @@ class Contract(object):\n         self.method = method\n         self.args = args\n \n-    def prepare_request(self, request):\n+    def add_pre_hook(self, request):\n         cb = request.callback\n         @wraps(cb)\n         def wrapper(response):\n             self.pre_process(response)\n+            return list(iterate_spider_output(cb(response)))\n+\n+        request.callback = wrapper\n+        return request\n+\n+    def add_post_hook(self, request):\n+        cb = request.callback\n+        @wraps(cb)\n+        def wrapper(response):\n             output = list(iterate_spider_output(cb(response)))\n             self.post_process(output)\n             return output\n \n         request.callback = wrapper\n-        request = self.modify_request(request)\n         return request\n \n     def adjust_request_args(self, args):\n         return args\n \n-    def modify_request(self, request):\n-        return request\n-\n     def pre_process(self, response):\n         pass\n \n\n@@ -17,34 +17,44 @@ class UrlContract(Contract):\n         args['url'] = self.args[0]\n         return args\n \n+\n class ReturnsContract(Contract):\n     \"\"\" Contract to check the output of a callback\n-        @returns items, 1\n-        @returns requests, 1+\n+\n+        general form:\n+        @returns request(s)/item(s) [min=1 [max]]\n+\n+        e.g.:\n+        @returns request\n+        @returns request 2\n+        @returns request 2 10\n+        @returns request 0 10\n     \"\"\"\n \n     name = 'returns'\n     objects = {\n+        'request': Request,\n         'requests': Request,\n+        'item': BaseItem,\n         'items': BaseItem,\n     }\n \n     def __init__(self, *args, **kwargs):\n         super(ReturnsContract, self).__init__(*args, **kwargs)\n \n-        if len(self.args) != 2:\n-            raise ContractError(\"Returns Contract must have two arguments\")\n-        self.obj_name, self.raw_num = self.args\n-\n-        # validate input\n+        assert len(self.args) in [1, 2, 3]\n+        self.obj_name = self.args[0] or None\n         self.obj_type = self.objects[self.obj_name]\n \n-        self.modifier = self.raw_num[-1]\n-        if self.modifier in ['+', '-']:\n-            self.num = int(self.raw_num[:-1])\n-        else:\n-            self.num = int(self.raw_num)\n-            self.modifier = None\n+        try:\n+            self.min_bound = int(self.args[1])\n+        except IndexError:\n+            self.min_bound = 1\n+\n+        try:\n+            self.max_bound = int(self.args[2])\n+        except IndexError:\n+            self.max_bound = float('inf')\n \n     def post_process(self, output):\n         occurences = 0\n@@ -52,21 +62,23 @@ class ReturnsContract(Contract):\n             if isinstance(x, self.obj_type):\n                 occurences += 1\n \n-        if self.modifier == '+':\n-            assertion = (occurences >= self.num)\n-        elif self.modifier == '-':\n-            assertion = (occurences <= self.num)\n-        else:\n-            assertion = (occurences == self.num)\n+        assertion = (self.min_bound <= occurences <= self.max_bound)\n \n         if not assertion:\n+            if self.min_bound == self.max_bound:\n+                expected = self.min_bound\n+            else:\n+                expected = '%s..%s' % (self.min_bound, self.max_bound)\n+\n             raise ContractFail(\"Returned %s %s, expected %s\" % \\\n-                (occurences, self.obj_name, self.raw_num))\n+                (occurences, self.obj_name, expected))\n+\n \n class ScrapesContract(Contract):\n     \"\"\" Contract to check presence of fields in scraped items\n-        @scrapes page_name, page_body\n+        @scrapes page_name page_body\n     \"\"\"\n+\n     name = 'scrapes'\n \n     def post_process(self, output):\n@@ -74,4 +86,4 @@ class ScrapesContract(Contract):\n             if isinstance(x, BaseItem):\n                 for arg in self.args:\n                     if not arg in x:\n-                        raise ContractFail('%r field is missing' % arg)\n+                        raise ContractFail(\"'%s' field is missing\" % arg)\n\n@@ -50,6 +50,6 @@ class ScrapyDeprecationWarning(Warning):\n     \"\"\"\n     pass\n \n-class ContractFail(Exception):\n-    \"\"\"Error in constructing contracts for a method\"\"\"\n+class ContractFail(AssertionError):\n+    \"\"\"Error raised in case of a failing contract\"\"\"\n     pass\n\n@@ -242,9 +242,9 @@ WEBSERVICE_RESOURCES_BASE = {\n     'scrapy.contrib.webservice.stats.StatsResource': 1,\n }\n \n-SPIDER_CONTRACTS = []\n-SPIDER_CONTRACTS_BASE = [\n-    'scrapy.contracts.default.UrlContract',\n-    'scrapy.contracts.default.ReturnsContract',\n-    'scrapy.contracts.default.ScrapesContract',\n-]\n+SPIDER_CONTRACTS = {}\n+SPIDER_CONTRACTS_BASE = {\n+    'scrapy.contracts.default.UrlContract' : 1,\n+    'scrapy.contracts.default.ReturnsContract': 2,\n+    'scrapy.contracts.default.ScrapesContract': 3,\n+}\n\n@@ -106,7 +106,7 @@ def md5sum(file):\n     return m.hexdigest()\n \n def get_spec(func):\n-    \"\"\"Returns (args, kwargs) touple for a function\n+    \"\"\"Returns (args, kwargs) tuple for a function\n \n     >>> import re\n     >>> get_spec(re.match)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3904f5f55a626b9f189d94e26ca9b3034442e6d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 25 | Churn Cumulative: 447 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,12 +3,13 @@ import os\n from subprocess import Popen, PIPE\n import unittest\n \n+from scrapy.utils.test import get_pythonpath\n+\n class CmdlineTest(unittest.TestCase):\n \n     def setUp(self):\n         self.env = os.environ.copy()\n-        if 'PYTHONPATH' in os.environ:\n-            self.env['PYTHONPATH'] = os.environ['PYTHONPATH']\n+        self.env['PYTHONPATH'] = get_pythonpath()\n         self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'\n \n     def _execute(self, *new_args, **kwargs):\n\n@@ -8,7 +8,7 @@ from tempfile import mkdtemp\n from twisted.trial import unittest\n \n from scrapy.utils.python import retry_on_eintr\n-\n+from scrapy.utils.test import get_pythonpath\n \n class ProjectTest(unittest.TestCase):\n     project_name = 'testproject'\n@@ -19,8 +19,7 @@ class ProjectTest(unittest.TestCase):\n         self.proj_path = join(self.temp_path, self.project_name)\n         self.proj_mod_path = join(self.proj_path, self.project_name)\n         self.env = os.environ.copy()\n-        if 'PYTHONPATH' in os.environ:\n-            self.env['PYTHONPATH'] = os.environ['PYTHONPATH']\n+        self.env['PYTHONPATH'] = get_pythonpath()\n \n     def tearDown(self):\n         rmtree(self.temp_path)\n\n@@ -2,13 +2,10 @@\n This module contains some assorted functions used in tests\n \"\"\"\n \n-import os\n+import os, sys\n \n from twisted.trial.unittest import SkipTest\n \n-from scrapy.crawler import Crawler\n-from scrapy.settings import CrawlerSettings\n-\n def libxml2debug(testfunction):\n     \"\"\"Decorator for debugging libxml2 memory leaks inside a function.\n     \n@@ -51,6 +48,9 @@ def get_crawler(settings_dict=None):\n     will be used as the settings present in the settings module of the\n     CrawlerSettings.\n     \"\"\"\n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import CrawlerSettings\n+\n     class SettingsModuleMock(object):\n         pass\n     settings_module = SettingsModuleMock()\n@@ -59,3 +59,10 @@ def get_crawler(settings_dict=None):\n             setattr(settings_module, k, v)\n     settings = CrawlerSettings(settings_module)\n     return Crawler(settings)\n+\n+def get_pythonpath():\n+    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n+    installation of Scrapy\"\"\"\n+    sep = ';' if sys.platform == 'win32' else ':'\n+    scrapy_path = __import__('scrapy').__path__[0]\n+    return os.path.dirname(scrapy_path) + sep + os.environ.get('PYTHONPATH', '')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94b40162a9f2da77fbc0fc509e8e8be664378e7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 12 | Files Changed: 6 | Hunks: 16 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 43 | Churn Cumulative: 708 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n from unittest import TestCase, main\n-from scrapy.http import Response, XmlResponse, Request\n+from scrapy.http import Response, XmlResponse\n from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n from scrapy.spider import BaseSpider\n from scrapy.tests import get_testdata\n+from scrapy.utils.test import assert_samelines\n \n \n def _test_data(formats):\n@@ -29,13 +30,13 @@ class DecompressionMiddlewareTest(TestCase):\n             new = self.mw.process_response(None, rsp, self.spider)\n             assert isinstance(new, XmlResponse), \\\n                     'Failed %s, response type %s' % (fmt, type(new).__name__)\n-            self.assertEqual(new.body, self.uncompressed_body, fmt)\n+            assert_samelines(self, new.body, self.uncompressed_body, fmt)\n \n     def test_plain_response(self):\n         rsp = Response(url='http://test.com', body=self.uncompressed_body)\n         new = self.mw.process_response(None, rsp, self.spider)\n         assert new is rsp\n-        self.assertEqual(new.body, rsp.body)\n+        assert_samelines(self, new.body, rsp.body)\n \n     def test_empty_response(self):\n         rsp = Response(url='http://test.com', body='')\n\n@@ -6,6 +6,7 @@ from scrapy.contrib_exp.iterators import xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse, Response\n from scrapy.tests import get_testdata\n \n+FOOBAR_NL = u\"foo\" + os.linesep + u\"bar\"\n \n class XmliterTestCase(unittest.TestCase):\n \n@@ -142,7 +143,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual(result,\n                          [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n                           {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n         # explicit type check cuz' we no like stinkin' autocasting! yarrr\n@@ -158,7 +159,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv],\n                          [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n                           {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n     def test_csviter_delimiter_binary_response_assume_utf8_encoding(self):\n@@ -169,7 +170,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv],\n                          [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n                           {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n \n@@ -196,7 +197,7 @@ class UtilsCsvTestCase(unittest.TestCase):\n         self.assertEqual([row for row in csv],\n                          [{u'id': u'1', u'name': u'alpha',   u'value': u'foobar'},\n                           {u'id': u'2', u'name': u'unicode', u'value': u'\\xfan\\xedc\\xf3d\\xe9\\u203d'},\n-                          {u'id': u'3', u'name': u'multi',   u'value': u'foo\\nbar'},\n+                          {u'id': u'3', u'name': u'multi',   u'value': FOOBAR_NL},\n                           {u'id': u'4', u'name': u'empty',   u'value': u''}])\n \n     def test_csviter_exception(self):\n\n@@ -66,3 +66,9 @@ def get_pythonpath():\n     sep = ';' if sys.platform == 'win32' else ':'\n     scrapy_path = __import__('scrapy').__path__[0]\n     return os.path.dirname(scrapy_path) + sep + os.environ.get('PYTHONPATH', '')\n+\n+def assert_samelines(testcase, text1, text2, msg=None):\n+    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n+    line endings between platforms\n+    \"\"\"\n+    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)\n\n@@ -18,9 +18,7 @@ class Launcher(Service):\n         self.processes = {}\n         self.finished = []\n         self.finished_to_keep = config.getint('finished_to_keep', 100)\n-        self.max_proc = config.getint('max_proc', 0)\n-        if not self.max_proc:\n-            self.max_proc = cpu_count() * config.getint('max_proc_per_cpu', 4)\n+        self.max_proc = self._get_max_proc(config)\n         self.runner = config.get('runner', 'scrapyd.runner')\n         self.app = app\n \n@@ -55,6 +53,15 @@ class Launcher(Service):\n         del self.finished[:-self.finished_to_keep] # keep last 100 finished jobs\n         self._wait_for_project(slot)\n \n+    def _get_max_proc(self, config):\n+        max_proc = config.getint('max_proc', 0)\n+        if not max_proc:\n+            try:\n+                cpus = cpu_count()\n+            except NotImplementedError:\n+                cpus = 1\n+            max_proc = cpus * config.getint('max_proc_per_cpu', 4)\n+        return max_proc\n \n class ScrapyProcessProtocol(protocol.ProcessProtocol):\n \n\n@@ -4,6 +4,7 @@ from cStringIO import StringIO\n \n from twisted.trial import unittest\n \n+from scrapy.utils.test import get_pythonpath\n from scrapyd.interfaces import IEggStorage\n from scrapyd.utils import get_crawl_args, get_spider_list\n from scrapyd import get_application\n@@ -45,5 +46,6 @@ class GetSpiderListTest(unittest.TestCase):\n         eggstorage = app.getComponent(IEggStorage)\n         eggfile = StringIO(get_data(__package__, 'mybot.egg'))\n         eggstorage.put(eggfile, 'mybot', 'r1')\n-        self.assertEqual(sorted(get_spider_list('mybot')), ['spider1', 'spider2'])\n+        spiders = get_spider_list('mybot', pythonpath=get_pythonpath())\n+        self.assertEqual(sorted(spiders), ['spider1', 'spider2'])\n \n\n@@ -49,12 +49,14 @@ def get_crawl_args(message):\n         args += ['%s=%s' % (k, v)]\n     return args\n \n-def get_spider_list(project, runner=None):\n+def get_spider_list(project, runner=None, pythonpath=None):\n     \"\"\"Return the spider list from the given project, using the given runner\"\"\"\n     if runner is None:\n         runner = Config().get('runner')\n     env = os.environ.copy()\n     env['SCRAPY_PROJECT'] = project\n+    if pythonpath:\n+        env['PYTHONPATH'] = pythonpath\n     pargs = [sys.executable, '-m', runner, 'list']\n     proc = Popen(pargs, stdout=PIPE, stderr=PIPE, env=env)\n     out, err = proc.communicate()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
