{"custom_id": "scrapy#407562b38b6ab375ae650c8799bdd511025527f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 32 | Files Changed: 4 | Hunks: 13 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 40 | Churn Cumulative: 1608 | Contributors (this commit): 38 | Commits (past 90d): 4 | Contributors (cumulative): 63 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,8 +28,8 @@ twisted_version = (_txv.major, _txv.minor, _txv.micro)\n \n \n # Check minimum required Python version\n-if sys.version_info < (3, 6):\n-    print(f\"Scrapy {__version__} requires Python 3.6+\")\n+if sys.version_info < (3, 7):\n+    print(f\"Scrapy {__version__} requires Python 3.7+\")\n     sys.exit(1)\n \n \n\n@@ -1,11 +0,0 @@\n-import warnings\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.asyncgen import collect_asyncgen  # noqa: F401\n-\n-\n-warnings.warn(\n-    \"Module `scrapy.utils.py36` is deprecated, please import from `scrapy.utils.asyncgen` instead.\",\n-    category=ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\n\n@@ -19,35 +19,29 @@ def has_environment_marker_platform_impl_support():\n \n \n install_requires = [\n-    'Twisted>=17.9.0',\n-    'cryptography>=2.0',\n+    'Twisted>=18.9.0',\n+    'cryptography>=2.8',\n     'cssselect>=0.9.1',\n     'itemloaders>=1.0.1',\n     'parsel>=1.5.0',\n-    'pyOpenSSL>=16.2.0',\n+    'pyOpenSSL>=19.1.0',\n     'queuelib>=1.4.2',\n     'service_identity>=16.0.0',\n     'w3lib>=1.17.0',\n-    'zope.interface>=4.1.3',\n+    'zope.interface>=5.1.0',\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'setuptools',\n     'tldextract',\n+    'lxml>=4.3.0',\n ]\n extras_require = {}\n cpython_dependencies = [\n-    'lxml>=3.5.0',\n     'PyDispatcher>=2.0.5',\n ]\n if has_environment_marker_platform_impl_support():\n     extras_require[':platform_python_implementation == \"CPython\"'] = cpython_dependencies\n     extras_require[':platform_python_implementation == \"PyPy\"'] = [\n-        # Earlier lxml versions are affected by\n-        # https://foss.heptapod.net/pypy/pypy/-/issues/2498,\n-        # which was fixed in Cython 0.26, released on 2017-06-19, and used to\n-        # generate the C headers of lxml release tarballs published since then, the\n-        # first of which was:\n-        'lxml>=4.0.0',\n         'PyPyDispatcher>=2.1.0',\n     ]\n else:\n@@ -84,7 +78,6 @@ setup(\n         'Operating System :: OS Independent',\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n         'Programming Language :: Python :: 3.9',\n@@ -95,7 +88,7 @@ setup(\n         'Topic :: Software Development :: Libraries :: Application Frameworks',\n         'Topic :: Software Development :: Libraries :: Python Modules',\n     ],\n-    python_requires='>=3.6',\n+    python_requires='>=3.7',\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\n\n@@ -3,7 +3,6 @@ import gc\n import operator\n import platform\n import unittest\n-from datetime import datetime\n from itertools import count\n from warnings import catch_warnings, filterwarnings\n \n@@ -224,12 +223,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         elif platform.python_implementation() == 'PyPy':\n             self.assertEqual(get_func_args(str.split, stripself=True), ['sep', 'maxsplit'])\n             self.assertEqual(get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n-\n-            build_date = datetime.strptime(platform.python_build()[1], '%b %d %Y')\n-            if build_date >= datetime(2020, 4, 7):  # PyPy 3.6-v7.3.1\n             self.assertEqual(get_func_args(\" \".join, stripself=True), ['iterable'])\n-            else:\n-                self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n \n     def test_without_none_values(self):\n         self.assertEqual(without_none_values([1, None, 3, 4]), [1, 3, 4])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6770d1ec62012fcfe8a36fdebeeb89cb5157c2df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 44 | Files Changed: 3 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 45 | Churn Cumulative: 926 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,13 +1,11 @@\n import asyncio\n-from unittest import mock, SkipTest\n+from unittest import mock\n \n from pytest import mark\n-from twisted import version as twisted_version\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n-from twisted.python.versions import Version\n \n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n@@ -218,16 +216,6 @@ class MiddlewareUsingCoro(ManagerTestCase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n     def test_asyncdef(self):\n-        if (\n-            self.reactor_pytest == 'asyncio'\n-            and twisted_version < Version('twisted', 18, 4, 0)\n-        ):\n-            raise SkipTest(\n-                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n-                'hangs when using AsyncIO and Twisted versions lower than '\n-                '18.4.0'\n-            )\n-\n         resp = Response('http://example.com/index.html')\n \n         class CoroMiddleware:\n@@ -248,12 +236,6 @@ class MiddlewareUsingCoro(ManagerTestCase):\n \n     @mark.only_asyncio()\n     def test_asyncdef_asyncio(self):\n-        if twisted_version < Version('twisted', 18, 4, 0):\n-            raise SkipTest(\n-                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n-                'hangs when using Twisted versions lower than 18.4.0'\n-            )\n-\n         resp = Response('http://example.com/index.html')\n \n         class CoroMiddleware:\n\n@@ -1,13 +1,10 @@\n import asyncio\n-from unittest import SkipTest\n \n from pydispatch import dispatcher\n from pytest import mark\n from testfixtures import LogCapture\n-from twisted import version as twisted_version\n from twisted.internet import defer, reactor\n from twisted.python.failure import Failure\n-from twisted.python.versions import Version\n from twisted.trial import unittest\n \n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n@@ -81,16 +78,6 @@ class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n         return \"OK\"\n \n     def test_send_catch_log(self):\n-        if (\n-            self.reactor_pytest == 'asyncio'\n-            and twisted_version < Version('twisted', 18, 4, 0)\n-        ):\n-            raise SkipTest(\n-                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n-                'fails due to a timeout when using AsyncIO and Twisted '\n-                'versions lower than 18.4.0'\n-            )\n-\n         return super().test_send_catch_log()\n \n \n@@ -104,13 +91,6 @@ class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n         return await get_from_asyncio_queue(\"OK\")\n \n     def test_send_catch_log(self):\n-        if twisted_version < Version('twisted', 18, 4, 0):\n-            raise SkipTest(\n-                'Due to https://twistedmatrix.com/trac/ticket/9390, this test '\n-                'fails due to a timeout when using Twisted versions lower '\n-                'than 18.4.0'\n-            )\n-\n         return super().test_send_catch_log()\n \n \n\n@@ -4,10 +4,7 @@ Tests borrowed from the twisted.web.client tests.\n \"\"\"\n import os\n import shutil\n-import sys\n-from pkg_resources import parse_version\n \n-import cryptography\n import OpenSSL.SSL\n from twisted.trial import unittest\n from twisted.web import server, static, util, resource\n@@ -417,8 +414,6 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDisabledCipher(self):\n-        if sys.implementation.name == \"pypy\" and parse_version(cryptography.__version__) <= parse_version(\"2.3.1\"):\n-            self.skipTest(\"This test expects a failure, but the code does work in PyPy with cryptography<=2.3.1\")\n         s = \"0123456789\" * 10\n         settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': 'ECDHE-RSA-AES256-GCM-SHA384'})\n         client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ddfd192b704dddfefe2dd78345de239995a40159", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 123 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,6 +38,13 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h.getlist('X-Forwarded-For'), [b'ip1', b'ip2'])\n         assert h.getlist('X-Forwarded-For') is not hlist\n \n+    def test_multivalue_for_one_header(self):\n+        h = Headers(((\"a\", \"b\"), (\"a\", \"c\")))\n+        self.assertEqual(h[\"a\"], b\"c\")\n+        self.assertEqual(h.get(\"a\"), b\"c\")\n+        self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n+        assert h.getlist(\"a\") is not [\"b\", \"c\"]\n+\n     def test_encode_utf8(self):\n         h = Headers({'key': '\\xa3'}, encoding='utf-8')\n         key, val = dict(h).popitem()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a0bcf97cc6016cb966b92170709bc7518cf62c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 9 | Churn Cumulative: 101 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.python import to_unicode\n+from collections.abc import Mapping\n \n \n class Headers(CaselessDict):\n@@ -10,6 +11,13 @@ class Headers(CaselessDict):\n         self.encoding = encoding\n         super().__init__(seq)\n \n+    def update(self, seq):\n+        seq = seq.items() if isinstance(seq, Mapping) else seq\n+        iseq = {}\n+        for k, v in seq:\n+            iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n+        super().update(iseq)\n+\n     def normkey(self, key):\n         \"\"\"Normalize key to bytes\"\"\"\n         return self._tobytes(key.title())\n@@ -86,4 +94,5 @@ class Headers(CaselessDict):\n \n     def __copy__(self):\n         return self.__class__(self)\n+\n     copy = __copy__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a135d6caf050f4b7b5af28d4cccc5d5ef51dbaf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 104 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,8 @@\n+from collections.abc import Mapping\n+\n from w3lib.http import headers_dict_to_raw\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.python import to_unicode\n-from collections.abc import Mapping\n \n \n class Headers(CaselessDict):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#892c2a46554bdf80d49d3f28cc012c49cd1e19ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 124 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -43,7 +43,6 @@ class HeadersTest(unittest.TestCase):\n         self.assertEqual(h[\"a\"], b\"c\")\n         self.assertEqual(h.get(\"a\"), b\"c\")\n         self.assertEqual(h.getlist(\"a\"), [b\"b\", b\"c\"])\n-        assert h.getlist(\"a\") is not [\"b\", \"c\"]\n \n     def test_encode_utf8(self):\n         h = Headers({'key': '\\xa3'}, encoding='utf-8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9e265a2c1f6bccb551e8292785e09462594b8402", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 14 | Files Changed: 7 | Hunks: 17 | Methods Changed: 13 | Complexity Δ (Sum/Max): 12/10 | Churn Δ: 91 | Churn Cumulative: 3557 | Contributors (this commit): 49 | Commits (past 90d): 8 | Contributors (cumulative): 104 | DMM Complexity: 1.0\n\nDIFF:\n@@ -102,11 +102,11 @@ class FTPDownloadHandler:\n \n     def _build_response(self, result, request, protocol):\n         self.result = result\n-        respcls = responsetypes.from_args(url=request.url)\n         protocol.close()\n-        body = protocol.filename or protocol.body.read()\n         headers = {\"local filename\": protocol.filename or '', \"size\": protocol.size}\n-        return respcls(url=request.url, status=200, body=to_bytes(body), headers=headers)\n+        body = to_bytes(protocol.filename or protocol.body.read())\n+        respcls = responsetypes.from_args(url=request.url, body=body)\n+        return respcls(url=request.url, status=200, body=body, headers=headers)\n \n     def _failed(self, result, request):\n         message = result.getErrorMessage()\n\n@@ -112,7 +112,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         request.meta['download_latency'] = self.headers_time - self.start_time\n         status = int(self.status)\n         headers = Headers(self.response_headers)\n-        respcls = responsetypes.from_args(headers=headers, url=self._url)\n+        respcls = responsetypes.from_args(headers=headers, url=self._url, body=body)\n         return respcls(url=self._url, status=status, headers=headers, body=body, protocol=to_unicode(self.version))\n \n     def _set_connection_attributes(self, request):\n\n@@ -240,7 +240,7 @@ class DbmCacheStorage:\n         status = data['status']\n         headers = Headers(data['headers'])\n         body = data['body']\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n+        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n         response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n \n@@ -299,7 +299,7 @@ class FilesystemCacheStorage:\n         url = metadata.get('response_url')\n         status = metadata['status']\n         headers = Headers(headers_raw_to_dict(rawheaders))\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n+        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n         response = respcls(url=url, headers=headers, status=status, body=body)\n         return response\n \n\n@@ -95,11 +95,13 @@ class ResponseTypes:\n         chunk = to_bytes(chunk)\n         if not binary_is_text(chunk):\n             return self.from_mimetype('application/octet-stream')\n-        elif b\"<html>\" in chunk.lower():\n+        lowercase_chunk = chunk.lower()\n+        if b\"<html>\" in lowercase_chunk:\n             return self.from_mimetype('text/html')\n-        elif b\"<?xml\" in chunk.lower():\n+        if b\"<?xml\" in lowercase_chunk:\n             return self.from_mimetype('text/xml')\n-        else:\n+        if b'<!doctype html>' in lowercase_chunk:\n+            return self.from_mimetype('text/html')\n         return self.from_mimetype('text')\n \n     def from_args(self, headers=None, url=None, filename=None, body=None):\n\n@@ -25,7 +25,7 @@ from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.http import Headers, Request\n+from scrapy.http import Headers, HtmlResponse, Request\n from scrapy.http.response.text import TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n@@ -389,6 +389,23 @@ class HttpTestCase(unittest.TestCase):\n         d.addCallback(self.assertEqual, b'159')\n         return d\n \n+    def _test_response_class(self, filename, body, response_class):\n+        def _test(response):\n+            self.assertEqual(type(response), response_class)\n+\n+        request = Request(self.getURL(filename), body=body)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n+\n+    def test_response_class_from_url(self):\n+        return self._test_response_class('foo.html', b'', HtmlResponse)\n+\n+    def test_response_class_from_body(self):\n+        return self._test_response_class(\n+            'foo',\n+            b\"<!DOCTYPE html>\\n<title>.</title>\",\n+            HtmlResponse,\n+        )\n+\n \n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n@@ -971,6 +988,12 @@ class BaseFTPTestCase(unittest.TestCase):\n     password = \"passwd\"\n     req_meta = {\"ftp_user\": username, \"ftp_password\": password}\n \n+    test_files = (\n+        ('file.txt', b\"I have the power!\"),\n+        ('file with spaces.txt', b\"Moooooooooo power!\"),\n+        ('html-file-without-extension', b\"<!DOCTYPE html>\\n<title>.</title>\"),\n+    )\n+\n     def setUp(self):\n         from twisted.protocols.ftp import FTPRealm, FTPFactory\n         from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n@@ -981,8 +1004,8 @@ class BaseFTPTestCase(unittest.TestCase):\n         userdir = os.path.join(self.directory, self.username)\n         os.mkdir(userdir)\n         fp = FilePath(userdir)\n-        fp.child('file.txt').setContent(b\"I have the power!\")\n-        fp.child('file with spaces.txt').setContent(b\"Moooooooooo power!\")\n+        for filename, content in self.test_files:\n+            fp.child(filename).setContent(content)\n \n         # setup server\n         realm = FTPRealm(anonymousRoot=self.directory, userHome=self.directory)\n@@ -1069,6 +1092,27 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         return self._add_test_callbacks(d, _test)\n \n+    def _test_response_class(self, filename, response_class):\n+        f, local_fname = tempfile.mkstemp()\n+        local_fname = to_bytes(local_fname)\n+        os.close(f)\n+        meta = {}\n+        meta.update(self.req_meta)\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/{filename}\",\n+                          meta=meta)\n+        d = self.download_handler.download_request(request, None)\n+\n+        def _test(r):\n+            self.assertEqual(type(r), response_class)\n+            os.remove(local_fname)\n+        return self._add_test_callbacks(d, _test)\n+\n+    def test_response_class_from_url(self):\n+        return self._test_response_class('file.txt', TextResponse)\n+\n+    def test_response_class_from_body(self):\n+        return self._test_response_class('html-file-without-extension', HtmlResponse)\n+\n \n class FTPTestCase(BaseFTPTestCase):\n \n@@ -1104,8 +1148,8 @@ class AnonymousFTPTestCase(BaseFTPTestCase):\n         os.mkdir(self.directory)\n \n         fp = FilePath(self.directory)\n-        fp.child('file.txt').setContent(b\"I have the power!\")\n-        fp.child('file with spaces.txt').setContent(b\"Moooooooooo power!\")\n+        for filename, content in self.test_files:\n+            fp.child(filename).setContent(content)\n \n         # setup server for anonymous access\n         realm = FTPRealm(anonymousRoot=self.directory)\n\n@@ -122,6 +122,21 @@ class DefaultStorageTest(_BaseTest):\n             time.sleep(0.5)  # give the chance to expire\n             assert storage.retrieve_response(self.spider, self.request)\n \n+    def test_storage_no_content_type_header(self):\n+        \"\"\"Test that the response body is used to get the right response class\n+        even if there is no Content-Type header\"\"\"\n+        with self._storage() as storage:\n+            assert storage.retrieve_response(self.spider, self.request) is None\n+            response = Response(\n+                'http://www.example.com',\n+                body=b'<!DOCTYPE html>\\n<title>.</title>',\n+                status=202,\n+            )\n+            storage.store_response(self.spider, self.request, response)\n+            cached_response = storage.retrieve_response(self.spider, self.request)\n+            self.assertIsInstance(cached_response, HtmlResponse)\n+            self.assertEqualResponse(response, cached_response)\n+\n \n class DbmStorageTest(DefaultStorageTest):\n \n\n@@ -54,6 +54,8 @@ class ResponseTypesTest(unittest.TestCase):\n             (b'\\x03\\x02\\xdf\\xdd\\x23', Response),\n             (b'Some plain text\\ndata with tabs\\t and null bytes\\0', TextResponse),\n             (b'<html><head><title>Hello</title></head>', HtmlResponse),\n+            # https://codersblock.com/blog/the-smallest-valid-html5-page/\n+            (b'<!DOCTYPE html>\\n<title>.</title>', HtmlResponse),\n             (b'<?xml version=\"1.0\" encoding=\"utf-8\"', XmlResponse),\n         ]\n         for source, cls in mappings:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e994bda45ac4f95cec0517c4b1be68a38314914", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 16 | Files Changed: 4 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 17/7 | Churn Δ: 26 | Churn Cumulative: 6553 | Contributors (this commit): 44 | Commits (past 90d): 4 | Contributors (cumulative): 78 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,13 +2,13 @@\n Item Exporters are used to export/serialize items into different formats.\n \"\"\"\n \n-from collections import Mapping\n import csv\n import io\n import marshal\n import pickle\n import pprint\n import warnings\n+from collections.abc import Mapping\n from xml.sax.saxutils import XMLGenerator\n \n from itemadapter import is_item, ItemAdapter\n\n@@ -1,6 +1,5 @@\n import json\n import copy\n-from collections import OrderedDict\n from collections.abc import MutableMapping\n from importlib import import_module\n from pprint import pformat\n@@ -199,7 +198,7 @@ class BaseSettings(MutableMapping):\n         return dict(value)\n \n     def getdictorlist(self, name, default=None):\n-        \"\"\"Get a setting value as either an ``OrderedDict`` or a list.\n+        \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n \n         If the setting is already a dict or a list, a copy of it will be\n         returned.\n@@ -209,7 +208,7 @@ class BaseSettings(MutableMapping):\n \n         For example, settings populated from the command line will return:\n \n-        -   ``OrdetedDict([('key1', 'value1'), ('key2', 'value2')])`` if set to\n+        -   ``{'key1': 'value1', 'key2': 'value2'}`` if set to\n             ``'{\"key1\": \"value1\", \"key2\": \"value2\"}'``\n \n         -   ``['one', 'two']`` if set to ``'[\"one\", \"two\"]'`` or ``'one,two'``\n@@ -222,10 +221,10 @@ class BaseSettings(MutableMapping):\n         \"\"\"\n         value = self.get(name, default)\n         if value is None:\n-            return OrderedDict()\n+            return {}\n         if isinstance(value, str):\n             try:\n-                return json.loads(value, object_pairs_hook=OrderedDict)\n+                return json.loads(value)\n             except ValueError:\n                 return value.split(',')\n         return copy.deepcopy(value)\n\n@@ -4,7 +4,6 @@ import marshal\n import pickle\n import tempfile\n import unittest\n-from collections import OrderedDict\n from io import BytesIO\n from datetime import datetime\n from warnings import catch_warnings, filterwarnings\n@@ -114,11 +113,11 @@ class BaseItemExporterTest(unittest.TestCase):\n         self.assertEqual(name, 'John\\xa3')\n \n         ie = self._get_exporter(\n-            fields_to_export=OrderedDict([('name', u'名稱')])\n+            fields_to_export={'name': '名稱'}\n         )\n         self.assertEqual(\n             list(ie._get_serialized_fields(self.i)),\n-            [(u'名稱', u'John\\xa3')]\n+            [('名稱', 'John\\xa3')]\n         )\n \n     def test_field_custom_serializer(self):\n\n@@ -11,7 +11,7 @@ import sys\n import tempfile\n import warnings\n from abc import ABC, abstractmethod\n-from collections import defaultdict, OrderedDict\n+from collections import defaultdict\n from contextlib import ExitStack\n from io import BytesIO\n from logging import getLogger\n@@ -998,9 +998,7 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_items_field_names(self):\n         items = [{'foo': 'bar'}]\n-        header = OrderedDict((\n-            (\"foo\", \"Foo\"),\n-        ))\n+        header = {'foo': 'Foo'}\n         rows = [{'Foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': header}\n         yield self.assertExported(items, list(header.values()), rows,\n@@ -1023,9 +1021,7 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_items_json_field_names(self):\n         items = [{'foo': 'bar'}]\n-        header = OrderedDict((\n-            (\"foo\", \"Foo\"),\n-        ))\n+        header = {'foo': 'Foo'}\n         rows = [{'Foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': json.dumps(header)}\n         yield self.assertExported(items, list(header.values()), rows,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b9ed22becf03311ec014dc9b7e0c09ce87b612c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4253 | Contributors (this commit): 32 | Commits (past 90d): 2 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -1004,8 +1004,6 @@ class FeedExportTest(FeedExportTestBase):\n         yield self.assertExported(items, list(header.values()), rows,\n                                   settings=settings)\n \n-    @pytest.mark.skipif(sys.version_info < (3, 7),\n-                        reason='Only official in Python 3.7+')\n     @defer.inlineCallbacks\n     def test_export_items_dict_field_names(self):\n         items = [{'foo': 'bar'}]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24f382fa459434cccfa4c0a8884a48d09d75e243", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 19 | Churn Cumulative: 4272 | Contributors (this commit): 32 | Commits (past 90d): 3 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -657,8 +657,8 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         return data\n \n     @defer.inlineCallbacks\n-    def assertExported(self, items, header, rows, settings=None, ordered=True):\n-        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n+    def assertExported(self, items, header, rows, settings=None):\n+        yield self.assertExportedCsv(items, header, rows, settings)\n         yield self.assertExportedJsonLines(items, rows, settings)\n         yield self.assertExportedXml(items, rows, settings)\n         yield self.assertExportedPickle(items, rows, settings)\n@@ -719,7 +719,7 @@ class FeedExportTest(FeedExportTestBase):\n         return content\n \n     @defer.inlineCallbacks\n-    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n+    def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n@@ -730,10 +730,7 @@ class FeedExportTest(FeedExportTestBase):\n \n         reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n         got_rows = list(reader)\n-        if ordered:\n         self.assertEqual(reader.fieldnames, header)\n-        else:\n-            self.assertEqual(set(reader.fieldnames), set(header))\n \n         self.assertEqual(rows, got_rows)\n \n@@ -886,7 +883,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n         ]\n         header = self.MyItem.fields.keys()\n-        yield self.assertExported(items, header, rows, ordered=False)\n+        yield self.assertExported(items, header, rows)\n \n     @defer.inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n@@ -958,7 +955,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam4', 'foo': '', 'baz': ''},\n         ]\n         rows_jl = [dict(row) for row in items]\n-        yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n+        yield self.assertExportedCsv(items, header, rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -968,7 +965,7 @@ class FeedExportTest(FeedExportTestBase):\n         header = [\"foo\"]\n         rows = [{'foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': []}\n-        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+        yield self.assertExportedCsv(items, header, rows)\n         yield self.assertExportedJsonLines(items, rows, settings)\n \n     @defer.inlineCallbacks\n@@ -1146,7 +1143,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam', 'foo': 'bar'}\n         ]\n         rows_jl = items\n-        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv, ordered=False)\n+        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -2065,7 +2062,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n-    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n+    def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3729c6d26698ae6b8a7ef297606a1c7630d82619", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 4275 | Contributors (this commit): 32 | Commits (past 90d): 4 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,7 +21,6 @@ from unittest import mock\n from urllib.parse import urljoin, quote\n from urllib.request import pathname2url\n \n-import pytest\n import lxml.etree\n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -731,7 +730,6 @@ class FeedExportTest(FeedExportTestBase):\n         reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n         got_rows = list(reader)\n         self.assertEqual(reader.fieldnames, header)\n-\n         self.assertEqual(rows, got_rows)\n \n     @defer.inlineCallbacks\n@@ -1815,7 +1813,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_lzma_plugin_filters(self):\n-        import sys\n         if \"PyPy\" in sys.version:\n             # https://foss.heptapod.net/pypy/pypy/-/issues/3527\n             raise unittest.SkipTest(\"lzma filters doesn't work in PyPy\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e878490e823a8105276b71ca5f6dc789465d330", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 630 | Contributors (this commit): 57 | Commits (past 90d): 2 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -154,6 +154,7 @@ FEED_EXPORTERS = {}\n FEED_EXPORTERS_BASE = {\n     'json': 'scrapy.exporters.JsonItemExporter',\n     'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',\n+    'jsonl': 'scrapy.exporters.JsonLinesItemExporter',\n     'jl': 'scrapy.exporters.JsonLinesItemExporter',\n     'csv': 'scrapy.exporters.CsvItemExporter',\n     'xml': 'scrapy.exporters.XmlItemExporter',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#516e2d6ec0da77b8e0c01eb5188311b5fbeaa22e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 4295 | Contributors (this commit): 32 | Commits (past 90d): 5 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -656,8 +656,8 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         return data\n \n     @defer.inlineCallbacks\n-    def assertExported(self, items, header, rows, settings=None):\n-        yield self.assertExportedCsv(items, header, rows, settings)\n+    def assertExported(self, items, header, rows, settings=None, ordered=True):\n+        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n         yield self.assertExportedJsonLines(items, rows, settings)\n         yield self.assertExportedXml(items, rows, settings)\n         yield self.assertExportedPickle(items, rows, settings)\n@@ -718,7 +718,7 @@ class FeedExportTest(FeedExportTestBase):\n         return content\n \n     @defer.inlineCallbacks\n-    def assertExportedCsv(self, items, header, rows, settings=None):\n+    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n@@ -729,7 +729,11 @@ class FeedExportTest(FeedExportTestBase):\n \n         reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n         got_rows = list(reader)\n+        if ordered:\n             self.assertEqual(reader.fieldnames, header)\n+        else:\n+            self.assertEqual(set(reader.fieldnames), set(header))\n+\n         self.assertEqual(rows, got_rows)\n \n     @defer.inlineCallbacks\n@@ -881,7 +885,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n         ]\n         header = self.MyItem.fields.keys()\n-        yield self.assertExported(items, header, rows)\n+        yield self.assertExported(items, header, rows, ordered=False)\n \n     @defer.inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n@@ -953,7 +957,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam4', 'foo': '', 'baz': ''},\n         ]\n         rows_jl = [dict(row) for row in items]\n-        yield self.assertExportedCsv(items, header, rows_csv)\n+        yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -963,7 +967,7 @@ class FeedExportTest(FeedExportTestBase):\n         header = [\"foo\"]\n         rows = [{'foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': []}\n-        yield self.assertExportedCsv(items, header, rows)\n+        yield self.assertExportedCsv(items, header, rows, ordered=False)\n         yield self.assertExportedJsonLines(items, rows, settings)\n \n     @defer.inlineCallbacks\n@@ -1141,7 +1145,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam', 'foo': 'bar'}\n         ]\n         rows_jl = items\n-        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv)\n+        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv, ordered=False)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -2059,7 +2063,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n-    def assertExportedCsv(self, items, header, rows, settings=None):\n+    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc285f393ca8ff33ef715f98ef3367c973d23ab3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 4315 | Contributors (this commit): 32 | Commits (past 90d): 6 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -656,8 +656,8 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         return data\n \n     @defer.inlineCallbacks\n-    def assertExported(self, items, header, rows, settings=None, ordered=True):\n-        yield self.assertExportedCsv(items, header, rows, settings, ordered)\n+    def assertExported(self, items, header, rows, settings=None):\n+        yield self.assertExportedCsv(items, header, rows, settings)\n         yield self.assertExportedJsonLines(items, rows, settings)\n         yield self.assertExportedXml(items, rows, settings)\n         yield self.assertExportedPickle(items, rows, settings)\n@@ -718,7 +718,7 @@ class FeedExportTest(FeedExportTestBase):\n         return content\n \n     @defer.inlineCallbacks\n-    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n+    def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n@@ -729,11 +729,7 @@ class FeedExportTest(FeedExportTestBase):\n \n         reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n         got_rows = list(reader)\n-        if ordered:\n         self.assertEqual(reader.fieldnames, header)\n-        else:\n-            self.assertEqual(set(reader.fieldnames), set(header))\n-\n         self.assertEqual(rows, got_rows)\n \n     @defer.inlineCallbacks\n@@ -885,7 +881,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam2', 'foo': 'bar2', 'baz': 'quux2'}\n         ]\n         header = self.MyItem.fields.keys()\n-        yield self.assertExported(items, header, rows, ordered=False)\n+        yield self.assertExported(items, header, rows)\n \n     @defer.inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n@@ -957,7 +953,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam4', 'foo': '', 'baz': ''},\n         ]\n         rows_jl = [dict(row) for row in items]\n-        yield self.assertExportedCsv(items, header, rows_csv, ordered=False)\n+        yield self.assertExportedCsv(items, header, rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -967,7 +963,7 @@ class FeedExportTest(FeedExportTestBase):\n         header = [\"foo\"]\n         rows = [{'foo': 'bar'}]\n         settings = {'FEED_EXPORT_FIELDS': []}\n-        yield self.assertExportedCsv(items, header, rows, ordered=False)\n+        yield self.assertExportedCsv(items, header, rows)\n         yield self.assertExportedJsonLines(items, rows, settings)\n \n     @defer.inlineCallbacks\n@@ -1145,7 +1141,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam', 'foo': 'bar'}\n         ]\n         rows_jl = items\n-        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv, ordered=False)\n+        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n@@ -2063,7 +2059,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             self.assertEqual(expected_batch, got_batch)\n \n     @defer.inlineCallbacks\n-    def assertExportedCsv(self, items, header, rows, settings=None, ordered=True):\n+    def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n         settings.update({\n             'FEEDS': {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec5cf3e9cea3c66aca4cf1aad576f33edca3ad1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4323 | Contributors (this commit): 32 | Commits (past 90d): 7 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -726,11 +726,9 @@ class FeedExportTest(FeedExportTestBase):\n             },\n         })\n         data = yield self.exported_data(items, settings)\n-\n         reader = csv.DictReader(to_unicode(data['csv']).splitlines())\n-        got_rows = list(reader)\n-        self.assertEqual(reader.fieldnames, header)\n-        self.assertEqual(rows, got_rows)\n+        self.assertEqual(reader.fieldnames, list(header))\n+        self.assertEqual(rows, list(reader))\n \n     @defer.inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n@@ -1141,7 +1139,7 @@ class FeedExportTest(FeedExportTestBase):\n             {'egg': 'spam', 'foo': 'bar'}\n         ]\n         rows_jl = items\n-        yield self.assertExportedCsv(items, ['egg', 'foo'], rows_csv)\n+        yield self.assertExportedCsv(items, ['foo', 'egg'], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ef71829b22b7362d08d6897090595138107852f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 168 | Lines Deleted: 9 | Files Changed: 8 | Hunks: 14 | Methods Changed: 23 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 177 | Churn Cumulative: 3118 | Contributors (this commit): 34 | Commits (past 90d): 10 | Contributors (cumulative): 59 | DMM Complexity: 1.0\n\nDIFF:\n@@ -78,8 +78,7 @@ class Crawler:\n             if reactor_class:\n                 install_reactor(reactor_class, self.settings[\"ASYNCIO_EVENT_LOOP\"])\n             else:\n-                from twisted.internet import default\n-                default.install()\n+                from twisted.internet import reactor  # noqa: F401\n             log_reactor_info()\n         if reactor_class:\n             verify_installed_reactor(reactor_class)\n\n@@ -83,7 +83,7 @@ def verify_installed_reactor(reactor_path):\n     path.\"\"\"\n     from twisted.internet import reactor\n     reactor_class = load_object(reactor_path)\n-    if not isinstance(reactor, reactor_class):\n+    if not reactor.__class__ == reactor_class:\n         msg = (\"The installed reactor \"\n                f\"({reactor.__module__}.{reactor.__class__.__name__}) does not \"\n                f\"match the requested one ({reactor_path})\")\n\n@@ -0,0 +1,17 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from twisted.internet import reactor\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n+\n\n@@ -0,0 +1,20 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from twisted.internet import reactor\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n+})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n+\n+\n\n@@ -0,0 +1,19 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from twisted.internet import selectreactor\n+selectreactor.install()\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n+\n+\n\n@@ -0,0 +1,31 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from twisted.internet.main import installReactor\n+from twisted.internet.selectreactor import SelectReactor\n+\n+\n+class SelectReactorSubclass(SelectReactor):\n+    pass\n+\n+\n+reactor = SelectReactorSubclass()\n+installReactor(reactor)\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n+})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n+\n+\n+\n+\n\n@@ -0,0 +1,22 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+from twisted.internet import selectreactor\n+selectreactor.install()\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={\n+    \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n+})\n+\n+process.crawl(NoRequestsSpider)\n+process.start()\n+\n+\n+\n\n@@ -308,6 +308,57 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n         self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n \n+    def test_reactor_default(self):\n+        log = self.run_script('reactor_default.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+\n+    def test_reactor_default_twisted_reactor_select(self):\n+        log = self.run_script('reactor_default_twisted_reactor_select.py')\n+        if platform.system() == 'Windows':\n+            # The goal of this test function is to test that, when a reactor is\n+            # installed (the default one here) and a different reactor is\n+            # configured (select here), an error raises.\n+            #\n+            # In Windows the default reactor is the select reactor, so that\n+            # error does not raise.\n+            #\n+            # If that ever becomes the case on more platforms (i.e. if Linux\n+            # also starts using the select reactor by default in a future\n+            # version of Twisted), then we will need to rethink this test.\n+            self.assertIn('Spider closed (finished)', log)\n+        else:\n+            self.assertNotIn('Spider closed (finished)', log)\n+            self.assertIn(\n+                (\n+                    \"does not match the requested one \"\n+                    \"(twisted.internet.selectreactor.SelectReactor)\"\n+                ),\n+                log,\n+            )\n+\n+    def test_reactor_select(self):\n+        log = self.run_script('reactor_select.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+\n+    def test_reactor_select_twisted_reactor_select(self):\n+        log = self.run_script('reactor_select_twisted_reactor_select.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+\n+    def test_reactor_select_subclass_twisted_reactor_select(self):\n+        log = self.run_script('reactor_select_subclass_twisted_reactor_select.py')\n+        self.assertNotIn('Spider closed (finished)', log)\n+        self.assertIn(\n+            (\n+                \"does not match the requested one \"\n+                \"(twisted.internet.selectreactor.SelectReactor)\"\n+            ),\n+            log,\n+        )\n+\n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n@@ -340,33 +391,33 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             self.assertNotIn(\"TimeoutError\", log)\n             self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n \n-    def test_reactor_select(self):\n+    def test_twisted_reactor_select(self):\n         log = self.run_script(\"twisted_reactor_select.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n \n     @mark.skipif(platform.system() == 'Windows', reason=\"PollReactor is not supported on Windows\")\n-    def test_reactor_poll(self):\n+    def test_twisted_reactor_poll(self):\n         log = self.run_script(\"twisted_reactor_poll.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n \n-    def test_reactor_asyncio(self):\n+    def test_twisted_reactor_asyncio(self):\n         log = self.run_script(\"twisted_reactor_asyncio.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    def test_reactor_asyncio_custom_settings(self):\n+    def test_twisted_reactor_asyncio_custom_settings(self):\n         log = self.run_script(\"twisted_reactor_custom_settings.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    def test_reactor_asyncio_custom_settings_same(self):\n+    def test_twisted_reactor_asyncio_custom_settings_same(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_same.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    def test_reactor_asyncio_custom_settings_conflict(self):\n+    def test_twisted_reactor_asyncio_custom_settings_conflict(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n         self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n         self.assertIn(\"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
