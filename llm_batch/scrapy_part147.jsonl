{"custom_id": "scrapy#1d5967cee6c8ed9d42db69006f75f27c1ebe27bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 8 | Churn Cumulative: 68 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,3 +22,7 @@ class GzTest(unittest.TestCase):\n         with open(join(SAMPLEDIR, 'feed-sample1.xml'), 'rb') as f:\n             self.assertRaises(IOError, gunzip, f.read())\n \n+    def test_gunzip_truncated_short(self):\n+        with open(join(SAMPLEDIR, 'truncated-crc-error-short.gz'), 'rb') as f:\n+            text = gunzip(f.read())\n+            assert text.endswith('</html>')\n\n@@ -17,7 +17,9 @@ def gunzip(data):\n         except (IOError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n-            if output:\n+            # some pages are quite small so output is '' and f.extrabuf\n+            # contains the whole page content\n+            if output or f.extrabuf:\n                 output += f.extrabuf\n                 break\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b038bb8504c03142cb92f9a261710157b6489190", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ class StatsCollector(object):\n \n     def close_spider(self, spider, reason):\n         if self._dump:\n-            log.msg(\"Dumping spider stats:\\n\" + pprint.pformat(self._stats), \\\n+            log.msg(\"Dumping Scrapy stats:\\n\" + pprint.pformat(self._stats), \\\n                 spider=spider)\n         self._persist_stats(self._stats, spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d7106219a87519701e55641aa623a31a2ac021d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 114 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,10 @@\n # This module is kept for backwards compatibility, so users can import\n # scrapy.conf.settings and get the settings they expect\n-#\n-# TODO: Add deprecation warning once all scrapy.conf instances have been\n-# removed from Scrapy codebase.\n \n-import sys\n+from scrapy.project import crawler\n+settings = crawler.settings\n \n-if 'scrapy.cmdline' not in sys.modules:\n-    from scrapy.utils.project import get_project_settings\n-    settings = get_project_settings()\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+warnings.warn(\"Module `scrapy.conf` is deprecated, use `crawler.settings` attribute instead\",\n+    ScrapyDeprecationWarning, stacklevel=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0e2b6e3ff63a241346db19da4f0f0e237c6c56c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 497 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -96,11 +96,6 @@ def execute(argv=None, settings=None):\n         settings = get_project_settings()\n     check_deprecated_settings(settings)\n \n-    # --- backwards compatibility for scrapy.conf.settings singleton ---\n-    from scrapy import conf\n-    conf.settings = settings\n-    # ------------------------------------------------------------------\n-\n     crawler = CrawlerProcess(settings)\n     crawler.install()\n     inproject = inside_project()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9cef3ae69f0ea67eaf74d67ec4c1c5fd9242f93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 146 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ class Command(ScrapyCommand):\n         return \"[options] <spider>\"\n \n     def short_desc(self):\n-        return \"Check contracts for given spider\"\n+        return \"Check spider contracts\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4b8e2633e0c77d6e1d95ae05c64ce92c533b47e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 163 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 15, 1)\n-__version__ = \"0.15.1\"\n+version_info = (0, 17, 0)\n+__version__ = \"0.17.0\"\n \n import sys, os, warnings\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8e5a62c20b3d217cc55038f30e495ca183bbba7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 627 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -96,6 +96,15 @@ def execute(argv=None, settings=None):\n         settings = get_project_settings()\n     check_deprecated_settings(settings)\n \n+    # --- backwards compatibility for scrapy.conf.settings singleton ---\n+    import warnings\n+    from scrapy.exceptions import ScrapyDeprecationWarning\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n+        from scrapy import conf\n+        conf.settings = settings\n+    # ------------------------------------------------------------------\n+\n     crawler = CrawlerProcess(settings)\n     crawler.install()\n     inproject = inside_project()\n\n@@ -1,8 +1,11 @@\n # This module is kept for backwards compatibility, so users can import\n # scrapy.conf.settings and get the settings they expect\n \n-from scrapy.project import crawler\n-settings = crawler.settings\n+import sys\n+\n+if 'scrapy.cmdline' not in sys.modules:\n+    from scrapy.utils.project import get_project_settings\n+    settings = get_project_settings()\n \n import warnings\n from scrapy.exceptions import ScrapyDeprecationWarning\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b01be8619d85d7a8db79602e6faf3bb751caed7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 9 | Churn Cumulative: 109 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,7 +21,7 @@ class LogStats(object):\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        interval = settings.getfloat('LOGSTATS_INTERVAL')\n+        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n         if not interval:\n             raise NotConfigured\n         o = cls(interval)\n@@ -33,13 +33,6 @@ class LogStats(object):\n         crawler.signals.connect(o.engine_stopped, signal=signals.engine_stopped)\n         return o\n \n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        interval = crawler.settings.getfloat('LOGSTATS_INTERVAL')\n-        if not interval:\n-            raise NotConfigured\n-        return cls(interval)\n-\n     def item_scraped(self, spider):\n         self.slots[spider].items += 1\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94629a1f08eaf34d61d9bca068192b616c56a728", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 38 | Files Changed: 4 | Hunks: 39 | Methods Changed: 20 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 111 | Churn Cumulative: 627 | Contributors (this commit): 2 | Commits (past 90d): 19 | Contributors (cumulative): 7 | DMM Complexity: 0.8888888888888888\n\nDIFF:\n@@ -1,11 +1,14 @@\n from collections import defaultdict\n from functools import wraps\n+from unittest.runner import TextTestRunner\n \n+from scrapy import signals\n from scrapy.command import ScrapyCommand\n from scrapy.contracts import ContractsManager\n from scrapy.utils.misc import load_object\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.conf import build_component_list\n+from scrapy.xlib.pydispatch import dispatcher\n \n \n def _generate(cb):\n@@ -39,6 +42,7 @@ class Command(ScrapyCommand):\n             self.settings['SPIDER_CONTRACTS'],\n         )\n         self.conman = ContractsManager([load_object(c) for c in contracts])\n+        self.results = TextTestRunner()._makeResult()\n \n         # contract requests\n         contract_reqs = defaultdict(list)\n@@ -61,6 +65,8 @@ class Command(ScrapyCommand):\n                 for method in sorted(methods):\n                     print '  * %s' % method\n         else:\n+            dispatcher.connect(self.results.printErrors,\n+                    signals.engine_stopped)\n             self.crawler.start()\n \n     def get_requests(self, spider):\n@@ -69,7 +75,7 @@ class Command(ScrapyCommand):\n         for key, value in vars(type(spider)).items():\n             if callable(value) and value.__doc__:\n                 bound_method = value.__get__(spider, type(spider))\n-                request = self.conman.from_method(bound_method)\n+                request = self.conman.from_method(bound_method, self.results)\n \n                 if request:\n                     request.callback = _generate(request.callback)\n\n@@ -1,10 +1,22 @@\n+import sys\n import re\n from functools import wraps\n+from unittest.case import TestCase\n \n from scrapy.http import Request\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_spec\n-from scrapy.exceptions import ContractFail\n+\n+\n+def create_testcase(method):\n+    name = '%s.%s' % (method.__self__.__class__.__name__, method.__name__)\n+\n+    class ContractTestCase(TestCase):\n+        def __str__(self):\n+            return \"%s (%s)\" % (name, method.__self__.name)\n+\n+    setattr(ContractTestCase, name, lambda x: x)\n+    return ContractTestCase(name)\n \n \n class ContractsManager(object):\n@@ -27,7 +39,7 @@ class ContractsManager(object):\n \n         return contracts\n \n-    def from_method(self, method, fail=False):\n+    def from_method(self, method, results):\n         contracts = self.extract_contracts(method)\n         if contracts:\n             # calculate request args\n@@ -43,9 +55,9 @@ class ContractsManager(object):\n \n                 # execute pre and post hooks in order\n                 for contract in reversed(contracts):\n-                    request = contract.add_pre_hook(request, fail)\n+                    request = contract.add_pre_hook(request, results)\n                 for contract in contracts:\n-                    request = contract.add_post_hook(request, fail)\n+                    request = contract.add_post_hook(request, results)\n \n                 return request\n \n@@ -54,39 +66,43 @@ class Contract(object):\n     \"\"\" Abstract class for contracts \"\"\"\n \n     def __init__(self, method, *args):\n-        self.method = method\n+        self.testcase = create_testcase(method)\n         self.args = args\n \n-    def add_pre_hook(self, request, fail=False):\n+    def add_pre_hook(self, request, results):\n         cb = request.callback\n \n         @wraps(cb)\n         def wrapper(response):\n             try:\n                 self.pre_process(response)\n-            except ContractFail as e:\n-                if fail:\n-                    raise\n+            except AssertionError:\n+                results.addFailure(self.testcase, sys.exc_info())\n+            except Exception:\n+                results.addError(self.testcase, sys.exc_info())\n             else:\n-                    print e.format(self.method)\n+                results.addSuccess(self.testcase)\n+            finally:\n                 return list(iterate_spider_output(cb(response)))\n \n         request.callback = wrapper\n         return request\n \n-    def add_post_hook(self, request, fail=False):\n+    def add_post_hook(self, request, results):\n         cb = request.callback\n \n         @wraps(cb)\n         def wrapper(response):\n-            output = list(iterate_spider_output(cb(response)))\n             try:\n+                output = list(iterate_spider_output(cb(response)))\n                 self.post_process(output)\n-            except ContractFail as e:\n-                if fail:\n-                    raise\n+            except AssertionError:\n+                results.addFailure(self.testcase, sys.exc_info())\n+            except Exception:\n+                results.addError(self.testcase, sys.exc_info())\n             else:\n-                    print e.format(self.method)\n+                results.addSuccess(self.testcase)\n+            finally:\n                 return output\n \n         request.callback = wrapper\n\n@@ -52,7 +52,4 @@ class ScrapyDeprecationWarning(Warning):\n \n class ContractFail(AssertionError):\n     \"\"\"Error raised in case of a failing contract\"\"\"\n-    \n-    def format(self, method):\n-        return '[FAILED] %s:%s\\n>>> %s\\n' % \\\n-            (method.im_class.name, method.__name__, self)\n+    pass\n\n@@ -1,9 +1,10 @@\n+from unittest.runner import TextTestRunner\n+\n from twisted.trial import unittest\n \n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n-from scrapy.exceptions import ContractFail\n from scrapy.contracts import ContractsManager\n from scrapy.contracts.default import (\n     UrlContract,\n@@ -71,54 +72,69 @@ class TestSpider(BaseSpider):\n class ContractsManagerTest(unittest.TestCase):\n     contracts = [UrlContract, ReturnsContract, ScrapesContract]\n \n-    def test_contracts(self):\n-        conman = ContractsManager(self.contracts)\n+    def setUp(self):\n+        self.conman = ContractsManager(self.contracts)\n+        self.results = TextTestRunner()._makeResult()\n+        self.results.stream = None\n \n+    def should_succeed(self):\n+        self.assertFalse(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def should_fail(self):\n+        self.assertTrue(self.results.failures)\n+        self.assertFalse(self.results.errors)\n+\n+    def test_contracts(self):\n         # extract contracts correctly\n-        contracts = conman.extract_contracts(TestSpider.returns_request)\n+        contracts = self.conman.extract_contracts(TestSpider.returns_request)\n         self.assertEqual(len(contracts), 2)\n         self.assertEqual(frozenset(map(type, contracts)),\n             frozenset([UrlContract, ReturnsContract]))\n \n         # returns request for valid method\n-        request = conman.from_method(TestSpider.returns_request)\n+        request = self.conman.from_method(TestSpider.returns_request,\n+                self.results)\n         self.assertNotEqual(request, None)\n \n         # no request for missing url\n-        request = conman.from_method(TestSpider.parse_no_url)\n+        request = self.conman.from_method(TestSpider.parse_no_url,\n+                self.results)\n         self.assertEqual(request, None)\n \n     def test_returns(self):\n-        conman = ContractsManager(self.contracts)\n-\n         spider = TestSpider()\n         response = ResponseMock()\n \n         # returns_item\n-        request = conman.from_method(spider.returns_item, fail=True)\n+        request = self.conman.from_method(spider.returns_item, self.results)\n         output = request.callback(response)\n         self.assertEqual(map(type, output), [TestItem])\n+        self.should_succeed()\n \n         # returns_request\n-        request = conman.from_method(spider.returns_request, fail=True)\n+        request = self.conman.from_method(spider.returns_request, self.results)\n         output = request.callback(response)\n         self.assertEqual(map(type, output), [Request])\n+        self.should_succeed()\n \n         # returns_fail\n-        request = conman.from_method(spider.returns_fail, fail=True)\n-        self.assertRaises(ContractFail, request.callback, response)\n+        request = self.conman.from_method(spider.returns_fail, self.results)\n+        request.callback(response)\n+        self.should_fail()\n \n     def test_scrapes(self):\n-        conman = ContractsManager(self.contracts)\n-\n         spider = TestSpider()\n         response = ResponseMock()\n \n         # scrapes_item_ok\n-        request = conman.from_method(spider.scrapes_item_ok, fail=True)\n+        request = self.conman.from_method(spider.scrapes_item_ok, self.results)\n         output = request.callback(response)\n         self.assertEqual(map(type, output), [TestItem])\n+        self.should_succeed()\n \n         # scrapes_item_fail\n-        request = conman.from_method(spider.scrapes_item_fail, fail=True)\n-        self.assertRaises(ContractFail, request.callback, response)\n+        request = self.conman.from_method(spider.scrapes_item_fail,\n+                self.results)\n+        request.callback(response)\n+        self.should_fail()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6fad057568e9ba4178e2b161d8676afcef6e27d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 30 | Files Changed: 3 | Hunks: 24 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 65 | Churn Cumulative: 605 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,8 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n             help=\"only list contracts, without checking them\")\n+        parser.add_option(\"-v\", \"--verbose\", dest=\"verbose\", default=1, action=\"count\",\n+            help=\"print all contract hooks\")\n \n     def run(self, args, opts):\n         # load contracts\n@@ -42,7 +44,7 @@ class Command(ScrapyCommand):\n             self.settings['SPIDER_CONTRACTS'],\n         )\n         self.conman = ContractsManager([load_object(c) for c in contracts])\n-        self.results = TextTestRunner()._makeResult()\n+        self.results = TextTestRunner(verbosity=opts.verbose)._makeResult()\n \n         # contract requests\n         contract_reqs = defaultdict(list)\n\n@@ -8,17 +8,6 @@ from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_spec\n \n \n-def create_testcase(method):\n-    name = '%s.%s' % (method.__self__.__class__.__name__, method.__name__)\n-\n-    class ContractTestCase(TestCase):\n-        def __str__(self):\n-            return \"%s (%s)\" % (name, method.__self__.name)\n-\n-    setattr(ContractTestCase, name, lambda x: x)\n-    return ContractTestCase(name)\n-\n-\n class ContractsManager(object):\n     contracts = {}\n \n@@ -66,53 +55,67 @@ class Contract(object):\n     \"\"\" Abstract class for contracts \"\"\"\n \n     def __init__(self, method, *args):\n-        self.testcase = create_testcase(method)\n+        self.testcase_pre = self.create_testcase(method, 'pre-hook')\n+        self.testcase_post = self.create_testcase(method, 'post-hook')\n         self.args = args\n \n+    def create_testcase(self, method, hook):\n+        spider = method.__self__.name\n+\n+        class ContractTestCase(TestCase):\n+            def __str__(_self):\n+                return \"[%s] %s (@%s %s)\" % (spider, method.__name__, self.name, hook)\n+\n+        name = '%s_%s' % (spider, method.__name__)\n+        setattr(ContractTestCase, name, lambda x: x)\n+        return ContractTestCase(name)\n+\n     def add_pre_hook(self, request, results):\n+        if hasattr(self, 'pre_process'):\n             cb = request.callback\n \n             @wraps(cb)\n             def wrapper(response):\n                 try:\n+                    results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n+                    results.stopTest(self.testcase_pre)\n                 except AssertionError:\n-                results.addFailure(self.testcase, sys.exc_info())\n+                    results.addFailure(self.testcase_pre, sys.exc_info())\n                 except Exception:\n-                results.addError(self.testcase, sys.exc_info())\n+                    results.addError(self.testcase_pre, sys.exc_info())\n                 else:\n-                results.addSuccess(self.testcase)\n+                    results.addSuccess(self.testcase_pre)\n                 finally:\n                     return list(iterate_spider_output(cb(response)))\n \n             request.callback = wrapper\n+\n         return request\n \n     def add_post_hook(self, request, results):\n+        if hasattr(self, 'post_process'):\n             cb = request.callback\n \n             @wraps(cb)\n             def wrapper(response):\n                 try:\n                     output = list(iterate_spider_output(cb(response)))\n+                    results.startTest(self.testcase_post)\n                     self.post_process(output)\n+                    results.stopTest(self.testcase_post)\n                 except AssertionError:\n-                results.addFailure(self.testcase, sys.exc_info())\n+                    results.addFailure(self.testcase_post, sys.exc_info())\n                 except Exception:\n-                results.addError(self.testcase, sys.exc_info())\n+                    results.addError(self.testcase_post, sys.exc_info())\n                 else:\n-                results.addSuccess(self.testcase)\n+                    results.addSuccess(self.testcase_post)\n                 finally:\n                     return output\n \n             request.callback = wrapper\n+\n         return request\n \n     def adjust_request_args(self, args):\n         return args\n-\n-    def pre_process(self, response):\n-        pass\n-\n-    def post_process(self, output):\n-        pass\n\n@@ -86,20 +86,20 @@ class ContractsManagerTest(unittest.TestCase):\n         self.assertFalse(self.results.errors)\n \n     def test_contracts(self):\n+        spider = TestSpider()\n+\n         # extract contracts correctly\n-        contracts = self.conman.extract_contracts(TestSpider.returns_request)\n+        contracts = self.conman.extract_contracts(spider.returns_request)\n         self.assertEqual(len(contracts), 2)\n         self.assertEqual(frozenset(map(type, contracts)),\n             frozenset([UrlContract, ReturnsContract]))\n \n         # returns request for valid method\n-        request = self.conman.from_method(TestSpider.returns_request,\n-                self.results)\n+        request = self.conman.from_method(spider.returns_request, self.results)\n         self.assertNotEqual(request, None)\n \n         # no request for missing url\n-        request = self.conman.from_method(TestSpider.parse_no_url,\n-                self.results)\n+        request = self.conman.from_method(spider.parse_no_url, self.results)\n         self.assertEqual(request, None)\n \n     def test_returns(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0350920f1dd3764183ec0a95f9fee3483b489a12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 611 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n from collections import defaultdict\n from functools import wraps\n-from unittest.runner import TextTestRunner\n+from unittest import TextTestRunner\n \n from scrapy import signals\n from scrapy.command import ScrapyCommand\n\n@@ -1,7 +1,7 @@\n import sys\n import re\n from functools import wraps\n-from unittest.case import TestCase\n+from unittest import TestCase\n \n from scrapy.http import Request\n from scrapy.utils.spider import iterate_spider_output\n\n@@ -1,4 +1,4 @@\n-from unittest.runner import TextTestRunner\n+from unittest import TextTestRunner\n \n from twisted.trial import unittest\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#626662f0f3ef2ce7de63c424da89263443243e97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 68 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ class SpiderState(object):\n \n     def spider_opened(self, spider):\n         if self.jobdir and os.path.exists(self.statefn):\n-            with open(self.statefn) as f:\n+            with open(self.statefn, 'rb') as f:\n                 spider.state = pickle.load(f)\n         else:\n             spider.state = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a438be39f1a6b9a897fcfe0fff5306ef7f518706", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 39 | Churn Cumulative: 1743 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 0.6206896551724138\n\nDIFF:\n@@ -31,9 +31,9 @@ class FormRequest(Request):\n \n     @classmethod\n     def from_response(cls, response, formname=None, formnumber=0, formdata=None,\n-                      clickdata=None, dont_click=False, **kwargs):\n+                      clickdata=None, dont_click=False, formxpath=None, **kwargs):\n         kwargs.setdefault('encoding', response.encoding)\n-        form = _get_form(response, formname, formnumber)\n+        form = _get_form(response, formname, formnumber, formxpath)\n         formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n         url = form.action or form.base_url\n         return cls(url, method=form.method, formdata=formdata, **kwargs)\n@@ -45,7 +45,7 @@ def _urlencode(seq, enc):\n               for v in (vs if hasattr(vs, '__iter__') else [vs])]\n     return urllib.urlencode(values, doseq=1)\n \n-def _get_form(response, formname, formnumber):\n+def _get_form(response, formname, formnumber, formxpath):\n     \"\"\"Find the form element \"\"\"\n     from scrapy.selector.lxmldocument import LxmlDocument\n     root = LxmlDocument(response, lxml.html.HTMLParser)\n@@ -57,6 +57,19 @@ def _get_form(response, formname, formnumber):\n         if f:\n             return f[0]\n     \n+    # Get form element from xpath, if not found, go up\n+    if formxpath is not None:\n+        nodes = root.xpath(formxpath)\n+        if nodes:\n+            el = nodes[0]\n+            while True:\n+                if el.tag == 'form':\n+                    return el\n+                el = el.getparent()\n+                if el is None:\n+                    break\n+        raise ValueError('No <form> element found with %s' % formxpath)\n+\n     # If we get here, it means that either formname was None\n     # or invalid\n     if formnumber is not None:\n\n@@ -591,6 +591,26 @@ class FormRequestTest(RequestTest):\n         fs = _qs(req)\n         self.assertEqual(set(fs), set(['h2', 'i2', 'i1', 'i3', 'h1', 'i5', 'i4']))\n \n+    def test_from_response_xpath(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            <input type=\"hidden\" name=\"two\" value=\"2\">\n+            </form>\n+            <form action=\"post2.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"three\" value=\"3\">\n+            <input type=\"hidden\" name=\"four\" value=\"4\">\n+            </form>\"\"\")\n+        r1 = self.request_class.from_response(response, formxpath=\"//form[@action='post.php']\")\n+        fs = _qs(r1)\n+        self.assertEqual(fs['one'], ['1'])\n+\n+        r1 = self.request_class.from_response(response, formxpath=\"//form/input[@name='four']\")\n+        fs = _qs(r1)\n+        self.assertEqual(fs['three'], ['3'])\n+\n+        self.assertRaises(ValueError, self.request_class.from_response,\n+                          response, formxpath=\"//form/input[@name='abc']\")\n \n def _buildresponse(body, **kwargs):\n     kwargs.setdefault('body', body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d0e962cbb37e2a4db05e9a41639d46e3d44ce45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 13 | Churn Cumulative: 453 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -71,7 +71,7 @@ class BaseSgmlLinkExtractor(FixedSGMLParser):\n                 if self.scan_attr(attr):\n                     url = self.process_value(value)\n                     if url is not None:\n-                        link = Link(url=url)\n+                        link = Link(url=url, nofollow=True if dict(attrs).get('rel') == 'nofollow' else False)\n                         self.links.append(link)\n                         self.current_link = link\n \n\n@@ -85,6 +85,17 @@ class LinkExtractorTestCase(unittest.TestCase):\n         self.assertEqual(lx.matches(url1), True)\n         self.assertEqual(lx.matches(url2), True)\n \n+    def test_link_nofollow(self):\n+        html = \"\"\"\n+        <a href=\"page.html?action=print\" rel=\"nofollow\">Printer-friendly page</a>\n+        <a href=\"about.html\">About us</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/page.html\", body=html)\n+        lx = SgmlLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(response)],\n+            [ Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n+              Link(url='http://example.org/about.html', text=u'About us', nofollow=False) ])\n+\n class SgmlLinkExtractorTestCase(unittest.TestCase):\n     def setUp(self):\n         body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ebaa40ed2f1e5281615afc7b5134715cdada231", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 199 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -151,6 +151,11 @@ class ScrapyClientContextFactory(ClientContextFactory):\n     # see https://github.com/scrapy/scrapy/issues/82\n     # and https://github.com/scrapy/scrapy/issues/26\n \n+    def __init__(self):\n+        # see this issue on why we use TLSv1_METHOD by default\n+        # https://github.com/scrapy/scrapy/issues/194\n+        self.method = SSL.TLSv1_METHOD\n+\n     def getContext(self):\n         ctx = ClientContextFactory.getContext(self)\n         # Enable all workarounds to SSL bugs as documented by\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a60d4207284a7a4aeb9c3044475f71aff04731c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 200 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,6 @@\n from time import time\n from urlparse import urlparse, urlunparse, urldefrag\n \n-from twisted.python import failure\n from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n from twisted.internet import defer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#421dba98a7c6c2281d9fcd89fb8917c99c05bdda", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 347 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -21,8 +21,8 @@ class AutoThrottle(object):\n         self.last_lat = self.START_DELAY, 0.0\n \n     def _min_download_delay(self, settings):\n-        return max(settings.getint(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\"),\n-            settings.getint(\"DOWNLOAD_DELAY\"))\n+        return max(settings.getfloat(\"AUTOTHROTTLE_MIN_DOWNLOAD_DELAY\"),\n+            settings.getfloat(\"DOWNLOAD_DELAY\"))\n \n     def _max_concurency(self, settings):\n         delay = self._min_download_delay(settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9a96147edc051e038e3ffb402698716472b9add", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 200 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -91,7 +91,7 @@ setup_args = {\n     'version': version,\n     'url': 'http://scrapy.org',\n     'description': 'A high-level Python Screen Scraping framework',\n-    'long_description': 'Scrapy is a high level scraping and web crawling framework for writing spiders to crawl and parse web pages for all kinds of purposes, from information retrieval to monitoring or testing web sites.',\n+    'long_description': open('README.rst').read(),\n     'author': 'Scrapy developers',\n     'maintainer': 'Pablo Hoffman',\n     'maintainer_email': 'pablo@pablohoffman.com',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc405e98aa7cac7bb21a6234d80164573c0e420d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 24 | Files Changed: 3 | Hunks: 33 | Methods Changed: 13 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 78 | Churn Cumulative: 585 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,16 @@\n-import unittest, tempfile, shutil, time\n+import time\n+import tempfile\n+import shutil\n+import unittest\n+from contextlib import contextmanager\n \n from scrapy.http import Response, HtmlResponse, Request\n from scrapy.spider import BaseSpider\n-from scrapy.contrib.downloadermiddleware.httpcache import FilesystemCacheStorage, HttpCacheMiddleware\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n+from scrapy.contrib.downloadermiddleware.httpcache import \\\n+    FilesystemCacheStorage, HttpCacheMiddleware\n \n \n class HttpCacheMiddlewareTest(unittest.TestCase):\n@@ -16,8 +21,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         self.crawler = get_crawler()\n         self.spider = BaseSpider('example.com')\n         self.tmpdir = tempfile.mkdtemp()\n-        self.request = Request('http://www.example.com', headers={'User-Agent': 'test'})\n-        self.response = Response('http://www.example.com', headers={'Content-Type': 'text/html'}, body='test body', status=202)\n+        self.request = Request('http://www.example.com',\n+            headers={'User-Agent': 'test'})\n+        self.response = Response('http://www.example.com', headers=\n+            {'Content-Type': 'text/html'}, body='test body', status=202)\n         self.crawler.stats.open_spider(self.spider)\n \n     def tearDown(self):\n@@ -34,54 +41,71 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         settings.update(new_settings)\n         return Settings(settings)\n \n-    def _get_storage(self, **new_settings):\n-        return self.storage_class(self._get_settings(**new_settings))\n+    @contextmanager\n+    def _storage(self, **new_settings):\n+        settings = self._get_settings(**new_settings)\n+        storage = self.storage_class(settings)\n+        storage.open_spider(self.spider)\n+        try:\n+            yield storage\n+        finally:\n+            storage.close_spider(self.spider)\n \n-    def _get_middleware(self, **new_settings):\n-        mw = HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)\n+    @contextmanager\n+    def _middleware(self, **new_settings):\n+        settings = self._get_settings(**new_settings)\n+        mw = HttpCacheMiddleware(settings, self.crawler.stats)\n         mw.spider_opened(self.spider)\n-        return mw\n+        try:\n+            yield mw\n+        finally:\n+            mw.spider_closed(self.spider)\n \n     def test_storage(self):\n-        storage = self._get_storage()\n+        with self._storage() as storage:\n             request2 = self.request.copy()\n             assert storage.retrieve_response(self.spider, request2) is None\n+\n             storage.store_response(self.spider, self.request, self.response)\n             response2 = storage.retrieve_response(self.spider, request2)\n-        assert isinstance(response2, HtmlResponse) # inferred from content-type header\n+            assert isinstance(response2, HtmlResponse)  # content-type header\n             self.assertEqualResponse(self.response, response2)\n+\n             time.sleep(2)  # wait for cache to expire\n             assert storage.retrieve_response(self.spider, request2) is None\n \n     def test_storage_never_expire(self):\n-        storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=0)\n+        with self._storage(HTTPCACHE_EXPIRATION_SECS=0) as storage:\n             assert storage.retrieve_response(self.spider, self.request) is None\n             storage.store_response(self.spider, self.request, self.response)\n             time.sleep(0.5)  # give the chance to expire\n             assert storage.retrieve_response(self.spider, self.request)\n \n     def test_middleware(self):\n-        mw = self._get_middleware()\n+        with self._middleware() as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             mw.process_response(self.request, self.response, self.spider)\n+\n             response = mw.process_request(self.request, self.spider)\n             assert isinstance(response, HtmlResponse)\n             self.assertEqualResponse(self.response, response)\n             assert 'cached' in response.flags\n \n     def test_different_request_response_urls(self):\n-        mw = self._get_middleware()\n+        with self._middleware() as mw:\n             req = Request('http://host.com/path')\n             res = Response('http://host2.net/test.html')\n+\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n+\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response)\n             self.assertEqualResponse(res, cached)\n             assert 'cached' in cached.flags\n \n     def test_middleware_ignore_missing(self):\n-        mw = self._get_middleware(HTTPCACHE_IGNORE_MISSING=True)\n+        with self._middleware(HTTPCACHE_IGNORE_MISSING=True) as mw:\n             self.assertRaises(IgnoreRequest, mw.process_request, self.request, self.spider)\n             mw.process_response(self.request, self.response, self.spider)\n             response = mw.process_request(self.request, self.spider)\n@@ -92,9 +116,10 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n     def test_middleware_ignore_schemes(self):\n         # http responses are cached by default\n         req, res = Request('http://test.com/'), Response('http://test.com/')\n-        mw = self._get_middleware()\n+        with self._middleware() as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n+\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response), type(cached)\n             self.assertEqualResponse(res, cached)\n@@ -102,17 +127,19 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n \n         # file response is not cached by default\n         req, res = Request('file:///tmp/t.txt'), Response('file:///tmp/t.txt')\n-        mw = self._get_middleware()\n+        with self._middleware() as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n+\n             assert mw.storage.retrieve_response(self.spider, req) is None\n             assert mw.process_request(req, self.spider) is None\n \n         # s3 scheme response is cached by default\n         req, res = Request('s3://bucket/key'), Response('http://bucket/key')\n-        mw = self._get_middleware()\n+        with self._middleware() as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n+\n             cached = mw.process_request(req, self.spider)\n             assert isinstance(cached, Response), type(cached)\n             self.assertEqualResponse(res, cached)\n@@ -120,22 +147,24 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n \n         # ignore s3 scheme\n         req, res = Request('s3://bucket/key2'), Response('http://bucket/key2')\n-        mw = self._get_middleware(HTTPCACHE_IGNORE_SCHEMES=['s3'])\n+        with self._middleware(HTTPCACHE_IGNORE_SCHEMES=['s3']) as mw:\n             assert mw.process_request(req, self.spider) is None\n             mw.process_response(req, res, self.spider)\n+\n             assert mw.storage.retrieve_response(self.spider, req) is None\n             assert mw.process_request(req, self.spider) is None\n \n     def test_middleware_ignore_http_codes(self):\n         # test response is not cached\n-        mw = self._get_middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202])\n+        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[202]) as mw:\n             assert mw.process_request(self.request, self.spider) is None\n             mw.process_response(self.request, self.response, self.spider)\n+\n             assert mw.storage.retrieve_response(self.spider, self.request) is None\n             assert mw.process_request(self.request, self.spider) is None\n \n         # test response is cached\n-        mw = self._get_middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203])\n+        with self._middleware(HTTPCACHE_IGNORE_HTTP_CODES=[203]) as mw:\n             mw.process_response(self.request, self.response, self.spider)\n             response = mw.process_request(self.request, self.spider)\n             assert isinstance(response, HtmlResponse)\n@@ -150,4 +179,3 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n \n if __name__ == '__main__':\n     unittest.main()\n-\n\n@@ -8,9 +8,11 @@ from twisted.trial import unittest\n \n try:\n     from PIL import Image\n-    skip = False\n except ImportError, e:\n     skip = True\n+else:\n+    encoders = set(('jpeg_encoder', 'jpeg_decoder'))\n+    skip = not encoders.issubset(set(Image.core.__dict__))\n \n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n\n@@ -45,7 +45,7 @@ class FifoDiskQueue(object):\n         self.chunksize = self.info['chunksize']\n         self.headf = self._openchunk(self.info['head'][0], 'ab+')\n         self.tailf = self._openchunk(self.info['tail'][0])\n-        self.tailf.seek(self.info['tail'][2])\n+        os.lseek(self.tailf.fileno(), self.info['tail'][2], os.SEEK_SET)\n \n     def push(self, string):\n         hnum, hpos = self.info['head']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f899aa4ec7cbb98948f108a6cc6a5e82857ab4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 693 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -198,7 +198,7 @@ class ImagesPipeline(MediaPipeline):\n             key = self.image_key(request.url)\n             checksum = self.image_downloaded(response, request, info)\n         except ImageException, ex:\n-            log.err('image_downloaded hook failed',\n+            log.err('image_downloaded hook failed: %s' % ex,\n                     level=log.WARNING, spider=info.spider)\n             raise\n         except Exception:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcc0a540c071924aa122101cac18d9d3e9dbdc5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 16/10 | Churn Δ: 101 | Churn Cumulative: 101 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,59 @@\n+#!/usr/bin/env python\n+from time import time\n+from collections import deque\n+from twisted.web.server import Site, NOT_DONE_YET\n+from twisted.web.resource import Resource\n+from twisted.internet import reactor\n+\n+\n+class Root(Resource):\n+\n+    def __init__(self):\n+        Resource.__init__(self)\n+        self.concurrent = 0\n+        self.tail = deque(maxlen=100)\n+        self._reset_stats()\n+\n+    def _reset_stats(self):\n+        self.tail.clear()\n+        self.start = self.lastmark = self.lasttime = time()\n+\n+    def getChild(self, request, name):\n+        return self\n+\n+    def render(self, request):\n+        now = time()\n+        delta = now - self.lasttime\n+\n+        # reset stats on high iter-request times caused by client restarts\n+        if delta > 3: # seconds\n+            self._reset_stats()\n+            return ''\n+\n+        self.tail.appendleft(delta)\n+        self.lasttime = now\n+        self.concurrent += 1\n+\n+        if now - self.lastmark >= 3:\n+            self.lastmark = now\n+            qps = len(self.tail) / sum(self.tail)\n+            print('samplesize={0} concurrent={1} qps={2:0.2f}'.format(len(self.tail), self.concurrent, qps))\n+\n+        if 'sleep' in request.args:\n+            sleep = float(request.args['sleep'][0])\n+            reactor.callLater(sleep, self._finish, request)\n+            return NOT_DONE_YET\n+\n+        self.concurrent -= 1\n+        return ''\n+\n+    def _finish(self, request):\n+        self.concurrent -= 1\n+        if not request.finished and not request._disconnected:\n+            request.finish()\n+\n+\n+root = Root()\n+factory = Site(root)\n+reactor.listenTCP(8880, factory)\n+reactor.run()\n\n@@ -0,0 +1,42 @@\n+\"\"\"\n+A spider that generate light requests to meassure QPS troughput\n+\n+usage:\n+\n+    scrapy runspider qpsclient.py --loglevel=INFO --set RANDOMIZE_DOWNLOAD_DELAY=0 --set CONCURRENT_REQUESTS=50 -a qps=10 -a sleep=0.3\n+\n+\"\"\"\n+\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+\n+\n+class QPSSpider(BaseSpider):\n+\n+    name = 'qps'\n+    benchurl = 'http://localhost:8880/'\n+\n+\n+    # Max concurrency is limited by global CONCURRENT_REQUESTS setting\n+    max_concurrent_requests = 8\n+    # Requests per second goal\n+    qps = None # same as: 1 / download_delay\n+    # time in seconds to delay server responses\n+    sleep = None \n+\n+    def __init__(self, *a, **kw):\n+        super(QPSSpider, self).__init__(*a, **kw)\n+        if self.qps is not None:\n+            self.qps = float(self.qps)\n+            self.download_delay = 1 / self.qps\n+\n+    def start_requests(self):\n+        url = self.benchurl\n+        if self.sleep is not None:\n+            url += '?sleep={0}'.format(self.sleep)\n+\n+        while True:\n+            yield Request(url, dont_filter=True)\n+\n+    def parse(self, response):\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
