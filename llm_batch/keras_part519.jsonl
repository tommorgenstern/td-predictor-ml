{"custom_id": "keras#d3dd32f0a6fe7f5d90cf579677c905ebb2597328", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 10902 | Contributors (this commit): 25 | Commits (past 90d): 6 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5596,6 +5596,8 @@ def categorical_focal_crossentropy(\n         lambda: output,\n     )\n \n+    output = output / tf.reduce_sum(output, axis=axis, keepdims=True)\n+\n     epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n     output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d03ec1f9f123cecb1e759e47a9bc7e6921fc31e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 50 | Files Changed: 2 | Hunks: 2 | Methods Changed: 13 | Complexity Δ (Sum/Max): 5/10 | Churn Δ: 137 | Churn Cumulative: 1006 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -377,56 +377,6 @@ class KerasRegularizersTest(test_combinations.TestCase, parameterized.TestCase):\n         model.set_weights(weights)\n         self.assertAllClose(model(inputs), outputs, atol=1e-5)\n \n-    @test_utils.run_v2_only\n-    def test_regularizer_serialize_deserialize_json(self):\n-        @keras.utils.register_keras_serializable()\n-        class MyDense(keras.layers.Layer):\n-            def __init__(\n-                self,\n-                units,\n-                *,\n-                kernel_regularizer=None,\n-                kernel_initializer=None,\n-                **kwargs\n-            ):\n-                super().__init__(**kwargs)\n-                self._units = units\n-                self._kernel_regularizer = kernel_regularizer\n-                self._kernel_initializer = kernel_initializer\n-\n-            def get_config(self):\n-                return dict(\n-                    units=self._units,\n-                    kernel_initializer=self._kernel_initializer,\n-                    kernel_regularizer=self._kernel_regularizer,\n-                    **super().get_config()\n-                )\n-\n-            def build(self, input_shape):\n-                unused_batch_size, input_units = input_shape.as_list()\n-                self._kernel = self.add_weight(\n-                    \"kernel\",\n-                    [input_units, self._units],\n-                    dtype=tf.float32,\n-                    regularizer=self._kernel_regularizer,\n-                    initializer=self._kernel_initializer,\n-                )\n-\n-            def call(self, inputs):\n-                return tf.matmul(inputs, self._kernel)\n-\n-        reg = regularizers.L2(0.101)\n-        ini = keras.initializers.Constant(1.0)\n-        dense = MyDense(4, kernel_regularizer=reg, kernel_initializer=ini)\n-        inputs = keras.layers.Input(shape=[3])\n-        outputs = dense(inputs)\n-        model = keras.Model(inputs, outputs)\n-\n-        model_json = model.to_json()\n-        model2 = keras.models.model_from_json(model_json)\n-\n-        self.assertEqual(model_json, model2.to_json())\n-\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -345,6 +345,93 @@ class SerializationLibTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose(y_out_1, y_out_2, atol=1e-5)\n \n \n+@keras.utils.register_keras_serializable()\n+class MyDense(keras.layers.Layer):\n+    def __init__(\n+        self,\n+        units,\n+        *,\n+        kernel_regularizer=None,\n+        kernel_initializer=None,\n+        **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self._units = units\n+        self._kernel_regularizer = kernel_regularizer\n+        self._kernel_initializer = kernel_initializer\n+\n+    def get_config(self):\n+        return dict(\n+            units=self._units,\n+            kernel_initializer=self._kernel_initializer,\n+            kernel_regularizer=self._kernel_regularizer,\n+            **super().get_config()\n+        )\n+\n+    def build(self, input_shape):\n+        unused_batch_size, input_units = input_shape.as_list()\n+        self._kernel = self.add_weight(\n+            \"kernel\",\n+            [input_units, self._units],\n+            dtype=tf.float32,\n+            regularizer=self._kernel_regularizer,\n+            initializer=self._kernel_initializer,\n+        )\n+\n+    def call(self, inputs):\n+        return tf.matmul(inputs, self._kernel)\n+\n+\n+@keras.utils.register_keras_serializable()\n+class MyWrapper(keras.layers.Layer):\n+    def __init__(self, wrapped, **kwargs):\n+        super().__init__(**kwargs)\n+        self._wrapped = wrapped\n+\n+    def get_config(self):\n+        return dict(wrapped=self._wrapped, **super().get_config())\n+\n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"wrapped\"] = keras.utils.deserialize_keras_object(\n+            config[\"wrapped\"]\n+        )\n+        return cls(**config)\n+\n+    def call(self, inputs):\n+        return self._wrapped(inputs)\n+\n+\n+@test_utils.run_v2_only\n+class JsonSerializationTest(tf.test.TestCase, parameterized.TestCase):\n+    def test_serialize_deserialize_custom_layer_json(self):\n+        reg = keras.regularizers.L2(0.101)\n+        ini = keras.initializers.Constant(1.0)\n+        dense = MyDense(4, kernel_regularizer=reg, kernel_initializer=ini)\n+        inputs = keras.layers.Input(shape=[3])\n+        outputs = dense(inputs)\n+        model = keras.Model(inputs, outputs)\n+\n+        model_json = model.to_json()\n+        model2 = keras.models.model_from_json(model_json)\n+\n+        self.assertEqual(model_json, model2.to_json())\n+\n+    def test_serialize_deserialize_custom_layer_with_wrapper_json(self):\n+        reg = keras.regularizers.L2(0.101)\n+        ini = keras.initializers.Constant(1.0)\n+        dense = MyDense(4, kernel_regularizer=reg, kernel_initializer=ini)\n+        wrapper = MyWrapper(dense)\n+        inputs = keras.layers.Input(shape=[3])\n+        outputs = wrapper(inputs)\n+        model = keras.Model(inputs, outputs)\n+\n+        model_json = model.to_json()\n+        model2 = keras.models.model_from_json(model_json)\n+\n+        self.assertEqual(model_json, model2.to_json())\n+\n+\n @test_utils.run_v2_only\n class BackwardsCompatibilityTest(tf.test.TestCase, parameterized.TestCase):\n     def assert_old_format_can_be_deserialized(self, obj, custom_objects=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16adf8571a5f6415e3adb54415bf09d351fdf3af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 194 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 13 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 195 | Churn Cumulative: 15165 | Contributors (this commit): 28 | Commits (past 90d): 12 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5585,7 +5585,7 @@ def categorical_focal_crossentropy(\n         from_logits=False,\n         axis=-1,\n ):\n-\n+    target.shape.assert_is_compatible_with(output.shape)\n     output, from_logits = _get_logits(\n         output, from_logits, \"Softmax\", \"categorical_focal_crossentropy\"\n     )\n\n@@ -1809,6 +1809,199 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n                 str(w[-1].message),\n             )\n \n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n+    def test_config(self):\n+\n+        cce_obj = losses.CategoricalFocalCrossentropy(name=\"focal_cce\",\n+                                                      reduction=losses_utils.ReductionV2.SUM,\n+                                                      alpha=0.25,\n+                                                      gamma=2.0)\n+        self.assertEqual(cce_obj.name, \"focal_cce\")\n+        self.assertEqual(cce_obj.reduction, losses_utils.ReductionV2.SUM)\n+        self.assertEqual(cce_obj.alpha, 0.25)\n+        self.assertEqual(cce_obj.gamma, 2.0)\n+\n+        # Test alpha as a list\n+        cce_obj = losses.CategoricalFocalCrossentropy(alpha=[0.25, 0.5, 0.75])\n+        self.assertEqual(cce_obj.alpha, [0.25, 0.5, 0.75])\n+\n+    def test_all_correct_unweighted(self):\n+        y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.int64)\n+        y_pred = tf.constant(\n+            [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]],\n+            dtype=tf.float32,\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(alpha=0.25, gamma=2.0)\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n+\n+        # Test with logits.\n+        logits = tf.constant(\n+            [[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.0, 3)\n+\n+    def test_unweighted(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=tf.float32,\n+        )\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.02059, 3)\n+\n+        # Test with logits.\n+        logits = tf.constant(\n+            [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.000345, 3)\n+\n+    def test_scalar_weighted(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=tf.float32,\n+        )\n+        loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.047368, 3)\n+\n+        # Test with logits.\n+        logits = tf.constant(\n+            [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=2.3)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.000794, 4)\n+\n+    def test_sample_weighted(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        y_pred = tf.constant(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]],\n+            dtype=tf.float32,\n+        )\n+        sample_weight = tf.constant([[1.2], [3.4], [5.6]], shape=(3, 1))\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.06987, 3)\n+\n+        # Test with logits.\n+        logits = tf.constant(\n+            [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.001933, 3)\n+\n+    def test_no_reduction(self):\n+        y_true = tf.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n+        logits = tf.constant(\n+            [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            from_logits=True, reduction=losses_utils.ReductionV2.NONE\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose(\n+            (1.5096224e-09, 2.4136547e-11, 1.0360638e-03), self.evaluate(loss), 3\n+        )\n+\n+    def test_label_smoothing(self):\n+        logits = tf.constant([[4.9, -0.5, 2.05]])\n+        y_true = tf.constant([[1, 0, 0]])\n+        label_smoothing = 0.1\n+\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing)\n+        loss = cce_obj(y_true, logits)\n+\n+        expected_value = 0.06685\n+        self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n+\n+    def test_label_smoothing_ndarray(self):\n+        logits = np.asarray([[4.9, -0.5, 2.05]])\n+        y_true = np.asarray([[1, 0, 0]])\n+        label_smoothing = 0.1\n+\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing)\n+        loss = cce_obj(y_true, logits)\n+\n+        expected_value = 0.06685\n+        self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n+\n+    def test_shape_mismatch(self):\n+        y_true = tf.constant([[0], [1], [2]])\n+        y_pred = tf.constant(\n+            [[0.9, 0.05, 0.05], [0.5, 0.89, 0.6], [0.05, 0.01, 0.94]]\n+        )\n+\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        with self.assertRaisesRegex(ValueError, \"Shapes .+ are incompatible\"):\n+            cce_obj(y_true, y_pred)\n+\n+    def test_ragged_tensors(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])\n+        y_pred = tf.ragged.constant(\n+            [[[0.9, 0.05, 0.05], [0.5, 0.89, 0.6]], [[0.05, 0.01, 0.94]]],\n+            dtype=tf.float32,\n+        )\n+        # batch losses [[0.1054, 0.8047], [0.0619]]\n+        sample_weight = tf.constant([[1.2], [3.4]], shape=(2, 1))\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        self.assertAlmostEqual(self.evaluate(loss), 0.024754, 3)\n+\n+        # Test with logits.\n+        logits = tf.ragged.constant(\n+            [[[8.0, 1.0, 1.0], [0.0, 9.0, 1.0]], [[2.0, 3.0, 5.0]]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.00117, 3)\n+\n+    def test_ragged_tensors_ragged_sample_weights(self):\n+        cce_obj = losses.CategoricalFocalCrossentropy()\n+        y_true = tf.ragged.constant([[[1, 0, 0], [0, 1, 0]], [[0, 0, 1]]])\n+        y_pred = tf.ragged.constant(\n+            [[[0.9, 0.05, 0.05], [0.05, 0.89, 0.06]], [[0.05, 0.01, 0.94]]],\n+            dtype=tf.float32,\n+        )\n+        sample_weight = tf.ragged.constant(\n+            [[1.2, 3.4], [5.6]], dtype=tf.float32\n+        )\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.0006088, 4)\n+\n+        # Test with logits.\n+        logits = tf.ragged.constant(\n+            [[[8.0, 1.0, 1.0], [0.0, 9.0, 1.0]], [[2.0, 3.0, 5.0]]]\n+        )\n+        cce_obj = losses.CategoricalFocalCrossentropy(from_logits=True)\n+\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.001933, 3)\n+\n+    def test_binary_labels(self):\n+        # raise a warning if the shape of y_true and y_pred are all (None, 1).\n+        # categorical_crossentropy shouldn't be used with binary labels.\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter(\"always\")\n+            cce_obj = losses.CategoricalFocalCrossentropy()\n+            cce_obj(tf.constant([[1.0], [0.0]]), tf.constant([[1.0], [1.0]]))\n+            self.assertIs(w[-1].category, SyntaxWarning)\n+            self.assertIn(\n+                \"In loss categorical_focal_crossentropy, expected \",\n+                str(w[-1].message),\n+            )\n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc38e33153715846c5faaacb02f4e354a22b0561", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 47 | Churn Cumulative: 4868 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2244,6 +2244,19 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         self.assertArrayNear(result[0], [7.995, 0.022, 0.701], 1e-3)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_categorical_focal_crossentropy_with_softmax(self):\n+        t = backend.constant([[0, 1, 0]])\n+        logits = backend.constant([[8.0, 1.0, 1.0]])\n+        p = backend.softmax(logits)\n+        p = tf.identity(tf.identity(p))\n+        result = self.evaluate(\n+            backend.categorical_focal_crossentropy(t, p, gamma=2.0)\n+        )\n+        self.assertArrayNear(result, [1.747], 1e-3)\n+\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -2260,6 +2273,21 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         self.assertArrayNear(result[0], [7.995, 0.022, 0.701], 1e-3)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_categorical_focal_crossentropy_from_logits(self):\n+        t = backend.constant([[0, 1, 0]])\n+        logits = backend.constant([[8.0, 1.0, 1.0]])\n+        result = self.evaluate(\n+            backend.categorical_focal_crossentropy(\n+                target=t,\n+                output=logits,\n+                from_logits=True,\n+            )\n+        )\n+        self.assertArrayNear(result, [1.7472], 1e-3)\n+\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -2279,6 +2307,25 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         non_focal_result = self.evaluate(backend.binary_crossentropy(t, p))\n         self.assertArrayNear(focal_result[0], non_focal_result[0], 1e-3)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_categorical_focal_crossentropy_no_focal_effect(self):\n+        t = backend.constant([[0, 1, 0]])\n+        logits = backend.constant([[8.0, 1.0, 1.0]])\n+        p = backend.softmax(logits)\n+        p = tf.identity(tf.identity(p))\n+        focal_result = self.evaluate(\n+            backend.categorical_focal_crossentropy(\n+                target=t,\n+                output=p,\n+                gamma=0.0,\n+                alpha=1.0,\n+            )\n+        )\n+        non_focal_result = self.evaluate(backend.categorical_crossentropy(t, p))\n+        self.assertArrayNear(focal_result, non_focal_result, 1e-3)\n+\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#363baaf4c8c508de5a333b1e75a8cf6bbb9b6183", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 156 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 157 | Churn Cumulative: 17335 | Contributors (this commit): 46 | Commits (past 90d): 17 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5585,6 +5585,36 @@ def categorical_focal_crossentropy(\n         from_logits=False,\n         axis=-1,\n ):\n+    \"\"\"Categorical focal crossentropy (alpha balanced) between an output tensor and a target tensor.\n+            According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+            helps to apply a focal factor to down-weight easy examples and focus more on\n+            hard examples. By default, the focal tensor is computed as follows:\n+            It has pt defined as:\n+            pt = p, if y = 1 else 1 - p\n+            The authors use alpha-balanced variant of focal loss in the paper:\n+            FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n+            Extending this to multi-class case is straightforward:\n+            FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from negative log-likelihood and included in CE.\n+            `modulating_factor` is (1 − pt)^gamma,\n+            where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+            effect on the categorical crossentropy.\n+            Args:\n+              target: A tensor with the same shape as `output`.\n+              output: A tensor.\n+              alpha: A weight balancing factor for all classes, default is `0.25` as\n+                     mentioned in the reference. It can be a list of floats or a scalar.\n+                     In the multi-class case, alpha may be set by inverse class frequency by\n+                     using `compute_class_weight` from `sklearn.utils`.\n+              gamma: A focusing parameter, default is `2.0` as mentioned in the\n+                     reference. It helps to gradually reduce the importance given to\n+                     simple examples in a smooth manner.\n+              from_logits: Whether `output` is expected to be a logits tensor. By\n+                default, we consider that `output` encodes a probability distribution.\n+            Returns:\n+              A tensor.\n+            \"\"\"\n+    target = tf.convert_to_tensor(target)\n+    output = tf.convert_to_tensor(output)\n     target.shape.assert_is_compatible_with(output.shape)\n     output, from_logits = _get_logits(\n         output, from_logits, \"Softmax\", \"categorical_focal_crossentropy\"\n\n@@ -923,6 +923,76 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n class CategoricalFocalCrossentropy(LossFunctionWrapper):\n+    \"\"\"Computes the alpha balanced focal crossentropy loss between the labels and predictions.\n+        According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n+        helps to apply a focal factor to down-weight easy examples and focus more on\n+        hard examples. By default, the focal tensor is computed as follows:\n+        It has pt defined as:\n+        pt = p, if y = 1 else 1 - p\n+        The authors use alpha-balanced variant of focal loss in the paper:\n+        FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n+        Extending this to multi-class case is straightforward:\n+        FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from negative log-likelihood and included in CE.\n+        `modulating_factor` is (1 − pt)^gamma,\n+        where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n+        effect on the categorical crossentropy. And if alpha = 1, at the same time the loss is\n+        equivalent to the categorical crossentropy.\n+        In the snippet below, there is `# classes` floating pointing values per\n+        example. The shape of both `y_pred` and `y_true` are\n+        `[batch_size, num_classes]`.\n+        Standalone usage:\n+        >>> y_true = [[0., 1., 0.], [0., 0., 1.]]\n+        >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+        >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n+        >>> cce = tf.keras.losses.CategoricalFocalCrossentropy()\n+        >>> cce(y_true, y_pred).numpy()\n+        0.23315276\n+        >>> # Calling with 'sample_weight'.\n+        >>> cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()\n+        0.1632\n+        >>> # Using 'sum' reduction type.\n+        >>> cce = tf.keras.losses.CategoricalFocalCrossentropy(\n+        ...     reduction=tf.keras.losses.Reduction.SUM)\n+        >>> cce(y_true, y_pred).numpy()\n+        0.46631\n+        >>> # Using 'none' reduction type.\n+        >>> cce = tf.keras.losses.CategoricalFocalCrossentropy(\n+        ...     reduction=tf.keras.losses.Reduction.NONE)\n+        >>> cce(y_true, y_pred).numpy()\n+        array([3.2058331e-05, 4.6627346e-01], dtype=float32)\n+        Usage with the `compile()` API:\n+        ```python\n+        model.compile(optimizer='sgd',\n+                      loss=tf.keras.losses.CategoricalFocalCrossentropy())\n+        ```\n+        Args:\n+          alpha: A weight balancing factor for all classes, default is `0.25` as\n+                 mentioned in the reference. It can be a list of floats or a scalar.\n+                 In the multi-class case, alpha may be set by inverse class frequency by\n+                 using `compute_class_weight` from `sklearn.utils`.\n+          gamma: A focusing parameter, default is `2.0` as mentioned in the\n+                 reference. It helps to gradually reduce the importance given to\n+                 simple (easy) examples in a smooth manner.\n+          from_logits: Whether `output` is expected to be a logits tensor. By\n+            default, we consider that `output` encodes a probability distribution.\n+          label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n+            meaning the confidence on label values are relaxed. For example, if\n+            `0.1`, use `0.1 / num_classes` for non-target labels and\n+            `0.9 + 0.1 / num_classes` for target labels.\n+          axis: The axis along which to compute crossentropy (the features\n+            axis). Defaults to -1.\n+          reduction: Type of `tf.keras.losses.Reduction` to apply to\n+            loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n+            option will be determined by the usage context. For almost all cases\n+            this defaults to `SUM_OVER_BATCH_SIZE`. When used under a\n+            `tf.distribute.Strategy`, except via `Model.compile()` and\n+            `Model.fit()`, using `AUTO` or `SUM_OVER_BATCH_SIZE`\n+            will raise an error. Please see this custom training [tutorial](\n+            https://www.tensorflow.org/tutorials/distribute/custom_training)\n+            for more details.\n+          name: Optional name for the instance.\n+            Defaults to 'categorical_focal_crossentropy'.\n+        \"\"\"\n     def __init__(\n             self,\n             alpha=0.25,\n@@ -2073,7 +2143,35 @@ def categorical_focal_crossentropy(\n         label_smoothing=0.0,\n         axis=-1,\n ):\n-\n+    \"\"\"Computes the categorical focal crossentropy loss.\n+        Standalone usage:\n+        >>> y_true = [[0, 1, 0], [0, 0, 1]]\n+        >>> y_pred = [[0.05, 0.9, 0.05], [0.1, 0.85, 0.05]]\n+        >>> loss = tf.keras.losses.categorical_focal_crossentropy(y_true, y_pred)\n+        >>> assert loss.shape == (2,)\n+        >>> loss.numpy()\n+        array([2.63401289e-04, 6.75912094e-01], dtype=float32)\n+        Args:\n+          y_true: Tensor of one-hot true targets.\n+          y_pred: Tensor of predicted targets.\n+          alpha: A weight balancing factor for all classes, default is `0.25` as\n+             mentioned in the reference. It can be a list of floats or a scalar.\n+             In the multi-class case, alpha may be set by inverse class frequency by\n+             using `compute_class_weight` from `sklearn.utils`.\n+          gamma: A focusing parameter, default is `2.0` as mentioned in the\n+             reference. It helps to gradually reduce the importance given to\n+             simple examples in a smooth manner. When `gamma` = 0, there is no focal\n+             effect on the categorical crossentropy.\n+          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+          label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n+            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+            and `0.9 + 0.1 / num_classes` for target labels.\n+          axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+        Returns:\n+          Categorical focal crossentropy loss value.\n+        \"\"\"\n     if isinstance(axis, bool):\n         raise ValueError(\n             \"`axis` must be of type `int`. \"\n@@ -2121,6 +2219,33 @@ def _ragged_tensor_categorical_focal_crossentropy(\n     label_smoothing=0.0,\n     axis=-1,\n ):\n+    \"\"\"Implements support for handling RaggedTensors.\n+        Expected shape: (batch, sequence_len, n_classes) with sequence_len\n+        being variable per batch.\n+        Return shape: (batch, sequence_len).\n+        When used by CategoricalFocalCrossentropy() with the default reduction\n+        (SUM_OVER_BATCH_SIZE), the reduction averages the loss over the\n+        number of elements independent of the batch. E.g. if the RaggedTensor\n+        has 2 batches with [2, 1] values respectively the resulting loss is\n+        the sum of the individual loss values divided by 3.\n+        alpha: A weight balancing factor for all classes, default is `0.25` as\n+             mentioned in the reference. It can be a list of floats or a scalar.\n+             In the multi-class case, alpha may be set by inverse class frequency by\n+             using `compute_class_weight` from `sklearn.utils`.\n+        gamma: A focusing parameter, default is `2.0` as mentioned in the\n+             reference. It helps to gradually reduce the importance given to\n+             simple examples in a smooth manner. When `gamma` = 0, there is no focal\n+             effect on the categorical crossentropy.\n+        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n+            default, we assume that `y_pred` encodes a probability distribution.\n+        label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n+            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n+            and `0.9 + 0.1 / num_classes` for target labels.\n+        axis: Defaults to -1. The dimension along which the entropy is\n+            computed.\n+        Returns:\n+          Categorical focal crossentropy loss value.\n+        \"\"\"\n     fn = functools.partial(\n         categorical_focal_crossentropy,\n         alpha=alpha,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c267fa0a118dc748707e50c36d33838a9f55776d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 21623 | Contributors (this commit): 47 | Commits (past 90d): 25 | Contributors (cumulative): 69 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5642,6 +5642,7 @@ def categorical_focal_crossentropy(\n     focal_cce = tf.reduce_sum(focal_cce, axis=axis)\n     return focal_cce\n \n+\n @keras_export(\"keras.backend.sparse_categorical_crossentropy\")\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n\n@@ -921,6 +921,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             axis=axis,\n         )\n \n+\n @keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the alpha balanced focal crossentropy loss between the labels and predictions.\n@@ -993,6 +994,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n       name: Optional name for the instance.\n         Defaults to 'categorical_focal_crossentropy'.\n     \"\"\"\n+\n     def __init__(\n         self,\n         alpha=0.25,\n@@ -1026,6 +1028,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+\n @keras_export(\"keras.losses.SparseCategoricalCrossentropy\")\n class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     \"\"\"Computes the crossentropy loss between the labels and predictions.\n@@ -2207,7 +2210,9 @@ def categorical_focal_crossentropy(\n         alpha=alpha,\n         gamma=gamma,\n         from_logits=from_logits,\n-            axis=axis, )\n+        axis=axis,\n+    )\n+\n \n @dispatch.dispatch_for_types(categorical_focal_crossentropy, tf.RaggedTensor)\n def _ragged_tensor_categorical_focal_crossentropy(\n@@ -2256,6 +2261,7 @@ def _ragged_tensor_categorical_focal_crossentropy(\n     )\n     return _ragged_tensor_apply_loss(fn, y_true, y_pred)\n \n+\n @keras_export(\n     \"keras.metrics.sparse_categorical_crossentropy\",\n     \"keras.losses.sparse_categorical_crossentropy\",\n\n@@ -1809,14 +1809,17 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n                 str(w[-1].message),\n             )\n \n+\n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n     def test_config(self):\n \n-        cce_obj = losses.CategoricalFocalCrossentropy(name=\"focal_cce\",\n+        cce_obj = losses.CategoricalFocalCrossentropy(\n+            name=\"focal_cce\",\n             reduction=losses_utils.ReductionV2.SUM,\n             alpha=0.25,\n-                                                      gamma=2.0)\n+            gamma=2.0,\n+        )\n         self.assertEqual(cce_obj.name, \"focal_cce\")\n         self.assertEqual(cce_obj.reduction, losses_utils.ReductionV2.SUM)\n         self.assertEqual(cce_obj.alpha, 0.25)\n@@ -1909,7 +1912,9 @@ class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n         )\n         loss = cce_obj(y_true, logits)\n         self.assertAllClose(\n-            (1.5096224e-09, 2.4136547e-11, 1.0360638e-03), self.evaluate(loss), 3\n+            (1.5096224e-09, 2.4136547e-11, 1.0360638e-03),\n+            self.evaluate(loss),\n+            3,\n         )\n \n     def test_label_smoothing(self):\n@@ -1918,7 +1923,8 @@ class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n         label_smoothing = 0.1\n \n         cce_obj = losses.CategoricalFocalCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing)\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n         loss = cce_obj(y_true, logits)\n \n         expected_value = 0.06685\n@@ -1930,7 +1936,8 @@ class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n         label_smoothing = 0.1\n \n         cce_obj = losses.CategoricalFocalCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing)\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n         loss = cce_obj(y_true, logits)\n \n         expected_value = 0.06685\n@@ -2003,6 +2010,7 @@ class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n                 str(w[-1].message),\n             )\n \n+\n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n     def test_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c33117f62a7580b4a17f394c797edf2f80b6972", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 17388 | Contributors (this commit): 46 | Commits (past 90d): 21 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -5585,26 +5585,33 @@ def categorical_focal_crossentropy(\n     from_logits=False,\n     axis=-1,\n ):\n-    \"\"\"Categorical focal crossentropy (alpha balanced) between an output tensor and a target tensor.\n+    \"\"\"Computes the alpha balanced focal crossentropy loss between\n+    the labels and predictions.\n     According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n     helps to apply a focal factor to down-weight easy examples and focus more on\n     hard examples. By default, the focal tensor is computed as follows:\n+\n     It has pt defined as:\n     pt = p, if y = 1 else 1 - p\n+\n     The authors use alpha-balanced variant of focal loss in the paper:\n     FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n+\n     Extending this to multi-class case is straightforward:\n-    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from negative log-likelihood and included in CE.\n-    `modulating_factor` is (1 − pt)^gamma,\n-    where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n-    effect on the categorical crossentropy.\n+    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from\n+    negative log-likelihood and included in CE.\n+\n+    `modulating_factor` is (1 − pt)^gamma, where `gamma` is a focusing\n+    parameter. When `gamma` = 0, there is no focal effect on the categorical\n+    crossentropy. And if alpha = 1, at the same time the loss is equivalent\n+    to the categorical crossentropy.\n     Args:\n       target: A tensor with the same shape as `output`.\n       output: A tensor.\n       alpha: A weight balancing factor for all classes, default is `0.25` as\n              mentioned in the reference. It can be a list of floats or a scalar.\n-             In the multi-class case, alpha may be set by inverse class frequency by\n-             using `compute_class_weight` from `sklearn.utils`.\n+             In the multi-class case, alpha may be set by inverse class\n+             frequency by using `compute_class_weight` from `sklearn.utils`.\n       gamma: A focusing parameter, default is `2.0` as mentioned in the\n              reference. It helps to gradually reduce the importance given to\n              simple examples in a smooth manner.\n\n@@ -924,20 +924,27 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n @keras_export(\"keras.losses.CategoricalFocalCrossentropy\")\n class CategoricalFocalCrossentropy(LossFunctionWrapper):\n-    \"\"\"Computes the alpha balanced focal crossentropy loss between the labels and predictions.\n+    \"\"\"Computes the alpha balanced focal crossentropy loss between\n+    the labels and predictions.\n     According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n     helps to apply a focal factor to down-weight easy examples and focus more on\n     hard examples. By default, the focal tensor is computed as follows:\n+\n     It has pt defined as:\n     pt = p, if y = 1 else 1 - p\n+\n     The authors use alpha-balanced variant of focal loss in the paper:\n     FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n+\n     Extending this to multi-class case is straightforward:\n-    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from negative log-likelihood and included in CE.\n-    `modulating_factor` is (1 − pt)^gamma,\n-    where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n-    effect on the categorical crossentropy. And if alpha = 1, at the same time the loss is\n-    equivalent to the categorical crossentropy.\n+    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from\n+    negative log-likelihood and included in CE.\n+\n+    `modulating_factor` is (1 − pt)^gamma, where `gamma` is a focusing\n+    parameter. When `gamma` = 0, there is no focal effect on the categorical\n+    crossentropy. And if alpha = 1, at the same time the loss is equivalent to\n+    the categorical crossentropy.\n+\n     In the snippet below, there is `# classes` floating pointing values per\n     example. The shape of both `y_pred` and `y_true` are\n     `[batch_size, num_classes]`.\n@@ -969,8 +976,8 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     Args:\n       alpha: A weight balancing factor for all classes, default is `0.25` as\n              mentioned in the reference. It can be a list of floats or a scalar.\n-             In the multi-class case, alpha may be set by inverse class frequency by\n-             using `compute_class_weight` from `sklearn.utils`.\n+             In the multi-class case, alpha may be set by inverse class\n+             frequency by using `compute_class_weight` from `sklearn.utils`.\n       gamma: A focusing parameter, default is `2.0` as mentioned in the\n              reference. It helps to gradually reduce the importance given to\n              simple (easy) examples in a smooth manner.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ed1574c0e9d80273d854556db9d304e818e45e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 491 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n+from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n@@ -212,7 +213,7 @@ class GroupNormalization(Layer):\n         return gamma, beta\n \n     def _create_broadcast_shape(self, input_shape):\n-        broadcast_shape = [1] * input_shape.shape.rank\n+        broadcast_shape = [1] * backend.int_shape(input_shape)[0]\n \n         broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n         broadcast_shape.insert(self.axis, self.groups)\n\n@@ -34,7 +34,7 @@ def _build_group_normalization_model(norm):\n     return model\n \n \n-@test_utils.run_v2_only()\n+@test_utils.run_v2_only\n class GroupNormalizationTest(test_combinations.TestCase):\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n@@ -79,7 +79,7 @@ class GroupNormalizationTest(test_combinations.TestCase):\n             groups=1, axis=-1, input_shape=(8,), scale=False, center=False\n         )\n         layer_with_2_groups = GroupNormalization(\n-            groups=2, axis=-1, input_shape=(8,), scale=False, center=False\n+            groups=2, axis=1, input_shape=(8,), scale=False, center=False\n         )\n \n         inputs = tf.constant(\n@@ -111,7 +111,7 @@ class GroupNormalizationTest(test_combinations.TestCase):\n             groups=1, axis=-1, input_shape=(2, 4), scale=False, center=False\n         )\n         layer_with_2_groups = GroupNormalization(\n-            groups=2, axis=-1, input_shape=(2, 4), scale=False, center=False\n+            groups=2, axis=2, input_shape=(2, 4), scale=False, center=False\n         )\n \n         inputs = tf.constant(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50c4c8ece9b580352705715b8b167fcfd4aaf3db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 15/12 | Churn Δ: 159 | Churn Cumulative: 3756 | Contributors (this commit): 12 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: 0.8450704225352113\n\nDIFF:\n@@ -550,6 +550,11 @@ class BatchNormalizationBase(Layer):\n         # Determine a boolean value for `training`: could be True, False, or\n         # None.\n         training_value = control_flow_util.constant_value(training)\n+        _raise_for_non_sync_bn_with_renorm_and_dtensor_strategy(\n+            synchronized=self.synchronized,\n+            training=training,\n+            renorm=self.renorm,\n+        )\n \n         if self.virtual_batch_size is not None:\n             # Virtual batches (aka ghost batches) can be simulated by reshaping\n@@ -640,6 +645,8 @@ class BatchNormalizationBase(Layer):\n         if training_value == False:  # noqa: E712\n             mean, variance = self.moving_mean, self.moving_variance\n         else:\n+            # The following long block are handling mean/variance update during\n+            # the training stage in various of different settings.\n             if self.adjustment:\n                 adj_scale, adj_bias = self.adjustment(tf.shape(inputs))\n                 # Adjust only during training.\n@@ -689,6 +696,12 @@ class BatchNormalizationBase(Layer):\n                 # significant difference in the result.\n                 new_mean = tf.reduce_mean(mean, axis=1, keepdims=True)\n                 new_variance = tf.reduce_mean(variance, axis=1, keepdims=True)\n+            else:\n+                if _running_with_dtensor_strategy() and not self.synchronized:\n+                    new_mean = tf.math.reduce_mean(mean, axis=reduction_axes)\n+                    new_variance = tf.math.reduce_mean(\n+                        variance, axis=reduction_axes\n+                    )\n                 else:\n                     new_mean, new_variance = mean, variance\n \n@@ -761,6 +774,7 @@ class BatchNormalizationBase(Layer):\n \n             self.add_update(mean_update)\n             self.add_update(variance_update)\n+            # End of handling mean/variance calculation and update.\n \n         mean = tf.cast(mean, inputs.dtype)\n         variance = tf.cast(variance, inputs.dtype)\n@@ -1205,16 +1219,50 @@ class BatchNormalizationBase(Layer):\n     def _dtensor_no_sync_calculate_mean_and_var(\n         self, inputs, reduction_axes, keep_dims, mask=None\n     ):\n-        # For the DTensor non-sync BN, the mean/var need to be calculated based\n-        # on the local batch. Think about following example:\n-        # 2 replica with local batch size = 4, and global batch size = 8\n-        # inputs = {'replica_0': (4, x, y), 'replica_1': (4, x, y)}\n-        # From global dtensor context, it is (8, x, y).\n-        # Give the inputs, we need to first need to reshape the inputs into\n-        # (2, 4, x, y), so that when normalization happens, it will not cross\n-        # the replica boundary.\n-        # TODO(scottzhu): For next cl.\n-        raise NotImplementedError()\n+        replica_tensor = _expand_tensor_with_local_replica_group(inputs)\n+        local_batch_size = tf.shape(replica_tensor)[1]\n+\n+        # Since we added a new axis in the beginning, all the value in\n+        # reduction_axes need to be incremented by 1.\n+        updated_reduction_axes = [n + 1 for n in reduction_axes]\n+\n+        if mask is None:\n+            mean, var = tf.nn.moments(\n+                replica_tensor, updated_reduction_axes, keepdims=keep_dims\n+            )\n+        else:\n+            mask_weights = tf.cast(\n+                mask, self.compute_dtype, name=\"mask_weights\"\n+            )\n+            mask_weights = tf.expand_dims(\n+                mask_weights, axis=-1, name=\"mask_weights_broadcasted\"\n+            )\n+            mean, var = tf.nn.weighted_moments(\n+                replica_tensor,\n+                axes=updated_reduction_axes,\n+                frequency_weights=mask_weights,\n+                keepdims=keep_dims,\n+            )\n+        # Also note that the mean/var we have here will have an extra dim in\n+        # axis 0, which is represented for num local replica. Down the\n+        # stream, the mean/var will be used to update the moving_mean/var\n+        # and also normalize the inputs. To make the shape match, we will\n+        # expand the tensor shape from [num_replica, x, y] to\n+        # [batch_size, x, y] so that it can be properly used for\n+        # normalization. When it reaches the mean/var update, a separate\n+        # logic will be there to reduce_mean the value based on the batch\n+        # dim.\n+        mean = tf.repeat(mean, local_batch_size, axis=0)\n+        var = tf.repeat(var, local_batch_size, axis=0)\n+        if not keep_dims:\n+            # We need to fill the reduced dims so that the mean/var can be\n+            # properly broadcast to the input shapes. In the example above,\n+            # the original reduction_axes is [0, 1]. We ignore the first 0\n+            # (batch dim) here since we already expand and use it as num_replica\n+            for dim in reduction_axes[1:]:\n+                mean = tf.expand_dims(mean, axis=dim)\n+                var = tf.expand_dims(var, axis=dim)\n+        return mean, var\n \n     def _dtensor_sync_calculate_mean_and_var(\n         self, inputs, reduction_axes, keep_dims, mask=None\n@@ -1507,3 +1555,51 @@ def _running_with_dtensor_strategy():\n     # TODO(scottzhu): Finalize the strategy API to check if a strategy is backed\n     # by DTensor.\n     return getattr(strategy, \"_mesh\", None) is not None\n+\n+\n+def _expand_tensor_with_local_replica_group(inputs):\n+    \"\"\"Reshape the input tensor to have an extra dimension of replica group.\n+\n+    Under the DTensor usage, the normal batch norm still need to perform on\n+    a local batch size, which mean we can't directly do mean/var on a global\n+    tensor. In order to do a local mean/var, we have to add a new dimention to\n+    the tensor, so that the ops will not cross the replica boundary. E.g,\n+    a global tensor with shape [8, x, y] and has 2 local replica, the output of\n+    this will be [2, 4, x, y], where the first dim is for num of replica, and\n+    the second dim is for the local batch size. The follow ops can do reduces\n+    among the local batch dimension.\n+\n+    Note that this function should only be used under DTensor based strategy,\n+    and it will use the current strategy in the context to get the number of\n+    replica.\n+\n+    Args:\n+        inputs: Tensor with shape [global_batch_size, ...]\n+\n+    Returns:\n+        Tensor with shape [num_replica, local_batch_size, ...]\n+    \"\"\"\n+    # TODO(b/272382109): Implement this an an Op.\n+    input_shape = tf.shape(inputs)\n+    global_batch_size = input_shape[0]\n+    num_replica = tf.distribute.get_strategy().num_replicas_in_sync\n+    local_batch_size = global_batch_size // num_replica\n+    replica_shape = tf.stack([num_replica, local_batch_size])\n+    replica_shape = tf.concat([replica_shape, input_shape[1:]], axis=0)\n+    return tf.reshape(inputs, replica_shape)\n+\n+\n+def _raise_for_non_sync_bn_with_renorm_and_dtensor_strategy(\n+    synchronized, training, renorm\n+):\n+    if (\n+        _running_with_dtensor_strategy()\n+        and not synchronized\n+        and training == True\n+        and renorm\n+    ):\n+        raise NotImplementedError(\n+            \"Renorm for BatchNormalization under DTensor based distribution \"\n+            \"strategy is not supported at the moment. Please file a feature \"\n+            \"request if this is blocking your adoption.\"\n+        )\n\n@@ -30,9 +30,7 @@ from tensorflow.python.distribute.experimental import (\n )\n \n \n-class BatchNormalizationDTensorTest(\n-    test_util.DTensorBaseTest, parameterized.TestCase\n-):\n+class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n     def setUp(self):\n         super().setUp()\n \n@@ -67,21 +65,24 @@ class BatchNormalizationDTensorTest(\n                 batch_normalization._running_with_dtensor_strategy()\n             )\n \n-    @parameterized.named_parameters((\"training\", True), (\"inference\", False))\n+    @parameterized.product(\n+        training=[True, False],\n+        synchronized=[True, False],\n+        renorm=[True, False],\n+    )\n     @test_utils.run_v2_only\n-    def test_sync_bn_strategy(self, training):\n+    def test_batch_normalization_with_dtensor_strategy(\n+        self, training, synchronized, renorm\n+    ):\n         num_replica = 2\n         local_batch_size = 4\n         global_batch_size = num_replica * local_batch_size\n-        num_feature = 2\n-        global_inputs = tf.range(\n-            0, global_batch_size * num_feature, dtype=tf.float32\n-        )\n-        global_inputs = tf.reshape(\n-            global_inputs, (global_batch_size, num_feature)\n+        feature_shape = [3, 5]\n+        global_inputs = tf.random.uniform(\n+            shape=[global_batch_size, *feature_shape], dtype=tf.float32\n         )\n         replica_inputs = tf.reshape(\n-            global_inputs, (num_replica, local_batch_size, num_feature)\n+            global_inputs, [num_replica, local_batch_size, *feature_shape]\n         )\n \n         def value_fn(value_context):\n@@ -91,23 +92,37 @@ class BatchNormalizationDTensorTest(\n         dtensor_strategy = dtensor_mirrored_strategy.MirroredStrategy(\n             mesh=self.mesh\n         )\n-        bn_layer_0 = batch_normalization.BatchNormalization(synchronized=True)\n-        bn_layer_1 = batch_normalization.BatchNormalization(synchronized=True)\n+        init_kwargs = {\"synchronized\": synchronized, \"renorm\": renorm}\n+        bn_layer_0 = batch_normalization.BatchNormalization(**init_kwargs)\n+        bn_layer_1 = batch_normalization.BatchNormalization(**init_kwargs)\n         run_kwargs = {\"training\": training}\n \n         normal_strategy_result = self._run_bn_training_with_strategy(\n             normal_strategy, value_fn, bn_layer_0, run_kwargs\n         )\n+        if training and not synchronized and renorm:\n+            # This is an unsupported case at the moment.\n+            with self.assertRaisesRegexp(NotImplementedError, \"not supported\"):\n+                self._run_bn_training_with_strategy(\n+                    dtensor_strategy, value_fn, bn_layer_1, run_kwargs\n+                )\n+            return\n+        else:\n             dtensor_strategy_result = self._run_bn_training_with_strategy(\n                 dtensor_strategy, value_fn, bn_layer_1, run_kwargs\n             )\n         self.assertAllClose(\n             normal_strategy_result.values, dtensor_strategy_result.values\n         )\n+        self.assertAllClose(bn_layer_0.moving_mean, bn_layer_1.moving_mean)\n+        self.assertAllClose(\n+            bn_layer_0.moving_variance, bn_layer_1.moving_variance\n+        )\n \n     def _run_bn_training_with_strategy(\n         self, strategy, value_fn, bn_layer, run_kwargs\n     ):\n+        @tf.function\n         def run_fn(inputs):\n             return bn_layer(inputs, **run_kwargs)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
