{"custom_id": "keras#6079ae8cfd185b1e82eab85d757a5748c0a3b070", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 54 | Files Changed: 4 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 62 | Churn Cumulative: 26217 | Contributors (this commit): 142 | Commits (past 90d): 83 | Contributors (cumulative): 169 | DMM Complexity: 0.2413793103448276\n\nDIFF:\n@@ -485,12 +485,6 @@ class Sequential(functional.Functional):\n                     compile_config, base_class=Sequential\n                 )\n \n-            if build_input_shape:\n-                model.build(build_input_shape)\n-                if model.optimizer is not None:\n-                    model.optimizer.build(model.trainable_variables)\n-\n-        else:\n         if (\n             not model.inputs\n             and build_input_shape\n\n@@ -3143,17 +3143,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 if compile_config is not None:\n                     model._compile_from_config(compile_config, base_class=Model)\n \n-                    # Checking the existence of optimizer attribute because\n-                    # `compile()` may not have been called (if overridden).\n-                    if model.optimizer is not None:\n-                        # To bring the optimizer's state back to when it was\n-                        # saved, we build it so that the variables are created\n-                        # (and available for further state loading). Otherwise,\n-                        # the optimizer's variables are not there until the next\n-                        # time `Model.fit()` or `optimizer.apply_gradient()` is\n-                        # called.\n-                        model.optimizer.build(model.trainable_variables)\n-\n             return model\n \n     def to_json(self, **kwargs):\n\n@@ -724,14 +724,14 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     def _get_state(self):\n         \"\"\"Get the state of this optimizer object.\"\"\"\n         result = {}\n-        for k, variable in enumerate(self.variables()):\n-            result[str(k)] = variable.numpy()\n+        for variable in self.variables():\n+            result[variable.name] = variable.numpy()\n         return result\n \n     def _set_state(self, state):\n         \"\"\"Set the state of this optimizer object.\"\"\"\n-        for k, variable in enumerate(self.variables()):\n-            variable.assign(state[str(k)])\n+        for variable in self.variables():\n+            variable.assign(state[variable.name])\n \n     def _save_state(self, dir_path):\n         file_path = tf.io.gfile.join(dir_path, \"state.npz\")\n\n@@ -41,13 +41,11 @@ class MyDense(keras.layers.Dense):\n         self.additional_weights = [\n             self.add_weight(\n                 \"my_additional_weight\",\n-                shape=(1, 1),\n                 initializer=\"ones\",\n                 trainable=True,\n             ),\n             self.add_weight(\n                 \"my_additional_weight_2\",\n-                shape=(1, 1),\n                 initializer=\"ones\",\n                 trainable=True,\n             ),\n@@ -55,7 +53,6 @@ class MyDense(keras.layers.Dense):\n         self.weights_in_dict = {\n             \"my_weight\": self.add_weight(\n                 \"my_dict_weight\",\n-                shape=(1, 1),\n                 initializer=\"ones\",\n                 trainable=True,\n             ),\n@@ -64,11 +61,8 @@ class MyDense(keras.layers.Dense):\n         return super().build(input_shape)\n \n     def call(self, inputs):\n-        outputs = super().call(inputs)\n-        outputs = self.nested_layer(outputs)\n-        outputs = tf.matmul(outputs, self.additional_weights[0])\n-        outputs = tf.matmul(outputs, self.additional_weights[1])\n-        return tf.matmul(outputs, self.weights_in_dict[\"my_weight\"])\n+        call_result = super().call(inputs)\n+        return self.nested_layer(call_result)\n \n     def two(self):\n         return 2\n@@ -424,8 +418,8 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n \n         # Mutate the `Dense` layer custom weights to ensure that list and\n         # dict-contained weights get restored.\n-        model.layers[1].additional_weights[0].assign([[2]])\n-        model.layers[1].weights_in_dict[\"my_weight\"].assign([[2]])\n+        model.layers[1].additional_weights[0].assign(2)\n+        model.layers[1].weights_in_dict[\"my_weight\"].assign(2)\n         model.layers[1].nested_layer.kernel.assign([[1]])\n \n         model._save_experimental(temp_filepath)\n@@ -489,29 +483,6 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n         self.assertIn(\"keras_version\", metadata)\n         self.assertIn(\"date_saved\", metadata)\n \n-    @tf.__internal__.distribute.combinations.generate(\n-        tf.__internal__.test.combinations.combine(\n-            model_type=[\"subclassed\", \"functional\", \"sequential\"],\n-        )\n-    )\n-    def test_saving_optimizer_variables(self, model_type):\n-        temp_filepath = os.path.join(self.get_temp_dir(), \"my_model.keras\")\n-        model = getattr(self, f\"_get_{model_type}_model\")()\n-        x = np.random.random((100, 32))\n-        y = np.random.random((100, 1))\n-        model.fit(x, y, epochs=1)\n-        model._save_experimental(temp_filepath)\n-        loaded_model = saving_lib.load_model(temp_filepath)\n-\n-        self.assertEqual(\n-            len(model.optimizer.variables()),\n-            len(loaded_model.optimizer.variables()),\n-        )\n-        for original_weights, loaded_weights in zip(\n-            model.optimizer.variables(), loaded_model.optimizer.variables()\n-        ):\n-            np.testing.assert_allclose(original_weights, loaded_weights)\n-\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#51ef3d8edf9a86496c2cad66257904e0f9acf233", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1050 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import optimizer_v2\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Optimizer\")\n class Optimizer(optimizer_v2.OptimizerV2):\n     pass\n\n@@ -100,7 +100,11 @@ def name_scope_only_in_function_or_graph(name):\n         return NullContextmanager()\n \n \n-@keras_export(\"keras.optimizers.Optimizer\", metaclass=abc.ABCMeta)\n+@keras_export(\n+    \"keras.optimizers.Optimizer\",\n+    \"keras.optimizers.legacy.Optimizer\",\n+    metaclass=abc.ABCMeta,\n+)\n class OptimizerV2(tf.__internal__.tracking.Trackable):\n     \"\"\"Base class for Keras optimizers.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33fcc6363d7c2e10b6a64ae6624674cdb4dad16f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1060 | Contributors (this commit): 11 | Commits (past 90d): 11 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -16,6 +16,10 @@\n \n from keras.optimizers.optimizer_v2 import optimizer_v2\n \n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n \n+\n+@keras_export(\"keras.optimizers.legacy.Optimizer\")\n class Optimizer(optimizer_v2.OptimizerV2):\n     pass\n\n@@ -100,11 +100,7 @@ def name_scope_only_in_function_or_graph(name):\n         return NullContextmanager()\n \n \n-@keras_export(\n-    \"keras.optimizers.Optimizer\",\n-    \"keras.optimizers.legacy.Optimizer\",\n-    metaclass=abc.ABCMeta,\n-)\n+@keras_export(\"keras.optimizers.Optimizer\", metaclass=abc.ABCMeta)\n class OptimizerV2(tf.__internal__.tracking.Trackable):\n     \"\"\"Base class for Keras optimizers.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#42468b832907552c7b9c5bcabc4454987274c7aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1420 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -354,8 +354,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         # explicitly cast here to also allow integer inputs to be passed\n         inputs = tf.cast(inputs, self.compute_dtype)\n         if self.invert:\n-            return self.mean + (inputs * tf.maximum(\n-                tf.sqrt(self.variance), backend.epsilon())\n+            return self.mean + (\n+                inputs * tf.maximum(tf.sqrt(self.variance), backend.epsilon())\n             )\n         else:\n             return (inputs - self.mean) / tf.maximum(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23f889bd3b27990298d923c25104d000df232a17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 2558 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 19 | DMM Complexity: 0.625\n\nDIFF:\n@@ -821,6 +821,9 @@ class BatchNormalizationBase(Layer):\n     def call(self, inputs, training=None):\n         inputs = tf.cast(inputs, self.compute_dtype)\n         training = self._get_training_value(training)\n+        # Determine a boolean value for `training`: could be True, False, or\n+        # None.\n+        training_value = control_flow_util.constant_value(training)\n \n         if self.virtual_batch_size is not None:\n             # Virtual batches (aka ghost batches) can be simulated by reshaping\n@@ -829,9 +832,13 @@ class BatchNormalizationBase(Layer):\n             original_shape = tf.concat(\n                 [tf.constant([-1]), original_shape[1:]], axis=0\n             )\n+\n+            expanded_shape = (\n+                [self.virtual_batch_size, -1] if training_value else [-1, 1]\n+            )\n             expanded_shape = tf.concat(\n                 [\n-                    tf.constant([self.virtual_batch_size, -1]),\n+                    tf.constant(expanded_shape),\n                     original_shape[1:],\n                 ],\n                 axis=0,\n@@ -892,9 +899,6 @@ class BatchNormalizationBase(Layer):\n                 offset += then_offset\n             return (scale, offset)\n \n-        # Determine a boolean value for `training`: could be True, False, or\n-        # None.\n-        training_value = control_flow_util.constant_value(training)\n         if training_value == False:  # noqa: E712\n             mean, variance = self.moving_mean, self.moving_variance\n         else:\n\n@@ -408,12 +408,20 @@ class BatchNormalizationV2Test(test_combinations.TestCase):\n         wrapped_fn()\n \n     @test_combinations.run_all_keras_modes\n-    def test_basic_batchnorm_v2_none_shape_and_virtual_batch_size(self):\n+    def test_basic_batchnorm_v2_input_shape_and_virtual_batch_size(self):\n         # Test case for GitHub issue for 32380\n         norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n         inp = keras.layers.Input(shape=(None, None, 3))\n         _ = norm(inp)\n \n+        # Test case for https://github.com/tensorflow/tensorflow/issues/23050\n+        norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n+        _ = norm(np.ones((1, 28, 28)))\n+\n+        with self.assertRaisesRegex(Exception, \"requested shape requires\"):\n+            norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n+            _ = norm(np.ones((1, 28, 28)), training=True)\n+\n     @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n     def test_fused_batchnorm_empty_batch(self):\n         # Test case for https://github.com/tensorflow/tensorflow/issues/52986\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#919a6cd4a60337e65577e4ffbd42b3a0d5289f98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 196 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -57,6 +57,8 @@ setuptools.setup(\n     author_email=\"keras-users@googlegroups.com\",\n     packages=setuptools.find_packages(),\n     install_requires=REQUIRED_PACKAGES,\n+    # Supported Python versions\n+    python_requires=\">=3.7\",\n     # PyPI package information.\n     classifiers=[\n         \"Development Status :: 5 - Production/Stable\",\n@@ -68,6 +70,7 @@ setuptools.setup(\n         \"Programming Language :: Python :: 3.7\",\n         \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n+        \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3 :: Only\",\n         \"Topic :: Scientific/Engineering\",\n         \"Topic :: Scientific/Engineering :: Mathematics\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4a53041aea5a37f317df4a0cb0b6996a956cc077", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3133 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -447,7 +447,7 @@ def ConvNeXt(\n             img_input = input_tensor\n \n     if input_tensor is not None:\n-        inputs = utils.layer_utils.get_source_inputs(input_tensor)\n+        inputs = utils.layer_utils.get_source_inputs(input_tensor)[0]\n     else:\n         inputs = img_input\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5280dc6f252856915989e147cd709d4bb8040793", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2988 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -951,7 +951,7 @@ def RegNet(\n             img_input = input_tensor\n \n     if input_tensor is not None:\n-        inputs = layer_utils.get_source_inputs(input_tensor)\n+        inputs = layer_utils.get_source_inputs(input_tensor)[0]\n     else:\n         inputs = img_input\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61706ed1ebb6255236ae3d9c0afa5a062238dd48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 33 | Churn Cumulative: 5553 | Contributors (this commit): 17 | Commits (past 90d): 30 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -460,6 +460,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(type(optimizer), type(loaded_optimizer))\n         self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n         self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n+        self.assertAllClose(optimizer.variables(), loaded_optimizer.variables())\n \n         # Save in Keras SavedModel format.\n         model.fit(x, y)\n@@ -471,6 +472,8 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(type(optimizer), type(loaded_optimizer))\n         self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n         self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n+        loaded_optimizer.build(loaded_model.trainable_variables)\n+        self.assertAllClose(optimizer.variables(), loaded_optimizer.variables())\n \n     @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n     def testSparseGradientsWorkAsExpected(self, optimizer_fn):\n\n@@ -127,13 +127,7 @@ def save_model_to_hdf5(model, filepath, overwrite=True, include_optimizer=True):\n \n         # TODO(b/128683857): Add integration tests between tf.keras and external\n         # Keras, to avoid breaking TF.js users.\n-        if isinstance(model.optimizer, optimizer_experimental.Optimizer):\n-            logging.warning(\n-                \"HDF5 format does not save weights of\"\n-                \" `optimizer_experimental.Optimizer`, your optimizer will\"\n-                \" be recompiled at loading time.\"\n-            )\n-        elif (\n+        if (\n             include_optimizer\n             and model.optimizer\n             and not isinstance(model.optimizer, optimizer_v1.TFOptimizer)\n@@ -227,14 +221,13 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n             saving_utils.try_build_compiled_arguments(model)\n \n             # Set optimizer weights.\n-            if isinstance(model.optimizer, optimizer_experimental.Optimizer):\n-                logging.warning(\n-                    \"Loading model from HDF5 will not restore the \"\n-                    \"optimizer's weights, since the optimizer is an \"\n-                    \"instance of `optimizer_experimental.Optimizer`\"\n-                )\n-            elif \"optimizer_weights\" in f:\n+            if \"optimizer_weights\" in f:\n                 try:\n+                    if isinstance(\n+                        model.optimizer, optimizer_experimental.Optimizer\n+                    ):\n+                        model.optimizer.build(model.trainable_variables)\n+                    else:\n                         model.optimizer._create_all_weights(\n                             model.trainable_variables\n                         )\n@@ -675,7 +668,9 @@ def save_optimizer_weights_to_hdf5_group(hdf5_group, optimizer):\n         hdf5_group: HDF5 group.\n         optimizer: optimizer instance.\n     \"\"\"\n-\n+    if isinstance(optimizer, optimizer_experimental.Optimizer):\n+        symbolic_weights = optimizer.variables()\n+    else:\n         symbolic_weights = getattr(optimizer, \"weights\")\n     if symbolic_weights:\n         weights_group = hdf5_group.create_group(\"optimizer_weights\")\n\n@@ -486,11 +486,6 @@ class TestWholeModelSaving(test_combinations.TestCase):\n                 keras.optimizers.optimizer_experimental.Optimizer,\n             ):\n                 loaded_model.optimizer.build(loaded_model.trainable_variables)\n-                save_format = test_utils.get_save_format()\n-                if save_format == \"h5\":\n-                    # Experimental optimizer does not restore weights if saved\n-                    # in h5 format.\n-                    return\n                 self.assertAllClose(\n                     model.optimizer.variables(),\n                     loaded_model.optimizer.variables(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e4c420847998af168cec18aa71ceac3c170157a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 8 | Churn Cumulative: 238 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,3 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras' Distribution Strategy library.\"\"\"\n-\n-\n-from keras.distribute import sidecar_evaluator\n\n@@ -49,6 +49,9 @@ from keras.utils.layer_utils import get_source_inputs\n # Deprecated\n from keras.utils.np_utils import normalize\n from keras.utils.np_utils import to_categorical\n+\n+# Evaluation related\n+from keras.utils.sidecar_evaluator import SidecarEvaluator\n from keras.utils.text_dataset import text_dataset_from_directory\n from keras.utils.tf_utils import set_random_seed\n from keras.utils.timeseries_dataset import timeseries_dataset_from_array\n\n\n@@ -24,9 +24,9 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.distribute import sidecar_evaluator as sidecar_evaluator_lib\n from keras.optimizers.optimizer_experimental import sgd\n from keras.testing_infra import test_utils\n+from keras.utils import sidecar_evaluator as sidecar_evaluator_lib\n \n # isort: off\n from tensorflow.python.platform import tf_logging as logging\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91fcaf6f5fefa396b298e8131d207082061dde9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 2 | Churn Cumulative: 122 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.5\n\nDIFF:\n@@ -138,7 +138,6 @@ class SGD(optimizer.Optimizer):\n         if hasattr(self, \"_built\") and self._built:\n             return\n         self.momentums = []\n-        if self.momentum != 0:\n         for var in var_list:\n             self.momentums.append(\n                 self.add_variable_from_reference(\n@@ -152,7 +151,6 @@ class SGD(optimizer.Optimizer):\n         lr = tf.cast(self.learning_rate, variable.dtype)\n         m = None\n         var_key = self._var_key(variable)\n-        if self.momentum != 0:\n         momentum = tf.cast(self.momentum, variable.dtype)\n         m = self.momentums[self._index_dict[var_key]]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#51a6050b936ec87cd684fc1a052f79785ec9aaec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 39 | Churn Cumulative: 1722 | Contributors (this commit): 9 | Commits (past 90d): 52 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -209,9 +209,11 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         if self._var_key(variable) not in self._index_dict:\n             raise KeyError(\n                 f\"The optimizer cannot recognize variable {variable.name}. \"\n-                \"This usually means that you're reusing an optimizer \"\n-                \"previously created for a different model. Try creating a \"\n-                \"new optimizer instance.\"\n+                \"This usually means you are trying to call the optimizer to \"\n+                \"update different parts of the model separately. Please call \"\n+                \"`optimizer.build(variables)` with the full list of trainable \"\n+                \"variables before the training loop or use legacy optimizer \"\n+                \"`tf.keras.optimizers.legacy.{self.__class__.__name__}.\"\n             )\n         self.update_step(gradient, variable)\n \n@@ -673,7 +675,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         return config\n \n     @classmethod\n-    def from_config(cls, config):\n+    def from_config(cls, config, custom_objects=None):\n         \"\"\"Creates an optimizer from its config.\n \n         This method is the reverse of `get_config`, capable of instantiating the\n@@ -681,6 +683,8 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n \n         Args:\n             config: A Python dictionary, typically the output of get_config.\n+            custom_objects: A Python dictionary mapping names to additional\n+              user-defined Python objects needed to recreate this optimizer.\n \n         Returns:\n             An optimizer instance.\n@@ -688,7 +692,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         if \"learning_rate\" in config:\n             if isinstance(config[\"learning_rate\"], dict):\n                 config[\"learning_rate\"] = learning_rate_schedule.deserialize(\n-                    config[\"learning_rate\"]\n+                    config[\"learning_rate\"], custom_objects=custom_objects\n                 )\n         return cls(**config)\n \n\n@@ -327,8 +327,20 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllEqual([var1.numpy(), var2.numpy()], [-0.125, -0.125])\n \n     def testGetAndFromConfig(self):\n+        class CustomLRSchedule(learning_rate_schedule.LearningRateSchedule):\n+            def __init__(self, initial_learning_rate):\n+                self.initial_learning_rate = initial_learning_rate\n+\n+            def __call__(self, step):\n+                step = tf.cast(step, tf.float32)\n+                return self.initial_learning_rate / (step + 1)\n+\n+            def get_config(self):\n+                return {\"initial_learning_rate\": self.initial_learning_rate}\n+\n+        learning_rate = CustomLRSchedule(0.05)\n         optimizer = adam_new.Adam(\n-            learning_rate=np.float64(0.05),\n+            learning_rate=learning_rate,\n             beta_1=0.7,\n             beta_2=0.77,\n             amsgrad=True,\n@@ -342,7 +354,6 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         config = optimizer.get_config()\n         expected_config = {\n             \"name\": \"custom_adam\",\n-            \"learning_rate\": np.float32(0.05),\n             \"beta_1\": 0.7,\n             \"beta_2\": 0.77,\n             \"epsilon\": 0.001,\n@@ -355,8 +366,16 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n             \"ema_overwrite_frequency\": 50,\n             \"is_legacy_optimizer\": False,\n         }\n+        expected_learning_rate = {\n+            \"class_name\": \"CustomLRSchedule\",\n+            \"config\": {\"initial_learning_rate\": 0.05},\n+        }\n         self.assertDictContainsSubset(expected_config, config)\n-        restored_optimizer = adam_new.Adam.from_config(config)\n+        self.assertDictEqual(expected_learning_rate, config[\"learning_rate\"])\n+\n+        restored_optimizer = adam_new.Adam.from_config(\n+            config, custom_objects={\"CustomLRSchedule\": CustomLRSchedule}\n+        )\n         self.assertDictEqual(\n             restored_optimizer.get_config(), optimizer.get_config()\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3265fa48b6228d03addeeba4dc31f6c4bdcd138", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 28 | Churn Cumulative: 519 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -155,6 +155,20 @@ class GlobalAveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllEqual((2, 1, 4), output.shape)\n         self.assertAllClose(output[0, 0], model_input[0, 0, :])\n     \n+    def test_global_average_pooling_1d_invalid_input_dimension(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"\"\"Incorrect input shape\"\"\"\n+        ):\n+            layer = keras.layers.GlobalAveragePooling1D()\n+            layer.build((None, 0, 2))\n+    \n+    def test_global_average_pooling_3d_invalid_input_dimension(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"\"\"Incorrect input shape\"\"\"\n+        ):\n+            layer = keras.layers.GlobalAveragePooling3D(keepdims=True)\n+            layer.build((None, 0, 16, 16, 3))\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -122,6 +122,20 @@ class GlobalMaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n             expected_output_shape=(None, 1, 1, 1, 3),\n         )\n     \n+    def test_global_max_pooling_1d_invalid_input_dimension(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"\"\"Incorrect input shape\"\"\"\n+        ):\n+            layer = keras.layers.GlobalMaxPooling1D()\n+            layer.build((None, 0, 2))\n+    \n+    def test_global_max_pooling_3d_invalid_input_dimension(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"\"\"Incorrect input shape\"\"\"\n+        ):\n+            layer = keras.layers.GlobalMaxPooling3D(keepdims=True)\n+            layer.build((None, 0, 16, 16, 3))\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#79fef294b573ce304663316d4b7715ed0bd1b303", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 535 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -156,16 +156,12 @@ class GlobalAveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose(output[0, 0], model_input[0, 0, :])\n \n     def test_global_average_pooling_1d_invalid_input_dimension(self):\n-        with self.assertRaisesRegex(\n-            ValueError, r\"\"\"Incorrect input shape\"\"\"\n-        ):\n+        with self.assertRaisesRegex(ValueError, r\"\"\"Incorrect input shape\"\"\"):\n             layer = keras.layers.GlobalAveragePooling1D()\n             layer.build((None, 0, 2))\n \n     def test_global_average_pooling_3d_invalid_input_dimension(self):\n-        with self.assertRaisesRegex(\n-            ValueError, r\"\"\"Incorrect input shape\"\"\"\n-        ):\n+        with self.assertRaisesRegex(ValueError, r\"\"\"Incorrect input shape\"\"\"):\n             layer = keras.layers.GlobalAveragePooling3D(keepdims=True)\n             layer.build((None, 0, 16, 16, 3))\n \n\n@@ -123,16 +123,12 @@ class GlobalMaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n         )\n \n     def test_global_max_pooling_1d_invalid_input_dimension(self):\n-        with self.assertRaisesRegex(\n-            ValueError, r\"\"\"Incorrect input shape\"\"\"\n-        ):\n+        with self.assertRaisesRegex(ValueError, r\"\"\"Incorrect input shape\"\"\"):\n             layer = keras.layers.GlobalMaxPooling1D()\n             layer.build((None, 0, 2))\n \n     def test_global_max_pooling_3d_invalid_input_dimension(self):\n-        with self.assertRaisesRegex(\n-            ValueError, r\"\"\"Incorrect input shape\"\"\"\n-        ):\n+        with self.assertRaisesRegex(ValueError, r\"\"\"Incorrect input shape\"\"\"):\n             layer = keras.layers.GlobalMaxPooling3D(keepdims=True)\n             layer.build((None, 0, 16, 16, 3))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a90218e1f20eedecb30dedcd1b2b241d8d96bcdd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 1388 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -228,25 +228,26 @@ class NormalizationTest(\n         self.assertAllEqual(output.dtype, tf.float64)\n \n     def test_invert(self):\n-        data = np.array([0.0, 2.0, 0.0, 2.0])\n-        norm = normalization.Normalization(mean=1.0, variance=1.0)\n+        input_data = np.array([0.0, 4.0, 0.0, 4.0])\n+        norm = normalization.Normalization(mean=2.0, variance=4.0)\n         inv_norm = normalization.Normalization(\n-            mean=1.0, variance=1.0, invert=True\n+            mean=2.0, variance=4.0, invert=True\n         )\n-        output = norm(data)\n+        output = norm(input_data)\n         output2 = inv_norm(output)\n         self.assertListEqual(output2.shape.as_list(), [4])\n-        self.assertAllClose(output2, [0.0, 2.0, 0.0, 2.0])\n+        self.assertAllClose(input_data, output2)\n \n     @test_utils.run_v2_only\n     def test_invert_adapt(self):\n-        input_data = [[0.0], [2.0], [0.0], [2.0]]\n+        input_data = [[0.0], [4.0], [0.0], [4.0]]\n         norm = keras.layers.Normalization(axis=-1)\n         norm.adapt(input_data)\n         inv_norm = keras.layers.Normalization(axis=-1, invert=True)\n         inv_norm.adapt(input_data)\n         output = norm(input_data)\n         output2 = inv_norm(output)\n+        self.assertListEqual(output2.shape.as_list(), [4])\n         self.assertAllClose(input_data, output2)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#70e550307a6de96e276086164e365e8120093154", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1355 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -460,7 +460,7 @@ def experimental_tpu_test_loop(\n     callbacks._call_end_hook(mode)\n \n     scope.__exit__(None, None, None)\n-    if len(outs) >= 0:\n+    if len(outs) > 0:\n         outs[0] /= target_steps\n \n     if len(outs) == 1:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3784b451a07b06c2c84a60fc63bb8553fba60995", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 485 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 5 | Methods Changed: 20 | Complexity Δ (Sum/Max): 30/19 | Churn Δ: 485 | Churn Cumulative: 2600 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 36 | DMM Complexity: 0.8553054662379421\n\nDIFF:\n@@ -102,6 +102,7 @@ from keras.layers.normalization.batch_normalization import (\n )\n \n # Normalization layers.\n+from keras.layers.normalization.group_normalization import GroupNormalization\n from keras.layers.normalization.layer_normalization import LayerNormalization\n from keras.layers.normalization.unit_normalization import UnitNormalization\n \n\n@@ -0,0 +1,240 @@\n+# Copyright 2022 The Keras Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Group normalization layer\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.layers import InputSpec\n+from keras.layers import Layer\n+from keras.utils import tf_utils\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.layers.GroupNormalization\", v1=[])\n+class GroupNormalization(Layer):\n+    \"\"\"Group normalization layer.\n+\n+    Group Normalization divides the channels into groups and computes\n+    within each group the mean and variance for normalization.\n+    Empirically, its accuracy is more stable than batch norm in a wide\n+    range of small batch sizes, if learning rate is adjusted linearly\n+    with batch sizes.\n+\n+    Relation to Layer Normalization:\n+    If the number of groups is set to 1, then this operation becomes nearly\n+    identical to Layer Normalization (see Layer Normalization docs for details).\n+\n+    Relation to Instance Normalization:\n+    If the number of groups is set to the input dimension (number of groups is\n+    equal to number of channels), then this operation becomes identical to\n+    Instance Normalization.\n+\n+    Args:\n+      groups: Integer, the number of groups for Group Normalization. Can be in\n+        the range [1, N] where N is the input dimension. The input dimension\n+        must be divisible by the number of groups. Defaults to 32.\n+      axis: Integer or List/Tuple. The axis or axes to normalize across.\n+        Typically this is the features axis/axes. The left-out axes are\n+        typically the batch axis/axes. This argument defaults to `-1`, the last\n+        dimension in the input.\n+      epsilon: Small float added to variance to avoid dividing by zero. Defaults\n+        to 1e-3\n+      center: If True, add offset of `beta` to normalized tensor. If False,\n+        `beta` is ignored. Defaults to True.\n+      scale: If True, multiply by `gamma`. If False, `gamma` is not used.\n+        Defaults to True. When the next layer is linear (also e.g. `nn.relu`),\n+        this can be disabled since the scaling will be done by the next layer.\n+      beta_initializer: Initializer for the beta weight. Defaults to zeros.\n+      gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n+      beta_regularizer: Optional regularizer for the beta weight. None by\n+        default.\n+      gamma_regularizer: Optional regularizer for the gamma weight. None by\n+        default.\n+      beta_constraint: Optional constraint for the beta weight. None by default.\n+      gamma_constraint: Optional constraint for the gamma weight. None by\n+        default.  Input shape: Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis) when using this\n+        layer as the first layer in a model.  Output shape: Same shape as input.\n+    Reference: - [Yuxin Wu & Kaiming He, 2018](https://arxiv.org/abs/1803.08494)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        groups=32,\n+        axis=-1,\n+        epsilon=1e-3,\n+        center=True,\n+        scale=True,\n+        beta_initializer=\"zeros\",\n+        gamma_initializer=\"ones\",\n+        beta_regularizer=None,\n+        gamma_regularizer=None,\n+        beta_constraint=None,\n+        gamma_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.supports_masking = True\n+        self.groups = groups\n+        self.axis = axis\n+        self.epsilon = epsilon\n+        self.center = center\n+        self.scale = scale\n+        self.beta_initializer = initializers.get(beta_initializer)\n+        self.gamma_initializer = initializers.get(gamma_initializer)\n+        self.beta_regularizer = regularizers.get(beta_regularizer)\n+        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n+        self.beta_constraint = constraints.get(beta_constraint)\n+        self.gamma_constraint = constraints.get(gamma_constraint)\n+\n+    def build(self, input_shape):\n+        tf_utils.validate_axis(self.axis, input_shape)\n+\n+        dim = input_shape[self.axis]\n+        if dim is None:\n+            raise ValueError(\n+                f\"Axis {self.axis} of input tensor should have a defined \"\n+                \"dimension but the layer received an input with shape \"\n+                f\"{input_shape}.\"\n+            )\n+\n+        if self.groups == -1:\n+            self.groups = dim\n+\n+        if dim < self.groups:\n+            raise ValueError(\n+                f\"Number of groups ({self.groups}) cannot be more than the \"\n+                f\"number of channels ({dim}).\"\n+            )\n+\n+        if dim % self.groups != 0:\n+            raise ValueError(\n+                f\"Number of groups ({self.groups}) must be a multiple \"\n+                f\"of the number of channels ({dim}).\"\n+            )\n+\n+        self.input_spec = InputSpec(\n+            ndim=len(input_shape), axes={self.axis: dim}\n+        )\n+\n+        if self.scale:\n+            self.gamma = self.add_weight(\n+                shape=(dim,),\n+                name=\"gamma\",\n+                initializer=self.gamma_initializer,\n+                regularizer=self.gamma_regularizer,\n+                constraint=self.gamma_constraint,\n+            )\n+        else:\n+            self.gamma = None\n+\n+        if self.center:\n+            self.beta = self.add_weight(\n+                shape=(dim,),\n+                name=\"beta\",\n+                initializer=self.beta_initializer,\n+                regularizer=self.beta_regularizer,\n+                constraint=self.beta_constraint,\n+            )\n+        else:\n+            self.beta = None\n+\n+        super().build(input_shape)\n+\n+    def call(self, inputs):\n+        input_shape = tf.shape(inputs)\n+\n+        reshaped_inputs = self._reshape_into_groups(inputs)\n+\n+        normalized_inputs = self._apply_normalization(\n+            reshaped_inputs, input_shape\n+        )\n+\n+        return tf.reshape(normalized_inputs, input_shape)\n+\n+    def _reshape_into_groups(self, inputs):\n+        input_shape = tf.shape(inputs)\n+        group_shape = [input_shape[i] for i in range(inputs.shape.rank)]\n+\n+        group_shape[self.axis] = input_shape[self.axis] // self.groups\n+        group_shape.insert(self.axis, self.groups)\n+        group_shape = tf.stack(group_shape)\n+        reshaped_inputs = tf.reshape(inputs, group_shape)\n+        return reshaped_inputs\n+\n+    def _apply_normalization(self, reshaped_inputs, input_shape):\n+        group_reduction_axes = list(range(1, reshaped_inputs.shape.rank))\n+\n+        axis = -2 if self.axis == -1 else self.axis - 1\n+        group_reduction_axes.pop(axis)\n+\n+        mean, variance = tf.nn.moments(\n+            reshaped_inputs, group_reduction_axes, keepdims=True\n+        )\n+\n+        gamma, beta = self._get_reshaped_weights(input_shape)\n+        normalized_inputs = tf.nn.batch_normalization(\n+            reshaped_inputs,\n+            mean=mean,\n+            variance=variance,\n+            scale=gamma,\n+            offset=beta,\n+            variance_epsilon=self.epsilon,\n+        )\n+        return normalized_inputs\n+\n+    def _get_reshaped_weights(self, input_shape):\n+        broadcast_shape = self._create_broadcast_shape(input_shape)\n+        gamma = None\n+        beta = None\n+        if self.scale:\n+            gamma = tf.reshape(self.gamma, broadcast_shape)\n+\n+        if self.center:\n+            beta = tf.reshape(self.beta, broadcast_shape)\n+        return gamma, beta\n+\n+    def _create_broadcast_shape(self, input_shape):\n+        broadcast_shape = [1] * input_shape.shape.rank\n+\n+        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n+        broadcast_shape.insert(self.axis, self.groups)\n+\n+        return broadcast_shape\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"groups\": self.groups,\n+            \"axis\": self.axis,\n+            \"epsilon\": self.epsilon,\n+            \"center\": self.center,\n+            \"scale\": self.scale,\n+            \"beta_initializer\": initializers.serialize(self.beta_initializer),\n+            \"gamma_initializer\": initializers.serialize(self.gamma_initializer),\n+            \"beta_regularizer\": regularizers.serialize(self.beta_regularizer),\n+            \"gamma_regularizer\": regularizers.serialize(self.gamma_regularizer),\n+            \"beta_constraint\": constraints.serialize(self.beta_constraint),\n+            \"gamma_constraint\": constraints.serialize(self.gamma_constraint),\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,242 @@\n+# Copyright 2022 The Keras Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# =============================================================================\n+\n+import tensorflow.compat.v2 as tf\n+\n+import keras\n+from keras.initializers import Constant\n+from keras.layers import GroupNormalization\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+\n+\n+def _build_group_normalization_model(norm):\n+    model = keras.models.Sequential()\n+    model.add(norm)\n+    model.compile(\n+        loss=\"mse\",\n+        optimizer=\"rmsprop\",\n+        run_eagerly=test_utils.should_run_eagerly(),\n+    )\n+\n+    return model\n+\n+\n+@test_utils.run_v2_only()\n+class GroupNormalizationTest(test_combinations.TestCase):\n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_trainable_weights(self):\n+        # Check if weights get initialized correctly\n+        layer = GroupNormalization(groups=1, scale=False, center=False)\n+        layer.build((None, 3, 4))\n+        self.assertEqual(len(layer.trainable_weights), 0)\n+        self.assertEqual(len(layer.weights), 0)\n+\n+        # Check if weights get initialized correctly\n+        layer = GroupNormalization(groups=1, scale=True, center=True)\n+        layer.build((None, 3, 4))\n+        self.assertEqual(len(layer.trainable_weights), 2)\n+        self.assertEqual(len(layer.weights), 2)\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_groupnorm(self):\n+        test_utils.layer_test(\n+            GroupNormalization,\n+            kwargs={\n+                \"gamma_regularizer\": keras.regularizers.l2(0.01),\n+                \"beta_regularizer\": keras.regularizers.l2(0.01),\n+            },\n+            input_shape=(3, 4, 32),\n+        )\n+\n+        test_utils.layer_test(\n+            GroupNormalization,\n+            kwargs={\n+                \"groups\": 4,\n+                \"gamma_constraint\": keras.constraints.UnitNorm(),\n+                \"beta_constraint\": keras.constraints.UnitNorm(),\n+            },\n+            input_shape=(3, 4, 4),\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_correctness_1d(self):\n+        layer_with_1_group = GroupNormalization(\n+            groups=1, axis=-1, input_shape=(8,), scale=False, center=False\n+        )\n+        layer_with_2_groups = GroupNormalization(\n+            groups=2, axis=-1, input_shape=(8,), scale=False, center=False\n+        )\n+\n+        inputs = tf.constant(\n+            [-1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 0, -2.0], shape=(1, 8)\n+        )\n+\n+        expected_output_1_group = tf.constant(\n+            [-0.898, -0.898, 0.539, 0.539, 1.257, 1.257, -0.180, -1.616],\n+            shape=(1, 8),\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(layer_with_1_group)(inputs),\n+            expected_output_1_group,\n+            atol=1e-3,\n+        )\n+\n+        expected_output_2_groups = tf.constant(\n+            [-1.0, -1.0, 1.0, 1.0, 0.904, 0.904, -0.301, -1.507], shape=(1, 8)\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(layer_with_2_groups)(inputs),\n+            expected_output_2_groups,\n+            atol=1e-3,\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_correctness_2d(self):\n+        layer_with_1_group = GroupNormalization(\n+            groups=1, axis=-1, input_shape=(2, 4), scale=False, center=False\n+        )\n+        layer_with_2_groups = GroupNormalization(\n+            groups=2, axis=-1, input_shape=(2, 4), scale=False, center=False\n+        )\n+\n+        inputs = tf.constant(\n+            [[-1.0, -1.0, 2.0, 2.0], [1.0, 1.0, 0, -2.0]], shape=(1, 2, 4)\n+        )\n+\n+        expected_output_1_group = tf.constant(\n+            [[-0.898, -0.898, 1.257, 1.257], [0.539, 0.539, -0.180, -1.616]],\n+            shape=(1, 2, 4),\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(layer_with_1_group)(inputs),\n+            expected_output_1_group,\n+            atol=1e-3,\n+        )\n+\n+        expected_output_2_groups = tf.constant(\n+            [[-1.0, -1.0, 0.904, 0.904], [1.0, 1.0, -0.301, -1.507]],\n+            shape=(1, 2, 4),\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(layer_with_2_groups)(inputs),\n+            expected_output_2_groups,\n+            atol=1e-3,\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_correctness_instance_norm(self):\n+        instance_norm_layer = GroupNormalization(\n+            groups=4, axis=-1, input_shape=(2, 4), scale=False, center=False\n+        )\n+\n+        inputs = tf.constant(\n+            [[-1.0, 1.0, 0, 2.0], [1.0, 3.0, -4, -2.0]], shape=(1, 2, 4)\n+        )\n+\n+        expected_instance_norm_output = tf.constant(\n+            [[-1.0, -1.0, 1.0, 1.0], [1.0, 1.0, -1.0, -1.0]], shape=(1, 2, 4)\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(instance_norm_layer)(inputs),\n+            expected_instance_norm_output,\n+            atol=1e-3,\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_correctness_with_centering(self):\n+        normalization_layer = GroupNormalization(\n+            groups=2,\n+            axis=-1,\n+            input_shape=(8,),\n+            scale=False,\n+            center=True,\n+            beta_initializer=Constant(10),\n+        )\n+\n+        inputs = tf.constant(\n+            [-1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 0, -2.0], shape=(1, 8)\n+        )\n+\n+        expected_output = tf.constant(\n+            [9.0, 9.0, 11.0, 11.0, 10.904, 10.904, 9.699, 8.493], shape=(1, 8)\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(normalization_layer)(inputs),\n+            expected_output,\n+            atol=1e-3,\n+        )\n+\n+    @test_combinations.run_all_keras_modes\n+    def test_correctness_with_scaling(self):\n+        normalization_layer = GroupNormalization(\n+            groups=2,\n+            axis=-1,\n+            input_shape=(8,),\n+            scale=True,\n+            center=False,\n+            gamma_initializer=Constant(2),\n+        )\n+\n+        inputs = tf.constant(\n+            [-1.0, -1.0, 1.0, 1.0, 2.0, 2.0, 0, -2.0], shape=(1, 8)\n+        )\n+\n+        expected_output = tf.constant(\n+            [-2.0, -2.0, 2.0, 2.0, 1.809, 1.808, -0.602, -3.014], shape=(1, 8)\n+        )\n+        self.assertAllClose(\n+            _build_group_normalization_model(normalization_layer)(inputs),\n+            expected_output,\n+            atol=1e-3,\n+        )\n+\n+    def test_validates_groups_against_channels(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"must be a multiple of the number of channels\"\n+        ):\n+            norm = GroupNormalization(groups=3, axis=-1)\n+            norm.build(input_shape=(2, 10))\n+\n+        with self.assertRaisesRegex(\n+            ValueError, r\"cannot be more than the number of channels\"\n+        ):\n+            norm = GroupNormalization(groups=32, axis=-1)\n+            norm.build(input_shape=(2, 8))\n+\n+    def test_validates_known_number_of_channels(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"tensor should have a defined dimension\"\n+        ):\n+            norm = GroupNormalization(axis=-1)\n+            norm.build(input_shape=(1, 32, None))\n+\n+    def test_rejects_invalid_axis(self):\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Invalid value for `axis` argument\"\n+        ):\n+            norm = GroupNormalization(axis=-4)\n+            norm.build(input_shape=(64, 32, 32))\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Invalid value for `axis` argument\"\n+        ):\n+            norm = GroupNormalization(axis=3)\n+            norm.build(input_shape=(64, 32, 32))\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -33,6 +33,7 @@ from keras.layers import reshaping\n from keras.layers import rnn\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import batch_normalization_v1\n+from keras.layers.normalization import group_normalization\n from keras.layers.normalization import layer_normalization\n from keras.layers.normalization import unit_normalization\n from keras.layers.preprocessing import category_encoding\n@@ -66,6 +67,7 @@ ALL_MODULES = (\n     locally_connected,\n     merging,\n     batch_normalization_v1,\n+    group_normalization,\n     layer_normalization,\n     unit_normalization,\n     pooling,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
