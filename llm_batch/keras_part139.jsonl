{"custom_id": "keras#45ad5096114300443859644148ab0b69ec6f65c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 903 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -20,6 +20,9 @@ class LeakyReLU(Layer):\n \n     # Arguments\n         alpha: float >= 0. Negative slope coefficient.\n+\n+    # References\n+        - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n     '''\n     def __init__(self, alpha=0.3, **kwargs):\n         self.supports_masking = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#150e0fa8a6571233ed107c4dded52387a9d3739c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 12241 | Contributors (this commit): 61 | Commits (past 90d): 16 | Contributors (cumulative): 71 | DMM Complexity: 1.0\n\nDIFF:\n@@ -280,6 +280,10 @@ class Reshape(Layer):\n         # as intermediate layer in a Sequential model\n         model.add(Reshape((6, 2)))\n         # now: model.output_shape == (None, 6, 2)\n+\n+        # also supports shape inference using `-1` as dimension\n+        model.add(Reshape((-1, 2, 2)))\n+        # now: model.output_shape == (None, 3, 2, 2)\n     ```\n     '''\n     def __init__(self, target_shape, **kwargs):\n@@ -350,7 +354,7 @@ class Reshape(Layer):\n             elif hasattr(K, 'int_shape'):\n                 input_shape = K.int_shape(x)\n             if input_shape is not None:\n-                target_shape = self.get_output_shape_for(input_shape)\n+                target_shape = self.get_output_shape_for(input_shape)[1:]\n         return K.reshape(x, (-1,) + target_shape)\n \n     def get_config(self):\n\n@@ -255,6 +255,14 @@ def test_reshape():\n                kwargs={'target_shape': (8, 1)},\n                input_shape=(3, 2, 4))\n \n+    layer_test(core.Reshape,\n+               kwargs={'target_shape': (-1, 1)},\n+               input_shape=(3, 2, 4))\n+\n+    layer_test(core.Reshape,\n+               kwargs={'target_shape': (1, -1)},\n+               input_shape=(3, 2, 4))\n+\n \n @keras_test\n def test_permute():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0d95fd6c2cd8ffc0738819825c3065e3c89977c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 14 | Files Changed: 9 | Hunks: 12 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 2423 | Contributors (this commit): 24 | Commits (past 90d): 14 | Contributors (cumulative): 44 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,12 +42,11 @@ from scipy import ndimage\n import pylab\n from keras import backend as K\n from keras.layers.convolutional import Convolution2D, MaxPooling2D\n-from keras.layers import Input, Layer, Dense, Activation, Flatten\n-from keras.layers import Reshape, Lambda, merge, Permute, TimeDistributed\n+from keras.layers import Input, Dense, Activation\n+from keras.layers import Reshape, Lambda, merge\n from keras.models import Model\n from keras.layers.recurrent import GRU\n from keras.optimizers import SGD\n-from keras.utils import np_utils\n from keras.utils.data_utils import get_file\n from keras.preprocessing import image\n import keras.callbacks\n@@ -399,7 +398,6 @@ class VizCallback(keras.callbacks.Callback):\n def train(run_name, start_epoch, stop_epoch, img_w):\n     # Input Parameters\n     img_h = 64\n-    minibatch_size = 32\n     words_per_epoch = 16000\n     val_split = 0.2\n     val_words = int(words_per_epoch * (val_split))\n\n@@ -10,7 +10,7 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional\n+from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n from keras.datasets import imdb\n \n \n\n@@ -16,7 +16,6 @@ from keras.layers import Dense, Dropout, Activation\n from keras.layers import Embedding\n from keras.layers import Convolution1D, GlobalMaxPooling1D\n from keras.datasets import imdb\n-from keras import backend as K\n \n \n # set parameters:\n\n@@ -15,10 +15,9 @@ import numpy as np\n np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n-from keras.utils import np_utils\n from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Embedding\n-from keras.layers import LSTM, SimpleRNN, GRU\n+from keras.layers import Dense, Activation, Embedding\n+from keras.layers import LSTM\n from keras.datasets import imdb\n \n max_features = 20000\n\n@@ -12,7 +12,7 @@ has at least ~100k characters. ~1M is better.\n \n from __future__ import print_function\n from keras.models import Sequential\n-from keras.layers import Dense, Activation, Dropout\n+from keras.layers import Dense, Activation\n from keras.layers import LSTM\n from keras.optimizers import RMSprop\n from keras.utils.data_utils import get_file\n\n@@ -27,7 +27,7 @@ After 5 epochs: train acc: 0.9858, val acc: 0.9864\n from __future__ import print_function\n \n from keras.datasets import mnist\n-from keras.models import Sequential, Model\n+from keras.models import Model\n from keras.layers import Input, Dense, TimeDistributed\n from keras.layers import LSTM\n from keras.utils import np_utils\n\n@@ -12,7 +12,7 @@ np.random.seed(1337)  # for reproducibility\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n-from keras.optimizers import SGD, Adam, RMSprop\n+from keras.optimizers import RMSprop\n from keras.utils import np_utils\n \n \n\n@@ -19,7 +19,7 @@ import random\n from keras.datasets import mnist\n from keras.models import Sequential, Model\n from keras.layers import Dense, Dropout, Input, Lambda\n-from keras.optimizers import SGD, RMSprop\n+from keras.optimizers import RMSprop\n from keras import backend as K\n \n \n\n@@ -47,7 +47,7 @@ from scipy.optimize import fmin_l_bfgs_b\n from scipy.misc import imread, imsave\r\n \r\n from keras import backend as K\r\n-from keras.layers import Input, Convolution2D, MaxPooling2D, AveragePooling2D\r\n+from keras.layers import Input, AveragePooling2D\r\n from keras.models import Model\r\n from keras.preprocessing.image import load_img, img_to_array\r\n from keras.applications import vgg19\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0369909d0c5cc5c9cccbcf98b09c16fadea5457", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 16 | Churn Cumulative: 3983 | Contributors (this commit): 42 | Commits (past 90d): 27 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -312,7 +312,7 @@ def get_function_signature(function, method=True):\n     for a in args:\n         st += str(a) + ', '\n     for a, v in kwargs:\n-        if type(v) == str:\n+        if isinstance(v, str):\n             v = '\\'' + v + '\\''\n         st += str(a) + '=' + str(v) + ', '\n     if kwargs or args:\n\n@@ -140,7 +140,7 @@ loss += settings['dream_l2'] * K.sum(K.square(dream)) / np.prod(img_size)\n grads = K.gradients(loss, dream)\n \n outputs = [loss]\n-if type(grads) in {list, tuple}:\n+if isinstance(grads, (list, tuple)):\n     outputs += grads\n else:\n     outputs.append(grads)\n\n@@ -301,7 +301,7 @@ loss_grads = K.gradients(loss, target_image)\n \r\n # Evaluator class for computing efficiency\r\n outputs = [loss]\r\n-if type(loss_grads) in {list, tuple}:\r\n+if isinstance(loss_grads, (list, tuple)):\r\n     outputs += loss_grads\r\n else:\r\n     outputs.append(loss_grads)\r\n\n@@ -208,7 +208,7 @@ loss += total_variation_weight * total_variation_loss(combination_image)\n grads = K.gradients(loss, combination_image)\n \n outputs = [loss]\n-if type(grads) in {list, tuple}:\n+if isinstance(grads, (list, tuple)):\n     outputs += grads\n else:\n     outputs.append(grads)\n\n@@ -71,7 +71,7 @@ class PReLU(Layer):\n         self.supports_masking = True\n         self.init = initializations.get(init)\n         self.initial_weights = weights\n-        if type(shared_axes) is not list and type(shared_axes) is not tuple:\n+        if not isinstance(shared_axes, (list, tuple)):\n             self.shared_axes = [shared_axes]\n         else:\n             self.shared_axes = list(shared_axes)\n@@ -174,7 +174,7 @@ class ParametricSoftplus(Layer):\n         self.alpha_init = K.cast_to_floatx(alpha_init)\n         self.beta_init = K.cast_to_floatx(beta_init)\n         self.initial_weights = weights\n-        if type(shared_axes) is not list and type(shared_axes) is not tuple:\n+        if not isinstance(shared_axes, (list, tuple)):\n             self.shared_axes = [shared_axes]\n         else:\n             self.shared_axes = list(shared_axes)\n@@ -279,7 +279,7 @@ class SReLU(Layer):\n         self.a_left_init = a_left_init\n         self.t_right_init = t_right_init\n         self.a_right_init = a_right_init\n-        if type(shared_axes) is not list and type(shared_axes) is not tuple:\n+        if not isinstance(shared_axes, (list, tuple)):\n             self.shared_axes = [shared_axes]\n         else:\n             self.shared_axes = list(shared_axes)\n\n@@ -126,7 +126,7 @@ def count_total_params(layers, layer_set=None):\n         if layer in layer_set:\n             continue\n         layer_set.add(layer)\n-        if type(layer) in (Model, Sequential):\n+        if isinstance(layer, (Model, Sequential)):\n             t, nt = count_total_params(layer.layers, layer_set)\n             trainable_count += t\n             non_trainable_count += nt\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#293940600b448f9ad72cab1874d99fb8cc435e2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4490 | Contributors (this commit): 52 | Commits (past 90d): 34 | Contributors (cumulative): 52 | DMM Complexity: None\n\nDIFF:\n@@ -1196,7 +1196,7 @@ def normalize_batch_in_training(x, gamma, beta,\n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     '''Applies batch normalization on x given mean, var, beta and gamma:\n \n-    output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta\n+    output = (x - mean) / sqrt(var + epsilon) * gamma + beta\n     '''\n     return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5863fc74b1d8ba0f89af4795c6896ce0c0c6a5f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 51 | Files Changed: 1 | Hunks: 49 | Methods Changed: 19 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 128 | Churn Cumulative: 2099 | Contributors (this commit): 37 | Commits (past 90d): 14 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,7 +16,9 @@ from pkg_resources import parse_version\n \n \n class CallbackList(object):\n-    def __init__(self, callbacks=[], queue_length=10):\n+\n+    def __init__(self, callbacks=None, queue_length=10):\n+        callbacks = callbacks or []\n         self.callbacks = [c for c in callbacks]\n         self.queue_length = queue_length\n \n@@ -31,31 +33,36 @@ class CallbackList(object):\n         for callback in self.callbacks:\n             callback._set_model(model)\n \n-    def on_epoch_begin(self, epoch, logs={}):\n+    def on_epoch_begin(self, epoch, logs=None):\n+        logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_epoch_begin(epoch, logs)\n         self._delta_t_batch = 0.\n         self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n         self._delta_ts_batch_end = deque([], maxlen=self.queue_length)\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_epoch_end(epoch, logs)\n \n-    def on_batch_begin(self, batch, logs={}):\n+    def on_batch_begin(self, batch, logs=None):\n+        logs = logs or {}\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n             callback.on_batch_begin(batch, logs)\n         self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_begin)\n-        if self._delta_t_batch > 0. and delta_t_median > 0.95 * \\\n-           self._delta_t_batch and delta_t_median > 0.1:\n+        if (self._delta_t_batch > 0. and\n+           delta_t_median > 0.95 * self._delta_t_batch and\n+           delta_t_median > 0.1):\n             warnings.warn('Method on_batch_begin() is slow compared '\n                           'to the batch update (%f). Check your callbacks.'\n                           % delta_t_median)\n         self._t_enter_batch = time.time()\n \n-    def on_batch_end(self, batch, logs={}):\n+    def on_batch_end(self, batch, logs=None):\n+        logs = logs or {}\n         if not hasattr(self, '_t_enter_batch'):\n             self._t_enter_batch = time.time()\n         self._delta_t_batch = time.time() - self._t_enter_batch\n@@ -64,16 +71,19 @@ class CallbackList(object):\n             callback.on_batch_end(batch, logs)\n         self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_end)\n-        if self._delta_t_batch > 0. and (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1):\n+        if (self._delta_t_batch > 0. and\n+           (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n             warnings.warn('Method on_batch_end() is slow compared '\n                           'to the batch update (%f). Check your callbacks.'\n                           % delta_t_median)\n \n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n+        logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_train_begin(logs)\n \n-    def on_train_end(self, logs={}):\n+    def on_train_end(self, logs=None):\n+        logs = logs or {}\n         for callback in self.callbacks:\n             callback.on_train_end(logs)\n \n@@ -113,22 +123,22 @@ class Callback(object):\n     def _set_model(self, model):\n         self.model = model\n \n-    def on_epoch_begin(self, epoch, logs={}):\n+    def on_epoch_begin(self, epoch, logs=None):\n         pass\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n         pass\n \n-    def on_batch_begin(self, batch, logs={}):\n+    def on_batch_begin(self, batch, logs=None):\n         pass\n \n-    def on_batch_end(self, batch, logs={}):\n+    def on_batch_end(self, batch, logs=None):\n         pass\n \n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n         pass\n \n-    def on_train_end(self, logs={}):\n+    def on_train_end(self, logs=None):\n         pass\n \n \n@@ -139,11 +149,12 @@ class BaseLogger(Callback):\n     This callback is automatically applied to\n     every Keras model.\n     '''\n-    def on_epoch_begin(self, epoch, logs={}):\n+    def on_epoch_begin(self, epoch, logs=None):\n         self.seen = 0\n         self.totals = {}\n \n-    def on_batch_end(self, batch, logs={}):\n+    def on_batch_end(self, batch, logs=None):\n+        logs = logs or {}\n         batch_size = logs.get('size', 0)\n         self.seen += batch_size\n \n@@ -153,32 +164,34 @@ class BaseLogger(Callback):\n             else:\n                 self.totals[k] = v * batch_size\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        if logs is not None:\n             for k in self.params['metrics']:\n                 if k in self.totals:\n-                # make value available to next callbacks\n+                    # Make value available to next callbacks.\n                     logs[k] = self.totals[k] / self.seen\n \n \n class ProgbarLogger(Callback):\n     '''Callback that prints metrics to stdout.\n     '''\n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n         self.verbose = self.params['verbose']\n         self.nb_epoch = self.params['nb_epoch']\n \n-    def on_epoch_begin(self, epoch, logs={}):\n+    def on_epoch_begin(self, epoch, logs=None):\n         if self.verbose:\n             print('Epoch %d/%d' % (epoch + 1, self.nb_epoch))\n             self.progbar = Progbar(target=self.params['nb_sample'],\n                                    verbose=self.verbose)\n         self.seen = 0\n \n-    def on_batch_begin(self, batch, logs={}):\n+    def on_batch_begin(self, batch, logs=None):\n         if self.seen < self.params['nb_sample']:\n             self.log_values = []\n \n-    def on_batch_end(self, batch, logs={}):\n+    def on_batch_end(self, batch, logs=None):\n+        logs = logs or {}\n         batch_size = logs.get('size', 0)\n         self.seen += batch_size\n \n@@ -186,12 +199,13 @@ class ProgbarLogger(Callback):\n             if k in logs:\n                 self.log_values.append((k, logs[k]))\n \n-        # skip progbar update for the last batch;\n-        # will be handled by on_epoch_end\n+        # Skip progbar update for the last batch;\n+        # will be handled by on_epoch_end.\n         if self.verbose and self.seen < self.params['nb_sample']:\n             self.progbar.update(self.seen, self.log_values)\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n         for k in self.params['metrics']:\n             if k in logs:\n                 self.log_values.append((k, logs[k]))\n@@ -207,11 +221,12 @@ class History(Callback):\n     every Keras model. The `History` object\n     gets returned by the `fit` method of models.\n     '''\n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n         self.epoch = []\n         self.history = {}\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n         self.epoch.append(epoch)\n         for k, v in logs.items():\n             self.history.setdefault(k, []).append(v)\n@@ -281,7 +296,8 @@ class ModelCheckpoint(Callback):\n                 self.monitor_op = np.less\n                 self.best = np.Inf\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n         self.epochs_since_last_save += 1\n         if self.epochs_since_last_save >= self.period:\n             self.epochs_since_last_save = 0\n@@ -336,7 +352,8 @@ class EarlyStopping(Callback):\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n     '''\n-    def __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto'):\n+    def __init__(self, monitor='val_loss',\n+                 min_delta=0, patience=0, verbose=0, mode='auto'):\n         super(EarlyStopping, self).__init__()\n \n         self.monitor = monitor\n@@ -367,11 +384,11 @@ class EarlyStopping(Callback):\n         else:\n             self.min_delta *= -1\n \n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n         self.wait = 0       # Allow instances to be re-used\n         self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n         current = logs.get(self.monitor)\n         if current is None:\n             warnings.warn('Early stopping requires %s available!' %\n@@ -386,7 +403,7 @@ class EarlyStopping(Callback):\n                 self.model.stop_training = True\n             self.wait += 1\n \n-    def on_train_end(self, logs={}):\n+    def on_train_end(self, logs=None):\n         if self.stopped_epoch > 0 and self.verbose > 0:\n             print('Epoch %05d: early stopping' % (self.stopped_epoch))\n \n@@ -408,15 +425,17 @@ class RemoteMonitor(Callback):\n                  root='http://localhost:9000',\n                  path='/publish/epoch/end/',\n                  field='data',\n-                 headers={'Accept': 'application/json', 'Content-Type': 'application/json'}):\n+                 headers={'Accept': 'application/json',\n+                          'Content-Type': 'application/json'}):\n         super(RemoteMonitor, self).__init__()\n         self.root = root\n         self.path = path\n         self.field = field\n         self.headers = headers\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n         import requests\n+        logs = logs or {}\n         send = {}\n         send['epoch'] = epoch\n         for k, v in logs.items():\n@@ -442,15 +461,13 @@ class LearningRateScheduler(Callback):\n         super(LearningRateScheduler, self).__init__()\n         self.schedule = schedule\n \n-    def on_epoch_begin(self, epoch, logs={}):\n-        assert hasattr(self.model.optimizer, 'lr'), \\\n-            'Optimizer must have a \"lr\" attribute.'\n+    def on_epoch_begin(self, epoch, logs=None):\n+        if not hasattr(self.model.optimizer, 'lr'):\n+            raise ValueError('Optimizer must have a \"lr\" attribute.')\n         lr = self.schedule(epoch)\n-\n         if not isinstance(lr, (float, np.float32, np.float64)):\n             raise ValueError('The output of the \"schedule\" function '\n                              'should be float.')\n-\n         K.set_value(self.model.optimizer.lr, lr)\n \n \n@@ -483,7 +500,10 @@ class TensorBoard(Callback):\n             write_graph is set to True.\n     '''\n \n-    def __init__(self, log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False):\n+    def __init__(self, log_dir='./logs',\n+                 histogram_freq=0,\n+                 write_graph=True,\n+                 write_images=False):\n         super(TensorBoard, self).__init__()\n         if K._BACKEND != 'tensorflow':\n             raise RuntimeError('TensorBoard callback only works '\n@@ -543,8 +563,9 @@ class TensorBoard(Callback):\n             else:\n                 self.writer = tf.train.SummaryWriter(self.log_dir)\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n         import tensorflow as tf\n+        logs = logs or {}\n \n         if self.model.validation_data and self.histogram_freq:\n             if epoch % self.histogram_freq == 0:\n@@ -618,7 +639,8 @@ class ReduceLROnPlateau(Callback):\n \n         self.monitor = monitor\n         if factor >= 1.0:\n-            raise ValueError('ReduceLROnPlateau does not support a factor >= 1.0.')\n+            raise ValueError('ReduceLROnPlateau '\n+                             'does not support a factor >= 1.0.')\n         self.factor = factor\n         self.min_lr = min_lr\n         self.epsilon = epsilon\n@@ -635,7 +657,8 @@ class ReduceLROnPlateau(Callback):\n     def reset(self):\n         if self.mode not in ['auto', 'min', 'max']:\n             warnings.warn('Learning Rate Plateau Reducing mode %s is unknown, '\n-                          'fallback to auto mode.' % (self.mode), RuntimeWarning)\n+                          'fallback to auto mode.' % (self.mode),\n+                          RuntimeWarning)\n             self.mode = 'auto'\n         if self.mode == 'min' or (self.mode == 'auto' and 'acc' not in self.monitor):\n             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\n@@ -647,10 +670,11 @@ class ReduceLROnPlateau(Callback):\n         self.wait = 0\n         self.lr_epsilon = self.min_lr * 1e-4\n \n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n         self.reset()\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n         logs['lr'] = K.get_value(self.model.optimizer.lr)\n         current = logs.get(self.monitor)\n         if current is None:\n@@ -708,7 +732,7 @@ class CSVLogger(Callback):\n         self.append_header = True\n         super(CSVLogger, self).__init__()\n \n-    def on_train_begin(self, logs={}):\n+    def on_train_begin(self, logs=None):\n         if self.append:\n             if os.path.exists(self.filename):\n                 with open(self.filename) as f:\n@@ -717,7 +741,9 @@ class CSVLogger(Callback):\n         else:\n             self.csv_file = open(self.filename, 'w')\n \n-    def on_epoch_end(self, epoch, logs={}):\n+    def on_epoch_end(self, epoch, logs=None):\n+        logs = logs or {}\n+\n         def handle_value(k):\n             is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n             if isinstance(k, Iterable) and not is_zero_dim_ndarray:\n@@ -727,7 +753,8 @@ class CSVLogger(Callback):\n \n         if not self.writer:\n             self.keys = sorted(logs.keys())\n-            self.writer = csv.DictWriter(self.csv_file, fieldnames=['epoch'] + self.keys)\n+            self.writer = csv.DictWriter(self.csv_file,\n+                                         fieldnames=['epoch'] + self.keys)\n             if self.append_header:\n                 self.writer.writeheader()\n \n@@ -774,7 +801,6 @@ class LambdaCallback(Callback):\n \n         model.fit(..., callbacks=[batch_print_callback, plot_loss_callback, cleanup_callback])\n         ```\n-\n     \"\"\"\n \n     def __init__(self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b05aebc0ce0717297d65305801e27f62a38cff5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 7 | Hunks: 13 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 23256 | Contributors (this commit): 105 | Commits (past 90d): 61 | Contributors (cumulative): 163 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ document vector is considered to preserve both the word-level and\n sentence-level structure of the context.\n \n # References\n-    - [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://web.stanford.edu/~jurafsky/pubs/P15-1107.pdf)\n+    - [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/abs/1506.01057)\n         Encodes paragraphs and documents with HRNN.\n         Results have shown that HRNN outperforms standard\n         RNNs and may play some role in more sophisticated generation tasks like\n\n@@ -65,7 +65,7 @@ class PReLU(Layer):\n             set `shared_axes=[1, 2]`.\n \n     # References\n-        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/pdf/1502.01852v1.pdf)\n+        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n     '''\n     def __init__(self, init='zero', weights=None, shared_axes=None, **kwargs):\n         self.supports_masking = True\n@@ -124,7 +124,7 @@ class ELU(Layer):\n         alpha: scale for the negative factor.\n \n     # References\n-        - [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](http://arxiv.org/pdf/1511.07289v1.pdf)\n+        - [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289v1)\n     '''\n     def __init__(self, alpha=1.0, **kwargs):\n         self.supports_masking = True\n@@ -228,7 +228,7 @@ class ThresholdedReLU(Layer):\n         theta: float >= 0. Threshold location of activation.\n \n     # References\n-        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/pdf/1402.3337.pdf)\n+        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/abs/1402.3337)\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n         self.supports_masking = True\n\n@@ -583,7 +583,7 @@ class Deconvolution2D(Convolution2D):\n         `rows` and `cols` values might have changed due to padding.\n \n     # References\n-        [1] [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285 \"arXiv:1603.07285v1 [stat.ML]\")\n+        [1] [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)\n         [2] [Transposed convolution arithmetic](http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html#transposed-convolution-arithmetic)\n         [3] [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     '''\n\n@@ -243,7 +243,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n \n     # References\n         - [Convolutional LSTM Network: A Machine Learning Approach for\n-        Precipitation Nowcasting](http://arxiv.org/pdf/1506.04214v1.pdf)\n+        Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n         The current implementation does not include the feedback loop on the\n         cells output\n     '''\n\n@@ -116,7 +116,7 @@ class SpatialDropout1D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)\n+        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n     '''\n     def __init__(self, p, **kwargs):\n         super(SpatialDropout1D, self).__init__(p, **kwargs)\n@@ -154,7 +154,7 @@ class SpatialDropout2D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)\n+        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n     '''\n     def __init__(self, p, dim_ordering='default', **kwargs):\n         if dim_ordering == 'default':\n@@ -202,7 +202,7 @@ class SpatialDropout3D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)\n+        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n     '''\n     def __init__(self, p, dim_ordering='default', **kwargs):\n         if dim_ordering == 'default':\n@@ -875,7 +875,7 @@ class MaxoutDense(Layer):\n         2D tensor with shape: `(nb_samples, output_dim)`.\n \n     # References\n-        - [Maxout Networks](http://arxiv.org/pdf/1302.4389.pdf)\n+        - [Maxout Networks](http://arxiv.org/abs/1302.4389)\n     '''\n     def __init__(self, output_dim,\n                  nb_feature=4,\n@@ -1001,7 +1001,7 @@ class Highway(Layer):\n         2D tensor with shape: `(nb_samples, input_dim)`.\n \n     # References\n-        - [Highway Networks](http://arxiv.org/pdf/1505.00387v2.pdf)\n+        - [Highway Networks](http://arxiv.org/abs/1505.00387v2)\n     '''\n     def __init__(self,\n                  init='glorot_uniform',\n\n@@ -59,7 +59,7 @@ class BatchNormalization(Layer):\n         Same shape as input.\n \n     # References\n-        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://jmlr.org/proceedings/papers/v37/ioffe15.pdf)\n+        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n     '''\n     def __init__(self, epsilon=1e-3, mode=0, axis=-1, momentum=0.99,\n                  weights=None, beta_init='zero', gamma_init='one',\n\n@@ -424,8 +424,8 @@ class GRU(Recurrent):\n         dropout_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n \n     # References\n-        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](http://www.aclweb.org/anthology/W14-4012)\n-        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/pdf/1412.3555v1.pdf)\n+        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n+        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     '''\n     def __init__(self, output_dim,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50b4f7fad50176c93c9b7764a5fc4e6d1e5e068d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 4690 | Contributors (this commit): 32 | Commits (past 90d): 13 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -128,19 +128,19 @@ class Recurrent(Layer):\n     # Note on using statefulness in RNNs\n         You can set RNN layers to be 'stateful', which means that the states\n         computed for the samples in one batch will be reused as initial states\n-        for the samples in the next batch.\n-        This assumes a one-to-one mapping between\n-        samples in different successive batches.\n+        for the samples in the next batch. This assumes a one-to-one mapping\n+        between samples in different successive batches.\n \n         To enable statefulness:\n             - specify `stateful=True` in the layer constructor.\n             - specify a fixed batch size for your model, by passing\n                 if sequential model:\n-                  a `batch_input_shape=(...)` to the first layer in your model.\n+                  `batch_input_shape=(...)` to the first layer in your model.\n                 else for functional model with 1 or more Input layers:\n-                  a `batch_shape=(...)` to all the first layers in your model.\n+                  `batch_shape=(...)` to all the first layers in your model.\n                 This is the expected shape of your inputs *including the batch size*.\n                 It should be a tuple of integers, e.g. `(32, 10, 100)`.\n+            - specify `shuffle=False` when calling fit().\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b57b9d3f8ee4c8a809e4fbaa62eade89d3bea456", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 924 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -142,7 +142,7 @@ class ELU(Layer):\n \n class ParametricSoftplus(Layer):\n     '''Parametric Softplus:\n-    `alpha * log(1 + exp(beta * x))`\n+    `f(x) = alpha * log(1 + exp(beta * x))`\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -213,7 +213,7 @@ class ParametricSoftplus(Layer):\n \n class ThresholdedReLU(Layer):\n     '''Thresholded Rectified Linear Unit:\n-    `f(x) = x for x > theta`\n+    `f(x) = x for x > theta`,\n     `f(x) = 0 otherwise`.\n \n     # Input shape\n@@ -245,7 +245,10 @@ class ThresholdedReLU(Layer):\n \n \n class SReLU(Layer):\n-    '''S-shaped Rectified Linear Unit.\n+    '''S-shaped Rectified Linear Unit:\n+    `f(x) = t^r + a^r(x - t^r) for x >= t^r`,\n+    `f(x) = x for t^r > x > t^l`,\n+    `f(x) = t^l + a^l(x - t^l) for x <= t^l`.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe72033b2ebc8f7397edc99190018a41e6e0f787", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 51 | Files Changed: 1 | Hunks: 38 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 142 | Churn Cumulative: 2241 | Contributors (this commit): 37 | Commits (past 90d): 15 | Contributors (cumulative): 37 | DMM Complexity: 0.05263157894736842\n\nDIFF:\n@@ -9,7 +9,9 @@ import time\n import json\n import warnings\n \n-from collections import deque, OrderedDict, Iterable\n+from collections import deque\n+from collections import OrderedDict\n+from collections import Iterable\n from .utils.generic_utils import Progbar\n from keras import backend as K\n from pkg_resources import parse_version\n@@ -89,7 +91,7 @@ class CallbackList(object):\n \n \n class Callback(object):\n-    '''Abstract base class used to build new callbacks.\n+    \"\"\"Abstract base class used to build new callbacks.\n \n     # Properties\n         params: dict. Training parameters\n@@ -113,7 +115,8 @@ class Callback(object):\n             the number of samples in the current batch.\n         on_batch_end: logs include `loss`, and optionally `acc`\n             (if accuracy monitoring is enabled).\n-    '''\n+    \"\"\"\n+\n     def __init__(self):\n         pass\n \n@@ -143,12 +146,13 @@ class Callback(object):\n \n \n class BaseLogger(Callback):\n-    '''Callback that accumulates epoch averages of\n+    \"\"\"Callback that accumulates epoch averages of\n     the metrics being monitored.\n \n     This callback is automatically applied to\n     every Keras model.\n-    '''\n+    \"\"\"\n+\n     def on_epoch_begin(self, epoch, logs=None):\n         self.seen = 0\n         self.totals = {}\n@@ -173,8 +177,9 @@ class BaseLogger(Callback):\n \n \n class ProgbarLogger(Callback):\n-    '''Callback that prints metrics to stdout.\n-    '''\n+    \"\"\"Callback that prints metrics to stdout.\n+    \"\"\"\n+\n     def on_train_begin(self, logs=None):\n         self.verbose = self.params['verbose']\n         self.nb_epoch = self.params['nb_epoch']\n@@ -214,13 +219,14 @@ class ProgbarLogger(Callback):\n \n \n class History(Callback):\n-    '''Callback that records events\n+    \"\"\"Callback that records events\n     into a `History` object.\n \n     This callback is automatically applied to\n     every Keras model. The `History` object\n     gets returned by the `fit` method of models.\n-    '''\n+    \"\"\"\n+\n     def on_train_begin(self, logs=None):\n         self.epoch = []\n         self.history = {}\n@@ -233,15 +239,15 @@ class History(Callback):\n \n \n class ModelCheckpoint(Callback):\n-    '''Save the model after every epoch.\n+    \"\"\"Save the model after every epoch.\n \n     `filepath` can contain named formatting options,\n     which will be filled the value of `epoch` and\n     keys in `logs` (passed in `on_epoch_end`).\n \n     For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n-    then multiple files will be save with the epoch number and\n-    the validation loss.\n+    then the model checkpoints will be saved with the epoch number and\n+    the validation loss in the filename.\n \n     # Arguments\n         filepath: string, path to save the model file.\n@@ -262,8 +268,8 @@ class ModelCheckpoint(Callback):\n             saved (`model.save_weights(filepath)`), else the full model\n             is saved (`model.save(filepath)`).\n         period: Interval (number of epochs) between checkpoints.\n+    \"\"\"\n \n-    '''\n     def __init__(self, filepath, monitor='val_loss', verbose=0,\n                  save_best_only=False, save_weights_only=False,\n                  mode='auto', period=1):\n@@ -333,7 +339,7 @@ class ModelCheckpoint(Callback):\n \n \n class EarlyStopping(Callback):\n-    '''Stop training when a monitored quantity has stopped improving.\n+    \"\"\"Stop training when a monitored quantity has stopped improving.\n \n     # Arguments\n         monitor: quantity to be monitored.\n@@ -351,7 +357,8 @@ class EarlyStopping(Callback):\n             monitored has stopped increasing; in `auto`\n             mode, the direction is automatically inferred\n             from the name of the monitored quantity.\n-    '''\n+    \"\"\"\n+\n     def __init__(self, monitor='val_loss',\n                  min_delta=0, patience=0, verbose=0, mode='auto'):\n         super(EarlyStopping, self).__init__()\n@@ -409,17 +416,19 @@ class EarlyStopping(Callback):\n \n \n class RemoteMonitor(Callback):\n-    '''Callback used to stream events to a server.\n+    \"\"\"Callback used to stream events to a server.\n \n     Requires the `requests` library.\n-\n-    # Arguments\n-        root: root url to which the events will be sent (at the end\n-            of every epoch). Events are sent to\n-            `root + '/publish/epoch/end/'` by default. Calls are\n+    Events are sent to `root + '/publish/epoch/end/'` by default. Calls are\n     HTTP POST, with a `data` argument which is a\n     JSON-encoded dictionary of event data.\n-    '''\n+\n+    # Arguments\n+        root: Root url of the target server.\n+        path: Path relative to `root` to which the events will be sent.\n+        field: JSON field under which the data will be stored.\n+        headers: Optional custom HTTP headers.\n+    \"\"\"\n \n     def __init__(self,\n                  root='http://localhost:9000',\n@@ -444,19 +453,20 @@ class RemoteMonitor(Callback):\n             requests.post(self.root + self.path,\n                           {self.field: json.dumps(send)},\n                           headers=self.headers)\n-        except:\n-            print('Warning: could not reach RemoteMonitor '\n+        except requests.exceptions.RequestException:\n+            warnings.warn('Warning: could not reach RemoteMonitor '\n                           'root server at ' + str(self.root))\n \n \n class LearningRateScheduler(Callback):\n-    '''Learning rate scheduler.\n+    \"\"\"Learning rate scheduler.\n \n     # Arguments\n         schedule: a function that takes an epoch index as input\n             (integer, indexed from 0) and returns a new\n             learning rate as output (float).\n-    '''\n+    \"\"\"\n+\n     def __init__(self, schedule):\n         super(LearningRateScheduler, self).__init__()\n         self.schedule = schedule\n@@ -472,7 +482,7 @@ class LearningRateScheduler(Callback):\n \n \n class TensorBoard(Callback):\n-    ''' Tensorboard basic visualizations.\n+    \"\"\" Tensorboard basic visualizations.\n \n     This callback writes a log for TensorBoard, which allows\n     you to visualize dynamic graphs of your training and test\n@@ -498,7 +508,7 @@ class TensorBoard(Callback):\n         write_graph: whether to visualize the graph in Tensorboard.\n             The log file can become quite large when\n             write_graph is set to True.\n-    '''\n+    \"\"\"\n \n     def __init__(self, log_dir='./logs',\n                  histogram_freq=0,\n@@ -598,7 +608,7 @@ class TensorBoard(Callback):\n \n \n class ReduceLROnPlateau(Callback):\n-    '''Reduce learning rate when a metric has stopped improving.\n+    \"\"\"Reduce learning rate when a metric has stopped improving.\n \n     Models often benefit from reducing the learning rate by a factor\n     of 2-10 once learning stagnates. This callback monitors a\n@@ -631,11 +641,11 @@ class ReduceLROnPlateau(Callback):\n         cooldown: number of epochs to wait before resuming\n             normal operation after lr has been reduced.\n         min_lr: lower bound on the learning rate.\n-    '''\n+    \"\"\"\n \n     def __init__(self, monitor='val_loss', factor=0.1, patience=10,\n                  verbose=0, mode='auto', epsilon=1e-4, cooldown=0, min_lr=0):\n-        super(Callback, self).__init__()\n+        super(ReduceLROnPlateau, self).__init__()\n \n         self.monitor = monitor\n         if factor >= 1.0:\n@@ -660,7 +670,8 @@ class ReduceLROnPlateau(Callback):\n                           'fallback to auto mode.' % (self.mode),\n                           RuntimeWarning)\n             self.mode = 'auto'\n-        if self.mode == 'min' or (self.mode == 'auto' and 'acc' not in self.monitor):\n+        if (self.mode == 'min' or\n+           (self.mode == 'auto' and 'acc' not in self.monitor)):\n             self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\n             self.best = np.Inf\n         else:\n@@ -706,7 +717,7 @@ class ReduceLROnPlateau(Callback):\n \n \n class CSVLogger(Callback):\n-    '''Callback that streams epoch results to a csv file.\n+    \"\"\"Callback that streams epoch results to a csv file.\n     Supports all values that can be represented as a string,\n     including 1D iterables such as np.ndarray.\n \n@@ -721,7 +732,7 @@ class CSVLogger(Callback):\n         separator: string used to separate elements in the csv file.\n         append: True: append if file exists (useful for continuing\n             training). False: overwrite existing file,\n-    '''\n+    \"\"\"\n \n     def __init__(self, filename, separator=',', append=False):\n         self.sep = separator\n@@ -736,7 +747,7 @@ class CSVLogger(Callback):\n         if self.append:\n             if os.path.exists(self.filename):\n                 with open(self.filename) as f:\n-                    self.append_header = len(f.readline()) == 0\n+                    self.append_header = bool(len(f.readline()))\n             self.csv_file = open(self.filename, 'a')\n         else:\n             self.csv_file = open(self.filename, 'w')\n@@ -747,7 +758,7 @@ class CSVLogger(Callback):\n         def handle_value(k):\n             is_zero_dim_ndarray = isinstance(k, np.ndarray) and k.ndim == 0\n             if isinstance(k, Iterable) and not is_zero_dim_ndarray:\n-                return '\"[%s]\"' % (', '.join(map(lambda x: str(x), k)))\n+                return '\"[%s]\"' % (', '.join(map(str, k)))\n             else:\n                 return k\n \n@@ -763,7 +774,7 @@ class CSVLogger(Callback):\n         self.writer.writerow(row_dict)\n         self.csv_file.flush()\n \n-    def on_train_end(self, logs={}):\n+    def on_train_end(self, logs=None):\n         self.csv_file.close()\n \n \n@@ -773,9 +784,12 @@ class LambdaCallback(Callback):\n     This callback is constructed with anonymous functions that will be called\n     at the appropriate time. Note that the callbacks expects positional\n     arguments, as:\n-     - `on_epoch_begin` and `on_epoch_end` expect two positional arguments: `epoch`, `logs`\n-     - `on_batch_begin` and `on_batch_end` expect two positional arguments: `batch`, `logs`\n-     - `on_train_begin` and `on_train_end` expect one positional argument: `logs`\n+     - `on_epoch_begin` and `on_epoch_end` expect two positional arguments:\n+        `epoch`, `logs`\n+     - `on_batch_begin` and `on_batch_end` expect two positional arguments:\n+        `batch`, `logs`\n+     - `on_train_begin` and `on_train_end` expect one positional argument:\n+        `logs`\n \n     # Arguments\n         on_epoch_begin: called at the beginning of every epoch.\n@@ -788,18 +802,26 @@ class LambdaCallback(Callback):\n     # Example\n         ```python\n         # Print the batch number at the beginning of every batch.\n-        batch_print_callback = LambdaCallback(on_batch_begin=lambda batch, logs: print(batch))\n+        batch_print_callback = LambdaCallback(\n+            on_batch_begin=lambda batch,logs: print(batch))\n \n         # Plot the loss after every epoch.\n         import numpy as np\n         import matplotlib.pyplot as plt\n-        plot_loss_callback = LambdaCallback(on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch), logs['loss']))\n+        plot_loss_callback = LambdaCallback(\n+            on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch),\n+                                                      logs['loss']))\n \n         # Terminate some processes after having finished model training.\n         processes = ...\n-        cleanup_callback = LambdaCallback(on_train_end=lambda logs: [p.terminate() for p in processes if p.is_alive()])\n+        cleanup_callback = LambdaCallback(\n+            on_train_end=lambda logs: [\n+                p.terminate() for p in processes if p.is_alive()])\n \n-        model.fit(..., callbacks=[batch_print_callback, plot_loss_callback, cleanup_callback])\n+        model.fit(...,\n+                  callbacks=[batch_print_callback,\n+                             plot_loss_callback,\n+                             cleanup_callback])\n         ```\n     \"\"\"\n \n@@ -811,11 +833,29 @@ class LambdaCallback(Callback):\n                  on_train_begin=None,\n                  on_train_end=None,\n                  **kwargs):\n-        super(Callback, self).__init__()\n+        super(LambdaCallback, self).__init__()\n         self.__dict__.update(kwargs)\n-        self.on_epoch_begin = on_epoch_begin if on_epoch_begin else lambda epoch, logs: None\n-        self.on_epoch_end = on_epoch_end if on_epoch_end else lambda epoch, logs: None\n-        self.on_batch_begin = on_batch_begin if on_batch_begin else lambda batch, logs: None\n-        self.on_batch_end = on_batch_end if on_batch_end else lambda batch, logs: None\n-        self.on_train_begin = on_train_begin if on_train_begin else lambda logs: None\n-        self.on_train_end = on_train_end if on_train_end else lambda logs: None\n+        if on_epoch_begin is not None:\n+            self.on_epoch_begin = on_epoch_begin\n+        else:\n+            self.on_epoch_begin = lambda epoch, logs: None\n+        if on_epoch_end is not None:\n+            self.on_epoch_end = on_epoch_end\n+        else:\n+            self.on_epoch_end = lambda epoch, logs: None\n+        if on_batch_begin is not None:\n+            self.on_batch_begin = on_batch_begin\n+        else:\n+            self.on_batch_begin = lambda batch, logs: None\n+        if on_batch_end is not None:\n+            self.on_batch_end = on_batch_end\n+        else:\n+            self.on_batch_end = lambda batch, logs: None\n+        if on_train_begin is not None:\n+            self.on_train_begin = on_train_begin\n+        else:\n+            self.on_train_begin = lambda logs: None\n+        if on_train_end is not None:\n+            self.on_train_end = on_train_end\n+        else:\n+            self.on_train_end = lambda logs: None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
