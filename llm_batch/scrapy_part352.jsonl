{"custom_id": "scrapy#c327a92e971411e50e49dded772a73b293f9f0a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 911 | Contributors (this commit): 23 | Commits (past 90d): 3 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -340,8 +340,9 @@ def deferred_to_future(d: Deferred) -> Future:\n         class MySpider(Spider):\n             ...\n             async def parse(self, response):\n-                d = treq.get('https://example.com/additional')\n-                additional_response = await deferred_to_future(d)\n+                additional_request = scrapy.Request('https://example.org/price')\n+                deferred = self.crawler.engine.download(additional_request)\n+                additional_response = await deferred_to_future(deferred)\n     \"\"\"\n     return d.asFuture(_get_asyncio_event_loop())\n \n@@ -368,8 +369,9 @@ def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n         class MySpider(Spider):\n             ...\n             async def parse(self, response):\n-                d = treq.get('https://example.com/additional')\n-                extra_response = await maybe_deferred_to_future(d)\n+                additional_request = scrapy.Request('https://example.org/price')\n+                deferred = self.crawler.engine.download(additional_request)\n+                additional_response = await maybe_deferred_to_future(deferred)\n     \"\"\"\n     if not is_asyncio_reactor_installed():\n         return d\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6998e1c905ef6e5fa737b32ac0b6e7f1b7701c14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 1526 | Contributors (this commit): 42 | Commits (past 90d): 6 | Contributors (cumulative): 53 | DMM Complexity: None\n\nDIFF:\n@@ -5,10 +5,9 @@ Python Standard Library.\n This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n-import collections\n import weakref\n from collections.abc import Mapping\n-from typing import Any, Optional, Sequence, TypeVar\n+from typing import Any, Optional, OrderedDict, Sequence, TypeVar\n \n _KT = TypeVar(\"_KT\")\n _VT = TypeVar(\"_VT\")\n@@ -68,7 +67,7 @@ class CaselessDict(dict):\n         return dict.pop(self, self.normkey(key), *args)\n \n \n-class LocalCache(collections.OrderedDict[_KT, _VT]):\n+class LocalCache(OrderedDict[_KT, _VT]):\n     \"\"\"Dictionary with a finite number of keys.\n \n     Older items expires first.\n@@ -85,7 +84,7 @@ class LocalCache(collections.OrderedDict[_KT, _VT]):\n         super().__setitem__(key, value)\n \n \n-class LocalWeakReferencedCache(weakref.WeakKeyDictionary[_KT, _VT]):\n+class LocalWeakReferencedCache(weakref.WeakKeyDictionary):\n     \"\"\"\n     A weakref.WeakKeyDictionary implementation that uses LocalCache as its\n     underlying data structure, making it ordered and capable of being size-limited.\n\n@@ -232,9 +232,7 @@ def walk_callable(node: ast.AST) -> Generator[ast.AST, Any, None]:\n         yield node\n \n \n-_generator_callbacks_cache: LocalWeakReferencedCache[\n-    Callable, bool\n-] = LocalWeakReferencedCache(limit=128)\n+_generator_callbacks_cache = LocalWeakReferencedCache(limit=128)\n \n \n def is_generator_with_return_value(callable: Callable) -> bool:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84fb0edd5fc9c994931dc8931bf3d8a44a63aa57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 17 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 84 | Churn Cumulative: 196 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,26 +14,57 @@ class LogStatsExtended:\n     \"\"\"Log basic scraping stats periodically\"\"\"\n \n     def __init__(\n-            self, stats, interval=60.0, extended=False, ext_include=None, ext_exclude=None\n+            self, stats, interval=60.0,\n+            ext_stats_enabled=False,\n+            ext_stats_include=None,\n+            ext_stats_exclude=None,\n+            ext_delta_enabled=False,\n+            ext_delta_include=None,\n+            ext_delta_exclude=None,\n+            ext_timing_enabled=False,\n+            ext_timing_include=None,\n+            ext_timing_exclude=None,\n     ):\n         self.stats = stats\n         self.interval = interval\n         self.multiplier = 60.0 / self.interval\n         self.task = None\n-        self.extended = extended\n         self.encoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n-        self.ext_include = ext_include\n-        self.ext_exclude = ext_exclude\n+        self.ext_stats_enabled = ext_stats_enabled\n+        self.ext_stats_include = ext_stats_include\n+        self.ext_stats_exclude = ext_stats_exclude\n+        self.ext_delta_enabled = ext_delta_enabled\n+        self.ext_delta_include = ext_delta_include\n+        self.ext_delta_exclude = ext_delta_exclude\n+        self.ext_timing_enabled = ext_timing_enabled\n+        self.ext_timing_include = ext_timing_include\n+        self.ext_timing_exclude = ext_timing_exclude\n+\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n-        extended = crawler.settings.getbool(\"LOGSTATS_EXTENDED_ENABLED\")\n-        ext_include = crawler.settings.getlist(\"LOGSTATS_EXTENDED_INCLUDE\", [])\n-        ext_exclude = crawler.settings.getlist(\"LOGSTATS_EXTENDED_EXCLUDE\", [])\n+        ext_stats_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_STATS_ENABLED\")\n+        ext_stats_include = crawler.settings.getlist(\"LOGSTATS_EXT_STATS_INCLUDE\", [])\n+        ext_stats_exclude = crawler.settings.getlist(\"LOGSTATS_EXT_STATS_EXCLUDE\", [])\n+        ext_delta_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_DELTA_ENABLED\")\n+        ext_delta_include = crawler.settings.getlist(\"LOGSTATS_EXT_DELTA_INCLUDE\", [])\n+        ext_delta_exclude = crawler.settings.getlist(\"LOGSTATS_EXT_DELTA_EXCLUDE\", [])\n+        ext_timing_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_TIMING_ENABLED\")\n         if not interval:\n             raise NotConfigured\n-        o = cls(crawler.stats, interval, extended, ext_include, ext_exclude)\n+        if not (ext_stats_enabled or ext_delta_enabled or ext_timing_enabled):\n+            raise NotConfigured\n+        o = cls(crawler.stats,\n+                interval,\n+                ext_stats_enabled,\n+                ext_stats_include,\n+                ext_stats_exclude,\n+                ext_delta_enabled,\n+                ext_delta_include,\n+                ext_delta_exclude,\n+                ext_timing_enabled,\n+               )\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n         crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n@@ -48,17 +79,19 @@ class LogStatsExtended:\n \n     def log(self, spider):\n         data = {}\n+        if self.ext_timing_enabled:\n             data.update(self.log_timing())\n+        if self.ext_delta_enabled:\n             data.update(self.log_delta())\n+        if self.ext_stats_enabled:\n             data.update(self.log_crawler_stats())\n         logger.info(self.encoder.encode(data))\n \n-\n     def log_delta(self):\n         num_stats = {\n             k: v\n             for k, v in self.stats._stats.items()\n-            if isinstance(v, (int, float)) and self.delta_param_allowed(k)\n+            if isinstance(v, (int, float)) and self.param_allowed(k,self.ext_delta_include,self.ext_delta_exclude)\n         }\n         delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}\n         self.delta_prev = num_stats\n@@ -76,37 +109,24 @@ class LogStatsExtended:\n         self.time_prev = now\n         return {\"time\": time}\n \n-    def log_time(self):\n-        num_stats = {\n+    def log_crawler_stats(self):\n+        stats = {\n             k: v\n             for k, v in self.stats._stats.items()\n-            if isinstance(v, (int, float)) and self.delta_param_allowed(k)\n-        }\n-        delta = {k: v - self.stats_prev.get(k, 0) for k, v in num_stats.items()}\n-        self.stats_prev = num_stats\n-        return {\"delta\": delta}\n+            if self.param_allowed(k,self.ext_stats_include, self.ext_stats_exclude)}\n+        return {\"stats\": stats}\n \n-    def log_crawler_stats(self):\n-        return {\"stats\": self.stats.get_stats()}\n-\n-    def delta_param_allowed(self, stat_name):\n-        for p in self.ext_exclude:\n+    def param_allowed(self, stat_name, include, exclude):\n+        for p in exclude:\n             if p in stat_name:\n                 return False\n-        for p in self.ext_include:\n+        for p in include:\n             if p in stat_name:\n                 return True\n-        if self.ext_include:\n         return False\n-        else:\n-            return True\n+\n \n     def spider_closed(self, spider, reason):\n-        data = {}\n-        data.update(self.log_timing())\n-        data.update(self.log_delta())\n-        data.update(self.log_crawler_stats())\n-        logger.info(self.encoder.encode(data))\n-\n+        self.log(spider)\n         if self.task and self.task.running:\n             self.task.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2f238d927329ac308edcf09fde4e85dd9f61e44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 48 | Churn Cumulative: 244 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,54 +15,52 @@ class LogStatsExtended:\n \n     def __init__(\n             self, stats, interval=60.0,\n-            ext_stats_enabled=False,\n-            ext_stats_include=None,\n-            ext_stats_exclude=None,\n-            ext_delta_enabled=False,\n-            ext_delta_include=None,\n-            ext_delta_exclude=None,\n+            ext_stats={},\n+            ext_delta={},\n             ext_timing_enabled=False,\n-            ext_timing_include=None,\n-            ext_timing_exclude=None,\n     ):\n         self.stats = stats\n         self.interval = interval\n         self.multiplier = 60.0 / self.interval\n         self.task = None\n         self.encoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n-        self.ext_stats_enabled = ext_stats_enabled\n-        self.ext_stats_include = ext_stats_include\n-        self.ext_stats_exclude = ext_stats_exclude\n-        self.ext_delta_enabled = ext_delta_enabled\n-        self.ext_delta_include = ext_delta_include\n-        self.ext_delta_exclude = ext_delta_exclude\n+        self.ext_stats_enabled = bool(ext_stats)\n+        self.ext_stats_include = ext_stats.get(\"include\", [])\n+        self.ext_stats_exclude = ext_stats.get(\"exclude\", [])\n+        self.ext_delta_enabled = bool(ext_delta)\n+        self.ext_delta_include = ext_delta.get(\"include\", [])\n+        self.ext_delta_exclude = ext_delta.get(\"exclude\", [])\n         self.ext_timing_enabled = ext_timing_enabled\n-        self.ext_timing_include = ext_timing_include\n-        self.ext_timing_exclude = ext_timing_exclude\n \n \n     @classmethod\n     def from_crawler(cls, crawler):\n         interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n+        try:\n+            ext_stats = crawler.settings.getdict(\"PERIODIC_LOG_STATS\")\n+        except:\n+            ext_stats = {\"enabled\": True} if crawler.settings.getbool(\"PERIODIC_LOG_STATS\") else None\n+\n+        try:\n+            ext_delta = crawler.settings.getdict(\"PERIODIC_LOG_DELTA\")\n+        except:\n+            ext_delta = {\"enabled\": True} if crawler.settings.getdict(\"PERIODIC_LOG_DELTA\") else None\n+        ext_timing_enabled = crawler.settings.getbool(\"PERIODIC_LOG_TIMING_ENABLED\", False)\n         ext_stats_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_STATS_ENABLED\")\n         ext_stats_include = crawler.settings.getlist(\"LOGSTATS_EXT_STATS_INCLUDE\", [])\n         ext_stats_exclude = crawler.settings.getlist(\"LOGSTATS_EXT_STATS_EXCLUDE\", [])\n         ext_delta_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_DELTA_ENABLED\")\n         ext_delta_include = crawler.settings.getlist(\"LOGSTATS_EXT_DELTA_INCLUDE\", [])\n         ext_delta_exclude = crawler.settings.getlist(\"LOGSTATS_EXT_DELTA_EXCLUDE\", [])\n-        ext_timing_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_TIMING_ENABLED\")\n+        #ext_timing_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_TIMING_ENABLED\")\n         if not interval:\n             raise NotConfigured\n-        if not (ext_stats_enabled or ext_delta_enabled or ext_timing_enabled):\n+        if not (ext_stats or ext_delta or ext_timing_enabled):\n             raise NotConfigured\n         o = cls(crawler.stats,\n                 interval,\n-                ext_stats_enabled,\n-                ext_stats_include,\n-                ext_stats_exclude,\n-                ext_delta_enabled,\n-                ext_delta_include,\n-                ext_delta_exclude,\n+                ext_stats,\n+                ext_delta,\n                 ext_timing_enabled,\n                )\n         crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n@@ -117,6 +115,8 @@ class LogStatsExtended:\n         return {\"stats\": stats}\n \n     def param_allowed(self, stat_name, include, exclude):\n+        if not include and not exclude:\n+            return True\n         for p in exclude:\n             if p in stat_name:\n                 return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db794d351c6218c8b849682bb1aa0048788e7f1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 257 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,13 +46,6 @@ class LogStatsExtended:\n         except:\n             ext_delta = {\"enabled\": True} if crawler.settings.getdict(\"PERIODIC_LOG_DELTA\") else None\n         ext_timing_enabled = crawler.settings.getbool(\"PERIODIC_LOG_TIMING_ENABLED\", False)\n-        ext_stats_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_STATS_ENABLED\")\n-        ext_stats_include = crawler.settings.getlist(\"LOGSTATS_EXT_STATS_INCLUDE\", [])\n-        ext_stats_exclude = crawler.settings.getlist(\"LOGSTATS_EXT_STATS_EXCLUDE\", [])\n-        ext_delta_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_DELTA_ENABLED\")\n-        ext_delta_include = crawler.settings.getlist(\"LOGSTATS_EXT_DELTA_INCLUDE\", [])\n-        ext_delta_exclude = crawler.settings.getlist(\"LOGSTATS_EXT_DELTA_EXCLUDE\", [])\n-        #ext_timing_enabled = crawler.settings.getbool(\"LOGSTATS_EXT_TIMING_ENABLED\")\n         if not interval:\n             raise NotConfigured\n         if not (ext_stats or ext_delta or ext_timing_enabled):\n@@ -72,10 +65,10 @@ class LogStatsExtended:\n         self.delta_prev = {}\n         self.stats_prev = {}\n \n-        self.task = task.LoopingCall(self.log, spider)\n+        self.task = task.LoopingCall(self.log)\n         self.task.start(self.interval)\n \n-    def log(self, spider):\n+    def log(self):\n         data = {}\n         if self.ext_timing_enabled:\n             data.update(self.log_timing())\n@@ -127,6 +120,6 @@ class LogStatsExtended:\n \n \n     def spider_closed(self, spider, reason):\n-        self.log(spider)\n+        self.log()\n         if self.task and self.task.running:\n             self.task.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c91f1bb43c00f9d64d16ecc1a5a976e14895ea5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 852 | Contributors (this commit): 61 | Commits (past 90d): 2 | Contributors (cumulative): 61 | DMM Complexity: None\n\nDIFF:\n@@ -240,6 +240,10 @@ METAREFRESH_MAXDELAY = 100\n \n NEWSPIDER_MODULE = \"\"\n \n+PERIODIC_LOG_DELTA = None\n+PERIODIC_LOG_STATS = None\n+PERIODIC_LOG_TIMING_ENABLED = None\n+\n RANDOMIZE_DOWNLOAD_DELAY = True\n \n REACTOR_THREADPOOL_MAXSIZE = 10\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0c84903b703a9405f2807873837ebf274873a60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 35 | Churn Cumulative: 292 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.26666666666666666\n\nDIFF:\n@@ -14,7 +14,9 @@ class LogStatsExtended:\n     \"\"\"Log basic scraping stats periodically\"\"\"\n \n     def __init__(\n-            self, stats, interval=60.0,\n+        self,\n+        stats,\n+        interval=60.0,\n         ext_stats={},\n         ext_delta={},\n         ext_timing_enabled=False,\n@@ -32,25 +34,35 @@ class LogStatsExtended:\n         self.ext_delta_exclude = ext_delta.get(\"exclude\", [])\n         self.ext_timing_enabled = ext_timing_enabled\n \n-\n     @classmethod\n     def from_crawler(cls, crawler):\n         interval = crawler.settings.getfloat(\"LOGSTATS_INTERVAL\")\n         try:\n             ext_stats = crawler.settings.getdict(\"PERIODIC_LOG_STATS\")\n-        except:\n-            ext_stats = {\"enabled\": True} if crawler.settings.getbool(\"PERIODIC_LOG_STATS\") else None\n+        except ValueError:\n+            ext_stats = (\n+                {\"enabled\": True}\n+                if crawler.settings.getbool(\"PERIODIC_LOG_STATS\")\n+                else None\n+            )\n \n         try:\n             ext_delta = crawler.settings.getdict(\"PERIODIC_LOG_DELTA\")\n-        except:\n-            ext_delta = {\"enabled\": True} if crawler.settings.getdict(\"PERIODIC_LOG_DELTA\") else None\n-        ext_timing_enabled = crawler.settings.getbool(\"PERIODIC_LOG_TIMING_ENABLED\", False)\n+        except ValueError:\n+            ext_delta = (\n+                {\"enabled\": True}\n+                if crawler.settings.getdict(\"PERIODIC_LOG_DELTA\")\n+                else None\n+            )\n+        ext_timing_enabled = crawler.settings.getbool(\n+            \"PERIODIC_LOG_TIMING_ENABLED\", False\n+        )\n         if not interval:\n             raise NotConfigured\n         if not (ext_stats or ext_delta or ext_timing_enabled):\n             raise NotConfigured\n-        o = cls(crawler.stats,\n+        o = cls(\n+            crawler.stats,\n             interval,\n             ext_stats,\n             ext_delta,\n@@ -82,7 +94,8 @@ class LogStatsExtended:\n         num_stats = {\n             k: v\n             for k, v in self.stats._stats.items()\n-            if isinstance(v, (int, float)) and self.param_allowed(k,self.ext_delta_include,self.ext_delta_exclude)\n+            if isinstance(v, (int, float))\n+            and self.param_allowed(k, self.ext_delta_include, self.ext_delta_exclude)\n         }\n         delta = {k: v - self.delta_prev.get(k, 0) for k, v in num_stats.items()}\n         self.delta_prev = num_stats\n@@ -104,7 +117,8 @@ class LogStatsExtended:\n         stats = {\n             k: v\n             for k, v in self.stats._stats.items()\n-            if self.param_allowed(k,self.ext_stats_include, self.ext_stats_exclude)}\n+            if self.param_allowed(k, self.ext_stats_include, self.ext_stats_exclude)\n+        }\n         return {\"stats\": stats}\n \n     def param_allowed(self, stat_name, include, exclude):\n@@ -118,7 +132,6 @@ class LogStatsExtended:\n                 return True\n         return False\n \n-\n     def spider_closed(self, spider, reason):\n         self.log()\n         if self.task and self.task.running:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b60e0faf22eaee9cb26357c4c6637fba7e20f125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 34/34 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.utils.serialize import ScrapyJSONEncoder\n logger = logging.getLogger(__name__)\n \n \n-class LogStatsExtended:\n+class PeriodicLog:\n     \"\"\"Log basic scraping stats periodically\"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#49839d6071832aab23093c34fa6ceb961fcdf9d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 33 | Churn Cumulative: 3013 | Contributors (this commit): 26 | Commits (past 90d): 3 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,13 @@\n import argparse\n import json\n+import os\n import random\n import sys\n from pathlib import Path\n from shutil import rmtree\n from subprocess import PIPE, Popen\n from tempfile import mkdtemp\n+from typing import Dict\n from urllib.parse import urlencode\n \n from OpenSSL import SSL\n@@ -20,7 +22,6 @@ from twisted.web.static import File\n from twisted.web.util import redirectTo\n \n from scrapy.utils.python import to_bytes, to_unicode\n-from scrapy.utils.test import get_testenv\n \n \n def getarg(request, name, default=None, type=None):\n@@ -32,6 +33,16 @@ def getarg(request, name, default=None, type=None):\n     return default\n \n \n+def get_mockserver_env() -> Dict[str, str]:\n+    \"\"\"Return a OS environment dict suitable to run mockserver processes.\"\"\"\n+\n+    tests_path = Path(__file__).parent.parent\n+    pythonpath = str(tests_path) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n+    env = os.environ.copy()\n+    env[\"PYTHONPATH\"] = pythonpath\n+    return env\n+\n+\n # most of the following resources are copied from twisted.web.test.test_webclient\n class ForeverTakingResource(resource.Resource):\n     \"\"\"\n@@ -264,7 +275,7 @@ class MockServer:\n         self.proc = Popen(\n             [sys.executable, \"-u\", \"-m\", \"tests.mockserver\", \"-t\", \"http\"],\n             stdout=PIPE,\n-            env=get_testenv(),\n+            env=get_mockserver_env(),\n         )\n         http_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n         https_address = self.proc.stdout.readline().strip().decode(\"ascii\")\n@@ -308,7 +319,7 @@ class MockDNSServer:\n         self.proc = Popen(\n             [sys.executable, \"-u\", \"-m\", \"tests.mockserver\", \"-t\", \"dns\"],\n             stdout=PIPE,\n-            env=get_testenv(),\n+            env=get_mockserver_env(),\n         )\n         self.host = \"127.0.0.1\"\n         self.port = int(\n@@ -331,7 +342,7 @@ class MockFTPServer:\n         self.proc = Popen(\n             [sys.executable, \"-u\", \"-m\", \"tests.ftpserver\", \"-d\", str(self.path)],\n             stderr=PIPE,\n-            env=get_testenv(),\n+            env=get_mockserver_env(),\n         )\n         for line in self.proc.stderr:\n             if b\"starting FTP server\" in line:\n\n@@ -1,4 +1,5 @@\n import logging\n+import os\n import platform\n import subprocess\n import sys\n@@ -23,8 +24,8 @@ from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.misc import load_object\n from scrapy.utils.spider import DefaultSpider\n-from scrapy.utils.test import get_crawler, get_testenv\n-from tests.mockserver import MockServer\n+from scrapy.utils.test import get_crawler\n+from tests.mockserver import MockServer, get_mockserver_env\n \n \n class BaseCrawlerTest(unittest.TestCase):\n@@ -289,12 +290,16 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n class ScriptRunnerMixin:\n     script_dir: Path\n+    cwd = os.getcwd()\n \n     def run_script(self, script_name: str, *script_args):\n         script_path = self.script_dir / script_name\n         args = [sys.executable, str(script_path)] + list(script_args)\n         p = subprocess.Popen(\n-            args, env=get_testenv(), stdout=subprocess.PIPE, stderr=subprocess.PIPE\n+            args,\n+            env=get_mockserver_env(),\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n         )\n         stdout, stderr = p.communicate()\n         return stderr.decode(\"utf-8\")\n\n@@ -21,8 +21,6 @@ class MitmProxy:\n     auth_pass = \"scrapy\"\n \n     def start(self):\n-        from scrapy.utils.test import get_testenv\n-\n         script = \"\"\"\n import sys\n from mitmproxy.tools.main import mitmdump\n@@ -46,7 +44,6 @@ sys.exit(mitmdump())\n                 \"--ssl-insecure\",\n             ],\n             stdout=PIPE,\n-            env=get_testenv(),\n         )\n         line = self.proc.stdout.readline().decode(\"utf-8\")\n         host_port = re.search(r\"listening at http://([^:]+:\\d+)\", line).group(1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#493ea435384d4b5891129cbcf61d07b09a00ce7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 166 | Contributors (this commit): 15 | Commits (past 90d): 1 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import json\n+import os\n import pstats\n import shutil\n import sys\n@@ -14,6 +15,8 @@ from scrapy.utils.test import get_testenv\n class CmdlineTest(unittest.TestCase):\n     def setUp(self):\n         self.env = get_testenv()\n+        tests_path = Path(__file__).parent.parent\n+        self.env[\"PYTHONPATH\"] += os.pathsep + str(tests_path.parent)\n         self.env[\"SCRAPY_SETTINGS_MODULE\"] = \"tests.test_cmdline.settings\"\n \n     def _execute(self, *new_args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f92882be4b35cf476f0c7c284e4fe1ba498e873", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 1741 | Contributors (this commit): 40 | Commits (past 90d): 6 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -492,20 +492,6 @@ class FeedExporter:\n         :param uri_template: template of uri which contains %(batch_time)s or %(batch_id)d to create new uri\n         \"\"\"\n         storage = self._get_storage(uri, feed_options)\n-        file = storage.open(spider)\n-        if \"postprocessing\" in feed_options:\n-            file = PostProcessingManager(\n-                feed_options[\"postprocessing\"], file, feed_options\n-            )\n-\n-        exporter = self._get_exporter(\n-            file=file,\n-            format=feed_options[\"format\"],\n-            fields_to_export=feed_options[\"fields\"],\n-            encoding=feed_options[\"encoding\"],\n-            indent=feed_options[\"indent\"],\n-            **feed_options[\"item_export_kwargs\"],\n-        )\n         slot = FeedSlot(\n             storage=storage,\n             uri=uri,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#075ad6f196e7936edfd75b57d10b7313da7fe4f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 313 | Lines Deleted: 290 | Files Changed: 9 | Hunks: 151 | Methods Changed: 50 | Complexity Δ (Sum/Max): 7/3 | Churn Δ: 603 | Churn Cumulative: 4685 | Contributors (this commit): 44 | Commits (past 90d): 13 | Contributors (cumulative): 81 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,10 +1,10 @@\n-from collections import defaultdict, Mapping, OrderedDict\n-from inspect import isclass\n-import six\n import warnings\n+from collections import OrderedDict, defaultdict\n+from collections.abc import Mapping\n+from inspect import isclass\n \n-from pkg_resources import WorkingSet, Distribution, Requirement\n import zope.interface\n+from pkg_resources import Distribution, Requirement, WorkingSet\n from zope.interface.verify import verifyObject\n \n from scrapy.interfaces import IAddon\n@@ -14,7 +14,6 @@ from scrapy.utils.misc import load_module_or_object\n \n @zope.interface.implementer(IAddon)\n class Addon(object):\n-\n     basic_settings = None\n     \"\"\"``dict`` of settings that will be exported via :meth:`export_basics`.\"\"\"\n \n@@ -83,8 +82,8 @@ class Addon(object):\n             else:\n                 # e.g. for DOWNLOADER_MIDDLEWARES: {'myclass': 100}\n                 k = comp\n-                v = config.get('order', self.component_order)\n-            settings[self.component_type].update({k: v}, 'addon')\n+                v = config.get(\"order\", self.component_order)\n+            settings[self.component_type].update({k: v}, \"addon\")\n \n     def export_basics(self, settings):\n         \"\"\"Export the :attr:`basic_settings` attribute into the settings object.\n@@ -95,8 +94,8 @@ class Addon(object):\n         :param settings: Settings object into which to expose the basic settings\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n-        for setting, value in six.iteritems(self.basic_settings or {}):\n-            settings.set(setting, value, 'addon')\n+        for setting, value in (self.basic_settings or {}).items():\n+            settings.set(setting, value, \"addon\")\n \n     def export_config(self, config, settings):\n         \"\"\"Export the add-on configuration, all keys in caps and with\n@@ -121,14 +120,13 @@ class Addon(object):\n         prefix = self.settings_prefix or self.name\n         # Since default exported config is case-insensitive (everything will be\n         # uppercased), make mapped config case-insensitive as well\n-        conf_mapping = {k.lower(): v\n-                        for k, v in six.iteritems(self.config_mapping or {})}\n-        for key, val in six.iteritems(conf):\n+        conf_mapping = {k.lower(): v for k, v in (self.config_mapping or {}).items()}\n+        for key, val in conf.items():\n             if key.lower() in conf_mapping:\n                 key = conf_mapping[key.lower()]\n             else:\n-                key = (prefix + '_' + key).upper()\n-            settings.set(key, val, 'addon')\n+                key = (prefix + \"_\" + key).upper()\n+            settings.set(key, val, \"addon\")\n \n     def update_settings(self, config, settings):\n         \"\"\"Export both the basic settings and the add-on configuration. I.e.,\n@@ -210,11 +208,11 @@ class AddonManager(Mapping):\n         verifyObject(IAddon, addon)\n         name = addon.name\n         if name in self:\n-            raise ValueError(\"Addon '{}' already loaded\".format(name))\n+            raise ValueError(f\"Addon '{name}' already loaded\")\n         self._addons[name] = addon\n         self.configs[name] = config or {}\n         if name in self._disable_on_add:\n-            self.configs[name]['_enabled'] = False\n+            self.configs[name][\"_enabled\"] = False\n             self._disable_on_add.remove(name)\n \n     def remove(self, addon):\n@@ -229,7 +227,7 @@ class AddonManager(Mapping):\n         \"\"\"\n         if addon in self:\n             del self[addon]\n-        elif hasattr(addon, 'name') and addon.name in self:\n+        elif hasattr(addon, \"name\") and addon.name in self:\n             del self[addon.name]\n         else:\n             try:\n@@ -250,14 +248,14 @@ class AddonManager(Mapping):\n         :param path: Python or file path to an add-on\n         :type path: ``str``\n         \"\"\"\n-        if isinstance(path, six.string_types):\n+        if isinstance(path, str):\n             try:\n                 obj = load_module_or_object(path)\n             except NameError:\n-                raise NameError(\"Could not find add-on '%s'\" % path)\n+                raise NameError(f\"Could not find add-on '{path}'\")\n         else:\n             obj = path\n-        if hasattr(obj, '_addon'):\n+        if hasattr(obj, \"_addon\"):\n             obj = AddonManager.get_addon(obj._addon)\n         return obj\n \n@@ -284,7 +282,7 @@ class AddonManager(Mapping):\n             and values correspond to their configuration\n         :type addonsdict: ``dict``\n         \"\"\"\n-        for addonpath, addoncfg in six.iteritems(addonsdict):\n+        for addonpath, addoncfg in addonsdict.items():\n             self.add(addonpath, addoncfg)\n \n     def load_settings(self, settings):\n@@ -299,7 +297,7 @@ class AddonManager(Mapping):\n             which to read the add-on configuration\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n-        paths = build_component_list(settings['ADDONS'])\n+        paths = build_component_list(settings[\"ADDONS\"])\n         addons = [self.get_addon(path) for path in paths]\n         configs = [settings.getdict(addon.name.upper()) for addon in addons]\n         for a, c in zip(addons, configs):\n@@ -322,20 +320,23 @@ class AddonManager(Mapping):\n         add-on.\n         \"\"\"\n         # Collect all active add-ons and the components they provide\n-        ws = WorkingSet('')\n+        ws = WorkingSet(\"\")\n \n         def add_dist(project_name, version, **kwargs):\n-            if project_name in ws.entry_keys.get('scrapy', []):\n-                raise ImportError(\"Component {} provided by multiple add-ons\"\n-                                  \"\".format(project_name))\n+            if project_name in ws.entry_keys.get(\"scrapy\", []):\n+                raise ImportError(\n+                    f\"Component {project_name} provided by multiple add-ons\"\n+                )\n             else:\n-                dist = Distribution(project_name=project_name, version=version,\n-                                    **kwargs)\n-                ws.add(dist, entry='scrapy')\n+                dist = Distribution(\n+                    project_name=project_name, version=version, **kwargs\n+                )\n+                ws.add(dist, entry=\"scrapy\")\n+\n         for name in self:\n             ver = self[name].version\n             add_dist(name, ver)\n-            for provides_name in getattr(self[name], 'provides', []):\n+            for provides_name in getattr(self[name], \"provides\", []):\n                 add_dist(provides_name, ver)\n \n         # Collect all required and modified components\n@@ -345,8 +346,9 @@ class AddonManager(Mapping):\n                 for entry in getattr(self[name], attribute_name, []):\n                     attrs[entry].append(name)\n             return attrs\n-        modified = compile_attribute_dict('modifies')\n-        required = compile_attribute_dict('requires')\n+\n+        modified = compile_attribute_dict(\"modifies\")\n+        required = compile_attribute_dict(\"requires\")\n \n         req_or_mod = set(required.keys()).union(modified.keys())\n         for reqstr in req_or_mod:\n@@ -355,15 +357,16 @@ class AddonManager(Mapping):\n             # our own exception or is it helpful enough?\n             if ws.find(req) is None:\n                 raise ImportError(\n-                    \"Add-ons {} require or modify missing component {}\"\n-                    \"\".format(required[reqstr]+modified[reqstr], reqstr)\n+                    f\"Add-ons {required[reqstr] + modified[reqstr]} require\"\n+                    f\" or modify missing component {reqstr}\"\n                 )\n \n         mod_and_req = set(required.keys()).intersection(modified.keys())\n         for conflict in mod_and_req:\n-            warnings.warn(\"Component '{}', required by add-ons {}, is modified \"\n-                          \"by add-ons {}\".format(conflict, required[conflict],\n-                                                 modified[conflict]))\n+            warnings.warn(\n+                f\"Component '{conflict}', required by add-ons {required[conflict]},\"\n+                f\" is modified by add-ons {modified[conflict]}\"\n+            )\n \n     def disable(self, addon):\n         \"\"\"Disable an add-on, i.e. prevent its callbacks from being called.\n@@ -375,7 +378,7 @@ class AddonManager(Mapping):\n         :type addon: ``str``\n         \"\"\"\n         if addon in self:\n-            self.configs[addon]['_enabled'] = False\n+            self.configs[addon][\"_enabled\"] = False\n         else:\n             self._disable_on_add.append(addon)\n \n@@ -389,23 +392,23 @@ class AddonManager(Mapping):\n         :type addon: ``str``\n         \"\"\"\n         if addon in self:\n-            self.configs[addon]['_enabled'] = True\n+            self.configs[addon][\"_enabled\"] = True\n         elif addon in self._disable_on_add:\n             self._disable_on_add.remove(addon)\n         else:\n-            raise ValueError(\"Add-ons need to be added before they can be \"\n-                             \"enabled\")\n+            raise ValueError(\"Add-ons need to be added before they can be \" \"enabled\")\n \n     @property\n     def disabled(self):\n         \"\"\"Names of disabled add-ons\"\"\"\n-        return ([a for a in self if not self.configs[a].get('_enabled', True)] +\n-                self._disable_on_add)\n+        return [\n+            a for a in self if not self.configs[a].get(\"_enabled\", True)\n+        ] + self._disable_on_add\n \n     @property\n     def enabled(self):\n         \"\"\"Names of enabled add-ons\"\"\"\n-        return [a for a in self if self.configs[a].get('_enabled', True)]\n+        return [a for a in self if self.configs[a].get(\"_enabled\", True)]\n \n     def _call_if_exists(self, obj, cbname, *args, **kwargs):\n         if obj is None:\n@@ -418,9 +421,10 @@ class AddonManager(Mapping):\n             cb(*args, **kwargs)\n \n     def _call_addon(self, addonname, cbname, *args, **kwargs):\n-        if self.configs[addonname].get('_enabled', True):\n-            self._call_if_exists(self[addonname], cbname,\n-                                 self.configs[addonname], *args, **kwargs)\n+        if self.configs[addonname].get(\"_enabled\", True):\n+            self._call_if_exists(\n+                self[addonname], cbname, self.configs[addonname], *args, **kwargs\n+            )\n \n     def update_addons(self):\n         \"\"\"Call ``update_addons()`` of all held add-ons.\n@@ -432,7 +436,7 @@ class AddonManager(Mapping):\n         while called_addons != set(self):\n             for name in set(self).difference(called_addons):\n                 called_addons.add(name)\n-                self._call_addon(name, 'update_addons', self)\n+                self._call_addon(name, \"update_addons\", self)\n \n     def update_settings(self, settings):\n         \"\"\"Call ``update_settings()`` of all held add-ons.\n@@ -442,7 +446,7 @@ class AddonManager(Mapping):\n         :type settings: :class:`~scrapy.settings.Settings`\n         \"\"\"\n         for name in self:\n-            self._call_addon(name, 'update_settings', settings)\n+            self._call_addon(name, \"update_settings\", settings)\n \n     def check_configuration(self, crawler):\n         \"\"\"Call ``check_configuration()`` of all held add-ons.\n@@ -451,7 +455,7 @@ class AddonManager(Mapping):\n         :type crawler: :class:`~scrapy.crawler.Crawler`\n         \"\"\"\n         for name in self:\n-            self._call_addon(name, 'check_configuration', crawler)\n+            self._call_addon(name, \"check_configuration\", crawler)\n \n \n-from scrapy.addons.builtins import *\n+from scrapy.addons.builtins import *  # noqa\n\n@@ -1,23 +1,44 @@\n import scrapy\n from scrapy.addons import Addon\n \n-__all__ = ['make_builtin_addon',\n-\n-           'depth', 'httperror', 'offsite', 'referer', 'urllength',\n-\n-           'ajaxcrawl', 'chunked', 'cookies', 'defaultheaders',\n-           'downloadtimeout', 'httpauth', 'httpcache', 'httpcompression',\n-           'httpproxy', 'metarefresh', 'redirect', 'retry', 'robotstxt',\n-           'stats', 'useragent',\n-\n-           'autothrottle', 'corestats', 'closespider', 'debugger', 'feedexport',\n-           'logstats', 'memdebug', 'memusage', 'spiderstate', 'stacktracedump',\n-           'statsmailer', 'telnetconsole',\n+__all__ = [\n+    \"make_builtin_addon\",\n+    \"depth\",\n+    \"httperror\",\n+    \"offsite\",\n+    \"referer\",\n+    \"urllength\",\n+    \"ajaxcrawl\",\n+    \"chunked\",\n+    \"cookies\",\n+    \"defaultheaders\",\n+    \"downloadtimeout\",\n+    \"httpauth\",\n+    \"httpcache\",\n+    \"httpcompression\",\n+    \"httpproxy\",\n+    \"metarefresh\",\n+    \"redirect\",\n+    \"retry\",\n+    \"robotstxt\",\n+    \"stats\",\n+    \"useragent\",\n+    \"autothrottle\",\n+    \"corestats\",\n+    \"closespider\",\n+    \"debugger\",\n+    \"feedexport\",\n+    \"logstats\",\n+    \"memdebug\",\n+    \"memusage\",\n+    \"spiderstate\",\n+    \"stacktracedump\",\n+    \"statsmailer\",\n+    \"telnetconsole\",\n ]\n \n \n-def make_builtin_addon(addon_name, addon_default_config=None,\n-                       addon_version=None):\n+def make_builtin_addon(addon_name, addon_default_config=None, addon_version=None):\n     class ThisAddon(Addon):\n         name = addon_name\n         version = addon_version or scrapy.__version__\n@@ -33,59 +54,65 @@ def make_builtin_addon(addon_name, addon_default_config=None,\n \n # SPIDER MIDDLEWARES\n \n-depth = make_builtin_addon('depth')\n+depth = make_builtin_addon(\"depth\")\n \n-httperror = make_builtin_addon('httperror')\n+httperror = make_builtin_addon(\"httperror\")\n \n-offsite = make_builtin_addon('offsite')\n+offsite = make_builtin_addon(\"offsite\")\n \n-referer = make_builtin_addon('referer')\n+referer = make_builtin_addon(\"referer\")\n \n-urllength = make_builtin_addon('urllength')\n+urllength = make_builtin_addon(\"urllength\")\n \n \n # DOWNLOADER MIDDLEWARES\n \n-ajaxcrawl = make_builtin_addon('ajaxcrawl', {'enabled': True})\n+ajaxcrawl = make_builtin_addon(\"ajaxcrawl\", {\"enabled\": True})\n \n-chunked = make_builtin_addon('chunked')\n+chunked = make_builtin_addon(\"chunked\")\n+\n+cookies = make_builtin_addon(\"cookies\")\n+\n+defaultheaders = make_builtin_addon(\"defaultheaders\")\n \n-cookies = make_builtin_addon('cookies')\n \n-defaultheaders = make_builtin_addon('defaultheaders')\n # Assume every config entry is a header\n def defaultheaders_export_config(self, config, settings):\n     conf = self.default_config or {}\n     conf.update(config)\n-    settings.set('DEFAULT_REQUEST_HEADERS', conf, 'addon')\n+    settings.set(\"DEFAULT_REQUEST_HEADERS\", conf, \"addon\")\n+\n+\n defaultheaders.export_config = defaultheaders_export_config\n \n-downloadtimeout = make_builtin_addon('downloadtimeout')\n-downloadtimeout.config_mapping = {'timeout': 'DOWNLOAD_TIMEOUT',\n-                                  'download_timeout': 'DOWNLOAD_TIMEOUT'}\n+downloadtimeout = make_builtin_addon(\"downloadtimeout\")\n+downloadtimeout.config_mapping = {\n+    \"timeout\": \"DOWNLOAD_TIMEOUT\",\n+    \"download_timeout\": \"DOWNLOAD_TIMEOUT\",\n+}\n \n-httpauth = make_builtin_addon('httpauth')\n+httpauth = make_builtin_addon(\"httpauth\")\n \n-httpcache = make_builtin_addon('httpcache', {'enabled': True})\n+httpcache = make_builtin_addon(\"httpcache\", {\"enabled\": True})\n \n-httpcompression = make_builtin_addon('httpcompression')\n-httpcompression.config_mapping = {'enabled': 'COMPRESSION_ENABLED'}\n+httpcompression = make_builtin_addon(\"httpcompression\")\n+httpcompression.config_mapping = {\"enabled\": \"COMPRESSION_ENABLED\"}\n \n-httpproxy = make_builtin_addon('httpproxy')\n+httpproxy = make_builtin_addon(\"httpproxy\")\n \n-metarefresh = make_builtin_addon('metarefresh')\n-metarefresh.config_mapping = {'max_times': 'REDIRECT_MAX_TIMES'}\n+metarefresh = make_builtin_addon(\"metarefresh\")\n+metarefresh.config_mapping = {\"max_times\": \"REDIRECT_MAX_TIMES\"}\n \n-redirect = make_builtin_addon('redirect')\n+redirect = make_builtin_addon(\"redirect\")\n \n-retry = make_builtin_addon('retry')\n+retry = make_builtin_addon(\"retry\")\n \n-robotstxt = make_builtin_addon('robotstxt', {'obey': True})\n+robotstxt = make_builtin_addon(\"robotstxt\", {\"obey\": True})\n \n-stats = make_builtin_addon('stats')\n+stats = make_builtin_addon(\"stats\")\n \n-useragent = make_builtin_addon('useragent')\n-useragent.config_mapping = {'user_agent': 'USER_AGENT'}\n+useragent = make_builtin_addon(\"useragent\")\n+useragent.config_mapping = {\"user_agent\": \"USER_AGENT\"}\n \n \n # ITEM PIPELINES\n@@ -93,27 +120,27 @@ useragent.config_mapping = {'user_agent': 'USER_AGENT'}\n \n # EXTENSIONS\n \n-autothrottle = make_builtin_addon('autothrottle', {'enabled': True})\n+autothrottle = make_builtin_addon(\"autothrottle\", {\"enabled\": True})\n \n-corestats = make_builtin_addon('corestats')\n+corestats = make_builtin_addon(\"corestats\")\n \n-closespider = make_builtin_addon('closespider')\n+closespider = make_builtin_addon(\"closespider\")\n \n-debugger = make_builtin_addon('debugger')\n+debugger = make_builtin_addon(\"debugger\")\n \n-feedexport = make_builtin_addon('feedexport')\n-feedexport.settings_prefix = 'FEED'\n+feedexport = make_builtin_addon(\"feedexport\")\n+feedexport.settings_prefix = \"FEED\"\n \n-logstats = make_builtin_addon('logstats')\n+logstats = make_builtin_addon(\"logstats\")\n \n-memdebug = make_builtin_addon('memdebug', {'enabled': True})\n+memdebug = make_builtin_addon(\"memdebug\", {\"enabled\": True})\n \n-memusage = make_builtin_addon('memusage', {'enabled': True})\n+memusage = make_builtin_addon(\"memusage\", {\"enabled\": True})\n \n-spiderstate = make_builtin_addon('spiderstate')\n+spiderstate = make_builtin_addon(\"spiderstate\")\n \n-stacktracedump = make_builtin_addon('stacktracedump')\n+stacktracedump = make_builtin_addon(\"stacktracedump\")\n \n-statsmailer = make_builtin_addon('statsmailer')\n+statsmailer = make_builtin_addon(\"statsmailer\")\n \n-telnetconsole = make_builtin_addon('telnetconsole')\n+telnetconsole = make_builtin_addon(\"telnetconsole\")\n\n@@ -85,7 +85,7 @@ def load_module_or_object(path):\n         return load_object(path)\n     except (ValueError, NameError, ImportError):\n         pass\n-    raise NameError(\"Could not load '%s'\" % path)\n+    raise NameError(f\"Could not load '{path}'\")\n \n \n def walk_modules(path):\n\n@@ -1,145 +1,143 @@\n-from collections import OrderedDict\n import itertools\n-import os.path\n-import six\n-from tests import mock\n import unittest\n import warnings\n+from collections import OrderedDict\n+from unittest import mock\n \n from pkg_resources import VersionConflict\n-import zope.interface\n+from zope.interface import directlyProvides\n+from zope.interface.exceptions import BrokenImplementation, MultipleInvalid\n from zope.interface.verify import verifyObject\n-from zope.interface.exceptions import BrokenImplementation\n \n from scrapy.addons import Addon, AddonManager\n from scrapy.crawler import Crawler\n from scrapy.interfaces import IAddon\n from scrapy.settings import BaseSettings\n \n-from . import addons\n-from . import addonmod\n+from . import addonmod, addons\n \n \n class AddonTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.rawaddon = Addon()\n \n         class AddonWithAttributes(Addon):\n-            name = 'Test'\n-            version = '1.0'\n+            name = \"Test\"\n+            version = \"1.0\"\n+\n         self.testaddon = AddonWithAttributes()\n \n     def test_interface(self):\n         # Raw Addon should fail exactly b/c name and version are not given\n-        self.assertFalse(hasattr(self.rawaddon, 'name'))\n-        self.assertFalse(hasattr(self.rawaddon, 'version'))\n-        self.assertRaises(BrokenImplementation, verifyObject, IAddon,\n-                          self.rawaddon)\n+        self.assertFalse(hasattr(self.rawaddon, \"name\"))\n+        self.assertFalse(hasattr(self.rawaddon, \"version\"))\n+        self.assertRaises(MultipleInvalid, verifyObject, IAddon, self.rawaddon)\n         verifyObject(IAddon, self.testaddon)\n \n     def test_export_component(self):\n-        settings = BaseSettings({'ITEM_PIPELINES': BaseSettings(),\n-                                 'DOWNLOAD_HANDLERS': BaseSettings()},\n-                                'default')\n+        settings = BaseSettings(\n+            {\"ITEM_PIPELINES\": BaseSettings(), \"DOWNLOAD_HANDLERS\": BaseSettings()},\n+            \"default\",\n+        )\n         self.testaddon.component_type = None\n         self.testaddon.export_component({}, settings)\n-        self.assertEqual(len(settings['ITEM_PIPELINES']), 0)\n-        self.testaddon.component_type = 'ITEM_PIPELINES'\n-        self.testaddon.component = 'test.component'\n+        self.assertEqual(len(settings[\"ITEM_PIPELINES\"]), 0)\n+        self.testaddon.component_type = \"ITEM_PIPELINES\"\n+        self.testaddon.component = \"test.component\"\n         self.testaddon.export_component({}, settings)\n-        six.assertCountEqual(self, settings['ITEM_PIPELINES'],\n-                             ['test.component'])\n-        self.assertEqual(settings['ITEM_PIPELINES']['test.component'], 0)\n+        self.assertCountEqual(settings[\"ITEM_PIPELINES\"], [\"test.component\"])\n+        self.assertEqual(settings[\"ITEM_PIPELINES\"][\"test.component\"], 0)\n         self.testaddon.component_order = 313\n         self.testaddon.export_component({}, settings)\n-        self.assertEqual(settings['ITEM_PIPELINES']['test.component'], 313)\n-        self.testaddon.component_type = 'DOWNLOAD_HANDLERS'\n-        self.testaddon.component_key = 'http'\n+        self.assertEqual(settings[\"ITEM_PIPELINES\"][\"test.component\"], 313)\n+        self.testaddon.component_type = \"DOWNLOAD_HANDLERS\"\n+        self.testaddon.component_key = \"http\"\n         self.testaddon.export_component({}, settings)\n-        self.assertEqual(settings['DOWNLOAD_HANDLERS']['http'],\n-                         'test.component')\n+        self.assertEqual(settings[\"DOWNLOAD_HANDLERS\"][\"http\"], \"test.component\")\n \n     def test_export_basics(self):\n         settings = BaseSettings()\n-        self.testaddon.basic_settings = {'TESTKEY': 313, 'OTHERKEY': True}\n+        self.testaddon.basic_settings = {\"TESTKEY\": 313, \"OTHERKEY\": True}\n         self.testaddon.export_basics(settings)\n-        self.assertEqual(settings['TESTKEY'], 313)\n-        self.assertEqual(settings['OTHERKEY'], True)\n-        self.assertEqual(settings.getpriority('TESTKEY'), 15)\n+        self.assertEqual(settings[\"TESTKEY\"], 313)\n+        self.assertEqual(settings[\"OTHERKEY\"], True)\n+        self.assertEqual(settings.getpriority(\"TESTKEY\"), 15)\n \n     def test_export_config(self):\n         settings = BaseSettings()\n         self.testaddon.settings_prefix = None\n-        self.testaddon.config_mapping = {'MAPPED_key': 'MAPPING_WORKED'}\n-        self.testaddon.default_config = {'key': 55, 'defaultkey': 100}\n-        self.testaddon.export_config({'key': 313, 'OTHERKEY': True,\n-                                     'mapped_KEY': 99}, settings)\n-        self.assertEqual(settings['TEST_KEY'], 313)\n-        self.assertEqual(settings['TEST_DEFAULTKEY'], 100)\n-        self.assertEqual(settings['TEST_OTHERKEY'], True)\n-        self.assertNotIn('MAPPED_key', settings)\n-        self.assertNotIn('MAPPED_KEY', settings)\n-        self.assertEqual(settings['MAPPING_WORKED'], 99)\n-        self.assertEqual(settings.getpriority('TEST_KEY'), 15)\n+        self.testaddon.config_mapping = {\"MAPPED_key\": \"MAPPING_WORKED\"}\n+        self.testaddon.default_config = {\"key\": 55, \"defaultkey\": 100}\n+        self.testaddon.export_config(\n+            {\"key\": 313, \"OTHERKEY\": True, \"mapped_KEY\": 99}, settings\n+        )\n+        self.assertEqual(settings[\"TEST_KEY\"], 313)\n+        self.assertEqual(settings[\"TEST_DEFAULTKEY\"], 100)\n+        self.assertEqual(settings[\"TEST_OTHERKEY\"], True)\n+        self.assertNotIn(\"MAPPED_key\", settings)\n+        self.assertNotIn(\"MAPPED_KEY\", settings)\n+        self.assertEqual(settings[\"MAPPING_WORKED\"], 99)\n+        self.assertEqual(settings.getpriority(\"TEST_KEY\"), 15)\n \n-        self.testaddon.settings_prefix = 'PREF'\n-        self.testaddon.export_config({'newkey': 99}, settings)\n-        self.assertEqual(settings['PREF_NEWKEY'], 99)\n+        self.testaddon.settings_prefix = \"PREF\"\n+        self.testaddon.export_config({\"newkey\": 99}, settings)\n+        self.assertEqual(settings[\"PREF_NEWKEY\"], 99)\n \n-        with mock.patch.object(settings, 'set') as mock_set:\n+        with mock.patch.object(settings, \"set\") as mock_set:\n             self.testaddon.settings_prefix = False\n-            self.testaddon.export_config({'thirdnewkey': 99}, settings)\n+            self.testaddon.export_config({\"thirdnewkey\": 99}, settings)\n             self.assertEqual(mock_set.call_count, 0)\n \n     def test_update_settings(self):\n         settings = BaseSettings()\n-        settings.set('TEST_KEY1', 'default', priority='default')\n-        settings.set('TEST_KEY2', 'project', priority='project')\n+        settings.set(\"TEST_KEY1\", \"default\", priority=\"default\")\n+        settings.set(\"TEST_KEY2\", \"project\", priority=\"project\")\n         self.testaddon.settings_prefix = None\n-        self.testaddon.basic_settings = {'OTHERTEST_KEY': 'addon'}\n-        addon_config = {'key1': 'addon', 'key2': 'addon', 'key3': 'addon'}\n+        self.testaddon.basic_settings = {\"OTHERTEST_KEY\": \"addon\"}\n+        addon_config = {\"key1\": \"addon\", \"key2\": \"addon\", \"key3\": \"addon\"}\n         self.testaddon.update_settings(addon_config, settings)\n-        self.assertEqual(settings['OTHERTEST_KEY'], 'addon')\n-        self.assertEqual(settings['TEST_KEY1'], 'addon')\n-        self.assertEqual(settings['TEST_KEY2'], 'project')\n-        self.assertEqual(settings['TEST_KEY3'], 'addon')\n+        self.assertEqual(settings[\"OTHERTEST_KEY\"], \"addon\")\n+        self.assertEqual(settings[\"TEST_KEY1\"], \"addon\")\n+        self.assertEqual(settings[\"TEST_KEY2\"], \"project\")\n+        self.assertEqual(settings[\"TEST_KEY3\"], \"addon\")\n \n \n class AddonManagerTest(unittest.TestCase):\n-\n     def setUp(self):\n         self.manager = AddonManager()\n \n     def test_add(self):\n         manager = AddonManager()\n-        manager.add(addonmod, {'key': 'val1'})\n-        manager.add('tests.test_addons.addons.GoodAddon')\n-        six.assertCountEqual(self, manager, ['AddonModule', 'GoodAddon'])\n-        self.assertIsInstance(manager['GoodAddon'], addons.GoodAddon)\n-        six.assertCountEqual(self, manager.configs['AddonModule'], ['key'])\n-        self.assertEqual(manager.configs['AddonModule']['key'], 'val1')\n+        manager.add(addonmod, {\"key\": \"val1\"})\n+        manager.add(\"tests.test_addons.addons.GoodAddon\")\n+        self.assertCountEqual(manager, [\"AddonModule\", \"GoodAddon\"])\n+        self.assertIsInstance(manager[\"GoodAddon\"], addons.GoodAddon)\n+        self.assertCountEqual(manager.configs[\"AddonModule\"], [\"key\"])\n+        self.assertEqual(manager.configs[\"AddonModule\"][\"key\"], \"val1\")\n         self.assertRaises(ValueError, manager.add, addonmod)\n \n     def test_add_dont_instantiate_providing_classes(self):\n         class ProviderGoodAddon(addons.GoodAddon):\n             pass\n-        zope.interface.directlyProvides(ProviderGoodAddon, IAddon)\n+\n+        directlyProvides(ProviderGoodAddon, IAddon)\n         manager = AddonManager()\n         manager.add(ProviderGoodAddon)\n-        self.assertIs(manager['GoodAddon'], ProviderGoodAddon)\n+        self.assertIs(manager[\"GoodAddon\"], ProviderGoodAddon)\n \n     def test_add_verifies(self):\n-        brokenaddon = self.manager.get_addon(\n-            'tests.test_addons.addons.BrokenAddon')\n-        self.assertRaises(zope.interface.exceptions.BrokenImplementation,\n+        brokenaddon = self.manager.get_addon(\"tests.test_addons.addons.BrokenAddon\")\n+        self.assertRaises(\n+            BrokenImplementation,\n             self.manager.add,\n-                          brokenaddon)\n+            brokenaddon,\n+        )\n \n     def test_add_adds_missing_interface_declaration(self):\n         class GoodAddonWithoutDeclaration(object):\n-            name = 'GoodAddonWithoutDeclaration'\n-            version = '1.0'\n+            name = \"GoodAddonWithoutDeclaration\"\n+            version = \"1.0\"\n+\n         self.manager.add(GoodAddonWithoutDeclaration)\n \n     def test_remove(self):\n@@ -147,64 +145,66 @@ class AddonManagerTest(unittest.TestCase):\n \n         def test_gets_removed(removearg):\n             manager.add(addonmod)\n-            self.assertIn('AddonModule', manager)\n+            self.assertIn(\"AddonModule\", manager)\n             manager.remove(removearg)\n-            self.assertNotIn('AddonModule', manager)\n+            self.assertNotIn(\"AddonModule\", manager)\n \n-        test_gets_removed('AddonModule')\n+        test_gets_removed(\"AddonModule\")\n         test_gets_removed(addonmod)\n-        test_gets_removed('tests.test_addons.addonmod')\n-        self.assertRaises(KeyError, manager.remove, 'nonexistent')\n+        test_gets_removed(\"tests.test_addons.addonmod\")\n+        self.assertRaises(KeyError, manager.remove, \"nonexistent\")\n         self.assertRaises(KeyError, manager.remove, addons.GoodAddon())\n \n     def test_get_addon(self):\n-        goodaddon = self.manager.get_addon('tests.test_addons.addons.GoodAddon')\n+        goodaddon = self.manager.get_addon(\"tests.test_addons.addons.GoodAddon\")\n         self.assertIs(goodaddon, addons.GoodAddon)\n \n         loaded_addonmod = self.manager.get_addon(\"tests.test_addons.addonmod\")\n         self.assertIs(loaded_addonmod, addonmod)\n \n-        addonspath = os.path.join(os.path.dirname(__file__), 'addons.py')\n         goodaddon = self.manager.get_addon(\"tests.test_addons.addons\")\n         self.assertIsInstance(goodaddon, addons.GoodAddon)\n \n-        self.assertRaises(NameError, self.manager.get_addon, 'xy.n_onexistent')\n+        self.assertRaises(NameError, self.manager.get_addon, \"xy.n_onexistent\")\n \n     def test_get_addon_forward(self):\n         class SomeCls(object):\n-            _addon = 'tests.test_addons.addons.GoodAddon'\n+            _addon = \"tests.test_addons.addons.GoodAddon\"\n+\n         self.assertIs(self.manager.get_addon(SomeCls()), addons.GoodAddon)\n \n     def test_get_addon_nested(self):\n-        x = addons.GoodAddon('outer')\n-        x._addon = addons.GoodAddon('middle')\n-        x._addon._addon = addons.GoodAddon('inner')\n+        x = addons.GoodAddon(\"outer\")\n+        x._addon = addons.GoodAddon(\"middle\")\n+        x._addon._addon = addons.GoodAddon(\"inner\")\n         self.assertIs(self.manager.get_addon(x), x._addon._addon)\n \n     def test_load_dict_load_settings(self):\n         def _test_load_method(func, *args, **kwargs):\n             manager = AddonManager()\n             getattr(manager, func)(*args, **kwargs)\n-            six.assertCountEqual(self, manager, ['GoodAddon', 'AddonModule'])\n-            self.assertIsInstance(manager['GoodAddon'], addons.GoodAddon)\n-            six.assertCountEqual(self, manager.configs['GoodAddon'], ['key'])\n-            self.assertEqual(manager.configs['GoodAddon']['key'], 'val2')\n-            self.assertEqual(manager['AddonModule'], addonmod)\n-            self.assertIn('key', manager.configs['AddonModule'])\n-            self.assertEqual(manager.configs['AddonModule']['key'], 'val1')\n+            self.assertCountEqual(manager, [\"GoodAddon\", \"AddonModule\"])\n+            self.assertIsInstance(manager[\"GoodAddon\"], addons.GoodAddon)\n+            self.assertCountEqual(manager.configs[\"GoodAddon\"], [\"key\"])\n+            self.assertEqual(manager.configs[\"GoodAddon\"][\"key\"], \"val2\")\n+            self.assertEqual(manager[\"AddonModule\"], addonmod)\n+            self.assertIn(\"key\", manager.configs[\"AddonModule\"])\n+            self.assertEqual(manager.configs[\"AddonModule\"][\"key\"], \"val1\")\n \n         addonsdict = {\n-            \"tests.test_addons.addonmod\": {'key': 'val1'},\n-            'tests.test_addons.addons.GoodAddon': {'key': 'val2'},\n+            \"tests.test_addons.addonmod\": {\"key\": \"val1\"},\n+            \"tests.test_addons.addons.GoodAddon\": {\"key\": \"val2\"},\n         }\n-        _test_load_method('load_dict', addonsdict)\n+        _test_load_method(\"load_dict\", addonsdict)\n \n         settings = BaseSettings()\n-        settings.set('ADDONS', {\"tests.test_addons.addonmod\": 0,\n-                                'tests.test_addons.addons.GoodAddon': 0})\n-        settings.set('ADDONMODULE', {'key': 'val1'})\n-        settings.set('GOODADDON', {'key': 'val2'})\n-        _test_load_method('load_settings', settings)\n+        settings.set(\n+            \"ADDONS\",\n+            {\"tests.test_addons.addonmod\": 0, \"tests.test_addons.addons.GoodAddon\": 0},\n+        )\n+        settings.set(\"ADDONMODULE\", {\"key\": \"val1\"})\n+        settings.set(\"GOODADDON\", {\"key\": \"val2\"})\n+        _test_load_method(\"load_settings\", settings)\n \n     def test_load_dict_load_settings_order(self):\n         def _test_load_method(expected_order, func, *args, **kwargs):\n@@ -218,72 +218,71 @@ class AddonManagerTest(unittest.TestCase):\n         for ordered_addons in itertools.permutations(addonlist):\n             expected_order = [a.name for a in ordered_addons]\n             addonsdict = OrderedDict((a, {}) for a in ordered_addons)\n-            _test_load_method(expected_order, 'load_dict', addonsdict)\n-            settings = BaseSettings({\n-                'ADDONS': {a: i for i, a in enumerate(ordered_addons)}\n-            })\n-            _test_load_method(expected_order, 'load_settings', settings)\n+            _test_load_method(expected_order, \"load_dict\", addonsdict)\n+            settings = BaseSettings(\n+                {\"ADDONS\": {a: i for i, a in enumerate(ordered_addons)}}\n+            )\n+            _test_load_method(expected_order, \"load_settings\", settings)\n \n     def test_enabled_disabled(self):\n         manager = AddonManager()\n-        manager.add(addons.GoodAddon('FirstAddon'))\n-        manager.add(addons.GoodAddon('SecondAddon'))\n-        self.assertEqual(set(manager.enabled),\n-                         set(('FirstAddon', 'SecondAddon')))\n+        manager.add(addons.GoodAddon(\"FirstAddon\"))\n+        manager.add(addons.GoodAddon(\"SecondAddon\"))\n+        self.assertEqual(set(manager.enabled), set((\"FirstAddon\", \"SecondAddon\")))\n         self.assertEqual(manager.disabled, [])\n-        manager.disable('FirstAddon')\n-        self.assertEqual(manager.enabled, ['SecondAddon'])\n-        self.assertEqual(manager.disabled, ['FirstAddon'])\n-        manager.enable('FirstAddon')\n-        self.assertEqual(set(manager.enabled),\n-                         set(('FirstAddon', 'SecondAddon')))\n+        manager.disable(\"FirstAddon\")\n+        self.assertEqual(manager.enabled, [\"SecondAddon\"])\n+        self.assertEqual(manager.disabled, [\"FirstAddon\"])\n+        manager.enable(\"FirstAddon\")\n+        self.assertEqual(set(manager.enabled), set((\"FirstAddon\", \"SecondAddon\")))\n         self.assertEqual(manager.disabled, [])\n \n     def test_enable_before_add(self):\n         manager = AddonManager()\n-        self.assertRaises(ValueError, manager.enable, 'FirstAddon')\n-        manager.disable('FirstAddon')\n-        manager.enable('FirstAddon')\n-        manager.add(addons.GoodAddon('FirstAddon'))\n-        self.assertIn('FirstAddon', manager.enabled)\n+        self.assertRaises(ValueError, manager.enable, \"FirstAddon\")\n+        manager.disable(\"FirstAddon\")\n+        manager.enable(\"FirstAddon\")\n+        manager.add(addons.GoodAddon(\"FirstAddon\"))\n+        self.assertIn(\"FirstAddon\", manager.enabled)\n \n     def test_disable_before_add(self):\n         manager = AddonManager()\n-        manager.disable('FirstAddon')\n-        manager.add(addons.GoodAddon('FirstAddon'))\n-        self.assertEqual(manager.disabled, ['FirstAddon'])\n+        manager.disable(\"FirstAddon\")\n+        manager.add(addons.GoodAddon(\"FirstAddon\"))\n+        self.assertEqual(manager.disabled, [\"FirstAddon\"])\n \n     def test_callbacks(self):\n-        first_addon = addons.GoodAddon('FirstAddon')\n-        second_addon = addons.GoodAddon('SecondAddon')\n+        first_addon = addons.GoodAddon(\"FirstAddon\")\n+        second_addon = addons.GoodAddon(\"SecondAddon\")\n \n         manager = AddonManager()\n-        manager.add(first_addon, {'test': 'first'})\n-        manager.add(second_addon, {'test': 'second'})\n+        manager.add(first_addon, {\"test\": \"first\"})\n+        manager.add(second_addon, {\"test\": \"second\"})\n         crawler = mock.create_autospec(Crawler)\n         settings = BaseSettings()\n \n-        with mock.patch.object(first_addon, 'update_addons') as ua_first, \\\n-             mock.patch.object(second_addon, 'update_addons') as ua_second, \\\n-             mock.patch.object(first_addon, 'update_settings') as us_first, \\\n-             mock.patch.object(second_addon, 'update_settings') as us_second, \\\n-             mock.patch.object(first_addon, 'check_configuration') as cc_first, \\\n-             mock.patch.object(second_addon, 'check_configuration') as cc_second:\n+        with mock.patch.object(\n+            first_addon, \"update_addons\"\n+        ) as ua_first, mock.patch.object(\n+            second_addon, \"update_addons\"\n+        ) as ua_second, mock.patch.object(\n+            first_addon, \"update_settings\"\n+        ) as us_first, mock.patch.object(\n+            second_addon, \"update_settings\"\n+        ) as us_second, mock.patch.object(\n+            first_addon, \"check_configuration\"\n+        ) as cc_first, mock.patch.object(\n+            second_addon, \"check_configuration\"\n+        ) as cc_second:\n             manager.update_addons()\n-            ua_first.assert_called_once_with(manager.configs['FirstAddon'],\n-                                             manager)\n-            ua_second.assert_called_once_with(manager.configs['SecondAddon'],\n-                                              manager)\n+            ua_first.assert_called_once_with(manager.configs[\"FirstAddon\"], manager)\n+            ua_second.assert_called_once_with(manager.configs[\"SecondAddon\"], manager)\n             manager.update_settings(settings)\n-            us_first.assert_called_once_with(manager.configs['FirstAddon'],\n-                                             settings)\n-            us_second.assert_called_once_with(manager.configs['SecondAddon'],\n-                                              settings)\n+            us_first.assert_called_once_with(manager.configs[\"FirstAddon\"], settings)\n+            us_second.assert_called_once_with(manager.configs[\"SecondAddon\"], settings)\n             manager.check_configuration(crawler)\n-            cc_first.assert_called_once_with(manager.configs['FirstAddon'],\n-                                             crawler)\n-            cc_second.assert_called_once_with(manager.configs['SecondAddon'],\n-                                              crawler)\n+            cc_first.assert_called_once_with(manager.configs[\"FirstAddon\"], crawler)\n+            cc_second.assert_called_once_with(manager.configs[\"SecondAddon\"], crawler)\n             self.assertEqual(ua_first.call_count, 1)\n             self.assertEqual(ua_second.call_count, 1)\n             self.assertEqual(us_first.call_count, 1)\n@@ -291,10 +290,10 @@ class AddonManagerTest(unittest.TestCase):\n \n             us_first.reset_mock()\n             us_second.reset_mock()\n-            manager.disable('FirstAddon')\n+            manager.disable(\"FirstAddon\")\n             manager.update_settings(settings)\n             self.assertEqual(us_first.call_count, 0)\n-            manager.enable('FirstAddon')\n+            manager.enable(\"FirstAddon\")\n             manager.update_settings(settings)\n             self.assertEqual(us_first.call_count, 1)\n             self.assertEqual(us_second.call_count, 2)\n@@ -302,44 +301,42 @@ class AddonManagerTest(unittest.TestCase):\n         # This will become relevant when we let spiders implement the add-on\n         # interface and should be replaced with a test where\n         # AddonManager.spidercls = None then.\n-        manager._call_if_exists(None, 'irrelevant')\n+        manager._call_if_exists(None, \"irrelevant\")\n \n     def test_update_addons_last_minute_add(self):\n         class AddedAddon(addons.GoodAddon):\n-            name = 'AddedAddon'\n+            name = \"AddedAddon\"\n \n         class FirstAddon(addons.GoodAddon):\n-            name = 'FirstAddon'\n+            name = \"FirstAddon\"\n \n             def update_addons(self, config, addons):\n                 addons.add(AddedAddon())\n \n         manager = AddonManager()\n         first_addon = FirstAddon()\n-        with mock.patch.object(first_addon, 'update_addons',\n-                               wraps=first_addon.update_addons) as ua_first, \\\n-             mock.patch.object(AddedAddon, 'update_addons') as ua_added:\n-            manager.add(first_addon, {'non-empty': 'dict'})\n+        with mock.patch.object(\n+            first_addon, \"update_addons\", wraps=first_addon.update_addons\n+        ) as ua_first, mock.patch.object(AddedAddon, \"update_addons\") as ua_added:\n+            manager.add(first_addon, {\"non-empty\": \"dict\"})\n             manager.update_addons()\n-            six.assertCountEqual(self, manager, ['FirstAddon', 'AddedAddon'])\n-            ua_first.assert_called_once_with(manager.configs['FirstAddon'],\n-                                             manager)\n-            ua_added.assert_called_once_with(manager.configs['AddedAddon'],\n-                                             manager)\n+            self.assertCountEqual(manager, [\"FirstAddon\", \"AddedAddon\"])\n+            ua_first.assert_called_once_with(manager.configs[\"FirstAddon\"], manager)\n+            ua_added.assert_called_once_with(manager.configs[\"AddedAddon\"], manager)\n \n     def test_check_dependency_clashes_attributes(self):\n         provides = addons.GoodAddon(\"ProvidesAddon\")\n-        provides.provides = ('test', )\n+        provides.provides = (\"test\",)\n         provides2 = addons.GoodAddon(\"ProvidesAddon2\")\n-        provides2.provides = ('test', )\n+        provides2.provides = (\"test\",)\n         requires = addons.GoodAddon(\"RequiresAddon\")\n-        requires.requires = ('test', )\n+        requires.requires = (\"test\",)\n         requires_name = addons.GoodAddon(\"RequiresNameAddon\")\n-        requires_name.requires = ('ProvidesAddon', )\n+        requires_name.requires = (\"ProvidesAddon\",)\n         requires_newer = addons.GoodAddon(\"RequiresNewerAddon\")\n-        requires_newer.requires = ('test>=2.0', )\n+        requires_newer.requires = (\"test>=2.0\",)\n         modifies = addons.GoodAddon(\"ModifiesAddon\")\n-        modifies.modifies = ('test', )\n+        modifies.modifies = (\"test\",)\n \n         def check_with(*addons):\n             manager = AddonManager()\n\n@@ -9,8 +9,10 @@ FROM = \"test_addons.addonmod\"\n name = \"AddonModule\"\n version = \"1.0\"\n \n+\n def update_settings(config, settings):\n     pass\n \n+\n def check_configuration(config, crawler):\n     pass\n\n@@ -1,18 +1,16 @@\n import zope.interface\n \n-from scrapy.addons import Addon\n from scrapy.interfaces import IAddon\n \n \n class Addon(object):\n-    FROM = 'test_addons.addons'\n+    FROM = \"test_addons.addons\"\n \n \n @zope.interface.declarations.implementer(IAddon)\n class GoodAddon(object):\n-\n-    name = 'GoodAddon'\n-    version = '1.0'\n+    name = \"GoodAddon\"\n+    version = \"1.0\"\n \n     def __init__(self, name=None, version=None):\n         if name is not None:\n@@ -32,8 +30,7 @@ class GoodAddon(object):\n \n @zope.interface.declarations.implementer(IAddon)\n class BrokenAddon(object):\n-\n-    name = 'BrokenAddon'\n+    name = \"BrokenAddon\"\n     # No version\n \n \n\n@@ -7,19 +7,17 @@ from scrapy.settings import Settings\n \n \n class BuiltinAddonsTest(unittest.TestCase):\n-\n     def test_make_builtin_addon(self):\n-        httpcache = make_builtin_addon('httpcache', {'enabled': True})\n-        self.assertEqual(httpcache.name, 'httpcache')\n-        self.assertEqual(httpcache.default_config, {'enabled': True})\n+        httpcache = make_builtin_addon(\"httpcache\", {\"enabled\": True})\n+        self.assertEqual(httpcache.name, \"httpcache\")\n+        self.assertEqual(httpcache.default_config, {\"enabled\": True})\n         self.assertEqual(httpcache.version, scrapy.__version__)\n-        httpcache = make_builtin_addon('httpcache', {'enabled': True}, '99.9')\n-        self.assertEqual(httpcache.version, '99.9')\n+        httpcache = make_builtin_addon(\"httpcache\", {\"enabled\": True}, \"99.9\")\n+        self.assertEqual(httpcache.version, \"99.9\")\n \n     def test_defaultheaders_export_config(self):\n         settings = Settings()\n         dh = scrapy.addons.defaultheaders()\n-        dh.export_config({'X-Test-Header': 'val'}, settings)\n-        self.assertIn('X-Test-Header', settings['DEFAULT_REQUEST_HEADERS'])\n-        self.assertEqual(settings['DEFAULT_REQUEST_HEADERS']['X-Test-Header'],\n-                         'val')\n+        dh.export_config({\"X-Test-Header\": \"val\"}, settings)\n+        self.assertIn(\"X-Test-Header\", settings[\"DEFAULT_REQUEST_HEADERS\"])\n+        self.assertEqual(settings[\"DEFAULT_REQUEST_HEADERS\"][\"X-Test-Header\"], \"val\")\n\n@@ -419,7 +419,7 @@ with multiples lines\n \n         addonmgr = AddonManager()\n         addonmgr.add(FailedCheckAddon())\n-        crawler = self.runner.create_crawler(SimpleSpider)\n+        crawler = get_crawler(SimpleSpider)\n         crawler.addons = addonmgr\n         # Doesn't work in 'precise' test environment:\n         # with self.assertRaises(ValueError):\n\n@@ -90,14 +90,12 @@ class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_instances_from_settings(self):\n         settings = Settings()\n-        myM3 = M3()\n \n         class InstanceTestMiddlewareManager(MiddlewareManager):\n             @classmethod\n             def _get_mwlist_from_settings(cls, settings):\n-                return [\"tests.test_middleware.M1\", M2, myM3]\n+                return [\"tests.test_middleware.M1\", M2]\n \n         mwman = InstanceTestMiddlewareManager.from_settings(settings)\n         self.assertIsInstance(mwman.middlewares[0], M1)\n         self.assertIsInstance(mwman.middlewares[1], M2)\n-        self.assertIs(mwman.middlewares[2], myM3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
