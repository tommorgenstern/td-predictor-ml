{"custom_id": "keras#b35b9433642ddb65081adaf4f08e06f667ee7115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 109 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,4 +48,6 @@ def linear(x):\n \n from .utils.generic_utils import get_from_module\n def get(identifier):\n+    if identifier is None:\n+        return linear\n     return get_from_module(identifier, globals(), 'activation function')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47c09d9557a9c4614270912cb39d55d7e7c01268", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 282 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 12 | Methods Changed: 12 | Complexity Δ (Sum/Max): 38/32 | Churn Δ: 292 | Churn Cumulative: 7255 | Contributors (this commit): 36 | Commits (past 90d): 30 | Contributors (cumulative): 47 | DMM Complexity: 0.3076923076923077\n\nDIFF:\n@@ -1164,7 +1164,7 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n     padding = _preprocess_border_mode(border_mode)\n     strides = (1,) + strides + (1,)\n \n-    tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n+    x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                                strides, padding)\n     return _postprocess_conv2d_output(x, dim_ordering)\n \n\n@@ -59,7 +59,8 @@ class Convolution1D(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n         input_dim: Number of channels/dimensions in the input.\n             Either this argument or the keyword argument `input_shape`must be\n             provided when using this layer as the first layer in a model.\n@@ -235,7 +236,8 @@ class Convolution2D(Layer):\n             It defaults to the `image_dim_ordering` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"th\".\n-        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n \n     # Input shape\n         4D tensor with shape:\n@@ -450,7 +452,8 @@ class AtrousConv2D(Convolution2D):\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation='linear', weights=None,\n-                 border_mode='valid', subsample=(1, 1), atrous_rate=(1, 1), dim_ordering=K.image_dim_ordering(),\n+                 border_mode='valid', subsample=(1, 1),\n+                 atrous_rate=(1, 1), dim_ordering=K.image_dim_ordering(),\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n                  bias=True, **kwargs):\n@@ -513,6 +516,236 @@ class AtrousConv2D(Convolution2D):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class SeparableConv2D(Layer):\n+    '''Separable convolution operator for 2D inputs.\n+\n+    Separable convolutions consist in first performing\n+    a depthwise spatial convolution\n+    (which acts on each input channel separately)\n+    followed by a pointwise convolution which mixes together the resulting\n+    output channels. The `depth_multiplier` argument controls how many\n+    output channels are generated per input channel in the depthwise step.\n+\n+    Intuitively, separable convolutions can be understood as\n+    a way to factorize a convolution kernel into two smaller kernels,\n+    or as an extreme version of an Inception block.\n+\n+    When using this layer as the first layer in a model,\n+    provide the keyword argument `input_shape`\n+    (tuple of integers, does not include the sample axis),\n+    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n+\n+    # Arguments\n+        nb_filter: Number of convolution filters to use.\n+        nb_row: Number of rows in the convolution kernel.\n+        nb_col: Number of columns in the convolution kernel.\n+        init: name of initialization function for the weights of the layer\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization.\n+            This parameter is only relevant if you don't pass\n+            a `weights` argument.\n+        activation: name of activation function to use\n+            (see [activations](../activations.md)),\n+            or alternatively, elementwise Theano function.\n+            If you don't specify anything, no activation is applied\n+            (ie. \"linear\" activation: a(x) = x).\n+        weights: list of numpy arrays to set as initial weights.\n+        border_mode: 'valid' or 'same'.\n+        subsample: tuple of length 2. Factor by which to subsample output.\n+            Also called strides elsewhere.\n+        depth_multiplier: how many output channel to use per input channel\n+            for the depthwise convolution step.\n+        atrous_rate: tuple of length 2. Factor for kernel dilation.\n+            Also called filter_dilation elsewhere.\n+        depthwise_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the depthwise weights matrix.\n+        pointwise_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the pointwise weights matrix.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n+            applied to the network output.\n+        depthwise_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the depthwise weights matrix.\n+        pointwise_constraint: instance of the [constraints](../constraints.md) module\n+            (eg. maxnorm, nonneg), applied to the pointwise weights matrix.\n+        b_constraint: instance of the [constraints](../constraints.md) module,\n+            applied to the bias.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"th\".\n+        bias: whether to include a bias\n+            (i.e. make the layer affine rather than linear).\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n+        `rows` and `cols` values might have changed due to padding.\n+    '''\n+    def __init__(self, nb_filter, nb_row, nb_col,\n+                 init='glorot_uniform', activation='linear', weights=None,\n+                 border_mode='valid', subsample=(1, 1),\n+                 depth_multiplier=1, dim_ordering=K.image_dim_ordering(),\n+                 depthwise_regularizer=None, pointwise_regularizer=None,\n+                 b_regularizer=None, activity_regularizer=None,\n+                 depthwise_constraint=None, pointwise_constraint=None,\n+                 b_constraint=None,\n+                 bias=True, **kwargs):\n+\n+        if K._BACKEND != 'tensorflow':\n+            raise Exception('SeparableConv2D is only available '\n+                            'with TensorFlow for the time being.')\n+\n+        if border_mode not in {'valid', 'same'}:\n+            raise Exception('Invalid border mode for AtrousConv2D:', border_mode)\n+\n+        if border_mode not in {'valid', 'same'}:\n+            raise Exception('Invalid border mode for Convolution2D:', border_mode)\n+        self.nb_filter = nb_filter\n+        self.nb_row = nb_row\n+        self.nb_col = nb_col\n+        self.init = initializations.get(init, dim_ordering=dim_ordering)\n+        self.activation = activations.get(activation)\n+        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n+        self.border_mode = border_mode\n+        self.subsample = tuple(subsample)\n+        self.depth_multiplier = depth_multiplier\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+\n+        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n+        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.depthwise_constraint = constraints.get(depthwise_constraint)\n+        self.pointwise_constraint = constraints.get(pointwise_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+\n+        self.bias = bias\n+        self.input_spec = [InputSpec(ndim=4)]\n+        self.initial_weights = weights\n+        super(SeparableConv2D, self).__init__(**kwargs)\n+\n+    def build(self, input_shape):\n+        if self.dim_ordering == 'th':\n+            stack_size = input_shape[1]\n+            depthwise_shape = (self.depth_multiplier, stack_size, self.nb_row, self.nb_col)\n+            pointwise_shape = (self.nb_filter, self.depth_multiplier * stack_size, 1, 1)\n+        elif self.dim_ordering == 'tf':\n+            stack_size = input_shape[3]\n+            depthwise_shape = (self.nb_row, self.nb_col, stack_size, self.depth_multiplier)\n+            pointwise_shape = (1, 1, self.depth_multiplier * stack_size, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+        self.depthwise_kernel = self.init(depthwise_shape,\n+                                          name='{}_depthwise_kernel'.format(self.name))\n+        self.pointwise_kernel = self.init(pointwise_shape,\n+                                          name='{}_pointwise_kernel'.format(self.name))\n+        if self.bias:\n+            self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n+            self.trainable_weights = [self.depthwise_kernel,\n+                                      self.pointwise_kernel,\n+                                      self.b]\n+        else:\n+            self.trainable_weights = [self.depthwise_kernel,\n+                                      self.pointwise_kernel]\n+        self.regularizers = []\n+        if self.depthwise_regularizer:\n+            self.depthwise_regularizer.set_param(self.depthwise_kernel)\n+            self.regularizers.append(self.depthwise_regularizer)\n+        if self.pointwise_regularizer:\n+            self.pointwise_regularizer.set_param(self.pointwise_kernel)\n+            self.regularizers.append(self.pointwise_regularizer)\n+        if self.bias and self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        self.constraints = {}\n+        if self.depthwise_constraint:\n+            self.constraints[self.depthwise_kernel] = self.depthwise_constraint\n+        if self.pointwise_constraint:\n+            self.constraints[self.pointwise_kernel] = self.pointwise_constraint\n+        if self.bias and self.b_constraint:\n+            self.constraints[self.b] = self.b_constraint\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+\n+    def get_output_shape_for(self, input_shape):\n+        if self.dim_ordering == 'th':\n+            rows = input_shape[2]\n+            cols = input_shape[3]\n+        elif self.dim_ordering == 'tf':\n+            rows = input_shape[1]\n+            cols = input_shape[2]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        rows = conv_output_length(rows, self.nb_row,\n+                                  self.border_mode, self.subsample[0])\n+        cols = conv_output_length(cols, self.nb_col,\n+                                  self.border_mode, self.subsample[1])\n+\n+        if self.dim_ordering == 'th':\n+            return (input_shape[0], self.nb_filter, rows, cols)\n+        elif self.dim_ordering == 'tf':\n+            return (input_shape[0], rows, cols, self.nb_filter)\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+    def call(self, x, mask=None):\n+        output = K.separable_conv2d(x, self.depthwise_kernel,\n+                                    self.pointwise_kernel,\n+                                    strides=self.subsample,\n+                                    border_mode=self.border_mode,\n+                                    dim_ordering=self.dim_ordering)\n+        if self.bias:\n+            if self.dim_ordering == 'th':\n+                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+            elif self.dim_ordering == 'tf':\n+                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n+            else:\n+                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+        output = self.activation(output)\n+        return output\n+\n+    def get_config(self):\n+        config = {'nb_filter': self.nb_filter,\n+                  'nb_row': self.nb_row,\n+                  'nb_col': self.nb_col,\n+                  'init': self.init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'border_mode': self.border_mode,\n+                  'subsample': self.subsample,\n+                  'depth_multiplier': self.depth_multiplier,\n+                  'dim_ordering': self.dim_ordering,\n+                  'depthwise_regularizer': self.depthwise_regularizer.get_config() if self.depthwise_regularizer else None,\n+                  'pointwise_regularizer': self.depthwise_regularizer.get_config() if self.depthwise_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'depthwise_constraint': self.depthwise_constraint.get_config() if self.depthwise_constraint else None,\n+                  'pointwise_constraint': self.pointwise_constraint.get_config() if self.pointwise_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias}\n+        base_config = super(SeparableConv2D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class Convolution3D(Layer):\n     '''Convolution operator for filtering windows of three-dimensional inputs.\n     When using this layer as the first layer in a model,\n\n@@ -53,7 +53,7 @@ def test_averagepooling_1d():\n \n \n def test_convolution_2d():\n-    nb_samples = 8\n+    nb_samples = 2\n     nb_filter = 3\n     stack_size = 4\n     nb_row = 10\n@@ -85,7 +85,7 @@ def test_convolution_2d():\n \n \n def test_atrous_conv_2d():\n-    nb_samples = 8\n+    nb_samples = 2\n     nb_filter = 3\n     stack_size = 4\n     nb_row = 10\n@@ -121,6 +121,45 @@ def test_atrous_conv_2d():\n                            input_shape=(nb_samples, stack_size, nb_row, nb_col))\n \n \n+@pytest.mark.skipif(K._BACKEND != 'tensorflow', reason=\"Requires TF backend\")\n+def test_separable_conv_2d():\n+    nb_samples = 2\n+    nb_filter = 8\n+    stack_size = 4\n+    nb_row = 10\n+    nb_col = 6\n+\n+    for border_mode in ['valid', 'same']:\n+        for subsample in [(1, 1), (2, 2)]:\n+            for multiplier in [1, 2]:\n+                if border_mode == 'same' and subsample != (1, 1):\n+                    continue\n+\n+                layer_test(convolutional.SeparableConv2D,\n+                           kwargs={'nb_filter': nb_filter,\n+                                   'nb_row': 3,\n+                                   'nb_col': 3,\n+                                   'border_mode': border_mode,\n+                                   'subsample': subsample,\n+                                   'depth_multiplier': multiplier},\n+                           input_shape=(nb_samples, stack_size, nb_row, nb_col))\n+\n+                layer_test(convolutional.SeparableConv2D,\n+                           kwargs={'nb_filter': nb_filter,\n+                                   'nb_row': 3,\n+                                   'nb_col': 3,\n+                                   'border_mode': border_mode,\n+                                   'depthwise_regularizer': 'l2',\n+                                   'pointwise_regularizer': 'l2',\n+                                   'b_regularizer': 'l2',\n+                                   'activity_regularizer': 'activity_l2',\n+                                   'pointwise_constraint': 'unitnorm',\n+                                   'depthwise_constraint': 'unitnorm',\n+                                   'subsample': subsample,\n+                                   'depth_multiplier': multiplier},\n+                           input_shape=(nb_samples, stack_size, nb_row, nb_col))\n+\n+\n def test_maxpooling_2d():\n     pool_size = (3, 3)\n \n@@ -209,7 +248,7 @@ def test_averagepooling_3d():\n \n \n def test_zero_padding_2d():\n-    nb_samples = 9\n+    nb_samples = 2\n     stack_size = 7\n     input_nb_row = 11\n     input_nb_col = 12\n@@ -235,7 +274,7 @@ def test_zero_padding_2d():\n \n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_zero_padding_3d():\n-    nb_samples = 9\n+    nb_samples = 2\n     stack_size = 7\n     input_len_dim1 = 10\n     input_len_dim2 = 11\n@@ -268,7 +307,7 @@ def test_upsampling_1d():\n \n \n def test_upsampling_2d():\n-    nb_samples = 9\n+    nb_samples = 2\n     stack_size = 7\n     input_nb_row = 11\n     input_nb_col = 12\n@@ -309,7 +348,7 @@ def test_upsampling_2d():\n \n @pytest.mark.skipif(K._BACKEND != 'theano', reason=\"Requires Theano backend\")\n def test_upsampling_3d():\n-    nb_samples = 9\n+    nb_samples = 2\n     stack_size = 7\n     input_len_dim1 = 10\n     input_len_dim2 = 11\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e836c10c6f6a17004e6b385304b7836103ea3398", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 57 | Churn Cumulative: 2626 | Contributors (this commit): 30 | Commits (past 90d): 28 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -479,6 +479,33 @@ def cos(x):\n     return tf.cos(x)\n \n \n+def normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.0001):\n+    '''\n+    '''\n+    mean, std = tf.nn.moments(x, reduction_axes, shift=None, name=None, keep_dims=False)\n+    target_shape = []\n+    for axis in range(ndim(x)):\n+        if axis in reduction_axes:\n+            target_shape.append(1)\n+        else:\n+            target_shape.append(tf.shape(x)[axis])\n+    print('target_shape:', target_shape)\n+    target_shape = tf.pack(target_shape)\n+\n+    broadcast_mean = tf.reshape(mean, target_shape)\n+    broadcast_std = tf.reshape(std, target_shape)\n+    broadcast_gamma = tf.reshape(gamma, target_shape)\n+    broadcast_beta = tf.reshape(beta, target_shape)\n+    normed = tf.nn.batch_normalization(x, broadcast_mean, broadcast_std,\n+                                       broadcast_beta, broadcast_gamma,\n+                                       epsilon)\n+    return normed, mean, std\n+\n+\n+def batch_normalization(x, mean, std, beta, gamma, epsilon=0.0001):\n+    return tf.nn.batch_normalization(x, mean, std, beta, gamma, epsilon)\n+\n+\n # SHAPE OPERATIONS\n \n def concatenate(tensors, axis=-1):\n\n@@ -98,18 +98,17 @@ class BatchNormalization(Layer):\n             broadcast_shape = [1] * len(input_shape)\n             broadcast_shape[self.axis] = input_shape[self.axis]\n \n-            # case: train mode (uses stats of the current batch)\n-            mean = K.mean(x, axis=reduction_axes)\n-            brodcast_mean = K.reshape(mean, broadcast_shape)\n-            std = K.mean(K.square(x - brodcast_mean) + self.epsilon, axis=reduction_axes)\n-            std = K.sqrt(std)\n-            brodcast_std = K.reshape(std, broadcast_shape)\n-            mean_update = self.momentum * self.running_mean + (1 - self.momentum) * mean\n-            std_update = self.momentum * self.running_std + (1 - self.momentum) * std\n+            # # case: train mode (uses stats of the current batch)\n+            # mean = K.mean(x, axis=reduction_axes)\n+            # brodcast_mean = K.reshape(mean, broadcast_shape)\n+            # std = K.mean(K.square(x - brodcast_mean) + self.epsilon, axis=reduction_axes)\n+            # std = K.sqrt(std)\n+            # brodcast_std = K.reshape(std, broadcast_shape)\n \n             if self.mode == 2:\n-                x_normed = (x - brodcast_mean) / (brodcast_std + self.epsilon)\n-                out = K.reshape(self.gamma, broadcast_shape) * x_normed + K.reshape(self.beta, broadcast_shape)\n+                x_normed, mean, std = K.normalize_batch_in_training(x, self.gamma, self.beta, reduction_axes, epsilon=self.epsilon)\n+                mean_update = self.momentum * self.running_mean + (1 - self.momentum) * mean\n+                std_update = self.momentum * self.running_std + (1 - self.momentum) * std\n             else:\n                 # mode 0\n                 if self.called_with not in {None, x}:\n@@ -123,26 +122,27 @@ class BatchNormalization(Layer):\n                                     '(see docs for a description of '\n                                     'the behavior).')\n                 self.called_with = x\n+                x_normed, mean, std = K.normalize_batch_in_training(x, self.gamma, self.beta, reduction_axes, epsilon=self.epsilon)\n+                mean_update = self.momentum * self.running_mean + (1 - self.momentum) * mean\n+                std_update = self.momentum * self.running_std + (1 - self.momentum) * std\n                 self.updates = [(self.running_mean, mean_update),\n                                 (self.running_std, std_update)]\n-                x_normed = (x - brodcast_mean) / (brodcast_std + self.epsilon)\n \n                 # case: test mode (uses running averages)\n                 brodcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n                 brodcast_running_std = K.reshape(self.running_std, broadcast_shape)\n-                x_normed_running = ((x - brodcast_running_mean) / (brodcast_running_std + self.epsilon))\n+                x_normed_running = K.batch_normalization(x, brodcast_running_mean, brodcast_running_std, self.beta, self.gamma, epsilon=self.epsilon)\n \n                 # pick the normalized form of x corresponding to the training phase\n                 x_normed = K.in_train_phase(x_normed, x_normed_running)\n-                out = K.reshape(self.gamma, broadcast_shape) * x_normed + K.reshape(self.beta, broadcast_shape)\n \n         elif self.mode == 1:\n             # sample-wise normalization\n             m = K.mean(x, axis=-1, keepdims=True)\n             std = K.sqrt(K.var(x, axis=-1, keepdims=True) + self.epsilon)\n             x_normed = (x - m) / (std + self.epsilon)\n-            out = self.gamma * x_normed + self.beta\n-        return out\n+            x_normed = self.gamma * x_normed + self.beta\n+        return x_normed\n \n     def get_config(self):\n         config = {\"epsilon\": self.epsilon,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a1e63990a92f643d4fd45796d371a3565b37058", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 70 | Churn Cumulative: 4513 | Contributors (this commit): 40 | Commits (past 90d): 49 | Contributors (cumulative): 63 | DMM Complexity: 0.6829268292682927\n\nDIFF:\n@@ -479,17 +479,24 @@ def cos(x):\n     return tf.cos(x)\n \n \n-def normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.0001):\n+def normalize_batch_in_training(x, gamma, beta,\n+                                reduction_axes, epsilon=0.0001):\n+    '''Compute mean and std for batch then apply batch_normalization on batch.\n     '''\n-    '''\n-    mean, std = tf.nn.moments(x, reduction_axes, shift=None, name=None, keep_dims=False)\n+    mean, std = tf.nn.moments(x, reduction_axes,\n+                              shift=None, name=None, keep_dims=False)\n+    if sorted(reduction_axes) == range(ndim(x))[:-1]:\n+        normed = tf.nn.batch_normalization(x, mean, std,\n+                                           beta, gamma,\n+                                           epsilon)\n+    else:\n+        # need broadcasting\n         target_shape = []\n         for axis in range(ndim(x)):\n             if axis in reduction_axes:\n                 target_shape.append(1)\n             else:\n                 target_shape.append(tf.shape(x)[axis])\n-    print('target_shape:', target_shape)\n         target_shape = tf.pack(target_shape)\n \n         broadcast_mean = tf.reshape(mean, target_shape)\n@@ -503,6 +510,8 @@ def normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.0001):\n \n \n def batch_normalization(x, mean, std, beta, gamma, epsilon=0.0001):\n+    '''Apply batch normalization on x given mean, std, beta and gamma.\n+    '''\n     return tf.nn.batch_normalization(x, mean, std, beta, gamma, epsilon)\n \n \n\n@@ -286,6 +286,38 @@ def cos(x):\n     return T.cos(x)\n \n \n+def normalize_batch_in_training(x, gamma, beta,\n+                                reduction_axes, epsilon=0.0001):\n+    '''Compute mean and std for batch then apply batch_normalization on batch.\n+    '''\n+    std = T.sqrt(x.var(reduction_axes) + epsilon)\n+    mean = x.mean(reduction_axes)\n+\n+    target_shape = []\n+    for axis in range(ndim(x)):\n+        if axis in reduction_axes:\n+            target_shape.append(1)\n+        else:\n+            target_shape.append(x.shape[axis])\n+    target_shape = T.stack(*target_shape)\n+\n+    broadcast_mean = T.reshape(mean, target_shape)\n+    broadcast_std = T.reshape(std, target_shape)\n+    broadcast_beta = T.reshape(beta, target_shape)\n+    broadcast_gamma = T.reshape(gamma, target_shape)\n+    normed = batch_normalization(x, broadcast_mean, broadcast_std,\n+                                 broadcast_beta, broadcast_gamma,\n+                                 epsilon)\n+    return normed, mean, std\n+\n+\n+def batch_normalization(x, mean, std, beta, gamma, epsilon=0.0001):\n+    '''Apply batch normalization on x given mean, std, beta and gamma.\n+    '''\n+    normed = (x - mean) * (gamma * T.inv(std + epsilon)) + beta\n+    return normed\n+\n+\n # SHAPE OPERATIONS\n \n def concatenate(tensors, axis=-1):\n\n@@ -128,10 +128,23 @@ class BatchNormalization(Layer):\n                 self.updates = [(self.running_mean, mean_update),\n                                 (self.running_std, std_update)]\n \n-                # case: test mode (uses running averages)\n-                brodcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n-                brodcast_running_std = K.reshape(self.running_std, broadcast_shape)\n-                x_normed_running = K.batch_normalization(x, brodcast_running_mean, brodcast_running_std, self.beta, self.gamma, epsilon=self.epsilon)\n+                if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n+                    x_normed_running = K.batch_normalization(x, self.running_mean,\n+                                                             self.running_std,\n+                                                             self.beta,\n+                                                             self.gamma,\n+                                                             epsilon=self.epsilon)\n+                else:\n+                    # need broadcasting\n+                    broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)\n+                    broadcast_running_std = K.reshape(self.running_std, broadcast_shape)\n+                    broadcast_beta = K.reshape(self.beta, broadcast_shape)\n+                    broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n+                    x_normed_running = K.batch_normalization(x, broadcast_running_mean,\n+                                                             broadcast_running_std,\n+                                                             broadcast_beta,\n+                                                             broadcast_gamma,\n+                                                             epsilon=self.epsilon)\n \n                 # pick the normalized form of x corresponding to the training phase\n                 x_normed = K.in_train_phase(x_normed, x_normed_running)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa2ec42da619f8f044e57686ea52b2df6da5a924", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 24 | Churn Cumulative: 4599 | Contributors (this commit): 35 | Commits (past 90d): 49 | Contributors (cumulative): 65 | DMM Complexity: 1.0\n\nDIFF:\n@@ -738,6 +738,13 @@ def gradients(loss, variables):\n     return tf.gradients(loss, variables)\n \n \n+def stop_gradient(variables):\n+    '''Returns `variables` but with zero gradient with respect to every other\n+    variables.\n+    '''\n+    return tf.stop_gradient(variables)\n+\n+\n # CONTROL FLOW\n \n def rnn(step_function, inputs, initial_states,\n\n@@ -576,6 +576,13 @@ def gradients(loss, variables):\n     return T.grad(loss, variables)\n \n \n+def stop_gradient(variables):\n+    '''Returns `variables` but with zero gradient with respect to every other\n+    variables.\n+    '''\n+    return theano.gradient.disconnected_grad(variables)\n+\n+\n # CONTROL FLOW\n \n def rnn(step_function, inputs, initial_states,\n\n@@ -208,14 +208,24 @@ class TestBackend(object):\n         exptf = xtf * KTF.exp(xtf)\n         lossth = KTH.sum(expth)\n         losstf = KTF.sum(exptf)\n+        zero_lossth = KTH.stop_gradient(lossth)\n+        zero_losstf = KTF.stop_gradient(losstf)\n \n         gradth = KTH.gradients(lossth, [expth])\n         gradtf = KTF.gradients(losstf, [exptf])\n+        zero_gradth = KTH.gradients(lossth + zero_lossth, [expth])\n+        zero_gradtf = KTF.gradients(losstf + zero_losstf, [exptf])\n \n         zth = KTH.eval(gradth[0])\n         ztf = KTF.eval(gradtf[0])\n+        zero_zth = KTH.eval(zero_gradth[0])\n+        zero_ztf = KTF.eval(zero_gradtf[0])\n         assert zth.shape == ztf.shape\n+        assert zero_zth.shape == zero_ztf.shape\n         assert_allclose(zth, ztf, atol=1e-05)\n+        assert_allclose(zero_zth, zero_ztf, atol=1e-05)\n+        assert_allclose(zero_zth, zth, atol=1e-05)\n+        assert_allclose(zero_ztf, ztf, atol=1e-05)\n \n     def test_function(self):\n         val = np.random.random((4, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9d904c12fd13837090233c5358cbfee2900f2fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 670 | Contributors (this commit): 12 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,6 +57,7 @@ class BatchNormalization(Layer):\n     '''\n     def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.9,\n                  weights=None, beta_init='zero', gamma_init='one', **kwargs):\n+        self.supports_masking = True\n         self.beta_init = initializations.get(beta_init)\n         self.gamma_init = initializations.get(gamma_init)\n         self.epsilon = epsilon\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fde0aac7330a26da80eecdc866a06c0642a66bd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 4352 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -379,7 +379,7 @@ class Convolution2D(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class AtrousConv2D(Convolution2D):\n+class AtrousConvolution2D(Convolution2D):\n     '''Atrous Convolution operator for filtering windows of two-dimensional inputs.\n     A.k.a dilated convolution or convolution with holes.\n     When using this layer as the first layer in a model,\n@@ -392,7 +392,7 @@ class AtrousConv2D(Convolution2D):\n     ```python\n         # apply a 3x3 convolution with atrous rate 2x2 and 64 output filters on a 256x256 image:\n         model = Sequential()\n-        model.add(AtrousConv2D(64, 3, 3, atrous_rate=(2,2), border_mode='valid', input_shape=(3, 256, 256)))\n+        model.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2), border_mode='valid', input_shape=(3, 256, 256)))\n         # now the actual kernel size is dilated from 3x3 to 5x5 (3+(3-1)*(2-1)=5)\n         # thus model.output_shape == (None, 64, 252, 252)\n     ```\n@@ -463,7 +463,7 @@ class AtrousConv2D(Convolution2D):\n \n         self.atrous_rate = tuple(atrous_rate)\n \n-        super(AtrousConv2D, self).__init__(nb_filter, nb_row, nb_col,\n+        super(AtrousConvolution2D, self).__init__(nb_filter, nb_row, nb_col,\n                                                   init=init, activation=activation,\n                                                   weights=weights, border_mode=border_mode,\n                                                   subsample=subsample, dim_ordering=dim_ordering,\n@@ -512,7 +512,7 @@ class AtrousConv2D(Convolution2D):\n \n     def get_config(self):\n         config = {'atrous_rate': self.atrous_rate}\n-        base_config = super(AtrousConv2D, self).get_config()\n+        base_config = super(AtrousConvolution2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1254,3 +1254,11 @@ class ZeroPadding3D(Layer):\n         config = {'padding': self.padding}\n         base_config = super(ZeroPadding3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+# Aliases\n+\n+Conv1D = Convolution1D\n+Conv2D = Convolution2D\n+Conv3D = Convolution3D\n+AtrousConv2D = AtrousConvolution2D\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d971e0cca5f09905a1a8c64db70081344942a7a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 4363 | Contributors (this commit): 19 | Commits (past 90d): 9 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -516,7 +516,7 @@ class AtrousConvolution2D(Convolution2D):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class SeparableConv2D(Layer):\n+class SeparableConvolution2D(Layer):\n     '''Separable convolution operator for 2D inputs.\n \n     Separable convolutions consist in first performing\n@@ -607,10 +607,10 @@ class SeparableConv2D(Layer):\n                             'with TensorFlow for the time being.')\n \n         if border_mode not in {'valid', 'same'}:\n-            raise Exception('Invalid border mode for AtrousConv2D:', border_mode)\n+            raise Exception('Invalid border mode for SeparableConv2D:', border_mode)\n \n         if border_mode not in {'valid', 'same'}:\n-            raise Exception('Invalid border mode for Convolution2D:', border_mode)\n+            raise Exception('Invalid border mode for SeparableConv2D:', border_mode)\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n@@ -635,7 +635,7 @@ class SeparableConv2D(Layer):\n         self.bias = bias\n         self.input_spec = [InputSpec(ndim=4)]\n         self.initial_weights = weights\n-        super(SeparableConv2D, self).__init__(**kwargs)\n+        super(SeparableConvolution2D, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -742,7 +742,7 @@ class SeparableConv2D(Layer):\n                   'pointwise_constraint': self.pointwise_constraint.get_config() if self.pointwise_constraint else None,\n                   'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n                   'bias': self.bias}\n-        base_config = super(SeparableConv2D, self).get_config()\n+        base_config = super(SeparableConvolution2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1262,3 +1262,4 @@ Conv1D = Convolution1D\n Conv2D = Convolution2D\n Conv3D = Convolution3D\n AtrousConv2D = AtrousConvolution2D\n+SeparableConv2D = SeparableConvolution2D\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#571db823717a3018c6b88f3cf76c222d3db3421c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 2015 | Contributors (this commit): 23 | Commits (past 90d): 23 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,17 @@ from .common import _FLOATX, _EPSILON, _IMAGE_DIM_ORDERING\n \n _SESSION = None\n _LEARNING_PHASE = tf.placeholder(dtype='uint8', name='keras_learning_phase')  # 0 = test, 1 = train\n+_MANUAL_VAR_INIT = False\n+\n+\n+def manual_variable_initialization(value):\n+    '''Whether variables should be initialized\n+    as they are instantiated (default), or if\n+    the user should handle the initialization\n+    (e.g. via tf.initialize_all_variables()).\n+    '''\n+    global _MANUAL_VAR_INIT\n+    _MANUAL_VAR_INIT = value\n \n \n def learning_phase():\n@@ -73,6 +84,8 @@ def variable(value, dtype=_FLOATX, name=None):\n         Tensor variable instance.\n     '''\n     v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n+    if _MANUAL_VAR_INIT:\n+        return v\n     if tf.get_default_graph() is get_session().graph:\n         try:\n             get_session().run(v.initializer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#489bb4eb1005e332c9cbc1391295155ed3ee7934", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 399 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,8 @@ class MaxPooling1D(_Pooling1D):\n \n     # Arguments\n         pool_length: factor by which to downscale. 2 will halve the input.\n-        stride: integer or None. Stride value.\n+        stride: integer, or None. Stride value.\n+            If None, it will default to `pool_length`.\n         border_mode: 'valid' or 'same'.\n             Note: 'same' will only work with TensorFlow for the time being.\n     '''\n@@ -84,7 +85,8 @@ class AveragePooling1D(_Pooling1D):\n \n     # Arguments\n         pool_length: factor by which to downscale. 2 will halve the input.\n-        stride: integer or None. Stride value.\n+        stride: integer, or None. Stride value.\n+            If None, it will default to `pool_length`.\n         border_mode: 'valid' or 'same'.\n             Note: 'same' will only work with TensorFlow for the time being.\n \n@@ -174,6 +176,7 @@ class MaxPooling2D(_Pooling2D):\n             factors by which to downscale (vertical, horizontal).\n             (2, 2) will halve the image in each dimension.\n         strides: tuple of 2 integers, or None. Strides values.\n+            If None, it will default to `pool_size`.\n         border_mode: 'valid' or 'same'.\n             Note: 'same' will only work with TensorFlow for the time being.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n@@ -215,6 +218,7 @@ class AveragePooling2D(_Pooling2D):\n             factors by which to downscale (vertical, horizontal).\n             (2, 2) will halve the image in each dimension.\n         strides: tuple of 2 integers, or None. Strides values.\n+            If None, it will default to `pool_size`.\n         border_mode: 'valid' or 'same'.\n             Note: 'same' will only work with TensorFlow for the time being.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#796f895f01d5a75f4c21dcc1a5ce1a8213645caf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 27 | Churn Cumulative: 6345 | Contributors (this commit): 40 | Commits (past 90d): 33 | Contributors (cumulative): 50 | DMM Complexity: 0.11764705882352941\n\nDIFF:\n@@ -72,6 +72,23 @@ def set_session(session):\n \n # VARIABLE MANIPULATION\n \n+def _convert_string_dtype(dtype):\n+    if dtype == 'float16':\n+        return tf.float16\n+    if dtype == 'float32':\n+        return tf.float32\n+    elif dtype == 'float64':\n+        return tf.float64\n+    elif dtype == 'int32':\n+        return tf.int32\n+    elif dtype == 'int64':\n+        return tf.int64\n+    elif dtype == 'uint8':\n+        return tf.int8\n+    else:\n+        raise ValueError('Unsupported dtype:', dtype)\n+\n+\n def variable(value, dtype=_FLOATX, name=None):\n     '''Instantiates a tensor.\n \n@@ -83,7 +100,7 @@ def variable(value, dtype=_FLOATX, name=None):\n     # Returns\n         Tensor variable instance.\n     '''\n-    v = tf.Variable(np.asarray(value, dtype=dtype), name=name)\n+    v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n     if _MANUAL_VAR_INIT:\n         return v\n     if tf.get_default_graph() is get_session().graph:\n@@ -167,13 +184,15 @@ def eval(x):\n def zeros(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-zeros tensor variable.\n     '''\n-    return variable(np.zeros(shape), dtype, name)\n+    return variable(lambda: tf.cast(tf.constant_initializer(0.)(shape), dtype),\n+                    dtype, name)\n \n \n def ones(shape, dtype=_FLOATX, name=None):\n     '''Instantiates an all-ones tensor variable.\n     '''\n-    return variable(np.ones(shape), dtype, name)\n+    return variable(lambda: tf.cast(tf.constant_initializer(1.)(shape), dtype),\n+                    dtype, name)\n \n \n def eye(size, dtype=_FLOATX, name=None):\n\n@@ -689,7 +689,7 @@ class LSTM(Recurrent):\n                                      name='{}_U'.format(self.name))\n \n             self.b = K.variable(np.hstack((np.zeros(self.output_dim),\n-                                           K.get_value(self.forget_bias_init(self.output_dim)),\n+                                           K.get_value(self.forget_bias_init((self.output_dim,))),\n                                            np.zeros(self.output_dim),\n                                            np.zeros(self.output_dim))),\n                                 name='{}_b'.format(self.name))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#892d9fae846b2e15e73de075e7290a8bfcfeb145", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 215 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -15,4 +15,4 @@ from . import objectives\n from . import optimizers\r\n from . import regularizers\r\n \r\n-__version__ = '1.0.5'\r\n+__version__ = '1.0.6'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='1.0.5',\n+      version='1.0.6',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/1.0.5',\n+      download_url='https://github.com/fchollet/keras/tarball/1.0.6',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f3e7245910eabd1bcaddc0d3e6bfd0f3eabd4eb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1859 | Contributors (this commit): 29 | Commits (past 90d): 20 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -314,7 +314,8 @@ def normalize_batch_in_training(x, gamma, beta,\n def batch_normalization(x, mean, std, beta, gamma, epsilon=0.0001):\n     '''Apply batch normalization on x given mean, std, beta and gamma.\n     '''\n-    normed = (x - mean) * (gamma * T.inv(std + epsilon)) + beta\n+    normed = T.nnet.bn.batch_normalization(x, gamma, beta, mean, std + epsilon,\n+                                           mode='high_mem')\n     return normed\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cfbaec60c77448e6249f912d4b4d4a8c079f2bcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 26 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 69 | Churn Cumulative: 6933 | Contributors (this commit): 44 | Commits (past 90d): 74 | Contributors (cumulative): 70 | DMM Complexity: 1.0\n\nDIFF:\n@@ -255,19 +255,28 @@ def batch_dot(x, y, axes=None):\n     If the number of dimensions is reduced to 1, we use `expand_dims` to\n     make sure that ndim is at least 2.\n \n-    # Example\n-        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n-        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n-        of x.dot(y.T), although we never have to calculate the off-diagonal\n-        elements.\n-\n-\n     # Arguments\n         x, y: tensors with ndim >= 2\n         axes: list (or single) int with target dimensions\n \n     # Returns\n-        Tensor with ndim >= 2\n+        A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1).\n+\n+    # Examples\n+        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n+        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n+        of x.dot(y.T), although we never have to calculate the off-diagonal\n+        elements.\n+       \n+        Shape inference:\n+        Let x's shape be (100, 20) and y's shape be (100, 30, 20). If dot_axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x's shape and y's shape:\n+        x.shape[0] : 100 : append to output shape\n+        x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. (dot_axes[0] = 1)\n+        y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y\n+        y.shape[1] : 30 : append to output shape\n+        y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. (dot_axes[1] = 2)\n+\n+        output_shape = (100, 30)\n     '''\n     if type(axes) == int:\n         axes = (axes, axes)\n\n@@ -122,24 +122,34 @@ def dot(x, y):\n \n \n def batch_dot(x, y, axes=None):\n-    '''batchwise dot product\n+    '''Batchwise dot product.\n+\n     batch_dot results in a tensor with less dimensions than the input.\n     If the number of dimensions is reduced to 1, we use `expand_dims` to\n     make sure that ndim is at least 2.\n \n-    # Example\n-        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n-        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n-        of x.dot(y.T), although we never have to calculate the off-diagonal\n-        elements.\n-\n-\n     # Arguments\n         x, y: tensors with ndim >= 2\n         axes: list (or single) int with target dimensions\n \n     # Returns\n-        Tensor with ndim >= 2\n+        A tensor with shape equal to the concatenation of x's shape (less the dimension that was summed over) and y's shape (less the batch dimension and the dimension that was summed over). If the final rank is 1, we reshape it to (batch_size, 1).\n+\n+    # Examples\n+        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n+        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n+        of x.dot(y.T), although we never have to calculate the off-diagonal\n+        elements.\n+       \n+        Shape inference:\n+        Let x's shape be (100, 20) and y's shape be (100, 30, 20). If dot_axes is (1, 2), to find the output shape of resultant tensor, loop through each dimension in x's shape and y's shape:\n+        x.shape[0] : 100 : append to output shape\n+        x.shape[1] : 20 : do not append to output shape, dimension 1 of x has been summed over. (dot_axes[0] = 1)\n+        y.shape[0] : 100 : do not append to output shape, always ignore first dimension of y\n+        y.shape[1] : 30 : append to output shape\n+        y.shape[2] : 20 : do not append to output shape, dimension 2 of y has been summed over. (dot_axes[1] = 2)\n+\n+        output_shape = (100, 30)\n     '''\n     if type(axes) == int:\n         axes = (axes, axes)\n\n@@ -1336,15 +1336,13 @@ class Merge(Layer):\n         elif self.mode in ['dot', 'cos']:\n             shape1 = list(input_shapes[0])\n             shape2 = list(input_shapes[1])\n-            dot_axes = [a - 1 for a in self.dot_axes]\n-            tensordot_output = np.tensordot(np.zeros(tuple(shape1[1:])),\n-                                            np.zeros(tuple(shape2[1:])),\n-                                            axes=dot_axes)\n-            if len(tensordot_output.shape) == 0:\n-                shape = (1,)\n-            else:\n-                shape = tensordot_output.shape\n-            return (shape1[0],) + shape\n+            shape1.pop(self.dot_axes[0])\n+            shape2.pop(self.dot_axes[1])\n+            shape2.pop(0)\n+            output_shape = shape1 + shape2\n+            if len(output_shape) == 1:\n+                output_shape += [1]\n+            return tuple(output_shape)\n \n     def compute_mask(self, inputs, mask=None):\n         if mask is None or all([m is None for m in mask]):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
