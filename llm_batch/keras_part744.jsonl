{"custom_id": "keras#a591cb474d6032e55410e7fec4c22b23c53b0d3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 75 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,10 +27,10 @@ class MyModel(keras.Model):\n @pytest.mark.requires_trainable_backend\n class BasicFlowTest(testing.TestCase):\n     def test_basic_fit(self):\n-        model = MyModel(hidden_dim=256, output_dim=16)\n+        model = MyModel(hidden_dim=2, output_dim=1)\n \n-        x = np.random.random((50000, 128))\n-        y = np.random.random((50000, 16))\n+        x = np.random.random((128, 4))\n+        y = np.random.random((128, 4))\n         batch_size = 32\n         epochs = 3\n \n@@ -45,7 +45,4 @@ class BasicFlowTest(testing.TestCase):\n         )\n         output_after_fit = model(x)\n \n-        print(\"History:\")\n-        print(history.history)\n-\n         self.assertNotAllClose(output_before_fit, output_after_fit)\n\n@@ -33,9 +33,8 @@ def setup_package():\n \n def create_virtualenv():\n     env_setup = [\n-        # Create and activate virtual environment\n+        # Create virtual environment\n         \"python3 -m venv test_env\",\n-        # \"source ./test_env/bin/activate\",\n     ]\n     os.environ[\"PATH\"] = (\n         \"/test_env/bin/\" + os.pathsep + os.environ.get(\"PATH\", \"\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0192c03ba63664dbe89e8d0a0a2e19dc64d17f6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 17 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ def manage_venv_installs(whl_path):\n         + \" \"\n         + BACKEND_REQ[other_backends[1]],\n         # Install `.whl` package\n-        \"pip install \" + whl_path + \" --force-reinstall --no-dependencies\",\n+        \"pip install \" + whl_path,\n     ]\n     run_commands_venv(install_setup)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21dc110303539b82f8ee768732c3d8d6c495bb73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -40,7 +40,7 @@ class BasicFlowTest(testing.TestCase):\n             metrics=[metrics.MeanSquaredError()],\n         )\n         output_before_fit = model(x)\n-        history = model.fit(\n+        model.fit(\n             x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n         )\n         output_after_fit = model(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eac46ff3498ad00ebe04836fb0555dd3371e0511", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 386 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 386 | Churn Cumulative: 386 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,386 @@\n+\"\"\"\n+Title: A walk through latent space with Stable Diffusion\n+Authors: Ian Stenbit, [fchollet](https://twitter.com/fchollet), [lukewood](https://twitter.com/luke_wood_ml)\n+Date created: 2022/09/28\n+Last modified: 2022/09/28\n+Description: Explore the latent manifold of Stable Diffusion.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Overview\n+\n+Generative image models learn a \"latent manifold\" of the visual world:\n+a low-dimensional vector space where each point maps to an image.\n+Going from such a point on the manifold back to a displayable image\n+is called \"decoding\" -- in the Stable Diffusion model, this is handled by\n+the \"decoder\" model.\n+\n+![The Stable Diffusion architecture](https://i.imgur.com/2uC8rYJ.png)\n+\n+This latent manifold of images is continuous and interpolative, meaning that:\n+\n+1. Moving a little on the manifold only changes the corresponding image a little (continuity).\n+2. For any two points A and B on the manifold (i.e. any two images), it is possible\n+to move from A to B via a path where each intermediate point is also on the manifold (i.e.\n+is also a valid image). Intermediate points would be called \"interpolations\" between\n+the two starting images.\n+\n+Stable Diffusion isn't just an image model, though, it's also a natural language model.\n+It has two latent spaces: the image representation space learned by the\n+encoder used during training, and the prompt latent space\n+which is learned using a combination of pretraining and training-time\n+fine-tuning.\n+\n+_Latent space walking_, or _latent space exploration_, is the process of\n+sampling a point in latent space and incrementally changing the latent\n+representation. Its most common application is generating animations\n+where each sampled point is fed to the decoder and is stored as a\n+frame in the final animation.\n+For high-quality latent representations, this produces coherent-looking\n+animations. These animations can provide insight into the feature map of the\n+latent space, and can ultimately lead to improvements in the training\n+process. One such GIF is displayed below:\n+\n+![Panda to Plane](/img/examples/generative/random_walks_with_stable_diffusion/panda2plane.gif)\n+\n+In this guide, we will show how to take advantage of the Stable Diffusion API\n+in KerasCV to perform prompt interpolation and circular walks through\n+Stable Diffusion's visual latent manifold, as well as through\n+the text encoder's latent manifold.\n+\n+This guide assumes the reader has a\n+high-level understanding of Stable Diffusion.\n+If you haven't already, you should start\n+by reading the [Stable Diffusion Tutorial](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/).\n+\n+To start, we import KerasCV and load up a Stable Diffusion model using the\n+optimizations discussed in the tutorial\n+[Generate images with Stable Diffusion](https://keras.io/guides/keras_cv/generate_images_with_stable_diffusion/).\n+Note that if you are running with a M1 Mac GPU you should not enable mixed precision.\n+\"\"\"\n+\n+\"\"\"shell\n+pip install keras-cv --upgrade --quiet\n+\"\"\"\n+\n+import keras_cv\n+from keras_cv.backend import keras\n+import matplotlib.pyplot as plt\n+from keras_cv.backend import ops\n+import numpy as np\n+import math\n+from PIL import Image\n+\n+# Enable mixed precision\n+# (only do this if you have a recent NVIDIA GPU)\n+keras.mixed_precision.set_global_policy(\"mixed_float16\")\n+\n+# Instantiate the Stable Diffusion model\n+model = keras_cv.models.StableDiffusion(jit_compile=True)\n+\n+\"\"\"\n+## Interpolating between text prompts\n+\n+In Stable Diffusion, a text prompt is first encoded into a vector,\n+and that encoding is used to guide the diffusion process.\n+The latent encoding vector has shape\n+77x768 (that's huge!), and when we give Stable Diffusion a text prompt, we're\n+generating images from just one such point on the latent manifold.\n+\n+To explore more of this manifold, we can interpolate between two text encodings\n+and generate images at those interpolated points:\n+\"\"\"\n+\n+prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n+prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n+interpolation_steps = 5\n+\n+encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n+encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n+\n+interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)\n+\n+# Show the size of the latent manifold\n+print(f\"Encoding shape: {encoding_1.shape}\")\n+\n+\"\"\"\n+Once we've interpolated the encodings, we can generate images from each point.\n+Note that in order to maintain some stability between the resulting images we\n+keep the diffusion noise constant between images.\n+\"\"\"\n+\n+seed = 12345\n+noise = keras.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n+\n+images = model.generate_image(\n+    interpolated_encodings,\n+    batch_size=interpolation_steps,\n+    diffusion_noise=noise,\n+)\n+\n+\"\"\"\n+Now that we've generated some interpolated images, let's take a look at them!\n+\n+Throughout this tutorial, we're going to export sequences of images as gifs so\n+that they can be easily viewed with some temporal context. For sequences of\n+images where the first and last images don't match conceptually, we rubber-band\n+the gif.\n+\n+If you're running in Colab, you can view your own GIFs by running:\n+\n+```\n+from IPython.display import Image as IImage\n+IImage(\"doggo-and-fruit-5.gif\")\n+```\n+\"\"\"\n+\n+\n+def export_as_gif(filename, images, frames_per_second=10, rubber_band=False):\n+    if rubber_band:\n+        images += images[2:-1][::-1]\n+    images[0].save(\n+        filename,\n+        save_all=True,\n+        append_images=images[1:],\n+        duration=1000 // frames_per_second,\n+        loop=0,\n+    )\n+\n+\n+export_as_gif(\n+    \"doggo-and-fruit-5.gif\",\n+    [Image.fromarray(img) for img in images],\n+    frames_per_second=2,\n+    rubber_band=True,\n+)\n+\n+\"\"\"\n+![Dog to Fruit 5](https://i.imgur.com/4ZCxZY4.gif)\n+\n+The results may seem surprising. Generally, interpolating between prompts\n+produces coherent looking images, and often demonstrates a progressive concept\n+shift between the contents of the two prompts. This is indicative of a high\n+quality representation space, that closely mirrors the natural structure\n+of the visual world.\n+\n+To best visualize this, we should do a much more fine-grained interpolation,\n+using hundreds of steps. In order to keep batch size small (so that we don't\n+OOM our GPU), this requires manually batching our interpolated\n+encodings.\n+\"\"\"\n+\n+interpolation_steps = 150\n+batch_size = 3\n+batches = interpolation_steps // batch_size\n+\n+interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)\n+batched_encodings = ops.split(interpolated_encodings, batches)\n+\n+images = []\n+for batch in range(batches):\n+    images += [\n+        Image.fromarray(img)\n+        for img in model.generate_image(\n+            batched_encodings[batch],\n+            batch_size=batch_size,\n+            num_steps=25,\n+            diffusion_noise=noise,\n+        )\n+    ]\n+\n+export_as_gif(\"doggo-and-fruit-150.gif\", images, rubber_band=True)\n+\n+\"\"\"\n+![Dog to Fruit 150](/img/examples/generative/random_walks_with_stable_diffusion/dog2fruit150.gif)\n+\n+The resulting gif shows a much clearer and more coherent shift between the two\n+prompts. Try out some prompts of your own and experiment!\n+\n+We can even extend this concept for more than one image. For example, we can\n+interpolate between four prompts:\n+\"\"\"\n+\n+prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n+prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n+prompt_3 = \"The eiffel tower in the style of starry night\"\n+prompt_4 = \"An architectural sketch of a skyscraper\"\n+\n+interpolation_steps = 6\n+batch_size = 3\n+batches = (interpolation_steps**2) // batch_size\n+\n+encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n+encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n+encoding_3 = ops.squeeze(model.encode_text(prompt_3))\n+encoding_4 = ops.squeeze(model.encode_text(prompt_4))\n+\n+interpolated_encodings = ops.linspace(\n+    ops.linspace(encoding_1, encoding_2, interpolation_steps),\n+    ops.linspace(encoding_3, encoding_4, interpolation_steps),\n+    interpolation_steps,\n+)\n+interpolated_encodings = ops.reshape(\n+    interpolated_encodings, (interpolation_steps**2, 77, 768)\n+)\n+batched_encodings = ops.split(interpolated_encodings, batches)\n+\n+images = []\n+for batch in range(batches):\n+    images.append(\n+        model.generate_image(\n+            batched_encodings[batch],\n+            batch_size=batch_size,\n+            diffusion_noise=noise,\n+        )\n+    )\n+\n+\n+def plot_grid(\n+    images,\n+    path,\n+    grid_size,\n+    scale=2,\n+):\n+    fig = plt.figure(figsize=(grid_size * scale, grid_size * scale))\n+    fig.tight_layout()\n+    plt.subplots_adjust(wspace=0, hspace=0)\n+    plt.margins(x=0, y=0)\n+    plt.axis(\"off\")\n+    images = images.astype(int)\n+    for row in range(grid_size):\n+        for col in range(grid_size):\n+            index = row * grid_size + col\n+            plt.subplot(grid_size, grid_size, index + 1)\n+            plt.imshow(images[index].astype(\"uint8\"))\n+            plt.axis(\"off\")\n+            plt.margins(x=0, y=0)\n+    plt.savefig(\n+        fname=path,\n+        pad_inches=0,\n+        bbox_inches=\"tight\",\n+        transparent=False,\n+        dpi=60,\n+    )\n+\n+\n+images = np.concatenate(images)\n+plot_grid(images, \"4-way-interpolation.jpg\", interpolation_steps)\n+\n+\"\"\"\n+We can also interpolate while allowing diffusion noise to vary by dropping\n+the `diffusion_noise` parameter:\n+\"\"\"\n+\n+images = []\n+for batch in range(batches):\n+    images.append(model.generate_image(batched_encodings[batch], batch_size=batch_size))\n+\n+images = np.concatenate(images)\n+plot_grid(images, \"4-way-interpolation-varying-noise.jpg\", interpolation_steps)\n+\n+\"\"\"\n+Next up -- let's go for some walks!\n+\n+## A walk around a text prompt\n+\n+Our next experiment will be to go for a walk around the latent manifold\n+starting from a point produced by a particular prompt.\n+\"\"\"\n+\n+walk_steps = 150\n+batch_size = 3\n+batches = walk_steps // batch_size\n+step_size = 0.005\n+\n+encoding = ops.squeeze(\n+    model.encode_text(\"The Eiffel Tower in the style of starry night\")\n+)\n+# Note that (77, 768) is the shape of the text encoding.\n+delta = ops.ones_like(encoding) * step_size\n+\n+walked_encodings = []\n+for step_index in range(walk_steps):\n+    walked_encodings.append(encoding)\n+    encoding += delta\n+walked_encodings = ops.stack(walked_encodings)\n+batched_encodings = ops.split(walked_encodings, batches)\n+\n+images = []\n+for batch in range(batches):\n+    images += [\n+        Image.fromarray(img)\n+        for img in model.generate_image(\n+            batched_encodings[batch],\n+            batch_size=batch_size,\n+            num_steps=25,\n+            diffusion_noise=noise,\n+        )\n+    ]\n+\n+export_as_gif(\"eiffel-tower-starry-night.gif\", images, rubber_band=True)\n+\n+\"\"\"\n+![Eiffel tower walk gif](https://i.imgur.com/9MMYtal.gif)\n+\n+Perhaps unsurprisingly, walking too far from the encoder's latent manifold\n+produces images that look incoherent. Try it for yourself by setting\n+your own prompt, and adjusting `step_size` to increase or decrease the magnitude\n+of the walk. Note that when the magnitude of the walk gets large, the walk often\n+leads into areas which produce extremely noisy images.\n+\n+## A circular walk through the diffusion noise space for a single prompt\n+\n+Our final experiment is to stick to one prompt and explore the variety of images\n+that the diffusion model can produce from that prompt. We do this by controlling\n+the noise that is used to seed the diffusion process.\n+\n+We create two noise components, `x` and `y`, and do a walk from 0 to 2π, summing\n+the cosine of our `x` component and the sin of our `y` component to produce noise.\n+Using this approach, the end of our walk arrives at the same noise inputs where\n+we began our walk, so we get a \"loopable\" result!\n+\"\"\"\n+\n+prompt = \"An oil paintings of cows in a field next to a windmill in Holland\"\n+encoding = ops.squeeze(model.encode_text(prompt))\n+walk_steps = 150\n+batch_size = 3\n+batches = walk_steps // batch_size\n+\n+walk_noise_x = keras.random.normal(noise.shape, dtype=\"float64\")\n+walk_noise_y = keras.random.normal(noise.shape, dtype=\"float64\")\n+\n+walk_scale_x = ops.cos(ops.linspace(0, 2, walk_steps) * math.pi)\n+walk_scale_y = ops.sin(ops.linspace(0, 2, walk_steps) * math.pi)\n+noise_x = ops.tensordot(walk_scale_x, walk_noise_x, axes=0)\n+noise_y = ops.tensordot(walk_scale_y, walk_noise_y, axes=0)\n+noise = ops.add(noise_x, noise_y)\n+batched_noise = ops.split(noise, batches)\n+\n+images = []\n+for batch in range(batches):\n+    images += [\n+        Image.fromarray(img)\n+        for img in model.generate_image(\n+            encoding,\n+            batch_size=batch_size,\n+            num_steps=25,\n+            diffusion_noise=batched_noise[batch],\n+        )\n+    ]\n+\n+export_as_gif(\"cows.gif\", images)\n+\n+\"\"\"\n+![Happy Cows](/img/examples/generative/random_walks_with_stable_diffusion/happycows.gif)\n+\n+Experiment with your own prompts and with different values of\n+`unconditional_guidance_scale`!\n+\n+## Conclusion\n+\n+Stable Diffusion offers a lot more than just single text-to-image generation.\n+Exploring the latent manifold of the text encoder and the noise space of the\n+diffusion model are two fun ways to experience the power of this model, and\n+KerasCV makes it easy!\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb70a8f0b22699c3637da2a3cfe1e7146acb4b9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 16 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 446 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -65,9 +65,9 @@ pip install keras-cv --upgrade --quiet\n \"\"\"\n \n import keras_cv\n-from keras_cv.backend import keras\n+from tensorflow import keras\n import matplotlib.pyplot as plt\n-from keras_cv.backend import ops\n+import tensorflow as tf\n import numpy as np\n import math\n from PIL import Image\n@@ -96,10 +96,10 @@ prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n interpolation_steps = 5\n \n-encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n-encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n+encoding_1 = tf.squeeze(model.encode_text(prompt_1))\n+encoding_2 = tf.squeeze(model.encode_text(prompt_2))\n \n-interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)\n+interpolated_encodings = tf.linspace(encoding_1, encoding_2, interpolation_steps)\n \n # Show the size of the latent manifold\n print(f\"Encoding shape: {encoding_1.shape}\")\n@@ -111,7 +111,7 @@ keep the diffusion noise constant between images.\n \"\"\"\n \n seed = 12345\n-noise = keras.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n+noise = tf.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n \n images = model.generate_image(\n     interpolated_encodings,\n@@ -174,8 +174,8 @@ interpolation_steps = 150\n batch_size = 3\n batches = interpolation_steps // batch_size\n \n-interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)\n-batched_encodings = ops.split(interpolated_encodings, batches)\n+interpolated_encodings = tf.linspace(encoding_1, encoding_2, interpolation_steps)\n+batched_encodings = tf.split(interpolated_encodings, batches)\n \n images = []\n for batch in range(batches):\n@@ -210,20 +210,20 @@ interpolation_steps = 6\n batch_size = 3\n batches = (interpolation_steps**2) // batch_size\n \n-encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n-encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n-encoding_3 = ops.squeeze(model.encode_text(prompt_3))\n-encoding_4 = ops.squeeze(model.encode_text(prompt_4))\n+encoding_1 = tf.squeeze(model.encode_text(prompt_1))\n+encoding_2 = tf.squeeze(model.encode_text(prompt_2))\n+encoding_3 = tf.squeeze(model.encode_text(prompt_3))\n+encoding_4 = tf.squeeze(model.encode_text(prompt_4))\n \n-interpolated_encodings = ops.linspace(\n-    ops.linspace(encoding_1, encoding_2, interpolation_steps),\n-    ops.linspace(encoding_3, encoding_4, interpolation_steps),\n+interpolated_encodings = tf.linspace(\n+    tf.linspace(encoding_1, encoding_2, interpolation_steps),\n+    tf.linspace(encoding_3, encoding_4, interpolation_steps),\n     interpolation_steps,\n )\n-interpolated_encodings = ops.reshape(\n+interpolated_encodings = tf.reshape(\n     interpolated_encodings, (interpolation_steps**2, 77, 768)\n )\n-batched_encodings = ops.split(interpolated_encodings, batches)\n+batched_encodings = tf.split(interpolated_encodings, batches)\n \n images = []\n for batch in range(batches):\n@@ -293,18 +293,18 @@ batch_size = 3\n batches = walk_steps // batch_size\n step_size = 0.005\n \n-encoding = ops.squeeze(\n+encoding = tf.squeeze(\n     model.encode_text(\"The Eiffel Tower in the style of starry night\")\n )\n # Note that (77, 768) is the shape of the text encoding.\n-delta = ops.ones_like(encoding) * step_size\n+delta = tf.ones_like(encoding) * step_size\n \n walked_encodings = []\n for step_index in range(walk_steps):\n     walked_encodings.append(encoding)\n     encoding += delta\n-walked_encodings = ops.stack(walked_encodings)\n-batched_encodings = ops.split(walked_encodings, batches)\n+walked_encodings = tf.stack(walked_encodings)\n+batched_encodings = tf.split(walked_encodings, batches)\n \n images = []\n for batch in range(batches):\n@@ -342,20 +342,20 @@ we began our walk, so we get a \"loopable\" result!\n \"\"\"\n \n prompt = \"An oil paintings of cows in a field next to a windmill in Holland\"\n-encoding = ops.squeeze(model.encode_text(prompt))\n+encoding = tf.squeeze(model.encode_text(prompt))\n walk_steps = 150\n batch_size = 3\n batches = walk_steps // batch_size\n \n-walk_noise_x = keras.random.normal(noise.shape, dtype=\"float64\")\n-walk_noise_y = keras.random.normal(noise.shape, dtype=\"float64\")\n+walk_noise_x = tf.random.normal(noise.shape, dtype=tf.float64)\n+walk_noise_y = tf.random.normal(noise.shape, dtype=tf.float64)\n \n-walk_scale_x = ops.cos(ops.linspace(0, 2, walk_steps) * math.pi)\n-walk_scale_y = ops.sin(ops.linspace(0, 2, walk_steps) * math.pi)\n-noise_x = ops.tensordot(walk_scale_x, walk_noise_x, axes=0)\n-noise_y = ops.tensordot(walk_scale_y, walk_noise_y, axes=0)\n-noise = ops.add(noise_x, noise_y)\n-batched_noise = ops.split(noise, batches)\n+walk_scale_x = tf.cos(tf.linspace(0, 2, walk_steps) * math.pi)\n+walk_scale_y = tf.sin(tf.linspace(0, 2, walk_steps) * math.pi)\n+noise_x = tf.tensordot(walk_scale_x, walk_noise_x, axes=0)\n+noise_y = tf.tensordot(walk_scale_y, walk_noise_y, axes=0)\n+noise = tf.add(noise_x, noise_y)\n+batched_noise = tf.split(noise, batches)\n \n images = []\n for batch in range(batches):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c04da8144f94b56e92b60efa062ea55db83d917d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 16 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 60 | Churn Cumulative: 506 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -65,9 +65,9 @@ pip install keras-cv --upgrade --quiet\n \"\"\"\n \n import keras_cv\n-from tensorflow import keras\n+from keras_cv.backend import keras\n import matplotlib.pyplot as plt\n-import tensorflow as tf\n+from keras_cv.backend import ops\n import numpy as np\n import math\n from PIL import Image\n@@ -96,10 +96,10 @@ prompt_1 = \"A watercolor painting of a Golden Retriever at the beach\"\n prompt_2 = \"A still life DSLR photo of a bowl of fruit\"\n interpolation_steps = 5\n \n-encoding_1 = tf.squeeze(model.encode_text(prompt_1))\n-encoding_2 = tf.squeeze(model.encode_text(prompt_2))\n+encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n+encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n \n-interpolated_encodings = tf.linspace(encoding_1, encoding_2, interpolation_steps)\n+interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)\n \n # Show the size of the latent manifold\n print(f\"Encoding shape: {encoding_1.shape}\")\n@@ -111,7 +111,7 @@ keep the diffusion noise constant between images.\n \"\"\"\n \n seed = 12345\n-noise = tf.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n+noise = keras.random.normal((512 // 8, 512 // 8, 4), seed=seed)\n \n images = model.generate_image(\n     interpolated_encodings,\n@@ -174,8 +174,8 @@ interpolation_steps = 150\n batch_size = 3\n batches = interpolation_steps // batch_size\n \n-interpolated_encodings = tf.linspace(encoding_1, encoding_2, interpolation_steps)\n-batched_encodings = tf.split(interpolated_encodings, batches)\n+interpolated_encodings = ops.linspace(encoding_1, encoding_2, interpolation_steps)\n+batched_encodings = ops.split(interpolated_encodings, batches)\n \n images = []\n for batch in range(batches):\n@@ -210,20 +210,20 @@ interpolation_steps = 6\n batch_size = 3\n batches = (interpolation_steps**2) // batch_size\n \n-encoding_1 = tf.squeeze(model.encode_text(prompt_1))\n-encoding_2 = tf.squeeze(model.encode_text(prompt_2))\n-encoding_3 = tf.squeeze(model.encode_text(prompt_3))\n-encoding_4 = tf.squeeze(model.encode_text(prompt_4))\n+encoding_1 = ops.squeeze(model.encode_text(prompt_1))\n+encoding_2 = ops.squeeze(model.encode_text(prompt_2))\n+encoding_3 = ops.squeeze(model.encode_text(prompt_3))\n+encoding_4 = ops.squeeze(model.encode_text(prompt_4))\n \n-interpolated_encodings = tf.linspace(\n-    tf.linspace(encoding_1, encoding_2, interpolation_steps),\n-    tf.linspace(encoding_3, encoding_4, interpolation_steps),\n+interpolated_encodings = ops.linspace(\n+    ops.linspace(encoding_1, encoding_2, interpolation_steps),\n+    ops.linspace(encoding_3, encoding_4, interpolation_steps),\n     interpolation_steps,\n )\n-interpolated_encodings = tf.reshape(\n+interpolated_encodings = ops.reshape(\n     interpolated_encodings, (interpolation_steps**2, 77, 768)\n )\n-batched_encodings = tf.split(interpolated_encodings, batches)\n+batched_encodings = ops.split(interpolated_encodings, batches)\n \n images = []\n for batch in range(batches):\n@@ -293,18 +293,18 @@ batch_size = 3\n batches = walk_steps // batch_size\n step_size = 0.005\n \n-encoding = tf.squeeze(\n+encoding = ops.squeeze(\n     model.encode_text(\"The Eiffel Tower in the style of starry night\")\n )\n # Note that (77, 768) is the shape of the text encoding.\n-delta = tf.ones_like(encoding) * step_size\n+delta = ops.ones_like(encoding) * step_size\n \n walked_encodings = []\n for step_index in range(walk_steps):\n     walked_encodings.append(encoding)\n     encoding += delta\n-walked_encodings = tf.stack(walked_encodings)\n-batched_encodings = tf.split(walked_encodings, batches)\n+walked_encodings = ops.stack(walked_encodings)\n+batched_encodings = ops.split(walked_encodings, batches)\n \n images = []\n for batch in range(batches):\n@@ -342,20 +342,20 @@ we began our walk, so we get a \"loopable\" result!\n \"\"\"\n \n prompt = \"An oil paintings of cows in a field next to a windmill in Holland\"\n-encoding = tf.squeeze(model.encode_text(prompt))\n+encoding = ops.squeeze(model.encode_text(prompt))\n walk_steps = 150\n batch_size = 3\n batches = walk_steps // batch_size\n \n-walk_noise_x = tf.random.normal(noise.shape, dtype=tf.float64)\n-walk_noise_y = tf.random.normal(noise.shape, dtype=tf.float64)\n+walk_noise_x = keras.random.normal(noise.shape, dtype=\"float64\")\n+walk_noise_y = keras.random.normal(noise.shape, dtype=\"float64\")\n \n-walk_scale_x = tf.cos(tf.linspace(0, 2, walk_steps) * math.pi)\n-walk_scale_y = tf.sin(tf.linspace(0, 2, walk_steps) * math.pi)\n-noise_x = tf.tensordot(walk_scale_x, walk_noise_x, axes=0)\n-noise_y = tf.tensordot(walk_scale_y, walk_noise_y, axes=0)\n-noise = tf.add(noise_x, noise_y)\n-batched_noise = tf.split(noise, batches)\n+walk_scale_x = ops.cos(ops.linspace(0, 2, walk_steps) * math.pi)\n+walk_scale_y = ops.sin(ops.linspace(0, 2, walk_steps) * math.pi)\n+noise_x = ops.tensordot(walk_scale_x, walk_noise_x, axes=0)\n+noise_y = ops.tensordot(walk_scale_y, walk_noise_y, axes=0)\n+noise = ops.add(noise_x, noise_y)\n+batched_noise = ops.split(noise, batches)\n \n images = []\n for batch in range(batches):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bba472898063220ea694779aaf30e9f0eb85a04f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 510 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -65,9 +65,9 @@ pip install keras-cv --upgrade --quiet\n \"\"\"\n \n import keras_cv\n-from keras_cv.backend import keras\n+import keras\n import matplotlib.pyplot as plt\n-from keras_cv.backend import ops\n+from keras import ops\n import numpy as np\n import math\n from PIL import Image\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb125174fcbf1fbaf8e52a1087af025dcb436145", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 407 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 14/14 | Churn Δ: 407 | Churn Cumulative: 407 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,407 @@\n+\"\"\"\n+Title: Image Super-Resolution using an Efficient Sub-Pixel CNN\n+Author: [Xingyu Long](https://github.com/xingyu-long)\n+Converted to Keras 3 by: [Md Awsfalur Rahman](https://awsaf49.github.io)\n+Date created: 2020/07/28\n+Last modified: 2020/08/27\n+Description: Implementing Super-Resolution using Efficient sub-pixel model on BSDS500.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+ESPCN (Efficient Sub-Pixel CNN), proposed by [Shi, 2016](https://arxiv.org/abs/1609.05158)\n+is a model that reconstructs a high-resolution version of an image given a low-resolution\n+version.\n+It leverages efficient \"sub-pixel convolution\" layers, which learns an array of\n+image upscaling filters.\n+\n+In this code example, we will implement the model from the paper and train it on a small\n+dataset,\n+[BSDS500](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html).\n+[BSDS500](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/resources.html).\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import keras as keras\n+from keras import layers\n+from keras import ops\n+from keras.utils import load_img\n+from keras.utils import array_to_img\n+from keras.utils import img_to_array\n+from keras.preprocessing import image_dataset_from_directory\n+import tensorflow as tf  #  only for data preprocessing\n+\n+import math\n+import numpy as np\n+\n+from IPython.display import display\n+\n+\"\"\"\n+## Load data: BSDS500 dataset\n+\n+### Download dataset\n+\n+We use the built-in `keras.utils.get_file` utility to retrieve the dataset.\n+\"\"\"\n+\n+dataset_url =\n+\"http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/BSR/BSR_bsds500.tgz\"\n+data_dir = keras.utils.get_file(origin=dataset_url, fname=\"BSR\", untar=True)\n+root_dir = os.path.join(data_dir, \"BSDS500/data\")\n+\n+\"\"\"\n+We create training and validation datasets via `image_dataset_from_directory`.\n+\"\"\"\n+\n+crop_size = 300\n+upscale_factor = 3\n+input_size = crop_size // upscale_factor\n+batch_size = 8\n+\n+train_ds = image_dataset_from_directory(\n+    root_dir,\n+    batch_size=batch_size,\n+    image_size=(crop_size, crop_size),\n+    validation_split=0.2,\n+    subset=\"training\",\n+    seed=1337,\n+    label_mode=None,\n+)\n+\n+valid_ds = image_dataset_from_directory(\n+    root_dir,\n+    batch_size=batch_size,\n+    image_size=(crop_size, crop_size),\n+    validation_split=0.2,\n+    subset=\"validation\",\n+    seed=1337,\n+    label_mode=None,\n+)\n+\n+\"\"\"\n+We rescale the images to take values in the range [0, 1].\n+\"\"\"\n+\n+\n+def scaling(input_image):\n+    input_image = input_image / 255.0\n+    return input_image\n+\n+\n+# Scale from (0, 255) to (0, 1)\n+train_ds = train_ds.map(scaling)\n+valid_ds = valid_ds.map(scaling)\n+\n+\"\"\"\n+Let's visualize a few sample images:\n+\"\"\"\n+\n+for batch in train_ds.take(1):\n+    for img in batch:\n+        display(array_to_img(img))\n+\n+\"\"\"\n+We prepare a dataset of test image paths that we will use for\n+visual evaluation at the end of this example.\n+\"\"\"\n+\n+dataset = os.path.join(root_dir, \"images\")\n+test_path = os.path.join(dataset, \"test\")\n+\n+test_img_paths = sorted(\n+    [\n+        os.path.join(test_path, fname)\n+        for fname in os.listdir(test_path)\n+        if fname.endswith(\".jpg\")\n+    ]\n+)\n+\n+\"\"\"\n+## Crop and resize images\n+\n+Let's process image data.\n+First, we convert our images from the RGB color space to the\n+[YUV colour space](https://en.wikipedia.org/wiki/YUV).\n+\n+For the input data (low-resolution images),\n+we crop the image, retrieve the `y` channel (luninance),\n+and resize it with the `area` method (use `BICUBIC` if you use PIL).\n+We only consider the luminance channel\n+in the YUV color space because humans are more sensitive to\n+luminance change.\n+\n+For the target data (high-resolution images), we just crop the image\n+and retrieve the `y` channel.\n+\"\"\"\n+\n+# Use TF Ops to process.\n+def process_input(input, input_size, upscale_factor):\n+    input = tf.image.rgb_to_yuv(input)\n+    last_dimension_axis = len(input.shape) - 1\n+    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n+    return tf.image.resize(y, [input_size, input_size], method=\"area\")\n+\n+\n+def process_target(input):\n+    input = tf.image.rgb_to_yuv(input)\n+    last_dimension_axis = len(input.shape) - 1\n+    y, u, v = tf.split(input, 3, axis=last_dimension_axis)\n+    return y\n+\n+\n+train_ds = train_ds.map(\n+    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n+)\n+train_ds = train_ds.prefetch(buffer_size=32)\n+\n+valid_ds = valid_ds.map(\n+    lambda x: (process_input(x, input_size, upscale_factor), process_target(x))\n+)\n+valid_ds = valid_ds.prefetch(buffer_size=32)\n+\n+\"\"\"\n+Let's take a look at the input and target data.\n+\"\"\"\n+\n+for batch in train_ds.take(1):\n+    for img in batch[0]:\n+        display(array_to_img(img))\n+    for img in batch[1]:\n+        display(array_to_img(img))\n+\n+\"\"\"\n+## Build a model\n+\n+Compared to the paper, we add one more layer and we use the `relu` activation function\n+instead of `tanh`.\n+It achieves better performance even though we train the model for fewer epochs.\n+\"\"\"\n+\n+\n+class DepthToSpace(layers.Layer):\n+    def __init__(self, block_size):\n+        super().__init__()\n+        self.block_size = block_size\n+\n+    def call(self, input):\n+        batch, height, width, depth = ops.shape(input)\n+        depth = depth // (self.block_size**2)\n+\n+        x = ops.reshape(\n+            input, [batch, height, width, self.block_size, self.block_size, depth]\n+        )\n+        x = ops.transpose(x, [0, 1, 3, 2, 4, 5])\n+        x = ops.reshape(\n+            x, [batch, height * self.block_size, width * self.block_size, depth]\n+        )\n+        return x\n+\n+\n+def get_model(upscale_factor=3, channels=1):\n+    conv_args = {\n+        \"activation\": \"relu\",\n+        \"kernel_initializer\": \"orthogonal\",\n+        \"padding\": \"same\",\n+    }\n+    inputs = keras.Input(shape=(None, None, channels))\n+    x = layers.Conv2D(64, 5, **conv_args)(inputs)\n+    x = layers.Conv2D(64, 3, **conv_args)(x)\n+    x = layers.Conv2D(32, 3, **conv_args)(x)\n+    x = layers.Conv2D(channels * (upscale_factor**2), 3, **conv_args)(x)\n+    outputs = DepthToSpace(upscale_factor)(x)\n+\n+    return keras.Model(inputs, outputs)\n+\n+\n+\"\"\"\n+## Define utility functions\n+\n+We need to define several utility functions to monitor our results:\n+\n+- `plot_results` to plot an save an image.\n+- `get_lowres_image` to convert an image to its low-resolution version.\n+- `upscale_image` to turn a low-resolution image to\n+a high-resolution version reconstructed by the model.\n+In this function, we use the `y` channel from the YUV color space\n+as input to the model and then combine the output with the\n+other channels to obtain an RGB image.\n+\"\"\"\n+\n+import matplotlib.pyplot as plt\n+from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n+from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n+import PIL\n+\n+\n+def plot_results(img, prefix, title):\n+    \"\"\"Plot the result with zoom-in area.\"\"\"\n+    img_array = img_to_array(img)\n+    img_array = img_array.astype(\"float32\") / 255.0\n+\n+    # Create a new figure with a default 111 subplot.\n+    fig, ax = plt.subplots()\n+    im = ax.imshow(img_array[::-1], origin=\"lower\")\n+\n+    plt.title(title)\n+    # zoom-factor: 2.0, location: upper-left\n+    axins = zoomed_inset_axes(ax, 2, loc=2)\n+    axins.imshow(img_array[::-1], origin=\"lower\")\n+\n+    # Specify the limits.\n+    x1, x2, y1, y2 = 200, 300, 100, 200\n+    # Apply the x-limits.\n+    axins.set_xlim(x1, x2)\n+    # Apply the y-limits.\n+    axins.set_ylim(y1, y2)\n+\n+    plt.yticks(visible=False)\n+    plt.xticks(visible=False)\n+\n+    # Make the line.\n+    mark_inset(ax, axins, loc1=1, loc2=3, fc=\"none\", ec=\"blue\")\n+    plt.savefig(str(prefix) + \"-\" + title + \".png\")\n+    plt.show()\n+\n+\n+def get_lowres_image(img, upscale_factor):\n+    \"\"\"Return low-resolution image to use as model input.\"\"\"\n+    return img.resize(\n+        (img.size[0] // upscale_factor, img.size[1] // upscale_factor),\n+        PIL.Image.BICUBIC,\n+    )\n+\n+\n+def upscale_image(model, img):\n+    \"\"\"Predict the result based on input image and restore the image as RGB.\"\"\"\n+    ycbcr = img.convert(\"YCbCr\")\n+    y, cb, cr = ycbcr.split()\n+    y = img_to_array(y)\n+    y = y.astype(\"float32\") / 255.0\n+\n+    input = np.expand_dims(y, axis=0)\n+    out = model.predict(input)\n+\n+    out_img_y = out[0]\n+    out_img_y *= 255.0\n+\n+    # Restore the image in RGB color space.\n+    out_img_y = out_img_y.clip(0, 255)\n+    out_img_y = out_img_y.reshape((np.shape(out_img_y)[0], np.shape(out_img_y)[1]))\n+    out_img_y = PIL.Image.fromarray(np.uint8(out_img_y), mode=\"L\")\n+    out_img_cb = cb.resize(out_img_y.size, PIL.Image.BICUBIC)\n+    out_img_cr = cr.resize(out_img_y.size, PIL.Image.BICUBIC)\n+    out_img = PIL.Image.merge(\"YCbCr\", (out_img_y, out_img_cb, out_img_cr)).convert(\n+        \"RGB\"\n+    )\n+    return out_img\n+\n+\n+\"\"\"\n+## Define callbacks to monitor training\n+\n+The `ESPCNCallback` object will compute and display\n+the [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) metric.\n+This is the main metric we use to evaluate super-resolution performance.\n+\"\"\"\n+\n+\n+class ESPCNCallback(keras.callbacks.Callback):\n+    def __init__(self):\n+        super().__init__()\n+        self.test_img = get_lowres_image(load_img(test_img_paths[0]), upscale_factor)\n+\n+    # Store PSNR value in each epoch.\n+    def on_epoch_begin(self, epoch, logs=None):\n+        self.psnr = []\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        print(\"Mean PSNR for epoch: %.2f\" % (np.mean(self.psnr)))\n+        if epoch % 20 == 0:\n+            prediction = upscale_image(self.model, self.test_img)\n+            plot_results(prediction, \"epoch-\" + str(epoch), \"prediction\")\n+\n+    def on_test_batch_end(self, batch, logs=None):\n+        self.psnr.append(10 * math.log10(1 / logs[\"loss\"]))\n+\n+\n+\"\"\"\n+Define `ModelCheckpoint` and `EarlyStopping` callbacks.\n+\"\"\"\n+\n+early_stopping_callback = keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10)\n+\n+checkpoint_filepath = \"/tmp/checkpoint.keras\"\n+\n+model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+    filepath=checkpoint_filepath,\n+    save_weights_only=False,\n+    monitor=\"loss\",\n+    mode=\"min\",\n+    save_best_only=True,\n+)\n+\n+model = get_model(upscale_factor=upscale_factor, channels=1)\n+model.summary()\n+\n+callbacks = [ESPCNCallback(), early_stopping_callback, model_checkpoint_callback]\n+loss_fn = keras.losses.MeanSquaredError()\n+optimizer = keras.optimizers.Adam(learning_rate=0.001)\n+\n+\"\"\"\n+## Train the model\n+\"\"\"\n+\n+epochs = 100\n+\n+model.compile(\n+    optimizer=optimizer,\n+    loss=loss_fn,\n+)\n+\n+model.fit(\n+    train_ds, epochs=epochs, callbacks=callbacks, validation_data=valid_ds, verbose=2\n+)\n+\n+# The model weights (that are considered the best) are loaded into the model.\n+model.load_weights(checkpoint_filepath)\n+\n+\"\"\"\n+## Run model prediction and plot the results\n+\n+Let's compute the reconstructed version of a few images and save the results.\n+\"\"\"\n+\n+total_bicubic_psnr = 0.0\n+total_test_psnr = 0.0\n+\n+for index, test_img_path in enumerate(test_img_paths[50:60]):\n+    img = load_img(test_img_path)\n+    lowres_input = get_lowres_image(img, upscale_factor)\n+    w = lowres_input.size[0] * upscale_factor\n+    h = lowres_input.size[1] * upscale_factor\n+    highres_img = img.resize((w, h))\n+    prediction = upscale_image(model, lowres_input)\n+    lowres_img = lowres_input.resize((w, h))\n+    lowres_img_arr = img_to_array(lowres_img)\n+    highres_img_arr = img_to_array(highres_img)\n+    predict_img_arr = img_to_array(prediction)\n+    bicubic_psnr = tf.image.psnr(lowres_img_arr, highres_img_arr, max_val=255)\n+    test_psnr = tf.image.psnr(predict_img_arr, highres_img_arr, max_val=255)\n+\n+    total_bicubic_psnr += bicubic_psnr\n+    total_test_psnr += test_psnr\n+\n+    print(\n+        \"PSNR of low resolution image and high resolution image is %.4f\" % bicubic_psnr\n+    )\n+    print(\"PSNR of predict and high resolution is %.4f\" % test_psnr)\n+    plot_results(lowres_img, index, \"lowres\")\n+    plot_results(highres_img, index, \"highres\")\n+    plot_results(prediction, index, \"prediction\")\n+\n+print(\"Avg. PSNR of lowres images is %.4f\" % (total_bicubic_psnr / 10))\n+print(\"Avg. PSNR of reconstructions is %.4f\" % (total_test_psnr / 10))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44f1be3525a66891c0faa7a2403a990706badd02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 24 | Files Changed: 2 | Hunks: 23 | Methods Changed: 11 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 70 | Churn Cumulative: 360 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -121,7 +121,7 @@ class TensorLayout:\n         https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n \n     Args:\n-        axes: list of strings that should map to the `axis_names` in\n+        axes: tuple of strings that should map to the `axis_names` in\n             a `DeviceMesh`. For any dimentions that doesn't need any sharding,\n             A `None` can be used a placeholder.\n         device_mesh: Optional `DeviceMesh` that will be used to create\n@@ -130,7 +130,7 @@ class TensorLayout:\n     \"\"\"\n \n     def __init__(self, axes, device_mesh=None):\n-        self._axes = axes\n+        self._axes = tuple(axes)\n         self._device_mesh = device_mesh\n         self._validate_axes()\n \n@@ -327,10 +327,10 @@ class ModelParallel(Distribution):\n     # will be split across 4 devices. Any other variable that doesn't\n     # match any key in the layout map will be fully replicated.\n     layout_map = LayoutMap(device_mesh)\n-    layout_map['.*dense.*kernel'] = TensorLayout([None, 'model'])\n-    layout_map['.*dense.*bias'] = TensorLayout(['model'])\n-    layout_map['.*conv2d.*kernel'] = TensorLayout([None, None, None, 'model'])\n-    layout_map['.*conv2d.*bias'] = TensorLayout(['model'])\n+    layout_map['.*dense.*kernel'] = (None, 'model')\n+    layout_map['.*dense.*bias'] = ('model',)\n+    layout_map['.*conv2d.*kernel'] = (None, None, None, 'model')\n+    layout_map['.*conv2d.*bias'] = ('model',)\n \n     distribution = ModelParallel(device_mesh=device_mesh,\n                                  layout_map=layout_map,\n@@ -410,12 +410,15 @@ class LayoutMap(collections.abc.MutableMapping):\n     In the normal case, the key to query is usually the `variable.path`, which\n     is the idenifier of the variable.\n \n+    As shortcut, tuple or list of axis names are also allowed when inserting\n+    as value, and will be converted to `TensorLayout`.\n+\n     ```python\n     layout_map = LayoutMap(device_mesh=None)\n-    layout_map['.*dense.*kernel'] = layout_2d\n-    layout_map['.*dense.*bias'] = layout_1d\n-    layout_map['.*conv2d.*kernel'] = layout_4d\n-    layout_map['.*conv2d.*bias'] = layout_1d\n+    layout_map['.*dense.*kernel'] = (None, 'model')         # layout_2d\n+    layout_map['.*dense.*bias'] = ('model',)                # layout_1d\n+    layout_map['.*conv2d.*kernel'] = TensorLayout((None, None, None, 'model'))\n+    layout_map['.*conv2d.*bias'] = TensorLayout(('model',))  # layout_1d\n \n     layout_1 = layout_map['dense_1.kernel']             # layout_1 == layout_2d\n     layout_2 = layout_map['dense_1.bias']               # layout_2 == layout_1d\n@@ -459,12 +462,22 @@ class LayoutMap(collections.abc.MutableMapping):\n         return None\n \n     def __setitem__(self, key, layout):\n+        \"\"\"Insert TensorLayout to the LayoutMap.\n+\n+        Args:\n+            key: String key for the `TensorLayout`.\n+            layout: The `TensorLayout`. As a shortcut, tuple of string and None\n+                are also acceptable, and will be converted to `TensorLayout`.\n+        \"\"\"\n         if key in self._layout_map:\n             raise ValueError(\n                 f\"{key} already exist in the LayoutMap with \"\n                 f\"value {self._layout_map[key]}. Please make sure to \"\n                 \"not use duplicated keys.\"\n             )\n+        if isinstance(layout, tuple):\n+            layout = TensorLayout(axes=layout, device_mesh=None)\n+\n         if not isinstance(layout, TensorLayout):\n             raise ValueError(\n                 f\"{layout} should be a TensorLayout type, got {type(layout)}\"\n@@ -491,6 +504,9 @@ class LayoutMap(collections.abc.MutableMapping):\n             layout.device_mesh = self.device_mesh\n \n \n+LayoutMap.get.__doc__ = LayoutMap.__getitem__.__doc__\n+\n+\n @keras_export(\"keras.distribution.distribution\")\n def distribution():\n     \"\"\"Retrieve the current distribution from global context.\"\"\"\n\n@@ -62,21 +62,21 @@ class TensorLayoutTest(testing.TestCase):\n         )\n \n     def test_tensor_layout_creation(self):\n-        axes = [\"data\", None]\n+        axes = (\"data\", None)\n         layout = distribution_lib.TensorLayout(axes, self.mesh)\n \n         self.assertEqual(layout.device_mesh, self.mesh)\n         self.assertEqual(layout.axes, axes)\n \n     def test_tensor_layout_validation(self):\n-        axes = [\"data\", \"unknown\", None]\n+        axes = (\"data\", \"unknown\", None)\n         with self.assertRaisesRegex(\n             ValueError, \"Invalid axis names for Layout\"\n         ):\n             distribution_lib.TensorLayout(axes, self.mesh)\n \n     def test_lazy_device_mesh_injection(self):\n-        axes = [\"data\", None]\n+        axes = (\"data\", None)\n         layout = distribution_lib.TensorLayout(axes, None)\n \n         self.assertIsNone(layout.device_mesh)\n@@ -88,7 +88,7 @@ class TensorLayoutTest(testing.TestCase):\n         self.assertEqual(layout.axes, axes)\n \n     def test_lazy_device_mesh_validation(self):\n-        axes = [\"data\", \"unknown\", None]\n+        axes = (\"data\", \"unknown\", None)\n         layout = distribution_lib.TensorLayout(axes, None)\n \n         self.assertIsNone(layout.device_mesh)\n@@ -180,7 +180,7 @@ class DataParallelDistributionTest(testing.TestCase):\n         data = np.arange(16).reshape((4, 2, 2))\n         data_layout = distribution.get_data_layout(data.shape)\n         self.assertIs(data_layout.device_mesh, self.device_mesh)\n-        self.assertEqual(data_layout.axes, [\"data\", None, None])\n+        self.assertEqual(data_layout.axes, (\"data\", None, None))\n \n     def test_get_variable_layout(self):\n         distribution = distribution_lib.DataParallel(\n@@ -190,7 +190,7 @@ class DataParallelDistributionTest(testing.TestCase):\n         variable = backend.Variable(initializer=[1, 2, 3])\n         variable_layout = distribution.get_variable_layout(variable)\n         self.assertIs(variable_layout.device_mesh, self.device_mesh)\n-        self.assertEqual(variable_layout.axes, [None])\n+        self.assertEqual(variable_layout.axes, (None,))\n \n \n class ModelParallelDistributionTest(testing.TestCase):\n@@ -218,15 +218,15 @@ class ModelParallelDistributionTest(testing.TestCase):\n \n         kernel_layout = distribution.get_variable_layout(kernel)\n         self.assertIs(kernel_layout.device_mesh, self.device_mesh)\n-        self.assertEqual(kernel_layout.axes, [None, \"model\"])\n+        self.assertEqual(kernel_layout.axes, (None, \"model\"))\n \n         bias_layout = distribution.get_variable_layout(bias)\n         self.assertIs(bias_layout.device_mesh, self.device_mesh)\n-        self.assertEqual(bias_layout.axes, [\"model\"])\n+        self.assertEqual(bias_layout.axes, (\"model\",))\n \n         rng_seed_layout = distribution.get_variable_layout(rng_seed)\n         self.assertIs(rng_seed_layout.device_mesh, self.device_mesh)\n-        self.assertEqual(rng_seed_layout.axes, [None])\n+        self.assertEqual(rng_seed_layout.axes, (None,))\n \n     def test_distribute_data(self):\n         layout_map = distribution_lib.LayoutMap(self.device_mesh)\n@@ -237,7 +237,7 @@ class ModelParallelDistributionTest(testing.TestCase):\n         data = np.arange(16).reshape((4, 2, 2))\n         data_layout = distribution.get_data_layout(data.shape)\n         self.assertIs(data_layout.device_mesh, self.device_mesh)\n-        self.assertEqual(data_layout.axes, [\"data\", None, None])\n+        self.assertEqual(data_layout.axes, (\"data\", None, None))\n \n \n class LayoutMapTest(testing.TestCase):\n@@ -260,24 +260,30 @@ class LayoutMapTest(testing.TestCase):\n         layout_map = distribution_lib.LayoutMap(self.device_mesh)\n         layout_map[\"dense/kernel\"] = self.sharded_2d\n         layout_map[\"dense/bias\"] = self.sharded_1d\n+        # Test for adding list/tuple as shortcut for TensorLayout\n+        layout_map[\"conv/bias\"] = (\"model\",)\n \n         # Make there are two items in the map, and we access them via the\n         # underlying container at layout_map._layout_map\n-        self.assertLen(layout_map, 2)\n+        self.assertLen(layout_map, 3)\n \n         kernel_layout = layout_map[\"dense/kernel\"]\n-        self.assertEqual(kernel_layout.axes, [None, \"model\"])\n+        self.assertEqual(kernel_layout.axes, (None, \"model\"))\n         self.assertIs(kernel_layout.device_mesh, self.device_mesh)\n \n         bias_layout = layout_map[\"dense/bias\"]\n-        self.assertEqual(bias_layout.axes, [\"model\"])\n+        self.assertEqual(bias_layout.axes, (\"model\",))\n+        self.assertIs(bias_layout.device_mesh, self.device_mesh)\n+\n+        conv_bias_layout = layout_map[\"conv/bias\"]\n+        self.assertEqual(conv_bias_layout.axes, (\"model\",))\n         self.assertIs(bias_layout.device_mesh, self.device_mesh)\n \n         with self.assertRaisesRegex(ValueError, \"dense/kernel already exist\"):\n             layout_map[\"dense/kernel\"] = self.sharded_2d\n \n         with self.assertRaisesRegex(ValueError, \"should be a TensorLayout\"):\n-            layout_map[\"conv.kernel\"] = [1, 2, 3]\n+            layout_map[\"conv.kernel\"] = [\"a\", \"b\"]\n \n     def test_get(self):\n         layout_map = distribution_lib.LayoutMap(self.device_mesh)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
