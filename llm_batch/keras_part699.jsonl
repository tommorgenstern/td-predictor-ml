{"custom_id": "keras#2f7c8c76cf180d02aa7e8285fef1453eccd338a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 11/5 | Churn Δ: 112 | Churn Cumulative: 7098 | Contributors (this commit): 16 | Commits (past 90d): 128 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -223,6 +223,12 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n     )\n \n \n+def digitize(x, bins):\n+    x = convert_to_tensor(x)\n+    bins = convert_to_tensor(bins)\n+    return cast(jnp.digitize(x, bins), \"int64\")\n+\n+\n def dot(x, y):\n     return jnp.dot(x, y)\n \n\n@@ -215,6 +215,10 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n     )\n \n \n+def digitize(x, bins):\n+    return np.digitize(x, bins)\n+\n+\n def dot(x, y):\n     return np.dot(x, y)\n \n\n@@ -226,6 +226,12 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n     )\n \n \n+def digitize(x, bins):\n+    x = convert_to_tensor(x)\n+    bins = list(bins)\n+    return tf.cast(tf.raw_ops.Bucketize(input=x, boundaries=bins), \"int64\")\n+\n+\n def dot(x, y):\n     return tfnp.dot(x, y)\n \n\n@@ -347,6 +347,12 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n     )\n \n \n+def digitize(x, bins):\n+    x = convert_to_tensor(x)\n+    bins = convert_to_tensor(bins)\n+    return cast(torch.bucketize(x, bins, right=True), \"int32\")\n+\n+\n def dot(x, y):\n     x, y = convert_to_tensor(x), convert_to_tensor(y)\n     if x.ndim == 0 or y.ndim == 0:\n\n@@ -38,6 +38,7 @@ cumsum\n diag\n diagonal\n diff\n+digitize\n divide\n dot\n dtype\n@@ -1631,6 +1632,43 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n     )\n \n \n+class Digitize(Operation):\n+    def call(self, x, bins):\n+        return backend.numpy.digitize(x, bins)\n+\n+    def compute_output_spec(self, x, bins):\n+        bins_shape = bins.shape\n+        if len(bins_shape) > 1:\n+            raise ValueError(\n+                \"`bins` must be an array of one dimension, but recieved `bins` \"\n+                f\"of shape {bins_shape}.\"\n+            )\n+        return KerasTensor(x.shape, dtype=\"int\")\n+\n+\n+@keras_core_export([\"keras_core.ops.digitize\", \"keras_core.ops.numpy.digitize\"])\n+def digitize(x, bins):\n+    \"\"\"Returns the indices of the bins to which each value in `x` belongs.\n+\n+    Args:\n+        x: Input array to be binned.\n+        bins: Array of bins. It has to be one-dimensional and monotonically\n+            increasing.\n+\n+    Returns:\n+        Output array of indices, of same shape as x.\n+\n+    Examples:\n+        >>> x = np.array([0.0, 1.0, 3.0, 1.6])\n+        >>> bins = np.array([0.0, 3.0, 4.5, 7.0])\n+        >>> keras_core.ops.digitize(x, bins)\n+        array([1, 1, 2, 1])\n+    \"\"\"\n+    if any_symbolic_tensors((x, bins)):\n+        return Digitize().symbolic_call(x, bins)\n+    return backend.numpy.digitize(x, bins)\n+\n+\n class Dot(Operation):\n     def call(self, x1, x2):\n         return backend.numpy.dot(x1, x2)\n\n@@ -3,6 +3,7 @@ from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import backend\n from keras_core import testing\n+from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.ops import numpy as knp\n \n@@ -660,6 +661,17 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor([2, 3, 4])\n             knp.logical_xor(x, y)\n \n+    def test_digitize(self):\n+        x = KerasTensor((2, 3))\n+        bins = KerasTensor((3,))\n+        self.assertEqual(knp.digitize(x, bins).shape, (2, 3))\n+        self.assertTrue(knp.digitize(x, bins).dtype == standardize_dtype(\"int\"))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3])\n+            bins = KerasTensor([2, 3, 4])\n+            knp.digitize(x, bins)\n+\n \n class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n     def test_mean(self):\n@@ -2140,6 +2152,46 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(knp.where(x > 1, x, y), np.where(x > 1, x, y))\n         self.assertAllClose(knp.Where()(x > 1, x, y), np.where(x > 1, x, y))\n \n+    def test_digitize(self):\n+        x = np.array([0.0, 1.0, 3.0, 1.6])\n+        bins = np.array([0.0, 3.0, 4.5, 7.0])\n+        self.assertAllClose(knp.digitize(x, bins), np.digitize(x, bins))\n+        self.assertAllClose(knp.Digitize()(x, bins), np.digitize(x, bins))\n+        self.assertTrue(\n+            standardize_dtype(knp.digitize(x, bins).dtype)\n+            == standardize_dtype(\"int\")\n+        )\n+        self.assertTrue(\n+            standardize_dtype(knp.Digitize()(x, bins).dtype)\n+            == standardize_dtype(\"int\")\n+        )\n+\n+        x = np.array([0.2, 6.4, 3.0, 1.6])\n+        bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])\n+        self.assertAllClose(knp.digitize(x, bins), np.digitize(x, bins))\n+        self.assertAllClose(knp.Digitize()(x, bins), np.digitize(x, bins))\n+        self.assertTrue(\n+            standardize_dtype(knp.digitize(x, bins).dtype)\n+            == standardize_dtype(\"int\")\n+        )\n+        self.assertTrue(\n+            standardize_dtype(knp.Digitize()(x, bins).dtype)\n+            == standardize_dtype(\"int\")\n+        )\n+\n+        x = np.array([1, 4, 10, 15])\n+        bins = np.array([4, 10, 14, 15])\n+        self.assertAllClose(knp.digitize(x, bins), np.digitize(x, bins))\n+        self.assertAllClose(knp.Digitize()(x, bins), np.digitize(x, bins))\n+        self.assertTrue(\n+            standardize_dtype(knp.digitize(x, bins).dtype)\n+            == standardize_dtype(\"int\")\n+        )\n+        self.assertTrue(\n+            standardize_dtype(knp.Digitize()(x, bins).dtype)\n+            == standardize_dtype(\"int\")\n+        )\n+\n \n class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_mean(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f4908d244b7c385caa89ee9d91cee27ecd40ea6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 34 | Churn Cumulative: 718 | Contributors (this commit): 7 | Commits (past 90d): 24 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,23 +6,30 @@ from keras_core.random.seed_generator import draw_seed\n from keras_core.random.seed_generator import make_default_seed\n \n \n+def jax_draw_seed(seed):\n+    if isinstance(seed, jax.Array):\n+        return seed\n+    else:\n+        return draw_seed(seed)\n+\n+\n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = draw_seed(seed)\n+    seed = jax_draw_seed(seed)\n     sample = jax.random.normal(seed, shape=shape, dtype=dtype)\n     return sample * stddev + mean\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = draw_seed(seed)\n+    seed = jax_draw_seed(seed)\n     return jax.random.uniform(\n         seed, shape=shape, dtype=dtype, minval=minval, maxval=maxval\n     )\n \n \n def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n-    seed = draw_seed(seed)\n+    seed = jax_draw_seed(seed)\n     output_shape = list(logits.shape)\n     output_shape[1] = num_samples\n     output_shape = tuple(output_shape)\n@@ -33,7 +40,7 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n \n \n def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n-    seed = draw_seed(seed)\n+    seed = jax_draw_seed(seed)\n     return jax.random.randint(\n         seed, shape=shape, dtype=dtype, minval=minval, maxval=maxval\n     )\n@@ -41,7 +48,7 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n-    seed = draw_seed(seed)\n+    seed = jax_draw_seed(seed)\n     sample = jax.random.truncated_normal(\n         seed, shape=shape, lower=-2.0, upper=2.0, dtype=dtype\n     )\n@@ -62,7 +69,7 @@ def _get_concrete_noise_shape(inputs, noise_shape):\n \n \n def dropout(inputs, rate, noise_shape=None, seed=None):\n-    seed = draw_seed(seed)\n+    seed = jax_draw_seed(seed)\n     keep_prob = 1.0 - rate\n     # The `noise_shape` may contain `None` so we need to convert it\n     # into a concrete shape before passing it on to jax.\n\n@@ -138,3 +138,18 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n             inputs, rate=0.3, noise_shape=[None, 3, 5, None], seed=0\n         )\n         self.assertEqual(x.shape, (2, 3, 5, 7))\n+\n+    @pytest.mark.skipif(\n+        keras_core.backend.backend() != \"jax\",\n+        reason=\"This test requires `jax` as the backend.\",\n+    )\n+    def test_jax_rngkey_seed(self):\n+        import jax\n+        import jax.numpy as jnp\n+\n+        seed = 1234\n+        rng = jax.random.PRNGKey(seed)\n+        self.assertEqual(rng.shape, (2,))\n+        self.assertEqual(rng.dtype, jnp.uint32)\n+        x = random.randint((3, 5), 0, 10, seed=rng)\n+        assert isinstance(x, jnp.ndarray)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f2076e1046cab7f34e46bb9ff2ae643379d81a62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 263 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -152,4 +152,4 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(rng.shape, (2,))\n         self.assertEqual(rng.dtype, jnp.uint32)\n         x = random.randint((3, 5), 0, 10, seed=rng)\n-        assert isinstance(x, jnp.ndarray)\n\\ No newline at end of file\n+        self.assertTrue(isinstance(x, jnp.ndarray))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91b56e26a31164f62b91b9bcea35605bc8894cd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 3584 | Contributors (this commit): 14 | Commits (past 90d): 78 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -226,7 +226,7 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n def digitize(x, bins):\n     x = convert_to_tensor(x)\n     bins = convert_to_tensor(bins)\n-    return cast(jnp.digitize(x, bins), \"int64\")\n+    return jnp.digitize(x, bins)\n \n \n def dot(x, y):\n\n@@ -229,7 +229,7 @@ def diagonal(x, offset=0, axis1=0, axis2=1):\n def digitize(x, bins):\n     x = convert_to_tensor(x)\n     bins = list(bins)\n-    return tf.cast(tf.raw_ops.Bucketize(input=x, boundaries=bins), \"int64\")\n+    return tf.raw_ops.Bucketize(input=x, boundaries=bins)\n \n \n def dot(x, y):\n\n@@ -1640,10 +1640,10 @@ class Digitize(Operation):\n         bins_shape = bins.shape\n         if len(bins_shape) > 1:\n             raise ValueError(\n-                \"`bins` must be an array of one dimension, but recieved `bins` \"\n-                f\"of shape {bins_shape}.\"\n+                f\"`bins` must be a 1D array. Received: bins={bins} \"\n+                f\"with shape bins.shape={bins_shape}\"\n             )\n-        return KerasTensor(x.shape, dtype=\"int\")\n+        return KerasTensor(x.shape, dtype=\"int32\")\n \n \n @keras_core_export([\"keras_core.ops.digitize\", \"keras_core.ops.numpy.digitize\"])\n@@ -1658,7 +1658,8 @@ def digitize(x, bins):\n     Returns:\n         Output array of indices, of same shape as x.\n \n-    Examples:\n+    Example:\n+\n     >>> x = np.array([0.0, 1.0, 3.0, 1.6])\n     >>> bins = np.array([0.0, 3.0, 4.5, 7.0])\n     >>> keras_core.ops.digitize(x, bins)\n\n@@ -665,7 +665,7 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor((2, 3))\n         bins = KerasTensor((3,))\n         self.assertEqual(knp.digitize(x, bins).shape, (2, 3))\n-        self.assertTrue(knp.digitize(x, bins).dtype == standardize_dtype(\"int\"))\n+        self.assertTrue(knp.digitize(x, bins).dtype == \"int32\")\n \n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n@@ -2158,12 +2158,10 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(knp.digitize(x, bins), np.digitize(x, bins))\n         self.assertAllClose(knp.Digitize()(x, bins), np.digitize(x, bins))\n         self.assertTrue(\n-            standardize_dtype(knp.digitize(x, bins).dtype)\n-            == standardize_dtype(\"int\")\n+            standardize_dtype(knp.digitize(x, bins).dtype) == \"int32\"\n         )\n         self.assertTrue(\n-            standardize_dtype(knp.Digitize()(x, bins).dtype)\n-            == standardize_dtype(\"int\")\n+            standardize_dtype(knp.Digitize()(x, bins).dtype) == \"int32\"\n         )\n \n         x = np.array([0.2, 6.4, 3.0, 1.6])\n@@ -2171,12 +2169,10 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(knp.digitize(x, bins), np.digitize(x, bins))\n         self.assertAllClose(knp.Digitize()(x, bins), np.digitize(x, bins))\n         self.assertTrue(\n-            standardize_dtype(knp.digitize(x, bins).dtype)\n-            == standardize_dtype(\"int\")\n+            standardize_dtype(knp.digitize(x, bins).dtype) == \"int32\"\n         )\n         self.assertTrue(\n-            standardize_dtype(knp.Digitize()(x, bins).dtype)\n-            == standardize_dtype(\"int\")\n+            standardize_dtype(knp.Digitize()(x, bins).dtype) == \"int32\"\n         )\n \n         x = np.array([1, 4, 10, 15])\n@@ -2184,12 +2180,10 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(knp.digitize(x, bins), np.digitize(x, bins))\n         self.assertAllClose(knp.Digitize()(x, bins), np.digitize(x, bins))\n         self.assertTrue(\n-            standardize_dtype(knp.digitize(x, bins).dtype)\n-            == standardize_dtype(\"int\")\n+            standardize_dtype(knp.digitize(x, bins).dtype) == \"int32\"\n         )\n         self.assertTrue(\n-            standardize_dtype(knp.Digitize()(x, bins).dtype)\n-            == standardize_dtype(\"int\")\n+            standardize_dtype(knp.Digitize()(x, bins).dtype) == \"int32\"\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d36822ec048f64f33fdb168d651149964ff0f18a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 329 | Lines Deleted: 100 | Files Changed: 3 | Hunks: 54 | Methods Changed: 23 | Complexity Δ (Sum/Max): 49/20 | Churn Δ: 429 | Churn Cumulative: 1063 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from numpy.lib.stride_tricks import as_strided\n \n from keras_core import backend\n from keras_core import layers\n@@ -109,9 +109,133 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def _same_padding(self, input_size, pool_size, stride):\n+        if input_size % stride == 0:\n+            return max(pool_size - stride, 0)\n+        else:\n+            return max(pool_size - (input_size % stride), 0)\n+\n+    def _np_avgpool1d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.swapaxes(1, 2)\n+        if isinstance(pool_size, (tuple, list)):\n+            pool_size = pool_size[0]\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, ch_x = x.shape\n+            pad_value = self._same_padding(h_x, pool_size, h_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, pad_value)\n+            x = np.pad(x, pad_width=npad, mode=\"edge\")\n+\n+        n_batch, h_x, ch_x = x.shape\n+        out_h = int((h_x - pool_size) / h_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, ch_x, pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            x.strides[2],\n+            x.strides[1],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.mean(windows, axis=(3,))\n+        if data_format == \"channels_first\":\n+            out = out.swapaxes(1, 2)\n+        return out\n+\n+    def _np_avgpool2d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 1))\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides)\n+\n+        h_pool_size, w_pool_size = pool_size\n+        h_stride, w_stride = strides\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            x = np.pad(x, pad_width=npad, mode=\"edge\")\n+\n+        n_batch, h_x, w_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            x.strides[3],\n+            x.strides[1],\n+            x.strides[2],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.mean(windows, axis=(4, 5))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 3, 1, 2))\n+        return out\n+\n+    def _np_avgpool3d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 4, 1))\n+\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides, strides)\n+\n+        h_pool_size, w_pool_size, d_pool_size = pool_size\n+        h_stride, w_stride, d_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, d_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            npad[3] = (0, d_padding)\n+            x = np.pad(x, pad_width=npad, mode=\"symmetric\")\n+\n+        n_batch, h_x, w_x, d_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+        out_d = int((d_x - d_pool_size) / d_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            d_stride * x.strides[3],\n+            x.strides[4],\n+            x.strides[1],\n+            x.strides[2],\n+            x.strides[3],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.mean(windows, axis=(5, 6, 7))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 4, 1, 2, 3))\n+        return out\n+\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n         ((2,), (2,), \"valid\", \"channels_last\"),\n+        ((2,), (2,), \"valid\", \"channels_first\"),\n     )\n     def test_average_pooling1d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n@@ -122,20 +246,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool1d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n+        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2,), (2,), \"same\", \"channels_last\"),\n+        ((2,), (2,), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -152,15 +273,10 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool1d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n@@ -175,20 +291,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool2d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, (2, 1), \"same\", \"channels_last\"),\n-        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n+        (2, (2, 1), \"same\", \"channels_first\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_last\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -204,20 +317,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool2d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n+        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_first\"),\n     )\n     def test_average_pooling3d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n@@ -228,20 +338,17 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool3d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n+        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n-        ((2, 3, 2), (2, 2, 1), \"same\", \"channels_last\"),\n+        ((2, 2, 2), (2, 2, 1), \"same\", \"channels_last\"),\n+        ((2, 2, 2), (2, 2, 1), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -258,13 +365,8 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_avgpool3d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n\n@@ -1,6 +1,5 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import layers\n@@ -93,61 +92,69 @@ class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_max_pooling1d(self, data_format, keepdims):\n-        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n+        def np_global_max_pool1d(x, data_format, keepdims):\n+            steps_axis = [1] if data_format == \"channels_last\" else [2]\n+            res = np.apply_over_axes(np.max, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n         layer = layers.GlobalMaxPooling1D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalMaxPooling1D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_global_max_pool1d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_max_pooling2d(self, data_format, keepdims):\n-        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n+        def np_global_max_pool2d(x, data_format, keepdims):\n+            steps_axis = [1, 2] if data_format == \"channels_last\" else [2, 3]\n+            res = np.apply_over_axes(np.max, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n         layer = layers.GlobalMaxPooling2D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalMaxPooling2D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_global_max_pool2d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n+        (\"channels_first\", True),\n     )\n     def test_global_max_pooling3d(self, data_format, keepdims):\n-        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n+        def np_global_max_pool3d(x, data_format, keepdims):\n+            steps_axis = (\n+                [1, 2, 3] if data_format == \"channels_last\" else [2, 3, 4]\n+            )\n+            res = np.apply_over_axes(np.max, x, steps_axis)\n+            if not keepdims:\n+                res = res.squeeze()\n+            return res\n \n+        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n         layer = layers.GlobalMaxPooling3D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n-        tf_keras_layer = tf.keras.layers.GlobalMaxPooling3D(\n-            data_format=data_format,\n-            keepdims=keepdims,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = np_global_max_pool3d(inputs, data_format, keepdims)\n         self.assertAllClose(outputs, expected)\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n-import tensorflow as tf\n from absl.testing import parameterized\n+from numpy.lib.stride_tricks import as_strided\n \n from keras_core import layers\n from keras_core import testing\n@@ -108,10 +108,141 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def _same_padding(self, input_size, pool_size, stride):\n+        if input_size % stride == 0:\n+            return max(pool_size - stride, 0)\n+        else:\n+            return max(pool_size - (input_size % stride), 0)\n+\n+    def _np_maxpool1d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.swapaxes(1, 2)\n+        if isinstance(pool_size, (tuple, list)):\n+            pool_size = pool_size[0]\n+        if isinstance(strides, (tuple, list)):\n+            h_stride = strides[0]\n+        else:\n+            h_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, ch_x = x.shape\n+            pad_value = self._same_padding(h_x, pool_size, h_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, pad_value)\n+            x = np.pad(\n+                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n+            )\n+\n+        n_batch, h_x, ch_x = x.shape\n+        out_h = int((h_x - pool_size) / h_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, ch_x, pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            x.strides[2],\n+            x.strides[1],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.max(windows, axis=(3,))\n+        if data_format == \"channels_first\":\n+            out = out.swapaxes(1, 2)\n+        return out\n+\n+    def _np_maxpool2d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 1))\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides)\n+\n+        h_pool_size, w_pool_size = pool_size\n+        h_stride, w_stride = strides\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            x = np.pad(\n+                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n+            )\n+\n+        n_batch, h_x, w_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            x.strides[3],\n+            x.strides[1],\n+            x.strides[2],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.max(windows, axis=(4, 5))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 3, 1, 2))\n+        return out\n+\n+    def _np_maxpool3d(self, x, pool_size, strides, padding, data_format):\n+        if data_format == \"channels_first\":\n+            x = x.transpose((0, 2, 3, 4, 1))\n+\n+        if isinstance(pool_size, int):\n+            pool_size = (pool_size, pool_size, pool_size)\n+        if isinstance(strides, int):\n+            strides = (strides, strides, strides)\n+\n+        h_pool_size, w_pool_size, d_pool_size = pool_size\n+        h_stride, w_stride, d_stride = strides\n+\n+        if padding == \"same\":\n+            n_batch, h_x, w_x, d_x, ch_x = x.shape\n+            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n+            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n+            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n+            npad = [(0, 0)] * x.ndim\n+            npad[1] = (0, h_padding)\n+            npad[2] = (0, w_padding)\n+            npad[3] = (0, d_padding)\n+            x = np.pad(\n+                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n+            )\n+\n+        n_batch, h_x, w_x, d_x, ch_x = x.shape\n+        out_h = int((h_x - h_pool_size) / h_stride) + 1\n+        out_w = int((w_x - w_pool_size) / w_stride) + 1\n+        out_d = int((d_x - d_pool_size) / d_stride) + 1\n+\n+        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n+        strides = (\n+            x.strides[0],\n+            h_stride * x.strides[1],\n+            w_stride * x.strides[2],\n+            d_stride * x.strides[3],\n+            x.strides[4],\n+            x.strides[1],\n+            x.strides[2],\n+            x.strides[3],\n+        )\n+        windows = as_strided(x, shape=stride_shape, strides=strides)\n+        out = np.max(windows, axis=(5, 6, 7))\n+        if data_format == \"channels_first\":\n+            out = out.transpose((0, 4, 1, 2, 3))\n+        return out\n+\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n+        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2,), (2,), \"valid\", \"channels_last\"),\n+        ((2,), (2,), \"valid\", \"channels_first\"),\n     )\n     def test_max_pooling1d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n@@ -122,23 +253,21 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.MaxPool1D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_maxpool1d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n-        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n+        (2, 1, \"valid\", \"channels_first\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_last\"),\n+        ((2, 2), (2, 2), \"same\", \"channels_first\"),\n+        ((2, 3), (3, 3), \"same\", \"channels_last\"),\n     )\n     def test_max_pooling2d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(300, dtype=\"float32\").reshape((3, 5, 5, 4))\n+        inputs = np.arange(100, dtype=\"float32\").reshape((1, 5, 5, 4))\n \n         layer = layers.MaxPooling2D(\n             pool_size=pool_size,\n@@ -146,21 +275,17 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.MaxPool2D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_maxpool2d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n+        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_first\"),\n     )\n     def test_max_pooling3d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n@@ -171,13 +296,8 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        tf_keras_layer = tf.keras.layers.MaxPool3D(\n-            pool_size=pool_size,\n-            strides=strides,\n-            padding=padding,\n-            data_format=data_format,\n-        )\n-\n         outputs = layer(inputs)\n-        expected = tf_keras_layer(inputs)\n+        expected = self._np_maxpool3d(\n+            inputs, pool_size, strides, padding, data_format\n+        )\n         self.assertAllClose(outputs, expected)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eee0bf6fe61a3e2acf293e97e85cc4c4ea593fd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 1639 | Contributors (this commit): 9 | Commits (past 90d): 22 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1129,7 +1129,7 @@ def average(x, axis=None, weights=None):\n     Args:\n         x: Input tensor.\n         axis: Integer along which to average `x`. The default, `axis=None`,\n-            will average over all of the elements of the input tendor. If axis\n+            will average over all of the elements of the input tensor. If axis\n             is negative it counts from the last to the first axis.\n         weights: Tensor of wieghts associated with the values in `x`. Each\n             value in `x` contributes to the average according to its\n@@ -1198,6 +1198,40 @@ class Bincount(Operation):\n \n @keras_core_export([\"keras_core.ops.bincount\", \"keras_core.ops.numpy.bincount\"])\n def bincount(x, weights=None, minlength=0):\n+    \"\"\"Count the number of occurrences of each value in a tensor of integers.\n+\n+    Args:\n+        x: Input tensor.\n+            It must be of dimension 1, and it must only contain non-negative\n+            integer(s).\n+        weights: Weight tensor.\n+            It must have the same length as `x`. The default value is `None`.\n+            If specified, `x` is weighted by it, i.e. if `n = x[i]`,\n+            `out[n] += weight[i]` instead of the default behavior `out[n] += 1`.\n+        minlength: An integer.\n+            The default value is 0. If specified, there will be at least\n+            this number of bins in the output tensor. If greater than\n+            `max(x) + 1`, each value of the output at an index higher than\n+            `max(x)` is set to 0.\n+\n+    Returns:\n+        1D tensor where each element gives the number of occurrence(s) of its\n+        index value in x. Its length is the maximum between `max(x) + 1` and\n+        minlength.\n+\n+    Examples:\n+    >>> x = keras_core.ops.array([1, 2, 2, 3], dtype=\"uint8\")\n+    >>> keras_core.ops.bincount(x)\n+    array([0, 1, 2, 1], dtype=int32)\n+    >>> weights = x / 2\n+    >>> weights\n+    array([0.5, 1., 1., 1.5], dtype=float64)\n+    >>> keras_core.ops.bincount(x, weights=weights)\n+    array([0., 0.5, 2., 1.5], dtype=float64)\n+    >>> minlength = (keras_core.ops.max(x).numpy() + 1) + 2 # 6\n+    >>> keras_core.ops.bincount(x, minlength=minlength)\n+    array([0, 1, 2, 1, 0, 0], dtype=int32)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Bincount(weights=weights, minlength=minlength).symbolic_call(x)\n     return backend.numpy.bincount(x, weights=weights, minlength=minlength)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ece81fca47997803c362f24072269a19b7839524", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 248 | Lines Deleted: 411 | Files Changed: 10 | Hunks: 86 | Methods Changed: 50 | Complexity Δ (Sum/Max): -45/5 | Churn Δ: 659 | Churn Cumulative: 5219 | Contributors (this commit): 21 | Commits (past 90d): 94 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,19 +20,6 @@ def get_global_attribute(name, default=None, set_to_default=False):\n     return attr\n \n \n-def set_global_setting(name, value):\n-    setattr(GLOBAL_SETTINGS_TRACKER, name, value)\n-\n-\n-def get_global_setting(name, default=None, set_to_default=False):\n-    attr = getattr(GLOBAL_SETTINGS_TRACKER, name, None)\n-    if attr is None and default is not None:\n-        attr = default\n-        if set_to_default:\n-            set_global_setting(name, attr)\n-    return attr\n-\n-\n @keras_core_export(\n     [\"keras_core.utils.clear_session\", \"keras_core.backend.clear_session\"]\n )\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n from absl.testing import parameterized\n-from numpy.lib.stride_tricks import as_strided\n \n from keras_core import backend\n from keras_core import layers\n@@ -109,133 +109,9 @@ class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    def _same_padding(self, input_size, pool_size, stride):\n-        if input_size % stride == 0:\n-            return max(pool_size - stride, 0)\n-        else:\n-            return max(pool_size - (input_size % stride), 0)\n-\n-    def _np_avgpool1d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.swapaxes(1, 2)\n-        if isinstance(pool_size, (tuple, list)):\n-            pool_size = pool_size[0]\n-        if isinstance(strides, (tuple, list)):\n-            h_stride = strides[0]\n-        else:\n-            h_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, ch_x = x.shape\n-            pad_value = self._same_padding(h_x, pool_size, h_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, pad_value)\n-            x = np.pad(x, pad_width=npad, mode=\"edge\")\n-\n-        n_batch, h_x, ch_x = x.shape\n-        out_h = int((h_x - pool_size) / h_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, ch_x, pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            x.strides[2],\n-            x.strides[1],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.mean(windows, axis=(3,))\n-        if data_format == \"channels_first\":\n-            out = out.swapaxes(1, 2)\n-        return out\n-\n-    def _np_avgpool2d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 1))\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides)\n-\n-        h_pool_size, w_pool_size = pool_size\n-        h_stride, w_stride = strides\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            x = np.pad(x, pad_width=npad, mode=\"edge\")\n-\n-        n_batch, h_x, w_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            x.strides[3],\n-            x.strides[1],\n-            x.strides[2],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.mean(windows, axis=(4, 5))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 3, 1, 2))\n-        return out\n-\n-    def _np_avgpool3d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 4, 1))\n-\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides, strides)\n-\n-        h_pool_size, w_pool_size, d_pool_size = pool_size\n-        h_stride, w_stride, d_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, d_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            npad[3] = (0, d_padding)\n-            x = np.pad(x, pad_width=npad, mode=\"symmetric\")\n-\n-        n_batch, h_x, w_x, d_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-        out_d = int((d_x - d_pool_size) / d_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            d_stride * x.strides[3],\n-            x.strides[4],\n-            x.strides[1],\n-            x.strides[2],\n-            x.strides[3],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.mean(windows, axis=(5, 6, 7))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 4, 1, 2, 3))\n-        return out\n-\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n-        (2, 1, \"valid\", \"channels_first\"),\n         ((2,), (2,), \"valid\", \"channels_last\"),\n-        ((2,), (2,), \"valid\", \"channels_first\"),\n     )\n     def test_average_pooling1d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n@@ -246,17 +122,20 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_avgpool1d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n-        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2,), (2,), \"same\", \"channels_last\"),\n-        ((2,), (2,), \"same\", \"channels_first\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -273,10 +152,15 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_avgpool1d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n@@ -291,17 +175,20 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_avgpool2d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, (2, 1), \"same\", \"channels_last\"),\n-        (2, (2, 1), \"same\", \"channels_first\"),\n-        ((2, 2), (2, 2), \"same\", \"channels_last\"),\n-        ((2, 2), (2, 2), \"same\", \"channels_first\"),\n+        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -317,17 +204,20 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_avgpool2d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n-        (2, 1, \"valid\", \"channels_first\"),\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n-        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_first\"),\n     )\n     def test_average_pooling3d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n@@ -338,17 +228,20 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_avgpool3d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n-        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n-        ((2, 2, 2), (2, 2, 1), \"same\", \"channels_last\"),\n-        ((2, 2, 2), (2, 2, 1), \"same\", \"channels_first\"),\n+        ((2, 3, 2), (2, 2, 1), \"same\", \"channels_last\"),\n     )\n     @pytest.mark.skipif(\n         backend.backend() == \"torch\",\n@@ -365,8 +258,13 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_avgpool3d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n\n@@ -1,5 +1,6 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n from absl.testing import parameterized\n \n from keras_core import layers\n@@ -92,69 +93,61 @@ class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n-        (\"channels_first\", True),\n     )\n     def test_global_max_pooling1d(self, data_format, keepdims):\n-        def np_global_max_pool1d(x, data_format, keepdims):\n-            steps_axis = [1] if data_format == \"channels_last\" else [2]\n-            res = np.apply_over_axes(np.max, x, steps_axis)\n-            if not keepdims:\n-                res = res.squeeze()\n-            return res\n-\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n+\n         layer = layers.GlobalMaxPooling1D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n+        tf_keras_layer = tf.keras.layers.GlobalMaxPooling1D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n         outputs = layer(inputs)\n-        expected = np_global_max_pool1d(inputs, data_format, keepdims)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n-        (\"channels_first\", True),\n     )\n     def test_global_max_pooling2d(self, data_format, keepdims):\n-        def np_global_max_pool2d(x, data_format, keepdims):\n-            steps_axis = [1, 2] if data_format == \"channels_last\" else [2, 3]\n-            res = np.apply_over_axes(np.max, x, steps_axis)\n-            if not keepdims:\n-                res = res.squeeze()\n-            return res\n-\n         inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n+\n         layer = layers.GlobalMaxPooling2D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n+        tf_keras_layer = tf.keras.layers.GlobalMaxPooling2D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n         outputs = layer(inputs)\n-        expected = np_global_max_pool2d(inputs, data_format, keepdims)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (\"channels_last\", False),\n         (\"channels_last\", True),\n         (\"channels_first\", False),\n-        (\"channels_first\", True),\n     )\n     def test_global_max_pooling3d(self, data_format, keepdims):\n-        def np_global_max_pool3d(x, data_format, keepdims):\n-            steps_axis = (\n-                [1, 2, 3] if data_format == \"channels_last\" else [2, 3, 4]\n-            )\n-            res = np.apply_over_axes(np.max, x, steps_axis)\n-            if not keepdims:\n-                res = res.squeeze()\n-            return res\n-\n         inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n+\n         layer = layers.GlobalMaxPooling3D(\n             data_format=data_format,\n             keepdims=keepdims,\n         )\n+        tf_keras_layer = tf.keras.layers.GlobalMaxPooling3D(\n+            data_format=data_format,\n+            keepdims=keepdims,\n+        )\n+\n         outputs = layer(inputs)\n-        expected = np_global_max_pool3d(inputs, data_format, keepdims)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n\n@@ -1,7 +1,7 @@\n import numpy as np\n import pytest\n+import tensorflow as tf\n from absl.testing import parameterized\n-from numpy.lib.stride_tricks import as_strided\n \n from keras_core import layers\n from keras_core import testing\n@@ -108,141 +108,10 @@ class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n \n \n class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n-    def _same_padding(self, input_size, pool_size, stride):\n-        if input_size % stride == 0:\n-            return max(pool_size - stride, 0)\n-        else:\n-            return max(pool_size - (input_size % stride), 0)\n-\n-    def _np_maxpool1d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.swapaxes(1, 2)\n-        if isinstance(pool_size, (tuple, list)):\n-            pool_size = pool_size[0]\n-        if isinstance(strides, (tuple, list)):\n-            h_stride = strides[0]\n-        else:\n-            h_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, ch_x = x.shape\n-            pad_value = self._same_padding(h_x, pool_size, h_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, pad_value)\n-            x = np.pad(\n-                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n-            )\n-\n-        n_batch, h_x, ch_x = x.shape\n-        out_h = int((h_x - pool_size) / h_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, ch_x, pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            x.strides[2],\n-            x.strides[1],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.max(windows, axis=(3,))\n-        if data_format == \"channels_first\":\n-            out = out.swapaxes(1, 2)\n-        return out\n-\n-    def _np_maxpool2d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 1))\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides)\n-\n-        h_pool_size, w_pool_size = pool_size\n-        h_stride, w_stride = strides\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            x = np.pad(\n-                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n-            )\n-\n-        n_batch, h_x, w_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            x.strides[3],\n-            x.strides[1],\n-            x.strides[2],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.max(windows, axis=(4, 5))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 3, 1, 2))\n-        return out\n-\n-    def _np_maxpool3d(self, x, pool_size, strides, padding, data_format):\n-        if data_format == \"channels_first\":\n-            x = x.transpose((0, 2, 3, 4, 1))\n-\n-        if isinstance(pool_size, int):\n-            pool_size = (pool_size, pool_size, pool_size)\n-        if isinstance(strides, int):\n-            strides = (strides, strides, strides)\n-\n-        h_pool_size, w_pool_size, d_pool_size = pool_size\n-        h_stride, w_stride, d_stride = strides\n-\n-        if padding == \"same\":\n-            n_batch, h_x, w_x, d_x, ch_x = x.shape\n-            h_padding = self._same_padding(h_x, h_pool_size, h_stride)\n-            w_padding = self._same_padding(w_x, w_pool_size, w_stride)\n-            d_padding = self._same_padding(d_x, d_pool_size, d_stride)\n-            npad = [(0, 0)] * x.ndim\n-            npad[1] = (0, h_padding)\n-            npad[2] = (0, w_padding)\n-            npad[3] = (0, d_padding)\n-            x = np.pad(\n-                x, pad_width=npad, mode=\"constant\", constant_values=-np.inf\n-            )\n-\n-        n_batch, h_x, w_x, d_x, ch_x = x.shape\n-        out_h = int((h_x - h_pool_size) / h_stride) + 1\n-        out_w = int((w_x - w_pool_size) / w_stride) + 1\n-        out_d = int((d_x - d_pool_size) / d_stride) + 1\n-\n-        stride_shape = (n_batch, out_h, out_w, out_d, ch_x, *pool_size)\n-        strides = (\n-            x.strides[0],\n-            h_stride * x.strides[1],\n-            w_stride * x.strides[2],\n-            d_stride * x.strides[3],\n-            x.strides[4],\n-            x.strides[1],\n-            x.strides[2],\n-            x.strides[3],\n-        )\n-        windows = as_strided(x, shape=stride_shape, strides=strides)\n-        out = np.max(windows, axis=(5, 6, 7))\n-        if data_format == \"channels_first\":\n-            out = out.transpose((0, 4, 1, 2, 3))\n-        return out\n-\n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n-        (2, 1, \"valid\", \"channels_first\"),\n-        (2, 1, \"same\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2,), (2,), \"valid\", \"channels_last\"),\n-        ((2,), (2,), \"valid\", \"channels_first\"),\n     )\n     def test_max_pooling1d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n@@ -253,21 +122,23 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_maxpool1d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.MaxPool1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n-        (2, 1, \"valid\", \"channels_first\"),\n-        ((2, 2), (2, 2), \"same\", \"channels_last\"),\n-        ((2, 2), (2, 2), \"same\", \"channels_first\"),\n-        ((2, 3), (3, 3), \"same\", \"channels_last\"),\n+        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n     )\n     def test_max_pooling2d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(100, dtype=\"float32\").reshape((1, 5, 5, 4))\n+        inputs = np.arange(300, dtype=\"float32\").reshape((3, 5, 5, 4))\n \n         layer = layers.MaxPooling2D(\n             pool_size=pool_size,\n@@ -275,17 +146,21 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_maxpool2d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.MaxPool2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n \n     @parameterized.parameters(\n         (2, 1, \"valid\", \"channels_last\"),\n         (2, 1, \"same\", \"channels_first\"),\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n-        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_first\"),\n     )\n     def test_max_pooling3d(self, pool_size, strides, padding, data_format):\n         inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n@@ -296,8 +171,13 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             padding=padding,\n             data_format=data_format,\n         )\n-        outputs = layer(inputs)\n-        expected = self._np_maxpool3d(\n-            inputs, pool_size, strides, padding, data_format\n+        tf_keras_layer = tf.keras.layers.MaxPool3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n         )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n         self.assertAllClose(outputs, expected)\n\n@@ -164,7 +164,7 @@ def set_dtype_policy(policy):\n                 f\"instance. Received: policy={policy} \"\n                 f\"(of type {type(policy)})\"\n             )\n-    global_state.set_global_setting(\"dtype_policy\", policy)\n+    global_state.set_global_attribute(\"dtype_policy\", policy)\n \n \n @keras_core_export(\n@@ -175,7 +175,7 @@ def set_dtype_policy(policy):\n )\n def dtype_policy():\n     \"\"\"Returns the current default dtype policy object.\"\"\"\n-    policy = global_state.get_global_setting(\"dtype_policy\", None)\n+    policy = global_state.get_global_attribute(\"dtype_policy\", None)\n     if policy is None:\n         policy = DTypePolicy(backend.floatx())\n         set_dtype_policy(policy)\n\n@@ -1129,7 +1129,7 @@ def average(x, axis=None, weights=None):\n     Args:\n         x: Input tensor.\n         axis: Integer along which to average `x`. The default, `axis=None`,\n-            will average over all of the elements of the input tensor. If axis\n+            will average over all of the elements of the input tendor. If axis\n             is negative it counts from the last to the first axis.\n         weights: Tensor of wieghts associated with the values in `x`. Each\n             value in `x` contributes to the average according to its\n@@ -1198,40 +1198,6 @@ class Bincount(Operation):\n \n @keras_core_export([\"keras_core.ops.bincount\", \"keras_core.ops.numpy.bincount\"])\n def bincount(x, weights=None, minlength=0):\n-    \"\"\"Count the number of occurrences of each value in a tensor of integers.\n-\n-    Args:\n-        x: Input tensor.\n-            It must be of dimension 1, and it must only contain non-negative\n-            integer(s).\n-        weights: Weight tensor.\n-            It must have the same length as `x`. The default value is `None`.\n-            If specified, `x` is weighted by it, i.e. if `n = x[i]`,\n-            `out[n] += weight[i]` instead of the default behavior `out[n] += 1`.\n-        minlength: An integer.\n-            The default value is 0. If specified, there will be at least\n-            this number of bins in the output tensor. If greater than\n-            `max(x) + 1`, each value of the output at an index higher than\n-            `max(x)` is set to 0.\n-\n-    Returns:\n-        1D tensor where each element gives the number of occurrence(s) of its\n-        index value in x. Its length is the maximum between `max(x) + 1` and\n-        minlength.\n-\n-    Examples:\n-    >>> x = keras_core.ops.array([1, 2, 2, 3], dtype=\"uint8\")\n-    >>> keras_core.ops.bincount(x)\n-    array([0, 1, 2, 1], dtype=int32)\n-    >>> weights = x / 2\n-    >>> weights\n-    array([0.5, 1., 1., 1.5], dtype=float64)\n-    >>> keras_core.ops.bincount(x, weights=weights)\n-    array([0., 0.5, 2., 1.5], dtype=float64)\n-    >>> minlength = (keras_core.ops.max(x).numpy() + 1) + 2 # 6\n-    >>> keras_core.ops.bincount(x, minlength=minlength)\n-    array([0, 1, 2, 1, 0, 0], dtype=int32)\n-    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Bincount(weights=weights, minlength=minlength).symbolic_call(x)\n     return backend.numpy.bincount(x, weights=weights, minlength=minlength)\n\n@@ -4,9 +4,10 @@ from absl.testing import parameterized\n \n import keras_core\n from keras_core import backend\n+from keras_core import ops\n from keras_core import testing\n-from keras_core.ops import numpy as knp\n from keras_core.random import random\n+from keras_core.random import seed_generator\n \n \n class RandomTest(testing.TestCase, parameterized.TestCase):\n@@ -37,8 +38,8 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         res = random.uniform(shape, minval=minval, maxval=maxval, seed=seed)\n         self.assertEqual(res.shape, shape)\n         self.assertEqual(res.shape, np_res.shape)\n-        self.assertLessEqual(knp.max(res), maxval)\n-        self.assertGreaterEqual(knp.max(res), minval)\n+        self.assertLessEqual(ops.max(res), maxval)\n+        self.assertGreaterEqual(ops.max(res), minval)\n \n     @parameterized.parameters(\n         {\"seed\": 10, \"num_samples\": 1, \"batch_size\": 1},\n@@ -79,8 +80,8 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(res.shape, shape)\n         self.assertEqual(res.shape, np_res.shape)\n-        self.assertLessEqual(knp.max(res), max)\n-        self.assertGreaterEqual(knp.max(res), min)\n+        self.assertLessEqual(ops.max(res), max)\n+        self.assertGreaterEqual(ops.max(res), min)\n         # Torch has incomplete dtype support for uints; will remap some dtypes.\n         if keras_core.backend.backend() != \"torch\":\n             self.assertEqual(backend.standardize_dtype(res.dtype), dtype)\n@@ -102,15 +103,15 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(res.shape, tuple(shape))\n         self.assertEqual(res.shape, np_res.shape)\n-        self.assertLessEqual(knp.max(res), mean + 2 * stddev)\n-        self.assertGreaterEqual(knp.max(res), mean - 2 * stddev)\n+        self.assertLessEqual(ops.max(res), mean + 2 * stddev)\n+        self.assertGreaterEqual(ops.max(res), mean - 2 * stddev)\n \n     def test_dropout(self):\n-        x = knp.ones((3, 5))\n+        x = ops.ones((3, 5))\n         self.assertAllClose(random.dropout(x, rate=0, seed=0), x)\n         x_res = random.dropout(x, rate=0.8, seed=0)\n-        self.assertGreater(knp.max(x_res), knp.max(x))\n-        self.assertGreater(knp.sum(x_res == 0), 2)\n+        self.assertGreater(ops.max(x_res), ops.max(x))\n+        self.assertGreater(ops.sum(x_res == 0), 2)\n \n     @pytest.mark.skipif(\n         keras_core.backend.backend() != \"jax\",\n@@ -120,7 +121,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         import jax\n         import jax.numpy as jnp\n \n-        x = knp.ones(3)\n+        x = ops.ones(3)\n \n         @jax.jit\n         def train_step(x):\n@@ -128,12 +129,11 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n                 x = keras_core.layers.Dropout(rate=0.1)(x, training=True)\n             return x\n \n-        keras_core.utils.traceback_utils.disable_traceback_filtering()\n         x = train_step(x)\n-        assert isinstance(x, jnp.ndarray)\n+        self.assertTrue(isinstance(x, jnp.ndarray))\n \n     def test_dropout_noise_shape(self):\n-        inputs = knp.ones((2, 3, 5, 7))\n+        inputs = ops.ones((2, 3, 5, 7))\n         x = random.dropout(\n             inputs, rate=0.3, noise_shape=[None, 3, 5, None], seed=0\n         )\n@@ -153,3 +153,47 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(rng.dtype, jnp.uint32)\n         x = random.randint((3, 5), 0, 10, seed=rng)\n         self.assertTrue(isinstance(x, jnp.ndarray))\n+\n+    def test_global_seed_generator(self):\n+        # Check that unseeded RNG calls use and update global_rng_state()\n+        # and that this works across traced function boundaries\n+\n+        def random_numbers(seed):\n+            rng_state = seed_generator.global_rng_state()\n+            rng_state.assign(seed)\n+            x = random.normal((), seed=None)\n+            y = random.normal((), seed=None)\n+            return x, y, rng_state.value\n+\n+        if backend.backend() == \"jax\":\n+            import jax\n+\n+            random_numbers = jax.jit(random_numbers)\n+        elif backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            random_numbers = tf.function(jit_compile=True)(random_numbers)\n+\n+        seed = ops.zeros((2,))\n+        seed0 = ops.convert_to_numpy(seed)\n+        x1, y1, seed = random_numbers(seed)\n+        seed1 = ops.convert_to_numpy(seed)\n+        x2, y2, seed = random_numbers(seed)\n+        seed2 = ops.convert_to_numpy(seed)\n+        x3, y3, seed = random_numbers(seed)\n+        seed3 = ops.convert_to_numpy(seed)\n+\n+        self.assertNotEqual(seed0[1], seed1[1])\n+        self.assertNotEqual(seed1[1], seed2[1])\n+        self.assertNotEqual(seed2[1], seed3[1])\n+\n+        self.assertGreater(np.abs(x1 - y1), 1e-4)\n+        self.assertGreater(np.abs(x1 - y1), 1e-4)\n+        self.assertGreater(np.abs(x2 - y2), 1e-4)\n+        self.assertGreater(np.abs(x3 - y3), 1e-4)\n+        self.assertGreater(np.abs(x1 - x2), 1e-4)\n+        self.assertGreater(np.abs(x1 - x3), 1e-4)\n+        self.assertGreater(np.abs(x2 - x3), 1e-4)\n+        self.assertGreater(np.abs(y1 - y2), 1e-4)\n+        self.assertGreater(np.abs(y1 - y3), 1e-4)\n+        self.assertGreater(np.abs(y2 - y3), 1e-4)\n\n@@ -4,6 +4,7 @@ import numpy as np\n \n from keras_core import backend\n from keras_core.api_export import keras_core_export\n+from keras_core.backend.common import global_state\n \n \n @keras_core_export(\"keras_core.random.SeedGenerator\")\n@@ -15,7 +16,7 @@ class SeedGenerator:\n     (such as `seed=42`), they will return the same values at each call.\n     In order to get different values at each call, you must use a\n     `SeedGenerator` instead as the seed argument. The `SeedGenerator`\n-    instead is stateful.\n+    object is stateful.\n \n     Example:\n \n@@ -24,9 +25,25 @@ class SeedGenerator:\n     values = keras_core.random.normal(shape=(2, 3), seed=seed_gen)\n     new_values = keras_core.random.normal(shape=(2, 3), seed=seed_gen)\n     ```\n+\n+    Usage in a layer:\n+\n+    ```python\n+    class Dropout(keras_core.Layer):\n+        def __init__(self, **kwargs):\n+            super().__init__(**kwargs)\n+            self.seed_generator = keras_core.random.SeedGenerator(1337)\n+\n+        def call(self, x, training=False):\n+            if training:\n+                return keras_core.random.dropout(\n+                    x, rate=0.5, seed=self.seed_generator\n+                )\n+            return x\n+    ```\n     \"\"\"\n \n-    def __init__(self, seed, **kwargs):\n+    def __init__(self, seed=None, **kwargs):\n         custom_backend = kwargs.pop(\"backend\", None)\n         if kwargs:\n             raise ValueError(f\"Unrecognized keyword arguments: {kwargs}\")\n@@ -56,6 +73,63 @@ class SeedGenerator:\n             name=\"seed_generator_state\",\n         )\n \n+    def next(self, ordered=True):\n+        seed_state = self.state\n+        # Use * 1 to create a copy\n+        new_seed_value = seed_state.value * 1\n+        if ordered:\n+            increment = self.backend.convert_to_tensor(\n+                np.array([0, 1]), dtype=\"uint32\"\n+            )\n+            self.state.assign(seed_state + increment)\n+        else:\n+            # This produces a sequence of near-unique numbers\n+            # between 0 and 1M\n+            self.state.assign((seed_state + 1) * 5387 % 933199)\n+        return new_seed_value\n+\n+\n+def global_seed_generator():\n+    gen = global_state.get_global_attribute(\"global_seed_generator\")\n+    if gen is None:\n+        gen = SeedGenerator()\n+        global_state.set_global_attribute(\"global_seed_generator\", gen)\n+    return gen\n+\n+\n+@keras_core_export(\"keras_core.random.global_rng_state\")\n+def global_rng_state():\n+    \"\"\"Returns the state variable for the default global RNG.\n+\n+    Returns:\n+        A `KerasVariable` with shape `(2,)` and dtype `uint32`.\n+\n+    This is the global state used by unseeded (`seed=None`) Keras\n+    random ops, e.g. `keras_core.random.normal(shape=(), seed=None)`.\n+\n+    In JAX, if you're using unseeded random ops, be mindful that\n+    their outputs will be unchanged across different calls of\n+    traced function (e.g. a `jax.jit`-transformed function) since\n+    traced functions in JAX are fully stateless. To get\n+    different outputs across different calls, you will need to pass the\n+    global RNG state in and out of the function boundary, like this:\n+\n+    ```python\n+    @jax.jit\n+    def random_numbers(seed):\n+        rng_state = keras_core.random.global_rng_state()\n+        rng_state.assign(seed)\n+        x = keras_core.random.normal((), seed=None)\n+        y = keras_core.random.normal((), seed=None)\n+        return x, y, rng_state.value\n+\n+    seed = jax.numpy.array([0, 0])\n+    x, y, seed = random_numbers(seed)\n+    new_x, new_y, seed = random_numbers(seed)\n+    ```\n+    \"\"\"\n+    return global_seed_generator().state\n+\n \n def make_default_seed():\n     return python_random.randint(1, int(1e9))\n@@ -65,18 +139,11 @@ def draw_seed(seed):\n     from keras_core.backend import convert_to_tensor\n \n     if isinstance(seed, SeedGenerator):\n-        seed_state = seed.state\n-        # Use * 1 to create a copy\n-        new_seed_value = seed_state.value * 1\n-        increment = seed.backend.convert_to_tensor(\n-            np.array([0, 1]), dtype=\"uint32\"\n-        )\n-        seed.state.assign(seed_state + increment)\n-        return new_seed_value\n+        return seed.next()\n     elif isinstance(seed, int):\n         return convert_to_tensor([seed, 0], dtype=\"uint32\")\n     elif seed is None:\n-        return convert_to_tensor([make_default_seed(), 0], dtype=\"uint32\")\n+        return global_seed_generator().next(ordered=False)\n     raise ValueError(\n         \"Argument `seed` must be either an integer \"\n         \"or an instance of `SeedGenerator`. \"\n\n@@ -45,20 +45,22 @@ class SafeModeScope:\n \n     def __enter__(self):\n         self.original_value = in_safe_mode()\n-        global_state.set_global_setting(\"safe_mode_saving\", self.safe_mode)\n+        global_state.set_global_attribute(\"safe_mode_saving\", self.safe_mode)\n \n     def __exit__(self, *args, **kwargs):\n-        global_state.set_global_setting(\"safe_mode_saving\", self.original_value)\n+        global_state.set_global_attribute(\n+            \"safe_mode_saving\", self.original_value\n+        )\n \n \n @keras_core_export(\"keras_core.config.enable_unsafe_deserialization\")\n def enable_unsafe_deserialization():\n     \"\"\"Disables safe mode globally, allowing deserialization of lambdas.\"\"\"\n-    global_state.set_global_setting(\"safe_mode_saving\", False)\n+    global_state.set_global_attribute(\"safe_mode_saving\", False)\n \n \n def in_safe_mode():\n-    return global_state.get_global_setting(\"safe_mode_saving\")\n+    return global_state.get_global_attribute(\"safe_mode_saving\")\n \n \n class ObjectSharingScope:\n\n@@ -19,7 +19,7 @@ def enable_interactive_logging():\n     This provides the best experience when using Keras in an interactive\n     environment such as a shell or a notebook.\n     \"\"\"\n-    global_state.set_global_setting(\"interactive_logging\", True)\n+    global_state.set_global_attribute(\"interactive_logging\", True)\n \n \n @keras_core_export(\n@@ -35,7 +35,7 @@ def disable_interactive_logging():\n     This is the best option when using Keras in a non-interactive\n     way, such as running a training or inference job on a server.\n     \"\"\"\n-    global_state.set_global_setting(\"interactive_logging\", False)\n+    global_state.set_global_attribute(\"interactive_logging\", False)\n \n \n @keras_core_export(\n@@ -55,7 +55,7 @@ def is_interactive_logging_enabled():\n         Boolean, `True` if interactive logging is enabled,\n         and `False` otherwise.\n     \"\"\"\n-    return global_state.get_global_setting(\"interactive_logging\", True)\n+    return global_state.get_global_attribute(\"interactive_logging\", True)\n \n \n def set_logging_verbosity(level):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
