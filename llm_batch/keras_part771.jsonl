{"custom_id": "keras#ef339e8adf005e24508fe2c2d1a885576be1378e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 69 | Churn Cumulative: 6755 | Contributors (this commit): 7 | Commits (past 90d): 57 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1462,20 +1462,26 @@ def tri(N, M=None, k=0, dtype=None):\n \n def tril(x, k=0):\n     x = convert_to_tensor(x)\n-    # TODO: tfnp.tril doesn't support bool\n-    if standardize_dtype(x.dtype) == \"bool\":\n-        x = tf.cast(x, \"uint8\")\n-        return tf.cast(tfnp.tril(x, k=k), \"bool\")\n-    return tfnp.tril(x, k=k)\n+    if k >= 0:\n+        return tf.linalg.band_part(x, -1, k)\n+\n+    # deal with negative k using mask\n+    k = -k - 1\n+    mask = tf.ones_like(x, dtype=\"bool\")\n+    mask = tf.logical_not(tf.linalg.band_part(mask, k, -1))\n+    return tf.where(mask, x, tf.constant(0, x.dtype))\n \n \n def triu(x, k=0):\n     x = convert_to_tensor(x)\n-    # TODO: tfnp.triu doesn't support bool\n-    if standardize_dtype(x.dtype) == \"bool\":\n-        x = tf.cast(x, \"uint8\")\n-        return tf.cast(tfnp.tril(x, k=k), \"bool\")\n-    return tfnp.triu(x, k=k)\n+    if k >= 0:\n+        return tf.linalg.band_part(x, k, -1)\n+\n+    # deal with negative k using mask\n+    k = -k\n+    mask = tf.ones_like(x, dtype=\"bool\")\n+    mask = tf.logical_not(tf.linalg.band_part(mask, k, -1))\n+    return tf.where(mask, tf.constant(0, x.dtype), x)\n \n \n def vdot(x1, x2):\n\n@@ -6,6 +6,7 @@ import pytest\n from absl.testing import parameterized\n from tensorflow.python.ops.numpy_ops import np_config\n \n+import keras\n from keras import backend\n from keras import testing\n from keras.backend.common import standardize_dtype\n@@ -3840,12 +3841,54 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.tril(x, -1), np.tril(x, -1))\n         self.assertAllClose(knp.Tril(-1)(x), np.tril(x, -1))\n \n+    def test_tril_in_layer(self):\n+        # https://github.com/keras-team/keras/issues/18890\n+        x = keras.Input((None, 3))\n+        y1 = keras.layers.Lambda(\n+            lambda x: keras.ops.tril(\n+                keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1]))\n+            )\n+        )(x)\n+        y2 = keras.layers.Lambda(\n+            lambda x: keras.ops.tril(\n+                keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1])),\n+                k=-1,\n+            )\n+        )(x)\n+        model = keras.Model(x, [y1, y2])\n+\n+        result = model(np.ones((1, 2, 3), \"float32\"))\n+        self.assertAllClose(\n+            result, [np.tril(np.ones((2, 2))), np.tril(np.ones((2, 2)), k=-1)]\n+        )\n+\n     def test_triu(self):\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         self.assertAllClose(knp.triu(x), np.triu(x))\n         self.assertAllClose(knp.triu(x, -1), np.triu(x, -1))\n         self.assertAllClose(knp.Triu(-1)(x), np.triu(x, -1))\n \n+    def test_triu_in_layer(self):\n+        # https://github.com/keras-team/keras/issues/18890\n+        x = keras.Input((None, 3))\n+        y1 = keras.layers.Lambda(\n+            lambda x: keras.ops.triu(\n+                keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1]))\n+            )\n+        )(x)\n+        y2 = keras.layers.Lambda(\n+            lambda x: keras.ops.triu(\n+                keras.ops.ones((keras.ops.shape(x)[1], keras.ops.shape(x)[1])),\n+                k=-1,\n+            )\n+        )(x)\n+        model = keras.Model(x, [y1, y2])\n+\n+        result = model(np.ones((1, 2, 3), \"float32\"))\n+        self.assertAllClose(\n+            result, [np.triu(np.ones((2, 2))), np.triu(np.ones((2, 2)), k=-1)]\n+        )\n+\n     def test_vstack(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         y = np.array([[4, 5, 6], [6, 5, 4]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07fb6b0eee008229b668e325e0b5ed5c227a970d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 154 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -476,7 +476,7 @@ class Trainer:\n                 `validation_split` or `validation_data` is not affected by\n                 regularization layers like noise and dropout.\n                 `validation_data` will override `validation_split`.\n-                `validation_data` could be:\n+                It could be:\n                   - A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n                   - A tuple `(x_val, y_val, val_sample_weights)` of NumPy\n                     arrays.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46074354c729a9183e57b731c95622bbcb7f4157", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 27 | Churn Cumulative: 1545 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -91,16 +91,6 @@ class ExportArchive:\n                 \"The export API is only compatible with JAX and TF backends.\"\n             )\n \n-        # TODO(nkovela): Make JAX version checking programatic.\n-        if backend.backend() == \"jax\":\n-            from jax import __version__ as jax_v\n-\n-            if jax_v > \"0.4.15\":\n-                raise ValueError(\n-                    \"The export API is only compatible with JAX version 0.4.15 \"\n-                    f\"and prior. Your JAX version: {jax_v}\"\n-                )\n-\n     @property\n     def variables(self):\n         return self._tf_trackable.variables\n\n@@ -1,6 +1,5 @@\n \"\"\"Tests for inference-only model/layer exporting utilities.\"\"\"\n import os\n-import sys\n \n import numpy as np\n import pytest\n@@ -29,10 +28,6 @@ def get_model():\n     backend.backend() not in (\"tensorflow\", \"jax\"),\n     reason=\"Export only currently supports the TF and JAX backends.\",\n )\n-@pytest.mark.skipif(\n-    backend.backend() == \"jax\" and sys.modules[\"jax\"].__version__ > \"0.4.15\",\n-    reason=\"The export API is only compatible with JAX version <= 0.4.15.\",\n-)\n class ExportArchiveTest(testing.TestCase):\n     def test_standard_model_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n@@ -542,18 +537,6 @@ class ExportArchiveTest(testing.TestCase):\n         )\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"jax\" or sys.modules[\"jax\"].__version__ <= \"0.4.15\",\n-    reason=\"This test is for invalid JAX versions, i.e. versions > 0.4.15.\",\n-)\n-class VersionTest(testing.TestCase):\n-    def test_invalid_jax_version(self):\n-        with self.assertRaisesRegex(\n-            ValueError, \"only compatible with JAX version\"\n-        ):\n-            _ = export_lib.ExportArchive()\n-\n-\n @pytest.mark.skipif(\n     backend.backend() != \"tensorflow\",\n     reason=\"TFSM Layer reloading is only for the TF backend.\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28bd33defa20500507d08d82c12bda6a461cf30c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 158 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -546,8 +546,8 @@ class Trainer:\n                 instances (since they generate batches).\n             validation_freq: Only relevant if validation data is provided.\n                 Specifies how many training epochs to run\n-              before a new validation run is performed, e.g. `validation_freq=2`\n-              runs validation every 2 epochs.\n+                before a new validation run is performed,\n+                e.g. `validation_freq=2` runs validation every 2 epochs.\n \n         Unpacking behavior for iterator-like inputs:\n             A common pattern is to pass an iterator like object such as a\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ffc449c250958d55bbfc5fec293e4d3c6c716200", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 872 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class Conv2DTranspose(BaseConvTranspose):\n         data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs. `\"channels_last\"`\n             corresponds to inputs with shape\n-            `(batch_size, channels, height, width)`\n+            `(batch_size, height, width, channels)`\n             while `\"channels_first\"` corresponds to inputs with shape\n             `(batch_size, channels, height, width)`. It defaults to the\n             `image_data_format` value found in your Keras config file at\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c5631ee48460b40f5280f7ff38aa5d3614784525", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 29 | Churn Cumulative: 796 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,7 @@\n \"\"\"Library for exporting inference-only Keras models/layers.\"\"\"\n \n+from absl import logging\n+\n from keras import backend\n from keras.api_export import keras_export\n from keras.layers import Layer\n@@ -441,10 +443,15 @@ class ExportArchive:\n     def _convert_jax2tf_function(self, fn, input_signature):\n         from jax.experimental import jax2tf\n \n+        native_serialization = self._check_device_compatible()\n         shapes = []\n         for spec in input_signature:\n             shapes.append(self._spec_to_poly_shape(spec))\n-        return jax2tf.convert(fn, polymorphic_shapes=shapes)\n+        return jax2tf.convert(\n+            fn,\n+            polymorphic_shapes=shapes,\n+            native_serialization=native_serialization,\n+        )\n \n     def _spec_to_poly_shape(self, spec):\n         if isinstance(spec, (dict, list)):\n@@ -453,6 +460,26 @@ class ExportArchive:\n         spec_shape = str(spec_shape).replace(\"None\", \"b\")\n         return spec_shape\n \n+    def _check_device_compatible(self):\n+        from jax import default_backend as jax_device\n+\n+        if (\n+            jax_device() == \"gpu\"\n+            and len(tf.config.list_physical_devices(\"GPU\")) == 0\n+        ):\n+            logging.warning(\n+                \"JAX backend is using GPU for export, but installed \"\n+                \"TF package cannot access GPU, so reloading the model with \"\n+                \"the TF runtime in the same environment will not work. \"\n+                \"To use JAX-native serialization for high-performance export \"\n+                \"and serving, please install `tensorflow-gpu` and ensure \"\n+                \"CUDA version compatiblity between your JAX and TF \"\n+                \"installations.\"\n+            )\n+            return False\n+        else:\n+            return True\n+\n \n def export_model(model, filepath):\n     export_archive = ExportArchive()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#170a47ac8f98b79a5daede24932bb360dca2c981", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 321 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 7 | Methods Changed: 12 | Complexity Δ (Sum/Max): 25/6 | Churn Δ: 321 | Churn Cumulative: 556 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -90,3 +90,27 @@ def gamma(shape, alpha, dtype=None, seed=None):\n     seed = jax_draw_seed(seed)\n     dtype = dtype or floatx()\n     return jax.random.gamma(seed, alpha, shape=shape, dtype=dtype)\n+\n+\n+def binomial(shape, counts, probabilities, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    seed = jax_draw_seed(seed)\n+    # jax doesn't accept python lists as arguments\n+    counts = jax.numpy.array(counts)\n+    probabilities = jax.numpy.array(probabilities)\n+    sample = jax.random.binomial(\n+        key=seed, n=counts, p=probabilities, shape=shape, dtype=dtype\n+    )\n+    return sample\n+\n+\n+def beta(shape, alpha, beta, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    seed = jax_draw_seed(seed)\n+    # jax doesn't accept python lists as arguments\n+    alpha = jax.numpy.array(alpha)\n+    beta = jax.numpy.array(beta)\n+    sample = jax.random.beta(\n+        key=seed, a=alpha, b=beta, shape=shape, dtype=dtype\n+    )\n+    return sample\n\n@@ -99,3 +99,19 @@ def gamma(shape, alpha, dtype=None, seed=None):\n     seed = draw_seed(seed)\n     rng = np.random.default_rng(seed)\n     return rng.gamma(alpha, scale=1.0, size=shape).astype(dtype)\n+\n+\n+def binomial(shape, counts, probabilities, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    seed = draw_seed(seed)\n+    rng = np.random.default_rng(seed)\n+    sample = rng.binomial(n=counts, p=probabilities, size=shape).astype(dtype)\n+    return sample\n+\n+\n+def beta(shape, alpha, beta, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    seed = draw_seed(seed)\n+    rng = np.random.default_rng(seed)\n+    sample = rng.beta(a=alpha, b=beta, size=shape).astype(dtype)\n+    return sample\n\n@@ -105,3 +105,60 @@ def gamma(shape, alpha, dtype=None, seed=None):\n         dtype=dtype,\n         seed=seed,\n     )\n+\n+\n+def binomial(shape, counts, probabilities, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    seed = tf_draw_seed(seed)\n+    sample = tf.random.stateless_binomial(\n+        shape=shape,\n+        seed=seed,\n+        counts=counts,\n+        probs=probabilities,\n+        output_dtype=dtype,\n+    )\n+    return sample\n+\n+\n+def beta(shape, alpha, beta, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    # since tensorflow doesn't offer a beta distribution function\n+    # so we'll use the formula U(a,b) = (X(a) / (X(a) + Y(b)),\n+    # where U(a,b) is a beta-distributed random variable with\n+    # parameters a and b, and X(a) and Y(b) are gamma-distributed\n+    # random variables with parameters a and b respectively.\n+\n+    # Additionally, we'll use two different seeds for our two\n+    # gamma random variables to prevent any unintended\n+    # dependencies and correlations between the generated values\n+    # due to the usage of same seed.\n+    seed_1 = tf_draw_seed(seed)\n+    # The choice of 12 is totally arbitrary, as we're\n+    # incrementing the first drawn seed by a CONSTANT to\n+    # ensure deterministic results.\n+    seed_2 = seed_1 + 12\n+\n+    alpha = tf.convert_to_tensor(alpha, dtype=dtype)\n+    beta = tf.convert_to_tensor(beta, dtype=dtype)\n+\n+    # tensorflow's tf.random.stateless_gamma has a bit of unconventional\n+    # implementation of the stateless_gamma function where it checks the\n+    # broadcastability of alpha's shape with ONLY the RIGHTMOST dimension of\n+    # the specified output shape instead of considering the whole.\n+    # Consequently, it then results in errors for perfectly broadcastable shapes\n+    # such as for output shape of (2, 3) and alpha shape of (1, 3)\n+    # So to resolve this, we explicitly broadcast alpha and beta to shape before\n+    # passing them to the stateless_gamma function.\n+    if tf.rank(alpha) > 1:\n+        alpha = tf.broadcast_to(alpha, shape)\n+    if tf.rank(beta) > 1:\n+        beta = tf.broadcast_to(beta, shape)\n+\n+    gamma_a = tf.random.stateless_gamma(\n+        shape=shape, seed=seed_1, alpha=alpha, dtype=dtype\n+    )\n+    gamma_b = tf.random.stateless_gamma(\n+        shape=shape, seed=seed_2, alpha=beta, dtype=dtype\n+    )\n+    sample = gamma_a / (gamma_a + gamma_b)\n+    return sample\n\n@@ -203,3 +203,35 @@ def gamma(shape, alpha, dtype=None, seed=None):\n     sample = gamma_distribution.sample().type(dtype)\n     torch.random.set_rng_state(prev_rng_state)\n     return sample\n+\n+\n+def binomial(shape, counts, probabilities, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    dtype = to_torch_dtype(dtype)\n+    counts = torch.ones(shape) * convert_to_tensor(counts)\n+    probabilities = torch.ones(shape) * convert_to_tensor(probabilities)\n+    prev_rng_state = torch.random.get_rng_state()\n+    first_seed, second_seed = draw_seed(seed)\n+    torch.manual_seed(first_seed + second_seed)\n+    binomial_distribution = torch.distributions.binomial.Binomial(\n+        total_count=counts, probs=probabilities\n+    )\n+    sample = binomial_distribution.sample().type(dtype)\n+    torch.random.set_rng_state(prev_rng_state)\n+    return sample\n+\n+\n+def beta(shape, alpha, beta, dtype=None, seed=None):\n+    dtype = dtype or floatx()\n+    dtype = to_torch_dtype(dtype)\n+    alpha = torch.ones(shape) * convert_to_tensor(alpha)\n+    beta = torch.ones(shape) * convert_to_tensor(beta)\n+    prev_rng_state = torch.random.get_rng_state()\n+    first_seed, second_seed = draw_seed(seed)\n+    torch.manual_seed(first_seed + second_seed)\n+    beta_distribution = torch.distributions.beta.Beta(\n+        concentration1=alpha, concentration0=beta\n+    )\n+    sample = beta_distribution.sample().type(dtype)\n+    torch.random.set_rng_state(prev_rng_state)\n+    return sample\n\n@@ -231,3 +231,68 @@ def gamma(shape, alpha, dtype=None, seed=None):\n             of `keras.random.SeedGenerator`.\n     \"\"\"\n     return backend.random.gamma(shape, alpha=alpha, dtype=dtype, seed=seed)\n+\n+\n+def binomial(shape, counts, probabilities, dtype=None, seed=None):\n+    \"\"\"Draw samples from a Binomial distribution.\n+\n+    The values are drawn from a binomial distribution with\n+    specified count and probability of success parameters.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        counts: A number or array of numbers representing the\n+            number of trials. It must be broadcastable with probabilities.\n+        probabilities: A float or array of floats representing the\n+            probability of success of an individual event.\n+            It must be broadcastable with counts.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.config.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.config.set_floatx(float_dtype)`).\n+        seed: A Python integer or instance of\n+            `keras.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras.random.SeedGenerator`.\n+    \"\"\"\n+    return backend.random.binomial(\n+        shape,\n+        counts=counts,\n+        probabilities=probabilities,\n+        dtype=dtype,\n+        seed=seed,\n+    )\n+\n+\n+def beta(shape, alpha, beta, dtype=None, seed=None):\n+    \"\"\"Draw samples from a Beta distribution.\n+\n+    The values are drawm from a Beta distribution parametrized\n+    by alpha and beta.\n+\n+    Args:\n+        shape: The shape of the random values to generate.\n+        alpha: Float or an array of floats representing the first\n+            parameter alpha. Must be broadcastable with beta and shape.\n+        beta: Float or an array of floats representing the second\n+            parameter beta. Must be broadcastable with alpha and shape.\n+        dtype: Optional dtype of the tensor. Only floating point types are\n+            supported. If not specified, `keras.config.floatx()` is used,\n+            which defaults to `float32` unless you configured it otherwise (via\n+            `keras.config.set_floatx(float_dtype)`).\n+        seed: A Python integer or instance of\n+            `keras.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras.random.SeedGenerator`.\n+    \"\"\"\n+    return backend.random.beta(\n+        shape=shape, alpha=alpha, beta=beta, dtype=dtype, seed=seed\n+    )\n\n@@ -8,6 +8,7 @@ from keras import ops\n from keras import testing\n from keras.random import random\n from keras.random import seed_generator\n+from keras.utils.rng_utils import set_random_seed\n \n \n class RandomTest(testing.TestCase, parameterized.TestCase):\n@@ -257,3 +258,129 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(ops.shape(values), shape)\n         self.assertEqual(backend.standardize_dtype(values.dtype), dtype)\n         self.assertGreater(np.min(ops.convert_to_numpy(values)), 0.0)\n+\n+    @parameterized.parameters(\n+        {\n+            \"seed\": 10,\n+            \"shape\": (5, 2),\n+            \"counts\": 5e4,\n+            \"probabilities\": 0.5,\n+            \"dtype\": \"float16\",\n+        },\n+        {\n+            \"seed\": 10,\n+            \"shape\": (2,),\n+            \"counts\": 1e5,\n+            \"probabilities\": 0.5,\n+            \"dtype\": \"float32\",\n+        },\n+        {\n+            \"seed\": 10,\n+            \"shape\": (2, 3),\n+            \"counts\": [[1e5, 2e5, 3e5], [4e5, 5e5, 6e5]],\n+            \"probabilities\": [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]],\n+            \"dtype\": \"float32\",\n+        },\n+    )\n+    def test_binomial(self, seed, shape, counts, probabilities, dtype):\n+        set_random_seed(1337)\n+        values = random.binomial(\n+            shape=shape,\n+            counts=counts,\n+            probabilities=probabilities,\n+            seed=seed,\n+            dtype=dtype,\n+        )\n+        self.assertEqual(ops.shape(values), shape)\n+        self.assertEqual(backend.standardize_dtype(values.dtype), dtype)\n+\n+        # The following test that ensures that the number of time\n+        # each event occurs doesn't exceed the total input count specified\n+        # by the user for that event.\n+        # Hence, we do an element wise comparison between `counts` array\n+        # and the (generated) `values` array.\n+        assert np.greater_equal(np.array(counts), np.array(values)).all()\n+\n+        # Following test computes the probabilities of each event\n+        # by dividing number of times an event occurs (which is the generated\n+        # value) by the corresponding value in the (total) counts array.\n+        # and then makes sure that the computed probabilities approximate\n+        # the input probabilities\n+        generated_probabilities = np.array(values) / np.array(counts)\n+        probabilities = np.ones(shape) * np.array(probabilities)\n+        self.assertAllClose(\n+            probabilities, generated_probabilities, rtol=0.005, atol=0.005\n+        )\n+\n+    @parameterized.parameters(\n+        {\n+            \"seed\": 10,\n+            \"shape\": (10000,),\n+            \"alpha\": 3.0,\n+            \"beta\": 2.0,\n+            \"dtype\": \"float16\",\n+        },\n+        {\n+            \"seed\": 10,\n+            \"shape\": (10000, 3),\n+            \"alpha\": [[7.0, 0.5, 1.5]],\n+            \"beta\": [[15.0, 0.9, 4.5]],\n+            \"dtype\": \"float32\",\n+        },\n+        {\n+            \"seed\": 10,\n+            \"shape\": (10000, 30),\n+            \"alpha\": 1.0,\n+            \"beta\": 1.0,\n+            \"dtype\": \"float32\",\n+        },\n+    )\n+    def test_beta(self, seed, shape, alpha, beta, dtype):\n+        set_random_seed(1337)\n+        values = random.beta(\n+            shape=shape, alpha=alpha, beta=beta, seed=seed, dtype=dtype\n+        )\n+        self.assertEqual(ops.shape(values), shape)\n+        self.assertEqual(backend.standardize_dtype(values.dtype), dtype)\n+        self.assertGreaterEqual(np.min(ops.convert_to_numpy(values)), b=0.0)\n+        self.assertLessEqual(np.max(ops.convert_to_numpy(values)), b=1.0)\n+\n+        _alpha_is_an_array = False\n+        if isinstance(alpha, list):\n+            alpha = np.array(alpha)\n+            beta = np.array(beta)\n+            _alpha_is_an_array = True\n+\n+        # Mean check:\n+        # For a beta distributed random variable,\n+        # mean = alpha / (alpha + beta)\n+        expected_mean = alpha / (alpha + beta)\n+\n+        if _alpha_is_an_array:\n+            actual_mean = np.mean(np.array(values), axis=0)\n+            self.assertAllClose(\n+                expected_mean.flatten(), actual_mean, atol=0.005, rtol=0.005\n+            )\n+        else:\n+            actual_mean = np.mean(np.array(values).flatten())\n+            self.assertAlmostEqual(expected_mean, actual_mean, decimal=2)\n+\n+        # Variance check:\n+        # For a beta distributed random variable,\n+        # variance = (alpha * beta) / ((alpha + beta)^2)(alpha + beta + 1)\n+        expected_variance = (alpha * beta) / (\n+            np.square(alpha + beta) * (alpha + beta + 1)\n+        )\n+        if _alpha_is_an_array:\n+            actual_variance = np.var(np.array(values), axis=0)\n+            self.assertAllClose(\n+                expected_variance.flatten(),\n+                actual_variance,\n+                atol=0.005,\n+                rtol=0.005,\n+            )\n+        else:\n+            actual_variance = np.var(np.array(values).flatten())\n+            self.assertAlmostEqual(\n+                expected_variance, actual_variance, decimal=2\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ebb34c52366f39526fccd581ee9aa690680c62d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 14 | Files Changed: 7 | Hunks: 18 | Methods Changed: 14 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 56 | Churn Cumulative: 3863 | Contributors (this commit): 28 | Commits (past 90d): 52 | Contributors (cumulative): 48 | DMM Complexity: 0.84\n\nDIFF:\n@@ -410,6 +410,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n \n \n def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     outputs = jnp.max(\n         one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n\n@@ -446,6 +446,7 @@ def conv_transpose(\n \n \n def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     input_shape = x.shape\n \n     # Shrink the last dimension if the shape is (..., 1).\n@@ -473,6 +474,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n \n \n def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     outputs = np.max(\n         one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n\n@@ -445,10 +445,12 @@ def conv_transpose(\n \n \n def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     return tf.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     outputs = tf.reduce_max(\n         one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n\n@@ -576,6 +576,7 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n \n \n def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+    x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     outputs = torch.amax(\n         one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n\n@@ -104,6 +104,9 @@ def load_data(\n     elif index_from:\n         x_train = [[w + index_from for w in x] for x in x_train]\n         x_test = [[w + index_from for w in x] for x in x_test]\n+    else:\n+        x_train = [[w for w in x] for x in x_train]\n+        x_test = [[w for w in x] for x in x_test]\n \n     if maxlen:\n         x_train, labels_train = remove_long_seq(maxlen, x_train, labels_train)\n@@ -132,8 +135,8 @@ def load_data(\n         xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n \n     idx = len(x_train)\n-    x_train, y_train = np.array(xs[:idx], dtype=\"object\"), labels[:idx]\n-    x_test, y_test = np.array(xs[idx:], dtype=\"object\"), labels[idx:]\n+    x_train, y_train = xs[:idx], labels[:idx]\n+    x_test, y_test = xs[idx:], labels[idx:]\n     return (x_train, y_train), (x_test, y_test)\n \n \n\n@@ -1544,16 +1544,22 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n \n \n class MultiHot(Operation):\n-    def __init__(self, num_tokens=None, axis=-1, dtype=None, name=None):\n-        super().__init__(name)\n-        self.num_tokens = num_tokens\n+    def __init__(\n+        self, num_classes=None, axis=-1, dtype=None, name=None, **kwargs\n+    ):\n+        if num_classes is None and \"num_tokens\" in kwargs:\n+            num_classes = kwargs.pop(\"num_tokens\")\n+        if num_classes is None:\n+            raise ValueError(\"Argument `num_classes` must be specified.\")\n+        super().__init__(name, **kwargs)\n+        self.num_classes = num_classes\n         self.axis = axis\n         self.dtype = dtype or backend.floatx()\n \n     def call(self, inputs):\n         return backend.nn.multi_hot(\n             inputs,\n-            num_classes=self.num_tokens,\n+            num_classes=self.num_classes,\n             axis=self.axis,\n             dtype=self.dtype,\n         )\n@@ -1561,9 +1567,9 @@ class MultiHot(Operation):\n     def compute_output_spec(self, inputs):\n         x_shape = list(getattr(inputs, \"shape\", []))\n         if self.axis == -1:\n-            x_shape.append(self.num_tokens)\n+            x_shape.append(self.num_classes)\n         elif self.axis >= 0 and self.axis < len(x_shape):\n-            x_shape.insert(self.axis, self.num_tokens)\n+            x_shape.insert(self.axis, self.num_classes)\n         else:\n             raise ValueError(\n                 f\"axis must be -1 or between [0, {len(inputs.shape)}), but \"\n@@ -1584,7 +1590,7 @@ class MultiHot(Operation):\n         \"keras.ops.nn.multi_hot\",\n     ]\n )\n-def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n+def multi_hot(inputs, num_classes=None, axis=-1, dtype=None, **kwargs):\n     \"\"\"Encodes integer labels as multi-hot vectors.\n \n     This function encodes integer labels as multi-hot vectors, where each label\n@@ -1592,7 +1598,7 @@ def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n \n     Args:\n         inputs: Tensor of integer labels to be converted to multi-hot vectors.\n-        num_tokens: Integer, the total number of unique tokens or classes.\n+        num_classes: Integer, the total number of unique classes.\n         axis: (optional) Axis along which the multi-hot encoding should be\n             added. Defaults to `-1`, which corresponds to the last dimension.\n         dtype: (optional) The data type of the resulting tensor. Default\n@@ -1604,14 +1610,19 @@ def multi_hot(inputs, num_tokens, axis=-1, dtype=None):\n     Example:\n \n     >>> data = keras.ops.convert_to_tensor([0, 4])\n-    >>> keras.ops.multi_hot(data, num_tokens=5)\n+    >>> keras.ops.multi_hot(data, num_classes=5)\n     array([1.0, 0.0, 0.0, 0.0, 1.0], dtype=float32)\n \n     \"\"\"\n-    if any_symbolic_tensors((inputs,)):\n-        return MultiHot(num_tokens, axis, dtype).symbolic_call(inputs)\n+    if num_classes is None and \"num_tokens\" in kwargs:\n+        num_classes = kwargs.pop(\"num_tokens\")\n+    if num_classes is None:\n+        raise ValueError(\"Argument `num_classes` must be specified.\")\n \n-    return backend.nn.multi_hot(inputs, num_tokens, axis, dtype)\n+    if any_symbolic_tensors((inputs,)):\n+        return MultiHot(num_classes, axis, dtype).symbolic_call(inputs)\n+\n+    return backend.nn.multi_hot(inputs, num_classes, axis, dtype)\n \n \n class Moments(Operation):\n\n@@ -1460,6 +1460,14 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.eye(4)[indices_1d],\n         )\n \n+        # Test 1D list one-hot.\n+        indices_1d = [0, 1, 2, 3]\n+        self.assertAllClose(knn.one_hot(indices_1d, 4), np.eye(4)[indices_1d])\n+        self.assertAllClose(\n+            knn.one_hot(indices_1d, 4, axis=0),\n+            np.eye(4)[indices_1d],\n+        )\n+\n         # Test 2D one-hot.\n         indices_2d = np.array([[0, 1], [2, 3]])\n         self.assertAllClose(knn.one_hot(indices_2d, 4), np.eye(4)[indices_2d])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a4096eef2d864aa64c607346b2a9c284ca098cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 174 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -236,16 +236,16 @@ def gamma(shape, alpha, dtype=None, seed=None):\n def binomial(shape, counts, probabilities, dtype=None, seed=None):\n     \"\"\"Draw samples from a Binomial distribution.\n \n-    The values are drawn from a binomial distribution with\n-    specified count and probability of success parameters.\n+    The values are drawn from a Binomial distribution with\n+    specified trial count and probability of success.\n \n     Args:\n         shape: The shape of the random values to generate.\n         counts: A number or array of numbers representing the\n-            number of trials. It must be broadcastable with probabilities.\n+            number of trials. It must be broadcastable with `probabilities`.\n         probabilities: A float or array of floats representing the\n             probability of success of an individual event.\n-            It must be broadcastable with counts.\n+            It must be broadcastable with `counts`.\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n@@ -277,9 +277,9 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n     Args:\n         shape: The shape of the random values to generate.\n         alpha: Float or an array of floats representing the first\n-            parameter alpha. Must be broadcastable with beta and shape.\n+            parameter alpha. Must be broadcastable with `beta` and `shape`.\n         beta: Float or an array of floats representing the second\n-            parameter beta. Must be broadcastable with alpha and shape.\n+            parameter beta. Must be broadcastable with `alpha` and `shape`.\n         dtype: Optional dtype of the tensor. Only floating point types are\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eaa6e687da411a183c1d3dea00d72da283ef20fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 116 | Churn Cumulative: 7157 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,6 +84,12 @@ class BatchNormalization(Layer):\n             - `training=False`: The layer will normalize its inputs using\n             the mean and variance of its moving statistics, learned during\n             training.\n+        mask: Binary tensor of shape broadcastable to `inputs` tensor, with\n+            `True` values indicating the positions for which mean and variance\n+            should be computed. Masked elements of the current inputs are not\n+            taken into account for mean and variance computation during\n+            training. Any prior unmasked element values will be taken into\n+            account until their momentum expires.\n \n     Reference:\n \n@@ -211,10 +217,9 @@ class BatchNormalization(Layer):\n             inputs = ops.cast(inputs, \"float32\")\n \n         if training and self.trainable:\n-            mean, variance = ops.moments(\n+            mean, variance = self._moments(\n                 inputs,\n-                axes=self._reduction_axes,\n-                synchronized=self.synchronized,\n+                mask,\n             )\n             moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n             moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n@@ -282,3 +287,44 @@ class BatchNormalization(Layer):\n             \"synchronized\": self.synchronized,\n         }\n         return {**base_config, **config}\n+\n+    def _moments(self, inputs, mask):\n+        if mask is None:\n+            return ops.moments(\n+                inputs,\n+                axes=self._reduction_axes,\n+                synchronized=self.synchronized,\n+            )\n+\n+        mask_weights = ops.cast(\n+            mask,\n+            inputs.dtype,\n+        )\n+        mask_weights_broadcasted = ops.expand_dims(\n+            mask_weights,\n+            axis=-1,\n+        )\n+        weighted_inputs = mask_weights_broadcasted * inputs\n+\n+        weighted_input_sum = ops.sum(\n+            weighted_inputs,\n+            self._reduction_axes,\n+            keepdims=True,\n+        )\n+        sum_of_weights = ops.sum(\n+            mask_weights_broadcasted,\n+            self._reduction_axes,\n+            keepdims=True,\n+        )\n+        mean = weighted_input_sum / (sum_of_weights + backend.config.epsilon())\n+\n+        difference = weighted_inputs - mean\n+        squared_difference = difference * difference\n+        weighted_distsq = ops.sum(\n+            mask_weights_broadcasted * squared_difference,\n+            self._reduction_axes,\n+            keepdims=True,\n+        )\n+        variance = weighted_distsq / (sum_of_weights + backend.config.epsilon())\n+\n+        return ops.squeeze(mean), ops.squeeze(variance)\n\n@@ -5,6 +5,8 @@ from absl.testing import parameterized\n from keras import backend\n from keras import layers\n from keras import testing\n+from keras.losses import MeanSquaredError\n+from keras.models import Model\n \n \n class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n@@ -113,6 +115,68 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         inference_out = layer(x, training=False)\n         self.assertAllClose(inference_out, training_out)\n \n+        # Masked result with no training should not differ\n+        x[:, 1, :] = 0.0\n+        unmasked_out = layer(x, training=False)\n+        masked = layers.Masking()(x)\n+        masked_out = layer(masked, training=False)\n+        self.assertAllClose(unmasked_out, masked_out)\n+\n+        # Masked result should differ from unmasked result\n+        unmasked_out = layer(x, training=False)\n+        x[:, 1, :] = 0.0\n+        masked = layers.Masking()(x)\n+        masked_out = layer(masked, training=True)\n+        self.assertNotAllClose(unmasked_out, masked_out)\n+\n+    @parameterized.product(\n+        synchronized=(False, True)\n+        if backend.backend == \"tensorflow\"\n+        else (False,),\n+    )\n+    def test_input_fully_masked(self, synchronized):\n+        norm = layers.BatchNormalization(\n+            scale=False,\n+            center=False,\n+            synchronized=synchronized,\n+        )\n+        x = np.zeros((4, 5))\n+        mask = np.zeros((4,), dtype=np.float32)\n+        y = norm(x, mask=mask, training=True)\n+        self.assertAllClose(y, np.zeros_like(x, dtype=np.float32))\n+\n+    @parameterized.product(run_eagerly=(True, False), mask_value=(0.0, 0.1, 1))\n+    @pytest.mark.requires_trainable_backend\n+    def test_bachnorm_ignore_masked_values(self, run_eagerly, mask_value):\n+        padded_data = np.array(\n+            [\n+                [\n+                    [1, 5],\n+                    [2, 5],\n+                    [mask_value, mask_value],\n+                    [mask_value, mask_value],\n+                ]\n+                for _ in range(10)\n+            ],\n+            dtype=\"float32\",\n+        )\n+\n+        inputs = layers.Input((None, 2))\n+        masked = layers.Masking(mask_value=mask_value)(inputs)\n+        normed = layers.BatchNormalization(momentum=0.0)(masked)\n+        model = Model(inputs, normed)\n+        loss = MeanSquaredError()\n+        model.compile(\n+            \"rmsprop\",\n+            loss=loss,\n+            run_eagerly=run_eagerly,\n+        )\n+        model.fit(x=padded_data, y=padded_data, batch_size=10, epochs=5)\n+        self.assertAllClose(model.layers[2].moving_mean.numpy(), [1.5, 5.0])\n+        self.assertAllClose(\n+            model.layers[2].moving_variance.numpy(), [0.25, 0.0]\n+        )\n+\n     def test_trainable_behavior(self):\n         layer = layers.BatchNormalization(axis=-1, momentum=0.8, epsilon=1e-7)\n         layer.build((1, 4, 4, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b6b929bda64cd48ddb4c96e2a9648d42bdc2acf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5496 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -319,7 +319,7 @@ class BatchNormalization(Layer):\n         mean = weighted_input_sum / (sum_of_weights + backend.config.epsilon())\n \n         difference = weighted_inputs - mean\n-        squared_difference = difference * difference\n+        squared_difference = ops.square(difference)\n         weighted_distsq = ops.sum(\n             mask_weights_broadcasted * squared_difference,\n             self._reduction_axes,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#19184e9a80f408e6812f0a21298892a25b14bf14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 271 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -208,8 +208,8 @@ def gamma(shape, alpha, dtype=None, seed=None):\n def binomial(shape, counts, probabilities, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n-    counts = torch.ones(shape) * convert_to_tensor(counts)\n-    probabilities = torch.ones(shape) * convert_to_tensor(probabilities)\n+    counts = torch.broadcast_to(convert_to_tensor(counts), shape)\n+    probabilities = torch.broadcast_to(convert_to_tensor(probabilities), shape)\n     prev_rng_state = torch.random.get_rng_state()\n     first_seed, second_seed = draw_seed(seed)\n     torch.manual_seed(first_seed + second_seed)\n@@ -224,8 +224,8 @@ def binomial(shape, counts, probabilities, dtype=None, seed=None):\n def beta(shape, alpha, beta, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n-    alpha = torch.ones(shape) * convert_to_tensor(alpha)\n-    beta = torch.ones(shape) * convert_to_tensor(beta)\n+    alpha = torch.broadcast_to(convert_to_tensor(alpha), shape)\n+    beta = torch.broadcast_to(convert_to_tensor(beta), shape)\n     prev_rng_state = torch.random.get_rng_state()\n     first_seed, second_seed = draw_seed(seed)\n     torch.manual_seed(first_seed + second_seed)\n\n@@ -299,14 +299,15 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         # by the user for that event.\n         # Hence, we do an element wise comparison between `counts` array\n         # and the (generated) `values` array.\n-        assert np.greater_equal(np.array(counts), np.array(values)).all()\n+        values_np = ops.convert_to_numpy(values)\n+        assert np.greater_equal(np.array(counts), values_np).all()\n \n         # Following test computes the probabilities of each event\n         # by dividing number of times an event occurs (which is the generated\n         # value) by the corresponding value in the (total) counts array.\n         # and then makes sure that the computed probabilities approximate\n         # the input probabilities\n-        generated_probabilities = np.array(values) / np.array(counts)\n+        generated_probabilities = values_np / np.array(counts)\n         probabilities = np.ones(shape) * np.array(probabilities)\n         self.assertAllClose(\n             probabilities, generated_probabilities, rtol=0.005, atol=0.005\n@@ -342,8 +343,9 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(ops.shape(values), shape)\n         self.assertEqual(backend.standardize_dtype(values.dtype), dtype)\n-        self.assertGreaterEqual(np.min(ops.convert_to_numpy(values)), b=0.0)\n-        self.assertLessEqual(np.max(ops.convert_to_numpy(values)), b=1.0)\n+        values_np = ops.convert_to_numpy(values)\n+        self.assertGreaterEqual(np.min(values_np), b=0.0)\n+        self.assertLessEqual(np.max(values_np), b=1.0)\n \n         _alpha_is_an_array = False\n         if isinstance(alpha, list):\n@@ -357,12 +359,12 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         expected_mean = alpha / (alpha + beta)\n \n         if _alpha_is_an_array:\n-            actual_mean = np.mean(np.array(values), axis=0)\n+            actual_mean = np.mean(values_np, axis=0)\n             self.assertAllClose(\n                 expected_mean.flatten(), actual_mean, atol=0.005, rtol=0.005\n             )\n         else:\n-            actual_mean = np.mean(np.array(values).flatten())\n+            actual_mean = np.mean(values_np.flatten())\n             self.assertAlmostEqual(expected_mean, actual_mean, decimal=2)\n \n         # Variance check:\n@@ -372,7 +374,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n             np.square(alpha + beta) * (alpha + beta + 1)\n         )\n         if _alpha_is_an_array:\n-            actual_variance = np.var(np.array(values), axis=0)\n+            actual_variance = np.var(values_np, axis=0)\n             self.assertAllClose(\n                 expected_variance.flatten(),\n                 actual_variance,\n@@ -380,7 +382,7 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n                 rtol=0.005,\n             )\n         else:\n-            actual_variance = np.var(np.array(values).flatten())\n+            actual_variance = np.var(values_np.flatten())\n             self.assertAlmostEqual(\n                 expected_variance, actual_variance, decimal=2\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
