{"custom_id": "scrapy#e18014d84d058d7a702c7b0f8c190ee5468abc7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 406 | Files Changed: 6 | Hunks: 8 | Methods Changed: 30 | Complexity Δ (Sum/Max): -22/0 | Churn Δ: 408 | Churn Cumulative: 3394 | Contributors (this commit): 26 | Commits (past 90d): 42 | Contributors (cumulative): 56 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import six\n import pytest\n \n \n@@ -8,7 +7,6 @@ collect_ignore = [\n ]\n \n \n-if six.PY3:\n for line in open('tests/py3-ignores.txt'):\n     file_path = line.strip()\n     if file_path and file_path[0] != '#':\n\n@@ -630,7 +630,8 @@ class Http11MockServerTestCase(unittest.TestCase):\n         # download_maxsize < 100, hence the CancelledError\n         self.assertIsInstance(failure.value, defer.CancelledError)\n \n-        if six.PY2:\n+        # See issue https://twistedmatrix.com/trac/ticket/8175\n+        raise unittest.SkipTest(\"xpayload fails on PY3\")\n         request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n         request = request.replace(url=self.mockserver.url('/xpayload'))\n         yield crawler.crawl(seed=request)\n@@ -639,9 +640,6 @@ class Http11MockServerTestCase(unittest.TestCase):\n         self.assertTrue(failure is None)\n         reason = crawler.spider.meta['close_reason']\n         self.assertTrue(reason, 'finished')\n-        else:\n-            # See issue https://twistedmatrix.com/trac/ticket/8175\n-            raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n \n \n class UriResource(resource.Resource):\n\n@@ -1,233 +0,0 @@\n-# -*- coding: utf-8 -*-\n-import unittest\n-from scrapy.linkextractors.regex import RegexLinkExtractor\n-from scrapy.http import HtmlResponse\n-from scrapy.link import Link\n-from scrapy.linkextractors.htmlparser import HtmlParserLinkExtractor\n-from scrapy.linkextractors.sgml import SgmlLinkExtractor, BaseSgmlLinkExtractor\n-from tests import get_testdata\n-\n-from tests.test_linkextractors import Base\n-\n-\n-class BaseSgmlLinkExtractorTestCase(unittest.TestCase):\n-    # XXX: should we move some of these tests to base link extractor tests?\n-\n-    def test_basic(self):\n-        html = \"\"\"<html><head><title>Page title<title>\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        <p><a href=\"/about.html\">About us</a></p>\n-        <img src=\"/logo.png\" alt=\"Company logo (not a link)\" />\n-        <p><a href=\"../othercat.html\">Other category</a></p>\n-        <p><a href=\"/\">&gt;&gt;</a></p>\n-        <p><a href=\"/\" /></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n-\n-        lx = BaseSgmlLinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='http://example.org/somepage/item/12.html', text='Item 12'),\n-                          Link(url='http://example.org/about.html', text='About us'),\n-                          Link(url='http://example.org/othercat.html', text='Other category'),\n-                          Link(url='http://example.org/', text='>>'),\n-                          Link(url='http://example.org/', text='')])\n-\n-    def test_base_url(self):\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"http://otherdomain.com/base/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p>\n-        </body></html>\"\"\"\n-        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=html)\n-\n-        lx = BaseSgmlLinkExtractor()  # default: tag=a, attr=href\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='http://otherdomain.com/base/item/12.html', text='Item 12')])\n-\n-        # base url is an absolute path and relative to host\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p></body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/somepage/index.html\", body=html)\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='https://example.org/item/12.html', text='Item 12')])\n-\n-        # base url has no scheme\n-        html = \"\"\"<html><head><title>Page title<title><base href=\"//noschemedomain.com/path/to/\" />\n-        <body><p><a href=\"item/12.html\">Item 12</a></p></body></html>\"\"\"\n-        response = HtmlResponse(\"https://example.org/somepage/index.html\", body=html)\n-        self.assertEqual(lx.extract_links(response),\n-                         [Link(url='https://noschemedomain.com/path/to/item/12.html', text='Item 12')])\n-\n-    def test_link_text_wrong_encoding(self):\n-        html = \"\"\"<body><p><a href=\"item/12.html\">Wrong: \\xed</a></p></body></html>\"\"\"\n-        response = HtmlResponse(\"http://www.example.com\", body=html, encoding='utf-8')\n-        lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://www.example.com/item/12.html', text=u'Wrong: \\ufffd'),\n-        ])\n-\n-    def test_extraction_encoding(self):\n-        body = get_testdata('link_extractor', 'linkextractor_noenc.html')\n-        response_utf8 = HtmlResponse(url='http://example.com/utf8', body=body, headers={'Content-Type': ['text/html; charset=utf-8']})\n-        response_noenc = HtmlResponse(url='http://example.com/noenc', body=body)\n-        body = get_testdata('link_extractor', 'linkextractor_latin1.html')\n-        response_latin1 = HtmlResponse(url='http://example.com/latin1', body=body)\n-\n-        lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.extract_links(response_utf8), [\n-            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-            Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n-        ])\n-\n-        self.assertEqual(lx.extract_links(response_noenc), [\n-            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-            Link(url='http://example.com/sample_%E2%82%AC.html', text='sample \\xe2\\x82\\xac text'.decode('utf-8')),\n-        ])\n-\n-        # document encoding does not affect URL path component, only query part\n-        # >>> u'sample_ñ.html'.encode('utf8')\n-        # b'sample_\\xc3\\xb1.html'\n-        # >>> u\"sample_á.html\".encode('utf8')\n-        # b'sample_\\xc3\\xa1.html'\n-        # >>> u\"sample_ö.html\".encode('utf8')\n-        # b'sample_\\xc3\\xb6.html'\n-        # >>> u\"£32\".encode('latin1')\n-        # b'\\xa332'\n-        # >>> u\"µ\".encode('latin1')\n-        # b'\\xb5'\n-        self.assertEqual(lx.extract_links(response_latin1), [\n-            Link(url='http://example.com/sample_%C3%B1.html', text=''),\n-            Link(url='http://example.com/sample_%C3%A1.html', text='sample \\xe1 text'.decode('latin1')),\n-            Link(url='http://example.com/sample_%C3%B6.html?price=%A332&%B5=unit', text=''),\n-        ])\n-\n-    def test_matches(self):\n-        url1 = 'http://lotsofstuff.com/stuff1/index'\n-        url2 = 'http://evenmorestuff.com/uglystuff/index'\n-\n-        lx = BaseSgmlLinkExtractor()\n-        self.assertEqual(lx.matches(url1), True)\n-        self.assertEqual(lx.matches(url2), True)\n-\n-\n-class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n-\n-    def setUp(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n-        self.response = HtmlResponse(url='http://example.com/index', body=body)\n-\n-    def test_extraction(self):\n-        # Default arguments\n-        lx = HtmlParserLinkExtractor()\n-        self.assertEqual(lx.extract_links(self.response), [\n-            Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-            Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-            Link(url='http://example.com/sample3.html', text=u'sample 3 repetition'),\n-            Link(url='http://example.com/sample3.html#foo', text=u'sample 3 repetition with fragment'),\n-            Link(url='http://www.google.com/something', text=u''),\n-            Link(url='http://example.com/innertag.html', text=u'inner tag'),\n-            Link(url='http://example.com/page%204.html', text=u'href with whitespaces'),\n-        ])\n-\n-    def test_link_wrong_href(self):\n-        html = \"\"\"\n-        <a href=\"http://example.org/item1.html\">Item 1</a>\n-        <a href=\"http://[example.org/item2.html\">Item 2</a>\n-        <a href=\"http://example.org/item3.html\">Item 3</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n-        lx = HtmlParserLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n-            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n-        ])\n-\n-\n-class SgmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n-    extractor_cls = SgmlLinkExtractor\n-    escapes_whitespace = True\n-\n-    def test_deny_extensions(self):\n-        html = \"\"\"<a href=\"page.html\">asd</a> and <a href=\"photo.jpg\">\"\"\"\n-        response = HtmlResponse(\"http://example.org/\", body=html)\n-        lx = SgmlLinkExtractor(deny_extensions=\"jpg\")\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://example.org/page.html', text=u'asd'),\n-        ])\n-\n-    def test_attrs_sgml(self):\n-        html = \"\"\"<html><area href=\"sample1.html\"></area>\n-        <a ref=\"sample2.html\">sample text 2</a></html>\"\"\"\n-        response = HtmlResponse(\"http://example.com/index.html\", body=html)\n-        lx = SgmlLinkExtractor(attrs=\"href\")\n-        self.assertEqual(lx.extract_links(response), [\n-            Link(url='http://example.com/sample1.html', text=u''),\n-        ])\n-\n-    def test_link_nofollow(self):\n-        html = \"\"\"\n-        <a href=\"page.html?action=print\" rel=\"nofollow\">Printer-friendly page</a>\n-        <a href=\"about.html\">About us</a>\n-        <a href=\"http://google.com/something\" rel=\"external nofollow\">Something</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/page.html\", body=html)\n-        lx = SgmlLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/page.html?action=print', text=u'Printer-friendly page', nofollow=True),\n-            Link(url='http://example.org/about.html', text=u'About us', nofollow=False),\n-            Link(url='http://google.com/something', text=u'Something', nofollow=True),\n-        ])\n-\n-\n-class RegexLinkExtractorTestCase(unittest.TestCase):\n-    # XXX: RegexLinkExtractor is not deprecated yet, but it must be rewritten\n-    # not to depend on SgmlLinkExractor. Its speed is also much worse\n-    # than it should be.\n-\n-    def setUp(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n-        self.response = HtmlResponse(url='http://example.com/index', body=body)\n-\n-    def test_extraction(self):\n-        # Default arguments\n-        lx = RegexLinkExtractor()\n-        self.assertEqual(lx.extract_links(self.response),\n-                         [Link(url='http://example.com/sample2.html', text=u'sample 2'),\n-                          Link(url='http://example.com/sample3.html', text=u'sample 3 text'),\n-                          Link(url='http://example.com/sample3.html#foo', text=u'sample 3 repetition with fragment'),\n-                          Link(url='http://www.google.com/something', text=u''),\n-                          Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n-\n-    def test_link_wrong_href(self):\n-        html = \"\"\"\n-        <a href=\"http://example.org/item1.html\">Item 1</a>\n-        <a href=\"http://[example.org/item2.html\">Item 2</a>\n-        <a href=\"http://example.org/item3.html\">Item 3</a>\n-        \"\"\"\n-        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n-        lx = RegexLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n-            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n-        ])\n-\n-    def test_html_base_href(self):\n-        html = \"\"\"\n-        <html>\n-            <head>\n-                <base href=\"http://b.com/\">\n-            </head>\n-            <body>\n-                <a href=\"test.html\"></a>\n-            </body>\n-        </html>\n-        \"\"\"\n-        response = HtmlResponse(\"http://a.com/\", body=html)\n-        lx = RegexLinkExtractor()\n-        self.assertEqual([link for link in lx.extract_links(response)], [\n-            Link(url='http://b.com/test.html', text=u'', nofollow=False),\n-        ])\n-\n-    @unittest.expectedFailure\n-    def test_extraction(self):\n-        # RegexLinkExtractor doesn't parse URLs with leading/trailing\n-        # whitespaces correctly.\n-        super(RegexLinkExtractorTestCase, self).test_extraction()\n\n@@ -1,120 +0,0 @@\n-import json\n-import os\n-import time\n-\n-from six.moves.urllib.parse import urlsplit, urlunsplit\n-from threading import Thread\n-from libmproxy import controller, proxy\n-from netlib import http_auth\n-from testfixtures import LogCapture\n-\n-from twisted.internet import defer\n-from twisted.trial.unittest import TestCase\n-from scrapy.utils.test import get_crawler\n-from scrapy.http import Request\n-from tests.spiders import SimpleSpider, SingleRequestSpider\n-from tests.mockserver import MockServer\n-\n-\n-class HTTPSProxy(controller.Master, Thread):\n-\n-    def __init__(self):\n-        password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n-        authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n-        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n-            'keys', 'mitmproxy-ca.pem')\n-        server = proxy.ProxyServer(proxy.ProxyConfig(\n-            authenticator = authenticator,\n-            cacert = cert_path),\n-            0)\n-        self.server = server\n-        Thread.__init__(self)\n-        controller.Master.__init__(self, server)\n-\n-    def http_address(self):\n-        return 'http://scrapy:scrapy@%s:%d' % self.server.socket.getsockname()\n-\n-\n-def _wrong_credentials(proxy_url):\n-    bad_auth_proxy = list(urlsplit(proxy_url))\n-    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')\n-    return urlunsplit(bad_auth_proxy)\n-\n-class ProxyConnectTestCase(TestCase):\n-\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n-        self._oldenv = os.environ.copy()\n-\n-        self._proxy = HTTPSProxy()\n-        self._proxy.start()\n-\n-        # Wait for the proxy to start.\n-        time.sleep(1.0)\n-        os.environ['https_proxy'] = self._proxy.http_address()\n-        os.environ['http_proxy'] = self._proxy.http_address()\n-\n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n-        self._proxy.shutdown()\n-        os.environ = self._oldenv\n-\n-    @defer.inlineCallbacks\n-    def test_https_connect_tunnel(self):\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_noconnect(self):\n-        proxy = os.environ['https_proxy']\n-        os.environ['https_proxy'] = proxy + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_connect_tunnel_error(self):\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n-        self._assert_got_tunnel_error(l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_tunnel_auth_error(self):\n-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        # The proxy returns a 407 error code but it does not reach the client;\n-        # he just sees a TunnelError.\n-        self._assert_got_tunnel_error(l)\n-\n-    @defer.inlineCallbacks\n-    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n-        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n-        crawler = get_crawler(SingleRequestSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(seed=request)\n-        self._assert_got_response_code(200, l)\n-        echo = json.loads(crawler.spider.meta['responses'][0].body)\n-        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n-\n-    @defer.inlineCallbacks\n-    def test_https_noconnect_auth_error(self):\n-        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(407, l)\n-\n-    def _assert_got_response_code(self, code, log):\n-        print(log)\n-        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n-\n-    def _assert_got_tunnel_error(self, log):\n-        print(log)\n-        self.assertIn('TunnelError', str(log))\n\n@@ -163,33 +163,6 @@ class UtilsPythonTestCase(unittest.TestCase):\n                 gc.collect()\n         self.assertFalse(len(wk._weakdict))\n \n-    @unittest.skipUnless(six.PY2, \"deprecated function\")\n-    def test_stringify_dict(self):\n-        d = {'a': 123, u'b': b'c', u'd': u'e', object(): u'e'}\n-        d2 = stringify_dict(d, keys_only=False)\n-        self.assertEqual(d, d2)\n-        self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n-\n-    @unittest.skipUnless(six.PY2, \"deprecated function\")\n-    def test_stringify_dict_tuples(self):\n-        tuples = [('a', 123), (u'b', 'c'), (u'd', u'e'), (object(), u'e')]\n-        d = dict(tuples)\n-        d2 = stringify_dict(tuples, keys_only=False)\n-        self.assertEqual(d, d2)\n-        self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()), d2.keys())\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n-\n-    @unittest.skipUnless(six.PY2, \"deprecated function\")\n-    def test_stringify_dict_keys_only(self):\n-        d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}\n-        d2 = stringify_dict(d)\n-        self.assertEqual(d, d2)\n-        self.assertIsNot(d, d2)  # shouldn't modify in place\n-        self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n-\n     def test_get_func_args(self):\n         def f1(a, b, c):\n             pass\n\n@@ -78,26 +78,6 @@ class ParseUrlTestCase(unittest.TestCase):\n                 to_bytes(x) if not isinstance(x, int) else x for x in test)\n             self.assertEqual(client._parse(url), test, url)\n \n-    def test_externalUnicodeInterference(self):\n-        \"\"\"\n-        L{client._parse} should return C{str} for the scheme, host, and path\n-        elements of its return tuple, even when passed an URL which has\n-        previously been passed to L{urlparse} as a C{unicode} string.\n-        \"\"\"\n-        if not six.PY2:\n-            raise unittest.SkipTest(\n-                \"Applies only to Py2, as urls can be ONLY unicode on Py3\")\n-        badInput = u'http://example.com/path'\n-        goodInput = badInput.encode('ascii')\n-        self._parse(badInput)  # cache badInput in urlparse_cached\n-        scheme, netloc, host, port, path = self._parse(goodInput)\n-        self.assertTrue(isinstance(scheme, str))\n-        self.assertTrue(isinstance(netloc, str))\n-        self.assertTrue(isinstance(host, str))\n-        self.assertTrue(isinstance(path, str))\n-        self.assertTrue(isinstance(port, int))\n-\n-\n \n class ScrapyHTTPPageGetterTests(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f066257e95f1d025d1e581350902067c671fd800", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 134 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 134 | Churn Cumulative: 691 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,134 @@\n+import json\n+import os\n+import re\n+from subprocess import Popen, PIPE\n+import sys\n+import time\n+\n+from six.moves.urllib.parse import urlsplit, urlunsplit\n+from testfixtures import LogCapture\n+\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+\n+from scrapy.utils.test import get_crawler\n+from scrapy.http import Request\n+from tests.spiders import SimpleSpider, SingleRequestSpider\n+from tests.mockserver import MockServer\n+\n+\n+class MitmProxy:\n+    auth_user = 'scrapy'\n+    auth_pass = 'scrapy'\n+\n+    def start(self):\n+        from scrapy.utils.test import get_testenv\n+        script = \"\"\"\n+import sys\n+from mitmproxy.tools.main import mitmdump\n+sys.argv[0] = \"mitmdump\"\n+sys.exit(mitmdump())\n+        \"\"\"\n+        cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n+            'keys', 'mitmproxy-ca.pem')\n+        self.proc = Popen([sys.executable,\n+                           '-c', script,\n+                           '--listen-host', '127.0.0.1',\n+                           '--listen-port', '0',\n+                           '--proxyauth', '%s:%s' % (self.auth_user, self.auth_pass),\n+                           '--certs', cert_path,\n+                           '--ssl-insecure',\n+                           ],\n+                           stdout=PIPE, env=get_testenv())\n+        line = self.proc.stdout.readline().decode('utf-8')\n+        host_port = re.search(r'listening at http://([^:]+:\\d+)', line).group(1)\n+        address = 'http://%s:%s@%s' % (self.auth_user, self.auth_pass, host_port)\n+        return address\n+\n+    def stop(self):\n+        self.proc.kill()\n+        self.proc.wait()\n+        time.sleep(0.2)\n+\n+\n+def _wrong_credentials(proxy_url):\n+    bad_auth_proxy = list(urlsplit(proxy_url))\n+    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')\n+    return urlunsplit(bad_auth_proxy)\n+\n+\n+class ProxyConnectTestCase(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self._oldenv = os.environ.copy()\n+\n+        self._proxy = MitmProxy()\n+        proxy_url = self._proxy.start()\n+        os.environ['https_proxy'] = proxy_url\n+        os.environ['http_proxy'] = proxy_url\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+        self._proxy.stop()\n+        os.environ = self._oldenv\n+\n+    @defer.inlineCallbacks\n+    def test_https_connect_tunnel(self):\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(200, l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_noconnect(self):\n+        proxy = os.environ['https_proxy']\n+        os.environ['https_proxy'] = proxy + '?noconnect'\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(200, l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_connect_tunnel_error(self):\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(\"https://localhost:99999/status?n=200\")\n+        self._assert_got_tunnel_error(l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_tunnel_auth_error(self):\n+        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        # The proxy returns a 407 error code but it does not reach the client;\n+        # he just sees a TunnelError.\n+        self._assert_got_tunnel_error(l)\n+\n+    @defer.inlineCallbacks\n+    def test_https_tunnel_without_leak_proxy_authorization_header(self):\n+        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(seed=request)\n+        self._assert_got_response_code(200, l)\n+        echo = json.loads(crawler.spider.meta['responses'][0].body)\n+        self.assertTrue('Proxy-Authorization' not in echo['headers'])\n+\n+    @defer.inlineCallbacks\n+    def test_https_noconnect_auth_error(self):\n+        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(407, l)\n+\n+    def _assert_got_response_code(self, code, log):\n+        print(log)\n+        self.assertEqual(str(log).count('Crawled (%d)' % code), 1)\n+\n+    def _assert_got_tunnel_error(self, log):\n+        print(log)\n+        self.assertIn('TunnelError', str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cbb6d0c6a71c709903048597cbbdb02616aed285", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 695 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -5,6 +5,7 @@ from subprocess import Popen, PIPE\n import sys\n import time\n \n+import pytest\n from six.moves.urllib.parse import urlsplit, urlunsplit\n from testfixtures import LogCapture\n \n@@ -81,6 +82,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n+    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n     @defer.inlineCallbacks\n     def test_https_noconnect(self):\n         proxy = os.environ['https_proxy']\n@@ -90,6 +92,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n+    @pytest.mark.xfail(reason='Python 3 fails this earlier')\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n@@ -117,6 +120,7 @@ class ProxyConnectTestCase(TestCase):\n         echo = json.loads(crawler.spider.meta['responses'][0].body)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n+    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n     @defer.inlineCallbacks\n     def test_https_noconnect_auth_error(self):\n         os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c327ad9ba6baee934b35ab9cb832e2fbb3311e13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1222 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,4 @@\n import os\n-import six\n import shutil\n import tempfile\n from unittest import mock\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ec6960732591b38a21bc0af9ae042007bb849f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 697 | Contributors (this commit): 7 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class ProxyConnectTestCase(TestCase):\n         with LogCapture() as l:\n             yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200, l)\n-        echo = json.loads(crawler.spider.meta['responses'][0].body)\n+        echo = json.loads(crawler.spider.meta['responses'][0].body.decode('utf-8'))\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n     @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5080180c759e74df9e20f7770534017272a9fea3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 699 | Contributors (this commit): 7 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class ProxyConnectTestCase(TestCase):\n         with LogCapture() as l:\n             yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200, l)\n-        echo = json.loads(crawler.spider.meta['responses'][0].body.decode('utf-8'))\n+        echo = json.loads(crawler.spider.meta['responses'][0].text)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n     @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5970d00eb9ade62523b0f61788cab50fe8f62eb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 701 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n-    @pytest.mark.xfail(reason='Python 3 fails this earlier')\n+    @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info.minor >= 6)\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n         crawler = get_crawler(SimpleSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b730a36706f9b75e1d1b168901dad9805f4da78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 704 | Contributors (this commit): 7 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,8 +48,7 @@ sys.exit(mitmdump())\n \n     def stop(self):\n         self.proc.kill()\n-        self.proc.wait()\n-        time.sleep(0.2)\n+        self.proc.communicate()\n \n \n def _wrong_credentials(proxy_url):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a7b640991d527c265d581cba1b1d1119db99e4c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 136 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ collect_ignore = [\n ]\n \n \n-for line in open('tests/py3-ignores.txt'):\n+for line in open('tests/ignores.txt'):\n     file_path = line.strip()\n     if file_path and file_path[0] != '#':\n         collect_ignore.append(file_path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#922a66cf07e68a9a42d59530ce9dc2dd29a093a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1061 | Contributors (this commit): 12 | Commits (past 90d): 21 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ import os\n import re\n from subprocess import Popen, PIPE\n import sys\n-import time\n \n import pytest\n from six.moves.urllib.parse import urlsplit, urlunsplit\n\n@@ -8,7 +8,7 @@ import six\n \n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n-    WeakKeyCache, stringify_dict, get_func_args, to_bytes, to_unicode,\n+    WeakKeyCache, get_func_args, to_bytes, to_unicode,\n     without_none_values, MutableChain)\n \n __doctests__ = ['scrapy.utils.python']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#beb7d80d6a8a82baf9cb8170758704b4ff5c63cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 732 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -80,16 +80,6 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n-    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n-    @defer.inlineCallbacks\n-    def test_https_noconnect(self):\n-        proxy = os.environ['https_proxy']\n-        os.environ['https_proxy'] = proxy + '?noconnect'\n-        crawler = get_crawler(SimpleSpider)\n-        with LogCapture() as l:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n-        self._assert_got_response_code(200, l)\n-\n     @pytest.mark.xfail(reason='Python 3.6+ fails this earlier', condition=sys.version_info.minor >= 6)\n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n@@ -118,6 +108,23 @@ class ProxyConnectTestCase(TestCase):\n         echo = json.loads(crawler.spider.meta['responses'][0].text)\n         self.assertTrue('Proxy-Authorization' not in echo['headers'])\n \n+    # The noconnect mode isn't supported by the current mitmproxy, it returns\n+    # \"Invalid request scheme: https\" as it doesn't seem to support full URLs in GET at all,\n+    # and it's not clear what behavior is intended by Scrapy and by mitmproxy here.\n+    # https://github.com/mitmproxy/mitmproxy/issues/848 may be related.\n+    # The Scrapy noconnect mode was required, at least in the past, to work with Crawlera,\n+    # and https://github.com/scrapy-plugins/scrapy-crawlera/pull/44 seems to be related.\n+\n+    @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n+    @defer.inlineCallbacks\n+    def test_https_noconnect(self):\n+        proxy = os.environ['https_proxy']\n+        os.environ['https_proxy'] = proxy + '?noconnect'\n+        crawler = get_crawler(SimpleSpider)\n+        with LogCapture() as l:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n+        self._assert_got_response_code(200, l)\n+\n     @pytest.mark.xfail(reason='mitmproxy gives an error for noconnect requests')\n     @defer.inlineCallbacks\n     def test_https_noconnect_auth_error(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42954d0df9a78e6641996fbf2eb76afdef9c1856", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 430 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -93,6 +93,10 @@ class ScrapyHTTPPageGetter(HTTPClient):\n                 (self.factory.url, self.factory.timeout)))\n \n \n+# This class used to inherit from Twisted’s\n+# twisted.web.client.HTTPClientFactory. When that class was deprecated in\n+# Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n+# non-overriden code into this class.\n class ScrapyHTTPClientFactory(ClientFactory):\n \n     protocol = ScrapyHTTPPageGetter\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6c5292a7c4391642897ee31ea2ded889b3b88dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 27 | Churn Cumulative: 676 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,17 +5,18 @@ discovering (through HTTP headers) to base Response class.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-import six\n-from six.moves.urllib.parse import urljoin\n+from contextlib import suppress\n \n import parsel\n-from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n-    html_body_declared_encoding, http_content_type_encoding\n+import six\n+from six.moves.urllib.parse import urljoin\n+from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n+                            http_content_type_encoding, resolve_encoding)\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.response import Response\n-from scrapy.utils.response import get_base_url\n from scrapy.utils.python import memoizemethod_noargs, to_native_str\n+from scrapy.utils.response import get_base_url\n \n \n class TextResponse(Response):\n@@ -197,10 +198,8 @@ class TextResponse(Response):\n                 selector_list = self.xpath(xpath)\n             urls = []\n             for selector in selector_list:\n-                try:\n+                with suppress(_InvalidSelector):\n                     urls.append(_url_from_selector(selector))\n-                except ValueError:\n-                    pass\n         return super(TextResponse, self).follow_all(\n             urls=urls,\n             callback=callback,\n@@ -217,18 +216,24 @@ class TextResponse(Response):\n         )\n \n \n+class _InvalidSelector(ValueError):\n+    \"\"\"\n+    Raised when a URL cannot be obtained from a Selector\n+    \"\"\"\n+\n+\n def _url_from_selector(sel):\n     # type: (parsel.Selector) -> str\n     if isinstance(sel.root, six.string_types):\n         # e.g. ::attr(href) result\n         return strip_html5_whitespace(sel.root)\n     if not hasattr(sel.root, 'tag'):\n-        raise ValueError(\"Unsupported selector: %s\" % sel)\n+        raise _InvalidSelector(\"Unsupported selector: %s\" % sel)\n     if sel.root.tag not in ('a', 'link'):\n-        raise ValueError(\"Only <a> and <link> elements are supported; got <%s>\" %\n+        raise _InvalidSelector(\"Only <a> and <link> elements are supported; got <%s>\" %\n                                sel.root.tag)\n     href = sel.root.get('href')\n     if href is None:\n-        raise ValueError(\"<%s> element has no href attribute: %s\" %\n+        raise _InvalidSelector(\"<%s> element has no href attribute: %s\" %\n                                (sel.root.tag, sel))\n     return strip_html5_whitespace(href)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b602c61e1cc00e04ce435078644d5ea0c0249903", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 4 | Churn Cumulative: 773 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -250,12 +250,12 @@ class BaseResponseTest(unittest.TestCase):\n             yield req\n \n     def _links_response(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        body = get_testdata('link_extractor', 'linkextractor.html')\n         resp = self.response_class('http://example.com/index', body=body)\n         return resp\n \n     def _links_response_no_href(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor_no_href.html')\n+        body = get_testdata('link_extractor', 'linkextractor_no_href.html')\n         resp = self.response_class('http://example.com/index', body=body)\n         return resp\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f4e84ecf95feabe15fda0819bc428f8e0d4d340", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 10 | Churn Cumulative: 1108 | Contributors (this commit): 14 | Commits (past 90d): 22 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,8 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n+from typing import Generator\n+\n from six.moves.urllib.parse import urljoin\n \n from scrapy.http.request import Request\n@@ -41,8 +43,8 @@ class Response(object_ref):\n         if isinstance(url, str):\n             self._url = url\n         else:\n-            raise TypeError('%s url must be str, got %s:' % (type(self).__name__,\n-                type(url).__name__))\n+            raise TypeError('%s url must be str, got %s:' %\n+                            (type(self).__name__, type(url).__name__))\n \n     url = property(_get_url, obsolete_setter(_set_url, 'url'))\n \n\n@@ -7,5 +7,6 @@ See documentation in docs/topics/request-response.rst\n \n from scrapy.http.response.text import TextResponse\n \n+\n class HtmlResponse(TextResponse):\n     pass\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n from contextlib import suppress\n+from typing import Generator\n \n import parsel\n import six\n@@ -14,6 +15,7 @@ from w3lib.encoding import (html_body_declared_encoding, html_to_unicode,\n                             http_content_type_encoding, resolve_encoding)\n from w3lib.html import strip_html5_whitespace\n \n+from scrapy.http import Request\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n\n@@ -7,5 +7,6 @@ See documentation in docs/topics/request-response.rst\n \n from scrapy.http.response.text import TextResponse\n \n+\n class XmlResponse(TextResponse):\n     pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6781d2f5b261305a4f6aa41a2b485a2e5cc7bc76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 18/20 | Churn Δ: 6 | Churn Cumulative: 1226 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ class Base:\n         escapes_whitespace = False\n \n         def setUp(self):\n-            body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+            body = get_testdata('link_extractor', 'linkextractor.html')\n             self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n         def test_urls_type(self):\n\n@@ -111,7 +111,7 @@ class BaseSgmlLinkExtractorTestCase(unittest.TestCase):\n class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n \n     def setUp(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        body = get_testdata('link_extractor', 'linkextractor.html')\n         self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def test_extraction(self):\n@@ -183,7 +183,7 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n     # than it should be.\n \n     def setUp(self):\n-        body = get_testdata('link_extractor', 'sgml_linkextractor.html')\n+        body = get_testdata('link_extractor', 'linkextractor.html')\n         self.response = HtmlResponse(url='http://example.com/index', body=body)\n \n     def test_extraction(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f80eff1e159ceb2e5f9375f6a95f69f75f6e1c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 179 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -279,3 +279,9 @@ intersphinx_mapping = {\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n     'twisted': ('https://twistedmatrix.com/documents/current', None),\n }\n+\n+\n+# Options for sphinx-hoverxref options\n+# ------------------------------------\n+\n+hoverxref_auto_ref = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fcfcabf1bdf6fc48105c6f58a4c4eb59e26a8f6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 181 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -275,7 +275,9 @@ coverage_ignore_pyobjects = [\n # -------------------------------------\n \n intersphinx_mapping = {\n+    'pytest': ('https://docs.pytest.org/en/latest', None),\n     'python': ('https://docs.python.org/3', None),\n     'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n+    'tox': ('https://tox.readthedocs.io/en/latest', None),\n     'twisted': ('https://twistedmatrix.com/documents/current', None),\n }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
