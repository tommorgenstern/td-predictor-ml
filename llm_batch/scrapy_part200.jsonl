{"custom_id": "scrapy#8ef5aa2ffce4b2b8955b58b532920098f6fe8ea4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 614 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,6 @@\n-import re\n import warnings\n import weakref\n-import six\n from twisted.trial import unittest\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.selector import Selector\n from scrapy.selector.lxmlsel import XmlXPathSelector, HtmlXPathSelector, XPathSelector\n@@ -14,7 +11,7 @@ class SelectorTestCase(unittest.TestCase):\n \n     def test_simple_selection(self):\n         \"\"\"Simple selector tests\"\"\"\n-        body = u\"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n+        body = b\"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n         response = TextResponse(url=\"http://example.com\", body=body, encoding='utf-8')\n         sel = Selector(response)\n \n@@ -53,7 +50,7 @@ class SelectorTestCase(unittest.TestCase):\n             self.assertIn('Ignoring deprecated `_root` argument', str(w[-1].message))\n \n     def test_flavor_detection(self):\n-        text = u'<div><img src=\"a.jpg\"><p>Hello</div>'\n+        text = b'<div><img src=\"a.jpg\"><p>Hello</div>'\n         sel = Selector(XmlResponse('http://example.com', body=text, encoding='utf-8'))\n         self.assertEqual(sel.type, 'xml')\n         self.assertEqual(sel.xpath(\"//div\").extract(),\n@@ -86,7 +83,7 @@ class SelectorTestCase(unittest.TestCase):\n     def test_badly_encoded_body(self):\n         # \\xe9 alone isn't valid utf8 sequence\n         r1 = TextResponse('http://www.example.com', \\\n-                          body=u'<html><p>an Jos\\xe9 de</p><html>', \\\n+                          body=b'<html><p>an Jos\\xe9 de</p><html>', \\\n                           encoding='utf-8')\n         Selector(r1).xpath('//text()').extract()\n \n@@ -100,7 +97,7 @@ class SelectorTestCase(unittest.TestCase):\n \n class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n-    text = u'<div><img src=\"a.jpg\"><p>Hello</div>'\n+    text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n \n     def test_warnings_xpathselector(self):\n         cls = XPathSelector\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e50610bd3a8c5a5bad3f19353ce5214f11333e74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 1180 | Contributors (this commit): 10 | Commits (past 90d): 18 | Contributors (cumulative): 15 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -46,6 +46,7 @@ class Selector(ParselSelector, object_ref):\n \n         if response is not None:\n             text = response.body_as_unicode()\n+            kwargs.setdefault('base_url', response.url)\n \n         self.response = response\n         super(Selector, self).__init__(text=text, type=st, root=root, **kwargs)\n\n@@ -33,6 +33,13 @@ class SelectorTestCase(unittest.TestCase):\n         self.assertEqual([x.extract() for x in sel.xpath(\"concat(//input[@name='a']/@value, //input[@name='b']/@value)\")],\n                          [u'12'])\n \n+    def test_root_base_url(self):\n+        body = b'<html><form action=\"/path\"><input name=\"a\" /></form></html>'\n+        url = \"http://example.com\"\n+        response = TextResponse(url=url, body=body, encoding='utf-8')\n+        sel = Selector(response)\n+        self.assertEqual(url, sel.root.base)\n+\n     def test_deprecated_root_argument(self):\n         with warnings.catch_warnings(record=True) as w:\n             root = etree.fromstring(u'<html/>')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e2f31f3018c0037f65982209c22f93b80a5d6e7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 66 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ class FileDownloadHandler(object):\n     @defers\n     def download_request(self, request, spider):\n         filepath = file_uri_to_path(request.url)\n-        body = open(filepath, 'rb').read()\n+        with open(filepath, 'rb') as fo:\n+            body = fo.read()\n         respcls = responsetypes.from_args(filename=filepath, body=body)\n         return respcls(url=request.url, body=body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#766c2551527574936d3210188c9b95b3f8ea726a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 39 | Churn Cumulative: 1219 | Contributors (this commit): 10 | Commits (past 90d): 20 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ XPath selectors based on lxml\n \"\"\"\n \n import warnings\n-from parsel import Selector as ParselSelector, SelectorList\n+from parsel import Selector as ParselSelector, SelectorList as ParselSelectorList\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.python import to_bytes\n from scrapy.http import HtmlResponse, XmlResponse\n@@ -26,9 +26,24 @@ def _response_from_text(text, st):\n               body=to_bytes(text, 'utf-8'))\n \n \n+class SelectorList(ParselSelectorList, object_ref):\n+    @deprecated(use_instead='.extract()')\n+    def extract_unquoted(self):\n+        return [x.extract_unquoted() for x in self]\n+\n+    @deprecated(use_instead='.xpath()')\n+    def x(self, xpath):\n+        return self.select(xpath)\n+\n+    @deprecated(use_instead='.xpath()')\n+    def select(self, xpath):\n+        return self.xpath(xpath)\n+\n+\n class Selector(ParselSelector, object_ref):\n \n     __slots__ = ['response']\n+    selectorlist_cls = SelectorList\n \n     def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n         st = _st(response, type or self._default_type)\n\n@@ -101,6 +101,28 @@ class SelectorTestCase(unittest.TestCase):\n         assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n             x.__class__.__name__\n \n+    def test_deprecated_selector_methods(self):\n+        sel = Selector(TextResponse(url=\"http://example.com\", body=b'<p>some text</p>'))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            sel.select('//p')\n+            self.assertSubstring('Use .xpath() instead', str(w[-1].message))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            sel.extract_unquoted()\n+            self.assertSubstring('Use .extract() instead', str(w[-1].message))\n+\n+    def test_deprecated_selectorlist_methods(self):\n+        sel = Selector(TextResponse(url=\"http://example.com\", body=b'<p>some text</p>'))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            sel.xpath('//p').select('.')\n+            self.assertSubstring('Use .xpath() instead', str(w[-1].message))\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            sel.xpath('//p').extract_unquoted()\n+            self.assertSubstring('Use .extract() instead', str(w[-1].message))\n+\n \n class DeprecatedXpathSelectorTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5abd19e846521b19886b4e7e2ac82f0c401b088", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 582 | Contributors (this commit): 9 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,7 @@ XPath selectors based on lxml\n \"\"\"\n \n import warnings\n-from parsel import Selector as ParselSelector, SelectorList as ParselSelectorList\n+from parsel import Selector as _ParselSelector\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.python import to_bytes\n from scrapy.http import HtmlResponse, XmlResponse\n@@ -26,7 +26,7 @@ def _response_from_text(text, st):\n               body=to_bytes(text, 'utf-8'))\n \n \n-class SelectorList(ParselSelectorList, object_ref):\n+class SelectorList(_ParselSelector.selectorlist_cls, object_ref):\n     @deprecated(use_instead='.extract()')\n     def extract_unquoted(self):\n         return [x.extract_unquoted() for x in self]\n@@ -40,7 +40,7 @@ class SelectorList(ParselSelectorList, object_ref):\n         return self.xpath(xpath)\n \n \n-class Selector(ParselSelector, object_ref):\n+class Selector(_ParselSelector, object_ref):\n \n     __slots__ = ['response']\n     selectorlist_cls = SelectorList\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ce0f53f7edc558ce70ef7b08cdcfd57fcd8b9fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -6,12 +6,14 @@ This package contains a collection of Link Extractors.\n For more info see docs/topics/link-extractors.rst\n \"\"\"\n import re\n-from six.moves.urllib.parse import urlparse\n \n-from scrapy.selector.csstranslator import ScrapyHTMLTranslator\n-from scrapy.utils.url import url_is_from_any_domain\n-from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension\n+from six.moves.urllib.parse import urlparse\n+from parsel.csstranslator import HTMLTranslator\n+\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.url import (\n+    canonicalize_url, url_is_from_any_domain, url_has_any_extension,\n+)\n \n \n # common file extensions that are not followed if they occur in links\n@@ -42,7 +44,7 @@ _is_valid_url = lambda url: url.split('://', 1)[0] in set(['http', 'https', 'fil\n \n class FilteringLinkExtractor(object):\n \n-    _csstranslator = ScrapyHTMLTranslator()\n+    _csstranslator = HTMLTranslator()\n \n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                  restrict_xpaths, canonicalize, deny_extensions, restrict_css):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f634598b6df1e96ceba41a69f8b8cd8cf6453bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 74 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ import sys\n import platform\n \n import twisted\n+import OpenSSL\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -31,6 +32,17 @@ class Command(ScrapyCommand):\n             print(\"libxml2   : %s\" % libxml2_version)\n             print(\"Twisted   : %s\" % twisted.version.short())\n             print(\"Python    : %s\" % sys.version.replace(\"\\n\", \"- \"))\n+            print(\"pyOpenSSL : %s\" % self._get_openssl_version())\n             print(\"Platform  : %s\" % platform.platform())\n         else:\n             print(\"Scrapy %s\" % scrapy.__version__)\n+\n+    def _get_openssl_version(self):\n+        try:\n+            openssl = OpenSSL.SSL.SSLeay_version(OpenSSL.SSL.SSLEAY_VERSION)\\\n+                .decode('ascii', errors='replace')\n+        # pyOpenSSL 0.12 does not expose openssl version\n+        except AttributeError:\n+            openssl = 'Unknown OpenSSL version'\n+\n+        return '{} ({})'.format(OpenSSL.version.__version__, openssl)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#43fc2f23b80083f89ae1e982bfdee5d4e0322556", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,4 +14,16 @@ class VersionTest(ProcessTest, unittest.TestCase):\n     def test_output(self):\n         encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n         _, out, _ = yield self.execute([])\n-        self.assertEqual(out.strip().decode(encoding), \"Scrapy %s\" % scrapy.__version__)\n+        self.assertEqual(\n+            out.strip().decode(encoding),\n+            \"Scrapy %s\" % scrapy.__version__,\n+        )\n+\n+    @defer.inlineCallbacks\n+    def test_verbose_output(self):\n+        encoding = getattr(sys.stdout, 'encoding') or 'utf-8'\n+        _, out, _ = yield self.execute(['-v'])\n+        headers = [l.partition(\":\")[0].strip()\n+                   for l in out.strip().decode(encoding).splitlines()]\n+        self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2', 'Twisted',\n+                                   'Python', 'pyOpenSSL', 'Platform'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27077d2b6cb398cfb54d58c061e233b929868ab7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 28 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,14 @@ os.environ['http_proxy'] = ''\n os.environ['https_proxy'] = ''\n os.environ['ftp_proxy'] = ''\n \n+# Absolutize paths to coverage config and output file because tests that\n+# spawn subprocesses also changes current working directory.\n+_sourceroot = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n+if 'COV_CORE_CONFIG' in os.environ:\n+    os.environ['COVERAGE_FILE'] = os.path.join(_sourceroot, '.coverage')\n+    os.environ['COV_CORE_CONFIG'] = os.path.join(_sourceroot,\n+                                                 os.environ['COV_CORE_CONFIG'])\n+\n try:\n     import unittest.mock as mock\n except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b4fd3a8dffbbfc0207d707cd11ca84d11e76064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 32 | Churn Cumulative: 323 | Contributors (this commit): 7 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -35,7 +35,7 @@ def get_s3_connection():\n class S3DownloadHandler(object):\n \n     def __init__(self, settings, aws_access_key_id=None, aws_secret_access_key=None, \\\n-            httpdownloadhandler=HTTPDownloadHandler):\n+            httpdownloadhandler=HTTPDownloadHandler, **kw):\n \n         _S3Connection = get_s3_connection()\n         if _S3Connection is None:\n@@ -46,8 +46,15 @@ class S3DownloadHandler(object):\n         if not aws_secret_access_key:\n             aws_secret_access_key = settings['AWS_SECRET_ACCESS_KEY']\n \n+        # If no credentials could be found anywhere,\n+        # consider this an anonymous connection request by default;\n+        # unless 'anon' was set explicitly (True/False).\n+        anon = kw.get('anon', None)\n+        if anon is None and not aws_access_key_id and not aws_secret_access_key:\n+            kw['anon'] = True\n+\n         try:\n-            self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key)\n+            self.conn = _S3Connection(aws_access_key_id, aws_secret_access_key, **kw)\n         except Exception as ex:\n             raise NotConfigured(str(ex))\n         self._download_http = httpdownloadhandler(settings).download_request\n\n@@ -403,6 +403,23 @@ class HttpDownloadHandlerMock(object):\n     def download_request(self, request, spider):\n         return request\n \n+class S3AnonTestCase(unittest.TestCase):\n+    skip = 'boto' not in optional_features and 'missing boto library'\n+\n+    def setUp(self):\n+        self.s3reqh = S3DownloadHandler(Settings(),\n+                httpdownloadhandler=HttpDownloadHandlerMock,\n+                #anon=True, # is implicit\n+        )\n+        self.download_request = self.s3reqh.download_request\n+        self.spider = Spider('foo')\n+\n+    def test_anon_request(self):\n+        req = Request('s3://aws-publicdatasets/')\n+        httpreq = self.download_request(req, self.spider)\n+        self.assertEqual(hasattr(self.s3reqh.conn, 'anon'), True)\n+        self.assertEqual(self.s3reqh.conn.anon, True)\n+\n class S3TestCase(unittest.TestCase):\n     download_handler_cls = S3DownloadHandler\n     try:\n@@ -420,8 +437,8 @@ class S3TestCase(unittest.TestCase):\n     AWS_SECRET_ACCESS_KEY = 'uV3F3YluFJax1cknvbcGwgjvx4QpvB+leU8dUj2o'\n \n     def setUp(self):\n-        s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID, \\\n-                self.AWS_SECRET_ACCESS_KEY, \\\n+        s3reqh = S3DownloadHandler(Settings(), self.AWS_ACCESS_KEY_ID,\n+                self.AWS_SECRET_ACCESS_KEY,\n                 httpdownloadhandler=HttpDownloadHandlerMock)\n         self.download_request = s3reqh.download_request\n         self.spider = Spider('foo')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#de15fcdf333397d0f1fb1830d221c48e2df2580e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 3 | Files Changed: 5 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 19/8 | Churn Δ: 77 | Churn Cumulative: 134 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 16 | DMM Complexity: 0.40298507462686567\n\nDIFF:\n@@ -41,7 +41,10 @@ class HtmlParserLinkExtractor(HTMLParser):\n         for link in links:\n             if isinstance(link.url, unicode):\n                 link.url = link.url.encode(response_encoding)\n+            try:\n                 link.url = urljoin(base_url, link.url)\n+            except ValueError:\n+                continue\n             link.url = safe_url_string(link.url, response_encoding)\n             link.text = link.text.decode(response_encoding)\n             ret.append(link)\n\n@@ -49,7 +49,11 @@ class LxmlParserLinkExtractor(object):\n         # hacky way to get the underlying lxml parsed document\n         for el, attr, attr_val in self._iter_links(selector.root):\n             # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n+            try:\n                 attr_val = urljoin(base_url, attr_val)\n+            except ValueError:\n+                continue # skipping bogus links\n+            else:\n                 url = self.process_attr(attr_val)\n                 if url is None:\n                     continue\n\n@@ -14,16 +14,25 @@ def clean_link(link_text):\n     \"\"\"Remove leading and trailing whitespace and punctuation\"\"\"\n     return link_text.strip(\"\\t\\r\\n '\\\"\")\n \n+\n class RegexLinkExtractor(SgmlLinkExtractor):\n     \"\"\"High performant link extractor\"\"\"\n \n     def _extract_links(self, response_text, response_url, response_encoding, base_url=None):\n+        def clean_text(text):\n+            return replace_escape_chars(remove_tags(text.decode(response_encoding))).strip()\n+\n+        def clean_url(url):\n+            clean_url = ''\n+            try:\n+                clean_url = urljoin(base_url, replace_entities(clean_link(url.decode(response_encoding))))\n+            except ValueError:\n+                pass\n+            return clean_url\n+\n         if base_url is None:\n             base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n \n-        clean_url = lambda u: urljoin(base_url, replace_entities(clean_link(u.decode(response_encoding))))\n-        clean_text = lambda t: replace_escape_chars(remove_tags(t.decode(response_encoding))).strip()\n-\n         links_text = linkre.findall(response_text)\n         return [Link(clean_url(url).encode(response_encoding),\n                      clean_text(text))\n\n@@ -42,7 +42,10 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         for link in self.links:\n             if isinstance(link.url, unicode):\n                 link.url = link.url.encode(response_encoding)\n+            try:\n                 link.url = urljoin(base_url, link.url)\n+            except ValueError:\n+                continue\n             link.url = safe_url_string(link.url, response_encoding)\n             link.text = to_unicode(link.text, response_encoding, errors='replace').strip()\n             ret.append(link)\n\n@@ -491,10 +491,36 @@ class SgmlLinkExtractorTestCase(unittest.TestCase):\n                           Link(url='http://example.com/nofollow2.html', text=u'Choose to follow or not', fragment='', nofollow=False)]\n                         )\n \n+    def test_link_wrong_href(self):\n+        html = \"\"\"\n+        <a href=\"http://example.org/item1.html\">Item 1</a>\n+        <a href=\"http://[example.org/item2.html\">Item 2</a>\n+        <a href=\"http://example.org/item3.html\">Item 3</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        lx = self.extractor_cls()\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n+            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n+        ])\n+\n \n class LxmlLinkExtractorTestCase(SgmlLinkExtractorTestCase):\n     extractor_cls = LxmlLinkExtractor\n \n+    def test_link_wrong_href(self):\n+        html = \"\"\"\n+        <a href=\"http://example.org/item1.html\">Item 1</a>\n+        <a href=\"http://[example.org/item2.html\">Item 2</a>\n+        <a href=\"http://example.org/item3.html\">Item 3</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        lx = self.extractor_cls()\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n+            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n+        ])\n+\n \n class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n \n@@ -512,6 +538,19 @@ class HtmlParserLinkExtractorTestCase(unittest.TestCase):\n                           Link(url='http://www.google.com/something', text=u''),\n                           Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n \n+    def test_link_wrong_href(self):\n+        html = \"\"\"\n+        <a href=\"http://example.org/item1.html\">Item 1</a>\n+        <a href=\"http://[example.org/item2.html\">Item 2</a>\n+        <a href=\"http://example.org/item3.html\">Item 3</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        lx = HtmlParserLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n+            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n+        ])\n+\n \n class RegexLinkExtractorTestCase(unittest.TestCase):\n \n@@ -528,6 +567,19 @@ class RegexLinkExtractorTestCase(unittest.TestCase):\n                           Link(url='http://www.google.com/something', text=u''),\n                           Link(url='http://example.com/innertag.html', text=u'inner tag'),])\n \n+    def test_link_wrong_href(self):\n+        html = \"\"\"\n+        <a href=\"http://example.org/item1.html\">Item 1</a>\n+        <a href=\"http://[example.org/item2.html\">Item 2</a>\n+        <a href=\"http://example.org/item3.html\">Item 3</a>\n+        \"\"\"\n+        response = HtmlResponse(\"http://example.org/index.html\", body=html)\n+        lx = RegexLinkExtractor()\n+        self.assertEqual([link for link in lx.extract_links(response)], [\n+            Link(url='http://example.org/item1.html', text=u'Item 1', nofollow=False),\n+            Link(url='http://example.org/item3.html', text=u'Item 3', nofollow=False),\n+        ])\n+\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#509cc8d41e362e995cfcfaef43891490d0bdbf44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 10 | Methods Changed: 13 | Complexity Δ (Sum/Max): 16/11 | Churn Δ: 140 | Churn Cumulative: 924 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 0.7166666666666667\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in docs/topics/shell.rst\n \"\"\"\n from __future__ import print_function\n \n+import os\n import signal\n import warnings\n \n@@ -21,6 +22,8 @@ from scrapy.spiders import Spider\n from scrapy.utils.console import start_python_console\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n+from scrapy.utils.conf import get_config\n+from scrapy.utils.console import DEFAULT_PYTHON_SHELLS\n \n \n class Shell(object):\n@@ -52,7 +55,28 @@ class Shell(object):\n         if self.code:\n             print(eval(self.code, globals(), self.vars))\n         else:\n-            start_python_console(self.vars)\n+            \"\"\"\n+            Detect interactive shell setting in scrapy.cfg\n+            e.g.: ~/.config/scrapy.cfg or ~/.scrapy.cfg\n+            [settings]\n+            # shell can be one of ipython, bpython or python;\n+            # to be used as the interactive python console, if available.\n+            # (default is ipython, fallbacks in the order listed above)\n+            shell = python\n+            \"\"\"\n+            cfg = get_config()\n+            section, option = 'settings', 'shell'\n+            env = os.environ.get('SCRAPY_PYTHON_SHELL')\n+            shells = []\n+            if env:\n+                shells += env.strip().lower().split(',')\n+            elif cfg.has_option(section, option):\n+                shells += [cfg.get(section, option).strip().lower()]\n+            else: # try all by default\n+                shells += DEFAULT_PYTHON_SHELLS.keys()\n+            # always add standard shell as fallback\n+            shells += ['python']\n+            start_python_console(self.vars, shells=shells)\n \n     def _schedule(self, request, spider):\n         spider = self._open_spider(request, spider)\n\n@@ -1,17 +1,8 @@\n+from functools import wraps\n+from collections import OrderedDict\n \n-def start_python_console(namespace=None, noipython=False, banner=''):\n-    \"\"\"Start Python console binded to the given namespace. If IPython is\n-    available, an IPython console will be started instead, unless `noipython`\n-    is True. Also, tab completion will be used on Unix systems.\n-    \"\"\"\n-    if namespace is None:\n-        namespace = {}\n-\n-    try:\n-        try: # use IPython if available\n-            if noipython:\n-                raise ImportError()\n-\n+def _embed_ipython_shell(namespace={}, banner=''):\n+    \"\"\"Start an IPython Shell\"\"\"\n     try:\n         from IPython.terminal.embed import InteractiveShellEmbed\n         from IPython.terminal.ipapp import load_default_config\n@@ -19,11 +10,24 @@ def start_python_console(namespace=None, noipython=False, banner=''):\n         from IPython.frontend.terminal.embed import InteractiveShellEmbed\n         from IPython.frontend.terminal.ipapp import load_default_config\n \n+    @wraps(_embed_ipython_shell)\n+    def wrapper(namespace=namespace, banner=''):\n         config = load_default_config()\n         shell = InteractiveShellEmbed(\n             banner1=banner, user_ns=namespace, config=config)\n         shell()\n-        except ImportError:\n+    return wrapper\n+\n+def _embed_bpython_shell(namespace={}, banner=''):\n+    \"\"\"Start a bpython shell\"\"\"\n+    import bpython\n+    @wraps(_embed_bpython_shell)\n+    def wrapper(namespace=namespace, banner=''):\n+        bpython.embed(locals_=namespace, banner=banner)\n+    return wrapper\n+\n+def _embed_standard_shell(namespace={}, banner=''):\n+    \"\"\"Start a standard python shell\"\"\"\n     import code\n     try: # readline module is only available on unix systems\n         import readline\n@@ -32,6 +36,44 @@ def start_python_console(namespace=None, noipython=False, banner=''):\n     else:\n         import rlcompleter\n         readline.parse_and_bind(\"tab:complete\")\n+    @wraps(_embed_standard_shell)\n+    def wrapper(namespace=namespace, banner=''):\n         code.interact(banner=banner, local=namespace)\n+    return wrapper\n+\n+DEFAULT_PYTHON_SHELLS = OrderedDict([\n+    ('ipython', _embed_ipython_shell),\n+    ('bpython', _embed_bpython_shell),\n+    ( 'python', _embed_standard_shell),\n+])\n+\n+def get_shell_embed_func(shells=None, known_shells=None):\n+    \"\"\"Return the first acceptable shell-embed function\n+    from a given list of shell names.\n+    \"\"\"\n+    if shells is None: # list, preference order of shells\n+        shells = DEFAULT_PYTHON_SHELLS.keys()\n+    if known_shells is None: # available embeddable shells\n+        known_shells = DEFAULT_PYTHON_SHELLS.copy()\n+    for shell in shells:\n+        if shell in known_shells:\n+            try:\n+                # function test: run all setup code (imports),\n+                # but dont fall into the shell\n+                return known_shells[shell]()\n+            except ImportError:\n+                continue\n+\n+def start_python_console(namespace=None, banner='', shells=None):\n+    \"\"\"Start Python console bound to the given namespace.\n+    Readline support and tab completion will be used on Unix, if available.\n+    \"\"\"\n+    if namespace is None:\n+        namespace = {}\n+\n+    try:\n+        shell = get_shell_embed_func(shells)\n+        if shell is not None:\n+            shell(namespace=namespace, banner=banner)\n     except SystemExit: # raised when using exit() in python code.interact\n         pass\n\n@@ -0,0 +1,44 @@\n+import unittest\n+\n+from scrapy.utils.console import get_shell_embed_func\n+try:\n+    import bpython\n+    bpy = True\n+    del bpython\n+except ImportError:\n+    bpy = False\n+try:\n+    import IPython\n+    ipy = True\n+    del IPython\n+except ImportError:\n+    ipy = False\n+\n+class UtilsConsoleTestCase(unittest.TestCase):\n+\n+    def test_get_shell_embed_func(self):\n+\n+        shell = get_shell_embed_func(['invalid'])\n+        self.assertEqual(shell, None)\n+\n+        shell = get_shell_embed_func(['invalid','python'])\n+        self.assertTrue(callable(shell))\n+        self.assertEqual(shell.__name__, '_embed_standard_shell')\n+\n+    @unittest.skipIf(not bpy, 'bpython not available in testenv')\n+    def test_get_shell_embed_func2(self):\n+\n+        shell = get_shell_embed_func(['bpython'])\n+        self.assertTrue(callable(shell))\n+        self.assertEqual(shell.__name__, '_embed_bpython_shell')\n+\n+    @unittest.skipIf(not ipy, 'IPython not available in testenv')\n+    def test_get_shell_embed_func3(self):\n+\n+        # default shell should be 'ipython'\n+        shell = get_shell_embed_func()\n+        self.assertEqual(shell.__name__, '_embed_ipython_shell')\n+\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee66382f29385bfeabb9b8e036cc56b274c8303a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 759 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 0.75\n\nDIFF:\n@@ -76,7 +76,8 @@ class Shell(object):\n                 shells += DEFAULT_PYTHON_SHELLS.keys()\n             # always add standard shell as fallback\n             shells += ['python']\n-            start_python_console(self.vars, shells=shells)\n+            start_python_console(self.vars, shells=shells,\n+                                 banner=self.vars.pop('banner', ''))\n \n     def _schedule(self, request, spider):\n         spider = self._open_spider(request, spider)\n@@ -127,21 +128,25 @@ class Shell(object):\n         self.vars['shelp'] = self.print_help\n         self.update_vars(self.vars)\n         if not self.code:\n-            self.print_help()\n+            self.vars['banner'] = self.get_help()\n \n     def print_help(self):\n-        self.p(\"Available Scrapy objects:\")\n+        print(self.get_help())\n+\n+    def get_help(self):\n+        b = []\n+        b.append(\"Available Scrapy objects:\")\n         for k, v in sorted(self.vars.items()):\n             if self._is_relevant(v):\n-                self.p(\"  %-10s %s\" % (k, v))\n-        self.p(\"Useful shortcuts:\")\n-        self.p(\"  shelp()           Shell help (print this help)\")\n+                b.append(\"  %-10s %s\" % (k, v))\n+        b.append(\"Useful shortcuts:\")\n+        b.append(\"  shelp()           Shell help (print this help)\")\n         if self.inthread:\n-            self.p(\"  fetch(req_or_url) Fetch request (or URL) and update local objects\")\n-        self.p(\"  view(response)    View response in a browser\")\n+            b.append(\"  fetch(req_or_url) Fetch request (or URL) and \"\n+                     \"update local objects\")\n+        b.append(\"  view(response)    View response in a browser\")\n \n-    def p(self, line=''):\n-        print(\"[s] %s\" % line)\n+        return \"\\n\".join([\"[s] %s\" % l for l in b])\n \n     def _is_relevant(self, value):\n         return isinstance(value, self.relevant_classes)\n@@ -165,6 +170,7 @@ def _request_deferred(request):\n     \"\"\"\n     request_callback = request.callback\n     request_errback = request.errback\n+\n     def _restore_callbacks(result):\n         request.callback = request_callback\n         request.errback = request_errback\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d164398a27736f75286cc435eca69b06ff7c1c06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 15 | Churn Cumulative: 58 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 0.8461538461538461\n\nDIFF:\n@@ -55,7 +55,9 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n     def process_response(self, request, response, spider):\n         if (request.meta.get('dont_redirect', False) or\n-               response.status in getattr(spider, 'handle_httpstatus_list', [])):\n+               response.status in getattr(spider, 'handle_httpstatus_list', []) or\n+               response.status in request.meta.get('handle_httpstatus_list', []) or\n+               request.meta.get('handle_httpstatus_all', False)):\n             return response\n \n         if request.method == 'HEAD':\n\n@@ -139,6 +139,17 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         r = self.mw.process_response(req, rsp, smartspider)\n         self.assertIs(r, rsp)\n \n+    def test_request_meta_handling(self):\n+        url = 'http://www.example.com/301'\n+        url2 = 'http://www.example.com/redirected'\n+        def _test_passthrough(req):\n+            rsp = Response(url, headers={'Location': url2}, status=301, request=req)\n+            r = self.mw.process_response(req, rsp, self.spider)\n+            self.assertIs(r, rsp)\n+        _test_passthrough(Request(url, meta={'handle_httpstatus_list':\n+                                                           [404, 301, 302]}))\n+        _test_passthrough(Request(url, meta={'handle_httpstatus_all': True}))\n+\n \n class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7da769feb24e03629d7b7acb94a631459a4a333c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 5 | Churn Cumulative: 13 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -50,6 +50,7 @@ class StatsCollector(object):\n     def _persist_stats(self, stats, spider):\n         pass\n \n+\n class MemoryStatsCollector(StatsCollector):\n \n     def __init__(self, crawler):\n\n@@ -4,6 +4,7 @@ from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector, DummyStatsCollector\n from scrapy.utils.test import get_crawler\n \n+\n class StatsCollectorTest(unittest.TestCase):\n \n     def setUp(self):\n@@ -50,6 +51,3 @@ class StatsCollectorTest(unittest.TestCase):\n         stats.set_value('test', 'value', spider=self.spider)\n         self.assertEqual(stats.get_stats(), {})\n         self.assertEqual(stats.get_stats('a'), {})\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#642af00bb7a676470155a907080839ba410a9adc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 37 | Churn Cumulative: 1302 | Contributors (this commit): 15 | Commits (past 90d): 11 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,7 @@ from scrapy import signals\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n+from scrapy.utils.request import referer_str\n \n logger = logging.getLogger(__name__)\n \n@@ -150,10 +151,9 @@ class Scraper(object):\n         if isinstance(exc, CloseSpider):\n             self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n-        referer = request.headers.get('Referer')\n         logger.error(\n             \"Spider error processing %(request)s (referer: %(referer)s)\",\n-            {'request': request, 'referer': referer},\n+            {'request': request, 'referer': referer_str(request)},\n             exc_info=failure_to_exc_info(_failure),\n             extra={'spider': spider}\n         )\n\n@@ -3,6 +3,7 @@ import logging\n \n from twisted.python.failure import Failure\n \n+from scrapy.utils.request import referer_str\n \n SCRAPEDMSG = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDMSG = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n@@ -38,13 +39,16 @@ class LogFormatter(object):\n             'args': {\n                 'status': response.status,\n                 'request': request,\n-                'referer': request.headers.get('Referer'),\n+                'referer': referer_str(request),\n                 'flags': flags,\n             }\n         }\n \n     def scraped(self, item, response, spider):\n-        src = response.getErrorMessage() if isinstance(response, Failure) else response\n+        if isinstance(response, Failure):\n+            src = response.getErrorMessage()\n+        else:\n+            src = response\n         return {\n             'level': logging.DEBUG,\n             'msg': SCRAPEDMSG,\n\n@@ -26,7 +26,8 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.misc import md5sum\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes\n+from scrapy.utils.request import referer_str\n \n logger = logging.getLogger(__name__)\n \n@@ -199,7 +200,7 @@ class FilesPipeline(MediaPipeline):\n             if age_days > self.EXPIRES:\n                 return  # returning None force download\n \n-            referer = _get_referer(request)\n+            referer = referer_str(request)\n             logger.debug(\n                 'File (uptodate): Downloaded %(medianame)s from %(request)s '\n                 'referred in <%(referer)s>',\n@@ -225,7 +226,7 @@ class FilesPipeline(MediaPipeline):\n \n     def media_failed(self, failure, request, info):\n         if not isinstance(failure.value, IgnoreRequest):\n-            referer = _get_referer(request)\n+            referer = referer_str(request)\n             logger.warning(\n                 'File (unknown-error): Error downloading %(medianame)s from '\n                 '%(request)s referred in <%(referer)s>: %(exception)s',\n@@ -237,7 +238,7 @@ class FilesPipeline(MediaPipeline):\n         raise FileException\n \n     def media_downloaded(self, response, request, info):\n-        referer = _get_referer(request)\n+        referer = referer_str(request)\n \n         if response.status != 200:\n             logger.warning(\n@@ -339,11 +340,3 @@ class FilesPipeline(MediaPipeline):\n     def file_key(self, url):\n         return self.file_path(url)\n     file_key._base = True\n-\n-\n-def _get_referer(request):\n-    \"\"\" Return Referer HTTP header suitable for logging \"\"\"\n-    referrer = request.headers.get('Referer')\n-    if referrer is None:\n-        return referrer\n-    return to_native_str(referrer, errors='replace')\n\n@@ -8,7 +8,6 @@ import hashlib\n import weakref\n from six.moves.urllib.parse import urlunparse\n \n-from twisted.internet.defer import Deferred\n from w3lib.http import basic_auth_header\n from scrapy.utils.python import to_bytes, to_native_str\n \n@@ -86,3 +85,10 @@ def request_httprepr(request):\n     s += request.body\n     return s\n \n+\n+def referer_str(request):\n+    \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n+    referrer = request.headers.get('Referer')\n+    if referrer is None:\n+        return referrer\n+    return to_native_str(referrer, errors='replace')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68a47ade46e6f6fb4235e1355b6a535e9c422f29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 15 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import unittest\n+import six\n \n from scrapy.spiders import Spider\n from scrapy.http import Request, Response\n@@ -42,7 +43,7 @@ class LoggingContribTest(unittest.TestCase):\n         logkws = self.formatter.dropped(item, exception, response, self.spider)\n         logline = logkws['msg'] % logkws['args']\n         lines = logline.splitlines()\n-        assert all(isinstance(x, unicode) for x in lines)\n+        assert all(isinstance(x, six.text_type) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n \n     def test_scraped(self):\n@@ -52,7 +53,7 @@ class LoggingContribTest(unittest.TestCase):\n         logkws = self.formatter.scraped(item, response, self.spider)\n         logline = logkws['msg'] % logkws['args']\n         lines = logline.splitlines()\n-        assert all(isinstance(x, unicode) for x in lines)\n+        assert all(isinstance(x, six.text_type) for x in lines)\n         self.assertEqual(lines, [u\"Scraped from <200 http://www.example.com>\", u'name: \\xa3'])\n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f5c54072ecec1b19390e9a624d0f1fe0882d2791", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 41 | Churn Cumulative: 41 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1 +1,42 @@\n+import os\n+from shutil import rmtree\n+from tempfile import mkdtemp\n+import unittest\n+from scrapy.utils.template import render_templatefile\n+\n+\n __doctests__ = ['scrapy.utils.template']\n+\n+\n+class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.tmp_path = mkdtemp()\n+\n+    def tearDown(self):\n+        rmtree(self.tmp_path)\n+\n+    def test_simple_render(self):\n+\n+        context = dict(project_name='proj', name='spi', classname='TheSpider')\n+        template = u'from ${project_name}.spiders.${name} import ${classname}'\n+        rendered = u'from proj.spiders.spi import TheSpider'\n+\n+        template_path = os.path.join(self.tmp_path, 'templ.py.tmpl')\n+        render_path = os.path.join(self.tmp_path, 'templ.py')\n+\n+        with open(template_path, 'wb') as tmpl_file:\n+            tmpl_file.write(template.encode('utf8'))\n+        assert os.path.isfile(template_path)  # Failure of test itself\n+\n+        render_templatefile(template_path, **context)\n+\n+        self.assertFalse(os.path.exists(template_path))\n+        with open(render_path, 'rb') as result:\n+            self.assertEqual(result.read().decode('utf8'), rendered)\n+\n+        os.remove(render_path)\n+        assert not os.path.exists(render_path)  # Failure of test iself\n+\n+if '__main__' == __name__:\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56b3cf07fff4d3794dcdbf99f6d7faa629fa243e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 36 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,8 @@ def render_templatefile(path, **kwargs):\n \n     content = string.Template(raw).substitute(**kwargs)\n \n-    with open(path.rstrip('.tmpl'), 'wb') as file:\n+    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n+    with open(render_path, 'wb') as file:\n         file.write(content)\n     if path.endswith('.tmpl'):\n         os.remove(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
