{"custom_id": "keras#01b0c0902ccabee3ae9011d4e8553d7fdb969142", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 24 | Churn Cumulative: 42 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,11 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n         )\n \n     def test_random_zoom_out_correctness(self):\n-        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (1, 5, 5, 1)\n+        else:\n+            input_shape = (1, 1, 5, 5)\n+        input_image = np.reshape(np.arange(0, 25), input_shape)\n         expected_output = np.asarray(\n             [\n                 [0, 0, 0, 0, 0],\n@@ -42,7 +46,7 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n             ]\n         )\n         expected_output = backend.convert_to_tensor(\n-            np.reshape(expected_output, (1, 5, 5, 1))\n+            np.reshape(expected_output, input_shape)\n         )\n         self.run_layer_test(\n             layers.RandomZoom,\n@@ -60,7 +64,11 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n         )\n \n     def test_random_zoom_in_correctness(self):\n-        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (1, 5, 5, 1)\n+        else:\n+            input_shape = (1, 1, 5, 5)\n+        input_image = np.reshape(np.arange(0, 25), input_shape)\n         expected_output = np.asarray(\n             [\n                 [6.0, 6.5, 7.0, 7.5, 8.0],\n@@ -71,7 +79,7 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n             ]\n         )\n         expected_output = backend.convert_to_tensor(\n-            np.reshape(expected_output, (1, 5, 5, 1))\n+            np.reshape(expected_output, input_shape)\n         )\n         self.run_layer_test(\n             layers.RandomZoom,\n@@ -89,7 +97,11 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n         )\n \n     def test_tf_data_compatibility(self):\n-        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (1, 5, 5, 1)\n+        else:\n+            input_shape = (1, 1, 5, 5)\n+        input_image = np.reshape(np.arange(0, 25), input_shape)\n         layer = layers.RandomZoom(\n             height_factor=(0.5, 0.5),\n             width_factor=(0.8, 0.8),\n@@ -105,7 +117,7 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n                 [0, 20, 22, 24, 0],\n                 [0, 0, 0, 0, 0],\n             ]\n-        ).reshape((1, 5, 5, 1))\n+        ).reshape(input_shape)\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(expected_output, output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b46a242be4710ddf8ea50e4be38affff608c86e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 413 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,8 +53,18 @@ class Reshape(Layer):\n             shape=output_shape, dtype=inputs.dtype, sparse=inputs.sparse\n         )\n \n+    def build(self, input_shape):\n+        self._sample_output_shape = (\n+            operation_utils.compute_reshape_output_shape(\n+                input_shape[1:], self.target_shape, \"target_shape\"\n+            )\n+        )\n+        self.built = True\n+\n     def call(self, inputs):\n-        return ops.reshape(inputs, (ops.shape(inputs)[0],) + self.target_shape)\n+        return ops.reshape(\n+            inputs, (ops.shape(inputs)[0],) + self._sample_output_shape\n+        )\n \n     def get_config(self):\n         config = {\"target_shape\": self.target_shape}\n\n@@ -4,6 +4,7 @@ from absl.testing import parameterized\n from keras import backend\n from keras import layers\n from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n \n \n class ReshapeTest(testing.TestCase, parameterized.TestCase):\n@@ -92,6 +93,12 @@ class ReshapeTest(testing.TestCase, parameterized.TestCase):\n         reshaped = layers.Reshape((8,))(input_layer)\n         self.assertEqual(reshaped.shape, (None, 8))\n \n+    def test_reshape_with_dynamic_batch_size_and_minus_one(self):\n+        input = KerasTensor((None, 6, 4))\n+        layer = layers.Reshape((-1, 8))\n+        reshaped = backend.compute_output_spec(layer.__call__, input)\n+        self.assertEqual(reshaped.shape, (None, 3, 8))\n+\n     def test_reshape_sets_static_shape(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         reshaped = layers.Reshape((3, 5))(input_layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3e52611774042ac06e0109db765c1fcff5c2045e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -177,12 +177,18 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(ref_out, out)\n \n     def test_tf_data_compatibility(self):\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (2, 10, 12, 3)\n+            output_shape = (2, 8, 9, 3)\n+        else:\n+            input_shape = (2, 3, 10, 12)\n+            output_shape = (2, 3, 8, 9)\n         layer = layers.Resizing(8, 9)\n-        input_data = np.random.random((2, 10, 12, 3))\n+        input_data = np.random.random(input_shape)\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output = output.numpy()\n-        self.assertEqual(list(output.shape), [2, 8, 9, 3])\n+        self.assertEqual(tuple(output.shape), output_shape)\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\",\n@@ -191,8 +197,14 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n     def test_tf_data_compatibility_sequential(self):\n         # Test compatibility when wrapping in a Sequential\n         # https://github.com/keras-team/keras/issues/347\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (2, 10, 12, 3)\n+            output_shape = (2, 8, 9, 3)\n+        else:\n+            input_shape = (2, 3, 10, 12)\n+            output_shape = (2, 3, 8, 9)\n         layer = layers.Resizing(8, 9)\n-        input_data = np.random.random((2, 10, 12, 3))\n+        input_data = np.random.random(input_shape)\n         ds = (\n             tf_data.Dataset.from_tensor_slices(input_data)\n             .batch(2)\n@@ -200,4 +212,4 @@ class ResizingTest(testing.TestCase, parameterized.TestCase):\n         )\n         for output in ds.take(1):\n             output = output.numpy()\n-        self.assertEqual(list(output.shape), [2, 8, 9, 3])\n+        self.assertEqual(tuple(output.shape), output_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f85775f035ad37ca43560b9836bc1cf99411a73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -28,7 +28,11 @@ class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n         )\n \n     def test_random_rotation_correctness(self):\n-        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (1, 5, 5, 1)\n+        else:\n+            input_shape = (1, 1, 5, 5)\n+        input_image = np.reshape(np.arange(0, 25), input_shape)\n         layer = layers.RandomRotation(factor=(0.5, 0.5))\n         actual_output = layer(input_image)\n         expected_output = np.asarray(\n@@ -39,7 +43,7 @@ class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n                 [9, 8, 7, 6, 5],\n                 [4, 3, 2, 1, 0],\n             ]\n-        ).reshape((1, 5, 5, 1))\n+        ).reshape(input_shape)\n \n         self.assertAllClose(\n             backend.convert_to_tensor(expected_output), actual_output, atol=1e-5\n@@ -52,7 +56,11 @@ class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(actual_output, input_image)\n \n     def test_tf_data_compatibility(self):\n-        input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1))\n+        if backend.config.image_data_format() == \"channels_last\":\n+            input_shape = (1, 5, 5, 1)\n+        else:\n+            input_shape = (1, 1, 5, 5)\n+        input_image = np.reshape(np.arange(0, 25), input_shape)\n         layer = layers.RandomRotation(factor=(0.5, 0.5))\n \n         ds = tf_data.Dataset.from_tensor_slices(input_image).map(layer)\n@@ -64,7 +72,7 @@ class RandomRotationTest(testing.TestCase, parameterized.TestCase):\n                 [9, 8, 7, 6, 5],\n                 [4, 3, 2, 1, 0],\n             ]\n-        ).reshape((5, 5, 1))\n+        ).reshape(input_shape[1:])\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(expected_output, output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6811943545e34b798311fd58ed50aa733919587", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 902 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -741,11 +741,6 @@ Stochastic sampling process (noisy images on top, predicted images on bottom, 80\n \n ![flowers stochastic generation gif](https://i.imgur.com/kRXOGzd.gif)\n \n-Trained model and demo available on HuggingFace:\n-\n-| Trained Model | Demo |\n-| :--: | :--: |\n-| [![model badge](https://img.shields.io/badge/%F0%9F%A4%97%20Model-DDIM-black.svg)](https://huggingface.co/keras-io/denoising-diffusion-implicit-models) | [![spaces badge](https://img.shields.io/badge/%F0%9F%A4%97%20Spaces-DDIM-black.svg)](https://huggingface.co/spaces/keras-io/denoising-diffusion-implicit-models) |\n \"\"\"\n \n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1dd85a8b0284202d2a0ab7ab86843f5292721a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 905 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -67,6 +67,9 @@ with deterministic sampling.\n ## Setup\n \"\"\"\n \n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import math\n import matplotlib.pyplot as plt\n import tensorflow as tf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0320b4dde2395540096f9f34207bcca393d25d5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 637 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -27,8 +27,6 @@ import os\n os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n \n import keras\n-\n-\n import numpy as np\n import tensorflow as tf\n \n@@ -202,8 +200,4 @@ basis\" of texture filters such as these.\n See also\n [this old blog post](https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html)\n for analysis and interpretation.\n-\n-Example available on HuggingFace.\n-\n-[![Generic badge](https://img.shields.io/badge/🤗%20Spaces-What%20Convnets%20Learn-black.svg)](https://huggingface.co/spaces/keras-io/what-convnets-learn)\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b5cacca1af9a56f9fae0dde9f839ebbbd48d8cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 842 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,8 @@ Accelerator: GPU\n \"\"\"\n \n import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n import re\n import numpy as np\n import matplotlib.pyplot as plt\n@@ -22,10 +24,7 @@ from keras import layers\n from keras.applications import efficientnet\n from keras.layers import TextVectorization\n \n-\n-seed = 111\n-np.random.seed(seed)\n-tf.random.set_seed(seed)\n+keras.utils.seed_random_seed(111)\n \n \"\"\"\n ## Download the dataset\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0a34582967d73668c4a436fc1296df44222edca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 844 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ from keras import layers\n from keras.applications import efficientnet\n from keras.layers import TextVectorization\n \n-keras.utils.seed_random_seed(111)\n+keras.utils.set_random_seed(111)\n \n \"\"\"\n ## Download the dataset\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6d1f8da7e8d627d9a38b37b04e1f2581a9a8b8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 846 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -572,7 +572,7 @@ early_stopping = keras.callbacks.EarlyStopping(\n \n # Learning Rate Scheduler for the optimizer\n class LRSchedule(\n-    keras.optimizers.schedules.learning_rate_schedule.LearningRateSchedule\n+    keras.optimizers.schedules.LearningRateSchedule\n ):\n     def __init__(self, post_warmup_learning_rate, warmup_steps):\n         super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a34b9a5d70c08c4f3b326090a3b04d0b68e0b490", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 339 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -11,9 +11,16 @@ Accelerator: GPU\n ## Setup\n \"\"\"\n \n+import os\n+\n+# Only the TensorFlow backend supports string inputs.\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+import pathlib\n import numpy as np\n import tensorflow.data as tf_data\n import keras\n+from keras import layers\n \n \"\"\"\n ## Introduction\n@@ -42,9 +49,6 @@ data_path = keras.utils.get_file(\n ## Let's take a look at the data\n \"\"\"\n \n-import os\n-import pathlib\n-\n data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n dirnames = os.listdir(data_dir)\n print(\"Number of directories:\", len(dirnames))\n@@ -123,9 +127,7 @@ Our layer will only consider the top 20,000 words, and will truncate or pad sequ\n be actually 200 tokens long.\n \"\"\"\n \n-from keras.layers import TextVectorization\n-\n-vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n+vectorizer = layers.TextVectorization(max_tokens=20000, output_sequence_length=200)\n text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128)\n vectorizer.adapt(text_ds)\n \n@@ -244,9 +246,7 @@ embedding_layer.set_weights([embedding_matrix])\n A simple 1D convnet with global max pooling and a classifier at the end.\n \"\"\"\n \n-from keras import layers\n-\n-int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n+int_sequences_input = keras.Input(shape=(None,), dtype=\"int32\")\n embedded_sequences = embedding_layer(int_sequences_input)\n x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n x = layers.MaxPooling1D(5)(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8fbf3dc2bf60d5e5a325ead2515ebf25fafc093", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 347 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -171,11 +171,11 @@ test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n Let's download pre-trained GloVe embeddings (a 822M zip file).\n \n You'll need to run the following commands:\n+\"\"\"\n \n-```\n-!wget http://nlp.stanford.edu/data/glove.6B.zip\n-!unzip -q glove.6B.zip\n-```\n+\"\"\"shell\n+wget http://nlp.stanford.edu/data/glove.6B.zip\n+unzip -q glove.6B.zip\n \"\"\"\n \n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91729e8083cb846c55c278751aa7b61ed0b43e7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 353 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -174,7 +174,7 @@ You'll need to run the following commands:\n \"\"\"\n \n \"\"\"shell\n-wget http://nlp.stanford.edu/data/glove.6B.zip\n+wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n unzip -q glove.6B.zip\n \"\"\"\n \n@@ -185,9 +185,7 @@ The archive contains text-encoded vectors of various sizes: 50-dimensional,\n Let's make a dict mapping words (strings) to their NumPy vector representation:\n \"\"\"\n \n-path_to_glove_file = os.path.join(\n-    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n-)\n+path_to_glove_file = \"glove.6B.100d.txt\"\n \n embeddings_index = {}\n with open(path_to_glove_file) as f:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f0a013b02bbafc8a016353a0e60ba94895fea07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 342 | Lines Deleted: 96 | Files Changed: 6 | Hunks: 63 | Methods Changed: 60 | Complexity Δ (Sum/Max): 16/15 | Churn Δ: 438 | Churn Cumulative: 7574 | Contributors (this commit): 7 | Commits (past 90d): 98 | Contributors (cumulative): 29 | DMM Complexity: 0.9118942731277533\n\nDIFF:\n@@ -358,6 +358,9 @@ def flip(x, axis=None):\n \n \n def floor(x):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        x = cast(x, config.floatx())\n     return jnp.floor(x)\n \n \n\n@@ -17,7 +17,17 @@ def add(x1, x2):\n \n \n def einsum(subscripts, *operands, **kwargs):\n-    return np.einsum(subscripts, *operands, **kwargs)\n+    operands = tree.map_structure(convert_to_tensor, operands)\n+    dtypes_to_resolve = []\n+    for x in operands:\n+        dtypes_to_resolve.append(getattr(x, \"dtype\", type(x)))\n+    result_dtype = dtypes.result_type(*dtypes_to_resolve)\n+    compute_dtype = result_dtype\n+    # TODO: np.einsum doesn't support bfloat16\n+    if compute_dtype == \"bfloat16\":\n+        compute_dtype = \"float32\"\n+    operands = tree.map_structure(lambda x: x.astype(compute_dtype), operands)\n+    return np.einsum(subscripts, *operands, **kwargs).astype(result_dtype)\n \n \n def subtract(x1, x2):\n@@ -413,6 +423,13 @@ def flip(x, axis=None):\n \n \n def floor(x):\n+    x = convert_to_tensor(x)\n+    dtype = (\n+        config.floatx()\n+        if standardize_dtype(x.dtype) == \"int64\"\n+        else dtypes.result_type(x.dtype, float)\n+    )\n+    x = x.astype(dtype)\n     return np.floor(x)\n \n \n@@ -434,6 +451,12 @@ def greater_equal(x1, x2):\n \n \n def hstack(xs):\n+    xs = tree.map_structure(convert_to_tensor, xs)\n+    dtypes_to_resolve = []\n+    for x in xs:\n+        dtypes_to_resolve.append(x.dtype)\n+    dtype = dtypes.result_type(*dtypes_to_resolve)\n+    xs = tree.map_structure(lambda x: x.astype(dtype), xs)\n     return np.hstack(xs)\n \n \n\n@@ -71,7 +71,21 @@ def bincount(x, weights=None, minlength=0):\n \n \n def einsum(subscripts, *operands, **kwargs):\n-    return tfnp.einsum(subscripts, *operands, **kwargs)\n+    operands = tf.nest.map_structure(convert_to_tensor, operands)\n+\n+    dtypes_to_resolve = []\n+    for x in operands:\n+        dtypes_to_resolve.append(x.dtype)\n+    result_dtype = dtypes.result_type(*dtypes_to_resolve)\n+    compute_dtype = result_dtype\n+    # TODO: tfnp.einsum doesn't support integer dtype with gpu\n+    if \"int\" in compute_dtype:\n+        compute_dtype = config.floatx()\n+\n+    operands = tf.nest.map_structure(\n+        lambda x: tf.cast(x, compute_dtype), operands\n+    )\n+    return tf.cast(tf.einsum(subscripts, *operands, **kwargs), result_dtype)\n \n \n @sparse.elementwise_binary_union(sparse.sparse_subtract)\n@@ -354,8 +368,6 @@ def arccos(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.arccos, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.acos(x)\n \n \n@@ -367,8 +379,6 @@ def arccosh(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.arccosh, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.acosh(x)\n \n \n@@ -380,8 +390,6 @@ def arcsin(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.arcsin, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.asin(x)\n \n \n@@ -393,8 +401,6 @@ def arcsinh(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.arcsinh, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.asinh(x)\n \n \n@@ -406,8 +412,6 @@ def arctan(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.arctan, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.atan(x)\n \n \n@@ -428,8 +432,6 @@ def arctanh(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.arctanh, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.atanh(x)\n \n \n@@ -460,15 +462,19 @@ def average(x, axis=None, weights=None):\n     if weights is not None:\n         weights = convert_to_tensor(weights)\n         dtypes_to_resolve.append(weights.dtype)\n-    dtype = dtypes.result_type(*dtypes_to_resolve)\n-    x = tf.cast(x, dtype)\n+    result_dtype = dtypes.result_type(*dtypes_to_resolve)\n+    compute_dtype = result_dtype\n+    # TODO: since tfnp.average incorrectly promote bfloat16 to float64, we\n+    # need to cast to float32 first and then cast back to bfloat16\n+    if compute_dtype == \"bfloat16\":\n+        compute_dtype = \"float32\"\n+    x = tf.cast(x, compute_dtype)\n     if weights is not None:\n-        weights = tf.cast(weights, dtype)\n+        weights = tf.cast(weights, compute_dtype)\n     for a in axis:\n         # `tfnp.average` does not handle multiple axes.\n         x = tfnp.average(x, weights=weights, axis=a)\n-    # TODO: tfnp.average incorrectly promote bfloat16 to float64\n-    return tf.cast(x, dtype)\n+    return tf.cast(x, result_dtype)\n \n \n def broadcast_to(x, shape):\n@@ -483,8 +489,6 @@ def ceil(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.ceil, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.ceil(x)\n \n \n@@ -492,8 +496,6 @@ def clip(x, x_min, x_max):\n     dtype = standardize_dtype(x.dtype)\n     if dtype == \"bool\":\n         x = tf.cast(x, \"int64\")\n-    # TODO: consider tfnp.clip, but currently, it incorrectly promotes\n-    # uint* to int*\n     return tf.clip_by_value(x, x_min, x_max)\n \n \n@@ -507,7 +509,8 @@ def concatenate(xs, axis=0):\n                 tf.sparse.to_dense(x) if isinstance(x, tf.SparseTensor) else x\n                 for x in xs\n             ]\n-    dtype_set = set([getattr(x, \"dtype\", type(x)) for x in xs])\n+    xs = tf.nest.map_structure(convert_to_tensor, xs)\n+    dtype_set = set([x.dtype for x in xs])\n     if len(dtype_set) > 1:\n         dtype = dtypes.result_type(*dtype_set)\n         xs = tf.nest.map_structure(lambda x: tf.cast(x, dtype), xs)\n@@ -537,8 +540,6 @@ def cos(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.cos, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.cos(x)\n \n \n@@ -550,8 +551,6 @@ def cosh(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.cosh, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.cosh(x)\n \n \n@@ -649,11 +648,11 @@ def empty(shape, dtype=None):\n def equal(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # TODO: currently, tfnp.equal doesn't support bfloat16\n-    if standardize_dtype(x1.dtype) == \"bfloat16\":\n-        x1 = tf.cast(x1, \"float32\")\n-    if standardize_dtype(x2.dtype) == \"bfloat16\":\n-        x2 = tf.cast(x2, \"float32\")\n+    # tfnp handles the casting internally during comparision, but it lacks\n+    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n     return tfnp.equal(x1, x2)\n \n \n@@ -663,8 +662,6 @@ def exp(x):\n     ori_dtype = standardize_dtype(x.dtype)\n     if \"int\" in ori_dtype or ori_dtype == \"bool\":\n         x = tf.cast(x, config.floatx())\n-    # TODO: consider tfnp.exp, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.exp(x)\n \n \n@@ -680,8 +677,6 @@ def expm1(x):\n     ori_dtype = standardize_dtype(x.dtype)\n     if \"int\" in ori_dtype or ori_dtype == \"bool\":\n         x = tf.cast(x, config.floatx())\n-    # TODO: consider tfnp.expm1, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.expm1(x)\n \n \n@@ -691,7 +686,14 @@ def flip(x, axis=None):\n \n @sparse.elementwise_unary\n def floor(x):\n-    return tfnp.floor(x)\n+    x = convert_to_tensor(x)\n+    dtype = (\n+        config.floatx()\n+        if standardize_dtype(x.dtype) == \"int64\"\n+        else dtypes.result_type(x.dtype, float)\n+    )\n+    x = tf.cast(x, dtype)\n+    return tf.floor(x)\n \n \n def full(shape, fill_value, dtype=None):\n@@ -706,26 +708,32 @@ def full_like(x, fill_value, dtype=None):\n def greater(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # TODO: currently, tfnp.greater doesn't support bfloat16\n-    if standardize_dtype(x1.dtype) == \"bfloat16\":\n-        x1 = tf.cast(x1, \"float32\")\n-    if standardize_dtype(x2.dtype) == \"bfloat16\":\n-        x2 = tf.cast(x2, \"float32\")\n+    # tfnp handles the casting internally during comparision, but it lacks\n+    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n     return tfnp.greater(x1, x2)\n \n \n def greater_equal(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # TODO: currently, tfnp.greater_equal doesn't support bfloat16\n-    if standardize_dtype(x1.dtype) == \"bfloat16\":\n-        x1 = tf.cast(x1, \"float32\")\n-    if standardize_dtype(x2.dtype) == \"bfloat16\":\n-        x2 = tf.cast(x2, \"float32\")\n+    # tfnp handles the casting internally during comparision, but it lacks\n+    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n     return tfnp.greater_equal(x1, x2)\n \n \n def hstack(xs):\n+    xs = tf.nest.map_structure(convert_to_tensor, xs)\n+    dtypes_to_resolve = []\n+    for x in xs:\n+        dtypes_to_resolve.append(x.dtype)\n+    dtype = dtypes.result_type(*dtypes_to_resolve)\n+    xs = tf.nest.map_structure(lambda x: tf.cast(x, dtype), xs)\n     return tfnp.hstack(xs)\n \n \n@@ -740,6 +748,13 @@ def imag(x):\n \n \n def isclose(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    # tfnp handles the casting internally during comparision, but it lacks\n+    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n     return tfnp.isclose(x1, x2)\n \n \n@@ -748,7 +763,9 @@ def isfinite(x):\n \n \n def isinf(x):\n-    return tfnp.isinf(x)\n+    # TODO: tfnp.isinf will get python bool when input is a scalar, so we\n+    # need the extra `convert_to_tensor`\n+    return convert_to_tensor(tfnp.isinf(x))\n \n \n def isnan(x):\n@@ -758,22 +775,22 @@ def isnan(x):\n def less(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # TODO: currently, tfnp.less doesn't support bfloat16\n-    if standardize_dtype(x1.dtype) == \"bfloat16\":\n-        x1 = tf.cast(x1, \"float32\")\n-    if standardize_dtype(x2.dtype) == \"bfloat16\":\n-        x2 = tf.cast(x2, \"float32\")\n+    # tfnp handles the casting internally during comparision, but it lacks\n+    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n     return tfnp.less(x1, x2)\n \n \n def less_equal(x1, x2):\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n-    # TODO: currently, tfnp.less_equal doesn't support bfloat16\n-    if standardize_dtype(x1.dtype) == \"bfloat16\":\n-        x1 = tf.cast(x1, \"float32\")\n-    if standardize_dtype(x2.dtype) == \"bfloat16\":\n-        x2 = tf.cast(x2, \"float32\")\n+    # tfnp handles the casting internally during comparision, but it lacks\n+    # support for bfloat16. Therefore we explicitly cast to the same dtype.\n+    dtype = dtypes.result_type(x1.dtype, x2.dtype)\n+    x1 = tf.cast(x1, dtype)\n+    x2 = tf.cast(x2, dtype)\n     return tfnp.less_equal(x1, x2)\n \n \n@@ -807,8 +824,6 @@ def log(x):\n         else dtypes.result_type(x.dtype, float)\n     )\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.log, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.log(x)\n \n \n@@ -821,8 +836,6 @@ def log10(x):\n         else dtypes.result_type(x.dtype, float)\n     )\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.log10, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.log(x) / tf.math.log(tf.constant(10, x.dtype))\n \n \n@@ -835,8 +848,6 @@ def log1p(x):\n         else dtypes.result_type(x.dtype, float)\n     )\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.log1p, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.log1p(x)\n \n \n@@ -849,8 +860,6 @@ def log2(x):\n         else dtypes.result_type(x.dtype, float)\n     )\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.log2, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.log(x) / tf.math.log(tf.constant(2, x.dtype))\n \n \n@@ -861,8 +870,6 @@ def logaddexp(x1, x2):\n     x1 = tf.cast(x1, dtype)\n     x2 = tf.cast(x2, dtype)\n \n-    # TODO: consider tfnp.logaddexp, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     # Below is the same implementation as tfnp.logaddexp using all native\n     # ops to prevent incorrect promotion of bfloat16.\n     delta = x1 - x2\n@@ -1193,8 +1200,6 @@ def sin(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.sin, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.sin(x)\n \n \n@@ -1206,8 +1211,6 @@ def sinh(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.sinh, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.sinh(x)\n \n \n@@ -1273,8 +1276,6 @@ def tan(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.tan, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.tan(x)\n \n \n@@ -1286,8 +1287,6 @@ def tanh(x):\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.tanh, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.tanh(x)\n \n \n@@ -1380,15 +1379,12 @@ def square(x):\n @sparse.elementwise_unary\n def sqrt(x):\n     x = convert_to_tensor(x)\n-    # upcast to float64 for int64 which matches JAX's behavior\n     dtype = (\n         config.floatx()\n         if standardize_dtype(x.dtype) == \"int64\"\n         else dtypes.result_type(x.dtype, float)\n     )\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.sqrt, but currently, it incorrectly promotes\n-    # bfloat16 to float64\n     return tf.math.sqrt(x)\n \n \n@@ -1433,8 +1429,6 @@ def sum(x, axis=None, keepdims=False):\n     elif dtype in (\"uint8\", \"uint16\"):\n         dtype = \"uint32\"\n     x = tf.cast(x, dtype)\n-    # TODO: consider tfnp.sum, but currently, it incorrectly promotes\n-    # int32 to int64 / uint32 to uint64\n     return tf.reduce_sum(x, axis=axis, keepdims=keepdims)\n \n \n\n@@ -562,6 +562,12 @@ def flip(x, axis=None):\n \n def floor(x):\n     x = convert_to_tensor(x)\n+    dtype = (\n+        config.floatx()\n+        if standardize_dtype(x.dtype) == \"int64\"\n+        else dtypes.result_type(x.dtype, float)\n+    )\n+    x = cast(x, dtype)\n     return torch.floor(x)\n \n \n@@ -579,6 +585,7 @@ def full(shape, fill_value, dtype=None):\n \n \n def full_like(x, fill_value, dtype=None):\n+    dtype = dtype or x.dtype\n     return full(shape=x.shape, fill_value=fill_value, dtype=dtype)\n \n \n@@ -616,12 +623,10 @@ def imag(x):\n \n \n def isclose(x1, x2):\n-    x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n-    if x1.dtype != x2.dtype:\n-        result_dtype = torch.result_type(x1, x2)\n-        if x1.dtype != result_dtype:\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n+    result_dtype = dtypes.result_type(x1.dtype, x2.dtype)\n     x1 = cast(x1, result_dtype)\n-        else:\n     x2 = cast(x2, result_dtype)\n     return torch.isclose(x1, x2)\n \n\n@@ -2352,11 +2352,10 @@ class Einsum(Operation):\n         output_shape = expanded_operands_shapes[0]\n         for shape in expanded_operands_shapes[1:]:\n             output_shape = broadcast_shapes(output_shape, shape)\n-        dtype = None\n+        dtypes_to_resolve = []\n         for x in operands:\n-            if hasattr(x, \"dtype\"):\n-                dtype = x.dtype\n-                break\n+            dtypes_to_resolve.append(getattr(x, \"dtype\", type(x)))\n+        dtype = dtypes.result_type(*dtypes_to_resolve)\n         return KerasTensor(output_shape, dtype=dtype)\n \n \n@@ -2637,7 +2636,12 @@ class Floor(Operation):\n \n     def compute_output_spec(self, x):\n         sparse = getattr(x, \"sparse\", False)\n-        return KerasTensor(x.shape, dtype=x.dtype, sparse=sparse)\n+        dtype = (\n+            backend.floatx()\n+            if backend.standardize_dtype(x.dtype) == \"int64\"\n+            else dtypes.result_type(x.dtype, float)\n+        )\n+        return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n \n @keras_export([\"keras.ops.floor\", \"keras.ops.numpy.floor\"])\n@@ -2686,6 +2690,7 @@ class FullLike(Operation):\n         return backend.numpy.full_like(x, fill_value, dtype=dtype)\n \n     def compute_output_spec(self, x, fill_value, dtype=None):\n+        dtype = dtype or x.dtype\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n@@ -2851,6 +2856,7 @@ class Hstack(Operation):\n     def compute_output_spec(self, xs):\n         first_shape = xs[0].shape\n         total_size_on_axis = 0\n+        dtypes_to_resolve = []\n         for x in xs:\n             if not shape_equal(x.shape, first_shape, axis=[1], allow_none=True):\n                 raise ValueError(\n@@ -2863,9 +2869,11 @@ class Hstack(Operation):\n                 total_size_on_axis = None\n             else:\n                 total_size_on_axis += x.shape[1]\n+            dtypes_to_resolve.append(getattr(x, \"dtype\", type(x)))\n         output_shape = list(first_shape)\n         output_shape[1] = total_size_on_axis\n-        return KerasTensor(output_shape)\n+        dtype = dtypes.result_type(*dtypes_to_resolve)\n+        return KerasTensor(output_shape, dtype=dtype)\n \n \n @keras_export([\"keras.ops.hstack\", \"keras.ops.numpy.hstack\"])\n@@ -2944,7 +2952,7 @@ class Isclose(Operation):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n-        return KerasTensor(output_shape, dtype=x1.dtype)\n+        return KerasTensor(output_shape, dtype=\"bool\")\n \n \n @keras_export([\"keras.ops.isclose\", \"keras.ops.numpy.isclose\"])\n@@ -3744,7 +3752,9 @@ class Meshgrid(Operation):\n         tmp = output_shape[0]\n         output_shape[0] = output_shape[1]\n         output_shape[1] = tmp\n-        return [KerasTensor(output_shape) for _ in range(len(x))]\n+        return [\n+            KerasTensor(output_shape, dtype=xi.dtype) for _ in range(len(x))\n+        ]\n \n \n @keras_export([\"keras.ops.meshgrid\", \"keras.ops.numpy.meshgrid\"])\n\n@@ -5164,6 +5164,33 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(knp.Dot().symbolic_call(x1, x2).dtype, expected_dtype)\n \n+    @parameterized.named_parameters(\n+        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+    )\n+    def test_einsum(self, dtypes):\n+        import jax.numpy as jnp\n+\n+        dtype1, dtype2 = dtypes\n+        x1 = knp.ones((1, 1, 1), dtype=dtype1)\n+        x2 = knp.ones((1, 1, 1), dtype=dtype2)\n+        x1_jax = jnp.ones((1, 1, 1), dtype=dtype1)\n+        x2_jax = jnp.ones((1, 1, 1), dtype=dtype2)\n+        subscripts = \"ijk,lkj->il\"\n+        expected_dtype = standardize_dtype(\n+            jnp.einsum(subscripts, x1_jax, x2_jax).dtype\n+        )\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.einsum(subscripts, x1, x2).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(\n+                knp.Einsum(subscripts).symbolic_call(x1, x2).dtype\n+            ),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_empty(self, dtype):\n         import jax.numpy as jnp\n@@ -5217,6 +5244,22 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             standardize_dtype(knp.Exp().symbolic_call(x).dtype), expected_dtype\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_expand_dims(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.expand_dims(x_jax, -1).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.expand_dims(x, -1).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.ExpandDims(-1).symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_expm1(self, dtype):\n         import jax.numpy as jnp\n@@ -5261,6 +5304,38 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_flip(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.flip(x_jax, -1).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.flip(x, -1).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Flip(-1).symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_floor(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.floor(x_jax).dtype)\n+        if dtype == \"int64\":\n+            expected_dtype = backend.floatx()\n+\n+        self.assertEqual(standardize_dtype(knp.floor(x).dtype), expected_dtype)\n+        self.assertEqual(\n+            standardize_dtype(knp.Floor().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_full(self, dtype):\n         import jax.numpy as jnp\n@@ -5280,6 +5355,22 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_full_like(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1,), dtype=dtype)\n+        x_jax = jnp.ones((1,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.full_like(x_jax, 0).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.full_like(x, 0).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.FullLike().symbolic_call(x, 0).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n@@ -5324,6 +5415,27 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(\n+        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+    )\n+    def test_hstack(self, dtypes):\n+        import jax.numpy as jnp\n+\n+        dtype1, dtype2 = dtypes\n+        x1 = knp.ones((1, 1), dtype=dtype1)\n+        x2 = knp.ones((1, 1), dtype=dtype2)\n+        x1_jax = jnp.ones((1, 1), dtype=dtype1)\n+        x2_jax = jnp.ones((1, 1), dtype=dtype2)\n+        expected_dtype = standardize_dtype(jnp.hstack([x1_jax, x2_jax]).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.hstack([x1, x2]).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Hstack().symbolic_call([x1, x2]).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_identity(self, dtype):\n         import jax.numpy as jnp\n@@ -5341,6 +5453,71 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(\n+        named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n+    )\n+    def test_isclose(self, dtypes):\n+        import jax.numpy as jnp\n+\n+        dtype1, dtype2 = dtypes\n+        x1 = knp.ones((), dtype=dtype1)\n+        x2 = knp.ones((), dtype=dtype2)\n+        x1_jax = jnp.ones((), dtype=dtype1)\n+        x2_jax = jnp.ones((), dtype=dtype2)\n+        expected_dtype = standardize_dtype(jnp.isclose(x1_jax, x2_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.isclose(x1, x2).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Isclose().symbolic_call(x1, x2).dtype),\n+            expected_dtype,\n+        )\n+\n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_isfinite(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.isfinite(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.isfinite(x).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knp.Isfinite().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_isinf(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.isinf(x_jax).dtype)\n+\n+        self.assertEqual(standardize_dtype(knp.isinf(x).dtype), expected_dtype)\n+        self.assertEqual(\n+            standardize_dtype(knp.Isinf().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_isnan(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((), dtype=dtype)\n+        x_jax = jnp.ones((), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.isnan(x_jax).dtype)\n+\n+        self.assertEqual(standardize_dtype(knp.isnan(x).dtype), expected_dtype)\n+        self.assertEqual(\n+            standardize_dtype(knp.Isnan().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n@@ -5667,7 +5844,26 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n-    # TODO: test_meshgrid\n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_meshgrid(self, dtype):\n+        import jax.numpy as jnp\n+\n+        if dtype == \"bool\":\n+            self.skipTest(\"meshgrid doesn't support bool dtype\")\n+        elif dtype is None:\n+            dtype = backend.floatx()\n+        x = knp.array([1, 2, 3], dtype=dtype)\n+        y = knp.array([4, 5, 6], dtype=dtype)\n+        x_jax = jnp.array([1, 2, 3], dtype=dtype)\n+        y_jax = jnp.array([4, 5, 6], dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.meshgrid(x_jax, y_jax)[0].dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.meshgrid(x, y)[0].dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            knp.Meshgrid().symbolic_call(x, y)[0].dtype, expected_dtype\n+        )\n \n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_min(self, dtype):\n@@ -5722,6 +5918,21 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_moveaxis(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((1, 1, 1), dtype=dtype)\n+        x_jax = jnp.ones((1, 1, 1), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.moveaxis(x_jax, -2, -1).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knp.moveaxis(x, -2, -1).dtype), expected_dtype\n+        )\n+        self.assertEqual(\n+            knp.Moveaxis(-2, -1).symbolic_call(x).dtype, expected_dtype\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_nan_to_num(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
