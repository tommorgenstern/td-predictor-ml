{"custom_id": "scrapy#f56079f6c71a77c1f70510cf291cd808617933cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 116 | Files Changed: 2 | Hunks: 26 | Methods Changed: 18 | Complexity Δ (Sum/Max): -20/0 | Churn Δ: 154 | Churn Cumulative: 715 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-from collections import deque\n import hashlib\n import logging\n from six import text_type\n@@ -80,7 +79,8 @@ class PrioritySlot:\n         return (self.priority, self.slot) < (other.priority, other.slot)\n \n     def __str__(self):\n-       return '_'.join([text_type(self.priority), _pathable(text_type(self.slot))])\n+        return '_'.join([text_type(self.priority),\n+                         _pathable(text_type(self.slot))])\n \n \n class PriorityAsTupleQueue(PriorityQueue):\n@@ -135,9 +135,10 @@ class SlotBasedPriorityQueue(object):\n         slot = _scheduler_slot(request)\n         is_new = False\n         if slot not in self.pqueues:\n-            is_new = True\n             self.pqueues[slot] = PriorityAsTupleQueue(self.qfactory)\n-        self.pqueues[slot].push(request, PrioritySlot(priority=priority, slot=slot))\n+        queue = self.pqueues[slot]\n+        is_new = queue.is_empty()\n+        queue.push(request, PrioritySlot(priority=priority, slot=slot))\n         return slot, is_new\n \n     def close(self):\n@@ -152,36 +153,6 @@ class SlotBasedPriorityQueue(object):\n         return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n \n \n-class RoundRobinPriorityQueue(SlotBasedPriorityQueue):\n-\n-    def __init__(self, qfactory, startprios={}):\n-        super(RoundRobinPriorityQueue, self).__init__(qfactory, startprios)\n-        self._slots = deque()\n-        for slot in self.pqueues:\n-            self._slots.append(slot)\n-\n-    def push(self, request, priority):\n-        slot, is_new = self.push_slot(request, priority)\n-        if is_new:\n-            self._slots.append(slot)\n-\n-    def pop(self):\n-        if not self._slots:\n-            return\n-\n-        slot = self._slots.popleft()\n-        request, is_empty = self.pop_slot(slot)\n-\n-        if not is_empty:\n-            self._slots.append(slot)\n-\n-        return request\n-\n-    def close(self):\n-        self._slots.clear()\n-        return super(RoundRobinPriorityQueue, self).close()\n-\n-\n class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n \n     _DOWNLOADER_AWARE_PQ_ID = 'DOWNLOADER_AWARE_PQ_ID'\n@@ -191,7 +162,8 @@ class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n         return cls(crawler, qfactory, startprios)\n \n     def __init__(self, crawler, qfactory, startprios={}):\n-        super(DownloaderAwarePriorityQueue, self).__init__(qfactory, startprios)\n+        super(DownloaderAwarePriorityQueue, self).__init__(qfactory,\n+                                                           startprios)\n         self._slots = {slot: 0 for slot in self.pqueues}\n         crawler.signals.connect(self.on_response_download,\n                                 signal=response_downloaded)\n\n@@ -1,4 +1,3 @@\n-import contextlib\n import shutil\n import tempfile\n import unittest\n@@ -10,15 +9,18 @@ from scrapy.pqueues import _scheduler_slot_read, _scheduler_slot_write\n from scrapy.signals import request_reached_downloader, response_downloaded\n from scrapy.spiders import Spider\n \n+\n class MockCrawler(Crawler):\n     def __init__(self, priority_queue_cls, jobdir):\n \n-        settings = dict(LOG_UNSERIALIZABLE_REQUESTS=False,\n+        settings = dict(\n+                LOG_UNSERIALIZABLE_REQUESTS=False,\n                 SCHEDULER_DISK_QUEUE='scrapy.squeues.PickleLifoDiskQueue',\n                 SCHEDULER_MEMORY_QUEUE='scrapy.squeues.LifoMemoryQueue',\n                 SCHEDULER_PRIORITY_QUEUE=priority_queue_cls,\n                 JOBDIR=jobdir,\n-                       DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter')\n+                DUPEFILTER_CLASS='scrapy.dupefilters.BaseDupeFilter'\n+                )\n         super(MockCrawler, self).__init__(Spider, settings)\n \n \n@@ -82,7 +84,8 @@ class BaseSchedulerInMemoryTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n-        self.assertEqual(priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n+        self.assertEqual(priorities,\n+                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n \n \n class BaseSchedulerOnDiskTester(SchedulerHandler):\n@@ -134,7 +137,8 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n-        self.assertEqual(priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n+        self.assertEqual(priorities,\n+                         sorted([x[1] for x in _PRIORITIES], key=lambda x: -x))\n \n \n class TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n@@ -153,75 +157,15 @@ _SLOTS = [(\"http://foo.com/a\", 'a'),\n           (\"http://foo.com/f\", 'c')]\n \n \n-class TestSchedulerWithRoundRobinInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n-    priority_queue_cls = 'scrapy.pqueues.RoundRobinPriorityQueue'\n+class TestMigration(unittest.TestCase):\n \n-    def test_round_robin(self):\n-        for url, slot in _SLOTS:\n-            request = Request(url)\n-            _scheduler_slot_write(request, slot)\n-            self.scheduler.enqueue_request(request)\n+    def setUp(self):\n+        self.tmpdir = tempfile.mkdtemp()\n \n-        slots = list()\n-        while self.scheduler.has_pending_requests():\n-            slots.append(_scheduler_slot_read(self.scheduler.next_request()))\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdir)\n \n-        for i in range(0, len(_SLOTS), 2):\n-            self.assertNotEqual(slots[i], slots[i+1])\n-\n-    def test_is_meta_set(self):\n-        url = \"http://foo.com/a\"\n-        request = Request(url)\n-        if _scheduler_slot_read(request):\n-            _scheduler_slot_write(request, None)\n-        self.scheduler.enqueue_request(request)\n-        self.assertIsNotNone(_scheduler_slot_read(request, None), None)\n-\n-\n-class TestSchedulerWithRoundRobinOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n-    priority_queue_cls = 'scrapy.pqueues.RoundRobinPriorityQueue'\n-\n-    def test_round_robin(self):\n-        for url, slot in _SLOTS:\n-            request = Request(url)\n-            _scheduler_slot_write(request, slot)\n-            self.scheduler.enqueue_request(request)\n-\n-        self.close_scheduler()\n-        self.create_scheduler()\n-\n-        slots = list()\n-        while self.scheduler.has_pending_requests():\n-            slots.append(_scheduler_slot_read(self.scheduler.next_request()))\n-\n-        for i in range(0, len(_SLOTS), 2):\n-            self.assertNotEqual(slots[i], slots[i+1])\n-\n-    def test_is_meta_set(self):\n-        url = \"http://foo.com/a\"\n-        request = Request(url)\n-        if _scheduler_slot_read(request):\n-            _scheduler_slot_write(request, None)\n-        self.scheduler.enqueue_request(request)\n-\n-        self.close_scheduler()\n-        self.create_scheduler()\n-\n-        self.assertIsNotNone(_scheduler_slot_read(request, None), None)\n-\n-\n-@contextlib.contextmanager\n-def mkdtemp():\n-    dir = tempfile.mkdtemp()\n-    try:\n-        yield dir\n-    finally:\n-        shutil.rmtree(dir)\n-\n-\n-def _migration():\n-\n-    with mkdtemp() as tmp_dir:\n+    def _migration(self, tmp_dir):\n         prev_scheduler_handler = SchedulerHandler()\n         prev_scheduler_handler.priority_queue_cls = 'queuelib.PriorityQueue'\n         prev_scheduler_handler.jobdir = tmp_dir\n@@ -232,18 +176,18 @@ def _migration():\n         prev_scheduler_handler.close_scheduler()\n \n         next_scheduler_handler = SchedulerHandler()\n-        next_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.RoundRobinPriorityQueue'\n+        next_scheduler_handler.priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n         next_scheduler_handler.jobdir = tmp_dir\n \n         next_scheduler_handler.create_scheduler()\n \n-\n-class TestMigration(unittest.TestCase):\n     def test_migration(self):\n-        self.assertRaises(ValueError, _migration)\n+        with self.assertRaises(ValueError):\n+            self._migration(self.tmpdir)\n \n \n-class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n+class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester,\n+                                               unittest.TestCase):\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n \n     def test_logic(self):\n@@ -266,10 +210,12 @@ class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester, unit\n         self.assertEqual(len(slots), len(_SLOTS))\n \n         for request in requests:\n-            self.mock_crawler.signals.send_catch_log(signal=response_downloaded,\n+            self.mock_crawler.signals.send_catch_log(\n+                    signal=response_downloaded,\n                     request=request,\n                     response=None,\n-                                                     spider=self.spider)\n+                    spider=self.spider\n+                    )\n \n         unique_slots = len(set(s for _, s in _SLOTS))\n         for i in range(0, len(_SLOTS), unique_slots):\n@@ -277,8 +223,10 @@ class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester, unit\n             self.assertEqual(len(part), len(set(part)))\n \n \n-class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n+class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n+                                             unittest.TestCase):\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n+\n     def test_logic(self):\n         for url, slot in _SLOTS:\n             request = Request(url)\n@@ -304,10 +252,12 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester, unittest\n         self.assertEqual(len(slots), len(_SLOTS))\n \n         for request in requests:\n-            self.mock_crawler.signals.send_catch_log(signal=response_downloaded,\n+            self.mock_crawler.signals.send_catch_log(\n+                    signal=response_downloaded,\n                     request=request,\n                     response=None,\n-                                                     spider=self.spider)\n+                    spider=self.spider\n+                    )\n \n         unique_slots = len(set(s for _, s in _SLOTS))\n         for i in range(0, len(_SLOTS), unique_slots):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e06b9a81672ec432d2fccc3cbacc823ea47b656", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 24 | Churn Cumulative: 312 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from queuelib import PriorityQueue\n from scrapy.core.downloader import Downloader\n from scrapy.http import Request\n from scrapy.signals import request_reached_downloader, response_downloaded\n+from scrapy.utils.httpobj import urlparse_cached\n \n \n logger = logging.getLogger(__name__)\n@@ -41,11 +42,26 @@ def _scheduler_slot_write(request, slot):\n \n def _scheduler_slot(request):\n \n-    slot = _scheduler_slot_read(request, None)\n-    if slot is None:\n-        url = _get_from_request(request, 'url')\n+    if isinstance(request, dict):\n+        meta = request.get('meta', dict())\n+    elif isinstance(request, Request):\n+        meta = request.meta\n+    else:\n+        raise ValueError('Bad type of request \"%s\"' % (request.__class__, ))\n+\n+    slot = meta.get(SCHEDULER_SLOT_META_KEY, None)\n+\n+    if slot is not None:\n+        return slot\n+\n+    if isinstance(request, dict):\n+        url = request.get('url', None)\n         slot = urlparse(url).hostname or ''\n-        _scheduler_slot_write(request, slot)\n+    elif isinstance(request, Request):\n+        url = request.url\n+        slot = urlparse_cached(request).hostname or ''\n+\n+    meta[SCHEDULER_SLOT_META_KEY] = slot\n \n     return slot\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#484927b08caff66ea622f8553468c831154df30a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 321 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,16 +28,11 @@ def _get_from_request(request, key, default=None):\n \n \n def _scheduler_slot_read(request, default=None):\n-    meta = _get_from_request(request, 'meta', dict())\n-    slot = meta.get(SCHEDULER_SLOT_META_KEY, default)\n-    return slot\n+    return request.meta.get(SCHEDULER_SLOT_META_KEY, default)\n \n \n def _scheduler_slot_write(request, slot):\n-    meta = _get_from_request(request, 'meta', None)\n-    if not isinstance(meta, dict):\n-        raise ValueError('No meta attribute in %s' % (request, ))\n-    meta[SCHEDULER_SLOT_META_KEY] = slot\n+    request.meta[SCHEDULER_SLOT_META_KEY] = slot\n \n \n def _scheduler_slot(request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a46613afa8acd136f4ba62df2ced2f3c87679512", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 327 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -64,10 +64,8 @@ def _scheduler_slot(request):\n def _pathable(x):\n     pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_' for c in x])\n \n-    \"\"\"\n-        as we replace some letters we can get collision for different slots\n-        add we add unique part\n-    \"\"\"\n+    # as we replace some letters we can get collision for different slots\n+    # add we add unique part\n     unique_slot = hashlib.md5(x.encode('utf8')).hexdigest()\n \n     return '-'.join([pathable_slot, unique_slot])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d970be64cc47c382bd615cd547e7e94c17e27b48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 46 | Churn Cumulative: 473 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,12 +2,17 @@ import shutil\n import tempfile\n import unittest\n \n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+\n from scrapy.crawler import Crawler\n from scrapy.core.scheduler import Scheduler\n from scrapy.http import Request\n from scrapy.pqueues import _scheduler_slot_read, _scheduler_slot_write\n from scrapy.signals import request_reached_downloader, response_downloaded\n from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n+from tests.mockserver import MockServer\n \n \n class MockCrawler(Crawler):\n@@ -223,6 +228,13 @@ class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester,\n             self.assertEqual(len(part), len(set(part)))\n \n \n+def _is_slots_unique(base_slots, result_slots):\n+    unique_slots = len(set(s for _, s in base_slots))\n+    for i in range(0, len(result_slots), unique_slots):\n+        part = result_slots[i:i + unique_slots]\n+        assert len(part) == len(set(part))\n+\n+\n class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n                                              unittest.TestCase):\n     priority_queue_cls = 'scrapy.pqueues.DownloaderAwarePriorityQueue'\n@@ -259,7 +271,33 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n                     spider=self.spider\n                     )\n \n-        unique_slots = len(set(s for _, s in _SLOTS))\n-        for i in range(0, len(_SLOTS), unique_slots):\n-            part = slots[i:i + unique_slots]\n-            self.assertEqual(len(part), len(set(part)))\n+        _is_slots_unique(_SLOTS, slots)\n+\n+\n+class StartUrlsSpider(Spider):\n+\n+    def __init__(self, start_urls):\n+        self.start_urls = start_urls\n+\n+\n+class TestIntegrationWithDownloaderAwareOnDisk(TestCase):\n+    def setUp(self):\n+        self.crawler = get_crawler(\n+                    StartUrlsSpider,\n+                    {'SCHEDULER_PRIORITY_QUEUE': 'scrapy.pqueues.DownloaderAwarePriorityQueue',\n+                     'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter'}\n+                    )\n+\n+    @defer.inlineCallbacks\n+    def tearDown(self):\n+        yield self.crawler.stop()\n+\n+    @defer.inlineCallbacks\n+    def test_integration_downloader_aware_priority_queue(self):\n+        with MockServer() as mockserver:\n+\n+            url = mockserver.url(\"/status?n=200\", is_secure=False)\n+            slots = [url] * 6\n+            yield self.crawler.crawl(slots)\n+            self.assertEqual(self.crawler.stats.get_value('downloader/response_count'),\n+                             len(slots))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#987c2ae4a964e45120c245235c9b0c49dc36b71f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 490 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -301,3 +301,20 @@ class TestIntegrationWithDownloaderAwareOnDisk(TestCase):\n             yield self.crawler.crawl(slots)\n             self.assertEqual(self.crawler.stats.get_value('downloader/response_count'),\n                              len(slots))\n+\n+\n+class TestIncompatibility(unittest.TestCase):\n+\n+    def _incompatible(self):\n+        settings = dict(\n+                SCHEDULER_PRIORITY_QUEUE='scrapy.pqueues.DownloaderAwarePriorityQueue',\n+                CONCURRENT_REQUESTS_PER_IP=1\n+                )\n+        crawler = Crawler(Spider, settings)\n+        scheduler = Scheduler.from_crawler(crawler)\n+        spider = Spider(name='spider')\n+        scheduler.open(spider)\n+\n+    def test_incompatibility(self):\n+        with self.assertRaises(ValueError):\n+            self._incompatible()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e8ce301b1a56e40f7e9c322a7b73b8dcfcefc43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 335 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -171,6 +171,14 @@ class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n         return cls(crawler, qfactory, startprios)\n \n     def __init__(self, crawler, qfactory, startprios={}):\n+        ip_concurrency_key = 'CONCURRENT_REQUESTS_PER_IP'\n+        ip_concurrency = crawler.settings.getint(ip_concurrency_key, 0)\n+\n+        if ip_concurrency > 0:\n+            raise ValueError('\"%s\" does not support %s=%d' % (self.__class__,\n+                                                              ip_concurrency_key,\n+                                                              ip_concurrency))\n+\n         super(DownloaderAwarePriorityQueue, self).__init__(qfactory,\n                                                            startprios)\n         self._slots = {slot: 0 for slot in self.pqueues}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c148fce5acc100f5f01719db374578bfca2512a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 16 | Churn Cumulative: 402 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,7 @@ See documentation in docs/topics/item.rst\n \n from pprint import pformat\n from collections import MutableMapping\n+from copy import deepcopy\n \n from abc import ABCMeta\n import six\n@@ -96,6 +97,13 @@ class DictItem(MutableMapping, BaseItem):\n     def copy(self):\n         return self.__class__(self)\n \n+    def deepcopy(self):\n+        \"\"\"Return a `deep copy`_ of this item.\n+\n+        .. _deep copy: https://docs.python.org/library/copy.html#copy.deepcopy\n+        \"\"\"\n+        return deepcopy(self)\n+\n \n @six.add_metaclass(ItemMeta)\n class Item(DictItem):\n\n@@ -249,6 +249,14 @@ class ItemTest(unittest.TestCase):\n         copied_item['name'] = copied_item['name'].upper()\n         self.assertNotEqual(item['name'], copied_item['name'])\n \n+    def test_deepcopy(self):\n+        class TestItem(Item):\n+            tags = Field()\n+        item = TestItem({'tags': ['tag1']})\n+        copied_item = item.deepcopy()\n+        item['tags'].append('tag2')\n+        assert item['tags'] != copied_item['tags']\n+\n \n class ItemMetaTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bbf24b7a1ce2e91eab57d1b8524d398822a1ddd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 63 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,6 +10,7 @@ import six\n \n from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n+from scrapy.utils.python import get_func_args\n from scrapy.spiders import Spider\n \n \n@@ -35,10 +36,8 @@ class Rule(object):\n         Wrapper around the request processing function to maintain backward compatibility\n         with functions that do not take a Response object as parameter.\n         \"\"\"\n-        argcount = self.process_request.__code__.co_argcount\n-        if hasattr(self.process_request, '__self__'):\n-            argcount = argcount - 1\n-        args = [request] if argcount == 1 else [request, response]\n+        arg_count = len(get_func_args(self.process_request))\n+        args = [request] if arg_count == 1 else [request, response]\n         return self.process_request(*args)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56929e77d98391255b77ffd3350abb49da18009e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 25 | Churn Cumulative: 88 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,16 +6,19 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n import copy\n+import warnings\n+\n import six\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.python import get_func_args\n from scrapy.spiders import Spider\n \n \n-def identity(x):\n-    return x\n+def _identity(request, response):\n+    return request\n \n \n class Rule(object):\n@@ -25,19 +28,21 @@ class Rule(object):\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n-        self.process_request = process_request or identity\n-        if follow is None:\n-            self.follow = False if callback else True\n-        else:\n-            self.follow = follow\n+        self.process_request = process_request or _identity\n+        self.follow = follow if follow is not None else not callback\n \n     def _process_request(self, request, response):\n         \"\"\"\n-        Wrapper around the request processing function to maintain backward compatibility\n-        with functions that do not take a Response object as parameter.\n+        Wrapper around the request processing function to maintain backward\n+        compatibility with functions that do not take a Response object\n         \"\"\"\n         arg_count = len(get_func_args(self.process_request))\n-        args = [request] if arg_count == 1 else [request, response]\n+        if arg_count == 1:\n+            args = [request]\n+            msg = 'Rule.process_request should accept two arguments (request, response), accepting only one is deprecated'\n+            warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=2)\n+        else:\n+            args = [request, response]\n         return self.process_request(*args)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1b4385b7e3f78694c0378455644b539d80d293a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 41 | Churn Cumulative: 614 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,13 @@ def _identity(request, response):\n     return request\n \n \n+def _get_method(method, spider):\n+    if callable(method):\n+        return method\n+    elif isinstance(method, six.string_types):\n+        return getattr(spider, method, None)\n+\n+\n class Rule(object):\n \n     def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n@@ -29,20 +36,24 @@ class Rule(object):\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n         self.process_request = process_request or _identity\n+        self.process_request_argcount = None\n         self.follow = follow if follow is not None else not callback\n \n+    def _compile(self, spider):\n+        self.callback = _get_method(self.callback, spider)\n+        self.process_links = _get_method(self.process_links, spider)\n+        self.process_request = _get_method(self.process_request, spider)\n+        self.process_request_argcount = len(get_func_args(self.process_request))\n+        if self.process_request_argcount == 1:\n+            msg = 'Rule.process_request should accept two arguments (request, response), accepting only one is deprecated'\n+            warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=2)\n+\n     def _process_request(self, request, response):\n         \"\"\"\n         Wrapper around the request processing function to maintain backward\n         compatibility with functions that do not take a Response object\n         \"\"\"\n-        arg_count = len(get_func_args(self.process_request))\n-        if arg_count == 1:\n-            args = [request]\n-            msg = 'Rule.process_request should accept two arguments (request, response), accepting only one is deprecated'\n-            warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=2)\n-        else:\n-            args = [request, response]\n+        args = [request] if self.process_request_argcount == 1 else [request, response]\n         return self.process_request(*args)\n \n \n@@ -98,17 +109,9 @@ class CrawlSpider(Spider):\n                 yield request_or_item\n \n     def _compile_rules(self):\n-        def get_method(method):\n-            if callable(method):\n-                return method\n-            elif isinstance(method, six.string_types):\n-                return getattr(self, method, None)\n-\n         self._rules = [copy.copy(r) for r in self.rules]\n         for rule in self._rules:\n-            rule.callback = get_method(rule.callback)\n-            rule.process_links = get_method(rule.process_links)\n-            rule.process_request = get_method(rule.process_request)\n+            rule._compile(self)\n \n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n\n@@ -275,6 +275,7 @@ class CrawlSpiderTest(SpiderTest):\n                 Rule(LinkExtractor(), process_request=process_request_change_domain),\n             )\n \n+        with warnings.catch_warnings(record=True) as cw:\n             spider = _CrawlSpider()\n             output = list(spider._requests_to_follow(response))\n             self.assertEqual(len(output), 3)\n@@ -283,6 +284,8 @@ class CrawlSpiderTest(SpiderTest):\n                              ['http://example.com/somepage/item/12.html',\n                               'http://example.com/about.html',\n                               'http://example.com/nofollow.html'])\n+            self.assertEqual(len(cw), 1)\n+            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n \n     def test_process_request_with_response(self):\n \n@@ -324,6 +327,7 @@ class CrawlSpiderTest(SpiderTest):\n             def process_request_upper(self, request):\n                 return request.replace(url=request.url.upper())\n \n+        with warnings.catch_warnings(record=True) as cw:\n             spider = _CrawlSpider()\n             output = list(spider._requests_to_follow(response))\n             self.assertEqual(len(output), 3)\n@@ -332,6 +336,8 @@ class CrawlSpiderTest(SpiderTest):\n                              ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',\n                               'http://EXAMPLE.ORG/ABOUT.HTML',\n                               'http://EXAMPLE.ORG/NOFOLLOW.HTML'])\n+            self.assertEqual(len(cw), 1)\n+            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n \n     def test_process_request_instance_method_with_response(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90934959d07db881aec5933fd7b77bcd2dccfa4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 100 | Files Changed: 2 | Hunks: 37 | Methods Changed: 32 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 182 | Churn Cumulative: 1007 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 0.35294117647058826\n\nDIFF:\n@@ -1,6 +1,6 @@\n import hashlib\n import logging\n-from six import text_type\n+from collections import namedtuple\n from six.moves.urllib.parse import urlparse\n \n from queuelib import PriorityQueue\n@@ -17,12 +17,12 @@ logger = logging.getLogger(__name__)\n SCHEDULER_SLOT_META_KEY = Downloader.DOWNLOAD_SLOT\n \n \n-def _get_from_request(request, key, default=None):\n+def _get_request_meta(request):\n     if isinstance(request, dict):\n-        return request.get(key, default)\n+        return request.setdefault('meta', {})\n \n     if isinstance(request, Request):\n-        return getattr(request, key, default)\n+        return request.meta\n \n     raise ValueError('Bad type of request \"%s\"' % (request.__class__, ))\n \n@@ -35,15 +35,8 @@ def _scheduler_slot_write(request, slot):\n     request.meta[SCHEDULER_SLOT_META_KEY] = slot\n \n \n-def _scheduler_slot(request):\n-\n-    if isinstance(request, dict):\n-        meta = request.get('meta', dict())\n-    elif isinstance(request, Request):\n-        meta = request.meta\n-    else:\n-        raise ValueError('Bad type of request \"%s\"' % (request.__class__, ))\n-\n+def _set_scheduler_slot(request):\n+    meta = _get_request_meta(request)\n     slot = meta.get(SCHEDULER_SLOT_META_KEY, None)\n \n     if slot is not None:\n@@ -53,43 +46,29 @@ def _scheduler_slot(request):\n         url = request.get('url', None)\n         slot = urlparse(url).hostname or ''\n     elif isinstance(request, Request):\n-        url = request.url\n         slot = urlparse_cached(request).hostname or ''\n \n     meta[SCHEDULER_SLOT_META_KEY] = slot\n-\n     return slot\n \n \n-def _pathable(x):\n-    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_' for c in x])\n-\n+def _path_safe(text):\n+    \"\"\" Return a filesystem-safe version of a string ``text`` \"\"\"\n+    pathable_slot = \"\".join([c if c.isalnum() or c in '-._' else '_'\n+                             for c in text])\n     # as we replace some letters we can get collision for different slots\n     # add we add unique part\n-    unique_slot = hashlib.md5(x.encode('utf8')).hexdigest()\n-\n+    unique_slot = hashlib.md5(text.encode('utf8')).hexdigest()\n     return '-'.join([pathable_slot, unique_slot])\n \n \n-class PrioritySlot:\n-    __slots__ = ('priority', 'slot')\n-\n-    def __init__(self, priority=0, slot=None):\n-        self.priority = priority\n-        self.slot = slot\n-\n-    def __hash__(self):\n-        return hash((self.priority, self.slot))\n-\n-    def __eq__(self, other):\n-        return (self.priority, self.slot) == (other.priority, other.slot)\n-\n-    def __lt__(self, other):\n-        return (self.priority, self.slot) < (other.priority, other.slot)\n+class PrioritySlot(namedtuple(\"PrioritySlot\", [\"priority\", \"slot\"])):\n+    \"\"\" ``(priority, slot)`` tuple which uses a path-safe slot name\n+    when converting to str \"\"\"\n+    __slots__ = ()\n \n     def __str__(self):\n-        return '_'.join([text_type(self.priority),\n-                         _pathable(text_type(self.slot))])\n+        return '%s_%s' % (self.priority, _path_safe(str(self.slot)))\n \n \n class PriorityAsTupleQueue(PriorityQueue):\n@@ -99,78 +78,65 @@ class PriorityAsTupleQueue(PriorityQueue):\n     json serializable structures\n     \"\"\"\n     def __init__(self, qfactory, startprios=()):\n-\n+        startprios = [PrioritySlot(priority=p[0], slot=p[1])\n+                      for p in startprios]\n         super(PriorityAsTupleQueue, self).__init__(\n-                qfactory,\n-                [PrioritySlot(priority=p[0], slot=p[1]) for p in startprios]\n-                )\n-\n-    def close(self):\n-        startprios = super(PriorityAsTupleQueue, self).close()\n-        return [(s.priority, s.slot) for s in startprios]\n-\n-    def is_empty(self):\n-        return not self.queues or len(self) == 0\n+            qfactory=qfactory,\n+            startprios=startprios)\n \n \n-class SlotBasedPriorityQueue(object):\n+class SlotPriorityQueues(object):\n+    \"\"\" Container for multiple priority queues. \"\"\"\n+    def __init__(self, pqfactory, slot_startprios=None):\n+        \"\"\"\n+        ``pqfactory`` is a factory for creating new PriorityQueues.\n+        It must be a function which accepts a single optional ``startprios``\n+        argument, with a list of priorities to create queues for.\n \n-    def __init__(self, qfactory, startprios={}):\n-        self.pqueues = dict()     # slot -> priority queue\n-        self.qfactory = qfactory  # factory for creating new internal queues\n-\n-        if not startprios:\n-            return\n-\n-        if not isinstance(startprios, dict):\n-            raise ValueError(\"Looks like your priorities file malforfemed. \"\n-                             \"Possible reason: You run scrapy with previous \"\n-                             \"version. Interrupted it. Updated scrapy. And \"\n-                             \"run again.\")\n-\n-        for slot, prios in startprios.items():\n-            self.pqueues[slot] = PriorityAsTupleQueue(self.qfactory, prios)\n+        ``slot_startprios`` is a ``{slot: startprios}`` dict.\n+        \"\"\"\n+        self.pqfactory = pqfactory\n+        self.pqueues = {}  # slot -> priority queue\n+        for slot, startprios in (slot_startprios or {}).items():\n+            self.pqueues[slot] = self.pqfactory(startprios)\n \n     def pop_slot(self, slot):\n+        \"\"\" Pop an object from a priority queue for this slot \"\"\"\n         queue = self.pqueues[slot]\n         request = queue.pop()\n-        is_empty = queue.is_empty()\n-        if is_empty:\n+        if len(queue) == 0:\n             del self.pqueues[slot]\n+        return request\n \n-        return request, is_empty\n-\n-    def push_slot(self, request, priority):\n-        slot = _scheduler_slot(request)\n-        is_new = False\n+    def push_slot(self, slot, obj, priority):\n+        \"\"\" Push an object to a priority queue for this slot \"\"\"\n         if slot not in self.pqueues:\n-            self.pqueues[slot] = PriorityAsTupleQueue(self.qfactory)\n+            self.pqueues[slot] = self.pqfactory()\n         queue = self.pqueues[slot]\n-        is_new = queue.is_empty()\n-        queue.push(request, PrioritySlot(priority=priority, slot=slot))\n-        return slot, is_new\n+        queue.push(obj, priority)\n \n     def close(self):\n-        startprios = dict()\n-        for slot, queue in self.pqueues.items():\n-            prios = queue.close()\n-            startprios[slot] = prios\n+        active = {slot: queue.close()\n+                  for slot, queue in self.pqueues.items()}\n         self.pqueues.clear()\n-        return startprios\n+        return active\n \n     def __len__(self):\n         return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n \n+    def __contains__(self, slot):\n+        return slot in self.pqueues\n \n-class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n+\n+class DownloaderAwarePriorityQueue(object):\n \n     _DOWNLOADER_AWARE_PQ_ID = 'DOWNLOADER_AWARE_PQ_ID'\n \n     @classmethod\n-    def from_crawler(cls, crawler, qfactory, startprios={}):\n+    def from_crawler(cls, crawler, qfactory, startprios=None):\n         return cls(crawler, qfactory, startprios)\n \n-    def __init__(self, crawler, qfactory, startprios={}):\n+    def __init__(self, crawler, qfactory, startprios=None):\n         ip_concurrency_key = 'CONCURRENT_REQUESTS_PER_IP'\n         ip_concurrency = crawler.settings.getint(ip_concurrency_key, 0)\n \n@@ -179,16 +145,25 @@ class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n                                                               ip_concurrency_key,\n                                                               ip_concurrency))\n \n-        super(DownloaderAwarePriorityQueue, self).__init__(qfactory,\n-                                                           startprios)\n-        self._slots = {slot: 0 for slot in self.pqueues}\n+        def pqfactory(startprios=()):\n+            return PriorityAsTupleQueue(qfactory, startprios)\n+\n+        if startprios and not isinstance(startprios, dict):\n+            raise ValueError(\"DownloaderAwarePriorityQueue accepts \"\n+                             \"``startprios`` as a dict; %r instance is passed.\"\n+                             \" Only a crawl started with the same priority \"\n+                             \"queue class can be resumed.\" % startprios.__class__)\n+        self._slot_pqueues = SlotPriorityQueues(pqfactory,\n+                                                slot_startprios=startprios)\n+\n+        self._active_downloads = {slot: 0 for slot in self._slot_pqueues.pqueues}\n         crawler.signals.connect(self.on_response_download,\n                                 signal=response_downloaded)\n         crawler.signals.connect(self.on_request_reached_downloader,\n                                 signal=request_reached_downloader)\n \n     def mark(self, request):\n-        meta = _get_from_request(request, 'meta', None)\n+        meta = _get_request_meta(request)\n         if not isinstance(meta, dict):\n             raise ValueError('No meta attribute in %s' % (request, ))\n         meta[self._DOWNLOADER_AWARE_PQ_ID] = id(self)\n@@ -197,39 +172,46 @@ class DownloaderAwarePriorityQueue(SlotBasedPriorityQueue):\n         return request.meta.get(self._DOWNLOADER_AWARE_PQ_ID, None) == id(self)\n \n     def pop(self):\n-        slots = [(d, s) for s, d in self._slots.items() if s in self.pqueues]\n+        slots = [(active_downloads, slot)\n+                 for slot, active_downloads in self._active_downloads.items()\n+                 if slot in self._slot_pqueues]\n \n         if not slots:\n             return\n \n         slot = min(slots)[1]\n-        request, _ = self.pop_slot(slot)\n+        request = self._slot_pqueues.pop_slot(slot)\n         self.mark(request)\n         return request\n \n     def push(self, request, priority):\n-        slot, _ = self.push_slot(request, priority)\n-        if slot not in self._slots:\n-            self._slots[slot] = 0\n+        slot = _set_scheduler_slot(request)\n+        priority_slot = PrioritySlot(priority=priority, slot=slot)\n+        self._slot_pqueues.push_slot(slot, request, priority_slot)\n+        if slot not in self._active_downloads:\n+            self._active_downloads[slot] = 0\n \n     def on_response_download(self, response, request, spider):\n         if not self.check_mark(request):\n             return\n \n         slot = _scheduler_slot_read(request)\n-        if slot not in self._slots or self._slots[slot] <= 0:\n+        if slot not in self._active_downloads or self._active_downloads[slot] <= 0:\n             raise ValueError('Get response for wrong slot \"%s\"' % (slot, ))\n-        self._slots[slot] = self._slots[slot] - 1\n-        if self._slots[slot] == 0 and slot not in self.pqueues:\n-            del self._slots[slot]\n+        self._active_downloads[slot] = self._active_downloads[slot] - 1\n+        if self._active_downloads[slot] == 0 and slot not in self._slot_pqueues:\n+            del self._active_downloads[slot]\n \n     def on_request_reached_downloader(self, request, spider):\n         if not self.check_mark(request):\n             return\n \n         slot = _scheduler_slot_read(request)\n-        self._slots[slot] = self._slots.get(slot, 0) + 1\n+        self._active_downloads[slot] = self._active_downloads.get(slot, 0) + 1\n \n     def close(self):\n-        self._slots.clear()\n-        return super(DownloaderAwarePriorityQueue, self).close()\n+        self._active_downloads.clear()\n+        return self._slot_pqueues.close()\n+\n+    def __len__(self):\n+        return len(self._slot_pqueues)\n\n@@ -260,7 +260,7 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n                     )\n             requests.append(request)\n \n-        self.assertEqual(self.scheduler.mqs._slots, {})\n+        self.assertEqual(self.scheduler.mqs._active_downloads, {})\n         self.assertEqual(len(slots), len(_SLOTS))\n \n         for request in requests:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#757f53a32461ef0c3d2fe4caf64197f67271b5f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 1026 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 0.5\n\nDIFF:\n@@ -36,6 +36,12 @@ def _scheduler_slot_write(request, slot):\n \n \n def _set_scheduler_slot(request):\n+    \"\"\"\n+        >>> _set_scheduler_slot({'url':'http://foo.com'}) == _set_scheduler_slot({'url':'http://bar.com'})\n+        False\n+        >>> _set_scheduler_slot({'url':'http://foo.com'}) == _set_scheduler_slot({'url':'http://foo.com'})\n+        True\n+    \"\"\"\n     meta = _get_request_meta(request)\n     slot = meta.get(SCHEDULER_SLOT_META_KEY, None)\n \n@@ -141,9 +147,8 @@ class DownloaderAwarePriorityQueue(object):\n         ip_concurrency = crawler.settings.getint(ip_concurrency_key, 0)\n \n         if ip_concurrency > 0:\n-            raise ValueError('\"%s\" does not support %s=%d' % (self.__class__,\n-                                                              ip_concurrency_key,\n-                                                              ip_concurrency))\n+            raise ValueError('\"%s\" does not support setting %s' % (self.__class__,\n+                                                                   ip_concurrency_key))\n \n         def pqfactory(startprios=()):\n             return PriorityAsTupleQueue(qfactory, startprios)\n@@ -171,6 +176,9 @@ class DownloaderAwarePriorityQueue(object):\n     def check_mark(self, request):\n         return request.meta.get(self._DOWNLOADER_AWARE_PQ_ID, None) == id(self)\n \n+    def unmark(self, request):\n+        del request.meta[self._DOWNLOADER_AWARE_PQ_ID]\n+\n     def pop(self):\n         slots = [(active_downloads, slot)\n                  for slot, active_downloads in self._active_downloads.items()\n@@ -194,6 +202,7 @@ class DownloaderAwarePriorityQueue(object):\n     def on_response_download(self, response, request, spider):\n         if not self.check_mark(request):\n             return\n+        self.unmark(request)\n \n         slot = _scheduler_slot_read(request)\n         if slot not in self._active_downloads or self._active_downloads[slot] <= 0:\n\n@@ -248,8 +248,8 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n         self.close_scheduler()\n         self.create_scheduler()\n \n-        slots = list()\n-        requests = list()\n+        slots = []\n+        requests = []\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             slots.append(_scheduler_slot_read(request))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b1db71dac8716878ff1b94ee0d1095e5c80795f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 11 | Methods Changed: 15 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 77 | Churn Cumulative: 1916 | Contributors (this commit): 9 | Commits (past 90d): 21 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -188,6 +188,9 @@ class Downloader(object):\n         def finish_transferring(_):\n             slot.transferring.remove(request)\n             self._process_queue(spider, slot)\n+            self.signals.send_catch_log(signal=signals.request_left_downloader,\n+                                        request=request,\n+                                        spider=spider)\n             return _\n \n         return dfd.addBoth(finish_transferring)\n\n@@ -7,7 +7,7 @@ from queuelib import PriorityQueue\n \n from scrapy.core.downloader import Downloader\n from scrapy.http import Request\n-from scrapy.signals import request_reached_downloader, response_downloaded\n+from scrapy.signals import request_reached_downloader, request_left_downloader\n from scrapy.utils.httpobj import urlparse_cached\n \n \n@@ -163,7 +163,7 @@ class DownloaderAwarePriorityQueue(object):\n \n         self._active_downloads = {slot: 0 for slot in self._slot_pqueues.pqueues}\n         crawler.signals.connect(self.on_response_download,\n-                                signal=response_downloaded)\n+                                signal=request_left_downloader)\n         crawler.signals.connect(self.on_request_reached_downloader,\n                                 signal=request_reached_downloader)\n \n@@ -199,7 +199,7 @@ class DownloaderAwarePriorityQueue(object):\n         if slot not in self._active_downloads:\n             self._active_downloads[slot] = 0\n \n-    def on_response_download(self, response, request, spider):\n+    def on_response_download(self, request, spider):\n         if not self.check_mark(request):\n             return\n         self.unmark(request)\n\n@@ -14,6 +14,7 @@ spider_error = object()\n request_scheduled = object()\n request_dropped = object()\n request_reached_downloader = object()\n+request_left_downloader = object()\n response_received = object()\n response_downloaded = object()\n item_scraped = object()\n\n@@ -0,0 +1,59 @@\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase\n+from scrapy.signals import request_left_downloader\n+from scrapy.spiders import Spider\n+from scrapy.utils.test import get_crawler\n+from tests.mockserver import MockServer\n+\n+class SignalCatcherSpider(Spider):\n+    name = 'signal_catcher'\n+\n+    def __init__(self, crawler, url, *args, **kwargs):\n+        super(SignalCatcherSpider, self).__init__(*args, **kwargs)\n+        crawler.signals.connect(self.on_response_download,\n+                                signal=request_left_downloader)\n+        self.catched_times = 0\n+        self.start_urls = [url]\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = cls(crawler, *args, **kwargs)\n+        return spider\n+\n+    def on_response_download(self, request, spider):\n+        self.catched_times = self.catched_times + 1\n+\n+\n+class TestCatching(TestCase):\n+\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_success(self):\n+        crawler = get_crawler(SignalCatcherSpider)\n+        yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n+        self.assertEqual(crawler.spider.catched_times, 1)\n+\n+    @defer.inlineCallbacks\n+    def test_timeout(self):\n+        crawler = get_crawler(SignalCatcherSpider,\n+                              {'DOWNLOAD_TIMEOUT': 0.1})\n+        yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n+        self.assertEqual(crawler.spider.catched_times, 1)\n+\n+    @defer.inlineCallbacks\n+    def test_disconnect(self):\n+        crawler = get_crawler(SignalCatcherSpider)\n+        yield crawler.crawl(self.mockserver.url(\"/drop\"))\n+        self.assertEqual(crawler.spider.catched_times, 1)\n+\n+    @defer.inlineCallbacks\n+    def test_noconnect(self):\n+        crawler = get_crawler(SignalCatcherSpider)\n+        yield crawler.crawl('http://thereisdefinetelynosuchdomain.com')\n+        self.assertEqual(crawler.spider.catched_times, 1)\n\n@@ -9,7 +9,7 @@ from scrapy.crawler import Crawler\n from scrapy.core.scheduler import Scheduler\n from scrapy.http import Request\n from scrapy.pqueues import _scheduler_slot_read, _scheduler_slot_write\n-from scrapy.signals import request_reached_downloader, response_downloaded\n+from scrapy.signals import request_reached_downloader, request_left_downloader\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n@@ -216,9 +216,8 @@ class TestSchedulerWithDownloaderAwareInMemory(BaseSchedulerInMemoryTester,\n \n         for request in requests:\n             self.mock_crawler.signals.send_catch_log(\n-                    signal=response_downloaded,\n+                    signal=request_left_downloader,\n                     request=request,\n-                    response=None,\n                     spider=self.spider\n                     )\n \n@@ -265,9 +264,8 @@ class TestSchedulerWithDownloaderAwareOnDisk(BaseSchedulerOnDiskTester,\n \n         for request in requests:\n             self.mock_crawler.signals.send_catch_log(\n-                    signal=response_downloaded,\n+                    signal=request_left_downloader,\n                     request=request,\n-                    response=None,\n                     spider=self.spider\n                     )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
