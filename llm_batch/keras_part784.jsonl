{"custom_id": "keras#e5f92f350719861fbca7abcd69f3730bd00ac096", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 100 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -260,9 +260,9 @@ class ArrayDataAdapter(DataAdapter):\n             def __init__(self, array):\n                 self.array = array\n \n-            def __getitem__(self, index):\n+            def __getitems__(self, indices):\n                 def slice_and_convert(x):\n-                    return convert_to_tensor(x[index])\n+                    return convert_to_tensor(np.take(x, indices, axis=0))\n \n                 return tree.map_structure(slice_and_convert, self.array)\n \n@@ -301,8 +301,15 @@ class ArrayDataAdapter(DataAdapter):\n                 drop_last=False,\n             )\n \n+        # Because ArrayDataset.__getitems__ returns full batches organized in\n+        # the expected structure, there is nothing to collate.\n+        def no_op_collate(batch):\n+            return batch\n+\n         dataset = ArrayDataset(self._inputs)\n-        return torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler)\n+        return torch.utils.data.DataLoader(\n+            dataset, batch_sampler=batch_sampler, collate_fn=no_op_collate\n+        )\n \n     @property\n     def num_batches(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d10fde1b14161712598c56f8c55b1b1fa28a0923", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 52 | Churn Cumulative: 426 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self.train_function = None\n         self.test_function = None\n         self.predict_function = None\n+        self._jax_state_synced = True\n \n     def compute_loss_and_updates(\n         self,\n@@ -414,7 +415,6 @@ class JAXTrainer(base_trainer.Trainer):\n             # Reattach state to the model (if not already done by a callback).\n             # NOTE: doing this after each step would be a big performance\n             # bottleneck.\n-            if not self._jax_state_synced:\n             self.jax_state_sync()\n \n             # Override with model metrics instead of last step logs\n@@ -559,7 +559,6 @@ class JAXTrainer(base_trainer.Trainer):\n                 break\n \n         # Reattach state back to model (if not already done by a callback).\n-        if not self._jax_state_synced:\n         self.jax_state_sync()\n \n         # The jax spmd_mode is need for multi-process context, since the\n@@ -655,7 +654,6 @@ class JAXTrainer(base_trainer.Trainer):\n             # during predict(), but it's allowed.\n             \"non_trainable_variables\": non_trainable_variables,\n         }\n-        if not self._jax_state_synced:\n         self.jax_state_sync()\n         callbacks.on_predict_end()\n         self._jax_state = None\n@@ -690,18 +688,14 @@ class JAXTrainer(base_trainer.Trainer):\n         self.make_train_function()\n \n         # Train step\n-        trainable_variables = [v.value for v in self.trainable_variables]\n-        non_trainable_variables = [\n-            v.value for v in self.non_trainable_variables\n-        ]\n-        optimizer_variables = [v.value for v in self.optimizer.variables]\n-        metrics_variables = [v.value for v in self.metrics_variables]\n-        state = (\n-            trainable_variables,\n-            non_trainable_variables,\n-            optimizer_variables,\n-            metrics_variables,\n+        state = self._get_jax_state(\n+            trainable_variables=True,\n+            non_trainable_variables=True,\n+            optimizer_variables=True,\n+            metrics_variables=True,\n+            purge_model_variables=False,\n         )\n+        self._jax_state_synced = False\n         logs, state = self.train_function(state, [data])\n \n         # State sync\n@@ -742,16 +736,13 @@ class JAXTrainer(base_trainer.Trainer):\n         self.make_test_function()\n \n         # Test step\n-        trainable_variables = [v.value for v in self.trainable_variables]\n-        non_trainable_variables = [\n-            v.value for v in self.non_trainable_variables\n-        ]\n-        metrics_variables = [v.value for v in self.metrics_variables]\n-        state = (\n-            trainable_variables,\n-            non_trainable_variables,\n-            metrics_variables,\n+        state = self._get_jax_state(\n+            trainable_variables=True,\n+            non_trainable_variables=True,\n+            metrics_variables=True,\n+            purge_model_variables=False,\n         )\n+        self._jax_state_synced = False\n         logs, state = self.test_function(state, [data])\n \n         # State sync\n@@ -777,11 +768,13 @@ class JAXTrainer(base_trainer.Trainer):\n         self._record_training_state_sharding_spec()\n         self.make_predict_function()\n \n-        trainable_variables = [v.value for v in self.trainable_variables]\n-        non_trainable_variables = [\n-            v.value for v in self.non_trainable_variables\n-        ]\n-        state = (trainable_variables, non_trainable_variables)\n+        state = self._get_jax_state(\n+            trainable_variables=True,\n+            non_trainable_variables=True,\n+            metrics_variables=False,\n+            purge_model_variables=False,\n+        )\n+        self._jax_state_synced = False\n         batch_outputs, non_trainable_variables = self.predict_function(\n             state, [(x,)]\n         )\n@@ -793,7 +786,7 @@ class JAXTrainer(base_trainer.Trainer):\n         return batch_outputs\n \n     def jax_state_sync(self):\n-        if not getattr(self, \"_jax_state\", None):\n+        if not getattr(self, \"_jax_state\", None) or self._jax_state_synced:\n             return\n \n         trainable_variables = self._jax_state.get(\"trainable_variables\", None)\n\n@@ -82,7 +82,6 @@ class Callback:\n             # attached to the model in the middle of an\n             # epoch. We have to force a sync before\n             # accessing model state for e.g. checkpointing.\n-            if not self._model._jax_state_synced:\n             self._model.jax_state_sync()\n         return self._model\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#92710ea1f4be052747ba0a057dcd565f8a3c692d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 120 | Lines Deleted: 33 | Files Changed: 6 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 13/4 | Churn Δ: 153 | Churn Cumulative: 1288 | Contributors (this commit): 12 | Commits (past 90d): 52 | Contributors (cumulative): 34 | DMM Complexity: 0.8\n\nDIFF:\n@@ -315,7 +315,8 @@ def while_loop(\n     loop_vars,\n     maximum_iterations=None,\n ):\n-    loop_vars = tuple(loop_vars)\n+    is_tuple = isinstance(loop_vars, (tuple, list))\n+    loop_vars = tuple(loop_vars) if is_tuple else (loop_vars,)\n     if maximum_iterations is not None:\n         current_iter = 0\n         loop_vars = loop_vars + (current_iter,)\n@@ -325,7 +326,9 @@ def while_loop(\n             return cond(*args[:-1]) & (args[-1] < maximum_iterations)\n \n         def _body(args):\n-            return tuple(body(*args[:-1])) + (args[-1] + 1,)\n+            outputs = body(*args[:-1])\n+            outputs = tuple(outputs) if is_tuple else (outputs,)\n+            return outputs + (args[-1] + 1,)\n \n     else:\n \n@@ -333,12 +336,13 @@ def while_loop(\n             return cond(*args)\n \n         def _body(args):\n-            return tuple(body(*args))\n+            outputs = body(*args)\n+            return tuple(outputs) if is_tuple else (outputs,)\n \n     outputs = jax.lax.while_loop(_cond, _body, loop_vars)\n     if maximum_iterations is not None:\n         outputs = outputs[:-1]\n-    return outputs\n+    return outputs if is_tuple else outputs[0]\n \n \n def fori_loop(lower, upper, body_fun, init_val):\n\n@@ -203,14 +203,16 @@ def while_loop(\n     iteration_check = (\n         lambda iter: maximum_iterations is None or iter < maximum_iterations\n     )\n-    loop_vars = tuple([convert_to_tensor(v) for v in loop_vars])\n+    is_tuple = isinstance(loop_vars, (tuple, list))\n+    loop_vars = tuple(loop_vars) if is_tuple else (loop_vars,)\n+    loop_vars = tree.map_structure(convert_to_tensor, loop_vars)\n     while cond(*loop_vars) and iteration_check(current_iter):\n         loop_vars = body(*loop_vars)\n         if not isinstance(loop_vars, (list, tuple)):\n             loop_vars = (loop_vars,)\n         loop_vars = tuple(loop_vars)\n         current_iter += 1\n-    return loop_vars\n+    return loop_vars if is_tuple else loop_vars[0]\n \n \n def fori_loop(lower, upper, body_fun, init_val):\n\n@@ -232,12 +232,20 @@ def while_loop(\n     loop_vars,\n     maximum_iterations=None,\n ):\n-    return tf.while_loop(\n+    is_tuple = isinstance(loop_vars, (tuple, list))\n+    loop_vars = tuple(loop_vars) if is_tuple else (loop_vars,)\n+\n+    def _body(*args):\n+        outputs = body(*args)\n+        return tuple(outputs) if is_tuple else (outputs,)\n+\n+    outputs = tf.while_loop(\n         cond,\n-        body,\n+        _body,\n         loop_vars,\n         maximum_iterations=maximum_iterations,\n     )\n+    return outputs if is_tuple else outputs[0]\n \n \n def fori_loop(lower, upper, body_fun, init_val):\n\n@@ -408,14 +408,16 @@ def while_loop(\n     iteration_check = (\n         lambda iter: maximum_iterations is None or iter < maximum_iterations\n     )\n-    loop_vars = tuple([convert_to_tensor(v) for v in loop_vars])\n+    is_tuple = isinstance(loop_vars, (tuple, list))\n+    loop_vars = tuple(loop_vars) if is_tuple else (loop_vars,)\n+    loop_vars = tree.map_structure(convert_to_tensor, loop_vars)\n     while cond(*loop_vars) and iteration_check(current_iter):\n         loop_vars = body(*loop_vars)\n         if not isinstance(loop_vars, (list, tuple)):\n             loop_vars = (loop_vars,)\n         loop_vars = tuple(loop_vars)\n         current_iter += 1\n-    return loop_vars\n+    return loop_vars if is_tuple else loop_vars[0]\n \n \n def fori_loop(lower, upper, body_fun, init_val):\n\n@@ -235,11 +235,15 @@ def while_loop(\n \n     Args:\n         cond: A callable that represents the termination condition of the loop.\n-            Must have the same number of args as `loop_vars`, and return a bool.\n-        body: A callable that represents the loop body. Must have the same\n-            number of args as `loop_vars`, and return a list/tuple of the same\n-            length, shape and dtype as `loop_vars`.\n-        loop_vars: A list/tuple of tensors, the loop variables.\n+            Must accept a `loop_vars` like structure as an argument. If\n+            `loop_vars` is a tuple or list, each element of `loop_vars` will be\n+            passed positionally to the callable.\n+        body: A callable that represents the loop body. Must accept a\n+            `loop_vars` like structure as an argument, and return update value\n+            with the same structure. If `loop_vars` is a tuple or list, each\n+            element of `loop_vars` will be passed positionally to the callable.\n+        loop_vars: An arbitrary nested structure of tensor state to persist\n+            across loop iterations.\n         maximum_iterations: Optional maximum number of iterations of the while\n             loop to run. If provided, the `cond` output is AND-ed with an\n             additional condition ensuring the number of iterations executed is\n@@ -253,8 +257,14 @@ def while_loop(\n     >>> i = 0\n     >>> cond = lambda i: i < 10\n     >>> body = lambda i: i + 1\n-    >>> keras.ops.while_loop(cond, body, [i])[0]\n+    >>> keras.ops.while_loop(cond, body, i)\n     10\n+\n+    >>> x, y = 0, 1\n+    >>> cond = lambda x, y: x < 10\n+    >>> body = lambda x, y: (x + 1, y + 1)\n+    >>> keras.ops.while_loop(cond, body, (x, y))\n+    10, 11\n     \"\"\"\n     return backend.core.while_loop(\n         cond,\n\n@@ -3,6 +3,7 @@ from unittest.mock import Mock\n \n import numpy as np\n import pytest\n+import tree\n from absl.testing import parameterized\n \n from keras import backend\n@@ -83,7 +84,7 @@ class CoreOpsStaticShapeTest(testing.TestCase):\n             core.unstack(x, axis=axis)\n \n \n-class CoreOpsCorrectnessTest(testing.TestCase):\n+class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_scatter(self):\n         # Test 1D\n         indices = np.array([[1], [3], [4], [7]])\n@@ -221,25 +222,85 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         outputs = core.slice_update(inputs, start_indices, updates)\n         self.assertAllClose(outputs[1:3, 1:3, 2:4, 2:4], np.zeros([2, 2, 2, 2]))\n \n-    def test_while_loop(self):\n-        def cond(x, y):\n-            return x[0, 0] < 10\n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"with_max\",\n+                \"state\": (np.array(0), np.array(1)),\n+                \"output\": (np.array(5), np.array(6)),\n+                \"maximum_iterations\": 5,\n+            },\n+            {\n+                \"testcase_name\": \"no_max\",\n+                \"state\": (np.array(0), np.array(1)),\n+                \"output\": (np.array(10), np.array(11)),\n+                \"maximum_iterations\": None,\n+            },\n+        ]\n+    )\n+    def test_while_loop_list_data(self, state, output, maximum_iterations):\n+        def cond(*args):\n+            return tree.flatten(args)[0] < 10\n \n-        def body(x, y):\n-            return x + 1, y + 1\n+        def body(*args):\n+            return tree.map_structure(lambda x: x + 1, args)\n \n-        x = np.ones((2, 3))\n-        y = np.ones((3, 2))\n-        x, y = core.while_loop(cond, body, (x, y))\n-        self.assertAllClose(x, np.ones((2, 3)) * 10)\n-        self.assertAllClose(y, np.ones((3, 2)) * 10)\n+        state = core.while_loop(\n+            cond, body, state, maximum_iterations=maximum_iterations\n+        )\n+        tree.map_structure(self.assertAllClose, state, output)\n \n-        # Test max iterations\n-        x = np.ones((2, 3))\n-        y = np.ones((3, 2))\n-        x, y = core.while_loop(cond, body, (x, y), maximum_iterations=5)\n-        self.assertAllClose(x, np.ones((2, 3)) * 6)\n-        self.assertAllClose(y, np.ones((3, 2)) * 6)\n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"scalar_data_with_max\",\n+                \"state\": np.array(0),\n+                \"output\": np.array(5),\n+                \"maximum_iterations\": 5,\n+            },\n+            {\n+                \"testcase_name\": \"scalar_data_no_max\",\n+                \"state\": np.array(0),\n+                \"output\": np.array(10),\n+                \"maximum_iterations\": None,\n+            },\n+            {\n+                \"testcase_name\": \"nested_data_with_max\",\n+                \"state\": {\n+                    \"a\": np.array(0),\n+                    \"b\": (np.array(1), np.array(2)),\n+                },\n+                \"output\": {\n+                    \"a\": np.array(5),\n+                    \"b\": (np.array(6), np.array(7)),\n+                },\n+                \"maximum_iterations\": 5,\n+            },\n+            {\n+                \"testcase_name\": \"nested_data_no_max\",\n+                \"state\": {\n+                    \"a\": np.array(0),\n+                    \"b\": (np.array(1), np.array(2)),\n+                },\n+                \"output\": {\n+                    \"a\": np.array(10),\n+                    \"b\": (np.array(11), np.array(12)),\n+                },\n+                \"maximum_iterations\": None,\n+            },\n+        ]\n+    )\n+    def test_while_loop(self, state, output, maximum_iterations):\n+        def cond(args):\n+            return tree.flatten(args)[0] < 10\n+\n+        def body(args):\n+            return tree.map_structure(lambda x: x + 1, args)\n+\n+        state = core.while_loop(\n+            cond, body, state, maximum_iterations=maximum_iterations\n+        )\n+        tree.map_structure(self.assertAllClose, state, output)\n \n     def test_fori_loop(self):\n         def body_fun(i, x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3ebd4b4bfcd21cc14e115905e0482fcdd85ac65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 89 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 12 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 89 | Churn Cumulative: 245 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+from unittest.mock import patch\n+\n from absl.testing import parameterized\n \n from keras import backend\n@@ -128,3 +130,90 @@ class DtypesTest(test_case.TestCase, parameterized.TestCase):\n             ValueError, \"Invalid value for argument `dtype`. Expected one of\"\n         ):\n             dtypes._respect_weak_type(\"invalid_dtype\", True)\n+\n+    def test_invalid_dtype_in_least_upper_bound(self):\n+        invalid_dtype = \"non_existent_dtype\"\n+        with self.assertRaisesRegex(\n+            ValueError, \"is not a valid dtype for Keras type promotion\"\n+        ):\n+            dtypes._least_upper_bound(invalid_dtype)\n+\n+    def test_empty_lub_in_least_upper_bound(self):\n+        dtype1 = \"float32\"\n+        dtype2 = \"int32\"\n+        with patch.dict(\n+            dtypes.LATTICE_UPPER_BOUNDS,\n+            {\"float32\": set(), \"int32\": set()},\n+            clear=True,\n+        ):\n+            with self.assertRaisesRegex(\n+                ValueError, \"no available implicit dtype promotion path\"\n+            ):\n+                dtypes._least_upper_bound(dtype1, dtype2)\n+\n+    def test_valid_dtype_leading_to_single_lub_element(self):\n+        self.assertEqual(\n+            dtypes._least_upper_bound(\"float32\", \"int32\"), \"float32\"\n+        )\n+\n+    def test_valid_dtype_leading_to_keyerror_and_valueerror(self):\n+        invalid_dtype = \"non_existent_dtype\"\n+        with self.assertRaisesRegex(\n+            ValueError, \"is not a valid dtype for Keras type promotion\"\n+        ):\n+            dtypes._least_upper_bound(invalid_dtype)\n+\n+    def test_resolve_weak_type_bool(self):\n+        self.assertEqual(dtypes._resolve_weak_type(\"bool\"), \"bool\")\n+\n+    def test_resolve_weak_type_int(self):\n+        self.assertEqual(\n+            dtypes._resolve_weak_type(\"int32\", precision=\"32\"), \"int32\"\n+        )\n+        self.assertEqual(\n+            dtypes._resolve_weak_type(\"int64\", precision=\"64\"), \"int64\"\n+        )\n+\n+    def test_resolve_weak_type_uint(self):\n+        self.assertEqual(\n+            dtypes._resolve_weak_type(\"uint32\", precision=\"32\"), \"uint32\"\n+        )\n+        self.assertEqual(\n+            dtypes._resolve_weak_type(\"uint64\", precision=\"64\"), \"uint64\"\n+        )\n+\n+    def test_resolve_weak_type_float(self):\n+        self.assertEqual(\n+            dtypes._resolve_weak_type(\"float32\", precision=\"32\"), \"float32\"\n+        )\n+        self.assertEqual(\n+            dtypes._resolve_weak_type(\"float64\", precision=\"64\"), \"float64\"\n+        )\n+\n+    def test_least_upper_bound_ensure_order_independence(self):\n+        # Test to ensure _least_upper_bound is order-independent.\n+        result1 = dtypes._least_upper_bound(\"float32\", \"int32\")\n+        result2 = dtypes._least_upper_bound(\"int32\", \"float32\")\n+        self.assertEqual(result1, result2)\n+\n+    def test_least_upper_bound_single_element(self):\n+        dtypes.LATTICE_UPPER_BOUNDS[\"test_dtype\"] = {\"test_dtype\"}\n+        self.assertEqual(dtypes._least_upper_bound(\"test_dtype\"), \"test_dtype\")\n+\n+    def test_least_upper_bound_no_element(self):\n+        dtypes.LATTICE_UPPER_BOUNDS[\"test_dtype\"] = set()\n+        with self.assertRaisesRegex(\n+            ValueError, \"no available implicit dtype promotion path\"\n+        ):\n+            dtypes._least_upper_bound(\"test_dtype\")\n+\n+    def test_least_upper_bound_with_no_common_upper_bound(self):\n+        with patch.dict(\n+            dtypes.LATTICE_UPPER_BOUNDS,\n+            {\"test_dtype1\": set(), \"test_dtype2\": set()},\n+            clear=True,\n+        ):\n+            with self.assertRaisesRegex(\n+                ValueError, \"no available implicit dtype promotion path\"\n+            ):\n+                dtypes._least_upper_bound(\"test_dtype1\", \"test_dtype2\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6300ed4ff93cfcd62411e5ccccf52da793be561", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 580 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -53,7 +53,7 @@ def load_data():\n     feature_names = (\"sentence1\", \"sentence2\")\n \n     def split_features(x):\n-        # GLUE comes with dictonary data, we convert it to a uniform format\n+        # GLUE comes with dictionary data, we convert it to a uniform format\n         # (features, label), where features is a tuple consisting of all\n         # features. This format is necessary for using KerasNLP preprocessors.\n         features = tuple([x[name] for name in feature_names])\n\n@@ -48,7 +48,7 @@ STEPS_PER_EPOCH = len(train_labels) // BATCH_SIZE\n \n \"\"\" Keras model\n Simple but non-trivial model with:\n-* Batch Normalization (non-trainable state updated during trainig, different training-time and inference behavior)\n+* Batch Normalization (non-trainable state updated during training, different training-time and inference behavior)\n * Dropout (randomness, different training time and inference behavior)\n \"\"\"\n \n@@ -263,7 +263,7 @@ def compute_loss(trainable_variables, non_trainable_variables, x, y):\n compute_gradients = jax.value_and_grad(compute_loss, has_aux=True)\n \n \n-# Trainig step: Keras provides a pure functional optimizer.stateless_apply\n+# Training step: Keras provides a pure functional optimizer.stateless_apply\n @jax.jit\n def train_step(train_state, x, y):\n     (loss_value, non_trainable_variables), grads = compute_gradients(\n@@ -284,7 +284,7 @@ def train_step(train_state, x, y):\n \n # training loop\n EPOCHS = 5\n-print(\"\\nTrainig:\")\n+print(\"\\nTraining:\")\n data_iter = iter(train_data)\n for epoch in range(EPOCHS):\n     for i in tqdm(range(STEPS_PER_EPOCH)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dda62ba594648792eaef5f5cace61ecd061e9a9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1626 | Contributors (this commit): 6 | Commits (past 90d): 19 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -3714,8 +3714,8 @@ def meshgrid(*x, indexing=\"xy\"):\n \n     Args:\n         x: 1-D tensors representing the coordinates of a grid.\n-        indexing: Cartesian (`\"xy\"`, default) or matrix (`\"ij\"`) indexing\n-            of output.\n+        indexing: `\"xy\"` or `\"ij\"`. \"xy\" is cartesian; `\"ij\"` is matrix\n+            indexing of output. Defaults to `\"xy\"`.\n \n     Returns:\n         Sequence of N tensors.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5bc8488c0ea3f43c70c70ebca919093cd56066eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1504 | Lines Deleted: 417 | Files Changed: 13 | Hunks: 23 | Methods Changed: 114 | Complexity Δ (Sum/Max): 178/92 | Churn Δ: 1921 | Churn Cumulative: 3061 | Contributors (this commit): 9 | Commits (past 90d): 34 | Contributors (cumulative): 33 | DMM Complexity: 0.8838174273858921\n\nDIFF:\n@@ -1,6 +1,7 @@\n from keras.backend.jax import core\n from keras.backend.jax import distribution_lib\n from keras.backend.jax import image\n+from keras.backend.jax import linalg\n from keras.backend.jax import math\n from keras.backend.jax import nn\n from keras.backend.jax import numpy\n\n@@ -0,0 +1,72 @@\n+import jax\n+import jax.numpy as jnp\n+import jax.scipy as jsp\n+\n+from keras.backend import config\n+from keras.backend import standardize_dtype\n+from keras.backend.common import dtypes\n+from keras.backend.jax.core import cast\n+from keras.backend.jax.core import convert_to_tensor\n+\n+\n+def cholesky(a):\n+    out = jnp.linalg.cholesky(a)\n+    if jnp.any(jnp.isnan(out)):\n+        raise ValueError(\n+            \"Cholesky decomposition failed. \"\n+            \"The input might not be a valid positive definite matrix.\"\n+        )\n+    return out\n+\n+\n+def det(a):\n+    return jnp.linalg.det(a)\n+\n+\n+def eig(x):\n+    return jnp.linalg.eig(x)\n+\n+\n+def inv(a):\n+    return jnp.linalg.inv(a)\n+\n+\n+def lu_factor(x):\n+    lu_factor_fn = jsp.linalg.lu_factor\n+    if x.ndim > 2:\n+        for i in range(x.ndim - 2):\n+            lu_factor_fn = jax.vmap(lu_factor_fn)\n+\n+    return lu_factor_fn(x)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        dtype = config.floatx()\n+    else:\n+        dtype = dtypes.result_type(x.dtype, float)\n+    x = cast(x, dtype)\n+    return jnp.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    return jnp.linalg.qr(x, mode=mode)\n+\n+\n+def solve(a, b):\n+    return jnp.linalg.solve(a, b)\n+\n+\n+def solve_triangular(a, b, lower=False):\n+    return jsp.linalg.solve_triangular(a, b, lower=lower)\n+\n+\n+def svd(x, full_matrices=True, compute_uv=True):\n+    return jnp.linalg.svd(x, full_matrices=full_matrices, compute_uv=compute_uv)\n\n@@ -1,5 +1,6 @@\n from keras.backend.numpy import core\n from keras.backend.numpy import image\n+from keras.backend.numpy import linalg\n from keras.backend.numpy import math\n from keras.backend.numpy import nn\n from keras.backend.numpy import numpy\n\n@@ -0,0 +1,78 @@\n+import numpy as np\n+import scipy.linalg as sl\n+\n+from keras.backend import standardize_dtype\n+from keras.backend.common import dtypes\n+from keras.backend.torch.core import convert_to_tensor\n+\n+\n+def cholesky(a):\n+    return np.linalg.cholesky(a)\n+\n+\n+def det(a):\n+    return np.linalg.det(a)\n+\n+\n+def eig(a):\n+    return np.linalg.eig(a)\n+\n+\n+def inv(a):\n+    return np.linalg.inv(a)\n+\n+\n+def lu_factor(a):\n+    if a.ndim == 2:\n+        return sl.lu_factor(a)\n+\n+    m, n = a.shape[-2:]\n+    signature = \"(m,n) -> (m,n), \"\n+    signature += \"(m)\" if m <= n else \"(n)\"\n+    _lu_factor_gufunc = np.vectorize(\n+        sl.lu_factor,\n+        signature=signature,\n+    )\n+    return _lu_factor_gufunc(a)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n+    dtype = standardize_dtype(x.dtype)\n+    if \"int\" in dtype or dtype == \"bool\":\n+        dtype = dtypes.result_type(x.dtype, \"float32\")\n+    return np.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims).astype(\n+        dtype\n+    )\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    return np.linalg.qr(x, mode=mode)\n+\n+\n+def solve(a, b):\n+    return np.linalg.solve(a, b)\n+\n+\n+def solve_triangular(a, b, lower=False):\n+    if a.ndim == 2:\n+        return sl.solve_triangular(a, b, lower=lower)\n+\n+    _vectorized_solve_triangular = np.vectorize(\n+        lambda a, b: sl.solve_triangular(a, b, lower=lower),\n+        signature=\"(n,n),(n,m)->(n,m)\",\n+    )\n+    if b.ndim == a.ndim - 1:\n+        b = np.expand_dims(b, axis=-1)\n+        return _vectorized_solve_triangular(a, b).squeeze(axis=-1)\n+    return _vectorized_solve_triangular(a, b)\n+\n+\n+def svd(x, full_matrices=True, compute_uv=True):\n+    return np.linalg.svd(x, full_matrices=full_matrices, compute_uv=compute_uv)\n\n@@ -1,6 +1,7 @@\n from keras.backend.tensorflow import core\n from keras.backend.tensorflow import distribution_lib\n from keras.backend.tensorflow import image\n+from keras.backend.tensorflow import linalg\n from keras.backend.tensorflow import math\n from keras.backend.tensorflow import nn\n from keras.backend.tensorflow import numpy\n\n@@ -0,0 +1,162 @@\n+import tensorflow as tf\n+from tensorflow.experimental import numpy as tfnp\n+\n+from keras.backend import config\n+from keras.backend import standardize_dtype\n+from keras.backend.common import dtypes\n+from keras.backend.tensorflow.core import cast\n+from keras.backend.tensorflow.core import convert_to_tensor\n+\n+\n+def cholesky(a):\n+    out = tf.linalg.cholesky(a)\n+    # tf.linalg.cholesky simply returns NaNs for non-positive definite matrices\n+    return tf.debugging.check_numerics(out, \"Cholesky\")\n+\n+\n+def det(a):\n+    return tf.linalg.det(a)\n+\n+\n+def eig(a):\n+    return tf.linalg.eig(a)\n+\n+\n+def inv(a):\n+    return tf.linalg.inv(a)\n+\n+\n+def lu_factor(a):\n+    lu, p = tf.linalg.lu(a)\n+    return lu, tf.math.invert_permutation(p)\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n+    x_shape = x.shape\n+    ndim = x_shape.rank\n+\n+    if axis is None:\n+        axis = tuple(range(ndim))\n+    elif isinstance(axis, int):\n+        axis = (axis,)\n+\n+    axis = axis[0] if len(axis) == 1 else axis\n+    num_axes = 1 if isinstance(axis, int) else len(axis)\n+\n+    if num_axes == 1 and ord is None:\n+        ord = \"euclidean\"\n+    elif num_axes == 2 and ord is None:\n+        ord = \"fro\"\n+\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        dtype = config.floatx()\n+    else:\n+        dtype = dtypes.result_type(x.dtype, float)\n+    x = cast(x, dtype)\n+\n+    # Fast path to utilze `tf.linalg.norm`\n+    if (num_axes == 1 and ord in (\"euclidean\", 1, 2, float(\"inf\"))) or (\n+        num_axes == 2 and ord in (\"euclidean\", \"fro\", 1, 2, float(\"inf\"))\n+    ):\n+        return tf.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+\n+    # Ref: jax.numpy.linalg.norm\n+    if num_axes == 1 and ord not in (\"fro\", \"nuc\"):\n+        if ord == float(\"-inf\"):\n+            return tf.math.reduce_min(\n+                tf.math.abs(x), axis=axis, keepdims=keepdims\n+            )\n+        elif ord == 0:\n+            return tf.math.reduce_sum(\n+                tf.cast(tf.not_equal(x, 0), dtype=x.dtype),\n+                axis=axis,\n+                keepdims=keepdims,\n+            )\n+        else:\n+            ord = convert_to_tensor(ord, dtype=x.dtype)\n+            out = tf.math.reduce_sum(\n+                tf.pow(tf.math.abs(x), ord), axis=axis, keepdims=keepdims\n+            )\n+            return tf.pow(out, 1.0 / ord)\n+    elif num_axes == 2 and ord in (\"nuc\", float(\"-inf\"), -2, -1):\n+        row_axis, col_axis = axis[0], axis[1]\n+        row_axis = row_axis + ndim if row_axis < 0 else row_axis\n+        col_axis = col_axis + ndim if col_axis < 0 else col_axis\n+        if ord == float(\"-inf\"):\n+            if not keepdims and row_axis > col_axis:\n+                row_axis -= 1\n+            x = tf.math.reduce_min(\n+                tf.reduce_sum(tf.math.abs(x), axis=col_axis, keepdims=keepdims),\n+                axis=row_axis,\n+                keepdims=keepdims,\n+            )\n+        elif ord == -1:\n+            if not keepdims and col_axis > row_axis:\n+                col_axis -= 1\n+            x = tf.math.reduce_min(\n+                tf.reduce_sum(tf.math.abs(x), axis=row_axis, keepdims=keepdims),\n+                axis=col_axis,\n+                keepdims=keepdims,\n+            )\n+        else:\n+            x = tfnp.moveaxis(x, axis, (-2, -1))\n+            if ord == -2:\n+                x = tf.math.reduce_min(\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\n+                )\n+            else:\n+                x = tf.math.reduce_sum(\n+                    tf.linalg.svd(x, compute_uv=False), axis=-1\n+                )\n+            if keepdims:\n+                x = tf.expand_dims(x, axis[0])\n+                x = tf.expand_dims(x, axis[1])\n+        return x\n+\n+    if num_axes == 1:\n+        raise ValueError(\n+            f\"Invalid `ord` argument for vector norm. Received: ord={ord}\"\n+        )\n+    elif num_axes == 2:\n+        raise ValueError(\n+            f\"Invalid `ord` argument for matrix norm. Received: ord={ord}\"\n+        )\n+    else:\n+        raise ValueError(f\"Invalid axis values. Received: axis={axis}\")\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    if mode == \"reduced\":\n+        return tf.linalg.qr(x)\n+    return tf.linalg.qr(x, full_matrices=True)\n+\n+\n+def solve(a, b):\n+    # tensorflow.linalg.solve only supports same rank inputs\n+    if tf.rank(b) == tf.rank(a) - 1:\n+        b = tf.expand_dims(b, axis=-1)\n+        return tf.squeeze(tf.linalg.solve(a, b), axis=-1)\n+    return tf.linalg.solve(a, b)\n+\n+\n+def solve_triangular(a, b, lower=False):\n+    if b.shape.ndims == a.shape.ndims - 1:\n+        b = tf.expand_dims(b, axis=-1)\n+        return tf.squeeze(\n+            tf.linalg.triangular_solve(a, b, lower=lower), axis=-1\n+        )\n+    return tf.linalg.triangular_solve(a, b, lower=lower)\n+\n+\n+def svd(x, full_matrices=True, compute_uv=True):\n+    s, u, v = tf.linalg.svd(\n+        x, full_matrices=full_matrices, compute_uv=compute_uv\n+    )\n+    return u, s, tf.linalg.adjoint(v)\n\n@@ -16,6 +16,7 @@ we are doing the following to automate device placement if a GPU is available:\n \n from keras.backend.torch import core\n from keras.backend.torch import image\n+from keras.backend.torch import linalg\n from keras.backend.torch import math\n from keras.backend.torch import nn\n from keras.backend.torch import numpy\n\n@@ -0,0 +1,70 @@\n+import torch\n+\n+from keras.backend import config\n+from keras.backend import standardize_dtype\n+from keras.backend.common import dtypes\n+from keras.backend.torch.core import cast\n+from keras.backend.torch.core import convert_to_tensor\n+\n+\n+def cholesky(x):\n+    return torch.cholesky(x)\n+\n+\n+def det(x):\n+    return torch.det(x)\n+\n+\n+def eig(x):\n+    return torch.linalg.eig(x)\n+\n+\n+def inv(x):\n+    return torch.linalg.inv(x)\n+\n+\n+def lu_factor(x):\n+    LU, pivots = torch.linalg.lu_factor(x)\n+    # torch retuns pivots with 1-based indexing\n+    return LU, pivots - 1\n+\n+\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"int64\":\n+        dtype = config.floatx()\n+    else:\n+        dtype = dtypes.result_type(x.dtype, float)\n+    x = cast(x, dtype)\n+    return torch.linalg.norm(x, ord=ord, dim=axis, keepdim=keepdims)\n+\n+\n+def qr(x, mode=\"reduced\"):\n+    if mode not in {\"reduced\", \"complete\"}:\n+        raise ValueError(\n+            \"`mode` argument value not supported. \"\n+            \"Expected one of {'reduced', 'complete'}. \"\n+            f\"Received: mode={mode}\"\n+        )\n+    return torch.linalg.qr(x, mode=mode)\n+\n+\n+def solve(a, b):\n+    return torch.linalg.solve(a, b)\n+\n+\n+def solve_triangular(a, b, lower=False):\n+    if b.ndim == a.ndim - 1:\n+        b = torch.unsqueeze(b, axis=-1)\n+        return torch.linalg.solve_triangular(a, b, upper=not lower).squeeze(\n+            axis=-1\n+        )\n+    return torch.linalg.solve_triangular(a, b, upper=not lower)\n+\n+\n+def svd(x, full_matrices=True, compute_uv=True):\n+    if not compute_uv:\n+        raise NotImplementedError(\n+            \"`compute_uv=False` is not supported for torch backend.\"\n+        )\n+    return torch.linalg.svd(x, full_matrices=full_matrices)\n\n@@ -10,6 +10,7 @@ from keras.backend import random\n from keras.ops import image\n from keras.ops import operation_utils\n from keras.ops.core import *  # noqa: F403\n+from keras.ops.linalg import *  # noqa: F403\n from keras.ops.math import *  # noqa: F403\n from keras.ops.nn import *  # noqa: F403\n from keras.ops.numpy import *  # noqa: F403\n\n@@ -0,0 +1,598 @@\n+from keras import backend\n+from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n+from keras.backend import any_symbolic_tensors\n+from keras.ops.operation import Operation\n+from keras.ops.operation_utils import reduce_shape\n+\n+\n+class Cholesky(Operation):\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return _cholesky(x)\n+\n+    def compute_output_spec(self, x):\n+        _assert_2d(x)\n+        _assert_square(x)\n+        return KerasTensor(x.shape, x.dtype)\n+\n+\n+@keras_export([\"keras.ops.cholesky\", \"keras.ops.linalg.cholesky\"])\n+def cholesky(x):\n+    \"\"\"Computes the Cholesky decomposition of a positive semi-definite matrix.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, M)`.\n+\n+    Returns:\n+        A tensor of shape `(..., M, M)` representing the lower triangular\n+        Cholesky factor of `x`.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Cholesky().symbolic_call(x)\n+    return _cholesky(x)\n+\n+\n+def _cholesky(x):\n+    x = backend.convert_to_tensor(x)\n+    _assert_2d(x)\n+    _assert_square(x)\n+    try:\n+        return backend.linalg.cholesky(x)\n+    except Exception as e:\n+        raise ValueError(f\"Cholesky decomposition failed: {e}\")\n+\n+\n+class Det(Operation):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return _det(x)\n+\n+    def compute_output_spec(self, x):\n+        _assert_2d(x)\n+        _assert_square(x)\n+        return KerasTensor(x.shape[:-2], x.dtype)\n+\n+\n+@keras_export([\"keras.ops.det\", \"keras.ops.linalg.det\"])\n+def det(x):\n+    \"\"\"Computes the determinant of a square tensor.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, M)`.\n+\n+    Returns:\n+        A tensor of shape `(...,)` represeting the determinant of `x`.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Det().symbolic_call(x)\n+    return _det(x)\n+\n+\n+def _det(x):\n+    x = backend.convert_to_tensor(x)\n+    _assert_2d(x)\n+    _assert_square(x)\n+    return backend.linalg.det(x)\n+\n+\n+class Eig(Operation):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return _eig(x)\n+\n+    def compute_output_spec(self, x):\n+        _assert_2d(x)\n+        _assert_square(x)\n+        return (\n+            KerasTensor(x.shape[:-1], x.dtype),\n+            KerasTensor(x.shape, x.dtype),\n+        )\n+\n+\n+@keras_export([\"keras.ops.eig\", \"keras.ops.linalg.eig\"])\n+def eig(x):\n+    \"\"\"Computes the eigenvalues and eigenvectors of a square matrix.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, M)`.\n+\n+    Returns:\n+        A tuple of two tensors: a tensor of shape `(..., M)` containing\n+        eigenvalues and a tensor of shape `(..., M, M)` containing eigenvectors.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Eig().symbolic_call(x)\n+    return _eig(x)\n+\n+\n+def _eig(x):\n+    x = backend.convert_to_tensor(x)\n+    _assert_2d(x)\n+    _assert_square(x)\n+    return backend.linalg.eig(x)\n+\n+\n+class Inv(Operation):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return _inv(x)\n+\n+    def compute_output_spec(self, x):\n+        _assert_2d(x)\n+        _assert_square(x)\n+        return KerasTensor(x.shape, x.dtype)\n+\n+\n+@keras_export([\"keras.ops.inv\", \"keras.ops.linalg.inv\"])\n+def inv(x):\n+    \"\"\"Computes the inverse of a square tensor.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, M)`.\n+\n+    Returns:\n+        A tensor of shape `(..., M, M)` representing the inverse of `x`.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Inv().symbolic_call(x)\n+    return _inv(x)\n+\n+\n+def _inv(x):\n+    x = backend.convert_to_tensor(x)\n+    _assert_2d(x)\n+    _assert_square(x)\n+    return backend.linalg.inv(x)\n+\n+\n+class LuFactor(Operation):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, x):\n+        return _lu_factor(x)\n+\n+    def compute_output_spec(self, x):\n+        _assert_2d(x)\n+        batch_shape = x.shape[:-2]\n+        m, n = x.shape[-2:]\n+        k = min(m, n)\n+        return (\n+            KerasTensor(batch_shape + (m, n), x.dtype),\n+            KerasTensor(batch_shape + (k,), x.dtype),\n+        )\n+\n+\n+@keras_export([\"keras.ops.lu_factor\", \"keras.ops.linalg.lu_factor\"])\n+def lu_factor(x):\n+    \"\"\"Computes the lower-upper decomposition of a square matrix.\n+\n+    Args:\n+        x: A tensor of shape `(..., M, M)`.\n+\n+    Returns:\n+        A tuple of two tensors: a tensor of shape `(..., M, M)` containing the\n+        lower and upper triangular matrices and a tensor of shape `(..., M)`\n+        containing the pivots.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return LuFactor().symbolic_call(x)\n+    return _lu_factor(x)\n+\n+\n+def _lu_factor(x):\n+    x = backend.convert_to_tensor(x)\n+    _assert_2d(x)\n+    if backend.backend() == \"tensorflow\":\n+        try:\n+            _assert_square(x)\n+        except ValueError as e:\n+            raise ValueError(\n+                f\"LU decomposition failed: {e}. LU decomposition is only \"\n+                \"supported for square matrices in Tensorflow.\"\n+            )\n+    return backend.linalg.lu_factor(x)\n+\n+\n+class Norm(Operation):\n+    def __init__(self, ord=None, axis=None, keepdims=False):\n+        super().__init__()\n+        if isinstance(ord, str):\n+            if ord not in (\"fro\", \"nuc\"):\n+                raise ValueError(\n+                    \"Invalid `ord` argument. \"\n+                    \"Expected one of {'fro', 'nuc'} when using string. \"\n+                    f\"Received: ord={ord}\"\n+                )\n+        if isinstance(axis, int):\n+            axis = [axis]\n+        self.ord = ord\n+        self.axis = axis\n+        self.keepdims = keepdims\n+\n+    def compute_output_spec(self, x):\n+        output_dtype = backend.standardize_dtype(x.dtype)\n+        if \"int\" in output_dtype or output_dtype == \"bool\":\n+            output_dtype = backend.floatx()\n+        if self.axis is None:\n+            axis = tuple(range(len(x.shape)))\n+        else:\n+            axis = self.axis\n+        num_axes = len(axis)\n+        if num_axes == 1 and isinstance(self.ord, str):\n+            raise ValueError(\n+                \"Invalid `ord` argument for vector norm. \"\n+                f\"Received: ord={self.ord}\"\n+            )\n+        elif num_axes == 2 and self.ord not in (\n+            None,\n+            \"fro\",\n+            \"nuc\",\n+            float(\"inf\"),\n+            float(\"-inf\"),\n+            1,\n+            -1,\n+            2,\n+            -2,\n+        ):\n+            raise ValueError(\n+                \"Invalid `ord` argument for matrix norm. \"\n+                f\"Received: ord={self.ord}\"\n+            )\n+        return KerasTensor(\n+            reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n+            dtype=output_dtype,\n+        )\n+\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        return backend.linalg.norm(\n+            x, ord=self.ord, axis=self.axis, keepdims=self.keepdims\n+        )\n+\n+\n+@keras_export([\"keras.ops.norm\", \"keras.ops.linalg.norm\"])\n+def norm(x, ord=None, axis=None, keepdims=False):\n+    \"\"\"Matrix or vector norm.\n+\n+    This function is able to return one of eight different matrix norms, or one\n+    of an infinite number of vector norms (described below), depending on the\n+    value of the `ord` parameter.\n+\n+    Args:\n+        x: Input tensor.\n+        ord: Order of the norm (see table under Notes). The default is `None`.\n+        axis: If `axis` is an integer, it specifies the axis of `x` along which\n+            to compute the vector norms. If `axis` is a 2-tuple, it specifies\n+            the axes that hold 2-D matrices, and the matrix norms of these\n+            matrices are computed.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one.\n+\n+    Note:\n+        For values of `ord < 1`, the result is, strictly speaking, not a\n+        mathematical 'norm', but it may still be useful for various numerical\n+        purposes. The following norms can be calculated:\n+        - For matrices:\n+            - `ord=None`: Frobenius norm\n+            - `ord=\"fro\"`: Frobenius norm\n+            - `ord=nuc`: nuclear norm\n+            - `ord=np.inf`: `max(sum(abs(x), axis=1))`\n+            - `ord=-np.inf`: `min(sum(abs(x), axis=1))`\n+            - `ord=0`: not supported\n+            - `ord=1`: `max(sum(abs(x), axis=0))`\n+            - `ord=-1`: `min(sum(abs(x), axis=0))`\n+            - `ord=2`: 2-norm (largest sing. value)\n+            - `ord=-2`: smallest singular value\n+            - other: not supported\n+        - For vectors:\n+            - `ord=None`: 2-norm\n+            - `ord=\"fro\"`: not supported\n+            - `ord=nuc`: not supported\n+            - `ord=np.inf`: `max(abs(x))`\n+            - `ord=-np.inf`: `min(abs(x))`\n+            - `ord=0`: `sum(x != 0)`\n+            - `ord=1`: as below\n+            - `ord=-1`: as below\n+            - `ord=2`: as below\n+            - `ord=-2`: as below\n+            - other: `sum(abs(x)**ord)**(1./ord)`\n+\n+    Returns:\n+        Norm of the matrix or vector(s).\n+\n+    Example:\n+\n+    >>> x = keras.ops.reshape(keras.ops.arange(9, dtype=\"float32\") - 4, (3, 3))\n+    >>> keras.ops.linalg.norm(x)\n+    7.7459664\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Norm(ord=ord, axis=axis, keepdims=keepdims).symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+\n+\n+class Qr(Operation):\n+    def __init__(self, mode=\"reduced\"):\n+        super().__init__()\n+        if mode not in {\"reduced\", \"complete\"}:\n+            raise ValueError(\n+                \"`mode` argument value not supported. \"\n+                \"Expected one of {'reduced', 'complete'}. \"\n+                f\"Received: mode={mode}\"\n+            )\n+        self.mode = mode\n+\n+    def compute_output_spec(self, x):\n+        if len(x.shape) < 2:\n+            raise ValueError(\n+                \"Input should have rank >= 2. Received: \"\n+                f\"input.shape = {x.shape}\"\n+            )\n+        m = x.shape[-2]\n+        n = x.shape[-1]\n+        if m is None or n is None:\n+            raise ValueError(\n+                \"Input should have its last 2 dimensions \"\n+                \"fully-defined. Received: \"\n+                f\"input.shape = {x.shape}\"\n+            )\n+        k = min(m, n)\n+        base = tuple(x.shape[:-2])\n+        if self.mode == \"reduced\":\n+            return (\n+                KerasTensor(shape=base + (m, k), dtype=x.dtype),\n+                KerasTensor(shape=base + (k, n), dtype=x.dtype),\n+            )\n+        # 'complete' mode.\n+        return (\n+            KerasTensor(shape=base + (m, m), dtype=x.dtype),\n+            KerasTensor(shape=base + (m, n), dtype=x.dtype),\n+        )\n+\n+    def call(self, x):\n+        x = backend.convert_to_tensor(x)\n+        return backend.linalg.qr(x, mode=self.mode)\n+\n+\n+@keras_export([\"keras.ops.qr\", \"keras.ops.linalg.qr\"])\n+def qr(x, mode=\"reduced\"):\n+    \"\"\"Computes the QR decomposition of a tensor.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, N)`.\n+        mode: A string specifying the mode of the QR decomposition.\n+            - 'reduced': Returns the reduced QR decomposition. (default)\n+            - 'complete': Returns the complete QR decomposition.\n+\n+    Returns:\n+        A tuple containing two tensors. The first tensor of shape `(..., M, K)`\n+        is the orthogonal matrix `q` and the second tensor of shape\n+        `(..., K, N)` is the upper triangular matrix `r`, where `K = min(M, N)`.\n+\n+    Example:\n+\n+    >>> x = keras.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n+    >>> q, r = qr(x)\n+    >>> print(q)\n+    array([[-0.16903079  0.897085]\n+           [-0.5070925   0.2760267 ]\n+           [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Qr(mode=mode).symbolic_call(x)\n+    x = backend.convert_to_tensor(x)\n+    return backend.linalg.qr(x, mode=mode)\n+\n+\n+class Solve(Operation):\n+\n+    def __init__(self):\n+        super().__init__()\n+\n+    def call(self, a, b):\n+        return _solve(a, b)\n+\n+    def compute_output_spec(self, a, b):\n+        _assert_2d(a)\n+        _assert_square(a)\n+        _assert_1d(b)\n+        _assert_a_b_compat(a, b)\n+        return KerasTensor(b.shape, b.dtype)\n+\n+\n+@keras_export([\"keras.ops.solve\", \"keras.ops.linalg.solve\"])\n+def solve(a, b):\n+    \"\"\"Solves a linear system of equations given by `a x = b`.\n+\n+    Args:\n+        a: A tensor of shape `(..., M, M)` representing the coefficients matrix.\n+        b: A tensor of shape `(..., M)` or `(..., M, N)` represeting the\n+        right-hand side or \"dependent variable\" matrix.\n+\n+    Returns:\n+        A tensor of shape `(..., M)` or `(..., M, N)` representing the solution\n+        of the linear system. Returned shape is identical to `b`.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((a, b)):\n+        return Solve().symbolic_call(a, b)\n+    return _solve(a, b)\n+\n+\n+def _solve(a, b):\n+    a = backend.convert_to_tensor(a)\n+    b = backend.convert_to_tensor(b)\n+    _assert_2d(a)\n+    _assert_square(a)\n+    _assert_1d(b)\n+    _assert_a_b_compat(a, b)\n+    return backend.linalg.solve(a, b)\n+\n+\n+class SolveTriangular(Operation):\n+\n+    def __init__(self, lower=False):\n+        super().__init__()\n+        self.lower = lower\n+\n+    def call(self, a, b):\n+        return _solve_triangular(a, b, self.lower)\n+\n+    def compute_output_spec(self, a, b):\n+        _assert_2d(a)\n+        _assert_square(a)\n+        _assert_1d(b)\n+        _assert_a_b_compat(a, b)\n+        return KerasTensor(b.shape, b.dtype)\n+\n+\n+@keras_export(\n+    [\"keras.ops.solve_triangular\", \"keras.ops.linalg.solve_triangular\"]\n+)\n+def solve_triangular(a, b, lower=False):\n+    \"\"\"Solves a linear system of equations given by `a x = b`.\n+\n+    Args:\n+        a: A tensor of shape `(..., M, M)` representing the coefficients matrix.\n+        b: A tensor of shape `(..., M)` or `(..., M, N)` represeting the\n+        right-hand side or \"dependent variable\" matrix.\n+\n+    Returns:\n+        A tensor of shape `(..., M)` or `(..., M, N)` representing the solution\n+        of the linear system. Returned shape is identical to `b`.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((a, b)):\n+        return SolveTriangular(lower).symbolic_call(a, b)\n+    return _solve_triangular(a, b, lower)\n+\n+\n+def _solve_triangular(a, b, lower=False):\n+    a = backend.convert_to_tensor(a)\n+    b = backend.convert_to_tensor(b)\n+    _assert_2d(a)\n+    _assert_square(a)\n+    _assert_1d(b)\n+    _assert_a_b_compat(a, b)\n+    return backend.linalg.solve_triangular(a, b, lower)\n+\n+\n+class SVD(Operation):\n+\n+    def __init__(self, full_matrices=True, compute_uv=True):\n+        super().__init__()\n+        self.full_matrices = full_matrices\n+        self.compute_uv = compute_uv\n+\n+    def call(self, x):\n+        return _svd(x, self.full_matrices, self.compute_uv)\n+\n+    def compute_output_spec(self, x):\n+        _assert_2d(x)\n+        rows, columns = x.shape[-2:]\n+        batches = x.shape[:-2]\n+        s_shape = batches + (min(rows, columns),)\n+        if self.full_matrices:\n+            u_shape = batches + (rows, rows)\n+            v_shape = batches + (columns, columns)\n+        else:\n+            u_shape = batches + (rows, min(rows, columns))\n+            v_shape = batches + (min(rows, columns), columns)\n+\n+        if self.compute_uv:\n+            return (\n+                KerasTensor(u_shape, x.dtype),\n+                KerasTensor(s_shape, x.dtype),\n+                KerasTensor(v_shape, x.dtype),\n+            )\n+        return KerasTensor(s_shape, x.dtype)\n+\n+\n+@keras_export([\"keras.ops.svd\", \"keras.ops.linalg.svd\"])\n+def svd(x, full_matrices=True, compute_uv=True):\n+    \"\"\"Computes the singular value decomposition of a matrix.\n+\n+    Args:\n+        x: Input tensor of shape `(..., M, N)`.\n+\n+    Returns:\n+        A tuple of three tensors: a tensor of shape `(..., M, M)` containing the\n+        left singular vectors, a tensor of shape `(..., M, N)` containing the\n+        singular values and a tensor of shape `(..., N, N)` containing the\n+        right singular vectors.\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return SVD(full_matrices, compute_uv).symbolic_call(x)\n+    return _svd(x, full_matrices, compute_uv)\n+\n+\n+def _svd(x, full_matrices=True, compute_uv=True):\n+    x = backend.convert_to_tensor(x)\n+    _assert_2d(x)\n+    return backend.linalg.svd(x, full_matrices, compute_uv)\n+\n+\n+def _assert_1d(*arrays):\n+    for a in arrays:\n+        if a.ndim < 1:\n+            raise ValueError(\n+                \"Expected input to have rank >= 1. \"\n+                \"Received scalar input {a}.\"\n+            )\n+\n+\n+def _assert_2d(*arrays):\n+    for a in arrays:\n+        if a.ndim < 2:\n+            raise ValueError(\n+                \"Expected input to have rank >= 2. \"\n+                \"Received input with shape {a.shape}.\"\n+            )\n+\n+\n+def _assert_square(*arrays):\n+    for a in arrays:\n+        m, n = a.shape[-2:]\n+        if m != n:\n+            raise ValueError(\n+                \"Expected a square matrix. \"\n+                f\"Received non-square input with shape {a.shape}\"\n+            )\n+\n+\n+def _assert_a_b_compat(a, b):\n+    if a.ndim == b.ndim:\n+        if a.shape[-2] != b.shape[-2]:\n+            raise ValueError(\n+                \"Incompatible shapes between `a` and `b`. \"\n+                \"Expected `a.shape[-2] == b.shape[-2]`. \"\n+                f\"Received: a.shape={a.shape}, b.shape={b.shape}\"\n+            )\n+    elif a.ndim == b.ndim - 1:\n+        if a.shape[-1] != b.shape[-1]:\n+            raise ValueError(\n+                \"Incompatible shapes between `a` and `b`. \"\n+                \"Expected `a.shape[-1] == b.shape[-1]`. \"\n+                f\"Received: a.shape={a.shape}, b.shape={b.shape}\"\n+            )\n\n@@ -0,0 +1,519 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras import backend\n+from keras import ops\n+from keras import testing\n+from keras.backend.common.keras_tensor import KerasTensor\n+from keras.ops import linalg\n+from keras.testing.test_utils import named_product\n+\n+\n+class LinalgOpsDynamicShapeTest(testing.TestCase):\n+    def test_cholesky(self):\n+        x = KerasTensor([None, 20, 20])\n+        out = linalg.cholesky(x)\n+        self.assertEqual(out.shape, (None, 20, 20))\n+\n+        x = KerasTensor([None, None, 20])\n+        with self.assertRaises(ValueError):\n+            linalg.cholesky(x)\n+\n+        x = KerasTensor([None, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.cholesky(x)\n+\n+    def test_det(self):\n+        x = KerasTensor([None, 20, 20])\n+        out = linalg.det(x)\n+        self.assertEqual(out.shape, (None,))\n+\n+        x = KerasTensor([None, None, 20])\n+        with self.assertRaises(ValueError):\n+            linalg.det(x)\n+\n+        x = KerasTensor([None, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.det(x)\n+\n+    def test_eig(self):\n+        x = KerasTensor([None, 20, 20])\n+        w, v = linalg.eig(x)\n+        self.assertEqual(w.shape, (None, 20))\n+        self.assertEqual(v.shape, (None, 20, 20))\n+\n+        x = KerasTensor([None, None, 20])\n+        with self.assertRaises(ValueError):\n+            linalg.eig(x)\n+\n+        x = KerasTensor([None, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.eig(x)\n+\n+    def test_inv(self):\n+        x = KerasTensor([None, 20, 20])\n+        out = linalg.inv(x)\n+        self.assertEqual(out.shape, (None, 20, 20))\n+\n+        x = KerasTensor([None, None, 20])\n+        with self.assertRaises(ValueError):\n+            linalg.inv(x)\n+\n+        x = KerasTensor([None, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.inv(x)\n+\n+    def test_lu_factor(self):\n+        x = KerasTensor([None, 4, 3])\n+        lu, p = linalg.lu_factor(x)\n+        self.assertEqual(lu.shape, (None, 4, 3))\n+        self.assertEqual(p.shape, (None, 3))\n+\n+        x = KerasTensor([None, 2, 3])\n+        lu, p = linalg.lu_factor(x)\n+        self.assertEqual(lu.shape, (None, 2, 3))\n+        self.assertEqual(p.shape, (None, 2))\n+\n+    def test_norm(self):\n+        x = KerasTensor((None, 3))\n+        self.assertEqual(linalg.norm(x).shape, ())\n+\n+        x = KerasTensor((None, 3, 3))\n+        self.assertEqual(linalg.norm(x, axis=1).shape, (None, 3))\n+        self.assertEqual(\n+            linalg.norm(x, axis=1, keepdims=True).shape, (None, 1, 3)\n+        )\n+\n+    def test_qr(self):\n+        x = KerasTensor((None, 4, 3), dtype=\"float32\")\n+        q, r = linalg.qr(x, mode=\"reduced\")\n+        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\"reduced\")\n+        qref_shape = (None,) + qref.shape[1:]\n+        rref_shape = (None,) + rref.shape[1:]\n+        self.assertEqual(q.shape, qref_shape)\n+        self.assertEqual(r.shape, rref_shape)\n+\n+        q, r = linalg.qr(x, mode=\"complete\")\n+        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\"complete\")\n+        qref_shape = (None,) + qref.shape[1:]\n+        rref_shape = (None,) + rref.shape[1:]\n+        self.assertEqual(q.shape, qref_shape)\n+        self.assertEqual(r.shape, rref_shape)\n+\n+    def test_solve(self):\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, 20, 5])\n+        out = linalg.solve(a, b)\n+        self.assertEqual(out.shape, (None, 20, 5))\n+\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, 20])\n+        out = linalg.solve(a, b)\n+        self.assertEqual(out.shape, (None, 20))\n+\n+        a = KerasTensor([None, None, 20])\n+        b = KerasTensor([None, 20, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve(a, b)\n+\n+        a = KerasTensor([None, 20, 15])\n+        b = KerasTensor([None, 20, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve(a, b)\n+\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, None, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve(a, b)\n+\n+    def test_solve_triangular(self):\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, 20, 5])\n+        out = linalg.solve_triangular(a, b)\n+        self.assertEqual(out.shape, (None, 20, 5))\n+\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, 20])\n+        out = linalg.solve_triangular(a, b)\n+        self.assertEqual(out.shape, (None, 20))\n+\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, 20, 5])\n+        out = linalg.solve_triangular(a, b, lower=True)\n+        self.assertEqual(out.shape, (None, 20, 5))\n+\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, 20])\n+        out = linalg.solve_triangular(a, b, lower=True)\n+        self.assertEqual(out.shape, (None, 20))\n+\n+        a = KerasTensor([None, 20, 15])\n+        b = KerasTensor([None, 20, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve_triangular(a, b)\n+\n+        a = KerasTensor([None, 20, 20])\n+        b = KerasTensor([None, None, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve_triangular(a, b)\n+\n+    def test_svd(self):\n+        x = KerasTensor((None, 3, 2))\n+        u, s, v = linalg.svd(x)\n+        self.assertEqual(u.shape, (None, 3, 3))\n+        self.assertEqual(s.shape, (None, 2))\n+        self.assertEqual(v.shape, (None, 2, 2))\n+\n+        u, s, v = linalg.svd(x, full_matrices=False)\n+        self.assertEqual(u.shape, (None, 3, 2))\n+        self.assertEqual(s.shape, (None, 2))\n+        self.assertEqual(v.shape, (None, 2, 2))\n+\n+        s = linalg.svd(x, compute_uv=False)\n+        self.assertEqual(s.shape, (None, 2))\n+\n+\n+class LinalgOpsStaticShapeTest(testing.TestCase):\n+    def test_cholesky(self):\n+        x = KerasTensor([4, 3, 3])\n+        out = linalg.cholesky(x)\n+        self.assertEqual(out.shape, (4, 3, 3))\n+\n+        x = KerasTensor([10, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.cholesky(x)\n+\n+    def test_det(self):\n+        x = KerasTensor([4, 3, 3])\n+        out = linalg.det(x)\n+        self.assertEqual(out.shape, (4,))\n+\n+        x = KerasTensor([10, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.det(x)\n+\n+    def test_eig(self):\n+        x = KerasTensor([4, 3, 3])\n+        w, v = linalg.eig(x)\n+        self.assertEqual(w.shape, (4, 3))\n+        self.assertEqual(v.shape, (4, 3, 3))\n+\n+        x = KerasTensor([10, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.eig(x)\n+\n+    def test_inv(self):\n+        x = KerasTensor([4, 3, 3])\n+        out = linalg.inv(x)\n+        self.assertEqual(out.shape, (4, 3, 3))\n+\n+        x = KerasTensor([10, 20, 15])\n+        with self.assertRaises(ValueError):\n+            linalg.inv(x)\n+\n+    def test_lu_factor(self):\n+        x = KerasTensor([10, 4, 3])\n+        lu, p = linalg.lu_factor(x)\n+        self.assertEqual(lu.shape, (10, 4, 3))\n+        self.assertEqual(p.shape, (10, 3))\n+\n+        x = KerasTensor([10, 2, 3])\n+        lu, p = linalg.lu_factor(x)\n+        self.assertEqual(lu.shape, (10, 2, 3))\n+        self.assertEqual(p.shape, (10, 2))\n+\n+    def test_norm(self):\n+        x = KerasTensor((10, 3))\n+        self.assertEqual(linalg.norm(x).shape, ())\n+\n+        x = KerasTensor((10, 3, 3))\n+        self.assertEqual(linalg.norm(x, axis=1).shape, (10, 3))\n+        self.assertEqual(\n+            linalg.norm(x, axis=1, keepdims=True).shape, (10, 1, 3)\n+        )\n+\n+    def test_qr(self):\n+        x = KerasTensor((4, 3), dtype=\"float32\")\n+        q, r = linalg.qr(x, mode=\"reduced\")\n+        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\"reduced\")\n+        self.assertEqual(q.shape, qref.shape)\n+        self.assertEqual(r.shape, rref.shape)\n+\n+        q, r = linalg.qr(x, mode=\"complete\")\n+        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\"complete\")\n+        self.assertEqual(q.shape, qref.shape)\n+        self.assertEqual(r.shape, rref.shape)\n+\n+        with self.assertRaises(ValueError):\n+            linalg.qr(x, mode=\"invalid\")\n+\n+    def test_solve(self):\n+        a = KerasTensor([4, 3, 3])\n+        b = KerasTensor([4, 3, 5])\n+        out = linalg.solve(a, b)\n+        self.assertEqual(out.shape, (4, 3, 5))\n+\n+        a = KerasTensor([4, 3, 3])\n+        b = KerasTensor([4, 3])\n+        out = linalg.solve(a, b)\n+        self.assertEqual(out.shape, (4, 3))\n+\n+        a = KerasTensor([10, 20, 15])\n+        b = KerasTensor([10, 20, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve(a, b)\n+\n+        a = KerasTensor([20, 20])\n+        b = KerasTensor([])\n+        with self.assertRaises(ValueError):\n+            linalg.solve(a, b)\n+\n+    def test_solve_triangular(self):\n+        a = KerasTensor([4, 3, 3])\n+        b = KerasTensor([4, 3, 5])\n+        out = linalg.solve_triangular(a, b)\n+        self.assertEqual(out.shape, (4, 3, 5))\n+\n+        a = KerasTensor([4, 3, 3])\n+        b = KerasTensor([4, 3])\n+        out = linalg.solve_triangular(a, b)\n+        self.assertEqual(out.shape, (4, 3))\n+\n+        a = KerasTensor([10, 20, 15])\n+        b = KerasTensor([10, 20, 5])\n+        with self.assertRaises(ValueError):\n+            linalg.solve_triangular(a, b)\n+\n+    def test_svd(self):\n+        x = KerasTensor((10, 3, 2))\n+        u, s, v = linalg.svd(x)\n+        self.assertEqual(u.shape, (10, 3, 3))\n+        self.assertEqual(s.shape, (10, 2))\n+        self.assertEqual(v.shape, (10, 2, 2))\n+\n+        u, s, v = linalg.svd(x, full_matrices=False)\n+        self.assertEqual(u.shape, (10, 3, 2))\n+        self.assertEqual(s.shape, (10, 2))\n+        self.assertEqual(v.shape, (10, 2, 2))\n+\n+        s = linalg.svd(x, compute_uv=False)\n+        self.assertEqual(s.shape, (10, 2))\n+\n+\n+class LinalgOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+\n+    def test_cholesky(self):\n+        x = np.random.rand(4, 3, 3).astype(\"float32\")\n+        with self.assertRaises(ValueError):\n+            linalg.cholesky(x)\n+        x_psd = x @ x.transpose((0, 2, 1)) + 1e-5 * np.eye(3)\n+        out = linalg.cholesky(x_psd)\n+        self.assertAllClose(out, np.linalg.cholesky(x_psd), atol=1e-4)\n+\n+    def test_det(self):\n+        x = np.random.rand(4, 3, 3)\n+        out = linalg.det(x)\n+        self.assertAllClose(out, np.linalg.det(x), atol=1e-5)\n+\n+        with self.assertRaises(ValueError):\n+            x = np.random.rand(4, 3, 4)\n+            linalg.det(x)\n+\n+    def test_eig(self):\n+        x = np.random.rand(2, 3, 3)\n+        x = x @ x.transpose((0, 2, 1))\n+        if backend.backend() == \"jax\":\n+            import jax\n+\n+            if jax.default_backend() == \"gpu\":\n+                # eig not implemented for jax on gpu backend\n+                with self.assertRaises(NotImplementedError):\n+                    linalg.eig(x)\n+                return\n+        w, v = map(ops.convert_to_numpy, linalg.eig(x))\n+        x_reconstructed = (v * w[..., None, :]) @ v.transpose((0, 2, 1))\n+        self.assertAllClose(x_reconstructed, x, atol=1e-4)\n+\n+    def test_inv(self):\n+        x = np.random.rand(4, 3, 3)\n+        x_inv = ops.convert_to_numpy(linalg.inv(x))\n+        x_reconstructed = x @ x_inv\n+        # high tolerance due to numerical instability\n+        self.assertAllClose(\n+            x_reconstructed, np.repeat(np.eye(3)[None], 4, 0), atol=1e-3\n+        )\n+\n+    def test_lu_factor(self):\n+        def _pivot_matrix(pivots, n):\n+            p_matrix = np.eye(n)\n+            for i, p in enumerate(pivots):\n+                identity = np.eye(n, n)\n+                q = identity[i, :].copy()\n+                identity[i, :] = identity[p, :]\n+                identity[p, :] = q\n+                p_matrix = np.dot(p_matrix, identity)\n+            return p_matrix\n+\n+        def _reconstruct(lu, pivots, m, n):\n+            lower = np.tril(lu[:, : min(m, n)], -1) + np.eye(m, min(m, n))\n+            upper = np.triu(lu[: min(m, n)])\n+\n+            # pivots are defined differently in tensorflow\n+            # compared to the other backends\n+            if backend.backend() == \"tensorflow\":\n+                p_matrix = np.eye(m)[pivots]\n+            else:\n+                p_matrix = _pivot_matrix(pivots, m)\n+            out = p_matrix @ lower @ upper\n+            return out\n+\n+        m, n = 4, 4\n+        x = np.random.rand(m, n)\n+        lu, pivots = map(ops.convert_to_numpy, linalg.lu_factor(x))\n+        x_reconstructed = _reconstruct(lu, pivots, m, n)\n+        self.assertAllClose(x_reconstructed, x, atol=1e-5)\n+\n+        m, n = 4, 3\n+        x = np.random.rand(m, n)\n+        if backend.backend() == \"tensorflow\":\n+            with self.assertRaises(ValueError):\n+                linalg.lu_factor(x)\n+        else:\n+            lu, pivots = map(ops.convert_to_numpy, linalg.lu_factor(x))\n+            x_reconstructed = _reconstruct(lu, pivots, m, n)\n+            self.assertAllClose(x_reconstructed, x, atol=1e-5)\n+\n+        # batched case\n+        m, n = 3, 4\n+        x = np.random.rand(2, m, n)\n+        if backend.backend() == \"tensorflow\":\n+            with self.assertRaises(ValueError):\n+                linalg.lu_factor(x)\n+        else:\n+            lu, pivots = map(ops.convert_to_numpy, linalg.lu_factor(x))\n+            for i in range(2):\n+                self.assertAllClose(\n+                    _reconstruct(lu[i], pivots[i], m, n), x[i], atol=1e-5\n+                )\n+\n+    @parameterized.named_parameters(\n+        named_product(\n+            ord=[None, \"fro\", \"nuc\", -np.inf, -2, -1, 0, 1, 2, np.inf, 3],\n+            axis=[None, 1, -1],\n+            keepdims=[False, True],\n+        )\n+    )\n+    def test_norm_vectors(self, ord, axis, keepdims):\n+        if axis is None:\n+            x = np.random.random((5,))\n+        else:\n+            x = np.random.random((5, 6))\n+        if ord in (\"fro\", \"nuc\"):\n+            error = RuntimeError if backend.backend() == \"torch\" else ValueError\n+            with self.assertRaises(error):\n+                linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+            return\n+        output = linalg.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n+        expected_result = np.linalg.norm(\n+            x, ord=ord, axis=axis, keepdims=keepdims\n+        )\n+        self.assertAllClose(output, expected_result)\n+\n+    def test_qr(self):\n+        x = np.random.random((4, 5))\n+        q, r = linalg.qr(x, mode=\"reduced\")\n+        qref, rref = np.linalg.qr(x, mode=\"reduced\")\n+        self.assertAllClose(qref, q)\n+        self.assertAllClose(rref, r)\n+\n+        q, r = linalg.qr(x, mode=\"complete\")\n+        qref, rref = np.linalg.qr(x, mode=\"complete\")\n+        self.assertAllClose(qref, q)\n+        self.assertAllClose(rref, r)\n+\n+    def test_solve(self):\n+        x1 = np.array([[1, 2], [4, 5]], dtype=\"float32\")\n+        x2 = np.array([[2, 4], [8, 10]], dtype=\"float32\")\n+        output = linalg.solve(x1, x2)\n+        expected_result = np.array([[2, 0], [0, 2]], dtype=\"float32\")\n+        self.assertAllClose(output, expected_result)\n+\n+    def test_solve_triangular(self):\n+\n+        # 2d-case\n+        x1 = np.array([[1, 2], [0, 5]], dtype=\"float32\")\n+        x2 = np.array([2, 10], dtype=\"float32\")\n+        output = linalg.solve_triangular(x1, x2, lower=True)\n+        expected_result = np.array([2, 2], dtype=\"float32\")\n+        self.assertAllClose(output, expected_result)\n+\n+        output = linalg.solve_triangular(x1, x2, lower=False)\n+        expected_result = np.array([-2, 2], dtype=\"float32\")\n+        self.assertAllClose(output, expected_result)\n+\n+        # batched case\n+        x1 = np.array([[[1, 2], [0, 5]], [[1, 2], [0, 5]]], dtype=\"float32\")\n+        x2 = np.array([[2, 10], [2, 10]], dtype=\"float32\")\n+        output = linalg.solve_triangular(x1, x2, lower=True)\n+        expected_result = np.array([[2, 2], [2, 2]], dtype=\"float32\")\n+        self.assertAllClose(output, expected_result)\n+\n+    def test_svd(self):\n+        x = np.random.rand(4, 30, 20)\n+        u, s, vh = linalg.svd(x)\n+        x_reconstructed = (u[..., :, : s.shape[-1]] * s[..., None, :]) @ vh[\n+            ..., : s.shape[-1], :\n+        ]\n+        self.assertAllClose(x_reconstructed, x, atol=1e-4)\n+\n+\n+class QrOpTest(testing.TestCase):\n+    def test_qr_init_mode_reduced(self):\n+        qr_op = linalg.Qr(mode=\"reduced\")\n+        self.assertIsNotNone(qr_op)\n+\n+    def test_qr_init_mode_complete(self):\n+        qr_op = linalg.Qr(mode=\"complete\")\n+        self.assertIsNotNone(qr_op)\n+\n+    def test_qr_init_invalid_mode(self):\n+        invalid_mode = \"invalid_mode\"\n+        expected_error = (\n+            r\"`mode` argument value not supported. \"\n+            r\"Expected one of \\{'reduced', 'complete'\\}. \"\n+            f\"Received: mode={invalid_mode}\"\n+        )\n+        with self.assertRaisesRegex(ValueError, expected_error):\n+            linalg.Qr(mode=invalid_mode)\n+\n+    def test_compute_output_spec_low_rank(self):\n+        qr_op = linalg.Qr(mode=\"reduced\")\n+        low_rank_input = np.random.rand(3)\n+        with self.assertRaisesRegex(\n+            ValueError, r\"Input should have rank >= 2. Received: .*\"\n+        ):\n+            qr_op.compute_output_spec(low_rank_input)\n+\n+    def test_compute_output_spec_undefined_dimensions(self):\n+        qr_op = linalg.Qr(mode=\"reduced\")\n+        undefined_dim_input = KerasTensor(shape=(None, 4), dtype=\"float32\")\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Input should have its last 2 dimensions \"\n+            r\"fully-defined. Received: .*\",\n+        ):\n+            qr_op.compute_output_spec(undefined_dim_input)\n+\n+    def test_qr_call_mode_reduced(self):\n+        qr_op = linalg.Qr(mode=\"reduced\")\n+        test_input = np.random.rand(10, 10)\n+        q, r = qr_op.call(test_input)\n+        self.assertEqual(q.shape, (10, 10))\n+        self.assertEqual(r.shape, (10, 10))\n+\n+    def test_qr_call_mode_complete(self):\n+        qr_op = linalg.Qr(mode=\"complete\")\n+        test_input = np.random.rand(10, 10)\n+        q, r = qr_op.call(test_input)\n+        self.assertEqual(q.shape, (10, 10))\n+        self.assertEqual(r.shape, (10, 10))\n\n@@ -244,78 +244,6 @@ def logsumexp(x, axis=None, keepdims=False):\n     return backend.math.logsumexp(x, axis=axis, keepdims=keepdims)\n \n \n-class Qr(Operation):\n-    def __init__(self, mode=\"reduced\"):\n-        super().__init__()\n-        if mode not in {\"reduced\", \"complete\"}:\n-            raise ValueError(\n-                \"`mode` argument value not supported. \"\n-                \"Expected one of {'reduced', 'complete'}. \"\n-                f\"Received: mode={mode}\"\n-            )\n-        self.mode = mode\n-\n-    def compute_output_spec(self, x):\n-        if len(x.shape) < 2:\n-            raise ValueError(\n-                \"Input should have rank >= 2. Received: \"\n-                f\"input.shape = {x.shape}\"\n-            )\n-        m = x.shape[-2]\n-        n = x.shape[-1]\n-        if m is None or n is None:\n-            raise ValueError(\n-                \"Input should have its last 2 dimensions \"\n-                \"fully-defined. Received: \"\n-                f\"input.shape = {x.shape}\"\n-            )\n-        k = min(m, n)\n-        base = tuple(x.shape[:-2])\n-        if self.mode == \"reduced\":\n-            return (\n-                KerasTensor(shape=base + (m, k), dtype=x.dtype),\n-                KerasTensor(shape=base + (k, n), dtype=x.dtype),\n-            )\n-        # 'complete' mode.\n-        return (\n-            KerasTensor(shape=base + (m, m), dtype=x.dtype),\n-            KerasTensor(shape=base + (m, n), dtype=x.dtype),\n-        )\n-\n-    def call(self, x):\n-        return backend.math.qr(x, mode=self.mode)\n-\n-\n-@keras_export(\"keras.ops.qr\")\n-def qr(x, mode=\"reduced\"):\n-    \"\"\"Computes the QR decomposition of a tensor.\n-\n-    Args:\n-        x: Input tensor.\n-        mode: A string specifying the mode of the QR decomposition.\n-            - 'reduced': Returns the reduced QR decomposition. (default)\n-            - 'complete': Returns the complete QR decomposition.\n-\n-    Returns:\n-        A tuple containing two tensors. The first tensor represents the\n-        orthogonal matrix Q, and the second tensor represents the upper\n-        triangular matrix R.\n-\n-    Example:\n-\n-    >>> x = keras.ops.convert_to_tensor([[1., 2.], [3., 4.], [5., 6.]])\n-    >>> q, r = qr(x)\n-    >>> print(q)\n-    array([[-0.16903079  0.897085]\n-           [-0.5070925   0.2760267 ]\n-           [-0.8451542  -0.34503305]], shape=(3, 2), dtype=float32)\n-    \"\"\"\n-\n-    if any_symbolic_tensors((x,)):\n-        return Qr(mode=mode).symbolic_call(x)\n-    return backend.math.qr(x, mode=mode)\n-\n-\n class ExtractSequences(Operation):\n     def __init__(self, sequence_length, sequence_stride):\n         super().__init__()\n@@ -996,157 +924,3 @@ def erfinv(x):\n         return Erfinv().symbolic_call(x)\n     x = backend.convert_to_tensor(x)\n     return backend.math.erfinv(x)\n-\n-\n-class Solve(Operation):\n-    def call(self, a, b):\n-        a = backend.convert_to_tensor(a)\n-        b = backend.convert_to_tensor(b)\n-        return backend.math.solve(a, b)\n-\n-    def compute_output_spec(self, a, b):\n-        return KerasTensor(shape=a.shape, dtype=a.dtype)\n-\n-\n-@keras_export(\"keras.ops.solve\")\n-def solve(a, b):\n-    \"\"\"Solves for `x` in the equation `a * x = b`.\n-\n-    Args:\n-        a: Input tensor.\n-        b: Input tensor.\n-\n-    Returns:\n-        A tensor with the same shape and dtype as `a`.\n-\n-    Example:\n-\n-    >>> a = np.array([[1, 2], [4, 5]], dtype=\"float32\")\n-    >>> b = np.array([[2, 4], [8, 10]], dtype=\"float32\")\n-    >>> keras.ops.solve(x1, x2)\n-    array([[2, 0], [0, 2]], dtype=\"float32\")\n-    \"\"\"\n-    if any_symbolic_tensors((a, b)):\n-        return Solve().symbolic_call(a, b)\n-    a = backend.convert_to_tensor(a)\n-    b = backend.convert_to_tensor(b)\n-    return backend.math.solve(a, b)\n-\n-\n-class Norm(Operation):\n-    def __init__(self, ord=None, axis=None, keepdims=False):\n-        super().__init__()\n-        if isinstance(ord, str):\n-            if ord not in (\"fro\", \"nuc\"):\n-                raise ValueError(\n-                    \"Invalid `ord` argument. \"\n-                    \"Expected one of {'fro', 'nuc'} when using string. \"\n-                    f\"Received: ord={ord}\"\n-                )\n-        if isinstance(axis, int):\n-            axis = [axis]\n-        self.ord = ord\n-        self.axis = axis\n-        self.keepdims = keepdims\n-\n-    def compute_output_spec(self, x):\n-        output_dtype = backend.standardize_dtype(x.dtype)\n-        if \"int\" in output_dtype or output_dtype == \"bool\":\n-            output_dtype = backend.floatx()\n-        if self.axis is None:\n-            axis = tuple(range(len(x.shape)))\n-        else:\n-            axis = self.axis\n-        num_axes = len(axis)\n-        if num_axes == 1 and isinstance(self.ord, str):\n-            raise ValueError(\n-                \"Invalid `ord` argument for vector norm. \"\n-                f\"Received: ord={self.ord}\"\n-            )\n-        elif num_axes == 2 and self.ord not in (\n-            None,\n-            \"fro\",\n-            \"nuc\",\n-            float(\"inf\"),\n-            float(\"-inf\"),\n-            1,\n-            -1,\n-            2,\n-            -2,\n-        ):\n-            raise ValueError(\n-                \"Invalid `ord` argument for matrix norm. \"\n-                f\"Received: ord={self.ord}\"\n-            )\n-        return KerasTensor(\n-            reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n-            dtype=output_dtype,\n-        )\n-\n-    def call(self, x):\n-        x = backend.convert_to_tensor(x)\n-        return backend.math.norm(\n-            x, ord=self.ord, axis=self.axis, keepdims=self.keepdims\n-        )\n-\n-\n-@keras_export(\"keras.ops.norm\")\n-def norm(x, ord=None, axis=None, keepdims=False):\n-    \"\"\"Matrix or vector norm.\n-\n-    This function is able to return one of eight different matrix norms, or one\n-    of an infinite number of vector norms (described below), depending on the\n-    value of the `ord` parameter.\n-\n-    Args:\n-        x: Input tensor.\n-        ord: Order of the norm (see table under Notes). The default is `None`.\n-        axis: If `axis` is an integer, it specifies the axis of `x` along which\n-            to compute the vector norms. If `axis` is a 2-tuple, it specifies\n-            the axes that hold 2-D matrices, and the matrix norms of these\n-            matrices are computed.\n-        keepdims: If this is set to `True`, the axes which are reduced are left\n-            in the result as dimensions with size one.\n-\n-    Note:\n-        For values of `ord < 1`, the result is, strictly speaking, not a\n-        mathematical 'norm', but it may still be useful for various numerical\n-        purposes. The following norms can be calculated:\n-        - For matrices:\n-            - `ord=None`: Frobenius norm\n-            - `ord=\"fro\"`: Frobenius norm\n-            - `ord=nuc`: nuclear norm\n-            - `ord=np.inf`: `max(sum(abs(x), axis=1))`\n-            - `ord=-np.inf`: `min(sum(abs(x), axis=1))`\n-            - `ord=0`: not supported\n-            - `ord=1`: `max(sum(abs(x), axis=0))`\n-            - `ord=-1`: `min(sum(abs(x), axis=0))`\n-            - `ord=2`: 2-norm (largest sing. value)\n-            - `ord=-2`: smallest singular value\n-            - other: not supported\n-        - For vectors:\n-            - `ord=None`: 2-norm\n-            - `ord=\"fro\"`: not supported\n-            - `ord=nuc`: not supported\n-            - `ord=np.inf`: `max(abs(x))`\n-            - `ord=-np.inf`: `min(abs(x))`\n-            - `ord=0`: `sum(x != 0)`\n-            - `ord=1`: as below\n-            - `ord=-1`: as below\n-            - `ord=2`: as below\n-            - `ord=-2`: as below\n-            - other: `sum(abs(x)**ord)**(1./ord)`\n-\n-    Returns:\n-        Norm of the matrix or vector(s).\n-\n-    Example:\n-\n-    >>> x = keras.ops.reshape(keras.ops.arange(9, dtype=\"float32\") - 4, (3, 3))\n-    >>> keras.ops.norm(x)\n-    7.7459664\n-    \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return Norm(ord=ord, axis=axis, keepdims=keepdims).symbolic_call(x)\n-    x = backend.convert_to_tensor(x)\n-    return backend.math.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n\n@@ -7,12 +7,9 @@ from absl.testing import parameterized\n \n from keras import backend\n from keras import testing\n-from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.ops import math as kmath\n-from keras.ops import numpy as knp\n-from keras.testing.test_utils import named_product\n \n \n def _stft(\n@@ -179,22 +176,6 @@ class MathOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         result = kmath.logsumexp(x)\n         self.assertEqual(result.shape, ())\n \n-    def test_qr(self):\n-        x = KerasTensor((None, 4, 3), dtype=\"float32\")\n-        q, r = kmath.qr(x, mode=\"reduced\")\n-        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\"reduced\")\n-        qref_shape = (None,) + qref.shape[1:]\n-        rref_shape = (None,) + rref.shape[1:]\n-        self.assertEqual(q.shape, qref_shape)\n-        self.assertEqual(r.shape, rref_shape)\n-\n-        q, r = kmath.qr(x, mode=\"complete\")\n-        qref, rref = np.linalg.qr(np.ones((2, 4, 3)), mode=\"complete\")\n-        qref_shape = (None,) + qref.shape[1:]\n-        rref_shape = (None,) + rref.shape[1:]\n-        self.assertEqual(q.shape, qref_shape)\n-        self.assertEqual(r.shape, rref_shape)\n-\n     def test_extract_sequences(self):\n         # Defined dimension\n         x = KerasTensor((None, 32), dtype=\"float32\")\n@@ -285,16 +266,6 @@ class MathOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         x = KerasTensor([None, 3])\n         self.assertEqual(kmath.rsqrt(x).shape, (None, 3))\n \n-    def test_norm(self):\n-        x = KerasTensor((None, 3))\n-        self.assertEqual(kmath.norm(x).shape, ())\n-\n-        x = KerasTensor((None, 3, 3))\n-        self.assertEqual(kmath.norm(x, axis=1).shape, (None, 3))\n-        self.assertEqual(\n-            kmath.norm(x, axis=1, keepdims=True).shape, (None, 1, 3)\n-        )\n-\n \n class MathOpsStaticShapeTest(testing.TestCase):\n     @pytest.mark.skipif(\n@@ -345,18 +316,6 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         result = kmath.logsumexp(x)\n         self.assertEqual(result.shape, ())\n \n-    def test_qr(self):\n-        x = KerasTensor((4, 3), dtype=\"float32\")\n-        q, r = kmath.qr(x, mode=\"reduced\")\n-        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\"reduced\")\n-        self.assertEqual(q.shape, qref.shape)\n-        self.assertEqual(r.shape, rref.shape)\n-\n-        q, r = kmath.qr(x, mode=\"complete\")\n-        qref, rref = np.linalg.qr(np.ones((4, 3)), mode=\"complete\")\n-        self.assertEqual(q.shape, qref.shape)\n-        self.assertEqual(r.shape, rref.shape)\n-\n     def test_extract_sequences(self):\n         x = KerasTensor((10, 16), dtype=\"float32\")\n         sequence_length = 3\n@@ -433,16 +392,6 @@ class MathOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(output.shape, ref.shape)\n \n-    def test_solve(self):\n-        x1 = KerasTensor((2, 2), dtype=\"float32\")\n-        x2 = KerasTensor((2, 2), dtype=\"float32\")\n-        outputs = kmath.solve(x1, x2)\n-        self.assertEqual(outputs.shape, (2, 2))\n-\n-    def test_norm(self):\n-        x = KerasTensor((2, 3))\n-        self.assertEqual(kmath.norm(x).shape, ())\n-\n \n class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     @pytest.mark.skipif(\n@@ -642,18 +591,6 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         expected = np.log(np.sum(np.exp(x), axis=1))\n         self.assertAllClose(outputs, expected)\n \n-    def test_qr(self):\n-        x = np.random.random((4, 5))\n-        q, r = kmath.qr(x, mode=\"reduced\")\n-        qref, rref = np.linalg.qr(x, mode=\"reduced\")\n-        self.assertAllClose(qref, q)\n-        self.assertAllClose(rref, r)\n-\n-        q, r = kmath.qr(x, mode=\"complete\")\n-        qref, rref = np.linalg.qr(x, mode=\"complete\")\n-        self.assertAllClose(qref, q)\n-        self.assertAllClose(rref, r)\n-\n     def test_extract_sequences(self):\n         # Test 1D case.\n         x = np.random.random((10,))\n@@ -924,59 +861,6 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             expected_output, output_from_edge_erfinv_op, atol=1e-4\n         )\n \n-    def test_solve(self):\n-        x1 = np.array([[1, 2], [4, 5]], dtype=\"float32\")\n-        x2 = np.array([[2, 4], [8, 10]], dtype=\"float32\")\n-        output = kmath.solve(x1, x2)\n-        expected_result = np.array([[2, 0], [0, 2]], dtype=\"float32\")\n-        self.assertAllClose(output, expected_result)\n-\n-    @parameterized.named_parameters(\n-        named_product(\n-            ord=[None, \"fro\", \"nuc\", -np.inf, -2, -1, 0, 1, 2, np.inf, 3],\n-            axis=[None, (0, 1), (0, 2)],\n-            keepdims=[False, True],\n-        )\n-    )\n-    def test_norm_matrices(self, ord, axis, keepdims):\n-        if axis is None:\n-            x = np.random.random((6, 7))\n-        else:\n-            x = np.random.random((5, 6, 7))\n-        if ord in (0, 3):\n-            error = RuntimeError if backend.backend() == \"torch\" else ValueError\n-            with self.assertRaises(error):\n-                kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n-            return\n-        output = kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n-        expected_result = np.linalg.norm(\n-            x, ord=ord, axis=axis, keepdims=keepdims\n-        )\n-        self.assertAllClose(output, expected_result, atol=1e-5, rtol=1e-5)\n-\n-    @parameterized.named_parameters(\n-        named_product(\n-            ord=[None, \"fro\", \"nuc\", -np.inf, -2, -1, 0, 1, 2, np.inf, 3],\n-            axis=[None, 1, -1],\n-            keepdims=[False, True],\n-        )\n-    )\n-    def test_norm_vectors(self, ord, axis, keepdims):\n-        if axis is None:\n-            x = np.random.random((5,))\n-        else:\n-            x = np.random.random((5, 6))\n-        if ord in (\"fro\", \"nuc\"):\n-            error = RuntimeError if backend.backend() == \"torch\" else ValueError\n-            with self.assertRaises(error):\n-                kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n-            return\n-        output = kmath.norm(x, ord=ord, axis=axis, keepdims=keepdims)\n-        expected_result = np.linalg.norm(\n-            x, ord=ord, axis=axis, keepdims=keepdims\n-        )\n-        self.assertAllClose(output, expected_result)\n-\n \n class MathDtypeTest(testing.TestCase, parameterized.TestCase):\n     \"\"\"Test the floating dtype to verify that the behavior matches JAX.\"\"\"\n@@ -1010,71 +894,6 @@ class MathDtypeTest(testing.TestCase, parameterized.TestCase):\n         self.jax_enable_x64.__exit__(None, None, None)\n         return super().tearDown()\n \n-    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n-    def test_norm(self, dtype):\n-        import jax.numpy as jnp\n-\n-        x = knp.ones((1,), dtype=dtype)\n-        x_jax = jnp.ones((1,), dtype=dtype)\n-        expected_dtype = standardize_dtype(jnp.linalg.norm(x_jax).dtype)\n-        if dtype == \"int64\":\n-            expected_dtype = \"float32\"\n-\n-        self.assertEqual(standardize_dtype(kmath.norm(x).dtype), expected_dtype)\n-        self.assertEqual(kmath.Norm().symbolic_call(x).dtype, expected_dtype)\n-\n-\n-class QrOpTest(testing.TestCase):\n-    def test_qr_init_mode_reduced(self):\n-        qr_op = kmath.Qr(mode=\"reduced\")\n-        self.assertIsNotNone(qr_op)\n-\n-    def test_qr_init_mode_complete(self):\n-        qr_op = kmath.Qr(mode=\"complete\")\n-        self.assertIsNotNone(qr_op)\n-\n-    def test_qr_init_invalid_mode(self):\n-        invalid_mode = \"invalid_mode\"\n-        expected_error = (\n-            r\"`mode` argument value not supported. \"\n-            r\"Expected one of \\{'reduced', 'complete'\\}. \"\n-            f\"Received: mode={invalid_mode}\"\n-        )\n-        with self.assertRaisesRegex(ValueError, expected_error):\n-            kmath.Qr(mode=invalid_mode)\n-\n-    def test_compute_output_spec_low_rank(self):\n-        qr_op = kmath.Qr(mode=\"reduced\")\n-        low_rank_input = np.random.rand(3)\n-        with self.assertRaisesRegex(\n-            ValueError, r\"Input should have rank >= 2. Received: .*\"\n-        ):\n-            qr_op.compute_output_spec(low_rank_input)\n-\n-    def test_compute_output_spec_undefined_dimensions(self):\n-        qr_op = kmath.Qr(mode=\"reduced\")\n-        undefined_dim_input = KerasTensor(shape=(None, 4), dtype=\"float32\")\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            r\"Input should have its last 2 dimensions \"\n-            r\"fully-defined. Received: .*\",\n-        ):\n-            qr_op.compute_output_spec(undefined_dim_input)\n-\n-    def test_qr_call_mode_reduced(self):\n-        qr_op = kmath.Qr(mode=\"reduced\")\n-        test_input = np.random.rand(10, 10)\n-        q, r = qr_op.call(test_input)\n-        self.assertEqual(q.shape, (10, 10))\n-        self.assertEqual(r.shape, (10, 10))\n-\n-    def test_qr_call_mode_complete(self):\n-        qr_op = kmath.Qr(mode=\"complete\")\n-        test_input = np.random.rand(10, 10)\n-        q, r = qr_op.call(test_input)\n-        self.assertEqual(q.shape, (10, 10))\n-        self.assertEqual(r.shape, (10, 10))\n-\n \n class ExtractSequencesOpTest(testing.TestCase):\n     def test_extract_sequences_init_length_1_stride_1(self):\n@@ -1236,16 +1055,6 @@ class FFTTest(testing.TestCase):\n         self.assertEqual(fft_op.axis, -1, \"Default axis should be -1\")\n \n \n-class SolveTest(testing.TestCase):\n-    def test_solve_call(self):\n-        solve_op = kmath.Solve()\n-        a = np.array([[3, 2], [1, 2]], dtype=np.float32)\n-        b = np.array([[9, 8], [5, 4]], dtype=np.float32)\n-        output = solve_op.call(a, b)\n-        expected_output = np.linalg.solve(a, b)\n-        self.assertAllClose(output, expected_output, atol=1e-6, rtol=1e-6)\n-\n-\n class FFT2Test(testing.TestCase):\n     def test_fft2_correct_input(self):\n         fft2_op = kmath.FFT2()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
