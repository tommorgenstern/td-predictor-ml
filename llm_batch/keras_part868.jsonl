{"custom_id": "keras#aae05209d5e64a0f69f46befaaaef2faff8465d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 208 | Lines Deleted: 159 | Files Changed: 6 | Hunks: 85 | Methods Changed: 77 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 367 | Churn Cumulative: 1585 | Contributors (this commit): 12 | Commits (past 90d): 19 | Contributors (cumulative): 31 | DMM Complexity: 0.9069767441860465\n\nDIFF:\n@@ -115,7 +115,6 @@ class Variable:\n             self._path = current_path() + \"/\" + name\n         else:\n             self._path = name\n-        self._dtype = standardize_dtype(dtype)\n         self._shape = None\n         self._initializer = None\n         self._regularizer = None\n@@ -139,6 +138,12 @@ class Variable:\n                     f\"Received: initializer={initializer} \"\n                     f\"and shape={shape}\"\n                 )\n+        else:\n+            initializer = self._convert_to_tensor(initializer, dtype=dtype)\n+            # If dtype is None and `initializer` is an array, use its dtype.\n+            if dtype is None:\n+                dtype = initializer.dtype\n+        self._dtype = standardize_dtype(dtype)\n \n         if in_stateless_scope():\n             if callable(initializer):\n\n@@ -6,6 +6,7 @@ from absl.testing import parameterized\n \n from keras.src import backend\n from keras.src import initializers\n+from keras.src import ops\n from keras.src.backend.common import dtypes\n from keras.src.backend.common.variables import AutocastScope\n from keras.src.backend.common.variables import shape_equal\n@@ -33,10 +34,32 @@ class VariableInitializationTest(test_case.TestCase):\n             with backend.StatelessScope():\n                 v = backend.Variable(initializer=0)\n \n-    def test_variable_initialization_with_non_callable(self):\n-        \"\"\"Test variable init with non-callable initializer.\"\"\"\n-        v = backend.Variable(initializer=np.ones((2, 2)))\n+    def test_variable_initialization_with_numpy_array(self):\n+        \"\"\"Test variable init with numpy array initializer.\"\"\"\n+        v = backend.Variable(\n+            initializer=np.ones((2, 2), dtype=np.int32), trainable=False\n+        )\n         self.assertAllClose(v.value, np.ones((2, 2)))\n+        self.assertEqual(v.dtype, \"int32\")\n+\n+    def test_variable_initialization_with_native_array(self):\n+        \"\"\"Test variable init with native array initializer.\"\"\"\n+        v = backend.Variable(\n+            initializer=ops.ones((2, 2), dtype=\"int32\"), trainable=False\n+        )\n+        self.assertAllClose(v.value, np.ones((2, 2)))\n+        self.assertEqual(v.dtype, \"int32\")\n+\n+    def test_variable_initialization_with_python_array(self):\n+        \"\"\"Test variable init with python array initializer.\"\"\"\n+        v = backend.Variable(initializer=[[1, 1], [1, 1]], trainable=False)\n+        self.assertAllClose(v.value, np.ones((2, 2)))\n+        self.assertEqual(v.dtype, \"int32\")\n+        v = backend.Variable(\n+            initializer=[[1.0, 1.0], [1.0, 1.0]], trainable=False\n+        )\n+        self.assertAllClose(v.value, np.ones((2, 2)))\n+        self.assertEqual(v.dtype, \"float32\")\n \n     def test_variable_initialization_with_strings(self):\n         \"\"\"Test variable init with non-callable initializer.\"\"\"\n@@ -66,8 +89,8 @@ class VariableInitializationTest(test_case.TestCase):\n \n     def test_variable_initialize(self):\n         \"\"\"Test initializing a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        init_value = np.array([4, 5, 6])\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        init_value = np.array([4.0, 5.0, 6.0])\n         v._initialize(value=init_value)\n         self.assertAllClose(v.value, init_value)\n \n@@ -276,9 +299,9 @@ class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n \n     def test_variable_numpy(self):\n         \"\"\"Test retrieving the value of a variable as a numpy array.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n         self.assertIsInstance(v.numpy(), np.ndarray)\n-        self.assertAllClose(v.numpy(), np.array([1, 2, 3]))\n+        self.assertAllClose(v.numpy(), np.array([1.0, 2.0, 3.0]))\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\",\n@@ -297,44 +320,44 @@ class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n \n     def test_variable_value(self):\n         \"\"\"Test retrieving the value of a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v.value, np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(v.value, np.array([1.0, 2.0, 3.0]))\n \n     def test_variable_assign(self):\n         \"\"\"Test assigning a new value to a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v.assign(np.array([4, 5, 6]))\n-        self.assertAllClose(v.value, np.array([4, 5, 6]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v.assign(np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(v.value, np.array([4.0, 5.0, 6.0]))\n \n     def test_variable_assign_return(self):\n         \"\"\"Test assigning a new value and returning.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        r = v.assign(np.array([4, 5, 6]))\n-        self.assertAllClose(r, np.array([4, 5, 6]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        r = v.assign(np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(r, np.array([4.0, 5.0, 6.0]))\n \n     def test_variable_assign_add(self):\n         \"\"\"Test the assign_add method on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v.assign_add(np.array([1, 1, 1]))\n-        self.assertAllClose(v.value, np.array([2, 3, 4]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v.assign_add(np.array([1.0, 1.0, 1.0]))\n+        self.assertAllClose(v.value, np.array([2.0, 3.0, 4.0]))\n \n     def test_variable_assign_add_return(self):\n         \"\"\"Test assign_add a new value and returning.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        r = v.assign_add(np.array([1, 1, 1]))\n-        self.assertAllClose(r, np.array([2, 3, 4]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        r = v.assign_add(np.array([1.0, 1.0, 1.0]))\n+        self.assertAllClose(r, np.array([2.0, 3.0, 4.0]))\n \n     def test_variable_assign_sub(self):\n         \"\"\"Test the assign_sub method on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([2, 3, 4]))\n-        v.assign_sub(np.array([1, 1, 1]))\n-        self.assertAllClose(v.value, np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([2.0, 3.0, 4.0]))\n+        v.assign_sub(np.array([1.0, 1.0, 1.0]))\n+        self.assertAllClose(v.value, np.array([1.0, 2.0, 3.0]))\n \n     def test_variable_assign_sub_return(self):\n         \"\"\"Test assign_sub a new value and returning.\"\"\"\n-        v = backend.Variable(initializer=np.array([2, 3, 4]))\n-        r = v.assign_sub(np.array([1, 1, 1]))\n-        self.assertAllClose(r, np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([2.0, 3.0, 4.0]))\n+        r = v.assign_sub(np.array([1.0, 1.0, 1.0]))\n+        self.assertAllClose(r, np.array([1.0, 2.0, 3.0]))\n \n     def test_deferred_initialize_within_stateless_scope(self):\n         \"\"\"Test deferred init within a stateless scope.\"\"\"\n@@ -355,22 +378,27 @@ class VariableDtypeShapeNdimRepr(test_case.TestCase):\n \n     def test_variable_dtype(self):\n         \"\"\"Test retrieving the dtype of a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v = backend.Variable(\n+            initializer=np.array([1.0, 2.0, 3.0], dtype=np.float32)\n+        )\n         self.assertEqual(v.dtype, \"float32\")\n \n     def test_variable_shape(self):\n         \"\"\"Test retrieving the shape of a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n+        v = backend.Variable(initializer=np.array([[1.0, 2.0], [3.0, 4.0]]))\n         self.assertEqual(v.shape, (2, 2))\n \n     def test_variable_ndim(self):\n         \"\"\"Test retrieving the number of dimensions of a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n+        v = backend.Variable(initializer=np.array([[1.0, 2.0], [3.0, 4.0]]))\n         self.assertEqual(v.ndim, 2)\n \n     def test_variable_repr(self):\n         \"\"\"Test the string representation of a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]), name=\"test_var\")\n+        v = backend.Variable(\n+            initializer=np.array([1.0, 2.0, 3.0], dtype=np.float32),\n+            name=\"test_var\",\n+        )\n         expected_repr = (\n             \"<Variable path=test_var, shape=(3,), dtype=float32, \"\n             \"value=[1. 2. 3.]>\"\n@@ -389,32 +417,35 @@ class VariableDtypeShapeNdimRepr(test_case.TestCase):\n \n     def test_variable_getitem(self):\n         \"\"\"Test getting an item from a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n         self.assertEqual(v[0], 1)\n \n     def test_variable_initialize(self):\n         \"\"\"Test initializing a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        init_value = np.array([4, 5, 6])\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        init_value = np.array([4.0, 5.0, 6.0])\n         v._initialize(value=init_value)\n         self.assertAllClose(v.value, init_value)\n \n     def test_variable_convert_to_tensor(self):\n         \"\"\"Test converting a variable to a tensor.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v._convert_to_tensor(v.value), np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(\n+            v._convert_to_tensor(v.value), np.array([1.0, 2.0, 3.0])\n+        )\n \n     def test_variable_convert_to_tensor_with_dtype(self):\n         \"\"\"Test converting a variable to a tensor with a dtype.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n         self.assertAllClose(\n-            v._convert_to_tensor(v.value, dtype=\"float32\"), np.array([1, 2, 3])\n+            v._convert_to_tensor(v.value, dtype=\"float32\"),\n+            np.array([1.0, 2.0, 3.0]),\n         )\n \n     def test_variable_array(self):\n         \"\"\"Test converting a variable to an array.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v.__array__(), np.array([1, 2, 3]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(v.__array__(), np.array([1.0, 2.0, 3.0]))\n \n \n class VariableOpsCorrectnessTest(test_case.TestCase):\n@@ -430,13 +461,13 @@ class VariableOpsCorrectnessTest(test_case.TestCase):\n \n     def test__neg__(self):\n         \"\"\"Test negating a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([-1, 2]), trainable=False)\n-        self.assertAllClose(v.__neg__(), np.array([1, -2]))\n+        v = backend.Variable(initializer=np.array([-1.0, 2.0]), trainable=False)\n+        self.assertAllClose(v.__neg__(), np.array([1.0, -2.0]))\n \n     def test__abs__(self):\n         \"\"\"Test absolute value on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([-1, 2]), trainable=False)\n-        self.assertAllClose(v.__abs__(), np.array([1, 2]))\n+        v = backend.Variable(initializer=np.array([-1.0, 2.0]), trainable=False)\n+        self.assertAllClose(v.__abs__(), np.array([1.0, 2.0]))\n \n     def test__invert__(self):\n         \"\"\"Test bitwise not on a variable.\"\"\"\n@@ -447,135 +478,145 @@ class VariableOpsCorrectnessTest(test_case.TestCase):\n \n     def test__eq__(self):\n         \"\"\"Test equality comparison on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n-        self.assertAllClose(v.__eq__(np.array([1, 2])), np.array([True, True]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0]), trainable=False)\n+        self.assertAllClose(\n+            v.__eq__(np.array([1.0, 2.0])), np.array([True, True])\n+        )\n \n     def test__ne__(self):\n         \"\"\"Test inequality comparison on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        v = backend.Variable(initializer=np.array([1.0, 2.0]), trainable=False)\n         self.assertAllClose(\n-            v.__ne__(np.array([1, 2])), np.array([False, False])\n+            v.__ne__(np.array([1.0, 2.0])), np.array([False, False])\n         )\n \n     def test__lt__(self):\n         \"\"\"Test less than comparison on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        v = backend.Variable(initializer=np.array([1.0, 2.0]), trainable=False)\n         self.assertAllClose(\n-            v.__lt__(np.array([1, 2])), np.array([False, False])\n+            v.__lt__(np.array([1.0, 2.0])), np.array([False, False])\n         )\n \n     def test__le__(self):\n         \"\"\"Test less than or equal to comparison on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n-        self.assertAllClose(v.__le__(np.array([1, 2])), np.array([True, True]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0]), trainable=False)\n+        self.assertAllClose(\n+            v.__le__(np.array([1.0, 2.0])), np.array([True, True])\n+        )\n \n     def test__gt__(self):\n         \"\"\"Test greater than comparison on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n+        v = backend.Variable(initializer=np.array([1.0, 2.0]), trainable=False)\n         self.assertAllClose(\n-            v.__gt__(np.array([1, 2])), np.array([False, False])\n+            v.__gt__(np.array([1.0, 2.0])), np.array([False, False])\n         )\n \n     def test__ge__(self):\n         \"\"\"Test greater than or equal to comparison on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([1, 2]), trainable=False)\n-        self.assertAllClose(v.__ge__(np.array([1, 2])), np.array([True, True]))\n+        v = backend.Variable(initializer=np.array([1.0, 2.0]), trainable=False)\n+        self.assertAllClose(\n+            v.__ge__(np.array([1.0, 2.0])), np.array([True, True])\n+        )\n \n     def test__add__(self):\n         \"\"\"Test addition operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        self.assertAllClose(v1.__add__(v2), np.array([5, 7, 9]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__add__(v2), np.array([5.0, 7.0, 9.0]))\n \n     def test__radd__(self):\n         \"\"\"Test reverse addition operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        self.assertAllClose(v1.__radd__(v2), np.array([5, 7, 9]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__radd__(v2), np.array([5.0, 7.0, 9.0]))\n \n     def test__sub__(self):\n         \"\"\"Test subtraction operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        self.assertAllClose(v1.__sub__(v2), np.array([-3, -3, -3]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__sub__(v2), np.array([-3.0, -3.0, -3.0]))\n \n     def test__rsub__(self):\n         \"\"\"Test reverse subtraction operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v1.__rsub__(v2), np.array([-3, -3, -3]))\n+        v1 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        v2 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(v1.__rsub__(v2), np.array([-3.0, -3.0, -3.0]))\n \n     def test__mul__(self):\n         \"\"\"Test multiplication operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        self.assertAllClose(v1.__mul__(v2), np.array([4, 10, 18]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__mul__(v2), np.array([4.0, 10.0, 18.0]))\n \n     def test__rmul__(self):\n         \"\"\"Test reverse multiplication operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        self.assertAllClose(v1.__rmul__(v2), np.array([4, 10, 18]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__rmul__(v2), np.array([4.0, 10.0, 18.0]))\n \n     def test__truediv__(self):\n         \"\"\"Test true division operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n         self.assertAllClose(v1.__truediv__(v2), np.array([0.25, 0.4, 0.5]))\n \n     def test__rtruediv__(self):\n         \"\"\"Test reverse true division operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([4, 5, 6]))\n-        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n+        v1 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n+        v2 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n         self.assertAllClose(v1.__rtruediv__(v2), np.array([0.25, 0.4, 0.5]))\n \n     def test__floordiv__(self):\n         \"\"\"Test floordiv operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([-4, 5, 6]))\n-        self.assertAllClose(v1.__floordiv__(v2), np.array([-1, 0, 0]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([-4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__floordiv__(v2), np.array([-1.0, 0.0, 0.0]))\n \n     def test__rfloordiv__(self):\n         \"\"\"Test reverse floordiv operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([-4, 5, 6]))\n-        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v1.__rfloordiv__(v2), np.array([-1, 0, 0]))\n+        v1 = backend.Variable(initializer=np.array([-4.0, 5.0, 6.0]))\n+        v2 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(v1.__rfloordiv__(v2), np.array([-1.0, 0.0, 0.0]))\n \n     def test__mod__(self):\n         \"\"\"Test mod operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([-4, 5, 6]))\n-        self.assertAllClose(v1.__mod__(v2), np.array([-3, 2, 3]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([-4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__mod__(v2), np.array([-3.0, 2.0, 3.0]))\n \n     def test__rmod__(self):\n         \"\"\"Test reverse mod operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v1.__rmod__(v2), np.array([0, 0, 0]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(v1.__rmod__(v2), np.array([0.0, 0.0, 0.0]))\n \n     def test__pow__(self):\n         \"\"\"Test pow operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([-4, 5, 6]))\n-        self.assertAllClose(v1.__pow__(v2), np.array([1, 32, 729]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([-4.0, 5.0, 6.0]))\n+        self.assertAllClose(v1.__pow__(v2), np.array([1.0, 32.0, 729.0]))\n \n     def test__rpow__(self):\n         \"\"\"Test reverse power operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        self.assertAllClose(v1.__rpow__(v2), np.array([1, 4, 27]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        self.assertAllClose(v1.__rpow__(v2), np.array([1.0, 4.0, 27.0]))\n \n     def test__matmul__(self):\n         \"\"\"Test matmul operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n-        v2 = backend.Variable(initializer=np.array([[5, 6], [7, 8]]))\n-        self.assertAllClose(v1.__matmul__(v2), np.array([[19, 22], [43, 50]]))\n+        v1 = backend.Variable(initializer=np.array([[1.0, 2.0], [3.0, 4.0]]))\n+        v2 = backend.Variable(initializer=np.array([[5.0, 6.0], [7.0, 8.0]]))\n+        self.assertAllClose(\n+            v1.__matmul__(v2), np.array([[19.0, 22.0], [43.0, 50.0]])\n+        )\n \n     def test__rmatmul__(self):\n         \"\"\"Test reverse matmul operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([[1, 2], [3, 4]]))\n-        v2 = backend.Variable(initializer=np.array([[5, 6], [7, 8]]))\n-        self.assertAllClose(v1.__rmatmul__(v2), np.array([[23, 34], [31, 46]]))\n+        v1 = backend.Variable(initializer=np.array([[1.0, 2.0], [3.0, 4.0]]))\n+        v2 = backend.Variable(initializer=np.array([[5.0, 6.0], [7.0, 8.0]]))\n+        self.assertAllClose(\n+            v1.__rmatmul__(v2), np.array([[23.0, 34.0], [31.0, 46.0]])\n+        )\n \n     def test__and__(self):\n         \"\"\"Test bitwise and operation on a variable.\"\"\"\n@@ -639,26 +680,26 @@ class VariableOpsCorrectnessTest(test_case.TestCase):\n \n     def test__pos__(self):\n         \"\"\"Test unary plus on a variable.\"\"\"\n-        v = backend.Variable(initializer=np.array([-1, 2]), trainable=False)\n-        self.assertAllClose(v.__pos__(), np.array([-1, 2]))\n+        v = backend.Variable(initializer=np.array([-1.0, 2.0]), trainable=False)\n+        self.assertAllClose(v.__pos__(), np.array([-1.0, 2.0]))\n \n     def test_variable_pow(self):\n         \"\"\"Test pow operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n         result = v1**v2\n-        self.assertAllClose(result, np.array([1, 32, 729]))\n+        self.assertAllClose(result, np.array([1.0, 32.0, 729.0]))\n \n     def test_variable_rpow(self):\n         \"\"\"Test reverse power operation on a variable.\"\"\"\n-        v1 = backend.Variable(initializer=np.array([1, 2, 3]))\n-        v2 = backend.Variable(initializer=np.array([4, 5, 6]))\n+        v1 = backend.Variable(initializer=np.array([1.0, 2.0, 3.0]))\n+        v2 = backend.Variable(initializer=np.array([4.0, 5.0, 6.0]))\n         result = v2**v1\n-        self.assertAllClose(result, np.array([4, 25, 216]))\n+        self.assertAllClose(result, np.array([4.0, 25.0, 216.0]))\n \n     def test_round(self):\n         v = backend.Variable(initializer=np.array([1.1, 2.2, 3.3]))\n-        self.assertAllClose(round(v), np.array([1, 2, 3]))\n+        self.assertAllClose(round(v), np.array([1.0, 2.0, 3.0]))\n \n \n class VariableOpsBehaviorTest(test_case.TestCase):\n@@ -731,8 +772,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.equal(x1_jax, x2_jax).dtype)\n@@ -746,8 +787,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.not_equal(x1_jax, x2_jax).dtype)\n@@ -761,8 +802,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.less(x1_jax, x2_jax).dtype)\n@@ -776,8 +817,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.less_equal(x1_jax, x2_jax).dtype)\n@@ -791,8 +832,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.greater(x1_jax, x2_jax).dtype)\n@@ -806,8 +847,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(\n@@ -823,8 +864,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.add(x1_jax, x2_jax).dtype)\n@@ -839,8 +880,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.add(x1_jax, x2_jax).dtype)\n@@ -855,8 +896,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.add(x1_jax, x2_jax).dtype)\n@@ -876,8 +917,12 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         # the expected dtype from 64 bit to 32 bit when using jax backend.\n         with jax.experimental.disable_x64():\n             dtype1, dtype2 = dtypes\n-            x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-            x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+            x1 = backend.Variable(\n+                \"ones\", shape=(1,), dtype=dtype1, trainable=False\n+            )\n+            x2 = backend.Variable(\n+                \"ones\", shape=(1,), dtype=dtype2, trainable=False\n+            )\n             x1_jax = jnp.ones((1,), dtype=dtype1)\n             x2_jax = jnp.ones((1,), dtype=dtype2)\n             expected_dtype = standardize_dtype(\n@@ -898,8 +943,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(\n@@ -916,8 +961,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.mod(x1_jax, x2_jax).dtype)\n@@ -932,8 +977,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.power(x1_jax, x2_jax).dtype)\n@@ -948,8 +993,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.matmul(x1_jax, x2_jax).dtype)\n@@ -964,8 +1009,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(\n@@ -982,8 +1027,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(jnp.logical_or(x1_jax, x2_jax).dtype)\n@@ -998,8 +1043,8 @@ class VariableOpsDTypeTest(test_case.TestCase):\n         import jax.numpy as jnp\n \n         dtype1, dtype2 = dtypes\n-        x1 = backend.Variable(np.ones((1,)), dtype=dtype1, trainable=False)\n-        x2 = backend.Variable(np.ones((1,)), dtype=dtype2, trainable=False)\n+        x1 = backend.Variable(\"ones\", shape=(1,), dtype=dtype1, trainable=False)\n+        x2 = backend.Variable(\"ones\", shape=(1,), dtype=dtype2, trainable=False)\n         x1_jax = jnp.ones((1,), dtype=dtype1)\n         x2_jax = jnp.ones((1,), dtype=dtype2)\n         expected_dtype = standardize_dtype(\n\n@@ -20,7 +20,6 @@ IS_THREAD_SAFE = True\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n-        value = jnp.array(value, dtype=self._dtype)\n         # Note that variable.shape is needed by distribution_lib\n         self._shape = tuple(value.shape)\n         # We can't import the keras/distribution/distribution_lib\n\n@@ -20,7 +20,7 @@ IS_THREAD_SAFE = True\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n-        self._value = np.array(value, dtype=self._dtype)\n+        self._value = value\n \n     def _direct_assign(self, value):\n         self._value = np.array(value, dtype=self._dtype)\n\n@@ -21,19 +21,19 @@ class RegularizersTest(testing.TestCase):\n         self.run_class_serialization_test(reg)\n \n     def test_l1(self):\n-        value = np.random.random((4, 4))\n+        value = np.random.random((4, 4)).astype(np.float32)\n         x = backend.Variable(value)\n         y = regularizers.L1(0.1)(x)\n         self.assertAllClose(y, 0.1 * np.sum(np.abs(value)))\n \n     def test_l2(self):\n-        value = np.random.random((4, 4))\n+        value = np.random.random((4, 4)).astype(np.float32)\n         x = backend.Variable(value)\n         y = regularizers.L2(0.1)(x)\n         self.assertAllClose(y, 0.1 * np.sum(np.square(value)))\n \n     def test_l1_l2(self):\n-        value = np.random.random((4, 4))\n+        value = np.random.random((4, 4)).astype(np.float32)\n         x = backend.Variable(value)\n         y = regularizers.L1L2(l1=0.1, l2=0.2)(x)\n         self.assertAllClose(\n@@ -41,7 +41,7 @@ class RegularizersTest(testing.TestCase):\n         )\n \n     def test_orthogonal_regularizer(self):\n-        value = np.random.random((4, 4))\n+        value = np.random.random((4, 4)).astype(np.float32)\n         x = backend.Variable(value)\n         y = regularizers.OrthogonalRegularizer(factor=0.1, mode=\"rows\")(x)\n \n@@ -103,7 +103,7 @@ class RegularizersTest(testing.TestCase):\n \n     def test_orthogonal_regularizer_input_rank_validation(self):\n         with self.assertRaises(ValueError) as context:\n-            value = np.random.random((4, 4, 4))\n+            value = np.random.random((4, 4, 4)).astype(np.float32)\n             x = backend.Variable(value)\n             regularizers.OrthogonalRegularizer(factor=0.1)(x)\n \n\n@@ -16,8 +16,8 @@ class TrackingTest(testing.TestCase):\n                 ),\n             }\n         )\n-        v1 = backend.Variable(1)\n-        v2 = backend.Variable(2)\n+        v1 = backend.Variable(1.0)\n+        v2 = backend.Variable(2.0)\n         lst = tracking.TrackedList([], tracker)\n         lst.append(v1)\n         lst.append(None)\n@@ -67,8 +67,8 @@ class TrackingTest(testing.TestCase):\n                 ),\n             }\n         )\n-        v1 = backend.Variable(1)\n-        v2 = backend.Variable(2)\n+        v1 = backend.Variable(1.0)\n+        v2 = backend.Variable(2.0)\n         tup = (v1, v2)\n         tup = tracker.track(tup)\n         self.assertIsInstance(tup, tuple)\n@@ -86,8 +86,8 @@ class TrackingTest(testing.TestCase):\n                 ),\n             }\n         )\n-        v1 = backend.Variable(1)\n-        v2 = backend.Variable(2)\n+        v1 = backend.Variable(1.0)\n+        v2 = backend.Variable(2.0)\n         nt = collections.namedtuple(\"NT\", [\"x\", \"y\"])\n         tup = nt(x=v1, y=v2)\n         tup = tracker.track(tup)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58ac1505085e1f2773cdeb293cd143da823c4963", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 357 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 23 | Complexity Δ (Sum/Max): 37/23 | Churn Δ: 357 | Churn Cumulative: 1124 | Contributors (this commit): 5 | Commits (past 90d): 24 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -145,6 +145,9 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n+    Equalization,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n     MaxNumBoundingBoxes,\n )\n\n@@ -145,6 +145,9 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n+    Equalization,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n     MaxNumBoundingBoxes,\n )\n\n@@ -88,6 +88,9 @@ from keras.src.layers.preprocessing.image_preprocessing.auto_contrast import (\n from keras.src.layers.preprocessing.image_preprocessing.center_crop import (\n     CenterCrop,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n+    Equalization,\n+)\n from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n     MaxNumBoundingBoxes,\n )\n\n@@ -0,0 +1,213 @@\n+from keras.src import backend\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+\n+\n+@keras_export(\"keras.layers.Equalization\")\n+class Equalization(BaseImagePreprocessingLayer):\n+    \"\"\"Preprocessing layer for histogram equalization on image channels.\n+\n+    Histogram equalization is a technique to adjust image intensities to\n+    enhance contrast by effectively spreading out the most frequent\n+    intensity values. This layer applies equalization on a channel-wise\n+    basis, which can improve the visibility of details in images.\n+\n+    This layer works with both grayscale and color images, performing\n+    equalization independently on each color channel. At inference time,\n+    the equalization is consistently applied.\n+\n+    **Note:** This layer is safe to use inside a `tf.data` pipeline\n+    (independently of which backend you're using).\n+\n+    Args:\n+        value_range: Optional list/tuple of 2 floats specifying the lower\n+            and upper limits of the input data values. Defaults to `[0, 255]`.\n+            If the input image has been scaled, use the appropriate range\n+            (e.g., `[0.0, 1.0]`). The equalization will be scaled to this\n+            range, and output values will be clipped accordingly.\n+        bins: Integer specifying the number of histogram bins to use for\n+            equalization. Defaults to 256, which is suitable for 8-bit images.\n+            Larger values can provide more granular intensity redistribution.\n+\n+    Input shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format,\n+        or `(..., channels, height, width)`, in `\"channels_first\"` format.\n+\n+    Output shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., target_height, target_width, channels)`,\n+        or `(..., channels, target_height, target_width)`,\n+        in `\"channels_first\"` format.\n+\n+    Example:\n+\n+    ```python\n+    # Create an equalization layer for standard 8-bit images\n+    equalizer = keras.layers.Equalization()\n+\n+    # An image with uneven intensity distribution\n+    image = [...] # your input image\n+\n+    # Apply histogram equalization\n+    equalized_image = equalizer(image)\n+\n+    # For images with custom value range\n+    custom_equalizer = keras.layers.Equalization(\n+        value_range=[0.0, 1.0],  # for normalized images\n+        bins=128  # fewer bins for more subtle equalization\n+    )\n+    custom_equalized = custom_equalizer(normalized_image)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self, value_range=(0, 255), bins=256, data_format=None, **kwargs\n+    ):\n+        super().__init__(**kwargs)\n+        self.bins = bins\n+        self._set_value_range(value_range)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+    def _set_value_range(self, value_range):\n+        if not isinstance(value_range, (tuple, list)):\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        if len(value_range) != 2:\n+            raise ValueError(\n+                self._VALUE_RANGE_VALIDATION_ERROR\n+                + f\"Received: value_range={value_range}\"\n+            )\n+        self.value_range = sorted(value_range)\n+\n+    def _custom_histogram_fixed_width(self, values, value_range, nbins):\n+        values = self.backend.cast(values, \"float32\")\n+        value_min, value_max = value_range\n+        value_min = self.backend.cast(value_min, \"float32\")\n+        value_max = self.backend.cast(value_max, \"float32\")\n+\n+        scaled = (values - value_min) * (nbins - 1) / (value_max - value_min)\n+        indices = self.backend.cast(scaled, \"int32\")\n+        indices = self.backend.numpy.clip(indices, 0, nbins - 1)\n+        flat_indices = self.backend.numpy.reshape(indices, [-1])\n+\n+        if backend.backend() == \"jax\":\n+            # for JAX bincount is never jittable because of output shape\n+            histogram = self.backend.numpy.zeros(nbins, dtype=\"int32\")\n+            for i in range(nbins):\n+                matches = self.backend.cast(\n+                    self.backend.numpy.equal(flat_indices, i), \"int32\"\n+                )\n+                bin_count = self.backend.numpy.sum(matches)\n+                one_hot = self.backend.cast(\n+                    self.backend.numpy.arange(nbins) == i, \"int32\"\n+                )\n+                histogram = histogram + (bin_count * one_hot)\n+            return histogram\n+        else:\n+            # TensorFlow/PyTorch/NumPy implementation using bincount\n+            return self.backend.numpy.bincount(\n+                flat_indices,\n+                minlength=nbins,\n+            )\n+\n+    def _scale_values(self, values, source_range, target_range):\n+        source_min, source_max = source_range\n+        target_min, target_max = target_range\n+        scale = (target_max - target_min) / (source_max - source_min)\n+        offset = target_min - source_min * scale\n+        return values * scale + offset\n+\n+    def _equalize_channel(self, channel, value_range):\n+        if value_range != (0, 255):\n+            channel = self._scale_values(channel, value_range, (0, 255))\n+\n+        hist = self._custom_histogram_fixed_width(\n+            channel, value_range=(0, 255), nbins=self.bins\n+        )\n+\n+        nonzero_bins = self.backend.numpy.count_nonzero(hist)\n+        equalized = self.backend.numpy.where(\n+            nonzero_bins <= 1, channel, self._apply_equalization(channel, hist)\n+        )\n+\n+        if value_range != (0, 255):\n+            equalized = self._scale_values(equalized, (0, 255), value_range)\n+\n+        return equalized\n+\n+    def _apply_equalization(self, channel, hist):\n+        cdf = self.backend.numpy.cumsum(hist)\n+\n+        if backend.backend() == \"jax\":\n+            mask = cdf > 0\n+            first_nonzero_idx = self.backend.numpy.argmax(mask)\n+            cdf_min = self.backend.numpy.take(cdf, first_nonzero_idx)\n+        else:\n+            cdf_min = self.backend.numpy.take(\n+                cdf, self.backend.numpy.nonzero(cdf)[0][0]\n+            )\n+\n+        denominator = cdf[-1] - cdf_min\n+        denominator = self.backend.numpy.where(\n+            denominator == 0,\n+            self.backend.numpy.ones_like(1, dtype=denominator.dtype),\n+            denominator,\n+        )\n+\n+        lookup_table = ((cdf - cdf_min) * 255) / denominator\n+        lookup_table = self.backend.numpy.clip(\n+            self.backend.numpy.round(lookup_table), 0, 255\n+        )\n+\n+        scaled_channel = (channel / 255.0) * (self.bins - 1)\n+        indices = self.backend.cast(\n+            self.backend.numpy.clip(scaled_channel, 0, self.bins - 1), \"int32\"\n+        )\n+        return self.backend.numpy.take(lookup_table, indices)\n+\n+    def transform_images(self, images, transformations=None, **kwargs):\n+        images = self.backend.cast(images, self.compute_dtype)\n+\n+        if self.data_format == \"channels_first\":\n+            channels = []\n+            for i in range(self.backend.core.shape(images)[-3]):\n+                channel = images[..., i, :, :]\n+                equalized = self._equalize_channel(channel, self.value_range)\n+                channels.append(equalized)\n+            equalized_images = self.backend.numpy.stack(channels, axis=-3)\n+        else:\n+            channels = []\n+            for i in range(self.backend.core.shape(images)[-1]):\n+                channel = images[..., i]\n+                equalized = self._equalize_channel(channel, self.value_range)\n+                channels.append(equalized)\n+            equalized_images = self.backend.numpy.stack(channels, axis=-1)\n+\n+        return self.backend.cast(equalized_images, self.compute_dtype)\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def compute_output_spec(self, inputs, **kwargs):\n+        return inputs\n+\n+    def transform_bounding_boxes(self, bounding_boxes, **kwargs):\n+        return bounding_boxes\n+\n+    def transform_labels(self, labels, transformations=None, **kwargs):\n+        return labels\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformations, **kwargs\n+    ):\n+        return segmentation_masks\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"bins\": self.bins, \"value_range\": self.value_range})\n+        return config\n\n@@ -0,0 +1,135 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+from tensorflow import data as tf_data\n+\n+from keras.src import layers\n+from keras.src import ops\n+from keras.src import testing\n+\n+\n+class EqualizationTest(testing.TestCase):\n+    def assertAllInRange(self, array, min_val, max_val):\n+        self.assertTrue(np.all(array >= min_val))\n+        self.assertTrue(np.all(array <= max_val))\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.Equalization,\n+            init_kwargs={\n+                \"value_range\": (0, 255),\n+                \"data_format\": \"channels_last\",\n+            },\n+            input_shape=(1, 2, 2, 3),\n+            supports_masking=False,\n+            expected_output_shape=(1, 2, 2, 3),\n+        )\n+\n+        self.run_layer_test(\n+            layers.Equalization,\n+            init_kwargs={\n+                \"value_range\": (0, 255),\n+                \"data_format\": \"channels_first\",\n+            },\n+            input_shape=(1, 3, 2, 2),\n+            supports_masking=False,\n+            expected_output_shape=(1, 3, 2, 2),\n+        )\n+\n+    def test_equalizes_to_all_bins(self):\n+        xs = np.random.uniform(size=(2, 512, 512, 3), low=0, high=255).astype(\n+            np.float32\n+        )\n+        layer = layers.Equalization(value_range=(0, 255))\n+        xs = layer(xs)\n+\n+        for i in range(0, 256):\n+            self.assertTrue(np.any(ops.convert_to_numpy(xs) == i))\n+\n+    @parameterized.named_parameters(\n+        (\"float32\", np.float32), (\"int32\", np.int32), (\"int64\", np.int64)\n+    )\n+    def test_input_dtypes(self, dtype):\n+        xs = np.random.uniform(size=(2, 512, 512, 3), low=0, high=255).astype(\n+            dtype\n+        )\n+        layer = layers.Equalization(value_range=(0, 255))\n+        xs = ops.convert_to_numpy(layer(xs))\n+\n+        for i in range(0, 256):\n+            self.assertTrue(np.any(xs == i))\n+        self.assertAllInRange(xs, 0, 255)\n+\n+    @parameterized.named_parameters((\"0_255\", 0, 255), (\"0_1\", 0, 1))\n+    def test_output_range(self, lower, upper):\n+        xs = np.random.uniform(\n+            size=(2, 512, 512, 3), low=lower, high=upper\n+        ).astype(np.float32)\n+        layer = layers.Equalization(value_range=(lower, upper))\n+        xs = ops.convert_to_numpy(layer(xs))\n+        self.assertAllInRange(xs, lower, upper)\n+\n+    def test_constant_regions(self):\n+        xs = np.zeros((1, 64, 64, 3), dtype=np.float32)\n+        xs[:, :21, :, :] = 50\n+        xs[:, 21:42, :, :] = 100\n+        xs[:, 42:, :, :] = 200\n+\n+        layer = layers.Equalization(value_range=(0, 255))\n+        equalized = ops.convert_to_numpy(layer(xs))\n+\n+        self.assertTrue(len(np.unique(equalized)) >= 3)\n+        self.assertAllInRange(equalized, 0, 255)\n+\n+    def test_grayscale_images(self):\n+        xs_last = np.random.uniform(0, 255, size=(2, 64, 64, 1)).astype(\n+            np.float32\n+        )\n+        layer_last = layers.Equalization(\n+            value_range=(0, 255), data_format=\"channels_last\"\n+        )\n+        equalized_last = ops.convert_to_numpy(layer_last(xs_last))\n+        self.assertEqual(equalized_last.shape[-1], 1)\n+        self.assertAllInRange(equalized_last, 0, 255)\n+\n+        xs_first = np.random.uniform(0, 255, size=(2, 1, 64, 64)).astype(\n+            np.float32\n+        )\n+        layer_first = layers.Equalization(\n+            value_range=(0, 255), data_format=\"channels_first\"\n+        )\n+        equalized_first = ops.convert_to_numpy(layer_first(xs_first))\n+        self.assertEqual(equalized_first.shape[1], 1)\n+        self.assertAllInRange(equalized_first, 0, 255)\n+\n+    def test_single_color_image(self):\n+        xs_last = np.full((1, 64, 64, 3), 128, dtype=np.float32)\n+        layer_last = layers.Equalization(\n+            value_range=(0, 255), data_format=\"channels_last\"\n+        )\n+        equalized_last = ops.convert_to_numpy(layer_last(xs_last))\n+        self.assertAllClose(equalized_last, 128.0)\n+\n+        xs_first = np.full((1, 3, 64, 64), 128, dtype=np.float32)\n+        layer_first = layers.Equalization(\n+            value_range=(0, 255), data_format=\"channels_first\"\n+        )\n+        equalized_first = ops.convert_to_numpy(layer_first(xs_first))\n+        self.assertAllClose(equalized_first, 128.0)\n+\n+    def test_different_bin_sizes(self):\n+        xs = np.random.uniform(0, 255, size=(1, 64, 64, 3)).astype(np.float32)\n+        bin_sizes = [16, 64, 128, 256]\n+        for bins in bin_sizes:\n+            layer = layers.Equalization(value_range=(0, 255), bins=bins)\n+            equalized = ops.convert_to_numpy(layer(xs))\n+            self.assertAllInRange(equalized, 0, 255)\n+\n+    def test_tf_data_compatibility(self):\n+        layer = layers.Equalization(value_range=(0, 255))\n+        input_data = np.random.random((2, 8, 8, 3)) * 255\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output_array = output.numpy()\n+            self.assertAllInRange(output_array, 0, 255)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
