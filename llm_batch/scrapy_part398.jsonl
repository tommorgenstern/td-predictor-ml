{"custom_id": "scrapy#9d35428770326a3e833a2720c4f641fa70b58d29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 62 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -24,12 +24,3 @@ item_dropped = object()\n item_error = object()\n feed_slot_closed = object()\n feed_exporter_closed = object()\n-\n-# for backward compatibility\n-stats_spider_opened = spider_opened\n-stats_spider_closing = spider_closed\n-stats_spider_closed = spider_closed\n-\n-item_passed = item_scraped\n-\n-request_received = request_scheduled\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4e0a3087e4f4f2bc118d0f09b71e7440e78c42d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 2652 | Contributors (this commit): 36 | Commits (past 90d): 6 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -120,12 +120,12 @@ class Crawler:\n                 install_reactor(reactor_class, event_loop)\n             else:\n                 from twisted.internet import reactor  # noqa: F401\n-            log_reactor_info()\n         if reactor_class:\n             verify_installed_reactor(reactor_class)\n             if is_asyncio_reactor_installed() and event_loop:\n                 verify_installed_asyncio_event_loop(event_loop)\n \n+        if self._init_reactor or reactor_class:\n             log_reactor_info()\n \n         self.extensions = ExtensionManager.from_crawler(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f041f26a6ff636b764d2bf584ddbc9b9e4334d1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 659 | Contributors (this commit): 26 | Commits (past 90d): 6 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -35,6 +35,7 @@ extensions = [\n     \"sphinx.ext.coverage\",\n     \"sphinx.ext.intersphinx\",\n     \"sphinx.ext.viewcode\",\n+    \"sphinx_rtd_dark_mode\",\n ]\n \n templates_path = [\"_templates\"]\n@@ -174,3 +175,5 @@ hoverxref_role_types = {\n     \"signal\": \"tooltip\",\n }\n hoverxref_roles = [\"command\", \"reqmeta\", \"setting\", \"signal\"]\n+\n+default_dark_mode = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8978d405c32ee63375e09bf0b66b1e803da3d08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 14 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 3419 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -27,7 +27,7 @@ from scrapy.utils.defer import (\n     maybe_deferred_to_future,\n     mustbe_deferred,\n )\n-from scrapy.utils.python import MutableAsyncChain, MutableChain\n+from scrapy.utils.python import MutableAsyncChain, MutableChain, global_object_name\n \n if TYPE_CHECKING:\n     from collections.abc import Generator\n@@ -51,10 +51,6 @@ def _isiterable(o: Any) -> bool:\n class SpiderMiddlewareManager(MiddlewareManager):\n     component_name = \"spider middleware\"\n \n-    def __init__(self, *middlewares: Any):\n-        super().__init__(*middlewares)\n-        self.downgrade_warning_done = False\n-\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings: BaseSettings) -> list[Any]:\n         return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n@@ -227,12 +223,13 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     # Iterable -> AsyncIterable\n                     result = as_async_generator(result)\n                 elif need_downgrade:\n-                    if not self.downgrade_warning_done:\n                     logger.warning(\n-                            f\"Async iterable passed to {method.__qualname__} \"\n-                            f\"was downgraded to a non-async one\"\n+                        f\"Async iterable passed to {method.__qualname__} was\"\n+                        f\" downgraded to a non-async one. This is deprecated and will\"\n+                        f\" stop working in a future version of Scrapy. Please see\"\n+                        f\" https://docs.scrapy.org/en/latest/topics/coroutines.html#mixing-synchronous-and-asynchronous-spider-middlewares\"\n+                        f\" for more information.\"\n                     )\n-                        self.downgrade_warning_done = True\n                     assert isinstance(result, AsyncIterable)\n                     # AsyncIterable -> Iterable\n                     result = yield deferred_from_coro(collect_asyncgen(result))\n@@ -340,10 +337,19 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         methodname_async = methodname + \"_async\"\n         async_method: Callable | None = getattr(mw, methodname_async, None)\n         if not async_method:\n+            if normal_method and not isasyncgenfunction(normal_method):\n+                logger.warning(\n+                    f\"Middleware {global_object_name(mw.__class__)} doesn't support\"\n+                    f\" asynchronous spider output, this is deprecated and will stop\"\n+                    f\" working in a future version of Scrapy. The middleware should\"\n+                    f\" be updated to support it. Please see\"\n+                    f\" https://docs.scrapy.org/en/latest/topics/coroutines.html#mixing-synchronous-and-asynchronous-spider-middlewares\"\n+                    f\" for more information.\"\n+                )\n             return normal_method\n         if not normal_method:\n             logger.error(\n-                f\"Middleware {mw.__qualname__} has {methodname_async} \"\n+                f\"Middleware {global_object_name(mw.__class__)} has {methodname_async} \"\n                 f\"without {methodname}, skipping this method.\"\n             )\n             return None\n\n@@ -152,6 +152,10 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n         self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n         self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n+        self.assertEqual(\n+            \"doesn't support asynchronous spider output\" in str(log),\n+            ProcessSpiderOutputSimpleMiddleware in mw_classes,\n+        )\n \n     @defer.inlineCallbacks\n     def _test_asyncgen_base(\n@@ -376,21 +380,21 @@ class UniversalMiddlewareManagerTest(TestCase):\n         self.mwman = SpiderMiddlewareManager()\n \n     def test_simple_mw(self):\n-        mw = ProcessSpiderOutputSimpleMiddleware\n+        mw = ProcessSpiderOutputSimpleMiddleware()\n         self.mwman._add_middleware(mw)\n         self.assertEqual(\n             self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n         )\n \n     def test_async_mw(self):\n-        mw = ProcessSpiderOutputAsyncGenMiddleware\n+        mw = ProcessSpiderOutputAsyncGenMiddleware()\n         self.mwman._add_middleware(mw)\n         self.assertEqual(\n             self.mwman.methods[\"process_spider_output\"][0], mw.process_spider_output\n         )\n \n     def test_universal_mw(self):\n-        mw = ProcessSpiderOutputUniversalMiddleware\n+        mw = ProcessSpiderOutputUniversalMiddleware()\n         self.mwman._add_middleware(mw)\n         self.assertEqual(\n             self.mwman.methods[\"process_spider_output\"][0],\n@@ -399,7 +403,7 @@ class UniversalMiddlewareManagerTest(TestCase):\n \n     def test_universal_mw_no_sync(self):\n         with LogCapture() as log:\n-            self.mwman._add_middleware(UniversalMiddlewareNoSync)\n+            self.mwman._add_middleware(UniversalMiddlewareNoSync())\n         self.assertIn(\n             \"UniversalMiddlewareNoSync has process_spider_output_async\"\n             \" without process_spider_output\",\n@@ -408,7 +412,7 @@ class UniversalMiddlewareManagerTest(TestCase):\n         self.assertEqual(self.mwman.methods[\"process_spider_output\"][0], None)\n \n     def test_universal_mw_both_sync(self):\n-        mw = UniversalMiddlewareBothSync\n+        mw = UniversalMiddlewareBothSync()\n         with LogCapture() as log:\n             self.mwman._add_middleware(mw)\n         self.assertIn(\n@@ -422,7 +426,7 @@ class UniversalMiddlewareManagerTest(TestCase):\n \n     def test_universal_mw_both_async(self):\n         with LogCapture() as log:\n-            self.mwman._add_middleware(UniversalMiddlewareBothAsync)\n+            self.mwman._add_middleware(UniversalMiddlewareBothAsync())\n         self.assertIn(\n             \"UniversalMiddlewareBothAsync.process_spider_output \"\n             \"is an async generator function while process_spider_output_async exists\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ede9e9c3c3f9a9049fea2a6be0339b2c7434b8a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 2805 | Contributors (this commit): 35 | Commits (past 90d): 8 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,7 +79,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 result = method(response=response, spider=spider)\n                 if result is not None:\n                     msg = (\n-                        f\"{method.__qualname__} must return None \"\n+                        f\"{global_object_name(method)} must return None \"\n                         f\"or raise an exception, got {type(result)}\"\n                     )\n                     raise _InvalidOutput(msg)\n@@ -168,12 +168,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     )\n                 # we forbid waiting here because otherwise we would need to return a deferred from\n                 # _process_spider_exception too, which complicates the architecture\n-                msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n+                msg = f\"Async iterable returned from {global_object_name(method)} cannot be downgraded\"\n                 raise _InvalidOutput(msg)\n             if result is None:\n                 continue\n             msg = (\n-                f\"{method.__qualname__} must return None \"\n+                f\"{global_object_name(method)} must return None \"\n                 f\"or an iterable, got {type(result)}\"\n             )\n             raise _InvalidOutput(msg)\n@@ -224,7 +224,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     result = as_async_generator(result)\n                 elif need_downgrade:\n                     logger.warning(\n-                        f\"Async iterable passed to {method.__qualname__} was\"\n+                        f\"Async iterable passed to {global_object_name(method)} was\"\n                         f\" downgraded to a non-async one. This is deprecated and will\"\n                         f\" stop working in a future version of Scrapy. Please see\"\n                         f\" https://docs.scrapy.org/en/latest/topics/coroutines.html#mixing-synchronous-and-asynchronous-spider-middlewares\"\n@@ -257,12 +257,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 if iscoroutine(result):\n                     result.close()  # Silence warning about not awaiting\n                     msg = (\n-                        f\"{method.__qualname__} must be an asynchronous \"\n+                        f\"{global_object_name(method)} must be an asynchronous \"\n                         f\"generator (i.e. use yield)\"\n                     )\n                 else:\n                     msg = (\n-                        f\"{method.__qualname__} must return an iterable, got \"\n+                        f\"{global_object_name(method)} must return an iterable, got \"\n                         f\"{type(result)}\"\n                     )\n                 raise _InvalidOutput(msg)\n@@ -355,13 +355,13 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             return None\n         if not isasyncgenfunction(async_method):\n             logger.error(\n-                f\"{async_method.__qualname__} is not \"\n+                f\"{global_object_name(async_method)} is not \"\n                 f\"an async generator function, skipping this method.\"\n             )\n             return normal_method\n         if isasyncgenfunction(normal_method):\n             logger.error(\n-                f\"{normal_method.__qualname__} is an async \"\n+                f\"{global_object_name(normal_method)} is an async \"\n                 f\"generator function while {methodname_async} exists, \"\n                 f\"skipping both methods.\"\n             )\n\n@@ -326,11 +326,13 @@ def without_none_values(\n \n \n def global_object_name(obj: Any) -> str:\n-    \"\"\"Return the full import path of the given class.\n+    \"\"\"Return the full import path of the given object.\n \n     >>> from scrapy import Request\n     >>> global_object_name(Request)\n     'scrapy.http.request.Request'\n+    >>> global_object_name(Request.replace)\n+    'scrapy.http.request.Request.replace'\n     \"\"\"\n     return f\"{obj.__module__}.{obj.__qualname__}\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d5b189c1147e8aad632d4ef6759cc391d2017ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 25 | Churn Cumulative: 308 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,9 @@ from __future__ import annotations\n \n import logging\n import re\n+\n+# Iterable is needed at the run time for the SitemapSpider._parse_sitemap() annotation\n+from collections.abc import Iterable, Sequence  # noqa: TC003\n from typing import TYPE_CHECKING, Any, cast\n \n from scrapy.http import Request, Response, XmlResponse\n@@ -11,8 +14,6 @@ from scrapy.utils.gz import gunzip, gzip_magic_number\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n if TYPE_CHECKING:\n-    from collections.abc import Iterable, Sequence\n-\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -0,0 +1,20 @@\n+\"\"\"Tests that make sure parts needed for the scrapy-poet stack work.\"\"\"\n+\n+from typing import get_type_hints\n+\n+from scrapy import Spider\n+from scrapy.spiders import CrawlSpider, CSVFeedSpider, SitemapSpider, XMLFeedSpider\n+\n+\n+def test_callbacks():\n+    \"\"\"Making sure annotations on all non-abstract callbacks can be resolved.\"\"\"\n+\n+    for cb in [\n+        Spider._parse,\n+        CrawlSpider._parse,\n+        CrawlSpider._callback,\n+        XMLFeedSpider._parse,\n+        CSVFeedSpider._parse,\n+        SitemapSpider._parse_sitemap,\n+    ]:\n+        get_type_hints(cb)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a898331d14f889c1d4860cf1a364ba28285090a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 276 | Lines Deleted: 284 | Files Changed: 40 | Hunks: 263 | Methods Changed: 97 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 560 | Churn Cumulative: 32144 | Contributors (this commit): 130 | Commits (past 90d): 95 | Contributors (cumulative): 527 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -41,7 +41,7 @@ if not H2_ENABLED:\n     )\n \n \n-@pytest.fixture()\n+@pytest.fixture\n def chdir(tmpdir):\n     \"\"\"Change to pytest-provided temporary directory\"\"\"\n     tmpdir.chdir()\n\n@@ -15,9 +15,10 @@ from unittest import TestCase, mock\n \n from twisted.trial.unittest import SkipTest\n \n-from scrapy import Spider\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore_available\n+from scrapy.utils.deprecate import create_deprecated_class\n+from scrapy.utils.spider import DefaultSpider\n \n if TYPE_CHECKING:\n     from collections.abc import Awaitable\n@@ -25,6 +26,7 @@ if TYPE_CHECKING:\n     from twisted.internet.defer import Deferred\n     from twisted.web.client import Response as TxResponse\n \n+    from scrapy import Spider\n     from scrapy.crawler import Crawler\n \n \n@@ -82,8 +84,7 @@ def get_ftp_content_and_delete(\n     return b\"\".join(ftp_data)\n \n \n-class TestSpider(Spider):\n-    name = \"test\"\n+TestSpider = create_deprecated_class(\"TestSpider\", DefaultSpider)\n \n \n def get_crawler(\n@@ -101,7 +102,7 @@ def get_crawler(\n     settings: dict[str, Any] = {}\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n-    crawler = runner.create_crawler(spidercls or TestSpider)\n+    crawler = runner.create_crawler(spidercls or DefaultSpider)\n     crawler._apply_settings()\n     return crawler\n \n\n@@ -19,7 +19,7 @@ from threading import Timer\n from typing import TYPE_CHECKING\n from unittest import mock, skipIf\n \n-from pytest import mark\n+import pytest\n from twisted.trial import unittest\n \n import scrapy\n@@ -822,7 +822,7 @@ class MySpider(scrapy.Spider):\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n         )\n \n-    @mark.requires_uvloop\n+    @pytest.mark.requires_uvloop\n     def test_custom_asyncio_loop_enabled_true(self):\n         log = self.get_log(\n             self.debug_log_spider,\n\n@@ -21,7 +21,7 @@ from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n-class TestItem(Item):\n+class DemoItem(Item):\n     name = Field()\n     url = Field()\n \n@@ -58,7 +58,7 @@ class CustomFormContract(Contract):\n         return args\n \n \n-class TestSpider(Spider):\n+class DemoSpider(Spider):\n     name = \"demo_spider\"\n \n     def returns_request(self, response):\n@@ -80,7 +80,7 @@ class TestSpider(Spider):\n         @url http://scrapy.org\n         @returns items 1 1\n         \"\"\"\n-        return TestItem(url=response.url)\n+        return DemoItem(url=response.url)\n \n     def returns_request_cb_kwargs(self, response, url):\n         \"\"\"method which returns request\n@@ -96,7 +96,7 @@ class TestSpider(Spider):\n         @cb_kwargs {\"name\": \"Scrapy\"}\n         @returns items 1 1\n         \"\"\"\n-        return TestItem(name=name, url=response.url)\n+        return DemoItem(name=name, url=response.url)\n \n     def returns_item_cb_kwargs_error_unexpected_keyword(self, response):\n         \"\"\"method which returns item\n@@ -104,14 +104,14 @@ class TestSpider(Spider):\n         @cb_kwargs {\"arg\": \"value\"}\n         @returns items 1 1\n         \"\"\"\n-        return TestItem(url=response.url)\n+        return DemoItem(url=response.url)\n \n     def returns_item_cb_kwargs_error_missing_argument(self, response, arg):\n         \"\"\"method which returns item\n         @url http://scrapy.org\n         @returns items 1 1\n         \"\"\"\n-        return TestItem(url=response.url)\n+        return DemoItem(url=response.url)\n \n     def returns_dict_item(self, response):\n         \"\"\"method which returns item\n@@ -125,7 +125,7 @@ class TestSpider(Spider):\n         @url http://scrapy.org\n         @returns items 0 0\n         \"\"\"\n-        return TestItem(url=response.url)\n+        return DemoItem(url=response.url)\n \n     def returns_dict_fail(self, response):\n         \"\"\"method which returns item\n@@ -140,7 +140,7 @@ class TestSpider(Spider):\n         @returns items 1 1\n         @scrapes name url\n         \"\"\"\n-        return TestItem(name=\"test\", url=response.url)\n+        return DemoItem(name=\"test\", url=response.url)\n \n     def scrapes_dict_item_ok(self, response):\n         \"\"\"returns item with name and url\n@@ -156,7 +156,7 @@ class TestSpider(Spider):\n         @returns items 1 1\n         @scrapes name url\n         \"\"\"\n-        return TestItem(url=response.url)\n+        return DemoItem(url=response.url)\n \n     def scrapes_dict_item_fail(self, response):\n         \"\"\"returns item with no name\n@@ -212,7 +212,7 @@ class TestSpider(Spider):\n         @meta {\"key\": \"example\"}\n         @returns items 1 1\n         \"\"\"\n-        return TestItem(name=\"example\", url=response.url)\n+        return DemoItem(name=\"example\", url=response.url)\n \n     def returns_error_missing_meta(self, response):\n         \"\"\"method which depends of metadata be defined\n@@ -242,7 +242,7 @@ class CustomContractFailSpider(Spider):\n         \"\"\"\n \n \n-class InheritsTestSpider(TestSpider):\n+class InheritsDemoSpider(DemoSpider):\n     name = \"inherits_demo_spider\"\n \n \n@@ -274,7 +274,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.assertTrue(self.results.errors)\n \n     def test_contracts(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n \n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request)\n@@ -293,7 +293,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.assertEqual(request, None)\n \n     def test_cb_kwargs(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n \n         # extract contracts correctly\n@@ -356,7 +356,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_error()\n \n     def test_meta(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n \n         # extract contracts correctly\n         contracts = self.conman.extract_contracts(spider.returns_request_meta)\n@@ -402,7 +402,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_error()\n \n     def test_returns(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n \n         # returns_item\n@@ -431,7 +431,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_fail()\n \n     def test_returns_async(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n \n         request = self.conman.from_method(spider.returns_request_async, self.results)\n@@ -439,7 +439,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_error()\n \n     def test_scrapes(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n \n         # scrapes_item_ok\n@@ -472,7 +472,7 @@ class ContractsManagerTest(unittest.TestCase):\n         assert message in self.results.failures[-1][-1]\n \n     def test_regex(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n \n         # invalid regex\n@@ -494,7 +494,7 @@ class ContractsManagerTest(unittest.TestCase):\n         self.should_error()\n \n     def test_errback(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n \n         try:\n@@ -522,11 +522,11 @@ class ContractsManagerTest(unittest.TestCase):\n \n             def parse_first(self, response):\n                 self.visited += 1\n-                return TestItem()\n+                return DemoItem()\n \n             def parse_second(self, response):\n                 self.visited += 1\n-                return TestItem()\n+                return DemoItem()\n \n         with MockServer() as mockserver:\n             contract_doc = f\"@url {mockserver.url('/status?n=200')}\"\n@@ -540,13 +540,13 @@ class ContractsManagerTest(unittest.TestCase):\n         self.assertEqual(crawler.spider.visited, 2)\n \n     def test_form_contract(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         request = self.conman.from_method(spider.custom_form, self.results)\n         self.assertEqual(request.method, \"POST\")\n         self.assertIsInstance(request, FormRequest)\n \n     def test_inherited_contracts(self):\n-        spider = InheritsTestSpider()\n+        spider = InheritsDemoSpider()\n \n         requests = self.conman.from_spider(spider, self.results)\n         self.assertTrue(requests)\n@@ -571,7 +571,7 @@ class CustomContractPrePostProcess(unittest.TestCase):\n         self.results = TextTestResult(stream=None, descriptions=False, verbosity=0)\n \n     def test_pre_hook_keyboard_interrupt(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n         contract = CustomFailContractPreProcess(spider.returns_request)\n         conman = ContractsManager([contract])\n@@ -590,7 +590,7 @@ class CustomContractPrePostProcess(unittest.TestCase):\n         self.assertFalse(self.results.errors)\n \n     def test_post_hook_keyboard_interrupt(self):\n-        spider = TestSpider()\n+        spider = DemoSpider()\n         response = ResponseMock()\n         contract = CustomFailContractPostProcess(spider.returns_request)\n         conman = ContractsManager([contract])\n\n@@ -6,7 +6,7 @@ from ipaddress import IPv4Address\n from socket import gethostbyname\n from urllib.parse import urlparse\n \n-from pytest import mark\n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.internet.ssl import Certificate\n@@ -536,7 +536,7 @@ class CrawlSpiderTestCase(TestCase):\n             )\n         self.assertIn(\"Got response 200\", str(log))\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n         crawler = get_crawler(\n@@ -551,7 +551,7 @@ class CrawlSpiderTestCase(TestCase):\n             )\n         self.assertIn(\"Got response 200\", str(log))\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n         log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)\n@@ -559,7 +559,7 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIn({\"id\": 1}, items)\n         self.assertIn({\"id\": 2}, items)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_items_single_element(self):\n         items = []\n@@ -576,7 +576,7 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIn(\"Got response 200\", str(log))\n         self.assertIn({\"foo\": 42}, items)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse(self):\n         log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)\n@@ -584,7 +584,7 @@ class CrawlSpiderTestCase(TestCase):\n         itemcount = stats.get_value(\"item_scraped_count\")\n         self.assertEqual(itemcount, 1)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)\n@@ -594,7 +594,7 @@ class CrawlSpiderTestCase(TestCase):\n         for i in range(10):\n             self.assertIn({\"foo\": i}, items)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_exc(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenExcSpider)\n@@ -606,7 +606,7 @@ class CrawlSpiderTestCase(TestCase):\n         for i in range(7):\n             self.assertIn({\"foo\": i}, items)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n         _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)\n@@ -618,20 +618,20 @@ class CrawlSpiderTestCase(TestCase):\n         for i in [10, 30, 122]:\n             self.assertIn({\"index2\": i}, items)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_reqs_list(self):\n         log, *_ = yield self._run_spider(AsyncDefAsyncioReqsReturnSpider)\n         for req_id in range(3):\n             self.assertIn(f\"Got response 200, req_id {req_id}\", str(log))\n \n-    @mark.only_not_asyncio()\n+    @pytest.mark.only_not_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_deferred_direct(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)\n         self.assertEqual(items, [{\"code\": 200}])\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_async_def_deferred_wrapped(self):\n         log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)\n@@ -659,7 +659,9 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertEqual(cert.getSubject().commonName, b\"localhost\")\n         self.assertEqual(cert.getIssuer().commonName, b\"localhost\")\n \n-    @mark.xfail(reason=\"Responses with no body return early and contain no certificate\")\n+    @pytest.mark.xfail(\n+        reason=\"Responses with no body return early and contain no certificate\"\n+    )\n     @defer.inlineCallbacks\n     def test_response_ssl_certificate_empty_response(self):\n         crawler = get_crawler(SingleRequestSpider)\n\n@@ -8,9 +8,9 @@ import warnings\n from pathlib import Path\n from typing import Any\n \n+import pytest\n from packaging.version import parse as parse_version\n from pexpect.popen_spawn import PopenSpawn\n-from pytest import mark, raises\n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.trial import unittest\n from w3lib import __version__ as w3lib_version\n@@ -77,14 +77,14 @@ class CrawlerTestCase(BaseCrawlerTest):\n         self.assertOptionIsDefault(crawler.settings, \"RETRY_ENABLED\")\n \n     def test_crawler_rejects_spider_objects(self):\n-        with raises(ValueError):\n+        with pytest.raises(ValueError, match=\"spidercls argument must be a class\"):\n             Crawler(DefaultSpider())\n \n     @inlineCallbacks\n     def test_crawler_crawl_twice_unsupported(self):\n         crawler = get_raw_crawler(NoRequestsSpider, BASE_SETTINGS)\n         yield crawler.crawl()\n-        with raises(RuntimeError, match=\"more than once on the same instance\"):\n+        with pytest.raises(RuntimeError, match=\"more than once on the same instance\"):\n             yield crawler.crawl()\n \n     def test_get_addon(self):\n@@ -203,7 +203,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n                     raise\n \n         crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n-        with raises(RuntimeError):\n+        with pytest.raises(RuntimeError):\n             yield crawler.crawl()\n \n     @inlineCallbacks\n@@ -282,7 +282,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n                     raise\n \n         crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n-        with raises(RuntimeError):\n+        with pytest.raises(RuntimeError):\n             yield crawler.crawl()\n \n     @inlineCallbacks\n@@ -361,7 +361,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n                     raise\n \n         crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n-        with raises(RuntimeError):\n+        with pytest.raises(RuntimeError):\n             yield crawler.crawl()\n \n     @inlineCallbacks\n@@ -440,7 +440,7 @@ class CrawlerTestCase(BaseCrawlerTest):\n                     raise\n \n         crawler = get_raw_crawler(MySpider, BASE_SETTINGS)\n-        with raises(RuntimeError):\n+        with pytest.raises(RuntimeError):\n             yield crawler.crawl()\n \n \n@@ -575,7 +575,7 @@ class NoRequestsSpider(scrapy.Spider):\n         return []\n \n \n-@mark.usefixtures(\"reactor_pytest\")\n+@pytest.mark.usefixtures(\"reactor_pytest\")\n class CrawlerRunnerHasSpider(unittest.TestCase):\n     def _runner(self):\n         return CrawlerRunner()\n@@ -744,7 +744,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log\n         )\n \n-    @mark.skipif(\n+    @pytest.mark.skipif(\n         parse_version(w3lib_version) >= parse_version(\"2.0.0\"),\n         reason=\"w3lib 2.0.0 and later do not allow invalid domains.\",\n     )\n@@ -781,7 +781,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"Using reactor: twisted.internet.selectreactor.SelectReactor\", log\n         )\n \n-    @mark.skipif(\n+    @pytest.mark.skipif(\n         platform.system() == \"Windows\", reason=\"PollReactor is not supported on Windows\"\n     )\n     def test_twisted_reactor_poll(self):\n@@ -820,7 +820,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             log,\n         )\n \n-    @mark.requires_uvloop\n+    @pytest.mark.requires_uvloop\n     def test_custom_loop_asyncio(self):\n         log = self.run_script(\"asyncio_custom_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -829,7 +829,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    @mark.requires_uvloop\n+    @pytest.mark.requires_uvloop\n     def test_custom_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -839,7 +839,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n-    @mark.requires_uvloop\n+    @pytest.mark.requires_uvloop\n     def test_asyncio_enabled_reactor_same_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_same_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -848,7 +848,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         )\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    @mark.requires_uvloop\n+    @pytest.mark.requires_uvloop\n     def test_asyncio_enabled_reactor_different_loop(self):\n         log = self.run_script(\"asyncio_enabled_reactor_different_loop.py\")\n         self.assertNotIn(\"Spider closed (finished)\", log)\n@@ -924,13 +924,13 @@ class CrawlerRunnerSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"DEBUG: Using asyncio event loop\", log)\n \n \n-@mark.parametrize(\n-    [\"settings\", \"items\"],\n-    (\n+@pytest.mark.parametrize(\n+    (\"settings\", \"items\"),\n+    [\n         ({}, default_settings.LOG_VERSIONS),\n         ({\"LOG_VERSIONS\": [\"itemadapter\"]}, [\"itemadapter\"]),\n         ({\"LOG_VERSIONS\": []}, None),\n-    ),\n+    ],\n )\n def test_log_scrapy_info(settings, items, caplog):\n     with caplog.at_level(\"INFO\"):\n\n@@ -1,7 +1,7 @@\n import asyncio\n from unittest import mock\n \n-from pytest import mark\n+import pytest\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.python.failure import Failure\n@@ -220,7 +220,7 @@ class MiddlewareUsingDeferreds(ManagerTestCase):\n         self.assertFalse(download_func.called)\n \n \n-@mark.usefixtures(\"reactor_pytest\")\n+@pytest.mark.usefixtures(\"reactor_pytest\")\n class MiddlewareUsingCoro(ManagerTestCase):\n     \"\"\"Middlewares using asyncio coroutines should work\"\"\"\n \n@@ -243,7 +243,7 @@ class MiddlewareUsingCoro(ManagerTestCase):\n         self.assertIs(results[0], resp)\n         self.assertFalse(download_func.called)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     def test_asyncdef_asyncio(self):\n         resp = Response(\"http://example.com/index.html\")\n \n\n@@ -7,8 +7,6 @@ from scrapy.http import HtmlResponse, Request, Response\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n \n-__doctests__ = [\"scrapy.downloadermiddlewares.ajaxcrawl\"]\n-\n \n @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n\n@@ -7,18 +7,18 @@ from scrapy.http import Request\n from scrapy.spiders import Spider\n \n \n-class TestSpiderLegacy(Spider):\n+class LegacySpider(Spider):\n     http_user = \"foo\"\n     http_pass = \"bar\"\n \n \n-class TestSpider(Spider):\n+class DomainSpider(Spider):\n     http_user = \"foo\"\n     http_pass = \"bar\"\n     http_auth_domain = \"example.com\"\n \n \n-class TestSpiderAny(Spider):\n+class AnyDomainSpider(Spider):\n     http_user = \"foo\"\n     http_pass = \"bar\"\n     http_auth_domain = None\n@@ -26,7 +26,7 @@ class TestSpiderAny(Spider):\n \n class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n     def setUp(self):\n-        self.spider = TestSpiderLegacy(\"foo\")\n+        self.spider = LegacySpider(\"foo\")\n \n     def test_auth(self):\n         with self.assertRaises(AttributeError):\n@@ -37,7 +37,7 @@ class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n class HttpAuthMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n         self.mw = HttpAuthMiddleware()\n-        self.spider = TestSpider(\"foo\")\n+        self.spider = DomainSpider(\"foo\")\n         self.mw.spider_opened(self.spider)\n \n     def tearDown(self):\n@@ -67,7 +67,7 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n class HttpAuthAnyMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n         self.mw = HttpAuthMiddleware()\n-        self.spider = TestSpiderAny(\"foo\")\n+        self.spider = AnyDomainSpider(\"foo\")\n         self.mw.spider_opened(self.spider)\n \n     def tearDown(self):\n\n@@ -353,7 +353,8 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 resc = mw.storage.retrieve_response(self.spider, req0)\n                 if shouldcache:\n                     self.assertEqualResponse(resc, res1)\n-                    assert \"cached\" in res2.flags and res2.status != 304\n+                    assert \"cached\" in res2.flags\n+                    assert res2.status != 304\n                 else:\n                     self.assertFalse(resc)\n                     assert \"cached\" not in res2.flags\n@@ -376,7 +377,8 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 resc = mw.storage.retrieve_response(self.spider, req0)\n                 if shouldcache:\n                     self.assertEqualResponse(resc, res1)\n-                    assert \"cached\" in res2.flags and res2.status != 304\n+                    assert \"cached\" in res2.flags\n+                    assert res2.status != 304\n                 else:\n                     self.assertFalse(resc)\n                     assert \"cached\" not in res2.flags\n\n@@ -131,7 +131,8 @@ class TestHttpProxyMiddleware(TestCase):\n         mw = HttpProxyMiddleware()\n         req = Request(\"http://noproxy.com\", meta={\"proxy\": None})\n         assert mw.process_request(req, spider) is None\n-        assert \"proxy\" in req.meta and req.meta[\"proxy\"] is None\n+        assert \"proxy\" in req.meta\n+        assert req.meta[\"proxy\"] is None\n \n     def test_no_proxy(self):\n         os.environ[\"http_proxy\"] = \"https://proxy.for.http:3128\"\n\n@@ -12,7 +12,7 @@ UNSET = object()\n \n @pytest.mark.parametrize(\n     (\"allowed_domain\", \"url\", \"allowed\"),\n-    (\n+    [\n         (\"example.com\", \"http://example.com/1\", True),\n         (\"example.com\", \"http://example.org/1\", False),\n         (\"example.com\", \"http://sub.example.com/1\", True),\n@@ -24,7 +24,7 @@ UNSET = object()\n         (\"example.com\", \"http://example.com.example\", False),\n         (\"a.example\", \"http://nota.example\", False),\n         (\"b.a.example\", \"http://notb.a.example\", False),\n-    ),\n+    ],\n )\n def test_process_request_domain_filtering(allowed_domain, url, allowed):\n     crawler = get_crawler(Spider)\n@@ -41,12 +41,12 @@ def test_process_request_domain_filtering(allowed_domain, url, allowed):\n \n @pytest.mark.parametrize(\n     (\"value\", \"filtered\"),\n-    (\n+    [\n         (UNSET, True),\n         (None, True),\n         (False, True),\n         (True, False),\n-    ),\n+    ],\n )\n def test_process_request_dont_filter(value, filtered):\n     crawler = get_crawler(Spider)\n@@ -66,7 +66,7 @@ def test_process_request_dont_filter(value, filtered):\n \n @pytest.mark.parametrize(\n     (\"allow_offsite\", \"dont_filter\", \"filtered\"),\n-    (\n+    [\n         (True, UNSET, False),\n         (True, None, False),\n         (True, False, False),\n@@ -75,7 +75,7 @@ def test_process_request_dont_filter(value, filtered):\n         (False, None, True),\n         (False, False, True),\n         (False, True, False),\n-    ),\n+    ],\n )\n def test_process_request_allow_offsite(allow_offsite, dont_filter, filtered):\n     crawler = get_crawler(Spider)\n@@ -97,11 +97,11 @@ def test_process_request_allow_offsite(allow_offsite, dont_filter, filtered):\n \n @pytest.mark.parametrize(\n     \"value\",\n-    (\n+    [\n         UNSET,\n         None,\n         [],\n-    ),\n+    ],\n )\n def test_process_request_no_allowed_domains(value):\n     crawler = get_crawler(Spider)\n@@ -133,7 +133,7 @@ def test_process_request_invalid_domains():\n \n @pytest.mark.parametrize(\n     (\"allowed_domain\", \"url\", \"allowed\"),\n-    (\n+    [\n         (\"example.com\", \"http://example.com/1\", True),\n         (\"example.com\", \"http://example.org/1\", False),\n         (\"example.com\", \"http://sub.example.com/1\", True),\n@@ -145,7 +145,7 @@ def test_process_request_invalid_domains():\n         (\"example.com\", \"http://example.com.example\", False),\n         (\"a.example\", \"http://nota.example\", False),\n         (\"b.a.example\", \"http://notb.a.example\", False),\n-    ),\n+    ],\n )\n def test_request_scheduled_domain_filtering(allowed_domain, url, allowed):\n     crawler = get_crawler(Spider)\n@@ -162,12 +162,12 @@ def test_request_scheduled_domain_filtering(allowed_domain, url, allowed):\n \n @pytest.mark.parametrize(\n     (\"value\", \"filtered\"),\n-    (\n+    [\n         (UNSET, True),\n         (None, True),\n         (False, True),\n         (True, False),\n-    ),\n+    ],\n )\n def test_request_scheduled_dont_filter(value, filtered):\n     crawler = get_crawler(Spider)\n@@ -187,11 +187,11 @@ def test_request_scheduled_dont_filter(value, filtered):\n \n @pytest.mark.parametrize(\n     \"value\",\n-    (\n+    [\n         UNSET,\n         None,\n         [],\n-    ),\n+    ],\n )\n def test_request_scheduled_no_allowed_domains(value):\n     crawler = get_crawler(Spider)\n\n@@ -1278,7 +1278,7 @@ class MetaRefreshMiddlewareTest(Base.Test):\n \n @pytest.mark.parametrize(\n     SCHEME_PARAMS,\n-    (\n+    [\n         *REDIRECT_SCHEME_CASES,\n         # data/file/ftp/s3/foo → * does not redirect\n         *(\n@@ -1300,7 +1300,7 @@ class MetaRefreshMiddlewareTest(Base.Test):\n             for scheme in NON_HTTP_SCHEMES\n             for location in (\"//example.com/b\", \"/b\")\n         ),\n-    ),\n+    ],\n )\n def test_meta_refresh_schemes(url, location, target):\n     crawler = get_crawler(Spider)\n\n@@ -42,7 +42,7 @@ from scrapy.utils.test import get_crawler\n from tests import get_testdata, tests_datadir\n \n \n-class TestItem(Item):\n+class MyItem(Item):\n     name = Field()\n     url = Field()\n     price = Field()\n@@ -62,7 +62,7 @@ class DataClassItem:\n     price: int = 0\n \n \n-class TestSpider(Spider):\n+class MySpider(Spider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n \n@@ -70,7 +70,7 @@ class TestSpider(Spider):\n     name_re = re.compile(r\"<h1>(.*?)</h1>\", re.MULTILINE)\n     price_re = re.compile(r\">Price: \\$(.*?)<\", re.MULTILINE)\n \n-    item_cls: type = TestItem\n+    item_cls: type = MyItem\n \n     def parse(self, response):\n         xlink = LinkExtractor()\n@@ -91,24 +91,24 @@ class TestSpider(Spider):\n         return adapter.item\n \n \n-class TestDupeFilterSpider(TestSpider):\n+class DupeFilterSpider(MySpider):\n     def start_requests(self):\n         return (Request(url) for url in self.start_urls)  # no dont_filter=True\n \n \n-class DictItemsSpider(TestSpider):\n+class DictItemsSpider(MySpider):\n     item_cls = dict\n \n \n-class AttrsItemsSpider(TestSpider):\n+class AttrsItemsSpider(MySpider):\n     item_cls = AttrsItem\n \n \n-class DataClassItemsSpider(TestSpider):\n+class DataClassItemsSpider(MySpider):\n     item_cls = DataClassItem\n \n \n-class ItemZeroDivisionErrorSpider(TestSpider):\n+class ItemZeroDivisionErrorSpider(MySpider):\n     custom_settings = {\n         \"ITEM_PIPELINES\": {\n             \"tests.pipelines.ProcessWithZeroDivisionErrorPipeline\": 300,\n@@ -116,7 +116,7 @@ class ItemZeroDivisionErrorSpider(TestSpider):\n     }\n \n \n-class ChangeCloseReasonSpider(TestSpider):\n+class ChangeCloseReasonSpider(MySpider):\n     @classmethod\n     def from_crawler(cls, crawler, *args, **kwargs):\n         spider = cls(*args, **kwargs)\n@@ -388,7 +388,7 @@ class EngineTest(EngineTestBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n-            TestSpider,\n+            MySpider,\n             DictItemsSpider,\n             AttrsItemsSpider,\n             DataClassItemsSpider,\n@@ -404,7 +404,7 @@ class EngineTest(EngineTestBase):\n \n     @defer.inlineCallbacks\n     def test_crawler_dupefilter(self):\n-        run = CrawlerRun(TestDupeFilterSpider)\n+        run = CrawlerRun(DupeFilterSpider)\n         yield run.run()\n         self._assert_scheduled_requests(run, count=8)\n         self._assert_dropped_requests(run)\n@@ -426,13 +426,13 @@ class EngineTest(EngineTestBase):\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n-        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n+        e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n         yield e.close()\n \n     @defer.inlineCallbacks\n     def test_start_already_running_exception(self):\n-        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n-        yield e.open_spider(TestSpider(), [])\n+        e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n+        yield e.open_spider(MySpider(), [])\n         e.start()\n         try:\n             yield self.assertFailure(e.start(), RuntimeError).addBoth(\n@@ -486,7 +486,7 @@ def test_request_scheduled_signal(caplog):\n         if \"drop\" in request.url:\n             raise IgnoreRequest\n \n-    spider = TestSpider()\n+    spider = MySpider()\n     crawler = get_crawler(spider.__class__)\n     engine = ExecutionEngine(crawler, lambda _: None)\n     engine.downloader._slot_gc_loop.stop()\n\n@@ -8,7 +8,7 @@ from tests.test_engine import (\n     DataClassItemsSpider,\n     DictItemsSpider,\n     EngineTestBase,\n-    TestSpider,\n+    MySpider,\n )\n \n \n@@ -22,7 +22,7 @@ class BytesReceivedEngineTest(EngineTestBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n-            TestSpider,\n+            MySpider,\n             DictItemsSpider,\n             AttrsItemsSpider,\n             DataClassItemsSpider,\n\n@@ -8,7 +8,7 @@ from tests.test_engine import (\n     DataClassItemsSpider,\n     DictItemsSpider,\n     EngineTestBase,\n-    TestSpider,\n+    MySpider,\n )\n \n \n@@ -22,7 +22,7 @@ class HeadersReceivedEngineTest(EngineTestBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n-            TestSpider,\n+            MySpider,\n             DictItemsSpider,\n             AttrsItemsSpider,\n             DataClassItemsSpider,\n\n@@ -31,7 +31,7 @@ def custom_serializer(value):\n     return str(int(value) + 2)\n \n \n-class TestItem(Item):\n+class MyItem(Item):\n     name = Field()\n     age = Field()\n \n@@ -42,7 +42,7 @@ class CustomFieldItem(Item):\n \n \n @dataclasses.dataclass\n-class TestDataClass:\n+class MyDataClass:\n     name: str\n     age: int\n \n@@ -54,7 +54,7 @@ class CustomFieldDataclass:\n \n \n class BaseItemExporterTest(unittest.TestCase):\n-    item_class: type = TestItem\n+    item_class: type = MyItem\n     custom_field_item_class: type = CustomFieldItem\n \n     def setUp(self):\n@@ -138,7 +138,7 @@ class BaseItemExporterTest(unittest.TestCase):\n \n \n class BaseItemExporterDataclassTest(BaseItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -207,7 +207,7 @@ class PythonItemExporterTest(BaseItemExporterTest):\n \n \n class PythonItemExporterDataclassTest(PythonItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -222,7 +222,7 @@ class PprintItemExporterTest(BaseItemExporterTest):\n \n \n class PprintItemExporterDataclassTest(PprintItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -259,7 +259,7 @@ class PickleItemExporterTest(BaseItemExporterTest):\n \n \n class PickleItemExporterDataclassTest(PickleItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -286,7 +286,7 @@ class MarshalItemExporterTest(BaseItemExporterTest):\n \n \n class MarshalItemExporterDataclassTest(MarshalItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -406,7 +406,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n \n \n class CsvItemExporterDataclassTest(CsvItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -517,7 +517,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n \n \n class XmlItemExporterDataclassTest(XmlItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -563,7 +563,7 @@ class JsonLinesItemExporterTest(BaseItemExporterTest):\n \n \n class JsonLinesItemExporterDataclassTest(JsonLinesItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n@@ -595,11 +595,11 @@ class JsonItemExporterTest(JsonLinesItemExporterTest):\n         self.assertTwoItemsExported(ItemAdapter(self.i).asdict())\n \n     def test_two_items_with_failure_between(self):\n-        i1 = TestItem(name=\"Joseph\\xa3\", age=\"22\")\n-        i2 = TestItem(\n+        i1 = MyItem(name=\"Joseph\\xa3\", age=\"22\")\n+        i2 = MyItem(\n             name=\"Maria\", age=1j\n         )  # Invalid datetimes didn't consistently fail between Python versions\n-        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        i3 = MyItem(name=\"Jesus\", age=\"44\")\n         self.ie.start_exporting()\n         self.ie.export_item(i1)\n         self.assertRaises(TypeError, self.ie.export_item, i2)\n@@ -652,9 +652,9 @@ class JsonItemExporterToBytesTest(BaseItemExporterTest):\n         return JsonItemExporter(self.output, **kwargs)\n \n     def test_two_items_with_failure_between(self):\n-        i1 = TestItem(name=\"Joseph\", age=\"22\")\n-        i2 = TestItem(name=\"\\u263a\", age=\"11\")\n-        i3 = TestItem(name=\"Jesus\", age=\"44\")\n+        i1 = MyItem(name=\"Joseph\", age=\"22\")\n+        i2 = MyItem(name=\"\\u263a\", age=\"11\")\n+        i3 = MyItem(name=\"Jesus\", age=\"44\")\n         self.ie.start_exporting()\n         self.ie.export_item(i1)\n         self.assertRaises(UnicodeEncodeError, self.ie.export_item, i2)\n@@ -665,12 +665,12 @@ class JsonItemExporterToBytesTest(BaseItemExporterTest):\n \n \n class JsonItemExporterDataclassTest(JsonItemExporterTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n     custom_field_item_class = CustomFieldDataclass\n \n \n class CustomExporterItemTest(unittest.TestCase):\n-    item_class: type = TestItem\n+    item_class: type = MyItem\n \n     def setUp(self):\n         if self.item_class is None:\n@@ -700,4 +700,4 @@ class CustomExporterItemTest(unittest.TestCase):\n \n \n class CustomExporterDataclassTest(CustomExporterItemTest):\n-    item_class = TestDataClass\n+    item_class = MyDataClass\n\n@@ -51,7 +51,7 @@ stats_dump_2 = {\n }\n \n \n-class TestExtPeriodicLog(PeriodicLog):\n+class CustomPeriodicLog(PeriodicLog):\n     def set_a(self):\n         self.stats._stats = stats_dump_1\n \n@@ -62,7 +62,7 @@ class TestExtPeriodicLog(PeriodicLog):\n def extension(settings=None):\n     crawler = Crawler(MetaSpider, settings=settings)\n     crawler._apply_settings()\n-    return TestExtPeriodicLog.from_crawler(crawler)\n+    return CustomPeriodicLog.from_crawler(crawler)\n \n \n class TestPeriodicLog(unittest.TestCase):\n\n@@ -13,15 +13,12 @@ from scrapy.settings.default_settings import (\n     DOWNLOAD_DELAY,\n )\n from scrapy.utils.misc import build_from_crawler\n+from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.test import get_crawler as _get_crawler\n \n UNSET = object()\n \n \n-class TestSpider(Spider):\n-    name = \"test\"\n-\n-\n def get_crawler(settings=None, spidercls=None):\n     settings = settings or {}\n     settings[\"AUTOTHROTTLE_ENABLED\"] = True\n@@ -30,11 +27,11 @@ def get_crawler(settings=None, spidercls=None):\n \n @pytest.mark.parametrize(\n     (\"value\", \"expected\"),\n-    (\n+    [\n         (UNSET, False),\n         (False, False),\n         (True, True),\n-    ),\n+    ],\n )\n def test_enabled(value, expected):\n     settings = {}\n@@ -50,10 +47,10 @@ def test_enabled(value, expected):\n \n @pytest.mark.parametrize(\n     \"value\",\n-    (\n+    [\n         0.0,\n         -1.0,\n-    ),\n+    ],\n )\n def test_target_concurrency_invalid(value):\n     settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": value}\n@@ -64,13 +61,13 @@ def test_target_concurrency_invalid(value):\n \n @pytest.mark.parametrize(\n     (\"spider\", \"setting\", \"expected\"),\n-    (\n+    [\n         (UNSET, UNSET, DOWNLOAD_DELAY),\n         (1.0, UNSET, 1.0),\n         (UNSET, 1.0, 1.0),\n         (1.0, 2.0, 1.0),\n         (3.0, 2.0, 3.0),\n-    ),\n+    ],\n )\n def test_mindelay_definition(spider, setting, expected):\n     settings = {}\n@@ -91,10 +88,10 @@ def test_mindelay_definition(spider, setting, expected):\n \n @pytest.mark.parametrize(\n     (\"value\", \"expected\"),\n-    (\n+    [\n         (UNSET, AUTOTHROTTLE_MAX_DELAY),\n         (1.0, 1.0),\n-    ),\n+    ],\n )\n def test_maxdelay_definition(value, expected):\n     settings = {}\n@@ -102,13 +99,13 @@ def test_maxdelay_definition(value, expected):\n         settings[\"AUTOTHROTTLE_MAX_DELAY\"] = value\n     crawler = get_crawler(settings)\n     at = build_from_crawler(AutoThrottle, crawler)\n-    at._spider_opened(TestSpider())\n+    at._spider_opened(DefaultSpider())\n     assert at.maxdelay == expected\n \n \n @pytest.mark.parametrize(\n     (\"min_spider\", \"min_setting\", \"start_setting\", \"expected\"),\n-    (\n+    [\n         (UNSET, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n         (AUTOTHROTTLE_START_DELAY - 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY),\n         (AUTOTHROTTLE_START_DELAY + 1.0, UNSET, UNSET, AUTOTHROTTLE_START_DELAY + 1.0),\n@@ -134,7 +131,7 @@ def test_maxdelay_definition(value, expected):\n             AUTOTHROTTLE_START_DELAY + 2.0,\n             AUTOTHROTTLE_START_DELAY + 2.0,\n         ),\n-    ),\n+    ],\n )\n def test_startdelay_definition(min_spider, min_setting, start_setting, expected):\n     settings = {}\n@@ -158,7 +155,7 @@ def test_startdelay_definition(min_spider, min_setting, start_setting, expected)\n \n @pytest.mark.parametrize(\n     (\"meta\", \"slot\"),\n-    (\n+    [\n         ({}, None),\n         ({\"download_latency\": 1.0}, None),\n         ({\"download_slot\": \"foo\"}, None),\n@@ -172,12 +169,12 @@ def test_startdelay_definition(min_spider, min_setting, start_setting, expected)\n             },\n             \"foo\",\n         ),\n-    ),\n+    ],\n )\n def test_skipped(meta, slot):\n     crawler = get_crawler()\n     at = build_from_crawler(AutoThrottle, crawler)\n-    spider = TestSpider()\n+    spider = DefaultSpider()\n     at._spider_opened(spider)\n     request = Request(\"https://example.com\", meta=meta)\n \n@@ -193,7 +190,7 @@ def test_skipped(meta, slot):\n \n @pytest.mark.parametrize(\n     (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n-    (\n+    [\n         (2.0, 2.0, 1.0, 1.0),\n         (1.0, 2.0, 1.0, 0.75),\n         (4.0, 2.0, 1.0, 2.0),\n@@ -201,13 +198,13 @@ def test_skipped(meta, slot):\n         (2.0, 4.0, 1.0, 0.75),\n         (2.0, 2.0, 0.5, 1.0),\n         (2.0, 2.0, 2.0, 1.5),\n-    ),\n+    ],\n )\n def test_adjustment(download_latency, target_concurrency, slot_delay, expected):\n     settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n     crawler = get_crawler(settings)\n     at = build_from_crawler(AutoThrottle, crawler)\n-    spider = TestSpider()\n+    spider = DefaultSpider()\n     at._spider_opened(spider)\n     meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n     request = Request(\"https://example.com\", meta=meta)\n@@ -227,11 +224,11 @@ def test_adjustment(download_latency, target_concurrency, slot_delay, expected):\n \n @pytest.mark.parametrize(\n     (\"mindelay\", \"maxdelay\", \"expected\"),\n-    (\n+    [\n         (0.5, 2.0, 1.0),\n         (0.25, 0.5, 0.5),\n         (2.0, 4.0, 2.0),\n-    ),\n+    ],\n )\n def test_adjustment_limits(mindelay, maxdelay, expected):\n     download_latency, target_concurrency, slot_delay = (2.0, 2.0, 1.0)\n@@ -243,7 +240,7 @@ def test_adjustment_limits(mindelay, maxdelay, expected):\n     }\n     crawler = get_crawler(settings)\n     at = build_from_crawler(AutoThrottle, crawler)\n-    spider = TestSpider()\n+    spider = DefaultSpider()\n     at._spider_opened(spider)\n     meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n     request = Request(\"https://example.com\", meta=meta)\n@@ -263,11 +260,11 @@ def test_adjustment_limits(mindelay, maxdelay, expected):\n \n @pytest.mark.parametrize(\n     (\"download_latency\", \"target_concurrency\", \"slot_delay\", \"expected\"),\n-    (\n+    [\n         (2.0, 2.0, 1.0, 1.0),\n         (1.0, 2.0, 1.0, 1.0),  # Instead of 0.75\n         (4.0, 2.0, 1.0, 2.0),\n-    ),\n+    ],\n )\n def test_adjustment_bad_response(\n     download_latency, target_concurrency, slot_delay, expected\n@@ -275,7 +272,7 @@ def test_adjustment_bad_response(\n     settings = {\"AUTOTHROTTLE_TARGET_CONCURRENCY\": target_concurrency}\n     crawler = get_crawler(settings)\n     at = build_from_crawler(AutoThrottle, crawler)\n-    spider = TestSpider()\n+    spider = DefaultSpider()\n     at._spider_opened(spider)\n     meta = {\"download_latency\": download_latency, \"download_slot\": \"foo\"}\n     request = Request(\"https://example.com\", meta=meta)\n@@ -297,7 +294,7 @@ def test_debug(caplog):\n     settings = {\"AUTOTHROTTLE_DEBUG\": True}\n     crawler = get_crawler(settings)\n     at = build_from_crawler(AutoThrottle, crawler)\n-    spider = TestSpider()\n+    spider = DefaultSpider()\n     at._spider_opened(spider)\n     meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n     request = Request(\"https://example.com\", meta=meta)\n@@ -327,7 +324,7 @@ def test_debug(caplog):\n def test_debug_disabled(caplog):\n     crawler = get_crawler()\n     at = build_from_crawler(AutoThrottle, crawler)\n-    spider = TestSpider()\n+    spider = DefaultSpider()\n     at._spider_opened(spider)\n     meta = {\"download_latency\": 1.0, \"download_slot\": \"foo\"}\n     request = Request(\"https://example.com\", meta=meta)\n\n@@ -258,7 +258,8 @@ class Https2ClientProtocolTestCase(TestCase):\n         :param path: Should have / at the starting compulsorily if not empty\n         :return: Complete url\n         \"\"\"\n-        assert len(path) > 0 and (path[0] == \"/\" or path[0] == \"&\")\n+        assert len(path) > 0\n+        assert path[0] == \"/\" or path[0] == \"&\"\n         return f\"{self.scheme}://{self.hostname}:{self.port_number}{path}\"\n \n     def make_request(self, request: Request) -> Deferred:\n\n@@ -2,8 +2,8 @@ import codecs\n import unittest\n from unittest import mock\n \n+import pytest\n from packaging.version import Version as parse_version\n-from pytest import mark\n from w3lib import __version__ as w3lib_version\n from w3lib.encoding import resolve_encoding\n \n@@ -218,7 +218,7 @@ class BaseResponseTest(unittest.TestCase):\n         r = self.response_class(\"http://example.com\")\n         self.assertRaises(ValueError, r.follow, None)\n \n-    @mark.xfail(\n+    @pytest.mark.xfail(\n         parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n         reason=\"https://github.com/scrapy/w3lib/pull/207\",\n         strict=True,\n@@ -226,7 +226,7 @@ class BaseResponseTest(unittest.TestCase):\n     def test_follow_whitespace_url(self):\n         self._assert_followed_url(\"foo \", \"http://example.com/foo\")\n \n-    @mark.xfail(\n+    @pytest.mark.xfail(\n         parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n         reason=\"https://github.com/scrapy/w3lib/pull/207\",\n         strict=True,\n@@ -473,10 +473,8 @@ class TextResponseTest(BaseResponseTest):\n         self._assert_response_encoding(r5, \"utf-8\")\n         self._assert_response_encoding(r8, \"utf-8\")\n         self._assert_response_encoding(r9, \"cp1252\")\n-        assert (\n-            r4._body_inferred_encoding() is not None\n-            and r4._body_inferred_encoding() != \"ascii\"\n-        )\n+        assert r4._body_inferred_encoding() is not None\n+        assert r4._body_inferred_encoding() != \"ascii\"\n         self._assert_response_values(r1, \"utf-8\", \"\\xa3\")\n         self._assert_response_values(r2, \"utf-8\", \"\\xa3\")\n         self._assert_response_values(r3, \"iso-8859-1\", \"\\xa3\")\n\n@@ -4,8 +4,8 @@ import pickle\n import re\n import unittest\n \n+import pytest\n from packaging.version import Version\n-from pytest import mark\n from w3lib import __version__ as w3lib_version\n \n from scrapy.http import HtmlResponse, XmlResponse\n@@ -930,7 +930,7 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n             ],\n         )\n \n-    @mark.skipif(\n+    @pytest.mark.skipif(\n         Version(w3lib_version) < Version(\"2.0.0\"),\n         reason=(\n             \"Before w3lib 2.0.0, w3lib.url.safe_url_string would not complain \"\n\n@@ -18,12 +18,12 @@ class NameItem(Item):\n     name = Field()\n \n \n-class TestItem(NameItem):\n+class SummaryItem(NameItem):\n     url = Field()\n     summary = Field()\n \n \n-class TestNestedItem(Item):\n+class NestedItem(Item):\n     name = Field()\n     name_div = Field()\n     name_value = Field()\n@@ -38,20 +38,20 @@ class AttrsNameItem:\n \n \n @dataclasses.dataclass\n-class TestDataClass:\n+class NameDataClass:\n     name: list = dataclasses.field(default_factory=list)\n \n \n # test item loaders\n class NameItemLoader(ItemLoader):\n-    default_item_class = TestItem\n+    default_item_class = SummaryItem\n \n \n class NestedItemLoader(ItemLoader):\n-    default_item_class = TestNestedItem\n+    default_item_class = NestedItem\n \n \n-class TestItemLoader(NameItemLoader):\n+class ProcessorItemLoader(NameItemLoader):\n     name_in = MapCompose(lambda v: v.title())\n \n \n@@ -68,11 +68,11 @@ def processor_with_args(value, other=None, loader_context=None):\n \n class BasicItemLoaderTest(unittest.TestCase):\n     def test_add_value_on_unknown_field(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         self.assertRaises(KeyError, il.add_value, \"wrong_field\", [\"lala\", \"lolo\"])\n \n     def test_load_item_using_default_loader(self):\n-        i = TestItem()\n+        i = SummaryItem()\n         i[\"summary\"] = \"lala\"\n         il = ItemLoader(item=i)\n         il.add_value(\"name\", \"marta\")\n@@ -82,7 +82,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(item[\"name\"], [\"marta\"])\n \n     def test_load_item_using_custom_loader(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.add_value(\"name\", \"marta\")\n         item = il.load_item()\n         self.assertEqual(item[\"name\"], [\"Marta\"])\n@@ -194,7 +194,7 @@ class InitializationFromAttrsItemTest(InitializationTestMixin, unittest.TestCase\n \n \n class InitializationFromDataClassTest(InitializationTestMixin, unittest.TestCase):\n-    item_class = TestDataClass\n+    item_class = NameDataClass\n \n \n class BaseNoInputReprocessingLoader(ItemLoader):\n@@ -289,11 +289,11 @@ class SelectortemLoaderTest(unittest.TestCase):\n     )\n \n     def test_init_method(self):\n-        l = TestItemLoader()\n+        l = ProcessorItemLoader()\n         self.assertEqual(l.selector, None)\n \n     def test_init_method_errors(self):\n-        l = TestItemLoader()\n+        l = ProcessorItemLoader()\n         self.assertRaises(RuntimeError, l.add_xpath, \"url\", \"//a/@href\")\n         self.assertRaises(RuntimeError, l.replace_xpath, \"url\", \"//a/@href\")\n         self.assertRaises(RuntimeError, l.get_xpath, \"//a/@href\")\n@@ -303,7 +303,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_init_method_with_selector(self):\n         sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n-        l = TestItemLoader(selector=sel)\n+        l = ProcessorItemLoader(selector=sel)\n         self.assertIs(l.selector, sel)\n \n         l.add_xpath(\"name\", \"//div/text()\")\n@@ -311,7 +311,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n \n     def test_init_method_with_selector_css(self):\n         sel = Selector(text=\"<html><body><div>marta</div></body></html>\")\n-        l = TestItemLoader(selector=sel)\n+        l = ProcessorItemLoader(selector=sel)\n         self.assertIs(l.selector, sel)\n \n         l.add_css(\"name\", \"div::text\")\n@@ -320,18 +320,18 @@ class SelectortemLoaderTest(unittest.TestCase):\n     def test_init_method_with_base_response(self):\n         \"\"\"Selector should be None after initialization\"\"\"\n         response = Response(\"https://scrapy.org\")\n-        l = TestItemLoader(response=response)\n+        l = ProcessorItemLoader(response=response)\n         self.assertIs(l.selector, None)\n \n     def test_init_method_with_response(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n \n         l.add_xpath(\"name\", \"//div/text()\")\n         self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n \n     def test_init_method_with_response_css(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n \n         l.add_css(\"name\", \"div::text\")\n@@ -350,12 +350,12 @@ class SelectortemLoaderTest(unittest.TestCase):\n         )\n \n     def test_add_xpath_re(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         l.add_xpath(\"name\", \"//div/text()\", re=\"ma\")\n         self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n \n     def test_replace_xpath(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n         l.add_xpath(\"name\", \"//div/text()\")\n         self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n@@ -366,7 +366,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\", \"Marta\"])\n \n     def test_get_xpath(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertEqual(l.get_xpath(\"//p/text()\"), [\"paragraph\"])\n         self.assertEqual(l.get_xpath(\"//p/text()\", TakeFirst()), \"paragraph\")\n         self.assertEqual(l.get_xpath(\"//p/text()\", TakeFirst(), re=\"pa\"), \"pa\")\n@@ -376,14 +376,14 @@ class SelectortemLoaderTest(unittest.TestCase):\n         )\n \n     def test_replace_xpath_multi_fields(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         l.add_xpath(None, \"//div/text()\", TakeFirst(), lambda x: {\"name\": x})\n         self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n         l.replace_xpath(None, \"//p/text()\", TakeFirst(), lambda x: {\"name\": x})\n         self.assertEqual(l.get_output_value(\"name\"), [\"Paragraph\"])\n \n     def test_replace_xpath_re(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n         l.add_xpath(\"name\", \"//div/text()\")\n         self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n@@ -391,7 +391,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n \n     def test_add_css_re(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         l.add_css(\"name\", \"div::text\", re=\"ma\")\n         self.assertEqual(l.get_output_value(\"name\"), [\"Ma\"])\n \n@@ -399,7 +399,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value(\"url\"), [\"www.scrapy.org\"])\n \n     def test_replace_css(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n         l.add_css(\"name\", \"div::text\")\n         self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n@@ -415,7 +415,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n \n     def test_get_css(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertEqual(l.get_css(\"p::text\"), [\"paragraph\"])\n         self.assertEqual(l.get_css(\"p::text\", TakeFirst()), \"paragraph\")\n         self.assertEqual(l.get_css(\"p::text\", TakeFirst(), re=\"pa\"), \"pa\")\n@@ -427,7 +427,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         )\n \n     def test_replace_css_multi_fields(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         l.add_css(None, \"div::text\", TakeFirst(), lambda x: {\"name\": x})\n         self.assertEqual(l.get_output_value(\"name\"), [\"Marta\"])\n         l.replace_css(None, \"p::text\", TakeFirst(), lambda x: {\"name\": x})\n@@ -439,7 +439,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n         self.assertEqual(l.get_output_value(\"url\"), [\"/images/logo.png\"])\n \n     def test_replace_css_re(self):\n-        l = TestItemLoader(response=self.response)\n+        l = ProcessorItemLoader(response=self.response)\n         self.assertTrue(l.selector)\n         l.add_css(\"url\", \"a::attr(href)\")\n         self.assertEqual(l.get_output_value(\"url\"), [\"http://www.scrapy.org\"])\n\n@@ -24,17 +24,17 @@ class NameItem(Item):\n     name = Field()\n \n \n-class TestItem(NameItem):\n+class SummaryItem(NameItem):\n     url = Field()\n     summary = Field()\n \n \n # test item loaders\n class NameItemLoader(ItemLoader):\n-    default_item_class = TestItem\n+    default_item_class = SummaryItem\n \n \n-class TestItemLoader(NameItemLoader):\n+class ProcessorItemLoader(NameItemLoader):\n     name_in = MapCompose(lambda v: v.title())\n \n \n@@ -51,7 +51,7 @@ def processor_with_args(value, other=None, loader_context=None):\n \n class BasicItemLoaderTest(unittest.TestCase):\n     def test_load_item_using_default_loader(self):\n-        i = TestItem()\n+        i = SummaryItem()\n         i[\"summary\"] = \"lala\"\n         il = ItemLoader(item=i)\n         il.add_value(\"name\", \"marta\")\n@@ -61,7 +61,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(item[\"name\"], [\"marta\"])\n \n     def test_load_item_using_custom_loader(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.add_value(\"name\", \"marta\")\n         item = il.load_item()\n         self.assertEqual(item[\"name\"], [\"Marta\"])\n@@ -125,7 +125,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         )\n \n     def test_add_value(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.add_value(\"name\", \"marta\")\n         self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\"])\n         self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n@@ -146,7 +146,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_collected_values(\"name\"), [0])\n \n     def test_replace_value(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.replace_value(\"name\", \"marta\")\n         self.assertEqual(il.get_collected_values(\"name\"), [\"Marta\"])\n         self.assertEqual(il.get_output_value(\"name\"), [\"Marta\"])\n@@ -229,7 +229,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), [\"mart\"])\n \n     def test_input_processor_inheritance(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildItemLoader(ProcessorItemLoader):\n             url_in = MapCompose(lambda v: v.lower())\n \n         il = ChildItemLoader()\n@@ -265,8 +265,8 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), [\"marta\"])\n \n     def test_extend_custom_input_processors(self):\n-        class ChildItemLoader(TestItemLoader):\n-            name_in = MapCompose(TestItemLoader.name_in, str.swapcase)\n+        class ChildItemLoader(ProcessorItemLoader):\n+            name_in = MapCompose(ProcessorItemLoader.name_in, str.swapcase)\n \n         il = ChildItemLoader()\n         il.add_value(\"name\", \"marta\")\n@@ -283,11 +283,11 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), [\"MART\"])\n \n     def test_output_processor_using_function(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.add_value(\"name\", [\"mar\", \"ta\"])\n         self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n-        class TakeFirstItemLoader(TestItemLoader):\n+        class TakeFirstItemLoader(ProcessorItemLoader):\n             name_out = \" \".join\n \n         il = TakeFirstItemLoader()\n@@ -296,7 +296,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n \n     def test_output_processor_error(self):\n         class TestItemLoader(ItemLoader):\n-            default_item_class = TestItem\n+            default_item_class = SummaryItem\n             name_out = MapCompose(float)\n \n         il = TestItemLoader()\n@@ -319,18 +319,18 @@ class BasicItemLoaderTest(unittest.TestCase):\n         assert expected_exc_str in s, s\n \n     def test_output_processor_using_classes(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.add_value(\"name\", [\"mar\", \"ta\"])\n         self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n-        class TakeFirstItemLoader(TestItemLoader):\n+        class TakeFirstItemLoader(ProcessorItemLoader):\n             name_out = Join()\n \n         il = TakeFirstItemLoader()\n         il.add_value(\"name\", [\"mar\", \"ta\"])\n         self.assertEqual(il.get_output_value(\"name\"), \"Mar Ta\")\n \n-        class TakeFirstItemLoader2(TestItemLoader):\n+        class TakeFirstItemLoader2(ProcessorItemLoader):\n             name_out = Join(\"<br>\")\n \n         il = TakeFirstItemLoader2()\n@@ -338,11 +338,11 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), \"Mar<br>Ta\")\n \n     def test_default_output_processor(self):\n-        il = TestItemLoader()\n+        il = ProcessorItemLoader()\n         il.add_value(\"name\", [\"mar\", \"ta\"])\n         self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n-        class LalaItemLoader(TestItemLoader):\n+        class LalaItemLoader(ProcessorItemLoader):\n             default_output_processor = Identity()\n \n         il = LalaItemLoader()\n@@ -350,7 +350,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"name\"), [\"Mar\", \"Ta\"])\n \n     def test_loader_context_on_declaration(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildItemLoader(ProcessorItemLoader):\n             url_in = MapCompose(processor_with_args, key=\"val\")\n \n         il = ChildItemLoader()\n@@ -360,7 +360,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n \n     def test_loader_context_on_instantiation(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildItemLoader(ProcessorItemLoader):\n             url_in = MapCompose(processor_with_args)\n \n         il = ChildItemLoader(key=\"val\")\n@@ -370,7 +370,7 @@ class BasicItemLoaderTest(unittest.TestCase):\n         self.assertEqual(il.get_output_value(\"url\"), [\"val\"])\n \n     def test_loader_context_on_assign(self):\n-        class ChildItemLoader(TestItemLoader):\n+        class ChildItemLoader(ProcessorItemLoader):\n             url_in = MapCompose(processor_with_args)\n \n         il = ChildItemLoader()\n@@ -384,10 +384,10 @@ class BasicItemLoaderTest(unittest.TestCase):\n         def processor(value, loader_context):\n             return loader_context[\"item\"][\"name\"]\n \n-        class ChildItemLoader(TestItemLoader):\n+        class ChildItemLoader(ProcessorItemLoader):\n             url_in = MapCompose(processor)\n \n-        it = TestItem(name=\"marta\")\n+        it = SummaryItem(name=\"marta\")\n         il = ChildItemLoader(item=it)\n         il.add_value(\"url\", \"text\")\n         self.assertEqual(il.get_output_value(\"url\"), [\"marta\"])\n\n@@ -40,7 +40,7 @@ class MOff:\n         raise NotConfigured(\"foo\")\n \n \n-class TestMiddlewareManager(MiddlewareManager):\n+class MyMiddlewareManager(MiddlewareManager):\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n         return [M1, MOff, M3]\n@@ -54,7 +54,7 @@ class TestMiddlewareManager(MiddlewareManager):\n class MiddlewareManagerTest(unittest.TestCase):\n     def test_init(self):\n         m1, m2, m3 = M1(), M2(), M3()\n-        mwman = TestMiddlewareManager(m1, m2, m3)\n+        mwman = MyMiddlewareManager(m1, m2, m3)\n         self.assertEqual(\n             list(mwman.methods[\"open_spider\"]), [m1.open_spider, m2.open_spider]\n         )\n@@ -64,7 +64,7 @@ class MiddlewareManagerTest(unittest.TestCase):\n         self.assertEqual(list(mwman.methods[\"process\"]), [m1.process, m3.process])\n \n     def test_methods(self):\n-        mwman = TestMiddlewareManager(M1(), M2(), M3())\n+        mwman = MyMiddlewareManager(M1(), M2(), M3())\n         self.assertEqual(\n             [x.__self__.__class__ for x in mwman.methods[\"open_spider\"]], [M1, M2]\n         )\n@@ -82,6 +82,6 @@ class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_enabled_from_settings(self):\n         crawler = get_crawler()\n-        mwman = TestMiddlewareManager.from_crawler(crawler)\n+        mwman = MyMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n         self.assertEqual(classes, [M1, M3])\n\n@@ -1,6 +1,6 @@\n import asyncio\n \n-from pytest import mark\n+import pytest\n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n@@ -118,14 +118,14 @@ class PipelineTestCase(unittest.TestCase):\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 1)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_asyncdef_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(self.items), 1)\n \n-    @mark.only_not_asyncio()\n+    @pytest.mark.only_not_asyncio\n     @defer.inlineCallbacks\n     def test_asyncdef_not_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefNotAsyncioPipeline)\n\n@@ -11,7 +11,7 @@ class CustomRequest(Request):\n \n class RequestSerializationTest(unittest.TestCase):\n     def setUp(self):\n-        self.spider = TestSpider()\n+        self.spider = MethodsSpider()\n \n     def test_basic(self):\n         r = Request(\"http://www.example.com\")\n@@ -96,18 +96,22 @@ class RequestSerializationTest(unittest.TestCase):\n     def test_private_reference_callback_serialization(self):\n         r = Request(\n             \"http://www.example.com\",\n-            callback=self.spider._TestSpider__parse_item_reference,\n-            errback=self.spider._TestSpider__handle_error_reference,\n+            callback=self.spider._MethodsSpider__parse_item_reference,\n+            errback=self.spider._MethodsSpider__handle_error_reference,\n         )\n         self._assert_serializes_ok(r, spider=self.spider)\n         request_dict = r.to_dict(spider=self.spider)\n-        self.assertEqual(request_dict[\"callback\"], \"_TestSpider__parse_item_reference\")\n-        self.assertEqual(request_dict[\"errback\"], \"_TestSpider__handle_error_reference\")\n+        self.assertEqual(\n+            request_dict[\"callback\"], \"_MethodsSpider__parse_item_reference\"\n+        )\n+        self.assertEqual(\n+            request_dict[\"errback\"], \"_MethodsSpider__handle_error_reference\"\n+        )\n \n     def test_private_callback_serialization(self):\n         r = Request(\n             \"http://www.example.com\",\n-            callback=self.spider._TestSpider__parse_item_private,\n+            callback=self.spider._MethodsSpider__parse_item_private,\n             errback=self.spider.handle_error,\n         )\n         self._assert_serializes_ok(r, spider=self.spider)\n@@ -115,7 +119,7 @@ class RequestSerializationTest(unittest.TestCase):\n     def test_mixin_private_callback_serialization(self):\n         r = Request(\n             \"http://www.example.com\",\n-            callback=self.spider._TestSpiderMixin__mixin_callback,\n+            callback=self.spider._SpiderMixin__mixin_callback,\n             errback=self.spider.handle_error,\n         )\n         self._assert_serializes_ok(r, spider=self.spider)\n@@ -152,18 +156,18 @@ class RequestSerializationTest(unittest.TestCase):\n \n     def test_callback_not_available(self):\n         \"\"\"Callback method is not available in the spider passed to from_dict\"\"\"\n-        spider = TestSpiderDelegation()\n+        spider = SpiderDelegation()\n         r = Request(\"http://www.example.com\", callback=spider.delegated_callback)\n         d = r.to_dict(spider=spider)\n         self.assertRaises(ValueError, request_from_dict, d, spider=Spider(\"foo\"))\n \n \n-class TestSpiderMixin:\n+class SpiderMixin:\n     def __mixin_callback(self, response):  # pylint: disable=unused-private-member\n         pass\n \n \n-class TestSpiderDelegation:\n+class SpiderDelegation:\n     def delegated_callback(self, response):\n         pass\n \n@@ -184,7 +188,7 @@ def private_handle_error(failure):\n     pass\n \n \n-class TestSpider(Spider, TestSpiderMixin):\n+class MethodsSpider(Spider, SpiderMixin):\n     name = \"test\"\n     parse_item_reference = parse_item\n     handle_error_reference = handle_error\n@@ -193,7 +197,7 @@ class TestSpider(Spider, TestSpiderMixin):\n \n     def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n-        self.delegated_callback = TestSpiderDelegation().delegated_callback\n+        self.delegated_callback = SpiderDelegation().delegated_callback\n \n     def parse_item(self, response):\n         pass\n\n@@ -51,8 +51,8 @@ class SimpleScheduler(MinimalScheduler):\n         return len(self.requests)\n \n \n-class TestSpider(Spider):\n-    name = \"test\"\n+class PathsSpider(Spider):\n+    name = \"paths\"\n \n     def __init__(self, mockserver, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -155,7 +155,7 @@ class MinimalSchedulerCrawlTest(TwistedTestCase):\n                 \"SCHEDULER\": self.scheduler_cls,\n             }\n             with LogCapture() as log:\n-                crawler = get_crawler(TestSpider, settings)\n+                crawler = get_crawler(PathsSpider, settings)\n                 yield crawler.crawl(mockserver)\n             for path in PATHS:\n                 self.assertIn(f\"{{'path': '{path}'}}\", str(log))\n\n@@ -1,4 +1,4 @@\n-from pytest import mark\n+import pytest\n from twisted.internet import defer\n from twisted.trial import unittest\n \n@@ -37,7 +37,7 @@ class AsyncSignalTestCase(unittest.TestCase):\n         item = await get_from_asyncio_queue(item)\n         self.items.append(item)\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_simple_pipeline(self):\n         crawler = get_crawler(ItemSpider)\n\n@@ -15,7 +15,7 @@ from scrapy.squeues import (\n )\n \n \n-class TestItem(Item):\n+class MyItem(Item):\n     name = Field()\n \n \n@@ -23,8 +23,8 @@ def _test_procesor(x):\n     return x + x\n \n \n-class TestLoader(ItemLoader):\n-    default_item_class = TestItem\n+class MyLoader(ItemLoader):\n+    default_item_class = MyItem\n     name_out = staticmethod(_test_procesor)\n \n \n@@ -80,19 +80,19 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n \n     def test_serialize_item(self):\n         q = self.queue()\n-        i = TestItem(name=\"foo\")\n+        i = MyItem(name=\"foo\")\n         q.push(i)\n         i2 = q.pop()\n-        assert isinstance(i2, TestItem)\n+        assert isinstance(i2, MyItem)\n         self.assertEqual(i, i2)\n \n     def test_serialize_loader(self):\n         q = self.queue()\n-        loader = TestLoader()\n+        loader = MyLoader()\n         q.push(loader)\n         loader2 = q.pop()\n-        assert isinstance(loader2, TestLoader)\n-        assert loader2.default_item_class is TestItem\n+        assert isinstance(loader2, MyLoader)\n+        assert loader2.default_item_class is MyItem\n         self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n \n     def test_serialize_request_recursive(self):\n@@ -161,19 +161,19 @@ class PickleLifoDiskQueueTest(t.LifoDiskQueueTest, LifoDiskQueueTestMixin):\n \n     def test_serialize_item(self):\n         q = self.queue()\n-        i = TestItem(name=\"foo\")\n+        i = MyItem(name=\"foo\")\n         q.push(i)\n         i2 = q.pop()\n-        assert isinstance(i2, TestItem)\n+        assert isinstance(i2, MyItem)\n         self.assertEqual(i, i2)\n \n     def test_serialize_loader(self):\n         q = self.queue()\n-        loader = TestLoader()\n+        loader = MyLoader()\n         q.push(loader)\n         loader2 = q.pop()\n-        assert isinstance(loader2, TestLoader)\n-        assert loader2.default_item_class is TestItem\n+        assert isinstance(loader2, MyLoader)\n+        assert loader2.default_item_class is MyItem\n         self.assertEqual(loader2.name_out(\"x\"), \"xx\")\n \n     def test_serialize_request_recursive(self):\n\n@@ -1,7 +1,7 @@\n import asyncio\n import warnings\n \n-from pytest import mark\n+import pytest\n from twisted.trial.unittest import TestCase\n \n from scrapy.utils.defer import deferred_f_from_coro_f\n@@ -12,7 +12,7 @@ from scrapy.utils.reactor import (\n )\n \n \n-@mark.usefixtures(\"reactor_pytest\")\n+@pytest.mark.usefixtures(\"reactor_pytest\")\n class AsyncioTest(TestCase):\n     def test_is_asyncio_reactor_installed(self):\n         # the result should depend only on the pytest --reactor argument\n@@ -30,7 +30,7 @@ class AsyncioTest(TestCase):\n \n         assert original_reactor == reactor\n \n-    @mark.only_asyncio()\n+    @pytest.mark.only_asyncio\n     @deferred_f_from_coro_f\n     async def test_set_asyncio_event_loop(self):\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n@@ -16,8 +16,6 @@ from scrapy.utils.datatypes import (\n )\n from scrapy.utils.python import garbage_collect\n \n-__doctests__ = [\"scrapy.utils.datatypes\"]\n-\n \n class CaseInsensitiveDictMixin:\n     def test_init_dict(self):\n\n@@ -1,6 +1,6 @@\n import random\n \n-from pytest import mark\n+import pytest\n from twisted.internet import defer, reactor\n from twisted.python.failure import Failure\n from twisted.trial import unittest\n@@ -150,7 +150,7 @@ class AsyncDefTestsuiteTest(unittest.TestCase):\n     async def test_deferred_f_from_coro_f_generator(self):\n         yield\n \n-    @mark.xfail(reason=\"Checks that the test is actually executed\", strict=True)\n+    @pytest.mark.xfail(reason=\"Checks that the test is actually executed\", strict=True)\n     @deferred_f_from_coro_f\n     async def test_deferred_f_from_coro_f_xfail(self):\n         raise RuntimeError(\"This is expected to be raised\")\n\n@@ -119,7 +119,7 @@ class StreamLoggerTest(unittest.TestCase):\n \n @pytest.mark.parametrize(\n     (\"base_extra\", \"log_extra\", \"expected_extra\"),\n-    (\n+    [\n         (\n             {\"spider\": \"test\"},\n             {\"extra\": {\"log_extra\": \"info\"}},\n@@ -135,7 +135,7 @@ class StreamLoggerTest(unittest.TestCase):\n             {\"extra\": {\"spider\": \"test2\"}},\n             {\"extra\": {\"spider\": \"test\"}},\n         ),\n-    ),\n+    ],\n )\n def test_spider_logger_adapter_process(\n     base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: dict\n\n@@ -17,8 +17,6 @@ from scrapy.utils.misc import (\n     walk_modules,\n )\n \n-__doctests__ = [\"scrapy.utils.misc\"]\n-\n \n class UtilsMiscTestCase(unittest.TestCase):\n     def test_load_object_class(self):\n\n@@ -20,8 +20,6 @@ from scrapy.utils.python import (\n     without_none_values,\n )\n \n-__doctests__ = [\"scrapy.utils.python\"]\n-\n \n class MutableChainTest(unittest.TestCase):\n     def test_mutablechain(self):\n\n@@ -15,8 +15,6 @@ from scrapy.utils.response import (\n     response_status_message,\n )\n \n-__doctests__ = [\"scrapy.utils.response\"]\n-\n \n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url=\"http://example.org/\", body=b\"dummy_response\")\n@@ -207,8 +205,8 @@ class ResponseUtilsTest(unittest.TestCase):\n \n \n @pytest.mark.parametrize(\n-    \"input_body,output_body\",\n-    (\n+    (\"input_body\", \"output_body\"),\n+    [\n         (\n             b\"a<!--\",\n             b\"a\",\n@@ -237,7 +235,7 @@ class ResponseUtilsTest(unittest.TestCase):\n             b\"a<!--b--><!--c-->d\",\n             b\"ad\",\n         ),\n-    ),\n+    ],\n )\n def test_remove_html_comments(input_body, output_body):\n     assert _remove_html_comments(input_body) == output_body, (\n\n@@ -1,7 +1,7 @@\n import asyncio\n \n+import pytest\n from pydispatch import dispatcher\n-from pytest import mark\n from testfixtures import LogCapture\n from twisted.internet import defer, reactor\n from twisted.python.failure import Failure\n@@ -67,7 +67,7 @@ class SendCatchLogDeferredTest2(SendCatchLogDeferredTest):\n         return d\n \n \n-@mark.usefixtures(\"reactor_pytest\")\n+@pytest.mark.usefixtures(\"reactor_pytest\")\n class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n@@ -76,7 +76,7 @@ class SendCatchLogDeferredAsyncDefTest(SendCatchLogDeferredTest):\n         return \"OK\"\n \n \n-@mark.only_asyncio()\n+@pytest.mark.only_asyncio\n class SendCatchLogDeferredAsyncioTest(SendCatchLogDeferredTest):\n     async def ok_handler(self, arg, handlers_called):\n         handlers_called.add(self.ok_handler)\n\n@@ -5,8 +5,6 @@ from tempfile import mkdtemp\n \n from scrapy.utils.template import render_templatefile\n \n-__doctests__ = [\"scrapy.utils.template\"]\n-\n \n class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n     def setUp(self):\n\n@@ -17,8 +17,6 @@ from scrapy.utils.url import (  # type: ignore[attr-defined]\n     url_is_from_spider,\n )\n \n-__doctests__ = [\"scrapy.utils.url\"]\n-\n \n class UrlUtilsTest(unittest.TestCase):\n     def test_url_is_from_any_domain(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
