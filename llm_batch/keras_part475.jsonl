{"custom_id": "keras#235068530b193c0786e9a038223f341d7008058f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 868 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -306,7 +306,7 @@ class TestConvUtils(tf.test.TestCase, parameterized.TestCase):\n             ),\n         )\n \n-    def DISABLED_test_conv_kernel_mask_rect_kernel(self, *input_shape):\n+    def test_conv_kernel_mask_rect_kernel(self, *input_shape):\n         padding = \"valid\"\n         ndims = len(input_shape)\n         strides = (1,) * ndims\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e486fa4da0e615114adf2d6547d6f47c26fdf80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 31 | Churn Cumulative: 9233 | Contributors (this commit): 108 | Commits (past 90d): 22 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -639,22 +639,23 @@ class Callback:\n     2. You will need to manually call all the `on_*` methods at the appropriate\n        locations in your loop. Like this:\n \n-       ```\n-       callbacks =  tf.keras.callbacks.CallbackList([...])\n-       callbacks.append(...)\n+    Example:\n+    \n+    >>> callbacks =  tf.keras.callbacks.CallbackList([...])\n+    >>> callbacks.append(...)\n+\n+    >>> callbacks.on_train_begin(...)\n+    >>> for epoch in range(EPOCHS):\n+    ...   callbacks.on_epoch_begin(epoch)\n+    ...   for i, data in dataset.enumerate():\n+    ...     callbacks.on_train_batch_begin(i)\n+    ...     batch_logs = model.train_step(data)\n+    ...     callbacks.on_train_batch_end(i, batch_logs)\n+    ...   epoch_logs = ...\n+    ...   callbacks.on_epoch_end(epoch, epoch_logs)\n+    >>> final_logs=...\n+    >>> callbacks.on_train_end(final_logs)\n     \n-       callbacks.on_train_begin(...)\n-       for epoch in range(EPOCHS):\n-         callbacks.on_epoch_begin(epoch)\n-         for i, data in dataset.enumerate():\n-           callbacks.on_train_batch_begin(i)\n-           batch_logs = model.train_step(data)\n-           callbacks.on_train_batch_end(i, batch_logs)\n-         epoch_logs = ...\n-         callbacks.on_epoch_end(epoch, epoch_logs)\n-       final_logs=...\n-       callbacks.on_train_end(final_logs)\n-       ```\n \n     Attributes:\n         params: Dict. Training parameters\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8462bc2cba2b19e84a5bbd0ec0bba18750fa84fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1128 | Contributors (this commit): 11 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -331,7 +331,7 @@ def Input(\n             ragged. Only one of 'ragged' and 'sparse' can be True. In this case,\n             values of 'None' in the 'shape' argument represent ragged\n             dimensions.  For more information about RaggedTensors, see\n-            [this guide](https://www.tensorflow.org/guide/ragged_tensors).\n+            [this guide](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).\n         type_spec: A `tf.TypeSpec` object to create the input placeholder from.\n             When provided, all other args except name must be None.\n         **kwargs: deprecated arguments support. Supports `batch_shape` and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#74d92358ce4522f078b9559f4b029986db4f0ebc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 9262 | Contributors (this commit): 108 | Commits (past 90d): 23 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -641,20 +641,21 @@ class Callback:\n \n     Example:\n     \n-    >>> callbacks =  tf.keras.callbacks.CallbackList([...])\n-    >>> callbacks.append(...)\n-\n-    >>> callbacks.on_train_begin(...)\n-    >>> for epoch in range(EPOCHS):\n-    ...   callbacks.on_epoch_begin(epoch)\n-    ...   for i, data in dataset.enumerate():\n-    ...     callbacks.on_train_batch_begin(i)\n-    ...     batch_logs = model.train_step(data)\n-    ...     callbacks.on_train_batch_end(i, batch_logs)\n-    ...   epoch_logs = ...\n-    ...   callbacks.on_epoch_end(epoch, epoch_logs)\n-    >>> final_logs=...\n-    >>> callbacks.on_train_end(final_logs)\n+    ```python\n+       callbacks =  tf.keras.callbacks.CallbackList([...])\n+       callbacks.append(...)\n+       callbacks.on_train_begin(...)\n+       for epoch in range(EPOCHS):\n+         callbacks.on_epoch_begin(epoch)\n+         for i, data in dataset.enumerate():\n+           callbacks.on_train_batch_begin(i)\n+           batch_logs = model.train_step(data)\n+           callbacks.on_train_batch_end(i, batch_logs)\n+         epoch_logs = ...\n+         callbacks.on_epoch_end(epoch, epoch_logs)\n+       final_logs=...\n+       callbacks.on_train_end(final_logs)\n+    ```\n     \n \n     Attributes:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0a55837cb002a1856bba361a44c6dfc566da76c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 5 | Files Changed: 9 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 46 | Churn Cumulative: 19280 | Contributors (this commit): 34 | Commits (past 90d): 70 | Contributors (cumulative): 76 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3641,7 +3641,9 @@ class BaseRandomLayer(Layer):\n         self._random_generator = backend.RandomGenerator(\n             seed, force_generator=force_generator, rng_type=rng_type\n         )\n-        # Eagerly init the generator to avoid any issue like b/206821407\n+\n+    def build(self, input_shape):\n+        super().build(input_shape)\n         self._random_generator._maybe_init()\n \n     def _trackable_children(self, save_type=\"checkpoint\", **kwargs):\n\n@@ -89,9 +89,6 @@ class Dropout(base_layer.BaseRandomLayer):\n         self.seed = seed\n         self.supports_masking = True\n \n-    def build(self, input_shape):\n-        self._random_generator._maybe_init()\n-\n     def _get_noise_shape(self, inputs):\n         # Subclasses of `Dropout` may implement `_get_noise_shape(self,\n         # inputs)`, which will override `self.noise_shape`, and allows for\n\n@@ -94,6 +94,18 @@ class DropoutTest(test_combinations.TestCase):\n         for name in checkpoint_var_names:\n             self.assertNotIn(\"dropout\", name)\n \n+    @test_utils.run_v2_only\n+    def test_state_variable_name(self):\n+        inputs = keras.Input(shape=(5, 10))\n+        layer = keras.layers.Dropout(\n+            0.5, force_generator=True, name=\"dropout_layer\"\n+        )\n+        layer(inputs)\n+        self.assertEqual(\n+            layer._random_generator._generator._state_var.name,\n+            \"dropout_layer/StateVar:0\",\n+        )\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -155,7 +155,7 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         self.state_size = (self.filters, self.filters)\n \n     def build(self, input_shape):\n-\n+        super().build(input_shape)\n         if self.data_format == \"channels_first\":\n             channel_axis = 1\n         else:\n\n@@ -182,6 +182,7 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n \n     @tf_utils.shape_type_conversion\n     def build(self, input_shape):\n+        super().build(input_shape)\n         input_dim = input_shape[-1]\n         default_caching_device = rnn_utils.caching_device(self)\n         self.kernel = self.add_weight(\n\n@@ -817,6 +817,17 @@ class GRULayerTest(test_combinations.TestCase):\n         # recurrent_dropout.\n         self.assertEqual(layer.implementation, 1)\n \n+    @test_utils.run_v2_only\n+    def test_dropout_variable_name(self):\n+        layer = keras.layers.RNN(\n+            keras.layers.GRUCell(2, dropout=0.1, force_generator=True)\n+        )\n+        layer(np.random.random((2, 3, 4)))\n+        self.assertEqual(\n+            layer.cell._random_generator._generator._state_var.name,\n+            \"rnn/gru_cell/StateVar:0\",\n+        )\n+\n     @parameterized.parameters([0, 1, 2])\n     def test_implementation_mode_gru(self, implementation_mode):\n         num_samples = 2\n\n@@ -185,6 +185,7 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n \n     @tf_utils.shape_type_conversion\n     def build(self, input_shape):\n+        super().build(input_shape)\n         default_caching_device = rnn_utils.caching_device(self)\n         input_dim = input_shape[-1]\n         self.kernel = self.add_weight(\n\n@@ -1009,6 +1009,17 @@ class LSTMLayerTest(test_combinations.TestCase):\n         # recurrent_dropout.\n         self.assertEqual(layer.implementation, 1)\n \n+    @test_utils.run_v2_only\n+    def test_dropout_variable_name(self):\n+        layer = keras.layers.RNN(\n+            keras.layers.LSTMCell(2, dropout=0.1, force_generator=True)\n+        )\n+        layer(np.random.random((2, 3, 4)))\n+        self.assertEqual(\n+            layer.cell._random_generator._generator._state_var.name,\n+            \"rnn/lstm_cell/StateVar:0\",\n+        )\n+\n     @parameterized.parameters([0, 1, 2])\n     def test_implementation_mode_LSTM(self, implementation_mode):\n         num_samples = 2\n\n@@ -159,6 +159,7 @@ class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n \n     @tf_utils.shape_type_conversion\n     def build(self, input_shape):\n+        super().build(input_shape)\n         default_caching_device = rnn_utils.caching_device(self)\n         self.kernel = self.add_weight(\n             shape=(input_shape[-1], self.units),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db9f76ac8d1945630061582b03381939349bb59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 233 | Lines Deleted: 70 | Files Changed: 9 | Hunks: 30 | Methods Changed: 20 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 303 | Churn Cumulative: 34377 | Contributors (this commit): 44 | Commits (past 90d): 94 | Contributors (cumulative): 113 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5618,7 +5618,7 @@ def sparse_categorical_crossentropy(\n         output = tf.reshape(output, [-1, output_shape[-1]])\n \n     if ignore_class is not None:\n-        valid_mask = tf.not_equal(target, ignore_class)\n+        valid_mask = tf.not_equal(target, cast(ignore_class, target.dtype))\n         target = target[valid_mask]\n         output = output[valid_mask]\n \n@@ -5635,19 +5635,8 @@ def sparse_categorical_crossentropy(\n     if ignore_class is not None:\n         res_shape = cast(output_shape[:-1], \"int64\")\n         valid_mask = tf.reshape(valid_mask, res_shape)\n-\n         res = tf.scatter_nd(tf.where(valid_mask), res, res_shape)\n-\n-        if output_rank is not None and output_rank >= 3:\n-            # The output is a 2-dimensional (or higher) label map,\n-            # and some pixels might be zero. We reduce the loss among the\n-            # valid entries to prevent an artificial decrease of the loss\n-            # value when many of them are invalid.\n-            reduce_axis = list(range(1, output_rank - 1))\n-            res = tf.math.divide_no_nan(\n-                tf.reduce_sum(res, axis=reduce_axis),\n-                tf.reduce_sum(cast(valid_mask, res.dtype), axis=reduce_axis),\n-            )\n+        res._keras_mask = valid_mask\n \n         return res\n \n\n@@ -28,6 +28,7 @@ from keras.engine import input_layer\n from keras.layers import activation\n from keras.layers.normalization import batch_normalization_v1\n from keras.testing_infra import test_combinations\n+from keras.utils import losses_utils\n from keras.utils import tf_inspect\n from keras.utils import tf_utils\n \n@@ -1973,7 +1974,7 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n     def test_sparse_categorical_crossentropy_loss_with_ignore_class(self):\n-        t = backend.constant([255, 1, 2, 2])\n+        tests = (([255, 1, 2, 2], 255), ([-1, 1, 2, 2], -1))\n         p = backend.softmax(\n             backend.constant(\n                 [\n@@ -1984,24 +1985,11 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n                 ]\n             )\n         )\n-        result = backend.sparse_categorical_crossentropy(t, p, ignore_class=255)\n-        self.assertArrayNear(\n-            self.evaluate(result),\n-            [0.0, 0.07428224, 0.13980183, 0.11967831],\n-            1e-3,\n-        )\n \n-        t = backend.constant([-1, 1, 2, 2])\n-        p = backend.constant(\n-            [\n-                [1.8, 1.2, 0.5],\n-                [0.2, 3.8, 0.8],\n-                [1.1, 0.4, 3.4],\n-                [1.3, 0.7, 3.8],\n-            ]\n-        )\n+        for t, ignore_class in tests:\n+            t = backend.constant(t)\n             result = backend.sparse_categorical_crossentropy(\n-            t, p, ignore_class=-1, from_logits=True\n+                t, p, ignore_class=ignore_class\n             )\n             self.assertArrayNear(\n                 self.evaluate(result),\n@@ -2014,10 +2002,7 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     )\n     def test_sparse_cce_loss_with_ignore_class_for_segmentation(self):\n         t = backend.constant(\n-            [\n-                [[0, 2], [-1, -1]],\n-                [[0, 2], [-1, -1]],\n-            ]\n+            [[[0, 2], [-1, -1]], [[0, 2], [-1, -1]], [[0, 0], [0, 0]]]\n         )\n         p = backend.constant(\n             [\n@@ -2029,14 +2014,44 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n                     [[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n                     [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]],\n                 ],\n+                [\n+                    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],\n+                    [[0.1, 0.9, 0.0], [0.2, 0.8, 0.0]],\n+                ],\n+            ]\n+        )\n+\n+        expected_result = [\n+            [[0.0, 0.0], [0.0, 0.0]],\n+            [[0.0, 0.693148], [0.0, 0.0]],\n+            [[0.0, 0.0], [2.302585, 1.609438]],\n+        ]\n+\n+        # total_entries = 12\n+        # valid_entries = 8\n+        expected_mask = backend.constant(\n+            [\n+                [[True, True], [False, False]],\n+                [[True, True], [False, False]],\n+                [[True, True], [True, True]],\n             ]\n         )\n \n         result = backend.sparse_categorical_crossentropy(t, p, ignore_class=-1)\n-        self.assertArrayNear(\n-            self.evaluate(result), [2.3841855e-07, 3.4657377e-01], 1e-3\n+        mask = losses_utils.get_mask(result)\n+\n+        self.assertIsNotNone(\n+            mask,\n+            \"expected sparse_categorical_crossentropy to set the \"\n+            \"`_keras_mask` attribute when `ignore_class is not None`, \"\n+            \"which indicates which loss values are valid.\",\n         )\n \n+        result = self.evaluate(result)\n+        mask = self.evaluate(mask)\n+        self.assertAllEqual(mask, expected_mask)\n+        self.assertAllClose(result, expected_result, atol=1e-6)\n+\n     @test_combinations.generate(test_combinations.combine(mode=[\"graph\"]))\n     def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(\n         self,\n\n@@ -261,7 +261,7 @@ class LossesContainer(Container):\n                 continue\n \n             y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n-            sw = apply_mask(y_p, sw, get_mask(y_p))\n+            sw = losses_utils.apply_mask(y_p, sw, losses_utils.get_mask(y_p))\n             loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n \n             total_loss_mean_value = loss_value\n@@ -596,8 +596,8 @@ class MetricsContainer(Container):\n                 continue\n \n             y_t, y_p, sw = match_dtype_and_rank(y_t, y_p, sw)\n-            mask = get_mask(y_p)\n-            sw = apply_mask(y_p, sw, mask)\n+            mask = losses_utils.get_mask(y_p)\n+            sw = losses_utils.apply_mask(y_p, sw, mask)\n \n             for metric_obj in metric_objs:\n                 if metric_obj is None:\n@@ -847,25 +847,6 @@ def match_dtype_and_rank(y_t, y_p, sw):\n     return y_t, y_p, sw\n \n \n-def get_mask(y_p):\n-    \"\"\"Returns Keras mask from tensor.\"\"\"\n-    return getattr(y_p, \"_keras_mask\", None)\n-\n-\n-def apply_mask(y_p, sw, mask):\n-    \"\"\"Applies any mask on predictions to sample weights.\"\"\"\n-    if mask is not None:\n-        mask = tf.cast(mask, y_p.dtype)\n-        if sw is not None:\n-            mask, _, sw = losses_utils.squeeze_or_expand_dimensions(\n-                mask, sample_weight=sw\n-            )\n-            sw *= mask\n-        else:\n-            sw = mask\n-    return sw\n-\n-\n def get_custom_object_name(obj):\n     \"\"\"Returns the name to use for a custom loss or metric callable.\n \n\n@@ -150,8 +150,13 @@ class Loss:\n                     self.call, tf.__internal__.autograph.control_status_ctx()\n                 )\n             losses = call_fn(y_true, y_pred)\n+            mask = losses_utils.get_mask(losses)\n+            reduction = self._get_reduction()\n+            sample_weight = losses_utils.apply_valid_mask(\n+                losses, sample_weight, mask, reduction\n+            )\n             return losses_utils.compute_weighted_loss(\n-                losses, sample_weight, reduction=self._get_reduction()\n+                losses, sample_weight, reduction=reduction\n             )\n \n     @classmethod\n@@ -977,6 +982,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     def __init__(\n         self,\n         from_logits=False,\n+        ignore_class=None,\n         reduction=losses_utils.ReductionV2.AUTO,\n         name=\"sparse_categorical_crossentropy\",\n     ):\n@@ -1003,6 +1009,7 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n             name=name,\n             reduction=reduction,\n             from_logits=from_logits,\n+            ignore_class=ignore_class,\n         )\n \n \n\n@@ -170,12 +170,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n         logits = backend.variable(np.random.random((5, 1)))\n         softmax_output = backend.softmax(logits)\n \n-        valid_entries = tf.reshape(\n-            tf.constant([0, 1, 0, 1, 1], target.dtype), (5, 1)\n-        )\n-        target.assign(\n-            target * valid_entries + (1 - valid_entries) * ignore_class\n-        )\n+        _valid = tf.constant([[0], [1], [0], [1], [1]], target.dtype)\n+        target.assign(target * _valid + (1 - _valid) * ignore_class)\n \n         output_from_logit = losses.sparse_categorical_crossentropy(\n             target, logits, ignore_class=ignore_class, from_logits=True\n@@ -183,6 +179,12 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n         output_from_softmax = losses.sparse_categorical_crossentropy(\n             target, softmax_output, ignore_class=ignore_class\n         )\n+\n+        # expected_mask = [False, True, False, True, True]\n+        # for o in (output_from_logit, output_from_softmax):\n+        #     mask = backend.eval(losses_utils.get_mask(o))\n+        #     np.testing.assert_array_equal(mask, expected_mask)\n+\n         np.testing.assert_allclose(\n             backend.eval(output_from_logit),\n             backend.eval(output_from_softmax),\n@@ -1838,6 +1840,70 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n         loss = cce_obj(y_true, logits)\n         self.assertAlmostEqual(self.evaluate(loss), 0.0573, 3)\n \n+    def test_unweighted_ignore_class(self):\n+        cce_obj = losses.SparseCategoricalCrossentropy(ignore_class=-1)\n+        y_true = tf.constant([0, 1, 2, -1])\n+        y_pred = tf.constant(\n+            [\n+                [0.9, 0.05, 0.05],\n+                [0.5, 0.89, 0.6],\n+                [0.05, 0.01, 0.94],\n+                [0.85, 0.14, 0.01],\n+            ],\n+            dtype=tf.float32,\n+        )\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.3239, 3)\n+\n+        # Test with logits.\n+        logits = tf.constant(\n+            [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0], [7.8, 2.0, 1.0]]\n+        )\n+        cce_obj = losses.SparseCategoricalCrossentropy(\n+            ignore_class=-1, from_logits=True\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.0573, 3)\n+\n+    def test_unweighted_ignore_class_for_segmentation(self):\n+        cce_obj = losses.SparseCategoricalCrossentropy(ignore_class=-1)\n+        y_true = tf.constant(\n+            [[[0, 2], [-1, -1]], [[0, 2], [-1, -1]], [[0, 0], [0, 0]]]\n+        )\n+        y_pred = tf.constant(\n+            [\n+                [\n+                    [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n+                    [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]],\n+                ],\n+                [\n+                    [[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n+                    [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]],\n+                ],\n+                [\n+                    [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0]],\n+                    [[0.1, 0.9, 0.0], [0.2, 0.8, 0.0]],\n+                ],\n+            ],\n+            dtype=tf.float32,\n+        )\n+\n+        # Expected loss values:\n+        # [[0.0, 0.0], [0.0, 0.0]],\n+        # [[0.0, 0.693148], [0.0, 0.0]],\n+        # [[0.0, 0.0], [2.302585, 1.609438]],\n+\n+        loss = cce_obj(y_true, y_pred)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.575646375, 3)\n+\n+        # # Test with logits.\n+        # logits = tf.constant(\n+        #     [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0]]\n+        # )\n+        # cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n+        # loss = cce_obj(y_true, logits)\n+        # self.assertAlmostEqual(self.evaluate(loss), 0.0573, 3)\n+\n     def test_scalar_weighted(self):\n         cce_obj = losses.SparseCategoricalCrossentropy()\n         y_true = tf.constant([[0], [1], [2]])\n@@ -1875,6 +1941,32 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n         loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n         self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)\n \n+    def test_sample_weighted_ignore_class(self):\n+        cce_obj = losses.SparseCategoricalCrossentropy(ignore_class=-1)\n+        y_true = tf.constant([[0], [1], [2], [-1]])\n+        y_pred = tf.constant(\n+            [\n+                [0.9, 0.05, 0.05],\n+                [0.5, 0.89, 0.6],\n+                [0.05, 0.01, 0.94],\n+                [0.85, 0.14, 0.01],\n+            ],\n+            dtype=tf.float32,\n+        )\n+        sample_weight = tf.constant([[1.2], [3.4], [5.6], [10.4]], shape=(4, 1))\n+        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 1.0696, 3)\n+\n+        # Test with logits.\n+        logits = tf.constant(\n+            [[8.0, 1.0, 1.0], [0.0, 9.0, 1.0], [2.0, 3.0, 5.0], [7.8, 2.0, 1.0]]\n+        )\n+        cce_obj = losses.SparseCategoricalCrossentropy(\n+            ignore_class=-1, from_logits=True\n+        )\n+        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAlmostEqual(self.evaluate(loss), 0.31829, 3)\n+\n     def test_no_reduction(self):\n         y_true = tf.constant([[0], [1], [2]])\n         logits = tf.constant(\n\n@@ -698,6 +698,10 @@ class MeanMetricWrapper(Mean):\n             self._fn, tf.__internal__.autograph.control_status_ctx()\n         )\n         matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n+        mask = losses_utils.get_mask(matches)\n+        sample_weight = losses_utils.apply_valid_mask(\n+            matches, sample_weight, mask, self.reduction\n+        )\n         return super().update_state(matches, sample_weight=sample_weight)\n \n     def get_config(self):\n@@ -915,6 +919,10 @@ class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n             self._fn, tf.__internal__.autograph.control_status_ctx()\n         )\n         matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n+        mask = losses_utils.get_mask(matches)\n+        sample_weight = losses_utils.apply_valid_mask(\n+            matches, sample_weight, mask, self.reduction\n+        )\n         return super().update_state(matches, sample_weight=sample_weight)\n \n     def get_config(self):\n\n@@ -2713,16 +2713,19 @@ class _IoUBase(base_metric.Metric):\n         if y_true.shape.ndims > 1:\n             y_true = tf.reshape(y_true, [-1])\n \n-        if self.ignore_class is not None:\n-            valid_mask = tf.not_equal(y_true, self.ignore_class)\n-            y_true = y_true[valid_mask]\n-            y_pred = y_pred[valid_mask]\n-\n         if sample_weight is not None:\n             sample_weight = tf.cast(sample_weight, self._dtype)\n             if sample_weight.shape.ndims > 1:\n                 sample_weight = tf.reshape(sample_weight, [-1])\n \n+        if self.ignore_class is not None:\n+            ignore_class = tf.cast(self.ignore_class, y_true.dtype)\n+            valid_mask = tf.not_equal(y_true, ignore_class)\n+            y_true = y_true[valid_mask]\n+            y_pred = y_pred[valid_mask]\n+            if sample_weight is not None:\n+                sample_weight = sample_weight[valid_mask]\n+\n         # Accumulate the prediction to current confusion matrix.\n         current_cm = tf.math.confusion_matrix(\n             y_true,\n\n@@ -1794,6 +1794,16 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n \n         self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n \n+    def test_unweighted_ignore_class(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(ignore_class=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([-1, 2])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        result = scce_obj(y_true, y_pred)\n+\n+        self.assertAllClose(self.evaluate(result), 2.3026, atol=1e-3)\n+\n     def test_unweighted_from_logits(self):\n         scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n         self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n@@ -1848,6 +1858,17 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n \n         self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n \n+    def test_weighted_ignore_class(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(ignore_class=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2, -1])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.1, 0.8, 0.1]])\n+        sample_weight = tf.constant([1.5, 2.0, 1.5])\n+        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n+\n     def test_weighted_from_logits(self):\n         scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n         self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n\n@@ -393,3 +393,50 @@ def cast_losses_to_common_dtype(losses):\n     if highest_float:\n         losses = [tf.cast(loss, highest_float) for loss in losses]\n     return losses\n+\n+\n+def get_mask(y_p):\n+    \"\"\"Returns Keras mask from tensor.\"\"\"\n+    return getattr(y_p, \"_keras_mask\", None)\n+\n+\n+def apply_mask(y_p, sw, mask):\n+    \"\"\"Applies any mask on predictions to sample weights.\"\"\"\n+    if mask is not None:\n+        mask = tf.cast(mask, y_p.dtype)\n+        if sw is not None:\n+            mask, _, sw = squeeze_or_expand_dimensions(mask, sample_weight=sw)\n+            sw *= mask\n+        else:\n+            sw = mask\n+    return sw\n+\n+\n+def apply_valid_mask(losses, sw, mask, reduction):\n+    \"\"\"Redistribute pair-wise weights considering only valid entries.\"\"\"\n+    if mask is not None:\n+        mask = tf.cast(mask, losses.dtype)\n+\n+        if reduction in (ReductionV2.AUTO, ReductionV2.SUM_OVER_BATCH_SIZE):\n+            # Valid entries have weight `# total / # valid`,\n+            # while invalid ones assume weight 0. When summed\n+            # over batch size, they will be reduced to:\n+            #\n+            # mean(loss * sample_weight * total / valid)\n+            #   = sum(loss * sample_weight * total / valid) / total\n+            #   = sum(loss * sample_weight) / total * total / valid\n+            #   = sum(loss * sample_weight) / valid\n+\n+            total = tf.cast(tf.size(mask), losses.dtype)\n+            valid = tf.reduce_sum(mask)\n+            mask *= total / valid\n+        elif reduction in (ReductionV2.NONE, ReductionV2.SUM):\n+            # Nothing to do. Nothing is being averaged.\n+            ...\n+        elif reduction == \"weighted_mean\":\n+            # Nothing to do. A binary mask is enough because\n+            # it will also be used in the mean operation's\n+            # denominator as `tf.reduce_sum(sample_weight)`.\n+            ...\n+\n+    return apply_mask(losses, sw, mask)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0714377683fe5c5c22d887c074a7e0fc85bac97e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1130 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -331,7 +331,7 @@ def Input(\n             ragged. Only one of 'ragged' and 'sparse' can be True. In this case,\n             values of 'None' in the 'shape' argument represent ragged\n             dimensions.  For more information about RaggedTensors, see\n-            [this guide](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).\n+            [this guide](https://www.tensorflow.org/guide/ragged_tensor).\n         type_spec: A `tf.TypeSpec` object to create the input placeholder from.\n             When provided, all other args except name must be None.\n         **kwargs: deprecated arguments support. Supports `batch_shape` and\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a003256bdc63c13db6840058b3566c0b3bf90f9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 132 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -110,7 +110,7 @@ def timeseries_dataset_from_array(\n     ```python\n     input_data = data[:-10]\n     targets = data[10:]\n-    dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n+    dataset = tf.keras.utils.timeseries_dataset_from_array(\n         input_data, targets, sequence_length=10)\n     for batch in dataset:\n       inputs, targets = batch\n@@ -133,9 +133,9 @@ def timeseries_dataset_from_array(\n     Y = X*2\n \n     sample_length = 20\n-    input_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n+    input_dataset = tf.keras.utils.timeseries_dataset_from_array(\n       X, None, sequence_length=sample_length, sequence_stride=sample_length)\n-    target_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n+    target_dataset = tf.keras.utils.timeseries_dataset_from_array(\n       Y, None, sequence_length=sample_length, sequence_stride=sample_length)\n \n     for batch in zip(input_dataset, target_dataset):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f8b4adb753f52107a409a7e05fff704ff3c7e92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 1942 | Contributors (this commit): 31 | Commits (past 90d): 17 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -272,7 +272,12 @@ def model_to_dot(\n         if show_shapes:\n \n             def format_shape(shape):\n-                return str(shape).replace(str(None), \"None\").replace(\"{\", \"/{\").replace(\"}\", \"/}\")\n+                return (\n+                    str(shape)\n+                    .replace(str(None), \"None\")\n+                    .replace(\"{\", \"/{\")\n+                    .replace(\"}\", \"/}\")\n+                )\n \n             try:\n                 outputlabels = format_shape(layer.output_shape)\n\n@@ -248,9 +248,10 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n             def call(self, inputs) -> tf.Tensor:\n                 tensor_input, dict_input = inputs\n                 return tf.concat(list(dict_input.values()), axis=1)\n+\n         inputs = {\n             \"a\": keras.Input(name=\"a\", shape=(1), dtype=tf.float32),\n-            \"b\": keras.Input(name=\"b\", shape=(1), dtype=tf.float32)\n+            \"b\": keras.Input(name=\"b\", shape=(1), dtype=tf.float32),\n         }\n         outputs = DictLayer()((inputs[\"a\"], inputs))\n         model = keras.Model(\n@@ -258,10 +259,9 @@ class ModelToDotFormatTest(tf.test.TestCase, parameterized.TestCase):\n             outputs=outputs,\n         )\n         try:\n-            vis_utils.plot_model(model,\n-                                 show_shapes=True,\n-                                 show_dtype=True,\n-                                 show_layer_names=True)\n+            vis_utils.plot_model(\n+                model, show_shapes=True, show_dtype=True, show_layer_names=True\n+            )\n         except ImportError:\n             pass\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f2f5331d00a32d97ea4f4e5306c8414d16e1f23d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 40 | Files Changed: 6 | Hunks: 21 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 95 | Churn Cumulative: 2890 | Contributors (this commit): 8 | Commits (past 90d): 33 | Contributors (cumulative): 30 | DMM Complexity: 0.8\n\nDIFF:\n@@ -48,10 +48,6 @@ class AdditiveAttention(BaseDenseAttention):\n     Args:\n       use_scale: If `True`, will create a variable to scale the attention\n         scores.\n-      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask\n-        such that position `i` cannot attend to positions `j > i`. This prevents\n-        the flow of information from the future towards the past.  Defaults to\n-        `False`.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         attention scores. Defaults to 0.0.\n \n@@ -74,6 +70,10 @@ class AdditiveAttention(BaseDenseAttention):\n         training mode (adding dropout) or in inference mode (no dropout).\n       return_attention_scores: bool, it `True`, returns the attention scores\n         (after masking and softmax) as an additional output argument.\n+      use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds a\n+        mask such that position `i` cannot attend to positions `j > i`. This\n+        prevents the flow of information from the future towards the past.\n+        Defaults to `False`.`\n \n     Output:\n \n\n@@ -329,8 +329,8 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n             q = tf.cast(tf.random.uniform((2, 3, 4), seed=1), \"float16\")\n             v = tf.cast(tf.random.uniform((2, 3, 4), seed=2), \"float16\")\n             k = tf.cast(tf.random.uniform((2, 3, 4), seed=3), \"float16\")\n-            layer = keras.layers.AdditiveAttention(causal=True)\n-            _ = layer([q, v, k])\n+            layer = keras.layers.AdditiveAttention()\n+            _ = layer([q, v, k], use_causal_mask=True)\n \n \n if __name__ == \"__main__\":\n\n@@ -46,10 +46,6 @@ class Attention(BaseDenseAttention):\n     Args:\n       use_scale: If `True`, will create a scalar variable to scale the attention\n         scores.\n-      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask\n-        such that position `i` cannot attend to positions `j > i`. This prevents\n-        the flow of information from the future towards the past.  Defaults to\n-        `False`.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         attention scores. Defaults to 0.0.\n       score_mode: Function to use to compute attention scores, one of\n@@ -76,6 +72,10 @@ class Attention(BaseDenseAttention):\n         (after masking and softmax) as an additional output argument.\n       training: Python boolean indicating whether the layer should behave in\n         training mode (adding dropout) or in inference mode (no dropout).\n+      use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds a\n+        mask such that position `i` cannot attend to positions `j > i`. This\n+        prevents the flow of information from the future towards the past.\n+        Defaults to `False`.\n \n     Output:\n \n\n@@ -433,14 +433,18 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     def test_self_attention_causal(self, return_attention_scores):\n         # Query-value tensor of shape [1, 3, 1]\n         q = np.array([[[0.5], [0.8], [-0.3]]], dtype=np.float32)\n-        attention_layer = keras.layers.Attention(causal=True)\n+        attention_layer = keras.layers.Attention()\n         if return_attention_scores:\n             actual, actual_scores = attention_layer(\n-                [q, q], return_attention_scores=return_attention_scores\n+                [q, q],\n+                return_attention_scores=return_attention_scores,\n+                use_causal_mask=True,\n             )\n         else:\n             actual = attention_layer(\n-                [q, q], return_attention_scores=return_attention_scores\n+                [q, q],\n+                return_attention_scores=return_attention_scores,\n+                use_causal_mask=True,\n             )\n \n         # Expected scores of shape [1, 3, 3]\n@@ -480,6 +484,21 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         self.assertAllClose(expected, actual)\n \n+    def test_self_attention_causal_deprecated(self):\n+        \"\"\"Verify deprecated specification of causal masking still works.\"\"\"\n+        # Query-value tensor of shape [1, 3, 1]\n+        q = np.array([[[0.5], [0.8], [-0.3]]], dtype=np.float32)\n+        attention_layer_new = keras.layers.Attention()\n+        new_scores = attention_layer_new(\n+            [q, q],\n+            use_causal_mask=True,\n+        )\n+        attention_layer_old = keras.layers.Attention(causal=True)\n+        old_scores = attention_layer_old(\n+            [q, q],\n+        )\n+        self.assertAllClose(new_scores, old_scores)\n+\n     def test_inputs_not_list(self):\n         attention_layer = keras.layers.Attention()\n         q = np.array([[[1.1]]], dtype=np.float32)\n\n@@ -18,8 +18,8 @@ This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n \n-\n import tensorflow.compat.v2 as tf\n+from absl import logging\n \n from keras import backend\n from keras.engine import base_layer\n@@ -35,26 +35,21 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n     reuse the `apply_attention_scores()` method.\n \n     Args:\n-      causal: Boolean. Set to `True` for decoder self-attention. Adds a mask\n-        such that position `i` cannot attend to positions `j > i`. This prevents\n-        the flow of information from the future towards the past.\n       dropout: Float between 0 and 1. Fraction of the units to drop for the\n         attention scores.\n \n     Call Args:\n-\n       inputs: List of the following tensors:\n         * query: Query `Tensor` of shape `[batch_size, Tq, dim]`.\n         * value: Value `Tensor` of shape `[batch_size, Tv, dim]`.\n         * key: Optional key `Tensor` of shape `[batch_size, Tv, dim]`. If not\n-          given, will use `value` for both `key` and `value`, which is the\n-          most common case.\n+          given, will use `value` for both `key` and `value`, which is the most\n+          common case.\n       mask: List of the following tensors:\n-        * query_mask: A boolean mask `Tensor` of shape `[batch_size, Tq]`.\n-          If given, the output will be zero at the positions where\n-          `mask==False`.\n-        * value_mask: A boolean mask `Tensor` of shape `[batch_size, Tv]`.\n-          If given, will apply the mask such that values at positions where\n+        * query_mask: A boolean mask `Tensor` of shape `[batch_size, Tq]`. If\n+          given, the output will be zero at the positions where `mask==False`.\n+        * value_mask: A boolean mask `Tensor` of shape `[batch_size, Tv]`. If\n+          given, will apply the mask such that values at positions where\n           `mask==False` do not contribute to the result.\n       training: Python boolean indicating whether the layer should behave in\n         training mode (adding dropout) or in inference mode (no dropout).\n@@ -68,9 +63,16 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         `[batch_size, Tq, Tv]`.\n     \"\"\"\n \n-    def __init__(self, causal=False, dropout=0.0, **kwargs):\n+    def __init__(self, dropout=0.0, **kwargs):\n+        # Deprecated field `causal` determines whether to using causal masking.\n+        # Use `use_causal_mask` in call() method instead.\n+        if \"causal\" in kwargs:\n+            logging.warning(\n+                \"`causal` argument is deprecated. Please use `use_causal_mask` \"\n+                \"in call() method to specify causal masking.\"\n+            )\n+        self.causal = kwargs.pop(\"causal\", False)\n         super().__init__(**kwargs)\n-        self.causal = causal\n         self.dropout = dropout\n         self.supports_masking = True\n \n@@ -136,7 +138,12 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n \n     # TODO(b/125916026): Consider exposing a __call__ method with named args.\n     def call(\n-        self, inputs, mask=None, training=None, return_attention_scores=False\n+        self,\n+        inputs,\n+        mask=None,\n+        training=None,\n+        return_attention_scores=False,\n+        use_causal_mask=False,\n     ):\n         self._validate_call_args(inputs=inputs, mask=mask)\n         q = inputs[0]\n@@ -148,7 +155,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         if v_mask is not None:\n             # Mask of shape [batch_size, 1, Tv].\n             v_mask = tf.expand_dims(v_mask, axis=-2)\n-        if self.causal:\n+        if self.causal or use_causal_mask:\n             # Creates a lower triangular mask, so position i cannot attend to\n             # positions j>i. This prevents the flow of information from the\n             # future into the past.\n@@ -216,7 +223,6 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n \n     def get_config(self):\n         config = {\n-            \"causal\": self.causal,\n             \"dropout\": self.dropout,\n         }\n         base_config = super().get_config()\n\n@@ -225,16 +225,6 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n             lambda: bidirectional.Bidirectional(simple_rnn.SimpleRNN(units=4)),\n             (2, 2, 2),\n         ),\n-        (\n-            \"AttentionLayerCausal\",\n-            lambda: attention.Attention(causal=True),\n-            [(2, 2, 3), (2, 3, 3), (2, 3, 3)],\n-        ),\n-        (\n-            \"AdditiveAttentionLayerCausal\",\n-            lambda: attention.AdditiveAttention(causal=True),\n-            [(2, 3, 4), (2, 3, 4), (2, 3, 4)],\n-        ),\n         (\"NormalizationAdapt\", _create_normalization_layer_with_adapt, (4, 4)),\n         (\n             \"NormalizationNoAdapt\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cc4818fab6dbc5830e883930a912db8c2d0c7cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 9263 | Contributors (this commit): 108 | Commits (past 90d): 24 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -640,7 +640,6 @@ class Callback:\n        locations in your loop. Like this:\n \n     Example:\n-    \n     ```python\n        callbacks =  tf.keras.callbacks.CallbackList([...])\n        callbacks.append(...)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2f7678b0efebd115a39be3b3b43587384095e73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 590 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -110,7 +110,7 @@ def _KeyToFilePath(key, api_version):\n \n     def _ReplaceCapsWithDash(matchobj):\n         match = matchobj.group(0)\n-        return \"-%s\" % (match.lower())\n+        return f\"-{match.lower()}\"\n \n     case_insensitive_key = re.sub(\n         \"([A-Z]{1})\", _ReplaceCapsWithDash, six.ensure_str(key)\n@@ -118,7 +118,7 @@ def _KeyToFilePath(key, api_version):\n     api_folder = (\n         _API_GOLDEN_FOLDER_V2 if api_version == 2 else _API_GOLDEN_FOLDER_V1\n     )\n-    return os.path.join(api_folder, \"%s.pbtxt\" % case_insensitive_key)\n+    return os.path.join(api_folder, f\"{case_insensitive_key}.pbtxt\")\n \n \n def _FileNameToKey(filename):\n@@ -240,7 +240,7 @@ class ApiCompatibilityTest(tf.test.TestCase):\n                 )\n                 verbose_diff_message = diff_message\n             elif key in only_in_actual:\n-                diff_message = \"New object %s found (added).\" % key\n+                diff_message = f\"New object {key} found (added).\"\n                 verbose_diff_message = diff_message\n             else:\n                 # Do not truncate diff\n@@ -250,7 +250,7 @@ class ApiCompatibilityTest(tf.test.TestCase):\n                     self.assertProtoEquals(expected_dict[key], actual_dict[key])\n                 except AssertionError as e:\n                     updated_keys.append(key)\n-                    diff_message = \"Change detected in python object: %s.\" % key\n+                    diff_message = f\"Change detected in python object: {key}.\"\n                     verbose_diff_message = str(e)\n \n             # All difference cases covered above. If any difference found, add\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#743acf772c2ac5fb469c2cdfb4cf0520b3054224", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1350 | Contributors (this commit): 9 | Commits (past 90d): 25 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -146,7 +146,7 @@ def get_distribution_strategy(\n         )\n \n     raise ValueError(\n-        \"Unrecognized Distribution Strategy: %r\" % distribution_strategy\n+        f\"Unrecognized Distribution Strategy: {distribution_strategy!r}\"\n     )\n \n \n\n@@ -51,11 +51,11 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n         metrics = [\n             {\n                 \"name\": \"exp_per_sec\",\n-                \"value\": float(\"{0:.3f}\".format(num_iters / total_time)),\n+                \"value\": float(f\"{num_iters / total_time:.3f}\"),\n             },\n             {\n                 \"name\": \"us_per_exp\",\n-                \"value\": float(\"{0:.3f}\".format(total_time * 1e6 / num_iters)),\n+                \"value\": float(f\"{total_time * 1000000.0 / num_iters:.3f}\"),\n             },\n         ]\n         benchmark_name = self._get_benchmark_name()\n\n@@ -50,11 +50,11 @@ class LayerBenchmarksBase(tf.test.Benchmark):\n         metrics = [\n             {\n                 \"name\": \"examples_per_sec\",\n-                \"value\": float(\"{0:.3f}\".format(num_iters / total_time)),\n+                \"value\": float(f\"{num_iters / total_time:.3f}\"),\n             },\n             {\n                 \"name\": \"us_per_example\",\n-                \"value\": float(\"{0:.3f}\".format(us_mean_time)),\n+                \"value\": float(f\"{us_mean_time:.3f}\"),\n             },\n         ]\n \n\n@@ -43,5 +43,5 @@ def run_with_xprof(\n         for _ in range(num_iters_xprof):\n             func()\n     total_time = time.time() - start\n-    us_per_example = float(\"{0:.3f}\".format(total_time * 1e6 / num_iters_xprof))\n+    us_per_example = float(f\"{total_time * 1000000.0 / num_iters_xprof:.3f}\")\n     return logdir, us_per_example\n\n@@ -130,12 +130,12 @@ class KerasComponentsBenchmarks(tf.test.Benchmark):\n             metrics=[\n                 {\n                     \"name\": \"exp_per_sec\",\n-                    \"value\": float(\"{0:.3f}\".format(num_iters / total_time)),\n+                    \"value\": float(f\"{num_iters / total_time:.3f}\"),\n                 },\n                 {\n                     \"name\": \"us_per_exp\",\n                     \"value\": float(\n-                        \"{0:.3f}\".format(total_time * 1e6 / num_iters)\n+                        f\"{total_time * 1000000.0 / num_iters:.3f}\"\n                     ),\n                 },\n             ],\n\n@@ -54,13 +54,13 @@ def save_and_load_benchmark(app):\n     save_result = {\n         \"iters\": trials,\n         \"wall_time\": total_save_time / trials,\n-        \"name\": \"{}.save\".format(model_name),\n+        \"name\": f\"{model_name}.save\",\n     }\n \n     load_result = {\n         \"iters\": trials,\n         \"wall_time\": total_load_time / trials,\n-        \"name\": \"{}.load\".format(model_name),\n+        \"name\": f\"{model_name}.load\",\n     }\n     tf.compat.v1.gfile.DeleteRecursively(save_dir)\n     return save_result, load_result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
