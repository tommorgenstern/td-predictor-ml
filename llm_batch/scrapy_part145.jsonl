{"custom_id": "scrapy#b901e640445cae3b0546bb6ea8ac45ca37823ec0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 88 | Files Changed: 3 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 92 | Churn Cumulative: 419 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,7 +12,6 @@ from twisted.internet import task\n from scrapy import signals, log\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported\n from scrapy.utils.engine import get_engine_status\n \n class MemoryUsage(object):\n@@ -20,7 +19,9 @@ class MemoryUsage(object):\n     def __init__(self, crawler):\n         if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n-        if not procfs_supported():\n+        try:\n+            self.resource = __import__('resource')\n+        except ImportError:\n             raise NotConfigured\n \n         self.crawler = crawler\n@@ -38,7 +39,7 @@ class MemoryUsage(object):\n         return cls(crawler)\n \n     def get_virtual_size(self):\n-        return get_vmvalue_from_procfs('VmRSS')\n+        return self.resource.getrusage(self.resource.RUSAGE_SELF).ru_maxrss * 1024\n \n     def engine_started(self):\n         self.crawler.stats.set_value('memusage/startup', self.get_virtual_size())\n\n@@ -1,18 +0,0 @@\n-from twisted.trial import unittest\n-\n-from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported\n-\n-class UtilsMemoryTestCase(unittest.TestCase):\n-\n-    def test_get_vmvalue_from_procfs(self):\n-        if not procfs_supported():\n-            raise unittest.SkipTest('/proc filesystem not supported')\n-        vmsize = get_vmvalue_from_procfs('VmSize')\n-        vmrss = get_vmvalue_from_procfs('VmRSS')\n-        self.assert_(isinstance(vmsize, int))\n-        self.assert_(vmsize > 0)\n-        self.assert_(vmrss > 0)\n-        self.assert_(vmsize > vmrss)\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n\n@@ -1,67 +0,0 @@\n-import os\n-import sys\n-import struct\n-\n-_vmvalue_scale = {'kB': 1024, 'mB': 1024*1024, 'KB': 1024, 'MB': 1024*1024}\n-\n-def get_vmvalue_from_procfs(vmkey='VmSize', pid=None):\n-    \"\"\"Return virtual memory value (in bytes) for the given pid using the /proc\n-    filesystem. If pid is not given, it default to the current process pid.\n-    Available keys are: VmSize, VmRSS (default), VmStk\n-    \"\"\"\n-    if pid is None:\n-        pid = os.getpid()\n-    try:\n-        t = open('/proc/%d/status' % pid)\n-    except IOError:\n-        raise RuntimeError(\"/proc filesystem not supported\")\n-    if sys.platform == \"sunos5\":\n-        return _vmvalue_solaris(vmkey, pid)\n-    else:\n-        v = t.read()\n-        t.close()\n-        # get vmkey line e.g. 'VmRSS:  9999  kB\\n ...'\n-        i = v.index(vmkey + ':')\n-        v = v[i:].split(None, 3)  # whitespace\n-        if len(v) < 3:\n-            return 0  # invalid format?\n-        # convert Vm value to bytes\n-        return int(v[1]) * _vmvalue_scale[v[2]]\n-\n-def procfs_supported():\n-    try:\n-        open('/proc/%d/status' % os.getpid())\n-    except IOError:\n-        return False\n-    else:\n-        return True\n-\n-def _vmvalue_solaris(vmkey, pid):\n-\n-     # Memory layout for struct psinfo.\n-     # http://docs.sun.com/app/docs/doc/816-5174/proc-4?l=en&a=view\n-    _psinfo_struct_format = (\n-        \"10i\"   # pr_flag [0] through pr_egid [9]\n-        \"5L\"    # pr_addr [10] through pr_ttyydev [14]\n-        \"2H\"    # pr_pctcpu [15] and pr_pctmem [16]\n-        \"6l\"    # pr_start [17-18] through pr_ctime [21-22]\n-        \"16s\"   # pr_fname [23]\n-        \"80s\"   # pr_psargs [24]\n-        \"2i\"    # pr_wstat[25] and pr_argc [26]\n-        \"2L\"    # pr_argv [27] and pr_envp [28]\n-        \"b3x\"   # pr_dmodel[29] and pr_pad2\n-        \"7i\"    # pr_taskid [30] through pr_filler\n-        \"20i6l\" # pr_lwp\n-        )\n-    psinfo_file = os.path.join(\"/proc\", str(pid), \"psinfo\")\n-    with open(psinfo_file) as f:\n-        parts = struct.unpack(_psinfo_struct_format, f.read())\n-\n-    vmkey_index = {\n-        'VmSize' : 11, # pr_size\n-        'VmRSS'  : 12, # pr_rssize\n-    }\n-\n-    vm_in_kB = parts[vmkey_index[vmkey]]\n-\n-    return vm_in_kB * 1024\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#241abcb8aaa30d9ba7b9002b06b63cf37f571b26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 256 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,8 @@ import traceback\n import uuid\n from cStringIO import StringIO\n \n+from twisted.python import log\n+\n from scrapy.utils.txweb import JsonResource\n from .utils import get_spider_list\n \n@@ -17,6 +19,7 @@ class WsResource(JsonResource):\n         except Exception, e:\n             if self.root.debug:\n                 return traceback.format_exc()\n+            log.err()\n             r = {\"status\": \"error\", \"message\": str(e)}\n             return self.render_object(r, txrequest)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#caa64908d2a2717270f1e20bc10f8dfd47a72ee5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 40 | Churn Cumulative: 386 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ from itertools import count\n \n from scrapy.utils.python import str_to_unicode, unicode_to_str, \\\n     memoizemethod_noargs, isbinarytext, equal_attributes, \\\n-    WeakKeyCache, stringify_dict\n+    WeakKeyCache, stringify_dict, get_func_args\n \n __doctests__ = ['scrapy.utils.python']\n \n@@ -154,5 +154,33 @@ class UtilsPythonTestCase(unittest.TestCase):\n         self.failIf(d is d2) # shouldn't modify in place\n         self.failIf(any(isinstance(x, unicode) for x in d2.keys()))\n \n+    def test_get_func_args(self):\n+        def f1(a, b, c):\n+            pass\n+\n+        def f2(a, b=None, c=None):\n+            pass\n+\n+        class A(object):\n+            def __init__(self, a, b, c):\n+                pass\n+\n+            def method(self, a, b, c):\n+                pass\n+\n+        class Callable(object):\n+\n+            def __call__(self, a, b, c):\n+                pass\n+\n+        a = A(1, 2, 3)\n+        cal = Callable()\n+\n+        self.assertEqual(get_func_args(f1), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(f2), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(A), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(a.method), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(cal), ['a', 'b', 'c'])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -150,11 +150,15 @@ def get_func_args(func):\n     \"\"\"Return the argument name list of a callable\"\"\"\n     if inspect.isfunction(func):\n         func_args, _, _, _ = inspect.getargspec(func)\n+    elif inspect.isclass(func):\n+        func_args, _, _, _ = inspect.getargspec(func.__init__)\n+        func_args.pop(0) # self\n+    elif inspect.ismethod(func):\n+        func_args, _, _, _ = inspect.getargspec(func.__func__)\n+        func_args.pop(0) # self\n     elif hasattr(func, '__call__'):\n-        try:\n         func_args, _, _, _ = inspect.getargspec(func.__call__)\n-        except Exception:\n-            func_args = []\n+        func_args.pop(0) # self\n     else:\n         raise TypeError('%s is not callable' % type(func))\n     return func_args\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0135a5364e48d3fd449e66d8b096f506433fb12a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 790 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -185,7 +185,6 @@ class Scraper(object):\n         \"\"\"\n         if spider_failure is download_failure:\n             errmsg = spider_failure.getErrorMessage()\n-            spider_failure.printTraceback()\n             if errmsg:\n                 log.msg(format='Error downloading %(request)s: %(errmsg)s',\n                         level=log.ERROR, spider=spider, request=request, errmsg=errmsg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44fe4c6b78fd2fea844779d2c907bf19cba8655b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 343 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -134,6 +134,7 @@ class SpiderSlot(object):\n class FeedExporter(object):\n \n     def __init__(self, settings):\n+        self.settings = settings\n         self.urifmt = settings['FEED_URI']\n         if not self.urifmt:\n             raise NotConfigured\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c05fbb6a975b0cd4cea53919a20cff67bb909ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 249 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -164,7 +164,7 @@ def get_func_args(func, stripself=False):\n     else:\n         raise TypeError('%s is not callable' % type(func))\n     if stripself:\n-        func_args.pop(0) # self\n+        func_args.pop(0)\n     return func_args\n \n def equal_attributes(obj1, obj2, attributes):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1e2efe566445bef93f74aae710a5d621da9796de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 322 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,6 +15,7 @@ from w3lib.form import encode_multipart\n from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.http import basic_auth_header\n+from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.conf import get_config, closest_scrapy_cfg\n \n _SETUP_PY_TEMPLATE = \\\n@@ -206,7 +207,7 @@ def _build_egg():\n         _create_default_setup_py(settings=settings)\n     d = tempfile.mkdtemp()\n     f = tempfile.TemporaryFile(dir=d)\n-    check_call([sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)\n+    retry_on_eintr(check_call, [sys.executable, 'setup.py', 'clean', '-a', 'bdist_egg', '-d', d], stdout=f)\n     egg = glob.glob(os.path.join(d, '*.egg'))[0]\n     return egg, d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf8dc61fb714e78c2ff734decc33a2d00e60d92c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 13 | Churn Cumulative: 213 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,7 +60,9 @@ class Contract(object):\n         cb = request.callback\n         @wraps(cb)\n         def wrapper(response):\n-            self.pre_process(response)\n+            try: self.pre_process(response)\n+            except ContractFail as e:\n+                print e.format(self.method)\n             return list(iterate_spider_output(cb(response)))\n \n         request.callback = wrapper\n@@ -71,7 +73,9 @@ class Contract(object):\n         @wraps(cb)\n         def wrapper(response):\n             output = list(iterate_spider_output(cb(response)))\n-            self.post_process(output)\n+            try: self.post_process(output)\n+            except ContractFail as e:\n+                print e.format(self.method)\n             return output\n \n         request.callback = wrapper\n\n@@ -52,4 +52,7 @@ class ScrapyDeprecationWarning(Warning):\n \n class ContractFail(AssertionError):\n     \"\"\"Error raised in case of a failing contract\"\"\"\n-    pass\n+    \n+    def format(self, method):\n+        return '[FAILED] %s:%s\\n>>> %s\\n' % \\\n+            (method.im_class.name, method.__name__, self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a42a8cafefb00bdc7452d3ee33e04cc295ed1d5e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 261 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import traceback\n import uuid\n from cStringIO import StringIO\n+from datetime import datetime\n \n from twisted.python import log\n \n@@ -96,7 +97,9 @@ class ListJobs(WsResource):\n         running = [{\"id\": s.job, \"spider\": s.spider} for s in spiders if s.project == project]\n         queue = self.root.poller.queues[project]\n         pending = [{\"id\": x[\"_job\"], \"spider\": x[\"name\"]} for x in queue.list()]\n-        finished = [{\"id\": s.job, \"spider\": s.spider} for s in self.root.launcher.finished\n+        finished = [{\"id\": s.job, \"spider\": s.spider,\n+            \"start_time\": datetime.isoformat(s.start_time),\n+            \"end_time\": datetime.isoformat(s.end_time)} for s in self.root.launcher.finished\n             if s.project == project]\n         return {\"status\":\"ok\", \"pending\": pending, \"running\": running, \"finished\": finished}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ef593c5c223a2151ce53eed0b38a7618593c83f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 44 | Files Changed: 4 | Hunks: 17 | Methods Changed: 14 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 72 | Churn Cumulative: 588 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class MemoryUsage(object):\n         self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n         self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n-        self.mail = MailSender()\n+        self.mail = MailSender.from_settings(crawler.settings)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n\n@@ -10,23 +10,25 @@ from scrapy.exceptions import NotConfigured\n \n class StatsMailer(object):\n \n-    def __init__(self, stats, recipients):\n+    def __init__(self, stats, recipients, mail):\n         self.stats = stats\n         self.recipients = recipients\n+        self.mail = mail\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         recipients = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n-        o = cls(crawler.stats, recipients)\n-        crawler.connect(o.stats_spider_closed, signal=signals.stats_spider_closed)\n+        mail = MailSender.from_settings(crawler.settings)\n+        o = cls(crawler.stats, recipients, mail)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         return o\n         \n-    def stats_spider_closed(self, spider, spider_stats):\n-        mail = MailSender()\n+    def spider_closed(self, spider):\n+        spider_stats = self.stats.get_stats(spider)\n         body = \"Global stats\\n\\n\"\n         body += \"\\n\".join(\"%-50s : %s\" % i for i in self.stats.get_stats().items())\n         body += \"\\n\\n%s stats\\n\\n\" % spider.name\n         body += \"\\n\".join(\"%-50s : %s\" % i for i in spider_stats.items())\n-        mail.send(self.recipients, \"Scrapy stats for: %s\" % spider.name, body)\n+        return self.mail.send(self.recipients, \"Scrapy stats for: %s\" % spider.name, body)\n\n@@ -15,31 +15,24 @@ from twisted.internet import defer, reactor\n from twisted.mail.smtp import ESMTPSenderFactory\n \n from scrapy import log\n-from scrapy.exceptions import NotConfigured\n-from scrapy.conf import settings\n-\n-\n-# signal sent when message is sent\n-# args: to, subject, body, cc, attach, msg\n-mail_sent = object()\n-\n \n class MailSender(object):\n \n-    def __init__(self, smtphost=None, mailfrom=None, smtpuser=None, smtppass=None, \\\n-            smtpport=None, debug=False, crawler=None):\n-        self.smtphost = smtphost or settings['MAIL_HOST']\n-        self.smtpport = smtpport or settings.getint('MAIL_PORT')\n-        self.smtpuser = smtpuser or settings['MAIL_USER']\n-        self.smtppass = smtppass or settings['MAIL_PASS']\n-        self.mailfrom = mailfrom or settings['MAIL_FROM']\n+    def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n+            smtpuser=None, smtppass=None, smtpport=25, debug=False):\n+        self.smtphost = smtphost\n+        self.smtpport = smtpport\n+        self.smtpuser = smtpuser\n+        self.smtppass = smtppass\n+        self.mailfrom = mailfrom\n         self.debug = debug\n-        self.signals = crawler.signals if crawler else None\n \n-        if not self.smtphost or not self.mailfrom:\n-            raise NotConfigured(\"MAIL_HOST and MAIL_FROM settings are required\")\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls(settings['MAIL_HOST'], settings['MAIL_FROM'], settings['MAIL_USER'],\n+            settings['MAIL_PASS'], settings.getint('MAIL_PORT'))\n \n-    def send(self, to, subject, body, cc=None, attachs=()):\n+    def send(self, to, subject, body, cc=None, attachs=(), _callback=None):\n         if attachs:\n             msg = MIMEMultipart()\n         else:\n@@ -65,9 +58,8 @@ class MailSender(object):\n         else:\n             msg.set_payload(body)\n \n-        if self.signals:\n-            self.signals.send_catch_log(signal=mail_sent, to=to, subject=subject, body=body,\n-                       cc=cc, attach=attachs, msg=msg)\n+        if _callback:\n+            _callback(to=to, subject=subject, body=body, cc=cc, attach=attachs, msg=msg)\n \n         if self.debug:\n             log.msg(format='Debug mail sent OK: To=%(mailto)s Cc=%(mailcc)s Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n\n@@ -1,23 +1,13 @@\n from cStringIO import StringIO\n import unittest\n \n-from scrapy.mail import MailSender, mail_sent\n-from scrapy.utils.test import get_crawler\n-\n+from scrapy.mail import MailSender\n \n class MailSenderTest(unittest.TestCase):\n \n-    def setUp(self):\n-        self.catched_msg = None\n-        self.crawler = get_crawler()\n-        self.crawler.signals.connect(self._catch_mail_sent, signal=mail_sent)\n-\n-    def tearDown(self):\n-        self.crawler.signals.disconnect(self._catch_mail_sent, signal=mail_sent)\n-\n     def test_send(self):\n-        mailsender = MailSender(debug=True, crawler=self.crawler)\n-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body')\n+        mailsender = MailSender(debug=True)\n+        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body', _callback=self._catch_mail_sent)\n \n         assert self.catched_msg\n \n@@ -36,9 +26,9 @@ class MailSenderTest(unittest.TestCase):\n         attach.seek(0)\n         attachs = [('attachment', 'text/plain', attach)]\n \n-        mailsender = MailSender(debug=True, crawler=self.crawler)\n+        mailsender = MailSender(debug=True)\n         mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n-                       attachs=attachs)\n+                       attachs=attachs, _callback=self._catch_mail_sent)\n \n         assert self.catched_msg\n         self.assertEqual(self.catched_msg['to'], ['test@scrapy.org'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8642645109643be5471a3ce93313b57821343b4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 174 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -5,12 +5,12 @@ from twisted.trial import unittest\n \n from scrapy import log\n from scrapy.spider import BaseSpider\n-from scrapy.conf import settings\n+from scrapy.settings import default_settings\n \n class LogTest(unittest.TestCase):\n \n     def test_get_log_level(self):\n-        default_log_level = getattr(log, settings['LOG_LEVEL'])\n+        default_log_level = getattr(log, default_settings.LOG_LEVEL)\n         self.assertEqual(log._get_log_level(), default_log_level)\n         self.assertEqual(log._get_log_level('WARNING'), log.WARNING)\n         self.assertEqual(log._get_log_level(log.WARNING), log.WARNING)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7ace23c3856bc03d05ee655ea276382077347fe1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 103 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,14 +20,14 @@ class CmdlineTest(unittest.TestCase):\n \n     def test_default_settings(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n-            'default + loaded + started')\n+            'default + started')\n \n     def test_override_settings_using_set_arg(self):\n         self.assertEqual(self._execute('settings', '--get', 'TEST1', '-s', 'TEST1=override'), \\\n-            'override + loaded + started')\n+            'override + started')\n \n     def test_override_settings_using_envvar(self):\n         self.env['SCRAPY_TEST1'] = 'override'\n         self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n-            'override + loaded + started')\n+            'override + started')\n \n\n@@ -1,10 +1,10 @@\n \"\"\"A test extension used to check the settings loading order\"\"\"\n \n-from scrapy.conf import settings\n-\n-settings.overrides['TEST1'] = \"%s + %s\" % (settings['TEST1'], 'loaded')\n-\n class TestExtension(object):\n \n-    def __init__(self):\n+    def __init__(self, settings):\n         settings.overrides['TEST1'] = \"%s + %s\" % (settings['TEST1'], 'started')\n+\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler.settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5dde26d3c80ada59d8a8b5a702527eff759d5d12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 266 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,6 @@\n import traceback\n import uuid\n from cStringIO import StringIO\n-from datetime import datetime\n \n from twisted.python import log\n \n@@ -98,8 +97,8 @@ class ListJobs(WsResource):\n         queue = self.root.poller.queues[project]\n         pending = [{\"id\": x[\"_job\"], \"spider\": x[\"name\"]} for x in queue.list()]\n         finished = [{\"id\": s.job, \"spider\": s.spider,\n-            \"start_time\": datetime.isoformat(s.start_time),\n-            \"end_time\": datetime.isoformat(s.end_time)} for s in self.root.launcher.finished\n+            \"start_time\": s.start_time.isoformat(' '),\n+            \"end_time\": s.end_time.isoformat(' ')} for s in self.root.launcher.finished\n             if s.project == project]\n         return {\"status\":\"ok\", \"pending\": pending, \"running\": running, \"finished\": finished}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#263e302b6a23a9ca64b3c002a20c0beaa45dcbbb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 326 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,11 +47,25 @@ class Scheduler(object):\n     def enqueue_request(self, request):\n         if not request.dont_filter and self.df.request_seen(request):\n             return\n-        if not self._dqpush(request):\n+        dqok = self._dqpush(request)\n+        if dqok:\n+            self.stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n+        else:\n             self._mqpush(request)\n+            self.stats.inc_value('scheduler/memory_enqueued', spider=self.spider)\n+        self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n \n     def next_request(self):\n-        return self.mqs.pop() or self._dqpop()\n+        request = self.mqs.pop()\n+        if request:\n+            self.stats.inc_value('scheduler/memory_dequeued', spider=self.spider)\n+        else:\n+            request = self._dqpop()\n+            if request:\n+                self.stats.inc_value('scheduler/disk_dequeued', spider=self.spider)\n+        if request:\n+            self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n+        return request\n \n     def __len__(self):\n         return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)\n@@ -69,13 +83,9 @@ class Scheduler(object):\n                         request=request, reason=e)\n             return\n         else:\n-            if self.stats:\n-                self.stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n             return True\n \n     def _mqpush(self, request):\n-        if self.stats:\n-            self.stats.inc_value('scheduler/memory_enqueued', spider=self.spider)\n         self.mqs.push(request, -request.priority)\n \n     def _dqpop(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b4842055903ca3218e8e19b4669790921274872", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 334 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -49,20 +49,20 @@ class Scheduler(object):\n             return\n         dqok = self._dqpush(request)\n         if dqok:\n-            self.stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n+            self.stats.inc_value('scheduler/enqueued/disk', spider=self.spider)\n         else:\n             self._mqpush(request)\n-            self.stats.inc_value('scheduler/memory_enqueued', spider=self.spider)\n+            self.stats.inc_value('scheduler/enqueued/memory', spider=self.spider)\n         self.stats.inc_value('scheduler/enqueued', spider=self.spider)\n \n     def next_request(self):\n         request = self.mqs.pop()\n         if request:\n-            self.stats.inc_value('scheduler/memory_dequeued', spider=self.spider)\n+            self.stats.inc_value('scheduler/dequeued/memory', spider=self.spider)\n         else:\n             request = self._dqpop()\n             if request:\n-                self.stats.inc_value('scheduler/disk_dequeued', spider=self.spider)\n+                self.stats.inc_value('scheduler/dequeued/disk', spider=self.spider)\n         if request:\n             self.stats.inc_value('scheduler/dequeued', spider=self.spider)\n         return request\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#81ed2d2d0b7cbf2dd5cffc0da50367c5284416a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 79 | Files Changed: 5 | Hunks: 23 | Methods Changed: 25 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 97 | Churn Cumulative: 2445 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,16 +13,16 @@ class CoreStats(object):\n     @classmethod\n     def from_crawler(cls, crawler):\n         o = cls(crawler.stats)\n-        crawler.signals.connect(o.stats_spider_opened, signal=signals.stats_spider_opened)\n-        crawler.signals.connect(o.stats_spider_closing, signal=signals.stats_spider_closing)\n+        crawler.signals.connect(o.spider_opened, signal=signals.spider_opened)\n+        crawler.signals.connect(o.spider_closed, signal=signals.spider_closed)\n         crawler.signals.connect(o.item_scraped, signal=signals.item_scraped)\n         crawler.signals.connect(o.item_dropped, signal=signals.item_dropped)\n         return o\n \n-    def stats_spider_opened(self, spider):\n+    def spider_opened(self, spider):\n         self.stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n \n-    def stats_spider_closing(self, spider, reason):\n+    def spider_closed(self, spider, reason):\n         self.stats.set_value('finish_time', datetime.datetime.utcnow(), spider=spider)\n         self.stats.set_value('finish_reason', reason, spider=spider)\n \n\n@@ -286,4 +286,3 @@ class ExecutionEngine(object):\n     @defer.inlineCallbacks\n     def _finish_stopping_engine(self):\n         yield self.signals.send_catch_log_deferred(signal=signals.engine_stopped)\n-        yield self.crawler.stats.engine_stopped()\n\n@@ -16,8 +16,5 @@ response_received = object()\n response_downloaded = object()\n item_scraped = object()\n item_dropped = object()\n-stats_spider_opened = object()\n-stats_spider_closing = object()\n-stats_spider_closed = object()\n \n item_passed = item_scraped # for backwards compatibility\n\n@@ -3,68 +3,49 @@ Scrapy extension for collecting scraping stats\n \"\"\"\n import pprint\n \n-from scrapy.signals import stats_spider_opened, stats_spider_closing, \\\n-    stats_spider_closed\n from scrapy import log\n \n class StatsCollector(object):\n \n     def __init__(self, crawler):\n         self._dump = crawler.settings.getbool('STATS_DUMP')\n-        self._stats = {None: {}} # None is for global stats\n-        self._signals = crawler.signals\n+        self._stats = {}\n \n     def get_value(self, key, default=None, spider=None):\n-        return self._stats[spider].get(key, default)\n+        return self._stats.get(key, default)\n \n     def get_stats(self, spider=None):\n-        return self._stats[spider]\n+        return self._stats\n \n     def set_value(self, key, value, spider=None):\n-        self._stats[spider][key] = value\n+        self._stats[key] = value\n \n     def set_stats(self, stats, spider=None):\n-        self._stats[spider] = stats\n+        self._stats = stats\n \n     def inc_value(self, key, count=1, start=0, spider=None):\n-        d = self._stats[spider]\n+        d = self._stats\n         d[key] = d.setdefault(key, start) + count\n \n     def max_value(self, key, value, spider=None):\n-        d = self._stats[spider]\n-        d[key] = max(d.setdefault(key, value), value)\n+        self._stats[key] = max(self._stats.setdefault(key, value), value)\n \n     def min_value(self, key, value, spider=None):\n-        d = self._stats[spider]\n-        d[key] = min(d.setdefault(key, value), value)\n+        self._stats[key] = min(self._stats.setdefault(key, value), value)\n \n     def clear_stats(self, spider=None):\n-        self._stats[spider].clear()\n-\n-    def iter_spider_stats(self):\n-        return [x for x in self._stats.iteritems() if x[0]]\n+        self._stats.clear()\n \n     def open_spider(self, spider):\n-        self._stats[spider] = {}\n-        self._signals.send_catch_log(stats_spider_opened, spider=spider)\n+        self._stats = {}\n \n     def close_spider(self, spider, reason):\n-        self._signals.send_catch_log(stats_spider_closing, spider=spider, reason=reason)\n-        stats = self._stats.pop(spider)\n-        self._signals.send_catch_log(stats_spider_closed, spider=spider, reason=reason, \\\n-            spider_stats=stats)\n         if self._dump:\n-            log.msg(\"Dumping spider stats:\\n\" + pprint.pformat(stats), \\\n+            log.msg(\"Dumping spider stats:\\n\" + pprint.pformat(self._stats), \\\n                 spider=spider)\n-        self._persist_stats(stats, spider)\n+        self._persist_stats(self._stats, spider)\n \n-    def engine_stopped(self):\n-        stats = self.get_stats()\n-        if self._dump:\n-            log.msg(\"Dumping global stats:\\n\" + pprint.pformat(stats))\n-        self._persist_stats(stats, spider=None)\n-\n-    def _persist_stats(self, stats, spider=None):\n+    def _persist_stats(self, stats, spider):\n         pass\n \n class MemoryStatsCollector(StatsCollector):\n@@ -73,8 +54,7 @@ class MemoryStatsCollector(StatsCollector):\n         super(MemoryStatsCollector, self).__init__(crawler)\n         self.spider_stats = {}\n \n-    def _persist_stats(self, stats, spider=None):\n-        if spider is not None:\n+    def _persist_stats(self, stats, spider):\n         self.spider_stats[spider.name] = stats\n \n \n\n@@ -2,8 +2,6 @@ import unittest\n \n from scrapy.spider import BaseSpider\n from scrapy.statscol import StatsCollector, DummyStatsCollector\n-from scrapy.signals import stats_spider_opened, stats_spider_closing, \\\n-    stats_spider_closed\n from scrapy.utils.test import get_crawler\n \n class StatsCollectorTest(unittest.TestCase):\n@@ -53,40 +51,5 @@ class StatsCollectorTest(unittest.TestCase):\n         self.assertEqual(stats.get_stats(), {})\n         self.assertEqual(stats.get_stats('a'), {})\n \n-    def test_signals(self):\n-        signals_catched = set()\n-\n-        def spider_opened(spider):\n-            assert spider is self.spider\n-            signals_catched.add(stats_spider_opened)\n-\n-        def spider_closing(spider, reason):\n-            assert spider is self.spider\n-            assert reason == 'testing'\n-            signals_catched.add(stats_spider_closing)\n-\n-        def spider_closed(spider, reason, spider_stats):\n-            assert spider is self.spider\n-            assert reason == 'testing'\n-            assert spider_stats == {'test': 1}\n-            signals_catched.add(stats_spider_closed)\n-\n-        self.crawler.signals.connect(spider_opened, signal=stats_spider_opened)\n-        self.crawler.signals.connect(spider_closing, signal=stats_spider_closing)\n-        self.crawler.signals.connect(spider_closed, signal=stats_spider_closed)\n-\n-        stats = StatsCollector(self.crawler)\n-        stats.open_spider(self.spider)\n-        stats.set_value('test', 1, spider=self.spider)\n-        self.assertEqual([(self.spider, {'test': 1})], list(stats.iter_spider_stats()))\n-        stats.close_spider(self.spider, 'testing')\n-        assert stats_spider_opened in signals_catched\n-        assert stats_spider_closing in signals_catched\n-        assert stats_spider_closed in signals_catched\n-\n-        self.crawler.signals.disconnect(spider_opened, signal=stats_spider_opened)\n-        self.crawler.signals.disconnect(spider_closing, signal=stats_spider_closing)\n-        self.crawler.signals.disconnect(spider_closed, signal=stats_spider_closed)\n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f2dda12cc593e318c6ad924f7872491874cc612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 19 | Churn Cumulative: 289 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,6 @@ class StackTraceDump(object):\n \n     def __init__(self, crawler=None):\n         self.crawler = crawler\n-        self.dumprefs = crawler.settings.getbool('TRACK_REFS')\n         try:\n             signal.signal(signal.SIGUSR2, self.dump_stacktrace)\n             signal.signal(signal.SIGQUIT, self.dump_stacktrace)\n@@ -34,7 +33,7 @@ class StackTraceDump(object):\n     def dump_stacktrace(self, signum, frame):\n         stackdumps = self._thread_stacks()\n         enginestatus = format_engine_status(self.crawler.engine)\n-        liverefs = format_live_refs() if self.dumprefs else \"\"\n+        liverefs = format_live_refs()\n         msg = \"Dumping stack trace and engine status\" \\\n             \"\\n{0}\\n{1}\\n{2}\".format(enginestatus, liverefs, stackdumps)\n         log.msg(msg)\n\n@@ -12,20 +12,19 @@ from scrapy.utils.trackref import live_refs\n \n class MemoryDebugger(object):\n \n-    def __init__(self, stats, trackrefs=False):\n+    def __init__(self, stats):\n         try:\n             import libxml2\n             self.libxml2 = libxml2\n         except ImportError:\n             self.libxml2 = None\n         self.stats = stats\n-        self.trackrefs = trackrefs\n \n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('MEMDEBUG_ENABLED'):\n             raise NotConfigured\n-        o = cls(crawler.stats, crawler.settings.getbool('TRACK_REFS'))\n+        o = cls(crawler.stats)\n         crawler.signals.connect(o.engine_started, signals.engine_started)\n         crawler.signals.connect(o.engine_stopped, signals.engine_stopped)\n         return o\n@@ -40,7 +39,6 @@ class MemoryDebugger(object):\n             self.stats.set_value('memdebug/libxml2_leaked_bytes', self.libxml2.debugMemory(1))\n         gc.collect()\n         self.stats.set_value('memdebug/gc_garbage_count', len(gc.garbage))\n-        if self.trackrefs:\n         for cls, wdict in live_refs.iteritems():\n             if not wdict:\n                 continue\n\n@@ -2,8 +2,7 @@\n references to live object instances.\n \n If you want live objects for a particular class to be tracked, you only have to\n-subclass form object_ref (instead of object). Also, remember to turn on\n-tracking by enabling the TRACK_REFS setting.\n+subclass form object_ref (instead of object).\n \n About performance: This library has a minimal performance impact when enabled,\n and no performance penalty at all when disabled (as object_ref becomes just an\n@@ -16,8 +15,6 @@ from time import time\n from operator import itemgetter\n from types import NoneType\n \n-from scrapy.conf import settings\n-\n live_refs = defaultdict(weakref.WeakKeyDictionary)\n \n class object_ref(object):\n@@ -31,12 +28,7 @@ class object_ref(object):\n         live_refs[cls][obj] = time()\n         return obj\n \n-if not settings.getbool('TRACK_REFS'):\n-    object_ref = object\n-\n def format_live_refs(ignore=NoneType):\n-    if object_ref is object:\n-        return \"The trackref module is disabled. Use TRACK_REFS setting to enable it.\"\n     s = \"Live References\" + os.linesep + os.linesep\n     now = time()\n     for cls, wdict in live_refs.iteritems():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eed6eb49dade4ce780a6c633b6ee75904f57effe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 36 | Churn Cumulative: 1099 | Contributors (this commit): 9 | Commits (past 90d): 12 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -45,10 +45,10 @@ class HttpCacheMiddleware(object):\n         response = self.storage.retrieve_response(spider, request)\n         if response and self.is_cacheable_response(response):\n             response.flags.append('cached')\n-            self.stats.inc_value('httpcache/hit', spider=spider)\n+            self.stats.inc_value('httpcache/hits', spider=spider)\n             return response\n \n-        self.stats.inc_value('httpcache/miss', spider=spider)\n+        self.stats.inc_value('httpcache/misses', spider=spider)\n         if self.ignore_missing:\n             raise IgnoreRequest(\"Ignored request not in cache: %s\" % request)\n \n\n@@ -11,17 +11,17 @@ from scrapy.utils.project import data_path\n class DbmCacheStorage(object):\n \n     def __init__(self, settings):\n-        self.cachedir = data_path(settings['HTTPCACHE_DIR'])\n+        self.cachedir = data_path(settings['HTTPCACHE_DIR'], createdir=True)\n         self.expiration_secs = settings.getint('HTTPCACHE_EXPIRATION_SECS')\n         self.dbmodule = __import__(settings['HTTPCACHE_DBM_MODULE'])\n-        self.dbs = {}\n+        self.db = None\n \n     def open_spider(self, spider):\n         dbpath = os.path.join(self.cachedir, '%s.db' % spider.name)\n-        self.dbs[spider] = self.dbmodule.open(dbpath, 'c')\n+        self.db = self.dbmodule.open(dbpath, 'c')\n \n     def close_spider(self, spider):\n-        self.dbs[spider].close()\n+        self.db.close()\n \n     def retrieve_response(self, spider, request):\n         data = self._read_data(spider, request)\n@@ -43,12 +43,12 @@ class DbmCacheStorage(object):\n             'headers': dict(response.headers),\n             'body': response.body,\n         }\n-        self.dbs[spider]['%s_data' % key] = pickle.dumps(data, protocol=2)\n-        self.dbs[spider]['%s_time' % key] = str(time())\n+        self.db['%s_data' % key] = pickle.dumps(data, protocol=2)\n+        self.db['%s_time' % key] = str(time())\n \n     def _read_data(self, spider, request):\n         key = self._request_key(request)\n-        db = self.dbs[spider]\n+        db = self.db\n         tkey = '%s_time' % key\n         if not db.has_key(tkey):\n             return # not found\n\n@@ -137,7 +137,7 @@ FEED_EXPORTERS_BASE = {\n HTTPCACHE_ENABLED = False\n HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n-HTTPCACHE_STORAGE = 'scrapy.contrib.downloadermiddleware.httpcache.FilesystemCacheStorage'\n+HTTPCACHE_STORAGE = 'scrapy.contrib.httpcache.DbmCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n\n@@ -38,7 +38,9 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         return self.storage_class(self._get_settings(**new_settings))\n \n     def _get_middleware(self, **new_settings):\n-        return HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)\n+        mw = HttpCacheMiddleware(self._get_settings(**new_settings), self.crawler.stats)\n+        mw.spider_opened(self.spider)\n+        return mw\n \n     def test_storage(self):\n         storage = self._get_storage()\n@@ -59,7 +61,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         assert storage.retrieve_response(self.spider, self.request)\n \n     def test_middleware(self):\n-        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)\n+        mw = self._get_middleware()\n         assert mw.process_request(self.request, self.spider) is None\n         mw.process_response(self.request, self.response, self.spider)\n         response = mw.process_request(self.request, self.spider)\n@@ -68,7 +70,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         assert 'cached' in response.flags\n \n     def test_different_request_response_urls(self):\n-        mw = HttpCacheMiddleware(self._get_settings(), self.crawler.stats)\n+        mw = self._get_middleware()\n         req = Request('http://host.com/path')\n         res = Response('http://host2.net/test.html')\n         assert mw.process_request(req, self.spider) is None\n\n@@ -37,11 +37,15 @@ def project_data_dir(project='default'):\n         os.makedirs(d)\n     return d\n \n-def data_path(path):\n+def data_path(path, createdir=False):\n     \"\"\"If path is relative, return the given path inside the project data dir,\n     otherwise return the path unmodified\n     \"\"\"\n-    return path if isabs(path) else join(project_data_dir(), path)\n+    if not isabs(path):\n+        path = join(project_data_dir(), path)\n+    if createdir and not exists(path):\n+        os.makedirs(path)\n+    return path\n \n def get_project_settings():\n     if ENVVAR not in os.environ:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aadf7ff143f2caeecc8567b7395d35a9705c4319", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 69 | Files Changed: 10 | Hunks: 25 | Methods Changed: 12 | Complexity Δ (Sum/Max): 1/5 | Churn Δ: 119 | Churn Cumulative: 1609 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project, get_project_settings\n+from scrapy.settings.deprecated import check_deprecated_settings\n \n def _iter_command_classes(module_name):\n     # TODO: add `name` attribute to commands and and merge this function with\n@@ -80,10 +81,17 @@ def _run_print_help(parser, func, *a, **kw):\n             parser.print_help()\n         sys.exit(2)\n \n-def execute(argv=None):\n+def execute(argv=None, settings=None):\n     if argv is None:\n         argv = sys.argv\n+    if settings is None:\n         settings = get_project_settings()\n+    check_deprecated_settings(settings)\n+\n+    # backwards compatibility to support scrapy.conf.settings\n+    from scrapy import conf\n+    conf.settings = settings\n+\n     crawler = CrawlerProcess(settings)\n     crawler.install()\n     inproject = inside_project()\n\n@@ -28,7 +28,7 @@ class ScrapyCommand(object):\n     @property\n     def crawler(self):\n         if not log.started:\n-            log.start()\n+            log.start_from_settings(self.settings)\n         self._crawler.configure()\n         return self._crawler\n \n\n@@ -1,7 +1,5 @@\n-# This module is kept for backwards compatibility.\n+# This module is kept for backwards compatibility, so users can import\n+# scrapy.conf.settings and get the settings they expect\n #\n # TODO: Add deprecation warning once all scrapy.conf instances have been\n # removed from Scrapy codebase.\n-\n-from scrapy.utils.project import get_project_settings\n-settings = get_project_settings()\n\n@@ -9,12 +9,11 @@ from w3lib.encoding import html_to_unicode, resolve_encoding, \\\n     html_body_declared_encoding, http_content_type_encoding\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n-from scrapy.conf import settings\n \n \n class TextResponse(Response):\n \n-    _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']\n+    _DEFAULT_ENCODING = 'ascii'\n \n     def __init__(self, *args, **kwargs):\n         self._encoding = kwargs.pop('encoding', None)\n\n@@ -10,7 +10,6 @@ import warnings\n from twisted.python import log\n \n import scrapy\n-from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n from scrapy.utils.misc import load_object\n  \n@@ -32,6 +31,7 @@ level_names = {\n }\n \n started = False\n+formatter = None\n \n class ScrapyFileLogObserver(log.FileLogObserver):\n \n@@ -94,35 +94,22 @@ def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=\n     return ev\n \n def _get_log_level(level_name_or_id=None):\n-    if level_name_or_id is None:\n-        lvlname = settings['LOG_LEVEL']\n-        return globals()[lvlname]\n-    elif isinstance(level_name_or_id, int):\n+    if isinstance(level_name_or_id, int):\n         return level_name_or_id\n     elif isinstance(level_name_or_id, basestring):\n         return globals()[level_name_or_id]\n     else:\n         raise ValueError(\"Unknown log level: %r\" % level_name_or_id)\n \n-def start(logfile=None, loglevel=None, logstdout=None):\n-    global started\n-    if started or not settings.getbool('LOG_ENABLED'):\n-        return\n-    started = True\n-\n+def start(logfile=None, loglevel='INFO', logstdout=True, logencoding='utf-8'):\n     if log.defaultObserver: # check twisted log not already started\n         loglevel = _get_log_level(loglevel)\n-        logfile = logfile or settings['LOG_FILE']\n         file = open(logfile, 'a') if logfile else sys.stderr\n-        if logstdout is None:\n-            logstdout = settings.getbool('LOG_STDOUT')\n-        sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])\n+        sflo = ScrapyFileLogObserver(file, loglevel, logencoding)\n         _oldshowwarning = warnings.showwarning\n         log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n         # restore warnings, wrongly silenced by Twisted\n         warnings.showwarning = _oldshowwarning\n-        msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n-            settings['BOT_NAME']))\n \n def msg(message=None, _level=INFO, **kw):\n     kw['logLevel'] = kw.pop('level', _level)\n@@ -137,4 +124,16 @@ def err(_stuff=None, _why=None, **kw):\n     kw.setdefault('system', 'scrapy')\n     log.err(_stuff, _why, **kw)\n \n+def start_from_settings(settings):\n+    global started, formatter\n+    if started or not settings.getbool('LOG_ENABLED'):\n+        return\n+    started = True\n     formatter = load_object(settings['LOG_FORMATTER'])()\n+\n+    if not settings.getbool('LOG_ENABLED'):\n+        return\n+    start(settings['LOG_FILE'], settings['LOG_LEVEL'], settings['LOG_STDOUT'],\n+        settings['LOG_ENCODING'])\n+    msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n+        settings['BOT_NAME']))\n\n@@ -11,7 +11,6 @@ from cStringIO import StringIO\n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import isbinarytext\n-from scrapy.conf import settings\n \n class ResponseTypes(object):\n \n@@ -31,7 +30,6 @@ class ResponseTypes(object):\n     }\n \n     def __init__(self):\n-        self.CLASSES.update(settings.get('RESPONSE_CLASSES', {}))\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n         mimedata = get_data('scrapy', 'mime.types')\n\n@@ -1,22 +0,0 @@\n-\"\"\"\n-Dummy selectors\n-\"\"\"\n-\n-from .list import XPathSelectorList as XPathSelectorList\n-\n-__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n-    'XPathSelectorList']\n-\n-class XPathSelector(object):\n-\n-    def __init__(self, *a, **kw):\n-        pass\n-\n-    def _raise(self, *a, **kw):\n-        raise RuntimeError(\"No selectors backend available. \" \\\n-            \"Please install libxml2 or lxml\")\n-\n-    select = re = extract = register_namespace = __nonzero__ = _raise\n-\n-XmlXPathSelector = XPathSelector\n-HtmlXPathSelector = XPathSelector\n\n@@ -41,8 +41,6 @@ DEFAULT_REQUEST_HEADERS = {\n     'Accept-Language': 'en',\n }\n \n-DEFAULT_RESPONSE_ENCODING = 'ascii'\n-\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n DEPTH_PRIORITY = 0\n@@ -224,7 +222,7 @@ TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n \n URLLENGTH_LIMIT = 2083\n \n-USER_AGENT = 'Scrapy/0.15 (+http://scrapy.org)'\n+USER_AGENT = 'Scrapy/%s (+http://scrapy.org)' % __import__('scrapy').__version__\n \n TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n\n@@ -0,0 +1,20 @@\n+import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n+DEPRECATED_SETTINGS = [\n+    ('TRACK_REFS', 'no longer needed (trackref is always enabled)'),\n+    ('RESPONSE_CLASSES', 'no longer supported'),\n+    ('DEFAULT_RESPONSE_ENCODING', 'no longer supported'),\n+    ('BOT_VERSION', 'no longer used (user agent defaults to Scrapy now)'),\n+    ('ENCODING_ALIASES', 'no longer needed (encoding discovery uses w3lib now)'),\n+    ('STATS_ENABLED', 'no longer supported (change STATS_CLASS instead)'),\n+    ('SQLITE_DB', 'no longer supported'),\n+]\n+\n+def check_deprecated_settings(settings):\n+    deprecated = [x for x in DEPRECATED_SETTINGS if settings[x[0]] is not None]\n+    if deprecated:\n+        msg = \"You are using the following settings which are deprecated or obsolete\"\n+        msg += \" (ask scrapy-users@googlegroups.com for alternatives):\"\n+        msg = msg + \"\\n    \" + \"\\n    \".join(\"%s: %s\" % x for x in deprecated)\n+        warnings.warn(msg, ScrapyDeprecationWarning)\n\n@@ -1,17 +0,0 @@\n-import unittest\n-\n-from scrapy.http import TextResponse\n-from scrapy.selector.dummysel import XmlXPathSelector, HtmlXPathSelector, \\\n-    XPathSelector\n-\n-class XPathSelectorTestCase(unittest.TestCase):\n-\n-    def test_raises(self):\n-        response = TextResponse(url=\"http://example.com\", body='test')\n-        for cls in [XmlXPathSelector, HtmlXPathSelector, XPathSelector]:\n-            sel = cls(response)\n-            self.assertRaises(RuntimeError, sel.select, '//h2')\n-            self.assertRaises(RuntimeError, sel.re, 'lala')\n-            self.assertRaises(RuntimeError, sel.extract)\n-            self.assertRaises(RuntimeError, sel.register_namespace, 'a', 'b')\n-            self.assertRaises(RuntimeError, sel.__nonzero__, 'a', 'b')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
