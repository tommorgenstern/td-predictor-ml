{"custom_id": "scrapy#fd27cde24d273e30f72f53f2711515403270838a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 11 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 1390 | Contributors (this commit): 19 | Commits (past 90d): 13 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -62,7 +62,7 @@ class WrappedResponseTest(TestCase):\n         self.wrapped = WrappedResponse(self.response)\n \n     def test_info(self):\n-        self.assertTrue(self.wrapped.info() is self.wrapped)\n+        self.assertIs(self.wrapped.info(), self.wrapped)\n \n     def test_getheaders(self):\n         self.assertEqual(self.wrapped.getheaders('content-type'), ['text/html'])\n\n@@ -64,9 +64,9 @@ class RequestTest(unittest.TestCase):\n         h = Headers({'key1': u'val1', u'key2': 'val2'})\n         h[u'newkey'] = u'newval'\n         for k, v in h.iteritems():\n-            self.assertTrue(isinstance(k, bytes))\n+            self.assertIsInstance(k, bytes)\n             for s in v:\n-                self.assertTrue(isinstance(s, bytes))\n+                self.assertIsInstance(s, bytes)\n \n     def test_eq(self):\n         url = 'http://www.scrapy.org'\n\n@@ -437,7 +437,7 @@ class ProcessorsTest(unittest.TestCase):\n         self.assertRaises(TypeError, proc, [None, '', 'hello', 'world'])\n         self.assertEqual(proc(['', 'hello', 'world']), u' hello world')\n         self.assertEqual(proc(['hello', 'world']), u'hello world')\n-        self.assertTrue(isinstance(proc(['hello', 'world']), six.text_type))\n+        self.assertIsInstance(proc(['hello', 'world']), six.text_type)\n \n     def test_compose(self):\n         proc = Compose(lambda v: v[0], str.upper)\n@@ -482,7 +482,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n     def test_constructor_with_selector(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n-        self.assertTrue(l.selector is sel)\n+        self.assertIs(l.selector, sel)\n \n         l.add_xpath('name', '//div/text()')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n@@ -490,7 +490,7 @@ class SelectortemLoaderTest(unittest.TestCase):\n     def test_constructor_with_selector_css(self):\n         sel = Selector(text=u\"<html><body><div>marta</div></body></html>\")\n         l = TestItemLoader(selector=sel)\n-        self.assertTrue(l.selector is sel)\n+        self.assertIs(l.selector, sel)\n \n         l.add_css('name', 'div::text')\n         self.assertEqual(l.get_output_value('name'), [u'Marta'])\n\n@@ -156,7 +156,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d = {'a': 123, u'b': b'c', u'd': u'e', object(): u'e'}\n         d2 = stringify_dict(d, keys_only=False)\n         self.assertEqual(d, d2)\n-        self.assertFalse(d is d2)  # shouldn't modify in place\n+        self.assertIsNot(d, d2)  # shouldn't modify in place\n         self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n         self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n \n@@ -166,7 +166,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d = dict(tuples)\n         d2 = stringify_dict(tuples, keys_only=False)\n         self.assertEqual(d, d2)\n-        self.assertFalse(d is d2)  # shouldn't modify in place\n+        self.assertIsNot(d, d2)  # shouldn't modify in place\n         self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()), d2.keys())\n         self.assertFalse(any(isinstance(x, six.text_type) for x in d2.values()))\n \n@@ -175,7 +175,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         d = {'a': 123, u'b': 'c', u'd': u'e', object(): u'e'}\n         d2 = stringify_dict(d)\n         self.assertEqual(d, d2)\n-        self.assertFalse(d is d2)  # shouldn't modify in place\n+        self.assertIsNot(d, d2)  # shouldn't modify in place\n         self.assertFalse(any(isinstance(x, six.text_type) for x in d2.keys()))\n \n     def test_get_func_args(self):\n\n@@ -29,7 +29,7 @@ class SendCatchLogTest(unittest.TestCase):\n         self.assertIn('error_handler', record.getMessage())\n         self.assertEqual(record.levelname, 'ERROR')\n         self.assertEqual(result[0][0], self.error_handler)\n-        self.assertTrue(isinstance(result[0][1], Failure))\n+        self.assertIsInstance(result[0][1], Failure)\n         self.assertEqual(result[1], (self.ok_handler, \"OK\"))\n \n         dispatcher.disconnect(self.error_handler, signal=test_signal)\n\n@@ -326,7 +326,7 @@ class WebClientTestCase(unittest.TestCase):\n         return getPage(self.getURL('notsuchfile')).addCallback(self._cbNoSuchFile)\n \n     def _cbNoSuchFile(self, pageData):\n-        self.assertTrue(b'404 - No Such Resource' in pageData)\n+        self.assertIn(b'404 - No Such Resource', pageData)\n \n     def testFactoryInfo(self):\n         url = self.getURL('file')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1dcea6a9d4615afc463fa9839d002d861aed5274", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1494 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -40,7 +40,7 @@ class Crawler(object):\n         handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n         logging.root.addHandler(handler)\n         if get_scrapy_root_handler() is not None:\n-            # scrapy root handler alread installed: update it with new settings\n+            # scrapy root handler already installed: update it with new settings\n             install_scrapy_root_handler(self.settings)\n         # lambda is assigned to Crawler attribute because this way it is not\n         # garbage collected after leaving __init__ scope\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1968a8ec02913273a99a3137cb419e2649d69a5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1889 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n from scrapy.interfaces import ISpiderLoader\n from scrapy.extension import ExtensionManager\n-from scrapy.settings import Settings\n+from scrapy.settings import overridden_settings, Settings\n from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n@@ -34,6 +34,9 @@ class Crawler(object):\n         self.settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n \n+        d = dict(overridden_settings(self.settings))\n+        logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n+\n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n \n\n@@ -9,7 +9,7 @@ from twisted.python.failure import Failure\n from twisted.python import log as twisted_log\n \n import scrapy\n-from scrapy.settings import overridden_settings, Settings\n+from scrapy.settings import Settings\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.versions import scrapy_components_versions\n \n@@ -148,8 +148,6 @@ def log_scrapy_info(settings):\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n-    d = dict(overridden_settings(settings))\n-    logger.info(\"Overridden settings: %(settings)r\", {'settings': d})\n \n \n class StreamLogger(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a35a1ad4ad38b2f413528b0184d8709dea2a495", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 392 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -144,7 +144,7 @@ def _get_handler(settings):\n def log_scrapy_info(settings):\n     logger.info(\"Scrapy %(version)s started (bot: %(bot)s)\",\n                 {'version': scrapy.__version__, 'bot': settings['BOT_NAME']})\n-    logger.info(\"Versions: %(versions)s}\",\n+    logger.info(\"Versions: %(versions)s\",\n                 {'versions': \", \".join(\"%s %s\" % (name, version)\n                     for name, version in scrapy_components_versions()\n                     if name != \"Scrapy\")})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f16f040b661f18f2e61a35427199c099bfd2f90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 94 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -191,7 +191,7 @@ htmlhelp_basename = 'Scrapydoc'\n # Grouping the document tree into LaTeX files. List of tuples\n # (source start file, target name, title, author, document class [howto/manual]).\n latex_documents = [\n-  ('index', 'Scrapy.tex', ur'Scrapy Documentation',\n+  ('index', 'Scrapy.tex', u'Scrapy Documentation',\n    ur'Scrapy developers', 'manual'),\n ]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b7022360824cd1a8aa19fb0d65ea01700f06a208", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ _filename = None\n _contents = None\n \n # A regex that matches standard linkcheck output lines\n-line_re = re.compile(ur'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n+line_re = re.compile(u'(.*)\\:\\d+\\:\\s\\[(.*)\\]\\s(?:(.*)\\sto\\s(.*)|(.*))')\n \n # Read lines from the linkcheck output file\n try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b8fabeed8652d22725959345700b9e7d00073de4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 96 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -192,7 +192,7 @@ htmlhelp_basename = 'Scrapydoc'\n # (source start file, target name, title, author, document class [howto/manual]).\n latex_documents = [\n   ('index', 'Scrapy.tex', u'Scrapy Documentation',\n-   ur'Scrapy developers', 'manual'),\n+   u'Scrapy developers', 'manual'),\n ]\n \n # The name of an image file (relative to this directory) to place at the top of\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abaf466bb311f6416a58763ad7974825d88f4855", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 13 | Churn Cumulative: 191 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,8 +23,11 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         if opts.verbose:\n-            for name, version in scrapy_components_versions():\n-                print(\"%-9s : %s\" % (name, version))\n+            versions = scrapy_components_versions()\n+            width = max(len(n) for (n, _) in versions)\n+            patt = \"%-{}s : %s\".format(width)\n+            for name, version in versions:\n+                print(patt % (name, version))\n         else:\n             print(\"Scrapy %s\" % scrapy.__version__)\n \n\n@@ -17,6 +17,11 @@ def scrapy_components_versions():\n         w3lib_version = w3lib.__version__\n     except AttributeError:\n         w3lib_version = \"<1.14.3\"\n+    try:\n+        import cryptography\n+        cryptography_version = cryptography.__version__\n+    except ImportError:\n+        cryptography_version = \"unknown\"\n \n     return [\n         (\"Scrapy\", scrapy.__version__),\n@@ -28,6 +33,7 @@ def scrapy_components_versions():\n         (\"Twisted\", twisted.version.short()),\n         (\"Python\", sys.version.replace(\"\\n\", \"- \")),\n         (\"pyOpenSSL\", _get_openssl_version()),\n+        (\"cryptography\", cryptography_version),\n         (\"Platform\",  platform.platform()),\n     ]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d97f49e5e31955fb972dc63106298032ebe0e59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 803 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -164,7 +164,11 @@ class Shell(object):\n \n def inspect_response(response, spider):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n+    # Shell.start removes the SIGINT handler, so save it and re-add it after\n+    # the shell has closed\n+    sigint_handler = signal.getsignal(signal.SIGINT)\n     Shell(spider.crawler).start(response=response, spider=spider)\n+    signal.signal(signal.SIGINT, sigint_handler)\n \n \n def _request_deferred(request):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aab98080a06281ca3a88646990b81b92f492c517", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1506 | Contributors (this commit): 14 | Commits (past 90d): 3 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -237,15 +237,18 @@ class CrawlerProcess(CrawlerRunner):\n     The CrawlerProcess object must be instantiated with a\n     :class:`~scrapy.settings.Settings` object.\n \n+    :param install_root_handler: whether to install root logging handler\n+        (default: True)\n+\n     This class shouldn't be needed (since Scrapy is responsible of using it\n     accordingly) unless writing scripts that manually handle the crawling\n     process. See :ref:`run-from-script` for an example.\n     \"\"\"\n \n-    def __init__(self, settings=None):\n+    def __init__(self, settings=None, install_root_handler=True):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n-        configure_logging(self.settings)\n+        configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n \n     def _signal_shutdown(self, signum, _):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d71a0634039d637dc10509ebb63fa2f4ef595ebb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 39 | Churn Cumulative: 290 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -194,6 +194,41 @@ class S3FilesStore(object):\n         return extra\n \n \n+class GCSFilesStore(object):\n+\n+    GCS_PROJECT_ID = None\n+\n+    CACHE_CONTROL = 'max-age=172800'\n+\n+    def __init__(self, uri):\n+        from google.cloud import storage\n+        client = storage.Client(project=self.GCS_PROJECT_ID)\n+        bucket, prefix = uri[5:].split('/', 1)\n+        self.bucket = client.bucket(bucket)\n+        self.prefix = prefix\n+\n+    def stat_file(self, path, info):\n+        def _onsuccess(blob):\n+            if blob:\n+                checksum = blob.md5_hash\n+                last_modified = time.mktime(blob.updated.timetuple())\n+                return {'checksum': checksum, 'last_modified': last_modified}\n+            else:\n+                return {}\n+\n+        return threads.deferToThread(self.bucket.get_blob, path).addCallback(_onsuccess)\n+\n+    def persist_file(self, path, buf, info, meta=None, headers=None):\n+        blob = self.bucket.blob(self.prefix + path)\n+        blob.cache_control = self.CACHE_CONTROL\n+        blob.metadata = {k: str(v) for k, v in six.iteritems(meta or {})}\n+        return threads.deferToThread(\n+            blob.upload_from_string,\n+            data=buf.getvalue(),\n+            content_type='application/octet-stream'\n+        )\n+\n+\n class FilesPipeline(MediaPipeline):\n     \"\"\"Abstract pipeline that implement the file downloading\n \n@@ -219,6 +254,7 @@ class FilesPipeline(MediaPipeline):\n         '': FSFilesStore,\n         'file': FSFilesStore,\n         's3': S3FilesStore,\n+        'gs': GCSFilesStore,\n     }\n     DEFAULT_FILES_URLS_FIELD = 'file_urls'\n     DEFAULT_FILES_RESULT_FIELD = 'files'\n@@ -258,6 +294,9 @@ class FilesPipeline(MediaPipeline):\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n         s3store.POLICY = settings['FILES_STORE_S3_ACL']\n \n+        gcs_store = cls.STORE_SCHEMES['gs']\n+        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n+\n         store_uri = settings['FILES_STORE']\n         return cls(store_uri, settings=settings)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5d4364b2a0e7ae205605905ac0c5ac6fd8d15db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 43 | Churn Cumulative: 604 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,12 @@ def assert_aws_environ():\n     if 'AWS_ACCESS_KEY_ID' not in os.environ:\n         raise SkipTest(\"AWS keys not found\")\n \n+\n+def assert_gcs_environ():\n+    if 'GCS_PROJECT_ID' not in os.environ:\n+        raise SkipTest(\"GCS_PROJECT_ID not found\")\n+\n+\n def skip_if_no_boto():\n     try:\n         is_botocore()\n@@ -45,6 +51,15 @@ def get_s3_content_and_delete(bucket, path, with_key=False):\n         bucket.delete_key(path)\n     return (content, key) if with_key else content\n \n+def get_gcs_content_and_delete(bucket, path):\n+    from google.cloud import storage\n+    client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n+    bucket = client.get_bucket(bucket)\n+    blob = bucket.get_blob(path)\n+    content = blob.download_as_string()\n+    bucket.delete_blob(path)\n+    return content, blob\n+\n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n\n@@ -11,12 +11,13 @@ from six import BytesIO\n from twisted.trial import unittest\n from twisted.internet import defer\n \n-from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore\n+from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GCSFilesStore\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n+from scrapy.utils.test import assert_gcs_environ, get_gcs_content_and_delete\n from scrapy.utils.boto import is_botocore\n \n from tests import mock\n@@ -375,6 +376,31 @@ class TestS3FilesStore(unittest.TestCase):\n             self.assertEqual(key.content_type, 'image/png')\n \n \n+class TestGCSFilesStore(unittest.TestCase):\n+    @defer.inlineCallbacks\n+    def test_persist(self):\n+        assert_gcs_environ()\n+        uri = os.environ.get('GCS_TEST_FILE_URI')\n+        if not uri:\n+            raise unittest.SkipTest(\"No GCS URI available for testing\")\n+        data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n+        buf = BytesIO(data)\n+        meta = {'foo': 'bar'}\n+        path = 'full/filename'\n+        store = GCSFilesStore(uri)\n+        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n+        s = yield store.stat_file(path, info=None)\n+        self.assertIn('last_modified', s)\n+        self.assertIn('checksum', s)\n+        self.assertEqual(s['checksum'], 'zc2oVgXkbQr2EQdSdw3OPA==')\n+        u = urlparse(uri)\n+        content, blob = get_gcs_content_and_delete(u.hostname, u.path[1:]+path)\n+        self.assertEqual(content, data)\n+        self.assertEqual(blob.metadata, {'foo': 'bar'})\n+        self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)\n+        self.assertEqual(blob.content_type, 'application/octet-stream')\n+\n+\n class ItemWithFiles(Item):\n     file_urls = Field()\n     files = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee166ec44f38da7f5b99c6c164a7c6ff02b37c16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 508 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -218,6 +218,12 @@ class GCSFilesStore(object):\n \n         return threads.deferToThread(self.bucket.get_blob, path).addCallback(_onsuccess)\n \n+    def _get_content_type(self, headers):\n+        if headers and 'Content-Type' in headers:\n+            return headers['Content-Type']\n+        else:\n+            return 'application/octet-stream'\n+\n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         blob = self.bucket.blob(self.prefix + path)\n         blob.cache_control = self.CACHE_CONTROL\n@@ -225,7 +231,7 @@ class GCSFilesStore(object):\n         return threads.deferToThread(\n             blob.upload_from_string,\n             data=buf.getvalue(),\n-            content_type='application/octet-stream'\n+            content_type=self._get_content_type(headers)\n         )\n \n \n\n@@ -91,6 +91,9 @@ class ImagesPipeline(FilesPipeline):\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n         s3store.POLICY = settings['IMAGES_STORE_S3_ACL']\n \n+        gcs_store = cls.STORE_SCHEMES['gs']\n+        gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n+\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcb279bd6cc85cf1743b548e44b050edba6a2ed8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 59 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ IGNORED_EXTENSIONS = [\n \n     # video\n     '3gp', 'asf', 'asx', 'avi', 'mov', 'mp4', 'mpg', 'qt', 'rm', 'swf', 'wmv',\n-    'm4a',\n+    'm4a', 'm4v',\n \n     # office suites\n     'xls', 'xlsx', 'ppt', 'pptx', 'pps', 'doc', 'docx', 'odt', 'ods', 'odg',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e733f51d4b04f209bfec32d1bd7559a258f45d0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 26 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,4 +28,4 @@ class VersionTest(ProcessTest, unittest.TestCase):\n         self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2',\n                                    'cssselect', 'parsel', 'w3lib',\n                                    'Twisted', 'Python', 'pyOpenSSL',\n-                                   'Platform'])\n+                                   'cryptography', 'Platform'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59c3f6f095d7605a825a90f376d41493a78e5da7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 134 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         crawler = Crawler(spider, settings)\n         self.assertRaises(NotConfigured, partial(HttpProxyMiddleware.from_crawler, crawler))\n \n-    def test_no_enviroment_proxies(self):\n+    def test_no_environment_proxies(self):\n         os.environ = {'dummy_proxy': 'reset_env_and_do_not_raise'}\n         mw = HttpProxyMiddleware()\n \n@@ -38,7 +38,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n             self.assertEqual(req.url, url)\n             self.assertEqual(req.meta, {})\n \n-    def test_enviroment_proxies(self):\n+    def test_environment_proxies(self):\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n         os.environ['https_proxy'] = https_proxy = 'http://proxy.for.https:8080'\n         os.environ.pop('file_proxy', None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc406801f1783392935fcd7faf603d8339c74675", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 237 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,10 +5,10 @@ See documentation in docs/topics/email.rst\n \"\"\"\n import logging\n \n-from six.moves import cStringIO as StringIO\n import six\n \n from email.utils import COMMASPACE, formatdate\n+from io import BytesIO\n from six.moves.email_mime_multipart import MIMEMultipart\n from six.moves.email_mime_text import MIMEText\n from six.moves.email_mime_base import MIMEBase\n@@ -21,6 +21,14 @@ else:\n \n from twisted.internet import defer, reactor, ssl\n \n+try:\n+    from twisted.mail.smtp import ESMTPSenderFactory\n+except ImportError:\n+    \"\"\"\n+    twisted.mail.smtp was not available in\n+    older versions of Twisted on Python 3\n+    \"\"\"\n+\n from .utils.misc import arg_to_iter\n \n logger = logging.getLogger(__name__)\n@@ -110,9 +118,7 @@ class MailSender(object):\n                       'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n-        # Import twisted.mail here because it is not available in python3\n-        from twisted.mail.smtp import ESMTPSenderFactory\n-        msg = StringIO(msg)\n+        msg = BytesIO(msg.encode('utf-8'))\n         d = defer.Deferred()\n         factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n             to_addrs, msg, d, heloFallback=True, requireAuthentication=False, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12c7628fcbcfb3927f595e1fd6806ea6aefdf6fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 256 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,10 +5,13 @@ See documentation in docs/topics/email.rst\n \"\"\"\n import logging\n \n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n import six\n \n from email.utils import COMMASPACE, formatdate\n-from io import BytesIO\n from six.moves.email_mime_multipart import MIMEMultipart\n from six.moves.email_mime_text import MIMEText\n from six.moves.email_mime_base import MIMEBase\n@@ -21,14 +24,6 @@ else:\n \n from twisted.internet import defer, reactor, ssl\n \n-try:\n-    from twisted.mail.smtp import ESMTPSenderFactory\n-except ImportError:\n-    \"\"\"\n-    twisted.mail.smtp was not available in\n-    older versions of Twisted on Python 3\n-    \"\"\"\n-\n from .utils.misc import arg_to_iter\n \n logger = logging.getLogger(__name__)\n@@ -96,7 +91,7 @@ class MailSender(object):\n                           'mailattachs': len(attachs)})\n             return\n \n-        dfd = self._sendmail(rcpts, msg.as_string())\n+        dfd = self._sendmail(rcpts, msg.as_string().encode(charset or 'utf-8'))\n         dfd.addCallbacks(self._sent_ok, self._sent_failed,\n             callbackArgs=[to, cc, subject, len(attachs)],\n             errbackArgs=[to, cc, subject, len(attachs)])\n@@ -118,7 +113,9 @@ class MailSender(object):\n                       'mailattachs': nattachs, 'mailerr': errstr})\n \n     def _sendmail(self, to_addrs, msg):\n-        msg = BytesIO(msg.encode('utf-8'))\n+        # Import twisted.mail here because it is not available in python3\n+        from twisted.mail.smtp import ESMTPSenderFactory\n+        msg = BytesIO(msg)\n         d = defer.Deferred()\n         factory = ESMTPSenderFactory(self.smtpuser, self.smtppass, self.mailfrom, \\\n             to_addrs, msg, d, heloFallback=True, requireAuthentication=False, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80bb4fcf9710598138d7604190636d723d6392df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 263 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,8 @@ else:\n \n from twisted.internet import defer, reactor, ssl\n \n-from .utils.misc import arg_to_iter\n+from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.python import to_bytes\n \n logger = logging.getLogger(__name__)\n \n@@ -35,8 +36,8 @@ class MailSender(object):\n             smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n         self.smtphost = smtphost\n         self.smtpport = smtpport\n-        self.smtpuser = smtpuser\n-        self.smtppass = smtppass\n+        self.smtpuser = to_bytes(smtpuser)\n+        self.smtppass = to_bytes(smtppass)\n         self.smtptls = smtptls\n         self.smtpssl = smtpssl\n         self.mailfrom = mailfrom\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cd348d94af782008f1a561a0b36da9231878833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 267 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -36,8 +36,8 @@ class MailSender(object):\n             smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n         self.smtphost = smtphost\n         self.smtpport = smtpport\n-        self.smtpuser = to_bytes(smtpuser)\n-        self.smtppass = to_bytes(smtppass)\n+        self.smtpuser = to_bytes(smtpuser) if smtpuser is not None else None\n+        self.smtppass = to_bytes(smtppass) if smtppass is not None else None\n         self.smtptls = smtptls\n         self.smtpssl = smtpssl\n         self.mailfrom = mailfrom\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f729d74886be2290fcbbcaa21d366b770ff21008", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 277 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,14 +30,20 @@ from scrapy.utils.python import to_bytes\n logger = logging.getLogger(__name__)\n \n \n+def _to_bytes_or_none(text):\n+    if text is None:\n+        return None\n+    return to_bytes(text)\n+\n+\n class MailSender(object):\n \n     def __init__(self, smtphost='localhost', mailfrom='scrapy@localhost',\n             smtpuser=None, smtppass=None, smtpport=25, smtptls=False, smtpssl=False, debug=False):\n         self.smtphost = smtphost\n         self.smtpport = smtpport\n-        self.smtpuser = to_bytes(smtpuser) if smtpuser is not None else None\n-        self.smtppass = to_bytes(smtppass) if smtppass is not None else None\n+        self.smtpuser = _to_bytes_or_none(smtpuser)\n+        self.smtppass = _to_bytes_or_none(smtppass)\n         self.smtptls = smtptls\n         self.smtpssl = smtpssl\n         self.mailfrom = mailfrom\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e914556adf8e556d4184db415c49266cc4c91bf5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 5 | Churn Cumulative: 1575 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -412,9 +412,10 @@ class _ResponseReader(protocol.Protocol):\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n-                         \"max size (%(maxsize)s).\",\n+                         \"max size (%(maxsize)s) in request %(request)s.\",\n                          {'bytes': self._bytes_received,\n-                          'maxsize': self._maxsize})\n+                          'maxsize': self._maxsize,\n+                          'request': self._request})\n             # Clear buffer earlier to avoid keeping data in memory for a long\n             # time.\n             self._bodybuf.truncate(0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#938bc18405ca2cf60836bb8490c391c0fe445af1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1584 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -344,8 +344,8 @@ class ScrapyAgent(object):\n \n         if warnsize and expected_size > warnsize:\n             logger.warning(\"Expected response size (%(size)s) larger than \"\n-                           \"download warn size (%(warnsize)s).\",\n-                           {'size': expected_size, 'warnsize': warnsize})\n+                           \"download warn size (%(warnsize)s) in request (%(request)s).\",\n+                           {'size': expected_size, 'warnsize': warnsize, 'request': request})\n \n         def _cancel(_):\n             # Abort connection inmediately.\n@@ -412,10 +412,9 @@ class _ResponseReader(protocol.Protocol):\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n-                         \"max size (%(maxsize)s) in request %(request)s.\",\n+                         \"max size (%(maxsize)s).\",\n                          {'bytes': self._bytes_received,\n-                          'maxsize': self._maxsize,\n-                          'request': self._request})\n+                          'maxsize': self._maxsize})\n             # Clear buffer earlier to avoid keeping data in memory for a long\n             # time.\n             self._bodybuf.truncate(0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#345d948f2f55ad81ec7de7cb1f5619d80971a6c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1586 | Contributors (this commit): 17 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -344,7 +344,7 @@ class ScrapyAgent(object):\n \n         if warnsize and expected_size > warnsize:\n             logger.warning(\"Expected response size (%(size)s) larger than \"\n-                           \"download warn size (%(warnsize)s) in request (%(request)s).\",\n+                           \"download warn size (%(warnsize)s) in request %(request)s.\",\n                            {'size': expected_size, 'warnsize': warnsize, 'request': request})\n \n         def _cancel(_):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b8503011e1da0507e82e15631194ed99b7e699a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1591 | Contributors (this commit): 17 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -412,9 +412,10 @@ class _ResponseReader(protocol.Protocol):\n \n         if self._maxsize and self._bytes_received > self._maxsize:\n             logger.error(\"Received (%(bytes)s) bytes larger than download \"\n-                         \"max size (%(maxsize)s).\",\n+                         \"max size (%(maxsize)s) in request %(request)s.\",\n                          {'bytes': self._bytes_received,\n-                          'maxsize': self._maxsize})\n+                          'maxsize': self._maxsize,\n+                          'request': self._request})\n             # Clear buffer earlier to avoid keeping data in memory for a long\n             # time.\n             self._bodybuf.truncate(0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9d9d83a8c31b6a18d7aaac35a30ffb69db4bb81d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 24 | Files Changed: 17 | Hunks: 22 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 5258 | Contributors (this commit): 48 | Commits (past 90d): 24 | Contributors (cumulative): 125 | DMM Complexity: None\n\nDIFF:\n@@ -4,12 +4,12 @@ from six.moves import copyreg\n if sys.version_info[0] == 2:\n     from urlparse import urlparse\n \n-    # workaround for http://bugs.python.org/issue7904 - Python < 2.7\n+    # workaround for https://bugs.python.org/issue7904 - Python < 2.7\n     if urlparse('s3://bucket/key').netloc != 'bucket':\n         from urlparse import uses_netloc\n         uses_netloc.append('s3')\n \n-    # workaround for http://bugs.python.org/issue9374 - Python < 2.7.4\n+    # workaround for https://bugs.python.org/issue9374 - Python < 2.7.4\n     if urlparse('s3://bucket/key?key=value').query != 'key=value':\n         from urlparse import uses_query\n         uses_query.append('s3')\n\n@@ -64,7 +64,7 @@ if twisted_version >= (14, 0, 0):\n         \"\"\"\n         Twisted-recommended context factory for web clients.\n \n-        Quoting http://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html:\n+        Quoting https://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html:\n         \"The default is to use a BrowserLikePolicyForHTTPS,\n         so unless you have special requirements you can leave this as-is.\"\n \n@@ -100,6 +100,6 @@ else:\n         def getContext(self, hostname=None, port=None):\n             ctx = ClientContextFactory.getContext(self)\n             # Enable all workarounds to SSL bugs as documented by\n-            # http://www.openssl.org/docs/ssl/SSL_CTX_set_options.html\n+            # https://www.openssl.org/docs/manmaster/man3/SSL_CTX_set_options.html\n             ctx.set_options(SSL.OP_ALL)\n             return ctx\n\n@@ -83,7 +83,7 @@ class Crawler(object):\n             yield defer.maybeDeferred(self.engine.start)\n         except Exception:\n             # In Python 2 reraising an exception after yield discards\n-            # the original traceback (see http://bugs.python.org/issue7563),\n+            # the original traceback (see https://bugs.python.org/issue7563),\n             # so sys.exc_info() workaround is used.\n             # This workaround also works in Python 3, but it is not needed,\n             # and it is slower, so in Python 3 we use native `raise`.\n\n@@ -11,7 +11,7 @@ warnings.warn(\"Module `scrapy.downloadermiddlewares.chunked` is deprecated, \"\n \n class ChunkedTransferMiddleware(object):\n     \"\"\"This middleware adds support for chunked transfer encoding, as\n-    documented in: http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n+    documented in: https://en.wikipedia.org/wiki/Chunked_transfer_encoding\n     \"\"\"\n \n     def process_response(self, request, response, spider):\n\n@@ -75,7 +75,7 @@ class HttpCacheMiddleware(object):\n             return response\n \n         # RFC2616 requires origin server to set Date header,\n-        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n+        # https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.18\n         if 'Date' not in response.headers:\n             response.headers['Date'] = formatdate(usegmt=1)\n \n\n@@ -188,7 +188,7 @@ class XmlItemExporter(BaseItemExporter):\n         self.xg.endElement(name)\n         self._beautify_newline()\n \n-    # Workaround for http://bugs.python.org/issue17606\n+    # Workaround for https://bugs.python.org/issue17606\n     # Before Python 2.7.4 xml.sax.saxutils required bytes;\n     # since 2.7.4 it requires unicode. The bug is likely to be\n     # fixed in 2.7.6, but 2.7.6 will still support unicode,\n\n@@ -70,8 +70,8 @@ class RFC2616Policy(object):\n         return True\n \n     def should_cache_response(self, response, request):\n-        # What is cacheable - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\n-        # Response cacheability - http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n+        # What is cacheable - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec14.9.1\n+        # Response cacheability - https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.4\n         # Status code 206 is not included because cache can not deal with partial contents\n         cc = self._parse_cachecontrol(response)\n         # obey directive \"Cache-Control: no-store\"\n@@ -163,7 +163,7 @@ class RFC2616Policy(object):\n \n     def _compute_freshness_lifetime(self, response, request, now):\n         # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n-        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#410\n+        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#706\n         cc = self._parse_cachecontrol(response)\n         maxage = self._get_max_age(cc)\n         if maxage is not None:\n@@ -194,7 +194,7 @@ class RFC2616Policy(object):\n \n     def _compute_current_age(self, response, request, now):\n         # Reference nsHttpResponseHead::ComputeCurrentAge\n-        # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#366\n+        # https://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#658\n         currentage = 0\n         # If Date header is not set we assume it is a fast connection, and\n         # clock is in sync with the server\n@@ -414,7 +414,7 @@ class LeveldbCacheStorage(object):\n def parse_cachecontrol(header):\n     \"\"\"Parse Cache-Control header\n \n-    http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n+    https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n \n     >>> parse_cachecontrol(b'public, max-age=3600') == {b'public': None,\n     ...                                                 b'max-age': b'3600'}\n\n@@ -82,7 +82,7 @@ class TelnetConsole(protocol.ServerFactory):\n             'prefs': print_live_refs,\n             'hpy': hpy,\n             'help': \"This is Scrapy telnet console. For more info see: \" \\\n-                \"http://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n+                \"https://doc.scrapy.org/en/latest/topics/telnetconsole.html\",\n         }\n         self.crawler.signals.send_catch_log(update_telnet_vars, telnet_vars=telnet_vars)\n         return telnet_vars\n\n@@ -120,7 +120,7 @@ class S3FilesStore(object):\n \n     def _get_boto_bucket(self):\n         # disable ssl (is_secure=False) because of this python bug:\n-        # http://bugs.python.org/issue5103\n+        # https://bugs.python.org/issue5103\n         c = self.S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n         return c.get_bucket(self.bucket, validate=False)\n \n\n@@ -55,7 +55,7 @@ class SignalManager(object):\n         The keyword arguments are passed to the signal handlers (connected\n         through the :meth:`connect` method).\n \n-        .. _deferreds: http://twistedmatrix.com/documents/current/core/howto/defer.html\n+        .. _deferreds: https://twistedmatrix.com/documents/current/core/howto/defer.html\n         \"\"\"\n         kwargs.setdefault('sender', self.sender)\n         return _signal.send_catch_log_deferred(signal, **kwargs)\n\n@@ -57,7 +57,7 @@ def parallel(iterable, count, callable, *args, **named):\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n-    Taken from: http://jcalderone.livejournal.com/24285.html\n+    Taken from: https://jcalderone.livejournal.com/24285.html\n     \"\"\"\n     coop = task.Cooperator()\n     work = (callable(elem, *args, **named) for elem in iterable)\n\n@@ -71,8 +71,8 @@ def create_deprecated_class(name, new_class, clsdict=None,\n                 warnings.warn(msg, warn_category, stacklevel=2)\n             super(DeprecatedClass, cls).__init__(name, bases, clsdict_)\n \n-        # see http://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n-        # and http://docs.python.org/2/reference/datamodel.html#customizing-instance-and-subclass-checks\n+        # see https://www.python.org/dev/peps/pep-3119/#overloading-isinstance-and-issubclass\n+        # and https://docs.python.org/reference/datamodel.html#customizing-instance-and-subclass-checks\n         # for implementation details\n         def __instancecheck__(cls, inst):\n             return any(cls.__subclasscheck__(c)\n\n@@ -11,7 +11,7 @@ def decode_chunked_transfer(chunked_body):\n     decoded body.\n \n     For more info see:\n-    http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n+    https://en.wikipedia.org/wiki/Chunked_transfer_encoding\n \n     \"\"\"\n     body, h, t = '', '', chunked_body\n\n@@ -154,7 +154,7 @@ class StreamLogger(object):\n     \"\"\"Fake file-like stream object that redirects writes to a logger instance\n \n     Taken from:\n-        http://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n+        https://www.electricmonk.nl/log/2011/08/14/redirect-stdout-and-stderr-to-a-logger-in-python/\n     \"\"\"\n     def __init__(self, logger, log_level=logging.INFO):\n         self.logger = logger\n\n@@ -47,7 +47,7 @@ def parse_url(url, encoding=None):\n def escape_ajax(url):\n     \"\"\"\n     Return the crawleable url according to:\n-    http://code.google.com/web/ajaxcrawling/docs/getting-started.html\n+    https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n \n     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n     'www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue'\n\n@@ -29,7 +29,7 @@ if has_environment_marker_platform_impl_support():\n setup(\n     name='Scrapy',\n     version=version,\n-    url='http://scrapy.org',\n+    url='https://scrapy.org',\n     description='A high-level Web Crawling and Web Scraping framework',\n     long_description=open('README.rst').read(),\n     author='Scrapy developers',\n\n@@ -1,7 +1,7 @@\n \"\"\"\n tests: this package contains all Scrapy unittests\n \n-see http://doc.scrapy.org/en/latest/contributing.html#running-tests\n+see https://doc.scrapy.org/en/latest/contributing.html#running-tests\n \"\"\"\n \n import os\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
