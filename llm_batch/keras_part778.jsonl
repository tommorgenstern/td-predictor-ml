{"custom_id": "keras#dfec3efb53798fc2a96deefe32a998804973d60e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 42 | Churn Cumulative: 7301 | Contributors (this commit): 8 | Commits (past 90d): 55 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1568,26 +1568,34 @@ def tri(N, M=None, k=0, dtype=None):\n \n def tril(x, k=0):\n     x = convert_to_tensor(x)\n+\n     if k >= 0:\n         return tf.linalg.band_part(x, -1, k)\n \n-    # deal with negative k using mask\n-    k = -k - 1\n-    mask = tf.ones_like(x, dtype=\"bool\")\n-    mask = tf.logical_not(tf.linalg.band_part(mask, k, -1))\n-    return tf.where(mask, x, tf.constant(0, x.dtype))\n+    shape = tf.shape(x)\n+    rows, cols = shape[-2], shape[-1]\n+\n+    i, j = tf.meshgrid(tf.range(rows), tf.range(cols), indexing=\"ij\")\n+\n+    mask = i >= j - k\n+\n+    return tf.where(tf.broadcast_to(mask, shape), x, tf.zeros_like(x))\n \n \n def triu(x, k=0):\n     x = convert_to_tensor(x)\n-    if k >= 0:\n-        return tf.linalg.band_part(x, k, -1)\n \n-    # deal with negative k using mask\n-    k = -k\n-    mask = tf.ones_like(x, dtype=\"bool\")\n-    mask = tf.logical_not(tf.linalg.band_part(mask, k, -1))\n-    return tf.where(mask, tf.constant(0, x.dtype), x)\n+    if k <= 0:\n+        return tf.linalg.band_part(x, -k, -1)\n+\n+    shape = tf.shape(x)\n+    rows, cols = shape[-2], shape[-1]\n+\n+    i, j = tf.meshgrid(tf.range(rows), tf.range(cols), indexing=\"ij\")\n+\n+    mask = i <= j - k\n+\n+    return tf.where(tf.broadcast_to(mask, shape), x, tf.zeros_like(x))\n \n \n def vdot(x1, x2):\n\n@@ -3881,6 +3881,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.tril(x, -1), np.tril(x, -1))\n         self.assertAllClose(knp.Tril(-1)(x), np.tril(x, -1))\n \n+        x = np.ones([5, 5])\n+        self.assertAllClose(knp.tril(x), np.tril(x))\n+        self.assertAllClose(knp.tril(x, -1), np.tril(x, -1))\n+        self.assertAllClose(knp.Tril(-1)(x), np.tril(x, -1))\n+\n     def test_tril_in_layer(self):\n         # https://github.com/keras-team/keras/issues/18890\n         x = keras.Input((None, 3))\n@@ -3908,6 +3913,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(knp.triu(x, -1), np.triu(x, -1))\n         self.assertAllClose(knp.Triu(-1)(x), np.triu(x, -1))\n \n+        x = np.ones([5, 5])\n+        self.assertAllClose(knp.triu(x), np.triu(x))\n+        self.assertAllClose(knp.triu(x, -1), np.triu(x, -1))\n+        self.assertAllClose(knp.Triu(-1)(x), np.triu(x, -1))\n+\n     def test_triu_in_layer(self):\n         # https://github.com/keras-team/keras/issues/18890\n         x = keras.Input((None, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c42023ded39a806533854e4c94a13916196e604", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 3618 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -321,7 +321,7 @@ class Precision(Metric):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='binary_crossentropy',\n                   metrics=[keras.metrics.Precision()])\n     ```\n \n@@ -465,7 +465,7 @@ class Recall(Metric):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='binary_crossentropy',\n                   metrics=[keras.metrics.Recall()])\n     ```\n \n@@ -728,7 +728,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n     ```python\n     model.compile(\n         optimizer='sgd',\n-        loss='mse',\n+        loss='binary_crossentropy',\n         metrics=[keras.metrics.SensitivityAtSpecificity()])\n     ```\n     \"\"\"\n@@ -832,7 +832,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n     ```python\n     model.compile(\n         optimizer='sgd',\n-        loss='mse',\n+        loss='binary_crossentropy',\n         metrics=[keras.metrics.SpecificityAtSensitivity()])\n     ```\n     \"\"\"\n@@ -927,7 +927,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n     ```python\n     model.compile(\n         optimizer='sgd',\n-        loss='mse',\n+        loss='binary_crossentropy',\n         metrics=[keras.metrics.PrecisionAtRecall(recall=0.8)])\n     ```\n     \"\"\"\n@@ -1017,7 +1017,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n     ```python\n     model.compile(\n         optimizer='sgd',\n-        loss='mse',\n+        loss='binary_crossentropy',\n         metrics=[keras.metrics.RecallAtPrecision(precision=0.8)])\n     ```\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23d5eebd5c1641d9fafbe68985c462f3abc3f2a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 128 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -219,6 +219,11 @@ def compute_conv_output_shape(\n                 )\n     elif padding == \"same\" or padding == \"causal\":\n         output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n+    else:\n+        raise ValueError(\n+            \"`padding` must be either `'valid'` or `'same'`. Received \"\n+            f\"{padding}.\"\n+        )\n     output_spatial_shape = [int(i) for i in output_spatial_shape]\n     for i in none_dims:\n         output_spatial_shape[i] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4dd4fab5bd9491b32b4ab1e360d290ad8e8a238", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 132 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -136,8 +136,8 @@ def compute_pooling_output_shape(\n         output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n     else:\n         raise ValueError(\n-            \"`padding` must be either `'valid'` or `'same'`. Received \"\n-            f\"{padding}.\"\n+            \"Argument `padding` must be either 'valid' or 'same'. Received: \"\n+            f\"padding={padding}\"\n         )\n     output_spatial_shape = [int(i) for i in output_spatial_shape]\n     for i in none_dims:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9815ac1c81f0f24cf3ca24638e4ab11c5fc95bd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 63 | Files Changed: 4 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): -2/1 | Churn Δ: 95 | Churn Cumulative: 6130 | Contributors (this commit): 17 | Commits (past 90d): 35 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -505,23 +505,12 @@ def moments(x, axes, keepdims=False, synchronized=False):\n     # to float16\n     need_cast = False\n     ori_dtype = standardize_dtype(x.dtype)\n-    if ori_dtype == \"float16\":\n+    if ori_dtype in (\"float16\", \"bfloat16\"):\n         need_cast = True\n         x = cast(x, \"float32\")\n \n     mean = jnp.mean(x, axes, keepdims=True)\n-\n-    # The variance is computed using $Var = E[|x|^2] - |E[x]|^2$, It is faster\n-    # but less numerically stable.\n-    # Note: stop_gradient does not change the gradient to the mean, because that\n-    # gradient is zero.\n-    # The substraction operation does not guarantee a non-negative\n-    # result given float precision, so we clamp it to 0.\n-    variance = jnp.maximum(\n-        jnp.mean(jnp.square(x), axis=axes, keepdims=True)\n-        - jnp.square(jax.lax.stop_gradient(mean)),\n-        0.0,\n-    )\n+    variance = jnp.var(x, axis=axes, keepdims=True)\n \n     if not keepdims:\n         mean = jnp.squeeze(mean, axes)\n\n@@ -683,26 +683,37 @@ def binary_crossentropy(target, output, from_logits=False):\n \n \n def moments(x, axes, keepdims=False, synchronized=False):\n+    # The dynamic range of float16 is too limited for statistics. As a\n+    # workaround, we simply perform the operations on float32 and convert back\n+    # to float16\n+    need_cast = False\n+    ori_dtype = standardize_dtype(x.dtype)\n+    if ori_dtype in (\"float16\", \"bfloat16\"):\n+        need_cast = True\n+        x = cast(x, \"float32\")\n+\n     if synchronized:\n-        return _compute_moments_sync(x, axes, keepdims)\n+        mean, variance = _compute_moments_sync(x, axes, keepdims)\n     else:\n-        return _compute_moments(x, axes, keepdims)\n+        mean, variance = _compute_moments(x, axes, keepdims)\n+    if need_cast:\n+        # avoid overflow and underflow when casting from float16 to float32\n+        mean = tf.clip_by_value(mean, tf.float16.min, tf.float16.max)\n+        variance = tf.clip_by_value(variance, tf.float16.min, tf.float16.max)\n+        mean = cast(mean, ori_dtype)\n+        variance = cast(variance, ori_dtype)\n+    return mean, variance\n \n \n def _compute_moments_sync(x, axes, keepdims):\n-    # The dynamic range of fp16 is too limited to support the collection\n-    # of sufficient statistics. As a workaround we simply perform the\n-    # operations on 32-bit floats before converting the mean and\n-    # variance back to fp16\n-    y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n     replica_ctx = tf.distribute.get_replica_context()\n     if not replica_ctx:\n         return _compute_moments(x, axes, keepdims)\n \n-    local_count = tf.ones_like(y, name=\"count\")\n+    local_count = tf.ones_like(x, name=\"count\")\n \n-    local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n-    local_squared_sum = tf.reduce_sum(tf.square(y), axis=axes, keepdims=True)\n+    local_sum = tf.reduce_sum(x, axis=axes, keepdims=True)\n+    local_squared_sum = tf.reduce_sum(tf.square(x), axis=axes, keepdims=True)\n     local_count = tf.reduce_sum(local_count, axis=axes, keepdims=True)\n \n     # TODO(b/163099951): batch the all-reduces once we sort out the\n@@ -722,48 +733,12 @@ def _compute_moments_sync(x, axes, keepdims):\n     if not keepdims:\n         mean = tf.squeeze(mean, axes)\n         variance = tf.squeeze(variance, axes)\n-    if x.dtype == tf.float16:\n-        return (\n-            tf.cast(mean, tf.float16),\n-            tf.cast(variance, tf.float16),\n-        )\n+\n     return mean, variance\n \n \n def _compute_moments(x, axes, keepdims):\n-    # The dynamic range of float16 is too limited for statistics. As a\n-    # workaround, we simply perform the operations on float32 and convert back\n-    # to float16\n-    need_cast = False\n-    ori_dtype = standardize_dtype(x.dtype)\n-    if ori_dtype == \"float16\":\n-        need_cast = True\n-        x = cast(x, \"float32\")\n-\n-    mean = tf.reduce_mean(x, axes, keepdims=True)\n-\n-    # The variance is computed using $Var = E[|x|^2] - |E[x]|^2$, It is faster\n-    # but less numerically stable.\n-    # Note: stop_gradient does not change the gradient to the mean, because that\n-    # gradient is zero.\n-    # The substraction operation does not guarantee a non-negative\n-    # result given float precision, so we clamp it to 0.\n-    variance = tf.maximum(\n-        tf.reduce_mean(tf.square(x), axis=axes, keepdims=True)\n-        - tf.square(tf.stop_gradient(mean)),\n-        0.0,\n-    )\n-\n-    if not keepdims:\n-        mean = tf.squeeze(mean, axes)\n-        variance = tf.squeeze(variance, axes)\n-    if need_cast:\n-        # avoid overflow and underflow when casting from float16 to float32\n-        mean = tf.clip_by_value(mean, tf.float16.min, tf.float16.max)\n-        variance = tf.clip_by_value(variance, tf.float16.min, tf.float16.max)\n-        mean = cast(mean, ori_dtype)\n-        variance = cast(variance, ori_dtype)\n-    return mean, variance\n+    return tf.nn.moments(x, axes, keepdims=keepdims)\n \n \n def batch_normalization(\n\n@@ -714,8 +714,12 @@ def batch_normalization(\n     variance = convert_to_tensor(variance).detach()\n     if offset is not None:\n         offset = convert_to_tensor(offset)\n+    else:\n+        offset = torch.zeros_like(mean)\n     if scale is not None:\n         scale = convert_to_tensor(scale)\n+    else:\n+        scale = torch.ones_like(variance)\n \n     def _batch_norm():\n         return tnn.batch_norm(\n\n@@ -261,8 +261,9 @@ class BatchNormalization(Layer):\n             scale=gamma,\n             epsilon=self.epsilon,\n         )\n-\n-        return ops.cast(outputs, input_dtype)\n+        if input_dtype in (\"float16\", \"bfloat16\"):\n+            outputs = ops.cast(outputs, input_dtype)\n+        return outputs\n \n     def get_config(self):\n         base_config = super().get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b026ff7005003b46f165e9bfe49e9caf063729cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 434 | Lines Deleted: 307 | Files Changed: 21 | Hunks: 128 | Methods Changed: 83 | Complexity Δ (Sum/Max): 43/17 | Churn Δ: 741 | Churn Cumulative: 3598 | Contributors (this commit): 22 | Commits (past 90d): 64 | Contributors (cumulative): 82 | DMM Complexity: 0.0\n\nDIFF:\n@@ -61,11 +61,11 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n         if dtype and dtype != x.dtype:\n             return x.value.astype(dtype)\n         return x.value\n-    return jnp.array(x, dtype=dtype)\n+    return jnp.asarray(x, dtype=dtype)\n \n \n def convert_to_numpy(x):\n-    return np.array(x)\n+    return np.asarray(x)\n \n \n def is_tensor(x):\n\n@@ -904,11 +904,10 @@ def _distribute_data(data):\n \n \n class JAXEpochIterator(EpochIterator):\n-    def _get_iterator(self, return_type=\"auto\"):\n-        if return_type in (\"np\", \"auto\"):\n-            # enable prefetching when using numpy_iterator\n-            return self._prefetch_numpy_iterator(super()._get_iterator(\"np\"))\n-        return super()._get_iterator(return_type)\n+    def _get_iterator(self):\n+        return self._prefetch_numpy_iterator(\n+            self.data_adapter.get_jax_iterator()\n+        )\n \n     def _prefetch_numpy_iterator(self, numpy_iterator):\n         \"\"\"Shard and prefetch batches on device.\n\n@@ -198,7 +198,7 @@ class NumpyTrainer(base_trainer.Trainer):\n         self.stop_predicting = False\n         callbacks.on_predict_begin()\n         outputs = None\n-        for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+        for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_predict_batch_begin(step)\n             batch_outputs = self.predict_function(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n@@ -242,7 +242,7 @@ class NumpyTrainer(base_trainer.Trainer):\n \n         if not all(layer.built for layer in self._flatten_layers()):\n             # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+            for _, data in epoch_iterator.enumerate_epoch():\n                 data_batch = data[0]\n                 self._symbolic_build(data_batch)\n                 break\n@@ -264,7 +264,7 @@ class NumpyTrainer(base_trainer.Trainer):\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n-        for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+        for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n\n@@ -126,7 +126,7 @@ def convert_to_numpy(x):\n         x.set_shape(x_shape)\n     elif isinstance(x, tf.IndexedSlices):\n         x = tf.convert_to_tensor(x)\n-    return np.array(x)\n+    return np.asarray(x)\n \n \n def is_tensor(x):\n\n@@ -629,7 +629,7 @@ class TFEpochIterator(EpochIterator):\n     def __init__(self, distribute_strategy=None, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self._distribute_strategy = distribute_strategy\n-        dataset = self.data_adapter.get_tf_dataset()\n+        dataset = self._get_iterator()\n         if not isinstance(dataset, tf.distribute.DistributedDataset):\n             dataset = self._distribute_strategy.experimental_distribute_dataset(\n                 dataset\n@@ -637,6 +637,9 @@ class TFEpochIterator(EpochIterator):\n         self._distributed_dataset = dataset\n         self._steps_seen = 0\n \n+    def _get_iterator(self):\n+        return self.data_adapter.get_tf_dataset()\n+\n     def enumerate_epoch(self):\n         if self.steps_per_epoch:\n             if not self._current_iterator:\n\n@@ -1,5 +1,3 @@\n-import collections\n-import itertools\n import warnings\n \n import numpy as np\n@@ -10,7 +8,6 @@ from packaging.version import parse\n from keras import backend\n from keras import callbacks as callbacks_module\n from keras import optimizers as optimizers_module\n-from keras.trainers import data_adapters\n from keras.trainers import trainer as base_trainer\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.epoch_iterator import EpochIterator\n@@ -496,40 +493,5 @@ class TorchTrainer(base_trainer.Trainer):\n \n \n class TorchEpochIterator(EpochIterator):\n-    def _get_iterator(self, return_type=\"auto\"):\n-        if return_type == \"auto\" and isinstance(\n-            self.data_adapter, data_adapters.TorchDataLoaderAdapter\n-        ):\n+    def _get_iterator(self):\n         return self.data_adapter.get_torch_dataloader()\n-        elif return_type in (\"np\", \"auto\"):\n-            # enable prefetching when using numpy_iterator\n-            return self._prefetch_numpy_iterator(super()._get_iterator(\"np\"))\n-        return super()._get_iterator(return_type)\n-\n-    def _prefetch_numpy_data(self, data):\n-        return tree.map_structure(backend.convert_to_tensor, data)\n-\n-    def _prefetch_numpy_iterator(self, numpy_iterator):\n-        \"\"\"Prefetch batches on device.\n-\n-        The idea has been borrowed from\n-        `torchtnt.utils.data.CudaDataPrefetcher`\n-\n-        This utility takes an iterator and returns a new iterator which fills an\n-        on device prefetch buffer. Eager prefetching can improve the performance\n-        of training loops significantly by overlapping compute and data\n-        transfer.\n-        \"\"\"\n-        queue = collections.deque()\n-\n-        # If you're training on GPUs, 2 is generally the best choice because\n-        # this guarantees that you can overlap a training step on GPU with a\n-        # data prefetch step on CPU.\n-        def enqueue(n=2):\n-            for data in itertools.islice(numpy_iterator, n):\n-                queue.append(self._prefetch_numpy_data(data))\n-\n-        enqueue(n=2)  # TODO: should we make `n` configurable?\n-        while queue:\n-            yield queue.popleft()\n-            enqueue(1)\n\n@@ -92,8 +92,10 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n         broadcast_shape = [1] * len(input_shape)\n         broadcast_shape[axis] = input_shape[axis]\n         out = backend.convert_to_numpy(out)\n-        out -= np.reshape(backend.convert_to_numpy(layer.beta), broadcast_shape)\n-        out /= np.reshape(\n+        out = out - np.reshape(\n+            backend.convert_to_numpy(layer.beta), broadcast_shape\n+        )\n+        out = out / np.reshape(\n             backend.convert_to_numpy(layer.gamma), broadcast_shape\n         )\n \n@@ -200,8 +202,12 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n             out = layer(x, training=True)\n \n         out = backend.convert_to_numpy(out)\n-        out -= np.reshape(backend.convert_to_numpy(layer.beta), (1, 1, 1, 3))\n-        out /= np.reshape(backend.convert_to_numpy(layer.gamma), (1, 1, 1, 3))\n+        out = out - np.reshape(\n+            backend.convert_to_numpy(layer.beta), (1, 1, 1, 3)\n+        )\n+        out = out / np.reshape(\n+            backend.convert_to_numpy(layer.gamma), (1, 1, 1, 3)\n+        )\n \n         self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n         self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n\n@@ -6,6 +6,7 @@ import tree\n from keras import backend\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n+from keras.utils.dataset_utils import is_torch_tensor\n from keras.utils.nest import lists_to_tuples\n \n try:\n@@ -98,12 +99,22 @@ class ArrayDataAdapter(DataAdapter):\n \n     def get_numpy_iterator(self):\n         inputs = self._inputs\n-        if self._shuffle:\n+        if self._shuffle and self._shuffle != \"batch\":\n             inputs = data_adapter_utils.sync_shuffle(\n                 inputs, num_samples=self._num_samples\n             )\n         for i in range(self._size):\n-            start, stop = i * self._batch_size, (i + 1) * self._batch_size\n+            start = i * self._batch_size\n+            stop = min((i + 1) * self._batch_size, self._num_samples)\n+            if self._shuffle == \"batch\":\n+\n+                def slice_and_shuffle(x):\n+                    return data_adapter_utils.sync_shuffle(\n+                        x[start:stop], num_samples=(stop - start)\n+                    )\n+\n+                yield tree.map_structure(slice_and_shuffle, inputs)\n+            else:\n                 yield tree.map_structure(lambda x: x[start:stop], inputs)\n \n     def get_tf_dataset(self):\n@@ -237,6 +248,62 @@ class ArrayDataAdapter(DataAdapter):\n         dataset = dataset.with_options(options)\n         return dataset.prefetch(tf.data.AUTOTUNE)\n \n+    def get_jax_iterator(self):\n+        return data_adapter_utils.get_jax_iterator(self.get_numpy_iterator())\n+\n+    def get_torch_dataloader(self):\n+        import torch\n+\n+        from keras.backend.torch.core import convert_to_tensor\n+\n+        class ArrayDataset(torch.utils.data.Dataset):\n+            def __init__(self, array):\n+                self.array = array\n+\n+            def __getitem__(self, index):\n+                def slice_and_convert(x):\n+                    return convert_to_tensor(x[index])\n+\n+                return tree.map_structure(slice_and_convert, self.array)\n+\n+            def __len__(self):\n+                return len(self.array[0])\n+\n+        class RandomBatchSampler(torch.utils.data.Sampler):\n+            def __init__(self, sampler):\n+                self.sampler = sampler\n+\n+            def __iter__(self):\n+                for batch in self.sampler:\n+                    yield [batch[i] for i in torch.randperm(len(batch))]\n+\n+            def __len__(self):\n+                return len(self.sampler)\n+\n+        if self._shuffle == \"batch\":\n+            batch_sampler = RandomBatchSampler(\n+                torch.utils.data.BatchSampler(\n+                    range(self._num_samples),\n+                    batch_size=self._batch_size,\n+                    drop_last=False,\n+                )\n+            )\n+        elif self._shuffle:\n+            batch_sampler = torch.utils.data.BatchSampler(\n+                torch.utils.data.RandomSampler(range(self._num_samples)),\n+                batch_size=self._batch_size,\n+                drop_last=False,\n+            )\n+        else:\n+            batch_sampler = torch.utils.data.BatchSampler(\n+                torch.utils.data.SequentialSampler(range(self._num_samples)),\n+                batch_size=self._batch_size,\n+                drop_last=False,\n+            )\n+\n+        dataset = ArrayDataset(self._inputs)\n+        return torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler)\n+\n     @property\n     def num_batches(self):\n         return self._size\n@@ -315,7 +382,9 @@ def convert_to_arrays(arrays):\n             # `torch.Tensor`, as well as any other tensor-like object that has\n             # added numpy support.\n             if hasattr(x, \"__array__\"):\n-                x = backend.convert_to_numpy(x)\n+                if is_torch_tensor(x):\n+                    x = x.cpu()\n+                x = np.asarray(x)\n             else:\n                 raise ValueError(\n                     \"Expected a NumPy array, tf.Tensor, tf.RaggedTensor, \"\n\n@@ -1,36 +1,39 @@\n+import jax\n import numpy as np\n import pandas\n import pytest\n import tensorflow as tf\n+import torch\n from absl.testing import parameterized\n \n from keras import backend\n from keras import testing\n+from keras.testing.test_utils import named_product\n from keras.trainers.data_adapters import array_data_adapter\n \n \n class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n     def make_array(self, array_type, shape, dtype=\"float32\"):\n+        x = np.array([[i] * shape[1] for i in range(shape[0])], dtype=dtype)\n         if array_type == \"np\":\n-            return np.ones(shape, dtype=dtype)\n+            return x\n         elif array_type == \"tf\":\n-            return tf.ones(shape, dtype=dtype)\n-        elif array_type == \"backend\":\n-            if backend.backend() == \"jax\":\n-                import jax\n-\n-                return jax.numpy.ones(shape, dtype=dtype)\n-            elif backend.backend() == \"torch\":\n-                import torch\n-\n-                return torch.tensor(np.ones(shape, dtype=dtype))\n-            else:\n-                return tf.ones(shape, dtype=dtype)\n+            return tf.constant(x)\n+        elif array_type == \"jax\":\n+            return jax.numpy.array(x)\n+        elif array_type == \"torch\":\n+            return torch.as_tensor(x)\n         elif array_type == \"pandas\":\n-            return pandas.DataFrame(np.ones(shape, dtype=dtype))\n+            return pandas.DataFrame(x)\n \n-    @parameterized.parameters([(\"np\",), (\"tf\",), (\"backend\",), (\"pandas\",)])\n-    def test_basic_flow(self, array_type):\n+    @parameterized.named_parameters(\n+        named_product(\n+            array_type=[\"np\", \"tf\", \"jax\", \"torch\", \"pandas\"],\n+            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+            shuffle=[False, \"batch\", True],\n+        )\n+    )\n+    def test_basic_flow(self, array_type, iterator_type, shuffle):\n         x = self.make_array(array_type, (34, 4))\n         y = self.make_array(array_type, (34, 2))\n \n@@ -40,41 +43,46 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n             sample_weight=None,\n             batch_size=16,\n             steps=None,\n-            shuffle=False,\n+            shuffle=shuffle,\n         )\n         self.assertEqual(adapter.num_batches, 3)\n         self.assertEqual(adapter.batch_size, 16)\n         self.assertEqual(adapter.has_partial_batch, True)\n         self.assertEqual(adapter.partial_batch_size, 2)\n \n-        gen = adapter.get_numpy_iterator()\n-        for i, batch in enumerate(gen):\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.Tensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.Array\n+        elif iterator_type == \"torch\":\n+            it = adapter.get_torch_dataloader()\n+            expected_class = torch.Tensor\n+\n+        sample_order = []\n+        for i, batch in enumerate(it):\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n-            self.assertIsInstance(bx, np.ndarray)\n-            self.assertIsInstance(by, np.ndarray)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, backend.floatx())\n+            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n             if i < 2:\n                 self.assertEqual(bx.shape, (16, 4))\n                 self.assertEqual(by.shape, (16, 2))\n             else:\n                 self.assertEqual(bx.shape, (2, 4))\n                 self.assertEqual(by.shape, (2, 2))\n-        ds = adapter.get_tf_dataset()\n-        for i, batch in enumerate(ds):\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, tf.Tensor)\n-            self.assertIsInstance(by, tf.Tensor)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, backend.floatx())\n-            if i < 2:\n-                self.assertEqual(tuple(bx.shape), (16, 4))\n-                self.assertEqual(tuple(by.shape), (16, 2))\n+            for i in range(by.shape[0]):\n+                sample_order.append(by[i, 0])\n+        if shuffle:\n+            self.assertNotAllClose(sample_order, list(range(34)))\n         else:\n-                self.assertEqual(tuple(bx.shape), (2, 4))\n-                self.assertEqual(tuple(by.shape), (2, 2))\n+            self.assertAllClose(sample_order, list(range(34)))\n \n     def test_multi_inputs_and_outputs(self):\n         x1 = np.random.random((34, 1))\n@@ -153,7 +161,9 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(tuple(bw[0].shape), (2,))\n                 self.assertEqual(tuple(bw[1].shape), (2,))\n \n-    @parameterized.parameters([(\"int\",), (\"categorical\",)])\n+    @parameterized.named_parameters(\n+        named_product(target_encoding=[\"int\", \"categorical\"])\n+    )\n     def test_class_weights(self, target_encoding):\n         x = np.random.random((4, 2))\n         if target_encoding == \"int\":\n@@ -186,7 +196,9 @@ class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n         # TODO\n         pass\n \n-    @parameterized.parameters([(\"np\",), (\"tf\",), (\"backend\",), (\"pandas\",)])\n+    @parameterized.named_parameters(\n+        named_product(array_type=[\"np\", \"tf\", \"jax\", \"torch\", \"pandas\"])\n+    )\n     def test_integer_inputs(self, array_type):\n         x1 = self.make_array(array_type, (4, 4), dtype=\"float64\")\n         x2 = self.make_array(array_type, (4, 4), dtype=\"int32\")\n\n@@ -7,7 +7,8 @@ class DataAdapter(object):\n     \"\"\"\n \n     def get_numpy_iterator(self):\n-        \"\"\"Get a Python iterable for the DataAdapter, that yields NumPy arrays.\n+        \"\"\"Get a Python iterable for the `DataAdapter`, that yields NumPy\n+        arrays.\n \n         Returns:\n             A Python iterator.\n@@ -28,6 +29,22 @@ class DataAdapter(object):\n         \"\"\"\n         raise NotImplementedError\n \n+    def get_jax_iterator(self):\n+        \"\"\"Get a Python iterable for the `DataAdapter`, that yields JAX arrays.\n+\n+        Returns:\n+            A Python iterator.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def get_torch_dataloader(self):\n+        \"\"\"Get a Torch `DataLoader` for the `DataAdapter`.\n+\n+        Returns:\n+            A Torch `DataLoader`.\n+        \"\"\"\n+        raise NotImplementedError\n+\n     @property\n     def num_batches(self):\n         \"\"\"Return the size (number of batches) for the dataset created.\n\n@@ -5,6 +5,7 @@ import tree\n \n from keras import backend\n from keras.api_export import keras_export\n+from keras.utils.dataset_utils import is_torch_tensor\n \n try:\n     import pandas\n@@ -215,3 +216,44 @@ def class_weight_to_sample_weights(y, class_weight):\n     for i in range(y.shape[0]):\n         sample_weight[i] = class_weight.get(int(y[i]), 1.0)\n     return sample_weight\n+\n+\n+def get_jax_iterator(iterable):\n+    from keras.backend.jax.core import convert_to_tensor\n+\n+    for batch in iterable:\n+        yield tree.map_structure(convert_to_tensor, batch)\n+\n+\n+def get_numpy_iterator(iterable):\n+    def convert_to_numpy(x):\n+        if not isinstance(x, np.ndarray):\n+            # Using `__array__` should handle `tf.Tensor`, `jax.np.ndarray`,\n+            # `torch.Tensor`, as well as any other tensor-like object that\n+            # has added numpy support.\n+            if hasattr(x, \"__array__\"):\n+                if is_torch_tensor(x):\n+                    x = x.cpu()\n+                x = np.asarray(x)\n+        return x\n+\n+    for batch in iterable:\n+        yield tree.map_structure(convert_to_numpy, batch)\n+\n+\n+def get_torch_dataloader(iterable):\n+    import torch.utils.data as torch_data\n+\n+    from keras.backend.torch.core import convert_to_tensor\n+\n+    class ConverterIterableDataset(torch_data.IterableDataset):\n+        def __init__(self, iterable):\n+            self.iterable = iterable\n+\n+        def __iter__(self):\n+            for batch in self.iterable:\n+                yield tree.map_structure(convert_to_tensor, batch)\n+\n+    dataset = ConverterIterableDataset(iterable)\n+    # `batch_size=None` indicates that we should not re-batch\n+    return torch_data.DataLoader(dataset, batch_size=None)\n\n@@ -3,6 +3,8 @@ import itertools\n import numpy as np\n import tree\n \n+from keras import backend\n+from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n \n \n@@ -10,24 +12,22 @@ class GeneratorDataAdapter(DataAdapter):\n     \"\"\"Adapter for Python generators.\"\"\"\n \n     def __init__(self, generator):\n-        data, generator = peek_and_restore(generator)\n+        first_batch, generator = peek_and_restore(generator)\n         self.generator = generator\n+        self._first_batch = first_batch\n         self._output_signature = None\n-        if not isinstance(data, tuple):\n+        if not isinstance(first_batch, tuple):\n             raise ValueError(\n                 \"When passing a Python generator to a Keras model, \"\n                 \"the generator must return a tuple, either \"\n                 \"(input,) or (inputs, targets) or \"\n                 \"(inputs, targets, sample_weights). \"\n-                f\"Received: {data}\"\n+                f\"Received: {first_batch}\"\n             )\n \n     def _set_tf_output_signature(self):\n         from keras.utils.module_utils import tensorflow as tf\n \n-        data, generator = peek_and_restore(self.generator)\n-        self.generator = generator\n-\n         def get_tensor_spec(x):\n             shape = x.shape\n             if len(shape) < 1:\n@@ -39,18 +39,23 @@ class GeneratorDataAdapter(DataAdapter):\n                 )\n             shape = list(shape)\n             shape[0] = None  # The batch size is not guaranteed to be static.\n+            dtype = backend.standardize_dtype(x.dtype)\n             if isinstance(x, tf.RaggedTensor):\n-                return tf.RaggedTensorSpec(shape=shape, dtype=x.dtype.name)\n+                return tf.RaggedTensorSpec(shape=shape, dtype=dtype)\n             if isinstance(x, tf.SparseTensor) or is_scipy_sparse(x):\n-                return tf.SparseTensorSpec(shape=shape, dtype=x.dtype.name)\n+                return tf.SparseTensorSpec(shape=shape, dtype=dtype)\n             else:\n-                return tf.TensorSpec(shape=shape, dtype=x.dtype.name)\n+                return tf.TensorSpec(shape=shape, dtype=dtype)\n \n-        self._output_signature = tree.map_structure(get_tensor_spec, data)\n+        self._output_signature = tree.map_structure(\n+            get_tensor_spec, self._first_batch\n+        )\n \n     def get_numpy_iterator(self):\n-        for batch in self.generator:\n-            yield batch\n+        return data_adapter_utils.get_numpy_iterator(self.generator)\n+\n+    def get_jax_iterator(self):\n+        return data_adapter_utils.get_jax_iterator(self.generator)\n \n     def get_tf_dataset(self):\n         from keras.utils.module_utils import tensorflow as tf\n@@ -74,6 +79,9 @@ class GeneratorDataAdapter(DataAdapter):\n         ds = ds.prefetch(tf.data.AUTOTUNE)\n         return ds\n \n+    def get_torch_dataloader(self):\n+        return data_adapter_utils.get_torch_dataloader(self.generator)\n+\n     @property\n     def num_batches(self):\n         return None\n\n@@ -1,11 +1,15 @@\n import math\n \n+import jax\n import numpy as np\n import scipy\n import tensorflow as tf\n+import torch\n from absl.testing import parameterized\n+from jax import numpy as jnp\n \n from keras import testing\n+from keras.testing.test_utils import named_product\n from keras.trainers.data_adapters import generator_data_adapter\n \n \n@@ -25,18 +29,31 @@ def example_generator(x, y, sample_weight=None, batch_size=32):\n \n \n class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters(\n+    @parameterized.named_parameters(\n+        named_product(\n             [\n-            (True,),\n-            (False,),\n-        ]\n+                {\"testcase_name\": \"use_weight\", \"use_sample_weight\": True},\n+                {\"testcase_name\": \"no_weight\", \"use_sample_weight\": False},\n+            ],\n+            generator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n         )\n-    def test_basic_flow(self, use_sample_weight):\n-        x = np.random.random((64, 4))\n-        y = np.array([[i, i] for i in range(64)], dtype=\"float64\")\n-        if use_sample_weight:\n-            sw = np.random.random((64,))\n-        else:\n+    )\n+    def test_basic_flow(self, use_sample_weight, generator_type, iterator_type):\n+        x = np.random.random((34, 4)).astype(\"float32\")\n+        y = np.array([[i, i] for i in range(34)], dtype=\"float32\")\n+        sw = np.random.random((34,)).astype(\"float32\")\n+        if generator_type == \"tf\":\n+            x, y, sw = tf.constant(x), tf.constant(y), tf.constant(sw)\n+        elif generator_type == \"jax\":\n+            x, y, sw = jnp.array(x), jnp.array(y), jnp.array(sw)\n+        elif generator_type == \"torch\":\n+            x, y, sw = (\n+                torch.as_tensor(x),\n+                torch.as_tensor(y),\n+                torch.as_tensor(sw),\n+            )\n+        if not use_sample_weight:\n             sw = None\n         make_generator = example_generator(\n             x,\n@@ -44,53 +61,46 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n             sample_weight=sw,\n             batch_size=16,\n         )\n-        adapter = generator_data_adapter.GeneratorDataAdapter(make_generator())\n \n-        gen = adapter.get_numpy_iterator()\n+        adapter = generator_data_adapter.GeneratorDataAdapter(make_generator())\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.Tensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.Array\n+        elif iterator_type == \"torch\":\n+            it = adapter.get_torch_dataloader()\n+            expected_class = torch.Tensor\n+\n         sample_order = []\n-        for batch in gen:\n+        for i, batch in enumerate(it):\n             if use_sample_weight:\n                 self.assertEqual(len(batch), 3)\n                 bx, by, bsw = batch\n             else:\n                 self.assertEqual(len(batch), 2)\n                 bx, by = batch\n-\n-            self.assertIsInstance(bx, np.ndarray)\n-            self.assertIsInstance(by, np.ndarray)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.dtype, by.dtype)\n+            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n+            if i < 2:\n                 self.assertEqual(bx.shape, (16, 4))\n                 self.assertEqual(by.shape, (16, 2))\n-            if use_sample_weight:\n-                self.assertIsInstance(bsw, np.ndarray)\n-            for i in range(by.shape[0]):\n-                sample_order.append(by[i, 0])\n-        self.assertAllClose(sample_order, list(range(64)))\n-\n-        adapter = generator_data_adapter.GeneratorDataAdapter(\n-            make_generator(),\n-        )\n-        ds = adapter.get_tf_dataset()\n-        sample_order = []\n-        for batch in ds:\n-            if use_sample_weight:\n-                self.assertEqual(len(batch), 3)\n-                bx, by, bsw = batch\n             else:\n-                self.assertEqual(len(batch), 2)\n-                bx, by = batch\n-            self.assertIsInstance(bx, tf.Tensor)\n-            self.assertIsInstance(by, tf.Tensor)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(tuple(bx.shape), (16, 4))\n-            self.assertEqual(tuple(by.shape), (16, 2))\n+                self.assertEqual(bx.shape, (2, 4))\n+                self.assertEqual(by.shape, (2, 2))\n             if use_sample_weight:\n-                self.assertIsInstance(bsw, tf.Tensor)\n+                self.assertIsInstance(bsw, expected_class)\n             for i in range(by.shape[0]):\n                 sample_order.append(by[i, 0])\n-        self.assertAllClose(sample_order, list(range(64)))\n+        self.assertAllClose(sample_order, list(range(34)))\n \n-    def test_tf_sparse_tensors(self):\n+    def test_tf_sparse_tensors_with_tf_dataset(self):\n         def generate_tf():\n             for i in range(4):\n                 x = tf.SparseTensor(\n@@ -115,7 +125,7 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(bx.shape, (2, 4))\n             self.assertEqual(by.shape, (2, 2))\n \n-    def test_scipy_sparse_tensors(self):\n+    def test_scipy_sparse_tensors_with_tf_dataset(self):\n         def generate_scipy():\n             for i in range(4):\n                 x = scipy.sparse.coo_matrix(\n\n@@ -10,6 +10,7 @@ from contextlib import closing\n import numpy as np\n import tree\n \n+from keras import backend\n from keras.api_export import keras_export\n from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n@@ -200,7 +201,8 @@ class PyDatasetAdapter(DataAdapter):\n                 )\n             shape = list(shape)\n             shape[0] = None  # The batch size is not guaranteed to be static.\n-            return tf.TensorSpec(shape=shape, dtype=x.dtype.name)\n+            dtype = backend.standardize_dtype(x.dtype)\n+            return tf.TensorSpec(shape=shape, dtype=dtype)\n \n         # Grab the first example\n         batch = self.py_dataset[0]\n@@ -267,7 +269,7 @@ class PyDatasetAdapter(DataAdapter):\n \n         return generator_fn\n \n-    def get_numpy_iterator(self):\n+    def _get_iterator(self):\n         gen_fn = self._make_multiprocessed_generator_fn()\n         for i, batch in enumerate(gen_fn()):\n             batch = self._standardize_batch(batch)\n@@ -275,6 +277,12 @@ class PyDatasetAdapter(DataAdapter):\n             if i >= len(self.py_dataset) - 1 and self.enqueuer:\n                 self.enqueuer.stop()\n \n+    def get_numpy_iterator(self):\n+        return data_adapter_utils.get_numpy_iterator(self._get_iterator())\n+\n+    def get_jax_iterator(self):\n+        return data_adapter_utils.get_jax_iterator(self._get_iterator())\n+\n     def get_tf_dataset(self):\n         from keras.utils.module_utils import tensorflow as tf\n \n@@ -282,7 +290,7 @@ class PyDatasetAdapter(DataAdapter):\n             self._set_tf_output_signature()\n \n         ds = tf.data.Dataset.from_generator(\n-            self.get_numpy_iterator,\n+            self._get_iterator,\n             output_signature=self._output_signature,\n         )\n         if self.shuffle:\n@@ -290,6 +298,9 @@ class PyDatasetAdapter(DataAdapter):\n         ds = ds.prefetch(tf.data.AUTOTUNE)\n         return ds\n \n+    def get_torch_dataloader(self):\n+        return data_adapter_utils.get_torch_dataloader(self._get_iterator())\n+\n     def on_epoch_end(self):\n         if self.enqueuer:\n             self.enqueuer.stop()\n\n@@ -1,11 +1,14 @@\n import math\n import time\n \n+import jax\n import numpy as np\n import tensorflow as tf\n+import torch\n from absl.testing import parameterized\n \n from keras import testing\n+from keras.testing.test_utils import named_product\n from keras.trainers.data_adapters import py_dataset_adapter\n from keras.utils.rng_utils import set_random_seed\n \n@@ -61,22 +64,51 @@ class DictPyDataset(py_dataset_adapter.PyDataset):\n \n \n class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.parameters(\n+    @parameterized.named_parameters(\n+        named_product(\n             [\n-            (True, 2, True, 10),\n-            (False, 2, True, 10),\n-            (True, 2, False, 10),\n-            (False, 2, False, 10),\n-            (True, 0, False, 0),\n-            (False, 0, False, 0),\n-        ]\n+                {\n+                    \"testcase_name\": \"multi_on\",\n+                    \"workers\": 2,\n+                    \"use_multiprocessing\": True,\n+                    \"max_queue_size\": 10,\n+                },\n+                {\n+                    \"testcase_name\": \"multi_off\",\n+                    \"workers\": 2,\n+                    \"use_multiprocessing\": False,\n+                    \"max_queue_size\": 10,\n+                },\n+                {\n+                    \"testcase_name\": \"multi_off_zero\",\n+                    \"workers\": 0,\n+                    \"use_multiprocessing\": False,\n+                    \"max_queue_size\": 0,\n+                },\n+            ],\n+            shuffle=[True, False],\n+            dataset_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+            iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"],\n+        )\n     )\n     def test_basic_flow(\n-        self, shuffle, workers, use_multiprocessing, max_queue_size\n+        self,\n+        shuffle,\n+        workers,\n+        use_multiprocessing,\n+        max_queue_size,\n+        dataset_type,\n+        iterator_type,\n     ):\n         set_random_seed(1337)\n-        x = np.random.random((64, 4))\n-        y = np.array([[i, i] for i in range(64)], dtype=\"float64\")\n+        x = np.random.random((64, 4)).astype(\"float32\")\n+        y = np.array([[i, i] for i in range(64)], dtype=\"float32\")\n+        if dataset_type == \"tf\":\n+            x, y = tf.constant(x), tf.constant(y)\n+        elif dataset_type == \"jax\":\n+            x, y = jax.numpy.array(x), jax.numpy.array(y)\n+        elif dataset_type == \"torch\":\n+            x, y = torch.as_tensor(x), torch.as_tensor(y)\n         py_dataset = ExamplePyDataset(\n             x,\n             y,\n@@ -89,37 +121,33 @@ class PyDatasetAdapterTest(testing.TestCase, parameterized.TestCase):\n             py_dataset, shuffle=shuffle\n         )\n \n-        gen = adapter.get_numpy_iterator()\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.Tensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.Array\n+        elif iterator_type == \"torch\":\n+            it = adapter.get_torch_dataloader()\n+            expected_class = torch.Tensor\n+\n         sample_order = []\n-        for batch in gen:\n+        for batch in it:\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n-            self.assertIsInstance(bx, np.ndarray)\n-            self.assertIsInstance(by, np.ndarray)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.dtype, by.dtype)\n+            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n             self.assertEqual(bx.shape, (16, 4))\n             self.assertEqual(by.shape, (16, 2))\n             for i in range(by.shape[0]):\n                 sample_order.append(by[i, 0])\n         if shuffle:\n-            self.assertFalse(sample_order == list(range(64)))\n-        else:\n-            self.assertAllClose(sample_order, list(range(64)))\n-\n-        ds = adapter.get_tf_dataset()\n-        sample_order = []\n-        for batch in ds:\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, tf.Tensor)\n-            self.assertIsInstance(by, tf.Tensor)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(tuple(bx.shape), (16, 4))\n-            self.assertEqual(tuple(by.shape), (16, 2))\n-            for i in range(by.shape[0]):\n-                sample_order.append(by[i, 0])\n-        if shuffle:\n-            self.assertFalse(sample_order == list(range(64)))\n+            self.assertNotAllClose(sample_order, list(range(64)))\n         else:\n             self.assertAllClose(sample_order, list(range(64)))\n \n\n@@ -1,4 +1,3 @@\n-import numpy as np\n import tree\n \n from keras.trainers.data_adapters import data_adapter_utils\n@@ -37,20 +36,22 @@ class TFDatasetAdapter(DataAdapter):\n         self._dataset = dataset\n \n     def get_numpy_iterator(self):\n-        from keras.utils.module_utils import tensorflow as tf\n-\n-        def convert_to_numpy(x):\n-            if isinstance(x, tf.SparseTensor):\n-                x = tf.sparse.to_dense(x)\n-            # shared memory using `np.asarray`\n-            return np.asarray(x)\n+        from keras.backend.tensorflow.core import convert_to_numpy\n \n         for batch in self._dataset:\n             yield tree.map_structure(convert_to_numpy, batch)\n \n+    def get_jax_iterator(self):\n+        # We use numpy as an intermediary because the conversion\n+        # tf -> numpy -> jax is more than 2x faster than tf -> jax.\n+        return data_adapter_utils.get_jax_iterator(self.get_numpy_iterator())\n+\n     def get_tf_dataset(self):\n         return self._dataset\n \n+    def get_torch_dataloader(self):\n+        return data_adapter_utils.get_torch_dataloader(self._dataset)\n+\n     @property\n     def num_batches(self):\n         cardinality = self._dataset.cardinality\n\n@@ -1,14 +1,21 @@\n from unittest import mock\n \n+import jax\n import numpy as np\n import tensorflow as tf\n+import torch\n+from absl.testing import parameterized\n \n from keras import testing\n+from keras.testing.test_utils import named_product\n from keras.trainers.data_adapters import tf_dataset_adapter\n \n \n-class TestTFDatasetAdapter(testing.TestCase):\n-    def test_basic_flow(self):\n+class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n+    )\n+    def test_basic_flow(self, iterator_type):\n         x = tf.random.normal((34, 4))\n         y = tf.random.normal((34, 2))\n         base_ds = tf.data.Dataset.from_tensor_slices((x, y)).batch(16)\n@@ -19,34 +26,32 @@ class TestTFDatasetAdapter(testing.TestCase):\n         self.assertEqual(adapter.has_partial_batch, None)\n         self.assertEqual(adapter.partial_batch_size, None)\n \n-        gen = adapter.get_numpy_iterator()\n-        for i, batch in enumerate(gen):\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.Tensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.Array\n+        elif iterator_type == \"torch\":\n+            it = adapter.get_torch_dataloader()\n+            expected_class = torch.Tensor\n+\n+        for i, batch in enumerate(it):\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n-            self.assertIsInstance(bx, np.ndarray)\n-            self.assertIsInstance(by, np.ndarray)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, \"float32\")\n+            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n             if i < 2:\n                 self.assertEqual(bx.shape, (16, 4))\n                 self.assertEqual(by.shape, (16, 2))\n             else:\n                 self.assertEqual(bx.shape, (2, 4))\n                 self.assertEqual(by.shape, (2, 2))\n-        ds = adapter.get_tf_dataset()\n-        for i, batch in enumerate(ds):\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, tf.Tensor)\n-            self.assertIsInstance(by, tf.Tensor)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, \"float32\")\n-            if i < 2:\n-                self.assertEqual(tuple(bx.shape), (16, 4))\n-                self.assertEqual(tuple(by.shape), (16, 2))\n-            else:\n-                self.assertEqual(tuple(bx.shape), (2, 4))\n-                self.assertEqual(tuple(by.shape), (2, 2))\n \n     def _test_class_weights(self, target_encoding=\"int\"):\n         x = np.random.random((4, 2))\n\n@@ -1,6 +1,8 @@\n import numpy as np\n import tree\n \n+from keras import backend\n+from keras.trainers.data_adapters import data_adapter_utils\n from keras.trainers.data_adapters.data_adapter import DataAdapter\n \n \n@@ -28,8 +30,10 @@ class TorchDataLoaderAdapter(DataAdapter):\n                 tree.map_structure(lambda x: np.asarray(x.cpu()), batch)\n             )\n \n-    def get_torch_dataloader(self):\n-        return self._dataloader\n+    def get_jax_iterator(self):\n+        # We use numpy as an intermediary because the conversion\n+        # torch -> numpy -> jax is faster than torch -> jax.\n+        return data_adapter_utils.get_jax_iterator(self.get_numpy_iterator())\n \n     def get_tf_dataset(self):\n         from keras.utils.module_utils import tensorflow as tf\n@@ -40,6 +44,9 @@ class TorchDataLoaderAdapter(DataAdapter):\n             output_signature=output_signature,\n         )\n \n+    def get_torch_dataloader(self):\n+        return self._dataloader\n+\n     def peek_and_get_tensor_spec(self):\n         from keras.utils.module_utils import tensorflow as tf\n \n@@ -56,10 +63,7 @@ class TorchDataLoaderAdapter(DataAdapter):\n                 )\n             shape = list(shape)\n             shape[0] = None  # The batch size is not guaranteed to be static.\n-\n-            # No easy way to get string representation of dtype in torch\n-            # TODO: Figure out a better way to achieve this\n-            dtype = str(x.dtype).replace(\"torch.\", \"\")\n+            dtype = backend.standardize_dtype(x.dtype)\n             return tf.TensorSpec(shape=shape, dtype=dtype)\n \n         return tuple(tree.map_structure(get_tensor_spec, batch_data))\n\n@@ -1,28 +1,25 @@\n+import jax\n import numpy as np\n-import pytest\n import tensorflow as tf\n+import torch\n+from absl.testing import parameterized\n \n-from keras import backend\n from keras import testing\n+from keras.testing.test_utils import named_product\n from keras.trainers.data_adapters.torch_data_loader_adapter import (\n     TorchDataLoaderAdapter,\n )\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"torch\",\n-    reason=\"Backend does not support TorchDataLoaderAdapter.\",\n+class TestTorchDataLoaderAdapter(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(iterator_type=[\"np\", \"tf\", \"jax\", \"torch\"])\n     )\n-class TestTorchDataLoaderAdapter(testing.TestCase):\n-    def test_basic_dataloader(self):\n-        import torch\n-        from torch.utils.data import DataLoader\n-        from torch.utils.data import TensorDataset\n-\n+    def test_basic_dataloader(self, iterator_type):\n         x = torch.normal(2, 3, size=(34, 4))\n         y = torch.normal(1, 3, size=(34, 2))\n-        base_ds = TensorDataset(x, y)\n-        base_dataloader = DataLoader(base_ds, batch_size=16)\n+        base_ds = torch.utils.data.TensorDataset(x, y)\n+        base_dataloader = torch.utils.data.DataLoader(base_ds, batch_size=16)\n         adapter = TorchDataLoaderAdapter(base_dataloader)\n \n         self.assertEqual(adapter.num_batches, 3)\n@@ -30,47 +27,29 @@ class TestTorchDataLoaderAdapter(testing.TestCase):\n         self.assertEqual(adapter.has_partial_batch, True)\n         self.assertEqual(adapter.partial_batch_size, 2)\n \n-        gen = adapter.get_numpy_iterator()\n-        for i, batch in enumerate(gen):\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.Tensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.Array\n+        elif iterator_type == \"torch\":\n+            it = adapter.get_torch_dataloader()\n+            expected_class = torch.Tensor\n+\n+        for i, batch in enumerate(it):\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n-            self.assertIsInstance(bx, np.ndarray)\n-            self.assertIsInstance(by, np.ndarray)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, \"float32\")\n+            self.assertContainsExactSubsequence(str(bx.dtype), \"float32\")\n             if i < 2:\n                 self.assertEqual(bx.shape, (16, 4))\n                 self.assertEqual(by.shape, (16, 2))\n             else:\n                 self.assertEqual(bx.shape, (2, 4))\n                 self.assertEqual(by.shape, (2, 2))\n-\n-        ds = adapter.get_torch_dataloader()\n-        for i, batch in enumerate(ds):\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, torch.Tensor)\n-            self.assertIsInstance(by, torch.Tensor)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, torch.float32)\n-            if i < 2:\n-                self.assertEqual(tuple(bx.shape), (16, 4))\n-                self.assertEqual(tuple(by.shape), (16, 2))\n-            else:\n-                self.assertEqual(tuple(bx.shape), (2, 4))\n-                self.assertEqual(tuple(by.shape), (2, 2))\n-\n-        ds = adapter.get_tf_dataset()\n-        for i, batch in enumerate(ds):\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, tf.Tensor)\n-            self.assertIsInstance(by, tf.Tensor)\n-            self.assertEqual(bx.dtype, by.dtype)\n-            self.assertEqual(bx.dtype, tf.float32)\n-            if i < 2:\n-                self.assertEqual(tuple(bx.shape), (16, 4))\n-                self.assertEqual(tuple(by.shape), (16, 2))\n-            else:\n-                self.assertEqual(tuple(bx.shape), (2, 4))\n-                self.assertEqual(tuple(by.shape), (2, 2))\n\n@@ -71,23 +71,14 @@ class EpochIterator:\n         )\n         self._num_batches = self.data_adapter.num_batches\n \n-    def _get_iterator(self, return_type=\"auto\"):\n-        if return_type not in (\"np\", \"tf\", \"auto\"):\n-            raise ValueError(\n-                \"Argument `return_type` must be one of `{'np', 'tf', 'auto'}`. \"\n-                f\"Received instead: return_type={return_type}\"\n-            )\n-        if return_type == \"tf\":\n-            iterator = self.data_adapter.get_tf_dataset()\n-        else:\n-            iterator = self.data_adapter.get_numpy_iterator()\n-        return iterator\n+    def _get_iterator(self):\n+        return self.data_adapter.get_numpy_iterator()\n \n-    def enumerate_epoch(self, return_type=\"auto\"):\n+    def enumerate_epoch(self):\n         buffer = []\n         if self.steps_per_epoch:\n-            if not self._current_iterator:\n-                self._current_iterator = self._get_iterator(return_type)\n+            if self._current_iterator is None:\n+                self._current_iterator = iter(self._get_iterator())\n                 self._insufficient_data = False\n \n             for step in range(self.steps_per_epoch):\n@@ -114,7 +105,7 @@ class EpochIterator:\n             if buffer:\n                 yield step - len(buffer) + 1, buffer\n         else:\n-            for step, data in enumerate(self._get_iterator(return_type)):\n+            for step, data in enumerate(self._get_iterator()):\n                 buffer.append(data)\n                 if len(buffer) == self.steps_per_execution:\n                     yield step - len(buffer) + 1, buffer\n\n@@ -9,7 +9,7 @@ from keras.trainers import epoch_iterator\n \n \n class TestEpochIterator(testing.TestCase):\n-    def _test_basic_flow(self, return_type):\n+    def test_basic_flow(self):\n         x = np.random.random((100, 16))\n         y = np.random.random((100, 4))\n         sample_weight = np.random.random((100,))\n@@ -23,22 +23,13 @@ class TestEpochIterator(testing.TestCase):\n             shuffle=shuffle,\n         )\n         steps_seen = []\n-        for step, batch in iterator.enumerate_epoch(return_type=return_type):\n+        for step, batch in iterator.enumerate_epoch():\n             batch = batch[0]\n             steps_seen.append(step)\n             self.assertEqual(len(batch), 3)\n-            if return_type == \"np\":\n             self.assertIsInstance(batch[0], np.ndarray)\n-            else:\n-                self.assertIsInstance(batch[0], tf.Tensor)\n         self.assertEqual(steps_seen, [0, 1, 2, 3, 4, 5, 6])\n \n-    def test_basic_flow_np(self):\n-        self._test_basic_flow(\"np\")\n-\n-    def test_basic_flow_tf(self):\n-        self._test_basic_flow(\"tf\")\n-\n     def test_insufficient_data(self):\n         batch_size = 8\n         steps_per_epoch = 6\n@@ -97,7 +88,7 @@ class TestEpochIterator(testing.TestCase):\n             torch_dataset, batch_size=8, shuffle=True\n         )\n         iterator = epoch_iterator.EpochIterator(torch_dataloader)\n-        for _, batch in iterator.enumerate_epoch(return_type=\"np\"):\n+        for _, batch in iterator.enumerate_epoch():\n             batch = batch[0]\n             self.assertEqual(batch[0].shape, (8, 2))\n             self.assertEqual(batch[1].shape, (8, 1))\n@@ -180,14 +171,3 @@ class TestEpochIterator(testing.TestCase):\n         x = \"unsupported_data\"\n         with self.assertRaisesRegex(ValueError, \"Unrecognized data type\"):\n             _ = epoch_iterator.EpochIterator(x=x)\n-\n-    def test_invalid_return_type_in_get_iterator(self):\n-        x = np.random.random((100, 16))\n-        y = np.random.random((100, 4))\n-        epoch_iter = epoch_iterator.EpochIterator(x=x, y=y)\n-\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            \"Argument `return_type` must be one of `{'np', 'tf', 'auto'}`\",\n-        ):\n-            _ = epoch_iter._get_iterator(\"unsupported\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
