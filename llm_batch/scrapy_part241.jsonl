{"custom_id": "scrapy#ff5f717f7a2aaf0a1a1101019485201427edd536", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 789 | Contributors (this commit): 21 | Commits (past 90d): 2 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -168,6 +168,7 @@ def _get_clickable(clickdata, form):\n     if the latter is given. If not, it returns the first\n     clickable element found\n     \"\"\"\n+    print(\"form =\", form.__dict__)\n     clickables = [\n         el for el in form.xpath(\n             'descendant::*[(self::input or self::button)'\n@@ -177,6 +178,7 @@ def _get_clickable(clickdata, form):\n             '|descendant::button[not(@type)]',\n             namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n         ]\n+    print(\"clickables =\", clickables)\n     if not clickables:\n         return\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e25e2afe174bbe70ecbf88ea684937890e8ba4d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 791 | Contributors (this commit): 21 | Commits (past 90d): 3 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -168,7 +168,6 @@ def _get_clickable(clickdata, form):\n     if the latter is given. If not, it returns the first\n     clickable element found\n     \"\"\"\n-    print(\"form =\", form.__dict__)\n     clickables = [\n         el for el in form.xpath(\n             'descendant::*[(self::input or self::button)'\n@@ -178,7 +177,6 @@ def _get_clickable(clickdata, form):\n             '|descendant::button[not(@type)]',\n             namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n         ]\n-    print(\"clickables =\", clickables)\n     if not clickables:\n         return\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a5acc9373f8735e27c22de6fbe345fbed8f268c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 798 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,11 +170,8 @@ def _get_clickable(clickdata, form):\n     \"\"\"\n     clickables = [\n         el for el in form.xpath(\n-            'descendant::*[(self::input or self::button)'\n-            ' and re:test(@type, \"^submit$\", \"i\")]'\n-            '|descendant::*[(self::input or self::button)'\n-            ' and re:test(@type, \"^image$\", \"i\")]'\n-            '|descendant::button[not(@type)]',\n+            'descendant::input[re.test(@type, \"^(submit|image)$\", \"i\")]'\n+            '|descendant::button[not(@type) or re.test(@type, \"^submit$\", \"i\")]',\n             namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n         ]\n     if not clickables:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd064413a46356940151ac3f9ccd8a45bca2cbd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 802 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -170,8 +170,8 @@ def _get_clickable(clickdata, form):\n     \"\"\"\n     clickables = [\n         el for el in form.xpath(\n-            'descendant::input[re.test(@type, \"^(submit|image)$\", \"i\")]'\n-            '|descendant::button[not(@type) or re.test(@type, \"^submit$\", \"i\")]',\n+            'descendant::input[re:test(@type, \"^(submit|image)$\", \"i\")]'\n+            '|descendant::button[not(@type) or re:test(@type, \"^submit$\", \"i\")]',\n             namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n         ]\n     if not clickables:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e8994c6b55fa8e975ce30b25ae326f829a58aed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1037 | Contributors (this commit): 43 | Commits (past 90d): 3 | Contributors (cumulative): 57 | DMM Complexity: 1.0\n\nDIFF:\n@@ -207,6 +207,8 @@ class GCSFilesStore(object):\n     GCS_PROJECT_ID = None\n \n     CACHE_CONTROL = 'max-age=172800'\n+    POLICY = 'projectPrivate'  # Overriden from settings.FILES_STORE_GCS_ACL in\n+                               # FilesPipeline.from_settings.\n \n     def __init__(self, uri):\n         from google.cloud import storage\n@@ -239,7 +241,8 @@ class GCSFilesStore(object):\n         return threads.deferToThread(\n             blob.upload_from_string,\n             data=buf.getvalue(),\n-            content_type=self._get_content_type(headers)\n+            content_type=self._get_content_type(headers),\n+            predefined_acl=self.POLICY\n         )\n \n \n@@ -314,6 +317,7 @@ class FilesPipeline(MediaPipeline):\n \n         gcs_store = cls.STORE_SCHEMES['gs']\n         gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n+        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL']\n \n         store_uri = settings['FILES_STORE']\n         return cls(store_uri, settings=settings)\n\n@@ -93,6 +93,7 @@ class ImagesPipeline(FilesPipeline):\n \n         gcs_store = cls.STORE_SCHEMES['gs']\n         gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n+        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL']\n \n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n\n@@ -159,6 +159,7 @@ FEED_EXPORTERS_BASE = {\n FEED_EXPORT_INDENT = 0\n \n FILES_STORE_S3_ACL = 'private'\n+FILES_STORE_GCS_ACL = 'projectPrivate'\n \n FTP_USER = 'anonymous'\n FTP_PASSWORD = 'guest'\n@@ -181,6 +182,7 @@ HTTPPROXY_ENABLED = True\n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n IMAGES_STORE_S3_ACL = 'private'\n+IMAGES_STORE_GCS_ACL = 'projectPrivate'\n \n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5254ac393bdf712db698bfa41caf5fb2e682f883", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 612 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,8 +57,9 @@ def get_gcs_content_and_delete(bucket, path):\n     bucket = client.get_bucket(bucket)\n     blob = bucket.get_blob(path)\n     content = blob.download_as_string()\n+    acl = list(blob.acl)  # loads acl before it will be deleted\n     bucket.delete_blob(path)\n-    return content, blob\n+    return content, acl, blob\n \n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n\n@@ -388,17 +388,20 @@ class TestGCSFilesStore(unittest.TestCase):\n         meta = {'foo': 'bar'}\n         path = 'full/filename'\n         store = GCSFilesStore(uri)\n+        store.POLICY = 'authenticatedRead'\n+        expected_policy = {'role': 'READER', 'entity': 'allAuthenticatedUsers'}\n         yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n         s = yield store.stat_file(path, info=None)\n         self.assertIn('last_modified', s)\n         self.assertIn('checksum', s)\n         self.assertEqual(s['checksum'], 'zc2oVgXkbQr2EQdSdw3OPA==')\n         u = urlparse(uri)\n-        content, blob = get_gcs_content_and_delete(u.hostname, u.path[1:]+path)\n+        content, acl, blob = get_gcs_content_and_delete(u.hostname, u.path[1:]+path)\n         self.assertEqual(content, data)\n         self.assertEqual(blob.metadata, {'foo': 'bar'})\n         self.assertEqual(blob.cache_control, GCSFilesStore.CACHE_CONTROL)\n         self.assertEqual(blob.content_type, 'application/octet-stream')\n+        self.assertIn(expected_policy, acl)\n \n \n class ItemWithFiles(Item):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#464973489e9f457570d7dd1a2e4aab8c4c1778fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 836 | Contributors (this commit): 42 | Commits (past 90d): 4 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -207,8 +207,10 @@ class GCSFilesStore(object):\n     GCS_PROJECT_ID = None\n \n     CACHE_CONTROL = 'max-age=172800'\n-    POLICY = 'projectPrivate'  # Overriden from settings.FILES_STORE_GCS_ACL in\n-                               # FilesPipeline.from_settings.\n+\n+    # The bucket's default object ACL will be applied to the object.\n+    # Overriden from settings.FILES_STORE_GCS_ACL in FilesPipeline.from_settings.\n+    POLICY = None\n \n     def __init__(self, uri):\n         from google.cloud import storage\n\n@@ -159,7 +159,7 @@ FEED_EXPORTERS_BASE = {\n FEED_EXPORT_INDENT = 0\n \n FILES_STORE_S3_ACL = 'private'\n-FILES_STORE_GCS_ACL = 'projectPrivate'\n+FILES_STORE_GCS_ACL = None\n \n FTP_USER = 'anonymous'\n FTP_PASSWORD = 'guest'\n@@ -182,7 +182,7 @@ HTTPPROXY_ENABLED = True\n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n IMAGES_STORE_S3_ACL = 'private'\n-IMAGES_STORE_GCS_ACL = 'projectPrivate'\n+IMAGES_STORE_GCS_ACL = None\n \n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#560ee623fd84c3db986fccacaef7e9a31a8ea02c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 8 | Churn Cumulative: 1055 | Contributors (this commit): 43 | Commits (past 90d): 8 | Contributors (cumulative): 57 | DMM Complexity: None\n\nDIFF:\n@@ -319,7 +319,7 @@ class FilesPipeline(MediaPipeline):\n \n         gcs_store = cls.STORE_SCHEMES['gs']\n         gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n-        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL']\n+        gcs_store.POLICY = settings['FILES_STORE_GCS_ACL'] or None\n \n         store_uri = settings['FILES_STORE']\n         return cls(store_uri, settings=settings)\n\n@@ -93,7 +93,7 @@ class ImagesPipeline(FilesPipeline):\n \n         gcs_store = cls.STORE_SCHEMES['gs']\n         gcs_store.GCS_PROJECT_ID = settings['GCS_PROJECT_ID']\n-        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL']\n+        gcs_store.POLICY = settings['IMAGES_STORE_GCS_ACL'] or None\n \n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n\n@@ -159,7 +159,7 @@ FEED_EXPORTERS_BASE = {\n FEED_EXPORT_INDENT = 0\n \n FILES_STORE_S3_ACL = 'private'\n-FILES_STORE_GCS_ACL = None\n+FILES_STORE_GCS_ACL = ''\n \n FTP_USER = 'anonymous'\n FTP_PASSWORD = 'guest'\n@@ -182,7 +182,7 @@ HTTPPROXY_ENABLED = True\n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n IMAGES_STORE_S3_ACL = 'private'\n-IMAGES_STORE_GCS_ACL = None\n+IMAGES_STORE_GCS_ACL = ''\n \n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e75f721c04446f8f28d3bdfcd69f967f65981407", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 166 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -58,8 +58,8 @@ class CookieJar(object):\n     def clear_session_cookies(self, *args, **kwargs):\n         return self.jar.clear_session_cookies(*args, **kwargs)\n \n-    def clear(self):\n-        return self.jar.clear()\n+    def clear(self, domain=None, path=None, name=None):\n+        return self.jar.clear(domain, path, name)\n \n     def __iter__(self):\n         return iter(self.jar)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2dfc5d128bba42e2fe2bb24c0326fe47b0d3cd97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 530 | Contributors (this commit): 40 | Commits (past 90d): 4 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,7 @@ DEFAULT_REQUEST_HEADERS = {\n }\n \n DEPTH_LIMIT = 0\n-DEPTH_STATS = True\n+DEPTH_STATS_VERBOSE = False\n DEPTH_PRIORITY = 0\n \n DNSCACHE_ENABLED = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a182c955273745daf334033944c82da3aa4eb12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 6 | Churn Cumulative: 18 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ logger = logging.getLogger(__name__)\n \n class DepthMiddleware(object):\n \n-    def __init__(self, maxdepth, stats=None, verbose_stats=False, prio=1):\n+    def __init__(self, maxdepth, stats, verbose_stats=False, prio=1):\n         self.maxdepth = maxdepth\n         self.stats = stats\n         self.verbose_stats = verbose_stats\n@@ -41,7 +41,7 @@ class DepthMiddleware(object):\n                         extra={'spider': spider}\n                     )\n                     return False\n-                elif self.stats:\n+                else:\n                     if self.verbose_stats:\n                         self.stats.inc_value('request_depth_count/%s' % depth,\n                                              spider=spider)\n@@ -50,7 +50,7 @@ class DepthMiddleware(object):\n             return True\n \n         # base case (depth=0)\n-        if self.stats and 'depth' not in response.meta:\n+        if 'depth' not in response.meta:\n             response.meta['depth'] = 0\n             if self.verbose_stats:\n                 self.stats.inc_value('request_depth_count/0', spider=spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b364d27247b2d9b86c164569c7e0459fa3f8391b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 78 | Files Changed: 9 | Hunks: 78 | Methods Changed: 55 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 189 | Churn Cumulative: 2979 | Contributors (this commit): 21 | Commits (past 90d): 11 | Contributors (cumulative): 62 | DMM Complexity: 1.0\n\nDIFF:\n@@ -192,9 +192,15 @@ class MockServer():\n \n     def __enter__(self):\n         from scrapy.utils.test import get_testenv\n+\n         self.proc = Popen([sys.executable, '-u', '-m', 'tests.mockserver'],\n                           stdout=PIPE, env=get_testenv())\n-        self.proc.stdout.readline()\n+        http_address = self.proc.stdout.readline().strip().decode('ascii')\n+        https_address = self.proc.stdout.readline().strip().decode('ascii')\n+\n+        self.http_address = http_address\n+        self.https_address = https_address\n+\n         return self\n \n     def __exit__(self, exc_type, exc_value, traceback):\n@@ -202,6 +208,12 @@ class MockServer():\n         self.proc.wait()\n         time.sleep(0.2)\n \n+    def url(self, path, is_secure=False):\n+        host = self.http_address\n+        if is_secure:\n+            host = self.https_address\n+        return host + path\n+\n \n def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.crt'):\n     return ssl.DefaultOpenSSLContextFactory(\n@@ -213,14 +225,17 @@ def ssl_context_factory(keyfile='keys/localhost.key', certfile='keys/localhost.c\n if __name__ == \"__main__\":\n     root = Root()\n     factory = Site(root)\n-    httpPort = reactor.listenTCP(8998, factory)\n+    httpPort = reactor.listenTCP(0, factory)\n     contextFactory = ssl_context_factory()\n-    httpsPort = reactor.listenSSL(8999, factory, contextFactory)\n+    httpsPort = reactor.listenSSL(0, factory, contextFactory)\n \n     def print_listening():\n         httpHost = httpPort.getHost()\n         httpsHost = httpsPort.getHost()\n-        print(\"Mock server running at http://%s:%d and https://%s:%d\" % (\n-            httpHost.host, httpHost.port, httpsHost.host, httpsHost.port))\n+        httpAddress = 'http://%s:%d' % (httpHost.host, httpHost.port)\n+        httpsAddress = 'https://%s:%d' % (httpsHost.host, httpsHost.port)\n+        print(httpAddress)\n+        print(httpsAddress)\n+\n     reactor.callWhenRunning(print_listening)\n     reactor.run()\n\n@@ -11,7 +11,12 @@ from scrapy.item import Item\n from scrapy.linkextractors import LinkExtractor\n \n \n-class MetaSpider(Spider):\n+class MockServerSpider(Spider):\n+    def __init__(self, mockserver=None, *args, **kwargs):\n+        super(MockServerSpider, self).__init__(*args, **kwargs)\n+        self.mockserver = mockserver\n+\n+class MetaSpider(MockServerSpider):\n \n     name = 'meta'\n \n@@ -33,7 +38,7 @@ class FollowAllSpider(MetaSpider):\n         self.urls_visited = []\n         self.times = []\n         qargs = {'total': total, 'show': show, 'order': order, 'maxlatency': maxlatency}\n-        url = \"http://localhost:8998/follow?%s\" % urlencode(qargs, doseq=1)\n+        url = self.mockserver.url(\"/follow?%s\" % urlencode(qargs, doseq=1))\n         self.start_urls = [url]\n \n     def parse(self, response):\n@@ -55,7 +60,7 @@ class DelaySpider(MetaSpider):\n \n     def start_requests(self):\n         self.t1 = time.time()\n-        url = \"http://localhost:8998/delay?n=%s&b=%s\" % (self.n, self.b)\n+        url = self.mockserver.url(\"/delay?n=%s&b=%s\" % (self.n, self.b))\n         yield Request(url, callback=self.parse, errback=self.errback)\n \n     def parse(self, response):\n@@ -121,7 +126,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n \n         for s in range(100):\n             qargs = {'total': 10, 'seed': s}\n-            url = \"http://localhost:8998/follow?%s\" % urlencode(qargs, doseq=1)\n+            url = self.mockserver.url(\"/follow?%s\") % urlencode(qargs, doseq=1)\n             yield Request(url, meta={'seed': s})\n             if self.fail_yielding:\n                 2 / 0\n@@ -160,7 +165,7 @@ class SingleRequestSpider(MetaSpider):\n             return self.errback_func(failure)\n \n \n-class DuplicateStartRequestsSpider(Spider):\n+class DuplicateStartRequestsSpider(MockServerSpider):\n     dont_filter = True\n     name = 'duplicatestartrequests'\n     distinct_urls = 2\n@@ -169,7 +174,7 @@ class DuplicateStartRequestsSpider(Spider):\n     def start_requests(self):\n         for i in range(0, self.distinct_urls):\n             for j in range(0, self.dupe_factor):\n-                url = \"http://localhost:8998/echo?headers=1&body=test%d\" % i\n+                url = self.mockserver.url(\"/echo?headers=1&body=test%d\" % i)\n                 yield Request(url, dont_filter=self.dont_filter)\n \n     def __init__(self, url=\"http://localhost:8998\", *args, **kwargs):\n\n@@ -18,7 +18,7 @@ class TestCloseSpider(TestCase):\n     def test_closespider_itemcount(self):\n         close_on = 5\n         crawler = get_crawler(ItemSpider, {'CLOSESPIDER_ITEMCOUNT': close_on})\n-        yield crawler.crawl()\n+        yield crawler.crawl(mockserver=self.mockserver)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_itemcount')\n         itemcount = crawler.stats.get_value('item_scraped_count')\n@@ -28,7 +28,7 @@ class TestCloseSpider(TestCase):\n     def test_closespider_pagecount(self):\n         close_on = 5\n         crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_PAGECOUNT': close_on})\n-        yield crawler.crawl()\n+        yield crawler.crawl(mockserver=self.mockserver)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_pagecount')\n         pagecount = crawler.stats.get_value('response_received_count')\n@@ -38,7 +38,7 @@ class TestCloseSpider(TestCase):\n     def test_closespider_errorcount(self):\n         close_on = 5\n         crawler = get_crawler(ErrorSpider, {'CLOSESPIDER_ERRORCOUNT': close_on})\n-        yield crawler.crawl(total=1000000)\n+        yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_errorcount')\n         key = 'spider_exceptions/{name}'\\\n@@ -50,7 +50,7 @@ class TestCloseSpider(TestCase):\n     def test_closespider_timeout(self):\n         close_on = 0.1\n         crawler = get_crawler(FollowAllSpider, {'CLOSESPIDER_TIMEOUT': close_on})\n-        yield crawler.crawl(total=1000000)\n+        yield crawler.crawl(total=1000000, mockserver=self.mockserver)\n         reason = crawler.spider.meta['close_reason']\n         self.assertEqual(reason, 'closespider_timeout')\n         stats = crawler.stats\n\n@@ -26,7 +26,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_follow_all(self):\n         crawler = self.runner.create_crawler(FollowAllSpider)\n-        yield crawler.crawl()\n+        yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n \n     @defer.inlineCallbacks\n@@ -42,7 +42,7 @@ class CrawlTestCase(TestCase):\n     def _test_delay(self, delay, randomize):\n         settings = {\"DOWNLOAD_DELAY\": delay, 'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n         crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)\n-        yield crawler.crawl(maxlatency=delay * 2)\n+        yield crawler.crawl(maxlatency=delay * 2, mockserver=self.mockserver)\n         t = crawler.spider.times\n         totaltime = t[-1] - t[0]\n         avgd = totaltime / (len(t) - 1)\n@@ -53,7 +53,7 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n         crawler = self.runner.create_crawler(DelaySpider)\n-        yield crawler.crawl(n=0.5)\n+        yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 > 0)\n         self.assertTrue(crawler.spider.t2 > crawler.spider.t1)\n@@ -61,13 +61,13 @@ class CrawlTestCase(TestCase):\n     @defer.inlineCallbacks\n     def test_timeout_failure(self):\n         crawler = CrawlerRunner({\"DOWNLOAD_TIMEOUT\": 0.35}).create_crawler(DelaySpider)\n-        yield crawler.crawl(n=0.5)\n+        yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n         self.assertTrue(crawler.spider.t2_err > 0)\n         self.assertTrue(crawler.spider.t2_err > crawler.spider.t1)\n         # server hangs after receiving response headers\n-        yield crawler.crawl(n=0.5, b=1)\n+        yield crawler.crawl(n=0.5, b=1, mockserver=self.mockserver)\n         self.assertTrue(crawler.spider.t1 > 0)\n         self.assertTrue(crawler.spider.t2 == 0)\n         self.assertTrue(crawler.spider.t2_err > 0)\n@@ -77,14 +77,14 @@ class CrawlTestCase(TestCase):\n     def test_retry_503(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"http://localhost:8998/status?n=503\")\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_retry_conn_failed(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"http://localhost:65432/status?n=503\")\n+            yield crawler.crawl(\"http://localhost:65432/status?n=503\", mockserver=self.mockserver)\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n@@ -92,14 +92,14 @@ class CrawlTestCase(TestCase):\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n             # try to fetch the homepage of a non-existent domain\n-            yield crawler.crawl(\"http://dns.resolution.invalid./\")\n+            yield crawler.crawl(\"http://dns.resolution.invalid./\", mockserver=self.mockserver)\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n         with LogCapture('scrapy', level=logging.ERROR) as l:\n             crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n-            yield crawler.crawl(fail_before_yield=1)\n+            yield crawler.crawl(fail_before_yield=1, mockserver=self.mockserver)\n \n         self.assertEqual(len(l.records), 1)\n         record = l.records[0]\n@@ -110,7 +110,7 @@ class CrawlTestCase(TestCase):\n     def test_start_requests_bug_yielding(self):\n         with LogCapture('scrapy', level=logging.ERROR) as l:\n             crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n-            yield crawler.crawl(fail_yielding=1)\n+            yield crawler.crawl(fail_yielding=1, mockserver=self.mockserver)\n \n         self.assertEqual(len(l.records), 1)\n         record = l.records[0]\n@@ -121,7 +121,7 @@ class CrawlTestCase(TestCase):\n     def test_start_requests_lazyness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = CrawlerRunner(settings).create_crawler(BrokenStartRequestsSpider)\n-        yield crawler.crawl()\n+        yield crawler.crawl(mockserver=self.mockserver)\n         #self.assertTrue(False, crawler.spider.seedsseen)\n         #self.assertTrue(crawler.spider.seedsseen.index(None) < crawler.spider.seedsseen.index(99),\n         #                crawler.spider.seedsseen)\n@@ -130,10 +130,10 @@ class CrawlTestCase(TestCase):\n     def test_start_requests_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = CrawlerRunner(settings).create_crawler(DuplicateStartRequestsSpider)\n-        yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3)\n+        yield crawler.crawl(dont_filter=True, distinct_urls=2, dupe_factor=3, mockserver=self.mockserver)\n         self.assertEqual(crawler.spider.visited, 6)\n \n-        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4)\n+        yield crawler.crawl(dont_filter=False, distinct_urls=3, dupe_factor=4, mockserver=self.mockserver)\n         self.assertEqual(crawler.spider.visited, 3)\n \n     @defer.inlineCallbacks\n@@ -160,7 +160,7 @@ with multiples lines\n '''})\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"http://localhost:8998/raw?{0}\".format(query))\n+            yield crawler.crawl(self.mockserver.url(\"/raw?{0}\".format(query)), mockserver=self.mockserver)\n         self.assertEqual(str(l).count(\"Got response 200\"), 1)\n \n     @defer.inlineCallbacks\n@@ -168,7 +168,7 @@ with multiples lines\n         # connection lost after receiving data\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"http://localhost:8998/drop?abort=0\")\n+            yield crawler.crawl(self.mockserver.url(\"/drop?abort=0\"), mockserver=self.mockserver)\n         self._assert_retried(l)\n \n     @defer.inlineCallbacks\n@@ -176,7 +176,7 @@ with multiples lines\n         # connection lost before receiving data\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"http://localhost:8998/drop?abort=1\")\n+            yield crawler.crawl(self.mockserver.url(\"/drop?abort=1\"), mockserver=self.mockserver)\n         self._assert_retried(l)\n \n     def _assert_retried(self, log):\n@@ -186,7 +186,7 @@ with multiples lines\n     @defer.inlineCallbacks\n     def test_referer_header(self):\n         \"\"\"Referer header is set by RefererMiddleware unless it is already set\"\"\"\n-        req0 = Request('http://localhost:8998/echo?headers=1&body=0', dont_filter=1)\n+        req0 = Request(self.mockserver.url('/echo?headers=1&body=0'), dont_filter=1)\n         req1 = req0.replace()\n         req2 = req0.replace(headers={'Referer': None})\n         req3 = req0.replace(headers={'Referer': 'http://example.com'})\n@@ -194,7 +194,7 @@ with multiples lines\n         req1.meta['next'] = req2\n         req2.meta['next'] = req3\n         crawler = self.runner.create_crawler(SingleRequestSpider)\n-        yield crawler.crawl(seed=req0)\n+        yield crawler.crawl(seed=req0, mockserver=self.mockserver)\n         # basic asserts in case of weird communication errors\n         self.assertIn('responses', crawler.spider.meta)\n         self.assertNotIn('failures', crawler.spider.meta)\n@@ -220,7 +220,7 @@ with multiples lines\n             est.append(get_engine_status(crawler.engine))\n \n         crawler = self.runner.create_crawler(SingleRequestSpider)\n-        yield crawler.crawl(seed='http://localhost:8998/', callback_func=cb)\n+        yield crawler.crawl(seed=self.mockserver.url('/'), callback_func=cb, mockserver=self.mockserver)\n         self.assertEqual(len(est), 1, est)\n         s = dict(est[0])\n         self.assertEqual(s['engine.spider.name'], crawler.spider.name)\n@@ -244,7 +244,7 @@ with multiples lines\n                 raise TestError\n \n         crawler = self.runner.create_crawler(FaultySpider)\n-        yield self.assertFailure(crawler.crawl(), TestError)\n+        yield self.assertFailure(crawler.crawl(mockserver=self.mockserver), TestError)\n         self.assertFalse(crawler.crawling)\n \n     @defer.inlineCallbacks\n@@ -256,7 +256,7 @@ with multiples lines\n         }\n         crawler = CrawlerRunner(settings).create_crawler(SimpleSpider)\n         yield self.assertFailure(\n-            self.runner.crawl(crawler, \"http://localhost:8998/status?n=200\"),\n+            self.runner.crawl(crawler, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver),\n             ZeroDivisionError)\n         self.assertFalse(crawler.crawling)\n \n@@ -264,13 +264,13 @@ with multiples lines\n     def test_crawlerrunner_accepts_crawler(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            yield self.runner.crawl(crawler, \"http://localhost:8998/status?n=200\")\n+            yield self.runner.crawl(crawler, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n         self.assertIn(\"Got response 200\", str(log))\n \n     @defer.inlineCallbacks\n     def test_crawl_multiple(self):\n-        self.runner.crawl(SimpleSpider, \"http://localhost:8998/status?n=200\")\n-        self.runner.crawl(SimpleSpider, \"http://localhost:8998/status?n=503\")\n+        self.runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        self.runner.crawl(SimpleSpider, self.mockserver.url(\"/status?n=503\"), mockserver=self.mockserver)\n \n         with LogCapture() as log:\n             yield self.runner.join()\n\n@@ -534,14 +534,14 @@ class Http11MockServerTestCase(unittest.TestCase):\n         crawler = get_crawler(SingleRequestSpider)\n         # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n         # download it\n-        yield crawler.crawl(seed=Request(url='http://localhost:8998/partial', meta={'download_maxsize': 1000}))\n+        yield crawler.crawl(seed=Request(url=self.mockserver.url('/partial'), meta={'download_maxsize': 1000}))\n         failure = crawler.spider.meta['failure']\n         self.assertIsInstance(failure.value, defer.CancelledError)\n \n     @defer.inlineCallbacks\n     def test_download(self):\n         crawler = get_crawler(SingleRequestSpider)\n-        yield crawler.crawl(seed=Request(url='http://localhost:8998'))\n+        yield crawler.crawl(seed=Request(url=self.mockserver.url('')))\n         failure = crawler.spider.meta.get('failure')\n         self.assertTrue(failure == None)\n         reason = crawler.spider.meta['close_reason']\n@@ -551,7 +551,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n     def test_download_gzip_response(self):\n         crawler = get_crawler(SingleRequestSpider)\n         body = b'1' * 100  # PayloadResource requires body length to be 100\n-        request = Request('http://localhost:8998/payload', method='POST',\n+        request = Request(self.mockserver.url('/payload'), method='POST',\n                           body=body, meta={'download_maxsize': 50})\n         yield crawler.crawl(seed=request)\n         failure = crawler.spider.meta['failure']\n@@ -560,7 +560,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n         if six.PY2:\n             request.headers.setdefault(b'Accept-Encoding', b'gzip,deflate')\n-            request = request.replace(url='http://localhost:8998/xpayload')\n+            request = request.replace(url=self.mockserver.url('/xpayload'))\n             yield crawler.crawl(seed=request)\n             # download_maxsize = 50 is enough for the gzipped response\n             failure = crawler.spider.meta.get('failure')\n\n@@ -179,6 +179,7 @@ class FeedExportTest(unittest.TestCase):\n         try:\n             with MockServer() as s:\n                 runner = CrawlerRunner(Settings(defaults))\n+                spider_cls.start_urls = [s.url('/')]\n                 yield runner.crawl(spider_cls)\n \n             with open(res_name, 'rb') as f:\n@@ -194,7 +195,6 @@ class FeedExportTest(unittest.TestCase):\n         \"\"\"\n         class TestSpider(scrapy.Spider):\n             name = 'testspider'\n-            start_urls = ['http://localhost:8998/']\n \n             def parse(self, response):\n                 for item in items:\n@@ -210,7 +210,6 @@ class FeedExportTest(unittest.TestCase):\n         \"\"\"\n         class TestSpider(scrapy.Spider):\n             name = 'testspider'\n-            start_urls = ['http://localhost:8998/']\n \n             def parse(self, response):\n                 pass\n\n@@ -46,7 +46,7 @@ class RedirectedMediaDownloadSpider(MediaDownloadSpider):\n \n     def _process_url(self, url):\n         return add_or_replace_parameter(\n-                    'http://localhost:8998/redirect-to',\n+                    self.mockserver.url('/redirect-to'),\n                     'goto', url)\n \n \n@@ -134,7 +134,7 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media(self):\n         crawler = self._create_crawler(MediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key)\n         self._assert_files_downloaded(self.items, str(log))\n@@ -143,7 +143,7 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media_wrong_urls(self):\n         crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key)\n         self._assert_files_download_failure(crawler, self.items, 404, str(log))\n@@ -152,9 +152,10 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media_redirected_default_failure(self):\n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n-                media_urls_key=self.media_urls_key)\n+                media_urls_key=self.media_urls_key,\n+                mockserver=self.mockserver)\n         self._assert_files_download_failure(crawler, self.items, 302, str(log))\n \n     @defer.inlineCallbacks\n@@ -165,9 +166,10 @@ class FileDownloadCrawlTestCase(TestCase):\n \n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(\"http://localhost:8998/files/images/\",\n+            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n-                media_urls_key=self.media_urls_key)\n+                media_urls_key=self.media_urls_key,\n+                mockserver=self.mockserver)\n         self._assert_files_downloaded(self.items, str(log))\n         self.assertEqual(crawler.stats.get_value('downloader/response_status_count/302'), 3)\n \n\n@@ -2,6 +2,7 @@ import json\n import os\n import time\n \n+from six.moves.urllib.parse import urlsplit, urlunsplit\n from threading import Thread\n from libmproxy import controller, proxy\n from netlib import http_auth\n@@ -17,7 +18,7 @@ from tests.mockserver import MockServer\n \n class HTTPSProxy(controller.Master, Thread):\n \n-    def __init__(self, port):\n+    def __init__(self):\n         password_manager = http_auth.PassManSingleUser('scrapy', 'scrapy')\n         authenticator = http_auth.BasicProxyAuth(password_manager, \"mitmproxy\")\n         cert_path = os.path.join(os.path.abspath(os.path.dirname(__file__)),\n@@ -25,10 +26,19 @@ class HTTPSProxy(controller.Master, Thread):\n         server = proxy.ProxyServer(proxy.ProxyConfig(\n             authenticator = authenticator,\n             cacert = cert_path),\n-            port)\n+            0)\n+        self.server = server\n         Thread.__init__(self)\n         controller.Master.__init__(self, server)\n \n+    def http_address(self):\n+        return 'http://scrapy:scrapy@%s:%d' % self.server.socket.getsockname()\n+\n+\n+def _wrong_credentials(proxy_url):\n+    bad_auth_proxy = list(urlsplit(proxy_url))\n+    bad_auth_proxy[1] = bad_auth_proxy[1].replace('scrapy:scrapy@', 'wrong:wronger@')\n+    return urlunsplit(bad_auth_proxy)\n \n class ProxyConnectTestCase(TestCase):\n \n@@ -36,12 +46,14 @@ class ProxyConnectTestCase(TestCase):\n         self.mockserver = MockServer()\n         self.mockserver.__enter__()\n         self._oldenv = os.environ.copy()\n-        self._proxy = HTTPSProxy(8888)\n+\n+        self._proxy = HTTPSProxy()\n         self._proxy.start()\n+\n         # Wait for the proxy to start.\n         time.sleep(1.0)\n-        os.environ['http_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n-        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n+        os.environ['https_proxy'] = self._proxy.http_address()\n+        os.environ['http_proxy'] = self._proxy.http_address()\n \n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n@@ -52,17 +64,17 @@ class ProxyConnectTestCase(TestCase):\n     def test_https_connect_tunnel(self):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n \n     @defer.inlineCallbacks\n     def test_https_noconnect(self):\n-        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888?noconnect'\n+        proxy = os.environ['https_proxy']\n+        os.environ['https_proxy'] = proxy + '?noconnect'\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, l)\n-        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n     @defer.inlineCallbacks\n     def test_https_connect_tunnel_error(self):\n@@ -73,18 +85,17 @@ class ProxyConnectTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n-        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888'\n+        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy'])\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         # The proxy returns a 407 error code but it does not reach the client;\n         # he just sees a TunnelError.\n         self._assert_got_tunnel_error(l)\n-        os.environ['https_proxy'] = 'http://scrapy:scrapy@localhost:8888'\n \n     @defer.inlineCallbacks\n     def test_https_tunnel_without_leak_proxy_authorization_header(self):\n-        request = Request(\"https://localhost:8999/echo\")\n+        request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n         crawler = get_crawler(SingleRequestSpider)\n         with LogCapture() as l:\n             yield crawler.crawl(seed=request)\n@@ -94,10 +105,10 @@ class ProxyConnectTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_https_noconnect_auth_error(self):\n-        os.environ['https_proxy'] = 'http://wrong:wronger@localhost:8888?noconnect'\n+        os.environ['https_proxy'] = _wrong_credentials(os.environ['https_proxy']) + '?noconnect'\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as l:\n-            yield crawler.crawl(\"https://localhost:8999/status?n=200\")\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(407, l)\n \n     def _assert_got_response_code(self, code, log):\n\n@@ -11,20 +11,21 @@ from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n from scrapy.spidermiddlewares.httperror import HttpErrorMiddleware, HttpError\n from scrapy.settings import Settings\n+from tests.spiders import MockServerSpider\n \n \n-class _HttpErrorSpider(Spider):\n+class _HttpErrorSpider(MockServerSpider):\n     name = 'httperror'\n-    start_urls = [\n-        \"http://localhost:8998/status?n=200\",\n-        \"http://localhost:8998/status?n=404\",\n-        \"http://localhost:8998/status?n=402\",\n-        \"http://localhost:8998/status?n=500\",\n-    ]\n     bypass_status_codes = set()\n \n     def __init__(self, *args, **kwargs):\n         super(_HttpErrorSpider, self).__init__(*args, **kwargs)\n+        self.start_urls = [\n+           self.mockserver.url(\"/status?n=200\"),\n+           self.mockserver.url(\"/status?n=404\"),\n+           self.mockserver.url(\"/status?n=402\"),\n+           self.mockserver.url(\"/status?n=500\"),\n+        ]\n         self.failed = set()\n         self.skipped = set()\n         self.parsed = set()\n@@ -169,7 +170,7 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n     @defer.inlineCallbacks\n     def test_middleware_works(self):\n         crawler = get_crawler(_HttpErrorSpider)\n-        yield crawler.crawl()\n+        yield crawler.crawl(mockserver=self.mockserver)\n         assert not crawler.spider.skipped, crawler.spider.skipped\n         self.assertEqual(crawler.spider.parsed, {'200'})\n         self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n@@ -184,7 +185,7 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n     def test_logging(self):\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(bypass_status_codes={402})\n+            yield crawler.crawl(mockserver=self.mockserver, bypass_status_codes={402})\n         self.assertEqual(crawler.spider.parsed, {'200', '402'})\n         self.assertEqual(crawler.spider.skipped, {'402'})\n         self.assertEqual(crawler.spider.failed, {'404', '500'})\n@@ -199,7 +200,7 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         # HttpError logs ignored responses with level INFO\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture(level=logging.INFO) as log:\n-            yield crawler.crawl()\n+            yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(crawler.spider.parsed, {'200'})\n         self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n \n@@ -211,7 +212,7 @@ class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n         # with level WARNING, we shouldn't capture anything from HttpError\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture(level=logging.WARNING) as log:\n-            yield crawler.crawl()\n+            yield crawler.crawl(mockserver=self.mockserver)\n         self.assertEqual(crawler.spider.parsed, {'200'})\n         self.assertEqual(crawler.spider.failed, {'404', '402', '500'})\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a2d2c3b77bde1d74b46d7dbfb9488cb06e5021f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 160 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -30,25 +30,25 @@ def gunzip(data):\n     This is resilient to CRC checksum errors.\n     \"\"\"\n     f = GzipFile(fileobj=BytesIO(data))\n-    output = b''\n+    output_list = []\n     chunk = b'.'\n     while chunk:\n         try:\n             chunk = read1(f, 8196)\n-            output += chunk\n+            output_list.append(chunk)\n         except (IOError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n-            # some pages are quite small so output is '' and f.extrabuf\n+            # some pages are quite small so output_list is empty and f.extrabuf\n             # contains the whole page content\n-            if output or getattr(f, 'extrabuf', None):\n+            if output_list or getattr(f, 'extrabuf', None):\n                 try:\n-                    output += f.extrabuf[-f.extrasize:]\n+                    output_list.append(f.extrabuf[-f.extrasize:])\n                 finally:\n                     break\n             else:\n                 raise\n-    return output\n+    return b''.join(output_list)\n \n _is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search\n _is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#72d0899bce06190de5a453b24dd66c8910e6d0ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 1 | Files Changed: 5 | Hunks: 11 | Methods Changed: 13 | Complexity Δ (Sum/Max): 12/8 | Churn Δ: 84 | Churn Cumulative: 2865 | Contributors (this commit): 25 | Commits (past 90d): 5 | Contributors (cumulative): 51 | DMM Complexity: 0.6086956521739131\n\nDIFF:\n@@ -56,3 +56,6 @@ class Command(ScrapyCommand):\n \n         self.crawler_process.crawl(spname, **opts.spargs)\n         self.crawler_process.start()\n+\n+        if self.crawler_process.bootstrap_failed:\n+            self.exitcode = 1\n\n@@ -87,3 +87,6 @@ class Command(ScrapyCommand):\n \n         self.crawler_process.crawl(spidercls, **opts.spargs)\n         self.crawler_process.start()\n+\n+        if self.crawler_process.bootstrap_failed:\n+            self.exitcode = 1\n\n@@ -137,6 +137,7 @@ class CrawlerRunner(object):\n         self.spider_loader = _get_spider_loader(settings)\n         self._crawlers = set()\n         self._active = set()\n+        self.bootstrap_failed = False\n \n     @property\n     def spiders(self):\n@@ -178,6 +179,7 @@ class CrawlerRunner(object):\n         def _done(result):\n             self.crawlers.discard(crawler)\n             self._active.discard(d)\n+            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n             return result\n \n         return d.addBoth(_done)\n\n@@ -1,3 +1,4 @@\n+import inspect\n import os\n import sys\n import subprocess\n@@ -17,6 +18,7 @@ from scrapy.utils.python import retry_on_eintr\n from scrapy.utils.test import get_testenv\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n+from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n class ProjectTest(unittest.TestCase):\n@@ -220,6 +222,16 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"INFO: Closing spider (finished)\", log)\n         self.assertIn(\"INFO: Spider closed (finished)\", log)\n \n+    def test_run_fail_spider(self):\n+        proc = self.runspider(\"import scrapy\\n\" + inspect.getsource(ExceptionSpider))\n+        ret = proc.returncode\n+        self.assertNotEqual(ret, 0)\n+\n+    def test_run_good_spider(self):\n+        proc = self.runspider(\"import scrapy\\n\" + inspect.getsource(NoRequestsSpider))\n+        ret = proc.returncode\n+        self.assertEqual(ret, 0)\n+\n     def test_runspider_log_level(self):\n         log = self.get_log(self.debug_log_spider,\n                            args=('-s', 'LOG_LEVEL=INFO'))\n\n@@ -4,6 +4,9 @@ import tempfile\n import warnings\n import unittest\n \n+from twisted.internet import defer\n+import twisted.trial.unittest\n+\n import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n@@ -11,9 +14,9 @@ from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n+from scrapy.utils.test import get_crawler\n from scrapy.extensions.throttle import AutoThrottle\n \n-\n class BaseCrawlerTest(unittest.TestCase):\n \n     def assertOptionIsDefault(self, settings, key):\n@@ -181,3 +184,62 @@ class CrawlerProcessTest(BaseCrawlerTest):\n     def test_crawler_process_accepts_None(self):\n         runner = CrawlerProcess()\n         self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+\n+\n+class ExceptionSpider(scrapy.Spider):\n+    name = 'exception'\n+\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        raise ValueError('Exception in from_crawler method')\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+class CrawlerRunnerHasSpider(twisted.trial.unittest.TestCase):\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_runner_bootstrap_successful(self):\n+        runner = CrawlerRunner()\n+        yield runner.crawl(NoRequestsSpider)\n+        self.assertEqual(runner.bootstrap_failed, False)\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_runner_bootstrap_successful_for_several(self):\n+        runner = CrawlerRunner()\n+        yield runner.crawl(NoRequestsSpider)\n+        yield runner.crawl(NoRequestsSpider)\n+        self.assertEqual(runner.bootstrap_failed, False)\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_runner_bootstrap_failed(self):\n+        runner = CrawlerRunner()\n+\n+        try:\n+            yield runner.crawl(ExceptionSpider)\n+        except ValueError:\n+            pass\n+        else:\n+            self.fail('Exception should be raised from spider')\n+\n+        self.assertEqual(runner.bootstrap_failed, True)\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_runner_bootstrap_failed_for_several(self):\n+        runner = CrawlerRunner()\n+\n+        try:\n+            yield runner.crawl(ExceptionSpider)\n+        except ValueError:\n+            pass\n+        else:\n+            self.fail('Exception should be raised from spider')\n+\n+        yield runner.crawl(NoRequestsSpider)\n+\n+        self.assertEqual(runner.bootstrap_failed, True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
