{"custom_id": "scrapy#dd80f6acdf3d181a158013077893258003ef72e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 16 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 190 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,8 +27,8 @@ class MemoryUsage(object):\n             raise NotConfigured\n         if not os.path.exists('/proc'):\n             raise NotConfigured\n-        self.warned = False\n \n+        self.warned = False\n         self.notify_mails = settings.getlist('MEMUSAGE_NOTIFY')\n         self.limit = settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n@@ -37,13 +37,11 @@ class MemoryUsage(object):\n         dispatcher.connect(self.engine_started, signal=signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n-\n-    @property\n-    def virtual(self):\n+    def get_virtual_size(self):\n         return get_vmvalue_from_procfs('VmSize')\n \n     def engine_started(self):\n-        stats.set_value('memusage/startup', self.virtual)\n+        stats.set_value('memusage/startup', self.get_virtual_size())\n         self.tasks = []\n         tsk = task.LoopingCall(self.update)\n         self.tasks.append(tsk)\n@@ -63,10 +61,10 @@ class MemoryUsage(object):\n                 tsk.stop()\n \n     def update(self):\n-        stats.max_value('memusage/max', self.virtual)\n+        stats.max_value('memusage/max', self.get_virtual_size())\n \n     def _check_limit(self):\n-        if self.virtual > self.limit:\n+        if self.get_virtual_size() > self.limit:\n             stats.set_value('memusage/limit_reached', 1)\n             mem = self.limit/1024/1024\n             log.msg(\"Memory usage exceeded %dM. Shutting down Scrapy...\" % mem, level=log.ERROR)\n@@ -80,7 +78,7 @@ class MemoryUsage(object):\n     def _check_warning(self):\n         if self.warned: # warn only once\n             return\n-        if self.virtual > self.warning:\n+        if self.get_virtual_size() > self.warning:\n             stats.set_value('memusage/warning_reached', 1)\n             mem = self.warning/1024/1024\n             log.msg(\"Memory usage reached %dM\" % mem, level=log.WARNING)\n@@ -95,7 +93,7 @@ class MemoryUsage(object):\n         \"\"\"send notification mail with some additional useful info\"\"\"\n         s = \"Memory usage at engine startup : %dM\\r\\n\" % (stats.get_value('memusage/startup')/1024/1024)\n         s += \"Maximum memory usage           : %dM\\r\\n\" % (stats.get_value('memusage/max')/1024/1024)\n-        s += \"Current memory usage           : %dM\\r\\n\" % (self.virtual/1024/1024)\n+        s += \"Current memory usage           : %dM\\r\\n\" % (self.get_virtual_size()/1024/1024)\n \n         s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n         s += \"\\r\\n\"\n\n@@ -13,9 +13,9 @@ from time import time\n \n from scrapy.xlib.pydispatch import dispatcher\n \n-from scrapy.extension import extensions\n from scrapy.core import signals\n from scrapy.core.exceptions import NotConfigured\n+from scrapy.utils.memory import get_vmvalue_from_procfs\n from scrapy.stats import stats\n from scrapy.conf import settings\n \n@@ -24,15 +24,18 @@ class SpiderProfiler(object):\n     def __init__(self):\n         if not settings.getbool('SPIDERPROFILER_ENABLED'):\n             raise NotConfigured\n+        try:\n+            get_vmvalue_from_procfs('VmSize')\n+        except RuntimeError:\n+            self._mem_tracking = False\n+        else:\n+            self._mem_tracking = True\n         dispatcher.connect(self._request_received, signals.request_received)\n-        dispatcher.connect(self._engine_started, signals.engine_started)\n-\n-    def _engine_started(self):\n-        self.memusage = extensions.enabled.get('MemoryUsage', None)\n \n     def _request_received(self, request, spider):\n         old_cbs = request.deferred.callbacks[0]\n-        new_cbs = ((self._profiled_callback(old_cbs[0][0], spider), old_cbs[0][1], old_cbs[0][2]), old_cbs[1])\n+        new_cbs = ((self._profiled_callback(old_cbs[0][0], spider), old_cbs[0][1], \\\n+            old_cbs[0][2]), old_cbs[1])\n         request.deferred.callbacks[0] = new_cbs\n \n     def _profiled_callback(self, function, spider):\n@@ -45,16 +48,19 @@ class SpiderProfiler(object):\n             domain = spider.domain_name\n             tcc = stats.get_value('profiling/total_callback_time', 0, domain=domain)\n             sct = stats.get_value('profiling/slowest_callback_time', 0, domain=domain)\n-            stats.set_value('profiling/total_callback_time' % spider.domain_name, tcc+ct, domain=domain)\n+            stats.set_value('profiling/total_callback_time' % spider.domain_name, \\\n+                tcc+ct, domain=domain)\n             if ct > sct:\n                 stats.set_value('profiling/slowest_callback_time', ct, domain=domain)\n-                stats.set_value('profiling/slowest_callback_name', function.__name__, domain=domain)\n-                stats.set_value('profiling/slowest_callback_url', args[0].url, domain=domain)\n-            if self.memusage:\n-                tma = stats.get_value('profiling/total_mem_allocated_in_callbacks', 0, domain=domain)\n-                stats.set_value('profiling/total_mem_allocated_in_callbacks', tma+mafter-mbefore, domain=domain)\n+                stats.set_value('profiling/slowest_callback_name', function.__name__, \\\n+                    domain=domain)\n+                stats.set_value('profiling/slowest_callback_url', args[0].url, \\\n+                    domain=domain)\n+            if self._memusage:\n+                stats.inc_value('profiling/total_mem_allocated_in_callbacks', \\\n+                    count=mafter-mbefore, domain=domain)\n             return r\n         return new_callback\n \n     def _memusage(self):\n-        return self.memusage.virtual if self.memusage else 0.0\n+        return get_vmvalue_from_procfs('VmSize') if self._mem_tracking else 0.0\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8fab5249786e56355cc2aa5f96699cbf75a2f3c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 57 | Files Changed: 5 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/8 | Churn Δ: 121 | Churn Cumulative: 1122 | Contributors (this commit): 4 | Commits (past 90d): 62 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,6 +19,7 @@ from scrapy.mail import MailSender\n from scrapy.conf import settings\n from scrapy.stats import stats\n from scrapy.utils.memory import get_vmvalue_from_procfs\n+from scrapy.utils.engine import get_engine_status\n \n class MemoryUsage(object):\n     \n@@ -97,6 +98,6 @@ class MemoryUsage(object):\n \n         s += \"ENGINE STATUS ------------------------------------------------------- \\r\\n\"\n         s += \"\\r\\n\"\n-        s += scrapyengine.getstatus()\n+        s += get_engine_status()\n         s += \"\\r\\n\"\n         self.mail.send(rcpts, subject, s)\n\n@@ -2,7 +2,7 @@\n Scheduler information module for Scrapy webconsole\n \"\"\"\n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core.engine import scrapyengine\n+from scrapy.utils.engine import get_engine_status\n from scrapy.management.web import banner\n \n class EngineStatus(object):\n@@ -16,7 +16,7 @@ class EngineStatus(object):\n     def webconsole_render(self, wc_request):\n         s = banner(self)\n         s += \"<pre><code>\\n\"\n-        s += scrapyengine.getstatus()\n+        s += get_engine_status()\n         s += \"</pre></code>\\n\"\n         s += \"</body>\\n\"\n         s += \"</html>\\n\"\n\n@@ -286,56 +286,4 @@ class ExecutionEngine(object):\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n         self._mainloop()\n \n-    def getstatus(self):\n-        \"\"\"\n-        Return a report of the current engine status\n-        \"\"\"\n-        s = \"Execution engine status\\n\\n\"\n-\n-        global_tests = [\n-            \"datetime.utcnow()-self.start_time\", \n-            \"self.is_idle()\", \n-            \"self.scheduler.is_idle()\",\n-            \"len(self.scheduler.pending_requests)\",\n-            \"self.downloader.is_idle()\",\n-            \"len(self.downloader.sites)\",\n-            \"self.downloader.has_capacity()\",\n-            \"self.scraper.is_idle()\",\n-            \"len(self.scraper.sites)\",\n-            ]\n-        domain_tests = [\n-            \"self.domain_is_idle(domain)\",\n-            \"self.closing.get(domain)\",\n-            \"self.scheduler.domain_has_pending_requests(domain)\",\n-            \"len(self.scheduler.pending_requests[domain])\",\n-            \"len(self.downloader.sites[domain].queue)\",\n-            \"len(self.downloader.sites[domain].active)\",\n-            \"len(self.downloader.sites[domain].transferring)\",\n-            \"self.downloader.sites[domain].closing\",\n-            \"self.downloader.sites[domain].lastseen\",\n-            \"len(self.scraper.sites[domain].queue)\",\n-            \"len(self.scraper.sites[domain].active)\",\n-            \"self.scraper.sites[domain].active_size\",\n-            \"self.scraper.sites[domain].itemproc_size\",\n-            \"self.scraper.sites[domain].needs_backout()\",\n-            ]\n-\n-        for test in global_tests:\n-            try:\n-                s += \"%-47s : %s\\n\" % (test, eval(test))\n-            except Exception, e:\n-                s += \"%-47s : %s (exception)\\n\" % (test, type(e).__name__)\n-        s += \"\\n\"\n-        for domain in self.downloader.sites:\n-            s += \"%s\\n\" % domain\n-            for test in domain_tests:\n-                try:\n-                    s += \"  %-50s : %s\\n\" % (test, eval(test))\n-                except Exception, e:\n-                    s += \"  %-50s : %s (exception)\\n\" % (test, type(e).__name__)\n-        return s\n-\n-    def st(self): # shortcut for printing engine status (useful in telnet console)\n-        print self.getstatus()\n-\n scrapyengine = ExecutionEngine()\n\n@@ -16,6 +16,7 @@ from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.stats import stats\n from scrapy.utils.trackref import print_live_refs\n+from scrapy.utils.engine import print_engine_status\n from scrapy.conf import settings\n \n try:\n@@ -32,6 +33,7 @@ telnet_namespace = {\n     'stats': stats,\n     'spiders': spiders,\n     'settings': settings,\n+    'est': print_engine_status,\n     'p': pprint.pprint,\n     'prefs': print_live_refs,\n     'hpy': hpy,\n@@ -39,8 +41,7 @@ telnet_namespace = {\n \n def makeProtocol():\n     return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n-                                  insults.ServerProtocol,\n-                                  manhole.Manhole, telnet_namespace)\n+        insults.ServerProtocol, manhole.Manhole, telnet_namespace)\n \n class TelnetConsole(protocol.ServerFactory):\n \n\n@@ -0,0 +1,57 @@\n+\"\"\"Some debugging functions for working with the Scrapy engine\"\"\"\n+\n+from scrapy.core.engine import scrapyengine\n+\n+def get_engine_status(engine=None):\n+    \"\"\"Return a report of the current engine status\"\"\"\n+    if engine is None:\n+        engine = scrapyengine\n+\n+    global_tests = [\n+        \"datetime.utcnow()-engine.start_time\",\n+        \"engine.is_idle()\",\n+        \"engine.scheduler.is_idle()\",\n+        \"len(engine.scheduler.pending_requests)\",\n+        \"engine.downloader.is_idle()\",\n+        \"len(engine.downloader.sites)\",\n+        \"engine.downloader.has_capacity()\",\n+        \"engine.scraper.is_idle()\",\n+        \"len(engine.scraper.sites)\",\n+    ]\n+    domain_tests = [\n+        \"engine.domain_is_idle(domain)\",\n+        \"engine.closing.get(domain)\",\n+        \"engine.scheduler.domain_has_pending_requests(domain)\",\n+        \"len(engine.scheduler.pending_requests[domain])\",\n+        \"len(engine.downloader.sites[domain].queue)\",\n+        \"len(engine.downloader.sites[domain].active)\",\n+        \"len(engine.downloader.sites[domain].transferring)\",\n+        \"engine.downloader.sites[domain].closing\",\n+        \"engine.downloader.sites[domain].lastseen\",\n+        \"len(engine.scraper.sites[domain].queue)\",\n+        \"len(engine.scraper.sites[domain].active)\",\n+        \"engine.scraper.sites[domain].active_size\",\n+        \"engine.scraper.sites[domain].itemproc_size\",\n+        \"engine.scraper.sites[domain].needs_backout()\",\n+    ]\n+\n+    s = \"Execution engine status\\n\\n\"\n+\n+    for test in global_tests:\n+        try:\n+            s += \"%-47s : %s\\n\" % (test, eval(test))\n+        except Exception, e:\n+            s += \"%-47s : %s (exception)\\n\" % (test, type(e).__name__)\n+    s += \"\\n\"\n+    for domain in engine.downloader.sites:\n+        s += \"%s\\n\" % domain\n+        for test in domain_tests:\n+            try:\n+                s += \"  %-50s : %s\\n\" % (test, eval(test))\n+            except Exception, e:\n+                s += \"  %-50s : %s (exception)\\n\" % (test, type(e).__name__)\n+    return s\n+\n+def print_engine_status(engine=None):\n+    print get_engine_status(engine)\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#60299c49ca11ec9da8613b95e6b9cb1c39edd8f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 65 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -11,6 +11,7 @@ from twisted.conch.insults import insults\n from twisted.internet import reactor, protocol\n \n from scrapy.extension import extensions\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n@@ -47,7 +48,7 @@ class TelnetConsole(protocol.ServerFactory):\n \n     def __init__(self):\n         if not settings.getbool('TELNETCONSOLE_ENABLED'):\n-            return\n+            raise NotConfigured\n         self.protocol = makeProtocol\n         self.noisy = False\n         port = settings.getint('TELNETCONSOLE_PORT')\n\n@@ -10,8 +10,9 @@ from datetime import datetime\n \n from twisted.internet import reactor\n from twisted.web import server, resource\n-from scrapy.xlib.pydispatch import dispatcher\n \n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n \n@@ -84,8 +85,7 @@ class WebConsole(server.Site):\n \n     def __init__(self):\n         if not settings.getbool('WEBCONSOLE_ENABLED'):\n-            return\n-\n+            raise NotConfigured\n         logfile = settings['WEBCONSOLE_LOGFILE']\n         server.Site.__init__(self, WebConsoleResource(), logPath=logfile)\n         self.noisy = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6c1392f9b18905b2e3346891cec3e713ae02c8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 49 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.6944444444444444\n\nDIFF:\n@@ -0,0 +1,48 @@\n+from scrapy.command import ScrapyCommand\n+from scrapy.conf import settings as settings_\n+\n+class Command(ScrapyCommand):\n+\n+    requires_project = False\n+\n+    def syntax(self):\n+        return \"[options]\"\n+\n+    def short_desc(self):\n+        return \"Query Scrapy settings\"\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\", \\\n+            help=\"print raw setting value\")\n+        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\", \\\n+            help=\"print setting value, intepreted as a boolean\")\n+        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\", \\\n+            help=\"print setting value, intepreted as an integer\")\n+        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\", \\\n+            help=\"print setting value, intepreted as an float\")\n+        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\", \\\n+            help=\"print setting value, intepreted as an float\")\n+        parser.add_option(\"--init\", dest=\"init\", action=\"store_true\", \\\n+            help=\"print initial setting value (before loading extensions and spiders)\")\n+\n+    def process_options(self, args, opts):\n+        super(Command, self).process_options(args, opts)\n+        if opts.init:\n+            self._print_setting(opts)\n+\n+    def run(self, args, opts):\n+        if not opts.init:\n+            self._print_setting(opts)\n+\n+    def _print_setting(self, opts):\n+        if opts.get:\n+            print settings_.get(opts.get)\n+        elif opts.getbool:\n+            print settings_.getbool(opts.getbool)\n+        elif opts.getint:\n+            print settings_.getint(opts.getint)\n+        elif opts.getfloat:\n+            print settings_.getfloat(opts.getfloat)\n+        elif opts.getlist:\n+            print settings_.getlist(opts.getlist)\n\n@@ -0,0 +1 @@\n+LOG_ENABLED = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c58d06f0f5c4f5ecccd8b59d0250c0711a68214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 51 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: 0.875\n\nDIFF:\n@@ -14,11 +14,13 @@ class Command(ScrapyCommand):\n         return \"Fetch a URL using the Scrapy downloader\"\n \n     def long_desc(self):\n-        return \"Fetch a URL using the Scrapy downloader and print its content to stdout. You may want to use --nolog to disable logging\"\n+        return \"Fetch a URL using the Scrapy downloader and print its content \" \\\n+            \"to stdout. You may want to use --nolog to disable logging\"\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", help=\"print HTTP headers instead of body\")\n+        parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n+            help=\"print HTTP headers instead of body\")\n \n     def run(self, args, opts):\n         if not args:\n\n@@ -18,11 +18,16 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", help=\"don't show extracted links\")\n-        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", help=\"don't show scraped items\")\n-        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"avoid using pygments to colorize the output\")\n-        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\", help=\"try to match and parse the url with the defined rules (if any)\")\n-        parser.add_option(\"-c\", \"--callbacks\", dest=\"callbacks\", action=\"store\", help=\"use the provided callback(s) for parsing the url (separated with commas)\")\n+        parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", \\\n+            help=\"don't show extracted links\")\n+        parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", \\\n+            help=\"don't show scraped items\")\n+        parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", \\\n+            help=\"avoid using pygments to colorize the output\")\n+        parser.add_option(\"-r\", \"--rules\", dest=\"rules\", action=\"store_true\", \\\n+            help=\"try to match and parse the url with the defined rules (if any)\")\n+        parser.add_option(\"-c\", \"--callbacks\", dest=\"callbacks\", action=\"store\", \\\n+            help=\"use the provided callback(s) for parsing the url (separated with commas)\")\n \n     def process_options(self, args, opts):\n         super(Command, self).process_options(args, opts)\n@@ -91,7 +96,8 @@ class Command(ScrapyCommand):\n                             self.print_results(items, links, rule.callback, opts)\n                             break\n                 else:\n-                    log.msg('No rules found for spider \"%s\", please specify a callback for parsing' % spider.domain_name)\n+                    log.msg('No rules found for spider \"%s\", please specify a callback for parsing' \\\n+                        % spider.domain_name)\n                     continue\n \n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c6deaeffcc2f485d287d2aa02a6c643c073df9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 31 | Files Changed: 7 | Hunks: 24 | Methods Changed: 28 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 129 | Churn Cumulative: 577 | Contributors (this commit): 2 | Commits (past 90d): 32 | Contributors (cumulative): 10 | DMM Complexity: 0.9722222222222222\n\nDIFF:\n@@ -15,24 +15,24 @@ from scrapy.command.models import ScrapyCommand\n # This dict holds information about the executed command for later use\n command_executed = {}\n \n-def save_command_executed(cmdname, cmd, args, opts):\n+def _save_command_executed(cmdname, cmd, args, opts):\n     \"\"\"Save command executed info for later reference\"\"\"\n     command_executed['name'] = cmdname\n     command_executed['class'] = cmd\n     command_executed['args'] = args[:]\n     command_executed['opts'] = opts.__dict__.copy()\n \n-def find_commands(dir):\n+def _find_commands(dir):\n     try:\n         return [f[:-3] for f in os.listdir(dir) if not f.startswith('_') and \\\n             f.endswith('.py')]\n     except OSError:\n         return []\n \n-def get_commands_from_module(module):\n+def _get_commands_from_module(module):\n     d = {}\n     mod = __import__(module, {}, {}, [''])\n-    for cmdname in find_commands(mod.__path__[0]):\n+    for cmdname in _find_commands(mod.__path__[0]):\n         modname = '%s.%s' % (module, cmdname)\n         command = getattr(__import__(modname, {}, {}, [cmdname]), 'Command', None)\n         if callable(command):\n@@ -41,19 +41,19 @@ def get_commands_from_module(module):\n             print 'WARNING: Module %r does not define a Command class' % modname\n     return d\n \n-def get_commands_dict():\n-    cmds = get_commands_from_module('scrapy.command.commands')\n+def _get_commands_dict():\n+    cmds = _get_commands_from_module('scrapy.command.commands')\n     cmds_module = settings['COMMANDS_MODULE']\n     if cmds_module:\n-        cmds.update(get_commands_from_module(cmds_module))\n+        cmds.update(_get_commands_from_module(cmds_module))\n     return cmds\n \n-def get_command_name(argv):\n+def _get_command_name(argv):\n     for arg in argv[1:]:\n         if not arg.startswith('-'):\n             return arg\n \n-def print_usage(inside_project):\n+def _print_usage(inside_project):\n     if inside_project:\n         print \"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n             settings['BOT_NAME'])\n@@ -67,14 +67,14 @@ def print_usage(inside_project):\n     print \"  scrapy-ctl.py <command> -h\\n\"\n     print \"Available commands\"\n     print \"==================\\n\"\n-    cmds = get_commands_dict()\n+    cmds = _get_commands_dict()\n     for cmdname, cmdclass in sorted(cmds.iteritems()):\n         if inside_project or not cmdclass.requires_project:\n             print \"%s %s\" % (cmdname, cmdclass.syntax())\n             print \"  %s\" % cmdclass.short_desc()\n     print\n \n-def update_default_settings(module, cmdname):\n+def _update_default_settings(module, cmdname):\n     if not module:\n         return\n     try:\n@@ -90,11 +90,11 @@ def execute(argv=None):\n     if argv is None:\n         argv = sys.argv\n \n-    cmds = get_commands_dict()\n+    cmds = _get_commands_dict()\n \n-    cmdname = get_command_name(argv)\n-    update_default_settings('scrapy.conf.commands', cmdname)\n-    update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)\n+    cmdname = _get_command_name(argv)\n+    _update_default_settings('scrapy.conf.commands', cmdname)\n+    _update_default_settings(settings['COMMANDS_SETTINGS_MODULE'], cmdname)\n \n     parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\\n         conflict_handler='resolve', add_help_option=False)\n@@ -119,7 +119,7 @@ def execute(argv=None):\n         cmd.add_options(parser)\n         opts, args = parser.parse_args(args=argv)\n         cmd.process_options(args, opts)\n-        print_usage(settings.settings_module)\n+        _print_usage(settings.settings_module)\n         sys.exit(2)\n     else:\n         print \"Unknown command: %s\\n\" % cmdname\n@@ -127,14 +127,14 @@ def execute(argv=None):\n         sys.exit(2)\n \n     del args[0]  # remove command name from args\n-    save_command_executed(cmdname, cmd, args, opts)\n-    spiders.load()\n-    log.start()\n-    ret = run_command(cmd, args, opts)\n+    _save_command_executed(cmdname, cmd, args, opts)\n+    from scrapy.core.manager import scrapymanager\n+    scrapymanager.configure()\n+    ret = _run_command(cmd, args, opts)\n     if ret is False:\n         parser.print_help()\n \n-def run_command(cmd, args, opts):\n+def _run_command(cmd, args, opts):\n     if opts.profile or opts.lsprof:\n         if opts.profile:\n             log.msg(\"writing cProfile stats to %r\" % opts.profile)\n\n@@ -67,8 +67,7 @@ class ScrapyCommand(object):\n             help=\"write lsprof profiling stats to FILE\")\n         group.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n             help=\"write process ID to FILE\")\n-        group.add_option(\"--set\", dest=\"set\", action=\"append\", \\\n-            metavar=\"SETTING=VALUE\", default=[], \\\n+        group.add_option(\"--set\", dest=\"set\", action=\"append\", default=[], \\\n             help=\"set/override setting (may be repeated)\")\n         group.add_option(\"--settings\", dest=\"settings\", metavar=\"MODULE\",\n             help=\"python path to the Scrapy project settings\")\n@@ -78,6 +77,15 @@ class ScrapyCommand(object):\n         if opts.settings:\n             settings.set_settings_module(opts.settings)\n \n+        for setting in opts.set:\n+            if '=' in setting:\n+                name, val = setting.split('=', 1)\n+                settings.overrides[name] = val\n+            else:\n+                sys.stderr.write(\"%s: invalid argument --set %s - proper format \" \\\n+                    \"is --set SETTING=VALUE'\\n\" % (sys.argv[0], setting))\n+                sys.exit(2)\n+\n         if opts.version:\n             print scrapy.__version__\n             sys.exit()\n@@ -105,14 +113,6 @@ class ScrapyCommand(object):\n             with open(opts.pidfile, \"w\") as f:\n                 f.write(str(os.getpid()))\n \n-        for setting in opts.set:\n-            if '=' in setting:\n-                name, val = setting.split('=', 1)\n-                settings.overrides[name] = val\n-            else:\n-                sys.stderr.write(\"%s: invalid argument --set %s - proper format is --set SETTING=VALUE'\\n\" % (sys.argv[0], setting))\n-                sys.exit(2)\n-\n     def run(self, args, opts):\n         \"\"\"\n         Entry point for running commands\n\n@@ -40,6 +40,7 @@ class ExecutionManager(object):\n             level=log.DEBUG)\n \n         scrapyengine.configure()\n+        self.configured = True\n         \n     def crawl(self, *args):\n         \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n@@ -53,6 +54,7 @@ class ExecutionManager(object):\n \n     def runonce(self, *args):\n         \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n+        if not self.configured:\n             self.configure()\n         self.crawl(*args)\n         scrapyengine.start()\n\n@@ -0,0 +1,45 @@\n+import sys\n+import os\n+from subprocess import Popen, PIPE\n+import unittest\n+\n+class CmdlineTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.env = os.environ.copy()\n+        self.env.pop('SCRAPY_SETTINGS_DISABLED', None)\n+        self.env['SCRAPY_SETTINGS_MODULE'] = 'scrapy.tests.test_cmdline.settings'\n+\n+    def _execute(self, *new_args, **kwargs):\n+        args = (sys.executable, '-m', 'scrapy.command.cmdline') + new_args\n+        proc = Popen(args, stdout=PIPE, stderr=PIPE, env=self.env, **kwargs)\n+        comm = proc.communicate()\n+        return comm[0].strip()\n+\n+    def test_default_settings(self):\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init'), \\\n+            'default')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n+            'default + loaded + started')\n+\n+    def test_override_settings_using_settings_arg(self):\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init', \\\n+            '--settings', 'scrapy.tests.test_cmdline.settings2'), \\\n+            'override')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', \\\n+            '--settings', 'scrapy.tests.test_cmdline.settings2'), \\\n+            'override + loaded + started')\n+\n+    def test_override_settings_using_set_arg(self):\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init', '--set', 'TEST1=override'), \\\n+            'override')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--set', 'TEST1=override'), \\\n+            'override + loaded + started')\n+\n+    def test_override_settings_using_envvar(self):\n+        self.env['SCRAPY_TEST1'] = 'override'\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1', '--init'), \\\n+            'override')\n+        self.assertEqual(self._execute('settings', '--get', 'TEST1'), \\\n+            'override + loaded + started')\n+\n\n@@ -0,0 +1,10 @@\n+\"\"\"A test extension used to check the settings loading order\"\"\"\n+\n+from scrapy.conf import settings\n+\n+settings.overrides['TEST1'] = \"%s + %s\" % (settings['TEST1'], 'loaded')\n+\n+class TestExtension(object):\n+\n+    def __init__(self):\n+        settings.overrides['TEST1'] = \"%s + %s\" % (settings['TEST1'], 'started')\n\n@@ -0,0 +1,5 @@\n+EXTENSIONS = [\n+    'scrapy.tests.test_cmdline.extensions.TestExtension'\n+]\n+\n+TEST1 = 'default'\n\n@@ -0,0 +1,5 @@\n+EXTENSIONS = [\n+    'scrapy.tests.test_cmdline.extensions.TestExtension'\n+]\n+\n+TEST1 = 'override'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b152c99b5b8b02d48509347de902d4dc5e406a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 55 | Churn Cumulative: 55 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.2631578947368421\n\nDIFF:\n@@ -0,0 +1,55 @@\n+\"\"\"\n+File Export Pipeline\n+\n+See documentation in docs/topics/item-pipeline.rst\n+\"\"\"\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.core import signals\n+from scrapy.core.exceptions import NotConfigured\n+from scrapy.contrib import exporter\n+from scrapy.conf import settings\n+\n+class FileExportPipeline(object):\n+\n+    def __init__(self):\n+        self.exporter, self.file = self.get_exporter_and_file()\n+        self.exporter.start_exporting()\n+        dispatcher.connect(self.engine_stopped, signals.engine_stopped)\n+\n+    def process_item(self, domain, item):\n+        self.exporter.export_item(item)\n+        return item\n+\n+    def engine_stopped(self):\n+        self.exporter.finish_exporting()\n+        self.file.close()\n+\n+    def get_exporter_and_file(self):\n+        format = settings['EXPORT_FORMAT']\n+        filename = settings['EXPORT_FILE']\n+        if not format or not filename:\n+            raise NotConfigured\n+        exp_kwargs = {\n+            'fields_to_export': settings.getlist('EXPORT_FIELDS') or None,\n+            'export_empty_fields': settings.getbool('EXPORT_EMPTY', False),\n+            'encoding': settings.get('EXPORT_ENCODING', 'utf-8'),\n+        }\n+        file = open(filename, 'wb')\n+        if format == 'xml':\n+            exp = exporter.XmlItemExporter(file, **exp_kwargs)\n+        elif format == 'csv':\n+            exp = exporter.CsvItemExporter(file, **exp_kwargs)\n+        elif format == 'csv_headers':\n+            exp = exporter.CsvItemExporter(file, include_headers_line=True, \\\n+                **exp_kwargs)\n+        elif format == 'pprint':\n+            exp = exporter.PprintItemExporter(file, **exp_kwargs)\n+        elif format == 'pickle':\n+            exp = exporter.PickleItemExporter(file, **exp_kwargs)\n+        elif format == 'json':\n+            from scrapy.contrib.exporter import jsonlines\n+            exp = jsonlines.JsonLinesItemExporter(file, **exp_kwargs)\n+        else:\n+            raise NotConfigured(\"Unsupported export format: %s\" % format)\n+        return exp, file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df0e1f005f8296106f7eb782ff17d974220f3241", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 119 | Files Changed: 3 | Hunks: 5 | Methods Changed: 9 | Complexity Δ (Sum/Max): -31/0 | Churn Δ: 127 | Churn Cumulative: 420 | Contributors (this commit): 3 | Commits (past 90d): 35 | Contributors (cumulative): 7 | DMM Complexity: 0.4473684210526316\n\nDIFF:\n@@ -146,9 +146,7 @@ SCHEDULER_MIDDLEWARES_BASE = {\n \n SCHEDULER_ORDER = 'BFO'   # available orders: BFO (default), DFO\n \n-SPIDER_MODULES = []\n-\n-SPIDERPROFILER_ENABLED = False\n+SPIDER_MANAGER_CLASS = 'scrapy.contrib.spidermanager.TwistedPluginSpiderManager'\n \n SPIDER_MIDDLEWARES = {}\n \n@@ -165,6 +163,10 @@ SPIDER_MIDDLEWARES_BASE = {\n     # Spider side\n }\n \n+SPIDER_MODULES = []\n+\n+SPIDERPROFILER_ENABLED = False\n+\n STATS_CLASS = 'scrapy.stats.collector.MemoryStatsCollector'\n STATS_ENABLED = True\n STATS_DUMP = False\n\n@@ -1,4 +1,5 @@\n from scrapy.spider.models import BaseSpider\n-from scrapy.spider.manager import SpiderManager\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n \n-spiders = SpiderManager()\n+spiders = load_object(settings['SPIDER_MANAGER_CLASS'])()\n\n@@ -1,114 +0,0 @@\n-\"\"\"\n-SpiderManager is the class which locates and manages all website-specific\n-spiders\n-\"\"\"\n-import sys\n-import urlparse\n-\n-from twisted.plugin import getCache\n-\n-from scrapy.spider.models import ISpider\n-from scrapy import log\n-from scrapy.conf import settings\n-from scrapy.utils.url import url_is_from_spider\n-from scrapy.utils.misc import load_object\n-\n-class SpiderManager(object):\n-    \"\"\"Spider locator and manager\"\"\"\n-\n-    def __init__(self):\n-        self.loaded = False\n-        self.default_domain = None\n-        self.force_domain = None\n-        self.spider_modules = None\n-\n-    def fromdomain(self, domain_name):\n-        return self.asdict().get(domain_name)\n-\n-    def fromurl(self, url):\n-        self._load_on_demand()\n-        if self.force_domain:\n-            return self.asdict().get(self.force_domain)\n-        domain = urlparse.urlparse(url).hostname\n-        domain = str(domain).replace('www.', '')\n-        if domain:\n-            if domain in self.asdict():         # try first locating by domain\n-                return self.asdict()[domain]\n-            else:                               # else search spider by spider\n-                plist = self.asdict().values()\n-                for p in plist:\n-                    if url_is_from_spider(url, p):\n-                        return p\n-        spider = self.asdict().get(self.default_domain)\n-        if not spider:                          # create a custom spider\n-            spiderclassname = settings.get('DEFAULT_SPIDER')\n-            if spiderclassname:\n-                spider = load_object(spiderclassname)(domain)\n-                self.add_spider(spider)\n-            \n-        return spider\n-\n-    def asdict(self):\n-        self._load_on_demand()\n-        return self._spiders\n-\n-    def _load_on_demand(self):\n-        if not self.loaded:\n-            self.load()\n-\n-    def load(self, spider_modules=None):\n-        if spider_modules is None:\n-            spider_modules = settings.getlist('SPIDER_MODULES')\n-        self.spider_modules = spider_modules\n-        self._invaliddict = {}\n-        self._spiders = {}\n-\n-        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n-        for module in modules:\n-            for spider in self._getspiders(ISpider, module):\n-                self.add_spider(spider)\n-        self.loaded = True\n-\n-    def add_spider(self, spider):\n-        try:\n-            ISpider.validateInvariants(spider)\n-            self._spiders[spider.domain_name] = spider\n-        except Exception, e:\n-            self._invaliddict[spider.domain_name] = spider\n-            # we can't use the log module here because it may not be available yet\n-            print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n-\n-    def reload(self, spider_modules=None, skip_domains=None):\n-        \"\"\"Reload spiders by trying to discover any spiders added under the\n-        spiders module/packages, removes any spiders removed.\n-\n-        If skip_domains is passed those spiders won't be reloaded.\n-        \"\"\"\n-        skip_domains = set(skip_domains or [])\n-        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n-        for m in modules:\n-            reload(m)\n-        reloaded = 0\n-        pdict = self.asdict()\n-        for domain, spider in pdict.iteritems():\n-            if not domain in skip_domains:\n-                reload(sys.modules[spider.__module__])\n-                reloaded += 1\n-        self.load(spider_modules=spider_modules)  # second call to update spider instances\n-        log.msg(\"Reloaded %d/%d scrapy spiders\" % (reloaded, len(pdict)), level=log.DEBUG)\n-\n-    def _getspiders(self, interface, package):\n-        \"\"\"This is an override of twisted.plugin.getPlugin, because we're\n-        interested in catching exceptions thrown when loading spiders such as\n-        KeyboardInterrupt\n-        \"\"\"\n-        try:\n-            allDropins = getCache(package)\n-            for dropin in allDropins.itervalues():\n-                for plugin in dropin.plugins:\n-                    adapted = interface(plugin, None)\n-                    if adapted is not None:\n-                        yield adapted\n-        except KeyboardInterrupt:\n-            sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n-            sys.exit(2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1b33ef165e7e4b110623ce9887d6b6a98c34d6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 109 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 29/29 | Churn Δ: 109 | Churn Cumulative: 109 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.5352112676056338\n\nDIFF:\n@@ -0,0 +1,109 @@\n+\"\"\"\n+SpiderManager is the class which locates and manages all website-specific\n+spiders\n+\"\"\"\n+\n+import sys\n+import urlparse\n+\n+from twisted.plugin import getCache\n+\n+from scrapy.spider.models import ISpider\n+from scrapy import log\n+from scrapy.conf import settings\n+from scrapy.utils.url import url_is_from_spider\n+from scrapy.utils.misc import load_object\n+\n+class TwistedPluginSpiderManager(object):\n+    \"\"\"Spider locator and manager\"\"\"\n+\n+    def __init__(self):\n+        self.loaded = False\n+        self.default_domain = None\n+        self.force_domain = None\n+        self.spider_modules = None\n+\n+    def fromdomain(self, domain_name):\n+        return self.asdict().get(domain_name)\n+\n+    def fromurl(self, url):\n+        if self.force_domain:\n+            return self.asdict().get(self.force_domain)\n+        domain = urlparse.urlparse(url).hostname\n+        domain = str(domain).replace('www.', '')\n+        if domain:\n+            if domain in self.asdict():         # try first locating by domain\n+                return self.asdict()[domain]\n+            else:                               # else search spider by spider\n+                plist = self.asdict().values()\n+                for p in plist:\n+                    if url_is_from_spider(url, p):\n+                        return p\n+        spider = self.asdict().get(self.default_domain)\n+        if not spider:                          # create a custom spider\n+            spiderclassname = settings.get('DEFAULT_SPIDER')\n+            if spiderclassname:\n+                spider = load_object(spiderclassname)(domain)\n+                self.add_spider(spider)\n+            \n+        return spider\n+\n+    def asdict(self):\n+        return self._spiders\n+\n+    def load(self, spider_modules=None):\n+        if spider_modules is None:\n+            spider_modules = settings.getlist('SPIDER_MODULES')\n+        self.spider_modules = spider_modules\n+        self._invaliddict = {}\n+        self._spiders = {}\n+\n+        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n+        for module in modules:\n+            for spider in self._getspiders(ISpider, module):\n+                self.add_spider(spider)\n+        self.loaded = True\n+\n+    def add_spider(self, spider):\n+        try:\n+            ISpider.validateInvariants(spider)\n+            self._spiders[spider.domain_name] = spider\n+        except Exception, e:\n+            self._invaliddict[spider.domain_name] = spider\n+            # we can't use the log module here because it may not be available yet\n+            print \"WARNING: Could not load spider %s: %s\" % (spider, e)\n+\n+    def reload(self, spider_modules=None, skip_domains=None):\n+        \"\"\"Reload spiders by trying to discover any spiders added under the\n+        spiders module/packages, removes any spiders removed.\n+\n+        If skip_domains is passed those spiders won't be reloaded.\n+        \"\"\"\n+        skip_domains = set(skip_domains or [])\n+        modules = [__import__(m, {}, {}, ['']) for m in self.spider_modules]\n+        for m in modules:\n+            reload(m)\n+        reloaded = 0\n+        pdict = self.asdict()\n+        for domain, spider in pdict.iteritems():\n+            if not domain in skip_domains:\n+                reload(sys.modules[spider.__module__])\n+                reloaded += 1\n+        self.load(spider_modules=spider_modules)  # second call to update spider instances\n+        log.msg(\"Reloaded %d/%d scrapy spiders\" % (reloaded, len(pdict)), level=log.DEBUG)\n+\n+    def _getspiders(self, interface, package):\n+        \"\"\"This is an override of twisted.plugin.getPlugin, because we're\n+        interested in catching exceptions thrown when loading spiders such as\n+        KeyboardInterrupt\n+        \"\"\"\n+        try:\n+            allDropins = getCache(package)\n+            for dropin in allDropins.itervalues():\n+                for plugin in dropin.plugins:\n+                    adapted = interface(plugin, None)\n+                    if adapted is not None:\n+                        yield adapted\n+        except KeyboardInterrupt:\n+            sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n+            sys.exit(2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d3c51fd6f24dc5f750eddeb28a372b94df949c50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 67 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 74 | Churn Cumulative: 126 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,9 @@\n+\"\"\"\n+Images Pipeline\n+\n+See documentation in topics/images.rst\n+\"\"\"\n+\n from __future__ import with_statement\n import os\n import time\n@@ -23,33 +29,6 @@ class ImageException(Exception):\n \n \n class BaseImagesPipeline(MediaPipeline):\n-    \"\"\"Abstract pipeline that implement the image downloading and thumbnail generation logic\n-\n-    This pipeline tries to minimize network transfers and image processing,\n-    doing stat of the images and determining if image is new, uptodate or\n-    expired.\n-\n-    `new` images are those that pipeline never processed and needs to be\n-        downloaded from supplier site the first time.\n-\n-    `uptodate` images are the ones that the pipeline processed and are still\n-        valid images.\n-\n-    `expired` images are those that pipeline already processed but the last\n-        modification was made long time ago, so a reprocessing is recommended to\n-        refresh it in case of change.\n-\n-    IMAGES_EXPIRES setting controls the maximun days since an image was modified\n-    to consider it uptodate.\n-\n-    THUMBS is a tuple of tuples, each sub-tuple is a pair of thumb_id string\n-    and a compatible python image library size (a tuple).\n-    See thumbnail method at http://www.pythonware.com/library/pil/handbook/image.htm\n-\n-    Downloaded images are skipped if sizes aren't greater than MIN_WIDTH and\n-    MIN_HEIGHT limit. A proper log messages will be printed.\n-\n-    \"\"\"\n \n     MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n     MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n@@ -186,34 +165,9 @@ class BaseImagesPipeline(MediaPipeline):\n \n     # Required overradiable interface\n     def store_image(self, key, image, buf, info):\n-        \"\"\"Override this method with specific code to persist an image\n-\n-        This method is used to persist the full image and any defined\n-        thumbnail, one a time.\n-\n-        Return value is ignored.\n-\n-        \"\"\"\n         raise NotImplementedError\n \n     def stat_key(self, key, info):\n-        \"\"\"Override this method with specific code to stat an image\n-\n-        this method should return and dictionary with two parameters:\n-         * last_modified: the last modification time in seconds since the epoch\n-         * checksum: the md5sum of the content of the stored image if found\n-\n-         If an exception is raised or last_modified is None, then the image\n-         will be re-downloaded.\n-\n-         If the difference in days between last_modified and now is greater than\n-         IMAGES_EXPIRES settings, then the image will be re-downloaded\n-\n-         The checksum value is appended to returned image path after a hash\n-         sign (#), if checksum is None, then nothing is appended including the\n-         hash sign.\n-\n-        \"\"\"\n         raise NotImplementedError\n \n \n\n@@ -140,13 +140,7 @@ class MediaPipeline(object):\n         \"\"\"\n \n     def get_media_requests(self, item, info):\n-        \"\"\" Return a list of Request objects to download for this item\n-\n-        Should return None or an iterable\n-\n-        Defaults return None (no media to download)\n-\n-        \"\"\"\n+        pass\n \n     def media_downloaded(self, response, request, info):\n         \"\"\" Method called on success download of media request\n@@ -185,13 +179,5 @@ class MediaPipeline(object):\n         \"\"\"\n \n     def item_completed(self, results, item, info):\n-        \"\"\" Method called when all media requests for a single item has returned a result or failure.\n-\n-        The return value of this method is used as output of pipeline stage.\n-\n-        `item_completed` can return item itself or raise DropItem exception.\n-\n-        Default returns item\n-        \"\"\"\n         return item\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
