{"custom_id": "scrapy#a138fb05d4f0d90e2002e85a348a5be34904d3d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 58 | Files Changed: 20 | Hunks: 51 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 105 | Churn Cumulative: 9625 | Contributors (this commit): 77 | Commits (past 90d): 155 | Contributors (cumulative): 193 | DMM Complexity: None\n\nDIFF:\n@@ -174,7 +174,7 @@ def tunnel_request_data(host, port, proxy_auth_header=None):\n     r\"\"\"\n     Return binary content of a CONNECT request.\n \n-    >>> from scrapy.utils.python import to_native_str as s\n+    >>> from scrapy.utils.python import to_unicode as s\n     >>> s(tunnel_request_data(\"example.com\", 8080))\n     'CONNECT example.com:8080 HTTP/1.1\\r\\nHost: example.com:8080\\r\\n\\r\\n'\n     >>> s(tunnel_request_data(\"example.com\", 8080, b\"123\"))\n\n@@ -6,7 +6,7 @@ from collections import defaultdict\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n \n logger = logging.getLogger(__name__)\n \n@@ -53,7 +53,7 @@ class CookiesMiddleware(object):\n \n     def _debug_cookie(self, request, spider):\n         if self.debug:\n-            cl = [to_native_str(c, errors='replace')\n+            cl = [to_unicode(c, errors='replace')\n                   for c in request.headers.getlist('Cookie')]\n             if cl:\n                 cookies = \"\\n\".join(\"Cookie: {}\\n\".format(c) for c in cl)\n@@ -62,7 +62,7 @@ class CookiesMiddleware(object):\n \n     def _debug_set_cookie(self, response, spider):\n         if self.debug:\n-            cl = [to_native_str(c, errors='replace')\n+            cl = [to_unicode(c, errors='replace')\n                   for c in response.headers.getlist('Set-Cookie')]\n             if cl:\n                 cookies = \"\\n\".join(\"Set-Cookie: {}\\n\".format(c) for c in cl)\n\n@@ -5,15 +5,12 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \"\"\"\n \n import logging\n-import sys\n-import re\n \n from twisted.internet.defer import Deferred, maybeDeferred\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.python import to_native_str\n from scrapy.utils.misc import load_object\n \n logger = logging.getLogger(__name__)\n\n@@ -12,7 +12,7 @@ from six.moves import cPickle as pickle\n from xml.sax.saxutils import XMLGenerator\n \n from scrapy.utils.serialize import ScrapyJSONEncoder\n-from scrapy.utils.python import to_bytes, to_unicode, to_native_str, is_listlike\n+from scrapy.utils.python import to_bytes, to_unicode, is_listlike\n from scrapy.item import BaseItem\n from scrapy.exceptions import ScrapyDeprecationWarning\n import warnings\n@@ -232,7 +232,7 @@ class CsvItemExporter(BaseItemExporter):\n     def _build_row(self, values):\n         for s in values:\n             try:\n-                yield to_native_str(s, self.encoding)\n+                yield to_unicode(s, self.encoding)\n             except TypeError:\n                 yield s\n \n\n@@ -3,7 +3,7 @@ from six.moves.http_cookiejar import (\n     CookieJar as _CookieJar, DefaultCookiePolicy, IPV4_RE\n )\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n \n \n class CookieJar(object):\n@@ -165,13 +165,13 @@ class WrappedRequest(object):\n         return name in self.request.headers\n \n     def get_header(self, name, default=None):\n-        return to_native_str(self.request.headers.get(name, default),\n+        return to_unicode(self.request.headers.get(name, default),\n                              errors='replace')\n \n     def header_items(self):\n         return [\n-            (to_native_str(k, errors='replace'),\n-             [to_native_str(x, errors='replace') for x in v])\n+            (to_unicode(k, errors='replace'),\n+             [to_unicode(x, errors='replace') for x in v])\n             for k, v in self.request.headers.items()\n         ]\n \n@@ -189,7 +189,7 @@ class WrappedResponse(object):\n \n     # python3 cookiejars calls get_all\n     def get_all(self, name, default=None):\n-        return [to_native_str(v, errors='replace')\n+        return [to_unicode(v, errors='replace')\n                 for v in self.response.headers.getlist(name)]\n     # python2 cookiejars calls getheaders\n     getheaders = get_all\n\n@@ -16,7 +16,7 @@ from w3lib.html import strip_html5_whitespace\n from scrapy.http.request import Request\n from scrapy.http.response import Response\n from scrapy.utils.response import get_base_url\n-from scrapy.utils.python import memoizemethod_noargs, to_native_str\n+from scrapy.utils.python import memoizemethod_noargs, to_unicode\n \n \n class TextResponse(Response):\n@@ -32,7 +32,7 @@ class TextResponse(Response):\n \n     def _set_url(self, url):\n         if isinstance(url, six.text_type):\n-            self._url = to_native_str(url, self.encoding)\n+            self._url = to_unicode(url, self.encoding)\n         else:\n             super(TextResponse, self)._set_url(url)\n \n@@ -81,11 +81,11 @@ class TextResponse(Response):\n     @memoizemethod_noargs\n     def _headers_encoding(self):\n         content_type = self.headers.get(b'Content-Type', b'')\n-        return http_content_type_encoding(to_native_str(content_type))\n+        return http_content_type_encoding(to_unicode(content_type))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n-            content_type = to_native_str(self.headers.get(b'Content-Type', b''))\n+            content_type = to_unicode(self.headers.get(b'Content-Type', b''))\n             benc, ubody = html_to_unicode(content_type, self.body,\n                     auto_detect_fun=self._auto_detect_fun,\n                     default_encoding=self._DEFAULT_ENCODING)\n\n@@ -10,7 +10,7 @@ from w3lib.url import canonicalize_url\n \n from scrapy.link import Link\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n-from scrapy.utils.python import unique as unique_list, to_native_str\n+from scrapy.utils.python import unique as unique_list, to_unicode\n from scrapy.utils.response import get_base_url\n from scrapy.linkextractors import FilteringLinkExtractor\n \n@@ -67,7 +67,7 @@ class LxmlParserLinkExtractor(object):\n                 url = self.process_attr(attr_val)\n                 if url is None:\n                     continue\n-            url = to_native_str(url, encoding=response_encoding)\n+            url = to_unicode(url, encoding=response_encoding)\n             # to fix relative links after process_value\n             url = urljoin(response_url, url)\n             link = Link(url, _collect_string_content(el) or u'',\n\n@@ -10,7 +10,7 @@ import six\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n-from scrapy.utils.python import binary_is_text, to_bytes, to_native_str\n+from scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n \n \n class ResponseTypes(object):\n@@ -55,12 +55,12 @@ class ResponseTypes(object):\n         header \"\"\"\n         if content_encoding:\n             return Response\n-        mimetype = to_native_str(content_type).split(';')[0].strip().lower()\n+        mimetype = to_unicode(content_type).split(';')[0].strip().lower()\n         return self.from_mimetype(mimetype)\n \n     def from_content_disposition(self, content_disposition):\n         try:\n-            filename = to_native_str(content_disposition,\n+            filename = to_unicode(content_disposition,\n                 encoding='latin-1', errors='replace').split(';')[1].split('=')[1]\n             filename = filename.strip('\"\\'')\n             return self.from_filename(filename)\n\n@@ -3,14 +3,14 @@ import logging\n from abc import ABCMeta, abstractmethod\n from six import with_metaclass\n \n-from scrapy.utils.python import to_native_str, to_unicode\n+from scrapy.utils.python import to_unicode\n \n logger = logging.getLogger(__name__)\n \n def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n     try:\n         if to_native_str_type:\n-            robotstxt_body = to_native_str(robotstxt_body)\n+            robotstxt_body = to_unicode(robotstxt_body)\n         else:\n             robotstxt_body = robotstxt_body.decode('utf-8')\n     except UnicodeDecodeError:\n@@ -66,8 +66,8 @@ class PythonRobotParser(RobotParser):\n         return o\n \n     def allowed(self, url, user_agent):\n-        user_agent = to_native_str(user_agent)\n-        url = to_native_str(url)\n+        user_agent = to_unicode(user_agent)\n+        url = to_unicode(url)\n         return self.rp.can_fetch(user_agent, url)\n \n \n\n@@ -10,8 +10,7 @@ from w3lib.url import safe_url_string\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n-from scrapy.utils.python import to_native_str\n-from scrapy.utils.httpobj import urlparse_cached\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n from scrapy.utils.url import strip_url\n \n@@ -322,7 +321,7 @@ class RefererMiddleware(object):\n             if isinstance(resp_or_url, Response):\n                 policy_header = resp_or_url.headers.get('Referrer-Policy')\n                 if policy_header is not None:\n-                    policy_name = to_native_str(policy_header.decode('latin1'))\n+                    policy_name = to_unicode(policy_header.decode('latin1'))\n         if policy_name is None:\n             return self.default_policy()\n \n\n@@ -4,7 +4,7 @@ Helper functions for serializing (and deserializing) requests.\n import six\n \n from scrapy.http import Request\n-from scrapy.utils.python import to_unicode, to_native_str\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n \n \n@@ -54,7 +54,7 @@ def request_from_dict(d, spider=None):\n         eb = _get_method(spider, eb)\n     request_cls = load_object(d['_class']) if '_class' in d else Request\n     return request_cls(\n-        url=to_native_str(d['url']),\n+        url=to_unicode(d['url']),\n         callback=cb,\n         errback=eb,\n         method=d['method'],\n\n@@ -9,7 +9,7 @@ import weakref\n from six.moves.urllib.parse import urlunparse\n \n from w3lib.http import basic_auth_header\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes, to_unicode\n \n from w3lib.url import canonicalize_url\n from scrapy.utils.httpobj import urlparse_cached\n@@ -97,4 +97,4 @@ def referer_str(request):\n     referrer = request.headers.get('Referer')\n     if referrer is None:\n         return referrer\n-    return to_native_str(referrer, errors='replace')\n+    return to_unicode(referrer, errors='replace')\n\n@@ -8,7 +8,7 @@ import webbrowser\n import tempfile\n \n from twisted.web import http\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes, to_unicode\n from w3lib import html\n \n \n@@ -36,7 +36,7 @@ def response_status_message(status):\n     \"\"\"Return status code plus status text descriptive message\n     \"\"\"\n     message = http.RESPONSES.get(int(status), \"Unknown Status\")\n-    return '%s %s' % (status, to_native_str(message))\n+    return '%s %s' % (status, to_unicode(message))\n \n \n def response_httprepr(response):\n\n@@ -3,7 +3,7 @@\n import OpenSSL\n import OpenSSL._util as pyOpenSSLutil\n \n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n \n \n # The OpenSSL symbol is present since 1.1.1 but it's not currently supported in any version of pyOpenSSL.\n@@ -12,7 +12,7 @@ SSL_OP_NO_TLSv1_3 = getattr(pyOpenSSLutil.lib, 'SSL_OP_NO_TLSv1_3', 0)\n \n \n def ffi_buf_to_string(buf):\n-    return to_native_str(pyOpenSSLutil.ffi.string(buf))\n+    return to_unicode(pyOpenSSLutil.ffi.string(buf))\n \n \n def x509name_to_string(x509name):\n\n@@ -1,17 +1,16 @@\n import os\n from os.path import join, abspath\n-from twisted.trial import unittest\n from twisted.internet import defer\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n from tests.test_commands import CommandTest\n \n \n def _textmode(bstr):\n     \"\"\"Normalize input the same as writing to a file\n     and reading from it in text mode\"\"\"\n-    return to_native_str(bstr).replace(os.linesep, '\\n')\n+    return to_unicode(bstr).replace(os.linesep, '\\n')\n \n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n     command = 'parse'\n\n@@ -10,13 +10,10 @@ from contextlib import contextmanager\n from threading import Timer\n \n from twisted.trial import unittest\n-from twisted.internet import defer\n \n import scrapy\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n-from scrapy.utils.testsite import SiteTest\n-from scrapy.utils.testproc import ProcessTest\n from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n \n@@ -56,7 +53,7 @@ class ProjectTest(unittest.TestCase):\n         finally:\n             timer.cancel()\n \n-        return p, to_native_str(stdout), to_native_str(stderr)\n+        return p, to_unicode(stdout), to_unicode(stderr)\n \n \n class StartprojectTest(ProjectTest):\n\n@@ -26,8 +26,7 @@ from scrapy.extensions.feedexport import (\n     S3FeedStorage, StdoutFeedStorage,\n     BlockingFeedStorage)\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete, get_crawler\n-from scrapy.utils.python import to_native_str\n-from scrapy.utils.project import get_project_settings\n+from scrapy.utils.python import to_unicode\n \n from pathlib import Path\n \n@@ -459,7 +458,7 @@ class FeedExportTest(unittest.TestCase):\n         settings.update({'FEED_FORMAT': 'csv'})\n         data = yield self.exported_data(items, settings)\n \n-        reader = csv.DictReader(to_native_str(data).splitlines())\n+        reader = csv.DictReader(to_unicode(data).splitlines())\n         got_rows = list(reader)\n         if ordered:\n             self.assertEqual(reader.fieldnames, header)\n@@ -473,7 +472,7 @@ class FeedExportTest(unittest.TestCase):\n         settings = settings or {}\n         settings.update({'FEED_FORMAT': 'jl'})\n         data = yield self.exported_data(items, settings)\n-        parsed = [json.loads(to_native_str(line)) for line in data.splitlines()]\n+        parsed = [json.loads(to_unicode(line)) for line in data.splitlines()]\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         self.assertEqual(rows, parsed)\n \n\n@@ -7,12 +7,11 @@ from unittest import mock\n from urllib.parse import unquote_to_bytes\n import warnings\n \n-import six\n from six.moves import xmlrpc_client as xmlrpclib\n from six.moves.urllib.parse import urlparse, parse_qs, unquote\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, JsonRequest, Headers, HtmlResponse\n-from scrapy.utils.python import to_bytes, to_native_str\n+from scrapy.utils.python import to_bytes, to_unicode\n \n \n class RequestTest(unittest.TestCase):\n@@ -349,8 +348,8 @@ class FormRequestTest(RequestTest):\n     request_class = FormRequest\n \n     def assertQueryEqual(self, first, second, msg=None):\n-        first = to_native_str(first).split(\"&\")\n-        second = to_native_str(second).split(\"&\")\n+        first = to_unicode(first).split(\"&\")\n+        second = to_unicode(second).split(\"&\")\n         return self.assertEqual(sorted(first), sorted(second), msg)\n \n     def test_empty_formdata(self):\n\n@@ -7,7 +7,7 @@ from w3lib.encoding import resolve_encoding\n from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n                          XmlResponse, Headers)\n from scrapy.selector import Selector\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_unicode\n from scrapy.exceptions import NotSupported\n from scrapy.link import Link\n from tests import get_testdata\n@@ -204,11 +204,11 @@ class TextResponseTest(BaseResponseTest):\n         assert isinstance(resp.url, str)\n \n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='utf-8')\n-        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\\xc2\\xa3'))\n+        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\\xc2\\xa3'))\n         resp = self.response_class(url=u\"http://www.example.com/price/\\xa3\", encoding='latin-1')\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n         resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]})\n-        self.assertEqual(resp.url, to_native_str(b'http://www.example.com/price/\\xc2\\xa3'))\n+        self.assertEqual(resp.url, to_unicode(b'http://www.example.com/price/\\xc2\\xa3'))\n         resp = self.response_class(u\"http://www.example.com/price/\\xa3\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]})\n         self.assertEqual(resp.url, 'http://www.example.com/price/\\xa3')\n \n\n@@ -1,6 +1,5 @@\n # coding=utf-8\n from twisted.trial import unittest\n-from scrapy.utils.python import to_native_str\n \n \n def reppy_available():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87c23ba22d2ef714d778b62c794333acaf232f60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 162 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -80,8 +80,6 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_serializes_ok(r, spider=self.spider)\n \n     def test_mixin_private_callback_serialization(self):\n-        if sys.version_info[0] < 3:\n-            return\n         r = Request(\"http://www.example.com\",\n                     callback=self.spider._TestSpiderMixin__mixin_callback,\n                     errback=self.spider.handle_error)\n@@ -119,7 +117,6 @@ class RequestSerializationTest(unittest.TestCase):\n     def test_private_name_mangling(self):\n         self._assert_mangles_to(\n             self.spider, '_TestSpider__parse_item_private')\n-        if sys.version_info[0] >= 3:\n         self._assert_mangles_to(\n             self.spider, '_TestSpiderMixin__mixin_callback')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a4a77d49fa580d35d1e023ab4b54a397b88088a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 313 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,9 +21,8 @@ class MutableChainTest(unittest.TestCase):\n         m.extend([7, 8])\n         m.extend([9, 10], (11, 12))\n         self.assertEqual(next(m), 0)\n-        self.assertEqual(m.next(), 1)\n-        self.assertEqual(m.__next__(), 2)\n-        self.assertEqual(list(m), list(range(3, 13)))\n+        self.assertEqual(m.__next__(), 1)\n+        self.assertEqual(list(m), list(range(2, 13)))\n \n \n class ToUnicodeTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be6da52019990c1db45b1101dd99787752a14313", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 98 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -19,9 +19,12 @@ from scrapy.utils.url import (\n \n # common file extensions that are not followed if they occur in links\n IGNORED_EXTENSIONS = [\n+    # archives\n+    '7z', '7zip', 'bz2', 'rar', 'tar', 'tar.gz', 'xz', 'zip',\n+    \n     # images\n     'mng', 'pct', 'bmp', 'gif', 'jpg', 'jpeg', 'png', 'pst', 'psp', 'tif',\n-    'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg',\n+    'tiff', 'ai', 'drw', 'dxf', 'eps', 'ps', 'svg', 'cdr', 'ico',\n \n     # audio\n     'mp3', 'wma', 'ogg', 'wav', 'ra', 'aac', 'mid', 'au', 'aiff',\n@@ -35,7 +38,7 @@ IGNORED_EXTENSIONS = [\n     'odp',\n \n     # other\n-    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar', 'dmg', 'iso', 'apk', 'xz'\n+    'css', 'pdf', 'exe', 'bin', 'rss', 'dmg', 'iso', 'apk'\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e291460db67ef8c9e52de02a0a4a86f4bce39b9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2328 | Contributors (this commit): 31 | Commits (past 90d): 27 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ import signal\n import logging\n import warnings\n \n-import sys\n from twisted.internet import reactor, defer\n from zope.interface.verify import verifyClass, DoesNotImplement\n \n\n@@ -4,7 +4,6 @@ Item Exporters are used to export/serialize items into different formats.\n \n import csv\n import io\n-import sys\n import pprint\n import marshal\n import six\n\n@@ -4,6 +4,8 @@ This module defines the Link object used in Link extractors.\n For actual link extractors implementation see scrapy.linkextractors, or\n its documentation in: docs/topics/link-extractors.rst\n \"\"\"\n+\n+\n class Link(object):\n     \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n \n\n@@ -1,7 +1,5 @@\n from __future__ import print_function\n \n-import sys\n-\n from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#058bdda0afe967f99a3ffd59b395566b063d9c3f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 51 | Churn Cumulative: 527 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,25 +30,44 @@ class CrawlTestCase(TestCase):\n         self.assertEqual(len(crawler.spider.urls_visited), 11)  # 10 + start_url\n \n     @defer.inlineCallbacks\n-    def test_delay(self):\n-        # short to long delays\n-        yield self._test_delay(0.2, False)\n-        yield self._test_delay(1, False)\n-        # randoms\n-        yield self._test_delay(0.2, True)\n-        yield self._test_delay(1, True)\n+    def test_fixed_delay(self):\n+        yield self._test_delay(total=3, delay=0.1)\n \n     @defer.inlineCallbacks\n-    def _test_delay(self, delay, randomize):\n-        settings = {\"DOWNLOAD_DELAY\": delay, 'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n+    def test_randomized_delay(self):\n+        yield self._test_delay(total=3, delay=0.1, randomize=True)\n+\n+    @defer.inlineCallbacks\n+    def _test_delay(self, total, delay, randomize=False):\n+        crawl_kwargs = dict(\n+            maxlatency=delay * 2,\n+            mockserver=self.mockserver,\n+            total=total,\n+        )\n+        tolerance = (1 - (0.6 if randomize else 0.2))\n+\n+        settings = {\"DOWNLOAD_DELAY\": delay,\n+                    'RANDOMIZE_DOWNLOAD_DELAY': randomize}\n         crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)\n-        yield crawler.crawl(maxlatency=delay * 2, mockserver=self.mockserver)\n-        t = crawler.spider.times\n-        totaltime = t[-1] - t[0]\n-        avgd = totaltime / (len(t) - 1)\n-        tolerance = 0.6 if randomize else 0.2\n-        self.assertTrue(avgd > delay * (1 - tolerance),\n-                        \"download delay too small: %s\" % avgd)\n+        yield crawler.crawl(**crawl_kwargs)\n+        times = crawler.spider.times\n+        total_time = times[-1] - times[0]\n+        average = total_time / (len(times) - 1)\n+        self.assertTrue(average > delay * tolerance,\n+                        \"download delay too small: %s\" % average)\n+\n+        # Ensure that the same test parameters would cause a failure if no\n+        # download delay is set. Otherwise, it means we are using a combination\n+        # of ``total`` and ``delay`` values that are too small for the test\n+        # code above to have any meaning.\n+        settings[\"DOWNLOAD_DELAY\"] = 0\n+        crawler = CrawlerRunner(settings).create_crawler(FollowAllSpider)\n+        yield crawler.crawl(**crawl_kwargs)\n+        times = crawler.spider.times\n+        total_time = times[-1] - times[0]\n+        average = total_time / (len(times) - 1)\n+        self.assertFalse(average > delay / tolerance,\n+                         \"test total or delay values are too small\")\n \n     @defer.inlineCallbacks\n     def test_timeout_success(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0946eb335a285e1f210ba1185a564699f53b17d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 90 | Churn Cumulative: 426 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,9 @@\n from time import time\n from six.moves.urllib.parse import urlparse, urlunparse, urldefrag\n \n-from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n-from twisted.internet import defer\n+from twisted.internet import defer, reactor\n+from twisted.internet.protocol import ClientFactory\n \n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n@@ -93,18 +93,30 @@ class ScrapyHTTPPageGetter(HTTPClient):\n                 (self.factory.url, self.factory.timeout)))\n \n \n-class ScrapyHTTPClientFactory(HTTPClientFactory):\n-    \"\"\"Scrapy implementation of the HTTPClientFactory overwriting the\n-    setUrl method to make use of our Url object that cache the parse\n-    result.\n-    \"\"\"\n+class ScrapyHTTPClientFactory(ClientFactory):\n \n     protocol = ScrapyHTTPPageGetter\n+\n     waiting = 1\n     noisy = False\n     followRedirect = False\n     afterFoundGet = False\n \n+    def _build_response(self, body, request):\n+        request.meta['download_latency'] = self.headers_time-self.start_time\n+        status = int(self.status)\n+        headers = Headers(self.response_headers)\n+        respcls = responsetypes.from_args(headers=headers, url=self._url)\n+        return respcls(url=self._url, status=status, headers=headers, body=body)\n+\n+    def _set_connection_attributes(self, request):\n+        parsed = urlparse_cached(request)\n+        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n+        proxy = request.meta.get('proxy')\n+        if proxy:\n+            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n+            self.path = self.url\n+\n     def __init__(self, request, timeout=180):\n         self._url = urldefrag(request.url)[0]\n         # converting to bytes to comply to Twisted interface\n@@ -139,21 +151,59 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         elif self.method == b'POST':\n             self.headers['Content-Length'] = 0\n \n-    def _build_response(self, body, request):\n-        request.meta['download_latency'] = self.headers_time-self.start_time\n-        status = int(self.status)\n-        headers = Headers(self.response_headers)\n-        respcls = responsetypes.from_args(headers=headers, url=self._url)\n-        return respcls(url=self._url, status=status, headers=headers, body=body)\n+    def __repr__(self):\n+        return \"<%s: %s>\" % (self.__class__.__name__, self.url)\n \n-    def _set_connection_attributes(self, request):\n-        parsed = urlparse_cached(request)\n-        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(parsed)\n-        proxy = request.meta.get('proxy')\n-        if proxy:\n-            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n-            self.path = self.url\n+    def _cancelTimeout(self, result, timeoutCall):\n+        if timeoutCall.active():\n+            timeoutCall.cancel()\n+        return result\n+\n+    def buildProtocol(self, addr):\n+        p = ClientFactory.buildProtocol(self, addr)\n+        p.followRedirect = self.followRedirect\n+        p.afterFoundGet = self.afterFoundGet\n+        if self.timeout:\n+            timeoutCall = reactor.callLater(self.timeout, p.timeout)\n+            self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n+        return p\n \n     def gotHeaders(self, headers):\n         self.headers_time = time()\n         self.response_headers = headers\n+\n+    def gotStatus(self, version, status, message):\n+        \"\"\"\n+        Set the status of the request on us.\n+        @param version: The HTTP version.\n+        @type version: L{bytes}\n+        @param status: The HTTP status code, an integer represented as a\n+            bytestring.\n+        @type status: L{bytes}\n+        @param message: The HTTP status message.\n+        @type message: L{bytes}\n+        \"\"\"\n+        self.version, self.status, self.message = version, status, message\n+\n+    def page(self, page):\n+        if self.waiting:\n+            self.waiting = 0\n+            self.deferred.callback(page)\n+\n+    def noPage(self, reason):\n+        if self.waiting:\n+            self.waiting = 0\n+            self.deferred.errback(reason)\n+\n+    def clientConnectionFailed(self, _, reason):\n+        \"\"\"\n+        When a connection attempt fails, the request cannot be issued.  If no\n+        result has yet been provided to the result Deferred, provide the\n+        connection failure reason as an error result.\n+        \"\"\"\n+        if self.waiting:\n+            self.waiting = 0\n+            # If the connection attempt failed, there is nothing more to\n+            # disconnect, so just fire that Deferred now.\n+            self._disconnectedDeferred.callback(None)\n+            self.deferred.errback(reason)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe3a121f1358fc904915f5b32e276520523c553a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 319 | Contributors (this commit): 9 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -232,10 +232,10 @@ class UtilsPythonTestCase(unittest.TestCase):\n             self.assertEqual(get_func_args(operator.itemgetter(2)), [])\n         else:\n             self.assertEqual(\n-                get_func_args(six.text_type.split, True), ['sep', 'maxsplit'])\n-            self.assertEqual(get_func_args(\" \".join, True), ['list'])\n+                get_func_args(six.text_type.split, stripself=True), ['sep', 'maxsplit'])\n+            self.assertEqual(get_func_args(\" \".join, stripself=True), ['list'])\n             self.assertEqual(\n-                get_func_args(operator.itemgetter(2), True), ['obj'])\n+                get_func_args(operator.itemgetter(2), stripself=True), ['obj'])\n \n \n     def test_without_none_values(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3b2289ad012043b94b495d411316b2778bf3db35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 94 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -43,6 +43,6 @@ class LinkTest(unittest.TestCase):\n         l2 = eval(repr(l1))\n         self._assert_same_links(l1, l2)\n \n-    def test_non_str_url_py2(self):\n+    def test_bytes_url(self):\n         with self.assertRaises(TypeError):\n             Link(b\"http://www.example.com/\\xc2\\xa3\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e252f5a13be3195fdc3ec1d66a111ae01a0ab80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1418 | Contributors (this commit): 24 | Commits (past 90d): 19 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -615,7 +615,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n         crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=Request(url=self.mockserver.url('')))\n         failure = crawler.spider.meta.get('failure')\n-        self.assertTrue(failure == None)\n+        self.assertTrue(failure is None)\n         reason = crawler.spider.meta['close_reason']\n         self.assertTrue(reason, 'finished')\n \n@@ -636,7 +636,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n             yield crawler.crawl(seed=request)\n             # download_maxsize = 50 is enough for the gzipped response\n             failure = crawler.spider.meta.get('failure')\n-            self.assertTrue(failure == None)\n+            self.assertTrue(failure is None)\n             reason = crawler.spider.meta['close_reason']\n             self.assertTrue(reason, 'finished')\n         else:\n\n@@ -85,8 +85,8 @@ class _BaseTest(unittest.TestCase):\n \n     def assertEqualRequestButWithCacheValidators(self, request1, request2):\n         self.assertEqual(request1.url, request2.url)\n-        assert not b'If-None-Match' in request1.headers\n-        assert not b'If-Modified-Since' in request1.headers\n+        assert b'If-None-Match' not in request1.headers\n+        assert b'If-Modified-Since' not in request1.headers\n         assert any(h in request2.headers for h in (b'If-None-Match', b'If-Modified-Since'))\n         self.assertEqual(request1.body, request2.body)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99d8b05a0b1997033b2240aa9f945bbe659ee6dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 1078 | Contributors (this commit): 20 | Commits (past 90d): 15 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -392,3 +392,7 @@ class MutableChain(object):\n \n     def __next__(self):\n         return next(self.data)\n+\n+    @deprecated(\"scrapy.utils.python.MutableChain.__next__\")\n+    def next(self):\n+        return self.__next__()\n\n@@ -5,6 +5,7 @@ import unittest\n from itertools import count\n import platform\n import six\n+from warnings import catch_warnings\n \n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n@@ -22,7 +23,12 @@ class MutableChainTest(unittest.TestCase):\n         m.extend([9, 10], (11, 12))\n         self.assertEqual(next(m), 0)\n         self.assertEqual(m.__next__(), 1)\n-        self.assertEqual(list(m), list(range(2, 13)))\n+        with catch_warnings(record=True) as warnings:\n+            self.assertEqual(m.next(), 2)\n+            self.assertEqual(len(warnings), 1)\n+            self.assertIn('scrapy.utils.python.MutableChain.__next__',\n+                          str(warnings[0].message))\n+        self.assertEqual(list(m), list(range(3, 13)))\n \n \n class ToUnicodeTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e84cb18ca0b5b09c68cc76d6c48929d9ff933e5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 2227 | Contributors (this commit): 26 | Commits (past 90d): 19 | Contributors (cumulative): 46 | DMM Complexity: None\n\nDIFF:\n@@ -275,5 +275,6 @@ coverage_ignore_pyobjects = [\n \n intersphinx_mapping = {\n     'python': ('https://docs.python.org/3', None),\n-    'sphinx': ('https://www.sphinx-doc.org/en/stable', None),\n+    'sphinx': ('https://www.sphinx-doc.org/en/master', None),\n+    'twisted': ('https://twistedmatrix.com/documents/current', None),\n }\n\n@@ -67,15 +67,18 @@ class BrowserLikeContextFactory(ScrapyClientContextFactory):\n     \"\"\"\n     Twisted-recommended context factory for web clients.\n \n-    Quoting https://twistedmatrix.com/documents/current/api/twisted.web.client.Agent.html:\n-    \"The default is to use a BrowserLikePolicyForHTTPS,\n-    so unless you have special requirements you can leave this as-is.\"\n+    Quoting the documentation of the :class:`~twisted.web.client.Agent` class:\n \n-    creatorForNetloc() is the same as BrowserLikePolicyForHTTPS\n-    except this context factory allows setting the TLS/SSL method to use.\n+        The default is to use a\n+        :class:`~twisted.web.client.BrowserLikePolicyForHTTPS`, so unless you\n+        have special requirements you can leave this as-is.\n \n-    Default OpenSSL method is TLS_METHOD (also called SSLv23_METHOD)\n-    which allows TLS protocol negotiation.\n+    :meth:`creatorForNetloc` is the same as\n+    :class:`~twisted.web.client.BrowserLikePolicyForHTTPS` except this context\n+    factory allows setting the TLS/SSL method to use.\n+\n+    The default OpenSSL method is ``TLS_METHOD`` (also called\n+    ``SSLv23_METHOD``) which allows TLS protocol negotiation.\n     \"\"\"\n     def creatorForNetloc(self, hostname, port):\n \n\n@@ -110,7 +110,7 @@ class Crawler(object):\n class CrawlerRunner(object):\n     \"\"\"\n     This is a convenient helper class that keeps track of, manages and runs\n-    crawlers inside an already setup Twisted `reactor`_.\n+    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n \n     The CrawlerRunner object must be instantiated with a\n     :class:`~scrapy.settings.Settings` object.\n@@ -233,12 +233,13 @@ class CrawlerProcess(CrawlerRunner):\n     A class to run multiple scrapy crawlers in a process simultaneously.\n \n     This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n-    for starting a Twisted `reactor`_ and handling shutdown signals, like the\n-    keyboard interrupt command Ctrl-C. It also configures top-level logging.\n+    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n+    signals, like the keyboard interrupt command Ctrl-C. It also configures\n+    top-level logging.\n \n     This utility should be a better fit than\n     :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n-    Twisted `reactor`_ within your application.\n+    :mod:`~twisted.internet.reactor` within your application.\n \n     The CrawlerProcess object must be instantiated with a\n     :class:`~scrapy.settings.Settings` object.\n@@ -273,9 +274,9 @@ class CrawlerProcess(CrawlerRunner):\n \n     def start(self, stop_after_crawl=True):\n         \"\"\"\n-        This method starts a Twisted `reactor`_, adjusts its pool size to\n-        :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based\n-        on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n+        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n+        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n+        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n \n         If ``stop_after_crawl`` is True, the reactor will be stopped after all\n         crawlers have finished, using :meth:`join`.\n\n@@ -46,16 +46,14 @@ class SignalManager(object):\n \n     def send_catch_log_deferred(self, signal, **kwargs):\n         \"\"\"\n-        Like :meth:`send_catch_log` but supports returning `deferreds`_ from\n-        signal handlers.\n+        Like :meth:`send_catch_log` but supports returning\n+        :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n \n         Returns a Deferred that gets fired once all signal handlers\n         deferreds were fired. Send a signal, catch exceptions and log them.\n \n         The keyword arguments are passed to the signal handlers (connected\n         through the :meth:`connect` method).\n-\n-        .. _deferreds: https://twistedmatrix.com/documents/current/core/howto/defer.html\n         \"\"\"\n         kwargs.setdefault('sender', self.sender)\n         return _signal.send_catch_log_deferred(signal, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fed93515de4e306eb3262125c09eb49decdb2944", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 173 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -27,6 +27,7 @@ sys.path.insert(0, path.dirname(path.dirname(__file__)))\n # Add any Sphinx extension module names here, as strings. They can be extensions\n # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n extensions = [\n+    'hoverxref.extension',\n     'notfound.extension',\n     'scrapydocs',\n     'sphinx.ext.autodoc',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f261cf65e999573d95a575ba362c3e32b026f894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 149 | Lines Deleted: 2 | Files Changed: 68 | Hunks: 138 | Methods Changed: 1 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 151 | Churn Cumulative: 8385 | Contributors (this commit): 72 | Commits (past 90d): 145 | Contributors (cumulative): 381 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ from scrapy.exceptions import UsageError\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = False\n\n@@ -1,6 +1,7 @@\n from __future__ import print_function\n from scrapy.commands import ScrapyCommand\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = True\n\n@@ -4,6 +4,7 @@ import json\n from scrapy.commands import ScrapyCommand\n from scrapy.settings import BaseSettings\n \n+\n class Command(ScrapyCommand):\n \n     requires_project = False\n\n@@ -1,6 +1,7 @@\n from scrapy.commands import fetch, ScrapyCommand\n from scrapy.utils.response import open_in_browser\n \n+\n class Command(fetch.Command):\n \n     def short_desc(self):\n\n@@ -17,8 +17,8 @@ class DataURIDownloadHandler(object):\n         respcls = responsetypes.from_mimetype(uri.media_type)\n \n         resp_kwargs = {}\n-        if (issubclass(respcls, TextResponse) and\n-                uri.media_type.split('/')[0] == 'text'):\n+        if (issubclass(respcls, TextResponse)\n+                and uri.media_type.split('/')[0] == 'text'):\n             charset = uri.media_type_parameters.get('charset')\n             resp_kwargs['encoding'] = charset\n \n\n@@ -68,6 +68,8 @@ class AjaxCrawlMiddleware(object):\n \n # XXX: move it to w3lib?\n _ajax_crawlable_re = re.compile(six.u(r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'))\n+\n+\n def _has_ajaxcrawlable_meta(text):\n     \"\"\"\n     >>> _has_ajaxcrawlable_meta('<html><head><meta name=\"fragment\"  content=\"!\"/></head><body></body></html>')\n\n@@ -5,6 +5,7 @@ import logging\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import referer_str, request_fingerprint\n \n+\n class BaseDupeFilter(object):\n \n     @classmethod\n\n@@ -7,10 +7,12 @@ new exceptions here without documenting them there.\n \n # Internal\n \n+\n class NotConfigured(Exception):\n     \"\"\"Indicates a missing configuration situation\"\"\"\n     pass\n \n+\n class _InvalidOutput(TypeError):\n     \"\"\"\n     Indicates an invalid value has been returned by a middleware's processing method.\n@@ -18,15 +20,19 @@ class _InvalidOutput(TypeError):\n     \"\"\"\n     pass\n \n+\n # HTTP and crawling\n \n+\n class IgnoreRequest(Exception):\n     \"\"\"Indicates a decision was made not to process a request\"\"\"\n \n+\n class DontCloseSpider(Exception):\n     \"\"\"Request the spider not to be closed yet\"\"\"\n     pass\n \n+\n class CloseSpider(Exception):\n     \"\"\"Raise this from callbacks to request the spider to be closed\"\"\"\n \n@@ -34,30 +40,37 @@ class CloseSpider(Exception):\n         super(CloseSpider, self).__init__()\n         self.reason = reason\n \n+\n # Items\n \n+\n class DropItem(Exception):\n     \"\"\"Drop item from the item pipeline\"\"\"\n     pass\n \n+\n class NotSupported(Exception):\n     \"\"\"Indicates a feature or method is not supported\"\"\"\n     pass\n \n+\n # Commands\n \n+\n class UsageError(Exception):\n     \"\"\"To indicate a command-line usage error\"\"\"\n     def __init__(self, *a, **kw):\n         self.print_help = kw.pop('print_help', True)\n         super(UsageError, self).__init__(*a, **kw)\n \n+\n class ScrapyDeprecationWarning(Warning):\n     \"\"\"Warning category for deprecated features, since the default\n     DeprecationWarning is silenced on Python 2.7+\n     \"\"\"\n     pass\n \n+\n class ContractFail(AssertionError):\n     \"\"\"Error raised in case of a failing contract\"\"\"\n     pass\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/extensions.rst\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n \n+\n class ExtensionManager(MiddlewareManager):\n \n     component_name = 'extension'\n\n@@ -5,6 +5,7 @@ from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.job import job_dir\n \n+\n class SpiderState(object):\n     \"\"\"Store and load spider state during a scraping job\"\"\"\n \n\n@@ -7,5 +7,6 @@ See documentation in docs/topics/request-response.rst\n \n from scrapy.http.response.text import TextResponse\n \n+\n class HtmlResponse(TextResponse):\n     pass\n\n@@ -7,5 +7,6 @@ See documentation in docs/topics/request-response.rst\n \n from scrapy.http.response.text import TextResponse\n \n+\n class XmlResponse(TextResponse):\n     pass\n\n@@ -1,5 +1,6 @@\n from zope.interface import Interface\n \n+\n class ISpiderLoader(Interface):\n \n     def from_settings(settings):\n\n@@ -3,6 +3,7 @@\n from functools import partial\n from scrapy.utils.python import get_func_args\n \n+\n def wrap_loader_context(function, context):\n     \"\"\"Wrap functions that receive loader_context to contain the context\n     \"pre-loaded\" and expose a interface that receives only one argument\n\n@@ -7,6 +7,7 @@ See documentation in docs/item-pipeline.rst\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n \n+\n class ItemPipelineManager(MiddlewareManager):\n \n     component_name = 'item pipeline'\n\n@@ -7,6 +7,7 @@ from scrapy.utils.datatypes import LocalCache\n \n dnscache = LocalCache(10000)\n \n+\n class CachingThreadedResolver(ThreadedResolver):\n     def __init__(self, reactor, cache_size, timeout):\n         super(CachingThreadedResolver, self).__init__(reactor)\n\n@@ -5,8 +5,10 @@ from six import with_metaclass\n \n from scrapy.utils.python import to_unicode\n \n+\n logger = logging.getLogger(__name__)\n \n+\n def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n     try:\n         if to_native_str_type:\n@@ -23,6 +25,7 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n         robotstxt_body = ''\n     return robotstxt_body\n \n+\n class RobotParser(with_metaclass(ABCMeta)):\n     @classmethod\n     @abstractmethod\n\n@@ -1,6 +1,7 @@\n from functools import wraps\n from collections import OrderedDict\n \n+\n def _embed_ipython_shell(namespace={}, banner=''):\n     \"\"\"Start an IPython Shell\"\"\"\n     try:\n@@ -23,6 +24,7 @@ def _embed_ipython_shell(namespace={}, banner=''):\n         shell()\n     return wrapper\n \n+\n def _embed_bpython_shell(namespace={}, banner=''):\n     \"\"\"Start a bpython shell\"\"\"\n     import bpython\n@@ -31,6 +33,7 @@ def _embed_bpython_shell(namespace={}, banner=''):\n         bpython.embed(locals_=namespace, banner=banner)\n     return wrapper\n \n+\n def _embed_ptpython_shell(namespace={}, banner=''):\n     \"\"\"Start a ptpython shell\"\"\"\n     import ptpython.repl\n@@ -40,6 +43,7 @@ def _embed_ptpython_shell(namespace={}, banner=''):\n         ptpython.repl.embed(locals=namespace)\n     return wrapper\n \n+\n def _embed_standard_shell(namespace={}, banner=''):\n     \"\"\"Start a standard python shell\"\"\"\n     import code\n@@ -55,6 +59,7 @@ def _embed_standard_shell(namespace={}, banner=''):\n         code.interact(banner=banner, local=namespace)\n     return wrapper\n \n+\n DEFAULT_PYTHON_SHELLS = OrderedDict([\n     ('ptpython', _embed_ptpython_shell),\n     ('ipython', _embed_ipython_shell),\n@@ -62,6 +67,7 @@ DEFAULT_PYTHON_SHELLS = OrderedDict([\n     ('python', _embed_standard_shell),\n ])\n \n+\n def get_shell_embed_func(shells=None, known_shells=None):\n     \"\"\"Return the first acceptable shell-embed function\n     from a given list of shell names.\n@@ -79,6 +85,7 @@ def get_shell_embed_func(shells=None, known_shells=None):\n             except ImportError:\n                 continue\n \n+\n def start_python_console(namespace=None, banner='', shells=None):\n     \"\"\"Start Python console bound to the given namespace.\n     Readline support and tab completion will be used on Unix, if available.\n\n@@ -34,6 +34,7 @@ def defers(func):\n         return defer.maybeDeferred(func, *a, **kw)\n     return wrapped\n \n+\n def inthread(func):\n     \"\"\"Decorator to call a function in a thread and return a deferred with the\n     result\n\n@@ -7,6 +7,7 @@ from twisted.python import failure\n \n from scrapy.exceptions import IgnoreRequest\n \n+\n def defer_fail(_failure):\n     \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n     next reactor loop\n@@ -18,6 +19,7 @@ def defer_fail(_failure):\n     reactor.callLater(0.1, d.errback, _failure)\n     return d\n \n+\n def defer_succeed(result):\n     \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n     next reactor loop\n@@ -29,6 +31,7 @@ def defer_succeed(result):\n     reactor.callLater(0.1, d.callback, result)\n     return d\n \n+\n def defer_result(result):\n     if isinstance(result, defer.Deferred):\n         return result\n@@ -37,6 +40,7 @@ def defer_result(result):\n     else:\n         return defer_succeed(result)\n \n+\n def mustbe_deferred(f, *args, **kw):\n     \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n     callback/errback to next reactor loop\n@@ -53,6 +57,7 @@ def mustbe_deferred(f, *args, **kw):\n     else:\n         return defer_result(result)\n \n+\n def parallel(iterable, count, callable, *args, **named):\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n@@ -63,6 +68,7 @@ def parallel(iterable, count, callable, *args, **named):\n     work = (callable(elem, *args, **named) for elem in iterable)\n     return defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n \n+\n def process_chain(callbacks, input, *a, **kw):\n     \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n     d = defer.Deferred()\n@@ -71,6 +77,7 @@ def process_chain(callbacks, input, *a, **kw):\n     d.callback(input)\n     return d\n \n+\n def process_chain_both(callbacks, errbacks, input, *a, **kw):\n     \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n     d = defer.Deferred()\n@@ -83,6 +90,7 @@ def process_chain_both(callbacks, errbacks, input, *a, **kw):\n         d.callback(input)\n     return d\n \n+\n def process_parallel(callbacks, input, *a, **kw):\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n@@ -92,6 +100,7 @@ def process_parallel(callbacks, input, *a, **kw):\n     d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n     return d\n \n+\n def iter_errback(iterable, errback, *a, **kw):\n     \"\"\"Wraps an iterable calling an errback if an error is caught while\n     iterating it.\n\n@@ -6,6 +6,7 @@ from __future__ import print_function\n import sys\n from pprint import pformat as pformat_\n \n+\n def _colorize(text, colorize=True):\n     if not colorize or not sys.stdout.isatty():\n         return text\n@@ -17,8 +18,10 @@ def _colorize(text, colorize=True):\n     except ImportError:\n         return text\n \n+\n def pformat(obj, *args, **kwargs):\n     return _colorize(pformat_(obj), kwargs.pop('colorize', True))\n \n+\n def pprint(obj, *args, **kwargs):\n     print(pformat(obj, *args, **kwargs))\n\n@@ -3,6 +3,7 @@\n from __future__ import print_function\n from time import time # used in global tests code\n \n+\n def get_engine_status(engine):\n     \"\"\"Return a report of the current engine status\"\"\"\n     tests = [\n@@ -32,6 +33,7 @@ def get_engine_status(engine):\n \n     return checks\n \n+\n def format_engine_status(engine=None):\n     checks = get_engine_status(engine)\n     s = \"Execution engine status\\n\\n\"\n@@ -41,5 +43,6 @@ def format_engine_status(engine=None):\n \n     return s\n \n+\n def print_engine_status(engine):\n     print(format_engine_status(engine))\n\n@@ -1,6 +1,7 @@\n from ftplib import error_perm\n from posixpath import dirname\n \n+\n def ftp_makedirs_cwd(ftp, path, first_call=True):\n     \"\"\"Set the current directory of the FTP connection given in the ``ftp``\n     argument (as a ftplib.FTP object), creating all parent directories if they\n\n@@ -45,6 +45,7 @@ def gunzip(data):\n _is_gzipped = re.compile(br'^application/(x-)?gzip\\b', re.I).search\n _is_octetstream = re.compile(br'^(application|binary)/octet-stream\\b', re.I).search\n \n+\n @deprecated\n def is_gzipped(response):\n     \"\"\"Return True if the response is gzipped, or False otherwise\"\"\"\n\n@@ -4,7 +4,10 @@ import weakref\n \n from six.moves.urllib.parse import urlparse\n \n+\n _urlparse_cache = weakref.WeakKeyDictionary()\n+\n+\n def urlparse_cached(request_or_response):\n     \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n     Request or Response object\n\n@@ -1,5 +1,6 @@\n import os\n \n+\n def job_dir(settings):\n     path = settings['JOBDIR']\n     if path and not os.path.exists(path):\n\n@@ -165,6 +165,7 @@ def memoizemethod_noargs(method):\n _BINARYCHARS = {six.b(chr(i)) for i in range(32)} - {b\"\\0\", b\"\\t\", b\"\\n\", b\"\\r\"}\n _BINARYCHARS |= {ord(ch) for ch in _BINARYCHARS}\n \n+\n @deprecated(\"scrapy.utils.python.binary_is_text\")\n def isbinarytext(text):\n     \"\"\" This function is deprecated.\n\n@@ -1,5 +1,6 @@\n from twisted.internet import reactor, error\n \n+\n def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n\n@@ -16,6 +16,8 @@ from scrapy.utils.httpobj import urlparse_cached\n \n \n _fingerprint_cache = weakref.WeakKeyDictionary()\n+\n+\n def request_fingerprint(request, include_headers=None, keep_fragments=False):\n     \"\"\"\n     Return the request fingerprint.\n\n@@ -13,6 +13,8 @@ from w3lib import html\n \n \n _baseurl_cache = weakref.WeakKeyDictionary()\n+\n+\n def get_base_url(response):\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n@@ -23,6 +25,8 @@ def get_base_url(response):\n \n \n _metaref_cache = weakref.WeakKeyDictionary()\n+\n+\n def get_meta_refresh(response, ignore_tags=('script', 'noscript')):\n     \"\"\"Parse the http-equiv refrsh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n\n@@ -28,6 +28,7 @@ def iter_spider_classes(module):\n            getattr(obj, 'name', None):\n             yield obj\n \n+\n def spidercls_for_request(spider_loader, request, default_spidercls=None,\n                           log_none=False, log_multiple=False):\n     \"\"\"Return a spider class that handles the given Request.\n\n@@ -19,6 +19,8 @@ def render_templatefile(path, **kwargs):\n \n \n CAMELCASE_INVALID_CHARS = re.compile(r'[^a-zA-Z\\d]')\n+\n+\n def string_camelcase(string):\n     \"\"\" Convert a word  to its CamelCase version and remove invalid chars\n \n\n@@ -32,6 +32,7 @@ def skip_if_no_boto():\n     except NotConfigured as e:\n         raise SkipTest(e)\n \n+\n def get_s3_content_and_delete(bucket, path, with_key=False):\n     \"\"\" Get content from s3 key, and delete key afterwards.\n     \"\"\"\n@@ -51,6 +52,7 @@ def get_s3_content_and_delete(bucket, path, with_key=False):\n         bucket.delete_key(path)\n     return (content, key) if with_key else content\n \n+\n def get_gcs_content_and_delete(bucket, path):\n     from google.cloud import storage\n     client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n@@ -61,6 +63,7 @@ def get_gcs_content_and_delete(bucket, path):\n     bucket.delete_blob(path)\n     return content, acl, blob\n \n+\n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n@@ -72,12 +75,14 @@ def get_crawler(spidercls=None, settings_dict=None):\n     runner = CrawlerRunner(settings_dict)\n     return runner.create_crawler(spidercls or Spider)\n \n+\n def get_pythonpath():\n     \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n     installation of Scrapy\"\"\"\n     scrapy_path = import_module('scrapy').__path__[0]\n     return os.path.dirname(scrapy_path) + os.pathsep + os.environ.get('PYTHONPATH', '')\n \n+\n def get_testenv():\n     \"\"\"Return a OS environment dict suitable to fork processes that need to import\n     this installation of Scrapy, instead of a system installed one.\n@@ -86,6 +91,7 @@ def get_testenv():\n     env['PYTHONPATH'] = get_pythonpath()\n     return env\n \n+\n def assert_samelines(testcase, text1, text2, msg=None):\n     \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n     line endings between platforms\n\n@@ -13,6 +13,7 @@ error = KeyError\n \n _DATABASES = collections.defaultdict(DummyDB)\n \n+\n def open(file, flag='r', mode=0o666):\n     \"\"\"Open or create a dummy database compatible.\n \n\n@@ -2,6 +2,7 @@\n Some pipelines used for testing\n \"\"\"\n \n+\n class ZeroDivisionErrorPipeline(object):\n \n     def open_spider(self, spider):\n\n@@ -16,6 +16,7 @@ class MockServerSpider(Spider):\n         super(MockServerSpider, self).__init__(*args, **kwargs)\n         self.mockserver = mockserver\n \n+\n class MetaSpider(MockServerSpider):\n \n     name = 'meta'\n\n@@ -1,5 +1,6 @@\n \"\"\"A test extension used to check the settings loading order\"\"\"\n \n+\n class TestExtension(object):\n \n     def __init__(self, settings):\n\n@@ -12,6 +12,7 @@ def _textmode(bstr):\n     and reading from it in text mode\"\"\"\n     return to_unicode(bstr).replace(os.linesep, '\\n')\n \n+\n class ParseCommandTest(ProcessTest, SiteTest, CommandTest):\n     command = 'parse'\n \n\n@@ -1,6 +1,7 @@\n from importlib import import_module\n from twisted.trial import unittest\n \n+\n class ScrapyUtilsTest(unittest.TestCase):\n     def test_required_openssl_version(self):\n         try:\n\n@@ -5,8 +5,10 @@ from scrapy.spiders import Spider\n from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n+\n __doctests__ = ['scrapy.downloadermiddlewares.ajaxcrawl']\n \n+\n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n         crawler = get_crawler(Spider, {'AJAXCRAWL_ENABLED': True})\n\n@@ -149,6 +149,7 @@ class FilesystemStorageTest(DefaultStorageTest):\n \n     storage_class = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n \n+\n class FilesystemStorageGzipTest(FilesystemStorageTest):\n \n     def _get_settings(self, **new_settings):\n\n@@ -12,6 +12,7 @@ from scrapy.utils.job import job_dir\n from scrapy.utils.test import get_crawler\n from tests.spiders import SimpleSpider\n \n+\n class FromCrawlerRFPDupeFilter(RFPDupeFilter):\n \n     @classmethod\n\n@@ -3,6 +3,7 @@ import copy\n \n from scrapy.http import Headers\n \n+\n class HeadersTest(unittest.TestCase):\n \n     def assertSortedEqual(self, first, second, msg=None):\n\n@@ -118,6 +118,7 @@ class DropSomeItemsPipeline(object):\n         else:\n             self.drop = True\n \n+\n class ShowOrSkipMessagesTestCase(TwistedTestCase):\n     def setUp(self):\n         self.mockserver = MockServer()\n\n@@ -6,6 +6,7 @@ from email.charset import Charset\n \n from scrapy.mail import MailSender\n \n+\n class MailSenderTest(unittest.TestCase):\n \n     def test_send(self):\n\n@@ -4,6 +4,7 @@ from scrapy.settings import Settings\n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n \n+\n class M1(object):\n \n     def open_spider(self, spider):\n@@ -15,6 +16,7 @@ class M1(object):\n     def process(self, response, request, spider):\n         pass\n \n+\n class M2(object):\n \n     def open_spider(self, spider):\n@@ -25,6 +27,7 @@ class M2(object):\n \n     pass\n \n+\n class M3(object):\n \n     def process(self, response, request, spider):\n@@ -54,6 +57,7 @@ class TestMiddlewareManager(MiddlewareManager):\n         if hasattr(mw, 'process'):\n             self.methods['process'].append(mw.process)\n \n+\n class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_init(self):\n\n@@ -4,6 +4,7 @@ from scrapy.responsetypes import responsetypes\n \n from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse, Headers\n \n+\n class ResponseTypesTest(unittest.TestCase):\n \n     def test_from_filename(self):\n\n@@ -19,6 +19,7 @@ def rerp_available():\n         return False\n     return True\n \n+\n def protego_available():\n     # check if protego parser is installed\n     try:\n@@ -27,6 +28,7 @@ def protego_available():\n         return False\n     return True\n \n+\n class BaseRobotParserTest:\n     def _setUp(self, parser_cls):\n         self.parser_cls = parser_cls\n\n@@ -109,6 +109,7 @@ class SpiderLoaderTest(unittest.TestCase):\n             spiders = spider_loader.list()\n             self.assertEqual(spiders, [])\n \n+\n class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n\n@@ -1,5 +1,6 @@\n from scrapy.spiders import Spider\n \n+\n class Spider4(Spider):\n     name = \"spider4\"\n     allowed_domains = ['spider4.com']\n\n@@ -1,4 +1,5 @@\n from scrapy.spiders import Spider\n \n+\n class Spider0(Spider):\n     allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n@@ -1,5 +1,6 @@\n from scrapy.spiders import Spider\n \n+\n class Spider1(Spider):\n     name = \"spider1\"\n     allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n@@ -1,5 +1,6 @@\n from scrapy.spiders import Spider\n \n+\n class Spider2(Spider):\n     name = \"spider2\"\n     allowed_domains = [\"scrapy2.org\", \"scrapy3.org\"]\n\n@@ -1,5 +1,6 @@\n from scrapy.spiders import Spider\n \n+\n class Spider3(Spider):\n     name = \"spider3\"\n     allowed_domains = ['spider3.com']\n\n@@ -9,6 +9,7 @@ from scrapy.spidermiddlewares.offsite import URLWarning\n from scrapy.utils.test import get_crawler\n import warnings\n \n+\n class TestOffsiteMiddleware(TestCase):\n \n     def setUp(self):\n@@ -53,6 +54,7 @@ class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n         self.assertEqual(out, reqs)\n \n+\n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n \n     def _get_spider(self):\n\n@@ -34,6 +34,7 @@ class RecoverySpider(Spider):\n         if not response.meta.get('dont_fail'):\n             raise TabError()\n \n+\n class RecoveryMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n         spider.logger.info('Middleware: %s exception caught', exception.__class__.__name__)\n@@ -50,6 +51,7 @@ class FailProcessSpiderInputMiddleware:\n         spider.logger.info('Middleware: will raise IndexError')\n         raise IndexError()\n \n+\n class ProcessSpiderInputSpiderWithoutErrback(Spider):\n     name = 'ProcessSpiderInputSpiderWithoutErrback'\n     custom_settings = {\n@@ -177,6 +179,7 @@ class GeneratorRecoverMiddleware:\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         yield {'processed': [method]}\n \n+\n class GeneratorDoNothingAfterRecoveryMiddleware(_GeneratorDoNothingMiddleware):\n     pass\n \n@@ -247,6 +250,7 @@ class NotGeneratorRecoverMiddleware:\n         spider.logger.info('%s: %s caught', method, exception.__class__.__name__)\n         return [{'processed': [method]}]\n \n+\n class NotGeneratorDoNothingAfterRecoveryMiddleware(_NotGeneratorDoNothingMiddleware):\n     pass\n \n\n@@ -349,6 +349,7 @@ class TestSettingsCustomPolicy(TestRefererMiddleware):\n \n     ]\n \n+\n # --- Tests using Request meta dict to set policy\n class TestRequestMetaDefault(MixinDefault, TestRefererMiddleware):\n     req_meta = {'referrer_policy': POLICY_SCRAPY_DEFAULT}\n@@ -518,14 +519,17 @@ class TestPolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_UNSAFE_URL.upper()}\n \n+\n class TestPolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER.swapcase()}\n \n+\n class TestPolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n \n+\n class TestPolicyHeaderPredecence004(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n     \"\"\"\n     The empty string means \"no-referrer-when-downgrade\"\n\n@@ -7,16 +7,20 @@ from scrapy.http import Request\n from scrapy.loader import ItemLoader\n from scrapy.selector import Selector\n \n+\n class TestItem(Item):\n     name = Field()\n \n+\n def _test_procesor(x):\n     return x + x\n \n+\n class TestLoader(ItemLoader):\n     default_item_class = TestItem\n     name_out = staticmethod(_test_procesor)\n \n+\n def nonserializable_object_test(self):\n     q = self.queue()\n     try:\n@@ -35,6 +39,7 @@ def nonserializable_object_test(self):\n     sel = Selector(text='<html><body><p>some text</p></body></html>')\n     self.assertRaises(ValueError, q.push, sel)\n \n+\n class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n \n     chunksize = 100000\n@@ -53,15 +58,19 @@ class MarshalFifoDiskQueueTest(t.FifoDiskQueueTest):\n \n     test_nonserializable_object = nonserializable_object_test\n \n+\n class ChunkSize1MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 1\n \n+\n class ChunkSize2MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 2\n \n+\n class ChunkSize3MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 3\n \n+\n class ChunkSize4MarshalFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n     chunksize = 4\n \n@@ -100,15 +109,19 @@ class PickleFifoDiskQueueTest(MarshalFifoDiskQueueTest):\n         self.assertEqual(r.url, r2.url)\n         assert r2.meta['request'] is r2\n \n+\n class ChunkSize1PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 1\n \n+\n class ChunkSize2PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 2\n \n+\n class ChunkSize3PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 3\n \n+\n class ChunkSize4PickleFifoDiskQueueTest(PickleFifoDiskQueueTest):\n     chunksize = 4\n \n\n@@ -14,6 +14,7 @@ try:\n except ImportError:\n     ipy = False\n \n+\n class UtilsConsoleTestCase(unittest.TestCase):\n \n     def test_get_shell_embed_func(self):\n\n@@ -33,14 +33,23 @@ class MustbeDeferredTest(unittest.TestCase):\n         steps.append(2) # add another value, that should be catched by assertEqual\n         return dfd\n \n+\n def cb1(value, arg1, arg2):\n     return \"(cb1 %s %s %s)\" % (value, arg1, arg2)\n+\n+\n def cb2(value, arg1, arg2):\n     return defer.succeed(\"(cb2 %s %s %s)\" % (value, arg1, arg2))\n+\n+\n def cb3(value, arg1, arg2):\n     return \"(cb3 %s %s %s)\" % (value, arg1, arg2)\n+\n+\n def cb_fail(value, arg1, arg2):\n     return Failure(TypeError())\n+\n+\n def eb1(failure, arg1, arg2):\n     return \"(eb1 %s %s %s)\" % (failure.value.__class__.__name__, arg1, arg2)\n \n\n@@ -2,6 +2,7 @@ import unittest\n \n from scrapy.utils.http import decode_chunked_transfer\n \n+\n class ChunkedTest(unittest.TestCase):\n \n     def test_decode_chunked_transfer(self):\n\n@@ -4,6 +4,7 @@ from six.moves.urllib.parse import urlparse\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n \n+\n class HttpobjUtilsTest(unittest.TestCase):\n \n     def test_urlparse_cached(self):\n\n@@ -235,6 +235,7 @@ class LxmlXmliterTestCase(XmliterTestCase):\n         i = self.xmliter(42, 'product')\n         self.assertRaises(TypeError, next, i)\n \n+\n class UtilsCsvTestCase(unittest.TestCase):\n     sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')\n     sample_feed_path = os.path.join(sample_feeds_dir, 'feed-sample3.csv')\n\n@@ -4,6 +4,7 @@ from scrapy.http import Request\n from scrapy.utils.request import request_fingerprint, _fingerprint_cache, \\\n     request_authenticate, request_httprepr\n \n+\n class UtilsRequestTest(unittest.TestCase):\n \n     def test_request_fingerprint(self):\n\n@@ -66,6 +66,7 @@ class SendCatchLogDeferredTest2(SendCatchLogTest):\n     def _get_result(self, signal, *a, **kw):\n         return send_catch_log_deferred(signal, *a, **kw)\n \n+\n class SendCatchLogTest2(unittest.TestCase):\n \n     def test_error_logged_if_deferred_not_supported(self):\n\n@@ -2,6 +2,7 @@ import unittest\n \n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n+\n class SitemapTest(unittest.TestCase):\n \n     def test_sitemap(self):\n\n@@ -9,12 +9,15 @@ from scrapy.spiders import CrawlSpider\n class MyBaseSpider(CrawlSpider):\n     pass # abstract spider\n \n+\n class MySpider1(MyBaseSpider):\n     name = 'myspider1'\n \n+\n class MySpider2(MyBaseSpider):\n     name = 'myspider2'\n \n+\n class UtilsSpidersTestCase(unittest.TestCase):\n \n     def test_iterate_spider_output(self):\n\n@@ -187,6 +187,7 @@ class AddHttpIfNoScheme(unittest.TestCase):\n class GuessSchemeTest(unittest.TestCase):\n     pass\n \n+\n def create_guess_scheme_t(args):\n     def do_expected(self):\n         url = guess_scheme(args[0])\n@@ -195,6 +196,7 @@ def create_guess_scheme_t(args):\n                 args[0], url, args[1])\n     return do_expected\n \n+\n def create_skipped_scheme_t(args):\n     def do_expected(self):\n         raise unittest.SkipTest(args[2])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
