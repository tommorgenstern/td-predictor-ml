{"custom_id": "scrapy#6cfbe78d631516823ec13fe295faa37ccca6962f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.5\n\nDIFF:\n@@ -4,6 +4,7 @@ Function for dealing with databases\n import re\n from scrapy.conf import settings\n from scrapy.core import log\n+from scrapy.core.engine import scrapyengine\n \n def mysql_connect(db_uri, **kwargs):\n     \"\"\"\n@@ -27,4 +28,12 @@ def mysql_connect(db_uri, **kwargs):\n             dcopy[\"passwd\"] = \"********\"\n         log.msg(\"Connecting db with settings %s\" % dcopy )\n         \n-        return MySQLdb.connect(**d)\n+        conn = MySQLdb.connect(**d)\n+        \n+        #this is to maintain active the connection\n+        def _ping():\n+            log.msg(\"Pinging connection to %s/%s\" % (d.get('host'), d.get('db')) )\n+            conn.ping()\n+        scrapyengine.addtask(_ping, settings.getint(\"MYSQL_CONNECTION_PING_PERIOD\", 600))\n+        \n+        return conn\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29cd1bc3cb2db46e0a599af0b2de2daf06402b3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 9 | Methods Changed: 10 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 52 | Churn Cumulative: 1165 | Contributors (this commit): 2 | Commits (past 90d): 47 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,33 @@\n+from twisted.spread import pb\n+from twisted.internet import reactor\n+\n+from scrapy.conf import settings\n+from scrapy.core import log\n+from scrapy.core.manager import scrapymanager\n+\n+class Broker(pb.Referenceable):\n+    def __init__(self, crawler, remote):\n+        self.__remote = remote\n+        self.__crawler = crawler\n+        try:\n+            deferred = self.__remote.callRemote(\"register_crawler\", domain, self)\n+        except pb.DeadReferenceError:\n+            self._set_status(None)\n+            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+        else:\n+            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n+    def remote_stop(self):\n+        scrapymanager.stop()\n+    \n+class ClusterCrawler:\n+    def __init__(self):\n+        if not settings.getbool('CLUSTER_CRAWLER_ENABLED'):\n+            raise NotConfigured\n+\n+        self.worker = None\n+\n+        factory = pb.PBClientFactory()\n+        reactor.connectTCP(\"localhost\", settings.getint('CLUSTER_WORKER_PORT'), factory)\n+        d = factory.getRootObject()\n+        d.addCallbacks(callback=lambda obj: self.worker=Node(self, obj), errback=lambda reason: log.msg(reason, log.ERROR))\n+        \n\\ No newline at end of file\n\n@@ -27,7 +27,7 @@ def my_import(name):\n         mod = getattr(mod, comp)\n     return mod\n \n-class Node(pb.Referenceable):\n+class Broker(pb.Referenceable):\n     def __init__(self, remote, name, master):\n         self.__remote = remote\n         self.alive = False\n@@ -177,7 +177,7 @@ class ClusterMaster:\n         self.loading = []\n         self.nodes = {}\n         self.start_time = datetime.datetime.utcnow()\n-        #on how statistics works, see self.update_nodes() and Nodes.remote_update()\n+        #on how statistics works, see self.update_nodes() and Broker.remote_update()\n         self.statistics = {\"domains\": {\"running\": set(), \"scraped\": {}, \"lost_count\": {}, \"lost\": set()}, \"scraped_count\": 0 }\n         self.global_settings = {}\n         #load cluster global settings\n@@ -231,7 +231,7 @@ class ClusterMaster:\n             \n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n-        node = Node(cworker, name, self)\n+        node = Broker(cworker, name, self)\n         self.nodes[name] = node\n         log.msg(\"Added cluster worker %s\" % name)\n \n\n@@ -19,7 +19,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         self.env = {}\n         #We conserve original setting format for info purposes (avoid lots of unnecesary \"SCRAPY_\")\n         self.scrapy_settings = spider_settings or {}\n-        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': '0', 'WEBCONSOLE_ENABLED': '0'})\n+        self.scrapy_settings.update({'LOGFILE': self.logfile, 'CLUSTER_WORKER_ENABLED': 0, 'CLUSTER_CRAWLER_ENABLED': 1, 'WEBCONSOLE_ENABLED': 0})\n         pickled_settings = pickle.dumps(self.scrapy_settings)\n         self.env[\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\"] = pickled_settings\n         self.env[\"PYTHONPATH\"] = \":\".join(sys.path)#this is need so this crawl process knows where to locate local_scrapy_settings.\n@@ -41,6 +41,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n         del self.procman.running[self.domain]\n+        del self.procman.crawlers[self.domain]\n         self.procman.update_master(self.domain, \"scraped\")\n \n class ClusterWorker(pb.Root):\n@@ -51,7 +52,8 @@ class ClusterWorker(pb.Root):\n \n         self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n         self.logdir = settings['CLUSTER_LOGDIR']\n-        self.running = {}\n+        self.running = {}#a dict domain->ScrapyProcessControl \n+        self.crawlers = {}#a dict domain->scrapy process remote pb connection\n         self.starttime = datetime.datetime.utcnow()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n@@ -86,8 +88,8 @@ class ClusterWorker(pb.Root):\n         if domain in self.running:\n             proc = self.running[domain]\n             log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n-            proc.transport.signalProcess('INT')\n-            proc.status = \"closing\"\n+            d = self.crawler[\"domain\"].callRemote(\"stop\")\n+            d.addCallbacks(callback=lambda x: proc.status=\"closing\", errback=lambda reason: log.msg(reason, log.ERROR))\n             return self.status(0, \"Stopped process %s\" % proc)\n         else:\n             return self.status(1, \"%s: domain not running.\" % domain)\n@@ -118,3 +120,6 @@ class ClusterWorker(pb.Root):\n                 return self.status(0, \"Started process %s.\" % scrapy_proc)\n             return self.status(2, \"Domain %s already running.\" % domain )\n         return self.status(1, \"No free slot to run another process.\")\n+\n+    def remote_register_crawler(self, domain, crawler):\n+        self.crawlers['domain'] = crawler\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b4d9f8f85f0b145a15e83e4dd4d9d2886ab9bff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 419 | Contributors (this commit): 2 | Commits (past 90d): 22 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -89,7 +89,9 @@ class ClusterWorker(pb.Root):\n             proc = self.running[domain]\n             log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n             d = self.crawler[\"domain\"].callRemote(\"stop\")\n-            d.addCallbacks(callback=lambda x: proc.status=\"closing\", errback=lambda reason: log.msg(reason, log.ERROR))\n+            def _close():\n+                proc.status = \"closing\"\n+            d.addCallbacks(callback=_close, errback=lambda reason: log.msg(reason, log.ERROR))\n             return self.status(0, \"Stopped process %s\" % proc)\n         else:\n             return self.status(1, \"%s: domain not running.\" % domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#290702d988547b33f0422a49c3f07248c1d42f65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 8 | Files Changed: 4 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 476 | Contributors (this commit): 2 | Commits (past 90d): 28 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,2 +1,3 @@\n from scrapy.contrib.pbcluster.worker.manager import ClusterWorker\n from scrapy.contrib.pbcluster.master.web import ClusterMasterWeb\n+from scrapy.contrib.pbcluster.crawler.manager import ClusterCrawler\n\\ No newline at end of file\n\n\n@@ -1,21 +1,24 @@\n+import os\n+\n from twisted.spread import pb\n from twisted.internet import reactor\n \n from scrapy.conf import settings\n from scrapy.core import log\n from scrapy.core.manager import scrapymanager\n+from scrapy.core.exceptions import NotConfigured\n \n class Broker(pb.Referenceable):\n     def __init__(self, crawler, remote):\n         self.__remote = remote\n         self.__crawler = crawler\n         try:\n-            deferred = self.__remote.callRemote(\"register_crawler\", domain, self)\n+            deferred = self.__remote.callRemote(\"register_crawler\", os.getpid(), self)\n         except pb.DeadReferenceError:\n             self._set_status(None)\n             log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n         else:\n-            deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n+            deferred.addCallbacks(callback=lambda x: None, errback=lambda reason: log.msg(reason, log.ERROR))\n     def remote_stop(self):\n         scrapymanager.stop()\n     \n@@ -29,5 +32,7 @@ class ClusterCrawler:\n         factory = pb.PBClientFactory()\n         reactor.connectTCP(\"localhost\", settings.getint('CLUSTER_WORKER_PORT'), factory)\n         d = factory.getRootObject()\n-        d.addCallbacks(callback=lambda obj: self.worker=Node(self, obj), errback=lambda reason: log.msg(reason, log.ERROR))\n+        def _set_worker(obj):\n+            self.worker = Broker(self, obj)\n+        d.addCallbacks(callback=_set_worker, errback=lambda reason: log.msg(reason, log.ERROR))\n         \n\\ No newline at end of file\n\n@@ -41,7 +41,7 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         log.msg(\"Reason type: %s. value: %s\" % (reason.type, reason.value) )\n         del self.procman.running[self.domain]\n-        del self.procman.crawlers[self.domain]\n+        del self.procman.crawlers[self.pid]\n         self.procman.update_master(self.domain, \"scraped\")\n \n class ClusterWorker(pb.Root):\n@@ -53,7 +53,7 @@ class ClusterWorker(pb.Root):\n         self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n         self.logdir = settings['CLUSTER_LOGDIR']\n         self.running = {}#a dict domain->ScrapyProcessControl \n-        self.crawlers = {}#a dict domain->scrapy process remote pb connection\n+        self.crawlers = {}#a dict pid->scrapy process remote pb connection\n         self.starttime = datetime.datetime.utcnow()\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n@@ -88,7 +88,7 @@ class ClusterWorker(pb.Root):\n         if domain in self.running:\n             proc = self.running[domain]\n             log.msg(\"ClusterWorker: Sending shutdown signal to domain=%s, pid=%d\" % (domain, proc.pid))\n-            d = self.crawler[\"domain\"].callRemote(\"stop\")\n+            d = self.crawlers[proc.pid].callRemote(\"stop\")\n             def _close():\n                 proc.status = \"closing\"\n             d.addCallbacks(callback=_close, errback=lambda reason: log.msg(reason, log.ERROR))\n@@ -123,5 +123,5 @@ class ClusterWorker(pb.Root):\n             return self.status(2, \"Domain %s already running.\" % domain )\n         return self.status(1, \"No free slot to run another process.\")\n \n-    def remote_register_crawler(self, domain, crawler):\n-        self.crawlers['domain'] = crawler\n\\ No newline at end of file\n+    def remote_register_crawler(self, pid, crawler):\n+        self.crawlers[pid] = crawler\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a690d33f244e4ff1e504afeaf4c11bcd25b86d9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 35 | Churn Cumulative: 35 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,35 @@\n+\"\"\"\n+Limits the scheduler request queue from the point of view of the spider.\n+That is, any requests generated by the spider, which its scheduling action\n+would result in an excess over the allowed limit of scheduler request queue,\n+will be filtered out.\n+The limit is setted from the spider attribute \"requests_queue_size\". If not\n+found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n+will be applied. If given a value of 0, no limit will be applied.\n+\"\"\"\n+from scrapy.core.engine import scrapyengine\n+from scrapy.conf import settings\n+from scrapy.http import Request\n+from scrapy.core import log\n+\n+class RequestLimitMiddleware(object):\n+    _last_queue_size = 0\n+    def process_result(self, response, result, spider):\n+        requests = []\n+        other = []\n+        [requests.append(r) if isinstance(r, Request) else other.append(r) for r in result]\n+\n+        max_pending = spider.requests_queue_size if hasattr(spider,\"requests_queue_size\") else settings.getint(\"REQUESTS_QUEUE_SIZE\")\n+        if not max_pending:\n+            accepted = requests\n+        else:\n+            free_slots = max_pending - len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n+            accepted = requests[:free_slots]\n+            dropped = set(requests) - set(accepted)\n+            if dropped:\n+                for r in dropped:\n+                    log.msg(\"Ignoring link (max schedule queue size reached): %s \" % r.url, level=log.WARNING, domain=spider.domain_name)\n+        actual_size = len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n+        log.msg(\"queue size: %d (%+d)\" % (actual_size, actual_size - self._last_queue_size) )\n+        self._last_queue_size = actual_size\n+        return accepted + other\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e3f70a81013a6d1327dc61d47e3db12cb0381e72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 45 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ would result in an excess over the allowed limit of scheduler request queue,\n will be filtered out.\n The limit is setted from the spider attribute \"requests_queue_size\". If not\n found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n-will be applied. If given a value of 0, no limit will be applied.\n+will be applied.　If given a value of 0, no limit will be applied.\n \"\"\"\n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n@@ -13,7 +13,7 @@ from scrapy.http import Request\n from scrapy.core import log\n \n class RequestLimitMiddleware(object):\n-    _last_queue_size = 0\n+    #_last_queue_size = 0\n     def process_result(self, response, result, spider):\n         requests = []\n         other = []\n@@ -29,7 +29,7 @@ class RequestLimitMiddleware(object):\n             if dropped:\n                 for r in dropped:\n                     log.msg(\"Ignoring link (max schedule queue size reached): %s \" % r.url, level=log.WARNING, domain=spider.domain_name)\n-        actual_size = len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n-        log.msg(\"queue size: %d (%+d)\" % (actual_size, actual_size - self._last_queue_size) )\n-        self._last_queue_size = actual_size\n+        #actual_size = len(scrapyengine.scheduler.pending_requests[spider.domain_name])\n+        #log.msg(\"queue size: %d (%+d)\" % (actual_size, actual_size - self._last_queue_size) )\n+        #self._last_queue_size = actual_size\n         return accepted + other\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#baf32540d627ad8018a28c8004e950e0e32d100e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 46 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,4 @@\n+# -*- coding: utf8 -*-\n \"\"\"\n Limits the scheduler request queue from the point of view of the spider.\n That is, any requests generated by the spider, which its scheduling action\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d0f12be4c069d61631866c8802d868eb2c1831b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,3 @@\n-# -*- coding: utf8 -*-\n \"\"\"\n Limits the scheduler request queue from the point of view of the spider.\n That is, any requests generated by the spider, which its scheduling action\n@@ -6,7 +5,7 @@ would result in an excess over the allowed limit of scheduler request queue,\n will be filtered out.\n The limit is setted from the spider attribute \"requests_queue_size\". If not\n found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n-will be applied.　If given a value of 0, no limit will be applied.\n+will be applied. If given a value of 0, no limit will be applied.\n \"\"\"\n from scrapy.core.engine import scrapyengine\n from scrapy.conf import settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a018cadff528ec6b3dd56d6d58f1d4864a36ba0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 4 | Churn Cumulative: 578 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.5\n\nDIFF:\n@@ -86,6 +86,7 @@ class ExecutionEngine(object):\n     def removetask(self, tsk):\n         \"\"\"Remove a looping task previously added with addtask() method\"\"\"\n         self.tasks = [(t, i, n) for (t, i, n) in self.tasks if t is not tsk]\n+        if tsk.running:\n             tsk.stop()\n \n     def listenTCP(self, *args, **kwargs):\n@@ -130,6 +131,7 @@ class ExecutionEngine(object):\n                 spider = spiders.fromdomain(domain)\n                 signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status='cancelled')\n             for tsk, _, _ in self.tasks: # stop looping calls\n+                if tsk.running:\n                     tsk.stop()\n             self.tasks = []\n             for p in [p for p in self.ports if not isinstance(p, tuple)]:\n@@ -143,7 +145,7 @@ class ExecutionEngine(object):\n         self.paused = True\n \n     def resume(self):\n-        \"\"\"Resume the execution enine\"\"\"\n+        \"\"\"Resume the execution engine\"\"\"\n         self.paused = False\n \n     def is_idle(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8e72e4e60e5bb84702ebdecd95253aee19e6fa68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 29/21 | Churn Δ: 116 | Churn Cumulative: 349 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.27419354838709675\n\nDIFF:\n@@ -0,0 +1,79 @@\n+\"\"\"\n+Adaptors for item adaptation pipe.\n+\"\"\"\n+\n+from scrapy.item.models import BaseAdaptor\n+from scrapy.utils.python import flatten, unique\n+from scrapy.xpath import XPathSelector\n+from scrapy.utils.misc import unquote_html, location_str\n+\n+class ExtendedAdaptor(BaseAdaptor):\n+\n+    def do(self, function, a, *args, **kwargs):\n+        \"\"\"Execute a function of the pipeline examining kwargs for possible \n+        pre/post functions. Also prints a lot of useful debugging info.\"\"\"\n+    \n+        debug = kwargs.get('debug')\n+        fname = function.__name__\n+        f = kwargs.get('pre_%s' % fname)\n+        if f and a:\n+            if debug:\n+                print \"  pre_%s | input  >\" % fname, a\n+            a = f(a)\n+            if debug:\n+                print \"  pre_%s | output >\" % fname, a\n+    \n+        if debug:\n+            print repr(kwargs)\n+            print \"  %07s | input >\" % fname, location_str(a)\n+        a = function(a, *args, **kwargs)\n+        if debug:\n+            print \"  %07s | output>\" % fname, a\n+    \n+        f = kwargs.get('post_%s' % fname)\n+        if f and a:\n+            if debug:\n+                print \"  post_%s | input  >\" % fname, a\n+            a = f(a)\n+            if debug:\n+                print \"  post_%s | output >\" % fname, a\n+        return a\n+        \n+class ExtractAdaptor(ExtendedAdaptor):\n+\n+    def function(self, item, attrname, location, **pipeargs):\n+        return self.do(self._extract, location, **pipeargs)\n+\n+    def _extract(self, location, **kwargs):\n+        \"\"\"Extract a list of strings from the location passed.\n+        Receives a list of XPathSelectors or an XPathSelector,\n+        or a list of strings or a string.\n+        \n+        Return a list of strings extracted.\n+\n+        This function *always* returns a list.\n+        \"\"\"\n+        if not location:\n+            return []\n+        if isinstance(location, (list, tuple)):\n+            strings = flatten([self._extract(o, **kwargs) for o in location])\n+            if kwargs.get('remove_dupes', False):\n+                strings = unique(strings)\n+            if kwargs.get('first', False):\n+                return strings[:1]\n+            else:\n+                return strings\n+        # XPathSelector\n+        elif isinstance(location, XPathSelector):\n+            strings = [location.extract()]\n+        # Strings\n+        elif isinstance(location, unicode):\n+            strings = [unquote_html(location, keep_reserved=True)]\n+        elif isinstance(location, str):\n+            encoding = kwargs.get(\"encoding\", settings.get('DEFAULT_DATA_ENCODING'))\n+            strings = [unquote_html(unicode(location, encoding), keep_reserved=True)]\n+        else:\n+            raise TypeError, \"unsupported location type: %s\" % type(location)\n+\n+        return strings\n+        \n\n@@ -1,3 +1,19 @@\n+class BaseAdaptor:\n+    def function(self, item, attrname, value, **pipeargs):\n+        raise NotImplemented\n+\n+#default adaptors\n+class ExtractAdaptor(BaseAdaptor):\n+    def function(self, item, attrname, value, **pipeargs):\n+        if hasattr(value, 'extract'):\n+            value = value.extract()\n+        return value\n+\n+class AssignAdaptor(BaseAdaptor):\n+    def function(self, item, attrname, value, **pipeargs):\n+        if not hasattr(item, attrname):\n+            setattr(item, attrname, value)\n+\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n@@ -6,11 +22,12 @@ class ScrapedItem(object):\n     * guid (unique global indentifier)\n     * url (URL where that item was scraped from)\n     \"\"\"\n+    adaptors_pipe = [ExtractAdaptor(), AssignAdaptor()]\n     \n-    def assign(self, name, value):\n-        \"\"\"Assign an attribute. Can receive a Selector in value which will\n-        extract its data \"\"\"\n+    def set_adaptors_pipe(adaptors_pipes):\n+        ScrapedItem.adaptors_pipes = adaptors_pipes\n \n-        if hasattr(value, 'extract'):\n-            value = value.extract()\n-        setattr(self, name, value)\n+    def attribute(self, name, value, **pipeargs):\n+\n+        for adaptor in ScrapedItem.adaptors_pipe:\n+            value = adaptor.function(self, name, value, **pipeargs)\n\n@@ -202,4 +202,12 @@ def extract_regex(regex, text, encoding):\n     else:\n         return [unquote_html(unicode(s, encoding), keep_reserved=True) for s in strings]\n \n+_regex_type = type(re.compile(\"\", 0))\n \n+def location_str(location):\n+    \"\"\"Return a human friendly representation of a parser location\"\"\"\n+\n+    if isinstance(location, _regex_type):\n+        return \"(regex) \" + location.pattern\n+    else:\n+        return location\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#632b975ce7fa7abc68bd00a0d74933f6c96a2d3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 80 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -6,6 +6,7 @@ from scrapy.item.models import BaseAdaptor\n from scrapy.utils.python import flatten, unique\n from scrapy.xpath import XPathSelector\n from scrapy.utils.misc import unquote_html, location_str\n+from scrapy.conf import settings\n \n class ExtendedAdaptor(BaseAdaptor):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3975bf95c76814de7c0e62593e81921d933eac77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 254 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n request-response middleware extension\n \"\"\"\n-from scrapy.core import log\n+from scrapy.core import signals, log\n from scrapy.http import Request, Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_class, mustbe_deferred\n@@ -51,7 +51,7 @@ class DownloaderMiddlewareManager(object):\n         function.\n \n         If returns a Response object, Scrapy won't bother calling ANY other\n-        request or exception middleware, or the appropriate download function;\n+        request or exception middleware, or the appropiate download function;\n         it'll return that Response. Response middleware is always called on\n         every response.\n \n@@ -132,7 +132,7 @@ class DownloaderMiddlewareManager(object):\n             for method in self.request_middleware:\n                 response = method(request=request, spider=spider)\n                 assert response is None or isinstance(response, (Response, Request)), \\\n-                        'Middleware %s.process_request must returns None, Response or Request, got %s ' % \\\n+                        'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n                         (method.im_self.__class__.__name__, response.__class__.__name__)\n                 if response:\n                     return response\n@@ -146,10 +146,12 @@ class DownloaderMiddlewareManager(object):\n             for method in self.response_middleware:\n                 response = method(request=request, response=response, spider=spider)\n                 assert isinstance(response, (Response, Request)), \\\n-                    'Middleware %s.process_response must returns Response or Request, got %s ' % \\\n+                    'Middleware %s.process_response must return Response or Request, got %s' % \\\n                     (method.im_self.__class__.__name__, type(response))\n                 if isinstance(response, Request):\n+                    signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)\n                     return response\n+            signals.send_catch_log(signal=signals.response_received, sender=self.__class__, response=response, spider=spider)\n             return response\n \n         def process_exception(_failure):\n@@ -157,7 +159,7 @@ class DownloaderMiddlewareManager(object):\n             for method in self.exception_middleware:\n                 response = method(request=request, exception=exception, spider=spider)\n                 assert response is None or isinstance(response, (Response, Request)), \\\n-                    'Middleware %s.process_exception must returns None, Response or Request, got %s ' % \\\n+                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n                     (method.im_self.__class__.__name__, type(response))\n                 if response:\n                     return response\n\n@@ -43,6 +43,10 @@ request_received = object()\n # args: request, spider\n request_uploaded = object()\n \n+# When new response is received (by the engine) from the downloader (middleware)\n+# args: response, spider\n+response_received = object()\n+\n # When response arrives from the downloader\n # args: response, spider\n response_downloaded = object()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cdd88956144155dfca3357bfea7adf7ede84b9fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 246 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -56,7 +56,8 @@ class Replay(object):\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.item_passed, signal=signals.item_passed)\n-        dispatcher.connect(self.response_downloaded, signal=signals.response_downloaded)\n+        dispatcher.connect(self.response_received, signal=signals.response_received)\n+\n \n     def play(self, args=None, opts=None):\n         if self.recording:\n@@ -86,7 +87,7 @@ class Replay(object):\n \n     def engine_stopped(self):\n         if self.recording or self.updating:\n-            log.msg(\"Replay: recorded in %s: %d/%d scraped/passed items, %d downloaded responses\" % \\\n+            log.msg(\"Replay: recorded in %s: %d/%d scraped/passed items, %d received responses\" % \\\n                 (self.repfile, len(self.scraped_old), len(self.passed_old), len(self.responses_old)))\n         self._save()\n         self.cleanup()\n@@ -103,10 +104,10 @@ class Replay(object):\n         else:\n             self.passed_new[str(item.guid)] = item.copy()\n \n-    def response_downloaded(self, response, spider):\n+    def response_received(self, response, spider):\n         #key = response.request.fingerprint()\n         key = response.version()\n-        if self.recording and key:\n+        if (self.recording or self.updating) and key:\n             self.responses_old[key] = response.copy()\n         elif key:\n             self.responses_new[key] = response.copy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee59bd87ab973e47ca3ddaedad76e7eadb18c740", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 176 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -91,7 +91,7 @@ class Command(ScrapyCommand):\n                 s += display.pformat(r)\n         else:\n             s += \"\\n\".join([str(r) for r in self.replay.responses_old.values()]) + \"\\n\"\n-        s += \">>> Total: %d responses downloaded\\n\" % len(self.replay.responses_old)\n+        s += \">>> Total: %d responses received\\n\" % len(self.replay.responses_old)\n         return s\n \n     def action_diff(self, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b0dd66ec1d841f3c30a7cd733d63e64b06e843c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 55 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n \"\"\"\n Limits the scheduler request queue from the point of view of the spider.\n-That is, any requests generated by the spider, which its scheduling action\n-would result in an excess over the allowed limit of scheduler request queue,\n-will be filtered out.\n+That is, if the scheduler queue contains an equal or greater ammount of\n+requests than the specified limit, the further requests generated by the\n+spider will be ignored.\n The limit is setted from the spider attribute \"requests_queue_size\". If not\n found, from the scrapy setting \"REQUESTS_QUEUE_SIZE\". If not found, no limit\n will be applied. If given a value of 0, no limit will be applied.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#801b804a4d579e46e32688d84120bcdee87d36bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 43 | Churn Cumulative: 619 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,14 +42,14 @@ def custom_commands_dict():\n                 print 'WARNING: Custom command module %s exists but Command class not found' % modname\n     return d\n \n-def getcmdname():\n-    for arg in sys.argv[1:]:\n+def getcmdname(argv):\n+    for arg in argv[1:]:\n         if not arg.startswith('-'):\n             return arg\n \n-def usage():\n-    s  = \"usage: %s <subcommand> [options] [args]\\n\" % sys.argv[0]\n-    s += \"       %s <subcommand> -h\\n\\n\" % sys.argv[0]\n+def usage(argv):\n+    s  = \"usage: %s <subcommand> [options] [args]\\n\" % argv[0]\n+    s += \"       %s <subcommand> -h\\n\\n\" % argv[0]\n     s += \"Built-in subcommands:\\n\"\n \n     builtin_cmds = builtin_commands_dict()\n@@ -96,16 +96,19 @@ def command_settings(cmdname):\n command_executed = {}\n \n def execute():\n+    execute_with_args(sys.argv)\n+\n+def execute_with_args(argv):\n     spiders.load()\n     cmds = builtin_commands_dict()\n     cmds.update(custom_commands_dict())\n \n-    cmdname = getcmdname()\n+    cmdname = getcmdname(argv)\n     command_settings(cmdname)\n \n     if not cmdname:\n         print \"Scrapy %s\\n\" % scrapy.__version__\n-        print usage()\n+        print usage(argv)\n         sys.exit()\n \n     parser = optparse.OptionParser()\n@@ -118,10 +121,10 @@ def execute():\n     else:\n         print \"Scrapy %s\\n\" % scrapy.__version__\n         print \"Unknown command: %s\\n\" % cmdname\n-        print 'Type \"%s -h\" for help' % sys.argv[0]\n+        print 'Type \"%s -h\" for help' % argv[0]\n         sys.exit()\n \n-    (opts, args) = parser.parse_args()\n+    (opts, args) = parser.parse_args(args=argv[1:])\n     del args[0]  # args[0] is cmdname\n \n     # storing command executed info for later reference\n\n@@ -4,6 +4,7 @@ from scrapy.command import ScrapyCommand\n from scrapy.replay import Replay\n from scrapy.utils import display\n from scrapy.conf import settings\n+from scrapy.command import cmdline\n \n class Command(ScrapyCommand):\n     def syntax(self):\n@@ -31,6 +32,8 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--nocolour\", dest=\"nocolour\", action=\"store_true\", help=\"disable colorized output (for console only)\")\n         parser.add_option(\"-i\", \"--ignore\", dest=\"ignores\", action=\"append\", help=\"item attribute to ignore. can be passed multiple times\", metavar=\"ATTR\")\n         parser.add_option(\"--target\", dest=\"targets\", action=\"append\", help=\"crawl TARGET instead of recorded urls/domains. can be passed multiple times\")\n+        # adding option to update \n+        parser.add_option(\"--pages\", dest=\"pages\", action=\"store_true\", help=\"update all the pages in the replay file, recording it again.\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n@@ -75,7 +78,16 @@ class Command(ScrapyCommand):\n         self.replay.play(args=opts.targets)\n \n     def action_update(self, opts):\n-        self.action_crawl(opts)\n+        self.replay.update(args=opts.targets, opts=opts.__dict__)\n+        if (opts.pages):\n+            args = ['decobot-ctl.py', 'crawl']\n+            args.extend(self.replay.options['args'])\n+            for k in self.replay.options['opts']:                \n+                if self.replay.options['opts'][k]:                   \n+                    args.append(\"--%s\" % k)\n+                    if self.replay.options['opts'][k] != True:\n+                        args.append(self.replay.options['opts'][k])\n+            cmdline.execute_with_args(args)\n \n     def action_showitems(self, opts):\n         s = \"\"\n\n@@ -68,6 +68,7 @@ class Replay(object):\n         scrapymanager.runonce(*args, **opts)\n \n     def record(self, args=None, opts=None):\n+        self.recording = True\n         self.options.clear()\n         self.options['args'] = args or []\n         self.options['opts'] = opts or {}\n@@ -79,7 +80,12 @@ class Replay(object):\n \n     def update(self, args=None, opts=None):\n         self.updating = True\n-\n+        if (opts['pages']):\n+            args = self.options['args']\n+            opts = self.options['opts']\n+            settings.overrides['CACHE2_EXPIRATION_SECS'] = 0            \n+            settings.overrides['CACHE2_IGNORE_MISSING'] = False\n+        else:            \n             self.play(args, opts)\n \n     def engine_started(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8877426a13885b9983297cb04062770ae4f02a56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 130 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n \"\"\"\n-Adaptors for item adaptation pipe.\n+Generic Adaptors for item adaptation pipe. Performs tasks\n+usually needed by most cases\n \"\"\"\n \n from scrapy.item.models import BaseAdaptor\n\n@@ -1,4 +1,4 @@\n-class BaseAdaptor:\n+class BaseAdaptor(object):\n     def function(self, item, attrname, value, **pipeargs):\n         raise NotImplemented\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b3662ee8977643cf4b8187c077d7043f7f5801e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 7/8 | Churn Δ: 98 | Churn Cumulative: 228 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,8 @@ Generic Adaptors for item adaptation pipe. Performs tasks\n usually needed by most cases\n \"\"\"\n \n+import re\n+\n from scrapy.item.models import BaseAdaptor\n from scrapy.utils.python import flatten, unique\n from scrapy.xpath import XPathSelector\n@@ -12,18 +14,10 @@ from scrapy.conf import settings\n class ExtendedAdaptor(BaseAdaptor):\n \n     def do(self, function, a, *args, **kwargs):\n-        \"\"\"Execute a function of the pipeline examining kwargs for possible \n-        pre/post functions. Also prints a lot of useful debugging info.\"\"\"\n+        \"\"\"Executes an adaptor printing a lot of useful debugging info.\"\"\"\n     \n         debug = kwargs.get('debug')\n         fname = function.__name__\n-        f = kwargs.get('pre_%s' % fname)\n-        if f and a:\n-            if debug:\n-                print \"  pre_%s | input  >\" % fname, a\n-            a = f(a)\n-            if debug:\n-                print \"  pre_%s | output >\" % fname, a\n     \n         if debug:\n             print repr(kwargs)\n@@ -32,21 +26,14 @@ class ExtendedAdaptor(BaseAdaptor):\n         if debug:\n             print \"  %07s | output>\" % fname, a\n     \n-        f = kwargs.get('post_%s' % fname)\n-        if f and a:\n-            if debug:\n-                print \"  post_%s | input  >\" % fname, a\n-            a = f(a)\n-            if debug:\n-                print \"  post_%s | output >\" % fname, a\n         return a\n         \n class ExtractAdaptor(ExtendedAdaptor):\n \n-    def function(self, item, attrname, location, **pipeargs):\n-        return self.do(self._extract, location, **pipeargs)\n+    def function(self, item, location, **pipeargs):\n+        return self.do(self.extract, location, **pipeargs)\n \n-    def _extract(self, location, **kwargs):\n+    def extract(self, location, **kwargs):\n         \"\"\"Extract a list of strings from the location passed.\n         Receives a list of XPathSelectors or an XPathSelector,\n         or a list of strings or a string.\n@@ -58,7 +45,7 @@ class ExtractAdaptor(ExtendedAdaptor):\n         if not location:\n             return []\n         if isinstance(location, (list, tuple)):\n-            strings = flatten([self._extract(o, **kwargs) for o in location])\n+            strings = flatten([self.extract(o, **kwargs) for o in location])\n             if kwargs.get('remove_dupes', False):\n                 strings = unique(strings)\n             if kwargs.get('first', False):\n@@ -79,3 +66,59 @@ class ExtractAdaptor(ExtendedAdaptor):\n \n         return strings\n         \n+   \n+_clean_spaces_re = re.compile(\"\\s+\", re.U)\n+_remove_root_re = re.compile(r'^\\s*<.*?>(.*)</.*>\\s*$', re.DOTALL)\n+_xml_remove_tags_re = re.compile(r'<[a-zA-Z\\/!][^>]*?>')\n+_xml_remove_cdata_re = re.compile('<!\\[CDATA\\[(.*)\\]\\]', re.S)\n+_xml_cdata_split_re = re.compile('(<!\\[CDATA\\[.*?\\]\\]>)', re.S)\n+\n+class HtmlCleanAdaptor(ExtendedAdaptor):\n+\n+    def function(self, item, location, **pipeargs):\n+        return self.do(self.clean, string, **pipeargs)\n+    \n+    def clean(self, string, **kwargs):\n+        \"\"\"Clean (list of) strings removing newlines, spaces, etc\"\"\"\n+        if isinstance(string, list):\n+            return [self.clean(s, **kwargs) for s in string]\n+        \n+        xml = self._remove_tags(string, **kwargs)\n+\n+        if kwargs.get('remove_root', True) and not kwargs.get('remove_tags', True):\n+            m = _remove_root_re.search(xml)\n+            if m:\n+                xml = m.group(1)\n+\n+        if kwargs.get('remove_spaces', True):\n+            xml = _clean_spaces_re.sub(' ', xml)\n+        if kwargs.get('strip', True):\n+            xml = xml.strip()\n+    \n+        return xml\n+    \n+    def _remove_tags(self, xml, **kwargs):\n+        if kwargs.get('remove_tags', True):\n+            xml = _xml_remove_tags_re.sub(' ', xml)\n+        return xml\n+        \n+class XmlCleanAdaptor(HtmlCleanAdaptor):\n+    \n+    def _remove_tags(self, xml, **kwargs):\n+        #process in pieces the text that contains CDATA. The first check is to avoid unnecesary regex check\n+        if _xml_remove_cdata_re.search(xml):\n+            pieces = []\n+            \n+            for piece in _xml_cdata_split_re.split(xml):\n+                \n+                m = _xml_remove_cdata_re.search(piece)\n+                if m:\n+                    if kwargs.get('remove_cdata', True):#remove cdata special tag\n+                        pieces.append(HtmlCleanAdaptor._remove_tags(self, m.groups()[0], **kwargs))\n+                    else:\n+                        pieces.append(piece)#conserve intact the cdata\n+                else:\n+                    pieces.append(HtmlCleanAdaptor._remove_tags(self, piece, **kwargs))\n+\n+            xml = \"\".join(pieces)\n+        return xml\n\n@@ -1,19 +1,14 @@\n class BaseAdaptor(object):\n-    def function(self, item, attrname, value, **pipeargs):\n+    def function(self, item, value, **pipeargs):\n         raise NotImplemented\n \n #default adaptors\n class ExtractAdaptor(BaseAdaptor):\n-    def function(self, item, attrname, value, **pipeargs):\n+    def function(self, item, value, **pipeargs):\n         if hasattr(value, 'extract'):\n             value = value.extract()\n         return value\n \n-class AssignAdaptor(BaseAdaptor):\n-    def function(self, item, attrname, value, **pipeargs):\n-        if not hasattr(item, attrname):\n-            setattr(item, attrname, value)\n-\n class ScrapedItem(object):\n     \"\"\"\n     This is the base class for all scraped items.\n@@ -22,7 +17,7 @@ class ScrapedItem(object):\n     * guid (unique global indentifier)\n     * url (URL where that item was scraped from)\n     \"\"\"\n-    adaptors_pipe = [ExtractAdaptor(), AssignAdaptor()]\n+    adaptors_pipe = [ExtractAdaptor()]\n     \n     def set_adaptors_pipe(adaptors_pipes):\n         ScrapedItem.adaptors_pipes = adaptors_pipes\n@@ -30,4 +25,6 @@ class ScrapedItem(object):\n     def attribute(self, name, value, **pipeargs):\n \n         for adaptor in ScrapedItem.adaptors_pipe:\n-            value = adaptor.function(self, name, value, **pipeargs)\n+            value = adaptor.function(self, value, **pipeargs)\n+        if not hasattr(item, name):\n+            setattr(item, name, value)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f49c7c0d4eec8090e30795608c0a6a6a231cee3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 169 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,9 @@ class ExtractAdaptor(ExtendedAdaptor):\n     def function(self, item, location, **pipeargs):\n         return self.do(self.extract, location, **pipeargs)\n \n+    def _extract(self, location, **kwargs):\n+        return self.extract(location, **kwargs)\n+    \n     def extract(self, location, **kwargs):\n         \"\"\"Extract a list of strings from the location passed.\n         Receives a list of XPathSelectors or an XPathSelector,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
