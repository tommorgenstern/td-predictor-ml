{"custom_id": "keras#cbb64ab9ecd0a22055cf4e35007e036910b4f3e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 845 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -144,9 +144,6 @@ class ExportArchive:\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n             if backend.backend() == \"jax\":\n-                self._tf_trackable.variables += tree.flatten(\n-                    tree.map_structure(tf.Variable, resource.variables)\n-                )\n                 self._tf_trackable.trainable_variables += tree.flatten(\n                     tree.map_structure(\n                         tf.Variable, resource.trainable_variables\n@@ -157,6 +154,10 @@ class ExportArchive:\n                         tf.Variable, resource.non_trainable_variables\n                     )\n                 )\n+                self._tf_trackable.variables = (\n+                    self._tf_trackable.trainable_variables\n+                    + self._tf_trackable.non_trainable_variables\n+                )\n             else:\n                 self._tf_trackable.variables += resource.variables\n                 self._tf_trackable.trainable_variables += (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff2557efca15e00cfb9cdb348f5dbde248bfd86b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 39 | Files Changed: 8 | Hunks: 29 | Methods Changed: 16 | Complexity Δ (Sum/Max): 6/2 | Churn Δ: 146 | Churn Cumulative: 10912 | Contributors (this commit): 29 | Commits (past 90d): 34 | Contributors (cumulative): 77 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,13 @@ from keras.utils.naming import auto_name\n \n class KerasVariable:\n     def __init__(\n-        self, initializer, shape=None, dtype=None, trainable=True, name=None\n+        self,\n+        initializer,\n+        shape=None,\n+        dtype=None,\n+        trainable=True,\n+        autocast=True,\n+        name=None,\n     ):\n         name = name or auto_name(self.__class__.__name__)\n         if not isinstance(name, str) or \"/\" in name:\n@@ -34,6 +40,7 @@ class KerasVariable:\n         self._regularizer = None\n         self._constraint = None\n         self._trainable = trainable\n+        self._autocast = autocast\n         if isinstance(initializer, str):\n             from keras import initializers\n \n@@ -107,7 +114,7 @@ class KerasVariable:\n \n     def _maybe_autocast(self, value):\n         autocast_scope = get_autocast_scope()\n-        if autocast_scope is not None:\n+        if self._autocast and autocast_scope is not None:\n             return autocast_scope.maybe_cast(value)\n         return value\n \n@@ -157,7 +164,11 @@ class KerasVariable:\n     @property\n     def dtype(self):\n         autocast_scope = get_autocast_scope()\n-        if autocast_scope is not None and is_float_dtype(self._dtype):\n+        if (\n+            self._autocast\n+            and autocast_scope is not None\n+            and is_float_dtype(self._dtype)\n+        ):\n             return autocast_scope.dtype\n         return self._dtype\n \n\n@@ -139,6 +139,22 @@ class VariablePropertiesTest(test_case.TestCase, parameterized.TestCase):\n         with AutocastScope(\"float16\"):\n             self.assertEqual(backend.standardize_dtype(v.value.dtype), \"int32\")\n \n+        # Test autocast argument\n+        v = backend.Variable(\n+            initializer=initializers.RandomNormal(),\n+            shape=(2, 2),\n+            dtype=\"float32\",\n+            autocast=False,\n+        )\n+        self.assertEqual(v.dtype, \"float32\")\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n+        with AutocastScope(\"float16\"):\n+            self.assertEqual(\n+                backend.standardize_dtype(v.value.dtype),\n+                \"float32\",  # ignore AutocastScope\n+            )\n+        self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n+\n     @parameterized.parameters(\n         *((dtype for dtype in ALLOWED_DTYPES if dtype != \"string\"))\n     )\n\n@@ -424,6 +424,7 @@ class Layer(BackendLayer, Operation):\n         initializer,\n         dtype=None,\n         trainable=True,\n+        autocast=True,\n         regularizer=None,\n         constraint=None,\n         name=None,\n@@ -437,6 +438,7 @@ class Layer(BackendLayer, Operation):\n             initializer=initializer,\n             dtype=dtype,\n             trainable=trainable,\n+            autocast=autocast,\n             regularizer=regularizer,\n             constraint=constraint,\n             name=name,\n@@ -448,6 +450,7 @@ class Layer(BackendLayer, Operation):\n         initializer=None,\n         dtype=None,\n         trainable=True,\n+        autocast=True,\n         regularizer=None,\n         constraint=None,\n         name=None,\n@@ -455,29 +458,28 @@ class Layer(BackendLayer, Operation):\n         \"\"\"Add a weight variable to the layer.\n \n         Args:\n-            shape: Shape tuple for the variable.\n-                Must be fully-defined (no `None` entries).\n-                Defaults to `()` (scalar) if unspecified.\n-            initializer: Initializer object to use to\n-                populate the initial variable value,\n-                or string name of a built-in initializer\n-                (e.g. `\"random_normal\"`). If unspecified,\n-                defaults to `\"glorot_uniform\"`\n-                for floating-point variables and to `\"zeros\"`\n+            shape: Shape tuple for the variable. Must be fully-defined\n+                (no `None` entries). Defaults to `()` (scalar) if unspecified.\n+            initializer: Initializer object to use to populate the initial\n+                variable value, or string name of a built-in initializer\n+                (e.g. `\"random_normal\"`). If unspecified, defaults to\n+                `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"`\n                 for all other types (e.g. int, bool).\n-            dtype: Dtype of the variable to create,\n-                e.g. `\"float32\"`. If unspecified,\n-                defaults to the layer's\n-                variable dtype (which itself defaults to\n-                `\"float32\"` if unspecified).\n-            trainable: Boolean, whether the variable should\n-                be trainable via backprop or whether its\n-                updates are managed manually.\n-            constraint: Contrainst object to call on the\n-                variable after any optimizer update,\n-                or string name of a built-in constraint.\n-            name: String name of the variable. Useful\n-                for debugging purposes.\n+            dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n+                unspecified, defaults to the layer's variable dtype\n+                (which itself defaults to `\"float32\"` if unspecified).\n+            trainable: Boolean, whether the variable should be trainable via\n+                backprop or whether its updates are managed manually. Defaults\n+                to `True`.\n+            autocast: Boolean, whether to autocast the variable when accessing\n+                it. Defaults to `True`.\n+            regularizer: Regularizer object to call to apply penalty on the\n+                weight. These penalties are summed into the loss function\n+                during optimization. Defaults to `None`.\n+            constraint: Contrainst object to call on the variable after any\n+                optimizer update, or string name of a built-in constraint.\n+                Defaults to `None`.\n+            name: String name of the variable. Useful for debugging purposes.\n         \"\"\"\n         self._check_super_called()\n         if shape is None:\n@@ -498,6 +500,7 @@ class Layer(BackendLayer, Operation):\n                 shape=shape,\n                 dtype=dtype,\n                 trainable=trainable,\n+                autocast=autocast,\n                 name=name,\n             )\n         # Will be added to layer.losses\n\n@@ -464,6 +464,24 @@ class LayerTest(testing.TestCase):\n                 assertEqual(backend.standardize_dtype(self.v.dtype), \"float32\")\n                 return x + self.v\n \n+        # A layer that is explicitly mixed precision but with autocast=False\n+        # weight.\n+        class InnerLayerThree(layers.Layer):\n+            def __init__(self):\n+                super().__init__(dtype=\"mixed_float16\")\n+                self.v = self.add_weight(\n+                    shape=(),\n+                    initializer=\"ones\",\n+                    trainable=True,\n+                    autocast=False,\n+                )\n+                self.built = True\n+\n+            def call(self, x):\n+                # Should not autocast `self.v`.\n+                assertEqual(backend.standardize_dtype(self.v.dtype), \"float32\")\n+                return ops.add(x, self.v)\n+\n         # A layer that is explicitly mixed precision with inner layers.\n         class MixedPrecisionLayer(layers.Layer):\n             def __init__(self):\n@@ -475,16 +493,19 @@ class LayerTest(testing.TestCase):\n                 )\n                 self.inner_one = InnerLayerOne()\n                 self.inner_two = InnerLayerTwo()\n+                self.inner_three = InnerLayerThree()\n                 self.built = True\n \n             def call(self, x):\n                 # Should autocast.\n                 assertEqual(backend.standardize_dtype(self.v.dtype), \"float16\")\n-                return self.inner_two(self.inner_one(x + self.v))\n+                return self.inner_three(\n+                    self.inner_two(self.inner_one(x + self.v))\n+                )\n \n         layer = MixedPrecisionLayer()\n         y = layer(np.array(0.0))\n-        self.assertEqual(y, 3.0)\n+        self.assertEqual(y, 4.0)\n \n     @pytest.mark.skipif(\n         backend.backend() == \"numpy\",\n\n@@ -176,6 +176,7 @@ class BatchNormalization(Layer):\n                 regularizer=self.gamma_regularizer,\n                 constraint=self.gamma_constraint,\n                 trainable=True,\n+                autocast=False,\n             )\n         if self.center:\n             self.beta = self.add_weight(\n@@ -185,18 +186,21 @@ class BatchNormalization(Layer):\n                 regularizer=self.beta_regularizer,\n                 constraint=self.beta_constraint,\n                 trainable=True,\n+                autocast=False,\n             )\n         self.moving_mean = self.add_weight(\n             shape=shape,\n             name=\"moving_mean\",\n             initializer=self.moving_mean_initializer,\n             trainable=False,\n+            autocast=False,\n         )\n         self.moving_variance = self.add_weight(\n             shape=shape,\n             name=\"moving_variance\",\n             initializer=self.moving_variance_initializer,\n             trainable=False,\n+            autocast=False,\n         )\n         self.input_spec = InputSpec(\n             ndim=len(input_shape), axes={self.axis: input_shape[self.axis]}\n@@ -217,24 +221,15 @@ class BatchNormalization(Layer):\n             inputs = ops.cast(inputs, \"float32\")\n \n         if training and self.trainable:\n-            mean, variance = self._moments(\n-                inputs,\n-                mask,\n-            )\n+            mean, variance = self._moments(inputs, mask)\n             moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n             moving_variance = ops.cast(self.moving_variance, inputs.dtype)\n             self.moving_mean.assign(\n-                ops.cast(\n-                    moving_mean * self.momentum + mean * (1.0 - self.momentum),\n-                    inputs.dtype,\n-                )\n+                moving_mean * self.momentum + mean * (1.0 - self.momentum)\n             )\n             self.moving_variance.assign(\n-                ops.cast(\n                 moving_variance * self.momentum\n-                    + variance * (1.0 - self.momentum),\n-                    inputs.dtype,\n-                )\n+                + variance * (1.0 - self.momentum)\n             )\n         else:\n             moving_mean = ops.cast(self.moving_mean, inputs.dtype)\n\n@@ -4,6 +4,7 @@ from absl.testing import parameterized\n \n from keras import backend\n from keras import layers\n+from keras import ops\n from keras import testing\n from keras.losses import MeanSquaredError\n from keras.models import Model\n@@ -211,3 +212,12 @@ class BatchNormalizationTest(testing.TestCase, parameterized.TestCase):\n \n         self.assertAllClose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-3)\n         self.assertAllClose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-3)\n+\n+    def test_large_value_within_autocast_scope(self):\n+        layer = layers.BatchNormalization()\n+        layer.build((1, 4, 4, 3))\n+        # Use 70000 to trigger overflow for float16\n+        large_value = ops.full(layer.moving_variance.shape, 70000)\n+        with backend.AutocastScope(\"float16\"):\n+            layer.moving_variance.assign(large_value)\n+            self.assertAllClose(layer.moving_variance.value, large_value)\n\n@@ -158,6 +158,7 @@ class LayerNormalization(Layer):\n                 regularizer=self.gamma_regularizer,\n                 constraint=self.gamma_constraint,\n                 trainable=True,\n+                autocast=False,\n             )\n         else:\n             self.gamma = None\n@@ -170,6 +171,7 @@ class LayerNormalization(Layer):\n                 regularizer=self.beta_regularizer,\n                 constraint=self.beta_constraint,\n                 trainable=True,\n+                autocast=False,\n             )\n         else:\n             self.beta = None\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras import backend\n from keras import layers\n from keras import ops\n from keras import regularizers\n@@ -126,3 +127,12 @@ class LayerNormalizationTest(testing.TestCase):\n         inputs = np.arange(5).astype(\"float32\")[None, :]\n         out = layer(inputs)\n         self.assertAllClose(out, [[0.0, 0.70693, 1.41386, 2.12079, 2.82772]])\n+\n+    def test_large_value_within_autocast_scope(self):\n+        layer = layers.LayerNormalization()\n+        layer.build((1, 4, 4, 3))\n+        # Use 70000 to trigger overflow for float16\n+        large_value = ops.full(layer.gamma.shape, 70000)\n+        with backend.AutocastScope(\"float16\"):\n+            layer.gamma.assign(large_value)\n+            self.assertAllClose(layer.gamma.value, large_value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#431893b228f17c6d310fdbeececd8e8e92f16bc7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 450 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -57,7 +57,7 @@ class JAXTrainer(base_trainer.Trainer):\n             # Note that this is needed for the regularization loss, which need\n             # the latest value of train/non-trainable variables.\n             loss = self.compute_loss(\n-                x, y, y_pred, sample_weight, allow_empty=True\n+                x, y, y_pred, sample_weight=sample_weight, allow_empty=True\n             )\n         if losses:\n             loss += ops.sum(losses)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1b07f6012d80afac45836faf918887b958c2de5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 353 | Contributors (this commit): 8 | Commits (past 90d): 16 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -471,8 +471,8 @@ class Layer(BackendLayer, Operation):\n             trainable: Boolean, whether the variable should be trainable via\n                 backprop or whether its updates are managed manually. Defaults\n                 to `True`.\n-            autocast: Boolean, whether to autocast the variable when accessing\n-                it. Defaults to `True`.\n+            autocast: Boolean, whether to autocast layers variables when\n+                accessing them. Defaults to `True`.\n             regularizer: Regularizer object to call to apply penalty on the\n                 weight. These penalties are summed into the loss function\n                 during optimization. Defaults to `None`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21341c60853b32648699845902fa254acf1165d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 37 | Files Changed: 18 | Hunks: 25 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 18613 | Contributors (this commit): 55 | Commits (past 90d): 48 | Contributors (cumulative): 153 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -29,7 +29,7 @@ class RemoteMonitor(Callback):\n         path: String; path relative to `root` to which the events will be sent.\n         field: String; JSON field under which the data will be stored.\n             The field is used only if the payload is sent within a form\n-            (i.e. send_as_json is set to False).\n+            (i.e. when `send_as_json=False`).\n         headers: Dictionary; optional custom HTTP headers.\n         send_as_json: Boolean; whether the request should be\n             sent as `\"application/json\"`.\n\n@@ -48,8 +48,8 @@ class Embedding(Layer):\n             may take variable length input. If this is `True`,\n             then all subsequent layers in the model need\n             to support masking or an exception will be raised.\n-            If mask_zero is set to True, as a consequence,\n-            index 0 cannot be used in the vocabulary (input_dim should\n+            If `mask_zero` is set to `True`, as a consequence,\n+            index 0 cannot be used in the vocabulary (`input_dim` should\n             equal size of vocabulary + 1).\n         lora_rank: Optional integer. If set, the layer's forward pass\n             will implement LoRA (Low-Rank Adaptation)\n\n@@ -366,7 +366,7 @@ def dot(inputs, axes=-1, **kwargs):\n             axis or axes along which to take the dot product.\n         normalize: Whether to L2-normalize samples along the\n             dot product axis before taking the dot product.\n-            If set to True, then the output of the dot product\n+            If set to `True`, then the output of the dot product\n             is the cosine proximity between the two samples.\n         **kwargs: Standard layer keyword arguments.\n \n\n@@ -102,12 +102,11 @@ class Discretization(TFDataLayer):\n \n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n-                \"`sparse` can only be set to True with the \"\n-                \"TensorFlow backend.\"\n+                \"`sparse=True` can only be used with the \" \"TensorFlow backend.\"\n             )\n         if sparse and output_mode == \"int\":\n             raise ValueError(\n-                \"`sparse` may only be true if `output_mode` is \"\n+                \"`sparse=True` may only be used if `output_mode` is \"\n                 \"`'one_hot'`, `'multi_hot'`, or `'count'`. \"\n                 f\"Received: sparse={sparse} and \"\n                 f\"output_mode={output_mode}\"\n\n@@ -89,8 +89,7 @@ class HashedCrossing(Layer):\n         super().__init__(name=name, dtype=dtype)\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n-                \"`sparse` can only be set to True with the \"\n-                \"TensorFlow backend.\"\n+                \"`sparse=True` can only be used with the \" \"TensorFlow backend.\"\n             )\n \n         argument_validation.validate_string_arg(\n\n@@ -318,22 +318,21 @@ class IntegerLookup(IndexLookup):\n         if max_tokens is not None and max_tokens <= 1:\n             raise ValueError(\n                 \"If `max_tokens` is set for `IntegerLookup`, it must be \"\n-                f\"greater than 1. Received: max_tokens={max_tokens}.\"\n+                f\"greater than 1. Received: max_tokens={max_tokens}\"\n             )\n         if num_oov_indices < 0:\n             raise ValueError(\n                 \"The value of `num_oov_indices` argument for `IntegerLookup` \"\n-                \"must >= 0. Received num_oov_indices=\"\n-                f\"{num_oov_indices}.\"\n+                \"must >= 0. Received: num_oov_indices=\"\n+                f\"{num_oov_indices}\"\n             )\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n-                \"`sparse` can only be set to True with the \"\n-                \"TensorFlow backend.\"\n+                \"`sparse=True` can only be used with the \" \"TensorFlow backend.\"\n             )\n         if vocabulary_dtype != \"int64\":\n             raise ValueError(\n-                \"Only vocabulary_dtype='int64' is supported \"\n+                \"Only `vocabulary_dtype='int64'` is supported \"\n                 \"at this time. Received: \"\n                 f\"vocabulary_dtype={vocabulary_dtype}\"\n             )\n\n@@ -15,7 +15,7 @@ class RandomRotation(TFDataLayer):\n \n     By default, random rotations are only applied during training.\n     At inference time, the layer does nothing. If you need to apply random\n-    rotations at inference time, set `training` to True when calling the layer.\n+    rotations at inference time, pass `training=True` when calling the layer.\n \n     Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n     of integer or floating point dtype.\n\n@@ -314,8 +314,7 @@ class StringLookup(IndexLookup):\n             )\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n-                \"`sparse` can only be set to True with the \"\n-                \"TensorFlow backend.\"\n+                \"`sparse=True` can only be used with the \" \"TensorFlow backend.\"\n             )\n         super().__init__(\n             max_tokens=max_tokens,\n\n@@ -226,13 +226,11 @@ class TextVectorization(Layer):\n             )\n         if sparse and backend.backend() != \"tensorflow\":\n             raise ValueError(\n-                \"`sparse` can only be set to True with the \"\n-                \"TensorFlow backend.\"\n+                \"`sparse=True` can only be used with the \" \"TensorFlow backend.\"\n             )\n         if ragged and backend.backend() != \"tensorflow\":\n             raise ValueError(\n-                \"`ragged` can only be set to True with the \"\n-                \"TensorFlow backend.\"\n+                \"`ragged=True` can only be used with the \" \"TensorFlow backend.\"\n             )\n \n         # 'standardize' must be one of\n\n@@ -1187,14 +1187,14 @@ class ImageDataGenerator:\n         if \"sort\" in kwargs:\n             warnings.warn(\n                 \"sort is deprecated, batches will be created in the\"\n-                \"same order than the filenames provided if shuffle\"\n-                \"is set to False.\",\n+                \"same order than the filenames provided if `shuffle`\"\n+                \"is set to `False`.\",\n                 DeprecationWarning,\n             )\n         if class_mode == \"other\":\n             warnings.warn(\n-                '`class_mode` \"other\" is deprecated, please use '\n-                '`class_mode` \"raw\".',\n+                '`class_mode=\"other\"` is deprecated, please use '\n+                '`class_mode=\"raw\"`.',\n                 DeprecationWarning,\n             )\n             class_mode = \"raw\"\n@@ -1466,7 +1466,8 @@ class ImageDataGenerator:\n         data-dependent transformations, based on an array of sample data.\n \n         Only required if `featurewise_center` or\n-        `featurewise_std_normalization` or `zca_whitening` are set to True.\n+        `featurewise_std_normalization` or `zca_whitening`\n+        are set to `True`.\n \n         When `rescale` is set to a value, rescaling is applied to\n         sample data before computing the internal data stats.\n\n@@ -91,7 +91,7 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):\n         as part of the saved model, the model is already\n         compiled. Otherwise, the model is uncompiled and\n         a warning will be displayed. When `compile` is set\n-        to False, the compilation is omitted without any\n+        to `False`, the compilation is omitted without any\n         warning.\n \n     Raises:\n\n@@ -1136,7 +1136,7 @@ class AUC(Metric):\n             should be flattened into a single label before AUC computation. In\n             the latter case, when multilabel data is passed to AUC, each\n             label-prediction pair is treated as an individual data point. Should\n-            be set to False for multi-class data.\n+            be set to `False` for multi-class data.\n         num_labels: (Optional) The number of labels, used when `multi_label` is\n             True. If `num_labels` is not specified, then state variables get\n             created on the first call to `update_state`.\n\n@@ -29,7 +29,7 @@ class Adafactor(optimizer.Optimizer):\n         clip_threshold: float, defaults to 1.0. Clipping threshold. This is a\n             part of Adafactor algorithm, independent from `clipnorm`,\n             `clipvalue`, and `global_clipnorm`.\n-        relative_step: bool, defaults to True. If `learning_rate` is a\n+        relative_step: bool, defaults to `True`. If `learning_rate` is a\n             constant and `relative_step=True`, learning rate will be adjusted\n             based on current iterations. This is a default learning rate decay\n             in Adafactor.\n\n@@ -820,7 +820,8 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n             no higher than this value.\n         global_clipnorm: Float. If set, the gradient of all weights is clipped\n             so that their global norm is no higher than this value.\n-        use_ema: Boolean, defaults to False. If True, exponential moving average\n+        use_ema: Boolean, defaults to `False`.\n+            If `True`, exponential moving average\n             (EMA) is applied. EMA consists of computing an exponential moving\n             average of the weights of the model (as the weight values change\n             after each training batch), and periodically overwriting the\n\n@@ -126,7 +126,7 @@ def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n         safe_mode: Boolean, whether to disallow unsafe `lambda` deserialization.\n             When `safe_mode=False`, loading an object has the potential to\n             trigger arbitrary code execution. This argument is only\n-            applicable to the Keras v3 model format. Defaults to True.\n+            applicable to the Keras v3 model format. Defaults to `True`.\n \n     Returns:\n         A Keras model instance. If the original model was compiled,\n\n@@ -6,8 +6,8 @@ def standardize_tuple(value, n, name, allow_zero=False):\n         n: int. The size of the tuple to be returned.\n         name: string. The name of the argument being validated, e.g. \"strides\"\n             or \"kernel_size\". This is only used to format error messages.\n-        allow_zero: bool, defaults to False. A ValueError will raised if zero is\n-            received and this param is False.\n+        allow_zero: bool, defaults to `False`. A `ValueError` will raised\n+            if zero is received and this argument is `False`.\n \n     Returns:\n         A tuple of n integers.\n\n@@ -519,7 +519,7 @@ def index_directory(\n             to control the order of the classes\n             (otherwise alphanumerical order is used).\n         shuffle: Whether to shuffle the data. Defaults to `True`.\n-            If set to False, sorts the data in alphanumeric order.\n+            If set to `False`, sorts the data in alphanumeric order.\n         seed: Optional random seed for shuffling.\n         follow_links: Whether to visits subdirectories pointed to by symlinks.\n         verbose: Whether the function prints number of files found and classes.\n\n@@ -407,8 +407,8 @@ def load_image(\n \n     if pad_to_aspect_ratio and crop_to_aspect_ratio:\n         raise ValueError(\n-            \"Only one of pad_to_aspect_ratio, crop_to_aspect_ratio\"\n-            \" can be set to True\"\n+            \"Only one of `pad_to_aspect_ratio`, `crop_to_aspect_ratio`\"\n+            \" can be set to `True`.\"\n         )\n \n     if crop_to_aspect_ratio:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ad58d660f0d1c28e1a5d6097e15698eab683573", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 39 | Churn Cumulative: 4298 | Contributors (this commit): 14 | Commits (past 90d): 7 | Contributors (cumulative): 25 | DMM Complexity: 0.5333333333333333\n\nDIFF:\n@@ -160,9 +160,20 @@ class CategoryEncoding(TFDataLayer):\n             return self._count(inputs, count_weights=count_weights)\n \n     def compute_output_shape(self, input_shape):\n+        # Treat rank-0 inputs inconsistent with actual output .\n+        if (input_shape is not None) & (len(input_shape) == 0):\n+            # If output_mode == \"multi_hot\" return same shape .\n+            if self.output_mode == \"multi_hot\":\n+                return input_shape\n+            # If output_mode == \"one_hot\" append num_tokens to shape .\n+            # This is inline with actual output .\n+            elif self.output_mode == \"one_hot\":\n+                return (self.num_tokens,)\n         if self.output_mode == \"one_hot\":\n             if input_shape[-1] != 1:\n                 return tuple(input_shape + (self.num_tokens,))\n+            elif len(input_shape) == 1:\n+                return tuple(input_shape + (self.num_tokens,))\n             else:\n                 return tuple(input_shape[:-1] + (self.num_tokens,))\n         return tuple(input_shape[:-1] + (self.num_tokens,))\n\n@@ -145,6 +145,16 @@ class CategoryEncodingTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(\"float32\", output.dtype)\n         self.assertSparse(output, sparse)\n \n+        # Test compute_output_shape\n+        input_data = np.array((4))\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"multi_hot\", sparse=sparse\n+        )\n+        self.assertEqual(\n+            layer(input_data).shape,\n+            layer.compute_output_shape(input_data.shape),\n+        )\n+\n     @parameterized.named_parameters(TEST_CASES)\n     def test_one_hot(self, sparse):\n         input_data = np.array([3, 2, 0, 1])\n@@ -176,6 +186,24 @@ class CategoryEncodingTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(\"float32\", output.dtype)\n         self.assertSparse(output, sparse)\n \n+        # Test compute_output_shape\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"one_hot\", sparse=sparse\n+        )\n+        self.assertEqual(\n+            layer(input_data).shape,\n+            layer.compute_output_shape(input_data.shape),\n+        )\n+\n+        input_data = np.array((4,))\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"one_hot\", sparse=sparse\n+        )\n+        self.assertEqual(\n+            layer(input_data).shape,\n+            layer.compute_output_shape(input_data.shape),\n+        )\n+\n     @parameterized.named_parameters(TEST_CASES)\n     def test_batched_one_hot(self, sparse):\n         input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a994dfeb922a525ea58f5081034b10cceb17bd97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 559 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -507,12 +507,12 @@ def map_coordinates(\n class PadImages(Operation):\n     def __init__(\n         self,\n-        top_padding,\n-        bottom_padding,\n-        left_padding,\n-        right_padding,\n-        target_height,\n-        target_width,\n+        top_padding=None,\n+        bottom_padding=None,\n+        left_padding=None,\n+        right_padding=None,\n+        target_height=None,\n+        target_width=None,\n     ):\n         super().__init__()\n         self.top_padding = top_padding\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c24b98a3528c5346370b6c7158674476ebf1c19e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ class DropoutRNNCell:\n         if self._recurrent_dropout_mask is None and self.recurrent_dropout > 0:\n             ones = ops.ones_like(step_input)\n             self._recurrent_dropout_mask = backend.random.dropout(\n-                ones, rate=self.dropout, seed=self.seed_generator\n+                ones, rate=self.recurrent_dropout, seed=self.seed_generator\n             )\n         return self._recurrent_dropout_mask\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4adb561da53d97fc994d9fdc719e7b953ad303ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 98 | Churn Cumulative: 1861 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 0.5434782608695652\n\nDIFF:\n@@ -1,9 +1,12 @@\n \"\"\"Library for exporting inference-only Keras models/layers.\"\"\"\n \n+import inspect\n+\n from absl import logging\n \n from keras import backend\n from keras.api_export import keras_export\n+from keras.backend.common.stateless_scope import StatelessScope\n from keras.layers import Layer\n from keras.models import Functional\n from keras.models import Sequential\n@@ -89,6 +92,11 @@ class ExportArchive:\n         self._tf_trackable.trainable_variables = []\n         self._tf_trackable.non_trainable_variables = []\n \n+        if backend.backend() == \"jax\":\n+            self._backend_variables = []\n+            self._backend_trainable_variables = []\n+            self._backend_non_trainable_variables = []\n+\n         if backend.backend() not in (\"tensorflow\", \"jax\"):\n             raise NotImplementedError(\n                 \"The export API is only compatible with JAX and TF backends.\"\n@@ -144,16 +152,24 @@ class ExportArchive:\n             # Variables in the lists below are actually part of the trackables\n             # that get saved, because the lists are created in __init__.\n             if backend.backend() == \"jax\":\n-                self._tf_trackable.trainable_variables += tree.flatten(\n-                    tree.map_structure(\n-                        tf.Variable, resource.trainable_variables\n-                    )\n-                )\n-                self._tf_trackable.non_trainable_variables += tree.flatten(\n-                    tree.map_structure(\n-                        tf.Variable, resource.non_trainable_variables\n+\n+                trainable_variables = tree.flatten(resource.trainable_variables)\n+                non_trainable_variables = tree.flatten(\n+                    resource.non_trainable_variables\n                 )\n+                self._backend_trainable_variables += trainable_variables\n+                self._backend_non_trainable_variables += non_trainable_variables\n+                self._backend_variables = (\n+                    self._backend_trainable_variables\n+                    + self._backend_non_trainable_variables\n                 )\n+\n+                self._tf_trackable.trainable_variables += [\n+                    tf.Variable(v) for v in trainable_variables\n+                ]\n+                self._tf_trackable.non_trainable_variables += [\n+                    tf.Variable(v) for v in non_trainable_variables\n+                ]\n                 self._tf_trackable.variables = (\n                     self._tf_trackable.trainable_variables\n                     + self._tf_trackable.non_trainable_variables\n@@ -281,9 +297,45 @@ class ExportArchive:\n             if backend.backend() == \"tensorflow\":\n                 decorated_fn = tf.function(fn, input_signature=input_signature)\n             else:  # JAX backend\n-                fn = self._convert_jax2tf_function(fn, input_signature)\n+\n+                # 1. Create a stateless wrapper for `fn`\n+                # 2. jax2tf the stateless wrapper\n+                # 3. Create a stateful function that binds the variables with\n+                #    the jax2tf converted stateless wrapper\n+                # 4. Make the signature of the stateful function the same as the\n+                #    original function\n+                # 5. Wrap in a `tf.function`\n+                def stateless_fn(variables, *args, **kwargs):\n+                    state_mapping = zip(self._backend_variables, variables)\n+                    with StatelessScope(state_mapping=state_mapping):\n+                        return fn(*args, **kwargs)\n+\n+                variables_spec = [\n+                    _make_tensor_spec(v) for v in self._backend_variables\n+                ]\n+\n+                jax2tf_stateless_fn = self._convert_jax2tf_function(\n+                    stateless_fn, [variables_spec] + input_signature\n+                )\n+\n+                def stateful_fn(*args, **kwargs):\n+                    return jax2tf_stateless_fn(\n+                        self._tf_trackable.variables, *args, **kwargs\n+                    )\n+\n+                # Note: we truncate the number of parameters to what is\n+                # specified by `input_signature`.\n+                fn_signature = inspect.signature(fn)\n+                fn_parameters = list(fn_signature.parameters.values())\n+                stateful_fn.__signature__ = inspect.Signature(\n+                    parameters=fn_parameters[0 : len(input_signature)],\n+                    return_annotation=fn_signature.return_annotation,\n+                )\n+\n                 decorated_fn = tf.function(\n-                    fn, input_signature=input_signature, autograph=False\n+                    stateful_fn,\n+                    input_signature=input_signature,\n+                    autograph=False,\n                 )\n             self._endpoint_signatures[name] = input_signature\n         else:\n\n@@ -9,12 +9,22 @@ import tensorflow as tf\n from keras import backend\n from keras import layers\n from keras import models\n+from keras import random\n from keras import testing\n from keras import utils\n from keras.export import export_lib\n from keras.saving import saving_lib\n \n \n+class RandomLayer(layers.Layer):\n+    def __init__(self, **kwargs):\n+        super().__init__(**kwargs)\n+        self.seed_generator = backend.random.SeedGenerator()\n+\n+    def call(self, inputs):\n+        return inputs + random.uniform(inputs.shape, seed=self.seed_generator)\n+\n+\n def get_model():\n     layer_list = [\n         layers.Dense(10, activation=\"relu\"),\n@@ -42,6 +52,16 @@ class ExportArchiveTest(testing.TestCase):\n             ref_output, revived_model.serve(ref_input), atol=1e-6\n         )\n \n+    def test_model_with_rng_export(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = models.Sequential([RandomLayer()])\n+        ref_input = tf.random.normal((3, 10))\n+        ref_output = model(ref_input)\n+\n+        export_lib.export_model(model, temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertEqual(ref_output.shape, revived_model.serve(ref_input).shape)\n+\n     def test_low_level_model_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n \n@@ -452,12 +472,6 @@ class ExportArchiveTest(testing.TestCase):\n             atol=1e-6,\n         )\n \n-    # TODO(nkovela): Remove test when argument name preservation\n-    # workaround is created for JAX backend.\n-    @pytest.mark.skipif(\n-        backend.backend() != \"tensorflow\",\n-        reason=\"JAX2TF has issues with argument name preservation.\",\n-    )\n     def test_non_standard_layer_signature_with_kwargs(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a063684e1128944461809d923758f9a684e721de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 1426 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -488,7 +488,7 @@ class FlaxLayer(JaxLayer):\n     ```python\n     class MyFlaxModule(flax.linen.Module):\n         @flax.linen.compact\n-        def forward(self, input1, input1, deterministic):\n+        def forward(self, input1, input2, deterministic):\n             ...\n             return outputs\n \n@@ -497,7 +497,7 @@ class FlaxLayer(JaxLayer):\n         return module.forward(input1, input2, not training)\n \n     flax_module = MyFlaxModule()\n-    keras_layer = FlaxLayer(flax_module)\n+    keras_layer = FlaxLayer(\n         module=flax_module,\n         method=my_flax_module_wrapper,\n     )\n\n@@ -321,24 +321,7 @@ class TestJaxLayer(testing.TestCase, parameterized.TestCase):\n         verify_identical_model(model3)\n \n         # export, load back and compare results\n-        # TODO: fix and reenable this.\n         path = os.path.join(self.get_temp_dir(), \"jax_layer_export\")\n-        # export_archive = export_lib.ExportArchive()\n-        # export_archive.track(model2)\n-        # export_archive.add_endpoint(\n-        #     \"call\",\n-        #     model2.call,\n-        #     input_signature=[\n-        #         tf.TensorSpec(\n-        #             shape=(None,) + input_shape,\n-        #             dtype=tf.float32,\n-        #         )\n-        #     ],\n-        # )\n-        # export_archive.write_out(\n-        #     path,\n-        #     tf.saved_model.SaveOptions(experimental_custom_gradients=False),\n-        # )\n         export_lib.export_model(model2, path)\n         model4 = tf.saved_model.load(path)\n         output4 = model4.serve(x_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97b082dfee2552fcad1a7c7ea0fac9c72943360c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 114 | Lines Deleted: 65 | Files Changed: 9 | Hunks: 37 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 179 | Churn Cumulative: 12026 | Contributors (this commit): 24 | Commits (past 90d): 64 | Contributors (cumulative): 63 | DMM Complexity: 0.6\n\nDIFF:\n@@ -1,17 +1,11 @@\n import functools\n \n-from keras import backend\n from keras.api_export import keras_export\n-from keras.backend.common.variables import ALLOWED_DTYPES\n+from keras.backend import config\n from keras.backend.common.variables import standardize_dtype\n \n-\"\"\"\n-We adapted the type promotion lattice from JAX. Ref:\n-https://github.com/google/jax/blob/main/jax/_src/dtypes.py\n-\"\"\"\n-\n-BOOL_TYPES = [\"bool\"]\n-INT_TYPES = [\n+BOOL_TYPES = (\"bool\",)\n+INT_TYPES = (\n     \"uint8\",\n     \"uint16\",\n     \"uint32\",\n@@ -20,9 +14,44 @@ INT_TYPES = [\n     \"int16\",\n     \"int32\",\n     \"int64\",\n-]\n-FLOAT_TYPES = [\"bfloat16\", \"float16\", \"float32\", \"float64\"]\n-WEAK_TYPES = [\"int\", \"float\"]\n+)\n+FLOAT_TYPES = (\"bfloat16\", \"float16\", \"float32\", \"float64\")\n+WEAK_TYPES = (\"int\", \"float\")\n+# We need to separate float8 from float because there are no implicit\n+# conversions from float8 dtypes to other dtypes.\n+# Ref: https://github.com/google/jax/issues/16705\n+FLOAT8_TYPES = (\"float8_e4m3fn\", \"float8_e5m2\")\n+\n+# All supported dtypes in Keras\n+ALLOWED_DTYPES = (\n+    \"float16\",\n+    \"float32\",\n+    \"float64\",\n+    \"uint8\",\n+    \"uint16\",\n+    \"uint32\",\n+    \"uint64\",\n+    \"int8\",\n+    \"int16\",\n+    \"int32\",\n+    \"int64\",\n+    \"bfloat16\",\n+    \"bool\",\n+    \"string\",\n+    \"float8_e4m3fn\",\n+    \"float8_e5m2\",\n+)\n+PYTHON_DTYPES_MAP = {\n+    bool: \"bool\",\n+    int: \"int64\" if config.backend() == \"tensorflow\" else \"int32\",\n+    float: \"float32\",\n+    str: \"string\",\n+    # special case for string value\n+    \"int\": \"int64\" if config.backend() == \"tensorflow\" else \"int32\",\n+}\n+\n+# We adapted the type promotion lattice from JAX. Ref:\n+# https://github.com/google/jax/blob/main/jax/_src/dtypes.py\n \n \n def _type_promotion_lattice():\n@@ -168,7 +197,7 @@ def _respect_weak_type(dtype, weak_type):\n @functools.lru_cache(maxsize=None)\n def _resolve_weak_type(dtype, precision=\"32\"):\n     \"\"\"Resolve weak type by the precision of `backend.floatx()`.\"\"\"\n-    extended_allowed_dtypes = ALLOWED_DTYPES.union(WEAK_TYPES)\n+    extended_allowed_dtypes = set(ALLOWED_DTYPES).union(WEAK_TYPES)\n     if dtype not in extended_allowed_dtypes:\n         raise ValueError(\n             \"Invalid value for argument `dtype`. Expected one of \"\n@@ -234,7 +263,7 @@ def _lattice_result_type(*args):\n         out_weak_type = any(out_dtype is t for t in WEAK_TYPES)\n \n     out_weak_type = (out_dtype != \"bool\") and out_weak_type\n-    precision = backend.floatx()[-2:]\n+    precision = config.floatx()[-2:]\n     if out_weak_type:\n         out_dtype = _resolve_weak_type(out_dtype, precision=precision)\n     return out_dtype\n@@ -270,7 +299,13 @@ def result_type(*dtypes):\n     if len(dtypes) == 0:\n         # If no dtypes provided, default to floatx, this matches\n         # `ops.convert_to_tensor([])`\n-        return backend.floatx()\n-    return _lattice_result_type(\n-        *(backend.floatx() if arg is None else arg for arg in dtypes),\n+        return config.floatx()\n+    for dtype in dtypes:\n+        if dtype in FLOAT8_TYPES:\n+            raise ValueError(\n+                \"There is no implicit conversions from float8 dtypes to others.\"\n+                f\" You must cast it internally. Received: {dtypes}\"\n+            )\n+    return _lattice_result_type(\n+        *(config.floatx() if arg is None else arg for arg in dtypes),\n     )\n\n@@ -5,7 +5,6 @@ from absl.testing import parameterized\n from keras import backend\n from keras import ops\n from keras.backend.common import dtypes\n-from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.testing import test_case\n from keras.testing.test_utils import named_product\n \n@@ -18,14 +17,18 @@ class DtypesTest(test_case.TestCase, parameterized.TestCase):\n \n         # TODO: torch doesn't support uint64.\n         ALL_DTYPES = []\n-        for x in ALLOWED_DTYPES:\n+        for x in dtypes.ALLOWED_DTYPES:\n             if x not in [\"string\", \"uint64\"]:\n                 x = str(to_torch_dtype(x)).split(\".\")[-1]\n                 if x not in ALL_DTYPES:  # skip duplicates created by remapping\n                     ALL_DTYPES.append(x)\n         ALL_DTYPES += [None]\n     else:\n-        ALL_DTYPES = [x for x in ALLOWED_DTYPES if x != \"string\"] + [None]\n+        ALL_DTYPES = [x for x in dtypes.ALLOWED_DTYPES if x != \"string\"] + [\n+            None\n+        ]\n+    # Remove float8 dtypes for the following tests\n+    ALL_DTYPES = [x for x in ALL_DTYPES if x not in dtypes.FLOAT8_TYPES]\n \n     def setUp(self):\n         from jax.experimental import enable_x64\n@@ -217,3 +220,13 @@ class DtypesTest(test_case.TestCase, parameterized.TestCase):\n                 ValueError, \"no available implicit dtype promotion path\"\n             ):\n                 dtypes._least_upper_bound(\"test_dtype1\", \"test_dtype2\")\n+\n+    def test_invalid_float8_dtype(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"There is no implicit conversions from float8 dtypes\"\n+        ):\n+            dtypes.result_type(\"float8_e4m3fn\", \"bfloat16\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"There is no implicit conversions from float8 dtypes\"\n+        ):\n+            dtypes.result_type(\"float8_e5m2\", \"bfloat16\")\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras.api_export import keras_export\n from keras.backend import config\n+from keras.backend.common import dtypes\n from keras.backend.common import global_state\n from keras.backend.common.name_scope import current_path\n from keras.backend.common.stateless_scope import get_stateless_scope\n@@ -397,40 +398,13 @@ def initialize_all_variables():\n     global_state.set_global_attribute(\"uninitialized_variables\", [])\n \n \n-ALLOWED_DTYPES = {\n-    \"float16\",\n-    \"float32\",\n-    \"float64\",\n-    \"uint8\",\n-    \"uint16\",\n-    \"uint32\",\n-    \"uint64\",\n-    \"int8\",\n-    \"int16\",\n-    \"int32\",\n-    \"int64\",\n-    \"bfloat16\",\n-    \"bool\",\n-    \"string\",\n-}\n-\n-PYTHON_DTYPES_MAP = {\n-    bool: \"bool\",\n-    int: \"int64\" if config.backend() == \"tensorflow\" else \"int32\",\n-    float: \"float32\",\n-    str: \"string\",\n-    # special case for string value\n-    \"int\": \"int64\" if config.backend() == \"tensorflow\" else \"int32\",\n-}\n-\n-\n @keras_export(\n     [\"keras.utils.standardize_dtype\", \"keras.backend.standardize_dtype\"]\n )\n def standardize_dtype(dtype):\n     if dtype is None:\n         return config.floatx()\n-    dtype = PYTHON_DTYPES_MAP.get(dtype, dtype)\n+    dtype = dtypes.PYTHON_DTYPES_MAP.get(dtype, dtype)\n     if hasattr(dtype, \"name\"):\n         dtype = dtype.name\n     elif hasattr(dtype, \"__str__\") and (\n@@ -440,7 +414,7 @@ def standardize_dtype(dtype):\n     elif hasattr(dtype, \"__name__\"):\n         dtype = dtype.__name__\n \n-    if dtype not in ALLOWED_DTYPES:\n+    if dtype not in dtypes.ALLOWED_DTYPES:\n         raise ValueError(f\"Invalid dtype: {dtype}\")\n     return dtype\n \n\n@@ -4,7 +4,7 @@ from absl.testing import parameterized\n \n from keras import backend\n from keras import initializers\n-from keras.backend.common.variables import ALLOWED_DTYPES\n+from keras.backend.common import dtypes\n from keras.backend.common.variables import AutocastScope\n from keras.backend.common.variables import KerasVariable\n from keras.backend.common.variables import shape_equal\n@@ -156,7 +156,7 @@ class VariablePropertiesTest(test_case.TestCase, parameterized.TestCase):\n         self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n \n     @parameterized.parameters(\n-        *((dtype for dtype in ALLOWED_DTYPES if dtype != \"string\"))\n+        *((dtype for dtype in dtypes.ALLOWED_DTYPES if dtype != \"string\"))\n     )\n     def test_standardize_dtype(self, dtype):\n         \"\"\"Tests standardize_dtype for all ALLOWED_DTYPES except string.\"\"\"\n\n@@ -42,6 +42,8 @@ TORCH_DTYPES = {\n     \"int64\": torch.int64,\n     \"bfloat16\": torch.bfloat16,\n     \"bool\": torch.bool,\n+    \"float8_e4m3fn\": torch.float8_e4m3fn,\n+    \"float8_e5m2\": torch.float8_e5m2,\n }\n \n \n\n@@ -12,8 +12,8 @@ from keras import models\n from keras import ops\n from keras import optimizers\n from keras import testing\n+from keras.backend.common import dtypes\n from keras.backend.common.keras_tensor import KerasTensor\n-from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.ops import core\n from keras.utils import tree\n \n@@ -465,6 +465,27 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(x.shape, y.shape)\n         self.assertTrue(hasattr(y, \"_keras_history\"))\n \n+    @parameterized.named_parameters(\n+        (\"float8_e4m3fn\", \"float8_e4m3fn\"), (\"float8_e5m2\", \"float8_e5m2\")\n+    )\n+    def test_cast_float8(self, float8_dtype):\n+        # Cast to float8 and cast back\n+        x = ops.ones((2,), dtype=\"float32\")\n+        y = ops.cast(x, float8_dtype)\n+        self.assertIn(float8_dtype, str(y.dtype))\n+        x = ops.cast(y, \"float32\")\n+        self.assertIn(\"float32\", str(x.dtype))\n+\n+        x = ops.KerasTensor((2,), dtype=\"float32\")\n+        y = ops.cast(x, float8_dtype)\n+        self.assertEqual(float8_dtype, y.dtype)\n+        self.assertEqual(x.shape, y.shape)\n+        self.assertTrue(hasattr(y, \"_keras_history\"))\n+        x = ops.cast(y, \"float32\")\n+        self.assertEqual(\"float32\", x.dtype)\n+        self.assertEqual(x.shape, y.shape)\n+        self.assertTrue(hasattr(x, \"_keras_history\"))\n+\n     def test_vectorized_map(self):\n         def fn(x):\n             return x + 1\n@@ -555,7 +576,7 @@ class CoreOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n     # resulting in different behavior between JAX and Keras. Currently, we\n     # are skipping the test for uint64\n     ALL_DTYPES = [\n-        x for x in ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n+        x for x in dtypes.ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n     ] + [None]\n \n     if backend.backend() == \"torch\":\n@@ -563,6 +584,8 @@ class CoreOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n         ALL_DTYPES = [\n             x for x in ALL_DTYPES if x not in [\"uint16\", \"uint32\", \"uint64\"]\n         ]\n+    # Remove float8 dtypes for the following tests\n+    ALL_DTYPES = [x for x in ALL_DTYPES if x not in dtypes.FLOAT8_TYPES]\n \n     @parameterized.parameters(\n         ((), None, backend.floatx()),\n\n@@ -7,8 +7,8 @@ from absl.testing import parameterized\n \n from keras import backend\n from keras import testing\n+from keras.backend.common import dtypes\n from keras.backend.common.keras_tensor import KerasTensor\n-from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.ops import math as kmath\n \n \n@@ -869,10 +869,10 @@ class MathDtypeTest(testing.TestCase, parameterized.TestCase):\n     # resulting in different behavior between JAX and Keras. Currently, we\n     # are skipping the test for uint64\n     ALL_DTYPES = [\n-        x for x in ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n+        x for x in dtypes.ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n     ] + [None]\n-    INT_DTYPES = [x for x in ALLOWED_DTYPES if \"int\" in x and x != \"uint64\"]\n-    FLOAT_DTYPES = [x for x in ALLOWED_DTYPES if \"float\" in x]\n+    INT_DTYPES = [x for x in dtypes.INT_TYPES if x != \"uint64\"]\n+    FLOAT_DTYPES = dtypes.FLOAT_TYPES\n \n     if backend.backend() == \"torch\":\n         # TODO: torch doesn't support uint16, uint32 and uint64\n\n@@ -9,9 +9,9 @@ from keras import losses\n from keras import models\n from keras import ops\n from keras import testing\n+from keras.backend.common import dtypes\n from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n-from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.layers.convolutional.conv_test import np_conv1d\n from keras.layers.convolutional.conv_test import np_conv2d\n from keras.layers.convolutional.conv_test import np_conv3d\n@@ -1949,7 +1949,7 @@ class TestLogitRecovery(testing.TestCase):\n class NNOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n     \"\"\"Test the dtype to verify that the behavior matches JAX.\"\"\"\n \n-    FLOAT_DTYPES = [x for x in ALLOWED_DTYPES if \"float\" in x]\n+    FLOAT_DTYPES = dtypes.FLOAT_TYPES\n \n     def setUp(self):\n         from jax.experimental import enable_x64\n\n@@ -11,9 +11,9 @@ from absl.testing import parameterized\n import keras\n from keras import backend\n from keras import testing\n+from keras.backend.common import dtypes\n from keras.backend.common import standardize_dtype\n from keras.backend.common.keras_tensor import KerasTensor\n-from keras.backend.common.variables import ALLOWED_DTYPES\n from keras.ops import numpy as knp\n from keras.testing.test_utils import named_product\n \n@@ -4612,10 +4612,10 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n     # resulting in different behavior between JAX and Keras. Currently, we\n     # are skipping the test for uint64\n     ALL_DTYPES = [\n-        x for x in ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n+        x for x in dtypes.ALLOWED_DTYPES if x not in [\"string\", \"uint64\"]\n     ] + [None]\n-    INT_DTYPES = [x for x in ALLOWED_DTYPES if \"int\" in x and x != \"uint64\"]\n-    FLOAT_DTYPES = [x for x in ALLOWED_DTYPES if \"float\" in x]\n+    INT_DTYPES = [x for x in dtypes.INT_TYPES if x != \"uint64\"]\n+    FLOAT_DTYPES = dtypes.FLOAT_TYPES\n \n     if backend.backend() == \"torch\":\n         # TODO: torch doesn't support uint16, uint32 and uint64\n@@ -4625,6 +4625,8 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         INT_DTYPES = [\n             x for x in INT_DTYPES if x not in [\"uint16\", \"uint32\", \"uint64\"]\n         ]\n+    # Remove float8 dtypes for the following tests\n+    ALL_DTYPES = [x for x in ALL_DTYPES if x not in dtypes.FLOAT8_TYPES]\n \n     def setUp(self):\n         from jax.experimental import enable_x64\n@@ -6247,7 +6249,7 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n                 [np.array([0, 1], \"float32\"), np.array([10, 20], \"float32\")],\n             ],\n             num=[0, 1, 5],\n-            dtype=FLOAT_DTYPES + [None],\n+            dtype=FLOAT_DTYPES + (None,),\n         )\n     )\n     def test_linspace(self, start_and_stop, num, dtype):\n@@ -6371,7 +6373,7 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n                 [np.array([0, 1], \"float32\"), np.array([10, 20], \"float32\")],\n             ],\n             num=[0, 1, 5],\n-            dtype=FLOAT_DTYPES + [None],\n+            dtype=FLOAT_DTYPES + (None,),\n         )\n     )\n     def test_logspace(self, start_and_stop, num, dtype):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
