{"custom_id": "scrapy#fb43b935c6cb6c7651f6914dba9807ace0d87044", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 74 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -17/0 | Churn Δ: 74 | Churn Cumulative: 148 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,74 +0,0 @@\n-#!/usr/bin/env python\n-\"\"\"\n-Cluster control script\n-\"\"\"\n-\n-from optparse import OptionParser\n-import urllib\n-\n-def main():\n-    parser = OptionParser(usage=\"Usage: scrapy-cluster-ctl.py [domain [domain [...]]] [options]\" )\n-    parser.add_option(\"--disablenode\", dest=\"disable_node\", help=\"Disable given node (by name) so it will no accept more run requests.\")\n-    parser.add_option(\"--enablenode\", dest=\"enable_node\", help=\"Enable given node (by name) so it will accept again run requests\")\n-    parser.add_option(\"--format\", dest=\"format\", help=\"Output format. Default: pprint.\", default=\"pprint\")\n-    parser.add_option(\"--list\", metavar=\"FILE\", dest=\"list\", help=\"Specify a file from where to read domains, one per line.\")\n-    parser.add_option(\"--now\", action=\"store_true\", dest=\"now\", help=\"Schedule domains to run with priority now.\")\n-    parser.add_option(\"--output\", metavar=\"FILE\", dest=\"output\", help=\"Output file. If not given, output to stdout.\")\n-    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8080.\", default=8080)\n-    parser.add_option(\"--remove\", dest=\"remove\", action=\"store_true\", help=\"Remove from schedule domains given as args.\")\n-    parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n-    parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n-    parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n-    parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n-    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\", default=0)\n-    (opts, args) = parser.parse_args()\n-    \n-    output = \"\"\n-    domains = []\n-    urlstring = \"http://%s:%s/cluster_master/ws/\" % (opts.server, opts.port)\n-    post = {\"format\":opts.format, \"verbosity\":opts.verbosity}\n-    \n-    if args:\n-        domains = \",\".join(args)\n-    elif opts.list:\n-        try:\n-            domainlist = []\n-            for d in open(opts.list, \"r\").readlines():\n-                domainlist.append(d.strip())\n-            domains = \",\".join(domainlist)\n-        except IOError:\n-            print \"Can't open file %s\" % opts.list\n-    \n-    if opts.status:\n-        pass\n-    elif opts.schedule and domains:\n-        post[\"schedule\"] = domains\n-        if opts.now:\n-            post[\"priority\"] = \"PRIORITY_NOW\"\n-            post[\"settings\"] = \"UNAVAILABLES_NOTIFY=2\"\n-    elif opts.remove and domains:\n-        post[\"remove\"] = domains\n-    elif opts.stop and domains:\n-        post[\"stop\"] = domains\n-    elif opts.disable_node:\n-        post[\"disable_node\"] = opts.disable_node\n-    elif opts.enable_node:\n-        post[\"enable_node\"] = opts.enable_node\n-    else:\n-        parser.print_help()\n-        return\n-    \n-    f = urllib.urlopen(urlstring, urllib.urlencode(post))\n-    output=f.read()\n-\n-    if not opts.output:\n-        print output\n-    else:\n-        try:\n-            open(opts.output, \"w\").write(output)\n-        except IOError:\n-            open(\"/tmp/decobot-schedule.tmp\", \"w\").write(output)\n-            print \"Could not open file %s for writing. Output dumped to /tmp/decobot-schedule.tmp instead.\" % opts.output\n-\n-if __name__ == '__main__':\n-    main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#abde0e5f5125a152403400fb2809fade2a622a07", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 74 | Churn Cumulative: 222 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,74 @@\n+#!/usr/bin/env python\n+\"\"\"\n+Cluster control script\n+\"\"\"\n+\n+from optparse import OptionParser\n+import urllib\n+\n+def main():\n+    parser = OptionParser(usage=\"Usage: scrapy-cluster-ctl.py [domain [domain [...]]] [options]\" )\n+    parser.add_option(\"--disablenode\", dest=\"disable_node\", help=\"Disable given node (by name) so it will no accept more run requests.\")\n+    parser.add_option(\"--enablenode\", dest=\"enable_node\", help=\"Enable given node (by name) so it will accept again run requests\")\n+    parser.add_option(\"--format\", dest=\"format\", help=\"Output format. Default: pprint.\", default=\"pprint\")\n+    parser.add_option(\"--list\", metavar=\"FILE\", dest=\"list\", help=\"Specify a file from where to read domains, one per line.\")\n+    parser.add_option(\"--now\", action=\"store_true\", dest=\"now\", help=\"Schedule domains to run with priority now.\")\n+    parser.add_option(\"--output\", metavar=\"FILE\", dest=\"output\", help=\"Output file. If not given, output to stdout.\")\n+    parser.add_option(\"--port\", dest=\"port\", type=\"int\", help=\"Cluster master port. Default: 8080.\", default=8080)\n+    parser.add_option(\"--remove\", dest=\"remove\", action=\"store_true\", help=\"Remove from schedule domains given as args.\")\n+    parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n+    parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n+    parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n+    parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n+    parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\", default=0)\n+    (opts, args) = parser.parse_args()\n+    \n+    output = \"\"\n+    domains = []\n+    urlstring = \"http://%s:%s/cluster_master/ws/\" % (opts.server, opts.port)\n+    post = {\"format\":opts.format, \"verbosity\":opts.verbosity}\n+    \n+    if args:\n+        domains = \",\".join(args)\n+    elif opts.list:\n+        try:\n+            domainlist = []\n+            for d in open(opts.list, \"r\").readlines():\n+                domainlist.append(d.strip())\n+            domains = \",\".join(domainlist)\n+        except IOError:\n+            print \"Can't open file %s\" % opts.list\n+    \n+    if opts.status:\n+        pass\n+    elif opts.schedule and domains:\n+        post[\"schedule\"] = domains\n+        if opts.now:\n+            post[\"priority\"] = \"PRIORITY_NOW\"\n+            post[\"settings\"] = \"UNAVAILABLES_NOTIFY=2\"\n+    elif opts.remove and domains:\n+        post[\"remove\"] = domains\n+    elif opts.stop and domains:\n+        post[\"stop\"] = domains\n+    elif opts.disable_node:\n+        post[\"disable_node\"] = opts.disable_node\n+    elif opts.enable_node:\n+        post[\"enable_node\"] = opts.enable_node\n+    else:\n+        parser.print_help()\n+        return\n+    \n+    f = urllib.urlopen(urlstring, urllib.urlencode(post))\n+    output=f.read()\n+\n+    if not opts.output:\n+        print output\n+    else:\n+        try:\n+            open(opts.output, \"w\").write(output)\n+        except IOError:\n+            open(\"/tmp/decobot-schedule.tmp\", \"w\").write(output)\n+            print \"Could not open file %s for writing. Output dumped to /tmp/decobot-schedule.tmp instead.\" % opts.output\n+\n+if __name__ == '__main__':\n+    main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d765b133f01c76b8de31e58ff1f86995409f6fa8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 12 | Churn Cumulative: 222 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -39,7 +39,8 @@ class ScrapyCommand(object):\n         \"\"\"\n         parser.add_option(\"-f\", \"--logfile\", dest=\"logfile\", help=\"logfile to use. if omitted stderr will be used\", metavar=\"FILE\")\n         parser.add_option(\"-o\", \"--loglevel\", dest=\"loglevel\", default=None, help=\"log level\")\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n+        parser.add_option(\"--default-spider\", dest=\"default_spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n+        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"Force to use the given spider\")\n         parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n         parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n         parser.add_option(\"--pidfile\", dest=\"pidfile\", help=\"Write process pid to file FILE\", metavar=\"FILE\")\n@@ -56,10 +57,14 @@ class ScrapyCommand(object):\n         if opts.nolog:\n             settings.overrides['LOG_ENABLED'] = False\n \n-        if opts.spider:\n+        if opts.default_spider:\n             from scrapy.spider import spiders\n             spiders.default_domain = opts.spider\n             \n+        if opts.spider:\n+            from scrapy.spider import spiders\n+            spiders.force_domain = opts.spider\n+\n         if opts.pidfile:\n             pid = os.getpid()\n             open(opts.pidfile, \"w\").write(str(pid))\n\n@@ -20,6 +20,7 @@ class SpiderManager(object):\n         self.loaded = False\n         self.default_domain = None\n         self.spider_modules = settings.getlist('SPIDER_MODULES')\n+        self.force_domain = None\n \n     @property\n     def all(self):\n@@ -36,6 +37,8 @@ class SpiderManager(object):\n \n     def fromurl(self, url, include_disabled=True):\n         self._load_on_demand()\n+        if self.force_domain:\n+            return self._alldict.get(self.force_domain)\n         domain = urlparse.urlparse(url).hostname\n         domain = str(domain).replace('www.', '')\n         if domain:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c1ebfd16a7940a08166cf9369f34c017f6cc896", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 84 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -40,7 +40,7 @@ class ScrapyCommand(object):\n         parser.add_option(\"-f\", \"--logfile\", dest=\"logfile\", help=\"logfile to use. if omitted stderr will be used\", metavar=\"FILE\")\n         parser.add_option(\"-o\", \"--loglevel\", dest=\"loglevel\", default=None, help=\"log level\")\n         parser.add_option(\"--default-spider\", dest=\"default_spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"Force to use the given spider\")\n+        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"Force using the given spider when the arguments are urls\")\n         parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n         parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n         parser.add_option(\"--pidfile\", dest=\"pidfile\", help=\"Write process pid to file FILE\", metavar=\"FILE\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cdb3b0251096e53540526269a2320c27024b273a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 86 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ class ScrapyCommand(object):\n \n         if opts.default_spider:\n             from scrapy.spider import spiders\n-            spiders.default_domain = opts.spider\n+            spiders.default_domain = opts.default_spider\n             \n         if opts.spider:\n             from scrapy.spider import spiders\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b45d87d0fe313458d03cd533dee2d3d03e057a04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 67 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): -11/0 | Churn Δ: 67 | Churn Cumulative: 134 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,67 +0,0 @@\n-\"\"\"\n-Commmands for controlling Scrapy\n-\"\"\"\n-from scrapy.conf import settings\n-\n-class ScrapyCommand(object):\n-    def syntax(self):\n-        \"\"\"\n-        Command syntax (preferably one-line). Do not include command name.\n-        \"\"\"\n-        return \"\"\n-\n-    def short_desc(self):\n-        \"\"\"\n-        A short description of the command\n-        \"\"\"\n-        return \"\"\n-\n-    def long_desc(self):\n-        \"\"\"\n-        A long description of the command. Return short description when not\n-        available. It cannot contain newlines, since contents will be formatted\n-        by optparser which removes newlines and wraps text.\n-        \"\"\"\n-        return self.short_desc()\n-\n-    def help(self):\n-        \"\"\"\n-        An extensive help for the command. It will be shown when using the\n-        \"help\" command. It can contain newlines, since not post-formatting will\n-        be applied to its contents.\n-        \"\"\"\n-        return self.long_desc()\n-\n-    def add_options(self, parser):\n-        \"\"\"\n-        Populate option parse with options available for this command\n-        \"\"\"\n-        parser.add_option(\"-f\", \"--logfile\", dest=\"logfile\", help=\"logfile to use. if omitted stderr will be used\", metavar=\"FILE\")\n-        parser.add_option(\"-o\", \"--loglevel\", dest=\"loglevel\", default=None, help=\"log level\")\n-        parser.add_option(\"--spider\", dest=\"spider\", default=None, help=\"default spider (domain) to use if no spider is found\")\n-        parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n-        parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n-\n-    def process_options(self, args, opts):\n-        if opts.logfile:\n-            settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOGFILE'] = opts.logfile\n-\n-        if opts.loglevel:\n-            settings.overrides['LOG_ENABLED'] = True\n-            settings.overrides['LOGLEVEL'] = opts.loglevel\n-\n-        if opts.nolog:\n-            settings.overrides['LOG_ENABLED'] = False\n-\n-        if opts.spider:\n-            from scrapy.spider import spiders\n-            spiders.default_domain = opts.spider\n-\n-    def run(self, args, opts):\n-        \"\"\"\n-        Entry point for running commands\n-        \"\"\"\n-        raise NotImplementedError\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d2121141a3d2d3f1d0a9ea45d9eb48536a44b28b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 395 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -11,6 +11,7 @@ USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n DOWNLOAD_TIMEOUT = 180      # 3mins\n CONCURRENT_DOMAINS = 8    # number of domains to scrape in parallel\n REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n+CACHE2_EXPIRATION_SECS = 48 * 60 * 60 # seconds while cached response is still valid\n \n LOG_ENABLED = True  #\n LOGLEVEL = 'DEBUG'   # default loglevel\n\n@@ -111,7 +111,12 @@ class Cache(object):\n \n     def is_cached(self, domain, key):\n         requestpath = self.requestpath(domain, key)\n-        return os.path.exists(requestpath)\n+        if os.path.exists(requestpath):\n+            with open(os.path.join(requestpath, 'meta_data')) as f:\n+                metadata = eval(f.read())\n+            return datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS'))\n+        else:\n+            return False\n \n     def retrieve_response(self, domain, key):\n         \"\"\"\n\n@@ -1,6 +1,6 @@\n import urllib\n import warnings\n-from sha import sha\n+from hashlib import sha1\n from copy import copy\n from base64 import urlsafe_b64encode\n \n@@ -149,7 +149,8 @@ class Request(object):\n                     headers = dict([(k, v) for k, v in self.headers.items() if k.lower() not in keys])\n \n         # fingerprint generation\n-        fp = sha(canonicalize(self.url))\n+        fp = sha1()\n+        fp.update(canonicalize(self.url))\n         fp.update(self.method)\n \n         if self.body and self.method in ['POST', 'PUT']:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c345016b8a7430b41590cd8f9ff4da561d429af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 186 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,7 +114,11 @@ class Cache(object):\n         if os.path.exists(requestpath):\n             with open(os.path.join(requestpath, 'meta_data')) as f:\n                 metadata = eval(f.read())\n-            return datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS'))\n+            if datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n+                return True\n+            else:\n+                log.msg('dropping old cached response from %s' % metadata['timestamp'], log.INFO)\n+                return False\n         else:\n             return False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce905ce78f335d3af5e9508d93c1178fa16e7330", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 583 | Contributors (this commit): 2 | Commits (past 90d): 18 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -48,6 +48,7 @@ class Node:\n             else:\n                 status[\"running\"] = self.running\n             status[\"maxproc\"] = self.maxproc\n+            status[\"freeslots\"] = self.maxproc - len(self.running)\n             status[\"available\"] = self.available\n             status[\"starttime\"] = self.starttime\n             status[\"timestamp\"] = self.timestamp\n@@ -60,6 +61,9 @@ class Node:\n         else:\n             self.alive = True\n             self.running = status['running']\n+            for proc in self.running:\n+                if proc[\"domain\"] in self.master.loading:\n+                    self.master.loading.remove(proc[\"domain\"])\n             self.maxproc = status['maxproc']\n             self.starttime = status['starttime']\n             self.timestamp = status['timestamp']\n@@ -69,7 +73,7 @@ class Node:\n \n             #load domains by one, so to mix up better the domain loading between nodes. The next one in the same node will be loaded\n             #when there is no loading domain or in the next status update. This way also we load the nodes softly\n-        if self.alive and self.available and free_slots > 0 and self.master.pending:\n+            if self.available and free_slots > 0 and self.master.pending:\n                 pending = self.master.pending.pop(0)\n                 #if domain already running in some node, reschedule with same priority (so will be moved to run later)\n                 if pending['domain'] in self.master.running or pending['domain'] in self.master.loading:\n@@ -105,7 +109,6 @@ class Node:\n             log.msg(\"Domain %s rescheduled: lost connection to node.\" % pending['domain'], log.WARNING)\n             \n         def _run_callback(status):\n-            self.master.loading.remove(pending['domain'])\n             if status['callresponse'][0] == 1:\n                 #slots are complete. Reschedule in master with priority reduced by one.\n                 #self.master.loading check should avoid this to happen\n@@ -220,7 +223,9 @@ class ClusterMaster(pb.Root):\n             pd = self.find_ifpending(domain)\n             if pd: #domain already pending, so just change priority if new is higher\n                 if priority < pd['priority']:\n+                    self.pending.remove(pd)\n                     pd['priority'] = priority\n+                    self.pending.insert(i, pd)\n             else:\n                 final_spider_settings = self.get_spider_groupsettings(domain)\n                 final_spider_settings.update(self.global_settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76a9d2da1061b1d8098bd8a29003ef45e098daac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 6 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 898 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from __future__ import with_statement\n \n import os\n-import sha\n+import hashlib\n import datetime\n import urlparse\n from pydispatch import dispatcher\n@@ -83,7 +83,7 @@ class Cache(object):\n             os.makedirs(self.basesectorpath)\n \n     def domainsectorpath(self, domain):\n-        sector = sha.sha(domain).hexdigest()[0]\n+        sector = hashlib.sha1(domain).hexdigest()[0]\n         return os.path.join(self.basesectorpath, sector, domain)\n \n     def domainlinkpath(self, domain):\n\n@@ -1,4 +1,4 @@\n-import sha\n+import hashlib\n from datetime import datetime\n \n from pydispatch import dispatcher\n@@ -84,4 +84,4 @@ def urlkey(url):\n \n \n def hash(value):\n-    return sha.new(value).hexdigest() if value else None\n+    return hashlib.sha1(value).hexdigest() if value else None\n\n@@ -3,7 +3,7 @@ This module contains some extra base models for scraped items which could be\n useful in some Scrapy implementations\n \"\"\"\n \n-import sha\n+import hashlib\n \n from scrapy.item import ScrapedItem\n from scrapy.core.exceptions import UsageError, DropItem\n@@ -135,6 +135,6 @@ class RobustScrapedItem(ScrapedItem):\n         \"\"\"\n         if self._version:\n             return self._version\n-        hash_ = sha.new()\n+        hash_ = hashlib.sha1()\n         hash_.update(\"\".join([\"\".join([n, str(v)]) for n,v in sorted(self.__dict__.iteritems())]))\n         return hash_.hexdigest()\n\n@@ -1,7 +1,7 @@\n import re\n import urllib\n import simplejson\n-from sha import sha\n+import hashlib\n from twisted.internet import defer\n \n from scrapy.core.engine import scrapyengine\n@@ -22,7 +22,7 @@ CACHESIZE = settings.get('WS_CACHESIZE', 20)\n \n \n def _urlhash(request):\n-    h = sha()\n+    h = hashlib.sha1()\n     for a in sorted(request.ARGS):\n         h.update(request.ARGS[a])\n     return h.hexdigest()\n\n@@ -1,6 +1,6 @@\n import urllib\n import warnings\n-from hashlib import sha1\n+import hashlib\n from copy import copy\n from base64 import urlsafe_b64encode\n \n@@ -149,7 +149,7 @@ class Request(object):\n                     headers = dict([(k, v) for k, v in self.headers.items() if k.lower() not in keys])\n \n         # fingerprint generation\n-        fp = sha1()\n+        fp = hashlib.sha1()\n         fp.update(canonicalize(self.url))\n         fp.update(self.method)\n \n\n@@ -1,5 +1,5 @@\n import re\n-import sha\n+import hashlib\n import copy\n \n from BeautifulSoup import UnicodeDammit\n@@ -41,7 +41,7 @@ class Response(object) :\n     def version(self):\n         \"\"\"A hash of the contents of this response\"\"\"\n         if not hasattr(self, '_version'):\n-            self._version = sha.new(self.body.to_string()).hexdigest()\n+            self._version = hashlib.sha1(self.body.to_string()).hexdigest()\n         return self._version\n \n     def headers_encoding(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#97b2e7df55c11db8afb1eca4edd41408a18710fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 211 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import os\n import hashlib\n import datetime\n import urlparse\n+import cPickle\n from pydispatch import dispatcher\n from twisted.internet import defer\n \n@@ -110,14 +111,14 @@ class Cache(object):\n                     os.makedirs(linkpath)\n \n     def is_cached(self, domain, key):\n-        requestpath = self.requestpath(domain, key)\n+        pickled_meta = os.path.join(self.requestpath(domain, key), 'pickled_meta')\n         if os.path.exists(requestpath):\n-            with open(os.path.join(requestpath, 'meta_data')) as f:\n-                metadata = eval(f.read())\n-            if datetime.datetime.now() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n+            with open(pickled_meta) as f:\n+                metadata = cPickle.load(f)\n+            if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n                 return True\n             else:\n-                log.msg('dropping old cached response from %s' % metadata['timestamp'], log.INFO)\n+                log.msg('dropping old cached response from %s' % metadata['timestamp'])\n                 return False\n         else:\n             return False\n@@ -132,14 +133,13 @@ class Cache(object):\n             return None # not cached\n \n         metadata = responsebody = responseheaders = None\n-        with open(os.path.join(requestpath, 'meta_data')) as f:\n-            metadata = f.read()\n+        with open(os.path.join(requestpath, 'pickled_meta')) as f:\n+            metadata = cPickle.load(f)\n         with open(os.path.join(requestpath, 'response_body')) as f:\n             responsebody = f.read()\n         with open(os.path.join(requestpath, 'response_headers')) as f:\n             responseheaders = f.read()\n \n-        metadata = eval(metadata)\n         url = metadata['url']\n         original_url = metadata.get('original_url', url)\n         headers = Headers(responseheaders)\n@@ -162,12 +162,15 @@ class Cache(object):\n                 'status': response.status,\n                 'domain': response.domain,\n                 'original_url': response.original_url,\n-                'timestamp': datetime.datetime.now(),\n+                'timestamp': datetime.datetime.utcnow(),\n             }\n \n         # metadata\n         with open(os.path.join(requestpath, 'meta_data'), 'w') as f:\n             f.write(repr(metadata))\n+        # pickled metadata (to recover without using eval)\n+        with open(os.path.join(requestpath, 'pickled_meta'), 'wb') as f:\n+            cPickle.dump(metadata, f, -1)\n         # response\n         with open(os.path.join(requestpath, 'response_headers'), 'w') as f:\n             f.write(headers_dict_to_raw(response.headers))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5778ecf15dd4a3c30b1a6b0170aec6187662dce9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 161 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -78,7 +78,7 @@ class XPathSelector(object):\n         self.doc.xpathContext.xpathRegisterNs(prefix, uri)\n \n     def __str__(self):\n-        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, 'name'), self.expr)\n+        return \"<%s (%s) xpath=%s>\" % (type(self).__name__, getattr(self.xmlNode, 'name', type(self.xmlNode).__name__), self.expr)\n \n     __repr__ = __str__\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9851d8ec91d5f6ef047814e23b93098ee5f4a91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 382 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,7 +112,7 @@ class Cache(object):\n \n     def is_cached(self, domain, key):\n         pickled_meta = os.path.join(self.requestpath(domain, key), 'pickled_meta')\n-        if os.path.exists(requestpath):\n+        if os.path.exists(pickled_meta):\n             with open(pickled_meta) as f:\n                 metadata = cPickle.load(f)\n             if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n@@ -128,10 +128,10 @@ class Cache(object):\n         Return response dictionary if request has correspondent cache record;\n         return None if not.\n         \"\"\"\n-        requestpath = self.requestpath(domain, key)\n-        if not os.path.exists(requestpath):\n+        if not self.is_cached(domain, key):\n             return None # not cached\n \n+        requestpath = self.requestpath(domain, key)\n         metadata = responsebody = responseheaders = None\n         with open(os.path.join(requestpath, 'pickled_meta')) as f:\n             metadata = cPickle.load(f)\n\n@@ -86,6 +86,10 @@ class XPathSelector(object):\n class XPathSelectorList(list):\n     \"\"\"List of XPathSelector objects\"\"\"\n \n+    def __getitem__(self, i):\n+        return '__getitem__'\n+        #return XPathSelectorList(list.__getslice__(self, i, j))\n+\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7fbaa50d8203a76e94c4215cd94ac28c305ead9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 4 | Churn Cumulative: 169 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,10 +86,6 @@ class XPathSelector(object):\n class XPathSelectorList(list):\n     \"\"\"List of XPathSelector objects\"\"\"\n \n-    def __getitem__(self, i):\n-        return '__getitem__'\n-        #return XPathSelectorList(list.__getslice__(self, i, j))\n-\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30202c54a4a55bfd8077afb03e56f1cae3fcfffb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 221 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -113,7 +113,7 @@ class Cache(object):\n     def is_cached(self, domain, key):\n         pickled_meta = os.path.join(self.requestpath(domain, key), 'pickled_meta')\n         if os.path.exists(pickled_meta):\n-            with open(pickled_meta) as f:\n+            with open(pickled_meta, 'rb') as f:\n                 metadata = cPickle.load(f)\n             if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n                 return True\n@@ -133,7 +133,7 @@ class Cache(object):\n \n         requestpath = self.requestpath(domain, key)\n         metadata = responsebody = responseheaders = None\n-        with open(os.path.join(requestpath, 'pickled_meta')) as f:\n+        with open(os.path.join(requestpath, 'pickled_meta'), 'rb') as f:\n             metadata = cPickle.load(f)\n         with open(os.path.join(requestpath, 'response_body')) as f:\n             responsebody = f.read()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8b5a07a150491e270e490928fc23fb89ad09909", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 403 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import os\n import hashlib\n import datetime\n import urlparse\n-import cPickle\n+import cPickle as pickle\n from pydispatch import dispatcher\n from twisted.internet import defer\n \n@@ -114,7 +114,7 @@ class Cache(object):\n         pickled_meta = os.path.join(self.requestpath(domain, key), 'pickled_meta')\n         if os.path.exists(pickled_meta):\n             with open(pickled_meta, 'rb') as f:\n-                metadata = cPickle.load(f)\n+                metadata = pickle.load(f)\n             if datetime.datetime.utcnow() <= metadata['timestamp'] + datetime.timedelta(seconds=settings.getint('CACHE2_EXPIRATION_SECS')):\n                 return True\n             else:\n@@ -134,7 +134,7 @@ class Cache(object):\n         requestpath = self.requestpath(domain, key)\n         metadata = responsebody = responseheaders = None\n         with open(os.path.join(requestpath, 'pickled_meta'), 'rb') as f:\n-            metadata = cPickle.load(f)\n+            metadata = pickle.load(f)\n         with open(os.path.join(requestpath, 'response_body')) as f:\n             responsebody = f.read()\n         with open(os.path.join(requestpath, 'response_headers')) as f:\n@@ -170,7 +170,7 @@ class Cache(object):\n             f.write(repr(metadata))\n         # pickled metadata (to recover without using eval)\n         with open(os.path.join(requestpath, 'pickled_meta'), 'wb') as f:\n-            cPickle.dump(metadata, f, -1)\n+            pickle.dump(metadata, f, -1)\n         # response\n         with open(os.path.join(requestpath, 'response_headers'), 'w') as f:\n             f.write(headers_dict_to_raw(response.headers))\n\n@@ -86,6 +86,11 @@ class XPathSelector(object):\n class XPathSelectorList(list):\n     \"\"\"List of XPathSelector objects\"\"\"\n \n+    def __getitem__(self, i):\n+        print '__getitem__'\n+        self.__getitem__(i)\n+        #return XPathSelectorList(list.__getslice__(self, i, j))\n+\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85a4514603bc29096f7b7a6449252ae5dcea430f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 179 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,11 +86,6 @@ class XPathSelector(object):\n class XPathSelectorList(list):\n     \"\"\"List of XPathSelector objects\"\"\"\n \n-    def __getitem__(self, i):\n-        print '__getitem__'\n-        self.__getitem__(i)\n-        #return XPathSelectorList(list.__getslice__(self, i, j))\n-\n     def x(self, xpath):\n         \"\"\"Perform the given XPath query on each XPathSelector of the list and\n         return a new (flattened) XPathSelectorList of the results\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f62faea5fa851d859c4c0bd85e61a29d91aa77b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 8 | Files Changed: 5 | Hunks: 20 | Methods Changed: 16 | Complexity Δ (Sum/Max): 15/6 | Churn Δ: 62 | Churn Cumulative: 1578 | Contributors (this commit): 2 | Commits (past 90d): 52 | Contributors (cumulative): 9 | DMM Complexity: 0.47368421052631576\n\nDIFF:\n@@ -1,4 +1,4 @@\n-import sys\n+import sys, datetime\n import pickle\n \n from pydispatch import dispatcher\n@@ -34,6 +34,7 @@ class Node:\n         self.name = name\n         self.master = master\n         self.available = True\n+        self.statistics = {\"domains\": {}, \"scraped_total\": 0 }\n \n     def status_as_dict(self, verbosity=0):\n         status = {\"alive\": self.alive}\n@@ -82,7 +83,7 @@ class Node:\n                     self.run(pending)\n                     self.master.loading.append(pending['domain'])\n \n-    def get_status(self):\n+    def update_status(self):\n         try:\n             deferred = self.__remote.callRemote(\"status\")\n         except pb.DeadReferenceError:\n@@ -91,6 +92,19 @@ class Node:\n         else:\n             deferred.addCallbacks(callback=self._set_status, errback=lambda reason: log.msg(reason, log.ERROR))\n \n+    def update_statistics(self):\n+        \n+        def _set_statistics(statistics):\n+            self.statistics = statistics\n+\n+        try:\n+            deferred = self.__remote.callRemote(\"statistics\")\n+        except pb.DeadReferenceError:\n+            self._set_status(None)\n+            log.msg(\"Lost connection to node %s.\" % (self.name), log.ERROR)\n+        else:\n+            deferred.addCallbacks(callback=_set_statistics, errback=lambda reason: log.msg(reason, log.ERROR))\n+        \n     def stop(self, domain):\n         try:\n             deferred = self.__remote.callRemote(\"stop\", domain)\n@@ -128,6 +142,7 @@ class Node:\n         else:\n             deferred.addCallbacks(callback=_run_callback, errback=_run_errback)\n         \n+\n class ClusterMaster(pb.Root):\n \n     def __init__(self):\n@@ -147,7 +162,7 @@ class ClusterMaster(pb.Root):\n             self.pending = []\n         self.loading = []\n         self.nodes = {}\n-        \n+        self.statistics = {\"domains\": {}, \"scraped_total\": 0, \"start_time\": datetime.datetime.utcnow(), \"pending\": self.print_pending() }\n         self.global_settings = {}\n         #load cluster global settings\n         for sname in settings.getlist('GLOBAL_CLUSTER_SETTINGS'):\n@@ -188,10 +203,16 @@ class ClusterMaster(pb.Root):\n             _make_callback(factory, name, url)\n \n     def update_nodes(self):\n+        self.statistics[\"scraped_total\"] = 0\n         for name, url in settings.get('CLUSTER_MASTER_NODES', {}).iteritems():\n             if name in self.nodes and self.nodes[name].alive:\n                 log.msg(\"Updating node. name: %s, url: %s\" % (name, url) )\n-                self.nodes[name].get_status()\n+                self.nodes[name].update_status()\n+                self.nodes[name].update_statistics()\n+                self.statistics[\"scraped_total\"] += self.nodes[name].statistics[\"scraped_total\"]\n+                for domain in self.nodes[name].statistics[\"domains\"]:\n+                    if not domain in self.statistics[\"domains\"] or self.nodes[name].statistics[\"domains\"][domain][\"last_start_time\"] >= self.statistics[\"domains\"][domain][\"last_start_time\"]:\n+                        self.statistics[\"domains\"][domain] = self.nodes[name].statistics[\"domains\"][domain]\n             else:\n                 log.msg(\"Reloading node. name: %s, url: %s\" % (name, url) )\n                 self.load_node(name, url)\n@@ -199,7 +220,7 @@ class ClusterMaster(pb.Root):\n     def add_node(self, cworker, name):\n         \"\"\"Add node given its node\"\"\"\n         node = Node(cworker, None, name, self)\n-        node.get_status()\n+        node.update_status()\n         self.nodes[name] = node\n         log.msg(\"Added cluster worker %s\" % name)\n \n@@ -294,7 +315,6 @@ class ClusterMaster(pb.Root):\n         else:\n             return self.pending\n         \n-        \n     def _engine_started(self):\n         self.load_nodes()\n         scrapyengine.addtask(self.update_nodes, settings.getint('CLUSTER_MASTER_POLL_INTERVAL'))\n\n@@ -103,6 +103,9 @@ class ClusterMasterWeb(ClusterMaster):\n             self.enable_node(args[\"enable_node\"][0])\n             if ws:\n                 return self.ws_status(wc_request)\n+        if \"statistics\" in args:\n+            if ws:\n+                return self.ws_statistics(wc_request)\n             \n         if ws:\n             return self.ws_status(wc_request)\n@@ -232,3 +235,8 @@ class ClusterMasterWeb(ClusterMaster):\n         status[\"loading\"] = self.loading\n         content = serialize(status, format)\n         return content\n+        \n+    def ws_statistics(self, wc_request):\n+        format = wc_request.args['format'][0] if 'format' in wc_request.args else 'json'\n+        content = serialize(self.statistics, format)\n+        return content\n\\ No newline at end of file\n\n@@ -19,6 +19,7 @@ def main():\n     parser.add_option(\"--schedule\", dest=\"schedule\", action=\"store_true\", help=\"Schedule domains given as args.\")\n     parser.add_option(\"--server\", dest=\"server\", help=\"Cluster master server name. Default: localhost.\", default=\"localhost\")\n     parser.add_option(\"--status\", dest=\"status\", action=\"store_true\", help=\"Print cluster master status and quit.\")\n+    parser.add_option(\"--statistics\", dest=\"statistics\", action=\"store_true\", help=\"Print cluster statistics\")\n     parser.add_option(\"--stop\", dest=\"stop\", action=\"store_true\", help=\"Stops a running domain.\")\n     parser.add_option(\"--verbosity\", dest=\"verbosity\", type=\"int\", help=\"Sets the report status verbosity.\", default=0)\n     (opts, args) = parser.parse_args()\n@@ -41,6 +42,8 @@ def main():\n     \n     if opts.status:\n         pass\n+    elif opts.statistics:\n+        post[\"statistics\"] = True\n     elif opts.schedule and domains:\n         post[\"schedule\"] = domains\n         if opts.now:\n\n@@ -35,10 +35,13 @@ class ScrapyProcessProtocol(protocol.ProcessProtocol):\n         log.msg(\"ClusterWorker: started domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         self.transport.closeStdin()\n         self.status = \"running\"\n+        self.procman.statistics[\"domains\"][self.domain] = {\"status\": \"running\", \"last_start_time\": self.start_time}\n \n     def processEnded(self, status_object):\n         log.msg(\"ClusterWorker: finished domain=%s, pid=%d, log=%s\" % (self.domain, self.pid, self.logfile))\n         del self.procman.running[self.domain]\n+        self.procman.statistics[\"domains\"][self.domain].update({\"status\": \"scraped\", \"last_end_time\": datetime.datetime.utcnow()})\n+        self.procman.statistics[\"scraped_total\"] += 1\n \n class ClusterWorker(pb.Root):\n \n@@ -49,7 +52,8 @@ class ClusterWorker(pb.Root):\n         self.maxproc = settings.getint('CLUSTER_WORKER_MAXPROC')\n         self.logdir = settings['CLUSTER_LOGDIR']\n         self.running = {}\n-        self.starttime = time.time()\n+        self.starttime = datetime.datetime.utcnow()\n+        self.statistics = {\"domains\": {}, \"scraped_total\": 0}\n         port = settings.getint('CLUSTER_WORKER_PORT')\n         scrapyengine.listenTCP(port, pb.PBServerFactory(self))\n         log.msg(\"PYTHONPATH: %s\" % repr(sys.path))\n@@ -68,11 +72,20 @@ class ClusterWorker(pb.Root):\n     def remote_status(self):\n         return self.status()\n     \n+    def remote_statistics(self):\n+        #This can detect processes that were abnormally killed (for example, by the kernel\n+        #because of a memory ran out.)\n+        for domain in self.statistics[\"domains\"]:\n+            if self.statistics[\"domains\"][domain][\"status\"] == \"running\" and not domain in self.running:\n+                self.statistics[\"domains\"][domain][\"status\"] = \"lost\"\n+\n+        return self.statistics\n+    \n     def status(self, rcode=0, rstring=None):\n         status = {}\n         status[\"running\"] = [ self.running[k].as_dict() for k in self.running.keys() ]\n         status[\"starttime\"] = self.starttime\n-        status[\"timestamp\"] = time.time()\n+        status[\"timestamp\"] = datetime.datetime.utcnow()\n         status[\"maxproc\"] = self.maxproc\n         status[\"loadavg\"] = os.getloadavg()\n         status[\"logdir\"] = self.logdir\n\n@@ -17,6 +17,8 @@ elif sys.argv[0] == \"-s\":\n     d.addCallback(lambda object: object.callRemote(\"stop\", sys.argv[1]))\n elif sys.argv[0] == \"-r\":\n     d.addCallback(lambda object: object.callRemote(\"run\", sys.argv[1]))\n+elif sys.argv[0] == \"-t\":\n+    d.addCallback(lambda object: object.callRemote(\"statistics\"))\n \n d.addCallbacks(callback = util.println, errback = lambda reason: 'error: '+str(reason.value))\n d.addCallback(lambda _: reactor.stop())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
