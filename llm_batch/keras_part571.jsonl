{"custom_id": "keras#fa2ea78d3f7784ef96905cc87b9fb28ecaceea60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 33 | Files Changed: 8 | Hunks: 38 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 82 | Churn Cumulative: 665 | Contributors (this commit): 3 | Commits (past 90d): 23 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,11 +20,15 @@ class MyDense(layers.Layer):\n         w_shape = (input_dim, self.units)\n         w_value = initializers.GlorotUniform()(w_shape)\n         # State must be stored in backend.Variable objects.\n-        self.w = backend.Variable(w_value, name=\"kernel\")\n+        self.w = backend.Variable(w_value, name=\"kernel\", trainable=True)\n \n-        b_shape = (self.units,)\n-        b_value = initializers.Zeros()(b_shape)\n-        self.b = backend.Variable(b_value, name=\"bias\")\n+        # You can also use add_weight\n+        self.b = self.add_weight(\n+            shape=(self.units,),\n+            initializer=\"zeros\",\n+            name=\"bias\",\n+            trainable=True,\n+        )\n \n     def call(self, inputs):\n         # Use Keras ops to create backend-agnostic layers/metrics/etc.\n@@ -69,7 +73,7 @@ model = MyModel(hidden_dim=256, output_dim=16)\n x = np.random.random((50000, 128))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 10\n+epochs = 5\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n\n@@ -6,7 +6,7 @@ from keras_core import losses\n from keras_core import metrics\n from keras_core import optimizers\n \n-inputs = layers.Input((128,), batch_size=32)\n+inputs = layers.Input((100,), batch_size=32)\n x = layers.Dense(256, activation=\"relu\")(inputs)\n x = layers.Dense(256, activation=\"relu\")(x)\n x = layers.Dense(256, activation=\"relu\")(x)\n@@ -15,10 +15,10 @@ model = Model(inputs, outputs)\n \n model.summary()\n \n-x = np.random.random((50000, 128))\n+x = np.random.random((50000, 100))\n y = np.random.random((50000, 16))\n batch_size = 32\n-epochs = 6\n+epochs = 5\n \n model.compile(\n     optimizer=optimizers.SGD(learning_rate=0.001),\n\n@@ -1,5 +1,5 @@\n-from keras_core import operations as ops\n from keras_core import backend\n+from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.pooling.base_global_pooling import BaseGlobalPooling\n \n@@ -33,6 +33,7 @@ class GlobalAveragePooling1D(BaseGlobalPooling):\n             a given step should be masked (excluded from the average).\n \n     Input shape:\n+\n     - If `data_format='channels_last'`:\n         3D tensor with shape:\n         `(batch_size, steps, features)`\n@@ -41,19 +42,20 @@ class GlobalAveragePooling1D(BaseGlobalPooling):\n         `(batch_size, features, steps)`\n \n     Output shape:\n-    - If `keepdims`=False:\n+\n+    - If `keepdims=False`:\n         2D tensor with shape `(batch_size, features)`.\n-    - If `keepdims`=True:\n+    - If `keepdims=True`:\n         - If `data_format=\"channels_last\"`:\n             3D tensor with shape `(batch_size, 1, features)`\n         - If `data_format=\"channels_first\"`:\n             3D tensor with shape `(batch_size, features, 1)`\n \n-    Examples:\n+    Example:\n \n     >>> x = np.random.rand(2, 3, 4)\n     >>> y = keras_core.layers.GlobalAveragePooling1D()(x)\n-    >>> print(y.shape)\n+    >>> y.shape\n     (2, 4)\n     \"\"\"\n \n\n@@ -28,6 +28,7 @@ class GlobalAveragePooling2D(BaseGlobalPooling):\n             The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n     Input shape:\n+\n     - If `data_format='channels_last'`:\n         4D tensor with shape:\n         `(batch_size, height, width, channels)`\n@@ -36,19 +37,20 @@ class GlobalAveragePooling2D(BaseGlobalPooling):\n         `(batch_size, channels, height, width)`\n \n     Output shape:\n-    - If `keepdims`=False:\n+\n+    - If `keepdims=False`:\n         2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n+    - If `keepdims=True`:\n         - If `data_format=\"channels_last\"`:\n             4D tensor with shape `(batch_size, 1, 1, channels)`\n         - If `data_format=\"channels_first\"`:\n             4D tensor with shape `(batch_size, channels, 1, 1)`\n \n-    Examples:\n+    Example:\n \n     >>> x = np.random.rand(2, 4, 5, 3)\n     >>> y = keras_core.layers.GlobalAveragePooling2D()(x)\n-    >>> print(y.shape)\n+    >>> y.shape\n     (2, 3)\n     \"\"\"\n \n\n@@ -29,6 +29,7 @@ class GlobalAveragePooling3D(BaseGlobalPooling):\n             The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n     Input shape:\n+\n     - If `data_format='channels_last'`:\n         5D tensor with shape:\n         `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n@@ -37,19 +38,20 @@ class GlobalAveragePooling3D(BaseGlobalPooling):\n         `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n \n     Output shape:\n-    - If `keepdims`=False:\n+\n+    - If `keepdims=False`:\n         2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n+    - If `keepdims=True`:\n         - If `data_format=\"channels_last\"`:\n             5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n         - If `data_format=\"channels_first\"`:\n             5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n \n-    Examples:\n+    Example:\n \n     >>> x = np.random.rand(2, 4, 5, 4, 3)\n     >>> y = keras_core.layers.GlobalAveragePooling3D()(x)\n-    >>> print(y.shape)\n+    >>> y.shape\n     (2, 3)\n     \"\"\"\n \n\n@@ -27,6 +27,7 @@ class GlobalMaxPooling1D(BaseGlobalPooling):\n             The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n     Input shape:\n+\n     - If `data_format='channels_last'`:\n         3D tensor with shape:\n         `(batch_size, steps, features)`\n@@ -35,19 +36,20 @@ class GlobalMaxPooling1D(BaseGlobalPooling):\n         `(batch_size, features, steps)`\n \n     Output shape:\n-    - If `keepdims`=False:\n+\n+    - If `keepdims=False`:\n         2D tensor with shape `(batch_size, features)`.\n-    - If `keepdims`=True:\n+    - If `keepdims=True`:\n         - If `data_format=\"channels_last\"`:\n             3D tensor with shape `(batch_size, 1, features)`\n         - If `data_format=\"channels_first\"`:\n             3D tensor with shape `(batch_size, features, 1)`\n \n-    Examples:\n+    Example:\n \n     >>> x = np.random.rand(2, 3, 4)\n     >>> y = keras_core.layers.GlobalMaxPooling1D()(x)\n-    >>> print(y.shape)\n+    >>> y.shape\n     (2, 4)\n     \"\"\"\n \n\n@@ -28,6 +28,7 @@ class GlobalMaxPooling2D(BaseGlobalPooling):\n             The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n     Input shape:\n+\n     - If `data_format='channels_last'`:\n         4D tensor with shape:\n         `(batch_size, height, width, channels)`\n@@ -36,19 +37,20 @@ class GlobalMaxPooling2D(BaseGlobalPooling):\n         `(batch_size, channels, height, width)`\n \n     Output shape:\n-    - If `keepdims`=False:\n+\n+    - If `keepdims=False`:\n         2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n+    - If `keepdims=True`:\n         - If `data_format=\"channels_last\"`:\n             4D tensor with shape `(batch_size, 1, 1, channels)`\n         - If `data_format=\"channels_first\"`:\n             4D tensor with shape `(batch_size, channels, 1, 1)`\n \n-    Examples:\n+    Example:\n \n     >>> x = np.random.rand(2, 4, 5, 3)\n     >>> y = keras_core.layers.GlobalMaxPooling2D()(x)\n-    >>> print(y.shape)\n+    >>> y.shape\n     (2, 3)\n     \"\"\"\n \n\n@@ -29,6 +29,7 @@ class GlobalMaxPooling3D(BaseGlobalPooling):\n             The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n     Input shape:\n+\n     - If `data_format='channels_last'`:\n         5D tensor with shape:\n         `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n@@ -37,19 +38,20 @@ class GlobalMaxPooling3D(BaseGlobalPooling):\n         `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n \n     Output shape:\n-    - If `keepdims`=False:\n+\n+    - If `keepdims=False`:\n         2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n+    - If `keepdims=True`:\n         - If `data_format=\"channels_last\"`:\n             5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n         - If `data_format=\"channels_first\"`:\n             5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n \n-    Examples:\n+    Example:\n \n     >>> x = np.random.rand(2, 4, 5, 4, 3)\n     >>> y = keras_core.layers.GlobalMaxPooling3D()(x)\n-    >>> print(y.shape)\n+    >>> y.shape\n     (2, 3)\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9070bf4673e86d0b8379dac9515cafbf3bb16c40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 34/34 | Churn Δ: 154 | Churn Cumulative: 206 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 0.14942528735632185\n\nDIFF:\n@@ -8,6 +8,8 @@ from keras_core.layers.merging.add import Add\n from keras_core.layers.merging.add import add\n from keras_core.layers.merging.average import Average\n from keras_core.layers.merging.average import average\n+from keras_core.layers.merging.concatenate import Concatenate\n+from keras_core.layers.merging.concatenate import concatenate\n from keras_core.layers.merging.maximum import Maximum\n from keras_core.layers.merging.maximum import maximum\n from keras_core.layers.merging.minimum import Minimum\n\n@@ -0,0 +1,152 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Concatenate\")\n+class Concatenate(Merge):\n+    \"\"\"Concatenates a list of inputs.\n+\n+    It takes as input a list of tensors, all of the same shape except\n+    for the concatenation axis, and returns a single tensor that is the\n+    concatenation of all inputs.\n+\n+    Examples:\n+\n+    >>> x = np.arange(20).reshape(2, 2, 5)\n+    >>> y = np.arange(20, 30).reshape(2, 1, 5)\n+    >>> keras_core.layers.Concatenate(axis=1)([x, y])\n+\n+    Usage in a Keras model:\n+\n+    >>> x1 = keras_core.layers.Dense(8)(np.arange(10).reshape(5, 2))\n+    >>> x2 = keras_core.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n+    >>> y = keras_core.layers.Concatenate()([x1, x2])\n+\n+    Args:\n+        axis: Axis along which to concatenate.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor, the concatenation of the inputs alongside axis `axis`.\n+    \"\"\"\n+\n+    def __init__(self, axis=-1, **kwargs):\n+        super().__init__(**kwargs)\n+        self.axis = axis\n+        self.supports_masking = True\n+        self._reshape_required = False\n+\n+    def build(self, input_shape):\n+        # Used purely for shape validation.\n+        if len(input_shape) < 1 or not isinstance(input_shape[0], tuple):\n+            raise ValueError(\n+                \"A `Concatenate` layer should be called on a list of \"\n+                f\"at least 1 input. Received: input_shape={input_shape}\"\n+            )\n+        if all(shape is None for shape in input_shape):\n+            return\n+\n+        reduced_inputs_shapes = [list(shape) for shape in input_shape]\n+        shape_set = set()\n+\n+        for i in range(len(reduced_inputs_shapes)):\n+            del reduced_inputs_shapes[i][self.axis]\n+            shape_set.add(tuple(reduced_inputs_shapes[i]))\n+\n+        if len(shape_set) != 1:\n+            err_msg = (\n+                \"A `Concatenate` layer requires inputs with matching shapes \"\n+                \"except for the concatenation axis. \"\n+                f\"Received: input_shape={input_shape}\"\n+            )\n+            # Make sure all the shapes have same ranks.\n+            ranks = set(len(shape) for shape in shape_set)\n+            if len(ranks) != 1:\n+                raise ValueError(err_msg)\n+            # Get the only rank for the set.\n+            (rank,) = ranks\n+            for axis in range(rank):\n+                # Skip the Nones in the shape since they are dynamic, also the\n+                # axis for concat has been removed above.\n+                unique_dims = set(\n+                    shape[axis]\n+                    for shape in shape_set\n+                    if shape[axis] is not None\n+                )\n+                if len(unique_dims) > 1:\n+                    raise ValueError(err_msg)\n+\n+    def _merge_function(self, inputs):\n+        return ops.concatenate(inputs, axis=self.axis)\n+\n+    def compute_output_shape(self, input_shape):\n+        if (not isinstance(input_shape, (tuple, list))) or (\n+            not isinstance(input_shape[0], (tuple, list))\n+        ):\n+            raise ValueError(\n+                \"A `Concatenate` layer should be called on a list of inputs. \"\n+                f\"Received: input_shape={input_shape}\"\n+            )\n+        input_shapes = input_shape\n+        output_shape = list(input_shapes[0])\n+\n+        for shape in input_shapes[1:]:\n+            if output_shape[self.axis] is None or shape[self.axis] is None:\n+                output_shape[self.axis] = None\n+                break\n+            output_shape[self.axis] += shape[self.axis]\n+        return tuple(output_shape)\n+\n+    def compute_mask(self, inputs, mask=None):\n+        if mask is None:\n+            return None\n+        if not isinstance(mask, (tuple, list)):\n+            raise ValueError(f\"`mask` should be a list. Received mask={mask}\")\n+        if not isinstance(inputs, (tuple, list)):\n+            raise ValueError(\n+                f\"`inputs` should be a list. Received: inputs={inputs}\"\n+            )\n+        if len(mask) != len(inputs):\n+            raise ValueError(\n+                \"The lists `inputs` and `mask` should have the same length. \"\n+                f\"Received: inputs={inputs} of length {len(inputs)}, and \"\n+                f\"mask={mask} of length {len(mask)}\"\n+            )\n+        if all(m is None for m in mask):\n+            return None\n+        # Make a list of masks while making sure\n+        # the dimensionality of each mask\n+        # is the same as the corresponding input.\n+        masks = []\n+        for input_i, mask_i in zip(inputs, mask):\n+            if mask_i is None:\n+                # Input is unmasked. Append all 1s to masks,\n+                masks.append(ops.ones_like(input_i, dtype=\"bool\"))\n+            elif mask_i.ndim < input_i.ndim:\n+                # Mask is smaller than the input, expand it\n+                masks.append(ops.expand_dims(mask_i, axis=-1))\n+            else:\n+                masks.append(mask_i)\n+        concatenated = ops.concatenate(masks, axis=self.axis)\n+        return ops.all(concatenated, axis=-1, keepdims=False)\n+\n+    def get_config(self):\n+        config = {\"axis\": self.axis}\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.layers.concatenate\")\n+def concatenate(inputs, axis=-1, **kwargs):\n+    \"\"\"Functional interface to the `Concatenate` layer.\n+\n+    Args:\n+        inputs: A list of input tensors.\n+        axis: Concatenation axis.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor, the concatenation of the inputs alongside axis `axis`.\n+    \"\"\"\n+    return Concatenate(axis=axis, **kwargs)(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a9fc53a9515d0d9c099f7fb9613ff243fcc9d4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 27 | Churn Cumulative: 250 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,14 +1,17 @@\n import jax\n import numpy as np\n+import pandas\n import tensorflow as tf\n+from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import testing\n from keras_core.trainers.data_adapters import array_data_adapter\n \n \n-class TestArrayDataAdapter(testing.TestCase):\n-    def _test_basic_flow(self, array_type=\"np\"):\n+class TestArrayDataAdapter(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters([(\"np\",), (\"tf\",), (\"pandas\")])\n+    def test_basic_flow(self, array_type):\n         if array_type == \"np\":\n             x = np.random.random((34, 4))\n             y = np.random.random((34, 2))\n@@ -19,8 +22,9 @@ class TestArrayDataAdapter(testing.TestCase):\n             x = jax.numpy.ones((34, 4))\n             y = jax.numpy.ones((34, 2))\n         elif array_type == \"pandas\":\n-            # TODO\n-            raise ValueError(\"TODO\")\n+            x = pandas.DataFrame(np.random.random((34, 4)))\n+            y = pandas.DataFrame(np.random.random((34, 2)))\n+\n         adapter = array_data_adapter.ArrayDataAdapter(\n             x,\n             y=y,\n@@ -63,12 +67,6 @@ class TestArrayDataAdapter(testing.TestCase):\n                 self.assertEqual(tuple(bx.shape), (2, 4))\n                 self.assertEqual(tuple(by.shape), (2, 2))\n \n-    def test_basic_flow_np(self):\n-        self._test_basic_flow(\"np\")\n-\n-    def test_basic_flow_tf(self):\n-        self._test_basic_flow(\"tf\")\n-\n     def test_multi_inputs_and_outputs(self):\n         x1 = np.random.random((34, 1))\n         x2 = np.random.random((34, 2))\n@@ -146,7 +144,8 @@ class TestArrayDataAdapter(testing.TestCase):\n                 self.assertEqual(tuple(bw[0].shape), (2,))\n                 self.assertEqual(tuple(bw[1].shape), (2,))\n \n-    def _test_class_weights(self, target_encoding=\"int\"):\n+    @parameterized.parameters([(\"int\",), (\"categorical\",)])\n+    def test_class_weights(self, target_encoding):\n         x = np.random.random((4, 2))\n         if target_encoding == \"int\":\n             y = np.array([[0], [1], [2], [3]], dtype=\"int32\")\n@@ -174,12 +173,6 @@ class TestArrayDataAdapter(testing.TestCase):\n             _, _, bw = batch\n             self.assertAllClose(bw, [0.1, 0.2, 0.3, 0.4])\n \n-    def test_class_weights_int_targets(self):\n-        self._test_class_weights(target_encoding=\"int\")\n-\n-    def test_class_weights_categorical_targets(self):\n-        self._test_class_weights(target_encoding=\"categorical\")\n-\n     def test_errors(self):\n         # TODO\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2733b6e680c37d92d7d390013471a3d5f1c0a3ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 122/122 | Churn Δ: 8 | Churn Cumulative: 792 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.5\n\nDIFF:\n@@ -127,6 +127,10 @@ def save_model(model, filepath, weights_format=\"h5\"):\n     container (list, tuple, or dict), and the container is referenced via a\n     layer attribute.\n     \"\"\"\n+\n+    # API usage tracking for Keras V3 saving\n+    base_layer.keras_api_gauge.get_cell(\"save_model_v3\").set(True)\n+\n     filepath = str(filepath)\n     if not filepath.endswith(\".keras\"):\n         raise ValueError(\n@@ -286,6 +290,10 @@ def save_weights_only(model, filepath):\n     \"\"\"\n     # TODO: if h5 filepath is remote, create the file in a temporary directory\n     # then upload it\n+\n+    # API usage tracking for Keras V3 saving\n+    base_layer.keras_api_gauge.get_cell(\"save_weights_v3\").set(True)\n+\n     filepath = str(filepath)\n     if not filepath.endswith(\".weights.h5\"):\n         raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e3e94cf3c64a4186c289c7a4642ca9e10db80c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 134 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -113,6 +113,7 @@ for data in dataset:\n     print(\"Loss:\", loss)\n \n # Post-processing model state update\n+trainable_variables, non_trainable_variables, optimizer_variables = state\n for variable, value in zip(model.trainable_variables, trainable_variables):\n     variable.assign(value)\n for variable, value in zip(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cceabce3e3924a4e35e4d77f15128c0d5385b75c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9b92cafb5b5b796984ae15cbfa40b822a200f23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1055 | Lines Deleted: 0 | Files Changed: 8 | Hunks: 8 | Methods Changed: 42 | Complexity Δ (Sum/Max): 63/11 | Churn Δ: 1055 | Churn Cumulative: 1055 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 0.899236641221374\n\nDIFF:\n@@ -0,0 +1,190 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Adafactor\"])\n+class Adafactor(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adafactor algorithm.\n+\n+    Adafactor is commonly used in NLP tasks, and has the advantage\n+    of taking less memory because it only saves partial information of previous\n+    gradients.\n+\n+    The default argument setup is based on the original paper (see reference).\n+    When gradients are of dimension > 2, Adafactor optimizer will delete the\n+    last 2 dimensions separately in its accumulator variables.\n+\n+    Args:\n+        learning_rate: Initial value for the learning rate:\n+            a floating point value, Defaults to 0.001.\n+        beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n+        epsilon_1: float, defaults to 1e-30. A small offset to keep demoninator\n+            away from 0.\n+        epsilon_2: float, defaults to 1e-3. A small offset to avoid learning\n+            rate becoming too small by time.\n+        clip_threshold: float, defaults to 1.0. Clipping threshold. This is a\n+            part of Adafactor algorithm, independent from `clipnorm`,\n+            `clipvalue`, and `global_clipnorm`.\n+        relative_step: bool, defaults to True. If `learning_rate` is a\n+            constant and `relative_step=True`, learning rate will be adjusted\n+            based on current iterations. This is a default learning rate decay\n+            in Adafactor.\n+        {{base_optimizer_keyword_args}}\n+\n+    Reference:\n+\n+    - [Shazeer, Noam et al., 2018](https://arxiv.org/abs/1804.04235).\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_2_decay=-0.8,\n+        epsilon_1=1e-30,\n+        epsilon_2=1e-3,\n+        clip_threshold=1.0,\n+        relative_step=True,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"adafactor\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+        self.beta_2_decay = beta_2_decay\n+        self.epsilon_1 = epsilon_1\n+        self.epsilon_2 = epsilon_2\n+        self.clip_threshold = clip_threshold\n+        self.relative_step = relative_step\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adam optimizer has 3 types of variables: momentums, velocities and\n+        velocity_hat (only set when amsgrad is applied),\n+\n+        Args:\n+            var_list: list of model variables to build Adam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._r = []\n+        self._c = []\n+        self._v = []\n+        for var in var_list:\n+            if len(var.shape) < 2:\n+                # Don't factor if variable is of dimension < 2, but we still\n+                # need to create dummy variables as placeholder.\n+                self._r.append(backend.Variable(0, name=var.name))\n+                self._c.append(backend.Variable(0, name=var.name))\n+            else:\n+                # Always factor the last 2 dimenstions.\n+                r_shape = var.shape[:-1]\n+                c_shape = var.shape[:-2] + var.shape[-1]\n+                self._r.append(\n+                    self.add_variable(\n+                        shape=r_shape,\n+                        dtype=var.dtype,\n+                        name=var.name,\n+                    )\n+                )\n+                self._c.append(\n+                    self.add_variable(\n+                        shape=c_shape,\n+                        dtype=var.dtype,\n+                        name=var.name,\n+                    )\n+                )\n+            self._v.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"v\"\n+                )\n+            )\n+\n+    def _rms(self, x):\n+        return ops.sqrt(ops.mean(ops.square(x)))\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+        epsilon_2 = ops.cast(self.epsilon_2, variable.dtype)\n+        one = ops.cast(1.0, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        if self.relative_step:  # TODO: add learning_rate_schedule logic\n+            # If `relative_step=True` and learning rate is a constant, we\n+            # apply the relative step algorithm.\n+            lr = ops.minimum(lr, 1 / ops.sqrt(local_step))\n+\n+        r = self._r[self._get_variable_index(variable)]\n+        c = self._c[self._get_variable_index(variable)]\n+        v = self._v[self._get_variable_index(variable)]\n+\n+        rho_t = ops.minimum(lr, 1 / ops.sqrt(local_step))\n+        alpha_t = ops.maximum(epsilon_2, self._rms(variable)) * rho_t\n+        regulated_grad_square = ops.square(gradient) + self.epsilon_1\n+        beta_2_t = 1 - ops.power(local_step, self.beta_2_decay)\n+\n+        if len(variable.shape) >= 2:\n+            # `r` deletes the last dimension of gradient, so it is of shape\n+            # `gradient.shape[:-1]`.\n+            r.assign(\n+                beta_2_t * r\n+                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-1)\n+            )\n+            # `c` deletes the second last dimension of gradient, so it is of\n+            # shape `gradient.shape[:-2] + gradient.shape[-1]`.\n+            c.assign(\n+                beta_2_t * c\n+                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-2)\n+            )\n+            v.assign(\n+                ops.expand_dims(\n+                    r / ops.mean(r, axis=-1, keepdims=True), axis=-1\n+                )\n+                * ops.expand_dims(c, -2)\n+            )\n+        else:\n+            v.assign(beta_2_t * v + (1 - beta_2_t) * regulated_grad_square)\n+\n+        # `convert_to_tensor` unifies the handling of sparse and dense grads.\n+        u_t = gradient / ops.sqrt(v)\n+        u_t_hat = u_t / ops.maximum(one, (self._rms(u_t) / self.clip_threshold))\n+        variable.assign(variable - alpha_t * u_t_hat)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"beta_2_decay\": self.beta_2_decay,\n+                \"epsilon_1\": self.epsilon_1,\n+                \"epsilon_2\": self.epsilon_2,\n+                \"clip_threshold\": self.clip_threshold,\n+                \"relative_step\": self.relative_step,\n+            }\n+        )\n+        return config\n+\n+\n+Adafactor.__doc__ = Adafactor.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,93 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.adafactor import Adafactor\n+\n+\n+class AdafactorTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Adafactor(\n+            learning_rate=0.5,\n+            beta_2_decay=-0.65,\n+            epsilon_1=1e-15,\n+            epsilon_2=1e-4,\n+            clip_threshold=0.9,\n+            relative_step=False,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Adafactor(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [-0.3693, 0.6307, 1.6307, 2.6307], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Adafactor(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Adafactor(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Adafactor(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Adafactor(\n+            learning_rate=0.5,\n+            beta_2_decay=-0.65,\n+            epsilon_1=1e-15,\n+            epsilon_2=1e-4,\n+            clip_threshold=0.9,\n+            relative_step=False,\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55],\n+            [0.3031, 0.3026, 0.3025, 0.3024, 0.3024, 0.3024, 0.3024, 0.3024, 0.3024, 0.3024],\n+            [0.1671, 0.1665, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663],\n+            [0.0923, 0.0916, 0.0915, 0.0914, 0.0914, 0.0914, 0.0914, 0.0914, 0.0914, 0.0914],\n+            [0.0554, 0.0548, 0.0546, 0.0546, 0.0546, 0.0546, 0.0546, 0.0545, 0.0545, 0.0545]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Adafactor(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Adafactor(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,141 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Adamax\"])\n+class Adamax(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adamax algorithm.\n+\n+    Adamax, a variant of Adam based on the infinity norm, is a first-order\n+    gradient-based optimization method. Due to its capability of adjusting the\n+    learning rate based on data characteristics, it is suited to learn\n+    time-variant process, e.g., speech data with dynamically changed noise\n+    conditions. Default parameters follow those provided in the paper (see\n+    references below).\n+\n+    Initialization:\n+\n+    ```python\n+    m = 0  # Initialize initial 1st moment vector\n+    u = 0  # Initialize the exponentially weighted infinity norm\n+    t = 0  # Initialize timestep\n+    ```\n+\n+    The update rule for parameter `w` with gradient `g` is described at the end\n+    of section 7.1 of the paper (see the referenece section):\n+\n+    ```python\n+    t += 1\n+    m = beta1 * m + (1 - beta) * g\n+    u = max(beta2 * u, abs(g))\n+    current_lr = learning_rate / (1 - beta1 ** t)\n+    w = w - current_lr * m / (u + epsilon)\n+    ```\n+\n+    Args:\n+        learning_rate: A floating point value, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            learning rate. Defaults to `0.001`.\n+        beta_1: A float value or a constant float tensor. The exponential decay\n+            rate for the 1st moment estimates.\n+        beta_2: A float value or a constant float tensor. The exponential decay\n+            rate for the exponentially weighted infinity norm.\n+        epsilon: A small constant for numerical stability.\n+            {{base_optimizer_keyword_args}}\n+\n+    Reference:\n+\n+    - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"adamax\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adamax optimizer has 2 types of variables: momentums (denoted as m),\n+        exponentially weighted infinity norm (denoted as u).\n+\n+        Args:\n+            var_list: list of model variables to build Adamax variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._m = []\n+        self._u = []\n+        for var in var_list:\n+            self._m.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"m\"\n+                )\n+            )\n+            self._u.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"u\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        beta_1_power = ops.power(\n+            ops.cast(self.beta_1, variable.dtype), local_step\n+        )\n+\n+        m = self._m[self._get_variable_index(variable)]\n+        u = self._u[self._get_variable_index(variable)]\n+\n+        m.assign(m + (gradient - m) * (1 - self.beta_1))\n+        u.assign(ops.maximum(self.beta_2 * u, ops.abs(gradient)))\n+        variable.assign(\n+            variable - (lr * m) / ((1 - beta_1_power) * (u + self.epsilon))\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+            }\n+        )\n+        return config\n+\n+\n+Adamax.__doc__ = Adamax.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,84 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.adamax import Adamax\n+\n+\n+class AdamaxTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Adamax(\n+            learning_rate=0.5,\n+            beta_1=0.8,\n+            beta_2=0.95,\n+            epsilon=1e-5,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Adamax(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(vars, [0.5, 1.5, 2.5, 3.5], rtol=1e-4, atol=1e-4)\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Adamax(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Adamax(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Adamax(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Adamax(\n+            learning_rate=0.2, beta_1=0.85, beta_2=0.95, epsilon=1e-6\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n+            [0.6827, 0.6873, 0.6888, 0.6896, 0.6901, 0.6904, 0.6906, 0.6908, 0.6909, 0.691],\n+            [0.5333, 0.5407, 0.5431, 0.5444, 0.5451, 0.5456, 0.546, 0.5462, 0.5464, 0.5466],\n+            [0.368, 0.3773, 0.3804, 0.382, 0.3829, 0.3835, 0.384, 0.3843, 0.3846, 0.3848],\n+            [0.1933, 0.204, 0.2076, 0.2094, 0.2105, 0.2112, 0.2117, 0.2121, 0.2124, 0.2126]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Adamax(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Adamax(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,227 @@\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Ftrl\"])\n+class Ftrl(optimizer.Optimizer):\n+    r\"\"\"Optimizer that implements the FTRL algorithm.\n+\n+    \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm\n+    developed at Google for click-through rate prediction in the early 2010s. It\n+    is most suitable for shallow models with large and sparse feature spaces.\n+    The algorithm is described by\n+    [McMahan et al., 2013](https://research.google.com/pubs/archive/41159.pdf).\n+    The Keras version has support for both online L2 regularization\n+    (the L2 regularization described in the paper\n+    above) and shrinkage-type L2 regularization\n+    (which is the addition of an L2 penalty to the loss function).\n+\n+    Initialization:\n+\n+    ```python\n+    n = 0\n+    sigma = 0\n+    z = 0\n+    ```\n+\n+    Update rule for one variable `w`:\n+\n+    ```python\n+    prev_n = n\n+    n = n + g ** 2\n+    sigma = (n ** -lr_power - prev_n ** -lr_power) / lr\n+    z = z + g - sigma * w\n+    if abs(z) < lambda_1:\n+      w = 0\n+    else:\n+      w = (sgn(z) * lambda_1 - z) / ((beta + sqrt(n)) / alpha + lambda_2)\n+    ```\n+\n+    Notation:\n+\n+    - `lr` is the learning rate\n+    - `g` is the gradient for the variable\n+    - `lambda_1` is the L1 regularization strength\n+    - `lambda_2` is the L2 regularization strength\n+    - `lr_power` is the power to scale n.\n+\n+    Check the documentation for the `l2_shrinkage_regularization_strength`\n+    parameter for more details when shrinkage is enabled, in which case gradient\n+    is replaced with a gradient with shrinkage.\n+\n+    Args:\n+        learning_rate: A floating point value, or a callable that\n+            takes no arguments and returns the actual value to use. The learning\n+            rate.  Defaults to `0.001`.\n+        learning_rate_power: A float value, must be less or equal to zero.\n+            Controls how the learning rate decreases during training. Use zero\n+            for a fixed learning rate.\n+        initial_accumulator_value: The starting value for accumulators. Only\n+            zero or positive values are allowed.\n+        l1_regularization_strength: A float value, must be greater than or equal\n+            to zero. Defaults to `0.0`.\n+        l2_regularization_strength: A float value, must be greater than or equal\n+            to zero. Defaults to `0.0`.\n+        l2_shrinkage_regularization_strength: A float value, must be greater\n+            than or equal to zero. This differs from L2 above in that the L2\n+            above is a stabilization penalty, whereas this L2 shrinkage is a\n+            magnitude penalty. When input is sparse shrinkage will only happen\n+            on the active weights.\n+        beta: A float value, representing the beta value from the paper.\n+            Defaults to 0.0.\n+        {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        learning_rate_power=-0.5,\n+        initial_accumulator_value=0.1,\n+        l1_regularization_strength=0.0,\n+        l2_regularization_strength=0.0,\n+        l2_shrinkage_regularization_strength=0.0,\n+        beta=0.0,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"ftrl\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+\n+        if initial_accumulator_value < 0.0:\n+            raise ValueError(\n+                \"`initial_accumulator_value` needs to be positive or zero. \"\n+                \"Received: initial_accumulator_value=\"\n+                f\"{initial_accumulator_value}.\"\n+            )\n+        if learning_rate_power > 0.0:\n+            raise ValueError(\n+                \"`learning_rate_power` needs to be negative or zero. Received: \"\n+                f\"learning_rate_power={learning_rate_power}.\"\n+            )\n+        if l1_regularization_strength < 0.0:\n+            raise ValueError(\n+                \"`l1_regularization_strength` needs to be positive or zero. \"\n+                \"Received: l1_regularization_strength=\"\n+                f\"{l1_regularization_strength}.\"\n+            )\n+        if l2_regularization_strength < 0.0:\n+            raise ValueError(\n+                \"`l2_regularization_strength` needs to be positive or zero. \"\n+                \"Received: l2_regularization_strength=\"\n+                f\"{l2_regularization_strength}.\"\n+            )\n+        if l2_shrinkage_regularization_strength < 0.0:\n+            raise ValueError(\n+                \"`l2_shrinkage_regularization_strength` needs to be positive \"\n+                \"or zero. Received: l2_shrinkage_regularization_strength\"\n+                f\"={l2_shrinkage_regularization_strength}.\"\n+            )\n+\n+        self.learning_rate_power = learning_rate_power\n+        self.initial_accumulator_value = initial_accumulator_value\n+        self.l1_regularization_strength = l1_regularization_strength\n+        self.l2_regularization_strength = l2_regularization_strength\n+        self.l2_shrinkage_regularization_strength = (\n+            l2_shrinkage_regularization_strength\n+        )\n+        self.beta = beta\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Args:\n+            var_list: list of model variables to build Ftrl variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._accumulators = []\n+        self._linears = []\n+        for var in var_list:\n+            self._accumulators.append(\n+                self.add_variable(\n+                    shape=var.shape,\n+                    dtype=var.dtype,\n+                    name=\"accumulator\",\n+                    initializer=initializers.Constant(\n+                        self.initial_accumulator_value,\n+                    ),\n+                )\n+            )\n+            self._linears.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"linear\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+\n+        accum = self._accumulators[self._get_variable_index(variable)]\n+        linear = self._linears[self._get_variable_index(variable)]\n+\n+        lr_power = self.learning_rate_power\n+        l2_reg = self.l2_regularization_strength\n+        l2_reg = l2_reg + self.beta / (2.0 * lr)\n+\n+        # Ftrl optimizer has the same implementation for sparse and dense\n+        # gradients update.\n+        grad_to_use = (\n+            gradient + 2 * self.l2_shrinkage_regularization_strength * variable\n+        )\n+        new_accum = accum + ops.power(gradient, 2)\n+        linear.assign(\n+            linear\n+            + grad_to_use\n+            - (ops.power(new_accum, -lr_power) - ops.power(accum, -lr_power))\n+            / lr\n+            * variable\n+        )\n+        quadratic = ops.power(new_accum, (-lr_power)) / lr + 2 * l2_reg\n+        linear_clipped = ops.clip(\n+            linear,\n+            -self.l1_regularization_strength,\n+            self.l1_regularization_strength,\n+        )\n+        variable.assign((linear_clipped - linear) / quadratic)\n+        accum.assign(new_accum)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"learning_rate_power\": self.learning_rate_power,\n+                \"initial_accumulator_value\": self.initial_accumulator_value,\n+                \"l1_regularization_strength\": self.l1_regularization_strength,\n+                \"l2_regularization_strength\": self.l2_regularization_strength,\n+                \"l2_shrinkage_regularization_strength\": self.l2_shrinkage_regularization_strength,  # noqa: E501\n+                \"beta\": self.beta,\n+            }\n+        )\n+        return config\n+\n+\n+Ftrl.__doc__ = Ftrl.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,73 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.ftrl import Ftrl\n+\n+\n+class FtrlTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Ftrl(\n+            learning_rate=0.05,\n+            learning_rate_power=-0.2,\n+            initial_accumulator_value=0.4,\n+            l1_regularization_strength=0.05,\n+            l2_regularization_strength=0.15,\n+            l2_shrinkage_regularization_strength=0.01,\n+            beta=0.3,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Ftrl(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [0.2218, 1.3954, 2.3651, 2.8814], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Ftrl(\n+            learning_rate=0.05,\n+            learning_rate_power=-0.2,\n+            initial_accumulator_value=0.4,\n+            l1_regularization_strength=0.05,\n+            l2_regularization_strength=0.15,\n+            l2_shrinkage_regularization_strength=0.01,\n+            beta=0.3,\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n+            [-0.0034, -0.0077, -0.0118, -0.0157, -0.0194, -0.023, -0.0263, -0.0294, -0.0325, -0.0354],\n+            [-0.0078, -0.0162, -0.0242, -0.0317, -0.0387, -0.0454, -0.0516, -0.0575, -0.0631, -0.0685],\n+            [-0.0121, -0.0246, -0.0363, -0.0472, -0.0573, -0.0668, -0.0757, -0.0842, -0.0922, -0.0999],\n+            [-0.0164, -0.0328, -0.0481, -0.0623, -0.0753, -0.0875, -0.099, -0.1098, -0.1201, -0.1299]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Ftrl(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Ftrl(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,158 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Nadam\"])\n+class Nadam(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Nadam algorithm.\n+\n+    Much like Adam is essentially RMSprop with momentum, Nadam is Adam with\n+    Nesterov momentum.\n+\n+    Args:\n+        learning_rate: A floating point value or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            learning rate. Defaults to `0.001`.\n+        beta_1: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 1st moment estimates.\n+            Defaults to `0.9`.\n+        beta_2: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 2nd moment estimates. Defaults to\n+            `0.999`.\n+        epsilon: A small constant for numerical stability. This epsilon is\n+            \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n+            Defaults to `1e-7`.\n+        {{base_optimizer_keyword_args}}\n+\n+    Reference:\n+\n+    - [Dozat, 2015](http://cs229.stanford.edu/proj2015/054_report.pdf).\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"nadam\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Nadam optimizer has 2 types of variables: momentums and velocities.\n+\n+        Args:\n+            var_list: list of model variables to build Nadam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._momentums = []\n+        self._velocities = []\n+        self._u_product = backend.Variable(1.0, dtype=var_list[0].dtype)\n+        # Keep a counter on how many times of _u_product has been computed to\n+        # avoid duplicated computations.\n+        self._u_product_counter = 1\n+\n+        for var in var_list:\n+            self._momentums.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"m\"\n+                )\n+            )\n+            self._velocities.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"v\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        var_dtype = variable.dtype\n+        lr = ops.cast(learning_rate, var_dtype)\n+        gradient = ops.cast(gradient, var_dtype)\n+\n+        local_step = ops.cast(self.iterations + 1, var_dtype)\n+        next_step = ops.cast(self.iterations + 2, var_dtype)\n+        decay = ops.cast(0.96, var_dtype)\n+        beta_1 = ops.cast(self.beta_1, var_dtype)\n+        beta_2 = ops.cast(self.beta_2, var_dtype)\n+        u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n+        u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n+\n+        def get_cached_u_product():\n+            return self._u_product\n+\n+        def compute_new_u_product():\n+            u_product_t = self._u_product * u_t\n+            self._u_product.assign(u_product_t)\n+            self._u_product_counter += 1\n+            return u_product_t\n+\n+        if self._u_product_counter == (self.iterations + 2):\n+            u_product_t = get_cached_u_product()\n+        else:\n+            u_product_t = compute_new_u_product()\n+\n+        u_product_t_1 = u_product_t * u_t_1\n+        beta_2_power = ops.power(beta_2, local_step)\n+\n+        m = self._momentums[self._get_variable_index(variable)]\n+        v = self._velocities[self._get_variable_index(variable)]\n+\n+        m.assign(m + (gradient - m) * (1 - beta_1))\n+        v.assign(v + (ops.square(gradient) - v) * (1 - beta_2))\n+        m_hat = u_t_1 * m / (1 - u_product_t_1) + (1 - u_t) * gradient / (\n+            1 - u_product_t\n+        )\n+        v_hat = v / (1 - beta_2_power)\n+\n+        variable.assign(\n+            variable - (m_hat * lr) / (ops.sqrt(v_hat) + self.epsilon)\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+            }\n+        )\n+        return config\n+\n+\n+Nadam.__doc__ = Nadam.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,89 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.nadam import Nadam\n+\n+\n+class NadamTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Nadam(\n+            learning_rate=0.5,\n+            beta_1=0.5,\n+            beta_2=0.67,\n+            epsilon=1e-5,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Nadam(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [0.4686, 1.4686, 2.4686, 3.4686], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Nadam(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Nadam(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Nadam(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Nadam(\n+            learning_rate=0.5,\n+            beta_1=0.5,\n+            beta_2=0.67,\n+            epsilon=1e-5,\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281],\n+            [-0.1738, -0.1731, -0.1726, -0.1723, -0.1721, -0.172, -0.1719, -0.1718, -0.1718, -0.1717],\n+            [-0.7115, -0.7103, -0.7096, -0.7092, -0.709, -0.7088, -0.7086, -0.7085, -0.7085, -0.7084],\n+            [-1.2335, -1.2322, -1.2313, -1.2309, -1.2306, -1.2304, -1.2302, -1.2301, -1.23, -1.2299],\n+            [-1.7492, -1.7478, -1.7469, -1.7464, -1.7461, -1.7459, -1.7457, -1.7456, -1.7455, -1.7454]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Nadam(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Nadam(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
