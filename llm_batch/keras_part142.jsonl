{"custom_id": "keras#c39546ee10f8eeaf44abf724a528ea14a9d58d01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 11106 | Contributors (this commit): 58 | Commits (past 90d): 14 | Contributors (cumulative): 58 | DMM Complexity: None\n\nDIFF:\n@@ -17,8 +17,7 @@ from ..utils.generic_utils import func_dump, func_load\n \n \n class Masking(Layer):\n-    \"\"\"Masks an input sequence by using a mask value to\n-    identify timesteps to be skipped.\n+    \"\"\"Masks a sequence by using a mask value to skip timesteps.\n \n     For each timestep in the input tensor (dimension #1 in the tensor),\n     if all values in the input tensor at that timestep\n@@ -99,7 +98,9 @@ class Dropout(Layer):\n \n \n class SpatialDropout1D(Dropout):\n-    \"\"\"This version performs the same function as Dropout, however it drops\n+    \"\"\"Spatial 1D version of Dropout.\n+\n+    This version performs the same function as Dropout, however it drops\n     entire 1D feature maps instead of individual elements. If adjacent frames\n     within feature maps are strongly correlated (as is normally the case in\n     early convolution layers) then regular dropout will not regularize the\n@@ -131,7 +132,9 @@ class SpatialDropout1D(Dropout):\n \n \n class SpatialDropout2D(Dropout):\n-    \"\"\"This version performs the same function as Dropout, however it drops\n+    \"\"\"Spatial 2D version of Dropout.\n+\n+    This version performs the same function as Dropout, however it drops\n     entire 2D feature maps instead of individual elements. If adjacent pixels\n     within feature maps are strongly correlated (as is normally the case in\n     early convolution layers) then regular dropout will not regularize the\n@@ -179,7 +182,9 @@ class SpatialDropout2D(Dropout):\n \n \n class SpatialDropout3D(Dropout):\n-    \"\"\"This version performs the same function as Dropout, however it drops\n+    \"\"\"Spatial 3D version of Dropout.\n+\n+    This version performs the same function as Dropout, however it drops\n     entire 3D feature maps instead of individual elements. If adjacent voxels\n     within feature maps are strongly correlated (as is normally the case in\n     early convolution layers) then regular dropout will not regularize the\n@@ -499,8 +504,7 @@ class RepeatVector(Layer):\n \n \n class Lambda(Layer):\n-    \"\"\"Used for evaluating an arbitrary Theano / TensorFlow expression\n-    on the output of the previous layer.\n+    \"\"\"Used for evaluating an arbitrary expressions on an input.\n \n     # Examples\n \n@@ -667,7 +671,7 @@ class Lambda(Layer):\n \n \n class Dense(Layer):\n-    \"\"\"Just your regular fully connected NN layer.\n+    \"\"\"Just your regular densely-connected NN layer.\n \n     # Example\n \n@@ -809,8 +813,7 @@ class Dense(Layer):\n \n \n class ActivityRegularization(Layer):\n-    \"\"\"Layer that returns its input unchanged, but applies an update\n-    to the cost function based on the activity of the input.\n+    \"\"\"Layer that applies an update to the cost function based input activity.\n \n     # Arguments\n         l1: L1 regularization factor (positive float).\n@@ -975,8 +978,9 @@ class MaxoutDense(Layer):\n \n \n class Highway(Layer):\n-    \"\"\"Densely connected highway network,\n-    a natural extension of LSTMs to feedforward networks.\n+    \"\"\"Densely connected highway network.\n+\n+    Highway layers are a natural extension of LSTMs to feedforward networks.\n \n     # Arguments\n         init: name of initialization function for the weights of the layer\n@@ -1112,6 +1116,7 @@ class Highway(Layer):\n \n class TimeDistributedDense(Layer):\n     \"\"\"Apply a same Dense layer for each dimension[1] (time_dimension) input.\n+\n     Especially useful after a recurrent network with 'return_sequence=True'.\n \n     Note: this layer is deprecated, prefer using the `TimeDistributed` wrapper:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8cd940cf850353a64846c7755274c8c2ba6fddb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 235 | Contributors (this commit): 12 | Commits (past 90d): 7 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -17,13 +17,13 @@ def to_categorical(y, nb_classes=None):\n     # Returns\n         A binary matrix representation of the input.\n     \"\"\"\n-    y = np.array(y, dtype='int')\n+    y = np.array(y, dtype='int').ravel()\n     if not nb_classes:\n         nb_classes = np.max(y)+1\n-    Y = np.zeros((len(y), nb_classes))\n-    for i in range(len(y)):\n-        Y[i, y[i]] = 1.\n-    return Y\n+    n = y.shape[0]\n+    categorical = np.zeros((n, nb_classes))\n+    categorical[np.arange(n), y] = 1\n+    return categorical\n \n \n def normalize(a, axis=-1, order=2):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ccad186fd950b77dab771686200fc4a23bc0c2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 34 | Churn Cumulative: 4988 | Contributors (this commit): 52 | Commits (past 90d): 35 | Contributors (cumulative): 52 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1860,13 +1860,21 @@ def switch(condition, then_expression, else_expression):\n         then_expression: TensorFlow operation.\n         else_expression: TensorFlow operation.\n     \"\"\"\n-    x_shape = copy.copy(then_expression.get_shape())\n     if condition.dtype != tf.bool:\n         condition = tf.cast(condition, 'bool')\n+    if not callable(then_expression):\n+        def then_expression_fn():\n+            return then_expression\n+    else:\n+        then_expression_fn = then_expression\n+    if not callable(else_expression):\n+        def else_expression_fn():\n+            return else_expression\n+    else:\n+        else_expression_fn = else_expression\n     x = _cond(condition,\n-              lambda: then_expression,\n-              lambda: else_expression)\n-    x.set_shape(x_shape)\n+              then_expression_fn,\n+              else_expression_fn)\n     return x\n \n \n@@ -1970,6 +1978,10 @@ def categorical_crossentropy(output, target, from_logits=False):\n         return - tf.reduce_sum(target * tf.log(output),\n                                reduction_indices=len(output.get_shape()) - 1)\n     else:\n+        try:\n+            return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n+                                                           logits=output)\n+        except TypeError:\n             return tf.nn.softmax_cross_entropy_with_logits(output, target)\n \n \n@@ -1985,9 +1997,15 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n         output = tf.log(output)\n \n     output_shape = output.get_shape()\n+    targets = cast(flatten(target), 'int64')\n+    logits = tf.reshape(output, [-1, int(output_shape[-1])])\n+    try:\n         res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n-        tf.reshape(output, [-1, int(output_shape[-1])]),\n-        cast(flatten(target), 'int64'))\n+            labels=targets,\n+            logits=logits)\n+    except TypeError:\n+        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n+            logits, targets)\n     if len(output_shape) == 3:\n         # if our output includes timesteps we need to reshape\n         return tf.reshape(res, tf.shape(output)[:-1])\n@@ -2005,6 +2023,10 @@ def binary_crossentropy(output, target, from_logits=False):\n         epsilon = _to_tensor(_EPSILON, output.dtype.base_dtype)\n         output = tf.clip_by_value(output, epsilon, 1 - epsilon)\n         output = tf.log(output / (1 - output))\n+    try:\n+        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target,\n+                                                       logits=output)\n+    except TypeError:\n         return tf.nn.sigmoid_cross_entropy_with_logits(output, target)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c93d8ec0681521bffd903eab9bba1a6c42a1bd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 18 | Churn Cumulative: 3438 | Contributors (this commit): 48 | Commits (past 90d): 27 | Contributors (cumulative): 48 | DMM Complexity: 0.8\n\nDIFF:\n@@ -1147,6 +1147,10 @@ def rnn(step_function, inputs, initial_states,\n def switch(condition, then_expression, else_expression):\n     \"\"\"condition: scalar tensor.\n     \"\"\"\n+    if callable(then_expression):\n+        then_expression = then_expression()\n+    if callable(else_expression):\n+        else_expression = else_expression()\n     return T.switch(condition, then_expression, else_expression)\n \n \n@@ -1155,6 +1159,10 @@ def in_train_phase(x, alt):\n         return x\n     elif _LEARNING_PHASE is 0:\n         return alt\n+    if callable(x):\n+        x = x()\n+    if callable(alt):\n+        alt = alt()\n     x = theano.ifelse.ifelse(_LEARNING_PHASE, x, alt)\n     x._uses_learning_phase = True\n     return x\n@@ -1165,6 +1173,10 @@ def in_test_phase(x, alt):\n         return alt\n     elif _LEARNING_PHASE is 0:\n         return x\n+    if callable(x):\n+        x = x()\n+    if callable(alt):\n+        alt = alt()\n     x = theano.ifelse.ifelse(_LEARNING_PHASE, alt, x)\n     x._uses_learning_phase = True\n     return x\n@@ -1267,6 +1279,8 @@ def dropout(x, level, noise_shape=None, seed=None):\n         raise ValueError('Dropout level must be in interval [0, 1[.')\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n+    if isinstance(noise_shape, list):\n+        noise_shape = tuple(noise_shape)\n \n     rng = RandomStreams(seed=seed)\n     retain_prob = 1. - level\n@@ -1275,8 +1289,8 @@ def dropout(x, level, noise_shape=None, seed=None):\n         random_tensor = rng.binomial(x.shape, p=retain_prob, dtype=x.dtype)\n     else:\n         random_tensor = rng.binomial(noise_shape, p=retain_prob, dtype=x.dtype)\n-        random_tensor = T.patternbroadcast(random_tensor, [dim == 1 for dim in noise_shape])\n-\n+        random_tensor = T.patternbroadcast(random_tensor,\n+                                           [dim == 1 for dim in noise_shape])\n     x *= random_tensor\n     x /= retain_prob\n     return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8c0c3774e6cf88704f685784f8baba9694220d4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 12387 | Contributors (this commit): 62 | Commits (past 90d): 21 | Contributors (cumulative): 72 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,25 +70,37 @@ class Dropout(Layer):\n \n     # Arguments\n         p: float between 0 and 1. Fraction of the input units to drop.\n+        noise_shape: 1D integer tensor representing the shape of the\n+            binary dropout mask that will be multiplied with the input.\n+            For instance, if your inputs ahve shape\n+            `(batch_size, timesteps, features)` and\n+            you want the dropout mask to be the same for all timesteps,\n+            you can use `noise_shape=(batch_size, 1, features)`.\n+        seed: A Python integer to use as random seed.\n \n     # References\n         - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n \n-    def __init__(self, p, **kwargs):\n+    def __init__(self, p, noise_shape=None, seed=None, **kwargs):\n         self.p = p\n+        self.noise_shape = noise_shape\n+        self.seed = seed\n         if 0. < self.p < 1.:\n             self.uses_learning_phase = True\n         self.supports_masking = True\n         super(Dropout, self).__init__(**kwargs)\n \n-    def _get_noise_shape(self, x):\n-        return None\n+    def _get_noise_shape(self, _):\n+        return self.noise_shape\n \n     def call(self, x, mask=None):\n         if 0. < self.p < 1.:\n             noise_shape = self._get_noise_shape(x)\n-            x = K.in_train_phase(K.dropout(x, self.p, noise_shape), x)\n+\n+            def dropped_inputs():\n+                return K.dropout(x, self.p, noise_shape, seed=self.seed)\n+            x = K.in_train_phase(dropped_inputs, lambda: x)\n         return x\n \n     def get_config(self):\n\n@@ -223,6 +223,10 @@ def test_dropout():\n                kwargs={'p': 0.5},\n                input_shape=(3, 2))\n \n+    layer_test(core.Dropout,\n+               kwargs={'p': 0.5, 'noise_shape': [3, 1]},\n+               input_shape=(3, 2))\n+\n     layer_test(core.SpatialDropout1D,\n                kwargs={'p': 0.5},\n                input_shape=(2, 3, 4))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa18604fec4a309658bcc83b6aa595b5f0838d36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 20 | Churn Cumulative: 2311 | Contributors (this commit): 38 | Commits (past 90d): 18 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -543,7 +543,10 @@ class TensorBoard(Callback):\n             for layer in self.model.layers:\n \n                 for weight in layer.weights:\n+                    if hasattr(tf, 'histogram_summary'):\n                         tf.histogram_summary(weight.name, weight)\n+                    else:\n+                        tf.summary.histogram(weight.name, weight)\n \n                     if self.write_images:\n                         w_img = tf.squeeze(weight)\n@@ -557,17 +560,26 @@ class TensorBoard(Callback):\n \n                         w_img = tf.expand_dims(tf.expand_dims(w_img, 0), -1)\n \n+                        if hasattr(tf, 'image_summary'):\n                             tf.image_summary(weight.name, w_img)\n+                        else:\n+                            tf.summary.image(weight.name, w_img)\n \n                 if hasattr(layer, 'output'):\n+                    if hasattr(tf, 'histogram_summary'):\n                         tf.histogram_summary('{}_out'.format(layer.name),\n                                              layer.output)\n-        if parse_version(tf.__version__) >= parse_version('0.12.0'):\n-            self.merged = tf.summary.merge_all()\n                     else:\n+                        tf.summary.histogram('{}_out'.format(layer.name),\n+                                             layer.output)\n+\n+        if hasattr(tf, 'merge_all_summaries'):\n             self.merged = tf.merge_all_summaries()\n+        else:\n+            self.merged = tf.summary.merge_all()\n+\n         if self.write_graph:\n-            if parse_version(tf.__version__) >= parse_version('0.12.0'):\n+            if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n                 self.writer = tf.summary.FileWriter(self.log_dir,\n                                                     self.sess.graph)\n             elif parse_version(tf.__version__) >= parse_version('0.8.0'):\n@@ -577,7 +589,7 @@ class TensorBoard(Callback):\n                 self.writer = tf.train.SummaryWriter(self.log_dir,\n                                                      self.sess.graph_def)\n         else:\n-            if parse_version(tf.__version__) >= parse_version('0.12.0'):\n+            if hasattr(tf, 'summary') and hasattr(tf.summary, 'FileWriter'):\n                 self.writer = tf.summary.FileWriter(self.log_dir)\n             else:\n                 self.writer = tf.train.SummaryWriter(self.log_dir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38719480a846f70a97726ea489cfa54788c0e602", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 419 | Lines Deleted: 91 | Files Changed: 1 | Hunks: 109 | Methods Changed: 81 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 510 | Churn Cumulative: 5520 | Contributors (this commit): 53 | Commits (past 90d): 37 | Contributors (cumulative): 53 | DMM Complexity: 1.0\n\nDIFF:\n@@ -839,17 +839,17 @@ def batch_dot(x, y, axes=None):\n         Shape inference:\n         Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n         If `axes` is (1, 2), to find the output shape of resultant tensor,\n-            loop through each dimension in x's shape and y's shape:\n+            loop through each dimension in `x`'s shape and `y`'s shape:\n \n         * `x.shape[0]` : 100 : append to output shape\n         * `x.shape[1]` : 20 : do not append to output shape,\n-            dimension 1 of x has been summed over. (`dot_axes[0]` = 1)\n+            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n         * `y.shape[0]` : 100 : do not append to output shape,\n-            always ignore first dimension of y\n+            always ignore first dimension of `y`\n         * `y.shape[1]` : 30 : append to output shape\n         * `y.shape[2]` : 20 : do not append to output shape,\n-            dimension 2 of y has been summed over. (`dot_axes[1]` = 2)\n-        output_shape = `(100, 30)`\n+            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n+        `output_shape` = `(100, 30)`\n \n     ```python\n         >>> x_batch = K.ones(shape=(32, 20, 1))\n@@ -946,6 +946,17 @@ def _normalize_axis(axis, ndim):\n \n def max(x, axis=None, keepdims=False):\n     \"\"\"Maximum value in a tensor.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to find maximum values.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1. If `keepdims` is `True`,\n+            the reduced dimension is retained with length 1.\n+\n+    # Returns\n+        A tensor with maximum values of `x`.\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_max(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -953,6 +964,17 @@ def max(x, axis=None, keepdims=False):\n \n def min(x, axis=None, keepdims=False):\n     \"\"\"Minimum value in a tensor.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to find minimum values.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1. If `keepdims` is `True`,\n+            the reduced dimension is retained with length 1.\n+\n+    # Returns\n+        A tensor with miminum values of `x`.\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_min(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -960,6 +982,17 @@ def min(x, axis=None, keepdims=False):\n \n def sum(x, axis=None, keepdims=False):\n     \"\"\"Sum of the values in a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to sum over.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1. If `keepdims` is `True`,\n+            the reduced dimension is retained with length 1.\n+\n+    # Returns\n+        A tensor with sum of `x`.\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_sum(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -967,6 +1000,17 @@ def sum(x, axis=None, keepdims=False):\n \n def prod(x, axis=None, keepdims=False):\n     \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the product.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1. If `keepdims` is `True`,\n+            the reduced dimension is retained with length 1.\n+\n+    # Returns\n+        A tensor with the product of elements of `x`.\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     return tf.reduce_prod(x, reduction_indices=axis, keep_dims=keepdims)\n@@ -974,6 +1018,17 @@ def prod(x, axis=None, keepdims=False):\n \n def var(x, axis=None, keepdims=False):\n     \"\"\"Variance of a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the variance.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1. If `keepdims` is `True`,\n+            the reduced dimension is retained with length 1.\n+\n+    # Returns\n+        A tensor with the variance of elements of `x`.\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n@@ -987,12 +1042,34 @@ def var(x, axis=None, keepdims=False):\n \n def std(x, axis=None, keepdims=False):\n     \"\"\"Standard deviation of a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: An integer, the axis to compute the standard deviation.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1. If `keepdims` is `True`,\n+            the reduced dimension is retained with length 1.\n+\n+    # Returns\n+        A tensor with the standard deviation of elements of `x`.\n     \"\"\"\n     return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n \n \n def mean(x, axis=None, keepdims=False):\n     \"\"\"Mean of a tensor, alongside the specified axis.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+        axis: A list of integer. Axes to compute the mean.\n+        keepdims: A boolean, whether to keep the dimensions or not.\n+            If `keepdims` is `False`, the rank of the tensor is reduced\n+            by 1 for each entry in `axis`. If `keep_dims` is `True`,\n+            the reduced dimensions are retained with length 1.\n+\n+    # Returns\n+        A tensor with the mean of elements of `x`.\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     if x.dtype.base_dtype == tf.bool:\n@@ -1003,7 +1080,8 @@ def mean(x, axis=None, keepdims=False):\n def any(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical OR).\n \n-    Returns an uint8 tensor (0s and 1s).\n+    # Returns\n+        A uint8 tensor (0s and 1s).\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n@@ -1014,7 +1092,8 @@ def any(x, axis=None, keepdims=False):\n def all(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical AND).\n \n-    Returns an uint8 tensor\n+    # Returns\n+        A uint8 tensor (0s and 1s).\n     \"\"\"\n     axis = _normalize_axis(axis, ndim(x))\n     x = tf.cast(x, tf.bool)\n@@ -1025,6 +1104,9 @@ def all(x, axis=None, keepdims=False):\n def argmax(x, axis=-1):\n     \"\"\"Returns the index of the maximum value\n     along a tensor axis.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n@@ -1034,6 +1116,9 @@ def argmax(x, axis=-1):\n def argmin(x, axis=-1):\n     \"\"\"Returns the index of the minimum value\n     along a tensor axis.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if axis < 0:\n         axis = axis % len(x.get_shape())\n@@ -1042,18 +1127,27 @@ def argmin(x, axis=-1):\n \n def square(x):\n     \"\"\"Element-wise square.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.square(x)\n \n \n def abs(x):\n     \"\"\"Element-wise absolute value.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.abs(x)\n \n \n def sqrt(x):\n     \"\"\"Element-wise square root.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     zero = _to_tensor(0., x.dtype.base_dtype)\n     inf = _to_tensor(np.inf, x.dtype.base_dtype)\n@@ -1063,36 +1157,54 @@ def sqrt(x):\n \n def exp(x):\n     \"\"\"Element-wise exponential.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.exp(x)\n \n \n def log(x):\n     \"\"\"Element-wise log.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.log(x)\n \n \n def round(x):\n     \"\"\"Element-wise rounding to the closest integer.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.round(x)\n \n \n def sign(x):\n     \"\"\"Element-wise sign.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.sign(x)\n \n \n def pow(x, a):\n     \"\"\"Element-wise exponentiation.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.pow(x, a)\n \n \n def clip(x, min_value, max_value):\n     \"\"\"Element-wise value clipping.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if max_value < min_value:\n         max_value = min_value\n@@ -1103,66 +1215,90 @@ def clip(x, min_value, max_value):\n \n def equal(x, y):\n     \"\"\"Element-wise equality between two tensors.\n-    Returns a bool tensor.\n+\n+    # Returns\n+        A bool tensor.\n     \"\"\"\n     return tf.equal(x, y)\n \n \n def not_equal(x, y):\n     \"\"\"Element-wise inequality between two tensors.\n-    Returns a bool tensor.\n+\n+    # Returns\n+        A bool tensor.\n     \"\"\"\n     return tf.not_equal(x, y)\n \n \n def greater(x, y):\n     \"\"\"Element-wise truth value of (x > y).\n-    Returns a bool tensor.\n+\n+    # Returns\n+        A bool tensor.\n     \"\"\"\n     return tf.greater(x, y)\n \n \n def greater_equal(x, y):\n     \"\"\"Element-wise truth value of (x >= y).\n-    Returns a bool tensor.\n+\n+    # Returns\n+        A bool tensor.\n     \"\"\"\n     return tf.greater_equal(x, y)\n \n \n def lesser(x, y):\n     \"\"\"Element-wise truth value of (x < y).\n-    Returns a bool tensor.\n+\n+    # Returns\n+        A bool tensor.\n     \"\"\"\n     return tf.less(x, y)\n \n \n def lesser_equal(x, y):\n     \"\"\"Element-wise truth value of (x <= y).\n-    Returns a bool tensor.\n+\n+    # Returns\n+        A bool tensor.\n     \"\"\"\n     return tf.less_equal(x, y)\n \n \n def maximum(x, y):\n     \"\"\"Element-wise maximum of two tensors.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.maximum(x, y)\n \n \n def minimum(x, y):\n     \"\"\"Element-wise minimum of two tensors.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.minimum(x, y)\n \n \n def sin(x):\n     \"\"\"Computes sin of x element-wise.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.sin(x)\n \n \n def cos(x):\n     \"\"\"Computes cos of x element-wise.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.cos(x)\n \n@@ -1170,6 +1306,9 @@ def cos(x):\n def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n     \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n+\n+    # Returns\n+        A tuple length of 3, `(normalized_tensor, mean, variance)`.\n     \"\"\"\n     mean, var = tf.nn.moments(x, reduction_axes,\n                               shift=None, name=None, keep_dims=False)\n@@ -1200,7 +1339,10 @@ def normalize_batch_in_training(x, gamma, beta,\n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n     \"\"\"Applies batch normalization on x given mean, var, beta and gamma:\n \n-    output = (x - mean) / sqrt(var + epsilon) * gamma + beta\n+    output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.nn.batch_normalization(x, mean, var, beta, gamma, epsilon)\n \n@@ -1209,6 +1351,9 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n \n def concatenate(tensors, axis=-1):\n     \"\"\"Concatenates a list of tensors alongside the specified axis.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if axis < 0:\n         dims = ndim(tensors[0])\n@@ -1228,6 +1373,9 @@ def concatenate(tensors, axis=-1):\n \n def reshape(x, shape):\n     \"\"\"Reshapes a tensor to the specified shape.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.reshape(x, shape)\n \n@@ -1238,16 +1386,22 @@ def permute_dimensions(x, pattern):\n     # Arguments\n         pattern: should be a tuple of\n             dimension indices, e.g. (0, 2, 1).\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.transpose(x, perm=pattern)\n \n \n def resize_images(X, height_factor, width_factor, dim_ordering):\n     \"\"\"Resizes the images contained in a 4D tensor of shape\n-    - [batch, channels, height, width] (for 'th' dim_ordering)\n-    - [batch, height, width, channels] (for 'tf' dim_ordering)\n-    by a factor of (height_factor, width_factor). Both factors should be\n+    - `[batch, channels, height, width]` (for 'th' dim_ordering)\n+    - `[batch, height, width, channels]` (for 'tf' dim_ordering)\n+    by a factor of `(height_factor, width_factor)`. Both factors should be\n     positive integers.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if dim_ordering == 'th':\n         original_shape = int_shape(X)\n@@ -1273,10 +1427,13 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n \n def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n     \"\"\"Resizes the volume contained in a 5D tensor of shape\n-    - [batch, channels, depth, height, width] (for 'th' dim_ordering)\n-    - [batch, depth, height, width, channels] (for 'tf' dim_ordering)\n-    by a factor of (depth_factor, height_factor, width_factor).\n+    - `[batch, channels, depth, height, width]` (for 'th' dim_ordering)\n+    - `[batch, depth, height, width, channels]` (for 'tf' dim_ordering)\n+    by a factor of `(depth_factor, height_factor, width_factor)`.\n     All three factors should be positive integers.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if dim_ordering == 'th':\n         output = repeat_elements(X, depth_factor, axis=2)\n@@ -1293,10 +1450,13 @@ def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n \n \n def repeat_elements(x, rep, axis):\n-    \"\"\"Repeats the elements of a tensor along an axis, like np.repeat\n+    \"\"\"Repeats the elements of a tensor along an axis, like `np.repeat`.\n \n-    If x has shape (s1, s2, s3) and axis=1, the output\n-    will have shape (s1, s2 * rep, s3)\n+    If `x` has shape `(s1, s2, s3)` and `axis` is `1`, the output\n+    will have shape `(s1, s2 * rep, s3)`.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     x_shape = x.get_shape().as_list()\n     # slices along the repeat axis\n@@ -1310,10 +1470,13 @@ def repeat_elements(x, rep, axis):\n \n \n def repeat(x, n):\n-    \"\"\"Repeats a 2D tensor:\n+    \"\"\"Repeats a 2D tensor.\n \n-    if x has shape (samples, dim) and n=2,\n-    the output will have shape (samples, 2, dim)\n+    if `x` has shape (samples, dim) and `n` is `2`,\n+    the output will have shape `(samples, 2, dim)`.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     assert ndim(x) == 2\n     x = tf.expand_dims(x, 1)\n@@ -1328,7 +1491,7 @@ def arange(start, stop=None, step=1, dtype='int32'):\n     Theano's arange: if only one argument is provided,\n     it is in fact the \"stop\" argument.\n \n-    The default type of the returned tensor is 'int32' to\n+    The default type of the returned tensor is `'int32'` to\n     match TensorFlow's default.\n     \"\"\"\n     # Match the behavior of numpy and Theano by returning an empty seqence.\n@@ -1341,18 +1504,38 @@ def arange(start, stop=None, step=1, dtype='int32'):\n \n \n def tile(x, n):\n+    \"\"\"Creates a tensor by tiling `x` by `n`.\n+\n+    # Arguments\n+        x: A tensor or variable\n+        n: A list of integer. The length must be the same as the number of\n+            dimensions in `x`.\n+\n+    # Returns\n+        A tiled tensor.\n+    \"\"\"\n     if isinstance(n, int):\n         n = [n]\n     return tf.tile(x, n)\n \n \n def flatten(x):\n+    \"\"\"Flatten a tensor.\n+\n+    # Returns\n+        A tensor, reshaped into 1-D\n+    \"\"\"\n     return tf.reshape(x, [-1])\n \n \n def batch_flatten(x):\n     \"\"\"Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n+\n+    In other words, it flattens each data samples of a batch.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     x = tf.reshape(x, stack([-1, prod(shape(x)[1:])]))\n     return x\n@@ -1360,12 +1543,18 @@ def batch_flatten(x):\n \n def expand_dims(x, dim=-1):\n     \"\"\"Adds a 1-sized dimension at index \"dim\".\n+\n+    # Returns\n+        A tensor with expended dimensions.\n     \"\"\"\n     return tf.expand_dims(x, dim)\n \n \n def squeeze(x, axis):\n     \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n+\n+    # Returns\n+        A tensor with the same data as `x` but reduced dimensions.\n     \"\"\"\n     return tf.squeeze(x, [axis])\n \n@@ -1373,6 +1562,9 @@ def squeeze(x, axis):\n def temporal_padding(x, padding=1):\n     \"\"\"Pads the middle dimension of a 3D tensor\n     with \"padding\" zeros left and right.\n+\n+    # Returns\n+        A padded 3D tensor.\n     \"\"\"\n     pattern = [[0, 0], [padding, padding], [0, 0]]\n     return tf.pad(x, pattern)\n@@ -1381,6 +1573,9 @@ def temporal_padding(x, padding=1):\n def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n     \"\"\"Pad the middle dimension of a 3D tensor\n     with \"left_pad\" zeros left and \"right_pad\" right.\n+\n+    # Returns\n+        A padded 3D tensor.\n     \"\"\"\n     pattern = [[0, 0], [left_pad, right_pad], [0, 0]]\n     return tf.pad(x, pattern)\n@@ -1389,6 +1584,9 @@ def asymmetric_temporal_padding(x, left_pad=1, right_pad=1):\n def spatial_2d_padding(x, padding=(1, 1), dim_ordering='default'):\n     \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor\n     with \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n+\n+    # Returns\n+        A padded 4D tensor.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -1411,6 +1609,9 @@ def asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1,\n     \"\"\"Pad the rows and columns of a 4D tensor\n     with \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\n     rows on top, bottom; cols on left, right.\n+\n+    # Returns\n+        A padded 4D tensor.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -1436,6 +1637,9 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n \n     For 'tf' dim_ordering, the 2nd, 3rd and 4th dimension will be padded.\n     For 'th' dim_ordering, the 3rd, 4th and 5th dimension will be padded.\n+\n+    # Returns\n+        A padded 5D tensor.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -1462,6 +1666,11 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n \n \n def stack(x):\n+    \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n+\n+    # Returns\n+        A tensor.\n+    \"\"\"\n     try:\n         return tf.stack(x)\n     except AttributeError:\n@@ -1469,15 +1678,21 @@ def stack(x):\n \n \n def one_hot(indices, nb_classes):\n-    \"\"\"Input: nD integer tensor of shape (batch_size, dim1, dim2, ... dim(n-1))\n+    \"\"\"Input: nD integer tensor of shape `(batch_size, dim1, dim2, ... dim(n-1))`\n     Output: (n + 1)D one hot representation of the input\n-    with shape (batch_size, dim1, dim2, ... dim(n-1), nb_classes)\n+    with shape `(batch_size, dim1, dim2, ... dim(n-1), nb_classes)`\n+\n+    # Returns\n+        The one-hot tensor.\n     \"\"\"\n     return tf.one_hot(indices, depth=nb_classes, axis=-1)\n \n \n def reverse(x, axes):\n     \"\"\"Reverse a tensor along the the specified axes\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     if isinstance(axes, int):\n         axes = [axes]\n@@ -1493,15 +1708,19 @@ def reverse(x, axes):\n \n \n def get_value(x):\n-    \"\"\"Returns the value of a tensor variable,\n-    as a Numpy array.\n+    \"\"\"Returns the value of a variable/\n+\n+    # Returns\n+        A Numpy array.\n     \"\"\"\n     return x.eval(session=get_session())\n \n \n def batch_get_value(xs):\n-    \"\"\"Returns the value of more than one tensor variable,\n-    as a list of Numpy arrays.\n+    \"\"\"Returns the value of more than one tensor variable.\n+\n+    # Returns\n+        A list of Numpy arrays.\n     \"\"\"\n     if xs:\n         return get_session().run(xs)\n@@ -1510,8 +1729,9 @@ def batch_get_value(xs):\n \n \n def set_value(x, value):\n-    \"\"\"Sets the value of a tensor variable,\n-    from a Numpy array.\n+    \"\"\"Sets the value of a variable,\n+    from a Numpy array. It returns `None`.\n+\n     \"\"\"\n     value = np.asarray(value)\n     tf_dtype = _convert_string_dtype(x.dtype.name.split('_')[0])\n@@ -1528,6 +1748,7 @@ def set_value(x, value):\n \n def batch_set_value(tuples):\n     \"\"\"Sets the values of many tensor variables at once.\n+    It returns `None`.\n \n     # Arguments\n         tuples: a list of tuples `(tensor, value)`.\n@@ -1554,6 +1775,14 @@ def batch_set_value(tuples):\n \n \n def get_variable_shape(x):\n+    \"\"\"Returns shape of a variable.\n+\n+    # Arguments\n+        A variable.\n+\n+    # Returns\n+        A tuple of integers.\n+    \"\"\"\n     return int_shape(x)\n \n \n@@ -1647,16 +1876,16 @@ def rnn(step_function, inputs, initial_states,\n     \"\"\"Iterates over the time dimension of a tensor.\n \n     # Arguments\n-        inputs: tensor of temporal data of shape (samples, time, ...)\n+        inputs: tensor of temporal data of shape `(samples, time, ...)`\n             (at least 3D).\n         step_function:\n             Parameters:\n-                input: tensor with shape (samples, ...) (no time dimension),\n+                input: tensor with shape `(samples, ...)` (no time dimension),\n                     representing input for the batch of samples at a certain\n                     time step.\n                 states: list of tensors.\n             Returns:\n-                output: tensor with shape (samples, output_dim)\n+                output: tensor with shape `(samples, output_dim)`\n                     (no time dimension).\n                 new_states: list of tensors, same length and shapes\n                     as 'states'. The first state in the list must be the\n@@ -1667,7 +1896,7 @@ def rnn(step_function, inputs, initial_states,\n             the step function.\n         go_backwards: boolean. If True, do the iteration over\n             the time dimension in reverse order.\n-        mask: binary tensor with shape (samples, time, 1),\n+        mask: binary tensor with shape `(samples, time, 1)`,\n             with a zero for every element that is masked.\n         constants: a list of constant values passed at each step.\n         unroll: with TensorFlow the RNN is always unrolled, but with Theano you\n@@ -1676,14 +1905,14 @@ def rnn(step_function, inputs, initial_states,\n             Must be specified if using unrolling with Theano.\n \n     # Returns\n-        A tuple (last_output, outputs, new_states).\n+        A tuple, `(last_output, outputs, new_states)`.\n \n-        last_output: the latest output of the rnn, of shape (samples, ...)\n-        outputs: tensor with shape (samples, time, ...) where each\n-            entry outputs[s, t] is the output of the step function\n-            at time t for sample s.\n+            last_output: the latest output of the rnn, of shape `(samples, ...)`\n+            outputs: tensor with shape `(samples, time, ...)` where each\n+                entry `outputs[s, t]` is the output of the step function\n+                at time `t` for sample `s`.\n             new_states: list of tensors, latest states returned by\n-            the step function, of shape (samples, ...).\n+                the step function, of shape `(samples, ...)`.\n     \"\"\"\n     ndim = len(inputs.get_shape())\n     if ndim < 3:\n@@ -1859,7 +2088,7 @@ def _cond(condition, then_lambda, else_lambda):\n \n def switch(condition, then_expression, else_expression):\n     \"\"\"Switches between two operations\n-    depending on a scalar value (int or bool).\n+    depending on a scalar value (`int` or `bool`).\n     Note that both `then_expression` and `else_expression`\n     should be symbolic tensors of the *same shape*.\n \n@@ -1889,6 +2118,9 @@ def switch(condition, then_expression, else_expression):\n def in_train_phase(x, alt):\n     \"\"\"Selects `x` in train phase, and `alt` otherwise.\n     Note that `alt` should have the *same shape* as `x`.\n+\n+    # Returns\n+        Either `x` or `alt` based on `K.learning_phase`.\n     \"\"\"\n     if learning_phase() is 1:\n         return x\n@@ -1903,6 +2135,9 @@ def in_train_phase(x, alt):\n def in_test_phase(x, alt):\n     \"\"\"Selects `x` in test phase, and `alt` otherwise.\n     Note that `alt` should have the *same shape* as `x`.\n+\n+    # Returns\n+        Either `x` or `alt` based on `K.learning_phase`.\n     \"\"\"\n     if learning_phase() is 1:\n         return alt\n@@ -1917,11 +2152,16 @@ def in_test_phase(x, alt):\n # NN OPERATIONS\n \n def relu(x, alpha=0., max_value=None):\n-    \"\"\"Rectified linear unit\n+    \"\"\"Rectified linear unit.\n+    With default values, it returns element-wise `max(x, 0)`.\n \n     # Arguments\n-        alpha: slope of negative section.\n-        max_value: saturation threshold.\n+        x: A tensor or variable.\n+        alpha: A scalar, slope of negative section (default=`0.`).\n+        max_value: Saturation threshold.\n+\n+    # Returns\n+        A tensor\n     \"\"\"\n     if alpha != 0.:\n         negative_part = tf.nn.relu(-x)\n@@ -1940,8 +2180,8 @@ def elu(x, alpha=1.):\n     \"\"\"Exponential linear unit.\n \n     # Arguments\n-        x: Tensor to compute the activation function for.\n-        alpha: scalar\n+        x: A tenor or variable to compute the activation function for.\n+        alpha: A scalar, slope of positive section.\n     \"\"\"\n     res = tf.nn.elu(x)\n     if alpha == 1:\n@@ -1952,18 +2192,36 @@ def elu(x, alpha=1.):\n \n def softmax(x):\n     \"\"\"Softmax of a tensor.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.nn.softmax(x)\n \n \n def softplus(x):\n     \"\"\"Softplus of a tensor.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.nn.softplus(x)\n \n \n def softsign(x):\n     \"\"\"Softsign of a tensor.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.nn.softsign(x)\n \n@@ -2040,6 +2298,12 @@ def binary_crossentropy(output, target, from_logits=False):\n \n def sigmoid(x):\n     \"\"\"Element-wise sigmoid.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.nn.sigmoid(x)\n \n@@ -2047,6 +2311,14 @@ def sigmoid(x):\n def hard_sigmoid(x):\n     \"\"\"Segment-wise linear approximation of sigmoid.\n     Faster than sigmoid.\n+    Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n+    In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     x = (0.2 * x) + 0.5\n     zero = _to_tensor(0., x.dtype.base_dtype)\n@@ -2057,6 +2329,12 @@ def hard_sigmoid(x):\n \n def tanh(x):\n     \"\"\"Element-wise tanh.\n+\n+    # Arguments\n+        x: A tensor or variable.\n+\n+    # Returns\n+        A tensor.\n     \"\"\"\n     return tf.nn.tanh(x)\n \n@@ -2093,13 +2371,13 @@ def in_top_k(predictions, targets, k):\n     \"\"\"Returns whether the `targets` are in the top `k` `predictions`\n \n     # Arguments\n-        predictions: A tensor of shape batch_size x classess and type float32.\n-        targets: A tensor of shape batch_size and type int32 or int64.\n-        k: An int, number of top elements to consider.\n+        predictions: A tensor of shape `batch_size` x classes and type `float32`.\n+        targets: A tensor of shape batch_size and type `int32` or `int64`.\n+        k: An `int`, number of top elements to consider.\n \n     # Returns\n-        A tensor of shape batch_size and type bool. output_i is True if\n-        targets_i is within top-k values of predictions_i\n+        A tensor of shape `batch_size` and type `bool`. `output_i` is `True` if\n+        `targets_i` is within top-k values of `predictions_i`\n     \"\"\"\n     return tf.nn.in_top_k(predictions, targets, k)\n \n@@ -2198,7 +2476,10 @@ def conv1d(x, kernel, stride=1, border_mode='valid',\n     # Arguments\n         kernel: kernel tensor.\n         strides: stride integer.\n-        border_mode: string, \"same\" or \"valid\".\n+        border_mode: string, `\"same\"` or `\"valid\"`.\n+\n+    # Returns\n+        A tensor, result of 1D convolution.\n     \"\"\"\n     # pre-process dtype\n     x_dtype = dtype(x)\n@@ -2221,10 +2502,13 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid',\n     # Arguments\n         kernel: kernel tensor.\n         strides: strides tuple.\n-        border_mode: string, \"same\" or \"valid\".\n-        dim_ordering: \"tf\" or \"th\".\n+        border_mode: string, `\"same\"` or `\"valid\"`.\n+        dim_ordering: `\"tf\"` or `\"th\"`.\n             Whether to use Theano or TensorFlow dimension ordering\n             for inputs/kernels/ouputs.\n+\n+    # Returns\n+        A tensor, result of 2D convolution.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -2255,8 +2539,8 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n         kernel: kernel tensor.\n         output_shape: 1D int tensor for the output shape.\n         strides: strides tuple.\n-        border_mode: string, \"same\" or \"valid\".\n-        dim_ordering: \"tf\" or \"th\".\n+        border_mode: string, `\"same\"` or `\"valid\"`.\n+        dim_ordering: `\"tf\"` or `\"th\"`.\n             Whether to use Theano or TensorFlow dimension ordering\n             for inputs/kernels/ouputs.\n     \"\"\"\n@@ -2281,6 +2565,19 @@ def atrous_conv2d(x, kernel, rate=1,\n                   border_mode='valid',\n                   dim_ordering='default',\n                   image_shape=None, filter_shape=None):\n+    \"\"\"Atrous 2D convolution. Also as known as dilated convolution.\n+\n+    # Arguments\n+        x: input tensor.\n+        kernel: kernel tensor.\n+        rate: integer > 0, the sample stride.\n+        output_shape: 1D int tensor for the output shape.\n+        strides: strides tuple.\n+        border_mode: string, `\"same\"` or `\"valid\"`.\n+        dim_ordering: `\"tf\"` or `\"th\"`.\n+            Whether to use Theano or TensorFlow dimension ordering\n+            for inputs/kernels/ouputs.\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2299,6 +2596,8 @@ def atrous_conv2d(x, kernel, rate=1,\n \n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      border_mode='valid', dim_ordering='default'):\n+    \"\"\"2-D convolution with separable filters.\n+    \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n     if dim_ordering not in {'th', 'tf'}:\n@@ -2325,8 +2624,8 @@ def conv3d(x, kernel, strides=(1, 1, 1),\n     # Arguments\n         kernel: kernel tensor.\n         strides: strides tuple.\n-        border_mode: string, \"same\" or \"valid\".\n-        dim_ordering: \"tf\" or \"th\".\n+        border_mode: string, `\"same\"` or `\"valid\"`.\n+        dim_ordering: `\"tf\"` or `\"th\"`.\n             Whether to use Theano or TensorFlow dimension ordering\n             for inputs/kernels/ouputs.\n     \"\"\"\n@@ -2352,9 +2651,9 @@ def pool2d(x, pool_size, strides=(1, 1),\n     # Arguments\n         pool_size: tuple of 2 integers.\n         strides: tuple of 2 integers.\n-        border_mode: one of \"valid\", \"same\".\n-        dim_ordering: one of \"th\", \"tf\".\n-        pool_mode: one of \"max\", \"avg\".\n+        border_mode: one of `\"valid\"`, `\"same\"`.\n+        dim_ordering: one of `\"th\"`, `\"tf\"`.\n+        pool_mode: one of `\"max\"`, `\"avg\"`.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -2384,9 +2683,9 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n     # Arguments\n         pool_size: tuple of 3 integers.\n         strides: tuple of 3 integers.\n-        border_mode: one of \"valid\", \"same\".\n-        dim_ordering: one of \"th\", \"tf\".\n-        pool_mode: one of \"max\", \"avg\".\n+        border_mode: one of `\"valid\"`, `\"same\"`.\n+        dim_ordering: one of `\"th\"`, `\"tf\"`.\n+        pool_mode: one of `\"max\"`, `\"avg\"`.\n     \"\"\"\n     if dim_ordering == 'default':\n         dim_ordering = image_dim_ordering()\n@@ -2412,6 +2711,16 @@ def pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid',\n # RANDOMNESS\n \n def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n+    \"\"\"Returns a tensor with normal distribution\n+\n+    # Arguments\n+        shape: A tuple of integers, the shape of tensor to create.\n+        mean: A float, mean of the normal distribution to draw samples.\n+        std: A float, standard deviation of the normal distribution\n+            to draw samples.\n+        dtype: String, dtype of returned tensor.\n+        seed: Integer, random seed.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if seed is None:\n@@ -2421,6 +2730,17 @@ def random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None):\n \n \n def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n+    \"\"\"Returns a tensor with uniform distribution\n+\n+    # Arguments\n+        shape: A tuple of integers, the shape of tensor to create.\n+        low: A float, lower boundary of the uniform distribution\n+            to draw samples.\n+        high: A float, upper boundary of the uniform distribution\n+            to draw samples.\n+        dtype: String, dtype of returned tensor.\n+        seed: Integer, random seed.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if seed is None:\n@@ -2430,6 +2750,14 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None):\n \n \n def random_binomial(shape, p=0.0, dtype=None, seed=None):\n+    \"\"\"Returns a tensor with binomlai distribution\n+\n+    # Arguments\n+        shape: A tuple of integers, the shape of tensor to create.\n+        p: A float, `0. <= p <= 1`, probability of binomlai distribution.\n+        dtype: String, dtype of returned tensor.\n+        seed: Integer, random seed.\n+    \"\"\"\n     if dtype is None:\n         dtype = floatx()\n     if seed is None:\n@@ -2479,13 +2807,13 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     \"\"\"Runs CTC loss algorithm on each batch element.\n \n     # Arguments\n-        y_true: tensor (samples, max_string_length) containing the truth labels\n-        y_pred: tensor (samples, time_steps, num_categories) containing the prediction,\n-                or output of the softmax\n-        input_length: tensor (samples,1) containing the sequence length for\n-                each batch item in y_pred\n-        label_length: tensor (samples,1) containing the sequence length for\n-                each batch item in y_true\n+        y_true: tensor `(samples, max_string_length)` containing the truth labels.\n+        y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction,\n+                or output of the softmax.\n+        input_length: tensor `(samples, 1)` containing the sequence length for\n+                each batch item in `y_pred`.\n+        label_length: tensor `(samples, 1)` containing the sequence length for\n+                each batch item in `y_true`.\n \n     # Returns\n         Tensor with shape (samples,1) containing the\n@@ -2509,22 +2837,22 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n        search.\n \n     # Arguments\n-        y_pred: tensor (samples, time_steps, num_categories) containing the prediction,\n-                or output of the softmax\n-        input_length: tensor (samples,) containing the sequence length for\n-                each batch item in y_pred\n-        greedy: perform much faster best-path search if true.  This does\n+        y_pred: tensor `(samples, time_steps, num_categories)` containing the prediction,\n+                or output of the softmax.\n+        input_length: tensor `(samples, )` containing the sequence length for\n+                each batch item in `y_pred`.\n+        greedy: perform much faster best-path search if `true`. This does\n                 not use a dictionary\n-        beam_width: if greedy is false: a beam search decoder will be used\n+        beam_width: if `greedy` is `false`: a beam search decoder will be used\n                 with a beam of this width\n-        top_paths: if greedy is false: how many of the most probable paths will be returned\n+        top_paths: if `greedy` is `false`: how many of the most probable paths will be returned\n \n     # Returns\n         Tuple:\n-            List: if greedy is true, returns a list of one element that contains\n-                the decoded sequence. If false, returns the `top_paths` most probable\n-                decoded sequences. Important: blank labels are returned as -1\n-            Tensor (top_paths,) that contains the log probability of each decoded sequence\n+            List: if `greedy` is `true`, returns a list of one element that contains\n+                the decoded sequence. If `false`, returns the `top_paths` most probable\n+                decoded sequences. Important: blank labels are returned as `-1`.\n+            Tensor `(top_paths, )` that contains the log probability of each decoded sequence\n     \"\"\"\n     y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n     input_length = tf.to_int32(input_length)\n@@ -2567,9 +2895,9 @@ def foldl(fn, elems, initializer=None, name=None):\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems and an\n-            accumulator, for instance lambda acc, x: acc + x\n+            accumulator, for instance `lambda acc, x: acc + x`\n         elems: tensor\n-        initializer: The first value used (elems[0] in case of None)\n+        initializer: The first value used (`elems[0]` in case of None)\n         name: A string name for the foldl node in the graph\n \n     # Returns\n@@ -2583,9 +2911,9 @@ def foldr(fn, elems, initializer=None, name=None):\n \n     # Arguments\n         fn: Callable that will be called upon each element in elems and an\n-            accumulator, for instance lambda acc, x: acc + x\n+            accumulator, for instance `lambda acc, x: acc + x`\n         elems: tensor\n-        initializer: The first value used (elems[-1] in case of None)\n+        initializer: The first value used (`elems[-1]` in case of None)\n         name: A string name for the foldr node in the graph\n \n     # Returns\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
