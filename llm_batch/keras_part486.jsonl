{"custom_id": "keras#b4b65288c53ca8a6c1757de5507bfd476e783367", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1113 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -151,7 +151,7 @@ def load_model(filepath, custom_objects=None):\n             \"Invalid filename: expected a `.keras` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n-    saving_v3_enabled_value = _SAVING_V3_ENABLED.value\n+    saving_v3_enabled_value = getattr(_SAVING_V3_ENABLED, \"value\", False)\n     _SAVING_V3_ENABLED.value = True\n     temp_path = _get_temp_dir()\n     try:\n@@ -302,7 +302,7 @@ def save_model(model, filepath):\n             \"on some data.\",\n             stacklevel=2,\n         )\n-    saving_v3_enabled_value = _SAVING_V3_ENABLED.value\n+    saving_v3_enabled_value = getattr(_SAVING_V3_ENABLED, \"value\", False)\n     _SAVING_V3_ENABLED.value = True\n \n     serialized_model_dict = serialize_keras_object(model)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4122cac4f9e8388f0d2bd5eec7feed9fb26a69f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 766 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -82,8 +82,12 @@ class Conv1D(Conv):\n         where the model should not violate the temporal order.\n         See [WaveNet: A Generative Model for Raw Audio, section\n           2.1](https://arxiv.org/abs/1609.03499).\n-      data_format: A string,\n-        one of `channels_last` (default) or `channels_first`.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape `(batch_size, width,\n+        channels)` while `channels_first` corresponds to inputs with shape\n+        `(batch_size, channels, width)`. Note that the `channels_first` format\n+        is currently not supported on CPU.\n       dilation_rate: an integer or tuple/list of a single integer, specifying\n         the dilation rate to use for dilated convolution.\n         Currently, specifying any `dilation_rate` value != 1 is\n\n@@ -104,7 +104,8 @@ class Conv2D(Conv):\n         shape `(batch_size, channels, height, width)`. It defaults to the\n         `image_data_format` value found in your Keras config file at\n         `~/.keras/keras.json`. If you never set it, then it will be\n-        `channels_last`.\n+        `channels_last`. Note that the `channels_first` format is currently not\n+        supported on CPU.\n       dilation_rate: an integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n\n@@ -86,7 +86,8 @@ class Conv3D(Conv):\n         (channels, spatial_dim1, spatial_dim2, spatial_dim3)`. It defaults to\n         the `image_data_format` value found in your Keras config file at\n         `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+        \"channels_last\". Note that the `channels_first` format is currently not\n+        supported on CPU.\n       dilation_rate: an integer or tuple/list of 3 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f93c6e248a96de999c2f1fd1096f74c22577cec1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 772 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -87,7 +87,7 @@ class Conv1D(Conv):\n         `channels_last` corresponds to inputs with shape `(batch_size, width,\n         channels)` while `channels_first` corresponds to inputs with shape\n         `(batch_size, channels, width)`. Note that the `channels_first` format\n-        is currently not supported on CPU.\n+        is currently not supported by TensorFlow on CPU.\n       dilation_rate: an integer or tuple/list of a single integer, specifying\n         the dilation rate to use for dilated convolution.\n         Currently, specifying any `dilation_rate` value != 1 is\n\n@@ -105,7 +105,7 @@ class Conv2D(Conv):\n         `image_data_format` value found in your Keras config file at\n         `~/.keras/keras.json`. If you never set it, then it will be\n         `channels_last`. Note that the `channels_first` format is currently not\n-        supported on CPU.\n+        supported by TensorFlow on CPU.\n       dilation_rate: an integer or tuple/list of 2 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n\n@@ -87,7 +87,7 @@ class Conv3D(Conv):\n         the `image_data_format` value found in your Keras config file at\n         `~/.keras/keras.json`. If you never set it, then it will be\n         \"channels_last\". Note that the `channels_first` format is currently not\n-        supported on CPU.\n+        supported by TensorFlow on CPU.\n       dilation_rate: an integer or tuple/list of 3 integers, specifying the\n         dilation rate to use for dilated convolution. Can be a single integer to\n         specify the same value for all spatial dimensions. Currently, specifying\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#722350f75178bec4e5da0be36bab11e8caa911d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 94 | Files Changed: 3 | Hunks: 20 | Methods Changed: 12 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 158 | Churn Cumulative: 24942 | Contributors (this commit): 142 | Commits (past 90d): 44 | Contributors (cumulative): 161 | DMM Complexity: 1.0\n\nDIFF:\n@@ -479,39 +479,11 @@ class Sequential(functional.Functional):\n             model.add(layer)\n \n         if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n-            # Grab the information from the `config` for `compile()` and\n-            # `build()`.\n-            is_compiled = config.pop(\"is_compiled\", False)\n-            optimizer, loss = None, None\n-            optimizer_dict = config.pop(\"optimizer\", {})\n-            if optimizer_dict:\n-                optimizer = saving_lib.deserialize_keras_object(\n-                    optimizer_dict, custom_objects\n+            compile_config = config.get(\"compile_config\", None)\n+            if compile_config is not None:\n+                model._compile_from_config(\n+                    compile_config, base_class=Sequential\n                 )\n-            loss_dict = config.pop(\"loss\", {})\n-            if loss_dict:\n-                loss = saving_lib.deserialize_keras_object(\n-                    loss_dict, custom_objects\n-                )\n-\n-            has_overridden_compile = cls.compile != Sequential.compile\n-            has_overridden_from_config = (\n-                cls.from_config.__func__.__qualname__\n-                != Sequential.from_config.__func__.__qualname__\n-            )\n-            if has_overridden_compile and (not has_overridden_from_config):\n-                logging.warning(\n-                    \"`compile()` was not called as part of model loading \"\n-                    \"because the model's `compile()` method is custom. \"\n-                    \"All subclassed Models that have `compile()` \"\n-                    \"overridden should also override `from_config()` in order \"\n-                    \"to call `compile()`. Alternatively, you can call \"\n-                    \"`compile()` manually after loading.\"\n-                )\n-\n-            if (not has_overridden_compile) and is_compiled:\n-                # TODO(rchao): Handle other compile args.\n-                model.compile(optimizer=optimizer, loss=loss)\n \n         if (\n             not model.inputs\n\n@@ -701,6 +701,16 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             **kwargs: Arguments supported for backwards compatibility only.\n         \"\"\"\n         base_layer.keras_api_gauge.get_cell(\"compile\").set(True)\n+        self._compile_config = CompileConfig(\n+            optimizer=optimizer,\n+            loss=loss,\n+            metrics=metrics,\n+            loss_weights=loss_weights,\n+            weighted_metrics=weighted_metrics,\n+            run_eagerly=run_eagerly,\n+            steps_per_execution=steps_per_execution,\n+            jit_compile=jit_compile,\n+        )\n         with self.distribute_strategy.scope():\n             if \"experimental_steps_per_execution\" in kwargs:\n                 logging.warning(\n@@ -3072,40 +3082,17 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # they don't override `from_config()`, which would use `cls(**config)`\n         # as a result.\n         config = {}\n-\n         if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n-            config[\"is_compiled\"] = self._is_compiled\n-            if self.optimizer:\n-                config[\"optimizer\"] = saving_lib.serialize_keras_object(\n-                    self.optimizer\n-                )\n-            if self.compiled_loss:\n-                config[\"loss\"] = saving_lib.serialize_keras_object(\n-                    self.compiled_loss\n-                )\n+            if self._is_compiled and hasattr(self, \"_compile_config\"):\n+                config[\"compile_config\"] = self._compile_config.serialize()\n             if self.built:\n-                config[\"input_shape\"] = self._build_input_shape\n-\n+                config[\"build_input_shape\"] = self._build_input_shape\n         return config\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n-\n-        # Grab the information from the `config` for `compile()` and\n-        # `build()`.\n-        is_compiled = config.pop(\"is_compiled\", False)\n-        optimizer, loss = None, None\n-        optimizer_dict = config.pop(\"optimizer\", {})\n-        if optimizer_dict:\n-            optimizer = saving_lib.deserialize_keras_object(\n-                optimizer_dict, custom_objects\n-            )\n-        loss_dict = config.pop(\"loss\", {})\n-        if loss_dict:\n-            loss = saving_lib.deserialize_keras_object(\n-                loss_dict, custom_objects\n-            )\n-        input_shape = config.pop(\"input_shape\", {})\n+        compile_config = config.pop(\"compile_config\", None)\n+        build_input_shape = config.pop(\"build_input_shape\", None)\n \n         # `from_config` assumes `cls` is either `Functional` or a child class of\n         # `Functional`. In the case that `cls` is meant to behave like a child\n@@ -3151,27 +3138,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                     )\n \n             if getattr(saving_lib._SAVING_V3_ENABLED, \"value\", False):\n-                has_overridden_compile = cls.compile != Model.compile\n-                has_overridden_from_config = (\n-                    cls.from_config.__func__.__qualname__\n-                    != Model.from_config.__func__.__qualname__\n-                )\n-\n-                if has_overridden_compile and (not has_overridden_from_config):\n-                    logging.warning(\n-                        \"`compile()` was not called as part of model loading \"\n-                        \"because the model's `compile()` method is custom. \"\n-                        \"All subclassed Models that have `compile()` \"\n-                        \"overridden should also override `from_config()` in \"\n-                        \"order to call `compile()`. Alternatively, you can \"\n-                        \"call `compile()` manually after loading.\"\n-                    )\n-                elif (not has_overridden_compile) and is_compiled:\n-                    # TODO(rchao): Handle other compile args.\n-                    model.compile(optimizer=optimizer, loss=loss)\n-\n-                if input_shape:\n-                    model.build(input_shape)\n+                if build_input_shape:\n+                    model.build(build_input_shape)\n+                if compile_config is not None:\n+                    model._compile_from_config(compile_config, base_class=Model)\n \n             return model\n \n@@ -3791,6 +3761,27 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 f\"type {type(validation_freq)}.\"\n             )\n \n+    def _compile_from_config(self, compile_config, base_class):\n+        has_overridden_compile = self.__class__.compile != base_class.compile\n+        has_overridden_from_config = (\n+            self.__class__.from_config.__func__.__qualname__\n+            != base_class.from_config.__func__.__qualname__\n+        )\n+\n+        if not has_overridden_compile:\n+            compile_config = saving_lib.deserialize_keras_object(compile_config)\n+            self.compile(**compile_config)\n+        else:\n+            if not has_overridden_from_config:\n+                logging.warning(\n+                    \"`compile()` was not called as part of model loading \"\n+                    \"because the model's `compile()` method is custom. \"\n+                    \"All subclassed Models that have `compile()` \"\n+                    \"overridden should also override `from_config()` in \"\n+                    \"order to call `compile()`. Alternatively, you can \"\n+                    \"call `compile()` manually after loading.\"\n+                )\n+\n     ######################################################################\n     # Functions below exist only as v1 / v2 compatibility shims.\n     ######################################################################\n@@ -3806,7 +3797,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           Dictionary of arguments that were used when compiling the model.\n         \"\"\"\n         self._assert_compile_was_called()\n-\n         saved_metrics = self.compiled_metrics._user_metrics\n         saved_weighted_metrics = self.compiled_metrics._user_weighted_metrics\n \n@@ -3823,7 +3813,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             \"weighted_metrics\": saved_weighted_metrics,\n             \"loss_weights\": self.compiled_loss._user_loss_weights,\n         }\n-\n         return compile_args\n \n     def _get_callback_model(self):\n@@ -4188,3 +4177,11 @@ def is_functional_model_init_params(args, kwargs):\n         or \"inputs\" in kwargs\n         and \"outputs\" in kwargs\n     )\n+\n+\n+class CompileConfig:\n+    def __init__(self, **config):\n+        self.config = config\n+\n+    def serialize(self):\n+        return saving_lib.serialize_keras_object(self.config)\n\n@@ -108,8 +108,8 @@ class CompileOverridingModel(keras.Model):\n         super().__init__(*args, **kwargs)\n         self.dense1 = MyDense(1)\n \n-    def compile(self, some_random_arg):\n-        pass\n+    def compile(self, *args, **kwargs):\n+        super().compile(*args, **kwargs)\n \n     def call(self, inputs):\n         return self.dense1(inputs)\n@@ -119,8 +119,8 @@ class CompileOverridingModel(keras.Model):\n     package=\"my_custom_package\"\n )\n class CompileOverridingSequential(keras.Sequential):\n-    def compile(self, some_random_arg):\n-        pass\n+    def compile(self, *args, **kwargs):\n+        super().compile(*args, **kwargs)\n \n \n @keras.utils.generic_utils.register_keras_serializable(\n@@ -337,19 +337,17 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n             config_dict[\"registered_name\"], \"my_custom_package>CustomModelX\"\n         )\n         self.assertEqual(\n-            config_dict[\"config\"][\"optimizer\"][\"module\"],\n+            config_dict[\"config\"][\"compile_config\"][\"optimizer\"][\"module\"],\n             \"keras.optimizers.experimental\",\n         )\n         self.assertEqual(\n-            config_dict[\"config\"][\"optimizer\"][\"class_name\"],\n+            config_dict[\"config\"][\"compile_config\"][\"optimizer\"][\"class_name\"],\n             \"Adam\",\n         )\n+        self.assertLen(config_dict[\"config\"][\"compile_config\"][\"loss\"], 4)\n         self.assertEqual(\n-            config_dict[\"config\"][\"loss\"][\"module\"],\n-            \"keras.engine.compile_utils\",\n-        )\n-        self.assertEqual(\n-            config_dict[\"config\"][\"loss\"][\"class_name\"], \"LossesContainer\"\n+            config_dict[\"config\"][\"compile_config\"][\"loss\"][0],\n+            \"mse\",\n         )\n \n     @tf.__internal__.distribute.combinations.generate(\n@@ -471,10 +469,13 @@ class SavingV3Test(tf.test.TestCase, parameterized.TestCase):\n                 [keras.layers.Embedding(4, 1), MyDense(1), MyDense(1)]\n             )\n         )\n+        model.compile(\"rmsprop\", \"mse\")\n         model._save_experimental(temp_filepath)\n \n         with mock.patch.object(logging, \"warning\") as mock_warn:\n             saving_lib.load_model(temp_filepath)\n+        if not mock_warn.call_args_list:\n+            raise AssertionError(\"Did not warn.\")\n         self.assertIn(\n             \"`compile()` was not called as part of model loading \"\n             \"because the model's `compile()` method is custom. \",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#417094d9bdf1788db29e16b6e1e02b0f0c0bf916", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2176 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -89,7 +89,7 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n       unit_forget_bias: Boolean (default `True`). If True, add 1 to the bias of\n         the forget gate at initialization. Setting it to true will also force\n         `bias_initializer=\"zeros\"`. This is recommended in [Jozefowicz et\n-          al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+          al.](https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)\n       kernel_regularizer: Regularizer function applied to the `kernel` weights\n         matrix. Default: `None`.\n       recurrent_regularizer: Regularizer function applied to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4688836a0556d5580f55d463d4e61e6e68e91b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 2 | Churn Cumulative: 9350 | Contributors (this commit): 111 | Commits (past 90d): 16 | Contributors (cumulative): 111 | DMM Complexity: None\n\nDIFF:\n@@ -2025,7 +2025,7 @@ class EarlyStopping(Callback):\n \n     def on_epoch_end(self, epoch, logs=None):\n         current = self.get_monitor_value(logs)\n-        if current is None or epoch <= self.start_from_epoch:\n+        if current is None or epoch < self.start_from_epoch:\n             # If no monitor value exists or still in initial warm-up stage.\n             return\n         if self.restore_best_weights and self.best_weights is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#814092c71159746dc63f1efc38b3dc620f400f85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 46 | Churn Cumulative: 754 | Contributors (this commit): 7 | Commits (past 90d): 23 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -497,19 +497,22 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n         x1 = tf.Variable(np.ones([10]), dtype=tf.float64)\n         x2 = tf.Variable(np.ones([10]), dtype=tf.float64)\n         grads = tf.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n+        first_grads = tf.constant([0.01] * 10, dtype=tf.float64)\n         sparse_grads = tf.IndexedSlices(\n             tf.convert_to_tensor([0, 0.2, 0.4, 0.8, 0.8], dtype=tf.float64),\n             tf.convert_to_tensor([0, 2, 4, 6, 6]),\n             dense_shape=tf.convert_to_tensor([len(grads)]),\n         )\n \n+        old_optimizer.apply_gradients(zip([first_grads], [x1]))\n+        new_optimizer.apply_gradients(zip([first_grads], [x2]))\n         for _ in range(5):\n-            self.assertAllClose(x1, x2)\n+            self.assertAllClose(x1, x2, rtol=5e-4, atol=5e-4)\n             old_optimizer.apply_gradients(zip([grads], [x1]))\n             new_optimizer.apply_gradients(zip([grads], [x2]))\n \n         for _ in range(5):\n-            self.assertAllClose(x1, x2)\n+            self.assertAllClose(x1, x2, rtol=5e-4, atol=5e-4)\n             old_optimizer.apply_gradients(zip([sparse_grads], [x1]))\n             new_optimizer.apply_gradients(zip([sparse_grads], [x2]))\n \n@@ -530,7 +533,10 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n         self._compare_numerical(ftrl_old.Ftrl(), ftrl_new.Ftrl())\n \n     def testRMSprop(self):\n-        self._compare_numerical(rmsprop_old.RMSprop(), rmsprop_new.RMSprop())\n+        self._compare_numerical(\n+            rmsprop_old.RMSprop(centered=True),\n+            rmsprop_new.RMSprop(centered=True),\n+        )\n \n     @parameterized.product(nesterov=[True, False])\n     def testSgd(self, nesterov):\n\n@@ -161,35 +161,35 @@ class RMSprop(optimizer.Optimizer):\n                         gradient.values * (1 - rho), gradient.indices\n                     )\n                 )\n-                velocity.assign_add(-tf.square(average_grad))\n-            velocity_value = tf.gather(velocity, gradient.indices)\n-            transformed_grad = tf.IndexedSlices(\n-                gradient.values / (tf.sqrt(velocity_value) + self.epsilon),\n+                denominator = velocity - tf.square(average_grad) + self.epsilon\n+            else:\n+                denominator = velocity + self.epsilon\n+            denominator_slices = tf.gather(denominator, gradient.indices)\n+            increment = tf.IndexedSlices(\n+                lr * gradient.values * tf.math.rsqrt(denominator_slices),\n                 gradient.indices,\n             )\n \n             if self.momentum > 0:\n                 momentum.assign(self.momentum * momentum)\n-                momentum.scatter_add(transformed_grad)\n-                variable.assign_add(-lr * momentum)\n+                momentum.scatter_add(increment)\n+                variable.assign_add(-momentum)\n             else:\n-                variable.scatter_add(\n-                    tf.IndexedSlices(\n-                        -lr * transformed_grad.values, transformed_grad.indices\n-                    )\n-                )\n+                variable.scatter_add(-increment)\n         else:\n             # Dense gradients.\n             velocity.assign(rho * velocity + (1 - rho) * tf.square(gradient))\n             if self.centered:\n                 average_grad.assign(rho * average_grad + (1 - rho) * gradient)\n-                velocity.assign_add(-tf.square(average_grad))\n-            transformed_grad = gradient / (tf.sqrt(velocity) + self.epsilon)\n-            if self.momentum > 0:\n-                momentum.assign(self.momentum * momentum + transformed_grad)\n-                variable.assign_add(-lr * momentum)\n+                denominator = velocity - tf.square(average_grad) + self.epsilon\n             else:\n-                variable.assign_add(-lr * transformed_grad)\n+                denominator = velocity + self.epsilon\n+            increment = lr * gradient * tf.math.rsqrt(denominator)\n+            if self.momentum > 0:\n+                momentum.assign(self.momentum * momentum + increment)\n+                variable.assign_add(-momentum)\n+            else:\n+                variable.assign_add(-increment)\n \n     def get_config(self):\n         config = super().get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01ba4945d34c37ad7d8bf3a6395498c25a4e149e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 270 | Files Changed: 5 | Hunks: 33 | Methods Changed: 22 | Complexity Δ (Sum/Max): -9/5 | Churn Δ: 476 | Churn Cumulative: 13420 | Contributors (this commit): 37 | Commits (past 90d): 52 | Contributors (cumulative): 52 | DMM Complexity: 0.6229508196721312\n\nDIFF:\n@@ -41,7 +41,6 @@ from keras.engine import node as node_module\n from keras.mixed_precision import autocast_variable\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import policy\n-from keras.saving.experimental import saving_lib\n from keras.saving.saved_model import layer_serialization\n from keras.utils import generic_utils\n from keras.utils import layer_utils\n@@ -3409,18 +3408,14 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             # TODO(rchao): Store non-variable states in the dict as well.\n             if isinstance(child_obj, tf.Variable):\n                 result[child_attr] = child_obj.numpy()\n-            elif saving_lib.is_container(child_obj):\n+            elif isinstance(child_obj, (list, tuple)):\n                 for k, contained_obj in enumerate(child_obj):\n                     if isinstance(contained_obj, tf.Variable):\n-                        # Handling the case where `child_obj` is a list/tuple.\n                         result[f\"{child_attr}-{k}\"] = contained_obj.numpy()\n-                    elif isinstance(child_obj, dict) and isinstance(\n-                        child_obj[contained_obj], tf.Variable\n-                    ):\n-                        # Handling the case where `child_obj` is a dict.\n-                        result[f\"{child_attr}-{contained_obj}\"] = child_obj[\n-                            contained_obj\n-                        ].numpy()\n+            elif isinstance(child_obj, dict):\n+                for k, v in child_obj.items():\n+                    if isinstance(v, tf.Variable):\n+                        result[f\"{child_attr}-{k}\"] = v.numpy()\n         return result\n \n     def _set_state(self, state):\n@@ -3430,18 +3425,14 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             # TODO(rchao): Give a warning for mismatches.\n             if isinstance(child_obj, tf.Variable):\n                 child_obj.assign(state[child_attr])\n-            elif saving_lib.is_container(child_obj):\n+            elif isinstance(child_obj, (list, tuple)):\n                 for k, contained_obj in enumerate(child_obj):\n                     if isinstance(contained_obj, tf.Variable):\n-                        # Handling the case where `child_obj` is a list/tuple.\n                         contained_obj.assign(state[f\"{child_attr}-{k}\"])\n-                    elif isinstance(child_obj, dict) and isinstance(\n-                        child_obj[contained_obj], tf.Variable\n-                    ):\n-                        # Handling the case where `child_obj` is a dict.\n-                        child_obj[contained_obj].assign(\n-                            state[f\"{child_attr}-{contained_obj}\"]\n-                        )\n+            elif isinstance(child_obj, dict):\n+                for k, v in child_obj.items():\n+                    if isinstance(v, tf.Variable):\n+                        child_obj[k].assign(state[f\"{child_attr}-{k}\"])\n \n     def _save_state(self, dirpath):\n         filepath = tf.io.gfile.join(dirpath, \"weights.npz\")\n\n@@ -497,22 +497,19 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n         x1 = tf.Variable(np.ones([10]), dtype=tf.float64)\n         x2 = tf.Variable(np.ones([10]), dtype=tf.float64)\n         grads = tf.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n-        first_grads = tf.constant([0.01] * 10, dtype=tf.float64)\n         sparse_grads = tf.IndexedSlices(\n             tf.convert_to_tensor([0, 0.2, 0.4, 0.8, 0.8], dtype=tf.float64),\n             tf.convert_to_tensor([0, 2, 4, 6, 6]),\n             dense_shape=tf.convert_to_tensor([len(grads)]),\n         )\n \n-        old_optimizer.apply_gradients(zip([first_grads], [x1]))\n-        new_optimizer.apply_gradients(zip([first_grads], [x2]))\n         for _ in range(5):\n-            self.assertAllClose(x1, x2, rtol=5e-4, atol=5e-4)\n+            self.assertAllClose(x1, x2)\n             old_optimizer.apply_gradients(zip([grads], [x1]))\n             new_optimizer.apply_gradients(zip([grads], [x2]))\n \n         for _ in range(5):\n-            self.assertAllClose(x1, x2, rtol=5e-4, atol=5e-4)\n+            self.assertAllClose(x1, x2)\n             old_optimizer.apply_gradients(zip([sparse_grads], [x1]))\n             new_optimizer.apply_gradients(zip([sparse_grads], [x2]))\n \n@@ -533,10 +530,7 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n         self._compare_numerical(ftrl_old.Ftrl(), ftrl_new.Ftrl())\n \n     def testRMSprop(self):\n-        self._compare_numerical(\n-            rmsprop_old.RMSprop(centered=True),\n-            rmsprop_new.RMSprop(centered=True),\n-        )\n+        self._compare_numerical(rmsprop_old.RMSprop(), rmsprop_new.RMSprop())\n \n     @parameterized.product(nesterov=[True, False])\n     def testSgd(self, nesterov):\n\n@@ -161,35 +161,35 @@ class RMSprop(optimizer.Optimizer):\n                         gradient.values * (1 - rho), gradient.indices\n                     )\n                 )\n-                denominator = velocity - tf.square(average_grad) + self.epsilon\n-            else:\n-                denominator = velocity + self.epsilon\n-            denominator_slices = tf.gather(denominator, gradient.indices)\n-            increment = tf.IndexedSlices(\n-                lr * gradient.values * tf.math.rsqrt(denominator_slices),\n+                velocity.assign_add(-tf.square(average_grad))\n+            velocity_value = tf.gather(velocity, gradient.indices)\n+            transformed_grad = tf.IndexedSlices(\n+                gradient.values / (tf.sqrt(velocity_value) + self.epsilon),\n                 gradient.indices,\n             )\n \n             if self.momentum > 0:\n                 momentum.assign(self.momentum * momentum)\n-                momentum.scatter_add(increment)\n-                variable.assign_add(-momentum)\n+                momentum.scatter_add(transformed_grad)\n+                variable.assign_add(-lr * momentum)\n             else:\n-                variable.scatter_add(-increment)\n+                variable.scatter_add(\n+                    tf.IndexedSlices(\n+                        -lr * transformed_grad.values, transformed_grad.indices\n+                    )\n+                )\n         else:\n             # Dense gradients.\n             velocity.assign(rho * velocity + (1 - rho) * tf.square(gradient))\n             if self.centered:\n                 average_grad.assign(rho * average_grad + (1 - rho) * gradient)\n-                denominator = velocity - tf.square(average_grad) + self.epsilon\n-            else:\n-                denominator = velocity + self.epsilon\n-            increment = lr * gradient * tf.math.rsqrt(denominator)\n+                velocity.assign_add(-tf.square(average_grad))\n+            transformed_grad = gradient / (tf.sqrt(velocity) + self.epsilon)\n             if self.momentum > 0:\n-                momentum.assign(self.momentum * momentum + increment)\n-                variable.assign_add(-momentum)\n+                momentum.assign(self.momentum * momentum + transformed_grad)\n+                variable.assign_add(-lr * momentum)\n             else:\n-                variable.assign_add(-increment)\n+                variable.assign_add(-lr * transformed_grad)\n \n     def get_config(self):\n         config = super().get_config()\n\n@@ -24,7 +24,6 @@ import warnings\n import zipfile\n \n import tensorflow.compat.v2 as tf\n-from absl import logging\n \n import keras\n from keras import losses\n@@ -45,223 +44,23 @@ _STATES_ROOT_DIRNAME = \"model\"\n _SAVING_V3_ENABLED = threading.local()\n _SAVING_V3_ENABLED.value = False\n \n-\n-def _print_archive(zipfile, action):\n-    # TODO(fchollet): move to debugging logs.\n-    io_utils.print_msg(f\"Keras model is being {action} an archive:\")\n-    # Same as `ZipFile.printdir()` except for using Keras' printing utility.\n-    io_utils.print_msg(\n-        \"%-46s %19s %12s\" % (\"File Name\", \"Modified    \", \"Size\")\n-    )\n-    for zinfo in zipfile.filelist:\n-        date = \"%d-%02d-%02d %02d:%02d:%02d\" % zinfo.date_time[:6]\n-        io_utils.print_msg(\n-            \"%-46s %s %12d\" % (zinfo.filename, date, zinfo.file_size)\n-        )\n-\n-\n-def _is_keras_trackable(object):\n-    from keras.metrics import base_metric  # To avoid circular import\n-\n-    return (\n-        isinstance(object, base_layer.Layer)\n-        or isinstance(object, optimizer.Optimizer)\n-        or isinstance(object, base_metric.Metric)\n-        or isinstance(object, losses.Loss)\n-    )\n-\n-\n-def is_container(object):\n-    return (\n-        isinstance(object, list)\n-        or isinstance(object, tuple)\n-        or isinstance(object, dict)\n-    )\n-\n-\n-def _extract_dir(zipfile_to_load, root_system_path, zip_dir):\n-    for zip_path in zipfile_to_load.namelist():\n-        if zip_path.startswith(zip_dir):\n-            created_path = zipfile_to_load.extract(zip_path, root_system_path)\n-            logging.debug(\n-                f\"Extracting {zip_path} into {root_system_path}. \"\n-                f\"Created {created_path}.\"\n-            )\n-\n-\n-def _load_state(trackable, zip_dirpath, temp_path, zipfile_to_load):\n-    states_dirpath = tf.io.gfile.join(zip_dirpath, _SELF_DIRNAME)\n-    # Extract the whole directory that represents the states of the trackable\n-    # into a temporary directory to be removed at the end.\n-    _extract_dir(zipfile_to_load, temp_path, states_dirpath)\n-    dirpath_to_load_state = tf.io.gfile.join(temp_path, states_dirpath)\n-    # TODO(rchao): Make `.set_state()` and `.load_state()` exported methods\n-    # and remove the attr check.\n-    if hasattr(trackable, \"_load_state\"):\n-        trackable._load_state(dirpath_to_load_state)\n-    if tf.io.gfile.exists(dirpath_to_load_state):\n-        tf.io.gfile.rmtree(dirpath_to_load_state)\n-\n-    # Recursively load states for Keras trackables such as layers/optimizers.\n-    for child_attr in dir(trackable):\n-        if (\n-            child_attr == \"_self_tracked_trackables\"\n-            or child_attr == \"_layer_call_argspecs\"\n-            or child_attr == \"_output_layers\"\n-        ):\n-            # Avoid certain attribute names to allow readable state file paths,\n-            # e.g., `layers`.\n-            continue\n-        try:\n-            child_obj = getattr(trackable, child_attr)\n-        except Exception:\n-            # Avoid raising exceptions when visiting attributes.\n-            continue\n-        if _is_keras_trackable(child_obj):\n-            _load_state(\n-                child_obj,\n-                tf.io.gfile.join(zip_dirpath, child_attr),\n-                temp_path,\n-                zipfile_to_load,\n-            )\n-        elif is_container(child_obj):\n-            _load_container_state(\n-                child_obj,\n-                tf.io.gfile.join(zip_dirpath, child_attr),\n-                temp_path,\n-                zipfile_to_load,\n-            )\n-\n-\n-def _load_container_state(container, zip_dirpath, temp_path, zipfile_to_load):\n-    for trackable in container:\n-        if _is_keras_trackable(trackable):\n-            _load_state(\n-                trackable,\n-                tf.io.gfile.join(zip_dirpath, trackable.name),\n-                temp_path,\n-                zipfile_to_load,\n-            )\n-\n-\n-def load_model(filepath, custom_objects=None):\n-    \"\"\"Load a zip archive representing a Keras model.\"\"\"\n-    if not filepath.endswith(\".keras\"):\n-        raise ValueError(\n-            \"Invalid filename: expected a `.keras` extension. \"\n-            f\"Received: filepath={filepath}\"\n-        )\n-    saving_v3_enabled_value = getattr(_SAVING_V3_ENABLED, \"value\", False)\n-    _SAVING_V3_ENABLED.value = True\n-    temp_path = _get_temp_dir()\n-    try:\n-        with zipfile.ZipFile(filepath, \"r\") as zipfile_to_load:\n-            _print_archive(zipfile_to_load, \"loaded from\")\n-            with zipfile_to_load.open(_CONFIG_FILENAME, \"r\") as c:\n-                config_json = c.read()\n-            logging.debug(f\"Read config: {config_json} from {c}\")\n-            # Note: we should NOT use a custom JSON decoder. Anything that\n-            # needs custom decoding must be handled in deserialize_keras_object.\n-            config_dict = json.loads(config_json)\n-            # Construct the model from the configuration file in the archive.\n-            model = deserialize_keras_object(config_dict, custom_objects)\n-            _load_state(model, _STATES_ROOT_DIRNAME, temp_path, zipfile_to_load)\n-    except Exception as e:\n-        raise e\n-    else:\n-        return model\n-    finally:\n-        _SAVING_V3_ENABLED.value = saving_v3_enabled_value\n-        if tf.io.gfile.exists(temp_path):\n-            tf.io.gfile.rmtree(temp_path)\n-\n-\n-def _write_recursively(zipfile_to_save, system_path, zip_path):\n-    if not tf.io.gfile.isdir(system_path):\n-        zipfile_to_save.write(system_path, zip_path)\n-        logging.debug(f\"Written {system_path} into {zip_path} in the zip.\")\n-    else:\n-        for file_name in tf.io.gfile.listdir(system_path):\n-            system_file_path = tf.io.gfile.join(system_path, file_name)\n-            zip_file_path = tf.io.gfile.join(zip_path, file_name)\n-            _write_recursively(zipfile_to_save, system_file_path, zip_file_path)\n-\n-\n-def _save_state(\n-    trackable, zip_dirpath, temp_path, zipfile_to_save, saved_trackables\n-):\n-    # Check whether this trackable has been saved; if so, do not duplicate the\n-    # saving.\n-    if trackable in saved_trackables:\n-        return\n-\n-    # TODO(rchao): Make `.get_state()` and `.save_state()` exported methods\n-    # and remove the attr check.\n-    if hasattr(trackable, \"_save_state\"):\n-        # Designate a `self` directory for the trackable object to save.\n-        states_dirpath = tf.io.gfile.join(temp_path, _SELF_DIRNAME)\n-        if not tf.io.gfile.exists(states_dirpath):\n-            tf.io.gfile.mkdir(states_dirpath)\n-        trackable._save_state(states_dirpath)\n-        if states_dirpath is not None:\n-            # Recursively write the states (represented by files inside the\n-            # directory) into the zip file.\n-            _write_recursively(\n-                zipfile_to_save,\n-                states_dirpath,\n-                tf.io.gfile.join(zip_dirpath, _SELF_DIRNAME),\n-            )\n-            tf.io.gfile.rmtree(states_dirpath)\n-        saved_trackables.add(trackable)\n-\n-    # Recursively ask contained trackable (layers, optimizers,\n-    # etc.) to save states.\n-    attr_skiplist = {\n+ATTR_SKIPLIST = frozenset(\n+    {\n         \"_self_tracked_trackables\",\n         \"_layer_call_argspecs\",\n+        \"_self_unconditional_dependency_names\",\n         \"_output_layers\",\n+        \"_input_layers\",\n+        \"submodules\",\n+        \"weights\",\n+        \"non_trainable_weights\",\n+        \"trainable_weights\",\n+        \"variables\",\n+        \"non_trainable_variables\",\n+        \"trainable_variables\",\n         \"updates\",  # Would raise a warning if visited.\n         \"state_updates\",  # Would raise a warning if visited.\n     }\n-    for child_attr in dir(trackable):\n-        if child_attr in attr_skiplist:\n-            # Avoid certain attribute names to allow readable state file paths,\n-            # e.g., `layers`.\n-            continue\n-        try:\n-            child_obj = getattr(trackable, child_attr)\n-        except Exception:\n-            # Avoid raising the exception when visiting the attributes.\n-            continue\n-        if _is_keras_trackable(child_obj):\n-            _save_state(\n-                child_obj,\n-                tf.io.gfile.join(zip_dirpath, child_attr),\n-                temp_path,\n-                zipfile_to_save,\n-                saved_trackables,\n-            )\n-        elif is_container(child_obj):\n-            _save_container_state(\n-                child_obj,\n-                tf.io.gfile.join(zip_dirpath, child_attr),\n-                temp_path,\n-                zipfile_to_save,\n-                saved_trackables,\n-            )\n-\n-\n-def _save_container_state(\n-    container, zip_dirpath, temp_path, zipfile_to_save, saved_trackables\n-):\n-    for trackable in container:\n-        if _is_keras_trackable(trackable):\n-            _save_state(\n-                trackable,\n-                tf.io.gfile.join(zip_dirpath, trackable.name),\n-                temp_path,\n-                zipfile_to_save,\n-                saved_trackables,\n )\n \n \n@@ -306,28 +105,31 @@ def save_model(model, filepath):\n     _SAVING_V3_ENABLED.value = True\n \n     serialized_model_dict = serialize_keras_object(model)\n-    config_json = json.dumps(serialized_model_dict).encode()\n+    config_json = json.dumps(serialized_model_dict)\n     # TODO(fchollet): consider saving dependencies list / versions in metadata.\n     metadata_json = json.dumps(\n         {\n             \"keras_version\": keras.__version__,\n             \"date_saved\": datetime.datetime.now().strftime(\"%Y-%m-%d@%H:%M:%S\"),\n         }\n-    ).encode()\n+    )\n \n-    # Utilize a temporary directory for the storing files prior to zipping.\n+    # Use a temporary directory for the storing files prior to zipping.\n     temp_path = _get_temp_dir()\n     try:\n-        # Save the configuration json and state files.\n-        with zipfile.ZipFile(filepath, \"x\") as zipfile_to_save:\n-            with zipfile_to_save.open(_METADATA_FILENAME, \"w\") as c:\n-                c.write(metadata_json)\n-            with zipfile_to_save.open(_CONFIG_FILENAME, \"w\") as c:\n-                c.write(config_json)\n+        # Write files locally before zipping.\n+        with open(tf.io.gfile.join(temp_path, _METADATA_FILENAME), \"w\") as f:\n+            f.write(metadata_json)\n+        with open(tf.io.gfile.join(temp_path, _CONFIG_FILENAME), \"w\") as f:\n+            f.write(config_json)\n         _save_state(\n-                model, _STATES_ROOT_DIRNAME, temp_path, zipfile_to_save, set()\n+            model, tf.io.gfile.join(temp_path, _STATES_ROOT_DIRNAME), set()\n         )\n-            _print_archive(zipfile_to_save, \"saved in\")\n+\n+        # Zip local files into an archive.\n+        with zipfile.ZipFile(filepath, \"w\") as zipfile_to_save:\n+            _write_recursively(zipfile_to_save, temp_path, \"\")\n+        _print_archive(zipfile_to_save, \"saving\")\n     except Exception as e:\n         raise e\n     finally:\n@@ -336,6 +138,128 @@ def save_model(model, filepath):\n         tf.io.gfile.rmtree(temp_path)\n \n \n+def load_model(filepath, custom_objects=None):\n+    \"\"\"Load a zip archive representing a Keras model.\"\"\"\n+    if not filepath.endswith(\".keras\"):\n+        raise ValueError(\n+            \"Invalid filename: expected a `.keras` extension. \"\n+            f\"Received: filepath={filepath}\"\n+        )\n+    saving_v3_enabled_value = getattr(_SAVING_V3_ENABLED, \"value\", False)\n+    _SAVING_V3_ENABLED.value = True\n+    temp_path = _get_temp_dir()\n+    try:\n+        with zipfile.ZipFile(filepath, \"r\") as zipfile_to_load:\n+            _print_archive(zipfile_to_load, \"loading\")\n+            zipfile_to_load.extractall(temp_path)\n+\n+        with open(tf.io.gfile.join(temp_path, _CONFIG_FILENAME), \"r\") as f:\n+            config_json = f.read()\n+        # Note: we should NOT use a custom JSON decoder. Anything that\n+        # needs custom decoding must be handled in deserialize_keras_object.\n+        config_dict = json.loads(config_json)\n+        # Construct the model from the configuration file in the archive.\n+        model = deserialize_keras_object(config_dict, custom_objects)\n+        _load_state(model, tf.io.gfile.join(temp_path, _STATES_ROOT_DIRNAME))\n+    except Exception as e:\n+        raise e\n+    else:\n+        return model\n+    finally:\n+        _SAVING_V3_ENABLED.value = saving_v3_enabled_value\n+        if tf.io.gfile.exists(temp_path):\n+            tf.io.gfile.rmtree(temp_path)\n+\n+\n+def _write_recursively(zipfile_to_save, system_path, zip_path):\n+    if not tf.io.gfile.isdir(system_path):\n+        zipfile_to_save.write(system_path, zip_path)\n+    else:\n+        for file_name in tf.io.gfile.listdir(system_path):\n+            system_file_path = tf.io.gfile.join(system_path, file_name)\n+            zip_file_path = tf.io.gfile.join(zip_path, file_name)\n+            _write_recursively(zipfile_to_save, system_file_path, zip_file_path)\n+\n+\n+def _save_state(trackable, temp_path, saved_trackables):\n+    # If the trackable has already been saved, skip it.\n+    if id(trackable) in saved_trackables:\n+        return\n+\n+    # TODO(rchao): Make `.get_state()` and `.save_state()` exported methods.\n+    if hasattr(trackable, \"_save_state\"):\n+        if not tf.io.gfile.exists(temp_path):\n+            tf.io.gfile.makedirs(temp_path)\n+        trackable._save_state(temp_path)\n+        saved_trackables.add(id(trackable))\n+\n+    # Recursively save state of children trackables (layers, optimizers, etc.)\n+    for child_attr in dir(trackable):\n+        if child_attr in ATTR_SKIPLIST:\n+            continue\n+        try:\n+            child_obj = getattr(trackable, child_attr)\n+        except Exception:\n+            # Avoid raising the exception when visiting the attributes.\n+            continue\n+        if _is_keras_trackable(child_obj):\n+            _save_state(\n+                child_obj,\n+                tf.io.gfile.join(temp_path, child_attr),\n+                saved_trackables,\n+            )\n+        elif isinstance(child_obj, (list, dict, tuple)):\n+            _save_container_state(\n+                child_obj,\n+                tf.io.gfile.join(temp_path, child_attr),\n+                saved_trackables,\n+            )\n+\n+\n+def _load_state(trackable, temp_path):\n+    if hasattr(trackable, \"_load_state\"):\n+        trackable._load_state(temp_path)\n+\n+    # Recursively load states for Keras trackables such as layers/optimizers.\n+    for child_attr in dir(trackable):\n+        if child_attr in ATTR_SKIPLIST:\n+            continue\n+        try:\n+            child_obj = getattr(trackable, child_attr)\n+        except Exception:\n+            # Avoid raising exceptions when visiting attributes.\n+            continue\n+        if _is_keras_trackable(child_obj):\n+            _load_state(\n+                child_obj,\n+                tf.io.gfile.join(temp_path, child_attr),\n+            )\n+        elif isinstance(child_obj, (list, dict, tuple)):\n+            _load_container_state(\n+                child_obj,\n+                tf.io.gfile.join(temp_path, child_attr),\n+            )\n+\n+\n+def _save_container_state(container, temp_path, saved_trackables):\n+    for trackable in container:\n+        if _is_keras_trackable(trackable):\n+            _save_state(\n+                trackable,\n+                tf.io.gfile.join(temp_path, trackable.name),\n+                saved_trackables,\n+            )\n+\n+\n+def _load_container_state(container, temp_path):\n+    for trackable in container:\n+        if _is_keras_trackable(trackable):\n+            _load_state(\n+                trackable,\n+                tf.io.gfile.join(temp_path, trackable.name),\n+            )\n+\n+\n def _get_temp_dir():\n     temp_dir = tempfile.mkdtemp()\n     try:\n@@ -351,3 +275,31 @@ def _get_temp_dir():\n         temp_dir = f\"ram://{uuid.uuid4()}\"\n         tf.io.gfile.mkdir(temp_dir)\n     return temp_dir\n+\n+\n+def _print_archive(zipfile, action):\n+    # TODO(fchollet): move to debugging logs.\n+    io_utils.print_msg(f\"Keras model {action}:\")\n+    # Same as `ZipFile.printdir()` except for using Keras' printing utility.\n+    io_utils.print_msg(\n+        \"%-46s %19s %12s\" % (\"File Name\", \"Modified    \", \"Size\")\n+    )\n+    for zinfo in zipfile.filelist:\n+        date = \"%d-%02d-%02d %02d:%02d:%02d\" % zinfo.date_time[:6]\n+        io_utils.print_msg(\n+            \"%-46s %s %12d\" % (zinfo.filename, date, zinfo.file_size)\n+        )\n+\n+\n+def _is_keras_trackable(obj):\n+    from keras.metrics import base_metric  # To avoid circular import\n+\n+    return isinstance(\n+        obj,\n+        (\n+            base_layer.Layer,\n+            optimizer.Optimizer,\n+            base_metric.Metric,\n+            losses.Loss,\n+        ),\n+    )\n\n@@ -51,9 +51,6 @@ def serialize_keras_object(obj):\n     if isinstance(obj, (list, tuple)):\n         return [serialize_keras_object(x) for x in obj]\n     if isinstance(obj, dict):\n-        if \"class_name\" in obj and \"config\" in obj:\n-            # Already serialized.\n-            return obj\n         return serialize_dict(obj)\n \n     # Special cases:\n@@ -84,6 +81,8 @@ def serialize_keras_object(obj):\n         else:\n             # Treat numpy floats / etc as plain types.\n             return obj.item()\n+    if isinstance(obj, tf.DType):\n+        return obj.name\n \n     # This gets the `keras.*` exported name, such as \"keras.optimizers.Adam\".\n     keras_api_name = tf_export.get_canonical_name_for_symbol(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
