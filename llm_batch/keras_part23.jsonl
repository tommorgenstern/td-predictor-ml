{"custom_id": "keras#848e1fb5c58fe034aceace7668a5fd39cec129c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 9 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 2079 | Contributors (this commit): 18 | Commits (past 90d): 87 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -88,24 +88,24 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n     else:\n         return np.ones(y.shape[:-1] + (1,))\n \n-def sequential_from_yaml(yamlString):\n+def sequential_from_yaml(yaml_string):\n     '''\n         Returns a compiled Sequential model generated from a local yaml file,\n         which is either created by hand or from Sequential.to_yaml\n     '''\n-    modelYaml = yaml.load(yamlString)\n+    model_yaml = yaml.load(yaml_string)\n     model = Sequential()\n \n-    class_mode = modelYaml.get('class_mode')\n-    theano_mode = modelYaml.get('theano_mode')\n-    loss = globals()[modelYaml.get('loss')]\n+    class_mode = model_yaml.get('class_mode')\n+    theano_mode = model_yaml.get('theano_mode')\n+    loss = globals()[model_yaml.get('loss')]\n \n-    optim = modelYaml.get('optimizer')\n-    optimName = optim.get('name')\n+    optim = model_yaml.get('optimizer')\n+    optim_name = optim.get('name')\n     optim.pop('name')\n-    optimizer = globals()[optimName](**optim)\n+    optimizer = globals()[optim_name](**optim)\n \n-    layers = modelYaml.get('layers')\n+    layers = model_yaml.get('layers')\n     for layer in layers:\n         name = layer.get('name')\n         layer.pop('name')\n@@ -119,15 +119,15 @@ def sequential_from_yaml(yamlString):\n                 vname = v.get('name')\n                 v.pop('name')\n                 layer[k] = globals()[vname](**v)\n-        initLayer = globals()[name](**layer)\n+        init_layer = globals()[name](**layer)\n         if hasParams:\n-            shapedParams = []\n+            shaped_params = []\n             for param in params:\n                 data = np.asarray(param.get('data'))\n                 shape = tuple(param.get('shape'))\n-                shapedParams.append(data.reshape(shape))\n-            initLayer.set_weights(shapedParams)\n-        model.add(initLayer)\n+                shaped_params.append(data.reshape(shape))\n+            init_layer.set_weights(shaped_params)\n+        model.add(init_layer)\n \n     model.compile(loss=loss, optimizer=optimizer, class_mode=class_mode, theano_mode=theano_mode)\n     return model\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#319e18ad8f104f66f45d4250678eff824a4e5b3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 2103 | Contributors (this commit): 18 | Commits (past 90d): 88 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -522,25 +522,25 @@ class Sequential(Model, containers.Sequential):\n         f.close()\n \n \n-    def to_yaml(self, storeParams=True):\n+    def to_yaml(self, store_params=True):\n         '''\n             Stores compiled Sequential model to yaml string, optionally storing all learnable parameters\n         '''\n-        modelDict = {}\n-        modelDict['class_mode'] = self.class_mode\n-        modelDict['theano_mode'] = self.theano_mode\n-        modelDict['loss'] = self.unweighted_loss.__name__\n-        modelDict['optimizer'] = self.optimizer.get_config()\n+        model_dict = {}\n+        model_dict['class_mode'] = self.class_mode\n+        model_dict['theano_mode'] = self.theano_mode\n+        model_dict['loss'] = self.unweighted_loss.__name__\n+        model_dict['optimizer'] = self.optimizer.get_config()\n \n         layers = []\n         for layer in self.layers:\n-            layerConf = layer.get_config()\n-            if storeParams:\n-                layerConf['parameters'] = [{'shape':list(param.get_value().shape), 'data':param.get_value().tolist()} for param in layer.params]\n-            layers.append(layerConf)\n-        modelDict['layers'] = layers\n+            layer_conf = layer.get_config()\n+            if store_params:\n+                layer_conf['parameters'] = [{'shape':list(param.get_value().shape), 'data':param.get_value().tolist()} for param in layer.params]\n+            layers.append(layer_conf)\n+        model_dict['layers'] = layers\n \n-        return yaml.dump(modelDict)\n+        return yaml.dump(model_dict)\n \n \n class Graph(Model, containers.Graph):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#48ea8dfb4748a57c90801f1cfd570b26996e365a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 15 | Churn Cumulative: 2118 | Contributors (this commit): 19 | Commits (past 90d): 89 | Contributors (cumulative): 19 | DMM Complexity: 0.8\n\nDIFF:\n@@ -23,6 +23,17 @@ def standardize_y(y):\n         y = np.expand_dims(y, 1)\n     return y\n \n+def batch_shuffle(index_array, batch_size):\n+    batch_count = int(len(index_array)/batch_size)\n+    # to reshape we need to be cleanly divisible by batch size\n+    # we stash extra items and reappend them after shuffling\n+    last_batch = index_array[batch_count*batch_size:]\n+    index_array = index_array[:batch_count*batch_size]\n+    index_array = index_array.reshape((batch_count, batch_size))\n+    np.random.shuffle(index_array)\n+    index_array = index_array.flatten()\n+    return np.append(index_array, last_batch)\n+\n \n def make_batches(size, batch_size):\n     nb_batch = int(np.ceil(size/float(batch_size)))\n@@ -120,7 +131,9 @@ class Model(object):\n         self.stop_training = False\n         for epoch in range(nb_epoch):\n             callbacks.on_epoch_begin(epoch)\n-            if shuffle:\n+            if shuffle == 'batch':\n+                index_array = batch_shuffle(index_array, batch_size)\n+            elif shuffle:\n                 np.random.shuffle(index_array)\n \n             batches = make_batches(nb_train_sample, batch_size)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2603fa37d2998a4179ec276e031183fe3e89bbd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 84 | Churn Cumulative: 84 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,84 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import numpy as np\n+np.random.seed(1337) # for reproducibility\n+\n+from keras.preprocessing import sequence\n+from keras.optimizers import RMSprop\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation, Flatten\n+from keras.layers.embeddings import Embedding\n+from keras.layers.convolutional import Convolution1D, MaxPooling1D\n+from keras.datasets import imdb\n+\n+# set parameters:\n+max_features = 5000\n+maxlen = 100\n+batch_size = 10\n+embedding_dims = 100\n+nb_filters = 250\n+filter_length = 3\n+hidden_dims = 250\n+nb_epochs = 10\n+\n+print(\"Loading data...\")\n+(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n+                                                      test_split=0.2)\n+print(len(X_train), 'train sequences')\n+print(len(X_test), 'test sequences')\n+\n+print(\"Pad sequences (samples x time)\")\n+X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n+X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n+print('X_train shape:', X_train.shape)\n+print('X_test shape:', X_test.shape)\n+\n+print('Build model...')\n+model = Sequential()\n+\n+# we start off with an efficient embedding layer which maps\n+# our vocab indices into embedding_dims dimensions\n+model.add(Embedding(max_features, embedding_dims))\n+\n+# we add a Convolution1D, which will learn nb_filters\n+# word group filters of size filter_length:\n+model.add(Convolution1D(input_dim=embedding_dims,\n+                        nb_filter=nb_filters,\n+                        filter_length=filter_length,\n+                        border_mode=\"valid\",\n+                        activation=\"relu\",\n+                        subsample_length=1))\n+\n+# we use standard max pooling (halving the output of the previous layer):\n+model.add(MaxPooling1D(pool_length=2))\n+\n+# We flatten the output of the conv layer, so that we can add a vanilla dense layer:\n+model.add(Flatten())\n+\n+# Computing the output shape of a conv layer can be tricky;\n+# for an good tutorial, see: http://cs231n.github.io/convolutional-networks/\n+output_size = nb_filters * (((maxlen - filter_length)/1)+1)/2\n+\n+# We add a vanilla hidden layer:\n+model.add(Dense(output_size, hidden_dims))\n+model.add(Dropout(0.5))\n+model.add(Activation('relu'))\n+\n+# We project onto a single unit output layer, and squash it with a sigmoid:\n+model.add(Dense(hidden_dims, 1))\n+model.add(Activation('sigmoid'))\n+\n+# we use some mild clipping to protect our gradients from vanishing:\n+rms = RMSprop(clipnorm=0.1)\n+model.compile(loss='binary_crossentropy', optimizer=rms, class_mode=\"binary\")\n+\n+# The performance of this toy model seems comparable to that of the LSTM in imdb_lstm.py,\n+# reaching an acceptable 83.16% on the test data at epoch 10.\n+# One epoch took around 268 seconds on a Quadro K600.\n+print(\"Train...\")\n+for e in range(nb_epochs):\n+    print(\">>> Epoch \", e+1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True)\n+    score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n+    print('\\t- Test loss:', score)\n+    print('\\t- Test accuracy:', acc)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22bd7cfab6ef749b033a79c364cd720755d6fbd3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 98 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,7 +19,7 @@ def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'),\n \n     for fname in os.listdir(path):\n         if 'sgm' in fname:\n-            s = open(path + fname).read()\n+            s = open(os.path.join(path, fname)).read()\n             tag = '<TOPICS>'\n             while tag in s:\n                 s = s[s.find(tag)+len(tag):]\n@@ -71,6 +71,9 @@ def make_reuters_dataset(path=os.path.join('datasets', 'temp', 'reuters21578'),\n     print('Sanity check:')\n     for w in [\"banana\", \"oil\", \"chocolate\", \"the\", \"dsft\"]:\n         print('...index of', w, ':', tokenizer.word_index.get(w))\n+    print('text reconstruction:')\n+    reverse_word_index = dict([(v, k) for k, v in tokenizer.word_index.items()])\n+    print(' '.join(reverse_word_index[i] for i in X[10]))\n \n     dataset = (X, labels) \n     print('-')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6899a9db16e04505b8a17770e7fb068a3c586347", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 118 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,15 +11,24 @@ from keras.layers.embeddings import Embedding\n from keras.layers.convolutional import Convolution1D, MaxPooling1D\n from keras.datasets import imdb\n \n+'''\n+    This example demonstrates the use of Convolution1D\n+    for text classification.\n+\n+    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python imdb_cnn.py\n+\n+    Get to 0.8330 test accuracy after 3 epochs. 100s/epoch on K520 GPU.\n+'''\n+\n # set parameters:\n max_features = 5000\n maxlen = 100\n-batch_size = 10\n+batch_size = 32\n embedding_dims = 100\n nb_filters = 250\n filter_length = 3\n hidden_dims = 250\n-nb_epochs = 10\n+nb_epoch = 3\n \n print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features,\n@@ -39,6 +48,7 @@ model = Sequential()\n # we start off with an efficient embedding layer which maps\n # our vocab indices into embedding_dims dimensions\n model.add(Embedding(max_features, embedding_dims))\n+model.add(Dropout(0.25))\n \n # we add a Convolution1D, which will learn nb_filters\n # word group filters of size filter_length:\n@@ -56,29 +66,17 @@ model.add(MaxPooling1D(pool_length=2))\n model.add(Flatten())\n \n # Computing the output shape of a conv layer can be tricky;\n-# for an good tutorial, see: http://cs231n.github.io/convolutional-networks/\n+# for a good tutorial, see: http://cs231n.github.io/convolutional-networks/\n output_size = nb_filters * (((maxlen - filter_length) / 1) + 1) / 2\n \n # We add a vanilla hidden layer:\n model.add(Dense(output_size, hidden_dims))\n-model.add(Dropout(0.5))\n+model.add(Dropout(0.25))\n model.add(Activation('relu'))\n \n # We project onto a single unit output layer, and squash it with a sigmoid:\n model.add(Dense(hidden_dims, 1))\n model.add(Activation('sigmoid'))\n \n-# we use some mild clipping to protect our gradients from vanishing:\n-rms = RMSprop(clipnorm=0.1)\n-model.compile(loss='binary_crossentropy', optimizer=rms, class_mode=\"binary\")\n-\n-# The performance of this toy model seems comparable to that of the LSTM in imdb_lstm.py,\n-# reaching an acceptable 83.16% on the test data at epoch 10.\n-# One epoch took around 268 seconds on a Quadro K600.\n-print(\"Train...\")\n-for e in range(nb_epochs):\n-    print(\">>> Epoch \", e+1)\n-    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=1, show_accuracy=True)\n-    score, acc = model.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True)\n-    print('\\t- Test loss:', score)\n-    print('\\t- Test accuracy:', acc)\n+model.compile(loss='binary_crossentropy', optimizer='rmsprop', class_mode=\"binary\")\n+model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, validation_data=(X_test, y_test))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8a2f46f798bd8e26fee401a4324a017ce0fb637", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94c930e99e8908d2188213672bed050b54ebdb5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 45 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/6 | Churn Δ: 58 | Churn Cumulative: 1559 | Contributors (this commit): 13 | Commits (past 90d): 69 | Contributors (cumulative): 15 | DMM Complexity: 0.55\n\nDIFF:\n@@ -425,12 +425,11 @@ class AutoEncoder(Layer):\n         If output_reconstruction then dim(input) = dim(output)\n         else dim(output) = dim(hidden)\n     '''\n-    def __init__(self, encoder, decoder, output_reconstruction=True, tie_weights=False, weights=None):\n+    def __init__(self, encoder, decoder, output_reconstruction=True, weights=None):\n \n         super(AutoEncoder, self).__init__()\n \n         self.output_reconstruction = output_reconstruction\n-        self.tie_weights = tie_weights\n         self.encoder = encoder\n         self.decoder = decoder\n \n@@ -440,11 +439,12 @@ class AutoEncoder(Layer):\n         self.regularizers = []\n         self.constraints = []\n         for layer in [self.encoder, self.decoder]:\n-            self.params += layer.params\n-            if hasattr(layer, 'regularizers'):\n-                self.regularizers += layer.regularizers\n-            if hasattr(layer, 'constraints'):\n-                self.constraints += layer.constraints\n+            params, regularizers, constraints = layer.get_params()\n+            self.constraints += constraints\n+            for p, r in zip(params, regularizers):\n+                if p not in self.params:\n+                    self.params.append(p)\n+                    self.regularizers.append(r)\n \n         if weights is not None:\n             self.set_weights(weights)\n@@ -477,23 +477,13 @@ class AutoEncoder(Layer):\n         if not train and not self.output_reconstruction:\n             return self.encoder.get_output(train)\n \n-        decoded = self.decoder.get_output(train)\n-\n-        if self.tie_weights:\n-            encoder_params = self.encoder.get_weights()\n-            decoder_params = self.decoder.get_weights()\n-            for dec_param, enc_param in zip(decoder_params, encoder_params):\n-                if len(dec_param.shape) > 1:\n-                    enc_param = dec_param.T\n-\n-        return decoded\n+        return self.decoder.get_output(train)\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n                 \"encoder_config\":self.encoder.get_config(),\n                 \"decoder_config\":self.decoder.get_config(),\n-                \"output_reconstruction\":self.output_reconstruction,\n-                \"tie_weights\":self.tie_weights}\n+                \"output_reconstruction\":self.output_reconstruction}\n \n \n \n\n@@ -2,7 +2,7 @@ from __future__ import absolute_import\n from __future__ import print_function\n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import DenoisingAutoEncoder, AutoEncoder, Dense, Activation, TimeDistributedDense, Flatten\n+from keras.layers.core import AutoEncoder, Dense, Activation, TimeDistributedDense, Flatten\n from keras.layers.recurrent import LSTM\n from keras.layers.embeddings import Embedding\n from keras.layers.core import Layer\n@@ -67,29 +67,13 @@ def build_lstm_autoencoder(autoencoder, X_train, X_test):\n \tautoencoder.add(TimeDistributedDense(input_dim, 16))\n \tautoencoder.add(AutoEncoder(encoder=LSTM(16, 8, activation=activation, return_sequences=True),\n \t\t\t\t\t\t\t\tdecoder=LSTM(8, input_dim, activation=activation, return_sequences=True),\n-\t\t\t\t\t\t\t\toutput_reconstruction=False, tie_weights=True))\n+\t\t\t\t\t\t\t\toutput_reconstruction=False))\n \treturn autoencoder, X_train, X_test\n \n def build_deep_classical_autoencoder(autoencoder):\n \tencoder = containers.Sequential([Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)])\n \tdecoder = containers.Sequential([Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)])\n-\tautoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False, tie_weights=True))\n-\treturn autoencoder\n-\n-def build_denoising_autoencoder(autoencoder):\n-\t# You need another layer before a denoising autoencoder\n-\t# This is similar to the dropout layers, etc..\n-\tautoencoder.add(Dense(input_dim, input_dim))\n-\tautoencoder.add(DenoisingAutoEncoder(encoder=Dense(input_dim, hidden_dim, activation=activation),\n-\t\t\t\t\t\t\t\t\t\t decoder=Dense(hidden_dim, input_dim, activation=activation),\n-\t\t\t\t\t\t\t\t\t\t output_reconstruction=False, tie_weights=True, corruption_level=0.3))\n-\treturn autoencoder\n-\n-def build_deep_denoising_autoencoder(autoencoder):\n-\tencoder = containers.Sequential([Dense(input_dim, hidden_dim, activation=activation), Dense(hidden_dim, hidden_dim/2, activation=activation)])\n-\tdecoder = containers.Sequential([Dense(hidden_dim/2, hidden_dim, activation=activation), Dense(hidden_dim, input_dim, activation=activation)])\n-\tautoencoder.add(Dense(input_dim, input_dim))\n-\tautoencoder.add(DenoisingAutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False, tie_weights=True))\n+\tautoencoder.add(AutoEncoder(encoder=encoder, decoder=decoder, output_reconstruction=False))\n \treturn autoencoder\n \n \n@@ -97,7 +81,7 @@ def build_deep_denoising_autoencoder(autoencoder):\n # Try different things here: 'lstm' or 'classical' or 'denoising'\n # or 'deep_denoising'\n \n-for autoencoder_type in ['classical', 'denoising', 'deep_denoising', 'lstm']:\n+for autoencoder_type in ['classical', 'lstm']:\n \tprint(autoencoder_type)\n \tprint('-'*40)\n \t# Build our autoencoder model\n@@ -105,12 +89,6 @@ for autoencoder_type in ['classical', 'denoising', 'deep_denoising', 'lstm']:\n \tif autoencoder_type == 'lstm':\n \t\tprint(\"Training LSTM AutoEncoder\")\n \t\tautoencoder, X_train, X_test = build_lstm_autoencoder(autoencoder, X_train, X_test)\n-\telif autoencoder_type == 'denoising':\n-\t\tprint(\"Training Denoising AutoEncoder\")\n-\t\tautoencoder = build_denoising_autoencoder(autoencoder)\n-\telif autoencoder_type == 'deep_denoising':\n-\t    print (\"Training Deep Denoising AutoEncoder\")\n-\t    autoencoder = build_deep_denoising_autoencoder(autoencoder)\n \telif autoencoder_type == 'classical':\n \t\tprint(\"Training Classical AutoEncoder\")\n \t\tautoencoder = build_deep_classical_autoencoder(autoencoder)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53331e43c2a95f45aaaa91f2c0fe204fd4d8d530", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 69 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -39,5 +39,5 @@ nonneg = NonNeg\n unitnorm = UnitNorm\n \n from .utils.generic_utils import get_from_module\n-def get(identifier):\n-    return get_from_module(identifier, globals(), 'constraint', instantiate=True)\n\\ No newline at end of file\n+def get(identifier, kwargs=None):\n+    return get_from_module(identifier, globals(), 'constraint', instantiate=True, kwargs=kwargs)\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eef82c486a50b8c4c10cddc6af6b88f5761b4ae6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 33 | Churn Cumulative: 375 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,6 +82,15 @@ class Sequential(Layer):\n         return {\"name\":self.__class__.__name__,\n             \"layers\":[layer.get_config() for layer in self.layers]}\n \n+    def layer_to_yaml(self, store_params=True):\n+        seq_config = {}\n+        seq_config['name'] = self.__class__.__name__\n+        layers = []\n+        for layer in self.layers:\n+            layer_conf = layer.layer_to_yaml(store_params)\n+            layers.append(layer_conf)\n+        seq_config['layers'] = layers\n+        return seq_config\n \n class Graph(Layer):\n     '''\n@@ -151,7 +160,7 @@ class Graph(Layer):\n                 raise Exception('Type \"int\" can only be used with ndim==2.')\n         layer.input.name = name\n         self.inputs[name] = layer\n-        self.output_config.append({'name':name, 'ndim':ndim, 'dtype':dtype})\n+        self.input_config.append({'name':name, 'ndim':ndim, 'dtype':dtype})\n \n     def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat'):\n         if hasattr(layer, 'set_name'):\n@@ -214,5 +223,27 @@ class Graph(Layer):\n             \"node_config\":self.node_config,\n             \"nodes\":[self.nodes[c[\"name\"]].get_config() for c in self.node_config]}\n     \n+    def layer_to_yaml(self, store_params=True):\n+        graph_config = {}\n+        graph_config['name'] = self.__class__.__name__\n \n+        inputs = []\n+        for input_conf in self.input_config:\n+            inputs.append(input_conf)\n+        graph_config['inputs'] = inputs\n \n+        outputs = []\n+        for output_conf in self.output_config:\n+            outputs.append(output_conf)\n+        graph_config['outputs'] = outputs\n+\n+        nodes = []\n+        for node_conf in self.node_config:\n+            name = node_conf.get('name')\n+            layer = self.nodes[name]\n+            layer_conf = layer.layer_to_yaml(store_params)\n+            node_conf['layer'] = layer_conf\n+            nodes.append(node_conf)\n+        graph_config['nodes'] = nodes\n+\n+        return graph_config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c695493c97f2f7fcfda38d2f28174c38c7ed152", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 14 | Methods Changed: 7 | Complexity Δ (Sum/Max): 21/16 | Churn Δ: 40 | Churn Cumulative: 583 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -13,7 +13,7 @@ class Convolution1D(Layer):\n     def __init__(self, input_dim, nb_filter, filter_length,\n         init='uniform', activation='linear', weights=None,\n         border_mode='valid', subsample_length=1,\n-        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Convolution1D,self).__init__()\n \n@@ -36,14 +36,17 @@ class Convolution1D(Layer):\n         self.regularizers = []\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n+        if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n         self.b_regularizer = regularizers.get(b_regularizer)\n+        if self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n         self.activity_regularizer = regularizers.get(activity_regularizer)\n+        if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n@@ -70,11 +73,11 @@ class Convolution1D(Layer):\n             \"activation\":self.activation.__name__,\n             \"border_mode\":self.border_mode,\n             \"subsample_length\":self.subsample_length,\n-            \"W_regularizer\":self.W_regularizer.get_config(),\n-            \"b_regularizer\":self.b_regularizer.get_config(),\n-            \"activity_regularizer\":self.activity_regularizer.get_config(),\n-            \"W_constraint\":self.W_constraint.get_config(),\n-            \"b_constraint\":self.b_constraint.get_config()}\n+            \"W_regularizer\":self.W_regularizer.get_config() if self.W_regularizer else None,\n+            \"b_regularizer\":self.b_regularizer.get_config() if self.b_regularizer else None,\n+            \"activity_regularizer\":self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+            \"W_constraint\":self.W_constraint.get_config() if self.W_constraint else None,\n+            \"b_constraint\":self.b_constraint.get_config() if self.b_constraint else None}\n \n \n class MaxPooling1D(Layer):\n@@ -111,7 +114,7 @@ class Convolution2D(Layer):\n     def __init__(self, nb_filter, stack_size, nb_row, nb_col,\n         init='glorot_uniform', activation='linear', weights=None,\n         border_mode='valid', subsample=(1, 1),\n-        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Convolution2D,self).__init__()\n         self.init = initializations.get(init)\n@@ -134,14 +137,17 @@ class Convolution2D(Layer):\n         self.regularizers = []\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n+        if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n         self.b_regularizer = regularizers.get(b_regularizer)\n+        if self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n         self.activity_regularizer = regularizers.get(activity_regularizer)\n+        if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n@@ -169,11 +175,11 @@ class Convolution2D(Layer):\n             \"activation\":self.activation.__name__,\n             \"border_mode\":self.border_mode,\n             \"subsample\":self.subsample,\n-            \"W_regularizer\":self.W_regularizer.get_config(),\n-            \"b_regularizer\":self.b_regularizer.get_config(),\n-            \"activity_regularizer\":self.activity_regularizer.get_config(),\n-            \"W_constraint\":self.W_constraint.get_config(),\n-            \"b_constraint\":self.b_constraint.get_config()}\n+            \"W_regularizer\":self.W_regularizer.get_config() if self.W_regularizer else None,\n+            \"b_regularizer\":self.b_regularizer.get_config() if self.b_regularizer else None,\n+            \"activity_regularizer\":self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+            \"W_constraint\":self.W_constraint.get_config() if self.W_constraint else None,\n+            \"b_constraint\":self.b_constraint.get_config() if self.b_constraint else None}\n \n \n class MaxPooling2D(Layer):\n\n@@ -18,7 +18,7 @@ class Embedding(Layer):\n         @out_dim: size of dense representation\n     '''\n     def __init__(self, input_dim, output_dim, init='uniform',\n-        W_regularizer='identity', activity_regularizer='identity', W_constraint='identity',\n+        W_regularizer=None, activity_regularizer=None, W_constraint=None,\n         mask_zero=False, weights=None):\n \n         super(Embedding,self).__init__()\n@@ -38,10 +38,12 @@ class Embedding(Layer):\n         self.regularizers = []\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n+        if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n         self.activity_regularizer = regularizers.get(activity_regularizer)\n+        if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n@@ -65,9 +67,9 @@ class Embedding(Layer):\n             \"input_dim\":self.input_dim,\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n-            \"activity_regularizer\":self.activity_regularizer.get_config(),\n-            \"W_regularizer\":self.W_regularizer.get_config(),\n-            \"W_constraint\":self.W_constraint.get_config()}\n+            \"activity_regularizer\":self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+            \"W_regularizer\":self.W_regularizer.get_config() if self.W_regularizer else None,\n+            \"W_constraint\":self.W_constraint.get_config() if self.W_constraint else None}\n \n \n class WordContextProduct(Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c90f98eaec8e4d0999309862ccc6a453fade82df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 18 | Methods Changed: 8 | Complexity Δ (Sum/Max): 23/23 | Churn Δ: 61 | Churn Cumulative: 1563 | Contributors (this commit): 13 | Commits (past 90d): 65 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,7 +10,6 @@ from ..utils.theano_utils import shared_zeros, floatX\n from ..utils.generic_utils import make_tuple\n from ..regularizers import ActivityRegularizer, Regularizer\n \n-\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n srng = RandomStreams(seed=np.random.randint(10e6))\n@@ -91,6 +90,11 @@ class Layer(object):\n \n         return self.params, regularizers, consts\n \n+    def layer_to_yaml(self, store_params=True):\n+        layer_conf = self.get_config()\n+        if store_params:\n+            layer_conf['parameters'] = [{'shape':list(param.get_value().shape), 'data':param.get_value().tolist()} for param in self.params]\n+        return layer_conf\n \n class MaskedLayer(Layer):\n     '''\n@@ -186,6 +190,16 @@ class Merge(object):\n             \"layers\":[l.get_config() for l in self.layers],\n             \"mode\":self.mode}\n \n+    def layer_to_yaml(self, store_params=True):\n+        merge_conf = {}\n+        merge_conf['name'] = self.__class__.__name__\n+        merge_conf['mode'] = self.mode\n+        layers = []\n+        for layer in self.layers:\n+            layer_conf = layer.layer_to_yaml(store_params)\n+            layers.append(layer_conf)\n+        merge_conf['layers'] = layers\n+        return merge_conf     \n \n class Dropout(MaskedLayer):\n     '''\n@@ -293,7 +307,7 @@ class Dense(Layer):\n         Just your regular fully connected NN layer.\n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, name=None,\n-        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(Dense, self).__init__()\n         self.init = initializations.get(init)\n@@ -310,14 +324,17 @@ class Dense(Layer):\n         self.regularizers = []\n         \n         self.W_regularizer = regularizers.get(W_regularizer)\n+        if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n         self.b_regularizer = regularizers.get(b_regularizer)\n+        if self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n         self.activity_regularizer = regularizers.get(activity_regularizer)\n+        if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n@@ -346,11 +363,11 @@ class Dense(Layer):\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,\n-            \"W_regularizer\":self.W_regularizer.get_config(),\n-            \"b_regularizer\":self.b_regularizer.get_config(),\n-            \"activity_regularizer\":self.activity_regularizer.get_config(),\n-            \"W_constraint\":self.W_constraint.get_config(),\n-            \"b_constraint\":self.b_constraint.get_config()}\n+            \"W_regularizer\":self.W_regularizer.get_config() if self.W_regularizer else None,\n+            \"b_regularizer\":self.b_regularizer.get_config() if self.b_regularizer else None,\n+            \"activity_regularizer\":self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+            \"W_constraint\":self.W_constraint.get_config() if self.W_constraint else None,\n+            \"b_constraint\":self.b_constraint.get_config() if self.b_constraint else None}\n \n \n class ActivityRegularization(Layer):\n@@ -385,7 +402,7 @@ class TimeDistributedDense(MaskedLayer):\n \n     '''\n     def __init__(self, input_dim, output_dim, init='glorot_uniform', activation='linear', weights=None, \n-        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(TimeDistributedDense, self).__init__()\n         self.init = initializations.get(init)\n@@ -402,14 +419,17 @@ class TimeDistributedDense(MaskedLayer):\n         self.regularizers = []\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n+        if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n         self.b_regularizer = regularizers.get(b_regularizer)\n+        if self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n         self.activity_regularizer = regularizers.get(activity_regularizer)\n+        if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n@@ -432,11 +452,11 @@ class TimeDistributedDense(MaskedLayer):\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n             \"activation\":self.activation.__name__,            \n-            \"W_regularizer\":self.W_regularizer.get_config(),\n-            \"b_regularizer\":self.b_regularizer.get_config(),\n-            \"activity_regularizer\":self.activity_regularizer.get_config(),\n-            \"W_constraint\":self.W_constraint.get_config(),\n-            \"b_constraint\":self.b_constraint.get_config()}\n+            \"W_regularizer\":self.W_regularizer.get_config() if self.W_regularizer else None,\n+            \"b_regularizer\":self.b_regularizer.get_config() if self.b_regularizer else None,\n+            \"activity_regularizer\":self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+            \"W_constraint\":self.W_constraint.get_config() if self.W_constraint else None,\n+            \"b_constraint\":self.b_constraint.get_config() if self.b_constraint else None}\n \n \n class AutoEncoder(Layer):\n@@ -523,7 +543,7 @@ class MaxoutDense(Layer):\n         Refer to http://arxiv.org/pdf/1302.4389.pdf\n     '''\n     def __init__(self, input_dim, output_dim, nb_feature=4, init='glorot_uniform', weights=None, \n-        W_regularizer='identity', b_regularizer='identity', activity_regularizer='identity', W_constraint='identity', b_constraint='identity'):\n+        W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None):\n \n         super(MaxoutDense, self).__init__()\n         self.init = initializations.get(init)\n@@ -540,14 +560,17 @@ class MaxoutDense(Layer):\n         self.regularizers = []\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n+        if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n         self.b_regularizer = regularizers.get(b_regularizer)\n+        if self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n         self.activity_regularizer = regularizers.get(activity_regularizer)\n+        if self.activity_regularizer:\n             self.activity_regularizer.set_layer(self)\n             self.regularizers.append(self.activity_regularizer)\n \n@@ -570,8 +593,8 @@ class MaxoutDense(Layer):\n             \"output_dim\":self.output_dim,\n             \"init\":self.init.__name__,\n             \"nb_feature\" : self.nb_feature,\n-            \"W_regularizer\":self.W_regularizer.get_config(),\n-            \"b_regularizer\":self.b_regularizer.get_config(),\n-            \"activity_regularizer\":self.activity_regularizer.get_config(),\n-            \"W_constraint\":self.W_constraint.get_config(),\n-            \"b_constraint\":self.b_constraint.get_config()}\n+            \"W_regularizer\":self.W_regularizer.get_config() if self.W_regularizer else None,\n+            \"b_regularizer\":self.b_regularizer.get_config() if self.b_regularizer else None,\n+            \"activity_regularizer\":self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+            \"W_constraint\":self.W_constraint.get_config() if self.W_constraint else None,\n+            \"b_constraint\":self.b_constraint.get_config() if self.b_constraint else None}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b7a3cbaa973fa7f25a5d84471d0dcccaadf31fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 60 | Files Changed: 1 | Hunks: 17 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 101 | Churn Cumulative: 2219 | Contributors (this commit): 19 | Commits (past 90d): 87 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,24 +5,14 @@ import theano.tensor as T\n import numpy as np\n import warnings, time, copy, yaml\n \n-from .layers.convolutional import *\n-from .layers.core import *\n-from .layers.embeddings import *\n-from .layers.noise import *\n-from .layers.normalization import *\n-from .layers.recurrent import *\n-\n-from .optimizers import *\n-from .objectives import *\n-from .regularizers import *\n-from .constraints import *\n-\n from . import optimizers\n from . import objectives\n from . import regularizers\n from . import constraints\n from . import callbacks as cbks\n+\n import time, copy, pprint\n+from .utils.layer_utils import from_yaml\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n from six.moves import range\n@@ -88,48 +78,29 @@ def standardize_weights(y, sample_weight=None, class_weight=None):\n     else:\n         return np.ones(y.shape[:-1] + (1,))\n \n-def sequential_from_yaml(yaml_string):\n+def model_from_yaml(yaml_string):\n     '''\n-        Returns a compiled Sequential model generated from a local yaml file,\n-        which is either created by hand or from Sequential.to_yaml\n+        Returns a model generated from a local yaml file,\n+        which is either created by hand or from to_yaml method of Sequential or Graph\n     '''\n-    model_yaml = yaml.load(yaml_string)\n-    model = Sequential()\n+    model_dict = yaml.load(yaml_string)\n+    model_name = model_dict.get('name')\n \n-    class_mode = model_yaml.get('class_mode')\n-    theano_mode = model_yaml.get('theano_mode')\n-    loss = globals()[model_yaml.get('loss')]\n+    # Create a container layer and set class to respective model\n+    model = from_yaml(model_dict)\n+    model.__class__ = get(model_name)\n \n-    optim = model_yaml.get('optimizer')\n+    if model_dict.has_key('optimizer'): # if it has an optimizer, the model is assumed to be compiled\n+        loss = objectives.get(model_dict.get('loss'))\n+        class_mode = model_dict.get('class_mode')\n+        theano_mode = model_dict.get('theano_mode')\n+\n+        optim = model_dict.get('optimizer')\n         optim_name = optim.get('name')\n         optim.pop('name')\n-    optimizer = globals()[optim_name](**optim)\n-\n-    layers = model_yaml.get('layers')\n-    for layer in layers:\n-        name = layer.get('name')\n-        layer.pop('name')\n-        hasParams = False\n-        if layer.has_key('parameters'):\n-            params = layer.get('parameters')\n-            layer.pop('parameters')\n-            hasParams = True\n-        for k, v in layer.iteritems():\n-            if isinstance(v, dict):\n-                vname = v.get('name')\n-                v.pop('name')\n-                layer[k] = globals()[vname](**v)\n-        init_layer = globals()[name](**layer)\n-        if hasParams:\n-            shaped_params = []\n-            for param in params:\n-                data = np.asarray(param.get('data'))\n-                shape = tuple(param.get('shape'))\n-                shaped_params.append(data.reshape(shape))\n-            init_layer.set_weights(shaped_params)\n-        model.add(init_layer)\n-\n+        optimizer = optimizers.get(optim_name, optim)\n         model.compile(loss=loss, optimizer=optimizer, class_mode=class_mode, theano_mode=theano_mode)\n+\n     return model\n \n class Model(object):\n@@ -274,7 +245,6 @@ class Model(object):\n             outs[i] /= nb_sample\n         return outs\n \n-\n class Sequential(Model, containers.Sequential):\n     '''\n         Inherits from Model the following methods:\n@@ -524,27 +494,20 @@ class Sequential(Model, containers.Sequential):\n \n     def to_yaml(self, store_params=True):\n         '''\n-            Stores compiled Sequential model to yaml string, optionally storing all learnable parameters\n+            Stores a model to yaml string, optionally with all learnable parameters\n+            If the model is compiled, it will also serialize the necessary components \n         '''\n-        model_dict = {}\n+        model_dict = self.layer_to_yaml(store_params)    \n+        if hasattr(self, 'optimizer'):\n             model_dict['class_mode'] = self.class_mode\n             model_dict['theano_mode'] = self.theano_mode\n             model_dict['loss'] = self.unweighted_loss.__name__\n             model_dict['optimizer'] = self.optimizer.get_config()\n-\n-        layers = []\n-        for layer in self.layers:\n-            layer_conf = layer.get_config()\n-            if store_params:\n-                layer_conf['parameters'] = [{'shape':list(param.get_value().shape), 'data':param.get_value().tolist()} for param in layer.params]\n-            layers.append(layer_conf)\n-        model_dict['layers'] = layers\n-\n         return yaml.dump(model_dict)\n \n \n class Graph(Model, containers.Graph):\n-    def compile(self, optimizer, loss, theano_mode=None):\n+    def compile(self, optimizer, loss, theano_mode=None, class_mode=None):\n         # loss is a dictionary mapping output name to loss functions\n         ys = []\n         ys_train = []\n@@ -575,6 +538,8 @@ class Graph(Model, containers.Graph):\n             train_loss = r(train_loss)\n         self.optimizer = optimizers.get(optimizer)\n         updates = self.optimizer.get_updates(self.params, self.constraints, train_loss)\n+        self.theano_mode = theano_mode\n+        self.loss = loss\n \n         self._train = theano.function(train_ins, train_loss, \n             updates=updates, allow_input_downcast=True, mode=theano_mode)\n@@ -669,3 +634,19 @@ class Graph(Model, containers.Graph):\n             pp = pprint.PrettyPrinter(indent=4)\n             pp.pprint(config)\n         return config\n+\n+    def to_yaml(self, store_params=True):\n+        '''\n+            Stores a model to yaml string, optionally with all learnable parameters\n+            If the model is compiled, it will also serialize the necessary components \n+        '''\n+        model_dict = self.layer_to_yaml(store_params)    \n+        if hasattr(self, 'optimizer'):\n+            model_dict['theano_mode'] = self.theano_mode\n+            model_dict['loss'] = self.loss\n+            model_dict['optimizer'] = self.optimizer.get_config()\n+        return yaml.dump(model_dict)\n+\n+from .utils.generic_utils import get_from_module\n+def get(identifier):\n+    return get_from_module(identifier, globals(), 'model')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7da91d94ef96dff3ec6f525237b7770c7531aa0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 476 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -206,5 +206,5 @@ adadelta = Adadelta\n adam = Adam\n \n from .utils.generic_utils import get_from_module\n-def get(identifier):\n-    return get_from_module(identifier, globals(), 'optimizer', instantiate=True)\n+def get(identifier, kwargs=None):\n+    return get_from_module(identifier, globals(), 'optimizer', instantiate=True, kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
