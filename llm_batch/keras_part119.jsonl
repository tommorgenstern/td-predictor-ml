{"custom_id": "keras#1d0d79f61a02934dc17090dac011aa7474e2b63a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 38 | Churn Cumulative: 652 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.5454545454545454\n\nDIFF:\n@@ -286,7 +286,7 @@ class LSTMConv2D(RecurrentConv2D):\n         assert dim_ordering in {'tf', \"th\"}, 'dim_ordering must be in {tf,\"th}'\n \n         if dim_ordering == \"th\":\n-            print(\"Warning, unlike convolution3D the time must be the \" +\\\n+            print(\"Warning, unlike convolution3D the time must be the \"\n                   \"first dimention\")\n         self.dim_ordering = dim_ordering\n \n@@ -333,21 +333,26 @@ class LSTMConv2D(RecurrentConv2D):\n             # initial states: 2 all-zero tensor of shape (nb_filter)\n             self.states = [None, None, None, None]\n \n-        self.W_i = self.init(self.W_shape)\n-        self.U_i = self.inner_init(self.W_shape1)\n-        self.b_i = K.zeros((self.nb_filter,))\n+        self.W_i = self.init(self.W_shape, name='{}_W_i'.format(self.name))\n+        self.U_i = self.inner_init(self.W_shape1,\n+                                   name='{}_U_i'.format(self.name))\n+        self.b_i = K.zeros((self.nb_filter,), name='{}_b_i'.format(self.name))\n \n-        self.W_f = self.init(self.W_shape)\n-        self.U_f = self.inner_init(self.W_shape1)\n-        self.b_f = self.forget_bias_init((self.nb_filter,))\n+        self.W_f = self.init(self.W_shape, name='{}_W_f'.format(self.name))\n+        self.U_f = self.inner_init(self.W_shape1,\n+                                   name='{}_U_f'.format(self.name))\n+        self.b_f = self.forget_bias_init((self.nb_filter,),\n+                                         name='{}_b_f'.format(self.name))\n \n-        self.W_c = self.init(self.W_shape)\n-        self.U_c = self.inner_init(self.W_shape1)\n-        self.b_c = K.zeros((self.nb_filter,))\n+        self.W_c = self.init(self.W_shape, name='{}_W_c'.format(self.name))\n+        self.U_c = self.inner_init(self.W_shape1,\n+                                   name='{}_U_c'.format(self.name))\n+        self.b_c = K.zeros((self.nb_filter,), name='{}_b_c'.format(self.name))\n \n-        self.W_o = self.init(self.W_shape)\n-        self.U_o = self.inner_init(self.W_shape1)\n-        self.b_o = K.zeros((self.nb_filter,))\n+        self.W_o = self.init(self.W_shape, name='{}_W_o'.format(self.name))\n+        self.U_o = self.inner_init(self.W_shape1,\n+                                   name='{}_U_o'.format(self.name))\n+        self.b_o = K.zeros((self.nb_filter,), name='{}_b_o'.format(self.name))\n \n         self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                   self.W_c, self.U_c, self.b_c,\n\n@@ -15,13 +15,14 @@ def test_shape2():\n     input_a = np.zeros([batch]+input_shape)\n     gt_shape = (batch, input_shape[0], input_shape[1], input_shape[2], nfilter)\n     gt = np.zeros(gt_shape)\n+    input_shape = tuple(input_shape)\n     seq = Sequential()\n     seq.add(LSTMConv2D(nb_filter=20, nb_row=4, nb_col=4,\n                        input_shape=input_shape, border_mode=\"same\",\n                        return_sequences=True))\n     seq.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n     assert seq.predict(input_a).shape == gt_shape\n-    #seq.fit(input_a, gt, nb_epoch=1)\n+    seq.fit(input_a, gt, nb_epoch=1)\n \n \n def test_shape_th_return_sequences():\n@@ -31,6 +32,7 @@ def test_shape_th_return_sequences():\n     input_a = np.zeros([batch]+input_shape)\n     gt_shape = (batch, input_shape[0], nfilter, input_shape[2], input_shape[3])\n     gt = np.zeros(gt_shape)\n+    input_shape = tuple(input_shape)\n     seq = Sequential()\n     seq.add(LSTMConv2D(nb_filter=nfilter, nb_row=4, nb_col=4,\n                        dim_ordering=\"th\", input_shape=input_shape,\n@@ -49,7 +51,9 @@ def test_shape_th():\n     input_a = np.zeros([batch]+input_shape)\n     gt_shape = (batch, nfilter, input_shape[2], input_shape[3])\n     gt = np.zeros(gt_shape)\n+    input_shape = tuple(input_shape)\n     seq = Sequential()\n+    input_shape = tuple(input_shape)\n     seq.add(LSTMConv2D(nb_filter=nfilter, nb_row=4, nb_col=4,\n                        dim_ordering=\"th\", input_shape=input_shape,\n             border_mode=\"same\", return_sequences=False))\n@@ -65,6 +69,7 @@ def test_shape():\n     input_a = np.zeros([batch]+input_shape)\n     gt_shape = (batch, input_shape[1], input_shape[2], nfilter)\n     gt = np.zeros(gt_shape)\n+    input_shape = tuple(input_shape)\n     seq = Sequential()\n     seq.add(LSTMConv2D(nb_filter=nfilter, nb_row=4, nb_col=4,\n                        input_shape=input_shape,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#240fd5b68e033ee31167581fcd061dc027282fcf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2861 | Contributors (this commit): 34 | Commits (past 90d): 52 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import tensorflow as tf\n+from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.training import moving_averages\n try:\n     from tensorflow.python.ops import ctc_ops as ctc\n@@ -1284,7 +1285,7 @@ def switch(condition, then_expression, else_expression):\n         else_expression: TensorFlow operation.\n     '''\n     x_shape = copy.copy(then_expression.get_shape())\n-    x = tf.python.control_flow_ops.cond(tf.cast(condition, 'bool'),\n+    x = control_flow_ops.cond(tf.cast(condition, 'bool'),\n                               lambda: then_expression,\n                               lambda: else_expression)\n     x.set_shape(x_shape)\n@@ -1301,7 +1302,7 @@ def in_train_phase(x, alt):\n         return alt\n     # else: assume learning phase is a placeholder.\n     x_shape = copy.copy(x.get_shape())\n-    x = tf.python.control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n+    x = control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n                               lambda: x,\n                               lambda: alt)\n     x._uses_learning_phase = True\n@@ -1318,7 +1319,7 @@ def in_test_phase(x, alt):\n     elif _LEARNING_PHASE is 0:\n         return x\n     x_shape = copy.copy(x.get_shape())\n-    x = tf.python.control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n+    x = control_flow_ops.cond(tf.cast(_LEARNING_PHASE, 'bool'),\n                               lambda: alt,\n                               lambda: x)\n     x._uses_learning_phase = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#929669bd1bfa682678393949d1765e3314eb2c49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 555 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -447,7 +447,6 @@ class _GlobalPooling2D(Layer):\n         super(_GlobalPooling2D, self).__init__(**kwargs)\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n-        print(dim_ordering)\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#570fdf31c5cb9a580496d1d93320bc7ab1b9ad46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 208 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ def func_reconstruct_closure(values):\n     src += [\"  return lambda:(%s)\" % ','.join([\"_%d\" % n for n in nums]), \"\"]\n     src = '\\n'.join(src)\n     try:\n-        exec(src)\n+        exec(src, globals())\n     except:\n         raise SyntaxError(src)\n     return func(values).__closure__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b18a908b8ee6e9b58c834a3f7b1944e002764a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2865 | Contributors (this commit): 35 | Commits (past 90d): 53 | Contributors (cumulative): 35 | DMM Complexity: None\n\nDIFF:\n@@ -1789,9 +1789,9 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     max_num_labels_tns = tf.pack([label_shape[1]])\n \n     def range_less_than(previous_state, current_input):\n-        return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input\n+        return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\n \n-    init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)\n+    init = tf.cast(tf.fill([1, label_shape[1]], 0), tf.bool)\n     dense_mask = functional_ops.scan(range_less_than, label_lengths,\n                                      initializer=init, parallel_iterations=1)\n     dense_mask = dense_mask[:, 0, :]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ed71386855d62f1c1934826d23e6ff929491bec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 234 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n import numpy as np\n from . import backend as K\n+from .utils.generic_utils import get_from_module\n \n \n def binary_accuracy(y_true, y_pred):\n@@ -92,7 +93,7 @@ def matthews_correlation(y_true, y_pred):\n     return numerator / (denominator + K.epsilon())\n \n \n-def fbetascore(y_true, y_pred, beta=1):\n+def fbeta_score(y_true, y_pred, beta=1):\n     '''Compute F score, the weighted harmonic mean of precision and recall.\n \n     This is useful for multi-label classification where input samples can be\n@@ -140,6 +141,5 @@ msle = MSLE = mean_squared_logarithmic_error\n cosine = cosine_proximity\n \n \n-from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'metric')\n\n@@ -46,14 +46,14 @@ def test_matthews_correlation():\n     assert expected - epsilon <= actual <= expected + epsilon\n \n \n-def test_fbetascore():\n+def test_fbeta_score():\n     y_true = K.variable(np.array([0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0]))\n     y_pred = K.variable(np.array([1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]))\n \n     # Calculated using sklearn.metrics.f1_score\n     expected = 0.33333333333333331\n \n-    actual = K.eval(metrics.fbetascore(y_true, y_pred))\n+    actual = K.eval(metrics.fbeta_score(y_true, y_pred))\n     epsilon = 1e-05\n     assert expected - epsilon <= actual <= expected + epsilon\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#530eff62e5463e00d73e72c51cc830b9ac3a14ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 96 | Churn Cumulative: 1937 | Contributors (this commit): 14 | Commits (past 90d): 21 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -519,3 +519,83 @@ class GlobalMaxPooling2D(_GlobalPooling2D):\n             return K.max(x, axis=[1, 2])\n         else:\n             return K.max(x, axis=[2, 3])\n+\n+\n+class _GlobalPooling3D(Layer):\n+\n+    def __init__(self, dim_ordering='default', **kwargs):\n+        super(_GlobalPooling3D, self).__init__(**kwargs)\n+        if dim_ordering == 'default':\n+            dim_ordering = K.image_dim_ordering()\n+        self.dim_ordering = dim_ordering\n+        self.input_spec = [InputSpec(ndim=5)]\n+\n+    def get_output_shape_for(self, input_shape):\n+        if self.dim_ordering == 'tf':\n+            return (input_shape[0], input_shape[4])\n+        else:\n+            return (input_shape[0], input_shape[1])\n+\n+    def call(self, x, mask=None):\n+        raise NotImplementedError\n+\n+    def get_config(self):\n+        config = {'dim_ordering': self.dim_ordering}\n+        base_config = super(_GlobalPooling3D, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class GlobalAveragePooling3D(_GlobalPooling3D):\n+    '''Global Average pooling operation for 3D data.\n+\n+    # Arguments\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"tf\".\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        2D tensor with shape:\n+        `(nb_samples, channels)`\n+    '''\n+\n+    def call(self, x, mask=None):\n+        if self.dim_ordering == 'tf':\n+            return K.mean(x, axis=[1, 2, 3])\n+        else:\n+            return K.mean(x, axis=[2, 3, 4])\n+\n+\n+class GlobalMaxPooling3D(_GlobalPooling3D):\n+    '''Global Max pooling operation for 3D data.\n+\n+    # Arguments\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"tf\".\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        2D tensor with shape:\n+        `(nb_samples, channels)`\n+    '''\n+\n+    def call(self, x, mask=None):\n+        if self.dim_ordering == 'tf':\n+            return K.max(x, axis=[1, 2, 3])\n+        else:\n+            return K.max(x, axis=[2, 3, 4])\n\n@@ -269,6 +269,22 @@ def test_globalpooling_2d():\n                input_shape=(3, 5, 6, 4))\n \n \n+@keras_test\n+def test_globalpooling_3d():\n+    layer_test(pooling.GlobalMaxPooling3D,\n+               kwargs={'dim_ordering': 'th'},\n+               input_shape=(3, 4, 3, 4, 3))\n+    layer_test(pooling.GlobalMaxPooling3D,\n+               kwargs={'dim_ordering': 'tf'},\n+               input_shape=(3, 4, 3, 4, 3))\n+    layer_test(pooling.GlobalAveragePooling3D,\n+               kwargs={'dim_ordering': 'th'},\n+               input_shape=(3, 4, 3, 4, 3))\n+    layer_test(pooling.GlobalAveragePooling3D,\n+               kwargs={'dim_ordering': 'tf'},\n+               input_shape=(3, 4, 3, 4, 3))\n+\n+\n @keras_test\n def test_maxpooling_2d():\n     pool_size = (3, 3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52ee2380e43bf96f5ab60af5d31d7e0988990763", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/1 | Churn Δ: 49 | Churn Cumulative: 5561 | Contributors (this commit): 53 | Commits (past 90d): 91 | Contributors (cumulative): 85 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1485,6 +1485,20 @@ def l2_normalize(x, axis):\n         axis = axis % len(x.get_shape())\n     return tf.nn.l2_normalize(x, dim=axis)\n \n+def in_top_k(predictions, targets, k):\n+    '''Says whether the `targets` are in the top `k` `predictions`\n+\n+    # Arguments\n+        predictions: A tensor of shape batch_size x classess and type float32.\n+        targets: A tensor of shape batch_size and type int32 or int64.\n+        k: An int, number of top elements to consider.\n+\n+    # Returns\n+        A tensor of shape batch_size and type bool. output_i is True if\n+        targets_i is within top-k values of predictions_i\n+    '''\n+    return tf.nn.in_top_k(predictions, targets, k)\n+\n \n # CONVOLUTIONS\n \n\n@@ -1043,6 +1043,23 @@ def l2_normalize(x, axis):\n     return x / norm\n \n \n+def in_top_k(predictions, targets, k):\n+    '''Says whether the `targets` are in the top `k` `predictions`\n+\n+    # Arguments\n+        predictions: A tensor of shape batch_size x classess and type float32.\n+        targets: A tensor of shape batch_size and type int32 or int64.\n+        k: An int, number of top elements to consider.\n+\n+    # Returns\n+        A tensor of shape batch_size and type int. output_i is 1 if\n+        targets_i is within top-k values of predictions_i\n+    '''\n+    predictions_top_k = T.argsort(predictions)[:, -k:]\n+    result, _ = theano.map(lambda prediction, target: any(equal(prediction, target)), sequences=[predictions_top_k, targets])\n+    return result\n+\n+\n # CONVOLUTIONS\n \n def _preprocess_conv2d_input(x, dim_ordering):\n\n@@ -17,6 +17,10 @@ def sparse_categorical_accuracy(y_true, y_pred):\n                           K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n \n \n+def top_k_categorical_accuracy(y_true, y_pred, k=5):\n+    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n+\n+\n def mean_squared_error(y_true, y_pred):\n     return K.mean(K.square(y_pred - y_true))\n \n\n@@ -65,5 +65,19 @@ def test_sparse_metrics():\n         assert K.eval(metric(y_a, y_b)).shape == ()\n \n \n+def test_top_k_categorical_accuracy():\n+    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n+    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n+    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n+                            k=3))\n+    assert success_result == 1\n+    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n+                            k=2))\n+    assert partial_result == 0.5\n+    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n+                            k=1))\n+    assert failure_result == 0\n+\n+\n if __name__ == \"__main__\":\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7df184d3aa8a9790d181c837ab22a31b5aebb5ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 505 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1,14 +1,13 @@\n-# -*- coding: utf-8 -*-\n import gzip\n from ..utils.data_utils import get_file\n from six.moves import cPickle\n import sys\n \n \n-def load_data(path=\"mnist.pkl.gz\"):\n-    path = get_file(path, origin=\"https://s3.amazonaws.com/img-datasets/mnist.pkl.gz\")\n+def load_data(path='mnist.pkl.gz'):\n+    path = get_file(path, origin='https://s3.amazonaws.com/img-datasets/mnist.pkl.gz')\n \n-    if path.endswith(\".gz\"):\n+    if path.endswith('.gz'):\n         f = gzip.open(path, 'rb')\n     else:\n         f = open(path, 'rb')\n@@ -16,7 +15,7 @@ def load_data(path=\"mnist.pkl.gz\"):\n     if sys.version_info < (3,):\n         data = cPickle.load(f)\n     else:\n-        data = cPickle.load(f, encoding=\"bytes\")\n+        data = cPickle.load(f, encoding='bytes')\n \n     f.close()\n     return data  # (X_train, y_train), (X_test, y_test)\n\n@@ -10,7 +10,8 @@ import sys\n def load_data(path='reuters.pkl', nb_words=None, skip_top=0,\n               maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n-    '''\n+    '''Loads the Reuters newswire classification dataset.\n+\n     # Arguments\n         path: where to store the data (in `/.keras/dataset`)\n         nb_words: max number of words to include. Words are ranked\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b7421c44890dd96df5680c7383fc7af4e341e24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 45 | Files Changed: 1 | Hunks: 26 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 89 | Churn Cumulative: 656 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from .. import activations, initializations, regularizers\n import numpy as np\n from ..engine import Layer, InputSpec\n from ..utils.np_utils import conv_output_length\n+import warnings\n \n \n class RecurrentConv2D(Layer):\n@@ -77,8 +78,7 @@ class RecurrentConv2D(Layer):\n     def __init__(self, weights=None,\n                  return_sequences=False, go_backwards=False, stateful=False,\n                  nb_row=None, nb_col=None, nb_filter=None,\n-                 dim_ordering=None,\n-                 input_dim=None, input_length=None, **kwargs):\n+                 dim_ordering=None, **kwargs):\n         self.return_sequences = return_sequences\n         self.initial_weights = weights\n         self.go_backwards = go_backwards\n@@ -90,11 +90,6 @@ class RecurrentConv2D(Layer):\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=5)]\n \n-        self.input_dim = input_dim\n-        self.input_length = input_length\n-        # if self.input_dim:\n-        #    kwargs['input_shape'] = (self.input_length, self.input_dim)\n-\n         super(RecurrentConv2D, self).__init__(**kwargs)\n \n     def compute_mask(self, input, mask):\n@@ -106,11 +101,11 @@ class RecurrentConv2D(Layer):\n     def get_output_shape_for(self, input_shape):\n \n         if self.dim_ordering == 'th':\n-            rows = input_shape[2+1]\n-            cols = input_shape[3+1]\n+            rows = input_shape[3]\n+            cols = input_shape[4]\n         elif self.dim_ordering == 'tf':\n-            rows = input_shape[1+1]\n-            cols = input_shape[2+1]\n+            rows = input_shape[2]\n+            cols = input_shape[3]\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n@@ -147,7 +142,6 @@ class RecurrentConv2D(Layer):\n         initial_state = K.zeros_like(X)\n         # (samples,row, col, filter)\n         initial_state = K.sum(initial_state, axis=1)\n-        # initial_state = initial_state[::,]\n         initial_state = self.conv_step(initial_state, K.zeros(self.W_shape),\n                                        border_mode=self.border_mode)\n \n@@ -161,7 +155,7 @@ class RecurrentConv2D(Layer):\n \n         assert K.ndim(x) == 5\n         input_shape = self.input_spec[0].shape\n-\n+        unroll = False\n         if K._BACKEND == 'tensorflow':\n             if not input_shape[1]:\n                 raise Exception('When using TensorFlow, you should define ' +\n@@ -169,6 +163,8 @@ class RecurrentConv2D(Layer):\n                                 'your sequences. Make sure the first layer ' +\n                                 'has a \"batch_input_shape\" argument ' +\n                                 'including the samples axis.')\n+            else:\n+                unroll = True\n \n         if self.stateful:\n             initial_states = self.states\n@@ -183,6 +179,7 @@ class RecurrentConv2D(Layer):\n                                              go_backwards=self.go_backwards,\n                                              mask=mask,\n                                              constants=constants,\n+                                             unroll=unroll,\n                                              input_length=input_shape[1])\n         if self.stateful:\n             self.updates = []\n@@ -195,15 +192,12 @@ class RecurrentConv2D(Layer):\n             return last_output\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"return_sequences\": self.return_sequences,\n-                  \"go_backwards\": self.go_backwards,\n-                  \"stateful\": self.stateful}\n+        config = {'name': self.__class__.__name__,\n+                  'return_sequences': self.return_sequences,\n+                  'go_backwards': self.go_backwards,\n+                  'stateful': self.stateful}\n         if self.stateful:\n             config['batch_input_shape'] = self.input_shape\n-        else:\n-            config['input_dim'] = self.input_dim\n-            config['input_length'] = self.input_length\n \n         base_config = super(RecurrentConv2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -268,8 +262,9 @@ class LSTMConv2D(RecurrentConv2D):\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  forget_bias_init='one', activation='tanh',\n-                 inner_activation='hard_sigmoid', dim_ordering=\"tf\",\n-                 border_mode=\"valid\", sub_sample=(1, 1),\n+                 inner_activation='hard_sigmoid',\n+                 dim_ordering=K.image_dim_ordering(),\n+                 border_mode='valid', sub_sample=(1, 1),\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n         self.nb_filter = nb_filter\n@@ -283,17 +278,21 @@ class LSTMConv2D(RecurrentConv2D):\n         self.border_mode = border_mode\n         self.subsample = sub_sample\n \n-        assert dim_ordering in {'tf', \"th\"}, 'dim_ordering must be in {tf,\"th}'\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf,th}'\n \n-        if dim_ordering == \"th\":\n-            print(\"Warning, unlike convolution3D the time must be the \"\n-                  \"first dimention\")\n+        if dim_ordering == 'th':\n+            warnings.warn('Be carefull if used with convolution3D layers:\\n'\n+                          'th in convolution 3D corresponds to '\n+                          '(samples, channels, conv_dim1, conv_dim2,'\n+                          'conv_dim3)\\n'\n+                          'while for this network it corresponds to: '\n+                          '(samples, time, channels, rows, cols)')\n         self.dim_ordering = dim_ordering\n \n-        kwargs[\"nb_filter\"] = nb_filter\n-        kwargs[\"nb_row\"] = nb_row\n-        kwargs[\"nb_col\"] = nb_col\n-        kwargs[\"dim_ordering\"] = dim_ordering\n+        kwargs['nb_filter'] = nb_filter\n+        kwargs['nb_row'] = nb_row\n+        kwargs['nb_col'] = nb_col\n+        kwargs['dim_ordering'] = dim_ordering\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.U_regularizer = regularizers.get(U_regularizer)\n@@ -308,11 +307,11 @@ class LSTMConv2D(RecurrentConv2D):\n         self.input_spec = [InputSpec(shape=input_shape)]\n \n         if self.dim_ordering == 'th':\n-            stack_size = input_shape[1+1]\n+            stack_size = input_shape[2]\n             self.W_shape = (self.nb_filter, stack_size,\n                             self.nb_row, self.nb_col)\n         elif self.dim_ordering == 'tf':\n-            stack_size = input_shape[3+1]\n+            stack_size = input_shape[4]\n             self.W_shape = (self.nb_row, self.nb_col,\n                             stack_size, self.nb_filter)\n         else:\n@@ -404,7 +403,7 @@ class LSTMConv2D(RecurrentConv2D):\n                            K.zeros((input_shape[0],\n                                     out_row, out_col, out_filter))]\n \n-    def conv_step(self, x, W, b=None, border_mode=\"valid\"):\n+    def conv_step(self, x, W, b=None, border_mode='valid'):\n         input_shape = self.input_spec[0].shape\n \n         conv_out = K.conv2d(x, W, strides=self.subsample,\n@@ -425,7 +424,7 @@ class LSTMConv2D(RecurrentConv2D):\n \n         return conv_out\n \n-    def conv_step_hidden(self, x, W, border_mode=\"valid\"):\n+    def conv_step_hidden(self, x, W, border_mode='valid'):\n         # This new function was defined because the\n         # image shape must be hardcoded\n         input_shape = self.input_spec[0].shape\n@@ -464,13 +463,13 @@ class LSTMConv2D(RecurrentConv2D):\n         # U : from nb_filter to nb_filter\n         # Same because must be stable in the ouptut space\n         h_i = self.conv_step_hidden(h_tm1, self.U_i * B_U[0],\n-                                    border_mode=\"same\")\n+                                    border_mode='same')\n         h_f = self.conv_step_hidden(h_tm1, self.U_f * B_U[1],\n-                                    border_mode=\"same\")\n+                                    border_mode='same')\n         h_c = self.conv_step_hidden(h_tm1, self.U_c * B_U[2],\n-                                    border_mode=\"same\")\n+                                    border_mode='same')\n         h_o = self.conv_step_hidden(h_tm1, self.U_o * B_U[3],\n-                                    border_mode=\"same\")\n+                                    border_mode='same')\n \n         i = self.inner_activation(x_i + h_i)\n         f = self.inner_activation(x_f + h_f)\n@@ -504,16 +503,16 @@ class LSTMConv2D(RecurrentConv2D):\n         return constants\n \n     def get_config(self):\n-        config = {\"name\": self.__class__.__name__,\n-                  \"nb_filter\": self.nb_filter,\n+        config = {'name': self.__class__.__name__,\n+                  'nb_filter': self.nb_filter,\n                   'nb_row': self.nb_row,\n                   'nb_col': self.nb_col,\n-                  \"init\": self.init.__name__,\n-                  \"inner_init\": self.inner_init.__name__,\n-                  \"forget_bias_init\": self.forget_bias_init.__name__,\n-                  \"activation\": self.activation.__name__,\n+                  'init': self.init.__name__,\n+                  'inner_init': self.inner_init.__name__,\n+                  'forget_bias_init': self.forget_bias_init.__name__,\n+                  'activation': self.activation.__name__,\n                   'dim_ordering': self.dim_ordering,\n                   'border_mode': self.border_mode,\n-                  \"inner_activation\": self.inner_activation.__name__}\n+                  'inner_activation': self.inner_activation.__name__}\n         base_config = super(LSTMConv2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#85c2d28e992f8f2a752393d7e9f65c8f3cbb7a7c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 1896 | Contributors (this commit): 27 | Commits (past 90d): 10 | Contributors (cumulative): 27 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -617,14 +617,14 @@ class ReduceLROnPlateau(Callback):\n             warnings.warn('Learning Rate Plateau Reducing requires %s available!' %\n                           self.monitor, RuntimeWarning)\n         else:\n-            if self.cooldown_counter > 0:\n+            if self.in_cooldown():\n                 self.cooldown_counter -= 1\n                 self.wait = 0\n \n             if self.monitor_op(current, self.best):\n                 self.best = current\n                 self.wait = 0\n-            elif self.cooldown_counter <= 0:\n+            elif not self.in_cooldown():\n                 if self.wait >= self.patience:\n                     old_lr = float(K.get_value(self.model.optimizer.lr))\n                     if old_lr > self.min_lr + self.lr_epsilon:\n@@ -634,8 +634,12 @@ class ReduceLROnPlateau(Callback):\n                         if self.verbose > 0:\n                             print('\\nEpoch %05d: reducing learning rate to %s.' % (epoch, new_lr))\n                         self.cooldown_counter = self.cooldown\n+                        self.wait = 0\n                 self.wait += 1\n \n+    def in_cooldown(self):\n+        return self.cooldown_counter > 0\n+\n \n class CSVLogger(Callback):\n     '''Callback that streams epoch results to a csv file.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#999f4028295217324be0fe12487444fc510ca6f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 165 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,6 +66,12 @@ def binary_crossentropy(y_true, y_pred):\n     return K.mean(K.binary_crossentropy(y_pred, y_true))\n \n \n+def kullback_leibler_divergence(y_true, y_pred):\n+    y_true = K.clip(y_true, K.epsilon(), 1)\n+    y_pred = K.clip(y_pred, K.epsilon(), 1)\n+    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n+\n+\n def poisson(y_true, y_pred):\n     return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef7911310daeb810f25d3b85001e5f5efef2ebfc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 13 | Churn Cumulative: 2443 | Contributors (this commit): 40 | Commits (past 90d): 27 | Contributors (cumulative): 40 | DMM Complexity: 0.0\n\nDIFF:\n@@ -395,6 +395,19 @@ def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=0.0001):\n     '''Compute mean and std for batch then apply batch_normalization on batch.\n     '''\n+    dev = theano.config.device\n+    use_cudnn = ndim(x) < 5 and reduction_axes == [0, 2, 3] and (dev.startswith('cuda') or dev.startswith('gpu'))\n+    if use_cudnn:\n+        broadcast_beta = beta.dimshuffle('x', 0, 'x', 'x')\n+        broadcast_gamma = gamma.dimshuffle('x', 0, 'x', 'x')\n+        try:\n+            normed, mean, stdinv = theano.sandbox.cuda.dnn.dnn_batch_normalization_train(\n+                x, broadcast_gamma, broadcast_beta, 'spatial', epsilon)\n+            var = T.inv(stdinv ** 2)\n+            return normed, T.flatten(mean), T.flatten(var)\n+        except AttributeError:\n+            pass\n+\n     var = x.var(reduction_axes)\n     mean = x.mean(reduction_axes)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e42b0e4a77fb171295b541a6ae9a3a4a79f9c87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 56 | Churn Cumulative: 2441 | Contributors (this commit): 23 | Commits (past 90d): 14 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,9 @@ import time\n import numpy as np\n import multiprocessing\n import threading\n+\n+import six\n+\n try:\n     import queue\n except ImportError:\n@@ -635,6 +638,15 @@ class Model(Container):\n         # list of same size as output_names.\n         # contains tuples (metrics for output, names of metrics)\n         nested_metrics = collect_metrics(metrics, self.output_names)\n+\n+        def append_metric(layer_num, metric_name, metric_tensor):\n+            \"\"\"Helper function, used in loop below\"\"\"\n+            if len(self.output_names) > 1:\n+                metric_name = self.output_layers[layer_num].name + '_' + metric_name\n+\n+            self.metrics_names.append(metric_name)\n+            self.metrics_tensors.append(metric_tensor)\n+\n         for i in range(len(self.outputs)):\n             y_true = self.targets[i]\n             y_pred = self.outputs[i]\n@@ -644,27 +656,28 @@ class Model(Container):\n                 if metric == 'accuracy' or metric == 'acc':\n                     # custom handling of accuracy (because of class mode duality)\n                     output_shape = self.internal_output_shapes[i]\n+                    acc_fn = None\n                     if output_shape[-1] == 1 or self.loss_functions[i] == objectives.binary_crossentropy:\n                         # case: binary accuracy\n-                        self.metrics_tensors.append(metrics_module.binary_accuracy(y_true, y_pred))\n+                        acc_fn = metrics_module.binary_accuracy\n                     elif self.loss_functions[i] == objectives.sparse_categorical_crossentropy:\n                         # case: categorical accuracy with sparse targets\n-                        self.metrics_tensors.append(\n-                            metrics_module.sparse_categorical_accuracy(y_true, y_pred))\n+                        acc_fn = metrics_module.sparse_categorical_accuracy\n                     else:\n-                        # case: categorical accuracy with dense targets\n-                        self.metrics_tensors.append(metrics_module.categorical_accuracy(y_true, y_pred))\n-                    if len(self.output_names) == 1:\n-                        self.metrics_names.append('acc')\n-                    else:\n-                        self.metrics_names.append(self.output_layers[i].name + '_acc')\n+                        acc_fn = metrics_module.categorical_accuracy\n+\n+                    append_metric(i, 'acc', acc_fn(y_true, y_pred))\n                 else:\n                     metric_fn = metrics_module.get(metric)\n-                    self.metrics_tensors.append(metric_fn(y_true, y_pred))\n-                    if len(self.output_names) == 1:\n-                        self.metrics_names.append(metric_fn.__name__)\n-                    else:\n-                        self.metrics_names.append(self.output_layers[i].name + '_' + metric_fn.__name__)\n+                    metric_result = metric_fn(y_true, y_pred)\n+\n+                    if not isinstance(metric_result, dict):\n+                        metric_result = {\n+                            metric_fn.__name__: metric_result\n+                        }\n+\n+                    for name, tensor in six.iteritems(metric_result):\n+                        append_metric(i, name, tensor)\n \n         # prepare gradient updates and state updates\n         self.optimizer = optimizers.get(optimizer)\n\n@@ -148,15 +148,24 @@ def test_model_methods():\n \n     # test with a custom metric function\n     mse = lambda y_true, y_pred: K.mean(K.pow(y_true - y_pred, 2))\n-    model.compile(optimizer, loss, metrics=[mse],\n+\n+    def mse_powers(y_true, y_pred):\n+        m = mse(y_true, y_pred)\n+        return {\n+            'mse_squared': K.pow(m, 2),\n+            'mse_cubed': K.pow(m, 3)\n+        }\n+\n+    model.compile(optimizer, loss, metrics=[mse, mse_powers],\n                   sample_weight_mode=None)\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n-    assert len(out) == 5\n+    out_len = 1 + 2 * 4  # total loss, per layer: loss + 3 metrics\n+    assert len(out) == out_len\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n-    assert len(out) == 5\n+    assert len(out) == out_len\n \n     input_a_np = np.random.random((10, 3))\n     input_b_np = np.random.random((10, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d7a2338b45d646772934da0bef30567811479b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 167 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,8 +6,8 @@ Bags of Tricks for Efficient Text Classification\n https://arxiv.org/abs/1607.01759\n \n Results on IMDB datasets with uni and bi-gram embeddings:\n-    Uni-gram: 0.8813 test accuracy after 5 epochs. 15s/epoch on i7 cpu.\n-    Bi-gram : 0.9056 test accuracy after 5 epochs. 5s/epoch on GTX 1080 gpu.\n+    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n+    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTX 980M gpu.\n '''\n \n from __future__ import print_function\n@@ -16,10 +16,11 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers import Dense, Flatten\n+from keras.layers import Dense\n from keras.layers import Embedding\n-from keras.layers import AveragePooling1D\n+from keras.layers import GlobalAveragePooling1D\n from keras.datasets import imdb\n+from keras import backend as K\n \n \n def create_ngram_set(input_list, ngram_value=2):\n@@ -119,12 +120,9 @@ model.add(Embedding(max_features,\n                     embedding_dims,\n                     input_length=maxlen))\n \n-# we add a AveragePooling1D, which will average the embeddings\n+# we add a GlobalAveragePooling1D, which will average the embeddings\n # of all words in the document\n-model.add(AveragePooling1D(pool_length=model.output_shape[1]))\n-\n-# We flatten the output of the AveragePooling1D layer\n-model.add(Flatten())\n+model.add(GlobalAveragePooling1D())\n \n # We project onto a single unit output layer, and squash it with a sigmoid:\n model.add(Dense(1, activation='sigmoid'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d7d1db5d79fa8dee72335c82334ddab6c6919a6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1306 | Contributors (this commit): 12 | Commits (past 90d): 14 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -299,12 +299,10 @@ def test_maxpooling_2d():\n \n @keras_test\n def test_averagepooling_2d():\n-    pool_size = (3, 3)\n-\n     for border_mode in ['valid', 'same']:\n         for pool_size in [(2, 2), (3, 3), (4, 4), (5, 5)]:\n             for strides in [(1, 1), (2, 2)]:\n-                layer_test(convolutional.MaxPooling2D,\n+                layer_test(convolutional.AveragePooling2D,\n                            kwargs={'strides': strides,\n                                    'border_mode': border_mode,\n                                    'pool_size': pool_size},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53552b1d6eab5ead1ad94eb670b610f81c348802", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 80 | Files Changed: 3 | Hunks: 23 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 134 | Churn Cumulative: 887 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,3 +10,4 @@ from .embeddings import *\n from .noise import *\n from .advanced_activations import *\n from .wrappers import *\n+from .recurrent_convolutional import *\n\\ No newline at end of file\n\n@@ -75,15 +75,13 @@ class RecurrentConv2D(Layer):\n         a specific layer, or on your entire model.\n     '''\n \n-    def __init__(self, weights=None,\n+    def __init__(self, weights=None,nb_row=None, nb_col=None, nb_filter=None,\n                  return_sequences=False, go_backwards=False, stateful=False,\n-                 nb_row=None, nb_col=None, nb_filter=None,\n                  dim_ordering=None, **kwargs):\n         self.return_sequences = return_sequences\n-        self.initial_weights = weights\n         self.go_backwards = go_backwards\n         self.stateful = stateful\n-\n+        self.initial_weights = weights\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n         self.nb_filter = nb_filter\n@@ -152,11 +150,10 @@ class RecurrentConv2D(Layer):\n         return x\n \n     def call(self, x, mask=None):\n-\n         assert K.ndim(x) == 5\n         input_shape = self.input_spec[0].shape\n         unroll = False\n-        if K._BACKEND == 'tensorflow':\n+        if K.backend() == 'tensorflow':\n             if not input_shape[1]:\n                 raise Exception('When using TensorFlow, you should define ' +\n                                 'explicitely the number of timesteps of ' +\n@@ -192,8 +189,7 @@ class RecurrentConv2D(Layer):\n             return last_output\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'return_sequences': self.return_sequences,\n+        config = {'return_sequences': self.return_sequences,\n                   'go_backwards': self.go_backwards,\n                   'stateful': self.stateful}\n         if self.stateful:\n@@ -263,10 +259,15 @@ class LSTMConv2D(RecurrentConv2D):\n                  init='glorot_uniform', inner_init='orthogonal',\n                  forget_bias_init='one', activation='tanh',\n                  inner_activation='hard_sigmoid',\n-                 dim_ordering=K.image_dim_ordering(),\n+                 dim_ordering='default',\n                  border_mode='valid', sub_sample=(1, 1),\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n+\n+        if dim_ordering == 'default':\n+            dim_ordering = K.image_dim_ordering()\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf,th}',dim_ordering)\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n@@ -278,7 +279,7 @@ class LSTMConv2D(RecurrentConv2D):\n         self.border_mode = border_mode\n         self.subsample = sub_sample\n \n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf,th}'\n+        \n \n         if dim_ordering == 'th':\n             warnings.warn('Be carefull if used with convolution3D layers:\\n'\n@@ -380,15 +381,16 @@ class LSTMConv2D(RecurrentConv2D):\n     def reset_states(self):\n         assert self.stateful, 'Layer must be stateful.'\n         input_shape = self.input_spec[0].shape\n+        output_shape = self.get_output_shape_for(input_shape)\n         if not input_shape[0]:\n             raise Exception('If a RNN is stateful, a complete ' +\n                             'input_shape must be provided ' +\n                             '(including batch size).')\n \n         if self.return_sequences:\n-            out_row, out_col, out_filter = self.output_shape[2:]\n+            out_row, out_col, out_filter = output_shape[2:]\n         else:\n-            out_row, out_col, out_filter = self.output_shape[1:]\n+            out_row, out_col, out_filter = output_shape[1:]\n \n         if hasattr(self, 'states'):\n             K.set_value(self.states[0],\n@@ -503,8 +505,7 @@ class LSTMConv2D(RecurrentConv2D):\n         return constants\n \n     def get_config(self):\n-        config = {'name': self.__class__.__name__,\n-                  'nb_filter': self.nb_filter,\n+        config = {'nb_filter': self.nb_filter,\n                   'nb_row': self.nb_row,\n                   'nb_col': self.nb_col,\n                   'init': self.init.__name__,\n\n@@ -4,80 +4,52 @@ from numpy.testing import assert_allclose\n \n from keras import backend as K\n from keras.models import Sequential\n-from keras.layers.recurrent_convolutional import LSTMConv2D\n+from keras.layers import recurrent_convolutional\n+from keras.utils.test_utils import layer_test\n \n \n-def test_shape2():\n-    # With return_sequences = True\n-    input_shape = [10, 30, 30, 3]\n-    batch = 5\n-    nfilter = 20\n-    input_a = np.zeros([batch]+input_shape)\n-    gt_shape = (batch, input_shape[0], input_shape[1], input_shape[2], nfilter)\n-    gt = np.zeros(gt_shape)\n-    input_shape = tuple(input_shape)\n-    seq = Sequential()\n-    seq.add(LSTMConv2D(nb_filter=20, nb_row=4, nb_col=4,\n-                       input_shape=input_shape, border_mode=\"same\",\n-                       return_sequences=True))\n-    seq.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n-    assert seq.predict(input_a).shape == gt_shape\n-    seq.fit(input_a, gt, nb_epoch=1)\n+def test_recurrent_convolutional():\n+    # First test for ouptput shape:\n+    nb_row = 4\n+    nb_col = 4\n+    nb_filter = 20\n+    nb_samples = 5\n+    input_channel = 3\n+    input_nb_row = 30\n+    input_nb_col = 30\n+    sequence_len  = 10\n+    for dim_ordering in ['th', 'tf']:\n \n+        if dim_ordering == 'th':\n+            input = np.random.rand(nb_samples, sequence_len,\n+                                   input_channel,\n+                                   input_nb_row, input_nb_col)\n+        else:  # tf\n+            input = np.random.rand(nb_samples, sequence_len,\n+                                   input_nb_row, input_nb_col,\n+                                   input_channel)\n  \n-def test_shape_th_return_sequences():\n-    input_shape = [10, 3, 30, 30]\n-    batch = 5\n-    nfilter = 20\n-    input_a = np.zeros([batch]+input_shape)\n-    gt_shape = (batch, input_shape[0], nfilter, input_shape[2], input_shape[3])\n-    gt = np.zeros(gt_shape)\n-    input_shape = tuple(input_shape)\n-    seq = Sequential()\n-    seq.add(LSTMConv2D(nb_filter=nfilter, nb_row=4, nb_col=4,\n-                       dim_ordering=\"th\", input_shape=input_shape,\n-            border_mode=\"same\", return_sequences=True))\n+        for return_sequences in [True,False]:\n+            output = layer_test(recurrent_convolutional.LSTMConv2D,\n+                       kwargs={'dim_ordering' : dim_ordering,\n+                               'return_sequences' : return_sequences,\n+                               'nb_filter': nb_filter,\n+                               'nb_row' : nb_row,\n+                               'nb_col' : nb_col,\n+                               'border_mode': \"same\"},\n+                       input_shape=input.shape)\n          \n-    seq.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n-    assert seq.predict(input_a).shape == gt_shape\n+            output_shape = [nb_samples, input_nb_row, input_nb_col]\n \n-    # seq.fit(input_a, gt, nb_epoch=1)\n+            if dim_ordering == 'th':\n+                output_shape.insert(1, nb_filter)\n+            else:\n+                output_shape.insert(3, nb_filter)\n \n+            if return_sequences:\n+                output_shape.insert(1, sequence_len)\n                 \n-def test_shape_th():\n-    input_shape = [10, 3, 30, 30]\n-    batch = 5\n-    nfilter = 20\n-    input_a = np.zeros([batch]+input_shape)\n-    gt_shape = (batch, nfilter, input_shape[2], input_shape[3])\n-    gt = np.zeros(gt_shape)\n-    input_shape = tuple(input_shape)\n-    seq = Sequential()\n-    input_shape = tuple(input_shape)\n-    seq.add(LSTMConv2D(nb_filter=nfilter, nb_row=4, nb_col=4,\n-                       dim_ordering=\"th\", input_shape=input_shape,\n-            border_mode=\"same\", return_sequences=False))\n-    seq.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n-    assert seq.predict(input_a).shape == gt_shape\n-    # seq.fit(input_a, gt, nb_epoch=1)\n-\n-\n-def test_shape():\n-    input_shape = [10, 30, 30, 3]\n-    batch = 5\n-    nfilter = 20\n-    input_a = np.zeros([batch]+input_shape)\n-    gt_shape = (batch, input_shape[1], input_shape[2], nfilter)\n-    gt = np.zeros(gt_shape)\n-    input_shape = tuple(input_shape)\n-    seq = Sequential()\n-    seq.add(LSTMConv2D(nb_filter=nfilter, nb_row=4, nb_col=4,\n-                       input_shape=input_shape,\n-                       border_mode=\"same\", return_sequences=False))\n-    seq.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\n-    assert seq.predict(input_a).shape == gt_shape\n-    # seq.fit(input_a, gt, nb_epoch=1)\n-\n+            assert output.shape == tuple(output_shape)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
