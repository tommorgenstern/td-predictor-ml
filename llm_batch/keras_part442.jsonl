{"custom_id": "keras#ce8d99b2815c1b93b6b71246f0685246f3732b10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1233 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 35 | Complexity Δ (Sum/Max): 60/60 | Churn Δ: 1233 | Churn Cumulative: 1233 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.8563829787234043\n\nDIFF:\n@@ -0,0 +1,1233 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+# pylint: disable=invalid-name\n+# pylint: disable=missing-docstring\n+# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n+\"\"\"ConvNeXt models for Keras.\n+\n+References:\n+\n+- [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n+  (CVPR 2022)\n+\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+from keras import backend, layers\n+from keras.applications import imagenet_utils\n+from keras.engine import training\n+from keras.utils import data_utils, layer_utils\n+from tensorflow.python.util.tf_export import keras_export\n+\n+BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/convnext-tf/keras-applications/convnext/\"\n+\n+WEIGHTS_HASHES = {\n+    \"x002\":\n+        (\"49fb46e56cde07fdaf57bffd851461a86548f6a3a4baef234dd37290b826c0b8\",\n+         \"5445b66cd50445eb7ecab094c1e78d4d3d29375439d1a7798861c4af15ffff21\"),\n+    \"x004\":\n+        (\"3523c7f5ac0dbbcc2fd6d83b3570e7540f7449d3301cc22c29547302114e4088\",\n+         \"de139bf07a66c9256f2277bf5c1b6dd2d5a3a891a5f8a925a10c8a0a113fd6f3\"),\n+    \"x006\":\n+        (\"340216ef334a7bae30daac9f414e693c136fac9ab868704bbfcc9ce6a5ec74bb\",\n+         \"a43ec97ad62f86b2a96a783bfdc63a5a54de02eef54f26379ea05e1bf90a9505\"),\n+    \"x008\":\n+        (\"8f145d6a5fae6da62677bb8d26eb92d0b9dfe143ec1ebf68b24a57ae50a2763d\",\n+         \"3c7e4b0917359304dc18e644475c5c1f5e88d795542b676439c4a3acd63b7207\"),\n+    \"x016\":\n+        (\"31c386f4c7bfef4c021a583099aa79c1b3928057ba1b7d182f174674c5ef3510\",\n+         \"1b8e3d545d190271204a7b2165936a227d26b79bb7922bac5ee4d303091bf17a\"),\n+    \"x032\":\n+        (\"6c025df1409e5ea846375bc9dfa240956cca87ef57384d93fef7d6fa90ca8c7f\",\n+         \"9cd4522806c0fcca01b37874188b2bd394d7c419956d77472a4e072b01d99041\"),\n+    \"x040\":\n+        (\"ba128046c588a26dbd3b3a011b26cb7fa3cf8f269c184c132372cb20b6eb54c1\",\n+         \"b4ed0ca0b9a98e789e05000e830403a7ade4d8afa01c73491c44610195198afe\"),\n+    \"x064\":\n+        (\"0f4489c3cd3ad979bd6b0324213998bcb36dc861d178f977997ebfe53c3ba564\",\n+         \"3e706fa416a18dfda14c713423eba8041ae2509db3e0a611d5f599b5268a46c4\"),\n+    \"x080\":\n+        (\"76320e43272719df648db37271a247c22eb6e810fe469c37a5db7e2cb696d162\",\n+         \"7b1ce8e29ceefec10a6569640ee329dba7fbc98b5d0f6346aabade058b66cf29\"),\n+    \"x120\":\n+        (\"5cafc461b78897d5e4f24e68cb406d18e75f31105ef620e7682b611bb355eb3a\",\n+         \"36174ddd0299db04a42631d028abcb1cc7afec2b705e42bd28fcd325e5d596bf\"),\n+    \"x160\":\n+        (\"8093f57a5824b181fb734ea21ae34b1f7ee42c5298e63cf6d587c290973195d2\",\n+         \"9d1485050bdf19531ffa1ed7827c75850e0f2972118a996b91aa9264b088fd43\"),\n+    \"x320\":\n+        (\"91fb3e6f4e9e44b3687e80977f7f4412ee9937c0c704232664fc83e4322ea01e\",\n+         \"9db7eacc37b85c98184070e1a172e6104c00846f44bcd4e727da9e50d9692398\"),\n+    \"y002\":\n+        (\"1e8091c674532b1a61c04f6393a9c570113e0197f22bd1b98cc4c4fe800c6465\",\n+         \"f63221f63d625b8e201221499682587bfe29d33f50a4c4f4d53be00f66c0f12c\"),\n+    \"y004\":\n+        (\"752fdbad21c78911bf1dcb8c513e5a0e14697b068e5d9e73525dbaa416d18d8e\",\n+         \"45e6ba8309a17a77e67afc05228454b2e0ee6be0dae65edc0f31f1da10cc066b\"),\n+    \"y006\":\n+        (\"98942e07b273da500ff9699a1f88aca78dfad4375faabb0bab784bb0dace80a9\",\n+         \"b70261cba4e60013c99d130cc098d2fce629ff978a445663b6fa4f8fc099a2be\"),\n+    \"y008\":\n+        (\"1b099377cc9a4fb183159a6f9b24bc998e5659d25a449f40c90cbffcbcfdcae4\",\n+         \"b11f5432a216ee640fe9be6e32939defa8d08b8d136349bf3690715a98752ca1\"),\n+    \"y016\":\n+        (\"b7ce1f5e223f0941c960602de922bcf846288ce7a4c33b2a4f2e4ac4b480045b\",\n+         \"d7404f50205e82d793e219afb9eb2bfeb781b6b2d316a6128c6d7d7dacab7f57\"),\n+    \"y032\":\n+        (\"6a6a545cf3549973554c9b94f0cd40e25f229fffb1e7f7ac779a59dcbee612bd\",\n+         \"eb3ac1c45ec60f4f031c3f5180573422b1cf7bebc26c004637517372f68f8937\"),\n+    \"y040\":\n+        (\"98d00118b335162bbffe8f1329e54e5c8e75ee09b2a5414f97b0ddfc56e796f6\",\n+         \"b5be2a5e5f072ecdd9c0b8a437cd896df0efa1f6a1f77e41caa8719b7dfcb05d\"),\n+    \"y064\":\n+        (\"65c948c7a18aaecaad2d1bd4fd978987425604ba6669ef55a1faa0069a2804b7\",\n+         \"885c4b7ed7ea339daca7dafa1a62cb7d41b1068897ef90a5a3d71b4a2e2db31a\"),\n+    \"y080\":\n+        (\"7a2c62da2982e369a4984d3c7c3b32d6f8d3748a71cb37a31156c436c37f3e95\",\n+         \"3d119577e1e3bf8d153b895e8ea9e4ec150ff2d92abdca711b6e949c3fd7115d\"),\n+    \"y120\":\n+        (\"a96ab0d27d3ae35a422ee7df0d789069b3e3217a99334e0ce861a96595bc5986\",\n+         \"4a6fa387108380b730b71feea2ad80b5224b5ea9dc21dc156c93fe3c6186485c\"),\n+    \"y160\":\n+        (\"45067240ffbc7ca2591313fee2f80dbdda6d66ec1a7451446f9a6d00d8f7ac6e\",\n+         \"ead1e6b568be8f34447ec8941299a9df4368736ba9a8205de5427fa20a1fb316\"),\n+    \"y320\": (\"b05e173e4ae635cfa22d06392ee3741284d17dadfee68f2aa6fd8cb2b7561112\",\n+             \"cad78f74a586e24c61d38be17f3ae53bb9674380174d2585da1a526b8c20e1fd\")\n+}\n+\n+\n+MODEL_CONFIGS = {\n+    \"tiny\": {\n+        \"depths\": [3, 3, 9, 3],\n+        \"projection_dims\": [96, 192, 384, 768],\n+        \"default_size\": 224,\n+    },\n+    \"small\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [96, 192, 384, 768],\n+        \"default_size\": 224,\n+    },\n+    \"base\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [128, 256, 512, 1024],\n+        \"default_size\": 224,\n+    },\n+    \"large\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [192, 384, 768, 1536],\n+        \"default_size\": 224,\n+    },\n+    \"xlarge\": {\n+        \"depths\": [3, 3, 27, 3],\n+        \"projection_dims\": [256, 512, 1024, 2048],\n+        \"default_size\": 224,\n+    },\n+}\n+\n+BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n+\n+  Reference:\n+    - [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n+    (CVPR 2022)\n+\n+  For image classification use cases, see\n+  [this page for detailed examples](\n+  https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+  For transfer learning use cases, make sure to read the\n+  [guide to transfer learning & fine-tuning](\n+    https://keras.io/guides/transfer_learning/).\n+\n+  Note: Each Keras Application expects a specific kind of input preprocessing.\n+  For ConvNeXt, preprocessing is included in the model using a `Normalization` layer.\n+  ConvNeXt models expect their inputs to be float or uint8 tensors of pixels with\n+  values in the [0-255] range.\n+\n+  Args:\n+    include_top: Whether to include the fully-connected\n+        layer at the top of the network. Defaults to True.\n+    weights: One of `None` (random initialization),\n+          `\"imagenet\"` (pre-training on ImageNet), or the path to the weights\n+          file to be loaded. Defaults to `\"imagenet\"`.\n+    input_tensor: Optional Keras tensor\n+        (i.e. output of `layers.Input()`)\n+        to use as image input for the model.\n+    input_shape: Optional shape tuple, only to be specified\n+        if `include_top` is False.\n+        It should have exactly 3 inputs channels.\n+    pooling: Optional pooling mode for feature extraction\n+        when `include_top` is `False`. Defaults to None.\n+        - `None` means that the output of the model will be\n+            the 4D tensor output of the\n+            last convolutional layer.\n+        - `avg` means that global average pooling\n+            will be applied to the output of the\n+            last convolutional layer, and thus\n+            the output of the model will be a 2D tensor.\n+        - `max` means that global max pooling will\n+            be applied.\n+    classes: Optional number of classes to classify images\n+        into, only to be specified if `include_top` is True, and\n+        if no `weights` argument is specified. Defaults to 1000 (number of\n+        ImageNet classes).\n+    classifier_activation: A `str` or callable. The activation function to use\n+        on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        `classifier_activation=None` to return the logits of the \"top\" layer.\n+        Defaults to `\"softmax\"`.\n+        When loading pretrained weights, `classifier_activation` can only\n+        be `None` or `\"softmax\"`.\n+\n+  Returns:\n+    A `keras.Model` instance.\n+\"\"\"\n+\n+class StochasticDepth(layers.Layer):\n+    \"\"\"Stochastic Depth module. It performs batch-wise dropping rather than\n+      sample-wise. In libraries like `timm`, it's similar to `DropPath` layers\n+      that drops residual paths sample-wise.\n+\n+    Reference:\n+      - github.com.rwightman/pytorch-image-models\n+\n+    Args:\n+      drop_path (float): Probability of dropping paths. Should be within [0, 1].\n+    \n+    Returns:\n+      Tensor either with the residual path dropped or kept.\n+\n+    \"\"\"\n+\n+    def __init__(self, drop_path, **kwargs):\n+      super().__init__(**kwargs)\n+      self.drop_path = drop_path\n+\n+    def call(self, x, training):\n+      if training:\n+        keep_prob = 1 - self.drop_path\n+        shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n+        random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n+        random_tensor = tf.floor(random_tensor)\n+        return (x / keep_prob) * random_tensor\n+      return x\n+\n+\n+class Block(tf.keras.Model):\n+    \"\"\"ConvNeXt block.\n+    \n+    References:\n+      - https://arxiv.org/abs/2201.03545\n+      - https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py\n+\n+    Notes:\n+      In the original ConvNeXt implementation (linked above), the authors use\n+      `Dense` layers for pointwise convolutions for increased efficiency. Following\n+      that, this implementation also uses the same.\n+\n+    Args:\n+      projection_dim (int): Number of filters for convolution layers. In the ConvNeXt paper, this is\n+        referred to as projection dimension.\n+      drop_path (float): Probability of dropping paths. Should be within [0, 1].\n+      layer_scale_init_value (float): Layer scale value. Should be a small float number.\n+\n+    Returns:\n+      A keras.Model instance.\n+    \"\"\"\n+    def __init__(self, projection_dim, drop_path=0.0, layer_scale_init_value=1e-6, **kwargs):\n+      super().__init__(**kwargs)\n+      self.projection_dim = projection_dim\n+      self.name = kwargs[\"name\"]\n+      \n+      if layer_scale_init_value > 0:\n+        self.gamma = tf.Variable(layer_scale_init_value * tf.ones((projection_dim,)), name=self.name + \"_layer_scale_gamma\")\n+      else:\n+        self.gamma = None\n+      \n+      self.dw_conv_1 = layers.Conv2D(\n+        filters=projection_dim, kernel_size=7, padding=\"same\", groups=projection_dim,\n+        name=self.name + \"_depthwise_conv\"\n+      )\n+      self.layer_norm = layers.LayerNormalization(epsilon=1e-6, name=self.name + \"_layernorm\")\n+      self.pw_conv_1 = layers.Dense(4 * projection_dim, name=self.name + \"_pointwise_conv_1\")\n+      self.act_fn = layers.Activation(\"gelu\", name=self.name + \"_gelu\")\n+      self.pw_conv_2 = layers.Dense(projection_dim, name=self.name + \"_pointwise_conv_2\")\n+      self.drop_path = (\n+        StochasticDepth(drop_path, name=self.name + \"_stochastic_depth\")\n+        if drop_path > 0.0\n+        else layers.Activation(\"linear\", name=self.name + \"_identity\")\n+      )\n+\n+    def call(self, inputs):\n+        x = inputs\n+\n+        x = self.dw_conv_1(x)\n+        x = self.layer_norm(x)\n+        x = self.pw_conv_1(x)\n+        x = self.act_fn(x)\n+        x = self.pw_conv_2(x)\n+\n+        if self.gamma is not None:\n+            x = self.gamma * x\n+\n+        return inputs + self.drop_path(x)\n+\n+\n+def PreStem(name=None):\n+  \"\"\"Normalizes inputs with ImageNet-1k mean and std.\n+\n+  Args:\n+    name (str): Name prefix.\n+\n+  Returns:\n+    Normalized tensor.\n+  \"\"\"\n+  if name is None:\n+    name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n+\n+  def apply(x):\n+    x = layers.Normalization(\n+      mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n+      variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n+      name=name + \"_prestem_normalization\"\n+    )\n+    return x\n+\n+  return apply\n+\n+\n+def Head(num_classes=1000, name=None):\n+  \"\"\"Implementation of classification head of RegNet.\n+\n+  Args:\n+    num_classes: number of classes for Dense layer\n+    name: name prefix\n+\n+  Returns:\n+    Output logits tensor.\n+  \"\"\"\n+  if name is None:\n+    name = str(backend.get_uid(\"head\"))\n+\n+  def apply(x):\n+    x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n+    x = layers.LayerNormalization(epsilon=1e-6, name=name + \"_head_layernorm\")(x)\n+    x = layers.Dense(num_classes, name=name + \"head_dense\")(x)\n+    return x\n+\n+  return apply\n+\n+\n+def ConvNeXt(depths,\n+           projection_dims,\n+           drop_path_rate,\n+           layer_scale_init_value,\n+           default_size,\n+           model_name=\"convnext\",\n+           include_preprocessing=True,\n+           include_top=True,\n+           weights=\"imagenet\",\n+           input_tensor=None,\n+           input_shape=None,\n+           pooling=None,\n+           classes=1000,\n+           classifier_activation=\"softmax\"):\n+  \"\"\"Instantiates ConvNeXt architecture given specific configuration.\n+\n+  Args:\n+    depths: An iterable containing depths for each individual stages.\n+    projection_dims: An iterable containing output number of channels of each individual\n+      stages.\n+    drop_path_rate: Stochastic depth probability.\n+    layer_scale_init_value: Layer scale coefficient.\n+    default_size: Default input image size.\n+    model_name: An optional name for the model.\n+    include_preprocessing: boolean denoting whther to include preprocessing in\n+      the model.\n+    include_top: Boolean denoting whether to include classification head to the\n+      model.\n+    weights: one of `None` (random initialization), \"imagenet\" (pre-training on\n+      ImageNet), or the path to the weights file to be loaded.\n+    input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use\n+      as image input for the model.\n+    input_shape: optional shape tuple, only to be specified if `include_top` is\n+      False. It should have exactly 3 inputs channels.\n+    pooling: optional pooling mode for feature extraction when `include_top` is\n+      `False`. - `None` means that the output of the model will be the 4D tensor\n+      output of the last convolutional layer. - `avg` means that global average\n+      pooling will be applied to the output of the last convolutional layer, and\n+      thus the output of the model will be a 2D tensor. - `max` means that\n+      global max pooling will be applied.\n+    classes: optional number of classes to classify images into, only to be\n+      specified if `include_top` is True, and if no `weights` argument is\n+      specified.\n+    classifier_activation: A `str` or callable. The activation function to use\n+      on the \"top\" layer. Ignored unless `include_top=True`. Set\n+      `classifier_activation=None` to return the logits of the \"top\" layer.\n+\n+  Returns:\n+    A `keras.Model` instance.\n+\n+  Raises:\n+      ValueError: in case of invalid argument for `weights`,\n+        or invalid input shape.\n+      ValueError: if `classifier_activation` is not `softmax` or `None` when\n+        using a pretrained top layer.\n+      ValueError: if `include_top` is True but `num_classes` is not 1000.\n+  \"\"\"\n+  if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n+    raise ValueError(\"The `weights` argument should be either \"\n+                     \"`None` (random initialization), `imagenet` \"\n+                     \"(pre-training on ImageNet), \"\n+                     \"or the path to the weights file to be loaded.\")\n+\n+  if weights == \"imagenet\" and include_top and classes != 1000:\n+    raise ValueError(\"If using `weights` as `'imagenet'` with `include_top`\"\n+                     \" as true, `classes` should be 1000\")\n+\n+  # Determine proper input shape.\n+  input_shape = imagenet_utils.obtain_input_shape(\n+      input_shape,\n+      default_size=default_size,\n+      min_size=32,\n+      data_format=backend.image_data_format(),\n+      require_flatten=include_top,\n+      weights=weights)\n+\n+  if input_tensor is None:\n+    img_input = layers.Input(shape=input_shape)\n+  else:\n+    if not backend.is_keras_tensor(input_tensor):\n+      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+    else:\n+      img_input = input_tensor\n+\n+  if input_tensor is not None:\n+    inputs = layer_utils.get_source_inputs(input_tensor)\n+  else:\n+    inputs = img_input\n+\n+  x = inputs\n+  if include_preprocessing:\n+    x = PreStem(name=model_name)(x)\n+  \n+  # Stem block.\n+  stem = tf.keras.Sequential(\n+    [\n+      layers.Conv2D(projection_dims[0], kernel_size=4, strides=4, name=model_name + \"_stem_conv\"),\n+      layers.LayerNormalization(epsilon=1e-6, name=model_name + \"_stem_layernorm\"),\n+    ],\n+    name=model_name + \"_stem\",\n+  )\n+\n+  # Downsampling blocks.\n+  downsample_layers = []\n+  downsample_layers.append(stem)\n+  for i in range(3):\n+    downsample_layer = tf.keras.Sequential(\n+        [\n+          layers.LayerNormalization(epsilon=1e-6, name=model_name + \"_downsampling_layernorm_\" + str(i+1)),\n+          layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2, name=model_name + \"_downsampling_conv_\" + str(i+1)),\n+        ],\n+        name=model_name + \"_downsampling_block_\" + str(i+1),\n+    )\n+    downsample_layers.append(downsample_layer)\n+  \n+  # Stochastic depth schedule.\n+  dp_rates = [x for x in tf.linspace(0.0, drop_path_rate, sum(depths))]\n+\n+  # ConvNeXt stages.\n+  stages = []\n+  cur = 0\n+  for i in range(4):\n+    stage = tf.keras.Sequential(\n+      [\n+        *[\n+            Block(\n+              dim=projection_dims[i],\n+              drop_path=dp_rates[cur + j],\n+              layer_scale_init_value=layer_scale_init_value,\n+              name=model_name + f\"stage_{i}_block_{j}\",\n+            )\n+            for j in range(depths[i])\n+        ]\n+      ],\n+      name=model_name + f\"_stage_{i}\",\n+    )\n+    stages.append(stage)\n+    cur += depths[i]\n+\n+  if include_top:\n+    x = Head(num_classes=classes)(x)\n+    imagenet_utils.validate_activation(classifier_activation, weights)\n+\n+  else:\n+    if pooling == \"avg\":\n+      x = layers.GlobalAveragePooling2D()(x)\n+    elif pooling == \"max\":\n+      x = layers.GlobalMaxPooling2D()(x)\n+      x = layers.LayerNormalization(epsilon=1e-6)(x)\n+\n+  model = training.Model(inputs=inputs, outputs=x, name=model_name)\n+\n+  # Load weights.\n+  if weights == \"imagenet\":\n+    if include_top:\n+      file_suffix = \".h5\"\n+      file_hash = WEIGHTS_HASHES[model_name[-4:]][0]\n+    else:\n+      file_suffix = \"_notop.h5\"\n+      file_hash = WEIGHTS_HASHES[model_name[-4:]][1]\n+    file_name = model_name + file_suffix\n+    weights_path = data_utils.get_file(\n+        file_name,\n+        BASE_WEIGHTS_PATH + file_name,\n+        cache_subdir=\"models\",\n+        file_hash=file_hash)\n+    model.load_weights(weights_path)\n+  elif weights is not None:\n+    model.load_weights(weights)\n+\n+  return model\n+\n+\n+## Instantiating variants ##\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX002\",\n+              \"keras.applications.RegNetX002\")\n+def RegNetX002(model_name=\"regnetx002\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x002\"][\"depths\"],\n+      MODEL_CONFIGS[\"x002\"][\"widths\"],\n+      MODEL_CONFIGS[\"x002\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x002\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x002\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX004\",\n+              \"keras.applications.RegNetX004\")\n+def RegNetX004(model_name=\"regnetx004\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x004\"][\"depths\"],\n+      MODEL_CONFIGS[\"x004\"][\"widths\"],\n+      MODEL_CONFIGS[\"x004\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x004\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x004\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX006\",\n+              \"keras.applications.RegNetX006\")\n+def RegNetX006(model_name=\"regnetx006\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x006\"][\"depths\"],\n+      MODEL_CONFIGS[\"x006\"][\"widths\"],\n+      MODEL_CONFIGS[\"x006\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x006\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x006\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX008\",\n+              \"keras.applications.RegNetX008\")\n+def RegNetX008(model_name=\"regnetx008\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x008\"][\"depths\"],\n+      MODEL_CONFIGS[\"x008\"][\"widths\"],\n+      MODEL_CONFIGS[\"x008\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x008\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x008\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX016\",\n+              \"keras.applications.RegNetX016\")\n+def RegNetX016(model_name=\"regnetx016\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x016\"][\"depths\"],\n+      MODEL_CONFIGS[\"x016\"][\"widths\"],\n+      MODEL_CONFIGS[\"x016\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x016\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x016\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX032\",\n+              \"keras.applications.RegNetX032\")\n+def RegNetX032(model_name=\"regnetx032\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x032\"][\"depths\"],\n+      MODEL_CONFIGS[\"x032\"][\"widths\"],\n+      MODEL_CONFIGS[\"x032\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x032\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x032\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX040\",\n+              \"keras.applications.RegNetX040\")\n+def RegNetX040(model_name=\"regnetx040\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x040\"][\"depths\"],\n+      MODEL_CONFIGS[\"x040\"][\"widths\"],\n+      MODEL_CONFIGS[\"x040\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x040\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x040\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX064\",\n+              \"keras.applications.RegNetX064\")\n+def RegNetX064(model_name=\"regnetx064\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x064\"][\"depths\"],\n+      MODEL_CONFIGS[\"x064\"][\"widths\"],\n+      MODEL_CONFIGS[\"x064\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x064\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x064\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX080\",\n+              \"keras.applications.RegNetX080\")\n+def RegNetX080(model_name=\"regnetx080\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x080\"][\"depths\"],\n+      MODEL_CONFIGS[\"x080\"][\"widths\"],\n+      MODEL_CONFIGS[\"x080\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x080\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x080\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX120\",\n+              \"keras.applications.RegNetX120\")\n+def RegNetX120(model_name=\"regnetx120\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x120\"][\"depths\"],\n+      MODEL_CONFIGS[\"x120\"][\"widths\"],\n+      MODEL_CONFIGS[\"x120\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x120\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x120\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX160\",\n+              \"keras.applications.RegNetX160\")\n+def RegNetX160(model_name=\"regnetx160\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x160\"][\"depths\"],\n+      MODEL_CONFIGS[\"x160\"][\"widths\"],\n+      MODEL_CONFIGS[\"x160\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x160\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x160\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetX320\",\n+              \"keras.applications.RegNetX320\")\n+def RegNetX320(model_name=\"regnetx320\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"x320\"][\"depths\"],\n+      MODEL_CONFIGS[\"x320\"][\"widths\"],\n+      MODEL_CONFIGS[\"x320\"][\"group_width\"],\n+      MODEL_CONFIGS[\"x320\"][\"block_type\"],\n+      MODEL_CONFIGS[\"x320\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY002\",\n+              \"keras.applications.RegNetY002\")\n+def RegNetY002(model_name=\"regnety002\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y002\"][\"depths\"],\n+      MODEL_CONFIGS[\"y002\"][\"widths\"],\n+      MODEL_CONFIGS[\"y002\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y002\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y002\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY004\",\n+              \"keras.applications.RegNetY004\")\n+def RegNetY004(model_name=\"regnety004\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y004\"][\"depths\"],\n+      MODEL_CONFIGS[\"y004\"][\"widths\"],\n+      MODEL_CONFIGS[\"y004\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y004\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y004\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY006\",\n+              \"keras.applications.RegNetY006\")\n+def RegNetY006(model_name=\"regnety006\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y006\"][\"depths\"],\n+      MODEL_CONFIGS[\"y006\"][\"widths\"],\n+      MODEL_CONFIGS[\"y006\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y006\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y006\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY008\",\n+              \"keras.applications.RegNetY008\")\n+def RegNetY008(model_name=\"regnety008\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y008\"][\"depths\"],\n+      MODEL_CONFIGS[\"y008\"][\"widths\"],\n+      MODEL_CONFIGS[\"y008\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y008\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y008\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY016\",\n+              \"keras.applications.RegNetY016\")\n+def RegNetY016(model_name=\"regnety016\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y016\"][\"depths\"],\n+      MODEL_CONFIGS[\"y016\"][\"widths\"],\n+      MODEL_CONFIGS[\"y016\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y016\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y016\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY032\",\n+              \"keras.applications.RegNetY032\")\n+def RegNetY032(model_name=\"regnety032\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y032\"][\"depths\"],\n+      MODEL_CONFIGS[\"y032\"][\"widths\"],\n+      MODEL_CONFIGS[\"y032\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y032\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y032\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY040\",\n+              \"keras.applications.RegNetY040\")\n+def RegNetY040(model_name=\"regnety040\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y040\"][\"depths\"],\n+      MODEL_CONFIGS[\"y040\"][\"widths\"],\n+      MODEL_CONFIGS[\"y040\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y040\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y040\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY064\",\n+              \"keras.applications.RegNetY064\")\n+def RegNetY064(model_name=\"regnety064\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y064\"][\"depths\"],\n+      MODEL_CONFIGS[\"y064\"][\"widths\"],\n+      MODEL_CONFIGS[\"y064\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y064\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y064\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY080\",\n+              \"keras.applications.RegNetY080\")\n+def RegNetY080(model_name=\"regnety080\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y080\"][\"depths\"],\n+      MODEL_CONFIGS[\"y080\"][\"widths\"],\n+      MODEL_CONFIGS[\"y080\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y080\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y080\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY120\",\n+              \"keras.applications.RegNetY120\")\n+def RegNetY120(model_name=\"regnety120\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y120\"][\"depths\"],\n+      MODEL_CONFIGS[\"y120\"][\"widths\"],\n+      MODEL_CONFIGS[\"y120\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y120\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y120\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY160\",\n+              \"keras.applications.RegNetY160\")\n+def RegNetY160(model_name=\"regnety160\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y160\"][\"depths\"],\n+      MODEL_CONFIGS[\"y160\"][\"widths\"],\n+      MODEL_CONFIGS[\"y160\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y160\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y160\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+@keras_export(\"keras.applications.regnet.RegNetY320\",\n+              \"keras.applications.RegNetY320\")\n+def RegNetY320(model_name=\"regnety320\",\n+               include_top=True,\n+               include_preprocessing=True,\n+               weights=\"imagenet\",\n+               input_tensor=None,\n+               input_shape=None,\n+               pooling=None,\n+               classes=1000,\n+               classifier_activation=\"softmax\"):\n+  return RegNet(\n+      MODEL_CONFIGS[\"y320\"][\"depths\"],\n+      MODEL_CONFIGS[\"y320\"][\"widths\"],\n+      MODEL_CONFIGS[\"y320\"][\"group_width\"],\n+      MODEL_CONFIGS[\"y320\"][\"block_type\"],\n+      MODEL_CONFIGS[\"y320\"][\"default_size\"],\n+      model_name=model_name,\n+      include_top=include_top,\n+      include_preprocessing=include_preprocessing,\n+      weights=weights,\n+      input_tensor=input_tensor,\n+      input_shape=input_shape,\n+      pooling=pooling,\n+      classes=classes,\n+      classifier_activation=classifier_activation)\n+\n+\n+RegNetX002.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX002\")\n+RegNetX004.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX004\")\n+RegNetX006.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX006\")\n+RegNetX008.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX008\")\n+RegNetX016.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX016\")\n+RegNetX032.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX032\")\n+RegNetX040.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX040\")\n+RegNetX064.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX064\")\n+RegNetX080.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX080\")\n+RegNetX120.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX120\")\n+RegNetX160.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX160\")\n+RegNetX320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetX320\")\n+\n+RegNetY002.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY002\")\n+RegNetY004.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY004\")\n+RegNetY006.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY006\")\n+RegNetY008.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY008\")\n+RegNetY016.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY016\")\n+RegNetY032.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY032\")\n+RegNetY040.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY040\")\n+RegNetY064.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY064\")\n+RegNetY080.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY080\")\n+RegNetY120.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY120\")\n+RegNetY160.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY160\")\n+RegNetY320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY320\")\n+\n+\n+@keras_export(\"keras.applications.regnet.preprocess_input\")\n+def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+  \"\"\"A placeholder method for backward compatibility.\n+\n+  The preprocessing logic has been included in the efficientnet model\n+  implementation. Users are no longer required to call this method to normalize\n+  the input data. This method does nothing and only kept as a placeholder to\n+  align the API surface between old and new version of model.\n+\n+  Args:\n+    x: A floating point `numpy.array` or a `tf.Tensor`.\n+    data_format: Optional data format of the image tensor/array. Defaults to\n+      None, in which case the global setting\n+      `tf.keras.backend.image_data_format()` is used (unless you changed it, it\n+      defaults to \"channels_last\").{mode}\n+\n+  Returns:\n+    Unchanged `numpy.array` or `tf.Tensor`.\n+  \"\"\"\n+  return x\n+\n+\n+@keras_export(\"keras.applications.regnet.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+  return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5dcdb669b1a767b05d9694f699395ae3b7d1c09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 20 | Methods Changed: 27 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 106 | Churn Cumulative: 1067 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -108,13 +108,12 @@ def _convert_dataset_to_list(dataset,data_size_warning_flag = True):\n       dataset_iterator = iter(zip(*dataset))\n     else:\n       dataset_iterator = iter(dataset)\n-\n-  elif isinstance(dataset,np.ndarray):\n-    dataset_iterator = iter(dataset)\n   elif isinstance(dataset,tf.data.Dataset):\n     if is_batched(dataset):\n       dataset = dataset.unbatch()\n     dataset_iterator = iter(dataset)\n+  elif isinstance(dataset,np.ndarray):\n+    dataset_iterator = iter(dataset)\n   else:\n     raise TypeError('`dataset` must be either a tf.data.Dataset object'\n                    f' or a list/tuple of arrays. Received : {type(dataset)}')\n\n@@ -9,8 +9,7 @@ from keras.utils import dataset_utils\n \n class SplitDatasetTest(tf.test.TestCase):\n   \n-  def test_with_basic_dataset_values(self):\n-    # numpy array\n+  def test_numpy_array(self):\n     dataset=np.ones(shape=(200, 32))\n     res = dataset_utils.split_dataset(dataset, left_size=0.8,right_size=0.2)\n     self.assertLen(res, 2)\n@@ -24,7 +23,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(dataset[-40:] ,list(right_split))\n     \n     \n-    # list of numpy array\n+  def test_list_of_numpy_arrays(self):\n     dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(200, 32))]\n     res = dataset_utils.split_dataset(dataset, left_size=4)\n     self.assertLen(res, 2)\n@@ -38,8 +37,13 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertAllEqual(list(left_split)+list(right_split), \n                         list(zip(*dataset)))\n     \n+    dataset=[np.ones(shape=(200, 32))]\n+    left_split,right_split = dataset_utils.split_dataset(dataset, \n+                                                             left_size=4)\n+    self.assertAllEqual(list(zip(*dataset)),\n+                        list(left_split)+list(right_split))\n     \n-    # fail with invalid shape\n+  def test_illegal_shaped_numpy_array(self):\n     with self.assertRaises(ValueError):\n       dataset=[np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n       dataset_utils.split_dataset(dataset, left_size=4)\n@@ -48,42 +52,87 @@ class SplitDatasetTest(tf.test.TestCase):\n       dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(201, 32)))\n       dataset_utils.split_dataset(dataset, left_size=4)\n     \n-    # list with single np array\n-    dataset=[np.ones(shape=(200, 32))]\n-    left_split,right_split = dataset_utils.split_dataset(dataset, \n-                                                             left_size=4)\n-    self.assertAllEqual(list(zip(*dataset)),\n-                        list(left_split)+list(right_split))\n-    \n-    # Tule of np arrays\n+  def test_tuple_of_numpy_arrays(self):\n     dataset=(np.ones(shape=(200, 32)), np.zeros(shape=(200, 32)))\n     left_split,right_split = dataset_utils.split_dataset(dataset, \n                                                              left_size=80)\n     self.assertAllEqual(list(zip(*dataset))[:80] ,list(left_split))\n     self.assertAllEqual(list(zip(*dataset))[80:] ,list(right_split))\n     \n-    \n-    # Batched tf.data.Dataset that yields batches of vectors\n-    dataset = tf.data.Dataset.from_tensor_slices(np.ones(shape=(16, 16,3)))\n+  def test_batched_tf_dataset_of_vectors(self):\n+    dataset = tf.data.Dataset.from_tensor_slices(np.ones(shape=(100,32, 32,1)))\n     dataset = dataset.batch(10)\n     left_split,right_split=dataset_utils.split_dataset(dataset,left_size=2)\n+\n+    self.assertAllEqual(np.array(list(left_split)).shape,(2,32,32,1))\n+    self.assertAllEqual(np.array(list(right_split)).shape,(98,32,32,1))\n     self.assertAllEqual(list(dataset.unbatch()),\n                         list(left_split)+list(right_split)) \n     \n-    # Batched tf.data.Dataset that yields batches of tuples of vectors\n-    dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(32,32), np.random.rand(32,32)))\n+  def test_batched_tf_dataset_of_tuple_of_vectors(self):\n+    dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(10,32,32), \n+                                                  np.random.rand(10,32,32)))\n     dataset = dataset.batch(2)\n-    left_split,right_split=dataset_utils.split_dataset(dataset,left_size=5)\n+    left_split,right_split=dataset_utils.split_dataset(dataset,left_size=4)\n+  \n+    self.assertAllEqual(np.array(list(left_split)).shape,(4,2,32,32))\n+    self.assertAllEqual(np.array(list(right_split)).shape,(6,2,32,32))\n+\n     self.assertAllEqual(list(dataset.unbatch()),\n                         list(left_split)+list(right_split))\n     \n     \n \n+  def test_unbatched_tf_dataset_of_vectors(self):\n+    dataset = tf.data.Dataset.from_tensor_slices(np.ones(shape=(100,16, 16,3)))\n+    \n+    left_split,right_split=dataset_utils.split_dataset(dataset,left_size=0.25)\n+    \n+    self.assertAllEqual(np.array(list(left_split)).shape,(25,16, 16,3))\n+    self.assertAllEqual(np.array(list(right_split)).shape,(75,16, 16,3))\n+    \n+    self.assertAllEqual(list(dataset),\n+                        list(left_split)+list(right_split)) \n+    \n+\n+\n+  def test_unbatched_tf_dataset_of_tuple_of_vectors(self):\n+    dataset = tf.data.Dataset.from_tensor_slices((np.random.rand(10,32,32,1), \n+                                                  np.random.rand(10,32,32,1)))\n+\n+    left_split,right_split=dataset_utils.split_dataset(dataset,left_size=5)\n+    \n+    self.assertAllEqual(np.array(list(left_split)).shape,(5,2,32,32,1))\n+    self.assertAllEqual(np.array(list(right_split)).shape,(5,2,32,32,1))\n+     \n+  \n+    self.assertAllEqual(list(dataset),list(left_split)+list(right_split)) \n+    \n+  def test_with_mnist_dataset(self):\n+    pass\n+    # (x_train,y_train),(_,_) = tf.keras.datasets.mnist.load_data()\n+\n+    # self.assertEqual(x_train.shape,(60000,28,28,1))\n+    # self.assertEqual(y_train.shape,(60000,))\n     \n     \n     \n     \n-  def test_with_list_dataset(self):\n+    \n+  # def test_unbatched_tf_dataset_of_dicts_of_vectors(self):\n+  #   dataset = tf.data.Dataset.from_tensor_slices(\n+  #     {'images': np.random.rand(32,32,3), \n+  #     'labels': np.random.rand(32,1)\n+  #     })\n+  #   dataset = dataset.batch(2)\n+  #   left_split,right_split=dataset_utils.split_dataset(dataset,left_size=5)\n+    # self.assertAllEqual(len(list(dataset.unbatch())),len(list(left_split))) \n+\n+    \n+    \n+    \n+    \n+  def test_list_dataset(self):\n     dataset = [np.ones(shape=(10,10,10)) for _ in range(10)]\n     left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                              left_size=5,\n@@ -100,7 +149,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(len(right_split), 4)\n \n     \n-  def test_with_tuple_dataset(self):\n+  def test_tuple_dataset(self):\n     dataset = (np.ones(shape=(10,10,10)),np.zeros(shape=(10,10,10)))\n     left_split,right_split = dataset_utils.split_dataset(dataset,\n                                                              left_size=0.75,\n@@ -116,7 +165,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertIsInstance(left_split, tf.data.Dataset)\n     self.assertIsInstance(right_split, tf.data.Dataset)\n     \n-  def test_with_invalid_dataset(self):\n+  def test_invalid_dataset(self):\n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(dataset=None, left_size=5)  \n     with self.assertRaises(TypeError):\n@@ -128,7 +177,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(dataset=float('INF'), left_size=5)\n \n-  def test_with_valid_left_and_right_sizes(self):\n+  def test_valid_left_and_right_sizes(self):\n     \n     dataset = np.array([1,2,3])\n     splitted_dataset = dataset_utils.split_dataset(dataset, \n@@ -193,7 +242,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(list(left_split), [1,2])\n     self.assertEqual(list(right_split), [6,7,8,9,10])\n     \n-  def test_with_float_left_and_right_sizes(self):\n+  def test_float_left_and_right_sizes(self):\n     dataset = tf.data.Dataset.from_tensor_slices(np.array([[0.1,0.2,0.3],\n                                                            [0.4,0.5,0.6],\n                                                            [0.7,0.8,0.9]]))\n@@ -203,7 +252,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     self.assertEqual(len(left_split), 2)\n     self.assertEqual(len(right_split), 1)\n     \n-  def test_with_invalid_float_left_and_right_sizes(self):\n+  def test_invalid_float_left_and_right_sizes(self):\n     with self.assertRaises(ValueError):\n       dataset = [np.ones(shape=(200, 32,32)), np.zeros(shape=(200, 32,32))]\n       dataset_utils.split_dataset(dataset, left_size=1.5,right_size=0.2)\n@@ -213,7 +262,7 @@ class SplitDatasetTest(tf.test.TestCase):\n \n     \n       \n-  def test_with_None_and_zero_left_and_right_size(self):    \n+  def test_None_and_zero_left_and_right_size(self):    \n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(dataset=np.array([1,2,3]), left_size=None)  \n     with self.assertRaises(ValueError):\n@@ -231,7 +280,7 @@ class SplitDatasetTest(tf.test.TestCase):\n     with self.assertRaises(ValueError):\n       dataset_utils.split_dataset(np.array([1,2,3]), left_size=0,right_size=0)\n       \n-  def test_with_invalid_left_and_right_size_types(self):      \n+  def test_invalid_left_and_right_size_types(self):      \n     with self.assertRaises(TypeError):\n       dataset_utils.split_dataset(np.array([1,2,3]), \n                                   left_size='1',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
