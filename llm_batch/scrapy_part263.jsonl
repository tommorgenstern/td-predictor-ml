{"custom_id": "scrapy#0e8770a2f4e96ee18e2fd0fc7bfb5f9bcd2f623d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 43 | Churn Cumulative: 709 | Contributors (this commit): 16 | Commits (past 90d): 2 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ This module contains some assorted functions used in tests\n \"\"\"\n \n from __future__ import absolute_import\n+from posixpath import split\n import os\n \n from importlib import import_module\n@@ -61,6 +62,23 @@ def get_gcs_content_and_delete(bucket, path):\n     bucket.delete_blob(path)\n     return content, acl, blob\n \n+def get_ftp_content_and_delete(path, host ,port,\n+        username, password, use_active_mode=False):\n+    from ftplib import FTP\n+    ftp = FTP()\n+    ftp.connect(host, port)\n+    ftp.login(username, password)\n+    if use_active_mode:\n+        ftp.set_pasv(False)\n+    ftp_data = []\n+    def buffer_data(data):\n+        ftp_data.append(data)\n+    ftp.retrbinary('RETR %s' % path, buffer_data)\n+    dirname, filename = split(path)\n+    ftp.cwd(dirname)\n+    ftp.delete(filename)\n+    return \"\".join(ftp_data)\n+    \n def get_crawler(spidercls=None, settings_dict=None):\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n     will be used to populate the crawler settings with a project level\n\n@@ -11,13 +11,14 @@ from six import BytesIO\n from twisted.trial import unittest\n from twisted.internet import defer\n \n-from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GCSFilesStore\n+from scrapy.pipelines.files import FilesPipeline, FSFilesStore, S3FilesStore, GCSFilesStore, FTPFilesStore\n from scrapy.item import Item, Field\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import assert_aws_environ, get_s3_content_and_delete\n from scrapy.utils.test import assert_gcs_environ, get_gcs_content_and_delete\n+from scrapy.utils.test import get_ftp_content_and_delete\n from scrapy.utils.boto import is_botocore\n \n from tests import mock\n@@ -365,6 +366,28 @@ class TestGCSFilesStore(unittest.TestCase):\n         self.assertEqual(blob.content_type, 'application/octet-stream')\n         self.assertIn(expected_policy, acl)\n \n+class TestFTPFileStore(unittest.TestCase):\n+    @defer.inlineCallbacks\n+    def test_persist(self):\n+        uri = os.environ.get('FTP_TEST_FILE_URI')\n+        if not uri:\n+            raise unittest.SkipTest(\"No FTP URI available for testing\")\n+        data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n+        buf = BytesIO(data)\n+        meta = {'foo': 'bar'}\n+        path = 'full/filename'\n+        store = FTPFilesStore(uri)\n+        empty_dict = yield store.stat_file(path, info=None)\n+        self.assertEqual(empty_dict, {})\n+        yield store.persist_file(path, buf, info=None, meta=meta, headers=None)\n+        stat = yield store.stat_file(path, info=None)\n+        self.assertIn('last_modified', stat)\n+        self.assertIn('checksum', stat)\n+        self.assertEqual(stat['checksum'], 'd113d66b2ec7258724a268bd88eef6b6')\n+        path = '%s/%s' % (store.basedir, path)\n+        content = get_ftp_content_and_delete(path, store.host, store.port,\n+            store.username, store.password, store.USE_ACTIVE_MODE)\n+        self.assertEqual(data.decode(), content)\n \n class ItemWithFiles(Item):\n     file_urls = Field()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9578f490991fbefc4ad643588c87862a57a9f032", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 576 | Contributors (this commit): 49 | Commits (past 90d): 8 | Contributors (cumulative): 49 | DMM Complexity: None\n\nDIFF:\n@@ -246,7 +246,7 @@ RETRY_HTTP_CODES = [500, 502, 503, 504, 522, 524, 408, 429]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n-ROBOTSTXT_PARSER = 'scrapy.robotstxt.PythonRobotParser'\n+ROBOTSTXT_PARSER = 'scrapy.robotstxt.ProtegoRobotParser'\n ROBOTSTXT_USER_AGENT = None\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c7f2bdfdbed0e82a385b05ea080768d38d6626cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 494 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -77,6 +77,7 @@ setup(\n         'six>=1.10.0',\n         'w3lib>=1.17.0',\n         'zope.interface>=4.1.3',\n+        'protego>=0.1.15',\n     ],\n     extras_require=extras_require,\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5612f387bcf34dc47044b81f802804a719e858f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 62 | Files Changed: 5 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 62 | Churn Cumulative: 200 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,22 +1,12 @@\n-import glob\n import six\n import pytest\n-from twisted import version as twisted_version\n-\n-\n-def _py_files(folder):\n-    return glob.glob(folder + \"/*.py\") + glob.glob(folder + \"/*/*.py\")\n \n \n collect_ignore = [\n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n-\n ]\n \n-if (twisted_version.major, twisted_version.minor, twisted_version.micro) >= (15, 5, 0):\n-    collect_ignore += _py_files(\"scrapy/xlib/tx\")\n-\n \n if six.PY3:\n     for line in open('tests/py3-ignores.txt'):\n\n@@ -1,2 +0,0 @@\n-\"\"\"This package contains some third party modules that are distributed along\n-with Scrapy\"\"\"\n\n@@ -1,19 +0,0 @@\n-from __future__ import absolute_import\n-\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-from pydispatch import (\n-    dispatcher,\n-    errors,\n-    robust,\n-    robustapply,\n-    saferef,\n-)\n-\n-warnings.warn(\"Importing from scrapy.xlib.pydispatch is deprecated and will\"\n-              \" no longer be supported in future Scrapy versions.\"\n-              \" If you just want to connect signals use the from_crawler class method,\"\n-              \" otherwise import pydispatch directly if needed.\"\n-              \" See: https://github.com/scrapy/scrapy/issues/1762\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -1,19 +0,0 @@\n-from __future__ import absolute_import\n-\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-from twisted.web import client\n-from twisted.internet import endpoints\n-\n-Agent = client.Agent  # since < 11.1\n-ProxyAgent = client.ProxyAgent  # since 11.1\n-ResponseDone = client.ResponseDone  # since 11.1\n-ResponseFailed = client.ResponseFailed  # since 11.1\n-HTTPConnectionPool = client.HTTPConnectionPool  # since 12.1\n-TCP4ClientEndpoint = endpoints.TCP4ClientEndpoint  # since 10.1\n-\n-warnings.warn(\"Importing from scrapy.xlib.tx is deprecated and will\"\n-              \" no longer be supported in future Scrapy versions.\"\n-              \" Update your code to import from twisted proper.\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -1,12 +0,0 @@\n-import unittest\n-import warnings\n-from six.moves import reload_module\n-\n-\n-class DeprecatedPydispatchTest(unittest.TestCase):\n-    def test_import_xlib_pydispatch_show_warning(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            from scrapy.xlib import pydispatch\n-            reload_module(pydispatch)\n-        self.assertIn('Importing from scrapy.xlib.pydispatch is deprecated',\n-                      str(w[0].message))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21ad8e20b9189eeca9bc3f2641eb6d27fe40ce4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 32 | Churn Cumulative: 727 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,9 +12,10 @@ import six\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, HtmlResponse\n-from scrapy.utils.spider import iterate_spider_output\n-from scrapy.utils.python import get_func_args\n+from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n+from scrapy.utils.python import get_func_args\n+from scrapy.utils.spider import iterate_spider_output\n \n \n def _identity(request, response):\n@@ -28,10 +29,13 @@ def _get_method(method, spider):\n         return getattr(spider, method, None)\n \n \n+_default_link_extractor = LinkExtractor()\n+\n+\n class Rule(object):\n \n-    def __init__(self, link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n-        self.link_extractor = link_extractor\n+    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n+        self.link_extractor = link_extractor or _default_link_extractor\n         self.callback = callback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links\n\n@@ -177,6 +177,26 @@ class CrawlSpiderTest(SpiderTest):\n     </body></html>\"\"\"\n     spider_class = CrawlSpider\n \n+    def test_rule_without_link_extractor(self):\n+\n+        response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n+\n+        class _CrawlSpider(self.spider_class):\n+            name = \"test\"\n+            allowed_domains = ['example.org']\n+            rules = (\n+                Rule(),\n+            )\n+\n+        spider = _CrawlSpider()\n+        output = list(spider._requests_to_follow(response))\n+        self.assertEqual(len(output), 3)\n+        self.assertTrue(all(map(lambda r: isinstance(r, Request), output)))\n+        self.assertEqual([r.url for r in output],\n+                         ['http://example.org/somepage/item/12.html',\n+                          'http://example.org/about.html',\n+                          'http://example.org/nofollow.html'])\n+\n     def test_process_links(self):\n \n         response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#13735bcf34b0caf60a52e95a907ef390324fdddc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 15 | Churn Cumulative: 943 | Contributors (this commit): 17 | Commits (past 90d): 12 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in topics/media-pipeline.rst\n \"\"\"\n import functools\n import hashlib\n+import mimetypes\n import os\n import os.path\n import time\n@@ -14,6 +15,7 @@ from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n \n+\n try:\n     from cStringIO import StringIO as BytesIO\n except ImportError:\n@@ -473,4 +475,11 @@ class FilesPipeline(MediaPipeline):\n     def file_path(self, request, response=None, info=None):\n         media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         media_ext = os.path.splitext(request.url)[1]\n+        # Handles empty and wild extensions by trying to guess the\n+        # mime type then extension or default to empty string otherwise\n+        if media_ext not in mimetypes.types_map:\n+            media_ext = ''\n+            media_type = mimetypes.guess_type(request.url)[0]\n+            if media_type:\n+                media_ext = mimetypes.guess_extension(media_type)\n         return 'full/%s%s' % (media_guid, media_ext)\n\n@@ -57,6 +57,12 @@ class FilesPipelineTestCase(unittest.TestCase):\n                                    response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n                                    info=object()),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n+        self.assertEqual(file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg.bohaha\")),\n+                         'full/76c00cef2ef669ae65052661f68d451162829507')\n+        self.assertEqual(file_path(Request(\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAR0AAACxCAMAAADOHZloAAACClBMVEX/\\\n+                                    //+F0tzCwMK76ZKQ21AMqr7oAAC96JvD5aWM2kvZ78J0N7fmAAC46Y4Ap7y\")),\n+                         'full/178059cbeba2e34120a67f2dc1afc3ecc09b61cb.png')\n+                         \n \n     def test_fs_store(self):\n         assert isinstance(self.pipeline.store, FSFilesStore)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b52fa6ca9ec8916e4ebcaa7ee148e0b20c2068f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 9 | Methods Changed: 11 | Complexity Δ (Sum/Max): 12/9 | Churn Δ: 75 | Churn Cumulative: 3567 | Contributors (this commit): 30 | Commits (past 90d): 4 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -235,8 +235,9 @@ class ExecutionEngine(object):\n             if isinstance(response, Response):\n                 response.request = request  # tie request to response received\n                 logkws = self.logformatter.crawled(request, response, spider)\n+                if logkws is not None:\n                     logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n-                self.signals.send_catch_log(signal=signals.response_received, \\\n+                self.signals.send_catch_log(signal=signals.response_received,\n                     response=response, request=request, spider=spider)\n             return response\n \n\n@@ -225,6 +225,7 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 logkws = self.logformatter.dropped(item, ex, response, spider)\n+                if logkws is not None:\n                     logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n                 return self.signals.send_catch_log_deferred(\n                     signal=signals.item_dropped, item=item, response=response,\n@@ -238,6 +239,7 @@ class Scraper(object):\n                     spider=spider, failure=output)\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n+            if logkws is not None:\n                 logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n             return self.signals.send_catch_log_deferred(\n                 signal=signals.item_scraped, item=output, response=response,\n\n@@ -29,6 +29,10 @@ class LogFormatter(object):\n     *   ``args`` should be a tuple or dict with the formatting placeholders for ``msg``.\n         The final log message is computed as ``msg % args``.\n \n+    Users can define their own ``LogFormatter`` class if they want to customise how\n+    each action is logged or if they want to omit it entirely. In order to omit\n+    logging an action the method must return ``None``.\n+\n     Here is an example on how to create a custom log formatter to lower the severity level of\n     the log message when an item is dropped from the pipeline::\n \n\n@@ -1,10 +1,18 @@\n import unittest\n+\n+from testfixtures import LogCapture\n+from twisted.internet import defer\n+from twisted.trial.unittest import TestCase as TwistedTestCase\n import six\n \n-from scrapy.spiders import Spider\n+from scrapy.crawler import CrawlerRunner\n+from scrapy.exceptions import DropItem\n from scrapy.http import Request, Response\n from scrapy.item import Item, Field\n from scrapy.logformatter import LogFormatter\n+from scrapy.spiders import Spider\n+from tests.mockserver import MockServer\n+from tests.spiders import ItemSpider\n \n \n class CustomItem(Item):\n@@ -89,5 +97,61 @@ class LogformatterSubclassTest(LoggingContribTest):\n         pass\n \n \n+class SkipMessagesLogFormatter(LogFormatter):\n+    def crawled(self, *args, **kwargs):\n+        return None\n+\n+    def scraped(self, *args, **kwargs):\n+        return None\n+\n+    def dropped(self, *args, **kwargs):\n+        return None\n+\n+\n+class DropSomeItemsPipeline(object):\n+    drop = True\n+\n+    def process_item(self, item, spider):\n+        if self.drop:\n+            self.drop = False\n+            raise DropItem(\"Ignoring item\")\n+        else:\n+            self.drop = True\n+\n+class ShowOrSkipMessagesTestCase(TwistedTestCase):\n+    def setUp(self):\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n+        self.base_settings = {\n+            'LOG_LEVEL': 'DEBUG',\n+            'ITEM_PIPELINES': {\n+                __name__ + '.DropSomeItemsPipeline': 300,\n+            },\n+        }\n+\n+    def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def test_show_messages(self):\n+        crawler = CrawlerRunner(self.base_settings).create_crawler(ItemSpider)\n+        with LogCapture() as lc:\n+            yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n+        self.assertIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n+        self.assertIn(\"Dropped: Ignoring item\", str(lc))\n+\n+    @defer.inlineCallbacks\n+    def test_skip_messages(self):\n+        settings = self.base_settings.copy()\n+        settings['LOG_FORMATTER'] = __name__ + '.SkipMessagesLogFormatter'\n+        crawler = CrawlerRunner(settings).create_crawler(ItemSpider)\n+        with LogCapture() as lc:\n+            yield crawler.crawl(mockserver=self.mockserver)\n+        self.assertNotIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n+        self.assertNotIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n+        self.assertNotIn(\"Dropped: Ignoring item\", str(lc))\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6872189b96595625e428331ddd4d1c620047642", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 1254 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -231,9 +231,9 @@ class Scraper(object):\n                     signal=signals.item_dropped, item=item, response=response,\n                     spider=spider, exception=output.value)\n             else:\n-                logger.error('Error processing %(item)s', {'item': item},\n-                             exc_info=failure_to_exc_info(output),\n-                             extra={'spider': spider})\n+                logkws = self.logformatter.error(item, ex, response, spider)\n+                logger.log(*logformatter_adapter(logkws), extra={'spider': spider},\n+                           exc_info=failure_to_exc_info(output))\n                 return self.signals.send_catch_log_deferred(\n                     signal=signals.item_error, item=item, response=response,\n                     spider=spider, failure=output)\n\n@@ -8,6 +8,7 @@ from scrapy.utils.request import referer_str\n SCRAPEDMSG = u\"Scraped from %(src)s\" + os.linesep + \"%(item)s\"\n DROPPEDMSG = u\"Dropped: %(exception)s\" + os.linesep + \"%(item)s\"\n CRAWLEDMSG = u\"Crawled (%(status)s) %(request)s%(request_flags)s (referer: %(referer)s)%(response_flags)s\"\n+ERRORMSG = u\"'Error processing %(item)s'\"\n \n \n class LogFormatter(object):\n@@ -92,6 +93,16 @@ class LogFormatter(object):\n             }\n         }\n \n+    def error(self, item, exception, response, spider):\n+        \"\"\"Logs a message when an item causes an error while it is passing through the item pipeline.\"\"\"\n+        return {\n+            'level': logging.ERROR,\n+            'msg': ERRORMSG,\n+            'args': {\n+                'item': item,\n+            }\n+        }\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27436cbbc9e7331d18d4b63f3c894e6621226efb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 30 | Churn Cumulative: 148 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,7 @@ class CustomItem(Item):\n         return \"name: %s\" % self['name']\n \n \n-class LoggingContribTest(unittest.TestCase):\n+class LoggingFormatterTest(unittest.TestCase):\n \n     def setUp(self):\n         self.formatter = LogFormatter()\n@@ -62,6 +62,16 @@ class LoggingContribTest(unittest.TestCase):\n         assert all(isinstance(x, six.text_type) for x in lines)\n         self.assertEqual(lines, [u\"Dropped: \\u2018\", '{}'])\n     \n+    def test_error(self):\n+        # In practice, the complete traceback is shown by passing the\n+        # 'exc_info' argument to the logging function\n+        item = {'key': 'value'}\n+        exception = Exception()\n+        response = Response(\"http://www.example.com\")\n+        logkws = self.formatter.error(item, exception, response, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline, u\"'Error processing {'key': 'value'}'\")\n+\n     def test_scraped(self):\n         item = CustomItem()\n         item['name'] = u'\\xa3'\n@@ -75,26 +85,30 @@ class LoggingContribTest(unittest.TestCase):\n \n class LogFormatterSubclass(LogFormatter):\n     def crawled(self, request, response, spider):\n-        kwargs = super(LogFormatterSubclass, self).crawled(\n-        request, response, spider)\n+        kwargs = super(LogFormatterSubclass, self).crawled(request, response, spider)\n         CRAWLEDMSG = (\n-            u\"Crawled (%(status)s) %(request)s (referer: \"\n-            u\"%(referer)s)%(flags)s\"\n+            u\"Crawled (%(status)s) %(request)s (referer: %(referer)s) %(flags)s\"\n         )\n+        log_args = kwargs['args']\n+        log_args['flags'] = str(request.flags)\n         return {\n             'level': kwargs['level'],\n             'msg': CRAWLEDMSG,\n-            'args': kwargs['args']\n+            'args': log_args,\n         }\n \n \n-class LogformatterSubclassTest(LoggingContribTest):\n+class LogformatterSubclassTest(unittest.TestCase):\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider('default')\n \n     def test_flags_in_request(self):\n-        pass\n+        req = Request(\"http://www.example.com\", flags=['test','flag'])\n+        res = Response(\"http://www.example.com\")\n+        logkws = self.formatter.crawled(req, res, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline, \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\")\n \n \n class SkipMessagesLogFormatter(LogFormatter):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#447b3d9d8133da0448cba871314df01887b645d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 134 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ def request_fingerprint(request, include_headers=None):\n     and are equivalent (ie. they should return the same response).\n \n     Another example are cookies used to store session ids. Suppose the\n-    following page is only accesible to authenticated users:\n+    following page is only accessible to authenticated users:\n \n     http://www.example.com/members/offers.html\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1236e9e81ecd83de716cabc61374fbbacd82d0aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 11 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 66 | Churn Cumulative: 645 | Contributors (this commit): 17 | Commits (past 90d): 10 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -262,6 +262,9 @@ coverage_ignore_pyobjects = [\n     # details that are not documented.\n     r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[A-Z]\\w*?\\.',  # methods\n     r'^scrapy\\.extensions\\.[a-z]\\w*?\\.[a-z]',  # helper functions\n+\n+    # Never documented before, and deprecated now.\n+    r'^scrapy\\.item\\.DictItem$',\n ]\n \n \n\n@@ -4,13 +4,15 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n-from abc import ABCMeta\n-from pprint import pformat\n-from copy import deepcopy\n import collections\n+from abc import ABCMeta\n+from copy import deepcopy\n+from pprint import pformat\n+from warnings import warn\n \n import six\n \n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n \n@@ -21,7 +23,19 @@ else:\n \n \n class BaseItem(object_ref):\n-    \"\"\"Base class for all scraped items.\"\"\"\n+    \"\"\"Base class for all scraped items.\n+\n+    In Scrapy, an object is considered an *item* if it is an instance of either\n+    :class:`BaseItem` or :class:`dict`. For example, when the output of a\n+    spider callback is evaluated, only instances of :class:`BaseItem` or\n+    :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`.\n+\n+    If you need instances of a custom class to be considered items by Scrapy,\n+    you must inherit from either :class:`BaseItem` or :class:`dict`.\n+\n+    Unlike instances of :class:`dict`, instances of :class:`BaseItem` may be\n+    :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n+    \"\"\"\n     pass\n \n \n@@ -30,6 +44,10 @@ class Field(dict):\n \n \n class ItemMeta(ABCMeta):\n+    \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n+\n+    .. _metaclass: https://realpython.com/python-metaclasses\n+    \"\"\"\n \n     def __new__(mcs, class_name, bases, attrs):\n         classcell = attrs.pop('__classcell__', None)\n@@ -56,6 +74,13 @@ class DictItem(MutableMapping, BaseItem):\n \n     fields = {}\n \n+    def __new__(cls, *args, **kwargs):\n+        if issubclass(cls, DictItem) and not issubclass(cls, Item):\n+            warn('scrapy.item.DictItem is deprecated, please use '\n+                 'scrapy.item.Item instead',\n+                 ScrapyDeprecationWarning, stacklevel=2)\n+        return super(DictItem, cls).__new__(cls, *args, **kwargs)\n+\n     def __init__(self, *args, **kwargs):\n         self._values = {}\n         if args or kwargs:  # avoid creating dict for most common case\n\n@@ -1,9 +1,11 @@\n import sys\n import unittest\n+from warnings import catch_warnings\n \n import six\n \n-from scrapy.item import ABCMeta, Item, ItemMeta, Field\n+from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.item import ABCMeta, DictItem, Field, Item, ItemMeta\n from tests import mock\n \n \n@@ -257,6 +259,17 @@ class ItemTest(unittest.TestCase):\n         item['tags'].append('tag2')\n         assert item['tags'] != copied_item['tags']\n \n+    def test_dictitem_deprecation_warning(self):\n+        \"\"\"Make sure the DictItem deprecation warning is not issued for\n+        Item\"\"\"\n+        with catch_warnings(record=True) as warnings:\n+            item = Item()\n+            self.assertEqual(len(warnings), 0)\n+            class SubclassedItem(Item):\n+                pass\n+            subclassed_item = SubclassedItem()\n+            self.assertEqual(len(warnings), 0)\n+\n \n class ItemMetaTest(unittest.TestCase):\n \n@@ -302,5 +315,20 @@ class ItemMetaClassCellRegression(unittest.TestCase):\n                 super(MyItem, self).__init__(*args, **kwargs)\n \n \n+class DictItemTest(unittest.TestCase):\n+\n+    def test_deprecation_warning(self):\n+        with catch_warnings(record=True) as warnings:\n+            dict_item = DictItem()\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+        with catch_warnings(record=True) as warnings:\n+            class SubclassedDictItem(DictItem):\n+                pass\n+            subclassed_dict_item = SubclassedDictItem()\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c14692e603873cf5b5783ad62ebe47474d2d7a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 26 | Churn Cumulative: 1140 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,10 +60,12 @@ class Command(ScrapyCommand):\n \n     @property\n     def max_level(self):\n-        levels = list(self.items.keys()) + list(self.requests.keys())\n-        if not levels:\n-            return 0\n-        return max(levels)\n+        max_items, max_requests = 0, 0\n+        if self.items:\n+            max_items = max(self.items)\n+        if self.requests:\n+            max_requests = max(self.requests)\n+        return max(max_items, max_requests)\n \n     def add_items(self, lvl, new_items):\n         old_items = self.items.get(lvl, [])\n@@ -84,9 +86,8 @@ class Command(ScrapyCommand):\n \n     def print_requests(self, lvl=None, colour=True):\n         if lvl is None:\n-            levels = list(self.requests.keys())\n-            if levels:\n-                requests = self.requests[max(levels)]\n+            if self.requests:\n+                requests = self.requests[max(self.requests)]\n             else:\n                 requests = []\n         else:\n\n@@ -60,14 +60,13 @@ class Command(ScrapyCommand):\n             else:\n                 self.settings.set('FEED_URI', opts.output, priority='cmdline')\n             feed_exporters = without_none_values(self.settings.getwithbase('FEED_EXPORTERS'))\n-            valid_output_formats = feed_exporters.keys()\n             if not opts.output_format:\n                 opts.output_format = os.path.splitext(opts.output)[1].replace(\".\", \"\")\n-            if opts.output_format not in valid_output_formats:\n+            if opts.output_format not in feed_exporters:\n                 raise UsageError(\"Unrecognized output format '%s', set one\"\n                                  \" using the '-t' switch or as a file extension\"\n                                  \" from the supported list %s\" % (opts.output_format,\n-                                                                  tuple(valid_output_formats)))\n+                                                                  tuple(feed_exporters)))\n             self.settings.set('FEED_FORMAT', opts.output_format, priority='cmdline')\n \n     def run(self, args, opts):\n\n@@ -108,6 +108,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                            '-a', 'test_arg=1',\n                                            '-c', 'parse',\n+                                           '--verbose',\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n@@ -117,12 +118,14 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                            '--meta', raw_json_string,\n                                            '-c', 'parse_request_with_meta',\n+                                           '--verbose',\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                            '-m', raw_json_string,\n                                            '-c', 'parse_request_with_meta',\n+                                           '--verbose',\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n@@ -132,6 +135,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                            '--cbkwargs', raw_json_string,\n                                            '-c', 'parse_request_with_cb_kwargs',\n+                                           '--verbose',\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n@@ -139,6 +143,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n     def test_request_without_meta(self):\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                           '-c', 'parse_request_without_meta',\n+                                          '--nolinks',\n                                            self.url('/html')])\n         self.assertIn(\"DEBUG: It Works!\", _textmode(stderr))\n \n@@ -148,6 +153,7 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         _, _, stderr = yield self.execute(['--spider', self.spider_name,\n                                            '--pipelines',\n                                            '-c', 'parse',\n+                                           '--verbose',\n                                            self.url('/html')])\n         self.assertIn(\"INFO: It Works!\", _textmode(stderr))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f4f98fd38d3fdf6b45a9d0289df0cbb48bcd22b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 53 | Churn Cumulative: 854 | Contributors (this commit): 20 | Commits (past 90d): 10 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -265,6 +265,10 @@ coverage_ignore_pyobjects = [\n \n     # Never documented before, and deprecated now.\n     r'^scrapy\\.item\\.DictItem$',\n+    r'^scrapy\\.linkextractors\\.FilteringLinkExtractor$',\n+\n+    # Implementation detail of LxmlLinkExtractor\n+    r'^scrapy\\.linkextractors\\.lxmlhtml\\.LxmlParserLinkExtractor',\n ]\n \n \n\n@@ -6,11 +6,13 @@ This package contains a collection of Link Extractors.\n For more info see docs/topics/link-extractors.rst\n \"\"\"\n import re\n+from warnings import warn\n \n from six.moves.urllib.parse import urlparse\n from parsel.csstranslator import HTMLTranslator\n from w3lib.url import canonicalize_url\n \n+from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import (\n     url_is_from_any_domain, url_has_any_extension,\n@@ -49,6 +51,15 @@ class FilteringLinkExtractor(object):\n \n     _csstranslator = HTMLTranslator()\n \n+    def __new__(cls, *args, **kwargs):\n+        from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n+        if (issubclass(cls, FilteringLinkExtractor) and\n+                not issubclass(cls, LxmlLinkExtractor)):\n+            warn('scrapy.linkextractors.FilteringLinkExtractor is deprecated, '\n+                 'please use scrapy.linkextractors.LinkExtractor instead',\n+                 ScrapyDeprecationWarning, stacklevel=2)\n+        return super(FilteringLinkExtractor, cls).__new__(cls)\n+\n     def __init__(self, link_extractor, allow, deny, allow_domains, deny_domains,\n                  restrict_xpaths, canonicalize, deny_extensions, restrict_css, restrict_text):\n \n\n@@ -117,6 +117,14 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n                                                 restrict_text=restrict_text)\n \n     def extract_links(self, response):\n+        \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n+        specified :class:`response <scrapy.http.Response>`.\n+\n+        Only links that match the settings passed to the link extractor\n+        constructor are returned.\n+\n+        Duplicate links are omitted.\n+        \"\"\"\n         base_url = get_base_url(response)\n         if self.restrict_xpaths:\n             docs = [subdoc\n\n@@ -1,10 +1,13 @@\n import re\n import unittest\n+from warnings import catch_warnings\n \n import pytest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.link import Link\n+from scrapy.linkextractors import FilteringLinkExtractor\n from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n from tests import get_testdata\n \n@@ -506,3 +509,30 @@ class LxmlLinkExtractorTestCase(Base.LinkExtractorTestCase):\n     @pytest.mark.xfail\n     def test_restrict_xpaths_with_html_entities(self):\n         super(LxmlLinkExtractorTestCase, self).test_restrict_xpaths_with_html_entities()\n+\n+    def test_filteringlinkextractor_deprecation_warning(self):\n+        \"\"\"Make sure the FilteringLinkExtractor deprecation warning is not\n+        issued for LxmlLinkExtractor\"\"\"\n+        with catch_warnings(record=True) as warnings:\n+            extractor = LxmlLinkExtractor()\n+            self.assertEqual(len(warnings), 0)\n+            class SubclassedItem(LxmlLinkExtractor):\n+                pass\n+            subclassed_extractor = SubclassedItem()\n+            self.assertEqual(len(warnings), 0)\n+\n+\n+class FilteringLinkExtractorTest(unittest.TestCase):\n+\n+    def test_deprecation_warning(self):\n+        args = [None]*10\n+        with catch_warnings(record=True) as warnings:\n+            extractor = FilteringLinkExtractor(*args)\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+        with catch_warnings(record=True) as warnings:\n+            class SubclassedFilteringLinkExtractor(FilteringLinkExtractor):\n+                pass\n+            subclassed_extractor = SubclassedFilteringLinkExtractor(*args)\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7632e375c268d31c570904452902799db180c415", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -47,7 +47,7 @@ class ScrapyCommand(object):\n \n     def help(self):\n         \"\"\"An extensive help for the command. It will be shown when using the\n-        \"help\" command. It can contain newlines, since not post-formatting will\n+        \"help\" command. It can contain newlines, since no post-formatting will\n         be applied to its contents.\n         \"\"\"\n         return self.long_desc()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07a31b13db96b2aef3074157808f7ad300780c3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 23 | Churn Cumulative: 171 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,13 +23,13 @@ class CustomItem(Item):\n         return \"name: %s\" % self['name']\n \n \n-class LoggingFormatterTest(unittest.TestCase):\n+class LogFormatterTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.formatter = LogFormatter()\n         self.spider = Spider('default')\n \n-    def test_crawled(self):\n+    def test_crawled_with_referer(self):\n         req = Request(\"http://www.example.com\")\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n@@ -37,6 +37,7 @@ class LoggingFormatterTest(unittest.TestCase):\n         self.assertEqual(logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n \n+    def test_crawled_without_referer(self):\n         req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n         res = Response(\"http://www.example.com\", flags=['cached'])\n         logkws = self.formatter.crawled(req, res, self.spider)\n@@ -98,11 +99,27 @@ class LogFormatterSubclass(LogFormatter):\n         }\n \n \n-class LogformatterSubclassTest(unittest.TestCase):\n+class LogformatterSubclassTest(LogFormatterTestCase):\n     def setUp(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider('default')\n \n+    def test_crawled_with_referer(self):\n+        req = Request(\"http://www.example.com\")\n+        res = Response(\"http://www.example.com\")\n+        logkws = self.formatter.crawled(req, res, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline,\n+            \"Crawled (200) <GET http://www.example.com> (referer: None) []\")\n+\n+    def test_crawled_without_referer(self):\n+        req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'}, flags=['cached'])\n+        res = Response(\"http://www.example.com\")\n+        logkws = self.formatter.crawled(req, res, self.spider)\n+        logline = logkws['msg'] % logkws['args']\n+        self.assertEqual(logline,\n+            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n+\n     def test_flags_in_request(self):\n         req = Request(\"http://www.example.com\", flags=['test','flag'])\n         res = Response(\"http://www.example.com\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0fabab5cc740bfbef4c13d9d7611278bc7cc335", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 60 | Churn Cumulative: 78 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,16 @@\n \"\"\"\n Extension for collecting core stats like items scraped and start/finish times\n \"\"\"\n-import datetime\n+from datetime import datetime\n \n from scrapy import signals\n \n+\n class CoreStats(object):\n \n     def __init__(self, stats):\n         self.stats = stats\n+        self.start_time = None\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -21,11 +23,12 @@ class CoreStats(object):\n         return o\n \n     def spider_opened(self, spider):\n-        self.stats.set_value('start_time', datetime.datetime.utcnow(), spider=spider)\n+        self.start_time = datetime.utcnow()\n+        self.stats.set_value('start_time', self.start_time, spider=spider)\n \n     def spider_closed(self, spider, reason):\n-        finish_time = datetime.datetime.utcnow()\n-        elapsed_time = finish_time - self.stats.get_value('start_time')\n+        finish_time = datetime.utcnow()\n+        elapsed_time = finish_time - self.start_time\n         elapsed_time_seconds = elapsed_time.total_seconds()\n         self.stats.set_value('elapsed_time_seconds', elapsed_time_seconds, spider=spider)\n         self.stats.set_value('finish_time', finish_time, spider=spider)\n\n@@ -1,10 +1,59 @@\n+from datetime import datetime\n import unittest\n \n+try:\n+    from unittest import mock\n+except ImportError:\n+    import mock\n+\n+from scrapy.extensions.corestats import CoreStats\n from scrapy.spiders import Spider\n from scrapy.statscollectors import StatsCollector, DummyStatsCollector\n from scrapy.utils.test import get_crawler\n \n \n+class CoreStatsExtensionTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('foo')\n+\n+    @mock.patch('scrapy.extensions.corestats.datetime')\n+    def test_core_stats_default_stats_collector(self, mock_datetime):\n+        fixed_datetime = datetime(2019, 12, 1, 11, 38)\n+        mock_datetime.utcnow = mock.Mock(return_value=fixed_datetime)\n+        self.crawler.stats = StatsCollector(self.crawler)\n+        ext = CoreStats.from_crawler(self.crawler)\n+        ext.spider_opened(self.spider)\n+        ext.item_scraped({}, self.spider)\n+        ext.response_received(self.spider)\n+        ext.item_dropped({}, self.spider, ZeroDivisionError())\n+        ext.spider_closed(self.spider, 'finished')\n+        self.assertEqual(\n+            ext.stats._stats,\n+            {\n+                'start_time': fixed_datetime,\n+                'finish_time': fixed_datetime,\n+                'item_scraped_count': 1,\n+                'response_received_count': 1,\n+                'item_dropped_count': 1,\n+                'item_dropped_reasons_count/ZeroDivisionError': 1,\n+                'finish_reason': 'finished',\n+                'elapsed_time_seconds': 0.0,\n+            }\n+        )\n+\n+    def test_core_stats_dummy_stats_collector(self):\n+        self.crawler.stats = DummyStatsCollector(self.crawler)\n+        ext = CoreStats.from_crawler(self.crawler)\n+        ext.spider_opened(self.spider)\n+        ext.item_scraped({}, self.spider)\n+        ext.response_received(self.spider)\n+        ext.item_dropped({}, self.spider, ZeroDivisionError())\n+        ext.spider_closed(self.spider, 'finished')\n+        self.assertEqual(ext.stats._stats, {})\n+\n+\n class StatsCollectorTest(unittest.TestCase):\n \n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f52148143be614779ccfdb34cac995b16f8264ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 2 | Churn Cumulative: 83 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -35,7 +35,7 @@ IGNORED_EXTENSIONS = [\n     'odp',\n \n     # other\n-    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar',\n+    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar', 'dmg', 'iso', 'apk'\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
