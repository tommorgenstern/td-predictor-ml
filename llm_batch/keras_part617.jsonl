{"custom_id": "keras#23d7609fd7d3ca835f3d206765b40225f8987b3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 37 | Files Changed: 12 | Hunks: 18 | Methods Changed: 12 | Complexity Δ (Sum/Max): -10/1 | Churn Δ: 119 | Churn Cumulative: 3646 | Contributors (this commit): 4 | Commits (past 90d): 82 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping1D\")\n@@ -42,9 +43,9 @@ class Cropping1D(Layer):\n \n     def __init__(self, cropping=(1, 1), name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        if isinstance(cropping, int):\n-            cropping = (cropping, cropping)\n-        self.cropping = cropping\n+        self.cropping = argument_validation.standardize_tuple(\n+            cropping, 2, \"cropping\", allow_zero=True\n+        )\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -56,6 +56,14 @@ class Cropping1DTest(testing.TestCase):\n         cropped = layers.Cropping1D((1, 2))(input_layer)\n         self.assertEqual(cropped.shape, (1, None, 7))\n \n+    def test_cropping_1d_errors_if_cropping_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.Cropping1D(cropping=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping1D(cropping=(1, 2, 3))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping1D(cropping=\"1\")\n+\n     def test_cropping_1d_errors_if_cropping_more_than_available(self):\n         with self.assertRaises(ValueError):\n             input_layer = layers.Input(batch_shape=(3, 5, 7))\n\n@@ -2,6 +2,7 @@ from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping2D\")\n@@ -65,12 +66,12 @@ class Cropping2D(Layer):\n                     \"`cropping` should have two elements. \"\n                     f\"Received: cropping={cropping}.\"\n                 )\n-            height_cropping = cropping[0]\n-            if isinstance(height_cropping, int):\n-                height_cropping = (height_cropping, height_cropping)\n-            width_cropping = cropping[1]\n-            if isinstance(width_cropping, int):\n-                width_cropping = (width_cropping, width_cropping)\n+            height_cropping = argument_validation.standardize_tuple(\n+                cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n+            )\n+            width_cropping = argument_validation.standardize_tuple(\n+                cropping[1], 2, \"2nd entry of cropping\", allow_zero=True\n+            )\n             self.cropping = (height_cropping, width_cropping)\n         else:\n             raise ValueError(\n\n@@ -97,3 +97,9 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n             layers.Cropping2D(cropping=(1, 2, 3))\n         with self.assertRaises(ValueError):\n             layers.Cropping2D(cropping=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=((1, 2), (3, 4, 5)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=((1, 2), (3, -4)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=((1, 2), \"3\"))\n\n@@ -2,6 +2,7 @@ from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.Cropping3D\")\n@@ -75,15 +76,15 @@ class Cropping3D(Layer):\n                 raise ValueError(\n                     f\"`cropping` should have 3 elements. Received: {cropping}.\"\n                 )\n-            dim1_cropping = cropping[0]\n-            if isinstance(dim1_cropping, int):\n-                dim1_cropping = (dim1_cropping, dim1_cropping)\n-            dim2_cropping = cropping[1]\n-            if isinstance(dim2_cropping, int):\n-                dim2_cropping = (dim2_cropping, dim2_cropping)\n-            dim3_cropping = cropping[2]\n-            if isinstance(dim3_cropping, int):\n-                dim3_cropping = (dim3_cropping, dim3_cropping)\n+            dim1_cropping = argument_validation.standardize_tuple(\n+                cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n+            )\n+            dim2_cropping = argument_validation.standardize_tuple(\n+                cropping[1], 2, \"2nd entry of cropping\", allow_zero=True\n+            )\n+            dim3_cropping = argument_validation.standardize_tuple(\n+                cropping[2], 2, \"3rd entry of cropping\", allow_zero=True\n+            )\n             self.cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n         else:\n             raise ValueError(\n\n@@ -159,3 +159,9 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             layers.Cropping3D(cropping=(1, 2, 3, 4))\n         with self.assertRaises(ValueError):\n             layers.Cropping3D(cropping=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.Cropping3D(cropping=((1, 2), (3, 4), (5, 6, 7)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping3D(cropping=((1, 2), (3, 4), (5, -6)))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping3D(cropping=((1, 2), (3, 4), \"5\"))\n\n@@ -2,6 +2,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding1D\")\n@@ -48,9 +49,9 @@ class ZeroPadding1D(Layer):\n \n     def __init__(self, padding=1, name=None, dtype=None):\n         super().__init__(name=name, dtype=dtype)\n-        if isinstance(padding, int):\n-            padding = (padding, padding)\n-        self.padding = padding\n+        self.padding = argument_validation.standardize_tuple(\n+            padding, 2, \"padding\", allow_zero=True\n+        )\n         self.input_spec = InputSpec(ndim=3)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -33,3 +33,11 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n         padded = layers.ZeroPadding1D((1, 2))(input_layer)\n         self.assertEqual(padded.shape, (1, None, 3))\n+\n+    def test_zero_padding_1d_errors_if_padding_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding1D(padding=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding1D(padding=(1, 2, 3))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding1D(padding=\"1\")\n\n@@ -3,6 +3,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding2D\")\n@@ -78,12 +79,12 @@ class ZeroPadding2D(Layer):\n                     \"`padding` should have two elements. \"\n                     f\"Received: padding={padding}.\"\n                 )\n-            height_padding = padding[0]\n-            if isinstance(height_padding, int):\n-                height_padding = (height_padding, height_padding)\n-            width_padding = padding[1]\n-            if isinstance(width_padding, int):\n-                width_padding = (width_padding, width_padding)\n+            height_padding = argument_validation.standardize_tuple(\n+                padding[0], 2, \"1st entry of padding\", allow_zero=True\n+            )\n+            width_padding = argument_validation.standardize_tuple(\n+                padding[1], 2, \"2nd entry of padding\", allow_zero=True\n+            )\n             self.padding = (height_padding, width_padding)\n         else:\n             raise ValueError(\n\n@@ -74,3 +74,9 @@ class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n             layers.ZeroPadding2D(padding=(1, 2, 3))\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding2D(padding=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding2D(padding=((1, 2), (3, 4, 5)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding2D(padding=((1, 2), (3, -4)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding2D(padding=((1, 2), \"3\"))\n\n@@ -3,6 +3,7 @@ from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.input_spec import InputSpec\n from keras_core.layers.layer import Layer\n+from keras_core.utils import argument_validation\n \n \n @keras_core_export(\"keras_core.layers.ZeroPadding3D\")\n@@ -76,16 +77,15 @@ class ZeroPadding3D(Layer):\n                 raise ValueError(\n                     f\"`padding` should have 3 elements. Received: {padding}.\"\n                 )\n-            dim1_padding = padding[0]\n-            if isinstance(dim1_padding, int):\n-                dim1_padding = (dim1_padding, dim1_padding)\n-            dim2_padding = padding[1]\n-            if isinstance(dim2_padding, int):\n-                dim2_padding = (dim2_padding, dim2_padding)\n-            dim3_padding = padding[2]\n-            if isinstance(dim3_padding, int):\n-                dim3_padding = (dim3_padding, dim3_padding)\n-            self.padding = (dim1_padding, dim2_padding, dim3_padding)\n+            dim1_padding = argument_validation.standardize_tuple(\n+                padding[0], 2, \"1st entry of padding\", allow_zero=True\n+            )\n+            dim2_padding = argument_validation.standardize_tuple(\n+                padding[1], 2, \"2nd entry of padding\", allow_zero=True\n+            )\n+            dim3_padding = argument_validation.standardize_tuple(\n+                padding[2], 2, \"3rd entry of padding\", allow_zero=True\n+            )\n             self.padding = (dim1_padding, dim2_padding, dim3_padding)\n         else:\n             raise ValueError(\n\n@@ -82,3 +82,9 @@ class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n             layers.ZeroPadding3D(padding=(1, 2, 3, 4))\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding3D(padding=\"1\")\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=((1, 2), (3, 4), (5, 6, 7)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=((1, 2), (3, 4), (5, -6)))\n+        with self.assertRaises(ValueError):\n+            layers.ZeroPadding3D(padding=((1, 2), (3, 4), \"5\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d567c299bb536177ec2fcf9a5428f78e3a5266b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 861 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 48/48 | Churn Δ: 861 | Churn Cumulative: 1028 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.5217391304347826\n\nDIFF:\n@@ -13,6 +13,7 @@ from keras_core.applications import inception_v3\n from keras_core.applications import mobilenet\n from keras_core.applications import mobilenet_v2\n from keras_core.applications import mobilenet_v3\n+from keras_core.applications import nasnet\n from keras_core.applications import vgg16\n from keras_core.applications import vgg19\n from keras_core.applications import xception\n@@ -65,6 +66,9 @@ MODEL_LIST = [\n     (convnext.ConvNeXtBase, 1024, convnext),\n     (convnext.ConvNeXtLarge, 1536, convnext),\n     (convnext.ConvNeXtXLarge, 2048, convnext),\n+    # nasnet\n+    (nasnet.NASNetMobile, 1056, nasnet),\n+    (nasnet.NASNetLarge, 4032, nasnet),\n ]\n # Add names for `named_parameters`.\n MODEL_LIST = [(e[0].__name__, *e) for e in MODEL_LIST]\n\n@@ -0,0 +1,857 @@\n+import warnings\n+\n+from tensorflow.io import gfile\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core.api_export import keras_core_export\n+from keras_core.applications import imagenet_utils\n+from keras_core.models import Functional\n+from keras_core.operations import operation_utils\n+from keras_core.utils import file_utils\n+\n+BASE_WEIGHTS_PATH = (\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/nasnet/\"\n+)\n+NASNET_MOBILE_WEIGHT_PATH = BASE_WEIGHTS_PATH + \"NASNet-mobile.h5\"\n+NASNET_MOBILE_WEIGHT_PATH_NO_TOP = BASE_WEIGHTS_PATH + \"NASNet-mobile-no-top.h5\"\n+NASNET_LARGE_WEIGHT_PATH = BASE_WEIGHTS_PATH + \"NASNet-large.h5\"\n+NASNET_LARGE_WEIGHT_PATH_NO_TOP = BASE_WEIGHTS_PATH + \"NASNet-large-no-top.h5\"\n+\n+\n+def NASNet(\n+    input_shape=None,\n+    penultimate_filters=4032,\n+    num_blocks=6,\n+    stem_block_filters=96,\n+    skip_reduction=True,\n+    filter_multiplier=2,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    pooling=None,\n+    classes=1000,\n+    default_size=None,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates a NASNet model.\n+\n+    Reference:\n+    - [Learning Transferable Architectures for Scalable Image Recognition](\n+        https://arxiv.org/abs/1707.07012) (CVPR 2018)\n+\n+    For image classification use cases, see\n+    [this page for detailed examples](\n+      https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n+\n+    For transfer learning use cases, make sure to read the\n+    [guide to transfer learning & fine-tuning](\n+      https://keras.io/guides/transfer_learning/).\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For NasNet, call `keras_core.applications.nasnet.preprocess_input`\n+    on your inputs before passing them to the model.\n+    `nasnet.preprocess_input` will scale input pixels between -1 and 1.\n+\n+    Args:\n+        input_shape: Optional shape tuple, the input shape\n+            is by default `(331, 331, 3)` for NASNetLarge and\n+            `(224, 224, 3)` for NASNetMobile.\n+            It should have exactly 3 input channels,\n+            and width and height should be no smaller than 32.\n+            E.g. `(224, 224, 3)` would be one valid value.\n+        penultimate_filters: Number of filters in the penultimate layer.\n+            NASNet models use the notation `NASNet (N @ P)`, where:\n+                -   N is the number of blocks\n+                -   P is the number of penultimate filters\n+        num_blocks: Number of repeated blocks of the NASNet model.\n+            NASNet models use the notation `NASNet (N @ P)`, where:\n+                -   N is the number of blocks\n+                -   P is the number of penultimate filters\n+        stem_block_filters: Number of filters in the initial stem block\n+        skip_reduction: Whether to skip the reduction step at the tail\n+            end of the network.\n+        filter_multiplier: Controls the width of the network.\n+            - If `filter_multiplier` < 1.0, proportionally decreases the number\n+                of filters in each layer.\n+            - If `filter_multiplier` > 1.0, proportionally increases the number\n+                of filters in each layer.\n+            - If `filter_multiplier` = 1, default number of filters from the\n+                paper are used at each layer.\n+        include_top: Whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: `None` (random initialization) or\n+            `imagenet` (ImageNet weights)\n+        input_tensor: Optional Keras tensor (i.e. output of\n+            `layers.Input()`)\n+            to use as image input for the model.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model\n+                will be the 4D tensor output of the\n+                last convolutional block.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional block, and thus\n+                the output of the model will be a\n+                2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n+        classes: Optional number of classes to classify images\n+            into, only to be specified if `include_top` is `True`, and\n+            if no `weights` argument is specified.\n+        default_size: Specifies the default image size of the model\n+        classifier_activation: A `str` or callable.\n+            The activation function to use on the \"top\" layer.\n+            Ignored unless `include_top=True`.\n+            Set `classifier_activation=None` to return the logits\n+            of the \"top\" layer. When loading pretrained weights,\n+            `classifier_activation` can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A model instance.\n+    \"\"\"\n+    if not (weights in {\"imagenet\", None} or gfile.exists(weights)):\n+        raise ValueError(\n+            \"The `weights` argument should be either \"\n+            \"`None` (random initialization), `imagenet` \"\n+            \"(pre-training on ImageNet), \"\n+            \"or the path to the weights file to be loaded.\"\n+        )\n+\n+    if weights == \"imagenet\" and include_top and classes != 1000:\n+        raise ValueError(\n+            'If using `weights` as `\"imagenet\"` with `include_top` '\n+            \"as true, `classes` should be 1000\"\n+        )\n+\n+    if (\n+        isinstance(input_shape, tuple)\n+        and None in input_shape\n+        and weights == \"imagenet\"\n+    ):\n+        raise ValueError(\n+            \"When specifying the input shape of a NASNet\"\n+            \" and loading `ImageNet` weights, \"\n+            \"the input_shape argument must be static \"\n+            \"(no None entries). Got: `input_shape=\" + str(input_shape) + \"`.\"\n+        )\n+\n+    if default_size is None:\n+        default_size = 331\n+\n+    # Determine proper input shape and default size.\n+    input_shape = imagenet_utils.obtain_input_shape(\n+        input_shape,\n+        default_size=default_size,\n+        min_size=32,\n+        data_format=backend.image_data_format(),\n+        require_flatten=include_top,\n+        weights=weights,\n+    )\n+\n+    if backend.image_data_format() != \"channels_last\":\n+        warnings.warn(\n+            \"The NASNet family of models is only available \"\n+            'for the input data format \"channels_last\" '\n+            \"(width, height, channels). \"\n+            \"However your settings specify the default \"\n+            'data format \"channels_first\" (channels, width, height).'\n+            ' You should set `image_data_format=\"channels_last\"` '\n+            \"in your Keras config located at ~/.keras/keras.json. \"\n+            \"The model being returned right now will expect inputs \"\n+            'to follow the \"channels_last\" data format.',\n+            stacklevel=2,\n+        )\n+        backend.set_image_data_format(\"channels_last\")\n+        old_data_format = \"channels_first\"\n+    else:\n+        old_data_format = None\n+\n+    if input_tensor is None:\n+        img_input = layers.Input(shape=input_shape)\n+    else:\n+        if not backend.is_keras_tensor(input_tensor):\n+            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n+        else:\n+            img_input = input_tensor\n+\n+    if penultimate_filters % (24 * (filter_multiplier**2)) != 0:\n+        raise ValueError(\n+            \"For NASNet-A models, the `penultimate_filters` must be a multiple \"\n+            \"of 24 * (`filter_multiplier` ** 2). Current value: %d\"\n+            % penultimate_filters\n+        )\n+\n+    channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n+    filters = penultimate_filters // 24\n+\n+    x = layers.Conv2D(\n+        stem_block_filters,\n+        (3, 3),\n+        strides=(2, 2),\n+        padding=\"valid\",\n+        use_bias=False,\n+        name=\"stem_conv1\",\n+        kernel_initializer=\"he_normal\",\n+    )(img_input)\n+\n+    x = layers.BatchNormalization(\n+        axis=channel_dim, momentum=0.9997, epsilon=1e-3, name=\"stem_bn1\"\n+    )(x)\n+\n+    p = None\n+    x, p = _reduction_a_cell(\n+        x, p, filters // (filter_multiplier**2), block_id=\"stem_1\"\n+    )\n+    x, p = _reduction_a_cell(\n+        x, p, filters // filter_multiplier, block_id=\"stem_2\"\n+    )\n+\n+    for i in range(num_blocks):\n+        x, p = _normal_a_cell(x, p, filters, block_id=\"%d\" % (i))\n+\n+    x, p0 = _reduction_a_cell(\n+        x, p, filters * filter_multiplier, block_id=\"reduce_%d\" % (num_blocks)\n+    )\n+\n+    p = p0 if not skip_reduction else p\n+\n+    for i in range(num_blocks):\n+        x, p = _normal_a_cell(\n+            x,\n+            p,\n+            filters * filter_multiplier,\n+            block_id=\"%d\" % (num_blocks + i + 1),\n+        )\n+\n+    x, p0 = _reduction_a_cell(\n+        x,\n+        p,\n+        filters * filter_multiplier**2,\n+        block_id=\"reduce_%d\" % (2 * num_blocks),\n+    )\n+\n+    p = p0 if not skip_reduction else p\n+\n+    for i in range(num_blocks):\n+        x, p = _normal_a_cell(\n+            x,\n+            p,\n+            filters * filter_multiplier**2,\n+            block_id=\"%d\" % (2 * num_blocks + i + 1),\n+        )\n+\n+    x = layers.Activation(\"relu\")(x)\n+\n+    if include_top:\n+        x = layers.GlobalAveragePooling2D()(x)\n+        imagenet_utils.validate_activation(classifier_activation, weights)\n+        x = layers.Dense(\n+            classes, activation=classifier_activation, name=\"predictions\"\n+        )(x)\n+    else:\n+        if pooling == \"avg\":\n+            x = layers.GlobalAveragePooling2D()(x)\n+        elif pooling == \"max\":\n+            x = layers.GlobalMaxPooling2D()(x)\n+\n+    # Ensure that the model takes into account\n+    # any potential predecessors of `input_tensor`.\n+    if input_tensor is not None:\n+        inputs = operation_utils.get_source_inputs(input_tensor)\n+    else:\n+        inputs = img_input\n+\n+    model = Functional(inputs, x, name=\"NASNet\")\n+\n+    # Load weights.\n+    if weights == \"imagenet\":\n+        if default_size == 224:  # mobile version\n+            if include_top:\n+                weights_path = file_utils.get_file(\n+                    \"nasnet_mobile.h5\",\n+                    NASNET_MOBILE_WEIGHT_PATH,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"020fb642bf7360b370c678b08e0adf61\",\n+                )\n+            else:\n+                weights_path = file_utils.get_file(\n+                    \"nasnet_mobile_no_top.h5\",\n+                    NASNET_MOBILE_WEIGHT_PATH_NO_TOP,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"1ed92395b5b598bdda52abe5c0dbfd63\",\n+                )\n+            model.load_weights(weights_path)\n+        elif default_size == 331:  # large version\n+            if include_top:\n+                weights_path = file_utils.get_file(\n+                    \"nasnet_large.h5\",\n+                    NASNET_LARGE_WEIGHT_PATH,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"11577c9a518f0070763c2b964a382f17\",\n+                )\n+            else:\n+                weights_path = file_utils.get_file(\n+                    \"nasnet_large_no_top.h5\",\n+                    NASNET_LARGE_WEIGHT_PATH_NO_TOP,\n+                    cache_subdir=\"models\",\n+                    file_hash=\"d81d89dc07e6e56530c4e77faddd61b5\",\n+                )\n+            model.load_weights(weights_path)\n+        else:\n+            raise ValueError(\n+                \"ImageNet weights can only be loaded with NASNetLarge\"\n+                \" or NASNetMobile\"\n+            )\n+    elif weights is not None:\n+        model.load_weights(weights)\n+\n+    if old_data_format:\n+        backend.set_image_data_format(old_data_format)\n+\n+    return model\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.nasnet.NASNetMobile\",\n+        \"keras_core.applications.NASNetMobile\",\n+    ]\n+)\n+def NASNetMobile(\n+    input_shape=None,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates a Mobile NASNet model in ImageNet mode.\n+\n+    Reference:\n+    - [Learning Transferable Architectures for Scalable Image Recognition](\n+        https://arxiv.org/abs/1707.07012) (CVPR 2018)\n+\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For NASNet, call `keras_core.applications.nasnet.preprocess_input` on your\n+    inputs before passing them to the model.\n+\n+    Args:\n+        input_shape: Optional shape tuple, only to be specified\n+            if `include_top` is False (otherwise the input shape\n+            has to be `(224, 224, 3)` for NASNetMobile\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 32.\n+            E.g. `(224, 224, 3)` would be one valid value.\n+        include_top: Whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: `None` (random initialization) or\n+            `imagenet` (ImageNet weights). For loading `imagenet` weights,\n+            `input_shape` should be (224, 224, 3)\n+        input_tensor: Optional Keras tensor (i.e. output of\n+            `layers.Input()`)\n+            to use as image input for the model.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model\n+                will be the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a\n+                2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n+        classes: Optional number of classes to classify images\n+            into, only to be specified if `include_top` is `True`, and\n+            if no `weights` argument is specified.\n+        classifier_activation: A `str` or callable. The activation function to\n+            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.  When loading pretrained weights, `classifier_activation` can\n+            only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A Keras model instance.\n+    \"\"\"\n+    return NASNet(\n+        input_shape,\n+        penultimate_filters=1056,\n+        num_blocks=4,\n+        stem_block_filters=32,\n+        skip_reduction=False,\n+        filter_multiplier=2,\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        pooling=pooling,\n+        classes=classes,\n+        default_size=224,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.applications.nasnet.NASNetLarge\",\n+        \"keras_core.applications.NASNetLarge\",\n+    ]\n+)\n+def NASNetLarge(\n+    input_shape=None,\n+    include_top=True,\n+    weights=\"imagenet\",\n+    input_tensor=None,\n+    pooling=None,\n+    classes=1000,\n+    classifier_activation=\"softmax\",\n+):\n+    \"\"\"Instantiates a NASNet model in ImageNet mode.\n+\n+    Reference:\n+    - [Learning Transferable Architectures for Scalable Image Recognition](\n+        https://arxiv.org/abs/1707.07012) (CVPR 2018)\n+\n+    Optionally loads weights pre-trained on ImageNet.\n+    Note that the data format convention used by the model is\n+    the one specified in your Keras config at `~/.keras/keras.json`.\n+\n+    Note: each Keras Application expects a specific kind of input preprocessing.\n+    For NASNet, call `keras_core.applications.nasnet.preprocess_input` on your\n+    inputs before passing them to the model.\n+\n+    Args:\n+        input_shape: Optional shape tuple, only to be specified\n+            if `include_top` is False (otherwise the input shape\n+            has to be `(331, 331, 3)` for NASNetLarge.\n+            It should have exactly 3 inputs channels,\n+            and width and height should be no smaller than 32.\n+            E.g. `(224, 224, 3)` would be one valid value.\n+        include_top: Whether to include the fully-connected\n+            layer at the top of the network.\n+        weights: `None` (random initialization) or\n+            `imagenet` (ImageNet weights).  For loading `imagenet` weights,\n+            `input_shape` should be (331, 331, 3)\n+        input_tensor: Optional Keras tensor (i.e. output of\n+            `layers.Input()`)\n+            to use as image input for the model.\n+        pooling: Optional pooling mode for feature extraction\n+            when `include_top` is `False`.\n+            - `None` means that the output of the model\n+                will be the 4D tensor output of the\n+                last convolutional layer.\n+            - `avg` means that global average pooling\n+                will be applied to the output of the\n+                last convolutional layer, and thus\n+                the output of the model will be a\n+                2D tensor.\n+            - `max` means that global max pooling will\n+                be applied.\n+        classes: Optional number of classes to classify images\n+            into, only to be specified if `include_top` is `True`, and\n+            if no `weights` argument is specified.\n+        classifier_activation: A `str` or callable. The activation function to\n+            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.  When loading pretrained weights, `classifier_activation`\n+            can only be `None` or `\"softmax\"`.\n+\n+    Returns:\n+        A Keras model instance.\n+    \"\"\"\n+    return NASNet(\n+        input_shape,\n+        penultimate_filters=4032,\n+        num_blocks=6,\n+        stem_block_filters=96,\n+        skip_reduction=True,\n+        filter_multiplier=2,\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        pooling=pooling,\n+        classes=classes,\n+        default_size=331,\n+        classifier_activation=classifier_activation,\n+    )\n+\n+\n+def _separable_conv_block(\n+    ip, filters, kernel_size=(3, 3), strides=(1, 1), block_id=None\n+):\n+    \"\"\"Adds 2 blocks of [relu-separable conv-batchnorm].\n+\n+    Args:\n+        ip: Input tensor\n+        filters: Number of output filters per layer\n+        kernel_size: Kernel size of separable convolutions\n+        strides: Strided convolution for downsampling\n+        block_id: String block_id\n+\n+    Returns:\n+        A Keras tensor\n+    \"\"\"\n+    channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n+\n+    with backend.name_scope(f\"separable_conv_block_{block_id}\"):\n+        x = layers.Activation(\"relu\")(ip)\n+        if strides == (2, 2):\n+            x = layers.ZeroPadding2D(\n+                padding=imagenet_utils.correct_pad(x, kernel_size),\n+                name=f\"separable_conv_1_pad_{block_id}\",\n+            )(x)\n+            conv_pad = \"valid\"\n+        else:\n+            conv_pad = \"same\"\n+        x = layers.SeparableConv2D(\n+            filters,\n+            kernel_size,\n+            strides=strides,\n+            name=f\"separable_conv_1_{block_id}\",\n+            padding=conv_pad,\n+            use_bias=False,\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=channel_dim,\n+            momentum=0.9997,\n+            epsilon=1e-3,\n+            name=f\"separable_conv_1_bn_{block_id}\",\n+        )(x)\n+        x = layers.Activation(\"relu\")(x)\n+        x = layers.SeparableConv2D(\n+            filters,\n+            kernel_size,\n+            name=f\"separable_conv_2_{block_id}\",\n+            padding=\"same\",\n+            use_bias=False,\n+        )(x)\n+        x = layers.BatchNormalization(\n+            axis=channel_dim,\n+            momentum=0.9997,\n+            epsilon=1e-3,\n+            name=f\"separable_conv_2_bn_{block_id}\",\n+        )(x)\n+    return x\n+\n+\n+def _adjust_block(p, ip, filters, block_id=None):\n+    \"\"\"Adjusts the input `previous path` to match the shape of the `input`.\n+\n+    Used in situations where the output number of filters needs to be changed.\n+\n+    Args:\n+        p: Input tensor which needs to be modified\n+        ip: Input tensor whose shape needs to be matched\n+        filters: Number of output filters to be matched\n+        block_id: String block_id\n+\n+    Returns:\n+        Adjusted Keras tensor\n+    \"\"\"\n+    channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n+    img_dim = 2 if backend.image_data_format() == \"channels_first\" else -2\n+\n+    with backend.name_scope(\"adjust_block\"):\n+        if p is None:\n+            p = ip\n+\n+        elif p.shape[img_dim] != ip.shape[img_dim]:\n+            with backend.name_scope(f\"adjust_reduction_block_{block_id}\"):\n+                p = layers.Activation(\"relu\", name=f\"adjust_relu_1_{block_id}\")(\n+                    p\n+                )\n+                p1 = layers.AveragePooling2D(\n+                    (1, 1),\n+                    strides=(2, 2),\n+                    padding=\"valid\",\n+                    name=f\"adjust_avg_pool_1_{block_id}\",\n+                )(p)\n+                p1 = layers.Conv2D(\n+                    filters // 2,\n+                    (1, 1),\n+                    padding=\"same\",\n+                    use_bias=False,\n+                    name=f\"adjust_conv_1_{block_id}\",\n+                    kernel_initializer=\"he_normal\",\n+                )(p1)\n+\n+                p2 = layers.ZeroPadding2D(padding=((0, 1), (0, 1)))(p)\n+                p2 = layers.Cropping2D(cropping=((1, 0), (1, 0)))(p2)\n+                p2 = layers.AveragePooling2D(\n+                    (1, 1),\n+                    strides=(2, 2),\n+                    padding=\"valid\",\n+                    name=f\"adjust_avg_pool_2_{block_id}\",\n+                )(p2)\n+                p2 = layers.Conv2D(\n+                    filters // 2,\n+                    (1, 1),\n+                    padding=\"same\",\n+                    use_bias=False,\n+                    name=f\"adjust_conv_2_{block_id}\",\n+                    kernel_initializer=\"he_normal\",\n+                )(p2)\n+\n+                p = layers.concatenate([p1, p2], axis=channel_dim)\n+                p = layers.BatchNormalization(\n+                    axis=channel_dim,\n+                    momentum=0.9997,\n+                    epsilon=1e-3,\n+                    name=f\"adjust_bn_{block_id}\",\n+                )(p)\n+\n+        elif p.shape[channel_dim] != filters:\n+            with backend.name_scope(f\"adjust_projection_block_{block_id}\"):\n+                p = layers.Activation(\"relu\")(p)\n+                p = layers.Conv2D(\n+                    filters,\n+                    (1, 1),\n+                    strides=(1, 1),\n+                    padding=\"same\",\n+                    name=f\"adjust_conv_projection_{block_id}\",\n+                    use_bias=False,\n+                    kernel_initializer=\"he_normal\",\n+                )(p)\n+                p = layers.BatchNormalization(\n+                    axis=channel_dim,\n+                    momentum=0.9997,\n+                    epsilon=1e-3,\n+                    name=f\"adjust_bn_{block_id}\",\n+                )(p)\n+    return p\n+\n+\n+def _normal_a_cell(ip, p, filters, block_id=None):\n+    \"\"\"Adds a Normal cell for NASNet-A (Fig. 4 in the paper).\n+\n+    Args:\n+        ip: Input tensor `x`\n+        p: Input tensor `p`\n+        filters: Number of output filters\n+        block_id: String block_id\n+\n+    Returns:\n+        A Keras tensor\n+    \"\"\"\n+    channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n+\n+    with backend.name_scope(f\"normal_A_block_{block_id}\"):\n+        p = _adjust_block(p, ip, filters, block_id)\n+\n+        h = layers.Activation(\"relu\")(ip)\n+        h = layers.Conv2D(\n+            filters,\n+            (1, 1),\n+            strides=(1, 1),\n+            padding=\"same\",\n+            name=f\"normal_conv_1_{block_id}\",\n+            use_bias=False,\n+            kernel_initializer=\"he_normal\",\n+        )(h)\n+        h = layers.BatchNormalization(\n+            axis=channel_dim,\n+            momentum=0.9997,\n+            epsilon=1e-3,\n+            name=f\"normal_bn_1_{block_id}\",\n+        )(h)\n+\n+        with backend.name_scope(\"block_1\"):\n+            x1_1 = _separable_conv_block(\n+                h,\n+                filters,\n+                kernel_size=(5, 5),\n+                block_id=f\"normal_left1_{block_id}\",\n+            )\n+            x1_2 = _separable_conv_block(\n+                p, filters, block_id=f\"normal_right1_{block_id}\"\n+            )\n+            x1 = layers.add([x1_1, x1_2], name=f\"normal_add_1_{block_id}\")\n+\n+        with backend.name_scope(\"block_2\"):\n+            x2_1 = _separable_conv_block(\n+                p, filters, (5, 5), block_id=f\"normal_left2_{block_id}\"\n+            )\n+            x2_2 = _separable_conv_block(\n+                p, filters, (3, 3), block_id=f\"normal_right2_{block_id}\"\n+            )\n+            x2 = layers.add([x2_1, x2_2], name=f\"normal_add_2_{block_id}\")\n+\n+        with backend.name_scope(\"block_3\"):\n+            x3 = layers.AveragePooling2D(\n+                (3, 3),\n+                strides=(1, 1),\n+                padding=\"same\",\n+                name=f\"normal_left3_{block_id}\",\n+            )(h)\n+            x3 = layers.add([x3, p], name=f\"normal_add_3_{block_id}\")\n+\n+        with backend.name_scope(\"block_4\"):\n+            x4_1 = layers.AveragePooling2D(\n+                (3, 3),\n+                strides=(1, 1),\n+                padding=\"same\",\n+                name=f\"normal_left4_{block_id}\",\n+            )(p)\n+            x4_2 = layers.AveragePooling2D(\n+                (3, 3),\n+                strides=(1, 1),\n+                padding=\"same\",\n+                name=f\"normal_right4_{block_id}\",\n+            )(p)\n+            x4 = layers.add([x4_1, x4_2], name=f\"normal_add_4_{block_id}\")\n+\n+        with backend.name_scope(\"block_5\"):\n+            x5 = _separable_conv_block(\n+                h, filters, block_id=f\"normal_left5_{block_id}\"\n+            )\n+            x5 = layers.add([x5, h], name=f\"normal_add_5_{block_id}\")\n+\n+        x = layers.concatenate(\n+            [p, x1, x2, x3, x4, x5],\n+            axis=channel_dim,\n+            name=f\"normal_concat_{block_id}\",\n+        )\n+    return x, ip\n+\n+\n+def _reduction_a_cell(ip, p, filters, block_id=None):\n+    \"\"\"Adds a Reduction cell for NASNet-A (Fig. 4 in the paper).\n+\n+    Args:\n+      ip: Input tensor `x`\n+      p: Input tensor `p`\n+      filters: Number of output filters\n+      block_id: String block_id\n+\n+    Returns:\n+      A Keras tensor\n+    \"\"\"\n+    channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n+\n+    with backend.name_scope(f\"reduction_A_block_{block_id}\"):\n+        p = _adjust_block(p, ip, filters, block_id)\n+\n+        h = layers.Activation(\"relu\")(ip)\n+        h = layers.Conv2D(\n+            filters,\n+            (1, 1),\n+            strides=(1, 1),\n+            padding=\"same\",\n+            name=f\"reduction_conv_1_{block_id}\",\n+            use_bias=False,\n+            kernel_initializer=\"he_normal\",\n+        )(h)\n+        h = layers.BatchNormalization(\n+            axis=channel_dim,\n+            momentum=0.9997,\n+            epsilon=1e-3,\n+            name=f\"reduction_bn_1_{block_id}\",\n+        )(h)\n+        h3 = layers.ZeroPadding2D(\n+            padding=imagenet_utils.correct_pad(h, 3),\n+            name=f\"reduction_pad_1_{block_id}\",\n+        )(h)\n+\n+        with backend.name_scope(\"block_1\"):\n+            x1_1 = _separable_conv_block(\n+                h,\n+                filters,\n+                (5, 5),\n+                strides=(2, 2),\n+                block_id=f\"reduction_left1_{block_id}\",\n+            )\n+            x1_2 = _separable_conv_block(\n+                p,\n+                filters,\n+                (7, 7),\n+                strides=(2, 2),\n+                block_id=f\"reduction_right1_{block_id}\",\n+            )\n+            x1 = layers.add([x1_1, x1_2], name=f\"reduction_add_1_{block_id}\")\n+\n+        with backend.name_scope(\"block_2\"):\n+            x2_1 = layers.MaxPooling2D(\n+                (3, 3),\n+                strides=(2, 2),\n+                padding=\"valid\",\n+                name=f\"reduction_left2_{block_id}\",\n+            )(h3)\n+            x2_2 = _separable_conv_block(\n+                p,\n+                filters,\n+                (7, 7),\n+                strides=(2, 2),\n+                block_id=f\"reduction_right2_{block_id}\",\n+            )\n+            x2 = layers.add([x2_1, x2_2], name=f\"reduction_add_2_{block_id}\")\n+\n+        with backend.name_scope(\"block_3\"):\n+            x3_1 = layers.AveragePooling2D(\n+                (3, 3),\n+                strides=(2, 2),\n+                padding=\"valid\",\n+                name=f\"reduction_left3_{block_id}\",\n+            )(h3)\n+            x3_2 = _separable_conv_block(\n+                p,\n+                filters,\n+                (5, 5),\n+                strides=(2, 2),\n+                block_id=f\"reduction_right3_{block_id}\",\n+            )\n+            x3 = layers.add([x3_1, x3_2], name=f\"reduction_add3_{block_id}\")\n+\n+        with backend.name_scope(\"block_4\"):\n+            x4 = layers.AveragePooling2D(\n+                (3, 3),\n+                strides=(1, 1),\n+                padding=\"same\",\n+                name=f\"reduction_left4_{block_id}\",\n+            )(x1)\n+            x4 = layers.add([x2, x4])\n+\n+        with backend.name_scope(\"block_5\"):\n+            x5_1 = _separable_conv_block(\n+                x1, filters, (3, 3), block_id=f\"reduction_left4_{block_id}\"\n+            )\n+            x5_2 = layers.MaxPooling2D(\n+                (3, 3),\n+                strides=(2, 2),\n+                padding=\"valid\",\n+                name=f\"reduction_right5_{block_id}\",\n+            )(h3)\n+            x5 = layers.add([x5_1, x5_2], name=f\"reduction_add4_{block_id}\")\n+\n+        x = layers.concatenate(\n+            [x2, x3, x4, x5],\n+            axis=channel_dim,\n+            name=f\"reduction_concat_{block_id}\",\n+        )\n+        return x, ip\n+\n+\n+@keras_core_export(\"keras_core.applications.nasnet.preprocess_input\")\n+def preprocess_input(x, data_format=None):\n+    return imagenet_utils.preprocess_input(\n+        x, data_format=data_format, mode=\"tf\"\n+    )\n+\n+\n+@keras_core_export(\"keras_core.applications.nasnet.decode_predictions\")\n+def decode_predictions(preds, top=5):\n+    return imagenet_utils.decode_predictions(preds, top=top)\n+\n+\n+preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n+    mode=\"\",\n+    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TF,\n+    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC,\n+)\n+decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a5e81e2c61dc848c6fda072bf57c7d3d53b4472", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 118 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 25/25 | Churn Δ: 126 | Churn Cumulative: 451 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.19047619047619047\n\nDIFF:\n@@ -1,8 +1,11 @@\n+import numpy as np\n import torch\n import torch.nn.functional as tnn\n \n+from keras_core.backend import standardize_data_format\n from keras_core.backend.config import epsilon\n from keras_core.backend.torch.core import convert_to_tensor\n+from keras_core.utils.argument_validation import standardize_tuple\n \n \n def relu(x):\n@@ -98,28 +101,135 @@ def log_softmax(x, axis=None):\n     return tnn.log_softmax(logits, dim=axis)\n \n \n+def _compute_padding_length(kernel_length):\n+    \"\"\"Compute padding length along one dimension.\"\"\"\n+    total_padding_length = kernel_length - 1\n+    left_padding = int(np.floor(total_padding_length / 2))\n+    right_padding = int(np.ceil(total_padding_length / 2))\n+    return (left_padding, right_padding)\n+\n+\n+def _apply_same_padding(inputs, kernel_size):\n+    spatial_shape = inputs.shape[2:]\n+    num_spatial_dims = len(spatial_shape)\n+    padding = ()\n+    for i in range(num_spatial_dims):\n+        padding_size = _compute_padding_length(kernel_size[i])\n+        padding = padding_size + padding\n+\n+    return tnn.pad(inputs, padding, mode=\"replicate\")\n+\n+\n+def _transpose_spatial_inputs(inputs):\n+    num_spatial_dims = inputs.ndim - 2\n+    # Torch pooling does not support `channels_last` format, so\n+    # we need to transpose to `channels_first` format.\n+    if num_spatial_dims == 1:\n+        inputs = torch.permute(inputs, (0, 2, 1))\n+    elif num_spatial_dims == 2:\n+        inputs = torch.permute(inputs, (0, 3, 1, 2))\n+    elif num_spatial_dims == 3:\n+        inputs = torch.permute(inputs, (0, 4, 1, 2, 3))\n+    else:\n+        raise ValueError(\n+            \"Pooling inputs's shape must be 3, 4 or 5, corresponding to 1D, 2D \"\n+            f\"and 3D inputs. But received shape: {inputs.shape}.\"\n+        )\n+    return inputs\n+\n+\n+def _transpose_spatial_outputs(outputs):\n+    # Undo the tranpose in `_transpose_spatial_inputs`.\n+    num_spatial_dims = len(outputs.shape) - 2\n+    if num_spatial_dims == 1:\n+        outputs = torch.permute(outputs, (0, 2, 1))\n+    elif num_spatial_dims == 2:\n+        outputs = torch.permute(outputs, (0, 2, 3, 1))\n+    elif num_spatial_dims == 3:\n+        outputs = torch.permute(outputs, (0, 2, 3, 4, 1))\n+    return outputs\n+\n+\n def max_pool(\n     inputs,\n     pool_size,\n     strides=None,\n     padding=\"valid\",\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n-    raise NotImplementedError(\n-        \"`max_pool` not yet implemented for PyTorch Backend\"\n+    inputs = convert_to_tensor(inputs)\n+    num_spatial_dims = inputs.ndim - 2\n+    pool_size = standardize_tuple(pool_size, num_spatial_dims, \"pool_size\")\n+    if strides is None:\n+        strides = pool_size\n+    else:\n+        strides = standardize_tuple(strides, num_spatial_dims, \"strides\")\n+\n+    data_format = standardize_data_format(data_format)\n+    if data_format == \"channels_last\":\n+        inputs = _transpose_spatial_inputs(inputs)\n+\n+    if padding == \"same\":\n+        # Torch does not natively support `\"same\"` padding, we need to manually\n+        # apply the right amount of padding to `inputs`.\n+        inputs = _apply_same_padding(inputs, pool_size)\n+\n+    if num_spatial_dims == 1:\n+        outputs = tnn.max_pool1d(inputs, kernel_size=pool_size, stride=strides)\n+    elif num_spatial_dims == 2:\n+        outputs = tnn.max_pool2d(inputs, kernel_size=pool_size, stride=strides)\n+    elif num_spatial_dims == 3:\n+        outputs = tnn.max_pool3d(inputs, kernel_size=pool_size, stride=strides)\n+    else:\n+        raise ValueError(\n+            \"Inputs to pooling operation must have ndim=3, 4, or 5 \"\n+            \"corresponding to 1D, 2D and 3D inputs. Received: \"\n+            f\"inputs.shape={inputs.shape}.\"\n         )\n+    if data_format == \"channels_last\":\n+        outputs = _transpose_spatial_outputs(outputs)\n+    return outputs\n \n \n def average_pool(\n     inputs,\n     pool_size,\n-    strides,\n-    padding,\n-    data_format=\"channels_last\",\n+    strides=None,\n+    padding=\"valid\",\n+    data_format=None,\n ):\n-    raise NotImplementedError(\n-        \"`average_pool` not yet implemented for PyTorch Backend\"\n+    inputs = convert_to_tensor(inputs)\n+    num_spatial_dims = inputs.ndim - 2\n+    pool_size = standardize_tuple(pool_size, num_spatial_dims, \"pool_size\")\n+    if strides is None:\n+        strides = pool_size\n+    else:\n+        strides = standardize_tuple(strides, num_spatial_dims, \"strides\")\n+\n+    data_format = standardize_data_format(data_format)\n+    if data_format == \"channels_last\":\n+        inputs = _transpose_spatial_inputs(inputs)\n+\n+    if padding == \"same\":\n+        # Torch does not natively support `\"same\"` padding, we need to manually\n+        # apply the right amount of padding to `inputs`.\n+        inputs = _apply_same_padding(inputs, pool_size)\n+\n+    if num_spatial_dims == 1:\n+        outputs = tnn.avg_pool1d(inputs, kernel_size=pool_size, stride=strides)\n+    elif num_spatial_dims == 2:\n+        outputs = tnn.avg_pool2d(inputs, kernel_size=pool_size, stride=strides)\n+    elif num_spatial_dims == 3:\n+        outputs = tnn.avg_pool3d(inputs, kernel_size=pool_size, stride=strides)\n+    else:\n+        raise ValueError(\n+            \"Inputs to pooling operation must have ndim=3, 4, or 5 \"\n+            \"corresponding to 1D, 2D and 3D inputs. Received: \"\n+            f\"inputs.shape={inputs.shape}.\"\n         )\n+    if data_format == \"channels_last\":\n+        outputs = _transpose_spatial_outputs(outputs)\n+    return outputs\n \n \n def conv(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
