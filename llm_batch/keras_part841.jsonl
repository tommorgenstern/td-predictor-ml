{"custom_id": "keras#efec34132d70d392474665edb585755f57247599", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 534 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -945,7 +945,7 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             dtype=\"float32\",\n         )\n         out = kmath.logdet(x)\n-        self.assertAllClose(out, -1.1178946)\n+        self.assertAllClose(out, -1.1178946, atol=1e-3)\n \n \n class MathDtypeTest(testing.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb6ece3e2ead7baec874cb71e579fbf310b1b05c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 151 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -170,7 +170,7 @@ def DenseNet(\n                 be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is `True`, and\n-            if no `weights` argument is specified.\n+            if no `weights` argument is specified. Defaults to `1000`.\n         classifier_activation: A `str` or callable.\n             The activation function to use\n             on the \"top\" layer. Ignored unless `include_top=True`. Set\n\n@@ -574,7 +574,7 @@ Args:\n         - `max` means that global max pooling will be applied.\n     classes: optional number of classes to classify images into, only to be\n         specified if `include_top` is `True`, and if no `weights` argument is\n-        specified.\n+        specified. Defaults to `1000`.\n     classifier_activation: A `str` or callable. The activation function to\n         use on the \"top\" layer. Ignored unless `include_top=True`. Set\n         `classifier_activation=None` to return the logits of the \"top\" layer.\n\n@@ -81,7 +81,7 @@ def Xception(\n                 be applied.\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is `True`, and\n-            if no `weights` argument is specified.\n+            if no `weights` argument is specified. Defaults to `1000`.\n         classifier_activation: A `str` or callable. The activation function to\n             use on the \"top\" layer. Ignored unless `include_top=True`. Set\n             `classifier_activation=None` to return the logits of the \"top\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ad85bdba40e3927382ddcd8bed20925b6404ad6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 42 | Churn Cumulative: 890 | Contributors (this commit): 14 | Commits (past 90d): 19 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,11 +40,7 @@ class UnitNormalization(Layer):\n         self.built = True\n \n     def call(self, inputs):\n-        x = ops.cast(inputs, self.compute_dtype)\n-\n-        square_sum = ops.sum(ops.square(x), axis=self.axis, keepdims=True)\n-        x_inv_norm = ops.rsqrt(ops.maximum(square_sum, 1e-12))\n-        return ops.multiply(x, x_inv_norm)\n+        return ops.normalize(inputs, axis=self.axis, order=2, epsilon=1e-12)\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n\n@@ -1975,16 +1975,19 @@ def ctc_decode(\n \n \n class Normalize(Operation):\n-    def __init__(self, axis=-1, order=2):\n+    def __init__(self, axis=-1, order=2, epsilon=None):\n         super().__init__()\n         self.axis = axis\n         self.order = order\n+        self.epsilon = epsilon\n \n     def compute_output_spec(self, x):\n         return KerasTensor(shape=x.shape)\n \n     def call(self, x):\n-        return _normalize(x, axis=self.axis, order=self.order)\n+        return _normalize(\n+            x, axis=self.axis, order=self.order, epsilon=self.epsilon\n+        )\n \n \n @keras_export(\n@@ -1993,7 +1996,7 @@ class Normalize(Operation):\n         \"keras.ops.nn.normalize\",\n     ]\n )\n-def normalize(x, axis=-1, order=2):\n+def normalize(x, axis=-1, order=2, epsilon=None):\n     \"\"\"Normalizes `x` over the specified axis.\n \n     It is defined as: `normalize(x) = x / max(norm(x), epsilon)`.\n@@ -2004,6 +2007,8 @@ def normalize(x, axis=-1, order=2):\n             Default to -1.\n         order: The exponent value in the norm formulation.\n             Defaults to 2.\n+        epsilon: A lower bound value for the norm.\n+            Defaults to `backend.epsilon()`.\n \n     Returns:\n         The normalized array.\n@@ -2018,11 +2023,13 @@ def normalize(x, axis=-1, order=2):\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Normalize(axis=axis, order=order).symbolic_call(x)\n-    return _normalize(x, axis=axis, order=order)\n+        return Normalize(axis=axis, order=order, epsilon=epsilon).symbolic_call(\n+            x\n+        )\n+    return _normalize(x, axis=axis, order=order, epsilon=epsilon)\n \n \n-def _normalize(x, axis=-1, order=2):\n+def _normalize(x, axis=-1, order=2, epsilon=None):\n     if not isinstance(order, int) or not order >= 1:\n         raise ValueError(\n             f\"Argument `order` must be an int >= 1. Received: order={order}\"\n@@ -2030,7 +2037,17 @@ def _normalize(x, axis=-1, order=2):\n     x = backend.convert_to_tensor(x)\n     if len(x.shape) == 0:\n         x = backend.numpy.expand_dims(x, axis=0)\n+    if epsilon is None:\n         epsilon = backend.epsilon()\n+    if 2 == order:\n+        # A special case: L2 normalization with `x * rsqrt(...)`\n+        # instead of `x / sqrt(...)`\n+        square_sum = backend.numpy.sum(\n+            backend.numpy.square(x), axis=axis, keepdims=True\n+        )\n+        inv_norm = backend.math.rsqrt(square_sum)\n+        inv_norm = backend.numpy.minimum(inv_norm, 1.0 / epsilon)\n+        return x * inv_norm\n     norm = backend.linalg.norm(x, ord=order, axis=axis, keepdims=True)\n     denom = backend.numpy.maximum(norm, epsilon)\n     return backend.numpy.divide(x, denom)\n\n@@ -2110,6 +2110,13 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             ],\n         )\n \n+        # linalg.norm(x, ...) < epsilon\n+        x = np.array([[1e-6, 1e-8]], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.normalize(x, axis=-1, order=2, epsilon=1e-5),\n+            [[1e-1, 1e-3]],\n+        )\n+\n     def test_psnr(self):\n         x1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n         x2 = np.array([[0.2, 0.2, 0.3], [0.4, 0.6, 0.6]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f3a85945e58ae33f8adb4b8012c01b499925c87e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 107 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -282,7 +282,7 @@ class FeatureSpaceTest(testing.TestCase):\n         self.assertEqual(out[\"string_2_X_int_2\"].shape, (10, 32))\n \n         # Test batched call on tensors\n-        if backend.backend == \"tensorflow\":\n+        if backend.backend() == \"tensorflow\":\n             out = fs(self._get_train_data_dict(as_tensors=True))\n             self.assertIsInstance(out, dict)\n             self.assertLen(out, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d85e7d3f267eaca3908918695f6276bf98f1ea8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 15 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 720 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -627,7 +627,7 @@ def _walk_saveable(saveable):\n         )\n \n     obj_type = saveable._obj_type()\n-    attr_skiplist = get_attr_skiplist(obj_type)\n+    attr_skipset = get_attr_skipset(obj_type)\n \n     # Save all layers directly tracked by Sequential and Functional first.\n     # This helps avoid ordering concerns for subclassed Sequential or Functional\n@@ -636,7 +636,7 @@ def _walk_saveable(saveable):\n         yield \"layers\", saveable.layers\n \n     for child_attr in sorted(dir(saveable), key=lambda x: _name_key(x)):\n-        if child_attr.startswith(\"__\") or child_attr in attr_skiplist:\n+        if child_attr.startswith(\"__\") or child_attr in attr_skipset:\n             continue\n         try:\n             child_obj = getattr(saveable, child_attr)\n@@ -1061,48 +1061,50 @@ def get_temp_dir():\n     return temp_dir\n \n \n-def get_attr_skiplist(obj_type):\n-    skiplist = global_state.get_global_attribute(\n+def get_attr_skipset(obj_type):\n+    skipset = global_state.get_global_attribute(\n         f\"saving_attr_skiplist_{obj_type}\", None\n     )\n-    if skiplist is not None:\n-        return skiplist\n+    if skipset is not None:\n+        return skipset\n \n-    skiplist = [\n+    skipset = set(\n+        [\n             \"_self_unconditional_dependency_names\",\n         ]\n+    )\n     if obj_type == \"Layer\":\n         ref_obj = Layer()\n-        skiplist += dir(ref_obj)\n+        skipset.update(dir(ref_obj))\n     elif obj_type == \"Functional\":\n         ref_obj = Layer()\n-        skiplist += dir(ref_obj) + [\"operations\", \"_operations\"]\n+        skipset.update(dir(ref_obj) + [\"operations\", \"_operations\"])\n     elif obj_type == \"Sequential\":\n         ref_obj = Layer()\n-        skiplist += dir(ref_obj) + [\"_functional\"]\n+        skipset.update(dir(ref_obj) + [\"_functional\"])\n     elif obj_type == \"Metric\":\n         ref_obj_a = Metric()\n         ref_obj_b = CompileMetrics([], [])\n-        skiplist += dir(ref_obj_a) + dir(ref_obj_b)\n+        skipset.update(dir(ref_obj_a) + dir(ref_obj_b))\n     elif obj_type == \"Optimizer\":\n         ref_obj = Optimizer(1.0)\n-        skiplist += dir(ref_obj)\n-        skiplist.remove(\"variables\")\n+        skipset.update(dir(ref_obj))\n+        skipset.remove(\"variables\")\n     elif obj_type == \"Loss\":\n         ref_obj = Loss()\n-        skiplist += dir(ref_obj)\n+        skipset.update(dir(ref_obj))\n     else:\n         raise ValueError(\n-            f\"get_attr_skiplist got invalid {obj_type=}. \"\n+            f\"get_attr_skipset got invalid {obj_type=}. \"\n             \"Accepted values for `obj_type` are \"\n             \"['Layer', 'Functional', 'Sequential', 'Metric', \"\n             \"'Optimizer', 'Loss']\"\n         )\n \n     global_state.set_global_attribute(\n-        f\"saving_attr_skiplist_{obj_type}\", skiplist\n+        f\"saving_attr_skipset_{obj_type}\", skipset\n     )\n-    return skiplist\n+    return skipset\n \n \n def is_memory_sufficient(model):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7496e18905e516219e6f0eab8fa1f26ccefbdf41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 15 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 179 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -149,7 +149,7 @@ def categorical_accuracy(y_true, y_pred):\n \n     # If the predicted output and actual output types don't match, force cast\n     # them to match.\n-    if y_pred.dtype != y_true.dtype:\n+    if y_pred.dtype is not y_true.dtype:\n         y_pred = ops.cast(y_pred, dtype=y_true.dtype)\n     matches = ops.cast(ops.equal(y_true, y_pred), backend.floatx())\n     if reshape_matches:\n@@ -235,7 +235,7 @@ def sparse_categorical_accuracy(y_true, y_pred):\n \n     # If the predicted output and actual output types don't match, force cast\n     # them to match.\n-    if y_pred.dtype != y_true.dtype:\n+    if y_pred.dtype is not y_true.dtype:\n         y_pred = ops.cast(y_pred, y_true.dtype)\n     matches = ops.cast(ops.equal(y_true, y_pred), backend.floatx())\n     if reshape_matches:\n\n@@ -154,7 +154,7 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n     Returns:\n         iterator: An `iterator` object.\n     \"\"\"\n-    if dataset_type_spec == list:\n+    if dataset_type_spec is list:\n         if len(dataset) == 0:\n             raise ValueError(\n                 \"Received an empty list dataset. \"\n@@ -180,7 +180,7 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 )\n \n         return iter(zip(*dataset))\n-    elif dataset_type_spec == tuple:\n+    elif dataset_type_spec is tuple:\n         if len(dataset) == 0:\n             raise ValueError(\n                 \"Received an empty list dataset.\"\n@@ -206,14 +206,14 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                 )\n \n         return iter(zip(*dataset))\n-    elif dataset_type_spec == tf.data.Dataset:\n+    elif dataset_type_spec is tf.data.Dataset:\n         if is_batched(dataset):\n             dataset = dataset.unbatch()\n         return iter(dataset)\n \n     elif is_torch_dataset(dataset):\n         return iter(dataset)\n-    elif dataset_type_spec == np.ndarray:\n+    elif dataset_type_spec is np.ndarray:\n         return iter(dataset)\n     raise ValueError(f\"Invalid dataset_type_spec: {dataset_type_spec}\")\n \n@@ -350,9 +350,9 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n \n     # check left_size is non-negative and less than 1 and less than total_length\n     if (\n-        left_size_type == int\n+        left_size_type is int\n         and (left_size <= 0 or left_size >= total_length)\n-        or left_size_type == float\n+        or left_size_type is float\n         and (left_size <= 0 or left_size >= 1)\n     ):\n         raise ValueError(\n@@ -365,9 +365,9 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n     # check right_size is non-negative and less than 1 and less than\n     # total_length\n     if (\n-        right_size_type == int\n+        right_size_type is int\n         and (right_size <= 0 or right_size >= total_length)\n-        or right_size_type == float\n+        or right_size_type is float\n         and (right_size <= 0 or right_size >= 1)\n     ):\n         raise ValueError(\n@@ -380,7 +380,7 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n     # check sum of left_size and right_size is less than or equal to\n     # total_length\n     if (\n-        right_size_type == left_size_type == float\n+        right_size_type is left_size_type is float\n         and right_size + left_size > 1\n     ):\n         raise ValueError(\n@@ -388,14 +388,14 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n             \"than 1. It must be less than or equal to 1.\"\n         )\n \n-    if left_size_type == float:\n+    if left_size_type is float:\n         left_size = round(left_size * total_length)\n-    elif left_size_type == int:\n+    elif left_size_type is int:\n         left_size = float(left_size)\n \n-    if right_size_type == float:\n+    if right_size_type is float:\n         right_size = round(right_size * total_length)\n-    elif right_size_type == int:\n+    elif right_size_type is int:\n         right_size = float(right_size)\n \n     if left_size is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3691f0f6ecd92df8ba7cebdaa7990f396d049be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/3 | Churn Δ: 133 | Churn Cumulative: 3557 | Contributors (this commit): 11 | Commits (past 90d): 20 | Contributors (cumulative): 26 | DMM Complexity: 0.8313253012048193\n\nDIFF:\n@@ -297,7 +297,7 @@ def resize(\n             resized = tf.transpose(resized, (0, 3, 1, 2))\n         elif len(images.shape) == 3:\n             resized = tf.transpose(resized, (2, 0, 1))\n-    return tf.cast(resized, images.dtype)\n+    return resized\n \n \n AFFINE_TRANSFORM_INTERPOLATIONS = (\n\n@@ -1,6 +1,7 @@\n from keras.src import backend\n from keras.src.api_export import keras_export\n from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.ops.core import _saturate_cast\n \n \n @keras_export(\"keras.layers.Resizing\")\n@@ -83,7 +84,7 @@ class Resizing(TFDataLayer):\n \n     def call(self, inputs):\n         size = (self.height, self.width)\n-        return self.backend.image.resize(\n+        resized = self.backend.image.resize(\n             inputs,\n             size=size,\n             interpolation=self.interpolation,\n@@ -93,6 +94,11 @@ class Resizing(TFDataLayer):\n             fill_mode=self.fill_mode,\n             fill_value=self.fill_value,\n         )\n+        if resized.dtype == inputs.dtype:\n+            return resized\n+        if backend.is_int_dtype(inputs.dtype):\n+            resized = self.backend.numpy.round(resized)\n+        return _saturate_cast(resized, inputs.dtype, self.backend)\n \n     def compute_output_shape(self, input_shape):\n         input_shape = list(input_shape)\n\n@@ -868,7 +868,8 @@ def saturate_cast(x, dtype):\n     return _saturate_cast(x, dtype)\n \n \n-def _saturate_cast(x, dtype):\n+def _saturate_cast(x, dtype, backend_module=None):\n+    backend_module = backend_module or backend\n     dtype = backend.standardize_dtype(dtype)\n     in_dtype = backend.standardize_dtype(x.dtype)\n     in_info = np.iinfo(in_dtype) if \"int\" in in_dtype else np.finfo(in_dtype)\n@@ -889,9 +890,9 @@ def _saturate_cast(x, dtype):\n         max_limit = np.nextafter(max_limit, 0, dtype=in_dtype)\n \n     # Unconditionally apply `clip` to fix `inf` behavior.\n-    x = backend.numpy.clip(x, min_limit, max_limit)\n+    x = backend_module.numpy.clip(x, min_limit, max_limit)\n \n-    return cast(x, dtype)\n+    return backend_module.cast(x, dtype)\n \n \n @keras_export(\"keras.ops.convert_to_tensor\")\n\n@@ -240,7 +240,7 @@ class Resize(Operation):\n         self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, images):\n-        return backend.image.resize(\n+        return _resize(\n             images,\n             self.size,\n             interpolation=self.interpolation,\n@@ -362,7 +362,7 @@ def resize(\n             fill_mode=fill_mode,\n             fill_value=fill_value,\n         ).symbolic_call(images)\n-    return backend.image.resize(\n+    return _resize(\n         images,\n         size,\n         interpolation=interpolation,\n@@ -375,6 +375,37 @@ def resize(\n     )\n \n \n+def _resize(\n+    images,\n+    size,\n+    interpolation=\"bilinear\",\n+    antialias=False,\n+    crop_to_aspect_ratio=False,\n+    pad_to_aspect_ratio=False,\n+    fill_mode=\"constant\",\n+    fill_value=0.0,\n+    data_format=None,\n+):\n+    resized = backend.image.resize(\n+        images,\n+        size,\n+        interpolation=interpolation,\n+        antialias=antialias,\n+        crop_to_aspect_ratio=crop_to_aspect_ratio,\n+        data_format=data_format,\n+        pad_to_aspect_ratio=pad_to_aspect_ratio,\n+        fill_mode=fill_mode,\n+        fill_value=fill_value,\n+    )\n+    if resized.dtype == images.dtype:\n+        # Only `torch` backend will cast result to original dtype with\n+        # correct rounding and without dtype overflow\n+        return resized\n+    if backend.is_int_dtype(images.dtype):\n+        resized = ops.round(resized)\n+    return ops.saturate_cast(resized, images.dtype)\n+\n+\n class AffineTransform(Operation):\n     def __init__(\n         self,\n\n@@ -607,6 +607,87 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )(x)\n         self.assertAllClose(ref_out, out, atol=1e-4)\n \n+    def test_resize_uint8_round(self):\n+        x = np.array([0, 1, 254, 255], dtype=\"uint8\").reshape(1, 2, 2, 1)\n+        expected = np.array(\n+            # OpenCV as gold standard.\n+            # [\n+            #     [0, 0, 1, 1],\n+            #     [64, 64, 64, 65],\n+            #     [191, 191, 191, 192],\n+            #     [254, 254, 255, 255],\n+            # ]\n+            #\n+            # Resize without `round` - differences in 8 points\n+            # [\n+            #     [0, 0, 0, 1],\n+            #     [63, 63, 64, 64],\n+            #     [190, 190, 191, 191],\n+            #     [254, 254, 254, 255],\n+            # ]\n+            #\n+            # Resize with `round` - differences in 2 points\n+            [\n+                [0, 0, 1, 1],\n+                [64, 64, 64, 64],\n+                [190, 191, 191, 192],\n+                [254, 254, 255, 255],\n+            ],\n+            dtype=\"uint8\",\n+        ).reshape(1, 4, 4, 1)\n+        out = kimage.resize(\n+            x,\n+            size=(4, 4),\n+            interpolation=\"bilinear\",\n+            antialias=False,\n+        )\n+        self.assertEqual(tuple(out.shape), tuple(expected.shape))\n+        self.assertEqual(backend.standardize_dtype(out.dtype), \"uint8\")\n+        self.assertAllClose(out, expected, atol=1e-4)\n+\n+    def test_resize_uint8_round_saturate(self):\n+        x = np.array([0, 1, 254, 255], dtype=\"uint8\").reshape(1, 2, 2, 1)\n+        expected = np.array(\n+            # OpenCV as gold standard. Same for `torch` backend.\n+            (\n+                [\n+                    [0, 0, 0, 0],\n+                    [57, 58, 58, 59],\n+                    [196, 197, 197, 198],\n+                    [255, 255, 255, 255],\n+                ]\n+                if \"torch\" == backend.backend()\n+                else\n+                # Resize without `round` and `saturate_cast` - differences in\n+                # 16 points\n+                # [\n+                #     [234, 234, 235, 235],\n+                #     [-5, -6, -5, -6],\n+                #     [5, 4, 5, 4],\n+                #     [-235, -235, -234, -234],\n+                # ]\n+                #\n+                # Resize with `round` and `saturate_cast` - differences in\n+                # 8 points\n+                [\n+                    [0, 0, 0, 0],\n+                    [53, 53, 53, 54],\n+                    [201, 202, 202, 202],\n+                    [255, 255, 255, 255],\n+                ]\n+            ),\n+            dtype=\"uint8\",\n+        ).reshape(1, 4, 4, 1)\n+        out = kimage.resize(\n+            x,\n+            size=(4, 4),\n+            interpolation=\"bicubic\",\n+            antialias=False,\n+        )\n+        self.assertEqual(tuple(out.shape), tuple(expected.shape))\n+        self.assertEqual(backend.standardize_dtype(out.dtype), \"uint8\")\n+        self.assertAllClose(out, expected, atol=1e-4)\n+\n     def test_resize_with_crop(self):\n         # Test channels_last\n         x = np.random.random((60, 50, 3)).astype(\"float32\") * 255\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b4a78cda38c800d6df4b1cdd8f93aaf4d2d24dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 12 | Churn Cumulative: 36 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,6 +43,18 @@ class UnitNormalization(Layer):\n         return ops.normalize(inputs, axis=self.axis, order=2, epsilon=1e-12)\n \n     def compute_output_shape(self, input_shape):\n+        # Ensure axis is always treated as a list\n+        if isinstance(self.axis, int):\n+            axes = [self.axis]\n+        else:\n+            axes = self.axis\n+\n+        for axis in axes:\n+            if axis >= len(input_shape) or axis < -len(input_shape):\n+                raise ValueError(\n+                    f\"Axis {self.axis} is out of bounds for \"\n+                    f\"input shape {input_shape}.\"\n+                )\n         return input_shape\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#723f8a0932ee8e5d6c7b33ab34b120b65cb6eee1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 20 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 96 | Churn Cumulative: 118 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,13 +7,33 @@ from keras.src.initializers.initializer import Initializer\n from keras.src.saving import serialization_lib\n \n \n+class RandomInitializer(Initializer):\n+    def __init__(self, seed=None):\n+        self._init_seed = seed\n+        if seed is None:\n+            seed = random.make_default_seed()\n+        elif isinstance(seed, dict):\n+            seed = serialization_lib.deserialize_keras_object(seed)\n+        elif not isinstance(seed, (int, random.SeedGenerator)):\n+            raise ValueError(\n+                \"`seed` argument should be an instance of \"\n+                \"`keras.random.SeedGenerator()` or an integer. \"\n+                f\"Received: seed={seed}\"\n+            )\n+        self.seed = seed\n+\n+    def get_config(self):\n+        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n+        return {\"seed\": seed_config}\n+\n+\n @keras_export(\n     [\n         \"keras.initializers.RandomNormal\",\n         \"keras.initializers.random_normal\",\n     ]\n )\n-class RandomNormal(Initializer):\n+class RandomNormal(RandomInitializer):\n     \"\"\"Random normal initializer.\n \n     Draws samples from a normal distribution for given parameters.\n@@ -46,9 +66,7 @@ class RandomNormal(Initializer):\n     def __init__(self, mean=0.0, stddev=0.05, seed=None):\n         self.mean = mean\n         self.stddev = stddev\n-        self._init_seed = seed\n-        self.seed = seed if seed is not None else random.make_default_seed()\n-        super().__init__()\n+        super().__init__(seed=seed)\n \n     def __call__(self, shape, dtype=None):\n         return random.normal(\n@@ -60,8 +78,9 @@ class RandomNormal(Initializer):\n         )\n \n     def get_config(self):\n-        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n-        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": seed_config}\n+        base_config = super().get_config()\n+        config = {\"mean\": self.mean, \"stddev\": self.stddev}\n+        return {**base_config, **config}\n \n \n @keras_export(\n@@ -70,7 +89,7 @@ class RandomNormal(Initializer):\n         \"keras.initializers.truncated_normal\",\n     ]\n )\n-class TruncatedNormal(Initializer):\n+class TruncatedNormal(RandomInitializer):\n     \"\"\"Initializer that generates a truncated normal distribution.\n \n     The values generated are similar to values from a\n@@ -106,9 +125,7 @@ class TruncatedNormal(Initializer):\n     def __init__(self, mean=0.0, stddev=0.05, seed=None):\n         self.mean = mean\n         self.stddev = stddev\n-        self._init_seed = seed\n-        self.seed = seed if seed is not None else random.make_default_seed()\n-        super().__init__()\n+        super().__init__(seed=seed)\n \n     def __call__(self, shape, dtype=None):\n         return random.truncated_normal(\n@@ -120,8 +137,9 @@ class TruncatedNormal(Initializer):\n         )\n \n     def get_config(self):\n-        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n-        return {\"mean\": self.mean, \"stddev\": self.stddev, \"seed\": seed_config}\n+        base_config = super().get_config()\n+        config = {\"mean\": self.mean, \"stddev\": self.stddev}\n+        return {**base_config, **config}\n \n \n @keras_export(\n@@ -130,7 +148,7 @@ class TruncatedNormal(Initializer):\n         \"keras.initializers.random_uniform\",\n     ]\n )\n-class RandomUniform(Initializer):\n+class RandomUniform(RandomInitializer):\n     \"\"\"Random uniform initializer.\n \n     Draws samples from a uniform distribution for given parameters.\n@@ -163,9 +181,7 @@ class RandomUniform(Initializer):\n     def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n         self.minval = minval\n         self.maxval = maxval\n-        self._init_seed = seed\n-        self.seed = seed if seed is not None else random.make_default_seed()\n-        super().__init__()\n+        super().__init__(seed=seed)\n \n     def __call__(self, shape, dtype=None):\n         return random.uniform(\n@@ -177,12 +193,9 @@ class RandomUniform(Initializer):\n         )\n \n     def get_config(self):\n-        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n-        return {\n-            \"minval\": self.minval,\n-            \"maxval\": self.maxval,\n-            \"seed\": seed_config,\n-        }\n+        base_config = super().get_config()\n+        config = {\"minval\": self.minval, \"maxval\": self.maxval}\n+        return {**base_config, **config}\n \n \n @keras_export(\n@@ -191,7 +204,7 @@ class RandomUniform(Initializer):\n         \"keras.initializers.variance_scaling\",\n     ]\n )\n-class VarianceScaling(Initializer):\n+class VarianceScaling(RandomInitializer):\n     \"\"\"Initializer that adapts its scale to the shape of its input tensors.\n \n     With `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\n@@ -267,8 +280,7 @@ class VarianceScaling(Initializer):\n         self.scale = scale\n         self.mode = mode\n         self.distribution = distribution\n-        self._init_seed = seed\n-        self.seed = seed if seed is not None else random.make_default_seed()\n+        super().__init__(seed=seed)\n \n     def __call__(self, shape, dtype=None):\n         scale = self.scale\n@@ -296,13 +308,13 @@ class VarianceScaling(Initializer):\n             )\n \n     def get_config(self):\n-        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n-        return {\n+        base_config = super().get_config()\n+        config = {\n             \"scale\": self.scale,\n             \"mode\": self.mode,\n             \"distribution\": self.distribution,\n-            \"seed\": seed_config,\n         }\n+        return {**base_config, **config}\n \n \n @keras_export(\n@@ -632,7 +644,7 @@ def compute_fans(shape):\n         \"keras.initializers.orthogonal\",\n     ]\n )\n-class OrthogonalInitializer(Initializer):\n+class OrthogonalInitializer(RandomInitializer):\n     \"\"\"Initializer that generates an orthogonal matrix.\n \n     If the shape of the tensor to initialize is two-dimensional, it is\n@@ -668,8 +680,7 @@ class OrthogonalInitializer(Initializer):\n \n     def __init__(self, gain=1.0, seed=None):\n         self.gain = gain\n-        self._init_seed = seed\n-        self.seed = seed if seed is not None else random.make_default_seed()\n+        super().__init__(seed=seed)\n \n     def __call__(self, shape, dtype=None):\n         if len(shape) < 2:\n@@ -699,5 +710,6 @@ class OrthogonalInitializer(Initializer):\n         return self.gain * ops.reshape(q, shape)\n \n     def get_config(self):\n-        seed_config = serialization_lib.serialize_keras_object(self._init_seed)\n-        return {\"gain\": self.gain, \"seed\": seed_config}\n+        base_config = super().get_config()\n+        config = {\"gain\": self.gain}\n+        return {**base_config, **config}\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras.src import backend\n from keras.src import initializers\n+from keras.src import random\n from keras.src import testing\n from keras.src import utils\n \n@@ -191,3 +192,20 @@ class InitializersTest(testing.TestCase):\n                 mode=\"fan_in\",\n                 distribution=\"invalid_dist\",\n             )\n+\n+    def test_serialization_with_seed_generator(self):\n+        seed = random.SeedGenerator()\n+        initializer = initializers.OrthogonalInitializer(seed=seed)\n+        self.run_class_serialization_test(initializer)\n+\n+        seed = random.SeedGenerator()\n+        initializer = initializers.VarianceScaling(seed=seed)\n+        self.run_class_serialization_test(initializer)\n+\n+        seed = random.SeedGenerator()\n+        initializer = initializers.RandomUniform(seed=seed)\n+        self.run_class_serialization_test(initializer)\n+\n+        seed = random.SeedGenerator()\n+        initializer = initializers.RandomNormal(seed=seed)\n+        self.run_class_serialization_test(initializer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b91dade5045ba19dd3e26c4133842736df07992b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 76 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -131,12 +131,12 @@ class CategoryEncoding(TFDataLayer):\n             return (self.num_tokens,)\n         if self.output_mode == \"one_hot\":\n             if input_shape[-1] != 1:\n-                return tuple(input_shape + (self.num_tokens,))\n+                return tuple(input_shape) + (self.num_tokens,)\n             elif len(input_shape) == 1:\n-                return tuple(input_shape + (self.num_tokens,))\n+                return tuple(input_shape) + (self.num_tokens,)\n             else:\n-                return tuple(input_shape[:-1] + (self.num_tokens,))\n-        return tuple(input_shape[:-1] + (self.num_tokens,))\n+                return tuple(input_shape[:-1]) + (self.num_tokens,)\n+        return tuple(input_shape[:-1]) + (self.num_tokens,)\n \n     def compute_output_spec(self, inputs, count_weights=None):\n         output_shape = self.compute_output_shape(inputs.shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a23c2bb14580e16a79597262d65a2fe0aa283090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 114 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class TestCase(unittest.TestCase):\n             x1 = backend.convert_to_numpy(x1)\n         if not isinstance(x2, np.ndarray):\n             x2 = backend.convert_to_numpy(x2)\n-        np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n+        np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol, err_msg=msg)\n \n     def assertNotAllClose(self, x1, x2, atol=1e-6, rtol=1e-6, msg=None):\n         try:\n@@ -62,7 +62,7 @@ class TestCase(unittest.TestCase):\n             x1 = backend.convert_to_numpy(x1)\n         if not isinstance(x2, np.ndarray):\n             x2 = backend.convert_to_numpy(x2)\n-        np.testing.assert_almost_equal(x1, x2, decimal=decimal)\n+        np.testing.assert_almost_equal(x1, x2, decimal=decimal, err_msg=msg)\n \n     def assertAllEqual(self, x1, x2, msg=None):\n         self.assertEqual(len(x1), len(x2), msg=msg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#698cc2f1486b5f80be31ce4fdcdc42a3e6b0fc91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/4 | Churn Δ: 38 | Churn Cumulative: 117 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -219,6 +219,18 @@ class BatchNormalization(Layer):\n         self.built = True\n \n     def compute_output_shape(self, input_shape):\n+        if isinstance(self.axis, int):\n+            axes = [self.axis]\n+        else:\n+            axes = self.axis\n+\n+        for axis in axes:\n+            if axis >= len(input_shape) or axis < -len(input_shape):\n+                raise ValueError(\n+                    f\"Axis {axis} is out of bounds for \"\n+                    f\"input shape {input_shape}. \"\n+                    f\"Received: axis={self.axis}\"\n+                )\n         return input_shape\n \n     def call(self, inputs, training=None, mask=None):\n\n@@ -199,6 +199,18 @@ class GroupNormalization(Layer):\n         return broadcast_shape\n \n     def compute_output_shape(self, input_shape):\n+        if isinstance(self.axis, int):\n+            axes = [self.axis]\n+        else:\n+            axes = self.axis\n+\n+        for axis in axes:\n+            if axis >= len(input_shape) or axis < -len(input_shape):\n+                raise ValueError(\n+                    f\"Axis {axis} is out of bounds for \"\n+                    f\"input shape {input_shape}. \"\n+                    f\"Received: axis={self.axis}\"\n+                )\n         return input_shape\n \n     def get_config(self):\n\n@@ -117,7 +117,7 @@ class LayerNormalization(Layer):\n         gamma_regularizer=None,\n         beta_constraint=None,\n         gamma_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(**kwargs)\n         if isinstance(axis, (list, tuple)):\n@@ -235,6 +235,18 @@ class LayerNormalization(Layer):\n         return ops.cast(outputs, input_dtype)\n \n     def compute_output_shape(self, input_shape):\n+        if isinstance(self.axis, int):\n+            axes = [self.axis]\n+        else:\n+            axes = self.axis\n+\n+        for axis in axes:\n+            if axis >= len(input_shape) or axis < -len(input_shape):\n+                raise ValueError(\n+                    f\"Axis {axis} is out of bounds for \"\n+                    f\"input shape {input_shape}. \"\n+                    f\"Received: axis={self.axis}\"\n+                )\n         return input_shape\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c20c69a694c0b3f9867a9e18e4e5a956a04f1116", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 19/15 | Churn Δ: 139 | Churn Cumulative: 576 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 0.36792452830188677\n\nDIFF:\n@@ -98,7 +98,7 @@ def clone_model(\n             config[\"seed\"] = 1337\n         return layer.__class__.from_config(config)\n \n-    new_model = clone_model(model)\n+    new_model = clone_model(model, clone_function=clone_function)\n     ```\n \n     Using a `call_function` to add a `Dropout` layer after each `Dense` layer\n\n@@ -2,6 +2,7 @@ import numpy as np\n \n from keras.src import backend\n from keras.src.api_export import keras_export\n+from keras.src.utils import tf_utils\n \n \n @keras_export(\"keras.utils.normalize\")\n@@ -119,7 +120,8 @@ def encode_categorical_inputs(\n         dtype: the dtype of the output, unless `count_weights` is not `None`.\n         sparse: whether the output should be sparse for backends supporting it.\n         count_weights: weights to apply if `output_mode` is `\"count\"`.\n-        backend_module: the backend to use instead of the curren one.\n+        backend_module: the backend to use instead of the current one.\n+\n     Returns: the encoded inputs.\n     \"\"\"\n     backend_module = backend_module or backend\n@@ -131,8 +133,26 @@ def encode_categorical_inputs(\n \n     # In all cases, we should uprank scalar input to a single sample.\n     if rank_of_inputs == 0:\n-        # We need to update `rank_of_inputs` if necessary.\n         inputs = backend_module.numpy.expand_dims(inputs, -1)\n+        rank_of_inputs = 1\n+\n+    if (\n+        backend_module.__name__.endswith(\"tensorflow\")\n+        and rank_of_inputs <= 2\n+        and output_mode in (\"multi_hot\", \"count\")\n+    ):\n+        # TF only fastpath. Uses bincount; faster. Doesn't work for rank 3+.\n+        try:\n+            return tf_utils.tf_encode_categorical_inputs(\n+                inputs,\n+                output_mode,\n+                depth,\n+                dtype=dtype,\n+                sparse=sparse,\n+                count_weights=count_weights,\n+            )\n+        except ValueError:\n+            pass\n \n     if output_mode == \"multi_hot\":\n         return backend_module.nn.multi_hot(\n\n@@ -38,3 +38,116 @@ def ensure_tensor(inputs, dtype=None):\n     if dtype is not None and inputs.dtype != dtype:\n         inputs = tf.cast(inputs, dtype)\n     return inputs\n+\n+\n+def sparse_bincount(inputs, depth, binary_output, dtype, count_weights=None):\n+    \"\"\"Apply binary or count encoding to an input and return a sparse tensor.\"\"\"\n+    result = tf.sparse.bincount(\n+        inputs,\n+        weights=count_weights,\n+        minlength=depth,\n+        maxlength=depth,\n+        axis=-1,\n+        binary_output=binary_output,\n+    )\n+    result = tf.cast(result, dtype)\n+    if inputs.shape.rank == 1:\n+        output_shape = (depth,)\n+    else:\n+        batch_size = tf.shape(result)[0]\n+        output_shape = (batch_size, depth)\n+    result = tf.SparseTensor(\n+        indices=result.indices, values=result.values, dense_shape=output_shape\n+    )\n+    return result\n+\n+\n+def dense_bincount(inputs, depth, binary_output, dtype, count_weights=None):\n+    \"\"\"Apply binary or count encoding to an input.\"\"\"\n+    result = tf.math.bincount(\n+        inputs,\n+        weights=count_weights,\n+        minlength=depth,\n+        maxlength=depth,\n+        dtype=dtype,\n+        axis=-1,\n+        binary_output=binary_output,\n+    )\n+    if inputs.shape.rank == 1:\n+        result.set_shape(tf.TensorShape((depth,)))\n+    else:\n+        batch_size = inputs.shape.as_list()[0]\n+        result.set_shape(tf.TensorShape((batch_size, depth)))\n+    return result\n+\n+\n+def expand_dims(inputs, axis):\n+    \"\"\"Expand dims on sparse, ragged, or dense tensors.\"\"\"\n+    if isinstance(inputs, tf.SparseTensor):\n+        return tf.sparse.expand_dims(inputs, axis)\n+    return tf.expand_dims(inputs, axis)\n+\n+\n+def tf_encode_categorical_inputs(\n+    inputs,\n+    output_mode,\n+    depth,\n+    dtype=\"float32\",\n+    sparse=False,\n+    count_weights=None,\n+    idf_weights=None,\n+):\n+    \"\"\"Encodes categorical inputs according to output_mode.\n+\n+    Faster method that relies on bincount.\n+    \"\"\"\n+\n+    if output_mode == \"int\":\n+        return tf.identity(tf.cast(inputs, dtype))\n+\n+    original_shape = inputs.shape\n+    # In all cases, we should uprank scalar input to a single sample.\n+    if inputs.shape.rank == 0:\n+        inputs = expand_dims(inputs, -1)\n+    # One hot will unprank only if the final output dimension is not already 1.\n+    if output_mode == \"one_hot\":\n+        if inputs.shape[-1] != 1:\n+            inputs = expand_dims(inputs, -1)\n+\n+    if inputs.shape.rank > 2:\n+        raise ValueError(\n+            \"When output_mode is not `'int'`, maximum supported output rank \"\n+            f\"is 2. Received output_mode {output_mode} and input shape \"\n+            f\"{original_shape}, \"\n+            f\"which would result in output rank {inputs.shape.rank}.\"\n+        )\n+\n+    binary_output = output_mode in (\"multi_hot\", \"one_hot\")\n+    if sparse:\n+        bincounts = sparse_bincount(\n+            inputs, depth, binary_output, dtype, count_weights\n+        )\n+    else:\n+        bincounts = dense_bincount(\n+            inputs, depth, binary_output, dtype, count_weights\n+        )\n+\n+    bincounts = tf.cast(bincounts, dtype)\n+    if output_mode != \"tf_idf\":\n+        return bincounts\n+\n+    if idf_weights is None:\n+        raise ValueError(\n+            \"When output mode is `'tf_idf'`, idf_weights must be provided. \"\n+            f\"Received: output_mode={output_mode} and idf_weights={idf_weights}\"\n+        )\n+\n+    if sparse:\n+        value_weights = tf.gather(idf_weights, bincounts.indices[:, -1])\n+        return tf.SparseTensor(\n+            bincounts.indices,\n+            value_weights * bincounts.values,\n+            bincounts.dense_shape,\n+        )\n+    else:\n+        return tf.multiply(bincounts, idf_weights)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67b26f5a576e3a42316c735a9c1629c592b4257e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 203 | Lines Deleted: 81 | Files Changed: 6 | Hunks: 37 | Methods Changed: 28 | Complexity Δ (Sum/Max): 55/17 | Churn Δ: 284 | Churn Cumulative: 591 | Contributors (this commit): 1 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: 0.4056603773584906\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras.src.backend import config\n from keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.validation import (\n     densify_bounding_boxes,\n )\n@@ -8,9 +9,13 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n \n     _FACTOR_BOUNDS = (-1, 1)\n \n-    def __init__(self, bounding_box_format=None, **kwargs):\n+    def __init__(\n+        self, factor=0.0, bounding_box_format=None, data_format=None, **kwargs\n+    ):\n         super().__init__(**kwargs)\n         self.bounding_box_format = bounding_box_format\n+        self.data_format = data_format or config.image_data_format()\n+        self._set_factor(factor)\n \n     def _set_factor(self, factor):\n         error_msg = (\n@@ -36,8 +41,7 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n             lower, upper = [max(-factor, self._FACTOR_BOUNDS[0]), factor]\n         else:\n             raise ValueError(error_msg)\n-        self.factor_lower = lower\n-        self.factor_upper = upper\n+        self.factor = lower, upper\n \n     def get_random_transformation(self, data, seed=None):\n         raise NotImplementedError()\n@@ -120,6 +124,14 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n                     training=training,\n                 )\n             if \"bounding_boxes\" in data:\n+                if not self.bounding_box_format:\n+                    raise ValueError(\n+                        \"You passed an input with a 'bounding_boxes' key, \"\n+                        \"but you didn't specify a bounding box format. \"\n+                        \"Pass a `bounding_box_format` argument to your \"\n+                        f\"{self.__class__.__name__} layer, e.g. \"\n+                        \"`bounding_box_format='xyxy'`.\"\n+                    )\n                 bounding_boxes = densify_bounding_boxes(\n                     data[\"bounding_boxes\"], backend=self.backend\n                 )\n@@ -179,3 +191,13 @@ class BaseImagePreprocessingLayer(TFDataLayer):\n             transformation=transformation,\n             training=training,\n         )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        if self.bounding_box_format is not None:\n+            config.update(\n+                {\n+                    \"bounding_box_format\": self.bounding_box_format,\n+                }\n+            )\n+        return config\n\n@@ -1,10 +1,12 @@\n from keras.src.api_export import keras_export\n-from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.random.seed_generator import SeedGenerator\n \n \n @keras_export(\"keras.layers.RandomBrightness\")\n-class RandomBrightness(TFDataLayer):\n+class RandomBrightness(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which randomly adjusts brightness during training.\n \n     This layer will randomly increase/reduce the brightness for the input RGB\n@@ -62,20 +64,15 @@ class RandomBrightness(TFDataLayer):\n     ```\n     \"\"\"\n \n-    _FACTOR_VALIDATION_ERROR = (\n-        \"The `factor` argument should be a number (or a list of two numbers) \"\n-        \"in the range [-1.0, 1.0]. \"\n-    )\n     _VALUE_RANGE_VALIDATION_ERROR = (\n         \"The `value_range` argument should be a list of two numbers. \"\n     )\n \n     def __init__(self, factor, value_range=(0, 255), seed=None, **kwargs):\n-        super().__init__(**kwargs)\n-        self._set_factor(factor)\n-        self._set_value_range(value_range)\n+        super().__init__(factor=factor, **kwargs)\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n+        self._set_value_range(value_range)\n \n     def _set_value_range(self, value_range):\n         if not isinstance(value_range, (tuple, list)):\n@@ -90,39 +87,11 @@ class RandomBrightness(TFDataLayer):\n             )\n         self.value_range = sorted(value_range)\n \n-    def _set_factor(self, factor):\n-        if isinstance(factor, (tuple, list)):\n-            if len(factor) != 2:\n-                raise ValueError(\n-                    self._FACTOR_VALIDATION_ERROR + f\"Received: factor={factor}\"\n-                )\n-            self._check_factor_range(factor[0])\n-            self._check_factor_range(factor[1])\n-            self._factor = sorted(factor)\n-        elif isinstance(factor, (int, float)):\n-            self._check_factor_range(factor)\n-            factor = abs(factor)\n-            self._factor = [-factor, factor]\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n         else:\n-            raise ValueError(\n-                self._FACTOR_VALIDATION_ERROR + f\"Received: factor={factor}\"\n-            )\n-\n-    def _check_factor_range(self, input_number):\n-        if input_number > 1.0 or input_number < -1.0:\n-            raise ValueError(\n-                self._FACTOR_VALIDATION_ERROR\n-                + f\"Received: input_number={input_number}\"\n-            )\n-\n-    def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        if training:\n-            return self._randomly_adjust_brightness(inputs)\n-        else:\n-            return inputs\n-\n-    def _randomly_adjust_brightness(self, images):\n+            images = data\n         images_shape = self.backend.shape(images)\n         rank = len(images_shape)\n         if rank == 3:\n@@ -136,27 +105,49 @@ class RandomBrightness(TFDataLayer):\n                 \"Expected the input image to be rank 3 or 4. Received \"\n                 f\"inputs.shape={images_shape}\"\n             )\n+        if not training:\n+            return {\"rgb_delta\": self.backend.numpy.zeros(rgb_delta_shape)}\n \n-        seed_generator = self._get_seed_generator(self.backend._backend)\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n         rgb_delta = self.backend.random.uniform(\n-            minval=self._factor[0],\n-            maxval=self._factor[1],\n+            minval=self.factor[0],\n+            maxval=self.factor[1],\n             shape=rgb_delta_shape,\n-            seed=seed_generator,\n+            seed=seed,\n         )\n         rgb_delta = rgb_delta * (self.value_range[1] - self.value_range[0])\n+        return {\"rgb_delta\": rgb_delta}\n+\n+    def augment_images(self, images, transformation, training=True):\n+        if training:\n+            rgb_delta = transformation[\"rgb_delta\"]\n             rgb_delta = self.backend.cast(rgb_delta, images.dtype)\n             images += rgb_delta\n             return self.backend.numpy.clip(\n                 images, self.value_range[0], self.value_range[1]\n             )\n+        return images\n+\n+    def augment_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def augment_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        return bounding_boxes\n+\n+    def augment_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n \n     def compute_output_shape(self, input_shape):\n         return input_shape\n \n     def get_config(self):\n         config = {\n-            \"factor\": self._factor,\n+            \"factor\": self.factor,\n             \"value_range\": self.value_range,\n             \"seed\": self.seed,\n         }\n\n@@ -114,3 +114,29 @@ class RandomBrightnessTest(testing.TestCase):\n             layer(\n                 wrong_rank_input, training=True\n             )  # Call the method that triggers the error\n+\n+    def test_dict_input(self):\n+        layer = layers.RandomBrightness(factor=0.1, bounding_box_format=\"xyxy\")\n+        data = {\n+            \"images\": np.random.random((2, 4, 5, 3)),\n+            \"labels\": np.random.random((2, 7)),\n+            \"segmentation_masks\": np.random.random((2, 4, 5, 7)),\n+            \"bounding_boxes\": {\n+                \"boxes\": np.array([[1, 2, 2, 3]]),\n+                \"labels\": np.array([0]),\n+            },\n+        }\n+        transformed_data = layer(data)\n+        self.assertEqual(\n+            data[\"images\"].shape[:-1],\n+            transformed_data[\"segmentation_masks\"].shape[:-1],\n+        )\n+        self.assertAllClose(data[\"labels\"], transformed_data[\"labels\"])\n+        self.assertAllClose(\n+            data[\"bounding_boxes\"][\"boxes\"],\n+            transformed_data[\"bounding_boxes\"][\"boxes\"],\n+        )\n+        self.assertAllClose(\n+            data[\"bounding_boxes\"][\"labels\"],\n+            transformed_data[\"bounding_boxes\"][\"labels\"],\n+        )\n\n@@ -1,10 +1,13 @@\n from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (\n+    BaseImagePreprocessingLayer,\n+)\n from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras.src.random.seed_generator import SeedGenerator\n \n \n @keras_export(\"keras.layers.RandomContrast\")\n-class RandomContrast(TFDataLayer):\n+class RandomContrast(BaseImagePreprocessingLayer):\n     \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n     This layer will randomly adjust the contrast of an image or images\n@@ -41,47 +44,85 @@ class RandomContrast(TFDataLayer):\n         seed: Integer. Used to create a random seed.\n     \"\"\"\n \n+    _FACTOR_BOUNDS = (0, 1)\n+\n     def __init__(self, factor, seed=None, **kwargs):\n         super().__init__(**kwargs)\n-        self.factor = factor\n-        if isinstance(factor, (tuple, list)):\n-            self.lower = factor[0]\n-            self.upper = factor[1]\n-        else:\n-            self.lower = self.upper = factor\n-        if self.lower < 0.0 or self.upper < 0.0 or self.lower > 1.0:\n-            raise ValueError(\n-                \"`factor` argument cannot have negative values or values \"\n-                \"greater than 1.\"\n-                f\"Received: factor={factor}\"\n-            )\n+        self._set_factor(factor)\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n \n-    def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        if training:\n-            seed_generator = self._get_seed_generator(self.backend._backend)\n-            factor = self.backend.random.uniform(\n-                shape=(),\n-                minval=1.0 - self.lower,\n-                maxval=1.0 + self.upper,\n-                seed=seed_generator,\n-                dtype=self.compute_dtype,\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+        images_shape = self.backend.shape(images)\n+        rank = len(images_shape)\n+        if rank == 3:\n+            factor_shape = (1, 1, 1)\n+        elif rank == 4:\n+            # Keep only the batch dim. This will ensure to have same adjustment\n+            # with in one image, but different across the images.\n+            factor_shape = [images_shape[0], 1, 1, 1]\n+        else:\n+            raise ValueError(\n+                \"Expected the input image to be rank 3 or 4. Received \"\n+                f\"inputs.shape={images_shape}\"\n             )\n \n-            outputs = self._adjust_constrast(inputs, factor)\n+        if not training:\n+            return {\"contrast_factor\": self.backend.numpy.zeros(factor_shape)}\n+\n+        if seed is None:\n+            seed = self._get_seed_generator(self.backend._backend)\n+\n+        factor = self.backend.random.uniform(\n+            shape=factor_shape,\n+            minval=1.0 - self.factor[0],\n+            maxval=1.0 + self.factor[1],\n+            seed=seed,\n+            dtype=self.compute_dtype,\n+        )\n+        return {\"contrast_factor\": factor}\n+\n+    def augment_images(self, images, transformation, training=True):\n+        if training:\n+            constrast_factor = transformation[\"contrast_factor\"]\n+            outputs = self._adjust_constrast(images, constrast_factor)\n             outputs = self.backend.numpy.clip(outputs, 0, 255)\n-            self.backend.numpy.reshape(outputs, self.backend.shape(inputs))\n+            self.backend.numpy.reshape(outputs, self.backend.shape(images))\n             return outputs\n-        else:\n-            return inputs\n+        return images\n+\n+    def augment_labels(self, labels, transformation, training=True):\n+        return labels\n+\n+    def augment_bounding_boxes(\n+        self, bounding_boxes, transformation, training=True\n+    ):\n+        return bounding_boxes\n+\n+    def augment_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        return segmentation_masks\n \n     def _adjust_constrast(self, inputs, contrast_factor):\n+        if self.data_format == \"channels_first\":\n+            height_axis = -2\n+            width_axis = -1\n+        else:\n+            height_axis = -3\n+            width_axis = -2\n         # reduce mean on height\n-        inp_mean = self.backend.numpy.mean(inputs, axis=-3, keepdims=True)\n+        inp_mean = self.backend.numpy.mean(\n+            inputs, axis=height_axis, keepdims=True\n+        )\n         # reduce mean on width\n-        inp_mean = self.backend.numpy.mean(inp_mean, axis=-2, keepdims=True)\n+        inp_mean = self.backend.numpy.mean(\n+            inp_mean, axis=width_axis, keepdims=True\n+        )\n \n         outputs = (inputs - inp_mean) * contrast_factor + inp_mean\n         return outputs\n\n@@ -20,6 +20,17 @@ class RandomContrastTest(testing.TestCase):\n             supports_masking=False,\n             expected_output_shape=(8, 3, 4, 3),\n         )\n+        self.run_layer_test(\n+            layers.RandomContrast,\n+            init_kwargs={\n+                \"factor\": 0.75,\n+                \"seed\": 1,\n+                \"data_format\": \"channels_first\",\n+            },\n+            input_shape=(8, 3, 4, 4),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 4),\n+        )\n \n     def test_random_contrast(self):\n         seed = 9809\n@@ -45,3 +56,29 @@ class RandomContrastTest(testing.TestCase):\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n             output.numpy()\n+\n+    def test_dict_input(self):\n+        layer = layers.RandomContrast(factor=0.1, bounding_box_format=\"xyxy\")\n+        data = {\n+            \"images\": np.random.random((2, 4, 5, 3)),\n+            \"labels\": np.random.random((2, 7)),\n+            \"segmentation_masks\": np.random.random((2, 4, 5, 7)),\n+            \"bounding_boxes\": {\n+                \"boxes\": np.array([[1, 2, 2, 3]]),\n+                \"labels\": np.array([0]),\n+            },\n+        }\n+        transformed_data = layer(data)\n+        self.assertEqual(\n+            data[\"images\"].shape[:-1],\n+            transformed_data[\"segmentation_masks\"].shape[:-1],\n+        )\n+        self.assertAllClose(data[\"labels\"], transformed_data[\"labels\"])\n+        self.assertAllClose(\n+            data[\"bounding_boxes\"][\"boxes\"],\n+            transformed_data[\"bounding_boxes\"][\"boxes\"],\n+        )\n+        self.assertAllClose(\n+            data[\"bounding_boxes\"][\"labels\"],\n+            transformed_data[\"bounding_boxes\"][\"labels\"],\n+        )\n\n@@ -141,7 +141,9 @@ class RandomCropTest(testing.TestCase):\n         self.assertEqual(tuple(output.shape), output_shape)\n \n     def test_dict_input(self):\n-        layer = layers.RandomCrop(3, 3, data_format=\"channels_last\")\n+        layer = layers.RandomCrop(\n+            3, 3, data_format=\"channels_last\", bounding_box_format=\"xyxy\"\n+        )\n         data = {\n             \"images\": np.random.random((2, 4, 5, 3)),\n             \"labels\": np.random.random((2, 7)),\n@@ -156,6 +158,9 @@ class RandomCropTest(testing.TestCase):\n             data[\"images\"].shape[:-1],\n             transformed_data[\"segmentation_masks\"].shape[:-1],\n         )\n-        self.assertAllEqual(data[\"labels\"], transformed_data[\"labels\"])\n+        self.assertAllClose(data[\"labels\"], transformed_data[\"labels\"])\n         self.assertEqual(data[\"bounding_boxes\"][\"boxes\"].shape, (1, 4))\n-        self.assertEqual(data[\"bounding_boxes\"][\"labels\"].shape, (1,))\n+        self.assertAllClose(\n+            data[\"bounding_boxes\"][\"labels\"],\n+            transformed_data[\"bounding_boxes\"][\"labels\"],\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
