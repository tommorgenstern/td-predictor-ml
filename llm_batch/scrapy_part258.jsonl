{"custom_id": "scrapy#312e57382c2c49d9676f861ac6cf4202f69a84e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 45 | Churn Cumulative: 179 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,9 @@ from tests.mockserver import MockServer\n \n \n class InjectArgumentsDownloaderMiddleware(object):\n+    \"\"\"\n+    Make sure downloader middlewares are able to update the keyword arguments\n+    \"\"\"\n     def process_request(self, request, spider):\n         if request.callback.__name__ == 'parse_downloader_mw':\n             request.cb_kwargs['from_process_request'] = True\n@@ -21,12 +24,38 @@ class InjectArgumentsDownloaderMiddleware(object):\n         return response\n \n \n+class InjectArgumentsSpiderMiddleware(object):\n+    \"\"\"\n+    Make sure spider middlewares are able to update the keyword arguments\n+    \"\"\"\n+    def process_start_requests(self, start_requests, spider):\n+        for request in start_requests:\n+            if request.callback.__name__ == 'parse_spider_mw':\n+                request.cb_kwargs['from_process_start_requests'] = True\n+            yield request\n+\n+    def process_spider_input(self, response, spider):\n+        request = response.request\n+        if request.callback.__name__ == 'parse_spider_mw':\n+            request.cb_kwargs['from_process_spider_input'] = True\n+        return None\n+\n+    def process_spider_output(self, response, result, spider):\n+        for element in result:\n+            if isinstance(element, Request) and element.callback.__name__ == 'parse_spider_mw_2':\n+                element.cb_kwargs['from_process_spider_output'] = True\n+            yield element\n+\n+\n class KeywordArgumentsSpider(MockServerSpider):\n     name = 'kwargs'\n     custom_settings = {\n         'DOWNLOADER_MIDDLEWARES': {\n             __name__ + '.InjectArgumentsDownloaderMiddleware': 750,\n-        }\n+        },\n+        'SPIDER_MIDDLEWARES': {\n+            __name__ + '.InjectArgumentsSpiderMiddleware': 750,\n+        },\n     }\n \n     checks = list()\n@@ -41,6 +70,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         yield Request(self.mockserver.url('/takes_less'), self.parse_takes_less, cb_kwargs=data)\n         yield Request(self.mockserver.url('/takes_more'), self.parse_takes_more, cb_kwargs=data)\n         yield Request(self.mockserver.url('/downloader_mw'), self.parse_downloader_mw)\n+        yield Request(self.mockserver.url('/spider_mw'), self.parse_spider_mw)\n \n     def parse_first(self, response, key, number):\n         self.checks.append(key == 'value')\n@@ -88,13 +118,20 @@ class KeywordArgumentsSpider(MockServerSpider):\n         \"\"\"\n \n     def parse_downloader_mw(self, response, from_process_request, from_process_response):\n-        \"\"\"\n-        Check if downloader middlewares are able to update the keyword arguments\n-        \"\"\"\n         self.checks.append(bool(from_process_request))\n         self.checks.append(bool(from_process_response))\n         self.crawler.stats.inc_value('boolean_checks', 2)\n \n+    def parse_spider_mw(self, response, from_process_spider_input, from_process_start_requests):\n+        self.checks.append(bool(from_process_spider_input))\n+        self.checks.append(bool(from_process_start_requests))\n+        self.crawler.stats.inc_value('boolean_checks', 2)\n+        return Request(self.mockserver.url('/spider_mw_2'), self.parse_spider_mw_2)\n+\n+    def parse_spider_mw_2(self, response, from_process_spider_output):\n+        self.checks.append(bool(from_process_spider_output))\n+        self.crawler.stats.inc_value('boolean_checks', 1)\n+\n \n class CallbackKeywordArgumentsTestCase(TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2e4dc20393c3a678a6576fb20817d4339f66235c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 984 | Contributors (this commit): 25 | Commits (past 90d): 3 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ from scrapy.utils.response import get_base_url\n \n \n class FormRequest(Request):\n-    valid_form_methods = ['GET', 'POST', 'DIALOG']\n+    valid_form_methods = ['GET', 'POST']\n \n     def __init__(self, *args, **kwargs):\n         formdata = kwargs.pop('formdata', None)\n@@ -50,7 +50,9 @@ class FormRequest(Request):\n         formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n         url = _get_form_url(form, kwargs.pop('url', None))\n \n-        method = kwargs.pop('method', form.method).upper()\n+        method = kwargs.pop('method', form.method)\n+        if method is not None:\n+            method = method.upper()\n             if method not in cls.valid_form_methods:\n                 method = 'GET'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69b1d5d3d7050b5c60a95cc547febaded1b3685f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 68 | Churn Cumulative: 196 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,6 +2,7 @@ import logging\n from OpenSSL import SSL\n \n from scrapy import twisted_version\n+from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n \n \n logger = logging.getLogger(__name__)\n@@ -20,6 +21,7 @@ openssl_methods = {\n     METHOD_TLSv12: getattr(SSL, 'TLSv1_2_METHOD', 6),   # TLS 1.2 only\n }\n \n+\n if twisted_version >= (14, 0, 0):\n     # ClientTLSOptions requires a recent-enough version of Twisted.\n     # Not having ScrapyClientTLSOptions should not matter for older\n@@ -65,13 +67,27 @@ if twisted_version >= (14, 0, 0):\n         Same as Twisted's private _sslverify.ClientTLSOptions,\n         except that VerificationError, CertificateError and ValueError\n         exceptions are caught, so that the connection is not closed, only\n-        logging warnings.\n+        logging warnings. Also, HTTPS connection parameters logging is added.\n         \"\"\"\n \n         def _identityVerifyingInfoCallback(self, connection, where, ret):\n             if where & SSL_CB_HANDSHAKE_START:\n                 set_tlsext_host_name(connection, self._hostnameBytes)\n             elif where & SSL_CB_HANDSHAKE_DONE:\n+                logger.debug('SSL connection to %s using protocol %s, cipher %s',\n+                             self._hostnameASCII,\n+                             connection.get_protocol_version_name(),\n+                             connection.get_cipher_name(),\n+                             )\n+                server_cert = connection.get_peer_certificate()\n+                logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n+                             x509name_to_string(server_cert.get_issuer()),\n+                             x509name_to_string(server_cert.get_subject()),\n+                             )\n+                key_info = get_temp_key_info(connection._ssl)\n+                if key_info:\n+                    logger.debug('SSL temp key: %s', key_info)\n+\n                 try:\n                     verifyHostname(connection, self._hostnameASCII)\n                 except verification_errors as e:\n\n@@ -0,0 +1,50 @@\n+# -*- coding: utf-8 -*-\n+\n+import OpenSSL._util as pyOpenSSLutil\n+\n+from scrapy.utils.python import to_native_str\n+\n+\n+def ffi_buf_to_string(buf):\n+    return to_native_str(pyOpenSSLutil.ffi.string(buf))\n+\n+\n+def x509name_to_string(x509name):\n+    # from OpenSSL.crypto.X509Name.__repr__\n+    result_buffer = pyOpenSSLutil.ffi.new(\"char[]\", 512)\n+    pyOpenSSLutil.lib.X509_NAME_oneline(x509name._name, result_buffer, len(result_buffer))\n+\n+    return ffi_buf_to_string(result_buffer)\n+\n+\n+def get_temp_key_info(ssl_object):\n+    if not hasattr(pyOpenSSLutil.lib, 'SSL_get_server_tmp_key'):  # requires OpenSSL 1.0.2\n+        return None\n+\n+    # adapted from OpenSSL apps/s_cb.c::ssl_print_tmp_key()\n+    temp_key_p = pyOpenSSLutil.ffi.new(\"EVP_PKEY **\")\n+    pyOpenSSLutil.lib.SSL_get_server_tmp_key(ssl_object, temp_key_p)\n+    if temp_key_p == pyOpenSSLutil.ffi.NULL:\n+        return None\n+\n+    temp_key = temp_key_p[0]\n+    pyOpenSSLutil.ffi.gc(temp_key, pyOpenSSLutil.lib.EVP_PKEY_free)\n+    key_info = []\n+    key_type = pyOpenSSLutil.lib.EVP_PKEY_id(temp_key)\n+    if key_type == pyOpenSSLutil.lib.EVP_PKEY_RSA:\n+        key_info.append('RSA')\n+    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_DH:\n+        key_info.append('DH')\n+    elif key_type == pyOpenSSLutil.lib.EVP_PKEY_EC:\n+        key_info.append('ECDH')\n+        ec_key = pyOpenSSLutil.lib.EVP_PKEY_get1_EC_KEY(temp_key)\n+        pyOpenSSLutil.ffi.gc(ec_key, pyOpenSSLutil.lib.EC_KEY_free)\n+        nid = pyOpenSSLutil.lib.EC_GROUP_get_curve_name(pyOpenSSLutil.lib.EC_KEY_get0_group(ec_key))\n+        cname = pyOpenSSLutil.lib.EC_curve_nid2nist(nid)\n+        if cname == pyOpenSSLutil.ffi.NULL:\n+            cname = pyOpenSSLutil.lib.OBJ_nid2sn(nid)\n+        key_info.append(ffi_buf_to_string(cname))\n+    else:\n+        key_info.append(ffi_buf_to_string(pyOpenSSLutil.lib.OBJ_nid2sn(key_type)))\n+    key_info.append('%s bits' % pyOpenSSLutil.lib.EVP_PKEY_bits(temp_key))\n+    return ', '.join(key_info)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f52dada551a7467436e53756be4d8cdbe798380", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 15 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 642 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from __future__ import print_function\n-import sys, os\n+import sys\n+import os\n import optparse\n import cProfile\n import inspect\n@@ -14,6 +15,7 @@ from scrapy.utils.project import inside_project, get_project_settings\n from scrapy.utils.python import garbage_collect\n from scrapy.settings.deprecated import check_deprecated_settings\n \n+\n def _iter_command_classes(module_name):\n     # TODO: add `name` attribute to commands and and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n@@ -25,6 +27,7 @@ def _iter_command_classes(module_name):\n                     not obj == ScrapyCommand:\n                 yield obj\n \n+\n def _get_commands_from_module(module, inproject):\n     d = {}\n     for cmd in _iter_command_classes(module):\n@@ -33,6 +36,7 @@ def _get_commands_from_module(module, inproject):\n             d[cmdname] = cmd()\n     return d\n \n+\n def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n     cmds = {}\n     for entry_point in pkg_resources.iter_entry_points(group):\n@@ -43,6 +47,7 @@ def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n             raise Exception(\"Invalid entry point %s\" % entry_point.name)\n     return cmds\n \n+\n def _get_commands_dict(settings, inproject):\n     cmds = _get_commands_from_module('scrapy.commands', inproject)\n     cmds.update(_get_commands_from_entry_points(inproject))\n@@ -51,6 +56,7 @@ def _get_commands_dict(settings, inproject):\n         cmds.update(_get_commands_from_module(cmds_module, inproject))\n     return cmds\n \n+\n def _pop_command_name(argv):\n     i = 0\n     for arg in argv[1:]:\n@@ -59,6 +65,7 @@ def _pop_command_name(argv):\n             return arg\n         i += 1\n \n+\n def _print_header(settings, inproject):\n     if inproject:\n         print(\"Scrapy %s - project: %s\\n\" % (scrapy.__version__, \\\n@@ -66,6 +73,7 @@ def _print_header(settings, inproject):\n     else:\n         print(\"Scrapy %s - no active project\\n\" % scrapy.__version__)\n \n+\n def _print_commands(settings, inproject):\n     _print_header(settings, inproject)\n     print(\"Usage:\")\n@@ -80,11 +88,13 @@ def _print_commands(settings, inproject):\n     print()\n     print('Use \"scrapy <command> -h\" to see more info about a command')\n \n+\n def _print_unknown_command(settings, cmdname, inproject):\n     _print_header(settings, inproject)\n     print(\"Unknown command: %s\\n\" % cmdname)\n     print('Use \"scrapy\" to see available commands')\n \n+\n def _run_print_help(parser, func, *a, **kw):\n     try:\n         func(*a, **kw)\n@@ -95,6 +105,7 @@ def _run_print_help(parser, func, *a, **kw):\n             parser.print_help()\n         sys.exit(2)\n \n+\n def execute(argv=None, settings=None):\n     if argv is None:\n         argv = sys.argv\n@@ -111,7 +122,8 @@ def execute(argv=None, settings=None):\n         # set EDITOR from environment if available\n         try:\n             editor = os.environ['EDITOR']\n-        except KeyError: pass\n+        except KeyError:\n+            pass\n         else:\n             settings['EDITOR'] = editor\n     check_deprecated_settings(settings)\n@@ -150,12 +162,14 @@ def execute(argv=None, settings=None):\n     _run_print_help(parser, _run_command, cmd, args, opts)\n     sys.exit(cmd.exitcode)\n \n+\n def _run_command(cmd, args, opts):\n     if opts.profile:\n         _run_command_profiled(cmd, args, opts)\n     else:\n         cmd.run(args, opts)\n \n+\n def _run_command_profiled(cmd, args, opts):\n     if opts.profile:\n         sys.stderr.write(\"scrapy: writing cProfile stats to %r\\n\" % opts.profile)\n@@ -165,6 +179,7 @@ def _run_command_profiled(cmd, args, opts):\n     if opts.profile:\n         p.dump_stats(opts.profile)\n \n+\n if __name__ == '__main__':\n     try:\n         execute()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a398b70868856f6bd17d36b7359e344f08f1440", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 721 | Files Changed: 26 | Hunks: 48 | Methods Changed: 62 | Complexity Δ (Sum/Max): -85/2 | Churn Δ: 729 | Churn Cumulative: 12694 | Contributors (this commit): 67 | Commits (past 90d): 90 | Contributors (cumulative): 230 | DMM Complexity: 0.044270833333333336\n\nDIFF:\n@@ -9,10 +9,6 @@ def _py_files(folder):\n \n \n collect_ignore = [\n-    # deprecated or moved modules\n-    \"scrapy/conf.py\",\n-    \"scrapy/log.py\",\n-\n     # not a test, but looks like a test\n     \"scrapy/utils/testsite.py\",\n \n\n@@ -99,13 +99,6 @@ def execute(argv=None, settings=None):\n     if argv is None:\n         argv = sys.argv\n \n-    # --- backward compatibility for scrapy.conf.settings singleton ---\n-    if settings is None and 'scrapy.conf' in sys.modules:\n-        from scrapy import conf\n-        if hasattr(conf, 'settings'):\n-            settings = conf.settings\n-    # ------------------------------------------------------------------\n-\n     if settings is None:\n         settings = get_project_settings()\n         # set EDITOR from environment if available\n@@ -116,15 +109,6 @@ def execute(argv=None, settings=None):\n             settings['EDITOR'] = editor\n     check_deprecated_settings(settings)\n \n-    # --- backward compatibility for scrapy.conf.settings singleton ---\n-    import warnings\n-    from scrapy.exceptions import ScrapyDeprecationWarning\n-    with warnings.catch_warnings():\n-        warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n-        from scrapy import conf\n-        conf.settings = settings\n-    # ------------------------------------------------------------------\n-\n     inproject = inside_project()\n     cmds = _get_commands_dict(settings, inproject)\n     cmdname = _pop_command_name(argv)\n\n@@ -1,13 +0,0 @@\n-# This module is kept for backward compatibility, so users can import\n-# scrapy.conf.settings and get the settings they expect\n-\n-import sys\n-\n-if 'scrapy.cmdline' not in sys.modules:\n-    from scrapy.utils.project import get_project_settings\n-    settings = get_project_settings()\n-\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.conf` is deprecated, use `crawler.settings` attribute instead\",\n-    ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -60,10 +60,6 @@ class Slot(object):\n \n def _get_concurrency_delay(concurrency, spider, settings):\n     delay = settings.getfloat('DOWNLOAD_DELAY')\n-    if hasattr(spider, 'DOWNLOAD_DELAY'):\n-        warnings.warn(\"%s.DOWNLOAD_DELAY attribute is deprecated, use %s.download_delay instead\" %\n-                      (type(spider).__name__, type(spider).__name__))\n-        delay = spider.DOWNLOAD_DELAY\n     if hasattr(spider, 'download_delay'):\n         delay = spider.download_delay\n \n\n@@ -1,15 +1,3 @@\n from __future__ import absolute_import\n from .http10 import HTTP10DownloadHandler\n from .http11 import HTTP11DownloadHandler as HTTPDownloadHandler\n-\n-\n-# backward compatibility\n-class HttpDownloadHandler(HTTP10DownloadHandler):\n-\n-    def __init__(self, *args, **kwargs):\n-        import warnings\n-        from scrapy.exceptions import ScrapyDeprecationWarning\n-        warnings.warn('HttpDownloadHandler is deprecated, import scrapy.core.downloader'\n-                      '.handlers.http10.HTTP10DownloadHandler instead',\n-                      category=ScrapyDeprecationWarning, stacklevel=1)\n-        super(HttpDownloadHandler, self).__init__(*args, **kwargs)\n\n@@ -332,14 +332,7 @@ class CrawlerProcess(CrawlerRunner):\n \n def _get_spider_loader(settings):\n     \"\"\" Get SpiderLoader instance from settings \"\"\"\n-    if settings.get('SPIDER_MANAGER_CLASS'):\n-        warnings.warn(\n-            'SPIDER_MANAGER_CLASS option is deprecated. '\n-            'Please use SPIDER_LOADER_CLASS.',\n-            category=ScrapyDeprecationWarning, stacklevel=2\n-        )\n-    cls_path = settings.get('SPIDER_MANAGER_CLASS',\n-                            settings.get('SPIDER_LOADER_CLASS'))\n+    cls_path = settings.get('SPIDER_LOADER_CLASS')\n     loader_cls = load_object(cls_path)\n     try:\n         verifyClass(ISpiderLoader, loader_cls)\n\n@@ -8,8 +8,6 @@ import six\n \n from scrapy.item import Item\n from scrapy.selector import Selector\n-from scrapy.utils.decorators import deprecated\n-from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.misc import arg_to_iter, extract_regex\n from scrapy.utils.python import flatten\n \n@@ -191,10 +189,6 @@ class ItemLoader(object):\n         values = self._get_xpathvalues(xpath, **kw)\n         return self.get_value(values, *processors, **kw)\n \n-    @deprecated(use_instead='._get_xpathvalues()')\n-    def _get_values(self, xpaths, **kw):\n-        return self._get_xpathvalues(xpaths, **kw)\n-\n     def _get_xpathvalues(self, xpaths, **kw):\n         self._check_selector_method()\n         xpaths = arg_to_iter(xpaths)\n@@ -216,5 +210,3 @@ class ItemLoader(object):\n         self._check_selector_method()\n         csss = arg_to_iter(csss)\n         return flatten(self.selector.css(css).getall() for css in csss)\n-\n-XPathItemLoader = create_deprecated_class('XPathItemLoader', ItemLoader)\n\n@@ -1,60 +0,0 @@\n-\"\"\"\n-This module is kept to provide a helpful warning about its removal.\n-\"\"\"\n-import logging\n-import warnings\n-\n-from twisted.python.failure import Failure\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.log import failure_to_exc_info\n-\n-logger = logging.getLogger(__name__)\n-\n-warnings.warn(\"Module `scrapy.log` has been deprecated, Scrapy now relies on \"\n-              \"the builtin Python library for logging. Read the updated \"\n-              \"logging entry in the documentation to learn more.\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-\n-# Imports and level_names variable kept for backward-compatibility\n-\n-DEBUG = logging.DEBUG\n-INFO = logging.INFO\n-WARNING = logging.WARNING\n-ERROR = logging.ERROR\n-CRITICAL = logging.CRITICAL\n-SILENT = CRITICAL + 1\n-\n-level_names = {\n-    logging.DEBUG: \"DEBUG\",\n-    logging.INFO: \"INFO\",\n-    logging.WARNING: \"WARNING\",\n-    logging.ERROR: \"ERROR\",\n-    logging.CRITICAL: \"CRITICAL\",\n-    SILENT: \"SILENT\",\n-}\n-\n-\n-def msg(message=None, _level=logging.INFO, **kw):\n-    warnings.warn('log.msg has been deprecated, create a python logger and '\n-                  'log through it instead',\n-                  ScrapyDeprecationWarning, stacklevel=2)\n-\n-    level = kw.pop('level', _level)\n-    message = kw.pop('format', message)\n-    # NOTE: logger.log doesn't handle well passing empty dictionaries with format\n-    # arguments because of some weird use-case:\n-    # https://hg.python.org/cpython/file/648dcafa7e5f/Lib/logging/__init__.py#l269\n-    logger.log(level, message, *[kw] if kw else [])\n-\n-\n-def err(_stuff=None, _why=None, **kw):\n-    warnings.warn('log.err has been deprecated, create a python logger and '\n-                  'use its error method instead',\n-                  ScrapyDeprecationWarning, stacklevel=2)\n-\n-    level = kw.pop('level', logging.ERROR)\n-    failure = kw.pop('failure', _stuff) or Failure()\n-    message = kw.pop('why', _why) or failure.value\n-    logger.log(level, message, *[kw] if kw else [], exc_info=failure_to_exc_info(failure))\n\n@@ -458,33 +458,6 @@ class FilesPipeline(MediaPipeline):\n         return item\n \n     def file_path(self, request, response=None, info=None):\n-        ## start of deprecation warning block (can be removed in the future)\n-        def _warn():\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n-            warnings.warn('FilesPipeline.file_key(url) method is deprecated, please use '\n-                          'file_path(request, response=None, info=None) instead',\n-                          category=ScrapyDeprecationWarning, stacklevel=1)\n-\n-        # check if called from file_key with url as first argument\n-        if not isinstance(request, Request):\n-            _warn()\n-            url = request\n-        else:\n-            url = request.url\n-\n-        # detect if file_key() method has been overridden\n-        if not hasattr(self.file_key, '_base'):\n-            _warn()\n-            return self.file_key(url)\n-        ## end of deprecation warning block\n-\n-        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n-        media_ext = os.path.splitext(url)[1]  # change to request.url after deprecation\n+        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n+        media_ext = os.path.splitext(request.url)[1]\n         return 'full/%s%s' % (media_guid, media_ext)\n-\n-    # deprecated\n-    def file_key(self, url):\n-        return self.file_path(url)\n-\n-    file_key._base = True\n\n@@ -165,69 +165,9 @@ class ImagesPipeline(FilesPipeline):\n         return item\n \n     def file_path(self, request, response=None, info=None):\n-        ## start of deprecation warning block (can be removed in the future)\n-        def _warn():\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n-            warnings.warn('ImagesPipeline.image_key(url) and file_key(url) methods are deprecated, '\n-                          'please use file_path(request, response=None, info=None) instead',\n-                          category=ScrapyDeprecationWarning, stacklevel=1)\n-\n-        # check if called from image_key or file_key with url as first argument\n-        if not isinstance(request, Request):\n-            _warn()\n-            url = request\n-        else:\n-            url = request.url\n-\n-        # detect if file_key() or image_key() methods have been overridden\n-        if not hasattr(self.file_key, '_base'):\n-            _warn()\n-            return self.file_key(url)\n-        elif not hasattr(self.image_key, '_base'):\n-            _warn()\n-            return self.image_key(url)\n-        ## end of deprecation warning block\n-\n-        image_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n+        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return 'full/%s.jpg' % (image_guid)\n \n     def thumb_path(self, request, thumb_id, response=None, info=None):\n-        ## start of deprecation warning block (can be removed in the future)\n-        def _warn():\n-            from scrapy.exceptions import ScrapyDeprecationWarning\n-            import warnings\n-            warnings.warn('ImagesPipeline.thumb_key(url) method is deprecated, please use '\n-                          'thumb_path(request, thumb_id, response=None, info=None) instead',\n-                          category=ScrapyDeprecationWarning, stacklevel=1)\n-\n-        # check if called from thumb_key with url as first argument\n-        if not isinstance(request, Request):\n-            _warn()\n-            url = request\n-        else:\n-            url = request.url\n-\n-        # detect if thumb_key() method has been overridden\n-        if not hasattr(self.thumb_key, '_base'):\n-            _warn()\n-            return self.thumb_key(url, thumb_id)\n-        ## end of deprecation warning block\n-\n-        thumb_guid = hashlib.sha1(to_bytes(url)).hexdigest()  # change to request.url after deprecation\n+        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n         return 'thumbs/%s/%s.jpg' % (thumb_id, thumb_guid)\n-\n-    # deprecated\n-    def file_key(self, url):\n-        return self.image_key(url)\n-    file_key._base = True\n-\n-    # deprecated\n-    def image_key(self, url):\n-        return self.file_path(url)\n-    image_key._base = True\n-\n-    # deprecated\n-    def thumb_key(self, url, thumb_id):\n-        return self.thumb_path(url, thumb_id)\n-    thumb_key._base = True\n\n@@ -2,4 +2,3 @@\n Selectors\n \"\"\"\n from scrapy.selector.unified import *\n-from scrapy.selector.lxmlsel import *\n\n@@ -1,15 +0,0 @@\n-from parsel.csstranslator import XPathExpr, GenericTranslator, HTMLTranslator\n-from scrapy.utils.deprecate import create_deprecated_class\n-\n-\n-ScrapyXPathExpr = create_deprecated_class(\n-    'ScrapyXPathExpr', XPathExpr,\n-    new_class_path='parsel.csstranslator.XPathExpr')\n-\n-ScrapyGenericTranslator = create_deprecated_class(\n-    'ScrapyGenericTranslator', GenericTranslator,\n-    new_class_path='parsel.csstranslator.GenericTranslator')\n-\n-ScrapyHTMLTranslator = create_deprecated_class(\n-    'ScrapyHTMLTranslator', HTMLTranslator,\n-    new_class_path='parsel.csstranslator.HTMLTranslator')\n\n@@ -1,50 +0,0 @@\n-\"\"\"\n-XPath selectors based on lxml\n-\"\"\"\n-from scrapy.utils.deprecate import create_deprecated_class\n-from .unified import Selector, SelectorList\n-\n-\n-__all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector',\n-           'XPathSelectorList']\n-\n-def _xpathselector_css(self, *a, **kw):\n-    raise RuntimeError('.css() method not available for %s, '\n-                        'instantiate scrapy.Selector '\n-                        'instead' % type(self).__name__)\n-\n-XPathSelector = create_deprecated_class(\n-    'XPathSelector',\n-    Selector,\n-    {\n-        '__slots__': (),\n-        '_default_type': 'html',\n-        'css': _xpathselector_css,\n-    },\n-    new_class_path='scrapy.Selector',\n-    old_class_path='scrapy.selector.XPathSelector',\n-)\n-\n-XmlXPathSelector = create_deprecated_class(\n-    'XmlXPathSelector',\n-    XPathSelector,\n-    clsdict={\n-        '__slots__': (),\n-        '_default_type': 'xml',\n-    },\n-    new_class_path='scrapy.Selector',\n-    old_class_path='scrapy.selector.XmlXPathSelector',\n-)\n-\n-HtmlXPathSelector = create_deprecated_class(\n-    'HtmlXPathSelector',\n-    XPathSelector,\n-    clsdict={\n-        '__slots__': (),\n-        '_default_type': 'html',\n-    },\n-    new_class_path='scrapy.Selector',\n-    old_class_path='scrapy.selector.HtmlXPathSelector',\n-)\n-\n-XPathSelectorList = create_deprecated_class('XPathSelectorList', SelectorList)\n\n@@ -8,7 +8,6 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.python import to_bytes\n from scrapy.http import HtmlResponse, XmlResponse\n from scrapy.utils.decorators import deprecated\n-from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n __all__ = ['Selector', 'SelectorList']\n@@ -31,17 +30,6 @@ class SelectorList(_ParselSelector.selectorlist_cls, object_ref):\n     The :class:`SelectorList` class is a subclass of the builtin ``list``\n     class, which provides a few additional methods.\n     \"\"\"\n-    @deprecated(use_instead='.extract()')\n-    def extract_unquoted(self):\n-        return [x.extract_unquoted() for x in self]\n-\n-    @deprecated(use_instead='.xpath()')\n-    def x(self, xpath):\n-        return self.select(xpath)\n-\n-    @deprecated(use_instead='.xpath()')\n-    def select(self, xpath):\n-        return self.xpath(xpath)\n \n \n class Selector(_ParselSelector, object_ref):\n@@ -78,21 +66,13 @@ class Selector(_ParselSelector, object_ref):\n     __slots__ = ['response']\n     selectorlist_cls = SelectorList\n \n-    def __init__(self, response=None, text=None, type=None, root=None, _root=None, **kwargs):\n+    def __init__(self, response=None, text=None, type=None, root=None, **kwargs):\n         if not(response is None or text is None):\n            raise ValueError('%s.__init__() received both response and text'\n                             % self.__class__.__name__)\n \n         st = _st(response, type or self._default_type)\n \n-        if _root is not None:\n-            warnings.warn(\"Argument `_root` is deprecated, use `root` instead\",\n-                          ScrapyDeprecationWarning, stacklevel=2)\n-            if root is None:\n-                root = _root\n-            else:\n-                warnings.warn(\"Ignoring deprecated `_root` argument, using provided `root`\")\n-\n         if text is not None:\n             response = _response_from_text(text, st)\n \n@@ -102,18 +82,3 @@ class Selector(_ParselSelector, object_ref):\n \n         self.response = response\n         super(Selector, self).__init__(text=text, type=st, root=root, **kwargs)\n-\n-    # Deprecated api\n-    @property\n-    def _root(self):\n-        warnings.warn(\"Attribute `_root` is deprecated, use `root` instead\",\n-                      ScrapyDeprecationWarning, stacklevel=2)\n-        return self.root\n-\n-    @deprecated(use_instead='.xpath()')\n-    def select(self, xpath):\n-        return self.xpath(xpath)\n-\n-    @deprecated(use_instead='.extract()')\n-    def extract_unquoted(self):\n-        return self.extract()\n\n@@ -10,7 +10,6 @@ from scrapy import signals\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n-from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import method_is_overridden\n \n@@ -52,15 +51,6 @@ class Spider(object_ref):\n         spider._set_crawler(crawler)\n         return spider\n \n-    def set_crawler(self, crawler):\n-        warnings.warn(\"set_crawler is deprecated, instantiate and bound the \"\n-                      \"spider to this crawler with from_crawler method \"\n-                      \"instead.\",\n-                      category=ScrapyDeprecationWarning, stacklevel=2)\n-        assert not hasattr(self, 'crawler'), \"Spider already bounded to a \" \\\n-                                             \"crawler\"\n-        self._set_crawler(crawler)\n-\n     def _set_crawler(self, crawler):\n         self.crawler = crawler\n         self.settings = crawler.settings\n@@ -109,22 +99,6 @@ class Spider(object_ref):\n     __repr__ = __str__\n \n \n-BaseSpider = create_deprecated_class('BaseSpider', Spider)\n-\n-\n-class ObsoleteClass(object):\n-    def __init__(self, message):\n-        self.message = message\n-\n-    def __getattr__(self, name):\n-        raise AttributeError(self.message)\n-\n-spiders = ObsoleteClass(\n-    '\"from scrapy.spider import spiders\" no longer works - use '\n-    '\"from scrapy.spiderloader import SpiderLoader\" and instantiate '\n-    'it with your project settings\"'\n-)\n-\n # Top-level imports\n from scrapy.spiders.crawl import CrawlSpider, Rule\n from scrapy.spiders.feed import XMLFeedSpider, CSVFeedSpider\n\n@@ -119,7 +119,3 @@ class CrawlSpider(Spider):\n         spider._follow_links = crawler.settings.getbool(\n             'CRAWLSPIDER_FOLLOW_LINKS', True)\n         return spider\n-\n-    def set_crawler(self, crawler):\n-        super(CrawlSpider, self).set_crawler(crawler)\n-        self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)\n\n@@ -1,7 +0,0 @@\n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.telnet` is deprecated, \"\n-              \"use `scrapy.extensions.telnet` instead\",\n-              ScrapyDeprecationWarning, stacklevel=2)\n-\n-from scrapy.extensions.telnet import *\n\n@@ -84,19 +84,6 @@ def unique(list_, key=lambda x: x):\n     return result\n \n \n-@deprecated(\"scrapy.utils.python.to_unicode\")\n-def str_to_unicode(text, encoding=None, errors='strict'):\n-    \"\"\" This function is deprecated.\n-    Please use scrapy.utils.python.to_unicode. \"\"\"\n-    return to_unicode(text, encoding, errors)\n-\n-\n-@deprecated(\"scrapy.utils.python.to_bytes\")\n-def unicode_to_str(text, encoding=None, errors='strict'):\n-    \"\"\" This function is deprecated. Please use scrapy.utils.python.to_bytes \"\"\"\n-    return to_bytes(text, encoding, errors)\n-\n-\n def to_unicode(text, encoding=None, errors='strict'):\n     \"\"\"Return the unicode representation of a bytes object ``text``. If\n     ``text`` is already an unicode object, return it as-is.\"\"\"\n\n@@ -11,14 +11,6 @@ from twisted.web import http\n from scrapy.utils.python import to_bytes, to_native_str\n from w3lib import html\n \n-from scrapy.utils.decorators import deprecated\n-\n-\n-@deprecated\n-def body_or_str(*a, **kw):\n-    from scrapy.utils.iterators import _body_or_str\n-    return _body_or_str(*a, **kw)\n-\n \n _baseurl_cache = weakref.WeakKeyDictionary()\n def get_base_url(response):\n\n@@ -176,27 +176,6 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n             sl_cls = load_object(runner.settings['SPIDER_LOADER_CLASS'])\n             self.assertIsInstance(spiders, sl_cls)\n \n-    def test_spidermanager_deprecation(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            runner = CrawlerRunner({\n-                'SPIDER_MANAGER_CLASS': 'tests.test_crawler.CustomSpiderLoader'\n-            })\n-            self.assertIsInstance(runner.spider_loader, CustomSpiderLoader)\n-            is_one_warning = len(w) == 1\n-            if not is_one_warning:\n-                for warning in w:\n-                    print(warning)\n-            self.assertIn('Please use SPIDER_LOADER_CLASS', str(w[0].message))\n-            self.assertTrue(is_one_warning)\n-\n-    def test_crawl_rejects_spider_objects(self):\n-        with raises(ValueError):\n-            CrawlerRunner().crawl(DefaultSpider())\n-\n-    def test_create_crawler_rejects_spider_objects(self):\n-        with raises(ValueError):\n-            CrawlerRunner().create_crawler(DefaultSpider())\n-\n \n class CrawlerProcessTest(BaseCrawlerTest):\n     def test_crawler_process_accepts_dict(self):\n\n@@ -24,7 +24,7 @@ from w3lib.url import path_to_file_uri\n from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.handlers.datauri import DataURIDownloadHandler\n from scrapy.core.downloader.handlers.file import FileDownloadHandler\n-from scrapy.core.downloader.handlers.http import HTTPDownloadHandler, HttpDownloadHandler\n+from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.core.downloader.handlers.http10 import HTTP10DownloadHandler\n from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n@@ -360,11 +360,6 @@ class HttpTestCase(unittest.TestCase):\n         return d\n \n \n-class DeprecatedHttpTestCase(HttpTestCase):\n-    \"\"\"HTTP 1.0 test case\"\"\"\n-    download_handler_cls = HttpDownloadHandler\n-\n-\n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n     download_handler_cls = HTTP10DownloadHandler\n@@ -656,11 +651,6 @@ class HttpProxyTestCase(unittest.TestCase):\n         return self.download_request(request, Spider('foo')).addCallback(_test)\n \n \n-class DeprecatedHttpProxyTestCase(unittest.TestCase):\n-    \"\"\"Old deprecated reference to http10 downloader handler\"\"\"\n-    download_handler_cls = HttpDownloadHandler\n-\n-\n class Http10ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls = HTTP10DownloadHandler\n \n\n@@ -108,44 +108,6 @@ class FilesPipelineTestCase(unittest.TestCase):\n             p.stop()\n \n \n-class DeprecatedFilesPipeline(FilesPipeline):\n-    def file_key(self, url):\n-        media_guid = hashlib.sha1(to_bytes(url)).hexdigest()\n-        media_ext = os.path.splitext(url)[1]\n-        return 'empty/%s%s' % (media_guid, media_ext)\n-\n-\n-class DeprecatedFilesPipelineTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.tempdir = mkdtemp()\n-\n-    def init_pipeline(self, pipeline_class):\n-        self.pipeline = pipeline_class.from_settings(Settings({'FILES_STORE': self.tempdir}))\n-        self.pipeline.download_func = _mocked_download_func\n-        self.pipeline.open_spider(None)\n-\n-    def test_default_file_key_method(self):\n-        self.init_pipeline(FilesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.file_key(\"https://dev.mydeco.com/mydeco.pdf\"),\n-                             'full/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('file_key(url) method is deprecated' in str(w[-1].message))\n-\n-    def test_overridden_file_key_method(self):\n-        self.init_pipeline(DeprecatedFilesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.file_path(Request(\"https://dev.mydeco.com/mydeco.pdf\")),\n-                             'empty/c9b564df929f4bc635bdd19fde4f3d4847c757c5.pdf')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('file_key(url) method is deprecated' in str(w[-1].message))\n-\n-    def tearDown(self):\n-        rmtree(self.tempdir)\n-\n-\n class FilesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n\n@@ -118,63 +118,6 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n         return 'thumbsup/%s/%s.jpg' % (thumb_id, thumb_guid)\n \n \n-class DeprecatedImagesPipelineTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.tempdir = mkdtemp()\n-\n-    def init_pipeline(self, pipeline_class):\n-        self.pipeline = pipeline_class(self.tempdir, download_func=_mocked_download_func)\n-        self.pipeline.open_spider(None)\n-\n-    def test_default_file_key_method(self):\n-        self.init_pipeline(ImagesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.file_key(\"https://dev.mydeco.com/mydeco.gif\"),\n-                             'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('image_key(url) and file_key(url) methods are deprecated' in str(w[-1].message))\n-\n-    def test_default_image_key_method(self):\n-        self.init_pipeline(ImagesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.image_key(\"https://dev.mydeco.com/mydeco.gif\"),\n-                             'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('image_key(url) and file_key(url) methods are deprecated' in str(w[-1].message))\n-\n-    def test_overridden_file_key_method(self):\n-        self.init_pipeline(DeprecatedImagesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.file_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n-                             'empty/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('image_key(url) and file_key(url) methods are deprecated' in str(w[-1].message))\n-\n-    def test_default_thumb_key_method(self):\n-        self.init_pipeline(ImagesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.thumb_key(\"file:///tmp/foo.jpg\", 50),\n-                             'thumbs/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('thumb_key(url) method is deprecated' in str(w[-1].message))\n-\n-    def test_overridden_thumb_key_method(self):\n-        self.init_pipeline(DeprecatedImagesPipeline)\n-        with warnings.catch_warnings(record=True) as w:\n-            warnings.simplefilter('always')\n-            self.assertEqual(self.pipeline.thumb_path(Request(\"file:///tmp/foo.jpg\"), 50),\n-                             'thumbsup/50/38a86208c36e59d4404db9e37ce04be863ef0335.jpg')\n-            self.assertEqual(len(w), 1)\n-            self.assertTrue('thumb_key(url) method is deprecated' in str(w[-1].message))\n-\n-    def tearDown(self):\n-        rmtree(self.tempdir)\n-\n-\n class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n     def test_item_fields_default(self):\n\n@@ -3,7 +3,6 @@ import weakref\n from twisted.trial import unittest\n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.selector import Selector\n-from scrapy.selector.lxmlsel import XmlXPathSelector, HtmlXPathSelector, XPathSelector\n from lxml import etree\n \n \n@@ -40,22 +39,6 @@ class SelectorTestCase(unittest.TestCase):\n         sel = Selector(response)\n         self.assertEqual(url, sel.root.base)\n \n-    def test_deprecated_root_argument(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            root = etree.fromstring(u'<html/>')\n-            sel = Selector(_root=root)\n-            self.assertIs(root, sel.root)\n-            self.assertEqual(str(w[-1].message),\n-                             'Argument `_root` is deprecated, use `root` instead')\n-\n-    def test_deprecated_root_argument_ambiguous(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            _root = etree.fromstring(u'<xml/>')\n-            root = etree.fromstring(u'<html/>')\n-            sel = Selector(_root=_root, root=root)\n-            self.assertIs(root, sel.root)\n-            self.assertIn('Ignoring deprecated `_root` argument', str(w[-1].message))\n-\n     def test_flavor_detection(self):\n         text = b'<div><img src=\"a.jpg\"><p>Hello</div>'\n         sel = Selector(XmlResponse('http://example.com', body=text, encoding='utf-8'))\n@@ -101,111 +84,6 @@ class SelectorTestCase(unittest.TestCase):\n         assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n             x.__class__.__name__\n \n-    def test_deprecated_selector_methods(self):\n-        sel = Selector(TextResponse(url=\"http://example.com\", body=b'<p>some text</p>'))\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            sel.select('//p')\n-            self.assertSubstring('Use .xpath() instead', str(w[-1].message))\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            sel.extract_unquoted()\n-            self.assertSubstring('Use .extract() instead', str(w[-1].message))\n-\n-    def test_deprecated_selectorlist_methods(self):\n-        sel = Selector(TextResponse(url=\"http://example.com\", body=b'<p>some text</p>'))\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            sel.xpath('//p').select('.')\n-            self.assertSubstring('Use .xpath() instead', str(w[-1].message))\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            sel.xpath('//p').extract_unquoted()\n-            self.assertSubstring('Use .extract() instead', str(w[-1].message))\n-\n     def test_selector_bad_args(self):\n         with self.assertRaisesRegexp(ValueError, 'received both response and text'):\n             Selector(TextResponse(url='http://example.com', body=b''), text=u'')\n-\n-\n-class DeprecatedXpathSelectorTest(unittest.TestCase):\n-\n-    text = '<div><img src=\"a.jpg\"><p>Hello</div>'\n-\n-    def test_warnings_xpathselector(self):\n-        cls = XPathSelector\n-        with warnings.catch_warnings(record=True) as w:\n-            class UserClass(cls):\n-                pass\n-\n-            # subclassing must issue a warning\n-            self.assertEqual(len(w), 1, str(cls))\n-            self.assertIn('scrapy.Selector', str(w[0].message))\n-\n-            # subclass instance doesn't issue a warning\n-            usel = UserClass(text=self.text)\n-            self.assertEqual(len(w), 1)\n-\n-            # class instance must issue a warning\n-            sel = cls(text=self.text)\n-            self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n-            self.assertIn('scrapy.Selector', str(w[1].message))\n-\n-            # subclass and instance checks\n-            self.assertTrue(issubclass(cls, Selector))\n-            self.assertTrue(isinstance(sel, Selector))\n-            self.assertTrue(isinstance(usel, Selector))\n-\n-    def test_warnings_xmlxpathselector(self):\n-        cls = XmlXPathSelector\n-        with warnings.catch_warnings(record=True) as w:\n-            class UserClass(cls):\n-                pass\n-\n-            # subclassing must issue a warning\n-            self.assertEqual(len(w), 1, str(cls))\n-            self.assertIn('scrapy.Selector', str(w[0].message))\n-\n-            # subclass instance doesn't issue a warning\n-            usel = UserClass(text=self.text)\n-            self.assertEqual(len(w), 1)\n-\n-            # class instance must issue a warning\n-            sel = cls(text=self.text)\n-            self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n-            self.assertIn('scrapy.Selector', str(w[1].message))\n-\n-            # subclass and instance checks\n-            self.assertTrue(issubclass(cls, Selector))\n-            self.assertTrue(issubclass(cls, XPathSelector))\n-            self.assertTrue(isinstance(sel, Selector))\n-            self.assertTrue(isinstance(usel, Selector))\n-            self.assertTrue(isinstance(sel, XPathSelector))\n-            self.assertTrue(isinstance(usel, XPathSelector))\n-\n-    def test_warnings_htmlxpathselector(self):\n-        cls = HtmlXPathSelector\n-        with warnings.catch_warnings(record=True) as w:\n-            class UserClass(cls):\n-                pass\n-\n-            # subclassing must issue a warning\n-            self.assertEqual(len(w), 1, str(cls))\n-            self.assertIn('scrapy.Selector', str(w[0].message))\n-\n-            # subclass instance doesn't issue a warning\n-            usel = UserClass(text=self.text)\n-            self.assertEqual(len(w), 1)\n-\n-            # class instance must issue a warning\n-            sel = cls(text=self.text)\n-            self.assertEqual(len(w), 2, str((cls, [x.message for x in w])))\n-            self.assertIn('scrapy.Selector', str(w[1].message))\n-\n-            # subclass and instance checks\n-            self.assertTrue(issubclass(cls, Selector))\n-            self.assertTrue(issubclass(cls, XPathSelector))\n-            self.assertTrue(isinstance(sel, Selector))\n-            self.assertTrue(isinstance(usel, Selector))\n-            self.assertTrue(isinstance(sel, XPathSelector))\n-            self.assertTrue(isinstance(usel, XPathSelector))\n\n@@ -1,22 +0,0 @@\n-\"\"\"\n-Selector tests for cssselect backend\n-\"\"\"\n-import warnings\n-from twisted.trial import unittest\n-from scrapy.selector.csstranslator import (\n-    ScrapyHTMLTranslator,\n-    ScrapyGenericTranslator,\n-    ScrapyXPathExpr\n-)\n-\n-\n-class DeprecatedClassesTest(unittest.TestCase):\n-\n-    def test_deprecated_warnings(self):\n-        for cls in [ScrapyHTMLTranslator, ScrapyGenericTranslator, ScrapyXPathExpr]:\n-            with warnings.catch_warnings(record=True) as w:\n-                obj = cls()\n-                self.assertIn('%s is deprecated' % cls.__name__, str(w[-1].message),\n-                              'Missing deprecate warning for %s' % cls.__name__)\n-\n-\n\n@@ -10,7 +10,7 @@ from scrapy import signals\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.spiders.init import InitSpider\n-from scrapy.spiders import Spider, BaseSpider, CrawlSpider, Rule, XMLFeedSpider, \\\n+from scrapy.spiders import Spider, CrawlSpider, Rule, XMLFeedSpider, \\\n     CSVFeedSpider, SitemapSpider\n from scrapy.linkextractors import LinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -51,17 +51,6 @@ class SpiderTest(unittest.TestCase):\n         self.assertRaises(ValueError, self.spider_class)\n         self.assertRaises(ValueError, self.spider_class, somearg='foo')\n \n-    def test_deprecated_set_crawler_method(self):\n-        spider = self.spider_class('example.com')\n-        crawler = get_crawler()\n-        with warnings.catch_warnings(record=True) as w:\n-            spider.set_crawler(crawler)\n-            self.assertIn(\"set_crawler\", str(w[0].message))\n-            self.assertTrue(hasattr(spider, 'crawler'))\n-            self.assertIs(spider.crawler, crawler)\n-            self.assertTrue(hasattr(spider, 'settings'))\n-            self.assertIs(spider.settings, crawler.settings)\n-\n     def test_from_crawler_crawler_and_settings_population(self):\n         crawler = get_crawler()\n         spider = self.spider_class.from_crawler(crawler, 'example.com')\n@@ -377,20 +366,6 @@ class CrawlSpiderTest(SpiderTest):\n         self.assertTrue(hasattr(spider, '_follow_links'))\n         self.assertFalse(spider._follow_links)\n \n-    def test_follow_links_attribute_deprecated_population(self):\n-        spider = self.spider_class('example.com')\n-        self.assertFalse(hasattr(spider, '_follow_links'))\n-\n-        spider.set_crawler(get_crawler())\n-        self.assertTrue(hasattr(spider, '_follow_links'))\n-        self.assertTrue(spider._follow_links)\n-\n-        spider = self.spider_class('example.com')\n-        settings_dict = {'CRAWLSPIDER_FOLLOW_LINKS': False}\n-        spider.set_crawler(get_crawler(settings_dict=settings_dict))\n-        self.assertTrue(hasattr(spider, '_follow_links'))\n-        self.assertFalse(spider._follow_links)\n-\n \n class SitemapSpiderTest(SpiderTest):\n \n@@ -578,57 +553,9 @@ Sitemap: /sitemap-relative-url.xml\n \n class DeprecationTest(unittest.TestCase):\n \n-    def test_basespider_is_deprecated(self):\n-        with warnings.catch_warnings(record=True) as w:\n-\n-            class MySpider1(BaseSpider):\n-                pass\n-\n-            self.assertEqual(len(w), 1)\n-            self.assertEqual(w[0].category, ScrapyDeprecationWarning)\n-            self.assertEqual(w[0].lineno, inspect.getsourcelines(MySpider1)[1])\n-\n-    def test_basespider_issubclass(self):\n-        class MySpider2(Spider):\n-            pass\n-\n-        class MySpider2a(MySpider2):\n-            pass\n-\n-        class Foo(object):\n-            pass\n-\n-        class Foo2(object_ref):\n-            pass\n-\n-        assert issubclass(MySpider2, BaseSpider)\n-        assert issubclass(MySpider2a, BaseSpider)\n-        assert not issubclass(Foo, BaseSpider)\n-        assert not issubclass(Foo2, BaseSpider)\n-\n-    def test_basespider_isinstance(self):\n-        class MySpider3(Spider):\n-            name = 'myspider3'\n-\n-        class MySpider3a(MySpider3):\n-            pass\n-\n-        class Foo(object):\n-            pass\n-\n-        class Foo2(object_ref):\n-            pass\n-\n-        assert isinstance(MySpider3(), BaseSpider)\n-        assert isinstance(MySpider3a(), BaseSpider)\n-        assert not isinstance(Foo(), BaseSpider)\n-        assert not isinstance(Foo2(), BaseSpider)\n-\n     def test_crawl_spider(self):\n         assert issubclass(CrawlSpider, Spider)\n-        assert issubclass(CrawlSpider, BaseSpider)\n         assert isinstance(CrawlSpider(name='foo'), Spider)\n-        assert isinstance(CrawlSpider(name='foo'), BaseSpider)\n \n     def test_make_requests_from_url_deprecated(self):\n         class MySpider4(Spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
