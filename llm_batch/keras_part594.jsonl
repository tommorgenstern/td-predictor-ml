{"custom_id": "keras#bffb93adc6480038557489ad3771835fce58a357", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 30/30 | Churn Δ: 136 | Churn Cumulative: 262 | Contributors (this commit): 9 | Commits (past 90d): 59 | Contributors (cumulative): 10 | DMM Complexity: 0.6555555555555556\n\nDIFF:\n@@ -94,3 +94,4 @@ from keras_core.layers.reshaping.repeat_vector import RepeatVector\n from keras_core.layers.reshaping.reshape import Reshape\n from keras_core.layers.reshaping.up_sampling1d import UpSampling1D\n from keras_core.layers.rnn.rnn import RNN\n+from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n\n@@ -0,0 +1,135 @@\n+from tensorflow import nest\n+\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.layers.StackedRNNCells\")\n+class StackedRNNCells(Layer):\n+    \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n+\n+    Used to implement efficient stacked RNNs.\n+\n+    Args:\n+      cells: List of RNN cell instances.\n+\n+    Examples:\n+\n+    ```python\n+    batch_size = 3\n+    sentence_length = 5\n+    num_features = 2\n+    new_shape = (batch_size, sentence_length, num_features)\n+    x = np.reshape(np.arange(30), new_shape)\n+\n+    rnn_cells = [keras_core.layers.LSTMCell(128) for _ in range(2)]\n+    stacked_lstm = keras_core.layers.StackedRNNCells(rnn_cells)\n+    lstm_layer = keras_core.layers.RNN(stacked_lstm)\n+\n+    result = lstm_layer(x)\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, cells, **kwargs):\n+        super().__init__(**kwargs)\n+        for cell in cells:\n+            if \"call\" not in dir(cell):\n+                raise ValueError(\n+                    \"All cells must have a `call` method. \"\n+                    f\"Received cell without a `call` method: {cell}\"\n+                )\n+            if \"state_size\" not in dir(cell):\n+                raise ValueError(\n+                    \"All cells must have a `state_size` attribute. \"\n+                    f\"Received cell without a `state_size`: {cell}\"\n+                )\n+        self.cells = cells\n+\n+    @property\n+    def state_size(self):\n+        return [c.state_size for c in self.cells]\n+\n+    @property\n+    def output_size(self):\n+        if getattr(self.cells[-1], \"output_size\", None) is not None:\n+            return self.cells[-1].output_size\n+        elif isinstance(self.cells[-1].state_size, (list, tuple)):\n+            return self.cells[-1].state_size[0]\n+        else:\n+            return self.cells[-1].state_size\n+\n+    def get_initial_state(self, batch_size=None):\n+        initial_states = []\n+        for cell in self.cells:\n+            get_initial_state_fn = getattr(cell, \"get_initial_state\", None)\n+            if get_initial_state_fn:\n+                initial_states.append(\n+                    get_initial_state_fn(batch_size=batch_size)\n+                )\n+            else:\n+                if isinstance(cell.state_size, int):\n+                    state_size = [cell.state_size]\n+                else:\n+                    state_size = cell.state_size\n+                initial_states.append(\n+                    [\n+                        ops.zeros((batch_size, d), dtype=self.compute_dtype)\n+                        for d in state_size\n+                    ]\n+                )\n+        return initial_states\n+\n+    def call(self, inputs, states, training=False, **kwargs):\n+        # Call the cells in order and store the returned states.\n+        new_states = []\n+        for cell, states in zip(self.cells, states):\n+            states = list(states) if nest.is_nested(states) else [states]\n+            if isinstance(cell, Layer) and cell._call_has_training_arg():\n+                kwargs[\"training\"] = training\n+            else:\n+                kwargs.pop(\"training\", None)\n+            cell_call_fn = cell.__call__ if callable(cell) else cell.call\n+            print(\"call\", cell.__class__, \"with\", states)\n+            inputs, states = cell_call_fn(inputs, states, **kwargs)\n+            new_states.append(states)\n+        return inputs, new_states\n+\n+    def build(self, input_shape):\n+        print(\"build called\")\n+        for cell in self.cells:\n+            if isinstance(cell, Layer) and not cell.built:\n+                print(\"build cell\", cell)\n+                cell.build(input_shape)\n+                cell.built = True\n+                print(cell, len(cell.trainable_weights))\n+            if getattr(cell, \"output_size\", None) is not None:\n+                output_dim = cell.output_size\n+            elif isinstance(cell.state_size, (list, tuple)):\n+                output_dim = cell.state_size[0]\n+            else:\n+                output_dim = cell.state_size\n+            batch_size = nest.flatten(input_shape)[0]\n+            input_shape = (batch_size, output_dim)\n+        self.built = True\n+        print(self, len(self.trainable_weights))\n+\n+    def get_config(self):\n+        cells = []\n+        for cell in self.cells:\n+            cells.append(serialization_lib.serialize_keras_object(cell))\n+        config = {\"cells\": cells}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        cells = []\n+        for cell_config in config.pop(\"cells\"):\n+            cells.append(\n+                serialization_lib.deserialize_keras_object(\n+                    cell_config, custom_objects=custom_objects\n+                )\n+            )\n+        return cls(cells, **config)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eed9c18fff59ddc38e62fb71733db53345e84b7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 122 | Churn Cumulative: 491 | Contributors (this commit): 9 | Commits (past 90d): 65 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,6 +73,7 @@ from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.random_brightness import RandomBrightness\n from keras_core.layers.preprocessing.random_contrast import RandomContrast\n from keras_core.layers.preprocessing.random_crop import RandomCrop\n+from keras_core.layers.preprocessing.random_flip import RandomFlip\n from keras_core.layers.preprocessing.random_translation import RandomTranslation\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n\n@@ -0,0 +1,68 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+HORIZONTAL = \"horizontal\"\n+VERTICAL = \"vertical\"\n+HORIZONTAL_AND_VERTICAL = \"horizontal_and_vertical\"\n+\n+\n+@keras_core_export(\"keras_core.layers.RandomFlip\")\n+class RandomFlip(Layer):\n+    \"\"\"A preprocessing layer which randomly flips images during training.\n+\n+    This layer will flip the images horizontally and or vertically based on the\n+    `mode` attribute. During inference time, the output will be identical to\n+    input. Call the layer with `training=True` to flip the input.\n+    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n+\n+    Input shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format.\n+\n+    Output shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format.\n+\n+    Args:\n+        mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n+            `\"vertical\"`, or `\"horizontal_and_vertical\"`. `\"horizontal\"` is a\n+            left-right flip and `\"vertical\"` is a top-bottom flip. Defaults to\n+            `\"horizontal_and_vertical\"`\n+        seed: Integer. Used to create a random seed.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, mode=HORIZONTAL_AND_VERTICAL, seed=None, name=None, **kwargs\n+    ):\n+        super().__init__(name=name)\n+        self.seed = seed or backend.random.make_default_seed()\n+        self.layer = tf.keras.layers.RandomFlip(\n+            mode=mode,\n+            name=name,\n+            seed=self.seed,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs, training=True):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n+\n+    def get_config(self):\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n+        return config\n\n@@ -0,0 +1,53 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core import utils\n+\n+\n+class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"random_flip_horizontal\", \"horizontal\"),\n+        (\"random_flip_vertical\", \"vertical\"),\n+        (\"random_flip_both\", \"horizontal_and_vertical\"),\n+    )\n+    def test_random_flip(self, mode):\n+        self.run_layer_test(\n+            layers.RandomFlip,\n+            init_kwargs={\n+                \"mode\": mode,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 4),\n+            supports_masking=False,\n+        )\n+\n+    def test_random_flip_horizontal(self):\n+        utils.set_random_seed(0)\n+        self.run_layer_test(\n+            layers.RandomFlip,\n+            init_kwargs={\n+                \"mode\": \"horizontal\",\n+                \"seed\": 42,\n+            },\n+            input_data=np.asarray([[[2, 3, 4], [5, 6, 7]]]),\n+            expected_output=backend.convert_to_tensor([[[5, 6, 7], [2, 3, 4]]]),\n+            supports_masking=False,\n+        )\n+\n+    def test_random_flip_vertical(self):\n+        utils.set_random_seed(0)\n+        self.run_layer_test(\n+            layers.RandomFlip,\n+            init_kwargs={\n+                \"mode\": \"vertical\",\n+                \"seed\": 42,\n+            },\n+            input_data=np.asarray([[[2, 3, 4]], [[5, 6, 7]]]),\n+            expected_output=backend.convert_to_tensor(\n+                [[[5, 6, 7]], [[2, 3, 4]]]\n+            ),\n+            supports_masking=False,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba07ca4f889b008351423960358660663d1c8dd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 200 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 209 | Churn Cumulative: 1756 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -384,7 +384,6 @@ class RNN(Layer):\n             zero_output_for_mask=self.zero_output_for_mask,\n             return_all_outputs=self.return_sequences,\n         )\n-        assert isinstance(states, (list, tuple))\n \n         if self.stateful:\n             for self_state, state in zip(\n\n@@ -70,13 +70,17 @@ class StackedRNNCells(Layer):\n                 )\n             else:\n                 if isinstance(cell.state_size, int):\n-                    state_size = [cell.state_size]\n+                    initial_states.append(\n+                        ops.zeros(\n+                            (batch_size, cell.state_size),\n+                            dtype=self.compute_dtype,\n+                        )\n+                    )\n                 else:\n-                    state_size = cell.state_size\n                     initial_states.append(\n                         [\n                             ops.zeros((batch_size, d), dtype=self.compute_dtype)\n-                        for d in state_size\n+                            for d in cell.state_size\n                         ]\n                     )\n         return initial_states\n@@ -91,19 +95,20 @@ class StackedRNNCells(Layer):\n             else:\n                 kwargs.pop(\"training\", None)\n             cell_call_fn = cell.__call__ if callable(cell) else cell.call\n-            print(\"call\", cell.__class__, \"with\", states)\n             inputs, states = cell_call_fn(inputs, states, **kwargs)\n+            if len(states) == 1:\n+                states = states[0]\n             new_states.append(states)\n+\n+        if len(new_states) == 1:\n+            new_states = new_states[0]\n         return inputs, new_states\n \n     def build(self, input_shape):\n-        print(\"build called\")\n         for cell in self.cells:\n             if isinstance(cell, Layer) and not cell.built:\n-                print(\"build cell\", cell)\n                 cell.build(input_shape)\n                 cell.built = True\n-                print(cell, len(cell.trainable_weights))\n             if getattr(cell, \"output_size\", None) is not None:\n                 output_dim = cell.output_size\n             elif isinstance(cell.state_size, (list, tuple)):\n@@ -113,7 +118,6 @@ class StackedRNNCells(Layer):\n             batch_size = nest.flatten(input_shape)[0]\n             input_shape = (batch_size, output_dim)\n         self.built = True\n-        print(self, len(self.trainable_weights))\n \n     def get_config(self):\n         cells = []\n\n@@ -0,0 +1,188 @@\n+import numpy as np\n+import pytest\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core.layers.rnn.rnn_test import OneStateRNNCell\n+from keras_core.layers.rnn.rnn_test import TwoStatesRNNCell\n+\n+\n+@pytest.mark.skipif(\n+    backend.backend() != \"tensorflow\",\n+    reason=\"Only implemented for TF for now.\",\n+)\n+class StackedRNNTest(testing.TestCase):\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\n+                \"cell\": [\n+                    OneStateRNNCell(3),\n+                    OneStateRNNCell(4),\n+                    OneStateRNNCell(5),\n+                ],\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 5),\n+            expected_num_trainable_weights=6,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+            custom_objects={\"OneStateRNNCell\": OneStateRNNCell},\n+        )\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\n+                \"cell\": [\n+                    OneStateRNNCell(3),\n+                    OneStateRNNCell(4),\n+                    OneStateRNNCell(5),\n+                ],\n+                \"return_sequences\": True,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 5),\n+            expected_num_trainable_weights=6,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+            custom_objects={\"OneStateRNNCell\": OneStateRNNCell},\n+        )\n+        # Two-state case.\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\n+                \"cell\": [\n+                    TwoStatesRNNCell(3),\n+                    TwoStatesRNNCell(4),\n+                    TwoStatesRNNCell(5),\n+                ],\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 5),\n+            expected_num_trainable_weights=9,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+            custom_objects={\"TwoStatesRNNCell\": TwoStatesRNNCell},\n+        )\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\n+                \"cell\": [\n+                    TwoStatesRNNCell(3),\n+                    TwoStatesRNNCell(4),\n+                    TwoStatesRNNCell(5),\n+                ],\n+                \"return_sequences\": True,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 5),\n+            expected_num_trainable_weights=9,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+            custom_objects={\"TwoStatesRNNCell\": TwoStatesRNNCell},\n+        )\n+\n+    def test_correctness_single_state_stack(self):\n+        sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n+        layer = layers.RNN([OneStateRNNCell(3), OneStateRNNCell(2)])\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[786.0, 786.0], [4386.0, 4386.0]]), output\n+        )\n+\n+        layer = layers.RNN(\n+            [OneStateRNNCell(3), OneStateRNNCell(2)], return_sequences=True\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [[18.0, 18.0], [156.0, 156.0], [786.0, 786.0]],\n+                    [[162.0, 162.0], [1020.0, 1020.0], [4386.0, 4386.0]],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+        layer = layers.RNN(\n+            [OneStateRNNCell(3), OneStateRNNCell(2)], return_state=True\n+        )\n+        output, state_1, state_2 = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[786.0, 786.0], [4386.0, 4386.0]]), output\n+        )\n+        self.assertAllClose(\n+            np.array([[158.0, 158.0, 158.0], [782.0, 782.0, 782.0]]), state_1\n+        )\n+        self.assertAllClose(\n+            np.array([[786.0, 786.0], [4386.0, 4386.0]]), state_2\n+        )\n+\n+        layer = layers.RNN(\n+            [OneStateRNNCell(3), OneStateRNNCell(2)],\n+            return_sequences=True,\n+            return_state=True,\n+        )\n+        output, state_1, state_2 = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [[18.0, 18.0], [156.0, 156.0], [786.0, 786.0]],\n+                    [[162.0, 162.0], [1020.0, 1020.0], [4386.0, 4386.0]],\n+                ]\n+            ),\n+            output,\n+        )\n+        self.assertAllClose(\n+            np.array([[158.0, 158.0, 158.0], [782.0, 782.0, 782.0]]), state_1\n+        )\n+        self.assertAllClose(\n+            np.array([[786.0, 786.0], [4386.0, 4386.0]]), state_2\n+        )\n+\n+    def test_correctness_two_states_stack(self):\n+        sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n+        layer = layers.RNN([TwoStatesRNNCell(3), TwoStatesRNNCell(2)])\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[3144.0, 3144.0], [17544.0, 17544.0]]), output\n+        )\n+\n+        layer = layers.RNN(\n+            [TwoStatesRNNCell(3), TwoStatesRNNCell(2)], return_sequences=True\n+        )\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array(\n+                [\n+                    [[72.0, 72.0], [624.0, 624.0], [3144.0, 3144.0]],\n+                    [[648.0, 648.0], [4080.0, 4080.0], [17544.0, 17544.0]],\n+                ]\n+            ),\n+            output,\n+        )\n+\n+        layer = layers.RNN(\n+            [TwoStatesRNNCell(3), TwoStatesRNNCell(2)], return_state=True\n+        )\n+        output, state_1, state_2 = layer(sequence)\n+\n+        self.assertAllClose(\n+            np.array([[3144.0, 3144.0], [17544.0, 17544.0]]), output\n+        )\n+        self.assertAllClose(\n+            np.array([[158.0, 158.0, 158.0], [782.0, 782.0, 782.0]]), state_1[0]\n+        )\n+        self.assertAllClose(\n+            np.array([[158.0, 158.0, 158.0], [782.0, 782.0, 782.0]]), state_1[1]\n+        )\n+        self.assertAllClose(\n+            np.array([[1572.0, 1572.0], [8772.0, 8772.0]]), state_2[0]\n+        )\n+        self.assertAllClose(\n+            np.array([[1572.0, 1572.0], [8772.0, 8772.0]]), state_2[1]\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb96cd5ad1f452ce523ed1f50e4c519f3473af2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 33 | Churn Cumulative: 1634 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -288,10 +288,12 @@ class RNN(Layer):\n \n     @tracking.no_automatic_dependency_tracking\n     def _create_state_variables(self, batch_size):\n-        self.states = [\n-            backend.Variable(value, trainable=False, dtype=self.variable_dtype)\n-            for value in self.get_initial_state(batch_size)\n-        ]\n+        self.states = nest.map_structure(\n+            lambda value: backend.Variable(\n+                value, trainable=False, dtype=self.variable_dtype\n+            ),\n+            self.get_initial_state(batch_size),\n+        )\n \n     def get_initial_state(self, batch_size):\n         get_initial_state_fn = getattr(self.cell, \"get_initial_state\", None)\n@@ -354,7 +356,6 @@ class RNN(Layer):\n                 initial_state = self.get_initial_state(\n                     batch_size=ops.shape(sequence)[0]\n                 )\n-        else:\n         # RNN expect the states in a list, even if single state.\n         if not nest.is_nested(initial_state):\n             initial_state = [initial_state]\n\n@@ -186,3 +186,25 @@ class StackedRNNTest(testing.TestCase):\n         self.assertAllClose(\n             np.array([[1572.0, 1572.0], [8772.0, 8772.0]]), state_2[1]\n         )\n+\n+    def test_statefullness_single_state_stack(self):\n+        sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n+        layer = layers.RNN(\n+            [OneStateRNNCell(3), OneStateRNNCell(2)], stateful=True\n+        )\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[34092.0, 34092.0], [173196.0, 173196.0]]), output\n+        )\n+\n+    def test_statefullness_two_states_stack(self):\n+        sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n+        layer = layers.RNN(\n+            [TwoStatesRNNCell(3), TwoStatesRNNCell(2)], stateful=True\n+        )\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[136368.0, 136368.0], [692784.0, 692784.0]]), output\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b299f226cb659d0ee05e8a501e8cf27be8daf31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 317 | Lines Deleted: 5 | Files Changed: 6 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 42/37 | Churn Δ: 322 | Churn Cumulative: 934 | Contributors (this commit): 9 | Commits (past 90d): 73 | Contributors (cumulative): 18 | DMM Complexity: 0.24875621890547264\n\nDIFF:\n@@ -89,6 +89,7 @@ from keras_core.layers.regularization.spatial_dropout import SpatialDropout1D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout2D\n from keras_core.layers.regularization.spatial_dropout import SpatialDropout3D\n from keras_core.layers.reshaping.cropping1d import Cropping1D\n+from keras_core.layers.reshaping.cropping2d import Cropping2D\n from keras_core.layers.reshaping.flatten import Flatten\n from keras_core.layers.reshaping.permute import Permute\n from keras_core.layers.reshaping.repeat_vector import RepeatVector\n\n@@ -13,7 +13,7 @@ class Cropping1D(Layer):\n \n     >>> input_shape = (2, 3, 2)\n     >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> print(x)\n+    >>> x\n     [[[ 0  1]\n       [ 2  3]\n       [ 4  5]]\n@@ -21,7 +21,7 @@ class Cropping1D(Layer):\n       [ 8  9]\n       [10 11]]]\n     >>> y = keras_core.layers.Cropping1D(cropping=1)(x)\n-    >>> print(y)\n+    >>> y\n     [[[2 3]]\n      [[8 9]]]\n \n\n@@ -58,5 +58,5 @@ class CroppingTest(testing.TestCase):\n \n     def test_cropping_1d_errors_if_cropping_more_than_available(self):\n         with self.assertRaises(ValueError):\n-            input_layer = layers.Input(shape=(3, 4, 7))\n+            input_layer = layers.Input(batch_shape=(3, 5, 7))\n             layers.Cropping1D(cropping=(2, 3))(input_layer)\n\n@@ -0,0 +1,212 @@\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+\n+\n+@keras_core_export(\"keras_core.layers.Cropping2D\")\n+class Cropping2D(Layer):\n+    \"\"\"Cropping layer for 2D input (e.g. picture).\n+\n+    It crops along spatial dimensions, i.e. height and width.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 28, 28, 3)\n+    >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n+    >>> y = keras_core.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)\n+    >>> y.shape\n+    (2, 24, 20, 3)\n+\n+    Args:\n+        cropping: Int, or tuple of 2 ints, or tuple of 2 tuples of 2 ints.\n+            - If int: the same symmetric cropping is applied to height and\n+              width.\n+            - If tuple of 2 ints: interpreted as two different symmetric\n+              cropping values for height and width:\n+              `(symmetric_height_crop, symmetric_width_crop)`.\n+            - If tuple of 2 tuples of 2 ints: interpreted as\n+              `((top_crop, bottom_crop), (left_crop, right_crop))`\n+        data_format: A string, one of `\"channels_last\"` (default) or\n+            `\"channels_first\"`. The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch_size, height, width, channels)` while `\"channels_first\"`\n+            corresponds to inputs with shape\n+            `(batch_size, channels, height, width)`.\n+            When unspecified, uses `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json` (if exists). Defaults to\n+            `\"channels_last\"`.\n+\n+    Input shape:\n+        4D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, height, width, channels)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, channels, height, width)`\n+\n+    Output shape:\n+        4D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, cropped_height, cropped_width, channels)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, channels, cropped_height, cropped_width)`\n+    \"\"\"\n+\n+    def __init__(\n+        self, cropping=((0, 0), (0, 0)), data_format=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.data_format = backend.standardize_data_format(data_format)\n+        if isinstance(cropping, int):\n+            self.cropping = ((cropping, cropping), (cropping, cropping))\n+        elif hasattr(cropping, \"__len__\"):\n+            if len(cropping) != 2:\n+                raise ValueError(\n+                    \"`cropping` should have two elements. \"\n+                    f\"Received: cropping={cropping}.\"\n+                )\n+            height_cropping = cropping[0]\n+            if isinstance(height_cropping, int):\n+                height_cropping = (height_cropping, height_cropping)\n+            width_cropping = cropping[1]\n+            if isinstance(width_cropping, int):\n+                width_cropping = (width_cropping, width_cropping)\n+            self.cropping = (height_cropping, width_cropping)\n+        else:\n+            raise ValueError(\n+                \"`cropping` should be either an int, a tuple of 2 ints \"\n+                \"(symmetric_height_crop, symmetric_width_crop), \"\n+                \"or a tuple of 2 tuples of 2 ints \"\n+                \"((top_crop, bottom_crop), (left_crop, right_crop)). \"\n+                f\"Received: cropping={cropping}.\"\n+            )\n+        self.input_spec = InputSpec(ndim=4)\n+\n+    def compute_output_shape(self, input_shape):\n+        if self.data_format == \"channels_first\":\n+            if (\n+                input_shape[2] is not None\n+                and sum(self.cropping[0]) >= input_shape[2]\n+            ) or (\n+                input_shape[3] is not None\n+                and sum(self.cropping[1]) >= input_shape[3]\n+            ):\n+                raise ValueError(\n+                    \"Values in `cropping` argument should be greater than the \"\n+                    \"corresponding spatial dimension of the input. Received: \"\n+                    f\"input_shape={input_shape}, cropping={self.cropping}\"\n+                )\n+            return (\n+                input_shape[0],\n+                input_shape[1],\n+                input_shape[2] - self.cropping[0][0] - self.cropping[0][1]\n+                if input_shape[2] is not None\n+                else None,\n+                input_shape[3] - self.cropping[1][0] - self.cropping[1][1]\n+                if input_shape[3] is not None\n+                else None,\n+            )\n+        else:\n+            if (\n+                input_shape[1] is not None\n+                and sum(self.cropping[0]) >= input_shape[1]\n+            ) or (\n+                input_shape[2] is not None\n+                and sum(self.cropping[1]) >= input_shape[2]\n+            ):\n+                raise ValueError(\n+                    \"Values in `cropping` argument should be greater than the \"\n+                    \"corresponding spatial dimension of the input. Received: \"\n+                    f\"input_shape={input_shape}, cropping={self.cropping}\"\n+                )\n+            return (\n+                input_shape[0],\n+                input_shape[1] - self.cropping[0][0] - self.cropping[0][1]\n+                if input_shape[1] is not None\n+                else None,\n+                input_shape[2] - self.cropping[1][0] - self.cropping[1][1]\n+                if input_shape[2] is not None\n+                else None,\n+                input_shape[3],\n+            )\n+\n+    def call(self, inputs):\n+        if self.data_format == \"channels_first\":\n+            if (\n+                inputs.shape[2] is not None\n+                and sum(self.cropping[0]) >= inputs.shape[2]\n+            ) or (\n+                inputs.shape[3] is not None\n+                and sum(self.cropping[1]) >= inputs.shape[3]\n+            ):\n+                raise ValueError(\n+                    \"Values in `cropping` argument should be greater than the \"\n+                    \"corresponding spatial dimension of the input. Received: \"\n+                    f\"inputs.shape={inputs.shape}, cropping={self.cropping}\"\n+                )\n+            if self.cropping[0][1] == self.cropping[1][1] == 0:\n+                return inputs[\n+                    :, :, self.cropping[0][0] :, self.cropping[1][0] :\n+                ]\n+            elif self.cropping[0][1] == 0:\n+                return inputs[\n+                    :,\n+                    :,\n+                    self.cropping[0][0] :,\n+                    self.cropping[1][0] : -self.cropping[1][1],\n+                ]\n+            elif self.cropping[1][1] == 0:\n+                return inputs[\n+                    :,\n+                    :,\n+                    self.cropping[0][0] : -self.cropping[0][1],\n+                    self.cropping[1][0] :,\n+                ]\n+            return inputs[\n+                :,\n+                :,\n+                self.cropping[0][0] : -self.cropping[0][1],\n+                self.cropping[1][0] : -self.cropping[1][1],\n+            ]\n+        else:\n+            if (\n+                inputs.shape[1] is not None\n+                and sum(self.cropping[0]) >= inputs.shape[1]\n+            ) or (\n+                inputs.shape[2] is not None\n+                and sum(self.cropping[1]) >= inputs.shape[2]\n+            ):\n+                raise ValueError(\n+                    \"Values in `cropping` argument should be greater than the \"\n+                    \"corresponding spatial dimension of the input. Received: \"\n+                    f\"inputs.shape={inputs.shape}, cropping={self.cropping}\"\n+                )\n+            if self.cropping[0][1] == self.cropping[1][1] == 0:\n+                return inputs[\n+                    :, self.cropping[0][0] :, self.cropping[1][0] :, :\n+                ]\n+            elif self.cropping[0][1] == 0:\n+                return inputs[\n+                    :,\n+                    self.cropping[0][0] :,\n+                    self.cropping[1][0] : -self.cropping[1][1],\n+                    :,\n+                ]\n+            elif self.cropping[1][1] == 0:\n+                return inputs[\n+                    :,\n+                    self.cropping[0][0] : -self.cropping[0][1],\n+                    self.cropping[1][0] :,\n+                    :,\n+                ]\n+            return inputs[\n+                :,\n+                self.cropping[0][0] : -self.cropping[0][1],\n+                self.cropping[1][0] : -self.cropping[1][1],\n+                :,\n+            ]\n+\n+    def get_config(self):\n+        config = {\"cropping\": self.cropping, \"data_format\": self.data_format}\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,99 @@\n+import numpy as np\n+import pytest\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import operations as ops\n+from keras_core import testing\n+\n+\n+class CroppingTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.product(\n+        (\n+            # different cropping values\n+            {\"cropping\": ((1, 2), (3, 4)), \"expected_ranges\": ((1, 5), (3, 5))},\n+            # same cropping values with 2 tuples\n+            {\"cropping\": ((2, 2), (2, 2)), \"expected_ranges\": ((2, 5), (2, 7))},\n+            # same cropping values with 1 tuple\n+            {\"cropping\": (2, 2), \"expected_ranges\": ((2, 5), (2, 7))},\n+            # same cropping values with an integer\n+            {\"cropping\": 2, \"expected_ranges\": ((2, 5), (2, 7))},\n+            # cropping right only in both dimensions\n+            {\"cropping\": ((0, 2), (0, 4)), \"expected_ranges\": ((0, 5), (0, 5))},\n+            # cropping left only in both dimensions\n+            {\"cropping\": ((1, 0), (3, 0)), \"expected_ranges\": ((1, 7), (3, 9))},\n+            # cropping left only in rows dimension\n+            {\"cropping\": ((1, 0), (3, 4)), \"expected_ranges\": ((1, 7), (3, 5))},\n+            # cropping left only in cols dimension\n+            {\"cropping\": ((1, 2), (3, 0)), \"expected_ranges\": ((1, 5), (3, 9))},\n+        ),\n+        (\n+            {\"data_format\": \"channels_first\"},\n+            {\"data_format\": \"channels_last\"},\n+        ),\n+    )\n+    def test_cropping_2d(self, cropping, data_format, expected_ranges):\n+        if data_format == \"channels_first\":\n+            inputs = np.random.rand(3, 5, 7, 9)\n+            expected_output = ops.convert_to_tensor(\n+                inputs[\n+                    :,\n+                    :,\n+                    expected_ranges[0][0] : expected_ranges[0][1],\n+                    expected_ranges[1][0] : expected_ranges[1][1],\n+                ]\n+            )\n+        else:\n+            inputs = np.random.rand(3, 7, 9, 5)\n+            expected_output = ops.convert_to_tensor(\n+                inputs[\n+                    :,\n+                    expected_ranges[0][0] : expected_ranges[0][1],\n+                    expected_ranges[1][0] : expected_ranges[1][1],\n+                    :,\n+                ]\n+            )\n+\n+        self.run_layer_test(\n+            layers.Cropping2D,\n+            init_kwargs={\"cropping\": cropping, \"data_format\": data_format},\n+            input_data=inputs,\n+            expected_output=expected_output,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_cropping_2d_with_dynamic_batch_size(self):\n+        input_layer = layers.Input(batch_shape=(None, 7, 9, 5))\n+        permuted = layers.Cropping2D(((1, 2), (3, 4)))(input_layer)\n+        self.assertEqual(permuted.shape, (None, 4, 2, 5))\n+\n+    @parameterized.product(\n+        (\n+            {\"cropping\": ((3, 6), (0, 0))},\n+            {\"cropping\": ((0, 0), (5, 4))},\n+        ),\n+        (\n+            {\"data_format\": \"channels_first\"},\n+            {\"data_format\": \"channels_last\"},\n+        ),\n+    )\n+    def test_cropping_2d_errors_if_cropping_more_than_available(\n+        self, cropping, data_format\n+    ):\n+        input_layer = layers.Input(batch_shape=(3, 7, 9, 5))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=cropping, data_format=data_format)(\n+                input_layer\n+            )\n+\n+    def test_cropping_2d_errors_if_cropping_argument_invalid(self):\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=(1,))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=(1, 2, 3))\n+        with self.assertRaises(ValueError):\n+            layers.Cropping2D(cropping=\"1\")\n\n@@ -14,13 +14,13 @@ class UpSampling1D(Layer):\n \n     >>> input_shape = (2, 2, 3)\n     >>> x = np.arange(np.prod(input_shape)).reshape(input_shape)\n-    >>> print(x)\n+    >>> x\n     [[[ 0  1  2]\n       [ 3  4  5]]\n      [[ 6  7  8]\n       [ 9 10 11]]]\n     >>> y = keras_core.layers.UpSampling1D(size=2)(x)\n-    >>> print(y)\n+    >>> y\n     [[[ 0.  1.  2.]\n       [ 0.  1.  2.]\n       [ 3.  4.  5.]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7e8a4e559053aa3ddc5cb83fcd2474b179864ea3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 929 | Lines Deleted: 1 | Files Changed: 6 | Hunks: 6 | Methods Changed: 13 | Complexity Δ (Sum/Max): 28/21 | Churn Δ: 930 | Churn Cumulative: 4206 | Contributors (this commit): 12 | Commits (past 90d): 86 | Contributors (cumulative): 21 | DMM Complexity: 0.8189473684210526\n\nDIFF:\n@@ -319,7 +319,9 @@ def separable_conv(\n             \"`num_spatial_dims` must be 1 or 2. Received: \"\n             f\"num_spatial_dims={num_spatial_dims}.\"\n         )\n-    tf_data_format = _convert_data_format(data_format, len(inputs.shape))\n+    # Because we use `tf.nn.separable_conv2d` for both 1D and 2D convs, we set\n+    # `tf_data_format` using 2D conv format.\n+    tf_data_format = _convert_data_format(data_format, 4)\n     padding = padding.upper()\n     if isinstance(strides, int):\n         strides = (strides,) * num_spatial_dims\n\n@@ -9,6 +9,8 @@ from keras_core.layers.convolutional.conv3d import Conv3D\n from keras_core.layers.convolutional.conv3d_transpose import Conv3DTranspose\n from keras_core.layers.convolutional.depthwise_conv1d import DepthwiseConv1D\n from keras_core.layers.convolutional.depthwise_conv2d import DepthwiseConv2D\n+from keras_core.layers.convolutional.separable_conv1d import SeparableConv1D\n+from keras_core.layers.convolutional.separable_conv2d import SeparableConv2D\n from keras_core.layers.core.dense import Dense\n from keras_core.layers.core.einsum_dense import EinsumDense\n from keras_core.layers.core.embedding import Embedding\n\n@@ -0,0 +1,302 @@\n+\"\"\"Keras abstract base layer for separable convolution.\"\"\"\n+\n+\n+from keras_core import activations\n+from keras_core import constraints\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core import regularizers\n+from keras_core.backend import standardize_data_format\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+from keras_core.operations.operation_utils import compute_conv_output_shape\n+\n+\n+class BaseSeparableConv(Layer):\n+    \"\"\"Abstract base layer for separable convolution.\n+\n+    This layer performs a depthwise convolution that acts separately on\n+    channels, followed by a pointwise convolution that mixes channels. If\n+    `use_bias` is True and a bias initializer is provided, it adds a bias vector\n+    to the output.\n+\n+    Args:\n+        rank: int, the rank of the convolution, e.g. 2 for 2D convolution.\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel. The total number of depthwise convolution\n+            output channels will be equal to `input_channel * depth_multiplier`.\n+        filters: int, the dimensionality of the output space (i.e. the number\n+            of filters in the pointwise convolution).\n+        kernel_size: int or tuple/list of `rank` integers, specifying the size\n+            of the depthwise convolution window.\n+        strides: int or tuple/list of `rank` integers, specifying the stride\n+            length of the depthwise convolution. If only one int is specified,\n+            the same stride size will be used for all dimensions.\n+            `stride value != 1` is incompatible with `dilation_rate != 1`.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+        dilation_rate: int or tuple/list of `rank` integers, specifying the\n+            dilation rate to use for dilated convolution. If only one int is\n+            specified, the same dilation rate will be used for all dimensions.\n+        activation: Activation function. If `None`, no activation is applied.\n+        use_bias: bool, if `True`, bias will be added to the output.\n+        depthwise_initializer: An initializer for the depthwise convolution\n+            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            will be used.\n+        pointwise_initializer: An initializer for the pointwise convolution\n+            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            will be used.\n+        bias_initializer: An initializer for the bias vector. If None, the\n+            default initializer ('\"zeros\"') will be used.\n+        depthwise_regularizer: Optional regularizer for the depthwise\n+            convolution kernel.\n+        pointwise_regularizer: Optional regularizer for the pointwise\n+            convolution kernel.\n+        bias_regularizer: Optional regularizer for the bias vector.\n+        activity_regularizer: Optional regularizer function for the output.\n+        depthwise_constraint: Optional projection function to be applied to the\n+            depthwise kernel after being updated by an `Optimizer` (e.g. used\n+            for norm constraints or value constraints for layer weights). The\n+            function must take as input the unprojected variable and must return\n+            the projected variable (which must have the same shape).\n+        pointwise_constraint: Optional projection function to be applied to the\n+            pointwise kernel after being updated by an `Optimizer`.\n+        bias_constraint: Optional projection function to be applied to the\n+            bias after being updated by an `Optimizer`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        rank,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides=1,\n+        padding=\"valid\",\n+        data_format=None,\n+        dilation_rate=1,\n+        activation=None,\n+        use_bias=True,\n+        depthwise_initializer=\"glorot_uniform\",\n+        pointwise_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        depthwise_regularizer=None,\n+        pointwise_regularizer=None,\n+        bias_regularizer=None,\n+        activity_regularizer=None,\n+        depthwise_constraint=None,\n+        pointwise_constraint=None,\n+        bias_constraint=None,\n+        trainable=True,\n+        name=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            trainable=trainable,\n+            name=name,\n+            activity_regularizer=regularizers.get(activity_regularizer),\n+            **kwargs,\n+        )\n+        self.rank = rank\n+        self.depth_multiplier = depth_multiplier\n+        self.filters = filters\n+\n+        if isinstance(kernel_size, int):\n+            kernel_size = (kernel_size,) * self.rank\n+        self.kernel_size = kernel_size\n+\n+        if isinstance(strides, int):\n+            strides = (strides,) * self.rank\n+        self.strides = strides\n+\n+        if isinstance(dilation_rate, int):\n+            dilation_rate = (dilation_rate,) * self.rank\n+        self.dilation_rate = dilation_rate\n+\n+        self.padding = padding\n+        self.data_format = standardize_data_format(data_format)\n+        self.activation = activations.get(activation)\n+        self.use_bias = use_bias\n+        self.depthwise_initializer = initializers.get(depthwise_initializer)\n+        self.pointwise_initializer = initializers.get(pointwise_initializer)\n+        self.bias_initializer = initializers.get(bias_initializer)\n+        self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n+        self.pointwise_regularizer = regularizers.get(pointwise_regularizer)\n+        self.bias_regularizer = regularizers.get(bias_regularizer)\n+        self.depthwise_constraint = constraints.get(depthwise_constraint)\n+        self.pointwise_constraint = constraints.get(pointwise_constraint)\n+        self.bias_constraint = constraints.get(bias_constraint)\n+        self.data_format = self.data_format\n+\n+        self.input_spec = InputSpec(min_ndim=self.rank + 2)\n+\n+        if self.depth_multiplier is not None and self.depth_multiplier <= 0:\n+            raise ValueError(\n+                \"Invalid value for argument `depth_multiplier`. Expected a \"\n+                \"strictly  positive value. Received \"\n+                f\"depth_multiplier={self.depth_multiplier}.\"\n+            )\n+\n+        if self.filters is not None and self.filters <= 0:\n+            raise ValueError(\n+                \"Invalid value for argument `filters`. Expected a strictly \"\n+                f\"positive value. Received filters={self.filters}.\"\n+            )\n+\n+        if not all(self.kernel_size):\n+            raise ValueError(\n+                \"The argument `kernel_size` cannot contain 0. Received: \"\n+                f\"kernel_size={self.kernel_size}.\"\n+            )\n+\n+        if not all(self.strides):\n+            raise ValueError(\n+                \"The argument `strides` cannot contains 0(s). Received: \"\n+                f\"strides={self.strides}\"\n+            )\n+\n+        if max(self.strides) > 1 and max(self.dilation_rate) > 1:\n+            raise ValueError(\n+                \"`strides > 1` not supported in conjunction with \"\n+                f\"`dilation_rate > 1`. Received: strides={self.strides} and \"\n+                f\"dilation_rate={self.dilation_rate}\"\n+            )\n+\n+    def build(self, input_shape):\n+        if self.data_format == \"channels_last\":\n+            channel_axis = -1\n+            input_channel = input_shape[-1]\n+        else:\n+            channel_axis = 1\n+            input_channel = input_shape[1]\n+        self.input_spec = InputSpec(\n+            min_ndim=self.rank + 2, axes={channel_axis: input_channel}\n+        )\n+        depthwise_kernel_shape = self.kernel_size + (\n+            input_channel,\n+            self.depth_multiplier,\n+        )\n+        pointwise_kernel_shape = (1,) * self.rank + (\n+            self.depth_multiplier * input_channel,\n+            self.filters,\n+        )\n+\n+        self.depthwise_kernel = self.add_weight(\n+            name=\"depthwise_kernel\",\n+            shape=depthwise_kernel_shape,\n+            initializer=self.depthwise_initializer,\n+            regularizer=self.depthwise_regularizer,\n+            constraint=self.depthwise_constraint,\n+            trainable=True,\n+            dtype=self.dtype,\n+        )\n+        self.pointwise_kernel = self.add_weight(\n+            name=\"pointwise_kernel\",\n+            shape=pointwise_kernel_shape,\n+            initializer=self.pointwise_initializer,\n+            regularizer=self.pointwise_regularizer,\n+            constraint=self.pointwise_constraint,\n+            trainable=True,\n+            dtype=self.dtype,\n+        )\n+        if self.use_bias:\n+            self.bias = self.add_weight(\n+                name=\"bias\",\n+                shape=(self.filters,),\n+                initializer=self.bias_initializer,\n+                regularizer=self.bias_regularizer,\n+                constraint=self.bias_constraint,\n+                trainable=True,\n+                dtype=self.dtype,\n+            )\n+        else:\n+            self.bias = None\n+        self.built = True\n+\n+    def call(self, inputs):\n+        outputs = ops.separable_conv(\n+            inputs,\n+            self.depthwise_kernel,\n+            self.pointwise_kernel,\n+            strides=self.strides,\n+            padding=self.padding,\n+            dilation_rate=self.dilation_rate,\n+            data_format=self.data_format,\n+        )\n+\n+        if self.use_bias:\n+            if self.data_format == \"channels_last\":\n+                bias_shape = (1,) * (self.rank + 1) + (self.filters,)\n+            else:\n+                bias_shape = (1, self.filters) + (1,) * self.rank\n+            bias = ops.reshape(self.bias, bias_shape)\n+            outputs += bias\n+\n+        if self.activation is not None:\n+            return self.activation(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return compute_conv_output_shape(\n+            input_shape,\n+            self.filters,\n+            self.kernel_size,\n+            strides=self.strides,\n+            padding=self.padding,\n+            data_format=self.data_format,\n+            dilation_rate=self.dilation_rate,\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"depth_multiplier\": self.depth_multiplier,\n+                \"filters\": self.filters,\n+                \"kernel_size\": self.kernel_size,\n+                \"strides\": self.strides,\n+                \"padding\": self.padding,\n+                \"data_format\": self.data_format,\n+                \"dilation_rate\": self.dilation_rate,\n+                \"activation\": activations.serialize(self.activation),\n+                \"use_bias\": self.use_bias,\n+                \"depthwise_initializer\": initializers.serialize(\n+                    self.depthwise_initializer\n+                ),\n+                \"pointwise_initializer\": initializers.serialize(\n+                    self.pointwise_initializer\n+                ),\n+                \"bias_initializer\": initializers.serialize(\n+                    self.bias_initializer\n+                ),\n+                \"depthwise_regularizer\": regularizers.serialize(\n+                    self.depthwise_regularizer\n+                ),\n+                \"pointwise_regularizer\": regularizers.serialize(\n+                    self.pointwise_regularizer\n+                ),\n+                \"bias_regularizer\": regularizers.serialize(\n+                    self.bias_regularizer\n+                ),\n+                \"activity_regularizer\": regularizers.serialize(\n+                    self.activity_regularizer\n+                ),\n+                \"depthwise_constraint\": constraints.serialize(\n+                    self.depthwise_constraint\n+                ),\n+                \"pointwise_constraint\": constraints.serialize(\n+                    self.pointwise_constraint\n+                ),\n+                \"bias_constraint\": constraints.serialize(self.bias_constraint),\n+            }\n+        )\n+        return config\n\n@@ -0,0 +1,143 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.convolutional.base_separable_conv import (\n+    BaseSeparableConv,\n+)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.SeparableConv1D\",\n+        \"keras_core.layers.SeparableConvolution1D\",\n+    ]\n+)\n+class SeparableConv1D(BaseSeparableConv):\n+    \"\"\"1D separable convolution layer.\n+\n+    This layer performs a depthwise convolution that acts separately on\n+    channels, followed by a pointwise convolution that mixes channels.\n+    If `use_bias` is True and a bias initializer is provided,\n+    it adds a bias vector to the output. It then optionally applies an\n+    activation function to produce the final output.\n+\n+    Args:\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel. The total number of depthwise convolution\n+            output channels will be equal to `input_channel * depth_multiplier`.\n+        filters: int, the dimensionality of the output space (i.e. the number\n+            of filters in the pointwise convolution).\n+        kernel_size: int or tuple/list of 1 integers, specifying the size of the\n+            depthwise convolution window.\n+        strides: int or tuple/list of 1 integers, specifying the stride length\n+            of the depthwise convolution. If only one int is specified, the same\n+            stride size will be used for all dimensions. `stride value != 1` is\n+            incompatible with `dilation_rate != 1`.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+        dilation_rate: int or tuple/list of 1 integers, specifying the dilation\n+            rate to use for dilated convolution. If only one int is specified,\n+            the same dilation rate will be used for all dimensions.\n+        activation: Activation function. If `None`, no activation is applied.\n+        use_bias: bool, if `True`, bias will be added to the output.\n+        depthwise_initializer: An initializer for the depthwise convolution\n+            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            will be used.\n+        pointwise_initializer: An initializer for the pointwise convolution\n+            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            will be used.\n+        bias_initializer: An initializer for the bias vector. If None, the\n+            default initializer ('\"zeros\"') will be used.\n+        depthwise_regularizer: Optional regularizer for the depthwise\n+            convolution kernel.\n+        pointwise_regularizer: Optional regularizer for the pointwise\n+            convolution kernel.\n+        bias_regularizer: Optional regularizer for the bias vector.\n+        activity_regularizer: Optional regularizer function for the output.\n+        depthwise_constraint: Optional projection function to be applied to the\n+            depthwise kernel after being updated by an `Optimizer` (e.g. used\n+            for norm constraints or value constraints for layer weights). The\n+            function must take as input the unprojected variable and must return\n+            the projected variable (which must have the same shape).\n+        pointwise_constraint: Optional projection function to be applied to the\n+            pointwise kernel after being updated by an `Optimizer`.\n+        bias_constraint: Optional projection function to be applied to the\n+            bias after being updated by an `Optimizer`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        A 3D tensor with shape: `(batch_shape, steps, channels)`\n+    - If `data_format=\"channels_first\"`:\n+        A 3D tensor with shape: `(batch_shape, channels, steps)`\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        A 3D tensor with shape: `(batch_shape, new_steps, filters)`\n+    - If `data_format=\"channels_first\"`:\n+        A 3D tensor with shape: `(batch_shape, filters, new_steps)`\n+\n+    Returns:\n+        A 3D tensor representing\n+        `activation(separable_conv1d(inputs, kernel) + bias)`.\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(4, 10, 12)\n+    >>> y = keras_core.layers.SeparableConv1D(3, 4, 3, 2, activation='relu')(x)\n+    >>> print(y.shape)\n+    (4, 4, 4)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides=1,\n+        padding=\"valid\",\n+        data_format=None,\n+        dilation_rate=1,\n+        activation=None,\n+        use_bias=True,\n+        depthwise_initializer=\"glorot_uniform\",\n+        pointwise_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        depthwise_regularizer=None,\n+        pointwise_regularizer=None,\n+        bias_regularizer=None,\n+        activity_regularizer=None,\n+        depthwise_constraint=None,\n+        pointwise_constraint=None,\n+        bias_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            rank=1,\n+            depth_multiplier=depth_multiplier,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+            activation=activation,\n+            use_bias=use_bias,\n+            depthwise_initializer=depthwise_initializer,\n+            pointwise_initializer=pointwise_initializer,\n+            bias_initializer=bias_initializer,\n+            depthwise_regularizer=depthwise_regularizer,\n+            pointwise_regularizer=pointwise_regularizer,\n+            bias_regularizer=bias_regularizer,\n+            activity_regularizer=activity_regularizer,\n+            depthwise_constraint=depthwise_constraint,\n+            pointwise_constraint=pointwise_constraint,\n+            bias_constraint=bias_constraint,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,143 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.convolutional.base_separable_conv import (\n+    BaseSeparableConv,\n+)\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.layers.SeparableConv2D\",\n+        \"keras_core.layers.SeparableConvolution2D\",\n+    ]\n+)\n+class SeparableConv2D(BaseSeparableConv):\n+    \"\"\"2D separable convolution layer.\n+\n+    This layer performs a depthwise convolution that acts separately on\n+    channels, followed by a pointwise convolution that mixes channels.\n+    If `use_bias` is True and a bias initializer is provided,\n+    it adds a bias vector to the output. It then optionally applies an\n+    activation function to produce the final output.\n+\n+    Args:\n+        depth_multiplier: The number of depthwise convolution output channels\n+            for each input channel. The total number of depthwise convolution\n+            output channels will be equal to `input_channel * depth_multiplier`.\n+        filters: int, the dimensionality of the output space (i.e. the number\n+            of filters in the pointwise convolution).\n+        kernel_size: int or tuple/list of 2 integers, specifying the size of the\n+            depthwise convolution window.\n+        strides: int or tuple/list of 2 integers, specifying the stride length\n+            of the depthwise convolution. If only one int is specified, the same\n+            stride size will be used for all dimensions. `stride value != 1` is\n+            incompatible with `dilation_rate != 1`.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+        dilation_rate: int or tuple/list of 2 integers, specifying the dilation\n+            rate to use for dilated convolution. If only one int is specified,\n+            the same dilation rate will be used for all dimensions.\n+        activation: Activation function. If `None`, no activation is applied.\n+        use_bias: bool, if `True`, bias will be added to the output.\n+        depthwise_initializer: An initializer for the depthwise convolution\n+            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            will be used.\n+        pointwise_initializer: An initializer for the pointwise convolution\n+            kernel. If None, then the default initializer ('\"glorot_uniform\"')\n+            will be used.\n+        bias_initializer: An initializer for the bias vector. If None, the\n+            default initializer ('\"zeros\"') will be used.\n+        depthwise_regularizer: Optional regularizer for the depthwise\n+            convolution kernel.\n+        pointwise_regularizer: Optional regularizer for the pointwise\n+            convolution kernel.\n+        bias_regularizer: Optional regularizer for the bias vector.\n+        activity_regularizer: Optional regularizer function for the output.\n+        depthwise_constraint: Optional projection function to be applied to the\n+            depthwise kernel after being updated by an `Optimizer` (e.g. used\n+            for norm constraints or value constraints for layer weights). The\n+            function must take as input the unprojected variable and must return\n+            the projected variable (which must have the same shape).\n+        pointwise_constraint: Optional projection function to be applied to the\n+            pointwise kernel after being updated by an `Optimizer`.\n+        bias_constraint: Optional projection function to be applied to the\n+            bias after being updated by an `Optimizer`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        A 4D tensor with shape: `(batch_size, height, width, channels)`\n+    - If `data_format=\"channels_first\"`:\n+        A 4D tensor with shape: `(batch_size, channels, height, width)`\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        A 4D tensor with shape: `(batch_size, new_height, new_width, filters)`\n+    - If `data_format=\"channels_first\"`:\n+        A 4D tensor with shape: `(batch_size, filters, new_height, new_width)`\n+\n+    Returns:\n+        A 4D tensor representing\n+        `activation(separable_conv2d(inputs, kernel) + bias)`.\n+\n+    Examples:\n+\n+    >>> x = np.random.rand(4, 10, 10, 12)\n+    >>> y = keras_core.layers.SeparableConv2D(3, 4, 3, 2, activation='relu')(x)\n+    >>> print(y.shape)\n+    (4, 4, 4, 4)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides=1,\n+        padding=\"valid\",\n+        data_format=None,\n+        dilation_rate=1,\n+        activation=None,\n+        use_bias=True,\n+        depthwise_initializer=\"glorot_uniform\",\n+        pointwise_initializer=\"glorot_uniform\",\n+        bias_initializer=\"zeros\",\n+        depthwise_regularizer=None,\n+        pointwise_regularizer=None,\n+        bias_regularizer=None,\n+        activity_regularizer=None,\n+        depthwise_constraint=None,\n+        pointwise_constraint=None,\n+        bias_constraint=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            rank=2,\n+            depth_multiplier=depth_multiplier,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+            activation=activation,\n+            use_bias=use_bias,\n+            depthwise_initializer=depthwise_initializer,\n+            pointwise_initializer=pointwise_initializer,\n+            bias_initializer=bias_initializer,\n+            depthwise_regularizer=depthwise_regularizer,\n+            pointwise_regularizer=pointwise_regularizer,\n+            bias_regularizer=bias_regularizer,\n+            activity_regularizer=activity_regularizer,\n+            depthwise_constraint=depthwise_constraint,\n+            pointwise_constraint=pointwise_constraint,\n+            bias_constraint=bias_constraint,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,336 @@\n+import numpy as np\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class SeparableConvBasicTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        {\n+            \"depth_multiplier\": 5,\n+            \"filters\": 5,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"input_shape\": (3, 5, 4),\n+            \"output_shape\": (3, 4, 5),\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"same\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2,),\n+            \"input_shape\": (3, 4, 4),\n+            \"output_shape\": (3, 4, 6),\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": (2,),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"input_shape\": (3, 5, 4),\n+            \"output_shape\": (3, 2, 6),\n+        },\n+    )\n+    def test_separable_conv1d_basic(\n+        self,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.SeparableConv1D,\n+            init_kwargs={\n+                \"depth_multiplier\": depth_multiplier,\n+                \"filters\": filters,\n+                \"kernel_size\": kernel_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+                \"dilation_rate\": dilation_rate,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=3,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        {\n+            \"depth_multiplier\": 5,\n+            \"filters\": 5,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+            \"input_shape\": (3, 5, 5, 4),\n+            \"output_shape\": (3, 4, 4, 5),\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"same\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2, 2),\n+            \"input_shape\": (3, 4, 4, 4),\n+            \"output_shape\": (3, 4, 4, 6),\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": (2, 2),\n+            \"strides\": (2, 2),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (1, 1),\n+            \"input_shape\": (3, 5, 5, 4),\n+            \"output_shape\": (3, 2, 2, 6),\n+        },\n+    )\n+    def test_separable_conv2d_basic(\n+        self,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.SeparableConv2D,\n+            init_kwargs={\n+                \"depth_multiplier\": depth_multiplier,\n+                \"filters\": filters,\n+                \"kernel_size\": kernel_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+                \"dilation_rate\": dilation_rate,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=3,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    def test_bad_init_args(self):\n+        # `depth_multiplier` is not positive.\n+        with self.assertRaises(ValueError):\n+            layers.SeparableConv1D(depth_multiplier=0, filters=1, kernel_size=1)\n+\n+        # `filters` is not positive.\n+        with self.assertRaises(ValueError):\n+            layers.SeparableConv1D(depth_multiplier=1, filters=0, kernel_size=1)\n+\n+        # `kernel_size` has 0.\n+        with self.assertRaises(ValueError):\n+            layers.SeparableConv2D(\n+                depth_multiplier=2, filters=2, kernel_size=(1, 0)\n+            )\n+\n+        # `strides` has 0.\n+        with self.assertRaises(ValueError):\n+            layers.SeparableConv2D(\n+                depth_multiplier=2,\n+                filters=2,\n+                kernel_size=(2, 2),\n+                strides=(1, 0),\n+            )\n+\n+        # `dilation_rate > 1` while `strides > 1`.\n+        with self.assertRaises(ValueError):\n+            layers.SeparableConv2D(\n+                depth_multiplier=2,\n+                filters=2,\n+                kernel_size=(2, 2),\n+                strides=2,\n+                dilation_rate=(2, 1),\n+            )\n+\n+\n+class SeparableConvCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        {\n+            \"depth_multiplier\": 5,\n+            \"filters\": 5,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"same\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2,),\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": (2,),\n+            \"strides\": (2,),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+        },\n+    )\n+    def test_separable_conv1d(\n+        self,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+    ):\n+        layer = layers.SeparableConv1D(\n+            depth_multiplier=depth_multiplier,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+        )\n+        tf_keras_layer = tf.keras.layers.SeparableConv1D(\n+            depth_multiplier=depth_multiplier,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+        )\n+\n+        inputs = np.random.normal(size=[2, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+        tf_keras_layer.build(input_shape=inputs.shape)\n+\n+        depthwise_kernel_shape = layer.depthwise_kernel.shape\n+        depthwise_kernel_weights = np.random.normal(size=depthwise_kernel_shape)\n+        layer.depthwise_kernel.assign(depthwise_kernel_weights)\n+        tf_keras_layer.depthwise_kernel.assign(depthwise_kernel_weights)\n+\n+        pointwise_kernel_shape = layer.pointwise_kernel.shape\n+        pointwise_kernel_weights = np.random.normal(size=pointwise_kernel_shape)\n+        layer.pointwise_kernel.assign(pointwise_kernel_weights)\n+        tf_keras_layer.pointwise_kernel.assign(pointwise_kernel_weights)\n+\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.bias.assign(bias_weights)\n+        tf_keras_layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n+\n+    @parameterized.parameters(\n+        {\n+            \"depth_multiplier\": 5,\n+            \"filters\": 5,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": 1,\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": 2,\n+            \"strides\": 1,\n+            \"padding\": \"same\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (2, 2),\n+        },\n+        {\n+            \"depth_multiplier\": 6,\n+            \"filters\": 6,\n+            \"kernel_size\": (2, 2),\n+            \"strides\": (2, 2),\n+            \"padding\": \"valid\",\n+            \"data_format\": \"channels_last\",\n+            \"dilation_rate\": (1, 1),\n+        },\n+    )\n+    def test_separable_conv2d(\n+        self,\n+        depth_multiplier,\n+        filters,\n+        kernel_size,\n+        strides,\n+        padding,\n+        data_format,\n+        dilation_rate,\n+    ):\n+        layer = layers.SeparableConv2D(\n+            depth_multiplier=depth_multiplier,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+        )\n+        tf_keras_layer = tf.keras.layers.SeparableConv2D(\n+            depth_multiplier=depth_multiplier,\n+            filters=filters,\n+            kernel_size=kernel_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+            dilation_rate=dilation_rate,\n+        )\n+\n+        inputs = np.random.normal(size=[2, 8, 8, 4])\n+        layer.build(input_shape=inputs.shape)\n+        tf_keras_layer.build(input_shape=inputs.shape)\n+\n+        depthwise_kernel_shape = layer.depthwise_kernel.shape\n+        depthwise_kernel_weights = np.random.normal(size=depthwise_kernel_shape)\n+        layer.depthwise_kernel.assign(depthwise_kernel_weights)\n+        tf_keras_layer.depthwise_kernel.assign(depthwise_kernel_weights)\n+\n+        pointwise_kernel_shape = layer.pointwise_kernel.shape\n+        pointwise_kernel_weights = np.random.normal(size=pointwise_kernel_shape)\n+        layer.pointwise_kernel.assign(pointwise_kernel_weights)\n+        tf_keras_layer.pointwise_kernel.assign(pointwise_kernel_weights)\n+\n+        bias_weights = np.random.normal(size=(filters,))\n+        layer.bias.assign(bias_weights)\n+        tf_keras_layer.bias.assign(bias_weights)\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected, rtol=1e-5, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
