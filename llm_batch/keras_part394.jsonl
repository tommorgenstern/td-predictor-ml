{"custom_id": "keras#c6c70f4d8fb6128438effcab363502a0262e6347", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 35 | Churn Cumulative: 1220 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 0.9\n\nDIFF:\n@@ -18,6 +18,7 @@ import os\n import textwrap\n \n import keras\n+from keras import initializers\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.layers import core\n@@ -405,6 +406,34 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n       model = testing_utils.get_model_from_layers([layer], input_shape=(1,))\n       model(tf.ones((4, 1)))\n \n+  @keras_parameterized.run_all_keras_modes\n+  @keras_parameterized.run_with_all_model_types\n+  def test_lambda_skip_state_variable_from_initializer(self):\n+    # Force the initializers to use the tf.random.Generator, which will contain\n+    # the state variable.\n+    kernel_initializer = initializers.RandomNormalV2()\n+    kernel_initializer._random_generator._force_generator = True\n+    dense = keras.layers.Dense(1, use_bias=False,\n+                               kernel_initializer=kernel_initializer)\n+\n+    def lambda_fn(x):\n+      return dense(x + 1)  # Dense layer is built on first call\n+\n+    # While it is generally not advised to mix Variables with Lambda layers, if\n+    # the variables are explicitly set as attributes then they are still\n+    # tracked. This is consistent with the base Layer behavior.\n+    layer = keras.layers.Lambda(lambda_fn)\n+    layer.dense = dense\n+\n+    model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n+    model.compile(\n+        keras.optimizer_v2.gradient_descent.SGD(0.1),\n+        'mae',\n+        run_eagerly=testing_utils.should_run_eagerly())\n+    x, y = np.ones((10, 10), 'float32'), 2 * np.ones((10, 10), 'float32')\n+    model.fit(x, y, batch_size=2, epochs=2, validation_data=(x, y))\n+    self.assertLen(model.trainable_weights, 1)\n+\n \n @keras_parameterized.run_all_keras_modes\n class CoreLayersTest(keras_parameterized.TestCase):\n\n@@ -204,6 +204,12 @@ class Lambda(Layer):\n       # checking only to immediately discard it.\n       return\n \n+    # Filter out the state variable in the tf.random.Generator, which is\n+    # commonly used for initializer or droput. The variable is intentionally\n+    # not tracked and it is not a trainable variable.\n+    created_variables = [v for v in created_variables\n+                         if 'StateVar' not in v.name]\n+\n     tracked_weights = set(v.ref() for v in self.weights)\n     untracked_new_vars = [\n         v for v in created_variables if v.ref() not in tracked_weights\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11a5e22fc67108bcd3cdd505c34fb285e228efb5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 2204 | Contributors (this commit): 29 | Commits (past 90d): 2 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,14 +191,16 @@ class KerasClassifier(BaseWrapper):\n   \"\"\"Implementation of the scikit-learn classifier API for Keras.\n \n   DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n-  See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n+  See https://www.adriangb.com/scikeras/stable/migration.html\n+  for help migrating.\n   \"\"\"\n \n   def __init__(self, build_fn=None, **sk_params):\n     warnings.warn(\n         'KerasClassifier is deprecated, '\n         'use Sci-Keras (https://github.com/adriangb/scikeras) instead. '\n-        'See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.',\n+        'See https://www.adriangb.com/scikeras/stable/migration.html '\n+        'for help migrating.',\n         DeprecationWarning,\n         stacklevel=2)\n     super().__init__(build_fn, **sk_params)\n@@ -329,7 +331,8 @@ class KerasRegressor(BaseWrapper):\n   \"\"\"Implementation of the scikit-learn regressor API for Keras.\n \n   DEPRECATED. Use [Sci-Keras](https://github.com/adriangb/scikeras) instead.\n-  See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n+  See https://www.adriangb.com/scikeras/stable/migration.html\n+  for help migrating.\n   \"\"\"\n \n   @doc_controls.do_not_doc_inheritable\n@@ -337,7 +340,8 @@ class KerasRegressor(BaseWrapper):\n     warnings.warn(\n         'KerasRegressor is deprecated, '\n         'use Sci-Keras (https://github.com/adriangb/scikeras) instead. '\n-        'See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.',\n+        'See https://www.adriangb.com/scikeras/stable/migration.html '\n+        'for help migrating.',\n         DeprecationWarning,\n         stacklevel=2)\n     super().__init__(build_fn, **sk_params)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b956b88baf0af8d8fb97bbad0efded3367cd06e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 24 | Churn Cumulative: 683 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 0.5333333333333333\n\nDIFF:\n@@ -72,11 +72,10 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       `[batch_size, Tq, Tv]`.\n   \"\"\"\n \n-  def __init__(self, causal=False, dropout=0.0, return_attention_scores=False, **kwargs):\n+  def __init__(self, causal=False, dropout=0.0, **kwargs):\n     super(BaseDenseAttention, self).__init__(**kwargs)\n     self.causal = causal\n     self.dropout = dropout\n-    self.return_attention_scores = return_attention_scores\n     self.supports_masking = True\n \n   def _calculate_scores(self, query, key):\n@@ -142,16 +141,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n            inputs,\n            mask=None,\n            training=None,\n-           return_attention_scores=None):\n-    if return_attention_scores is not None:\n-      tf_logging.warn(\n-        'return_attention_scores in call method is deprecated, '\n-        'you can set it in constructor. Now return_attention_scores '\n-        'in call method will override return_attention_scores value that '\n-        'has been set in constructor. It could cause unexpected behavior.',\n-        DeprecationWarning)\n-      self.return_attention_scores = return_attention_scores\n-\n+           return_attention_scores=False):\n     self._validate_call_args(inputs=inputs, mask=mask)\n     q = inputs[0]\n     v = inputs[1]\n@@ -181,7 +171,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       # Mask of shape [batch_size, Tq, 1].\n       q_mask = tf.expand_dims(q_mask, axis=-1)\n       result *= tf.cast(q_mask, dtype=result.dtype)\n-    if self.return_attention_scores:\n+    if return_attention_scores:\n       return result, attention_scores\n     return result\n \n@@ -195,13 +185,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n     return None\n \n   def compute_output_shape(self, input_shape):\n-     attention_outputs_shape = tf.TensorShape(input_shape[0])\n-     values_shape = tf.TensorShape(input_shape[1])\n-     if self.return_attention_scores:\n-       attention_scores_shape = tf.TensorShape(\n-         attention_outputs_shape[:2] + [values_shape[2]])\n-       return attention_outputs_shape, attention_scores_shape\n-     return attention_outputs_shape\n+    return tf.TensorShape(input_shape[0])\n \n   def _validate_call_args(self, inputs, mask):\n     \"\"\"Validates arguments of the call method.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#983df1fe3f66ab53c254ee37aa29a82554029d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 689 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,6 @@ from keras import backend\n from keras.engine import base_layer\n from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.platform import tf_logging\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -41,9 +40,6 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n       flow of information from the future towards the past.\n     dropout: Float between 0 and 1. Fraction of the units to drop for the\n       attention scores.\n-    return_attention_scores: bool, if `True`, layer returns the attention scores.\n-      If `True`, the layer is incompatible with TimeDistributed wrapper.\n-      Default is `False`.\n \n   Call Args:\n \n@@ -62,7 +58,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         `mask==False` do not contribute to the result.\n     training: Python boolean indicating whether the layer should behave in\n       training mode (adding dropout) or in inference mode (no dropout).\n-    return_attention_scores: bool, (deprecated) if `True`, returns the attention scores\n+    return_attention_scores: bool, if `True`, returns the attention scores\n       (after masking and softmax) as an additional output argument.\n \n   Output:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2fadc8d89ec08631d72028a6513a77e9f3b8fe2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 691 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -181,6 +181,8 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n     return None\n \n   def compute_output_shape(self, input_shape):\n+    # return_attention_scores argument of BaseDenseAttention.call method\n+    # is ignored. Output shape of attention_scores cannot be returned.\n     return tf.TensorShape(input_shape[0])\n \n   def _validate_call_args(self, inputs, mask):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f57715283090a411083aee14c7a83670330f8e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 16294 | Contributors (this commit): 117 | Commits (past 90d): 19 | Contributors (cumulative): 117 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1362,7 +1362,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if self.stop_training:\n           break\n \n-      # If eval data_hanlder exists, delete it after all epochs are done.\n+      # If eval data_handler exists, delete it after all epochs are done.\n       if getattr(self, '_eval_data_handler', None) is not None:\n         del self._eval_data_handler\n       callbacks.on_train_end(logs=training_logs)\n@@ -1872,10 +1872,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           x = x.with_options(options)\n         except ValueError:\n           warnings.warn(\n-              'Using Model.predict with '\n-              'MultiWorkerDistributionStrategy or TPUStrategy and '\n-              'AutoShardPolicy.FILE might lead to out-of-order result'\n-              '. Consider setting it to AutoShardPolicy.DATA.',\n+              'Using Model.predict with MultiWorkerMirroredStrategy or '\n+              'TPUStrategy and AutoShardPolicy.FILE might lead to out-of-order '\n+              'result. Consider setting it to AutoShardPolicy.DATA.',\n               stacklevel=2)\n \n       data_handler = data_adapter.get_data_handler(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c4608dc2cc2361192b361fdc00c261d26520ed0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 45 | Churn Cumulative: 1600 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 0.7058823529411765\n\nDIFF:\n@@ -1134,59 +1134,22 @@ def recursively_deserialize_keras_object(config, module_objects=None):\n         f'tuple or list. Received: config={config}')\n \n \n-def get_common_shape(x, y):\n-  \"\"\"Find a `TensorShape` that is compatible with both `x` and `y`.\"\"\"\n-  if x is None != y is None:\n-    raise RuntimeError(\n-        'Cannot find a common shape when LHS shape is None but RHS shape '\n-        'is not (or vice versa): %s vs. %s' % (x, y))\n-  if x is None:\n-    return None  # The associated input was not a Tensor, no shape generated.\n-  if not isinstance(x, tf.TensorShape):\n-    raise TypeError(\n-        f'Expected x to be a TensorShape but received {x} with type {type(x)}')\n-  if not isinstance(y, tf.TensorShape):\n-    raise TypeError(\n-        f'Expected y to be a TensorShape but received {y} with type {type(y)}')\n-  if x.rank != y.rank or x.rank is None:\n-    return tf.TensorShape(None)\n-  dims = []\n-  for dim_x, dim_y in zip(x.dims, y.dims):\n-    if (dim_x != dim_y\n-        or tf.compat.dimension_value(dim_x) is None\n-        or tf.compat.dimension_value(dim_y) is None):\n-      dims.append(None)\n-    else:\n-      dims.append(tf.compat.dimension_value(dim_x))\n-  return tf.TensorShape(dims)\n-\n-\n def infer_inputs_from_restored_call_function(fn):\n-  \"\"\"Returns TensorSpec of inputs from a restored call function.\n+  \"\"\"Returns TypeSpec of inputs from a restored call function.\n \n   Args:\n     fn: Restored layer call function. It is assumed that `fn` has at least\n         one concrete function and that the inputs are in the first argument.\n \n   Returns:\n-    TensorSpec of call function inputs in the form of (args, kwargs)\n+    TypeSpec of call function inputs in the form of (args, kwargs)\n   \"\"\"\n   def common_spec(x, y):\n-    if not isinstance(x, tf.TensorSpec):\n+    if not isinstance(x, tf.TypeSpec):\n       # Doesn't particularly matter what is returned in this case because the\n       # result will be filtered out in _set_input_shape.\n       return x\n-    common_shape = get_common_shape(x.shape, y.shape)\n-    if isinstance(x, tf.SparseTensorSpec):\n-      return tf.SparseTensorSpec(common_shape, x.dtype)\n-    elif isinstance(x, tf.RaggedTensorSpec):\n-      return tf.RaggedTensorSpec(\n-          common_shape,\n-          x.dtype,\n-          ragged_rank=x.ragged_rank,\n-          row_splits_dtype=x.row_splits_dtype,\n-          flat_values_spec=x.flat_values_spec)\n-    return tf.TensorSpec(common_shape, x.dtype, x.name)\n+    return x.most_specific_compatible_type(y)\n \n   spec = fn.concrete_functions[0].structured_input_signature\n   for concrete in fn.concrete_functions[1:]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3910655e709d0cdb1f22ab0a261ad058f2a0f96b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 7403 | Contributors (this commit): 100 | Commits (past 90d): 13 | Contributors (cumulative): 100 | DMM Complexity: None\n\nDIFF:\n@@ -1253,9 +1253,10 @@ class ModelCheckpoint(Callback):\n       options: Optional `tf.train.CheckpointOptions` object if\n         `save_weights_only` is true or optional `tf.saved_model.SaveOptions`\n         object if `save_weights_only` is false.\n-      initial_value_threshold: Initial \"best\" value of the metric to be monitored.\n-        Only overwrites the model weights already saved if the\n-        performance of current model is better than this value.\n+      initial_value_threshold: Initial \"best\" value of the metric to be\n+        monitored. Only applies if `save_best_value=True`. Only overwrites\n+        the model weights already saved if the performance of current\n+        model is better than this value.\n       **kwargs: Additional arguments for backwards compatibility. Possible key\n         is `period`.\n   \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1805611c349e017c7b6759344024a395f667d703", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 3386 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -853,13 +853,13 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     mode = 'max'\n     monitor = 'val_acc'\n     initial_value_threshold = 0\n+    save_best_only = True\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n             initial_value_threshold=initial_value_threshold,\n-            save_best_only=True,\n             mode=mode)\n     ]\n     model.fit(\n@@ -877,13 +877,13 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     mode = 'auto'\n     monitor = 'val_loss'\n     initial_value_threshold = None\n+    save_best_only = True\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n             initial_value_threshold=initial_value_threshold,\n-            save_best_only=True,\n             mode=mode)\n     ]\n     model.fit(\n@@ -901,13 +901,13 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     mode = 'min'\n     monitor = 'val_loss'\n     initial_value_threshold = 0\n+    save_best_only = True\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n             initial_value_threshold=initial_value_threshold,\n-            save_best_only=True,\n             mode=mode)\n     ]\n     model.fit(\n@@ -924,13 +924,13 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     mode = 'auto'\n     monitor = 'val_loss'\n     initial_value_threshold = 0\n+    save_best_only = True\n     cbks = [\n         keras.callbacks.ModelCheckpoint(\n             filepath,\n             monitor=monitor,\n             save_best_only=save_best_only,\n             initial_value_threshold=initial_value_threshold,\n-            save_best_only=True,\n             mode=mode)\n     ]\n     model.fit(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6a02df3aa882ae3468ead98829ef177025b3b8ab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 55 | Churn Cumulative: 3225 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -526,7 +526,7 @@ class _EagerVariableStore(tf.Module):\n       layer = create_layer_method()\n       self._layers[name] = layer\n       if isinstance(layer, base_layer.Layer):\n-        self._regularizers[name] = lambda: layer.losses\n+        self._regularizers[name] = lambda: tf.math.reduce_sum(layer.losses)\n     return self._layers[name]\n \n   def add_regularizer(self, var, regularizer):\n\n@@ -1429,7 +1429,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=[\"eager\"]))\n-  def test_get_or_create_layer_eager(self):\n+  def test_get_or_create_layer_with_regularizer_eager(self):\n \n     class NestedLayer(variable_scope_shim.VariableScopeLayer):\n \n@@ -1446,12 +1446,12 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n         return model\n \n       def forward_pass(self, inputs):\n+        # enter a variable scope to check module key naming\n+        with tf.compat.v1.variable_scope(\"test_scope\"):\n           model = variable_scope_shim.get_or_create_layer(\n               \"dense_model\", self.build_model)\n           return model(inputs)\n \n-    # enter a variable scope to check module key naming\n-    with tf.compat.v1.variable_scope(\"test_scope\"):\n     layer = NestedLayer(10)\n     x = tf.ones(shape=(5, 5))\n \n@@ -1471,7 +1471,52 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n     weights = {x.name: x for x in layer.variables}\n     self.assertEqual(weights.keys(), {\"dense/bias:0\", \"dense/kernel:0\"})\n     self.assertAllEqual(out2, tf.ones(shape=(1, 5, 10)) * 5)\n-      self.assertAllEqual(tf.add_n(layer.losses), [0.5])\n+    self.assertAllEqual(layer.losses, [0.5])\n+\n+  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  def test_get_or_create_layer_no_regularizer_eager(self):\n+\n+    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def build_model(self):\n+        inp = input_layer_module.Input(shape=(5, 5))\n+        dense_layer = core.Dense(\n+            10, name=\"dense\",\n+            kernel_initializer=tf.compat.v1.ones_initializer())\n+        model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n+        return model\n+\n+      def forward_pass(self, inputs):\n+        # enter a variable scope to check module key naming\n+        with tf.compat.v1.variable_scope(\"test_scope\"):\n+          model = variable_scope_shim.get_or_create_layer(\n+              \"dense_model\", self.build_model)\n+          return model(inputs)\n+\n+    layer = NestedLayer(10)\n+    x = tf.ones(shape=(5, 5))\n+\n+    out1 = layer(tf.expand_dims(x, 0))\n+\n+    model1 = layer.submodules[0]._layers[\"test_scope/dense_model\"]\n+\n+    out2 = layer(tf.expand_dims(x, 0))\n+    # Verify model produces same output on successive calls with same input\n+    self.assertAllEqual(out1, out2)\n+\n+    # Verify the model used on subsequent calls is the same\n+    model2 = layer.submodules[0]._layers[\"test_scope/dense_model\"]\n+    self.assertIs(model1, model2)\n+\n+    # Verify that stored layer computes outputs and losses correctly\n+    weights = {x.name: x for x in layer.variables}\n+    self.assertEqual(weights.keys(), {\"dense/bias:0\", \"dense/kernel:0\"})\n+    self.assertAllEqual(out2, tf.ones(shape=(1, 5, 10)) * 5)\n+    self.assertAllEqual(layer.losses, [0.0])\n \n   @combinations.generate(combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_tf_function(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8465b80c4ce70f5829418409e59a4f1bc2947cae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 4 | Files Changed: 6 | Hunks: 23 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 109 | Churn Cumulative: 2473 | Contributors (this commit): 3 | Commits (past 90d): 54 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,6 +55,9 @@ class Adadelta(optimizer.Optimizer):\n       attributes related to exponenatial moving average, such as `use_ema` (a\n       boolean field indicates if EMA is used) and EMA momentum. Default to None\n       (not applying EMA).\n+    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+        acceleration. `jit_compile` can only be False when using Parameter\n+        Server Strategy.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adadelta\"`.\n     **kwargs: keyword arguments only used for backward compatibility with\n@@ -71,11 +74,13 @@ class Adadelta(optimizer.Optimizer):\n                epsilon=1e-7,\n                gradients_clip_option=None,\n                ema_option=None,\n+               jit_compile=False,\n                name='Adadelta',\n                **kwargs):\n     super(Adadelta, self).__init__(\n         gradients_clip_option=gradients_clip_option,\n         ema_option=ema_option,\n+        jit_compile=jit_compile,\n         name=name,\n         **kwargs)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n\n@@ -49,6 +49,9 @@ class Adagrad(optimizer.Optimizer):\n       attributes related to exponenatial moving average, such as `use_ema` (a\n       boolean field indicates if EMA is used) and EMA momentum. Default to None\n       (not applying EMA).\n+    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+        acceleration. `jit_compile` can only be False when using Parameter\n+        Server Strategy.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adagrad\"`.\n     **kwargs: keyword arguments only used for backward compatibility with\n@@ -66,16 +69,18 @@ class Adagrad(optimizer.Optimizer):\n                epsilon=1e-7,\n                gradients_clip_option=None,\n                ema_option=None,\n+               jit_compile=False,\n                name='Adagrad',\n                **kwargs):\n     super(Adagrad, self).__init__(\n         gradients_clip_option=gradients_clip_option,\n         ema_option=ema_option,\n+        jit_compile=jit_compile,\n         name=name,\n         **kwargs)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.initial_accumulator_value = initial_accumulator_value\n-    self.epsilon = 1e-7\n+    self.epsilon = epsilon\n \n   def build(self, var_list):\n     super().build(var_list)\n\n@@ -58,6 +58,9 @@ class Adam(optimizer.Optimizer):\n       attributes related to exponenatial moving average, such as `use_ema` (a\n       boolean field indicates if EMA is used) and EMA momentum. Default to None\n       (not applying EMA).\n+    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+      acceleration. `jit_compile` can only be False when using Parameter\n+      Server Strategy.\n     name: Optional name for the operations created when applying gradients.\n       Defaults to `\"Adam\"`.\n     **kwargs: keyword arguments only used for backward compatibility with\n@@ -96,12 +99,14 @@ class Adam(optimizer.Optimizer):\n                amsgrad=False,\n                gradients_clip_option=None,\n                ema_option=None,\n+               jit_compile=False,\n                name='Adam',\n                **kwargs):\n     super(Adam, self).__init__(\n         name=name,\n         gradients_clip_option=gradients_clip_option,\n         ema_option=ema_option,\n+        jit_compile=jit_compile,\n         **kwargs)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.beta_1 = beta_1\n\n@@ -35,6 +35,7 @@ class _BaseOptimizer(tf.Module):\n                name,\n                gradients_clip_option=None,\n                ema_option=None,\n+               jit_compile=False,\n                **kwargs):\n     \"\"\"Create a new Optimizer.\n \n@@ -49,6 +50,9 @@ class _BaseOptimizer(tf.Module):\n         attributes related to exponenatial moving average, such as use_ema (a\n         boolean field indicates if EMA is used) and EMA momentum. Default to\n         None (not applying EMA).\n+      jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+        acceleration. `jit_compile` can only be False when using Parameter\n+        Server Strategy.\n       **kwargs: keyword arguments only used for backward compatibility with\n         `optimizer_v2.OptimizerV2`. Any new code using\n         `optimizer_experimental.Optimizer` should leave this parameter empty.\n@@ -56,6 +60,7 @@ class _BaseOptimizer(tf.Module):\n     self._name = name\n     self._gradients_clip_option = gradients_clip_option\n     self._ema_option = ema_option\n+    self._jit_compile = jit_compile\n \n     with tf.init_scope():\n       # Lift the variable creation to init scope to avoid environment issue.\n@@ -365,8 +370,11 @@ class _BaseOptimizer(tf.Module):\n     Args:\n       grads_and_vars: List of (gradient, variable) pairs.\n     \"\"\"\n+    update_step = self.update_step\n+    if self._jit_compile:\n+      update_step = tf.function(update_step, jit_compile=True)\n     for grad, var in grads_and_vars:\n-      self.update_step(grad, var)\n+      update_step(grad, var)\n     self.iterations.assign_add(1)\n \n   def _update_model_variables_moving_average(self, var_list):\n@@ -469,6 +477,7 @@ class Optimizer(_BaseOptimizer):\n                name,\n                gradients_clip_option=None,\n                ema_option=None,\n+               jit_compile=False,\n                **kwargs):\n     \"\"\"Create a new Optimizer.\n \n@@ -483,11 +492,15 @@ class Optimizer(_BaseOptimizer):\n         attributes related to exponenatial moving average, such as `use_ema` (a\n         boolean field indicates if EMA is used) and EMA momentum. Default to\n         None (not applying EMA).\n+      jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+        acceleration. `jit_compile` can only be False when using Parameter\n+        Server Strategy.\n       **kwargs: keyword arguments only used for backward compatibility with\n         `optimizer_v2.OptimizerV2`. Any new code using\n         `optimizer_experimental.Optimizer` should leave this parameter empty.\n     \"\"\"\n-    super().__init__(name, gradients_clip_option, ema_option, **kwargs)\n+    super().__init__(name, gradients_clip_option, ema_option, jit_compile,\n+                     **kwargs)\n     self._distribution_strategy = tf.distribute.get_strategy()\n \n   def add_variable_from_reference(self,\n@@ -583,7 +596,10 @@ class Optimizer(_BaseOptimizer):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n \n     def apply_grad_to_update_var(var, grad):\n-      return self.update_step(grad, var)\n+      update_step = self.update_step\n+      if self._jit_compile:\n+        update_step = tf.function(update_step, jit_compile=True)\n+      return update_step(grad, var)\n \n     for grad, var in grads_and_vars:\n       distribution.extended.update(\n\n@@ -375,6 +375,71 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n         train_step(ds)\n     self.assertEqual(self.evaluate(optimizer.iterations), 3)\n \n+  @ds_combinations.generate(\n+      tf.__internal__.test.combinations.combine(strategy=[\n+          ds_combinations.mirrored_strategy_with_two_gpus,\n+          ds_combinations.tpu_strategy,\n+          ds_combinations.multi_worker_mirrored_2x2_gpu,\n+          ds_combinations.central_storage_strategy_with_two_gpus,\n+      ]))\n+  def testJitCompile(self, strategy):\n+    # Test the optimizer yields same numerical results when jit_compile is\n+    # on and off.\n+    with strategy.scope():\n+      optimizer_1 = adam_new.Adam(\n+          ema_option=optimizer_lib.EMAOption(\n+              use_ema=True, ema_overwrite_frequency=1))\n+      optimizer_2 = adam_new.Adam(\n+          jit_compile=True,\n+          ema_option=optimizer_lib.EMAOption(\n+              use_ema=True, ema_overwrite_frequency=1))\n+      model_1 = keras.Sequential([\n+          keras.layers.Input(shape=(2,)),\n+          keras.layers.Dense(5),\n+          keras.layers.Dense(1)\n+      ])\n+      model_2 = keras.models.clone_model(model_1)\n+      model_2.set_weights(model_1.get_weights())\n+\n+      def per_worker_dataset_fn():\n+\n+        def dataset_fn(_):\n+          x = np.random.rand(6, 2)\n+          y = [1, 1, 1, 0, 0, 0]\n+          ds = tf.data.Dataset.from_tensor_slices((x, y))\n+          ds = ds.repeat().batch(6)\n+          return ds\n+\n+        return strategy.distribute_datasets_from_function(dataset_fn)\n+\n+      ds = per_worker_dataset_fn()\n+\n+      @tf.function\n+      def train_step(ds):\n+\n+        def replica_fn(data):\n+          features, labels = data\n+          with tf.GradientTape() as tape:\n+            output_1 = model_1(features)\n+            loss_1 = keras.losses.MeanSquaredError(\n+                reduction=losses_utils.ReductionV2.NONE)(labels, output_1)\n+          grads_1 = tape.gradient(loss_1, model_1.trainable_variables)\n+          optimizer_1.apply_gradients(zip(grads_1, model_1.trainable_variables))\n+\n+          with tf.GradientTape() as tape:\n+            output_2 = model_2(features)\n+            loss_2 = keras.losses.MeanSquaredError(\n+                reduction=losses_utils.ReductionV2.NONE)(labels, output_2)\n+          grads_2 = tape.gradient(loss_2, model_2.trainable_variables)\n+          optimizer_2.apply_gradients(zip(grads_2, model_2.trainable_variables))\n+\n+        strategy.run(replica_fn, args=(next(iter(ds)),))\n+\n+      for _ in range(3):\n+        train_step(ds)\n+        self.assertAllClose(model_1.trainable_variables[0][0],\n+                            model_2.trainable_variables[0][0])\n+\n \n if __name__ == \"__main__\":\n   tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -62,6 +62,9 @@ class SGD(optimizer.Optimizer):\n       attributes related to exponenatial moving average, such as use_ema (a\n       boolean field indicates if EMA is used) and EMA momentum. Default to None\n       (not apply EMA).\n+    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+        acceleration. `jit_compile` can only be False when using Parameter\n+        Server Strategy.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"SGD\"`.\n     **kwargs: keyword arguments only used for backward compatibility with\n@@ -105,12 +108,14 @@ class SGD(optimizer.Optimizer):\n                amsgrad=False,\n                gradients_clip_option=None,\n                ema_option=None,\n+               jit_compile=False,\n                name='SGD',\n                **kwargs):\n     super(SGD, self).__init__(\n         name=name,\n         gradients_clip_option=gradients_clip_option,\n         ema_option=ema_option,\n+        jit_compile=jit_compile,\n         **kwargs)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n     self.momentum = momentum\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#375bd08a9811836435562d1546b984fdec4db59d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 24 | Churn Cumulative: 325 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"Base class for testing saving/loading with DS.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import os\n \n from absl.testing import parameterized\n-import numpy as np\n-from keras import backend\n from keras.distribute import model_combinations\n+import numpy as np\n+\n+import tensorflow.compat.v2 as tf\n \n _RANDOM_SEED = 1337\n _DEFAULT_FUNCTION_KEY = 'serving_default'\n@@ -54,13 +53,6 @@ strategies = [\n ]\n \n \n-def get_tolerance(save_distribution, restore_distribution):\n-  if backend.is_tpu_strategy(save_distribution) or backend.is_tpu_strategy(\n-      restore_distribution):\n-    return _TPU_TOLERANCE\n-  return _TOLERANCE\n-\n-\n def simple_models_with_strategies():\n   return tf.__internal__.test.combinations.combine(\n       model_and_input=simple_models,\n@@ -194,8 +186,7 @@ class TestSavedModelBase(tf.test.TestCase, parameterized.TestCase):\n           saved_dir=saved_dir,\n           predict_dataset=predict_dataset)\n \n-    tolerance = get_tolerance(None, distribution)\n-    self.assertAllClose(result_before_save, result_after_save, atol=tolerance)\n+    self.assertAllClose(result_before_save, result_after_save)\n \n   def run_test_save_strategy_restore_no_strategy(self, model_and_input,\n                                                  distribution, save_in_scope):\n@@ -224,8 +215,7 @@ class TestSavedModelBase(tf.test.TestCase, parameterized.TestCase):\n         saved_dir=saved_dir,\n         predict_dataset=predict_dataset)\n \n-    tolerance = get_tolerance(distribution, None)\n-    self.assertAllClose(result_before_save, load_result, atol=tolerance)\n+    self.assertAllClose(result_before_save, load_result)\n \n   def run_test_save_strategy_restore_strategy(self, model_and_input,\n                                               distribution_for_saving,\n@@ -257,9 +247,7 @@ class TestSavedModelBase(tf.test.TestCase, parameterized.TestCase):\n           saved_dir=saved_dir,\n           predict_dataset=predict_dataset)\n \n-    tolerance = get_tolerance(distribution_for_saving,\n-                              distribution_for_restoring)\n-    self.assertAllClose(result_before_save, load_result, atol=tolerance)\n+    self.assertAllClose(result_before_save, load_result)\n \n   def run_test_save_strategy(self, model_and_input,\n                              distribution, save_in_scope):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d2993b78603bda911f0a4912d08b1e5f36eb633", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 16304 | Contributors (this commit): 118 | Commits (past 90d): 18 | Contributors (cumulative): 118 | DMM Complexity: None\n\nDIFF:\n@@ -3093,12 +3093,12 @@ def reduce_per_replica(values, strategy, reduction='first'):\n     if not _is_per_replica_instance(v):\n       return v\n     elif reduction == 'first':\n-      return strategy.unwrap(v)[0]\n+      return strategy.experimental_local_results(v)[0]\n     elif reduction == 'concat':\n       if _is_tpu_multi_host(strategy):\n         return _tpu_multi_host_concat(v, strategy)\n       else:\n-        return concat(strategy.unwrap(v))\n+        return concat(strategy.experimental_local_results(v))\n     else:\n       raise ValueError('`reduction` must be \"first\" or \"concat\". Received: '\n                        f'reduction={reduction}.')\n@@ -3120,11 +3120,11 @@ def _is_tpu_multi_host(strategy):\n \n def _tpu_multi_host_concat(v, strategy):\n   \"\"\"Correctly order TPU PerReplica objects.\"\"\"\n-  replicas = strategy.unwrap(v)\n+  replicas = strategy.experimental_local_results(v)\n   # When distributed datasets are created from Tensors / NumPy,\n   # TPUStrategy.experimental_distribute_dataset shards data in\n-  # (Replica, Host) order, and TPUStrategy.unwrap returns it in\n-  # (Host, Replica) order.\n+  # (Replica, Host) order, and TPUStrategy.experimental_local_results returns\n+  # it in (Host, Replica) order.\n   # TODO(b/150317897): Figure out long-term plan here.\n   num_replicas_per_host = strategy.extended.num_replicas_per_host\n   ordered_replicas = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d2f4f7031c2df887fa9e17cc63c561b5e85eb630", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 32 | Churn Cumulative: 2170 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -421,9 +421,7 @@ class GRU(recurrent.DropoutRNNCellMixin,\n     input_shape = backend.int_shape(inputs)\n     timesteps = input_shape[0] if self.time_major else input_shape[1]\n \n-    # TODO(b/156447398) Investigate why the cuDNN kernel fails with ragged\n-    # inputs.\n-    if is_ragged_input or not self._could_use_gpu_kernel:\n+    if not self._could_use_gpu_kernel:\n       kwargs = {'training': training}\n       self._maybe_reset_cell_dropout_mask(self.cell)\n \n@@ -616,7 +614,10 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n             go_backwards, sequence_lengths):\n   \"\"\"GRU with cuDNN implementation which is only available for GPU.\"\"\"\n-  if not time_major and mask is None:\n+  if mask is not None:\n+    sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n+\n+  if not time_major and sequence_lengths is None:\n     inputs = tf.transpose(inputs, perm=(1, 0, 2))\n     seq_axis, batch_axis = (0, 1)\n   else:\n@@ -649,9 +650,6 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n       shape=tf.constant([-1]),\n       transpose_weights=True)\n \n-  if mask is not None:\n-    sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n-\n   if sequence_lengths is not None:\n     if go_backwards:\n       # Three reversals are required. E.g.,\n@@ -683,7 +681,7 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n         is_training=True, rnn_mode='gru')\n \n   last_output = outputs[-1]\n-  if not time_major and mask is None:\n+  if not time_major and sequence_lengths is None:\n     outputs = tf.transpose(outputs, perm=[1, 0, 2])\n   h = tf.squeeze(h, axis=seq_axis)\n \n@@ -693,7 +691,7 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n   # get the final effect output instead just 0s at the last timestep.\n   # In order to mimic the default keras behavior, we copy the final h state as\n   # the last_output, since it is numerically same as the output.\n-  if mask is not None:\n+  if sequence_lengths is not None:\n     last_output = h\n \n   return last_output, outputs, h, _runtime(_RUNTIME_GPU)\n@@ -1150,9 +1148,7 @@ class LSTM(recurrent.DropoutRNNCellMixin,\n     input_shape = backend.int_shape(inputs)\n     timesteps = input_shape[0] if self.time_major else input_shape[1]\n \n-    # TODO(b/156447398) Investigate why the cuDNN kernel fails with ragged\n-    # inputs.\n-    if is_ragged_input or not self._could_use_gpu_kernel:\n+    if not self._could_use_gpu_kernel:\n       # Fall back to use the normal LSTM.\n       kwargs = {'training': training}\n       self._maybe_reset_cell_dropout_mask(self.cell)\n@@ -1434,7 +1430,10 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n     runtime: Constant string tensor which indicate real runtime hardware. This\n       value is for testing purpose and should not be used by user.\n   \"\"\"\n-  if not time_major and mask is None:\n+  if mask is not None:\n+    sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n+\n+  if not time_major and sequence_lengths is None:\n     inputs = tf.transpose(inputs, perm=(1, 0, 2))\n     seq_axis, batch_axis = (0, 1)\n   else:\n@@ -1469,9 +1468,6 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n       shape=tf.constant([-1]),\n       transpose_weights=True)\n \n-  if mask is not None:\n-    sequence_lengths = calculate_sequence_by_mask(mask, time_major)\n-\n   if sequence_lengths is not None:\n     if go_backwards:\n       # Three reversals are required. E.g.,\n@@ -1506,7 +1502,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n         is_training=True, rnn_mode='lstm')\n \n   last_output = outputs[-1]\n-  if not time_major and mask is None:\n+  if not time_major and sequence_lengths is None:\n     outputs = tf.transpose(outputs, perm=[1, 0, 2])\n   h = tf.squeeze(h, axis=seq_axis)\n   c = tf.squeeze(c, axis=seq_axis)\n@@ -1517,7 +1513,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n   # get the final effect output instead just 0s at the last timestep.\n   # In order to mimic the default keras behavior, we copy the final h state as\n   # the last_output, since it is numerically same as the output.\n-  if mask is not None:\n+  if sequence_lengths is not None:\n     last_output = h\n   return last_output, outputs, h, c, _runtime(_RUNTIME_GPU)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b057eff257dd49d8d6f931cbf72600ca2df06e2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 970 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -62,12 +62,15 @@ class _BaseOptimizer(tf.Module):\n     self._ema_option = ema_option\n     self._jit_compile = jit_compile\n \n+    self._create_iteration_variable()\n+    self._process_kwargs(kwargs)\n+\n+  def _create_iteration_variable(self):\n+    \"\"\"Create the iterations counter variable.\"\"\"\n     with tf.init_scope():\n       # Lift the variable creation to init scope to avoid environment issue.\n       self._iterations = tf.Variable(0, name=\"iteration\", dtype=tf.int64)\n \n-    self._process_kwargs(kwargs)\n-\n   def _process_kwargs(self, kwargs):\n     legacy_gradients_clip_kwargs = {\"clipnorm\", \"clipvalue\", \"global_clipnorm\"}\n     other_legacy_kwargs = {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#892813fe685989046ddb8d4b83eec223079844b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 1679 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -34,21 +34,21 @@ from keras.utils import layer_utils\n from keras.utils import data_utils\n from tensorflow.python.util.tf_export import keras_export\n \n-BASE_WEIGHTS_PATH = \"\"\n+BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/keras-applications/\"\n \n WEIGHTS_HASHES = {\n-    \"x002\": (\"\", \"\"),\n-    \"x004\": (\"\", \"\"),\n-    \"x006\": (\"\", \"\"),\n-    \"x008\": (\"\", \"\"),\n-    \"x016\": (\"\", \"\"),\n-    \"x032\": (\"\", \"\"),\n-    \"x040\": (\"\", \"\"),\n-    \"x064\": (\"\", \"\"),\n-    \"x080\": (\"\", \"\"),\n-    \"x120\": (\"\", \"\"),\n-    \"x160\": (\"\", \"\"),\n-    \"x320\": (\"\", \"\"),\n+    \"x002\": (\"49fb46e56cde07fdaf57bffd851461a86548f6a3a4baef234dd37290b826c0b8\", \"5445b66cd50445eb7ecab094c1e78d4d3d29375439d1a7798861c4af15ffff21\"),\n+    \"x004\": (\"3523c7f5ac0dbbcc2fd6d83b3570e7540f7449d3301cc22c29547302114e4088\", \"de139bf07a66c9256f2277bf5c1b6dd2d5a3a891a5f8a925a10c8a0a113fd6f3\"),\n+    \"x006\": (\"340216ef334a7bae30daac9f414e693c136fac9ab868704bbfcc9ce6a5ec74bb\", \"a43ec97ad62f86b2a96a783bfdc63a5a54de02eef54f26379ea05e1bf90a9505\"),\n+    \"x008\": (\"8f145d6a5fae6da62677bb8d26eb92d0b9dfe143ec1ebf68b24a57ae50a2763d\", \"3c7e4b0917359304dc18e644475c5c1f5e88d795542b676439c4a3acd63b7207\"),\n+    \"x016\": (\"31c386f4c7bfef4c021a583099aa79c1b3928057ba1b7d182f174674c5ef3510\", \"1b8e3d545d190271204a7b2165936a227d26b79bb7922bac5ee4d303091bf17a\"),\n+    \"x032\": (\"6c025df1409e5ea846375bc9dfa240956cca87ef57384d93fef7d6fa90ca8c7f\", \"9cd4522806c0fcca01b37874188b2bd394d7c419956d77472a4e072b01d99041\"),\n+    \"x040\": (\"ba128046c588a26dbd3b3a011b26cb7fa3cf8f269c184c132372cb20b6eb54c1\", \"b4ed0ca0b9a98e789e05000e830403a7ade4d8afa01c73491c44610195198afe\"),\n+    \"x064\": (\"0f4489c3cd3ad979bd6b0324213998bcb36dc861d178f977997ebfe53c3ba564\", \"3e706fa416a18dfda14c713423eba8041ae2509db3e0a611d5f599b5268a46c4\"),\n+    \"x080\": (\"76320e43272719df648db37271a247c22eb6e810fe469c37a5db7e2cb696d162\", \"7b1ce8e29ceefec10a6569640ee329dba7fbc98b5d0f6346aabade058b66cf29\"),\n+    \"x120\": (\"5cafc461b78897d5e4f24e68cb406d18e75f31105ef620e7682b611bb355eb3a\", \"36174ddd0299db04a42631d028abcb1cc7afec2b705e42bd28fcd325e5d596bf\"),\n+    \"x160\": (\"8093f57a5824b181fb734ea21ae34b1f7ee42c5298e63cf6d587c290973195d2\", \"9d1485050bdf19531ffa1ed7827c75850e0f2972118a996b91aa9264b088fd43\"),\n+    \"x320\": (\"91fb3e6f4e9e44b3687e80977f7f4412ee9937c0c704232664fc83e4322ea01e\", \"9db7eacc37b85c98184070e1a172e6104c00846f44bcd4e727da9e50d9692398\"),\n     \"y002\": (\"\", \"\"),\n     \"y004\": (\"\", \"\"),\n     \"y006\": (\"\", \"\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
