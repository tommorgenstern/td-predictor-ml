{"custom_id": "keras#409d74c11bacef33f7636f9089c8b316dd3cfcd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 622 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,6 @@ from keras.engine.training import Model\n from tensorflow.python import tf2\n from tensorflow.python.util.tf_export import keras_export\n \n-__version__ = \"2.13.0\"\n+__version__ = \"2.14.0\"\n \n keras_export(\"keras.__version__\").export_constant(__name__, \"__version__\")\n\n@@ -31,7 +31,7 @@ DOCLINES = __doc__.split(\"\\n\")\n # This version string is semver compatible, but incompatible with pip.\n # For pip, we will remove all '-' characters from this string, and use the\n # result for pip.\n-_VERSION = \"2.13.0\"\n+_VERSION = \"2.14.0\"\n \n REQUIRED_PACKAGES = [\n     # We depend on TensorFlow's declared pip dependencies.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7629ffc8ca70f6e028f1e7b33b4a342ff2efe96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 44 | Files Changed: 3 | Hunks: 12 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 95 | Churn Cumulative: 8318 | Contributors (this commit): 24 | Commits (past 90d): 26 | Contributors (cumulative): 33 | DMM Complexity: 0.2\n\nDIFF:\n@@ -1264,6 +1264,12 @@ class LossScaleOptimizerV3(\n     def apply_gradients(\n         self, grads_and_vars, skip_gradients_aggregation=False, **kwargs\n     ):\n+        grads_and_vars = list(grads_and_vars)\n+        grads, trainable_variables = zip(*grads_and_vars)\n+        with tf.init_scope():\n+            # Lift variable creation to init scope to avoid environment\n+            # issues.\n+            self.build(trainable_variables)\n         if tf.distribute.in_cross_replica_context():\n             raise ValueError(\n                 \"apply_gradients() must be called in a replica context.\"\n\n@@ -685,31 +685,21 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     def _update_model_variables_moving_average(self, var_list):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n         if self.use_ema:\n-            for var, average in zip(\n-                var_list, self._model_variables_moving_average\n-            ):\n+            for var in var_list:\n+                average = self._model_variables_moving_average[\n+                    self._index_dict[self._var_key(var)]\n+                ]\n                 average.assign(\n                     self.ema_momentum * average + (1 - self.ema_momentum) * var\n                 )\n \n     def _overwrite_model_variables_with_average_value(self, var_list):\n         \"\"\"Overwrite model variables with its moving average.\"\"\"\n-        if len(var_list) != len(self._model_variables_moving_average):\n-            raise ValueError(\n-                f\"The length of model variables ({len(var_list)}) to \"\n-                \"override does not match the length of model variables \"\n-                \"stored in the optimizer \"\n-                f\"({len(self._model_variables_moving_average)}). Please \"\n-                \"check if the optimizer was called on your model.\"\n-            )\n-        self._overwrite_model_variables_with_average_value_helper(var_list)\n-\n-    def _overwrite_model_variables_with_average_value_helper(self, var_list):\n-        \"\"\"Helper function that overwrites model variables.\"\"\"\n-        for var, average_var in zip(\n-            var_list, self._model_variables_moving_average\n-        ):\n-            var.assign(average_var)\n+        for var in var_list:\n+            average = self._model_variables_moving_average[\n+                self._index_dict[self._var_key(var)]\n+            ]\n+            var.assign(average)\n \n     def finalize_variable_values(self, var_list):\n         \"\"\"Set the final value of model's trainable variables.\n@@ -1263,8 +1253,8 @@ class Optimizer(_BaseOptimizer):\n             grads_and_vars,\n         )\n \n-    def _overwrite_model_variables_with_average_value_helper(self, var_list):\n-        \"\"\"Helper function to _overwrite_model_variables_with_average_value.\n+    def _overwrite_model_variables_with_average_value(self, var_list):\n+        \"\"\"Overwrite model variables with their moving average values.\n \n         This function overwrites variables on each device.\n         Args:\n@@ -1272,17 +1262,16 @@ class Optimizer(_BaseOptimizer):\n         \"\"\"\n         if self._mesh or self._run_with_dtensor:\n             # Skip any usage of strategy logic for DTensor\n-            super()._overwrite_model_variables_with_average_value_helper(\n-                var_list\n-            )\n+            super()._overwrite_model_variables_with_average_value(var_list)\n \n         strategy = self._distribution_strategy\n         # Override model variable by the stored average value on all devices.\n-        for var, average_var in zip(\n-            var_list, self._model_variables_moving_average\n-        ):\n+        for var in var_list:\n+            average = self._model_variables_moving_average[\n+                self._index_dict[self._var_key(var)]\n+            ]\n             strategy.extended.update(\n-                var, lambda a, b: a.assign(b), args=(average_var,)\n+                var, lambda a, b: a.assign(b), args=(average,)\n             )\n \n     def _build_learning_rate(self, learning_rate):\n@@ -1330,9 +1319,10 @@ class Optimizer(_BaseOptimizer):\n                     self.ema_momentum * average + (1 - self.ema_momentum) * var\n                 )\n \n-            for var, average in zip(\n-                var_list, self._model_variables_moving_average\n-            ):\n+            for var in var_list:\n+                average = self._model_variables_moving_average[\n+                    self._index_dict[self._var_key(var)]\n+                ]\n                 self._distribution_strategy.extended.update(\n                     average, update_average, args=(var,), group=False\n                 )\n\n@@ -337,22 +337,33 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n             ema_overwrite_frequency=3,\n         )\n \n-        var1, var2 = tf.Variable(2.0), tf.Variable(2.0)\n+        # `var2` does not produce gradients.\n+        var1, var2, var3 = tf.Variable(2.0), tf.Variable(2.0), tf.Variable(2.0)\n         with tf.GradientTape() as tape:\n-            loss = var1 + var2\n-        grads = tape.gradient(loss, [var1, var2])\n-        # First iteration: [var1, var2] = [1.0, 1.0]\n-        optimizer.apply_gradients(zip(grads, [var1, var2]))\n-        self.assertAllEqual([var1.numpy(), var2.numpy()], [1.0, 1.0])\n+            loss = var1 + var3\n+        grads = tape.gradient(loss, [var1, var2, var3])\n+        # First iteration: [var1, var2, var3] = [1.0, 2.0, 1.0]\n+        optimizer.apply_gradients(zip(grads, [var1, var2, var3]))\n+        self.assertAllEqual(\n+            [var1.numpy(), var2.numpy(), var3.numpy()],\n+            [1.0, 2.0, 1.0],\n+        )\n \n-        # Second iteration: [var1, var2] = [0.0, 0.0]\n-        optimizer.apply_gradients(zip(grads, [var1, var2]))\n-        self.assertAllEqual([var1.numpy(), var2.numpy()], [0.0, 0.0])\n+        # Second iteration: [var1, var2, var3] = [0.0, 2.0, 0.0]\n+        optimizer.apply_gradients(zip(grads, [var1, var2, var3]))\n+        self.assertAllEqual(\n+            [var1.numpy(), var2.numpy(), var3.numpy()],\n+            [0.0, 2.0, 0.0],\n+        )\n \n-        # Third iteration, without EMA, we should see [var1, var2] = [-1.0,\n-        # -1.0], but overwriting results in [var1, var2] = [-0.125, -0.125].\n-        optimizer.apply_gradients(zip(grads, [var1, var2]))\n-        self.assertAllEqual([var1.numpy(), var2.numpy()], [-0.125, -0.125])\n+        # Third iteration, without EMA, we should see [var1, var2, var3] =\n+        # [-1.0, 2.0 -1.0], but overwriting results in [var1, var2] =\n+        # [-0.125, 2.0, -0.125].\n+        optimizer.apply_gradients(zip(grads, [var1, var2, var3]))\n+        self.assertAllEqual(\n+            [var1.numpy(), var2.numpy(), var3.numpy()],\n+            [-0.125, 2.0, -0.125],\n+        )\n \n     def testGetAndFromConfig(self):\n         class CustomLRSchedule(learning_rate_schedule.LearningRateSchedule):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c79e431e1709c47bc19eb7cb64012f907155ecaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1055 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -480,7 +480,7 @@ if __name__ == \"__main__\":\n             f\"dist_directory={dist_directory}\\n\"\n             f\"package_directory={package_directory}\\n\"\n             f\"src_directory={src_directory}\\n\"\n-            f\"is_nightly={is_nightly}\"\n+            f\"is_nightly={is_nightly}\\n\"\n             f\"rc={rc}\"\n         )\n     if os.path.exists(build_directory):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#841b8d702d9518c751c59f08226d19085dac79f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1142 | Lines Deleted: 56 | Files Changed: 15 | Hunks: 20 | Methods Changed: 28 | Complexity Δ (Sum/Max): 32/12 | Churn Δ: 1198 | Churn Cumulative: 5507 | Contributors (this commit): 8 | Commits (past 90d): 58 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,6 +69,36 @@ def log_softmax(x, axis=None):\n     return tf.nn.log_softmax(x, axis=axis)\n \n \n+def _transpose_spatial_inputs(inputs):\n+    num_spatial_dims = len(inputs.shape) - 2\n+    # Tensorflow pooling does not support `channels_first` format, so\n+    # we need to transpose to `channels_last` format.\n+    if num_spatial_dims == 1:\n+        inputs = tf.transpose(inputs, (0, 2, 1))\n+    elif num_spatial_dims == 2:\n+        inputs = tf.transpose(inputs, (0, 2, 3, 1))\n+    elif num_spatial_dims == 3:\n+        inputs = tf.transpose(inputs, (0, 2, 3, 4, 1))\n+    else:\n+        raise ValueError(\n+            \"Pooling inputs's shape must be 3, 4 or 5, corresponding to 1D, 2D \"\n+            f\"and 3D inputs. But received shape: {inputs.shape}.\"\n+        )\n+    return inputs\n+\n+\n+def _transpose_spatial_outputs(outputs):\n+    # Undo the tranpose in `_transpose_spatial_inputs`.\n+    num_spatial_dims = len(outputs.shape) - 2\n+    if num_spatial_dims == 1:\n+        outputs = tf.transpose(outputs, (0, 2, 1))\n+    elif num_spatial_dims == 2:\n+        outputs = tf.transpose(outputs, (0, 3, 1, 2))\n+    elif num_spatial_dims == 3:\n+        outputs = tf.transpose(outputs, (0, 4, 1, 2, 3))\n+    return outputs\n+\n+\n def max_pool(\n     inputs,\n     pool_size,\n@@ -78,8 +108,22 @@ def max_pool(\n ):\n     strides = pool_size if strides is None else strides\n     padding = padding.upper()\n-    data_format = _convert_data_format(data_format, len(inputs.shape))\n-    return tf.nn.max_pool(inputs, pool_size, strides, padding, data_format)\n+    tf_data_format = _convert_data_format(\"channels_last\", len(inputs.shape))\n+    if data_format == \"channels_first\":\n+        # Tensorflow pooling does not support `channels_first` format, so\n+        # we need to transpose to `channels_last` format.\n+        inputs = _transpose_spatial_inputs(inputs)\n+\n+    outputs = tf.nn.max_pool(\n+        inputs,\n+        pool_size,\n+        strides,\n+        padding,\n+        tf_data_format,\n+    )\n+    if data_format == \"channels_first\":\n+        outputs = _transpose_spatial_outputs(outputs)\n+    return outputs\n \n \n def average_pool(\n@@ -91,8 +135,22 @@ def average_pool(\n ):\n     strides = pool_size if strides is None else strides\n     padding = padding.upper()\n-    data_format = _convert_data_format(data_format, len(inputs.shape))\n-    return tf.nn.avg_pool(inputs, pool_size, strides, padding, data_format)\n+    tf_data_format = _convert_data_format(\"channels_last\", len(inputs.shape))\n+    if data_format == \"channels_first\":\n+        # Tensorflow pooling does not support `channels_first` format, so\n+        # we need to transpose to `channels_last` format.\n+        inputs = _transpose_spatial_inputs(inputs)\n+\n+    outputs = tf.nn.avg_pool(\n+        inputs,\n+        pool_size,\n+        strides,\n+        padding,\n+        tf_data_format,\n+    )\n+    if data_format == \"channels_first\":\n+        outputs = _transpose_spatial_outputs(outputs)\n+    return outputs\n \n \n def _convert_data_format(data_format, ndim):\n\n@@ -8,6 +8,12 @@ from keras_core.layers.merging.add import Add\n from keras_core.layers.merging.add import add\n from keras_core.layers.merging.subtract import Subtract\n from keras_core.layers.merging.subtract import subtract\n+from keras_core.layers.pooling.average_pooling1d import AveragePooling1D\n+from keras_core.layers.pooling.average_pooling2d import AveragePooling2D\n+from keras_core.layers.pooling.average_pooling3d import AveragePooling3D\n+from keras_core.layers.pooling.max_pooling1d import MaxPooling1D\n+from keras_core.layers.pooling.max_pooling2d import MaxPooling2D\n+from keras_core.layers.pooling.max_pooling3d import MaxPooling3D\n from keras_core.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n\n\n@@ -0,0 +1,92 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_pooling import BasePooling\n+\n+\n+@keras_core_export(\n+    [\"keras_core.layers.AveragePooling1D\", \"keras_core.layers.AvgPool1D\"]\n+)\n+class AveragePooling1D(BasePooling):\n+    \"\"\"Average pooling for temporal data.\n+\n+    Downsamples the input representation by taking the average value over the\n+    window defined by `pool_size`. The window is shifted by `strides`.  The\n+    resulting output when using \"valid\" padding option has a shape of:\n+    `output_shape = (input_shape - pool_size + 1) / strides)`\n+\n+    The resulting output shape when using the \"same\" padding option is:\n+    `output_shape = input_shape / strides`\n+\n+    Args:\n+        pool_size: int, size of the max pooling window.\n+        strides: int or None. Specifies how much the pooling window moves\n+            for each pooling step. If None, it will default to `pool_size`.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        3D tensor with shape `(batch_size, steps, features)`.\n+    - If `data_format=\"channels_first\"`:\n+        3D tensor with shape `(batch_size, features, steps)`.\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        3D tensor with shape `(batch_size, downsampled_steps, features)`.\n+    - If `data_format=\"channels_first\"`:\n+        3D tensor with shape `(batch_size, features, downsampled_steps)`.\n+\n+    Examples:\n+\n+    `strides=1` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([1., 2., 3., 4., 5.])\n+    >>> x = np.reshape(x, [1, 5, 1])\n+    >>> avg_pool_1d = keras_core.layers.AveragePooling1D(pool_size=2,\n+    ...    strides=1, padding=\"valid\")\n+    >>> avg_pool_1d(x)\n+\n+    `strides=2` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([1., 2., 3., 4., 5.])\n+    >>> x = np.reshape(x, [1, 5, 1])\n+    >>> avg_pool_1d = keras_core.layers.AveragePooling1D(pool_size=2,\n+    ...    strides=2, padding=\"valid\")\n+    >>> avg_pool_1d(x)\n+\n+    `strides=1` and `padding=\"same\"`:\n+\n+    >>> x = np.array([1., 2., 3., 4., 5.])\n+    >>> x = np.reshape(x, [1, 5, 1])\n+    >>> avg_pool_1d = keras_core.layers.AveragePooling1D(pool_size=2,\n+    ...    strides=1, padding=\"same\")\n+    >>> avg_pool_1d(x)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        padding=\"valid\",\n+        data_format=None,\n+        name=None,\n+        **kwargs\n+    ):\n+        super().__init__(\n+            pool_size,\n+            strides,\n+            pool_dimensions=1,\n+            pool_mode=\"average\",\n+            padding=padding,\n+            data_format=data_format,\n+            name=name,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,109 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_pooling import BasePooling\n+\n+\n+@keras_core_export(\n+    [\"keras_core.layers.AveragePooling2D\", \"keras_core.layers.AvgPool2D\"]\n+)\n+class AveragePooling2D(BasePooling):\n+    \"\"\"Average pooling operation for 2D spatial data.\n+\n+    Downsamples the input along its spatial dimensions (height and width)\n+    by taking the average value over an input window\n+    (of size defined by `pool_size`) for each channel of the input.\n+    The window is shifted by `strides` along each dimension.\n+\n+    The resulting output when using the `\"valid\"` padding option has a spatial\n+    shape (number of rows or columns) of:\n+    `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+    (when `input_shape >= pool_size`)\n+\n+    The resulting output shape when using the `\"same\"` padding option is:\n+    `output_shape = math.floor((input_shape - 1) / strides) + 1`\n+\n+    Args:\n+        pool_size: int or tuple of 2 integers, factors by which to downscale\n+            (dim1, dim2). If only one integer is specified, the same\n+            window length will be used for all dimensions.\n+        strides: int or tuple of 2 integers, or None. Strides values. If None,\n+            it will default to `pool_size`. If only one int is specified, the\n+            same stride size will be used for all dimensions.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, width)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        4D tensor with shape `(batch_size, height, width, channels)`.\n+    - If `data_format=\"channels_first\"`:\n+        4D tensor with shape `(batch_size, channels, height, width)`.\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        4D tensor with shape\n+        `(batch_size, pooled_height, pooled_width, channels)`.\n+    - If `data_format=\"channels_first\"`:\n+        4D tensor with shape\n+        `(batch_size, channels, pooled_height, pooled_width)`.\n+\n+    Examples:\n+\n+    `strides=(1, 1)` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([[1., 2., 3.],\n+    ...               [4., 5., 6.],\n+    ...               [7., 8., 9.]])\n+    >>> x = np.reshape(x, [1, 3, 3, 1])\n+    >>> avg_pool_2d = keras_core.layers.AveragePooling2D(pool_size=(2, 2),\n+    ...    strides=(1, 1), padding=\"valid\")\n+    >>> avg_pool_2d(x)\n+\n+    `strides=(2, 2)` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([[1., 2., 3., 4.],\n+    ...              [5., 6., 7., 8.],\n+    ...              [9., 10., 11., 12.]])\n+    >>> x = np.reshape(x, [1, 3, 4, 1])\n+    >>> avg_pool_2d = keras_core.layers.AveragePooling2D(pool_size=(2, 2),\n+    ...    strides=(2, 2), padding=\"valid\")\n+    >>> avg_pool_2d(x)\n+\n+    `stride=(1, 1)` and `padding=\"same\"`:\n+\n+    >>> x = np.array([[1., 2., 3.],\n+    ...                  [4., 5., 6.],\n+    ...                  [7., 8., 9.]])\n+    >>> x = np.reshape(x, [1, 3, 3, 1])\n+    >>> avg_pool_2d = keras_core.layers.AveragePooling2D(pool_size=(2, 2),\n+    ...    strides=(1, 1), padding=\"same\")\n+    >>> avg_pool_2d(x)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        padding=\"valid\",\n+        data_format=\"channels_last\",\n+        name=None,\n+        **kwargs\n+    ):\n+        super().__init__(\n+            pool_size,\n+            strides,\n+            pool_dimensions=2,\n+            pool_mode=\"average\",\n+            padding=padding,\n+            data_format=data_format,\n+            name=name,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,85 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_pooling import BasePooling\n+\n+\n+@keras_core_export(\n+    [\"keras_core.layers.AveragePooling3D\", \"keras_core.layers.AvgPool3D\"]\n+)\n+class AveragePooling3D(BasePooling):\n+    \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n+\n+    Downsamples the input along its spatial dimensions (depth, height, and\n+    width) by taking the average value over an input window (of size defined by\n+    `pool_size`) for each channel of the input. The window is shifted by\n+    `strides` along each dimension.\n+\n+    Args:\n+        pool_size: int or tuple of 3 integers, factors by which to downscale\n+            (dim1, dim2, dim3). If only one integer is specified, the same\n+            window length will be used for all dimensions.\n+        strides: int or tuple of 3 integers, or None. Strides values. If None,\n+            it will default to `pool_size`. If only one int is specified, the\n+            same stride size will be used for all dimensions.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape\n+            `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)` while\n+            `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+            It defaults to the `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json`. If you never set it, then it\n+            will be `\"channels_last\"`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        5D tensor with shape:\n+        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format=\"channels_first\"`:\n+        5D tensor with shape:\n+        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        5D tensor with shape:\n+        `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n+    - If `data_format=\"channels_first\"`:\n+        5D tensor with shape:\n+        `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n+\n+    Example:\n+\n+    ```python\n+    depth = 30\n+    height = 30\n+    width = 30\n+    channels = 3\n+\n+    inputs = keras_core.layers.Input(shape=(depth, height, width, channels))\n+    layer = keras_core.layers.AveragePooling3D(pool_size=3)\n+    outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        padding=\"valid\",\n+        data_format=\"channels_last\",\n+        name=None,\n+        **kwargs\n+    ):\n+        super().__init__(\n+            pool_size,\n+            strides,\n+            pool_dimensions=3,\n+            pool_mode=\"average\",\n+            padding=padding,\n+            data_format=data_format,\n+            name=name,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,181 @@\n+import numpy as np\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class AveragePoolingBasicTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\", (3, 5, 4), (3, 4, 4)),\n+        (2, 1, \"same\", \"channels_first\", (3, 5, 4), (3, 5, 4)),\n+        ((2,), (2,), \"valid\", \"channels_last\", (3, 5, 4), (3, 2, 4)),\n+    )\n+    def test_average_pooling1d(\n+        self,\n+        pool_size,\n+        strides,\n+        padding,\n+        data_format,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.AveragePooling1D,\n+            init_kwargs={\n+                \"pool_size\": pool_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 4, 4, 4)),\n+        (2, 1, \"same\", \"channels_first\", (3, 5, 5, 4), (3, 5, 5, 4)),\n+        ((2, 3), (2, 2), \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 2, 2, 4)),\n+    )\n+    def test_average_pooling2d(\n+        self,\n+        pool_size,\n+        strides,\n+        padding,\n+        data_format,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.AveragePooling2D,\n+            init_kwargs={\n+                \"pool_size\": pool_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\", (3, 5, 5, 5, 4), (3, 4, 4, 4, 4)),\n+        (2, 1, \"same\", \"channels_first\", (3, 5, 5, 5, 4), (3, 5, 5, 5, 4)),\n+        (\n+            (2, 3, 2),\n+            (2, 2, 1),\n+            \"valid\",\n+            \"channels_last\",\n+            (3, 5, 5, 5, 4),\n+            (3, 2, 2, 4, 4),\n+        ),\n+    )\n+    def test_average_pooling3d(\n+        self,\n+        pool_size,\n+        strides,\n+        padding,\n+        data_format,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.AveragePooling3D,\n+            init_kwargs={\n+                \"pool_size\": pool_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+\n+class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"same\", \"channels_first\"),\n+        ((2,), (2,), \"valid\", \"channels_last\"),\n+    )\n+    def test_average_pooling1d(self, pool_size, strides, padding, data_format):\n+        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+\n+        layer = layers.AveragePooling1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        tf_keras_layer = tf.keras.layers.AveragePooling1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\"),\n+        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n+    )\n+    def test_average_pooling2d(self, pool_size, strides, padding, data_format):\n+        inputs = np.arange(300, dtype=np.float).reshape((3, 5, 5, 4))\n+\n+        layer = layers.AveragePooling2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        tf_keras_layer = tf.keras.layers.AveragePooling2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"same\", \"channels_first\"),\n+        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n+    )\n+    def test_average_pooling3d(self, pool_size, strides, padding, data_format):\n+        inputs = np.arange(240, dtype=np.float).reshape((2, 3, 4, 5, 2))\n+\n+        layer = layers.AveragePooling3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        tf_keras_layer = tf.keras.layers.AveragePooling3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n\n@@ -0,0 +1,76 @@\n+from keras_core import operations as ops\n+from keras_core.backend import image_data_format\n+from keras_core.layers.input_spec import InputSpec\n+from keras_core.layers.layer import Layer\n+from keras_core.operations.operation_utils import compute_pooling_output_shape\n+\n+\n+class BasePooling(Layer):\n+    \"\"\"Base pooling layer.\"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        pool_dimensions,\n+        pool_mode=\"max\",\n+        padding=\"valid\",\n+        data_format=None,\n+        name=None,\n+        **kwargs,\n+    ):\n+        super().__init__(name=name, **kwargs)\n+\n+        self.pool_size = pool_size\n+        self.strides = pool_size if strides is None else strides\n+        self.pool_mode = pool_mode\n+        self.padding = padding\n+        self.data_format = (\n+            image_data_format() if data_format is None else data_format\n+        )\n+\n+        self.input_spec = InputSpec(ndim=pool_dimensions + 2)\n+\n+    def call(self, inputs):\n+        if self.pool_mode == \"max\":\n+            return ops.max_pool(\n+                inputs,\n+                pool_size=self.pool_size,\n+                strides=self.strides,\n+                padding=self.padding,\n+                data_format=self.data_format,\n+            )\n+        elif self.pool_mode == \"average\":\n+            return ops.average_pool(\n+                inputs,\n+                pool_size=self.pool_size,\n+                strides=self.strides,\n+                padding=self.padding,\n+                data_format=self.data_format,\n+            )\n+        else:\n+            raise ValueError(\n+                \"`pool_mode` must be either 'max' or 'average'. Received: \"\n+                f\"{self.pool_mode}.\"\n+            )\n+\n+    def compute_output_shape(self, input_shape):\n+        return compute_pooling_output_shape(\n+            input_shape,\n+            self.pool_size,\n+            self.strides,\n+            self.padding,\n+            self.data_format,\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"pool_size\": self.pool_size,\n+                \"padding\": self.padding,\n+                \"strides\": self.strides,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n+        return config\n\n@@ -0,0 +1,93 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_pooling import BasePooling\n+\n+\n+@keras_core_export(\n+    [\"keras_core.layers.MaxPooling1D\", \"keras_core.layers.MaxPool1D\"]\n+)\n+class MaxPooling1D(BasePooling):\n+    \"\"\"Max pooling operation for 1D temporal data.\n+\n+    Downsamples the input representation by taking the maximum value over a\n+    spatial window of size `pool_size`. The window is shifted by `strides`.\n+\n+    The resulting output when using the `\"valid\"` padding option has a shape of:\n+    `output_shape = (input_shape - pool_size + 1) / strides)`.\n+\n+    The resulting output shape when using the `\"same\"` padding option is:\n+    `output_shape = input_shape / strides`\n+\n+    Args:\n+        pool_size: int, size of the max pooling window.\n+        strides: int or None. Specifies how much the pooling window moves\n+            for each pooling step. If None, it will default to `pool_size`.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, steps, features)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, features, steps)`. It defaults to the `image_data_format`\n+            value found in your Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be `\"channels_last\"`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        3D tensor with shape `(batch_size, steps, features)`.\n+    - If `data_format=\"channels_first\"`:\n+        3D tensor with shape `(batch_size, features, steps)`.\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        3D tensor with shape `(batch_size, downsampled_steps, features)`.\n+    - If `data_format=\"channels_first\"`:\n+        3D tensor with shape `(batch_size, features, downsampled_steps)`.\n+\n+    Examples:\n+\n+    `strides=1` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([1., 2., 3., 4., 5.])\n+    >>> x = np.reshape(x, [1, 5, 1])\n+    >>> max_pool_1d = keras_core.layers.MaxPooling1D(pool_size=2,\n+    ...    strides=1, padding=\"valid\")\n+    >>> max_pool_1d(x)\n+\n+    `strides=2` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([1., 2., 3., 4., 5.])\n+    >>> x = np.reshape(x, [1, 5, 1])\n+    >>> max_pool_1d = keras_core.layers.MaxPooling1D(pool_size=2,\n+    ...    strides=2, padding=\"valid\")\n+    >>> max_pool_1d(x)\n+\n+    `strides=1` and `padding=\"same\"`:\n+\n+    >>> x = np.array([1., 2., 3., 4., 5.])\n+    >>> x = np.reshape(x, [1, 5, 1])\n+    >>> max_pool_1d = keras_core.layers.MaxPooling1D(pool_size=2,\n+    ...    strides=1, padding=\"same\")\n+    >>> max_pool_1d(x)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        padding=\"valid\",\n+        data_format=None,\n+        name=None,\n+        **kwargs\n+    ):\n+        super().__init__(\n+            pool_size,\n+            strides,\n+            pool_dimensions=1,\n+            pool_mode=\"max\",\n+            padding=padding,\n+            data_format=data_format,\n+            name=name,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,109 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_pooling import BasePooling\n+\n+\n+@keras_core_export(\n+    [\"keras_core.layers.MaxPooling2D\", \"keras_core.layers.MaxPool2D\"]\n+)\n+class MaxPooling2D(BasePooling):\n+    \"\"\"Max pooling operation for 2D spatial data.\n+\n+    Downsamples the input along its spatial dimensions (height and width)\n+    by taking the maximum value over an input window\n+    (of size defined by `pool_size`) for each channel of the input.\n+    The window is shifted by `strides` along each dimension.\n+\n+    The resulting output when using the `\"valid\"` padding option has a spatial\n+    shape (number of rows or columns) of:\n+    `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+    (when `input_shape >= pool_size`)\n+\n+    The resulting output shape when using the `\"same\"` padding option is:\n+    `output_shape = math.floor((input_shape - 1) / strides) + 1`\n+\n+    Args:\n+        pool_size: int or tuple of 2 integers, factors by which to downscale\n+            (dim1, dim2). If only one integer is specified, the same\n+            window length will be used for all dimensions.\n+        strides: int or tuple of 2 integers, or None. Strides values. If None,\n+            it will default to `pool_size`. If only one int is specified, the\n+            same stride size will be used for all dimensions.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape `(batch, height, width, channels)`\n+            while `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, height, width)`. It defaults to the\n+            `image_data_format` value found in your Keras config file at\n+            `~/.keras/keras.json`. If you never set it, then it will be\n+            `\"channels_last\"`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        4D tensor with shape `(batch_size, height, width, channels)`.\n+    - If `data_format=\"channels_first\"`:\n+        4D tensor with shape `(batch_size, channels, height, width)`.\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        4D tensor with shape\n+        `(batch_size, pooled_height, pooled_width, channels)`.\n+    - If `data_format=\"channels_first\"`:\n+        4D tensor with shape\n+        `(batch_size, channels, pooled_height, pooled_width)`.\n+\n+    Examples:\n+\n+    `strides=(1, 1)` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([[1., 2., 3.],\n+    ...               [4., 5., 6.],\n+    ...               [7., 8., 9.]])\n+    >>> x = np.reshape(x, [1, 3, 3, 1])\n+    >>> max_pool_2d = keras_core.layers.MaxPooling2D(pool_size=(2, 2),\n+    ...    strides=(1, 1), padding=\"valid\")\n+    >>> max_pool_2d(x)\n+\n+    `strides=(2, 2)` and `padding=\"valid\"`:\n+\n+    >>> x = np.array([[1., 2., 3., 4.],\n+    ...               [5., 6., 7., 8.],\n+    ...               [9., 10., 11., 12.]])\n+    >>> x = np.reshape(x, [1, 3, 4, 1])\n+    >>> max_pool_2d = keras_core.layers.MaxPooling2D(pool_size=(2, 2),\n+    ...    strides=(2, 2), padding=\"valid\")\n+    >>> max_pool_2d(x)\n+\n+    `stride=(1, 1)` and `padding=\"same\"`:\n+\n+    >>> x = np.array([[1., 2., 3.],\n+    ...               [4., 5., 6.],\n+    ...               [7., 8., 9.]])\n+    >>> x = np.reshape(x, [1, 3, 3, 1])\n+    >>> max_pool_2d = keras_core.layers.MaxPooling2D(pool_size=(2, 2),\n+    ...    strides=(1, 1), padding=\"same\")\n+    >>> max_pool_2d(x)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        padding=\"valid\",\n+        data_format=None,\n+        name=None,\n+        **kwargs\n+    ):\n+        super().__init__(\n+            pool_size,\n+            strides,\n+            pool_dimensions=2,\n+            pool_mode=\"max\",\n+            padding=padding,\n+            data_format=data_format,\n+            name=name,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,85 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.pooling.base_pooling import BasePooling\n+\n+\n+@keras_core_export(\n+    [\"keras_core.layers.MaxPooling3D\", \"keras_core.layers.MaxPool3D\"]\n+)\n+class MaxPooling3D(BasePooling):\n+    \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n+\n+    Downsamples the input along its spatial dimensions (depth, height, and\n+    width) by taking the maximum value over an input window (of size defined by\n+    `pool_size`) for each channel of the input. The window is shifted by\n+    `strides` along each dimension.\n+\n+    Args:\n+        pool_size: int or tuple of 3 integers, factors by which to downscale\n+            (dim1, dim2, dim3). If only one integer is specified, the same\n+            window length will be used for all dimensions.\n+        strides: int or tuple of 3 integers, or None. Strides values. If None,\n+            it will default to `pool_size`. If only one int is specified, the\n+            same stride size will be used for all dimensions.\n+        padding: string, either `\"valid\"` or `\"same\"` (case-insensitive).\n+            `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+            the left/right or up/down of the input such that output has the same\n+            height/width dimension as the input.\n+        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n+            The ordering of the dimensions in the inputs. `\"channels_last\"`\n+            corresponds to inputs with shape\n+            `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)` while\n+            `\"channels_first\"` corresponds to inputs with shape\n+            `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+            It defaults to the `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json`. If you never set it, then it\n+            will be `\"channels_last\"`.\n+\n+    Input shape:\n+    - If `data_format=\"channels_last\"`:\n+        5D tensor with shape:\n+        `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format=\"channels_first\"`:\n+        5D tensor with shape:\n+        `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+    Output shape:\n+    - If `data_format=\"channels_last\"`:\n+        5D tensor with shape:\n+        `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n+    - If `data_format=\"channels_first\"`:\n+        5D tensor with shape:\n+        `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n+\n+    Example:\n+\n+    ```python\n+    depth = 30\n+    height = 30\n+    width = 30\n+    channels = 3\n+\n+    inputs = keras_core.layers.Input(shape=(depth, height, width, channels))\n+    layer = keras_core.layers.MaxPooling3D(pool_size=3)\n+    outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        pool_size,\n+        strides,\n+        padding=\"valid\",\n+        data_format=None,\n+        name=None,\n+        **kwargs\n+    ):\n+        super().__init__(\n+            pool_size,\n+            strides,\n+            pool_dimensions=3,\n+            pool_mode=\"max\",\n+            padding=padding,\n+            data_format=data_format,\n+            name=name,\n+            **kwargs,\n+        )\n\n@@ -0,0 +1,181 @@\n+import numpy as np\n+import tensorflow as tf\n+from absl.testing import parameterized\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class MaxPoolingBasicTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\", (3, 5, 4), (3, 4, 4)),\n+        (2, 1, \"same\", \"channels_first\", (3, 5, 4), (3, 5, 4)),\n+        ((2,), (2,), \"valid\", \"channels_last\", (3, 5, 4), (3, 2, 4)),\n+    )\n+    def test_max_pooling1d(\n+        self,\n+        pool_size,\n+        strides,\n+        padding,\n+        data_format,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.MaxPooling1D,\n+            init_kwargs={\n+                \"pool_size\": pool_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 4, 4, 4)),\n+        (2, 1, \"same\", \"channels_first\", (3, 5, 5, 4), (3, 5, 5, 4)),\n+        ((2, 3), (2, 2), \"valid\", \"channels_last\", (3, 5, 5, 4), (3, 2, 2, 4)),\n+    )\n+    def test_max_pooling2d(\n+        self,\n+        pool_size,\n+        strides,\n+        padding,\n+        data_format,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.MaxPooling2D,\n+            init_kwargs={\n+                \"pool_size\": pool_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\", (3, 5, 5, 5, 4), (3, 4, 4, 4, 4)),\n+        (2, 1, \"same\", \"channels_first\", (3, 5, 5, 5, 4), (3, 5, 5, 5, 4)),\n+        (\n+            (2, 3, 2),\n+            (2, 2, 1),\n+            \"valid\",\n+            \"channels_last\",\n+            (3, 5, 5, 5, 4),\n+            (3, 2, 2, 4, 4),\n+        ),\n+    )\n+    def test_max_pooling3d(\n+        self,\n+        pool_size,\n+        strides,\n+        padding,\n+        data_format,\n+        input_shape,\n+        output_shape,\n+    ):\n+        self.run_layer_test(\n+            layers.MaxPooling3D,\n+            init_kwargs={\n+                \"pool_size\": pool_size,\n+                \"strides\": strides,\n+                \"padding\": padding,\n+                \"data_format\": data_format,\n+            },\n+            input_shape=input_shape,\n+            expected_output_shape=output_shape,\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+\n+\n+class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"same\", \"channels_first\"),\n+        ((2,), (2,), \"valid\", \"channels_last\"),\n+    )\n+    def test_max_pooling1d(self, pool_size, strides, padding, data_format):\n+        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+\n+        layer = layers.MaxPooling1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        tf_keras_layer = tf.keras.layers.MaxPool1D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\"),\n+        ((2, 3), (2, 2), \"same\", \"channels_last\"),\n+    )\n+    def test_max_pooling2d(self, pool_size, strides, padding, data_format):\n+        inputs = np.arange(300, dtype=np.float).reshape((3, 5, 5, 4))\n+\n+        layer = layers.MaxPooling2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        tf_keras_layer = tf.keras.layers.MaxPool2D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n+\n+    @parameterized.parameters(\n+        (2, 1, \"valid\", \"channels_last\"),\n+        (2, 1, \"same\", \"channels_first\"),\n+        ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n+    )\n+    def test_max_pooling3d(self, pool_size, strides, padding, data_format):\n+        inputs = np.arange(240, dtype=np.float).reshape((2, 3, 4, 5, 2))\n+\n+        layer = layers.MaxPooling3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+        tf_keras_layer = tf.keras.layers.MaxPool3D(\n+            pool_size=pool_size,\n+            strides=strides,\n+            padding=padding,\n+            data_format=data_format,\n+        )\n+\n+        outputs = layer(inputs)\n+        expected = tf_keras_layer(inputs)\n+        self.assertAllClose(outputs, expected)\n\n@@ -9,6 +9,7 @@ from keras_core.backend import is_tensor\n from keras_core.backend import name_scope\n from keras_core.backend import random\n from keras_core.backend import shape\n+from keras_core.operations import operation_utils\n from keras_core.operations.math import *  # noqa: F403\n from keras_core.operations.nn import *  # noqa: F403\n from keras_core.operations.numpy import *  # noqa: F403\n\n@@ -37,6 +37,7 @@ from keras_core.backend import any_symbolic_tensors\n from keras_core.backend.common.backend_utils import (\n     compute_conv_transpose_output_shape,\n )\n+from keras_core.operations import operation_utils\n from keras_core.operations.operation import Operation\n \n \n@@ -290,34 +291,13 @@ class MaxPool(Operation):\n         )\n \n     def compute_output_spec(self, inputs):\n-        strides = self.pool_size if self.strides is None else self.strides\n-        input_shape = np.array(inputs.shape)\n-        if self.data_format == \"channels_last\":\n-            spatial_shape = input_shape[1:-1]\n-        else:\n-            spatial_shape = input_shape[2:]\n-        pool_size = np.array(self.pool_size)\n-        if self.padding == \"valid\":\n-            output_spatial_shape = (\n-                np.floor((spatial_shape - self.pool_size) / strides) + 1\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            inputs.shape,\n+            self.pool_size,\n+            self.strides,\n+            self.padding,\n+            self.data_format,\n         )\n-            negative_in_shape = np.all(output_spatial_shape < 0)\n-            if negative_in_shape:\n-                raise ValueError(\n-                    \"Computed output size would be negative. Received \"\n-                    f\"`inputs.shape={input_shape}` and `pool_size={pool_size}`.\"\n-                )\n-        elif self.padding == \"same\":\n-            output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n-        output_spatial_shape = [int(i) for i in output_spatial_shape]\n-        if self.data_format == \"channels_last\":\n-            output_shape = (\n-                [inputs.shape[0]]\n-                + list(output_spatial_shape)\n-                + [inputs.shape[-1]]\n-            )\n-        else:\n-            output_shape = inputs.shape[:2] + list(output_spatial_shape)\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n@@ -394,32 +374,13 @@ class AveragePool(Operation):\n         )\n \n     def compute_output_spec(self, inputs):\n-        strides = self.pool_size if self.strides is None else self.strides\n-        input_shape = np.array(inputs.shape)\n-        if self.data_format == \"channels_last\":\n-            spatial_shape = input_shape[1:-1]\n-        else:\n-            spatial_shape = input_shape[2:]\n-        pool_size = np.array(self.pool_size)\n-        if self.padding == \"valid\":\n-            output_spatial_shape = (\n-                np.floor((spatial_shape - self.pool_size) / strides) + 1\n+        output_shape = operation_utils.compute_pooling_output_shape(\n+            inputs.shape,\n+            self.pool_size,\n+            self.strides,\n+            self.padding,\n+            self.data_format,\n         )\n-            negative_in_shape = np.all(output_spatial_shape < 0)\n-            if negative_in_shape:\n-                raise ValueError(\n-                    \"Computed output size would be negative. Received \"\n-                    f\"`inputs.shape={input_shape}` and `pool_size={pool_size}`.\"\n-                )\n-        elif self.padding == \"same\":\n-            output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n-        output_spatial_shape = [int(i) for i in output_spatial_shape]\n-        if self.data_format == \"channels_last\":\n-            output_shape = (\n-                [inputs.shape[0]] + output_spatial_shape + [inputs.shape[-1]]\n-            )\n-        else:\n-            output_shape = inputs.shape[:2] + output_spatial_shape\n         return KerasTensor(output_shape, dtype=inputs.dtype)\n \n \n\n@@ -0,0 +1,49 @@\n+import numpy as np\n+\n+\n+def compute_pooling_output_shape(\n+    input_shape,\n+    pool_size,\n+    strides,\n+    padding=\"valid\",\n+    data_format=\"channels_last\",\n+):\n+    \"\"\"Compute the output shape of pooling ops.\"\"\"\n+    strides = pool_size if strides is None else strides\n+    input_shape_origin = list(input_shape)\n+    input_shape = np.array(input_shape)\n+    if data_format == \"channels_last\":\n+        spatial_shape = input_shape[1:-1]\n+    else:\n+        spatial_shape = input_shape[2:]\n+    pool_size = np.array(pool_size)\n+    if padding == \"valid\":\n+        output_spatial_shape = (\n+            np.floor((spatial_shape - pool_size) / strides) + 1\n+        )\n+        negative_in_shape = np.all(output_spatial_shape < 0)\n+        if negative_in_shape:\n+            raise ValueError(\n+                \"Computed output size would be negative. Received: \"\n+                f\"`inputs.shape={input_shape}` and `pool_size={pool_size}`.\"\n+            )\n+    elif padding == \"same\":\n+        output_spatial_shape = np.floor((spatial_shape - 1) / strides) + 1\n+    else:\n+        raise ValueError(\n+            \"`padding` must be either `'valid'` or `'same'`. Received \"\n+            f\"{padding}.\"\n+        )\n+    output_spatial_shape = tuple([int(i) for i in output_spatial_shape])\n+    if data_format == \"channels_last\":\n+        output_shape = (\n+            (input_shape_origin[0],)\n+            + output_spatial_shape\n+            + (input_shape_origin[-1],)\n+        )\n+    else:\n+        output_shape = (\n+            input_shape_origin[0],\n+            input_shape_origin[1],\n+        ) + output_spatial_shape\n+    return output_shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
