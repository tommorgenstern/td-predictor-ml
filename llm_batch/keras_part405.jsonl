{"custom_id": "keras#2a8fd05cf52cb7b8c79522078405f4ee50ac251e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 302 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -59,6 +59,8 @@ def save_model(model,\n   >>> x = tf.random.uniform((10, 3))\n   >>> assert np.allclose(model.predict(x), loaded_model.predict(x))\n   \n+  Note that `model.save()` is an alias for `tf.keras.models.save_model()`.\n+\n   The SavedModel and HDF5 file contains:\n \n   - the model's configuration (topology)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4a0f31fc60596508bd79a20214dbfeb65d232757", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 493 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -53,10 +53,14 @@ class Adam(optimizer_v2.OptimizerV2):\n       the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n     name: Optional name for the operations created when applying gradients.\n       Defaults to `\"Adam\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Usage:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1e309596ee8078afe46e26d1d358807e4501fed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 396 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -118,9 +118,9 @@ class Conv(Layer):\n \n     if isinstance(filters, float):\n       filters = int(filters)\n-    if filters is not None and filters < 0:\n-      raise ValueError(f'Received a negative value for `filters`.'\n-                       f'Was expecting a positive value. Received {filters}.')\n+    if filters is not None and filters =< 0:\n+      raise ValueError(f'Received a non-positive value for `filters`.'\n+                       f'Was expecting a strictly positive value. Received {filters}.')\n     self.filters = filters\n     self.groups = groups or 1\n     self.kernel_size = conv_utils.normalize_tuple(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89caadd4ae3c7f8ad9575c203d9d6e3d94030491", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 401 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,8 +119,9 @@ class Conv(Layer):\n     if isinstance(filters, float):\n       filters = int(filters)\n     if filters is not None and filters =< 0:\n-      raise ValueError(f'Received a non-positive value for `filters`.'\n-                       f'Was expecting a strictly positive value. Received {filters}.')\n+      raise ValueError('Invalid value for argument `filters`. '\n+                       'Expected a strictly positive value. '\n+                       f'Received filters={filters}.')\n     self.filters = filters\n     self.groups = groups or 1\n     self.kernel_size = conv_utils.normalize_tuple(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b1ab4cd5ad78d1006b1412e4635903fc352d0dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 219 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -52,12 +52,14 @@ class Adadelta(optimizer_v2.OptimizerV2):\n     epsilon: Small floating point value used to maintain numerical stability.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adadelta\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm and represents\n-      the maximum norm of each parameter;\n-      `\"clipvalue\"` (float) clips gradient by value and represents the\n-      maximum absolute value of each parameter.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Reference:\n     - [Zeiler, 2012](http://arxiv.org/abs/1212.5701)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3a096d1b731f1f0f8142b9cc62da4437c237022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 207 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -46,12 +46,14 @@ class Adagrad(optimizer_v2.OptimizerV2):\n     epsilon: Small floating point value used to maintain numerical stability.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"Adagrad\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm and represents\n-      the maximum L2 norm of each weight variable;\n-      `\"clipvalue\"` (float) clips gradient by value and represents the\n-      maximum absolute value of each weight variable.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value..\n \n   Reference:\n     - [Duchi et al., 2011](\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fc12b3a775448d8733f0812abeb363ec697c33c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 207 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -69,10 +69,14 @@ class Adamax(optimizer_v2.OptimizerV2):\n     epsilon: A small constant for numerical stability.\n     name: Optional name for the operations created when applying gradients.\n       Defaults to `\"Adamax\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Reference:\n     - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e98e3a64bb6ab6f94494f408903492846e43e26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 365 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -87,10 +87,14 @@ class Ftrl(optimizer_v2.OptimizerV2):\n       When input is sparse shrinkage will only happen on the active weights.\n     beta: A float value, representing the beta value from the paper.\n       Defaults to 0.0.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Reference:\n     - [McMahan et al., 2013](\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b58a673cede04ba91743023c8df30af401dfce0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 214 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -56,10 +56,14 @@ class SGD(optimizer_v2.OptimizerV2):\n       Defaults to `False`.\n     name: Optional name prefix for the operations created when applying\n       gradients.  Defaults to `\"SGD\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Usage:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df0db6e738d3ac12e1aa7fefff074c3e17c34d91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 245 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -36,10 +36,14 @@ class Nadam(optimizer_v2.OptimizerV2):\n     epsilon: A small constant for numerical stability.\n     name: Optional name for the operations created when applying gradients.\n       Defaults to `\"Nadam\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Usage Example:\n     >>> opt = tf.keras.optimizers.Nadam(learning_rate=0.2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6740e9f26f67b4bb1dce37bb0d109a9aa461acfe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 325 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -54,10 +54,14 @@ class RMSprop(optimizer_v2.OptimizerV2):\n       expensive in terms of computation and memory. Defaults to `False`.\n     name: Optional name prefix for the operations created when applying\n       gradients. Defaults to `\"RMSprop\"`.\n-    **kwargs: Keyword arguments. Allowed to be one of\n-      `\"clipnorm\"` or `\"clipvalue\"`.\n-      `\"clipnorm\"` (float) clips gradients by norm; `\"clipvalue\"` (float) clips\n-      gradients by value.\n+    **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n+      `clipnorm`, `global_clipnorm`.\n+      If `clipvalue` (float) is set, the gradient of each weight\n+      is clipped to be no higher than this value.\n+      If `clipnorm` (float) is set, the gradient of each weight\n+      is individually clipped so that its norm is no higher than this value.\n+      If `global_clipnorm` (float) is set the gradient of all weights is\n+      clipped so that their global norm is no higher than this value.\n \n   Note that in the dense implementation of this algorithm, variables and their\n   corresponding accumulators (momentum, gradient moving average, square\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e76f053e8823988626e74cb386fc01ec857859a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 55 | Files Changed: 1 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 110 | Churn Cumulative: 1433 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -254,11 +254,11 @@ class RandomUniform(Initializer):\n       random values to generate (inclusive).\n     maxval: A python scalar or a scalar tensor. Upper bound of the range of\n       random values to generate (exclusive).\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n   \"\"\"\n \n   def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n@@ -320,11 +320,11 @@ class RandomNormal(Initializer):\n       generate.\n     stddev: a python scalar or a scalar tensor. Standard deviation of the random\n       values to generate.\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n   \"\"\"\n \n   def __init__(self, mean=0.0, stddev=0.05, seed=None):\n@@ -388,11 +388,11 @@ class TruncatedNormal(Initializer):\n       to generate.\n     stddev: a python scalar or a scalar tensor. Standard deviation of the\n       random values to generate before truncation.\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n   \"\"\"\n \n   def __init__(self, mean=0.0, stddev=0.05, seed=None):\n@@ -465,11 +465,11 @@ class VarianceScaling(Initializer):\n     mode: One of \"fan_in\", \"fan_out\", \"fan_avg\".\n     distribution: Random distribution to use. One of \"truncated_normal\",\n       \"untruncated_normal\" and  \"uniform\".\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n   \"\"\"\n \n   def __init__(self,\n@@ -574,11 +574,11 @@ class Orthogonal(Initializer):\n \n   Args:\n     gain: multiplicative factor to apply to the orthogonal matrix\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [Saxe et al., 2014](https://openreview.net/forum?id=_wzZwKpTDF_9C)\n@@ -705,11 +705,11 @@ class GlorotUniform(VarianceScaling):\n   >>> layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n \n   Args:\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n@@ -751,11 +751,11 @@ class GlorotNormal(VarianceScaling):\n   >>> layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n \n   Args:\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n@@ -800,11 +800,11 @@ class LecunNormal(VarianceScaling):\n   >>> layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n \n   Args:\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n@@ -842,11 +842,11 @@ class LecunUniform(VarianceScaling):\n   >>> layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n \n   Args:\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n@@ -884,11 +884,11 @@ class HeNormal(VarianceScaling):\n   >>> layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n \n   Args:\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n@@ -926,11 +926,11 @@ class HeUniform(VarianceScaling):\n   >>> layer = tf.keras.layers.Dense(3, kernel_initializer=initializer)\n \n   Args:\n-    seed: A Python integer. Used to create random seeds. See\n-      `tf.compat.v1.set_random_seed` for behavior. Note that seeded\n-      initializer will not produce same random values across multiple calls,\n-      but multiple initializers will produce same sequence when constructed with\n-      same seed value.\n+    seed: A Python integer. Used to make the behavior of the initializer\n+      deterministic. Note that a seeded\n+      initializer will not produce the same random values across multiple calls,\n+      but multiple initializers will produce the same sequence when constructed\n+      with the same seed value.\n \n   References:\n     - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d8aee3ef9edf50369301d7f074fef3adc84d742", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/7 | Churn Δ: 95 | Churn Cumulative: 4030 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: 0.425\n\nDIFF:\n@@ -78,6 +78,49 @@ def _assign_if_finite(var, value):\n       tf.no_op)\n \n \n+def _maybe_warn_about_scaling(loss_has_been_scaled,\n+                              gradients_have_been_unscaled):\n+  \"\"\"Warn if the loss or gradients hasn't been scaled or unscaled.\"\"\"\n+  if loss_has_been_scaled and gradients_have_been_unscaled:\n+    return\n+\n+  example_code = \"\"\"\n+    with tf.GradientTape() as tape:\n+      loss = loss_fn()\n+      scaled_loss = opt.get_scaled_loss(loss)\n+    scaled_grads = tape.gradient(scaled_loss, vars)\n+    grads = opt.get_unscaled_gradients(scaled_grads)\n+    opt.apply_gradients([(grads, var)])\"\"\"\n+\n+  if not loss_has_been_scaled and not gradients_have_been_unscaled:\n+    tf_logging.warning(\n+        'You forgot to call LossScaleOptimizer.get_scaled_loss() and '\n+        'LossScaleOptimizer.get_unscaled_gradients() before calling '\n+        'LossScaleOptimizer.apply_gradients(). This will likely result in '\n+        'worse model quality, so please call them in the correct places! For '\n+        f'example:{example_code}\\nFor more information, see '\n+        'https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer'\n+    )\n+  elif not loss_has_been_scaled:\n+    tf_logging.warning(\n+        'You forgot to call LossScaleOptimizer.get_scaled_loss() before '\n+        'calling LossScaleOptimizer.apply_gradients() (you did call '\n+        'get_unscaled_gradients() however). This will likely result in worse '\n+        'model quality, so please call get_scaled_loss() in the correct place! '\n+        f'For example:{example_code}\\nFor more information, see '\n+        'https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer'\n+    )\n+  elif not gradients_have_been_unscaled:\n+    tf_logging.warning(\n+        'You forgot to call LossScaleOptimizer.get_unscaled_gradients() '\n+        'before calling LossScaleOptimizer.apply_gradients() (you did call '\n+        'get_scaled_loss() however). This will likely result in worse '\n+        'model quality, so please call get_unscaled_gradients() in the correct '\n+        f'place! For example:{example_code}\\nFor more information, see '\n+        'https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer'\n+    )\n+\n+\n class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n   \"\"\"The state of a dynamic loss scale.\"\"\"\n \n@@ -576,6 +619,11 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n         raise ValueError('\"dynamic_growth_steps\" must be None if \"dynamic\" '\n                          'is False, but got: %s' % (dynamic_growth_steps,))\n \n+    # Used to track whether get_scaled_loss() and get_unscaled_gradients() have\n+    # been called\n+    self._loss_has_been_scaled = False\n+    self._gradients_have_been_unscaled = False\n+\n     # To support restoring TensorFlow 2.2 checkpoints.\n     self._track_trackable(FakeOptimizerForRestoration(self._optimizer),\n                           'base_optimizer')\n@@ -618,6 +666,7 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n     return self._optimizer\n \n   def get_scaled_loss(self, loss):\n+    self._loss_has_been_scaled = True\n     if callable(loss):\n       def new_loss():\n         loss_val = loss()\n@@ -627,6 +676,7 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n       return loss * tf.cast(self.loss_scale, loss.dtype)\n \n   def get_unscaled_gradients(self, grads):\n+    self._gradients_have_been_unscaled = True\n     loss_scale_reciprocal = 1. / self.loss_scale\n     return [\n         _multiply_gradient(g, loss_scale_reciprocal) if g is not None else None\n@@ -664,6 +714,8 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n     # We check for the strategy here despite already checking in the constructor\n     # as frequently the optimizer is created outside the strategy's scope.\n     _raise_if_strategy_unsupported()\n+    _maybe_warn_about_scaling(self._loss_has_been_scaled,\n+                              self._gradients_have_been_unscaled)\n \n     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n     if experimental_aggregate_gradients:\n@@ -1180,6 +1232,11 @@ class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n         raise ValueError(f'\"dynamic_growth_steps\" must be None if \"dynamic\" '\n                          f'is False, but got: {dynamic_growth_steps}')\n \n+    # Used to track whether get_scaled_loss() and get_unscaled_gradients() have\n+    # been called\n+    self._loss_has_been_scaled = False\n+    self._gradients_have_been_unscaled = False\n+\n   @property\n   def dynamic(self):\n     return isinstance(self._loss_scale, _DynamicLossScaleState)\n@@ -1218,6 +1275,7 @@ class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n     return self._optimizer\n \n   def get_scaled_loss(self, loss):\n+    self._loss_has_been_scaled = True\n     if callable(loss):\n       def new_loss():\n         loss_val = loss()\n@@ -1227,6 +1285,7 @@ class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n       return loss * tf.cast(self.loss_scale, loss.dtype)\n \n   def get_unscaled_gradients(self, grads):\n+    self._gradients_have_been_unscaled = True\n     loss_scale_reciprocal = 1. / self.loss_scale\n     return [\n         _multiply_gradient(g, loss_scale_reciprocal) if g is not None else None\n@@ -1254,6 +1313,8 @@ class LossScaleOptimizerV3(tf.__internal__.tracking.DelegatingTrackableMixin,\n     # We check for the strategy here despite already checking in the constructor\n     # as frequently the optimizer is created outside the strategy's scope.\n     _raise_if_strategy_unsupported()\n+    _maybe_warn_about_scaling(self._loss_has_been_scaled,\n+                              self._gradients_have_been_unscaled)\n \n     grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n     if not skip_gradients_aggregation:\n\n@@ -15,6 +15,7 @@\n \"\"\"Tests for LossScaleOptimizer.\"\"\"\n \n import os\n+from unittest import mock\n \n from absl.testing import parameterized\n \n@@ -33,6 +34,7 @@ import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.framework import test_util\n from tensorflow.python.keras.optimizer_v2 import gradient_descent as legacy_sgd\n+from tensorflow.python.platform import tf_logging\n \n # If called outside any strategy.scope() calls, this will return the default\n # strategy.\n@@ -1238,6 +1240,38 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                    \"a bool, but got: 'dynamic'\"):\n       create_lso(opt, 'dynamic')\n \n+  @combinations.generate(opt_combinations_only())\n+  def testScalingWarning(self, opt_cls):\n+    var = tf.Variable(1.0)\n+    lso = create_lso(create_sgd(opt_cls))\n+    with mock.patch.object(tf_logging, 'warning') as mock_warn:\n+      lso.apply_gradients([(tf.constant(1.0), var)])\n+      self.assertIn(\n+          'You forgot to call LossScaleOptimizer.get_scaled_loss() and '\n+          'LossScaleOptimizer.get_unscaled_gradients() before',\n+          mock_warn.call_args_list[0][0][0])\n+    lso = create_lso(create_sgd(opt_cls))\n+    with mock.patch.object(tf_logging, 'warning') as mock_warn:\n+      lso.get_scaled_loss(tf.constant(1.0))\n+      lso.apply_gradients([(tf.constant(1.0), var)])\n+      self.assertIn(\n+          'You forgot to call LossScaleOptimizer.get_unscaled_gradients() '\n+          'before',\n+          mock_warn.call_args_list[0][0][0])\n+    lso = create_lso(create_sgd(opt_cls))\n+    with mock.patch.object(tf_logging, 'warning') as mock_warn:\n+      lso.get_unscaled_gradients([tf.constant(1.0)])\n+      lso.apply_gradients([(tf.constant(1.0), var)])\n+      self.assertIn(\n+          'You forgot to call LossScaleOptimizer.get_scaled_loss() before',\n+          mock_warn.call_args_list[0][0][0])\n+    lso = create_lso(create_sgd(opt_cls))\n+    with mock.patch.object(tf_logging, 'warning') as mock_warn:\n+      lso.get_scaled_loss(tf.constant(1.0))\n+      lso.get_unscaled_gradients([tf.constant(1.0)])\n+      lso.apply_gradients([(tf.constant(1.0), var)])\n+      mock_warn.assert_not_called()\n+\n   @combinations.generate(opt_combinations_only())\n   def testErrorWhenNesting(self, opt_cls):\n     opt = create_sgd(opt_cls)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea7bfbd2c0e33ddf96b5b2d2d7723dc1e40b1b51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 403 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -118,7 +118,7 @@ class Conv(Layer):\n \n     if isinstance(filters, float):\n       filters = int(filters)\n-    if filters is not None and filters =< 0:\n+    if filters is not None and filters <= 0:\n       raise ValueError('Invalid value for argument `filters`. '\n                        'Expected a strictly positive value. '\n                        f'Received filters={filters}.')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9733982976f62c351ea3db2aeb1f28a67bb5d2fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 123 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 128 | Churn Cumulative: 9573 | Contributors (this commit): 25 | Commits (past 90d): 9 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@\n import tensorflow.compat.v2 as tf\n \n import collections\n+import contextlib\n import copy\n import functools\n import itertools\n@@ -83,6 +84,42 @@ keras_premade_model_gauge = tf.__internal__.monitoring.BoolGauge(\n \n _is_name_scope_on_model_declaration_enabled = False\n \n+_name_scope_unnester_stack = threading.local()\n+\n+\n+@contextlib.contextmanager\n+def _name_scope_unnester(full_name_scope):\n+  \"\"\"Helper to get relative name scope from fully specified nested name scopes.\n+\n+  Args:\n+    full_name_scope: full(absolute) name scope path.\n+\n+  Yields:\n+    Relative name scope path from the parent `_name_scope_unnester` context\n+    manager.\n+\n+  Example:\n+  ```\n+  with _name_scope_unnester('a') as name1:  # name1 == 'a'\n+    with _name_scope_unnester('a/b') as name2:  # name2 == 'b'\n+      with _name_scope_unnester('a/b/c') as name3:  # name3 == 'c'\n+        pass\n+  ```\n+  \"\"\"\n+  if not getattr(_name_scope_unnester_stack, 'value', None):\n+    _name_scope_unnester_stack.value = ['']\n+\n+  _name_scope_unnester_stack.value.append(full_name_scope)\n+\n+  try:\n+    full_name_scope = _name_scope_unnester_stack.value[-1]\n+    outer_name_scope = _name_scope_unnester_stack.value[-2]\n+    relative_name_scope = full_name_scope.lstrip(outer_name_scope)\n+    relative_name_scope = relative_name_scope.lstrip('/')\n+    yield relative_name_scope\n+  finally:\n+    _name_scope_unnester_stack.value.pop()\n+\n \n @keras_export('keras.layers.Layer')\n class Layer(tf.Module, version_utils.LayerVersionSelector):\n@@ -461,7 +498,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     # Save outer name scope at layer declaration so that it is preserved at\n     # the actual layer construction.\n-    self._outer_name_scope = tf.get_current_name_scope()\n+    self._name_scope_on_declaration = tf.get_current_name_scope()\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   @generic_utils.default\n@@ -1074,17 +1111,23 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         training=training_mode):\n \n       input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n+\n       if eager:\n         call_fn = self.call\n         name_scope = self._name\n       else:\n-        name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n+        name_scope = self._get_unnested_name_scope()\n         call_fn = self._autographed_call()\n+\n       call_fn = traceback_utils.inject_argument_info_in_traceback(\n           call_fn,\n           object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n+      with contextlib.ExitStack() as namescope_stack:\n+        if _is_name_scope_on_model_declaration_enabled:\n+          namescope_stack.enter_context(_name_scope_unnester(\n+              self._name_scope_on_declaration))\n+        namescope_stack.enter_context(tf.name_scope(name_scope))\n \n-      with tf.name_scope(name_scope):\n         if not self.built:\n           self._maybe_build(inputs)\n \n@@ -1104,6 +1147,22 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         return outputs\n \n+  def _get_unnested_name_scope(self):\n+    if _is_name_scope_on_model_declaration_enabled:\n+      with _name_scope_unnester(self._name_scope_on_declaration\n+                               ) as relative_name_scope_on_declaration:\n+      # To avoid `tf.name_scope` autoincrement, use absolute path.\n+        relative_name_scope = filter(\n+            None,\n+            [tf.get_current_name_scope(), relative_name_scope_on_declaration])\n+        current_name_scope = '/'.join(relative_name_scope) + '/'\n+        with tf.name_scope(current_name_scope):\n+          name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n+    else:\n+      name_scope = self._name_scope()\n+\n+    return name_scope\n+\n   def _functional_construction_call(self, inputs, args, kwargs, input_list):\n     call_context = base_layer_utils.call_context()\n \n@@ -2489,8 +2548,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     if not tf.__internal__.tf2.enabled():\n       return self.name\n     name_scope = self.name\n-    if _is_name_scope_on_model_declaration_enabled and self._outer_name_scope:\n-      name_scope = self._outer_name_scope + '/' + name_scope\n     current_name_scope = tf.__internal__.get_name_scope()\n     if current_name_scope:\n       name_scope = current_name_scope + '/' + name_scope\n\n@@ -1416,6 +1416,67 @@ class NameScopingTest(keras_parameterized.TestCase):\n     ])\n     base_layer._apply_name_scope_on_model_declaration(False)\n \n+  @testing_utils.run_v2_only\n+  def test_apply_name_scope_on_nested_layer_model_declaration(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest('`apply_name_scope_on_model_declaration` API is supported'\n+                    ' only for V2 eager')\n+\n+    base_layer._apply_name_scope_on_model_declaration(True)\n+\n+    class ThreeDenses(layers.Layer):\n+\n+      def __init__(self, name='ThreeDenses', **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.inner_dense_1 = layers.Dense(10, name='NestedDense1')\n+        with tf.name_scope('inner1/inner2'):\n+          self.inner_dense_2 = layers.Dense(20, name='NestedDense2')\n+        self.inner_dense_3 = layers.Dense(30, name='NestedDense3')\n+\n+      def call(self, x):\n+        x = self.inner_dense_1(x)\n+        x = self.inner_dense_2(x)\n+        x = self.inner_dense_3(x)\n+        return x\n+\n+    inputs = input_layer.Input((3,))\n+    with tf.name_scope('outer'):\n+      x = ThreeDenses()(inputs)\n+    outputs = layers.Dense(10, name='OuterDense')(x)\n+\n+    model = training_lib.Model(inputs, outputs)\n+    node_names = self._get_model_node_names(model, np.random.random((1, 3)),\n+                                            'call_scope')\n+\n+    self.assertListEqual(node_names, [\n+        'call_scope/Const', 'call_scope/model/Cast',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp',\n+        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd',\n+        'call_scope/model/OuterDense/MatMul/ReadVariableOp/resource',\n+        'call_scope/model/OuterDense/MatMul/ReadVariableOp',\n+        'call_scope/model/OuterDense/MatMul',\n+        'call_scope/model/OuterDense/BiasAdd/ReadVariableOp/resource',\n+        'call_scope/model/OuterDense/BiasAdd/ReadVariableOp',\n+        'call_scope/model/OuterDense/BiasAdd', 'Identity', 'NoOp'\n+    ])\n+    base_layer._apply_name_scope_on_model_declaration(False)\n+\n   def _get_model_node_names(self, model, inputs, call_name_scope):\n     \"\"\"Returns a list of model's node names.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7379f454e5c03d0e8c47bd2d94468839ec0c03d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 123 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 128 | Churn Cumulative: 9701 | Contributors (this commit): 25 | Commits (past 90d): 11 | Contributors (cumulative): 34 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,7 +20,6 @@\n import tensorflow.compat.v2 as tf\n \n import collections\n-import contextlib\n import copy\n import functools\n import itertools\n@@ -84,42 +83,6 @@ keras_premade_model_gauge = tf.__internal__.monitoring.BoolGauge(\n \n _is_name_scope_on_model_declaration_enabled = False\n \n-_name_scope_unnester_stack = threading.local()\n-\n-\n-@contextlib.contextmanager\n-def _name_scope_unnester(full_name_scope):\n-  \"\"\"Helper to get relative name scope from fully specified nested name scopes.\n-\n-  Args:\n-    full_name_scope: full(absolute) name scope path.\n-\n-  Yields:\n-    Relative name scope path from the parent `_name_scope_unnester` context\n-    manager.\n-\n-  Example:\n-  ```\n-  with _name_scope_unnester('a') as name1:  # name1 == 'a'\n-    with _name_scope_unnester('a/b') as name2:  # name2 == 'b'\n-      with _name_scope_unnester('a/b/c') as name3:  # name3 == 'c'\n-        pass\n-  ```\n-  \"\"\"\n-  if not getattr(_name_scope_unnester_stack, 'value', None):\n-    _name_scope_unnester_stack.value = ['']\n-\n-  _name_scope_unnester_stack.value.append(full_name_scope)\n-\n-  try:\n-    full_name_scope = _name_scope_unnester_stack.value[-1]\n-    outer_name_scope = _name_scope_unnester_stack.value[-2]\n-    relative_name_scope = full_name_scope.lstrip(outer_name_scope)\n-    relative_name_scope = relative_name_scope.lstrip('/')\n-    yield relative_name_scope\n-  finally:\n-    _name_scope_unnester_stack.value.pop()\n-\n \n @keras_export('keras.layers.Layer')\n class Layer(tf.Module, version_utils.LayerVersionSelector):\n@@ -498,7 +461,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     # Save outer name scope at layer declaration so that it is preserved at\n     # the actual layer construction.\n-    self._name_scope_on_declaration = tf.get_current_name_scope()\n+    self._outer_name_scope = tf.get_current_name_scope()\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   @generic_utils.default\n@@ -1111,23 +1074,17 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         training=training_mode):\n \n       input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n-\n       if eager:\n         call_fn = self.call\n         name_scope = self._name\n       else:\n-        name_scope = self._get_unnested_name_scope()\n+        name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n         call_fn = self._autographed_call()\n-\n       call_fn = traceback_utils.inject_argument_info_in_traceback(\n           call_fn,\n           object_name=f'layer \"{self.name}\" (type {self.__class__.__name__})')\n-      with contextlib.ExitStack() as namescope_stack:\n-        if _is_name_scope_on_model_declaration_enabled:\n-          namescope_stack.enter_context(_name_scope_unnester(\n-              self._name_scope_on_declaration))\n-        namescope_stack.enter_context(tf.name_scope(name_scope))\n \n+      with tf.name_scope(name_scope):\n         if not self.built:\n           self._maybe_build(inputs)\n \n@@ -1147,22 +1104,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         return outputs\n \n-  def _get_unnested_name_scope(self):\n-    if _is_name_scope_on_model_declaration_enabled:\n-      with _name_scope_unnester(self._name_scope_on_declaration\n-                               ) as relative_name_scope_on_declaration:\n-      # To avoid `tf.name_scope` autoincrement, use absolute path.\n-        relative_name_scope = filter(\n-            None,\n-            [tf.get_current_name_scope(), relative_name_scope_on_declaration])\n-        current_name_scope = '/'.join(relative_name_scope) + '/'\n-        with tf.name_scope(current_name_scope):\n-          name_scope = self._name_scope()  # Avoid autoincrementing.  # pylint: disable=not-callable\n-    else:\n-      name_scope = self._name_scope()\n-\n-    return name_scope\n-\n   def _functional_construction_call(self, inputs, args, kwargs, input_list):\n     call_context = base_layer_utils.call_context()\n \n@@ -2548,6 +2489,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     if not tf.__internal__.tf2.enabled():\n       return self.name\n     name_scope = self.name\n+    if _is_name_scope_on_model_declaration_enabled and self._outer_name_scope:\n+      name_scope = self._outer_name_scope + '/' + name_scope\n     current_name_scope = tf.__internal__.get_name_scope()\n     if current_name_scope:\n       name_scope = current_name_scope + '/' + name_scope\n\n@@ -1416,67 +1416,6 @@ class NameScopingTest(keras_parameterized.TestCase):\n     ])\n     base_layer._apply_name_scope_on_model_declaration(False)\n \n-  @testing_utils.run_v2_only\n-  def test_apply_name_scope_on_nested_layer_model_declaration(self):\n-    if not tf.executing_eagerly():\n-      self.skipTest('`apply_name_scope_on_model_declaration` API is supported'\n-                    ' only for V2 eager')\n-\n-    base_layer._apply_name_scope_on_model_declaration(True)\n-\n-    class ThreeDenses(layers.Layer):\n-\n-      def __init__(self, name='ThreeDenses', **kwargs):\n-        super().__init__(name=name, **kwargs)\n-        self.inner_dense_1 = layers.Dense(10, name='NestedDense1')\n-        with tf.name_scope('inner1/inner2'):\n-          self.inner_dense_2 = layers.Dense(20, name='NestedDense2')\n-        self.inner_dense_3 = layers.Dense(30, name='NestedDense3')\n-\n-      def call(self, x):\n-        x = self.inner_dense_1(x)\n-        x = self.inner_dense_2(x)\n-        x = self.inner_dense_3(x)\n-        return x\n-\n-    inputs = input_layer.Input((3,))\n-    with tf.name_scope('outer'):\n-      x = ThreeDenses()(inputs)\n-    outputs = layers.Dense(10, name='OuterDense')(x)\n-\n-    model = training_lib.Model(inputs, outputs)\n-    node_names = self._get_model_node_names(model, np.random.random((1, 3)),\n-                                            'call_scope')\n-\n-    self.assertListEqual(node_names, [\n-        'call_scope/Const', 'call_scope/model/Cast',\n-        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp/resource',\n-        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul/ReadVariableOp',\n-        'call_scope/model/outer/ThreeDenses/NestedDense1/MatMul',\n-        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp/resource',\n-        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd/ReadVariableOp',\n-        'call_scope/model/outer/ThreeDenses/NestedDense1/BiasAdd',\n-        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp/resource',\n-        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul/ReadVariableOp',\n-        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/MatMul',\n-        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp/resource',\n-        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd/ReadVariableOp',\n-        'call_scope/model/outer/ThreeDenses/inner1/inner2/NestedDense2/BiasAdd',\n-        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp/resource',\n-        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul/ReadVariableOp',\n-        'call_scope/model/outer/ThreeDenses/NestedDense3/MatMul',\n-        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp/resource',\n-        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd/ReadVariableOp',\n-        'call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd',\n-        'call_scope/model/OuterDense/MatMul/ReadVariableOp/resource',\n-        'call_scope/model/OuterDense/MatMul/ReadVariableOp',\n-        'call_scope/model/OuterDense/MatMul',\n-        'call_scope/model/OuterDense/BiasAdd/ReadVariableOp/resource',\n-        'call_scope/model/OuterDense/BiasAdd/ReadVariableOp',\n-        'call_scope/model/OuterDense/BiasAdd', 'Identity', 'NoOp'\n-    ])\n-    base_layer._apply_name_scope_on_model_declaration(False)\n-\n   def _get_model_node_names(self, model, inputs, call_name_scope):\n     \"\"\"Returns a list of model's node names.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
