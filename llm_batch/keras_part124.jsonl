{"custom_id": "keras#11b73ae6b4a302eb010a320a3e11d438d499caf3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 9 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -153,15 +153,6 @@ class ConvRecurrent2D(Layer):\n         assert K.ndim(x) == 5\n         input_shape = self.input_spec[0].shape\n         unroll = False\n-        if K.backend() == 'tensorflow':\n-            if not input_shape[1]:\n-                raise Exception('When using TensorFlow, you should define ' +\n-                                'explicitely the number of timesteps of ' +\n-                                'your sequences. Make sure the first layer ' +\n-                                'has a \"batch_input_shape\" argument ' +\n-                                'including the samples axis.')\n-            else:\n-                unroll = True\n \n         if self.stateful:\n             initial_states = self.states\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18d7e5e6e4ef7f1f6b0a2884745a63b4f36a08ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 751 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -168,11 +168,11 @@ class BatchNormalization(Layer):\n         return x_normed\n \n     def get_config(self):\n-        config = {\"epsilon\": self.epsilon,\n-                  \"mode\": self.mode,\n-                  \"axis\": self.axis,\n-                  \"gamma_regularizer\": self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n-                  \"beta_regularizer\": self.beta_regularizer.get_config() if self.beta_regularizer else None,\n-                  \"momentum\": self.momentum}\n+        config = {'epsilon': self.epsilon,\n+                  'mode': self.mode,\n+                  'axis': self.axis,\n+                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,\n+                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None,\n+                  'momentum': self.momentum}\n         base_config = super(BatchNormalization, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f42253f46032dba423a2f63ed7cfc16304585c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 9480 | Contributors (this commit): 60 | Commits (past 90d): 19 | Contributors (cumulative): 69 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,7 @@ import os\n import numpy as np\n \n from . import backend as K\n+from . import optimizers\n from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n from .engine.topology import get_source_inputs, Node, Layer\n@@ -56,6 +57,18 @@ def save_model(model, filepath, overwrite=True):\n     model.save_weights_to_hdf5_group(model_weights_group)\n \n     if hasattr(model, 'optimizer'):\n+        if isinstance(model.optimizer, optimizers.TFOptimizer):\n+            warnings.warn(\n+                'TensorFlow optimizers do not '\n+                'make it possible to access '\n+                'optimizer attributes or optimizer state '\n+                'after instantiation. '\n+                'As a result, we cannot save the optimizer '\n+                'as part of the model save file.'\n+                'You will have to compile your model again after loading it. '\n+                'Prefer using a Keras optimizer instead '\n+                '(see keras.io/optimizers).')\n+        else:\n             f.attrs['training_config'] = json.dumps({\n                 'optimizer_config': {\n                     'class_name': model.optimizer.__class__.__name__,\n\n@@ -2,6 +2,7 @@ from __future__ import absolute_import\n from . import backend as K\n from .utils.generic_utils import get_from_module\n from six.moves import zip\n+import warnings\n \n \n def clip_norm(g, c, n):\n@@ -577,7 +578,6 @@ class TFOptimizer(Optimizer):\n                              'all weights constraints in your model, '\n                              'or use a Keras optimizer.')\n         grads = self.optimizer.compute_gradients(loss, params)\n-        self.updates.append(K.update_add(self.iterations, 1))\n         opt_update = self.optimizer.apply_gradients(\n             grads, global_step=self.iterations)\n         self.updates.append(opt_update)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d4087a1e9459dbadf3e47a050fe9d545f59218b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 25 | Files Changed: 3 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 235 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n-\"\"\" This script demonstrate the use of convolutional LSTM network\n-This network is used to predict the next frame of an artificialy\n-generated movie which contain moving squares.\n+\"\"\" This script demonstrates the use of a convolutional LSTM network.\n+This network is used to predict the next frame of an artificially\n+generated movie which contains moving squares.\n \"\"\"\n from keras.models import Sequential\n from keras.layers.convolutional import Convolution3D\n@@ -10,7 +10,7 @@ import numpy as np\n import pylab as plt\n \n # We create a layer which take as input movies of shape\n-# (n_frames, width, height, channel) and that returns a movie\n+# (n_frames, width, height, channels) and returns a movie\n # of identical shape.\n \n seq = Sequential()\n@@ -38,12 +38,12 @@ seq.add(Convolution3D(nb_filter=1, kernel_dim1=1, kernel_dim2=3,\n seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n \n \n-# Generating artificial data:\n+# Artificial data generation:\n # Generate movies with 3 to 7 moving squares inside.\n-# The squares are of shape one by one or two by two pixels and\n-# they move linearly trought time.\n-# For convenience we first create movies with bigger width and height, (80x80)\n-# and at the end we select a 40x40 window\n+# The squares are of shape 1x1 or 2x2 pixels,\n+# which move linearly over time.\n+# For convenience we first create movies with bigger width and height (80x80)\n+# and at the end we select a 40x40 window.\n \n def generate_movies(n_samples=1200, n_frames=15):\n     row = 80\n@@ -53,8 +53,7 @@ def generate_movies(n_samples=1200, n_frames=15):\n                               dtype=np.float)\n \n     for i in range(n_samples):\n-\n-        # add from 3 to 7 moving squares\n+        # Add 3 to 7 moving squares\n         n = np.random.randint(3, 8)\n \n         for j in range(n):\n@@ -75,10 +74,10 @@ def generate_movies(n_samples=1200, n_frames=15):\n                              y_shift - w: y_shift + w, 0] += 1\n \n                 # Make it more robust by adding noise.\n-                # The idea is that if during predict time,\n+                # The idea is that if during inference,\n                 # the value of the pixel is not exactly one,\n-                # we need to train the network to be robust and stille\n-                # consider it is a pixel belonging to a square.\n+                # we need to train the network to be robust and still\n+                # consider it as a pixel belonging to a square.\n                 if np.random.randint(0, 2):\n                     noise_f = (-1)**np.random.randint(0, 2)\n                     noisy_movies[i, t,\n@@ -86,13 +85,13 @@ def generate_movies(n_samples=1200, n_frames=15):\n                                  y_shift - w - 1: y_shift + w + 1,\n                                  0] += noise_f * 0.1\n \n-                # Shitf the ground truth by 1\n+                # Shift the ground truth by 1\n                 x_shift = xstart + directionx * (t + 1)\n                 y_shift = ystart + directiony * (t + 1)\n                 shifted_movies[i, t, x_shift - w: x_shift + w,\n                                y_shift - w: y_shift + w, 0] += 1\n \n-    # Cut to a forty's sized window\n+    # Cut to a 40x40 window\n     noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n     shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n     noisy_movies[noisy_movies >= 1] = 1\n\n@@ -256,7 +256,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n                  forget_bias_init='one', activation='tanh',\n                  inner_activation='hard_sigmoid',\n                  dim_ordering='default',\n-                 border_mode='valid', sub_sample=(1, 1),\n+                 border_mode='valid', subsample=(1, 1),\n                  W_regularizer=None, U_regularizer=None, b_regularizer=None,\n                  dropout_W=0., dropout_U=0., **kwargs):\n \n@@ -273,7 +273,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n         self.border_mode = border_mode\n-        self.subsample = sub_sample\n+        self.subsample = subsample\n \n         if dim_ordering == 'th':\n             warnings.warn('Be carefull if used with convolution3D layers:\\n'\n\n@@ -10,14 +10,13 @@ from keras import regularizers\n \n \n def test_recurrent_convolutional():\n-\n-    nb_row = 4\n-    nb_col = 4\n-    nb_filter = 20\n-    nb_samples = 5\n+    nb_row = 3\n+    nb_col = 3\n+    nb_filter = 5\n+    nb_samples = 2\n     input_channel = 2\n-    input_nb_row = 10\n-    input_nb_col = 10\n+    input_nb_row = 5\n+    input_nb_col = 5\n     sequence_len = 2\n     for dim_ordering in ['th', 'tf']:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a8bbcf611f64c1b36fda671534f25a0366c076b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 34 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ import warnings\n \n \n class ConvRecurrent2D(Layer):\n-    '''Abstract base class for convolutionnal recurrent layers.\n+    '''Abstract base class for convolutional recurrent layers.\n     Do not use in a model -- it's not a functional layer!\n \n     ConvLSTM2D\n@@ -457,7 +457,7 @@ class ConvLSTM2D(ConvRecurrent2D):\n                              border_mode=self.border_mode)\n \n         # U : from nb_filter to nb_filter\n-        # Same because must be stable in the ouptut space\n+        # Same because must be stable in the output space\n         h_i = self.conv_step_hidden(h_tm1 * B_U[0], self.U_i,\n                                     border_mode='same')\n         h_f = self.conv_step_hidden(h_tm1 * B_U[1], self.U_f,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4fa289166ad4d2acf7a9d754fb8286612a4f8cea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1918 | Contributors (this commit): 30 | Commits (past 90d): 9 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -432,7 +432,11 @@ class LearningRateScheduler(Callback):\n         assert hasattr(self.model.optimizer, 'lr'), \\\n             'Optimizer must have a \"lr\" attribute.'\n         lr = self.schedule(epoch)\n-        assert type(lr) == float, 'The output of the \"schedule\" function should be float.'\n+\n+        if not isinstance(lr, (float, np.float32, np.float64)):\n+            raise ValueError('The output of the \"schedule\" function '\n+                             'should be float.')\n+\n         K.set_value(self.model.optimizer.lr, lr)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#105dd031dd4b068e37457d1d2a2fd52029f1b574", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 245 | Lines Deleted: 238 | Files Changed: 1 | Hunks: 163 | Methods Changed: 33 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 483 | Churn Cumulative: 3827 | Contributors (this commit): 24 | Commits (past 90d): 14 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,9 +37,13 @@ class InputSpec(object):\n     '''\n     def __init__(self, dtype=None, shape=None, ndim=None):\n         if type(ndim) is str:\n-            assert '+' in ndim, 'When passing a str \"ndim\", it should have the form \"2+\", \"3+\", etc.'\n+            if '+' not in ndim:\n+                raise ValueError('When passing a str \"ndim\", '\n+                                 'it should have the form \"2+\", \"3+\", etc.')\n             int_ndim = ndim[:ndim.find('+')]\n-            assert int_ndim.isdigit(), 'When passing a str \"ndim\", it should have the form \"2+\", \"3+\", etc.'\n+            if not int_ndim.isdigit():\n+                raise ValueError('When passing a str \"ndim\", '\n+                                 'it should have the form \"2+\", \"3+\", etc.')\n         if shape is not None:\n             self.ndim = len(shape)\n         else:\n@@ -201,49 +205,48 @@ class Layer(object):\n     '''Abstract base layer class.\n \n     # Properties\n-        name: string, must be unique within a model.\n-        input_spec: list of InputSpec class instances\n+        name: String, must be unique within a model.\n+        input_spec: List of InputSpec class instances\n             each entry describes one required input:\n                 - ndim\n                 - dtype\n             A layer with `n` input tensors must have\n             an `input_spec` of length `n`.\n-        trainable: boolean, whether the layer weights\n+        trainable: Boolean, whether the layer weights\n             will be updated during training.\n-        uses_learning_phase: whether any operation\n+        uses_learning_phase: Whether any operation\n             of the layer uses `K.in_training_phase()`\n             or `K.in_test_phase()`.\n-        input_shape: shape tuple. Provided for convenience,\n+        input_shape: Shape tuple. Provided for convenience,\n             but note that there may be cases in which this\n             attribute is ill-defined (e.g. a shared layer\n             with multiple input shapes), in which case\n             requesting `input_shape` will raise an Exception.\n             Prefer using `layer.get_input_shape_for(input_shape)`,\n             or `layer.get_input_shape_at(node_index)`.\n-        output_shape: shape tuple. See above.\n-        inbound_nodes: list of nodes.\n-        outbound_nodes: list of nodes.\n-        supports_masking: boolean\n-        input, output: input/output tensor(s). Note that if the layer is used\n+        output_shape: Shape tuple. See above.\n+        inbound_nodes: List of nodes.\n+        outbound_nodes: List of nodes.\n+        supports_masking: Boolean.\n+        input, output: Input/output tensor(s). Note that if the layer is used\n             more than once (shared layer), this is ill-defined\n             and will raise an exception. In such cases, use\n             `layer.get_input_at(node_index)`.\n-        input_mask, output_mask: same as above, for masks.\n-\n-        trainable_weights: list of variables.\n-        non_trainable_weights: list of variables.\n-        regularizers: list of regularizers.\n-        constraints: dict mapping weights to constraints.\n+        input_mask, output_mask: Same as above, for masks.\n+        trainable_weights: List of variables.\n+        non_trainable_weights: List of variables.\n+        regularizers: List of regularizers.\n+        constraints: Dict mapping weights to constraints.\n \n     # Methods\n-        call(x, mask=None): where the layer's logic lives.\n-        __call__(x, mask=None): wrapper around the layer logic (`call`).\n-            if x is a Keras tensor:\n-                - connect current layer with last layer from tensor:\n+        call(x, mask=None): Where the layer's logic lives.\n+        __call__(x, mask=None): Wrapper around the layer logic (`call`).\n+            If x is a Keras tensor:\n+                - Connect current layer with last layer from tensor:\n                     `self.add_inbound_node(last_layer)`\n-                - add layer to tensor history\n-            if layer is not built:\n-                - build from x._keras_shape\n+                - Add layer to tensor history\n+            If layer is not built:\n+                - Build from x._keras_shape\n         get_weights()\n         set_weights(weights)\n         get_config()\n@@ -450,26 +453,26 @@ class Layer(object):\n         internal Keras references.\n \n         If a Keras tensor is passed:\n-            - we call self.add_inbound_node()\n-            - if necessary, we `build` the layer to match\n-                the _keras_shape of the input(s)\n-            - we update the _keras_shape of every input tensor with\n+            - We call self.add_inbound_node().\n+            - If necessary, we `build` the layer to match\n+                the _keras_shape of the input(s).\n+            - We update the _keras_shape of every input tensor with\n                 its new shape (obtained via self.get_output_shape_for).\n                 This is done as part of add_inbound_node().\n-            - we update the _keras_history of the output tensor(s)\n+            - We update the _keras_history of the output tensor(s)\n                 with the current layer.\n                 This is done as part of add_inbound_node().\n \n         # Arguments\n-            x: can be a tensor or list/tuple of tensors.\n-            mask: tensor or list/tuple of tensors.\n+            x: Can be a tensor or list/tuple of tensors.\n+            mask: Tensor or list/tuple of tensors.\n         '''\n         if not self.built:\n-            # raise exceptions in case the input is not compatible\n-            # with the input_spec specified in the layer constructor\n+            # Raise exceptions in case the input is not compatible\n+            # with the input_spec specified in the layer constructor.\n             self.assert_input_compatibility(x)\n \n-            # collect input shapes to build layer\n+            # Collect input shapes to build layer.\n             input_shapes = []\n             for x_elem in to_list(x):\n                 if hasattr(x_elem, '_keras_shape'):\n@@ -477,7 +480,7 @@ class Layer(object):\n                 elif hasattr(K, 'int_shape'):\n                     input_shapes.append(K.int_shape(x_elem))\n                 else:\n-                    raise Exception('You tried to call layer \"' + self.name +\n+                    raise ValueError('You tried to call layer \"' + self.name +\n                                      '\". This layer has no information'\n                                      ' about its expected input shape, '\n                                      'and thus cannot be built. '\n@@ -489,13 +492,11 @@ class Layer(object):\n                 self.build(input_shapes)\n             self.built = True\n \n-        # raise exceptions in case the input is not compatible\n-        # with the input_spec set at build time\n+        # Raise exceptions in case the input is not compatible\n+        # with the input_spec set at build time.\n         self.assert_input_compatibility(x)\n-        # build and connect layer\n-        input_added = False\n-        input_tensors = to_list(x)\n \n+        input_tensors = to_list(x)\n         inbound_layers = []\n         node_indices = []\n         tensor_indices = []\n@@ -509,37 +510,34 @@ class Layer(object):\n             else:\n                 inbound_layers = None\n                 break\n-        if inbound_layers:\n-            # this will call layer.build() if necessary\n-            self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n-            input_added = True\n \n-        # get the output tensor to be returned\n-        if input_added:\n-            # output was already computed when calling self.add_inbound_node\n+        if inbound_layers:\n+            # This will call layer.build() if necessary.\n+            self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n+            # Outputs were already computed when calling self.add_inbound_node.\n             outputs = self.inbound_nodes[-1].output_tensors\n-            # if single output tensor: return it,\n-            # else return a list (at least 2 elements)\n+            # If single output tensor: return it,\n+            # else return a list (at least 2 elements).\n             if len(outputs) == 1:\n                 return outputs[0]\n             else:\n                 return outputs\n         else:\n-            # this case appears if the input was not a Keras tensor\n+            # This case appears if the input was not a Keras tensor.\n             return self.call(x, mask)\n \n     def add_inbound_node(self, inbound_layers,\n                          node_indices=None, tensor_indices=None):\n         '''\n-        # Arguments:\n-            inbound_layers: can be a layer instance\n+        # Arguments\n+            inbound_layers: Can be a layer instance\n                 or a list/tuple of layer instances.\n-            node_indices: integer (or list of integers).\n+            node_indices: Integer (or list of integers).\n                 The input layer might have a number of\n                 parallel output streams;\n                 this is the index of the stream (in the input layer)\n                 where to connect the current layer.\n-            tensor_indices: integer or list of integers.\n+            tensor_indices: Integer or list of integers.\n                 The output of the inbound node might be a list/tuple\n                 of tensor, and we might only be interested in one specific entry.\n                 This index allows you to specify the index of the entry in the output list\n@@ -577,7 +575,7 @@ class Layer(object):\n         to match that input shape).\n \n         # Arguments\n-            input_shape: shape tuple (tuple of integers)\n+            input_shape: Shape tuple (tuple of integers)\n                 or list of shape tuples (one per output tensor of the layer).\n                 Shape tuples can include None for free dimensions,\n                 instead of an integer.\n@@ -589,8 +587,8 @@ class Layer(object):\n         (or list thereof) and an input mask (or list thereof).\n \n         # Arguments\n-            input: tensor or list of tensors.\n-            input_mask: tensor or list of tensors.\n+            input: Tensor or list of tensors.\n+            input_mask: Tensor or list of tensors.\n \n         # Returns\n             None or a tensor (or list of tensors,\n@@ -626,10 +624,10 @@ class Layer(object):\n         '''Retrieves an attribute (e.g. input_tensors) from a node.\n \n         # Arguments\n-            node_index: integer index of the node from which\n-                to retrieve the attribute\n-            attr: exact node attribute name\n-            attr_name: human-readable attribute name, for error messages\n+            node_index: Integer index of the node from which\n+                to retrieve the attribute.\n+            attr: Exact node attribute name.\n+            attr_name: Human-readable attribute name, for error messages.\n         '''\n         if not self.inbound_nodes:\n             raise Exception('The layer has never been called ' +\n@@ -710,7 +708,7 @@ class Layer(object):\n             raise Exception('Cannot `set_input` for layer ' + self.name +\n                             ' because it has more than one inbound connection.')\n         if len(self.inbound_nodes) == 1:\n-            # check that the inbound node is an Input node\n+            # Check that the inbound node is an Input node.\n             if self.inbound_nodes[0].inbound_layers:\n                 warnings.warn('You are manually setting the input for layer ' +\n                               self.name + ' but it is not an Input layer. '\n@@ -724,7 +722,7 @@ class Layer(object):\n                           'This will cause part of your model '\n                           'to be disconnected.')\n         if hasattr(K, 'int_shape'):\n-            # auto-infered shape takes priority\n+            # Auto-infered shape takes priority.\n             shape = K.int_shape(input_tensor)\n         elif not shape:\n             raise Exception('`set_input` needs to know the shape '\n@@ -732,13 +730,13 @@ class Layer(object):\n                             'Keras was not able to infer it automatically.'\n                             ' Specify it via: '\n                             '`model.set_input(input_tensor, shape)`')\n-        # reset layer connections\n+        # Reset layer connections.\n         self.inbound_nodes = []\n         self.outbound_nodes = []\n         input_shape = tuple(shape)\n         self.build(input_shape=input_shape)\n \n-        # set Keras tensor metadata\n+        # Set Keras tensor metadata.\n         input_tensor._uses_learning_phase = False\n         input_tensor._keras_history = (None, 0, 0)\n         input_tensor._keras_shape = input_shape\n@@ -752,7 +750,7 @@ class Layer(object):\n             output_tensor._keras_shape = output_shapes[i]\n             output_tensor._uses_learning_phase = self.uses_learning_phase\n \n-        # create node\n+        # Create node.\n         Node(self,\n              inbound_layers=[],\n              node_indices=[],\n@@ -923,7 +921,7 @@ class Layer(object):\n         (handled by Container), nor weights (handled by `set_weights`).\n \n         # Arguments\n-            config: a Python dictionary, typically the\n+            config: A Python dictionary, typically the\n                 output of get_config.\n         '''\n         return cls(**config)\n@@ -944,7 +942,20 @@ class Layer(object):\n \n \n class InputLayer(Layer):\n-    '''TODO: dosctring\n+    '''Layer to be used as an entry point into a graph.\n+    It can either wrap an existing tensor (pass an `input_tensor` argument)\n+    or create its a placeholder tensor (pass arguments `input_shape`\n+    or `batch_input_shape` as well as `input_dtype`).\n+\n+    # Arguments\n+        input_shape: Shape tuple, not including the batch axis.\n+        batch_input_shape: Shape tuple, including the batch axis.\n+        input_dtype: Datatype of the input.\n+        input_tensor: Optional tensor to use as layer input\n+            instead of creating a placeholder.\n+        sparse: Boolean, whether the placeholder created\n+            is meant to be sparse.\n+        name: Name of the layer (string).\n     '''\n     def __init__(self, input_shape=None, batch_input_shape=None,\n                  input_dtype=None, input_tensor=None, sparse=False, name=None):\n@@ -976,7 +987,7 @@ class InputLayer(Layer):\n                              'batch_input_shape argument to '\n                              'InputLayer, not both at the same time.')\n         if input_tensor is not None:\n-            # attempt automatic input shape inference\n+            # Attempt automatic input shape inference.\n             try:\n                 batch_input_shape = K.int_shape(input_tensor)\n             except:\n@@ -1010,8 +1021,8 @@ class InputLayer(Layer):\n                                          name=self.name)\n         else:\n             input_tensor._keras_shape = batch_input_shape\n-        # create an input node to add to self.outbound_node\n-        # and set output_tensors' _keras_history\n+        # Create an input node to add to self.outbound_node\n+        # and set output_tensors' _keras_history.\n         input_tensor._uses_learning_phase = False\n         input_tensor._keras_history = (self, 0, 0)\n         Node(self,\n@@ -1047,17 +1058,17 @@ def Input(shape=None, batch_shape=None,\n     `model = Model(input=[a, b], output=c)`\n \n     The added Keras attributes are:\n-        ._keras_shape: integer shape tuple propagated\n+        ._keras_shape: Integer shape tuple propagated\n             via Keras-side shape inference.\n-        ._keras_history: last layer applied to the tensor.\n+        ._keras_history: Last layer applied to the tensor.\n             the entire layer graph is retrievable from that layer,\n             recursively.\n \n     # Arguments\n-        shape: a shape tuple (integer), not including the batch size.\n+        shape: A shape tuple (integer), not including the batch size.\n             For instance, `shape=(32,)` indicates that the expected input\n             will be batches of 32-dimensional vectors.\n-        batch_shape: a shape tuple (integer), including the batch size.\n+        batch_shape: A shape tuple (integer), including the batch size.\n             For instance, `batch_shape=(10, 32)` indicates that\n             the expected input will be batches of 10 32-dimensional vectors.\n             `batch_shape=(None, 32)` indicates batches of an arbitrary number\n@@ -1067,7 +1078,8 @@ def Input(shape=None, batch_shape=None,\n             It will be autogenerated if it isn't provided.\n         dtype: The data type expected by the input, as a string\n             (`float32`, `float64`, `int32`...)\n-        sparse: a boolean specifying whether this will be a sparse tensor\n+        sparse: A boolean specifying whether the placeholder\n+            to be created is sparse.\n \n     # Example usage\n \n@@ -1089,8 +1101,8 @@ def Input(shape=None, batch_shape=None,\n                              name=name, input_dtype=dtype,\n                              sparse=sparse,\n                              input_tensor=tensor)\n-    # return tensor including _keras_shape and _keras_history\n-    # note that in this case train_output and test_output are the same pointer.\n+    # Return tensor including _keras_shape and _keras_history.\n+    # Note that in this case train_output and test_output are the same pointer.\n     outputs = input_layer.inbound_nodes[0].output_tensors\n     if len(outputs) == 1:\n         return outputs[0]\n@@ -1113,21 +1125,19 @@ class Merge(Layer):\n \n     merged_model = Sequential()\n     merged_model.add(Merge([model1, model2], mode='concat', concat_axis=1)\n-    # TODO: would this actually work? it needs to.\n-    # achieve this with get_source_inputs in Sequential.\n     ```\n \n     # Arguments\n-        layers: can be a list of Keras tensors or\n+        layers: Can be a list of Keras tensors or\n             a list of layer instances. Must be more\n             than one layer/tensor.\n-        mode: string or lambda/function. If string, must be one\n+        mode: String or lambda/function. If string, must be one\n             of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n-        concat_axis: integer, axis to use in mode `concat`.\n-        dot_axes: integer or tuple of integers, axes to use in mode `dot` or `cos`.\n-        output_shape: either a shape tuple (tuple of integers), or a lambda/function\n+        concat_axis: Integer, axis to use in mode `concat`.\n+        dot_axes: Integer or tuple of integers, axes to use in mode `dot` or `cos`.\n+        output_shape: Either a shape tuple (tuple of integers), or a lambda/function\n             to compute `output_shape` (only if merge mode is a lambda/function).\n             If the argument is a tuple,\n             it should be expected output shape, *not* including the batch size\n@@ -1135,14 +1145,14 @@ class Merge(Layer):\n             If the argument is callable, it should take as input a list of shape tuples\n             (1:1 mapping to input tensors) and return a single shape tuple, including the\n             batch size (same convention as the `get_output_shape_for` method of layers).\n-        node_indices: optional list of integers containing\n+        node_indices: Optional list of integers containing\n             the output node index for each input layer\n             (in case some input layers have multiple output nodes).\n             will default to an array of 0s if not provided.\n-        tensor_indices: optional list of indices of output tensors\n+        tensor_indices: Optional list of indices of output tensors\n             to consider for merging\n             (in case some input layer node returns multiple tensors).\n-        output_mask: mask or lambda/function to compute the output mask (only\n+        output_mask: Mask or lambda/function to compute the output mask (only\n             if merge mode is a lambda/function). If the latter case, it should\n             take as input a list of masks and return a single mask.\n     '''\n@@ -1157,7 +1167,7 @@ class Merge(Layer):\n         self.node_indices = node_indices\n         self._output_mask = output_mask\n \n-        # layer parameters\n+        # Layer parameters.\n         self.inbound_nodes = []\n         self.outbound_nodes = []\n         self.constraints = {}\n@@ -1166,20 +1176,20 @@ class Merge(Layer):\n         self.non_trainable_weights = []\n         self.supports_masking = True\n         self.uses_learning_phase = False\n-        self.input_spec = None  # compatible with whatever\n+        self.input_spec = None  # Compatible with anything.\n         if not name:\n             prefix = self.__class__.__name__.lower()\n             name = prefix + '_' + str(K.get_uid(prefix))\n         self.name = name\n \n         if layers:\n-            # this exists for backwards compatibility.\n+            # This exists for backwards compatibility.\n             # equivalent to:\n             # merge = Merge(layers=None)\n             # output = merge([input_tensor_1, input_tensor_2])\n             if not node_indices:\n-                # by default we connect to\n-                # the 1st output stream in the input layer\n+                # By default we connect to\n+                # the 1st output stream in the input layer.\n                 node_indices = [0 for _ in range(len(layers))]\n             self._arguments_validation(layers, mode,\n                                        concat_axis, dot_axes,\n@@ -1208,8 +1218,8 @@ class Merge(Layer):\n         for i, layer in enumerate(layers):\n             layer_output_shape = layer.get_output_shape_at(node_indices[i])\n             if type(layer_output_shape) is list:\n-                # case: the layer has multiple output tensors\n-                # and we only need a specific one\n+                # Case: the layer has multiple output tensors\n+                # and we only need a specific one.\n                 layer_output_shape = layer_output_shape[tensor_indices[i]]\n             input_shapes.append(layer_output_shape)\n \n@@ -1256,7 +1266,7 @@ class Merge(Layer):\n         if type(inputs) is not list or len(inputs) <= 1:\n             raise Exception('Merge must be called on a list of tensors '\n                             '(at least 2). Got: ' + str(inputs))\n-        # case: \"mode\" is a lambda or function.\n+        # Case: \"mode\" is a lambda or function.\n         if hasattr(self.mode, '__call__'):\n             # TODO: consider making it possible to\n             # pass custom arguments to lambda.\n@@ -1339,13 +1349,13 @@ class Merge(Layer):\n             self.add_inbound_node(layers, node_indices, tensor_indices)\n \n             outputs = self.inbound_nodes[-1].output_tensors\n-            return outputs[0]  # merge only returns a single tensor\n+            return outputs[0]  # Merge only returns a single tensor.\n         else:\n             return self.call(inputs, mask)\n \n     def get_output_shape_for(self, input_shape):\n-        assert type(input_shape) is list  # must have multiple input shape tuples\n-        # case: callable self._output_shape\n+        assert type(input_shape) is list  # Must have multiple input shape tuples.\n+        # Case: callable self._output_shape.\n         if hasattr(self.mode, '__call__'):\n             if hasattr(self._output_shape, '__call__'):\n                 output_shape = self._output_shape(input_shape)\n@@ -1353,17 +1363,17 @@ class Merge(Layer):\n             elif self._output_shape is not None:\n                 return (input_shape[0][0],) + tuple(self._output_shape)\n             else:\n-                # TODO: consider shape auto-inference with TF\n+                # TODO: consider shape auto-inference with TF.\n                 raise Exception('The Merge layer ' + self.name +\n                                 ' has a callable `mode` argument, ' +\n                                 'and we cannot infer its output shape because ' +\n                                 'no `output_shape` argument was provided.' +\n                                 'Make sure to pass a shape tuple (or a callable) ' +\n                                 '`output_shape` to Merge.')\n-        # pre-defined merge modes\n+        # Pre-defined merge modes.\n         input_shapes = input_shape\n         if self.mode in ['sum', 'mul', 'ave', 'max']:\n-            # all tuples in input_shapes should be the same\n+            # All tuples in input_shapes should be the same.\n             return input_shapes[0]\n         elif self.mode == 'concat':\n             output_shape = list(input_shapes[0])\n@@ -1416,7 +1426,7 @@ class Merge(Layer):\n             else:\n                 return self._output_mask\n         else:\n-            # this should have been caught earlier\n+            # This should have been caught earlier.\n             raise Exception('Invalid merge mode: {}'.format(self.mode))\n \n     def get_config(self):\n@@ -1485,22 +1495,22 @@ def merge(inputs, mode='sum', concat_axis=-1,\n     ```\n \n     # Arguments\n-        mode: string or lambda/function. If string, must be one\n+        mode: String or lambda/function. If string, must be one\n             of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n             If lambda/function, it should take as input a list of tensors\n             and return a single tensor.\n-        concat_axis: integer, axis to use in mode `concat`.\n-        dot_axes: integer or tuple of integers, axes to use in mode `dot` or `cos`.\n-        output_shape: shape tuple (tuple of integers), or lambda/function\n+        concat_axis: Integer, axis to use in mode `concat`.\n+        dot_axes: Integer or tuple of integers, axes to use in mode `dot` or `cos`.\n+        output_shape: Shape tuple (tuple of integers), or lambda/function\n             to compute output_shape (only if merge mode is a lambda/function).\n             If the latter case, it should take as input a list of shape tuples\n             (1:1 mapping to input tensors) and return a single shape tuple, including the\n             batch size (same convention as the `get_output_shape_for` method of layers).\n-        node_indices: optional list of integers containing\n+        node_indices: Optional list of integers containing\n             the output node index for each input layer\n             (in case some input layers have multiple output nodes).\n             will default to an array of 0s if not provided.\n-        tensor_indices: optional list of indices of output tensors\n+        tensor_indices: Optional list of indices of output tensors\n             to consider for merging\n             (in case some input layer node returns multiple tensors).\n     '''\n@@ -1538,7 +1548,10 @@ def merge(inputs, mode='sum', concat_axis=-1,\n \n \n class Container(Layer):\n-    '''TODO: dosctring\n+    '''A Container is a directed acyclic graph of layers.\n+\n+    It is the topological form of a \"model\". A Model\n+    is simply a Container with added training routines.\n \n     # Properties\n         name\n@@ -1546,7 +1559,6 @@ class Container(Layer):\n         outputs\n         input_layers\n         output_layers\n-\n         input_spec (list of class instances)\n             each entry describes one required input:\n                 - ndim\n@@ -1556,9 +1568,7 @@ class Container(Layer):\n         output_shape\n         inbound_nodes: list of nodes\n         outbound_nodes: list of nodes\n-\n-        (supports_masking (boolean))\n-\n+        supports_masking (boolean)\n         trainable_weights (list of variables)\n         non_trainable_weights (list of variables)\n         regularizers (list of regularizers)\n@@ -1576,18 +1586,18 @@ class Container(Layer):\n         from_config\n     '''\n     def __init__(self, input, output, name=None):\n-        # handle name argument\n+        # Handle name argument.\n         if not name:\n             prefix = self.__class__.__name__.lower()\n             name = prefix + '_' + str(K.get_uid(prefix))\n         self.name = name\n \n-        # whether container weights are trainable\n+        # Whether container weights are trainable.\n         self.trainable = True\n \n-        # Container-specific properties\n+        # Container-specific properties.\n         if type(input) in {list, tuple}:\n-            self.inputs = list(input)  # tensor or list of tensors\n+            self.inputs = list(input)  # Tensor or list of tensors.\n         else:\n             self.inputs = [input]\n         if type(output) in {list, tuple}:\n@@ -1595,14 +1605,14 @@ class Container(Layer):\n         else:\n             self.outputs = [output]\n \n-        # check for redundancy in inputs:\n+        # Check for redundancy in inputs.\n         inputs_set = set(self.inputs)\n         if len(inputs_set) != len(self.inputs):\n             raise Exception('The list of inputs passed to the model '\n                             'is redundant. All inputs should only appear once.'\n                             ' Found: ' + str(self.inputs))\n \n-        # list of initial layers (1 to 1 mapping with self.inputs,\n+        # List of initial layers (1 to 1 mapping with self.inputs,\n         # hence the same layer might appear twice)\n         self.input_layers = []\n         # TODO: probably useless because input layers must be Input layers (node_indices = [0], tensor_indices = [0])\n@@ -1629,15 +1639,15 @@ class Container(Layer):\n         self._output_tensor_cache = {}\n         self._output_shape_cache = {}\n \n-        # arguments validation\n+        # Arguments validation.\n         for x in self.inputs:\n-            # check that x is a Keras tensor\n+            # Check that x is a Keras tensor.\n             if not hasattr(x, '_keras_history'):\n                 cls_name = self.__class__.__name__\n                 raise Exception('Input tensors to a ' + cls_name + ' ' +\n                                 'must be Keras tensors. Found: ' + str(x) +\n                                 ' (missing Keras metadata).')\n-            # check that x is an input tensor\n+            # Check that x is an input tensor.\n             layer, node_index, tensor_index = x._keras_history\n             if len(layer.inbound_nodes) > 1 or (layer.inbound_nodes and layer.inbound_nodes[0].inbound_layers):\n                 cls_name = self.__class__.__name__\n@@ -1659,14 +1669,14 @@ class Container(Layer):\n                 cls_name = self.__class__.__name__\n                 raise Exception('Output tensors to a ' + cls_name + ' must be '\n                                 'Keras tensors. Found: ' + str(x))\n-        # build self.output_layers:\n+        # Build self.output_layers:\n         for x in self.outputs:\n             layer, node_index, tensor_index = x._keras_history\n             self.output_layers.append(layer)\n             self.output_layers_node_indices.append(node_index)\n             self.output_layers_tensor_indices.append(tensor_index)\n \n-        # fill in the output mask cache\n+        # Fill in the output mask cache.\n         masks = []\n         for x in self.inputs:\n             layer, node_index, tensor_index = x._keras_history\n@@ -1687,18 +1697,18 @@ class Container(Layer):\n             mask = masks\n         self._output_mask_cache[mask_cache_key] = mask\n \n-        # build self.input_layers:\n+        # Build self.input_layers:\n         for x in self.inputs:\n             layer, node_index, tensor_index = x._keras_history\n-            # it's supposed to be an input layer, so only one node\n-            # and one tensor output\n+            # It's supposed to be an input layer, so only one node\n+            # and one tensor output.\n             assert node_index == 0\n             assert tensor_index == 0\n             self.input_layers.append(layer)\n             self.input_layers_node_indices.append(node_index)\n             self.input_layers_tensor_indices.append(tensor_index)\n \n-        # build self.input_names and self.output_names\n+        # Build self.input_names and self.output_names.\n         self.input_names = []\n         self.output_names = []\n         for layer in self.input_layers:\n@@ -1709,12 +1719,12 @@ class Container(Layer):\n         self.internal_input_shapes = [x._keras_shape for x in self.inputs]\n         self.internal_output_shapes = [x._keras_shape for x in self.outputs]\n \n-        # container_nodes: set of nodes included in the graph\n+        # Container_nodes: set of nodes included in the graph\n         # (not all nodes included in the layers are relevant to the current graph).\n         container_nodes = set()  # ids of all nodes relevant to the Container\n-        nodes_depths = {}  # map {node: depth value}\n-        layers_depths = {}  # map {layer: depth value}\n-        layer_indices = {}  # map {layer: index in traversal}\n+        nodes_depths = {}  # dict {node: depth value}\n+        layers_depths = {}  # dict {layer: depth value}\n+        layer_indices = {}  # dict {layer: index in traversal}\n \n         def make_node_marker(node, depth):\n             return str(id(node)) + '-' + str(depth)\n@@ -1726,32 +1736,32 @@ class Container(Layer):\n             Does not try to detect cycles in graph (TODO?)\n \n             # Arguments\n-                tensor: some tensor in a graph\n-                seen_nodes: set of node ids (\"{layer.name}_ib-{node_index}\")\n+                tensor: Some tensor in a graph.\n+                seen_nodes: Set of node ids (\"{layer.name}_ib-{node_index}\")\n                     of nodes seen so far. Useful to prevent infinite loops.\n-                depth: current depth in the graph (0 = last output).\n-                layer: layer from which `tensor` comes from. If not provided,\n+                depth: Current depth in the graph (0 = last output).\n+                layer: Layer from which `tensor` comes from. If not provided,\n                     will be obtained from `tensor._keras_history`.\n-                node_index: node index from which `tensor` comes from.\n-                tensor_index: tensor_index from which `tensor` comes from.\n+                node_index: Node index from which `tensor` comes from.\n+                tensor_index: Tensor_index from which `tensor` comes from.\n             '''\n             if not layer or node_index is None or tensor_index is None:\n                 layer, node_index, tensor_index = tensor._keras_history\n             node = layer.inbound_nodes[node_index]\n \n-            # prevent cycles\n+            # Prevent cycles.\n             seen_nodes.add(make_node_marker(node, depth))\n \n             node_key = layer.name + '_ib-' + str(node_index)\n-            # update container_nodes\n+            # Update container_nodes.\n             container_nodes.add(node_key)\n-            # update nodes_depths\n+            # Update nodes_depths.\n             node_depth = nodes_depths.get(node)\n             if node_depth is None:\n                 nodes_depths[node] = depth\n             else:\n                 nodes_depths[node] = max(depth, node_depth)\n-            # update layers_depths\n+            # Update layers_depths.\n             previously_seen_depth = layers_depths.get(layer)\n             if previously_seen_depth is None:\n                 current_depth = depth\n@@ -1761,7 +1771,7 @@ class Container(Layer):\n             if layer not in layer_indices:\n                 layer_indices[layer] = len(layer_indices)\n \n-            # propagate to all previous tensors connected to this node\n+            # Propagate to all previous tensors connected to this node.\n             for i in range(len(node.inbound_layers)):\n                 x = node.input_tensors[i]\n                 layer = node.inbound_layers[i]\n@@ -1778,30 +1788,30 @@ class Container(Layer):\n             seen_nodes = set()\n             build_map_of_graph(x, seen_nodes, depth=0)\n \n-        # build a map {depth: list of nodes with this depth}\n+        # Build a dict {depth: list of nodes with this depth}\n         nodes_by_depth = {}\n         for node, depth in nodes_depths.items():\n             if depth not in nodes_by_depth:\n                 nodes_by_depth[depth] = []\n             nodes_by_depth[depth].append(node)\n \n-        # build a map {depth: list of layers with this depth}\n+        # Build a dict {depth: list of layers with this depth}\n         layers_by_depth = {}\n         for layer, depth in layers_depths.items():\n             if depth not in layers_by_depth:\n                 layers_by_depth[depth] = []\n             layers_by_depth[depth].append(layer)\n \n-        # get sorted list of layer depths\n+        # Get sorted list of layer depths.\n         depth_keys = list(layers_by_depth.keys())\n         depth_keys.sort(reverse=True)\n \n-        # set self.layers and self.layers_by_depth\n+        # Set self.layers and self.layers_by_depth.\n         layers = []\n         for depth in depth_keys:\n             layers_for_depth = layers_by_depth[depth]\n-            # container.layers needs to have a deterministic order:\n-            # here we order them by traversal order\n+            # Container.layers needs to have a deterministic order:\n+            # here we order them by traversal order.\n             if K.legacy_weight_ordering():\n                 layers_for_depth.sort(key=lambda x: x.name)\n             else:\n@@ -1811,18 +1821,18 @@ class Container(Layer):\n         self.layers = layers\n         self.layers_by_depth = layers_by_depth\n \n-        # get sorted list of node depths\n+        # Get sorted list of node depths.\n         depth_keys = list(nodes_by_depth.keys())\n         depth_keys.sort(reverse=True)\n \n-        # check that all tensors required are computable.\n+        # Check that all tensors required are computable.\n         # computable_tensors: all tensors in the graph\n-        # that can be computed from the inputs provided\n+        # that can be computed from the inputs provided.\n         computable_tensors = []\n         for x in self.inputs:\n             computable_tensors.append(x)\n \n-        layers_with_complete_input = []  # to provide a better error msg\n+        layers_with_complete_input = []  # To provide a better error msg.\n         for depth in depth_keys:\n             for node in nodes_by_depth[depth]:\n                 layer = node.outbound_layer\n@@ -1840,11 +1850,11 @@ class Container(Layer):\n                         computable_tensors.append(x)\n                     layers_with_complete_input.append(layer.name)\n \n-        # set self.nodes and self.nodes_by_depth\n+        # Set self.nodes and self.nodes_by_depth.\n         self.container_nodes = container_nodes\n         self.nodes_by_depth = nodes_by_depth\n \n-        # ensure name unicity, which will be crucial for serialization\n+        # Ensure name unicity, which will be crucial for serialization\n         # (since serialized nodes refer to layers by their name).\n         all_names = [layer.name for layer in self.layers]\n         for name in all_names:\n@@ -1854,26 +1864,26 @@ class Container(Layer):\n                                 ' times in the model. ' +\n                                 'All layer names should be unique.')\n \n-        # layer parameters\n-        # the new container starts with a single inbound node\n+        # Layer parameters.\n+        # The new container starts with a single inbound node\n         # for its inputs, and no outbound nodes.\n-        self.outbound_nodes = []  # will be appended to by future calls to __call__\n-        self.inbound_nodes = []  # will be appended to below, and by future calls to __call__\n-        # create the node linking internal inputs to internal outputs\n+        self.outbound_nodes = []  # Will be appended to by future calls to __call__\n+        self.inbound_nodes = []  # Will be appended to below, and by future calls to __call__\n+        # Create the node linking internal inputs to internal outputs.\n         Node(outbound_layer=self,\n              inbound_layers=[],\n              node_indices=[],\n              tensor_indices=[],\n              input_tensors=self.inputs,\n              output_tensors=self.outputs,\n-             # no container-level masking for now\n+             # No container-level masking for now.\n              input_masks=[None for _ in self.inputs],\n              output_masks=[None for _ in self.outputs],\n              input_shapes=[x._keras_shape for x in self.inputs],\n              output_shapes=[x._keras_shape for x in self.outputs])\n         self.built = True\n         self.supports_masking = False\n-        # the following are implemented as property functions:\n+        # The following are implemented as property functions:\n         # self.constraints\n         # self.regularizers\n         # self.trainable_weights\n@@ -1886,16 +1896,16 @@ class Container(Layer):\n         order of horizontal graph traversal (bottom-up).\n \n         # Arguments\n-            name: string, name of layer.\n-            index: integer, index of layer.\n+            name: String, name of layer.\n+            index: Integer, index of layer.\n \n         # Returns\n             A layer instance.\n         '''\n-        # it would be unreliable to build a dictionary\n+        # It would be unreliable to build a dictionary\n         # based on layer names, because names can potentially\n         # be changed at any point by the user\n-        # without the container being notified of it\n+        # without the container being notified of it.\n         if index:\n             if len(self.layers) <= index:\n                 raise Exception('Was asked to retrieve layer at index ' +\n@@ -2035,8 +2045,8 @@ class Container(Layer):\n         It is callable on non-Keras tensors.\n \n         # Arguments\n-            input: a tensor or list of tensors.\n-            mask: a mask or list of masks. A mask can be\n+            input: A tensor or list of tensors.\n+            mask: A mask or list of masks. A mask can be\n                 either a tensor or None (no mask).\n \n         # Returns\n@@ -2084,31 +2094,31 @@ class Container(Layer):\n                 return output_shapes[0]\n             return output_shapes\n         else:\n-            # bad luck, have to run the graph manually\n+            # Bad luck, we have to run the graph manually.\n             layers_to_output_shapes = {}\n             for i in range(len(input_shapes)):\n                 layer = self.input_layers[i]\n                 input_shape = input_shapes[i]\n-                # it's an input layer: get_output_shape_for is identity,\n+                # It's an input layer: get_output_shape_for is identity,\n                 # and there is only one node and one tensor output.\n                 shape_key = layer.name + '_0_0'\n                 layers_to_output_shapes[shape_key] = input_shape\n \n             depth_keys = list(self.nodes_by_depth.keys())\n             depth_keys.sort(reverse=True)\n-            # iterate over nodes, by depth level\n+            # Iterate over nodes, by depth level.\n             if len(depth_keys) > 1:\n                 for depth in depth_keys:\n                     nodes = self.nodes_by_depth[depth]\n                     for node in nodes:\n-                        # this is always a single layer, never a list\n+                        # This is always a single layer, never a list.\n                         layer = node.outbound_layer\n                         if layer in self.input_layers:\n-                            # we've already covered the input layers\n-                            # a few lines above\n+                            # We've already covered the input layers\n+                            # a few lines above.\n                             continue\n-                        # potentially redundant list,\n-                        # same size of node.input_tensors\n+                        # Potentially redundant list,\n+                        # same size of node.input_tensors.\n                         input_shapes = []\n                         for j in range(len(node.inbound_layers)):\n                             inbound_layer = node.inbound_layers[j]\n@@ -2129,7 +2139,7 @@ class Container(Layer):\n                             shape_key = layer.name + '_%s_%s' % (node_index, j)\n                             layers_to_output_shapes[shape_key] = output_shapes[j]\n \n-            # read final output shapes from layers_to_output_shapes\n+            # Read final output shapes from layers_to_output_shapes.\n             output_shapes = []\n             output_shape_keys = []\n             for i in range(len(self.output_layers)):\n@@ -2142,7 +2152,7 @@ class Container(Layer):\n             for i, key in enumerate(output_shape_keys):\n                 assert key in layers_to_output_shapes\n                 output_shapes.append(layers_to_output_shapes[key])\n-            # store in cache\n+            # Store in cache.\n             self._output_shape_cache[cache_key] = output_shapes\n             if type(output_shapes) is list and len(output_shapes) == 1:\n                 return output_shapes[0]\n@@ -2152,12 +2162,12 @@ class Container(Layer):\n         '''Computes output tensors for new inputs.\n \n         # Note:\n-            - expects `inputs` to be a list (potentially with 1 element).\n-            - can be run on non-Keras tensors.\n+            - Expects `inputs` to be a list (potentially with 1 element).\n+            - Can be run on non-Keras tensors.\n \n         # Arguments\n-            inputs: list of tensors\n-            masks: list of masks (tensors or None).\n+            inputs: List of tensors\n+            masks: List of masks (tensors or None).\n \n         # Returns\n             Three lists: output_tensors, output_masks, output_shapes\n@@ -2167,7 +2177,7 @@ class Container(Layer):\n             masks = [None for _ in range(len(inputs))]\n         assert type(masks) is list\n \n-        # dictionary mapping reference tensors to tuples (computed tensor, compute mask)\n+        # Dictionary mapping reference tensors to tuples (computed tensor, compute mask)\n         # we assume a 1:1 mapping from tensor to mask\n         # TODO: raise exception when a .compute_mask does not return a list the same size as call\n         tensor_map = {}\n@@ -2179,15 +2189,15 @@ class Container(Layer):\n         for depth in depth_keys:\n             nodes = self.nodes_by_depth[depth]\n             for node in nodes:\n-                # this is always a single layer, never a list\n+                # This is always a single layer, never a list.\n                 layer = node.outbound_layer\n \n                 reference_input_tensors = node.input_tensors\n                 reference_output_tensors = node.output_tensors\n \n-                # if all previous input tensors are available in tensor_map,\n-                # then call node.inbound_layer on them\n-                computed_data = []  # list of tuples (input, mask)\n+                # If all previous input tensors are available in tensor_map,\n+                # then call node.inbound_layer on them.\n+                computed_data = []  # List of tuples (input, mask).\n                 for x in reference_input_tensors:\n                     if str(id(x)) in tensor_map:\n                         computed_data.append(tensor_map[str(id(x))])\n@@ -2205,7 +2215,7 @@ class Container(Layer):\n                         output_tensors = to_list(layer.call(computed_tensors, computed_masks))\n                         output_masks = to_list(layer.compute_mask(computed_tensors, computed_masks))\n \n-                    # update _keras_shape\n+                    # Update _keras_shape.\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n                         if len(computed_tensors) == 1:\n                             shapes = to_list(layer.get_output_shape_for(computed_tensors[0]._keras_shape))\n@@ -2217,7 +2227,7 @@ class Container(Layer):\n                             x._keras_shape = s\n                             x._uses_learning_phase = uses_learning_phase\n \n-                    # update tensor_map\n+                    # Update tensor_map.\n                     for x, y, mask in zip(reference_output_tensors, output_tensors, output_masks):\n                         tensor_map[str(id(x))] = (y, mask)\n \n@@ -2225,7 +2235,7 @@ class Container(Layer):\n         output_masks = []\n         output_shapes = []\n         for x in self.outputs:\n-            # todo: better error msg\n+            # TODO: Better error message.\n             assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\n             tensor, mask = tensor_map[str(id(x))]\n             if hasattr(tensor, '_keras_shape') and output_shapes is not None:\n@@ -2236,7 +2246,7 @@ class Container(Layer):\n             output_tensors.append(tensor)\n             output_masks.append(mask)\n \n-        # update cache; keys are based on ids on input tensors and inputs masks\n+        # Update cache; keys are based on ids on input tensors and inputs masks.\n         cache_key = ','.join([str(id(x)) for x in inputs])\n         cache_key += '_' + ','.join([str(id(x)) for x in masks])\n \n@@ -2269,8 +2279,8 @@ class Container(Layer):\n         node_conversion_map = {}\n         for layer in self.layers:\n             if issubclass(layer.__class__, Container):\n-                # containers start with a pre-existing node\n-                # linking their input to output\n+                # Containers start with a pre-existing node\n+                # linking their input to output.\n                 kept_nodes = 1\n             else:\n                 kept_nodes = 0\n@@ -2280,15 +2290,15 @@ class Container(Layer):\n                     node_conversion_map[node_key] = kept_nodes\n                     kept_nodes += 1\n         layer_configs = []\n-        for layer in self.layers:  # from the earliest layers on\n+        for layer in self.layers:  # From the earliest layers on.\n             layer_class_name = layer.__class__.__name__\n             layer_config = layer.get_config()\n             filtered_inbound_nodes = []\n             for original_node_index, node in enumerate(layer.inbound_nodes):\n                 node_key = layer.name + '_ib-' + str(original_node_index)\n                 if node_key in self.container_nodes:\n-                    # the node is relevant to the model:\n-                    # add to filtered_inbound_nodes\n+                    # The node is relevant to the model:\n+                    # add to filtered_inbound_nodes.\n                     if node.inbound_layers:\n                         node_data = []\n                         for i in range(len(node.inbound_layers)):\n@@ -2296,7 +2306,6 @@ class Container(Layer):\n                             node_index = node.node_indices[i]\n                             tensor_index = node.tensor_indices[i]\n                             node_key = inbound_layer.name + '_ib-' + str(node_index)\n-                            # assert node_key in node_conversion_map, 'Node never seen before: %s' % node_key\n                             new_node_index = node_conversion_map.get(node_key, 0)\n                             node_data.append([inbound_layer.name,\n                                               new_node_index,\n@@ -2310,7 +2319,7 @@ class Container(Layer):\n             })\n         config['layers'] = layer_configs\n \n-        # gather info about inputs and outputs\n+        # Gather info about inputs and outputs.\n         model_inputs = []\n         for i in range(len(self.input_layers)):\n             layer = self.input_layers[i]\n@@ -2334,8 +2343,6 @@ class Container(Layer):\n     @classmethod\n     def from_config(cls, config, custom_objects={}):\n         '''Instantiates a Model from its config (output of `get_config()`).\n-\n-        TODO: support for custom objects\n         '''\n         from keras.utils.layer_utils import layer_from_config\n \n@@ -2344,16 +2351,16 @@ class Container(Layer):\n         created_layers = {}\n \n         def process_layer(layer_data):\n-            # iterate over saved layers, instantiate them,\n+            # Iterate over saved layers, instantiate them,\n             # then call them on appropriate inputs to create graph nodes\n             layer_name = layer_data['name']\n \n-            # instantiate layer\n+            # Instantiate layer.\n             layer = layer_from_config(layer_data,\n                                       custom_objects=custom_objects)\n             created_layers[layer_name] = layer\n \n-            # gather layer inputs\n+            # Gather layer inputs.\n             inbound_nodes_data = layer_data['inbound_nodes']\n             for node_data in inbound_nodes_data:\n                 input_tensors = []\n@@ -2363,8 +2370,8 @@ class Container(Layer):\n                     inbound_layer = created_layers[inbound_layer_name]\n                     inbound_node = inbound_layer.inbound_nodes[inbound_node_index]\n                     input_tensors.append(inbound_node.output_tensors[inbound_tensor_index])\n-                # call layer on its inputs, thus creating the node\n-                # and building the layer if needed\n+                # Call layer on its inputs, thus creating the node\n+                # and building the layer if needed.\n                 if input_tensors:\n                     if len(input_tensors) == 1:\n                         layer(input_tensors[0])\n@@ -2393,9 +2400,9 @@ class Container(Layer):\n \n     def save(self, filepath, overwrite=True):\n         '''Save into a single HDF5 file:\n-            - the model architecture, allowing to re-instantiate the model\n-            - the model weights\n-            - the state of the optimizer, allowing to resume training\n+            - The model architecture, allowing to re-instantiate the model.\n+            - The model weights.\n+            - The state of the optimizer, allowing to resume training\n                 exactly where you left off.\n \n         This allows you to save the entirety of the state of a model\n@@ -2427,15 +2434,15 @@ class Container(Layer):\n \n         The weight file has:\n             - `layer_names` (attribute), a list of strings\n-                (ordered names of model layers)\n-            - for every layer, a `group` named `layer.name`\n-                - for every such layer group, a group attribute `weight_names`,\n-                    a list of strings (ordered names of weights tensor of the layer)\n-                - for every weight in the layer, a dataset\n-                    storing the weight value, named after the weight tensor\n+                (ordered names of model layers).\n+            - For every layer, a `group` named `layer.name`\n+                - For every such layer group, a group attribute `weight_names`,\n+                    a list of strings (ordered names of weights tensor of the layer).\n+                - For every weight in the layer, a dataset\n+                    storing the weight value, named after the weight tensor.\n         '''\n         import h5py\n-        # if file exists and should not be overwritten\n+        # If file exists and should not be overwritten:\n         if not overwrite and os.path.isfile(filepath):\n             proceed = ask_to_proceed_with_overwrite(filepath)\n             if not proceed:\n@@ -2447,7 +2454,7 @@ class Container(Layer):\n \n     def save_weights_to_hdf5_group(self, f):\n         if hasattr(self, 'flattened_layers'):\n-            # support for legacy Sequential/Merge behavior\n+            # Support for legacy Sequential/Merge behavior.\n             flattened_layers = self.flattened_layers\n         else:\n             flattened_layers = self.layers\n@@ -2476,7 +2483,7 @@ class Container(Layer):\n                     param_dset[:] = val\n \n     def load_weights(self, filepath, by_name=False):\n-        '''Load all layer weights from a HDF5 save file.\n+        '''Loads all layer weights from a HDF5 save file.\n \n         If `by_name` is False (default) weights are loaded\n         based on the network's topology, meaning the architecture\n@@ -2510,13 +2517,13 @@ class Container(Layer):\n         Layers that have no weights are skipped.\n         '''\n         if hasattr(self, 'flattened_layers'):\n-            # support for legacy Sequential/Merge behavior\n+            # Support for legacy Sequential/Merge behavior.\n             flattened_layers = self.flattened_layers\n         else:\n             flattened_layers = self.layers\n \n         if 'nb_layers' in f.attrs:\n-            # legacy format\n+            # Legacy format.\n             nb_layers = f.attrs['nb_layers']\n             if nb_layers != len(flattened_layers):\n                 raise Exception('You are trying to load a weight file '\n@@ -2529,7 +2536,7 @@ class Container(Layer):\n                 weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n                 flattened_layers[k].set_weights(weights)\n         else:\n-            # new file format\n+            # New file format.\n             filtered_layers = []\n             for layer in flattened_layers:\n                 weights = layer.weights\n@@ -2551,7 +2558,7 @@ class Container(Layer):\n                                 ' layers into a model with ' +\n                                 str(len(flattened_layers)) + ' layers.')\n \n-            # we batch weight value assignments in a single backend call\n+            # We batch weight value assignments in a single backend call\n             # which provides a speedup in TensorFlow.\n             weight_value_tuples = []\n             for k, name in enumerate(layer_names):\n@@ -2572,12 +2579,12 @@ class Container(Layer):\n                                     str(len(weight_values)) +\n                                     ' elements.')\n                 if layer.__class__.__name__ == 'Convolution1D':\n-                    # this is for backwards compatibility with\n+                    # This is for backwards compatibility with\n                     # the old Conv1D weights format.\n                     w = weight_values[0]\n                     shape = w.shape\n                     if shape[:2] != (layer.filter_length, 1) or shape[3] != layer.nb_filter:\n-                        # legacy shape: (self.nb_filter, input_dim, self.filter_length, 1)\n+                        # Legacy shape: (self.nb_filter, input_dim, self.filter_length, 1)\n                         assert shape[0] == layer.nb_filter and shape[2:] == (layer.filter_length, 1)\n                         w = np.transpose(w, (2, 3, 1, 0))\n                         weight_values[0] = w\n@@ -2590,7 +2597,7 @@ class Container(Layer):\n         Layers that have no matching name are skipped.\n         '''\n         if hasattr(self, 'flattened_layers'):\n-            # support for legacy Sequential/Merge behavior\n+            # Support for legacy Sequential/Merge behavior.\n             flattened_layers = self.flattened_layers\n         else:\n             flattened_layers = self.layers\n@@ -2600,7 +2607,7 @@ class Container(Layer):\n                                 ' in a legacy format that does not support' +\n                                 ' name-based weight loading.')\n         else:\n-            # new file format\n+            # New file format.\n             layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n \n             # Reverse index of layer name to list of layers with name.\n@@ -2609,7 +2616,7 @@ class Container(Layer):\n                 if layer.name:\n                     index.setdefault(layer.name, []).append(layer)\n \n-            # we batch weight value assignments in a single backend call\n+            # We batch weight value assignments in a single backend call\n             # which provides a speedup in TensorFlow.\n             weight_value_tuples = []\n             for k, name in enumerate(layer_names):\n@@ -2627,13 +2634,13 @@ class Container(Layer):\n                                         ' weight(s), but the saved weights' +\n                                         ' have ' + str(len(weight_values)) +\n                                         ' element(s).')\n-                    # set values\n+                    # Set values.\n                     for i in range(len(weight_values)):\n                         weight_value_tuples.append((symbolic_weights[i], weight_values[i]))\n             K.batch_set_value(weight_value_tuples)\n \n     def _updated_config(self):\n-        '''shared between different serialization methods'''\n+        '''Shared between different serialization methods.'''\n         from keras import __version__ as keras_version\n \n         config = self.get_config()\n@@ -2653,11 +2660,11 @@ class Container(Layer):\n         import json\n \n         def get_json_type(obj):\n-            # if obj is any numpy type\n+            # If obj is any numpy type\n             if type(obj).__module__ == np.__name__:\n                 return obj.item()\n \n-            # if obj is a python 'type'\n+            # If obj is a python 'type'\n             if type(obj).__name__ == type.__name__:\n                 return obj.__name__\n \n@@ -2683,7 +2690,7 @@ class Container(Layer):\n         from keras.utils.layer_utils import print_summary\n \n         if hasattr(self, 'flattened_layers'):\n-            # support for legacy Sequential/Merge behavior\n+            # Support for legacy Sequential/Merge behavior.\n             flattened_layers = self.flattened_layers\n         else:\n             flattened_layers = self.layers\n@@ -2699,10 +2706,10 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n     (potentially with 1 element).\n \n     # Arguments\n-        tensor: the tensor to start from.\n-        layer: origin layer of the tensor. Will be\n+        tensor: The tensor to start from.\n+        layer: Origin layer of the tensor. Will be\n             determined via tensor._keras_history if not provided.\n-        node_index: origin node index of the tensor.\n+        node_index: Origin node index of the tensor.\n     '''\n     if not hasattr(tensor, '_keras_history'):\n         raise Exception('Tensor must be a Keras tensor. Found: ' + str(tensor))\n@@ -2714,7 +2721,7 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n     else:\n         node = layer.inbound_nodes[node_index]\n         if not node.inbound_layers:\n-            # reached an Input layer, stop recursion\n+            # Reached an Input layer, stop recursion.\n             return node.input_tensors\n         else:\n             source_tensors = []\n@@ -2725,7 +2732,7 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n                 previous_sources = get_source_inputs(x,\n                                                      layer,\n                                                      node_index)\n-                # avoid input redundancy\n+                # Avoid input redundancy.\n                 for x in previous_sources:\n                     if x not in source_tensors:\n                         source_tensors.append(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
