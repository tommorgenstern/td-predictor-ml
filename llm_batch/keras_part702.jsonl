{"custom_id": "keras#26815bdc874f939ccacb1557b7ce224fbd90e8bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 141 | Churn Cumulative: 3341 | Contributors (this commit): 14 | Commits (past 90d): 58 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,14 @@\n-import numpy as np\n-\n from keras_core import backend\n+from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n-from keras_core.utils.module_utils import tensorflow as tf\n+from keras_core.utils import image_utils\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,41 +52,122 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n-        if not tf.available:\n-            raise ImportError(\n-                \"Layer RandomCrop requires TensorFlow. \"\n-                \"Install it via `pip install tensorflow`.\"\n-            )\n-\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n-        if (\n-            backend.backend() != \"tensorflow\"\n-            and not backend_utils.in_tf_graph()\n-        ):\n-            outputs = backend.convert_to_tensor(outputs)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.numpy.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            input_height, input_width = (\n+                input_shape[self.height_axis],\n+                input_shape[self.width_axis],\n+            )\n+\n+            seed_generator = self._get_seed_generator(self.backend._backend)\n+            h_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_height - self.height + 1),\n+                    dtype=\"int\",\n+                    seed=seed_generator,\n+                ),\n+                \"int\",\n+            )\n+            w_start = self.backend.cast(\n+                ops.random.uniform(\n+                    (),\n+                    0,\n+                    maxval=float(input_width - self.width + 1),\n+                    dtype=\"int\",\n+                    seed=seed_generator,\n+                ),\n+                \"int\",\n+            )\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n+        outputs = (\n+            self.backend.numpy.squeeze(outputs, axis=0)\n+            if not is_batched\n+            else outputs\n+        )\n+\n+        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6950fda767c4ccf2efed7c83df4a2dd79c66c9c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 61 | Files Changed: 5 | Hunks: 22 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 116 | Churn Cumulative: 5939 | Contributors (this commit): 22 | Commits (past 90d): 124 | Contributors (cumulative): 45 | DMM Complexity: 0.72\n\nDIFF:\n@@ -25,8 +25,8 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     seed = tf_draw_seed(seed)\n     return tf.random.stateless_uniform(\n         shape=shape,\n-        minval=minval,\n-        maxval=maxval,\n+        minval=tf.cast(minval, dtype),\n+        maxval=tf.cast(maxval, dtype),\n         dtype=dtype,\n         seed=seed,\n     )\n\n@@ -63,7 +63,8 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n-    if len(shape) == 0:\n+    requested_shape = shape\n+    if len(requested_shape) == 0:\n         shape = (1,)\n     # Do not use generator during symbolic execution.\n     if get_device() == \"meta\":\n@@ -75,6 +76,9 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n         )\n \n     output = (maxval - minval) * rand_tensor + minval\n+\n+    if len(requested_shape) == 0:\n+        return output[0]\n     return output\n \n \n\n@@ -3,7 +3,6 @@ from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras_core.random.seed_generator import SeedGenerator\n-from keras_core.utils import backend_utils\n from keras_core.utils import image_utils\n \n \n@@ -25,14 +24,6 @@ class RandomCrop(TFDataLayer):\n     of integer or floating point dtype. By default, the layer will output\n     floats.\n \n-    **Note:** This layer wraps `tf.keras.layers.RandomCrop`. It cannot\n-    be used as part of the compiled computation graph of a model with\n-    any backend other than TensorFlow.\n-    It can however be used with any backend when running eagerly.\n-    It can also always be used as part of an input preprocessing pipeline\n-    with any backend (outside the model itself), which is how we recommend\n-    to use this layer.\n-\n     **Note:** This layer is safe to use inside a `tf.data` pipeline\n     (independently of which backend you're using).\n \n@@ -78,11 +69,8 @@ class RandomCrop(TFDataLayer):\n         inputs = self.backend.cast(inputs, self.compute_dtype)\n         input_shape = self.backend.shape(inputs)\n         is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.numpy.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n+        if not is_batched:\n+            inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n \n         h_diff = input_shape[self.height_axis] - self.height\n         w_diff = input_shape[self.width_axis] - self.width\n@@ -94,33 +82,28 @@ class RandomCrop(TFDataLayer):\n             )\n \n             seed_generator = self._get_seed_generator(self.backend._backend)\n-            h_start = self.backend.cast(\n-                ops.random.uniform(\n+            h_start = self.backend.cast(self.backend.random.uniform(\n                 (),\n                 0,\n                 maxval=float(input_height - self.height + 1),\n-                    dtype=\"int\",\n                 seed=seed_generator,\n-                ),\n-                \"int\",\n-            )\n-            w_start = self.backend.cast(\n-                ops.random.uniform(\n+            ), \"int32\")\n+            w_start = self.backend.cast(self.backend.random.uniform(\n                 (),\n                 0,\n                 maxval=float(input_width - self.width + 1),\n-                    dtype=\"int\",\n                 seed=seed_generator,\n-                ),\n-                \"int\",\n-            )\n+            ), \"int32\")\n             if self.data_format == \"channels_last\":\n+                # return self.backend.core.slice(inputs, self.backend.numpy.stack([0, h_start, w_start, 0]), self.backend.numpy.stack([self.backend.shape(inputs)[0], self.height, self.width, self.backend.shape(inputs)[3]]))\n                 return inputs[\n                     :,\n                     h_start : h_start + self.height,\n                     w_start : w_start + self.width,\n+                    :,\n                 ]\n             else:\n+                # return self.backend.core.slice(inputs, self.backend.numpy.stack([0, 0, h_start, w_start]), self.backend.numpy.stack([self.backend.shape(inputs)[0], self.backend.shape(inputs)[1], self.height, self.width]))\n                 return inputs[\n                     :,\n                     :,\n@@ -138,20 +121,15 @@ class RandomCrop(TFDataLayer):\n             # smart_resize will always output float32, so we need to re-cast.\n             return self.backend.cast(outputs, self.compute_dtype)\n \n+        predicate = self.backend.numpy.all(self.backend.numpy.stack([training, h_diff >= 0, w_diff >= 0]))\n         outputs = self.backend.cond(\n-            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n+            predicate,\n             random_crop,\n             resize,\n         )\n \n-        outputs = (\n-            self.backend.numpy.squeeze(outputs, axis=0)\n-            if not is_batched\n-            else outputs\n-        )\n-\n-        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n-            outputs = self.backend.convert_to_tensor(outputs)\n+        if not is_batched:\n+            outputs = self.backend.numpy.squeeze(outputs, axis=0)\n         return outputs\n \n     def compute_output_shape(self, input_shape, *args, **kwargs):\n\n@@ -96,7 +96,11 @@ class TestCase(unittest.TestCase):\n             revived_config, sort_keys=True, indent=4\n         )\n         self.assertEqual(config_json, revived_config_json)\n-        self.assertEqual(ref_dir, dir(revived_instance))\n+        new_dir = dir(revived_instance)[:]\n+        for lst in [ref_dir, new_dir]:\n+            if \"__annotations__\" in lst:\n+                lst.remove(\"__annotations__\")\n+        self.assertEqual(ref_dir, new_dir)\n         return revived_instance\n \n     def run_layer_test(\n\n@@ -411,32 +411,40 @@ def smart_resize(\n \n     if data_format == \"channels_last\":\n         if len(img.shape) == 4:\n-            img = img[\n-                :,\n-                crop_box_hstart : crop_box_hstart + crop_height,\n-                crop_box_wstart : crop_box_wstart + crop_width,\n-                :,\n-            ]\n+            img = backend_module.core.slice(img, backend_module.numpy.stack([0, crop_box_hstart, crop_box_wstart, 0]), backend_module.numpy.stack([backend_module.shape(img)[0], crop_height, crop_width, backend_module.shape(img)[3]]))\n+        \n+            # img = img[\n+            #     :,\n+            #     crop_box_hstart : crop_box_hstart + crop_height,\n+            #     crop_box_wstart : crop_box_wstart + crop_width,\n+            #     :,\n+            # ]\n         else:\n-            img = img[\n-                crop_box_hstart : crop_box_hstart + crop_height,\n-                crop_box_wstart : crop_box_wstart + crop_width,\n-                :,\n-            ]\n+            img = backend_module.core.slice(img, backend_module.numpy.stack([crop_box_hstart, crop_box_wstart, 0]), backend_module.numpy.stack([crop_height, crop_width, backend_module.shape(img)[2]]))\n+\n+            # img = img[\n+            #     crop_box_hstart : crop_box_hstart + crop_height,\n+            #     crop_box_wstart : crop_box_wstart + crop_width,\n+            #     :,\n+            # ]\n     else:\n         if len(img.shape) == 4:\n-            img = img[\n-                :,\n-                :,\n-                crop_box_hstart : crop_box_hstart + crop_height,\n-                crop_box_wstart : crop_box_wstart + crop_width,\n-            ]\n+            img = backend_module.core.slice(img, backend_module.numpy.stack([0, 0, crop_box_hstart, crop_box_wstart]), backend_module.numpy.stack([backend_module.shape(img)[0], backend_module.shape(img)[1], crop_height, crop_width]))\n+\n+            # img = img[\n+            #     :,\n+            #     :,\n+            #     crop_box_hstart : crop_box_hstart + crop_height,\n+            #     crop_box_wstart : crop_box_wstart + crop_width,\n+            # ]\n         else:\n-            img = img[\n-                :,\n-                crop_box_hstart : crop_box_hstart + crop_height,\n-                crop_box_wstart : crop_box_wstart + crop_width,\n-            ]\n+            img = backend_module.core.slice(img, backend_module.numpy.stack([0, crop_box_hstart, crop_box_wstart]), backend_module.numpy.stack([backend_module.shape(img)[0], crop_height, crop_width]))\n+\n+            # img = img[\n+            #     :,\n+            #     crop_box_hstart : crop_box_hstart + crop_height,\n+            #     crop_box_wstart : crop_box_wstart + crop_width,\n+            # ]\n \n     img = backend_module.image.resize(\n         img, size=size, interpolation=interpolation, data_format=data_format\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#77d63e4673e69039f94c3853b01e70d72e74ab79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1603 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n     This layer translates a set of arbitrary strings into integer output via a\n     table-based vocabulary lookup. This layer will perform no splitting or\n-    transformation of input strings. For a layer than can split and tokenize\n+    transformation of input strings. For a layer that can split and tokenize\n     natural language, see the `tf.keras.layers.TextVectorization` layer.\n \n     The vocabulary for the layer must be either supplied on construction or\n@@ -82,7 +82,7 @@ class StringLookup(index_lookup.IndexLookup):\n         indices. Defaults to `\"[UNK]\"`.\n       vocabulary: Optional. Either an array of strings or a string path to a\n         text file. If passing an array, can pass a tuple, list, 1D numpy array,\n-        or 1D tensor containing the string vocbulary terms. If passing a file\n+        or 1D tensor containing the string vocabulary terms. If passing a file\n         path, the file should contain one line per term in the vocabulary. If\n         this argument is set, there is no need to `adapt()` the layer.\n       idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list,\n@@ -114,13 +114,13 @@ class StringLookup(index_lookup.IndexLookup):\n             find the value in each token slot.\n         For `\"int\"` output, any shape of input and output is supported. For all\n         other output modes, currently only output up to rank 2 is supported.\n-        Defaults to `\"int\"`\n+        Defaults to `\"int\"`.\n       pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n         padded to `max_tokens` even if the number of unique tokens in the\n         vocabulary is less than max_tokens, resulting in a tensor of shape\n         [batch_size, max_tokens] regardless of vocabulary size. Defaults to\n-        False.\n+        `False`.\n       sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n         `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n         dense `Tensor`. Defaults to `False`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#81da36bd5e206e47725472ab08c87f336c363d61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 82 | Lines Deleted: 60 | Files Changed: 2 | Hunks: 15 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 142 | Churn Cumulative: 4088 | Contributors (this commit): 15 | Commits (past 90d): 67 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,4 @@\n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras_core.random.seed_generator import SeedGenerator\n@@ -82,34 +81,46 @@ class RandomCrop(TFDataLayer):\n             )\n \n             seed_generator = self._get_seed_generator(self.backend._backend)\n-            h_start = self.backend.cast(self.backend.random.uniform(\n+            h_start = self.backend.cast(\n+                self.backend.random.uniform(\n                     (),\n                     0,\n                     maxval=float(input_height - self.height + 1),\n                     seed=seed_generator,\n-            ), \"int32\")\n-            w_start = self.backend.cast(self.backend.random.uniform(\n+                ),\n+                \"int32\",\n+            )\n+            w_start = self.backend.cast(\n+                self.backend.random.uniform(\n                     (),\n                     0,\n                     maxval=float(input_width - self.width + 1),\n                     seed=seed_generator,\n-            ), \"int32\")\n+                ),\n+                \"int32\",\n+            )\n             if self.data_format == \"channels_last\":\n-                # return self.backend.core.slice(inputs, self.backend.numpy.stack([0, h_start, w_start, 0]), self.backend.numpy.stack([self.backend.shape(inputs)[0], self.height, self.width, self.backend.shape(inputs)[3]]))\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                    :,\n-                ]\n+                return self.backend.core.slice(\n+                    inputs,\n+                    self.backend.numpy.stack([0, h_start, w_start, 0]),\n+                    [\n+                        self.backend.shape(inputs)[0],\n+                        self.height,\n+                        self.width,\n+                        self.backend.shape(inputs)[3],\n+                    ],\n+                )\n             else:\n-                # return self.backend.core.slice(inputs, self.backend.numpy.stack([0, 0, h_start, w_start]), self.backend.numpy.stack([self.backend.shape(inputs)[0], self.backend.shape(inputs)[1], self.height, self.width]))\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n+                return self.backend.core.slice(\n+                    inputs,\n+                    self.backend.numpy.stack([0, 0, h_start, w_start]),\n+                    [\n+                        self.backend.shape(inputs)[0],\n+                        self.backend.shape(inputs)[1],\n+                        self.height,\n+                        self.width,\n+                    ],\n+                )\n \n         def resize():\n             outputs = image_utils.smart_resize(\n@@ -121,7 +132,16 @@ class RandomCrop(TFDataLayer):\n             # smart_resize will always output float32, so we need to re-cast.\n             return self.backend.cast(outputs, self.compute_dtype)\n \n-        predicate = self.backend.numpy.all(self.backend.numpy.stack([training, h_diff >= 0, w_diff >= 0]))\n+        if isinstance(h_diff, int) and isinstance(w_diff, int):\n+            if training and h_diff >= 0 and w_diff >= 0:\n+                outputs = random_crop()\n+            else:\n+                outputs = resize()\n+        else:\n+            predicate = self.backend.numpy.logical_and(\n+                training,\n+                self.backend.numpy.logical_and(h_diff >= 0, w_diff >= 0),\n+            )\n             outputs = self.backend.cond(\n                 predicate,\n                 random_crop,\n\n@@ -386,20 +386,30 @@ def smart_resize(\n         height, width = shape[-2], shape[-1]\n     target_height, target_width = size\n \n-    crop_height = backend_module.cast(\n-        backend_module.cast(width * target_height, \"float32\") / target_width,\n-        \"int32\",\n-    )\n-    crop_width = backend_module.cast(\n-        backend_module.cast(height * target_width, \"float32\") / target_height,\n-        \"int32\",\n-    )\n-\n     # Set back to input height / width if crop_height / crop_width is not\n     # smaller.\n+    if isinstance(height, int) and isinstance(width, int):\n+        # For JAX, we need to keep the slice indices as static integers\n+        crop_height = int(float(width * target_height) / target_width)\n+        crop_height = min(height, crop_height)\n+        crop_width = int(float(height * target_width) / target_height)\n+        crop_width = min(width, crop_width)\n+        crop_box_hstart = int(float(height - crop_height) / 2)\n+        crop_box_wstart = int(float(width - crop_width) / 2)\n+    else:\n+        crop_height = backend_module.cast(\n+            backend_module.cast(width * target_height, \"float32\")\n+            / target_width,\n+            \"int32\",\n+        )\n         crop_height = backend_module.numpy.minimum(height, crop_height)\n-    crop_width = backend_module.numpy.minimum(width, crop_width)\n         crop_height = backend_module.cast(crop_height, \"int32\")\n+        crop_width = backend_module.cast(\n+            backend_module.cast(height * target_width, \"float32\")\n+            / target_height,\n+            \"int32\",\n+        )\n+        crop_width = backend_module.numpy.minimum(width, crop_width)\n         crop_width = backend_module.cast(crop_width, \"int32\")\n \n         crop_box_hstart = backend_module.cast(\n@@ -411,40 +421,32 @@ def smart_resize(\n \n     if data_format == \"channels_last\":\n         if len(img.shape) == 4:\n-            img = backend_module.core.slice(img, backend_module.numpy.stack([0, crop_box_hstart, crop_box_wstart, 0]), backend_module.numpy.stack([backend_module.shape(img)[0], crop_height, crop_width, backend_module.shape(img)[3]]))\n-        \n-            # img = img[\n-            #     :,\n-            #     crop_box_hstart : crop_box_hstart + crop_height,\n-            #     crop_box_wstart : crop_box_wstart + crop_width,\n-            #     :,\n-            # ]\n+            img = img[\n+                :,\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+                :,\n+            ]\n         else:\n-            img = backend_module.core.slice(img, backend_module.numpy.stack([crop_box_hstart, crop_box_wstart, 0]), backend_module.numpy.stack([crop_height, crop_width, backend_module.shape(img)[2]]))\n-\n-            # img = img[\n-            #     crop_box_hstart : crop_box_hstart + crop_height,\n-            #     crop_box_wstart : crop_box_wstart + crop_width,\n-            #     :,\n-            # ]\n+            img = img[\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+                :,\n+            ]\n     else:\n         if len(img.shape) == 4:\n-            img = backend_module.core.slice(img, backend_module.numpy.stack([0, 0, crop_box_hstart, crop_box_wstart]), backend_module.numpy.stack([backend_module.shape(img)[0], backend_module.shape(img)[1], crop_height, crop_width]))\n-\n-            # img = img[\n-            #     :,\n-            #     :,\n-            #     crop_box_hstart : crop_box_hstart + crop_height,\n-            #     crop_box_wstart : crop_box_wstart + crop_width,\n-            # ]\n+            img = img[\n+                :,\n+                :,\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+            ]\n         else:\n-            img = backend_module.core.slice(img, backend_module.numpy.stack([0, crop_box_hstart, crop_box_wstart]), backend_module.numpy.stack([backend_module.shape(img)[0], crop_height, crop_width]))\n-\n-            # img = img[\n-            #     :,\n-            #     crop_box_hstart : crop_box_hstart + crop_height,\n-            #     crop_box_wstart : crop_box_wstart + crop_width,\n-            # ]\n+            img = img[\n+                :,\n+                crop_box_hstart : crop_box_hstart + crop_height,\n+                crop_box_wstart : crop_box_wstart + crop_width,\n+            ]\n \n     img = backend_module.image.resize(\n         img, size=size, interpolation=interpolation, data_format=data_format\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e73f16a2502ed840b72de6a038c7d1aba0f53e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 61 | Files Changed: 1 | Hunks: 26 | Methods Changed: 7 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 136 | Churn Cumulative: 136 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"\n Title: Object detection with Vision Transformers\n Author: [Karan V. Dave](https://www.linkedin.com/in/karan-dave-811413164/)\n+Converted to Keras Core by: [Gabriel Rasskin](https://github.com/grasskin), [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2022/03/27\n Last modified: 2022/03/27\n Description: A simple Keras implementation of object detection using Vision Transformers.\n@@ -21,17 +22,30 @@ and we train it on the\n [Caltech 101 dataset](http://www.vision.caltech.edu/datasets/)\n to detect an airplane in the given image.\n \n-This example requires TensorFlow 2.4 or higher.\n+This example requires TensorFlow 2.4 or higher, and\n+[TensorFlow Addons](https://www.tensorflow.org/addons/overview),\n+from which we import the `AdamW` optimizer.\n+\n+TensorFlow Addons can be installed via the following command:\n+\n+```\n+pip install -U git+https://github.com/keras-team/keras-core\n+```\n \"\"\"\n \n \"\"\"\n ## Imports and setup\n \"\"\"\n \n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n+\n+\n import numpy as np\n-import tensorflow as tf\n import keras_core as keras\n from keras_core import layers\n+from keras_core import ops\n import matplotlib.pyplot as plt\n import numpy as np\n import cv2\n@@ -46,33 +60,27 @@ We use the [Caltech 101 Dataset](https://data.caltech.edu/records/mzrjq-6wc02).\n \"\"\"\n \n # Path to images and annotations\n-path_images = \"./101_ObjectCategories/airplanes/\"\n-path_annot = \"./Annotations/Airplanes_Side_2/\"\n+path_images = \"/101_ObjectCategories/airplanes/\"\n+path_annot = \"/Annotations/Airplanes_Side_2/\"\n \n path_to_downloaded_file = keras.utils.get_file(\n     fname=\"caltech_101_zipped\",\n     origin=\"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\",\n     extract=True,\n     archive_format=\"zip\",  # downloaded file format\n-    cache_dir=\"./\",  # cache and extract in current directory\n+    cache_dir=\"/\",  # cache and extract in current directory\n )\n \n # Extracting tar files found inside main zip file\n-shutil.unpack_archive(\n-    \"./datasets/caltech-101/101_ObjectCategories.tar.gz\", \"./\"\n-)\n-shutil.unpack_archive(\"./datasets/caltech-101/Annotations.tar\", \"./\")\n+shutil.unpack_archive(\"/datasets/caltech-101/101_ObjectCategories.tar.gz\", \"/\")\n+shutil.unpack_archive(\"/datasets/caltech-101/Annotations.tar\", \"/\")\n \n # list of paths to images and annotations\n image_paths = [\n-    f\n-    for f in os.listdir(path_images)\n-    if os.path.isfile(os.path.join(path_images, f))\n+    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n ]\n annot_paths = [\n-    f\n-    for f in os.listdir(path_annot)\n-    if os.path.isfile(os.path.join(path_annot, f))\n+    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n ]\n \n image_paths.sort()\n@@ -134,7 +142,7 @@ as a reference.\n \n def mlp(x, hidden_units, dropout_rate):\n     for units in hidden_units:\n-        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n+        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n         x = layers.Dropout(dropout_rate)(x)\n     return x\n \n@@ -149,34 +157,29 @@ class Patches(layers.Layer):\n         super().__init__()\n         self.patch_size = patch_size\n \n-    #     Override function to avoid error while saving model\n-    def get_config(self):\n-        config = super().get_config().copy()\n-        config.update(\n-            {\n-                \"input_shape\": input_shape,\n-                \"patch_size\": patch_size,\n-                \"num_patches\": num_patches,\n-                \"projection_dim\": projection_dim,\n-                \"num_heads\": num_heads,\n-                \"transformer_units\": transformer_units,\n-                \"transformer_layers\": transformer_layers,\n-                \"mlp_head_units\": mlp_head_units,\n-            }\n-        )\n-        return config\n-\n     def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=[1, self.patch_size, self.patch_size, 1],\n-            strides=[1, self.patch_size, self.patch_size, 1],\n-            rates=[1, 1, 1, 1],\n-            padding=\"VALID\",\n+        input_shape = ops.shape(images)\n+        batch_size = input_shape[0]\n+        height = input_shape[1]\n+        width = input_shape[2]\n+        channels = input_shape[3]\n+        num_patches_h = height // self.patch_size\n+        num_patches_w = width // self.patch_size\n+        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n+        patches = ops.reshape(\n+            patches,\n+            (\n+                batch_size,\n+                num_patches_h * num_patches_w,\n+                self.patch_size * self.patch_size * channels,\n+            ),\n         )\n-        # return patches\n-        return tf.reshape(patches, [batch_size, -1, patches.shape[-1]])\n+        return patches\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"patch_size\": self.patch_size})\n+        return config\n \n \n \"\"\"\n@@ -189,20 +192,18 @@ plt.figure(figsize=(4, 4))\n plt.imshow(x_train[0].astype(\"uint8\"))\n plt.axis(\"off\")\n \n-patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n+patches = Patches(patch_size)(np.expand_dims(x_train[0], axis=0))\n print(f\"Image size: {image_size} X {image_size}\")\n print(f\"Patch size: {patch_size} X {patch_size}\")\n-print(\n-    f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\"\n-)\n+print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n \n \n n = int(np.sqrt(patches.shape[1]))\n plt.figure(figsize=(4, 4))\n for i, patch in enumerate(patches[0]):\n     ax = plt.subplot(n, n, i + 1)\n-    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n-    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n+    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n+    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n     plt.axis(\"off\")\n \n \"\"\"\n@@ -241,8 +242,11 @@ class PatchEncoder(layers.Layer):\n         return config\n \n     def call(self, patch):\n-        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n-        encoded = self.projection(patch) + self.position_embedding(positions)\n+        positions = ops.expand_dims(\n+            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n+        )\n+        projected_patches = self.projection(patch)\n+        encoded = projected_patches + self.position_embedding(positions)\n         return encoded\n \n \n@@ -269,7 +273,7 @@ def create_vit_object_detector(\n     transformer_layers,\n     mlp_head_units,\n ):\n-    inputs = layers.Input(shape=input_shape)\n+    inputs = keras.Input(shape=input_shape)\n     # Create patches\n     patches = Patches(patch_size)(inputs)\n     # Encode patches\n@@ -297,9 +301,7 @@ def create_vit_object_detector(\n     representation = layers.Flatten()(representation)\n     representation = layers.Dropout(0.3)(representation)\n     # Add MLP.\n-    features = mlp(\n-        representation, hidden_units=mlp_head_units, dropout_rate=0.3\n-    )\n+    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n \n     bounding_box = layers.Dense(4)(\n         features\n@@ -315,6 +317,7 @@ def create_vit_object_detector(\n \n \n def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n+\n     optimizer = keras.optimizers.AdamW(\n         learning_rate=learning_rate, weight_decay=weight_decay\n     )\n@@ -322,7 +325,7 @@ def run_experiment(model, learning_rate, weight_decay, batch_size, num_epochs):\n     # Compile model.\n     model.compile(optimizer=optimizer, loss=keras.losses.MeanSquaredError())\n \n-    checkpoint_filepath = \"logs/model.weights.h5\"\n+    checkpoint_filepath = \"vit_object_detector.weights.h5\"\n     checkpoint_callback = keras.callbacks.ModelCheckpoint(\n         checkpoint_filepath,\n         monitor=\"val_loss\",\n@@ -349,7 +352,7 @@ input_shape = (image_size, image_size, 3)  # input image shape\n learning_rate = 0.001\n weight_decay = 0.0001\n batch_size = 32\n-num_epochs = 1\n+num_epochs = 15\n num_patches = (image_size // patch_size) ** 2\n projection_dim = 64\n num_heads = 4\n@@ -382,6 +385,20 @@ history = run_experiment(\n )\n \n \n+def plot_history(item):\n+    plt.plot(history.history[item], label=item)\n+    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(item)\n+    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_history(\"loss\")\n+\n+\n \"\"\"\n ## Evaluate the model\n \"\"\"\n@@ -389,8 +406,7 @@ history = run_experiment(\n import matplotlib.patches as patches\n \n # Saves the model in current path\n-vit_object_detector.save(\"vit_object_detector.keras\", save_format=\"keras\")\n-\n+vit_object_detector.save(\"vit_object_detector.keras\")\n \n # To calculate IoU (intersection over union, given two bounding boxes)\n def bounding_box_intersection_over_union(box_predicted, box_truth):\n@@ -471,9 +487,7 @@ for input_image in x_test[:10]:\n \n     top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n \n-    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(\n-        y_test[i][3] * h\n-    )\n+    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n \n     box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01f8a8766faa7a6824b9226332b2f4af99031945", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 47 | Churn Cumulative: 557 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,26 +44,37 @@ class RandomFlip(TFDataLayer):\n         self.seed = seed\n         self.generator = SeedGenerator(seed)\n         self.mode = mode\n-        self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def _randomly_flip_inputs(self, inputs):\n+        unbatched = len(inputs.shape) == 3\n+        if unbatched:\n+            inputs = self.backend.numpy.expand_dims(inputs, axis=0)\n+        batch_size = self.backend.shape(inputs)[0]\n         flipped_outputs = inputs\n         seed_generator = self._get_seed_generator(self.backend._backend)\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n-            flipped_outputs = self.backend.cond(\n-                self.backend.random.uniform(shape=(), seed=seed_generator)\n+            flipped_outputs = self.backend.numpy.where(\n+                self.backend.random.uniform(\n+                    shape=(batch_size,), seed=seed_generator\n+                )\n                 <= 0.5,\n-                lambda: self.backend.numpy.flip(flipped_outputs, axis=-2),\n-                lambda: flipped_outputs,\n+                self.backend.numpy.flip(flipped_outputs, axis=-2),\n+                flipped_outputs,\n             )\n         if self.mode == VERTICAL or self.mode == HORIZONTAL_AND_VERTICAL:\n-            flipped_outputs = self.backend.cond(\n-                self.backend.random.uniform(shape=(), seed=seed_generator)\n+            flipped_outputs = self.backend.numpy.where(\n+                self.backend.random.uniform(\n+                    shape=(batch_size,), seed=seed_generator\n+                )\n                 <= 0.5,\n-                lambda: self.backend.numpy.flip(flipped_outputs, axis=-3),\n-                lambda: flipped_outputs,\n+                self.backend.numpy.flip(flipped_outputs, axis=-3),\n+                flipped_outputs,\n+            )\n+        if unbatched:\n+            flipped_outputs = self.backend.numpy.squeeze(\n+                flipped_outputs, axis=0\n             )\n         return flipped_outputs\n \n\n@@ -12,8 +12,15 @@ from keras_core import utils\n \n class MockedRandomFlip(layers.RandomFlip):\n     def call(self, inputs, training=True):\n+        unbatched = len(inputs.shape) == 3\n+        batch_size = 1 if unbatched else self.backend.shape(inputs)[0]\n+        mocked_value = self.backend.numpy.full(\n+            (batch_size,), 0.1, dtype=\"float32\"\n+        )\n         with unittest.mock.patch.object(\n-            self.backend.random, \"uniform\", return_value=0.1\n+            self.backend.random,\n+            \"uniform\",\n+            return_value=mocked_value,\n         ):\n             out = super().call(inputs, training=training)\n         return out\n@@ -26,6 +33,7 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n         (\"random_flip_both\", \"horizontal_and_vertical\"),\n     )\n     def test_random_flip(self, mode):\n+        run_training_check = False if backend.backend() == \"numpy\" else True\n         self.run_layer_test(\n             layers.RandomFlip,\n             init_kwargs={\n@@ -34,10 +42,11 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n             input_shape=(2, 3, 4),\n             expected_output_shape=(2, 3, 4),\n             supports_masking=False,\n-            run_training_check=False,\n+            run_training_check=run_training_check,\n         )\n \n     def test_random_flip_horizontal(self):\n+        run_training_check = False if backend.backend() == \"numpy\" else True\n         utils.set_random_seed(0)\n         self.run_layer_test(\n             MockedRandomFlip,\n@@ -48,10 +57,11 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n             input_data=np.asarray([[[2, 3, 4], [5, 6, 7]]]),\n             expected_output=backend.convert_to_tensor([[[5, 6, 7], [2, 3, 4]]]),\n             supports_masking=False,\n-            run_training_check=False,\n+            run_training_check=run_training_check,\n         )\n \n     def test_random_flip_vertical(self):\n+        run_training_check = False if backend.backend() == \"numpy\" else True\n         utils.set_random_seed(0)\n         self.run_layer_test(\n             MockedRandomFlip,\n@@ -64,7 +74,7 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n                 [[[5, 6, 7]], [[2, 3, 4]]]\n             ),\n             supports_masking=False,\n-            run_training_check=False,\n+            run_training_check=run_training_check,\n         )\n \n     def test_tf_data_compatibility(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cdffff886626e5a05bc5d54b8a4634f1e5db06cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 30 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 53 | Churn Cumulative: 19852 | Contributors (this commit): 31 | Commits (past 90d): 3 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -1087,9 +1087,7 @@ def name_scope(name):\n \n # Export V1 version.\n _v1_name_scope = tf.compat.v1.name_scope\n-keras_export(v1=[\"keras.backend.name_scope\"], allow_multiple_exports=True)(\n-    _v1_name_scope\n-)\n+keras_export(v1=[\"keras.backend.name_scope\"])(_v1_name_scope)\n \n \n @keras_export(\"keras.backend.variable\")\n\n@@ -29,35 +29,30 @@ _v1_identity = tf.compat.v1.initializers.identity\n _v1_glorot_uniform_initializer = tf.compat.v1.glorot_uniform_initializer\n _v1_glorot_normal_initializer = tf.compat.v1.glorot_normal_initializer\n \n+keras_export(v1=[\"keras.initializers.Zeros\", \"keras.initializers.zeros\"])(\n+    _v1_zeros_initializer\n+)\n+keras_export(v1=[\"keras.initializers.Ones\", \"keras.initializers.ones\"])(\n+    _v1_ones_initializer\n+)\n+keras_export(v1=[\"keras.initializers.Constant\", \"keras.initializers.constant\"])(\n+    _v1_constant_initializer\n+)\n+keras_export(v1=[\"keras.initializers.VarianceScaling\"])(\n+    _v1_variance_scaling_initializer\n+)\n keras_export(\n-    v1=[\"keras.initializers.Zeros\", \"keras.initializers.zeros\"],\n-    allow_multiple_exports=True,\n-)(_v1_zeros_initializer)\n-keras_export(\n-    v1=[\"keras.initializers.Ones\", \"keras.initializers.ones\"],\n-    allow_multiple_exports=True,\n-)(_v1_ones_initializer)\n-keras_export(\n-    v1=[\"keras.initializers.Constant\", \"keras.initializers.constant\"],\n-    allow_multiple_exports=True,\n-)(_v1_constant_initializer)\n-keras_export(\n-    v1=[\"keras.initializers.VarianceScaling\"], allow_multiple_exports=True\n-)(_v1_variance_scaling_initializer)\n-keras_export(\n-    v1=[\"keras.initializers.Orthogonal\", \"keras.initializers.orthogonal\"],\n-    allow_multiple_exports=True,\n+    v1=[\"keras.initializers.Orthogonal\", \"keras.initializers.orthogonal\"]\n )(_v1_orthogonal_initializer)\n-keras_export(\n-    v1=[\"keras.initializers.Identity\", \"keras.initializers.identity\"],\n-    allow_multiple_exports=True,\n-)(_v1_identity)\n-keras_export(\n-    v1=[\"keras.initializers.glorot_uniform\"], allow_multiple_exports=True\n-)(_v1_glorot_uniform_initializer)\n-keras_export(\n-    v1=[\"keras.initializers.glorot_normal\"], allow_multiple_exports=True\n-)(_v1_glorot_normal_initializer)\n+keras_export(v1=[\"keras.initializers.Identity\", \"keras.initializers.identity\"])(\n+    _v1_identity\n+)\n+keras_export(v1=[\"keras.initializers.glorot_uniform\"])(\n+    _v1_glorot_uniform_initializer\n+)\n+keras_export(v1=[\"keras.initializers.glorot_normal\"])(\n+    _v1_glorot_normal_initializer\n+)\n \n \n @keras_export(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#967304a60c246604f1b8fdff6448419c292099e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 16 | Methods Changed: 12 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 117 | Churn Cumulative: 117 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,9 @@\n \"\"\"\n Title: Compact Convolutional Transformers\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com), [Guillaume Baquiast](https://www.linkedin.com/in/guillaume-baquiast-478965ba/)\n Date created: 2021/06/30\n-Last modified: 2023/07/17\n+Last modified: 2023/08/07\n Description: Compact Convolutional Transformers for efficient image classification.\n Accelerator: GPU\n \"\"\"\n@@ -39,15 +39,10 @@ code snippets from another example,\n ## Imports\n \"\"\"\n \n-import os\n-\n-os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n-\n from keras_core import layers\n import keras_core as keras\n \n import matplotlib.pyplot as plt\n-import tensorflow as tf\n import numpy as np\n \n \"\"\"\n@@ -135,7 +130,7 @@ class CCTTokenizer(layers.Layer):\n             )\n             self.conv_model.add(layers.ZeroPadding2D(padding))\n             self.conv_model.add(\n-                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n+                layers.MaxPooling2D(pooling_kernel_size, pooling_stride, \"same\")\n             )\n \n         self.positional_emb = positional_emb\n@@ -144,32 +139,77 @@ class CCTTokenizer(layers.Layer):\n         outputs = self.conv_model(images)\n         # After passing the images through our mini-network the spatial dimensions\n         # are flattened to form sequences.\n-        reshaped = tf.reshape(\n+        reshaped = keras.ops.reshape(\n             outputs,\n             (\n                 -1,\n-                tf.shape(outputs)[1] * tf.shape(outputs)[2],\n-                tf.shape(outputs)[-1],\n+                keras.ops.shape(outputs)[1] * keras.ops.shape(outputs)[2],\n+                keras.ops.shape(outputs)[-1],\n             ),\n         )\n         return reshaped\n \n-    def positional_embedding(self, image_size):\n-        # Positional embeddings are optional in CCT. Here, we calculate\n-        # the number of sequences and initialize an `Embedding` layer to\n-        # compute the positional embeddings later.\n-        if self.positional_emb:\n-            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n-            dummy_outputs = self.call(dummy_inputs)\n-            sequence_length = tf.shape(dummy_outputs)[1]\n-            projection_dim = tf.shape(dummy_outputs)[-1]\n \n-            embed_layer = layers.Embedding(\n-                input_dim=sequence_length, output_dim=projection_dim\n+\"\"\"\n+Positional embeddings are optional in CCT. If we want to use them, we can use\n+the Layer defined below.\n+\"\"\"\n+\n+\n+class PositionEmbedding(keras.layers.Layer):\n+    def __init__(\n+        self,\n+        sequence_length,\n+        initializer=\"glorot_uniform\",\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        if sequence_length is None:\n+            raise ValueError(\n+                \"`sequence_length` must be an Integer, received `None`.\"\n             )\n-            return embed_layer, sequence_length\n-        else:\n-            return None\n+        self.sequence_length = int(sequence_length)\n+        self.initializer = keras.initializers.get(initializer)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"sequence_length\": self.sequence_length,\n+                \"initializer\": keras.initializers.serialize(self.initializer),\n+            }\n+        )\n+        return config\n+\n+    def build(self, input_shape):\n+        feature_size = input_shape[-1]\n+        self.position_embeddings = self.add_weight(\n+            name=\"embeddings\",\n+            shape=[self.sequence_length, feature_size],\n+            initializer=self.initializer,\n+            trainable=True,\n+        )\n+\n+        super().build(input_shape)\n+\n+    def call(self, inputs, start_index=0):\n+        shape = keras.ops.shape(inputs)\n+        feature_length = shape[-1]\n+        sequence_length = shape[-2]\n+        # trim to match the length of the input sequence, which might be less\n+        # than the sequence_length of the layer.\n+        position_embeddings = keras.ops.convert_to_tensor(\n+            self.position_embeddings\n+        )\n+        position_embeddings = keras.ops.slice(\n+            position_embeddings,\n+            (start_index, 0),\n+            (sequence_length, feature_length),\n+        )\n+        return keras.ops.broadcast_to(position_embeddings, shape)\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n \n \n \"\"\"\n@@ -186,11 +226,12 @@ class SequencePooling(layers.Layer):\n         self.attention = layers.Dense(1)\n \n     def call(self, x):\n-        attention_weights = tf.nn.softmax(self.attention(x), axis=1)\n-        weighted_representation = tf.matmul(\n-            attention_weights, x, transpose_a=True\n+        attention_weights = keras.ops.softmax(self.attention(x), axis=1)\n+        attention_weights = keras.ops.transpose(\n+            attention_weights, axes=(0, 2, 1)\n         )\n-        return tf.squeeze(weighted_representation, -2)\n+        weighted_representation = keras.ops.matmul(attention_weights, x)\n+        return keras.ops.squeeze(weighted_representation, -2)\n \n \n \"\"\"\n@@ -214,9 +255,9 @@ class StochasticDepth(layers.Layer):\n     def call(self, x, training=None):\n         if training:\n             keep_prob = 1 - self.drop_prob\n-            shape = (tf.shape(x)[0],) + (1,) * (tf.shape(x).shape[0] - 1)\n-            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n-            random_tensor = tf.floor(random_tensor)\n+            shape = (keras.ops.shape(x)[0],) + (1,) * (len(x.shape) - 1)\n+            random_tensor = keep_prob + keras.random.random.uniform(shape, 0, 1)\n+            random_tensor = keras.ops.floor(random_tensor)\n             return (x / keep_prob) * random_tensor\n         return x\n \n@@ -228,7 +269,7 @@ class StochasticDepth(layers.Layer):\n \n def mlp(x, hidden_units, dropout_rate):\n     for units in hidden_units:\n-        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n+        x = layers.Dense(units, activation=keras.ops.gelu)(x)\n         x = layers.Dropout(dropout_rate)(x)\n     return x\n \n@@ -278,10 +319,10 @@ def create_cct_model(\n \n     # Apply positional embedding.\n     if positional_emb:\n-        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n-        positions = tf.range(start=0, limit=seq_length, delta=1)\n-        position_embeddings = pos_embed(positions)\n-        encoded_patches += position_embeddings\n+        sequence_length = encoded_patches.shape[1]\n+        encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n+            encoded_patches\n+        )\n \n     # Calculate Stochastic Depth probabilities.\n     dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n@@ -383,7 +424,7 @@ plt.show()\n \n \"\"\"\n The CCT model we just trained has just **0.4 million** parameters, and it gets us to\n-~78% top-1 accuracy within 30 epochs. The plot above shows no signs of overfitting as\n+~79% top-1 accuracy within 30 epochs. The plot above shows no signs of overfitting as\n well. This means we can train this network for longer (perhaps with a bit more\n regularization) and may obtain even better performance. This performance can further be\n improved by additional recipes like cosine decay learning rate schedule, other data augmentation\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
