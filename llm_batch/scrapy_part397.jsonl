{"custom_id": "scrapy#98a57e241879e1b56ef8bffeb8f85f868e91c1e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2859 | Contributors (this commit): 36 | Commits (past 90d): 8 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,6 +67,6 @@ class _BenchSpider(scrapy.Spider):\n         return [scrapy.Request(url, dont_filter=True)]\n \n     def parse(self, response: Response) -> Any:\n-        assert isinstance(Response, TextResponse)\n+        assert isinstance(response, TextResponse)\n         for link in self.link_extractor.extract_links(response):\n             yield scrapy.Request(link.url, callback=self.parse)\n\n@@ -1034,6 +1034,7 @@ class BenchCommandTest(CommandTest):\n         )\n         self.assertIn(\"INFO: Crawled\", log)\n         self.assertNotIn(\"Unhandled Error\", log)\n+        self.assertNotIn(\"log_count/ERROR\", log)\n \n \n class ViewCommandTest(CommandTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d27c6b46b11c2ceaa61b35372ad8a3c98185aa18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 791 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,8 +2,10 @@\n \n from __future__ import annotations\n \n+import warnings\n from typing import TYPE_CHECKING\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n \n@@ -26,6 +28,11 @@ class HTTP10DownloadHandler:\n     lazy = False\n \n     def __init__(self, settings: BaseSettings, crawler: Crawler):\n+        warnings.warn(\n+            \"HTTP10DownloadHandler is deprecated and will be removed in a future Scrapy version.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n         self.HTTPClientFactory: type[ScrapyHTTPClientFactory] = load_object(\n             settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"]\n         )\n\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import re\n+import warnings\n from time import time\n from typing import TYPE_CHECKING\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n@@ -9,6 +10,7 @@ from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n from twisted.web.http import HTTPClient\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.httpobj import urlparse_cached\n@@ -49,6 +51,14 @@ def _parse(url: str) -> tuple[bytes, bytes, bytes, int, bytes]:\n class ScrapyHTTPPageGetter(HTTPClient):\n     delimiter = b\"\\n\"\n \n+    def __init__(self):\n+        warnings.warn(\n+            \"ScrapyHTTPPageGetter is deprecated and will be removed in a future Scrapy version.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+        super().__init__()\n+\n     def connectionMade(self):\n         self.headers = Headers()  # bucket for response headers\n \n@@ -140,6 +150,12 @@ class ScrapyHTTPClientFactory(ClientFactory):\n             self.path = self.url\n \n     def __init__(self, request: Request, timeout: float = 180):\n+        warnings.warn(\n+            \"ScrapyHTTPClientFactory is deprecated and will be removed in a future Scrapy version.\",\n+            category=ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+\n         self._url: str = urldefrag(request.url)[0]\n         # converting to bytes to comply to Twisted interface\n         self.url: bytes = to_bytes(self._url, encoding=\"ascii\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bc1aeeefc970fbd123699e0cb6d8486141bf8418", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 27 | Churn Cumulative: 822 | Contributors (this commit): 16 | Commits (past 90d): 7 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,7 @@ from scrapy.core.downloader.tls import (\n     openssl_methods,\n )\n from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.utils.deprecate import method_is_overridden\n from scrapy.utils.misc import build_from_crawler, load_object\n \n if TYPE_CHECKING:\n@@ -62,6 +63,13 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n             self.tls_ciphers = AcceptableCiphers.fromOpenSSLCipherString(tls_ciphers)\n         else:\n             self.tls_ciphers = DEFAULT_CIPHERS\n+        if method_is_overridden(type(self), ScrapyClientContextFactory, \"getContext\"):\n+            warnings.warn(\n+                \"Overriding ScrapyClientContextFactory.getContext() is deprecated and that method\"\n+                \" will be removed in a future Scrapy version. Override creatorForNetloc() instead.\",\n+                category=ScrapyDeprecationWarning,\n+                stacklevel=2,\n+            )\n \n     @classmethod\n     def from_settings(\n@@ -121,7 +129,6 @@ class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n     # kept for old-style HTTP/1.0 downloader context twisted calls,\n     # e.g. connectSSL()\n     def getContext(self, hostname: Any = None, port: Any = None) -> SSL.Context:\n-        # FIXME\n         ctx: SSL.Context = self.getCertificateOptions().getContext()\n         ctx.set_options(0x4)  # OP_LEGACY_SERVER_CONNECT\n         return ctx\n\n@@ -1,8 +1,10 @@\n from __future__ import annotations\n \n import shutil\n+import warnings\n from pathlib import Path\n from tempfile import mkdtemp\n+from typing import Any\n \n import OpenSSL.SSL\n import pytest\n@@ -92,6 +94,22 @@ class ContextFactoryTestCase(ContextFactoryBaseTestCase):\n         )\n         self.assertEqual(body, to_bytes(s))\n \n+    def test_override_getContext(self):\n+        class MyFactory(ScrapyClientContextFactory):\n+            def getContext(\n+                self, hostname: Any = None, port: Any = None\n+            ) -> OpenSSL.SSL.Context:\n+                ctx: OpenSSL.SSL.Context = super().getContext(hostname, port)\n+                return ctx\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            MyFactory()\n+            self.assertEqual(len(w), 1)\n+            self.assertIn(\n+                \"Overriding ScrapyClientContextFactory.getContext() is deprecated\",\n+                str(w[0].message),\n+            )\n+\n \n class ContextFactoryTLSMethodTestCase(ContextFactoryBaseTestCase):\n     async def _assert_factory_works(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d2d2892badb2b6f47c9b597564510871c7f1518", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 184 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from typing import Any\n import OpenSSL.SSL\n import pytest\n from twisted.internet import reactor\n-from twisted.internet.defer import inlineCallbacks\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.protocols.policies import WrappingFactory\n from twisted.trial import unittest\n from twisted.web import server, static\n@@ -79,7 +79,15 @@ class ContextFactoryBaseTestCase(unittest.TestCase):\n         response: TxResponse = await maybe_deferred_to_future(\n             agent.request(b\"GET\", url.encode(), bodyProducer=body_producer)\n         )\n-        return await maybe_deferred_to_future(readBody(response))  # type: ignore[arg-type]\n+        with warnings.catch_warnings():\n+            # https://github.com/twisted/twisted/issues/8227\n+            warnings.filterwarnings(\n+                \"ignore\",\n+                category=DeprecationWarning,\n+                message=r\".*does not have an abortConnection method\",\n+            )\n+            d: Deferred[bytes] = readBody(response)  # type: ignore[arg-type]\n+        return await maybe_deferred_to_future(d)\n \n \n class ContextFactoryTestCase(ContextFactoryBaseTestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a80871c3a2b353d870e337715bbbeedf6bc216e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 123 | Files Changed: 5 | Hunks: 22 | Methods Changed: 9 | Complexity Δ (Sum/Max): -6/1 | Churn Δ: 161 | Churn Cumulative: 8445 | Contributors (this commit): 57 | Commits (past 90d): 19 | Contributors (cumulative): 123 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from contextlib import suppress\n from io import BytesIO\n from time import time\n from typing import TYPE_CHECKING, Any, TypedDict, TypeVar\n-from urllib.parse import urldefrag, urlunparse\n+from urllib.parse import urldefrag, urlparse\n \n from twisted.internet import ssl\n from twisted.internet.defer import CancelledError, Deferred, succeed\n@@ -32,11 +32,12 @@ from zope.interface import implementer\n \n from scrapy import Request, Spider, signals\n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n-from scrapy.core.downloader.webclient import _parse\n from scrapy.exceptions import StopDownload\n from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes, to_unicode\n+from scrapy.utils.url import add_http_if_no_scheme\n \n if TYPE_CHECKING:\n     from twisted.internet.base import ReactorBase\n@@ -378,12 +379,15 @@ class ScrapyAgent:\n         bindaddress = request.meta.get(\"bindaddress\") or self._bindAddress\n         proxy = request.meta.get(\"proxy\")\n         if proxy:\n-            proxyScheme, proxyNetloc, proxyHost, proxyPort, proxyParams = _parse(proxy)\n-            scheme = _parse(request.url)[0]\n-            proxyHost_str = to_unicode(proxyHost)\n-            if scheme == b\"https\":\n+            proxy = add_http_if_no_scheme(proxy)\n+            proxy_parsed = urlparse(proxy)\n+            proxy_host = proxy_parsed.hostname\n+            proxy_port = proxy_parsed.port\n+            if not proxy_port:\n+                proxy_port = 443 if proxy_parsed.scheme == \"https\" else 80\n+            if urlparse_cached(request).scheme == \"https\":\n                 proxyAuth = request.headers.get(b\"Proxy-Authorization\", None)\n-                proxyConf = (proxyHost_str, proxyPort, proxyAuth)\n+                proxyConf = (proxy_host, proxy_port, proxyAuth)\n                 return self._TunnelingAgent(\n                     reactor=reactor,\n                     proxyConf=proxyConf,\n@@ -392,13 +396,9 @@ class ScrapyAgent:\n                     bindAddress=bindaddress,\n                     pool=self._pool,\n                 )\n-            proxyScheme = proxyScheme or b\"http\"\n-            proxyURI = urlunparse(\n-                (proxyScheme, proxyNetloc, proxyParams, b\"\", b\"\", b\"\")\n-            )\n             return self._ProxyAgent(\n                 reactor=reactor,\n-                proxyURI=to_bytes(proxyURI, encoding=\"ascii\"),\n+                proxyURI=to_bytes(proxy, encoding=\"ascii\"),\n                 connectTimeout=timeout,\n                 bindAddress=bindaddress,\n                 pool=self._pool,\n\n@@ -8,8 +8,8 @@ from twisted.internet.error import TimeoutError\n from twisted.web.client import URI\n \n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n-from scrapy.core.downloader.webclient import _parse\n from scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n if TYPE_CHECKING:\n@@ -75,10 +75,7 @@ class ScrapyH2Agent:\n         bind_address = request.meta.get(\"bindaddress\") or self._bind_address\n         proxy = request.meta.get(\"proxy\")\n         if proxy:\n-            _, _, proxy_host, proxy_port, proxy_params = _parse(proxy)\n-            scheme = _parse(request.url)[0]\n-\n-            if scheme == b\"https\":\n+            if urlparse_cached(request).scheme == \"https\":\n                 # ToDo\n                 raise NotImplementedError(\n                     \"Tunneling via CONNECT method using HTTP/2.0 is not yet supported\"\n\n@@ -2,11 +2,10 @@\n \n from __future__ import annotations\n \n-import re\n import warnings\n from time import time\n from typing import TYPE_CHECKING\n-from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n+from urllib.parse import urldefrag, urlparse, urlunparse\n \n from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n@@ -22,34 +21,6 @@ if TYPE_CHECKING:\n     from scrapy import Request\n \n \n-def _parsed_url_args(parsed: ParseResult) -> tuple[bytes, bytes, bytes, int, bytes]:\n-    # Assume parsed is urlparse-d from Request.url,\n-    # which was passed via safe_url_string and is ascii-only.\n-    path_str = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n-    path = to_bytes(path_str, encoding=\"ascii\")\n-    assert parsed.hostname is not None\n-    host = to_bytes(parsed.hostname, encoding=\"ascii\")\n-    port = parsed.port\n-    scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n-    netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n-    if port is None:\n-        port = 443 if scheme == b\"https\" else 80\n-    return scheme, netloc, host, port, path\n-\n-\n-def _parse(url: str) -> tuple[bytes, bytes, bytes, int, bytes]:\n-    \"\"\"Return tuple of (scheme, netloc, host, port, path),\n-    all in bytes except for port which is int.\n-    Assume url is from Request.url, which was passed via safe_url_string\n-    and is ascii-only.\n-    \"\"\"\n-    url = url.strip()\n-    if not re.match(r\"^\\w+://\", url):\n-        url = \"//\" + url\n-    parsed = urlparse(url)\n-    return _parsed_url_args(parsed)\n-\n-\n class ScrapyHTTPPageGetter(HTTPClient):\n     delimiter = b\"\\n\"\n \n@@ -142,14 +113,29 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         )\n \n     def _set_connection_attributes(self, request):\n-        parsed = urlparse_cached(request)\n-        self.scheme, self.netloc, self.host, self.port, self.path = _parsed_url_args(\n-            parsed\n-        )\n         proxy = request.meta.get(\"proxy\")\n         if proxy:\n-            self.scheme, _, self.host, self.port, _ = _parse(proxy)\n+            proxy_parsed = urlparse(to_bytes(proxy, encoding=\"ascii\"))\n+            self.scheme = proxy_parsed.scheme\n+            self.host = proxy_parsed.hostname\n+            self.port = proxy_parsed.port\n+            self.netloc = proxy_parsed.netloc\n+            if self.port is None:\n+                self.port = 443 if proxy_parsed.scheme == b\"https\" else 80\n             self.path = self.url\n+        else:\n+            parsed = urlparse_cached(request)\n+            path_str = urlunparse(\n+                (\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\")\n+            )\n+            self.path = to_bytes(path_str, encoding=\"ascii\")\n+            assert parsed.hostname is not None\n+            self.host = to_bytes(parsed.hostname, encoding=\"ascii\")\n+            self.port = parsed.port\n+            self.scheme = to_bytes(parsed.scheme, encoding=\"ascii\")\n+            self.netloc = to_bytes(parsed.netloc, encoding=\"ascii\")\n+            if self.port is None:\n+                self.port = 443 if self.scheme == b\"https\" else 80\n \n     def __init__(self, request: Request, timeout: float = 180):\n         warnings.warn(\n\n@@ -785,9 +785,6 @@ class HttpProxyTestCase(unittest.TestCase):\n class Http10ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls: type = HTTP10DownloadHandler\n \n-    def test_download_with_proxy_https_noconnect(self):\n-        raise unittest.SkipTest(\"noconnect is not supported in HTTP10DownloadHandler\")\n-\n \n class Http11ProxyTestCase(HttpProxyTestCase):\n     download_handler_cls: type = HTTP11DownloadHandler\n\n@@ -8,6 +8,7 @@ from __future__ import annotations\n import shutil\n from pathlib import Path\n from tempfile import mkdtemp\n+from urllib.parse import urlparse\n \n import OpenSSL.SSL\n import pytest\n@@ -61,72 +62,6 @@ def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n     ).deferred\n \n \n-class ParseUrlTestCase(unittest.TestCase):\n-    \"\"\"Test URL parsing facility and defaults values.\"\"\"\n-\n-    def _parse(self, url):\n-        f = client.ScrapyHTTPClientFactory(Request(url))\n-        return (f.scheme, f.netloc, f.host, f.port, f.path)\n-\n-    def testParse(self):\n-        lip = \"127.0.0.1\"\n-        tests = (\n-            (\n-                \"http://127.0.0.1?c=v&c2=v2#fragment\",\n-                (\"http\", lip, lip, 80, \"/?c=v&c2=v2\"),\n-            ),\n-            (\n-                \"http://127.0.0.1/?c=v&c2=v2#fragment\",\n-                (\"http\", lip, lip, 80, \"/?c=v&c2=v2\"),\n-            ),\n-            (\n-                \"http://127.0.0.1/foo?c=v&c2=v2#frag\",\n-                (\"http\", lip, lip, 80, \"/foo?c=v&c2=v2\"),\n-            ),\n-            (\n-                \"http://127.0.0.1:100?c=v&c2=v2#fragment\",\n-                (\"http\", lip + \":100\", lip, 100, \"/?c=v&c2=v2\"),\n-            ),\n-            (\n-                \"http://127.0.0.1:100/?c=v&c2=v2#frag\",\n-                (\"http\", lip + \":100\", lip, 100, \"/?c=v&c2=v2\"),\n-            ),\n-            (\n-                \"http://127.0.0.1:100/foo?c=v&c2=v2#frag\",\n-                (\"http\", lip + \":100\", lip, 100, \"/foo?c=v&c2=v2\"),\n-            ),\n-            (\"http://127.0.0.1\", (\"http\", lip, lip, 80, \"/\")),\n-            (\"http://127.0.0.1/\", (\"http\", lip, lip, 80, \"/\")),\n-            (\"http://127.0.0.1/foo\", (\"http\", lip, lip, 80, \"/foo\")),\n-            (\"http://127.0.0.1?param=value\", (\"http\", lip, lip, 80, \"/?param=value\")),\n-            (\"http://127.0.0.1/?param=value\", (\"http\", lip, lip, 80, \"/?param=value\")),\n-            (\n-                \"http://127.0.0.1:12345/foo\",\n-                (\"http\", lip + \":12345\", lip, 12345, \"/foo\"),\n-            ),\n-            (\"http://spam:12345/foo\", (\"http\", \"spam:12345\", \"spam\", 12345, \"/foo\")),\n-            (\n-                \"http://spam.test.org/foo\",\n-                (\"http\", \"spam.test.org\", \"spam.test.org\", 80, \"/foo\"),\n-            ),\n-            (\"https://127.0.0.1/foo\", (\"https\", lip, lip, 443, \"/foo\")),\n-            (\n-                \"https://127.0.0.1/?param=value\",\n-                (\"https\", lip, lip, 443, \"/?param=value\"),\n-            ),\n-            (\"https://127.0.0.1:12345/\", (\"https\", lip + \":12345\", lip, 12345, \"/\")),\n-            (\n-                \"http://scrapytest.org/foo \",\n-                (\"http\", \"scrapytest.org\", \"scrapytest.org\", 80, \"/foo\"),\n-            ),\n-            (\"http://egg:7890 \", (\"http\", \"egg:7890\", \"egg\", 7890, \"/\")),\n-        )\n-\n-        for url, test in tests:\n-            test = tuple(to_bytes(x) if not isinstance(x, int) else x for x in test)\n-            self.assertEqual(client._parse(url), test, url)\n-\n-\n @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n class ScrapyHTTPPageGetterTests(unittest.TestCase):\n     def test_earlyHeaders(self):\n@@ -388,9 +323,9 @@ class WebClientTestCase(unittest.TestCase):\n \n     def testFactoryInfo(self):\n         url = self.getURL(\"file\")\n-        _, _, host, port, _ = client._parse(url)\n+        parsed = urlparse(url)\n         factory = client.ScrapyHTTPClientFactory(Request(url))\n-        reactor.connectTCP(to_unicode(host), port, factory)\n+        reactor.connectTCP(parsed.hostname, parsed.port, factory)\n         return factory.deferred.addCallback(self._cbFactoryInfo, factory)\n \n     def _cbFactoryInfo(self, ignoredResult, factory):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#200d76afa96a78899faf9e2c30ef45273b71c600", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 44 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 90 | Churn Cumulative: 1659 | Contributors (this commit): 23 | Commits (past 90d): 6 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -243,46 +243,7 @@ class CrawlerRun:\n         self.signals_caught[sig] = signalargs\n \n \n-class EngineTest(unittest.TestCase):\n-    @defer.inlineCallbacks\n-    def test_crawler(self):\n-        for spider in (\n-            TestSpider,\n-            DictItemsSpider,\n-            AttrsItemsSpider,\n-            DataClassItemsSpider,\n-        ):\n-            run = CrawlerRun(spider)\n-            yield run.run()\n-            self._assert_visited_urls(run)\n-            self._assert_scheduled_requests(run, count=9)\n-            self._assert_downloaded_responses(run, count=9)\n-            self._assert_scraped_items(run)\n-            self._assert_signals_caught(run)\n-            self._assert_bytes_received(run)\n-\n-    @defer.inlineCallbacks\n-    def test_crawler_dupefilter(self):\n-        run = CrawlerRun(TestDupeFilterSpider)\n-        yield run.run()\n-        self._assert_scheduled_requests(run, count=8)\n-        self._assert_dropped_requests(run)\n-\n-    @defer.inlineCallbacks\n-    def test_crawler_itemerror(self):\n-        run = CrawlerRun(ItemZeroDivisionErrorSpider)\n-        yield run.run()\n-        self._assert_items_error(run)\n-\n-    @defer.inlineCallbacks\n-    def test_crawler_change_close_reason_on_idle(self):\n-        run = CrawlerRun(ChangeCloseReasonSpider)\n-        yield run.run()\n-        self.assertEqual(\n-            {\"spider\": run.spider, \"reason\": \"custom_reason\"},\n-            run.signals_caught[signals.spider_closed],\n-        )\n-\n+class EngineTestBase(unittest.TestCase):\n     def _assert_visited_urls(self, run: CrawlerRun):\n         must_be_visited = [\n             \"/\",\n@@ -422,6 +383,47 @@ class EngineTest(unittest.TestCase):\n             run.signals_caught[signals.spider_closed],\n         )\n \n+\n+class EngineTest(EngineTestBase):\n+    @defer.inlineCallbacks\n+    def test_crawler(self):\n+        for spider in (\n+            TestSpider,\n+            DictItemsSpider,\n+            AttrsItemsSpider,\n+            DataClassItemsSpider,\n+        ):\n+            run = CrawlerRun(spider)\n+            yield run.run()\n+            self._assert_visited_urls(run)\n+            self._assert_scheduled_requests(run, count=9)\n+            self._assert_downloaded_responses(run, count=9)\n+            self._assert_scraped_items(run)\n+            self._assert_signals_caught(run)\n+            self._assert_bytes_received(run)\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_dupefilter(self):\n+        run = CrawlerRun(TestDupeFilterSpider)\n+        yield run.run()\n+        self._assert_scheduled_requests(run, count=8)\n+        self._assert_dropped_requests(run)\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_itemerror(self):\n+        run = CrawlerRun(ItemZeroDivisionErrorSpider)\n+        yield run.run()\n+        self._assert_items_error(run)\n+\n+    @defer.inlineCallbacks\n+    def test_crawler_change_close_reason_on_idle(self):\n+        run = CrawlerRun(ChangeCloseReasonSpider)\n+        yield run.run()\n+        self.assertEqual(\n+            {\"spider\": run.spider, \"reason\": \"custom_reason\"},\n+            run.signals_caught[signals.spider_closed],\n+        )\n+\n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n         e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n\n@@ -7,7 +7,7 @@ from tests.test_engine import (\n     CrawlerRun,\n     DataClassItemsSpider,\n     DictItemsSpider,\n-    EngineTest,\n+    EngineTestBase,\n     TestSpider,\n )\n \n@@ -18,7 +18,7 @@ class BytesReceivedCrawlerRun(CrawlerRun):\n         raise StopDownload(fail=False)\n \n \n-class BytesReceivedEngineTest(EngineTest):\n+class BytesReceivedEngineTest(EngineTestBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n\n@@ -7,7 +7,7 @@ from tests.test_engine import (\n     CrawlerRun,\n     DataClassItemsSpider,\n     DictItemsSpider,\n-    EngineTest,\n+    EngineTestBase,\n     TestSpider,\n )\n \n@@ -18,7 +18,7 @@ class HeadersReceivedCrawlerRun(CrawlerRun):\n         raise StopDownload(fail=False)\n \n \n-class HeadersReceivedEngineTest(EngineTest):\n+class HeadersReceivedEngineTest(EngineTestBase):\n     @defer.inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a0dfbd32e8d96299c4f0f3d16cb2d52a73de339", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 17 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 46 | Churn Cumulative: 7854 | Contributors (this commit): 47 | Commits (past 90d): 9 | Contributors (cumulative): 47 | DMM Complexity: 0.6\n\nDIFF:\n@@ -13,7 +13,6 @@ import tempfile\n import warnings\n from abc import ABC, abstractmethod\n from collections import defaultdict\n-from contextlib import ExitStack\n from io import BytesIO\n from logging import getLogger\n from pathlib import Path\n@@ -623,8 +622,6 @@ class LogOnStoreFileStorage:\n \n \n class FeedExportTestBase(ABC, unittest.TestCase):\n-    __test__ = False\n-\n     class MyItem(scrapy.Item):\n         foo = scrapy.Field()\n         egg = scrapy.Field()\n@@ -641,8 +638,11 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n \n     def setUp(self):\n         self.temp_dir = tempfile.mkdtemp()\n+        self.mockserver = MockServer()\n+        self.mockserver.__enter__()\n \n     def tearDown(self):\n+        self.mockserver.__exit__(None, None, None)\n         shutil.rmtree(self.temp_dir, ignore_errors=True)\n \n     @defer.inlineCallbacks\n@@ -746,8 +746,6 @@ class ExceptionJsonItemExporter(JsonItemExporter):\n \n \n class FeedExportTest(FeedExportTestBase):\n-    __test__ = True\n-\n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data.\"\"\"\n@@ -760,8 +758,7 @@ class FeedExportTest(FeedExportTestBase):\n \n         content = {}\n         try:\n-            with MockServer() as s:\n-                spider_cls.start_urls = [s.url(\"/\")]\n+            spider_cls.start_urls = [self.mockserver.url(\"/\")]\n             crawler = get_crawler(spider_cls, settings)\n             yield crawler.crawl()\n \n@@ -890,8 +887,7 @@ class FeedExportTest(FeedExportTestBase):\n             },\n         }\n         crawler = get_crawler(ItemSpider, settings)\n-        with MockServer() as mockserver:\n-            yield crawler.crawl(mockserver=mockserver)\n+        yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIn(\n             \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n@@ -909,15 +905,11 @@ class FeedExportTest(FeedExportTestBase):\n             },\n         }\n         crawler = get_crawler(ItemSpider, settings)\n-        with ExitStack() as stack:\n-            mockserver = stack.enter_context(MockServer())\n-            stack.enter_context(\n-                mock.patch(\n+        with mock.patch(\n             \"scrapy.extensions.feedexport.FileFeedStorage.store\",\n             side_effect=KeyError(\"foo\"),\n-                )\n-            )\n-            yield crawler.crawl(mockserver=mockserver)\n+        ):\n+            yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIn(\n             \"feedexport/failed_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n@@ -938,8 +930,8 @@ class FeedExportTest(FeedExportTestBase):\n             },\n         }\n         crawler = get_crawler(ItemSpider, settings)\n-        with MockServer() as mockserver, mock.patch.object(S3FeedStorage, \"store\"):\n-            yield crawler.crawl(mockserver=mockserver)\n+        with mock.patch.object(S3FeedStorage, \"store\"):\n+            yield crawler.crawl(mockserver=self.mockserver)\n         self.assertIn(\n             \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n@@ -1730,8 +1722,6 @@ class FeedExportTest(FeedExportTestBase):\n \n \n class FeedPostProcessedExportsTest(FeedExportTestBase):\n-    __test__ = True\n-\n     items = [{\"foo\": \"bar\"}]\n     expected = b\"foo\\r\\nbar\\r\\n\"\n \n@@ -1764,8 +1754,7 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n         content = {}\n         try:\n-            with MockServer() as s:\n-                spider_cls.start_urls = [s.url(\"/\")]\n+            spider_cls.start_urls = [self.mockserver.url(\"/\")]\n             crawler = get_crawler(spider_cls, settings)\n             yield crawler.crawl()\n \n@@ -2253,7 +2242,6 @@ class FeedPostProcessedExportsTest(FeedExportTestBase):\n \n \n class BatchDeliveriesTest(FeedExportTestBase):\n-    __test__ = True\n     _file_mark = \"_%(batch_time)s_#%(batch_id)02d_\"\n \n     @defer.inlineCallbacks\n@@ -2265,9 +2253,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             build_url(file_path): feed for file_path, feed in FEEDS.items()\n         }\n         content = defaultdict(list)\n-        try:\n-            with MockServer() as s:\n-                spider_cls.start_urls = [s.url(\"/\")]\n+        spider_cls.start_urls = [self.mockserver.url(\"/\")]\n         crawler = get_crawler(spider_cls, settings)\n         yield crawler.crawl()\n \n@@ -2278,8 +2264,6 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 continue\n             for file in sorted(dir_name.iterdir()):\n                 content[feed[\"format\"]].append(file.read_bytes())\n-        finally:\n-            self.tearDown()\n         return content\n \n     @defer.inlineCallbacks\n@@ -2604,8 +2588,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             \"FEED_EXPORT_BATCH_ITEM_COUNT\": 1,\n         }\n         crawler = get_crawler(ItemSpider, settings)\n-        with MockServer() as mockserver:\n-            yield crawler.crawl(total=2, mockserver=mockserver)\n+        yield crawler.crawl(total=2, mockserver=self.mockserver)\n         self.assertIn(\n             \"feedexport/success_count/FileFeedStorage\", crawler.stats.get_stats()\n         )\n@@ -2675,8 +2658,7 @@ class BatchDeliveriesTest(FeedExportTestBase):\n             def parse(self, response):\n                 yield from items\n \n-        with MockServer() as server:\n-            TestSpider.start_urls = [server.url(\"/\")]\n+        TestSpider.start_urls = [self.mockserver.url(\"/\")]\n         crawler = get_crawler(TestSpider, settings)\n         yield crawler.crawl()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#783b98dedaea65e0c6658d9d588328dd887c2c8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 119 | Lines Deleted: 72 | Files Changed: 14 | Hunks: 34 | Methods Changed: 56 | Complexity Δ (Sum/Max): 9/2 | Churn Δ: 191 | Churn Cumulative: 16400 | Contributors (this commit): 83 | Commits (past 90d): 40 | Contributors (cumulative): 213 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,12 +13,14 @@ from tests.spiders import (\n \n \n class TestCloseSpider(TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_closespider_itemcount(self):\n\n@@ -56,12 +56,14 @@ from tests.spiders import (\n \n \n class CrawlTestCase(TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_follow_all(self):\n@@ -448,12 +450,14 @@ with multiples lines\n \n \n class CrawlSpiderTestCase(TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def _run_spider(self, spider_cls):\n\n@@ -693,12 +693,14 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n     settings_dict: dict | None = None\n \n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_download_with_content_length(self):\n\n@@ -50,13 +50,17 @@ class DownloaderSlotsSettingsTestSpider(MetaSpider):\n \n \n class CrawlTestCase(TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n-        self.runner = CrawlerRunner()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n+    def setUp(self):\n+        self.runner = CrawlerRunner()\n \n     @defer.inlineCallbacks\n     def test_delay(self):\n\n@@ -636,13 +636,19 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         filename = \"\".join(chars)\n         return Path(self.temp_dir, inter_dir, filename)\n \n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n     def setUp(self):\n         self.temp_dir = tempfile.mkdtemp()\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n \n     def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n         shutil.rmtree(self.temp_dir, ignore_errors=True)\n \n     @defer.inlineCallbacks\n\n@@ -262,9 +262,16 @@ class DropSomeItemsPipeline:\n \n \n class ShowOrSkipMessagesTestCase(TwistedTestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n     def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n         self.base_settings = {\n             \"LOG_LEVEL\": \"DEBUG\",\n             \"ITEM_PIPELINES\": {\n@@ -272,9 +279,6 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n             },\n         }\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n-\n     @defer.inlineCallbacks\n     def test_show_messages(self):\n         crawler = get_crawler(ItemSpider, self.base_settings)\n\n@@ -64,10 +64,16 @@ class FileDownloadCrawlTestCase(TestCase):\n         \"ed3f6538dc15d4d9179dae57319edc5f\",\n     }\n \n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n+    def setUp(self):\n         # prepare a directory for storing files\n         self.tmpmediastore = Path(mkdtemp())\n         self.settings = {\n@@ -80,7 +86,6 @@ class FileDownloadCrawlTestCase(TestCase):\n     def tearDown(self):\n         shutil.rmtree(self.tmpmediastore)\n         self.items = []\n-        self.mockserver.__exit__(None, None, None)\n \n     def _on_item_scraped(self, item):\n         self.items.append(item)\n\n@@ -77,12 +77,14 @@ class ItemSpider(Spider):\n \n \n class PipelineTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     def _on_item_scraped(self, item):\n         self.assertIsInstance(item, dict)\n\n@@ -62,14 +62,21 @@ def _wrong_credentials(proxy_url):\n \n \n class ProxyConnectTestCase(TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n     def setUp(self):\n         try:\n             import mitmproxy  # noqa: F401\n         except ImportError:\n             self.skipTest(\"mitmproxy is not installed\")\n \n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n         self._oldenv = os.environ.copy()\n \n         self._proxy = MitmProxy()\n@@ -78,7 +85,6 @@ class ProxyConnectTestCase(TestCase):\n         os.environ[\"http_proxy\"] = proxy_url\n \n     def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n         self._proxy.stop()\n         os.environ = self._oldenv\n \n\n@@ -57,12 +57,14 @@ class AlternativeCallbacksMiddleware:\n \n \n class CrawlTestCase(TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_response_200(self):\n\n@@ -154,12 +154,14 @@ class KeywordArgumentsSpider(MockServerSpider):\n class CallbackKeywordArgumentsTestCase(TestCase):\n     maxDiff = None\n \n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_callback_kwargs(self):\n\n@@ -25,12 +25,14 @@ class SignalCatcherSpider(Spider):\n \n \n class TestCatching(TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_success(self):\n\n@@ -21,13 +21,17 @@ class ItemSpider(Spider):\n \n \n class AsyncSignalTestCase(unittest.TestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n-        self.items = []\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n+    def setUp(self):\n+        self.items = []\n \n     async def _on_item_scraped(self, item):\n         item = await get_from_asyncio_queue(item)\n\n@@ -172,12 +172,14 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n \n \n class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n-    def setUp(self):\n-        self.mockserver = MockServer()\n-        self.mockserver.__enter__()\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n \n-    def tearDown(self):\n-        self.mockserver.__exit__(None, None, None)\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n \n     @defer.inlineCallbacks\n     def test_middleware_works(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#df688910e0499b0a874d220fe00ed7355a70fce0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 666 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ from scrapy.utils.test import get_crawler\n \n \n class _BaseTest(unittest.TestCase):\n-    storage_class = \"scrapy.extensions.httpcache.DbmCacheStorage\"\n+    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n     policy_class = \"scrapy.extensions.httpcache.RFC2616Policy\"\n \n     def setUp(self):\n@@ -161,11 +161,7 @@ class DbmStorageWithCustomDbmModuleTest(DbmStorageTest):\n             self.assertEqual(storage.dbmodule.__name__, self.dbm_module)\n \n \n-class FilesystemStorageTest(DefaultStorageTest):\n-    storage_class = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n-\n-\n-class FilesystemStorageGzipTest(FilesystemStorageTest):\n+class FilesystemStorageGzipTest(DefaultStorageTest):\n     def _get_settings(self, **new_settings):\n         new_settings.setdefault(\"HTTPCACHE_GZIP\", True)\n         return super()._get_settings(**new_settings)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#393ff96e45ffcb738bc9f8d3240cd4d999bd07df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 162 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,10 +3,11 @@ from __future__ import annotations\n import logging\n import re\n from typing import TYPE_CHECKING\n+from warnings import warn\n \n from w3lib import html\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Response\n \n if TYPE_CHECKING:\n@@ -30,6 +31,13 @@ class AjaxCrawlMiddleware:\n         if not settings.getbool(\"AJAXCRAWL_ENABLED\"):\n             raise NotConfigured\n \n+        warn(\n+            \"scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware is deprecated\"\n+            \" and will be removed in a future Scrapy version.\",\n+            ScrapyDeprecationWarning,\n+            stacklevel=2,\n+        )\n+\n         # XXX: Google parses at least first 100k bytes; scrapy's redirect\n         # middleware parses first 4k. 4k turns out to be insufficient\n         # for this middleware, and parsing 100k could be slow.\n@@ -75,7 +83,6 @@ class AjaxCrawlMiddleware:\n         return _has_ajaxcrawlable_meta(body)\n \n \n-# XXX: move it to w3lib?\n _ajax_crawlable_re: re.Pattern[str] = re.compile(\n     r'<meta\\s+name=[\"\\']fragment[\"\\']\\s+content=[\"\\']![\"\\']/?>'\n )\n\n@@ -1,5 +1,7 @@\n import unittest\n \n+import pytest\n+\n from scrapy.downloadermiddlewares.ajaxcrawl import AjaxCrawlMiddleware\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.spiders import Spider\n@@ -8,6 +10,7 @@ from scrapy.utils.test import get_crawler\n __doctests__ = [\"scrapy.downloadermiddlewares.ajaxcrawl\"]\n \n \n+@pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n class AjaxCrawlMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n         crawler = get_crawler(Spider, {\"AJAXCRAWL_ENABLED\": True})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4842bcbf1da41029a604d1bf743d4dc893960d39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 17 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 26 | Churn Cumulative: 5266 | Contributors (this commit): 55 | Commits (past 90d): 18 | Contributors (cumulative): 90 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from w3lib import html\n \n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Response\n+from scrapy.utils.url import escape_ajax\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n@@ -64,8 +65,7 @@ class AjaxCrawlMiddleware:\n         if not self._has_ajax_crawlable_variant(response):\n             return response\n \n-        # scrapy already handles #! links properly\n-        ajax_crawl_request = request.replace(url=request.url + \"#!\")\n+        ajax_crawl_request = request.replace(url=escape_ajax(request.url + \"#!\"))\n         logger.debug(\n             \"Downloading AJAX crawlable %(ajax_crawl_request)s instead of %(request)s\",\n             {\"ajax_crawl_request\": ajax_crawl_request, \"request\": request},\n\n@@ -27,7 +27,6 @@ from scrapy.http.headers import Headers\n from scrapy.utils.curl import curl_to_request_kwargs\n from scrapy.utils.python import to_bytes\n from scrapy.utils.trackref import object_ref\n-from scrapy.utils.url import escape_ajax\n \n if TYPE_CHECKING:\n     from collections.abc import Callable, Iterable, Mapping\n@@ -170,8 +169,7 @@ class Request(object_ref):\n         if not isinstance(url, str):\n             raise TypeError(f\"Request url must be str, got {type(url).__name__}\")\n \n-        s = safe_url_string(url, self.encoding)\n-        self._url = escape_ajax(s)\n+        self._url = safe_url_string(url, self.encoding)\n \n         if (\n             \"://\" not in self._url\n\n@@ -10,6 +10,7 @@ import warnings\n from importlib import import_module\n from typing import TYPE_CHECKING, Union\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n+from warnings import warn\n \n from w3lib.url import __all__ as _public_w3lib_objects\n from w3lib.url import add_or_replace_parameter as _add_or_replace_parameter\n@@ -83,6 +84,11 @@ def escape_ajax(url: str) -> str:\n     >>> escape_ajax(\"www.example.com/ajax.html\")\n     'www.example.com/ajax.html'\n     \"\"\"\n+    warn(\n+        \"escape_ajax() is deprecated and will be removed in a future Scrapy version.\",\n+        ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n     defrag, frag = urldefrag(url)\n     if not frag.startswith(\"!\"):\n         return url\n\n@@ -187,18 +187,6 @@ class RequestTest(unittest.TestCase):\n         assert isinstance(r4.body, bytes)\n         self.assertEqual(r4.body, b\"Price: \\xa3100\")\n \n-    def test_ajax_url(self):\n-        # ascii url\n-        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n-        self.assertEqual(\n-            r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\"\n-        )\n-        # unicode url\n-        r = self.request_class(url=\"http://www.example.com/ajax.html#!key=value\")\n-        self.assertEqual(\n-            r.url, \"http://www.example.com/ajax.html?_escaped_fragment_=key%3Dvalue\"\n-        )\n-\n     def test_copy(self):\n         \"\"\"Test Request copy\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
