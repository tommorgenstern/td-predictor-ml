{"custom_id": "scrapy#5902aab25ce2ef0b26e158e0455ee6f0846636bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 27 | Files Changed: 4 | Hunks: 26 | Methods Changed: 14 | Complexity Δ (Sum/Max): 6/8 | Churn Δ: 134 | Churn Cumulative: 2455 | Contributors (this commit): 32 | Commits (past 90d): 14 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,7 +14,12 @@ from scrapy.core.downloader.handlers import DownloadHandlers\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.resolver import dnscache\n-from scrapy.utils.asyncio import AsyncioLoopingCall, create_looping_call\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    CallLaterResult,\n+    call_later,\n+    create_looping_call,\n+)\n from scrapy.utils.defer import (\n     deferred_from_coro,\n     maybe_deferred_to_future,\n@@ -50,7 +55,7 @@ class Slot:\n         self.queue: deque[tuple[Request, Deferred[Response]]] = deque()\n         self.transferring: set[Request] = set()\n         self.lastseen: float = 0\n-        self.latercall = None\n+        self.latercall: CallLaterResult | None = None\n \n     def free_transfer_slots(self) -> int:\n         return self.concurrency - len(self.transferring)\n@@ -61,8 +66,9 @@ class Slot:\n         return self.delay\n \n     def close(self) -> None:\n-        if self.latercall and self.latercall.active():\n+        if self.latercall:\n             self.latercall.cancel()\n+            self.latercall = None\n \n     def __repr__(self) -> str:\n         cls_name = self.__class__.__name__\n@@ -191,9 +197,8 @@ class Downloader:\n             slot.active.remove(request)\n \n     def _process_queue(self, spider: Spider, slot: Slot) -> None:\n-        from twisted.internet import reactor\n-\n-        if slot.latercall and slot.latercall.active():\n+        if slot.latercall:\n+            # block processing until slot.latercall is called\n             return\n \n         # Delay queue processing if a download_delay is configured\n@@ -202,9 +207,7 @@ class Downloader:\n         if delay:\n             penalty = delay - now + slot.lastseen\n             if penalty > 0:\n-                slot.latercall = reactor.callLater(\n-                    penalty, self._process_queue, spider, slot\n-                )\n+                slot.latercall = call_later(penalty, self._latercall, spider, slot)\n                 return\n \n         # Process enqueued requests if there are free slots to transfer for this slot\n@@ -218,6 +221,10 @@ class Downloader:\n                 self._process_queue(spider, slot)\n                 break\n \n+    def _latercall(self, spider: Spider, slot: Slot) -> None:\n+        slot.latercall = None\n+        self._process_queue(spider, slot)\n+\n     async def _download(self, slot: Slot, request: Request, spider: Spider) -> Response:\n         # The order is very important for the following logic. Do not change!\n         slot.transferring.add(request)\n\n@@ -12,9 +12,15 @@ from typing import TYPE_CHECKING, Any\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n-from scrapy.utils.asyncio import create_looping_call\n+from scrapy.utils.asyncio import (\n+    AsyncioLoopingCall,\n+    CallLaterResult,\n+    call_later,\n+    create_looping_call,\n+)\n \n if TYPE_CHECKING:\n+    from twisted.internet.task import LoopingCall\n     from twisted.python.failure import Failure\n \n     # typing.Self requires Python 3.11\n@@ -31,6 +37,12 @@ class CloseSpider:\n     def __init__(self, crawler: Crawler):\n         self.crawler: Crawler = crawler\n \n+        # for CLOSESPIDER_TIMEOUT\n+        self.task: CallLaterResult | None = None\n+\n+        # for CLOSESPIDER_TIMEOUT_NO_ITEM\n+        self.task_no_item: AsyncioLoopingCall | LoopingCall | None = None\n+\n         self.close_on: dict[str, Any] = {\n             \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n             \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n@@ -92,14 +104,12 @@ class CloseSpider:\n             self.crawler.engine.close_spider(spider, \"closespider_pagecount_no_item\")\n \n     def spider_opened(self, spider: Spider) -> None:\n-        from twisted.internet import reactor\n-\n         assert self.crawler.engine\n-        self.task = reactor.callLater(\n+        self.task = call_later(\n             self.close_on[\"timeout\"],\n             self.crawler.engine.close_spider,\n             spider,\n-            reason=\"closespider_timeout\",\n+            \"closespider_timeout\",\n         )\n \n     def item_scraped(self, item: Any, spider: Spider) -> None:\n@@ -110,13 +120,14 @@ class CloseSpider:\n             self.crawler.engine.close_spider(spider, \"closespider_itemcount\")\n \n     def spider_closed(self, spider: Spider) -> None:\n-        task = getattr(self, \"task\", None)\n-        if task and task.active():\n-            task.cancel()\n+        if self.task:\n+            self.task.cancel()\n+            self.task = None\n \n-        task_no_item = getattr(self, \"task_no_item\", None)\n-        if task_no_item and task_no_item.running:\n-            task_no_item.stop()\n+        if self.task_no_item:\n+            if self.task_no_item.running:\n+                self.task_no_item.stop()\n+            self.task_no_item = None\n \n     def spider_opened_no_item(self, spider: Spider) -> None:\n         self.task_no_item = create_looping_call(self._count_items_produced, spider)\n\n@@ -15,10 +15,14 @@ from scrapy.utils.asyncgen import as_async_generator\n from scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\n \n if TYPE_CHECKING:\n+    from twisted.internet.base import DelayedCall\n+\n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n-    from typing_extensions import Concatenate, ParamSpec\n+    # typing.Self, typing.TypeVarTuple and typing.Unpack require Python 3.11\n+    from typing_extensions import Concatenate, ParamSpec, Self, TypeVarTuple, Unpack\n \n     _P = ParamSpec(\"_P\")\n+    _Ts = TypeVarTuple(\"_Ts\")\n \n \n _T = TypeVar(\"_T\")\n@@ -192,3 +196,60 @@ def create_looping_call(\n     if is_asyncio_available():\n         return AsyncioLoopingCall(func, *args, **kwargs)\n     return LoopingCall(func, *args, **kwargs)\n+\n+\n+def call_later(\n+    delay: float, func: Callable[[Unpack[_Ts]], object], *args: Unpack[_Ts]\n+) -> CallLaterResult:\n+    \"\"\"Schedule a function to be called after a delay.\n+\n+    This uses either ``loop.call_later()`` or ``reactor.callLater()``, depending\n+    on whether asyncio support is available.\n+    \"\"\"\n+    if is_asyncio_available():\n+        loop = asyncio.get_event_loop()\n+        return CallLaterResult.from_asyncio(loop.call_later(delay, func, *args))\n+\n+    from twisted.internet import reactor\n+\n+    return CallLaterResult.from_twisted(reactor.callLater(delay, func, *args))\n+\n+\n+class CallLaterResult:\n+    \"\"\"An universal result for :func:`call_later`, wrapping either\n+    :class:`asyncio.TimerHandle` or :class:`twisted.internet.base.DelayedCall`.\n+\n+    The provided API is close to the :class:`asyncio.TimerHandle` one: there is\n+    no ``active()`` (as there is no such public API in\n+    :class:`asyncio.TimerHandle`) but ``cancel()`` can be called on already\n+    called or cancelled instances.\n+    \"\"\"\n+\n+    _timer_handle: asyncio.TimerHandle | None = None\n+    _delayed_call: DelayedCall | None = None\n+\n+    @classmethod\n+    def from_asyncio(cls, timer_handle: asyncio.TimerHandle) -> Self:\n+        \"\"\"Create a CallLaterResult from an asyncio TimerHandle.\"\"\"\n+        o = cls()\n+        o._timer_handle = timer_handle\n+        return o\n+\n+    @classmethod\n+    def from_twisted(cls, delayed_call: DelayedCall) -> Self:\n+        \"\"\"Create a CallLaterResult from a Twisted DelayedCall.\"\"\"\n+        o = cls()\n+        o._delayed_call = delayed_call\n+        return o\n+\n+    def cancel(self) -> None:\n+        \"\"\"Cancel the underlying delayed call.\n+\n+        Does nothing if the delayed call was already called or cancelled.\n+        \"\"\"\n+        if self._timer_handle:\n+            self._timer_handle.cancel()\n+            self._timer_handle = None\n+        elif self._delayed_call and self._delayed_call.active():\n+            self._delayed_call.cancel()\n+            self._delayed_call = None\n\n@@ -16,13 +16,14 @@ if TYPE_CHECKING:\n     from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n     from collections.abc import Callable\n \n-    from twisted.internet.base import DelayedCall\n     from twisted.internet.protocol import ServerFactory\n     from twisted.internet.tcp import Port\n \n     # typing.ParamSpec requires Python 3.10\n     from typing_extensions import ParamSpec\n \n+    from scrapy.utils.asyncio import CallLaterResult\n+\n     _P = ParamSpec(\"_P\")\n \n _T = TypeVar(\"_T\")\n@@ -55,27 +56,27 @@ class CallLaterOnce(Generic[_T]):\n         self._func: Callable[_P, _T] = func\n         self._a: tuple[Any, ...] = a\n         self._kw: dict[str, Any] = kw\n-        self._call: DelayedCall | None = None\n+        self._call: CallLaterResult | None = None\n         self._deferreds: list[Deferred] = []\n \n     def schedule(self, delay: float = 0) -> None:\n-        from twisted.internet import reactor\n+        from scrapy.utils.asyncio import call_later\n \n         if self._call is None:\n-            self._call = reactor.callLater(delay, self)\n+            self._call = call_later(delay, self)\n \n     def cancel(self) -> None:\n         if self._call:\n             self._call.cancel()\n \n     def __call__(self) -> _T:\n-        from twisted.internet import reactor\n+        from scrapy.utils.asyncio import call_later\n \n         self._call = None\n         result = self._func(*self._a, **self._kw)\n \n         for d in self._deferreds:\n-            reactor.callLater(0, d.callback, None)\n+            call_later(0, d.callback, None)\n         self._deferreds = []\n \n         return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d602f13e8cd22154936ded9c9356e28fe3be4cd4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 16 | Methods Changed: 27 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 84 | Churn Cumulative: 5075 | Contributors (this commit): 43 | Commits (past 90d): 18 | Contributors (cumulative): 71 | DMM Complexity: 0.9433962264150944\n\nDIFF:\n@@ -189,6 +189,7 @@ class Scraper:\n             )\n \n         assert self.crawler.spider\n+        output: Iterable[Any] | AsyncIterator[Any]\n         if isinstance(result, Response):\n             try:\n                 # call the spider middlewares and the request callback with the response\n@@ -203,7 +204,7 @@ class Scraper:\n \n         try:\n             # call the request errback with the downloader error\n-            await self.call_spider_async(result, request)\n+            output = await self.call_spider_async(result, request)\n         except Exception as spider_exc:\n             # the errback didn't silence the exception\n             if not result.check(IgnoreRequest):\n@@ -218,6 +219,8 @@ class Scraper:\n             if spider_exc is not result.value:\n                 # the errback raised a different exception, handle it\n                 self.handle_spider_error(Failure(), request, result)\n+        else:\n+            await self.handle_spider_output_async(output, request, result)\n \n     def call_spider(\n         self, result: Response | Failure, request: Request, spider: Spider | None = None\n@@ -308,7 +311,7 @@ class Scraper:\n         self,\n         result: Iterable[_T] | AsyncIterator[_T],\n         request: Request,\n-        response: Response,\n+        response: Response | Failure,\n         spider: Spider | None = None,\n     ) -> Deferred[None]:\n         \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n@@ -326,7 +329,7 @@ class Scraper:\n         self,\n         result: Iterable[_T] | AsyncIterator[_T],\n         request: Request,\n-        response: Response,\n+        response: Response | Failure,\n     ) -> None:\n         \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n         it: Iterable[_T] | AsyncIterator[_T]\n@@ -361,7 +364,7 @@ class Scraper:\n         )\n \n     def _process_spidermw_output(\n-        self, output: Any, response: Response\n+        self, output: Any, response: Response | Failure\n     ) -> Deferred[None]:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider.\n@@ -371,7 +374,7 @@ class Scraper:\n         return deferred_from_coro(self._process_spidermw_output_async(output, response))\n \n     async def _process_spidermw_output_async(\n-        self, output: Any, response: Response\n+        self, output: Any, response: Response | Failure\n     ) -> None:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider.\n@@ -385,7 +388,9 @@ class Scraper:\n         if output is not None:\n             await self.start_itemproc_async(output, response=response)\n \n-    def start_itemproc(self, item: Any, *, response: Response | None) -> Deferred[None]:\n+    def start_itemproc(\n+        self, item: Any, *, response: Response | Failure | None\n+    ) -> Deferred[None]:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n         *response* is the source of the item data. If the item does not come\n@@ -394,7 +399,7 @@ class Scraper:\n         return deferred_from_coro(self.start_itemproc_async(item, response=response))\n \n     async def start_itemproc_async(\n-        self, item: Any, *, response: Response | None\n+        self, item: Any, *, response: Response | Failure | None\n     ) -> None:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n\n@@ -116,7 +116,7 @@ class LogFormatter:\n         self,\n         item: Any,\n         exception: BaseException,\n-        response: Response | None,\n+        response: Response | Failure | None,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n@@ -137,7 +137,7 @@ class LogFormatter:\n         self,\n         item: Any,\n         exception: BaseException,\n-        response: Response | None,\n+        response: Response | Failure | None,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item causes an error while it is passing\n\n@@ -5,6 +5,7 @@ import logging\n import unittest\n from ipaddress import IPv4Address\n from socket import gethostbyname\n+from typing import Any\n from urllib.parse import urlparse\n \n import pytest\n@@ -419,6 +420,8 @@ with multiples lines\n \n \n class TestCrawlSpider(TestCase):\n+    mockserver: MockServer\n+\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -756,6 +759,34 @@ class TestCrawlSpider(TestCase):\n             )\n         assert \"Spider error processing\" in str(log)\n \n+    @defer.inlineCallbacks\n+    def test_spider_errback_item(self):\n+        def eb(failure: Failure) -> Any:\n+            return {\"foo\": \"bar\"}\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"'item_scraped_count': 1\" in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_request(self):\n+        def eb(failure: Failure) -> Request:\n+            return Request(self.mockserver.url(\"/\"))\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"Crawled (200)\" in str(log)\n+\n     @defer.inlineCallbacks\n     def test_spider_errback_downloader_error(self):\n         failures = []\n@@ -774,7 +805,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Spider error processing\" not in str(log)\n \n     @defer.inlineCallbacks\n-    def test_spider_errback_exception_downloader_error(self):\n+    def test_spider_errback_downloader_error_exception(self):\n         def eb(failure: Failure) -> None:\n             raise ValueError(\"foo\")\n \n@@ -786,6 +817,34 @@ class TestCrawlSpider(TestCase):\n         assert \"Error downloading\" in str(log)\n         assert \"Spider error processing\" in str(log)\n \n+    @defer.inlineCallbacks\n+    def test_spider_errback_downloader_error_item(self):\n+        def eb(failure: Failure) -> Any:\n+            return {\"foo\": \"bar\"}\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"'item_scraped_count': 1\" in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_downloader_error_request(self):\n+        def eb(failure: Failure) -> Request:\n+            return Request(self.mockserver.url(\"/\"))\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"Crawled (200)\" in str(log)\n+\n     @defer.inlineCallbacks\n     def test_raise_closespider(self):\n         def cb(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#54474ceb0d1467d90bf047415d1c1f135263d983", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 14 | Methods Changed: 23 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 82 | Churn Cumulative: 5157 | Contributors (this commit): 43 | Commits (past 90d): 21 | Contributors (cumulative): 71 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -188,6 +188,7 @@ class Scraper:\n             )\n \n         assert self.crawler.spider\n+        output: Iterable[Any] | AsyncIterator[Any]\n         if isinstance(result, Response):\n             try:\n                 # call the spider middlewares and the request callback with the response\n@@ -204,7 +205,7 @@ class Scraper:\n \n         try:\n             # call the request errback with the downloader error\n-            await self.call_spider_async(result, request)\n+            output = await self.call_spider_async(result, request)\n         except Exception as spider_exc:\n             # the errback didn't silence the exception\n             if not result.check(IgnoreRequest):\n@@ -219,6 +220,8 @@ class Scraper:\n             if spider_exc is not result.value:\n                 # the errback raised a different exception, handle it\n                 self.handle_spider_error(Failure(), request, result)\n+        else:\n+            await self.handle_spider_output_async(output, request, result)\n \n     def call_spider(\n         self, result: Response | Failure, request: Request, spider: Spider | None = None\n@@ -309,7 +312,7 @@ class Scraper:\n         self,\n         result: Iterable[_T] | AsyncIterator[_T],\n         request: Request,\n-        response: Response,\n+        response: Response | Failure,\n         spider: Spider | None = None,\n     ) -> Deferred[None]:\n         \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n@@ -327,7 +330,7 @@ class Scraper:\n         self,\n         result: Iterable[_T] | AsyncIterator[_T],\n         request: Request,\n-        response: Response,\n+        response: Response | Failure,\n     ) -> None:\n         \"\"\"Pass items/requests produced by a callback to ``_process_spidermw_output()`` in parallel.\"\"\"\n         if isinstance(result, AsyncIterator):\n@@ -352,7 +355,9 @@ class Scraper:\n         )\n \n     @deferred_f_from_coro_f\n-    async def _process_spidermw_output(self, output: Any, response: Response) -> None:\n+    async def _process_spidermw_output(\n+        self, output: Any, response: Response | Failure\n+    ) -> None:\n         \"\"\"Process each Request/Item (given in the output parameter) returned\n         from the given spider.\n \n@@ -368,7 +373,9 @@ class Scraper:\n             )\n \n     @deferred_f_from_coro_f\n-    async def start_itemproc(self, item: Any, *, response: Response | None) -> None:\n+    async def start_itemproc(\n+        self, item: Any, *, response: Response | Failure | None\n+    ) -> None:\n         \"\"\"Send *item* to the item pipelines for processing.\n \n         *response* is the source of the item data. If the item does not come\n\n@@ -116,7 +116,7 @@ class LogFormatter:\n         self,\n         item: Any,\n         exception: BaseException,\n-        response: Response | None,\n+        response: Response | Failure | None,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item is dropped while it is passing through the item pipeline.\"\"\"\n@@ -137,7 +137,7 @@ class LogFormatter:\n         self,\n         item: Any,\n         exception: BaseException,\n-        response: Response | None,\n+        response: Response | Failure | None,\n         spider: Spider,\n     ) -> LogFormatterResult:\n         \"\"\"Logs a message when an item causes an error while it is passing\n\n@@ -5,6 +5,7 @@ import logging\n import unittest\n from ipaddress import IPv4Address\n from socket import gethostbyname\n+from typing import Any\n from urllib.parse import urlparse\n \n import pytest\n@@ -419,6 +420,8 @@ with multiples lines\n \n \n class TestCrawlSpider(TestCase):\n+    mockserver: MockServer\n+\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -756,6 +759,34 @@ class TestCrawlSpider(TestCase):\n             )\n         assert \"Spider error processing\" in str(log)\n \n+    @defer.inlineCallbacks\n+    def test_spider_errback_item(self):\n+        def eb(failure: Failure) -> Any:\n+            return {\"foo\": \"bar\"}\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"'item_scraped_count': 1\" in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_request(self):\n+        def eb(failure: Failure) -> Request:\n+            return Request(self.mockserver.url(\"/\"))\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/status?n=400\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"Crawled (200)\" in str(log)\n+\n     @defer.inlineCallbacks\n     def test_spider_errback_downloader_error(self):\n         failures = []\n@@ -774,7 +805,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Spider error processing\" not in str(log)\n \n     @defer.inlineCallbacks\n-    def test_spider_errback_exception_downloader_error(self):\n+    def test_spider_errback_downloader_error_exception(self):\n         def eb(failure: Failure) -> None:\n             raise ValueError(\"foo\")\n \n@@ -786,6 +817,34 @@ class TestCrawlSpider(TestCase):\n         assert \"Error downloading\" in str(log)\n         assert \"Spider error processing\" in str(log)\n \n+    @defer.inlineCallbacks\n+    def test_spider_errback_downloader_error_item(self):\n+        def eb(failure: Failure) -> Any:\n+            return {\"foo\": \"bar\"}\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"'item_scraped_count': 1\" in str(log)\n+\n+    @defer.inlineCallbacks\n+    def test_spider_errback_downloader_error_request(self):\n+        def eb(failure: Failure) -> Request:\n+            return Request(self.mockserver.url(\"/\"))\n+\n+        crawler = get_crawler(SingleRequestSpider)\n+        with LogCapture() as log:\n+            yield crawler.crawl(\n+                seed=self.mockserver.url(\"/drop?abort=1\"), errback_func=eb\n+            )\n+        assert \"HTTP status code is not handled or not allowed\" not in str(log)\n+        assert \"Spider error processing\" not in str(log)\n+        assert \"Crawled (200)\" in str(log)\n+\n     @defer.inlineCallbacks\n     def test_raise_closespider(self):\n         def cb(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b20995c9d8dd00618ae71d0f64cdc53f6d669cf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2421 | Contributors (this commit): 30 | Commits (past 90d): 11 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -193,7 +193,7 @@ class Scraper:\n             try:\n                 # call the spider middlewares and the request callback with the response\n                 output = await maybe_deferred_to_future(\n-                    self.spidermw.scrape_response(\n+                    self.spidermw.scrape_response(  # type: ignore[arg-type]\n                         self.call_spider, result, request, self.crawler.spider\n                     )\n                 )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#405d9bc8a247cfec4d698310c425112c456f134f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 12 | Churn Cumulative: 1213 | Contributors (this commit): 31 | Commits (past 90d): 9 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -202,6 +202,11 @@ def is_asyncio_reactor_installed() -> bool:\n     \"\"\"Check whether the installed reactor is :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor`.\n \n     Raise a :exc:`RuntimeError` if no reactor is installed.\n+\n+    .. versionchanged:: 2.13\n+       In earlier Scrapy versions this function silently installed the default\n+       reactor if there was no reactor installed. Now it raises an exception to\n+       prevent silent problems in this case.\n     \"\"\"\n     if not is_reactor_installed():\n         raise RuntimeError(\n\n@@ -18,7 +18,7 @@ from twisted.trial.unittest import SkipTest\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.deprecate import create_deprecated_class\n-from scrapy.utils.reactor import is_asyncio_reactor_installed\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, is_reactor_installed\n from scrapy.utils.spider import DefaultSpider\n \n if TYPE_CHECKING:\n@@ -117,6 +117,11 @@ def get_reactor_settings() -> dict[str, Any]:\n     settings, so tests that run the crawler in the current process may need to\n     pass a correct ``\"TWISTED_REACTOR\"`` setting value when creating it.\n     \"\"\"\n+    if not is_reactor_installed():\n+        raise RuntimeError(\n+            \"get_reactor_settings() called without an installed reactor,\"\n+            \" you may need to install a reactor explicitly when running your tests.\"\n+        )\n     settings: dict[str, Any] = {}\n     if not is_asyncio_reactor_installed():\n         settings[\"TWISTED_REACTOR\"] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#657e6cb2b57d52005740e92543c1270dbaf61ded", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 15 | Churn Cumulative: 5386 | Contributors (this commit): 40 | Commits (past 90d): 13 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -174,6 +174,7 @@ class ExecutionEngine:\n             return self.close_spider(\n                 self.spider, reason=\"shutdown\"\n             )  # will also close downloader\n+        if hasattr(self, \"downloader\"):\n             self.downloader.close()\n         return succeed(None)\n \n\n@@ -22,6 +22,7 @@ from unittest.mock import Mock\n from urllib.parse import urlparse\n \n import attr\n+import pytest\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n from twisted.internet import defer\n@@ -433,6 +434,19 @@ class TestEngine(TestEngineBase):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n         yield e.close()\n \n+    def test_close_without_downloader(self):\n+        class CustomException(Exception):\n+            pass\n+\n+        class BadDownloader:\n+            def __init__(self, crawler):\n+                raise CustomException\n+\n+        with pytest.raises(CustomException):\n+            ExecutionEngine(\n+                get_crawler(MySpider, {\"DOWNLOADER\": BadDownloader}), lambda _: None\n+            )\n+\n     @defer.inlineCallbacks\n     def test_start_already_running_exception(self):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d329eedfefd9a1fa7006e6d0a214e9d5e01a8e0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 15 | Churn Cumulative: 5401 | Contributors (this commit): 40 | Commits (past 90d): 15 | Contributors (cumulative): 58 | DMM Complexity: 1.0\n\nDIFF:\n@@ -170,6 +170,7 @@ class ExecutionEngine:\n             return self.close_spider(\n                 self.spider, reason=\"shutdown\"\n             )  # will also close downloader\n+        if hasattr(self, \"downloader\"):\n             self.downloader.close()\n         return succeed(None)\n \n\n@@ -22,6 +22,7 @@ from unittest.mock import Mock\n from urllib.parse import urlparse\n \n import attr\n+import pytest\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n from twisted.internet import defer, reactor\n@@ -431,6 +432,19 @@ class TestEngine(TestEngineBase):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n         yield e.close()\n \n+    def test_close_without_downloader(self):\n+        class CustomException(Exception):\n+            pass\n+\n+        class BadDownloader:\n+            def __init__(self, crawler):\n+                raise CustomException\n+\n+        with pytest.raises(CustomException):\n+            ExecutionEngine(\n+                get_crawler(MySpider, {\"DOWNLOADER\": BadDownloader}), lambda _: None\n+            )\n+\n     @defer.inlineCallbacks\n     def test_start_already_running_exception(self):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#744edb9ba9e293ddccfcfa03e0aef0a7c0da14b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 279 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import sys\n+from argparse import Namespace  # noqa: TC003\n from typing import TYPE_CHECKING\n \n from w3lib.url import is_url\n@@ -12,7 +13,7 @@ from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n \n if TYPE_CHECKING:\n-    from argparse import ArgumentParser, Namespace\n+    from argparse import ArgumentParser\n \n     from scrapy import Spider\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8251332845d48d2418f0055c27686cee04b5b9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 368 | Lines Deleted: 397 | Files Changed: 42 | Hunks: 376 | Methods Changed: 51 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 765 | Churn Cumulative: 42697 | Contributors (this commit): 116 | Commits (past 90d): 104 | Contributors (cumulative): 523 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,4 @@\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.utils.test import get_crawler\n@@ -22,7 +22,7 @@ class TestCloseSpider(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_itemcount(self):\n         close_on = 5\n         crawler = get_crawler(ItemSpider, {\"CLOSESPIDER_ITEMCOUNT\": close_on})\n@@ -32,7 +32,7 @@ class TestCloseSpider(TestCase):\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n         assert itemcount >= close_on\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_pagecount(self):\n         close_on = 5\n         crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_PAGECOUNT\": close_on})\n@@ -42,7 +42,7 @@ class TestCloseSpider(TestCase):\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n         assert pagecount >= close_on\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_pagecount_no_item(self):\n         close_on = 5\n         max_items = 5\n@@ -62,7 +62,7 @@ class TestCloseSpider(TestCase):\n         itemcount = crawler.stats.get_value(\"item_scraped_count\")\n         assert pagecount <= close_on + itemcount\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_pagecount_no_item_with_pagecount(self):\n         close_on_pagecount_no_item = 5\n         close_on_pagecount = 20\n@@ -79,7 +79,7 @@ class TestCloseSpider(TestCase):\n         pagecount = crawler.stats.get_value(\"response_received_count\")\n         assert pagecount < close_on_pagecount\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_errorcount(self):\n         close_on = 5\n         crawler = get_crawler(ErrorSpider, {\"CLOSESPIDER_ERRORCOUNT\": close_on})\n@@ -91,7 +91,7 @@ class TestCloseSpider(TestCase):\n         assert crawler.stats.get_value(\"spider_exceptions/count\") >= close_on\n         assert errorcount >= close_on\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_timeout(self):\n         close_on = 0.1\n         crawler = get_crawler(FollowAllSpider, {\"CLOSESPIDER_TIMEOUT\": close_on})\n@@ -101,7 +101,7 @@ class TestCloseSpider(TestCase):\n         total_seconds = crawler.stats.get_value(\"elapsed_time_seconds\")\n         assert total_seconds >= close_on\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_closespider_timeout_no_item(self):\n         timeout = 1\n         crawler = get_crawler(SlowSpider, {\"CLOSESPIDER_TIMEOUT_NO_ITEM\": timeout})\n\n@@ -1,4 +1,4 @@\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n \n from tests.utils.testproc import ProcessTest\n@@ -8,17 +8,17 @@ from tests.utils.testsite import SiteTest\n class TestFetchCommand(ProcessTest, SiteTest, unittest.TestCase):\n     command = \"fetch\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_output(self):\n         _, out, _ = yield self.execute([self.url(\"/text\")])\n         assert out.strip() == b\"Works\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_redirect_default(self):\n         _, out, _ = yield self.execute([self.url(\"/redirect\")])\n         assert out.strip() == b\"Redirected here\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_redirect_disabled(self):\n         _, out, err = yield self.execute(\n             [\"--no-redirect\", self.url(\"/redirect-no-meta-refresh\")]\n@@ -27,7 +27,7 @@ class TestFetchCommand(ProcessTest, SiteTest, unittest.TestCase):\n         assert b\"downloader/response_status_count/302\" in err, err\n         assert b\"downloader/response_status_count/200\" not in err, err\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_headers(self):\n         _, out, _ = yield self.execute([self.url(\"/text\"), \"--headers\"])\n         out = out.replace(b\"\\r\", b\"\")  # required on win32\n\n@@ -3,7 +3,7 @@ import os\n import re\n from pathlib import Path\n \n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n \n from scrapy.commands import parse\n from scrapy.settings import Settings\n@@ -171,7 +171,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n \"\"\"\n             )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_arguments(self):\n         _, _, stderr = yield self.execute(\n             [\n@@ -187,7 +187,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_request_with_meta(self):\n         raw_json_string = '{\"foo\" : \"baz\"}'\n         _, _, stderr = yield self.execute(\n@@ -218,7 +218,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_request_with_cb_kwargs(self):\n         raw_json_string = '{\"foo\" : \"bar\", \"key\": \"value\"}'\n         _, _, stderr = yield self.execute(\n@@ -239,7 +239,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n             \"DEBUG: request.callback signature: (response, foo=None, key=None)\" in log\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_request_without_meta(self):\n         _, _, stderr = yield self.execute(\n             [\n@@ -253,7 +253,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"DEBUG: It Works!\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_pipelines(self):\n         _, _, stderr = yield self.execute(\n             [\n@@ -268,7 +268,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"INFO: It Works!\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n         status, out, stderr = yield self.execute(\n             [\n@@ -283,7 +283,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         assert \"{'id': 1}\" in _textmode(out)\n         assert \"{'id': 2}\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse_items_single_element(self):\n         status, out, stderr = yield self.execute(\n             [\n@@ -297,7 +297,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         assert \"INFO: Got response 200\" in _textmode(stderr)\n         assert \"{'foo': 42}\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n         status, out, stderr = yield self.execute(\n             [\n@@ -312,7 +312,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         for i in range(10):\n             assert f\"{{'foo': {i}}}\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncgen_parse_exc(self):\n         status, out, stderr = yield self.execute(\n             [\n@@ -327,7 +327,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         for i in range(7):\n             assert f\"{{'foo': {i}}}\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n         _, _, stderr = yield self.execute(\n             [\n@@ -340,21 +340,21 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"DEBUG: Got response 200\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_parse_items(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, \"-c\", \"parse\", self.url(\"/html\")]\n         )\n         assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_parse_items_no_callback_passed(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, self.url(\"/html\")]\n         )\n         assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_wrong_callback_passed(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", self.spider_name, \"-c\", \"dummy\", self.url(\"/html\")]\n@@ -362,7 +362,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n         assert \"Cannot find callback\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_matching_rule_callback_set(self):\n         \"\"\"If a rule matches the URL, use it's defined callback.\"\"\"\n         status, out, stderr = yield self.execute(\n@@ -370,7 +370,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"[{}, {'foo': 'bar'}]\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_matching_rule_default_callback(self):\n         \"\"\"If a rule match but it has no callback set, use the 'parse' callback.\"\"\"\n         status, out, stderr = yield self.execute(\n@@ -378,7 +378,7 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         )\n         assert \"[{}, {'nomatch': 'default'}]\" in _textmode(out)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_with_no_rules_attribute(self):\n         \"\"\"Using -r with a spider with no rule should not produce items.\"\"\"\n         status, out, stderr = yield self.execute(\n@@ -387,14 +387,14 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n         assert \"No CrawlSpider rules found\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_missing_callback(self):\n         status, out, stderr = yield self.execute(\n             [\"--spider\", \"badcrawl\" + self.spider_name, \"-r\", self.url(\"/html\")]\n         )\n         assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_no_matching_rule(self):\n         \"\"\"The requested URL has no matching rule, so no items should be scraped\"\"\"\n         status, out, stderr = yield self.execute(\n@@ -403,12 +403,12 @@ ITEM_PIPELINES = {{'{self.project_name}.pipelines.MyPipeline': 1}}\n         assert re.search(r\"# Scraped Items  -+\\n\\[\\]\", _textmode(out))\n         assert \"Cannot find a rule that matches\" in _textmode(stderr)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_not_exists_with_not_matched_url(self):\n         status, out, stderr = yield self.execute([self.url(\"/invalid_url\")])\n         assert status == 0\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_output_flag(self):\n         \"\"\"Checks if a file was created successfully having\n         correct format containing correct data in it.\n\n@@ -10,7 +10,6 @@ from typing import TYPE_CHECKING\n from unittest import skipIf\n \n import pytest\n-from twisted.trial import unittest\n \n from tests.test_commands import TestCommandBase\n from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n@@ -376,7 +375,7 @@ class TestWindowsRunSpiderCommand(TestRunSpiderCommand):\n \n     def setUp(self):\n         if platform.system() != \"Windows\":\n-            raise unittest.SkipTest(\"Windows required for .pyw files\")\n+            pytest.skip(\"Windows required for .pyw files\")\n         return super().setUp()\n \n     def test_start_errors(self):\n@@ -385,4 +384,4 @@ class TestWindowsRunSpiderCommand(TestRunSpiderCommand):\n         assert \"badspider.pyw\" in log\n \n     def test_runspider_unable_to_load(self):\n-        raise unittest.SkipTest(\"Already Tested in 'RunSpiderCommandTest' \")\n+        pytest.skip(\"Already Tested in 'RunSpiderCommandTest' \")\n\n@@ -3,8 +3,9 @@ import sys\n from io import BytesIO\n from pathlib import Path\n \n+import pytest\n from pexpect.popen_spawn import PopenSpawn\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n \n from scrapy.utils.reactor import _asyncio_reactor_path\n@@ -17,52 +18,52 @@ from tests.utils.testsite import SiteTest\n class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n     command = \"shell\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_empty(self):\n         _, out, _ = yield self.execute([\"-c\", \"item\"])\n         assert b\"{}\" in out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_body(self):\n         _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"response.body\"])\n         assert b\"Works\" in out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_type_text(self):\n         _, out, _ = yield self.execute([self.url(\"/text\"), \"-c\", \"type(response)\"])\n         assert b\"TextResponse\" in out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_type_html(self):\n         _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", \"type(response)\"])\n         assert b\"HtmlResponse\" in out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_selector_html(self):\n         xpath = \"response.xpath(\\\"//p[@class='one']/text()\\\").get()\"\n         _, out, _ = yield self.execute([self.url(\"/html\"), \"-c\", xpath])\n         assert out.strip() == b\"Works\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_encoding_gb18030(self):\n         _, out, _ = yield self.execute(\n             [self.url(\"/enc-gb18030\"), \"-c\", \"response.encoding\"]\n         )\n         assert out.strip() == b\"gb18030\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_redirect(self):\n         _, out, _ = yield self.execute([self.url(\"/redirect\"), \"-c\", \"response.url\"])\n         assert out.strip().endswith(b\"/redirected\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_redirect_follow_302(self):\n         _, out, _ = yield self.execute(\n             [self.url(\"/redirect-no-meta-refresh\"), \"-c\", \"response.status\"]\n         )\n         assert out.strip().endswith(b\"200\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_redirect_not_follow_302(self):\n         _, out, _ = yield self.execute(\n             [\n@@ -74,7 +75,7 @@ class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n         )\n         assert out.strip().endswith(b\"302\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_fetch_redirect_follow_302(self):\n         \"\"\"Test that calling ``fetch(url)`` follows HTTP redirects by default.\"\"\"\n         url = self.url(\"/redirect-no-meta-refresh\")\n@@ -84,7 +85,7 @@ class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n         assert b\"Redirecting (302)\" in errout\n         assert b\"Crawled (200)\" in errout\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_fetch_redirect_not_follow_302(self):\n         \"\"\"Test that calling ``fetch(url, redirect=False)`` disables automatic redirects.\"\"\"\n         url = self.url(\"/redirect-no-meta-refresh\")\n@@ -93,27 +94,27 @@ class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n         assert errcode == 0, out\n         assert b\"Crawled (302)\" in errout\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_request_replace(self):\n         url = self.url(\"/text\")\n         code = f\"fetch('{url}') or fetch(response.request.replace(method='POST'))\"\n         errcode, out, _ = yield self.execute([\"-c\", code])\n         assert errcode == 0, out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_scrapy_import(self):\n         url = self.url(\"/text\")\n         code = f\"fetch(scrapy.Request('{url}'))\"\n         errcode, out, _ = yield self.execute([\"-c\", code])\n         assert errcode == 0, out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_local_file(self):\n         filepath = Path(tests_datadir, \"test_site\", \"index.html\")\n         _, out, _ = yield self.execute([str(filepath), \"-c\", \"item\"])\n         assert b\"{}\" in out\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_local_nofile(self):\n         filepath = \"file:///tests/sample_data/test_site/nothinghere.html\"\n         errcode, out, err = yield self.execute(\n@@ -122,16 +123,16 @@ class TestShellCommand(ProcessTest, SiteTest, unittest.TestCase):\n         assert errcode == 1, out or err\n         assert b\"No such file or directory\" in err\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_dns_failures(self):\n         if NON_EXISTING_RESOLVABLE:\n-            raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n+            pytest.skip(\"Non-existing hosts are resolvable\")\n         url = \"www.somedomainthatdoesntexi.st\"\n         errcode, out, err = yield self.execute([url, \"-c\", \"item\"], check_code=False)\n         assert errcode == 1, out or err\n         assert b\"DNS lookup failed\" in err\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_shell_fetch_async(self):\n         url = self.url(\"/html\")\n         code = f\"fetch('{url}')\"\n\n@@ -1,6 +1,6 @@\n import sys\n \n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n \n import scrapy\n@@ -10,13 +10,13 @@ from tests.utils.testproc import ProcessTest\n class TestVersionCommand(ProcessTest, unittest.TestCase):\n     command = \"version\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_output(self):\n         encoding = sys.stdout.encoding or \"utf-8\"\n         _, out, _ = yield self.execute([])\n         assert out.strip().decode(encoding) == f\"Scrapy {scrapy.__version__}\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_verbose_output(self):\n         encoding = sys.stdout.encoding or \"utf-8\"\n         _, out, _ = yield self.execute([\"-v\"])\n\n@@ -1,7 +1,7 @@\n from unittest import TextTestResult\n \n import pytest\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.python import failure\n from twisted.trial import unittest\n \n@@ -502,7 +502,7 @@ class TestContractsManager(unittest.TestCase):\n         assert not self.results.failures\n         assert self.results.errors\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_same_url(self):\n         class TestSameUrlSpider(Spider):\n             name = \"test_same_url\"\n\n@@ -2,7 +2,6 @@ from __future__ import annotations\n \n import json\n import logging\n-import unittest\n from ipaddress import IPv4Address\n from socket import gethostbyname\n from typing import Any\n@@ -10,7 +9,7 @@ from urllib.parse import urlparse\n \n import pytest\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.internet.ssl import Certificate\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n@@ -67,21 +66,21 @@ class TestCrawl(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_follow_all(self):\n         crawler = get_crawler(FollowAllSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert len(crawler.spider.urls_visited) == 11  # 10 + start_url\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_fixed_delay(self):\n         yield self._test_delay(total=3, delay=0.2)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_randomized_delay(self):\n         yield self._test_delay(total=3, delay=0.1, randomize=True)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _test_delay(self, total, delay, randomize=False):\n         crawl_kwargs = {\n             \"maxlatency\": delay * 2,\n@@ -110,7 +109,7 @@ class TestCrawl(TestCase):\n         average = total_time / (len(times) - 1)\n         assert average <= delay / tolerance, \"test total or delay values are too small\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_timeout_success(self):\n         crawler = get_crawler(DelaySpider)\n         yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n@@ -118,7 +117,7 @@ class TestCrawl(TestCase):\n         assert crawler.spider.t2 > 0\n         assert crawler.spider.t2 > crawler.spider.t1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_timeout_failure(self):\n         crawler = get_crawler(DelaySpider, {\"DOWNLOAD_TIMEOUT\": 0.35})\n         yield crawler.crawl(n=0.5, mockserver=self.mockserver)\n@@ -135,7 +134,7 @@ class TestCrawl(TestCase):\n         assert crawler.spider.t2_err > 0\n         assert crawler.spider.t2_err > crawler.spider.t1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_retry_503(self):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n@@ -144,7 +143,7 @@ class TestCrawl(TestCase):\n             )\n         self._assert_retried(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_retry_conn_failed(self):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n@@ -153,10 +152,10 @@ class TestCrawl(TestCase):\n             )\n         self._assert_retried(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_retry_dns_error(self):\n         if NON_EXISTING_RESOLVABLE:\n-            raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n+            pytest.skip(\"Non-existing hosts are resolvable\")\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             # try to fetch the homepage of a nonexistent domain\n@@ -165,7 +164,7 @@ class TestCrawl(TestCase):\n             )\n         self._assert_retried(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_bug_before_yield(self):\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(BrokenStartSpider)\n@@ -176,7 +175,7 @@ class TestCrawl(TestCase):\n         assert record.exc_info is not None\n         assert record.exc_info[0] is ZeroDivisionError\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_bug_yielding(self):\n         with LogCapture(\"scrapy\", level=logging.ERROR) as log:\n             crawler = get_crawler(BrokenStartSpider)\n@@ -187,7 +186,7 @@ class TestCrawl(TestCase):\n         assert record.exc_info is not None\n         assert record.exc_info[0] is ZeroDivisionError\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_items(self):\n         items = []\n \n@@ -202,7 +201,7 @@ class TestCrawl(TestCase):\n         assert len(log.records) == 0\n         assert items == [{\"name\": \"test item\"}]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_unsupported_output(self):\n         \"\"\"Anything that is not a request is assumed to be an item, avoiding a\n         potentially expensive call to itemadapter.is_item(), and letting\n@@ -223,7 +222,7 @@ class TestCrawl(TestCase):\n         assert len(items) == 3\n         assert not any(isinstance(item, Request) for item in items)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_dupes(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = get_crawler(DuplicateStartSpider, settings)\n@@ -241,7 +240,7 @@ class TestCrawl(TestCase):\n         )\n         assert crawler.spider.visited == 3\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_unbounded_response(self):\n         # Completeness of responses without Content-Length or Transfer-Encoding\n         # can not be determined, we treat them as valid but flagged as \"partial\"\n@@ -275,7 +274,7 @@ with multiples lines\n             )\n         assert str(log).count(\"Got response 200\") == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_retry_conn_lost(self):\n         # connection lost after receiving data\n         crawler = get_crawler(SimpleSpider)\n@@ -285,7 +284,7 @@ with multiples lines\n             )\n         self._assert_retried(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_retry_conn_aborted(self):\n         # connection lost before receiving data\n         crawler = get_crawler(SimpleSpider)\n@@ -299,7 +298,7 @@ with multiples lines\n         assert str(log).count(\"Retrying\") == 2\n         assert str(log).count(\"Gave up retrying\") == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_referer_header(self):\n         \"\"\"Referer header is set by RefererMiddleware unless it is already set\"\"\"\n         req0 = Request(self.mockserver.url(\"/echo?headers=1&body=0\"), dont_filter=1)\n@@ -327,7 +326,7 @@ with multiples lines\n         echo3 = json.loads(to_unicode(crawler.spider.meta[\"responses\"][3].body))\n         assert echo3[\"headers\"].get(\"Referer\") == [\"http://example.com\"]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_engine_status(self):\n         from scrapy.utils.engine import get_engine_status\n \n@@ -345,7 +344,7 @@ with multiples lines\n         assert s[\"engine.spider.name\"] == crawler.spider.name\n         assert s[\"len(engine.scraper.slot.active)\"] == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_format_engine_status(self):\n         from scrapy.utils.engine import format_engine_status\n \n@@ -370,7 +369,7 @@ with multiples lines\n         assert s[\"engine.spider.name\"] == crawler.spider.name\n         assert s[\"len(engine.scraper.slot.active)\"] == \"1\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_open_spider_error_on_faulty_pipeline(self):\n         settings = {\n             \"ITEM_PIPELINES\": {\n@@ -378,15 +377,13 @@ with multiples lines\n             }\n         }\n         crawler = get_crawler(SimpleSpider, settings)\n-        yield self.assertFailure(\n-            crawler.crawl(\n+        with pytest.raises(ZeroDivisionError):\n+            yield crawler.crawl(\n                 self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver\n-            ),\n-            ZeroDivisionError,\n             )\n         assert not crawler.crawling\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlerrunner_accepts_crawler(self):\n         crawler = get_crawler(SimpleSpider)\n         runner = CrawlerRunner()\n@@ -398,7 +395,7 @@ with multiples lines\n             )\n         assert \"Got response 200\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawl_multiple(self):\n         runner = CrawlerRunner(get_reactor_settings())\n         runner.crawl(\n@@ -431,7 +428,7 @@ class TestCrawlSpider(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _run_spider(self, spider_cls):\n         items = []\n \n@@ -446,7 +443,7 @@ class TestCrawlSpider(TestCase):\n             )\n         return log, items, crawler.stats\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_with_parse(self):\n         crawler = get_crawler(CrawlSpiderWithParseMethod)\n         with LogCapture() as log:\n@@ -456,7 +453,7 @@ class TestCrawlSpider(TestCase):\n         assert \"[parse] status 201 (foo: None)\" in str(log)\n         assert \"[parse] status 202 (foo: bar)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_with_async_callback(self):\n         crawler = get_crawler(CrawlSpiderWithAsyncCallback)\n         with LogCapture() as log:\n@@ -466,7 +463,7 @@ class TestCrawlSpider(TestCase):\n         assert \"[parse_async] status 201 (foo: None)\" in str(log)\n         assert \"[parse_async] status 202 (foo: bar)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_with_async_generator_callback(self):\n         crawler = get_crawler(CrawlSpiderWithAsyncGeneratorCallback)\n         with LogCapture() as log:\n@@ -476,7 +473,7 @@ class TestCrawlSpider(TestCase):\n         assert \"[parse_async_gen] status 201 (foo: None)\" in str(log)\n         assert \"[parse_async_gen] status 202 (foo: bar)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_with_errback(self):\n         crawler = get_crawler(CrawlSpiderWithErrback)\n         with LogCapture() as log:\n@@ -489,7 +486,7 @@ class TestCrawlSpider(TestCase):\n         assert \"[errback] status 500\" in str(log)\n         assert \"[errback] status 501\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawlspider_process_request_cb_kwargs(self):\n         crawler = get_crawler(CrawlSpiderWithProcessRequestCallbackKeywordArguments)\n         with LogCapture() as log:\n@@ -499,7 +496,7 @@ class TestCrawlSpider(TestCase):\n         assert \"[parse] status 201 (foo: process_request)\" in str(log)\n         assert \"[parse] status 202 (foo: bar)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_parse(self):\n         crawler = get_crawler(AsyncDefSpider)\n         with LogCapture() as log:\n@@ -509,7 +506,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Got response 200\" in str(log)\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse(self):\n         crawler = get_crawler(\n             AsyncDefAsyncioSpider,\n@@ -524,7 +521,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Got response 200\" in str(log)\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n         log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)\n         assert \"Got response 200\" in str(log)\n@@ -532,7 +529,7 @@ class TestCrawlSpider(TestCase):\n         assert {\"id\": 2} in items\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse_items_single_element(self):\n         items = []\n \n@@ -549,7 +546,7 @@ class TestCrawlSpider(TestCase):\n         assert {\"foo\": 42} in items\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncgen_parse(self):\n         log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)\n         assert \"Got response 200\" in str(log)\n@@ -557,7 +554,7 @@ class TestCrawlSpider(TestCase):\n         assert itemcount == 1\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)\n         assert \"Got response 200\" in str(log)\n@@ -567,7 +564,7 @@ class TestCrawlSpider(TestCase):\n             assert {\"foo\": i} in items\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncgen_parse_exc(self):\n         log, items, stats = yield self._run_spider(AsyncDefAsyncioGenExcSpider)\n         log = str(log)\n@@ -579,7 +576,7 @@ class TestCrawlSpider(TestCase):\n             assert {\"foo\": i} in items\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n         _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)\n         itemcount = stats.get_value(\"item_scraped_count\")\n@@ -591,37 +588,37 @@ class TestCrawlSpider(TestCase):\n             assert {\"index2\": i} in items\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_asyncio_parse_reqs_list(self):\n         log, *_ = yield self._run_spider(AsyncDefAsyncioReqsReturnSpider)\n         for req_id in range(3):\n             assert f\"Got response 200, req_id {req_id}\" in str(log)\n \n     @pytest.mark.only_not_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_deferred_direct(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredDirectSpider)\n         assert items == [{\"code\": 200}]\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_deferred_wrapped(self):\n         log, items, _ = yield self._run_spider(AsyncDefDeferredWrappedSpider)\n         assert items == [{\"code\": 200}]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_def_deferred_maybe_wrapped(self):\n         _, items, _ = yield self._run_spider(AsyncDefDeferredMaybeWrappedSpider)\n         assert items == [{\"code\": 200}]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_ssl_certificate_none(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=False)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         assert crawler.spider.meta[\"responses\"][0].certificate is None\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_ssl_certificate(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\", is_secure=True)\n@@ -634,7 +631,7 @@ class TestCrawlSpider(TestCase):\n     @pytest.mark.xfail(\n         reason=\"Responses with no body return early and contain no certificate\"\n     )\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_ssl_certificate_empty_response(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/status?n=200\", is_secure=True)\n@@ -644,7 +641,7 @@ class TestCrawlSpider(TestCase):\n         assert cert.getSubject().commonName == b\"localhost\"\n         assert cert.getIssuer().commonName == b\"localhost\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_dns_server_ip_address_none(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/status?n=200\")\n@@ -652,7 +649,7 @@ class TestCrawlSpider(TestCase):\n         ip_address = crawler.spider.meta[\"responses\"][0].ip_address\n         assert ip_address is None\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_dns_server_ip_address(self):\n         crawler = get_crawler(SingleRequestSpider)\n         url = self.mockserver.url(\"/echo?body=test\")\n@@ -662,7 +659,7 @@ class TestCrawlSpider(TestCase):\n         assert isinstance(ip_address, IPv4Address)\n         assert str(ip_address) == gethostbyname(expected_netloc)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_bytes_received_stop_download_callback(self):\n         crawler = get_crawler(BytesReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n@@ -676,7 +673,7 @@ class TestCrawlSpider(TestCase):\n             < crawler.spider.full_response_length\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_bytes_received_stop_download_errback(self):\n         crawler = get_crawler(BytesReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n@@ -692,7 +689,7 @@ class TestCrawlSpider(TestCase):\n             < crawler.spider.full_response_length\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_headers_received_stop_download_callback(self):\n         crawler = get_crawler(HeadersReceivedCallbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n@@ -702,7 +699,7 @@ class TestCrawlSpider(TestCase):\n             \"headers_received\"\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_headers_received_stop_download_errback(self):\n         crawler = get_crawler(HeadersReceivedErrbackSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n@@ -714,7 +711,7 @@ class TestCrawlSpider(TestCase):\n             \"failure\"\n         ].value.response.headers == crawler.spider.meta.get(\"headers_received\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback(self):\n         failures = []\n \n@@ -731,7 +728,7 @@ class TestCrawlSpider(TestCase):\n         assert \"HTTP status code is not handled or not allowed\" in str(log)\n         assert \"Spider error processing\" not in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_silence(self):\n         failures = []\n \n@@ -747,7 +744,7 @@ class TestCrawlSpider(TestCase):\n         assert \"HTTP status code is not handled or not allowed\" not in str(log)\n         assert \"Spider error processing\" not in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_exception(self):\n         def eb(failure: Failure) -> None:\n             raise ValueError(\"foo\")\n@@ -759,7 +756,7 @@ class TestCrawlSpider(TestCase):\n             )\n         assert \"Spider error processing\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_item(self):\n         def eb(failure: Failure) -> Any:\n             return {\"foo\": \"bar\"}\n@@ -773,7 +770,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Spider error processing\" not in str(log)\n         assert \"'item_scraped_count': 1\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_request(self):\n         def eb(failure: Failure) -> Request:\n             return Request(self.mockserver.url(\"/\"))\n@@ -787,7 +784,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Spider error processing\" not in str(log)\n         assert \"Crawled (200)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_downloader_error(self):\n         failures = []\n \n@@ -804,7 +801,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Error downloading\" in str(log)\n         assert \"Spider error processing\" not in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_downloader_error_exception(self):\n         def eb(failure: Failure) -> None:\n             raise ValueError(\"foo\")\n@@ -817,7 +814,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Error downloading\" in str(log)\n         assert \"Spider error processing\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_downloader_error_item(self):\n         def eb(failure: Failure) -> Any:\n             return {\"foo\": \"bar\"}\n@@ -831,7 +828,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Spider error processing\" not in str(log)\n         assert \"'item_scraped_count': 1\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_spider_errback_downloader_error_request(self):\n         def eb(failure: Failure) -> Request:\n             return Request(self.mockserver.url(\"/\"))\n@@ -845,7 +842,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Spider error processing\" not in str(log)\n         assert \"Crawled (200)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_raise_closespider(self):\n         def cb(response):\n             raise CloseSpider\n@@ -856,7 +853,7 @@ class TestCrawlSpider(TestCase):\n         assert \"Closing spider (cancelled)\" in str(log)\n         assert \"Spider error processing\" not in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_raise_closespider_reason(self):\n         def cb(response):\n             raise CloseSpider(\"my_reason\")\n\n@@ -1,25 +1,13 @@\n import os\n import re\n from configparser import ConfigParser\n-from importlib import import_module\n from pathlib import Path\n \n import pytest\n from twisted import version as twisted_version\n-from twisted.trial import unittest\n \n \n class TestScrapyUtils:\n-    def test_required_openssl_version(self):\n-        try:\n-            module = import_module(\"OpenSSL\")\n-        except ImportError:\n-            raise unittest.SkipTest(\"OpenSSL is not available\")\n-\n-        if hasattr(module, \"__version__\"):\n-            installed_version = [int(x) for x in module.__version__.split(\".\")[:2]]\n-            assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n-\n     def test_pinned_twisted_version(self):\n         \"\"\"When running tests within a Tox environment with pinned\n         dependencies, make sure that the version of Twisted is the pinned\n\n@@ -440,7 +440,7 @@ class TestFTPBase(unittest.TestCase):\n class TestFTP(TestFTPBase):\n     def test_invalid_credentials(self):\n         if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n-            raise unittest.SkipTest(\n+            pytest.skip(\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n             )\n         from twisted.protocols.ftp import ConnectionLost\n\n@@ -14,7 +14,7 @@ from unittest import mock\n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer, error\n-from twisted.internet.defer import maybeDeferred\n+from twisted.internet.defer import inlineCallbacks, maybeDeferred\n from twisted.protocols.policies import WrappingFactory\n from twisted.trial import unittest\n from twisted.web import resource, server, static, util\n@@ -186,7 +186,7 @@ class TestHttpBase(unittest.TestCase, ABC):\n             self.download_handler_cls, get_crawler()\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def tearDown(self):\n         yield self.port.stopListening()\n         if hasattr(self.download_handler, \"close\"):\n@@ -229,7 +229,7 @@ class TestHttpBase(unittest.TestCase, ABC):\n     async def test_timeout_download_from_spider_nodata_rcvd(self):\n         if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             # https://twistedmatrix.com/trac/ticket/10279\n-            raise unittest.SkipTest(\n+            pytest.skip(\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n             )\n \n@@ -245,7 +245,7 @@ class TestHttpBase(unittest.TestCase, ABC):\n     async def test_timeout_download_from_spider_server_hangs(self):\n         if self.reactor_pytest != \"default\" and sys.platform == \"win32\":\n             # https://twistedmatrix.com/trac/ticket/10279\n-            raise unittest.SkipTest(\n+            pytest.skip(\n                 \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n             )\n         # client connects, server send headers and some body bytes but hangs\n@@ -531,7 +531,7 @@ class TestSimpleHttpsBase(unittest.TestCase, ABC):\n         crawler = get_crawler(settings_dict=settings_dict)\n         self.download_handler = build_from_crawler(self.download_handler_cls, crawler)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def tearDown(self):\n         yield self.port.stopListening()\n         if hasattr(self.download_handler, \"close\"):\n@@ -665,7 +665,7 @@ class TestHttpProxyBase(unittest.TestCase, ABC):\n             self.download_handler_cls, get_crawler()\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def tearDown(self):\n         yield self.port.stopListening()\n         if hasattr(self.download_handler, \"close\"):\n\n@@ -2,7 +2,6 @@ from gzip import GzipFile\n from io import BytesIO\n from logging import WARNING\n from pathlib import Path\n-from unittest import SkipTest\n \n import pytest\n from testfixtures import LogCapture\n@@ -130,7 +129,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         response = self._getresponse(\"br\")\n         request = response.request\n         assert response.headers[\"Content-Encoding\"] == b\"br\"\n@@ -146,11 +145,11 @@ class TestHttpCompression:\n             try:\n                 import brotli  # noqa: F401\n \n-                raise SkipTest(\"Requires not having brotli support\")\n+                pytest.skip(\"Requires not having brotli support\")\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n \n-                raise SkipTest(\"Requires not having brotli support\")\n+                pytest.skip(\"Requires not having brotli support\")\n         except ImportError:\n             pass\n         response = self._getresponse(\"br\")\n@@ -180,7 +179,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         raw_content = None\n         for check_key in FORMAT:\n             if not check_key.startswith(\"zstd-\"):\n@@ -201,7 +200,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n \n-            raise SkipTest(\"Requires not having zstandard support\")\n+            pytest.skip(\"Requires not having zstandard support\")\n         except ImportError:\n             pass\n         response = self._getresponse(\"zstd-static-content-size\")\n@@ -520,7 +519,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         self._test_compression_bomb_setting(\"br\")\n \n     def test_compression_bomb_setting_deflate(self):\n@@ -533,7 +532,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         self._test_compression_bomb_setting(\"zstd\")\n \n     def _test_compression_bomb_spider_attr(self, compression_id):\n@@ -556,7 +555,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         self._test_compression_bomb_spider_attr(\"br\")\n \n     def test_compression_bomb_spider_attr_deflate(self):\n@@ -569,7 +568,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         self._test_compression_bomb_spider_attr(\"zstd\")\n \n     def _test_compression_bomb_request_meta(self, compression_id):\n@@ -590,7 +589,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         self._test_compression_bomb_request_meta(\"br\")\n \n     def test_compression_bomb_request_meta_deflate(self):\n@@ -603,7 +602,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         self._test_compression_bomb_request_meta(\"zstd\")\n \n     def _test_download_warnsize_setting(self, compression_id):\n@@ -639,7 +638,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         self._test_download_warnsize_setting(\"br\")\n \n     def test_download_warnsize_setting_deflate(self):\n@@ -652,7 +651,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_setting(\"zstd\")\n \n     def _test_download_warnsize_spider_attr(self, compression_id):\n@@ -690,7 +689,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         self._test_download_warnsize_spider_attr(\"br\")\n \n     def test_download_warnsize_spider_attr_deflate(self):\n@@ -703,7 +702,7 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_spider_attr(\"zstd\")\n \n     def _test_download_warnsize_request_meta(self, compression_id):\n@@ -739,7 +738,7 @@ class TestHttpCompression:\n             except ImportError:\n                 import brotlicffi  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no brotli\")\n+            pytest.skip(\"no brotli\")\n         self._test_download_warnsize_request_meta(\"br\")\n \n     def test_download_warnsize_request_meta_deflate(self):\n@@ -752,5 +751,5 @@ class TestHttpCompression:\n         try:\n             import zstandard  # noqa: F401\n         except ImportError:\n-            raise SkipTest(\"no zstd support (zstandard)\")\n+            pytest.skip(\"no zstd support (zstandard)\")\n         self._test_download_warnsize_request_meta(\"zstd\")\n\n@@ -235,11 +235,9 @@ Disallow: /some/randome/page.html\n         self, request: Request, middleware: RobotsTxtMiddleware\n     ) -> None:\n         spider = None  # not actually used\n+        with pytest.raises(IgnoreRequest):\n             await maybe_deferred_to_future(\n-            self.assertFailure(\n-                middleware.process_request(request, spider),  # type: ignore[arg-type]\n-                IgnoreRequest,\n-            )\n+                maybeDeferred(middleware.process_request, request, spider)  # type: ignore[call-overload]\n             )\n \n     def assertRobotsTxtRequested(self, base_url: str) -> None:\n\n@@ -1,6 +1,6 @@\n import time\n \n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy import Request\n@@ -62,7 +62,7 @@ class CrawlTestCase(TestCase):\n     def setUp(self):\n         self.runner = CrawlerRunner()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_delay(self):\n         crawler = get_crawler(DownloaderSlotsSettingsTestSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n\n@@ -26,6 +26,7 @@ import pytest\n from itemadapter import ItemAdapter\n from pydispatch import dispatcher\n from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n from twisted.web import server, static, util\n \n@@ -390,7 +391,7 @@ class TestEngineBase(unittest.TestCase):\n \n \n class TestEngine(TestEngineBase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n             MySpider,\n@@ -407,20 +408,20 @@ class TestEngine(TestEngineBase):\n             self._assert_signals_caught(run)\n             self._assert_bytes_received(run)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_dupefilter(self):\n         run = CrawlerRun(DupeFilterSpider)\n         yield run.run()\n         self._assert_scheduled_requests(run, count=8)\n         self._assert_dropped_requests(run)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_itemerror(self):\n         run = CrawlerRun(ItemZeroDivisionErrorSpider)\n         yield run.run()\n         self._assert_items_error(run)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler_change_close_reason_on_idle(self):\n         run = CrawlerRun(ChangeCloseReasonSpider)\n         yield run.run()\n@@ -429,7 +430,7 @@ class TestEngine(TestEngineBase):\n             \"reason\": \"custom_reason\",\n         } == run.signals_caught[signals.spider_closed]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_close_downloader(self):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n         yield e.close()\n@@ -447,18 +448,13 @@ class TestEngine(TestEngineBase):\n                 get_crawler(MySpider, {\"DOWNLOADER\": BadDownloader}), lambda _: None\n             )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_already_running_exception(self):\n         e = ExecutionEngine(get_crawler(MySpider), lambda _: None)\n         yield e.open_spider(MySpider(), [])\n         e.start()\n-\n-        def cb(exc: BaseException) -> None:\n-            assert str(exc), \"Engine already running\"\n-\n-        try:\n-            yield self.assertFailure(e.start(), RuntimeError).addBoth(cb)\n-        finally:\n+        with pytest.raises(RuntimeError, match=\"Engine already running\"):\n+            yield e.start()\n         yield e.stop()\n \n     def test_short_timeout(self):\n\n@@ -1,5 +1,5 @@\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n \n from scrapy.exceptions import StopDownload\n from tests.test_engine import (\n@@ -19,7 +19,7 @@ class BytesReceivedCrawlerRun(CrawlerRun):\n \n \n class TestBytesReceivedEngine(TestEngineBase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n             MySpider,\n\n@@ -1,5 +1,5 @@\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n \n from scrapy.exceptions import StopDownload\n from tests.test_engine import (\n@@ -19,7 +19,7 @@ class HeadersReceivedCrawlerRun(CrawlerRun):\n \n \n class TestHeadersReceivedEngine(TestEngineBase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawler(self):\n         for spider in (\n             MySpider,\n\n@@ -4,7 +4,6 @@ import marshal\n import pickle\n import re\n import tempfile\n-import unittest\n from datetime import datetime\n from io import BytesIO\n from typing import Any\n@@ -662,7 +661,7 @@ class TestCustomExporterItem:\n \n     def setup_method(self):\n         if self.item_class is None:\n-            raise unittest.SkipTest(\"item class is None\")\n+            pytest.skip(\"item class is None\")\n \n     def test_exporter_custom_serializer(self):\n         class CustomItemExporter(BaseItemExporter):\n\n@@ -1,7 +1,6 @@\n from __future__ import annotations\n \n import datetime\n-import unittest\n from typing import Any, Callable\n \n from scrapy.extensions.periodic_log import PeriodicLog\n@@ -66,7 +65,7 @@ def extension(settings: dict[str, Any] | None = None) -> CustomPeriodicLog:\n     return CustomPeriodicLog.from_crawler(crawler)\n \n \n-class TestPeriodicLog(unittest.TestCase):\n+class TestPeriodicLog:\n     def test_extension_enabled(self):\n         # Expected that settings for this extension loaded successfully\n         # And on certain conditions - extension raising NotConfigured\n\n@@ -1,6 +1,7 @@\n+import pytest\n from twisted.conch.telnet import ITelnetProtocol\n from twisted.cred import credentials\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n \n from scrapy.extensions.telnet import TelnetConsole\n@@ -21,15 +22,16 @@ class TelnetExtensionTest(unittest.TestCase):\n \n         return console, portal\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_bad_credentials(self):\n         console, portal = self._get_console_and_portal()\n         creds = credentials.UsernamePassword(b\"username\", b\"password\")\n         d = portal.login(creds, None, ITelnetProtocol)\n-        yield self.assertFailure(d, ValueError)\n+        with pytest.raises(ValueError, match=\"Invalid credentials\"):\n+            yield d\n         console.stop_listening()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_good_credentials(self):\n         console, portal = self._get_console_and_portal()\n         creds = credentials.UsernamePassword(\n@@ -39,7 +41,7 @@ class TelnetExtensionTest(unittest.TestCase):\n         yield d\n         console.stop_listening()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_custom_credentials(self):\n         settings = {\n             \"TELNETCONSOLE_USERNAME\": \"user\",\n\n@@ -26,6 +26,7 @@ import lxml.etree\n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n from w3lib.url import file_uri_to_path, path_to_file_uri\n from zope.interface import implementer\n@@ -131,7 +132,7 @@ class TestFileFeedStorage(unittest.TestCase):\n             FileFeedStorage(str(path), feed_options={\"overwrite\": True}), path\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _assert_stores(self, storage, path: Path, expected_content=b\"content\"):\n         spider = scrapy.Spider(\"default\")\n         file = storage.open(spider)\n@@ -172,7 +173,7 @@ class TestFTPFeedStorage(unittest.TestCase):\n         finally:\n             path.unlink()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_append(self):\n         with MockFTPServer() as ftp_server:\n             filename = \"file\"\n@@ -182,7 +183,7 @@ class TestFTPFeedStorage(unittest.TestCase):\n             yield self._store(url, b\"bar\", feed_options=feed_options)\n             self._assert_stored(ftp_server.path / filename, b\"foobar\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_overwrite(self):\n         with MockFTPServer() as ftp_server:\n             filename = \"file\"\n@@ -191,7 +192,7 @@ class TestFTPFeedStorage(unittest.TestCase):\n             yield self._store(url, b\"bar\")\n             self._assert_stored(ftp_server.path / filename, b\"bar\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_append_active_mode(self):\n         with MockFTPServer() as ftp_server:\n             settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n@@ -202,7 +203,7 @@ class TestFTPFeedStorage(unittest.TestCase):\n             yield self._store(url, b\"bar\", feed_options=feed_options, settings=settings)\n             self._assert_stored(ftp_server.path / filename, b\"foobar\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_overwrite_active_mode(self):\n         with MockFTPServer() as ftp_server:\n             settings = {\"FEED_STORAGE_FTP_ACTIVE\": True}\n@@ -290,7 +291,7 @@ class TestS3FeedStorage(unittest.TestCase):\n         assert storage.access_key == \"uri_key\"\n         assert storage.secret_key == \"uri_secret\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_store(self):\n         settings = {\n             \"AWS_ACCESS_KEY_ID\": \"access_key\",\n@@ -431,7 +432,7 @@ class TestS3FeedStorage(unittest.TestCase):\n         assert storage.region_name == region_name\n         assert storage.s3_client._client_config.region_name == region_name\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_store_without_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\",\n@@ -451,7 +452,7 @@ class TestS3FeedStorage(unittest.TestCase):\n         )\n         assert acl is None\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_store_with_acl(self):\n         storage = S3FeedStorage(\n             \"s3://mybucket/export.csv\", \"access_key\", \"secret_key\", \"custom-acl\"\n@@ -489,7 +490,7 @@ class TestGCSFeedStorage(unittest.TestCase):\n         try:\n             from google.cloud.storage import Client  # noqa: F401\n         except ImportError:\n-            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n+            pytest.skip(\"GCSFeedStorage requires google-cloud-storage\")\n \n         settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"publicRead\"}\n         crawler = get_crawler(settings_dict=settings)\n@@ -503,7 +504,7 @@ class TestGCSFeedStorage(unittest.TestCase):\n         try:\n             from google.cloud.storage import Client  # noqa: F401\n         except ImportError:\n-            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n+            pytest.skip(\"GCSFeedStorage requires google-cloud-storage\")\n \n         settings = {\"GCS_PROJECT_ID\": \"123\", \"FEED_STORAGE_GCS_ACL\": \"\"}\n         crawler = get_crawler(settings_dict=settings)\n@@ -515,12 +516,12 @@ class TestGCSFeedStorage(unittest.TestCase):\n         storage = GCSFeedStorage.from_crawler(crawler, \"gs://mybucket/export.csv\")\n         assert storage.acl is None\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_store(self):\n         try:\n             from google.cloud.storage import Client  # noqa: F401\n         except ImportError:\n-            raise unittest.SkipTest(\"GCSFeedStorage requires google-cloud-storage\")\n+            pytest.skip(\"GCSFeedStorage requires google-cloud-storage\")\n \n         uri = \"gs://mybucket/export.csv\"\n         project_id = \"myproject-123\"\n@@ -556,7 +557,7 @@ class TestGCSFeedStorage(unittest.TestCase):\n \n \n class TestStdoutFeedStorage(unittest.TestCase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_store(self):\n         out = BytesIO()\n         storage = StdoutFeedStorage(\"stdout:\", _stdout=out)\n@@ -669,7 +670,7 @@ class TestFeedExportBase(ABC, unittest.TestCase):\n     def tearDown(self):\n         shutil.rmtree(self.temp_dir, ignore_errors=True)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def exported_data(self, items, settings):\n         \"\"\"\n         Return exported data which a spider yielding ``items`` would return.\n@@ -684,7 +685,7 @@ class TestFeedExportBase(ABC, unittest.TestCase):\n         data = yield self.run_and_export(TestSpider, settings)\n         return data\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def exported_no_data(self, settings):\n         \"\"\"\n         Return exported data which a spider yielding no ``items`` would return.\n@@ -699,7 +700,7 @@ class TestFeedExportBase(ABC, unittest.TestCase):\n         data = yield self.run_and_export(TestSpider, settings)\n         return data\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExported(self, items, header, rows, settings=None):\n         yield self.assertExportedCsv(items, header, rows, settings)\n         yield self.assertExportedJsonLines(items, rows, settings)\n@@ -770,7 +771,7 @@ class ExceptionJsonItemExporter(JsonItemExporter):\n \n \n class TestFeedExport(TestFeedExportBase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data.\"\"\"\n \n@@ -800,7 +801,7 @@ class TestFeedExport(TestFeedExportBase):\n \n         return content\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -815,7 +816,7 @@ class TestFeedExport(TestFeedExportBase):\n         assert reader.fieldnames == list(header)\n         assert rows == list(reader)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -830,7 +831,7 @@ class TestFeedExport(TestFeedExportBase):\n         rows = [{k: v for k, v in row.items() if v} for row in rows]\n         assert rows == parsed\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedXml(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -846,7 +847,7 @@ class TestFeedExport(TestFeedExportBase):\n         got_rows = [{e.tag: e.text for e in it} for it in root.findall(\"item\")]\n         assert rows == got_rows\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedMultiple(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -867,7 +868,7 @@ class TestFeedExport(TestFeedExportBase):\n         json_rows = json.loads(to_unicode(data[\"json\"]))\n         assert rows == json_rows\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedPickle(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -884,7 +885,7 @@ class TestFeedExport(TestFeedExportBase):\n         result = self._load_until_eof(data[\"pickle\"], load_func=pickle.load)\n         assert result == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedMarshal(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -901,7 +902,7 @@ class TestFeedExport(TestFeedExportBase):\n         result = self._load_until_eof(data[\"marshal\"], load_func=marshal.load)\n         assert result == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_stats_file_success(self):\n         settings = {\n             \"FEEDS\": {\n@@ -915,7 +916,7 @@ class TestFeedExport(TestFeedExportBase):\n         assert \"feedexport/success_count/FileFeedStorage\" in crawler.stats.get_stats()\n         assert crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\") == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_stats_file_failed(self):\n         settings = {\n             \"FEEDS\": {\n@@ -933,7 +934,7 @@ class TestFeedExport(TestFeedExportBase):\n         assert \"feedexport/failed_count/FileFeedStorage\" in crawler.stats.get_stats()\n         assert crawler.stats.get_value(\"feedexport/failed_count/FileFeedStorage\") == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_stats_multiple_file(self):\n         settings = {\n             \"FEEDS\": {\n@@ -955,7 +956,7 @@ class TestFeedExport(TestFeedExportBase):\n             crawler.stats.get_value(\"feedexport/success_count/StdoutFeedStorage\") == 1\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items(self):\n         # feed exporters use field names from Item\n         items = [\n@@ -969,7 +970,7 @@ class TestFeedExport(TestFeedExportBase):\n         header = self.MyItem.fields.keys()\n         yield self.assertExported(items, header, rows)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n         for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n             settings = {\n@@ -981,7 +982,7 @@ class TestFeedExport(TestFeedExportBase):\n             data = yield self.exported_no_data(settings)\n             assert data[fmt] is None\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_finish_exporting_items(self):\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -1001,7 +1002,7 @@ class TestFeedExport(TestFeedExportBase):\n             assert not listener.start_without_finish\n             assert not listener.finish_without_start\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_finish_exporting_no_items(self):\n         items = []\n         settings = {\n@@ -1019,7 +1020,7 @@ class TestFeedExport(TestFeedExportBase):\n             assert not listener.start_without_finish\n             assert not listener.finish_without_start\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_finish_exporting_items_exception(self):\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -1040,7 +1041,7 @@ class TestFeedExport(TestFeedExportBase):\n             assert not listener.start_without_finish\n             assert not listener.finish_without_start\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_start_finish_exporting_no_items_exception(self):\n         items = []\n         settings = {\n@@ -1059,7 +1060,7 @@ class TestFeedExport(TestFeedExportBase):\n             assert not listener.start_without_finish\n             assert not listener.finish_without_start\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n             (\"json\", b\"[]\"),\n@@ -1079,7 +1080,7 @@ class TestFeedExport(TestFeedExportBase):\n             data = yield self.exported_no_data(settings)\n             assert expctd == data[fmt]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_no_items_multiple_feeds(self):\n         \"\"\"Make sure that `storage.store` is called for every feed.\"\"\"\n         settings = {\n@@ -1097,7 +1098,7 @@ class TestFeedExport(TestFeedExportBase):\n \n         assert str(log).count(\"Storage.store is called\") == 0\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_multiple_item_classes(self):\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -1119,7 +1120,7 @@ class TestFeedExport(TestFeedExportBase):\n         yield self.assertExportedCsv(items, header, rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_empty_field_list(self):\n         # FEED_EXPORT_FIELDS==[] means the same as default None\n         items = [{\"foo\": \"bar\"}]\n@@ -1129,7 +1130,7 @@ class TestFeedExport(TestFeedExportBase):\n         yield self.assertExportedCsv(items, header, rows)\n         yield self.assertExportedJsonLines(items, rows, settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_field_list(self):\n         items = [{\"foo\": \"bar\"}]\n         header = [\"foo\", \"baz\"]\n@@ -1137,7 +1138,7 @@ class TestFeedExport(TestFeedExportBase):\n         settings = {\"FEED_EXPORT_FIELDS\": header}\n         yield self.assertExported(items, header, rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_comma_separated_field_list(self):\n         items = [{\"foo\": \"bar\"}]\n         header = [\"foo\", \"baz\"]\n@@ -1145,7 +1146,7 @@ class TestFeedExport(TestFeedExportBase):\n         settings = {\"FEED_EXPORT_FIELDS\": \",\".join(header)}\n         yield self.assertExported(items, header, rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_json_field_list(self):\n         items = [{\"foo\": \"bar\"}]\n         header = [\"foo\", \"baz\"]\n@@ -1153,7 +1154,7 @@ class TestFeedExport(TestFeedExportBase):\n         settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n         yield self.assertExported(items, header, rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_field_names(self):\n         items = [{\"foo\": \"bar\"}]\n         header = {\"foo\": \"Foo\"}\n@@ -1161,7 +1162,7 @@ class TestFeedExport(TestFeedExportBase):\n         settings = {\"FEED_EXPORT_FIELDS\": header}\n         yield self.assertExported(items, list(header.values()), rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_dict_field_names(self):\n         items = [{\"foo\": \"bar\"}]\n         header = {\n@@ -1172,7 +1173,7 @@ class TestFeedExport(TestFeedExportBase):\n         settings = {\"FEED_EXPORT_FIELDS\": header}\n         yield self.assertExported(items, [\"Baz\", \"Foo\"], rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items_json_field_names(self):\n         items = [{\"foo\": \"bar\"}]\n         header = {\"foo\": \"Foo\"}\n@@ -1180,7 +1181,7 @@ class TestFeedExport(TestFeedExportBase):\n         settings = {\"FEED_EXPORT_FIELDS\": json.dumps(header)}\n         yield self.assertExported(items, list(header.values()), rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_based_on_item_classes(self):\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -1226,7 +1227,7 @@ class TestFeedExport(TestFeedExportBase):\n         for fmt, expected in formats.items():\n             assert data[fmt] == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_based_on_custom_filters(self):\n         items = [\n             self.MyItem({\"foo\": \"bar1\", \"egg\": \"spam1\"}),\n@@ -1285,7 +1286,7 @@ class TestFeedExport(TestFeedExportBase):\n         for fmt, expected in formats.items():\n             assert data[fmt] == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n         # a header for CSV, and all fields are used for JSON Lines.\n@@ -1298,7 +1299,7 @@ class TestFeedExport(TestFeedExportBase):\n         yield self.assertExportedCsv(items, [\"foo\", \"egg\"], rows_csv)\n         yield self.assertExportedJsonLines(items, rows_jl)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_tuple(self):\n         items = [\n             {\"foo\": \"bar1\", \"egg\": \"spam1\"},\n@@ -1309,7 +1310,7 @@ class TestFeedExport(TestFeedExportBase):\n         rows = [{\"foo\": \"bar1\", \"baz\": \"\"}, {\"foo\": \"bar2\", \"baz\": \"quux\"}]\n         yield self.assertExported(items, [\"foo\", \"baz\"], rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_feed_export_fields(self):\n         # FEED_EXPORT_FIELDS option allows to order export fields\n         # and to select a subset of fields to export, both for Items and dicts.\n@@ -1335,7 +1336,7 @@ class TestFeedExport(TestFeedExportBase):\n             rows = [{\"egg\": \"spam1\", \"baz\": \"\"}, {\"egg\": \"spam2\", \"baz\": \"quux2\"}]\n             yield self.assertExported(items, [\"egg\", \"baz\"], rows, settings=settings)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_encoding(self):\n         items = [{\"foo\": \"Test\\xd6\"}]\n \n@@ -1380,7 +1381,7 @@ class TestFeedExport(TestFeedExportBase):\n             data = yield self.exported_data(items, settings)\n             assert data[fmt] == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_multiple_configs(self):\n         items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n \n@@ -1420,7 +1421,7 @@ class TestFeedExport(TestFeedExportBase):\n         for fmt, expected in formats.items():\n             assert data[fmt] == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_indentation(self):\n         items = [\n             {\"foo\": [\"bar\"]},\n@@ -1576,7 +1577,7 @@ class TestFeedExport(TestFeedExportBase):\n             data = yield self.exported_data(items, settings)\n             assert data[row[\"format\"]] == row[\"expected\"]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_init_exporters_storages_with_crawler(self):\n         settings = {\n             \"FEED_EXPORTERS\": {\"csv\": FromCrawlerCsvItemExporter},\n@@ -1589,7 +1590,7 @@ class TestFeedExport(TestFeedExportBase):\n         assert FromCrawlerCsvItemExporter.init_with_crawler\n         assert FromCrawlerFileFeedStorage.init_with_crawler\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_str_uri(self):\n         settings = {\n             \"FEED_STORE_EMPTY\": True,\n@@ -1598,7 +1599,7 @@ class TestFeedExport(TestFeedExportBase):\n         data = yield self.exported_no_data(settings)\n         assert data[\"csv\"] == b\"\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_multiple_feeds_success_logs_blocking_feed_storage(self):\n         settings = {\n             \"FEEDS\": {\n@@ -1619,7 +1620,7 @@ class TestFeedExport(TestFeedExportBase):\n         for fmt in [\"json\", \"xml\", \"csv\"]:\n             assert f\"Stored {fmt} feed (2 items)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_multiple_feeds_failing_logs_blocking_feed_storage(self):\n         settings = {\n             \"FEEDS\": {\n@@ -1640,7 +1641,7 @@ class TestFeedExport(TestFeedExportBase):\n         for fmt in [\"json\", \"xml\", \"csv\"]:\n             assert f\"Error storing {fmt} feed (2 items)\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_extend_kwargs(self):\n         items = [{\"foo\": \"FOO\", \"bar\": \"BAR\"}]\n \n@@ -1677,7 +1678,7 @@ class TestFeedExport(TestFeedExportBase):\n             data = yield self.exported_data(items, settings)\n             assert data[feed_options[\"format\"]] == row[\"expected\"]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_storage_file_no_postprocessing(self):\n         @implementer(IFeedStorage)\n         class Storage:\n@@ -1699,7 +1700,7 @@ class TestFeedExport(TestFeedExportBase):\n         yield self.exported_no_data(settings)\n         assert Storage.open_file is Storage.store_file\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_storage_file_postprocessing(self):\n         @implementer(IFeedStorage)\n         class Storage:\n@@ -1752,7 +1753,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n     def _named_tempfile(self, name) -> str:\n         return str(Path(self.temp_dir, name))\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data with filename.\"\"\"\n \n@@ -1796,7 +1797,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         data_stream.seek(0)\n         return data_stream.read()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_gzip_plugin(self):\n         filename = self._named_tempfile(\"gzip_file\")\n \n@@ -1815,7 +1816,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         except OSError:\n             pytest.fail(\"Received invalid gzip data.\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_gzip_plugin_compresslevel(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"compresslevel_0\"): self.get_gzip_compressed(\n@@ -1852,7 +1853,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_gzip_plugin_mtime(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"mtime_123\"): self.get_gzip_compressed(\n@@ -1887,7 +1888,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_gzip_plugin_filename(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"filename_FILE1\"): self.get_gzip_compressed(\n@@ -1922,7 +1923,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_lzma_plugin(self):\n         filename = self._named_tempfile(\"lzma_file\")\n \n@@ -1941,7 +1942,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         except lzma.LZMAError:\n             pytest.fail(\"Received invalid lzma data.\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_lzma_plugin_format(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"format_FORMAT_XZ\"): lzma.compress(\n@@ -1974,7 +1975,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_lzma_plugin_check(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"check_CHECK_NONE\"): lzma.compress(\n@@ -2007,7 +2008,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_lzma_plugin_preset(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"preset_PRESET_0\"): lzma.compress(\n@@ -2040,11 +2041,11 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_lzma_plugin_filters(self):\n         if \"PyPy\" in sys.version:\n             # https://foss.heptapod.net/pypy/pypy/-/issues/3527\n-            raise unittest.SkipTest(\"lzma filters doesn't work in PyPy\")\n+            pytest.skip(\"lzma filters doesn't work in PyPy\")\n \n         filters = [{\"id\": lzma.FILTER_LZMA2}]\n         compressed = lzma.compress(self.expected, filters=filters)\n@@ -2065,7 +2066,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         result = lzma.decompress(data[filename])\n         assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_bz2_plugin(self):\n         filename = self._named_tempfile(\"bz2_file\")\n \n@@ -2084,7 +2085,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         except OSError:\n             pytest.fail(\"Received invalid bz2 data.\")\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_bz2_plugin_compresslevel(self):\n         filename_to_compressed = {\n             self._named_tempfile(\"compresslevel_1\"): bz2.compress(\n@@ -2117,7 +2118,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             assert compressed == data[filename]\n             assert result == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_custom_plugin(self):\n         filename = self._named_tempfile(\"csv_file\")\n \n@@ -2133,7 +2134,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         data = yield self.exported_data(self.items, settings)\n         assert data[filename] == self.expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_custom_plugin_with_parameter(self):\n         expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n         filename = self._named_tempfile(\"newline\")\n@@ -2151,7 +2152,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n         data = yield self.exported_data(self.items, settings)\n         assert data[filename] == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_custom_plugin_with_compression(self):\n         expected = b\"foo\\r\\n\\nbar\\r\\n\\n\"\n \n@@ -2196,7 +2197,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n             result = decompressor(data[filename])\n             assert result == expected\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_exports_compatibility_with_postproc(self):\n         import marshal\n         import pickle\n@@ -2254,7 +2255,7 @@ class TestFeedPostProcessedExports(TestFeedExportBase):\n class TestBatchDeliveries(TestFeedExportBase):\n     _file_mark = \"_%(batch_time)s_#%(batch_id)02d_\"\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n         \"\"\"Run spider with specified settings; return exported data.\"\"\"\n \n@@ -2276,7 +2277,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n                 content[feed[\"format\"]].append(file.read_bytes())\n         return content\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedJsonLines(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -2298,7 +2299,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedCsv(self, items, header, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -2318,7 +2319,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             assert list(got_batch) == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedXml(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -2339,7 +2340,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedMultiple(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -2371,7 +2372,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             expected_batch, json_rows = json_rows[:batch_size], json_rows[batch_size:]\n             assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedPickle(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -2393,7 +2394,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def assertExportedMarshal(self, items, rows, settings=None):\n         settings = settings or {}\n         settings.update(\n@@ -2415,7 +2416,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             expected_batch, rows = rows[:batch_size], rows[batch_size:]\n             assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_items(self):\n         \"\"\"Test partial deliveries in all supported formats\"\"\"\n         items = [\n@@ -2444,7 +2445,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n         with pytest.raises(NotConfigured):\n             FeedExporter(crawler)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_no_items_not_store_empty(self):\n         for fmt in (\"json\", \"jsonlines\", \"xml\", \"csv\"):\n             settings = {\n@@ -2460,7 +2461,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             data = dict(data)\n             assert len(data[fmt]) == 0\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n             (\"json\", b\"[]\"),\n@@ -2484,7 +2485,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             data = dict(data)\n             assert data[fmt][0] == expctd\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_export_multiple_configs(self):\n         items = [\n             {\"foo\": \"FOO\", \"bar\": \"BAR\"},\n@@ -2540,7 +2541,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_batch_item_count_feeds_setting(self):\n         items = [{\"foo\": \"FOO\"}, {\"foo\": \"FOO1\"}]\n         formats = {\n@@ -2564,7 +2565,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n             for expected_batch, got_batch in zip(expected, data[fmt]):\n                 assert got_batch == expected_batch\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_batch_path_differ(self):\n         \"\"\"\n         Test that the name of all batch files differ from each other.\n@@ -2586,7 +2587,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n         data = yield self.exported_data(items, settings)\n         assert len(items) == len(data[\"json\"])\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_stats_batch_file_success(self):\n         settings = {\n             \"FEEDS\": {\n@@ -2604,7 +2605,7 @@ class TestBatchDeliveries(TestFeedExportBase):\n         assert crawler.stats.get_value(\"feedexport/success_count/FileFeedStorage\") == 12\n \n     @pytest.mark.requires_boto3\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_s3_export(self):\n         bucket = \"mybucket\"\n         items = [\n\n@@ -9,7 +9,7 @@ from ipaddress import IPv4Address\n from pathlib import Path\n from tempfile import mkdtemp\n from typing import TYPE_CHECKING\n-from unittest import mock, skipIf\n+from unittest import mock\n from urllib.parse import urlencode\n \n import pytest\n@@ -183,7 +183,7 @@ def get_client_certificate(\n     return PrivateCertificate.loadPEM(pem)\n \n \n-@skipIf(not H2_ENABLED, \"HTTP/2 support in Twisted is not enabled\")\n+@pytest.mark.skipif(not H2_ENABLED, reason=\"HTTP/2 support in Twisted is not enabled\")\n class TestHttps2ClientProtocol(TestCase):\n     scheme = \"https\"\n     key_file = Path(__file__).parent / \"keys\" / \"localhost.key\"\n\n@@ -2,7 +2,7 @@ import logging\n \n import pytest\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.python.failure import Failure\n from twisted.trial.unittest import TestCase\n \n@@ -272,7 +272,7 @@ class TestShowOrSkipMessages(TestCase):\n             },\n         }\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_show_messages(self):\n         crawler = get_crawler(ItemSpider, self.base_settings)\n         with LogCapture() as lc:\n@@ -281,7 +281,7 @@ class TestShowOrSkipMessages(TestCase):\n         assert \"Crawled (200) <GET http://127.0.0.1:\" in str(lc)\n         assert \"Dropped: Ignoring item\" in str(lc)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_skip_messages(self):\n         settings = self.base_settings.copy()\n         settings[\"LOG_FORMATTER\"] = SkipMessagesLogFormatter\n\n@@ -6,7 +6,7 @@ from tempfile import mkdtemp\n from typing import TYPE_CHECKING, Any\n \n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n from w3lib.url import add_or_replace_parameter\n \n@@ -144,7 +144,7 @@ class TestFileDownloadCrawl(TestCase):\n         # check that no files were written to the media store\n         assert not list(self.tmpmediastore.iterdir())\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_download_media(self):\n         crawler = self._create_crawler(MediaDownloadSpider)\n         with LogCapture() as log:\n@@ -155,7 +155,7 @@ class TestFileDownloadCrawl(TestCase):\n             )\n         self._assert_files_downloaded(self.items, str(log))\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_download_media_wrong_urls(self):\n         crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n         with LogCapture() as log:\n@@ -166,7 +166,7 @@ class TestFileDownloadCrawl(TestCase):\n             )\n         self._assert_files_download_failure(crawler, self.items, 404, str(log))\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_download_media_redirected_default_failure(self):\n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n@@ -178,7 +178,7 @@ class TestFileDownloadCrawl(TestCase):\n             )\n         self._assert_files_download_failure(crawler, self.items, 302, str(log))\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_download_media_redirected_allowed(self):\n         settings = {\n             **self.settings,\n@@ -195,7 +195,7 @@ class TestFileDownloadCrawl(TestCase):\n         self._assert_files_downloaded(self.items, str(log))\n         assert crawler.stats.get_value(\"downloader/response_status_count/302\") == 3\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_download_media_file_path_error(self):\n         cls = load_object(self.pipeline_class)\n \n\n@@ -16,7 +16,7 @@ from urllib.parse import urlparse\n import attr\n import pytest\n from itemadapter import ItemAdapter\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial import unittest\n \n from scrapy.http import Request, Response\n@@ -159,7 +159,7 @@ class TestFilesPipeline(unittest.TestCase):\n         fullpath = Path(self.tempdir, \"some\", \"image\", \"key.jpg\")\n         assert self.pipeline.store._get_filesystem_path(path) == fullpath\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_file_not_expired(self):\n         item_url = \"http://example.com/file.pdf\"\n         item = _create_item_with_files(item_url)\n@@ -186,7 +186,7 @@ class TestFilesPipeline(unittest.TestCase):\n         for p in patchers:\n             p.stop()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_file_expired(self):\n         item_url = \"http://example.com/file2.pdf\"\n         item = _create_item_with_files(item_url)\n@@ -217,7 +217,7 @@ class TestFilesPipeline(unittest.TestCase):\n         for p in patchers:\n             p.stop()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_file_cached(self):\n         item_url = \"http://example.com/file3.pdf\"\n         item = _create_item_with_files(item_url)\n@@ -537,7 +537,7 @@ class TestFilesPipelineCustomSettings:\n \n @pytest.mark.requires_botocore\n class TestS3FilesStore(unittest.TestCase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_persist(self):\n         bucket = \"mybucket\"\n         key = \"export.csv\"\n@@ -577,7 +577,7 @@ class TestS3FilesStore(unittest.TestCase):\n             # The call to read does not happen with Stubber\n             assert buffer.method_calls == [mock.call.seek(0)]\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_stat(self):\n         bucket = \"mybucket\"\n         key = \"export.csv\"\n@@ -614,11 +614,11 @@ class TestS3FilesStore(unittest.TestCase):\n     \"GCS_PROJECT_ID\" not in os.environ, reason=\"GCS_PROJECT_ID not found\"\n )\n class TestGCSFilesStore(unittest.TestCase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_persist(self):\n         uri = os.environ.get(\"GCS_TEST_FILE_URI\")\n         if not uri:\n-            raise unittest.SkipTest(\"No GCS URI available for testing\")\n+            pytest.skip(\"No GCS URI available for testing\")\n         data = b\"TestGCSFilesStore: \\xe2\\x98\\x83\"\n         buf = BytesIO(data)\n         meta = {\"foo\": \"bar\"}\n@@ -639,7 +639,7 @@ class TestGCSFilesStore(unittest.TestCase):\n         assert blob.content_type == \"application/octet-stream\"\n         assert expected_policy in acl\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_blob_path_consistency(self):\n         \"\"\"Test to make sure that paths used to store files is the same as the one used to get\n         already uploaded files.\n@@ -647,7 +647,7 @@ class TestGCSFilesStore(unittest.TestCase):\n         try:\n             import google.cloud.storage  # noqa: F401\n         except ModuleNotFoundError:\n-            raise unittest.SkipTest(\"google-cloud-storage is not installed\")\n+            pytest.skip(\"google-cloud-storage is not installed\")\n         with (\n             mock.patch(\"google.cloud.storage\"),\n             mock.patch(\"scrapy.pipelines.files.time\"),\n@@ -666,7 +666,7 @@ class TestGCSFilesStore(unittest.TestCase):\n \n \n class TestFTPFileStore(unittest.TestCase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_persist(self):\n         data = b\"TestFTPFilesStore: \\xe2\\x98\\x83\"\n         buf = BytesIO(data)\n\n@@ -1,8 +1,7 @@\n import asyncio\n \n import pytest\n-from twisted.internet import defer\n-from twisted.internet.defer import Deferred\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.trial import unittest\n \n from scrapy import Request, Spider, signals\n@@ -100,33 +99,33 @@ class TestPipeline(unittest.TestCase):\n         self.items = []\n         return crawler\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_simple_pipeline(self):\n         crawler = self._create_crawler(SimplePipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert len(self.items) == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_deferred_pipeline(self):\n         crawler = self._create_crawler(DeferredPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert len(self.items) == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_asyncdef_pipeline(self):\n         crawler = self._create_crawler(AsyncDefPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert len(self.items) == 1\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_asyncdef_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n         assert len(self.items) == 1\n \n     @pytest.mark.only_not_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_asyncdef_not_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefNotAsyncioPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n\n@@ -8,7 +8,7 @@ from urllib.parse import urlsplit, urlunsplit\n \n import pytest\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n@@ -89,14 +89,14 @@ class TestProxyConnect(TestCase):\n         self._proxy.stop()\n         os.environ = self._oldenv\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_https_connect_tunnel(self):\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(self.mockserver.url(\"/status?n=200\", is_secure=True))\n         self._assert_got_response_code(200, log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_https_tunnel_auth_error(self):\n         os.environ[\"https_proxy\"] = _wrong_credentials(os.environ[\"https_proxy\"])\n         crawler = get_crawler(SimpleSpider)\n@@ -106,7 +106,7 @@ class TestProxyConnect(TestCase):\n         # he just sees a TunnelError.\n         self._assert_got_tunnel_error(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_https_tunnel_without_leak_proxy_authorization_header(self):\n         request = Request(self.mockserver.url(\"/echo\", is_secure=True))\n         crawler = get_crawler(SingleRequestSpider)\n\n@@ -1,5 +1,5 @@\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy import Request, signals\n@@ -66,7 +66,7 @@ class TestCrawl(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_200(self):\n         url = self.mockserver.url(\"/status?n=200\")\n         crawler = get_crawler(SingleRequestSpider)\n@@ -74,7 +74,7 @@ class TestCrawl(TestCase):\n         response = crawler.spider.meta[\"responses\"][0]\n         assert response.request.url == url\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_response_error(self):\n         for status in (\"404\", \"500\"):\n             url = self.mockserver.url(f\"/status?n={status}\")\n@@ -85,7 +85,7 @@ class TestCrawl(TestCase):\n             assert failure.request.url == url\n             assert response.request.url == url\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_downloader_middleware_raise_exception(self):\n         url = self.mockserver.url(\"/status?n=200\")\n         crawler = get_crawler(\n@@ -101,7 +101,7 @@ class TestCrawl(TestCase):\n         assert failure.request.url == url\n         assert isinstance(failure.value, ZeroDivisionError)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_downloader_middleware_override_request_in_process_response(self):\n         \"\"\"\n         Downloader middleware which returns a response with an specific 'request' attribute.\n@@ -144,7 +144,7 @@ class TestCrawl(TestCase):\n             ),\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_downloader_middleware_override_in_process_exception(self):\n         \"\"\"\n         An exception is raised but caught by the next middleware, which\n@@ -167,7 +167,7 @@ class TestCrawl(TestCase):\n         assert response.body == b\"Caught ZeroDivisionError\"\n         assert response.request.url == OVERRIDDEN_URL\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_downloader_middleware_do_not_override_in_process_exception(self):\n         \"\"\"\n         An exception is raised but caught by the next middleware, which\n@@ -190,7 +190,7 @@ class TestCrawl(TestCase):\n         assert response.body == b\"Caught ZeroDivisionError\"\n         assert response.request.url == url\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_downloader_middleware_alternative_callback(self):\n         \"\"\"\n         Downloader middleware which returns a response with a\n\n@@ -1,5 +1,5 @@\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n@@ -161,7 +161,7 @@ class TestCallbackKeywordArguments(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_callback_kwargs(self):\n         crawler = get_crawler(KeywordArgumentsSpider)\n         with LogCapture() as log:\n\n@@ -1,4 +1,4 @@\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.signals import request_left_downloader\n@@ -34,25 +34,25 @@ class TestCatching(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_success(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n         assert crawler.spider.caught_times == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_timeout(self):\n         crawler = get_crawler(SignalCatcherSpider, {\"DOWNLOAD_TIMEOUT\": 0.1})\n         yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n         assert crawler.spider.caught_times == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_disconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(self.mockserver.url(\"/drop\"))\n         assert crawler.spider.caught_times == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_noconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(\"http://thereisdefinetelynosuchdomain.com\")\n\n@@ -7,7 +7,7 @@ from collections import deque\n from typing import Any, NamedTuple\n \n import pytest\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.downloader import Downloader\n@@ -362,11 +362,11 @@ class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n             },\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def tearDown(self):\n         yield self.crawler.stop()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_integration_downloader_aware_priority_queue(self):\n         with MockServer() as mockserver:\n             url = mockserver.url(\"/status?n=200\", is_secure=False)\n\n@@ -5,6 +5,7 @@ from urllib.parse import urljoin\n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.scheduler import BaseScheduler\n@@ -118,7 +119,7 @@ class SimpleSchedulerTest(TestCase, InterfaceCheckMixin):\n     def setUp(self):\n         self.scheduler = SimpleScheduler()\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_enqueue_dequeue(self):\n         open_result = yield self.scheduler.open(Spider(\"foo\"))\n         assert open_result == \"open\"\n@@ -147,7 +148,7 @@ class SimpleSchedulerTest(TestCase, InterfaceCheckMixin):\n class MinimalSchedulerCrawlTest(TestCase):\n     scheduler_cls = MinimalScheduler\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_crawl(self):\n         with MockServer() as mockserver:\n             settings = {\n\n@@ -1,5 +1,5 @@\n import pytest\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy import Request, Spider, signals\n@@ -53,7 +53,7 @@ class MockServerTestCase(TestCase):\n         self.items.append(item)\n \n     @pytest.mark.only_asyncio\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_simple_pipeline(self):\n         crawler = get_crawler(ItemSpider)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n\n@@ -8,6 +8,7 @@ from unittest import mock\n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.core.spidermw import SpiderMiddlewareManager\n@@ -129,7 +130,7 @@ class TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n         yield {\"foo\": 2}\n         yield {\"foo\": 3}\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: int | None = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n         self.crawler = get_crawler(\n@@ -142,7 +143,7 @@ class TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n         )\n         return result\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _test_simple_base(\n         self, *mw_classes, downgrade: bool = False, start_index: int | None = None\n     ):\n@@ -159,7 +160,7 @@ class TestBaseAsyncSpiderMiddleware(TestSpiderMiddleware):\n             ProcessSpiderOutputSimpleMiddleware in mw_classes\n         )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _test_asyncgen_base(\n         self, *mw_classes, downgrade: bool = False, start_index: int | None = None\n     ):\n@@ -299,7 +300,7 @@ class ProcessSpiderOutputCoroutineMiddleware:\n \n \n class TestProcessSpiderOutputInvalidResult(TestBaseAsyncSpiderMiddleware):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_non_iterable(self):\n         with pytest.raises(\n             _InvalidOutput,\n@@ -309,7 +310,7 @@ class TestProcessSpiderOutputInvalidResult(TestBaseAsyncSpiderMiddleware):\n                 ProcessSpiderOutputNonIterableMiddleware,\n             )\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_coroutine(self):\n         with pytest.raises(\n             _InvalidOutput,\n@@ -444,7 +445,7 @@ class TestBuiltinMiddlewareSimple(TestBaseAsyncSpiderMiddleware):\n     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: int | None = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n         self.crawler = get_crawler(Spider, {\"SPIDER_MIDDLEWARES\": setting})\n@@ -519,7 +520,7 @@ class TestProcessSpiderException(TestBaseAsyncSpiderMiddleware):\n     def _scrape_func(self, *args, **kwargs):\n         1 / 0\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def _test_asyncgen_nodowngrade(self, *mw_classes):\n         with pytest.raises(\n             _InvalidOutput, match=\"Async iterable returned from .+ cannot be downgraded\"\n\n@@ -2,7 +2,7 @@ import logging\n \n import pytest\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request, Response\n@@ -182,7 +182,7 @@ class TestHttpErrorMiddlewareIntegrational(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_middleware_works(self):\n         crawler = get_crawler(_HttpErrorSpider)\n         yield crawler.crawl(mockserver=self.mockserver)\n@@ -196,7 +196,7 @@ class TestHttpErrorMiddlewareIntegrational(TestCase):\n         assert get_value(\"httperror/response_ignored_status_count/402\") == 1\n         assert get_value(\"httperror/response_ignored_status_count/500\") == 1\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_logging(self):\n         crawler = get_crawler(_HttpErrorSpider)\n         with LogCapture() as log:\n@@ -210,7 +210,7 @@ class TestHttpErrorMiddlewareIntegrational(TestCase):\n         assert \"Ignoring response <200\" not in str(log)\n         assert \"Ignoring response <402\" not in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_logging_level(self):\n         # HttpError logs ignored responses with level INFO\n         crawler = get_crawler(_HttpErrorSpider)\n\n@@ -1,5 +1,5 @@\n from testfixtures import LogCapture\n-from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.trial.unittest import TestCase\n \n from scrapy import Request, Spider\n@@ -308,14 +308,14 @@ class TestSpiderMiddleware(TestCase):\n     def tearDownClass(cls):\n         cls.mockserver.__exit__(None, None, None)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def crawl_log(self, spider):\n         crawler = get_crawler(spider)\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n         return log\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_recovery(self):\n         \"\"\"\n         (0) Recover from an exception in a spider callback. The final item count should be 3\n@@ -328,7 +328,7 @@ class TestSpiderMiddleware(TestCase):\n         assert str(log).count(\"Middleware: TabError exception caught\") == 1\n         assert \"'item_scraped_count': 3\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_recovery_asyncgen(self):\n         \"\"\"\n         Same as test_recovery but with an async callback.\n@@ -338,7 +338,7 @@ class TestSpiderMiddleware(TestCase):\n         assert str(log).count(\"Middleware: TabError exception caught\") == 1\n         assert \"'item_scraped_count': 3\" in str(log)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_process_spider_input_without_errback(self):\n         \"\"\"\n         (1.1) An exception from the process_spider_input chain should be caught by the\n@@ -348,7 +348,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"Middleware: will raise IndexError\" in str(log1)\n         assert \"Middleware: IndexError exception caught\" in str(log1)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_process_spider_input_with_errback(self):\n         \"\"\"\n         (1.2) An exception from the process_spider_input chain should not be caught by the\n@@ -362,7 +362,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"{'from': 'callback'}\" not in str(log1)\n         assert \"'item_scraped_count': 1\" in str(log1)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_generator_callback(self):\n         \"\"\"\n         (2) An exception from a spider callback (returning a generator) should\n@@ -373,7 +373,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"Middleware: ImportError exception caught\" in str(log2)\n         assert \"'item_scraped_count': 2\" in str(log2)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_async_generator_callback(self):\n         \"\"\"\n         Same as test_generator_callback but with an async callback.\n@@ -382,7 +382,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"Middleware: ImportError exception caught\" in str(log2)\n         assert \"'item_scraped_count': 2\" in str(log2)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_generator_callback_right_after_callback(self):\n         \"\"\"\n         (2.1) Special case of (2): Exceptions should be caught\n@@ -392,7 +392,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"Middleware: ImportError exception caught\" in str(log21)\n         assert \"'item_scraped_count': 2\" in str(log21)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_not_a_generator_callback(self):\n         \"\"\"\n         (3) An exception from a spider callback (returning a list) should\n@@ -402,7 +402,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"Middleware: ZeroDivisionError exception caught\" in str(log3)\n         assert \"item_scraped_count\" not in str(log3)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_not_a_generator_callback_right_after_callback(self):\n         \"\"\"\n         (3.1) Special case of (3): Exceptions should be caught\n@@ -414,7 +414,7 @@ class TestSpiderMiddleware(TestCase):\n         assert \"Middleware: ZeroDivisionError exception caught\" in str(log31)\n         assert \"item_scraped_count\" not in str(log31)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_generator_output_chain(self):\n         \"\"\"\n         (4) An exception from a middleware's process_spider_output method should be sent\n@@ -461,7 +461,7 @@ class TestSpiderMiddleware(TestCase):\n         assert str(item_recovered) in str(log4)\n         assert \"parse-second-item\" not in str(log4)\n \n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_not_a_generator_output_chain(self):\n         \"\"\"\n         (5) An exception from a middleware's process_spider_output method should be sent\n\n@@ -4,7 +4,6 @@ Queues that handle requests\n \n import shutil\n import tempfile\n-import unittest\n \n import pytest\n import queuelib\n@@ -46,7 +45,7 @@ class RequestQueueTestMixin:\n \n     def test_one_element_with_peek(self):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+            pytest.skip(\"The queuelib queues do not define peek\")\n         q = self.queue()\n         assert len(q) == 0\n         assert q.peek() is None\n@@ -63,7 +62,7 @@ class RequestQueueTestMixin:\n \n     def test_one_element_without_peek(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"The queuelib queues define peek\")\n+            pytest.skip(\"The queuelib queues define peek\")\n         q = self.queue()\n         assert len(q) == 0\n         assert q.pop() is None\n@@ -84,7 +83,7 @@ class RequestQueueTestMixin:\n class FifoQueueMixin(RequestQueueTestMixin):\n     def test_fifo_with_peek(self):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+            pytest.skip(\"The queuelib queues do not define peek\")\n         q = self.queue()\n         assert len(q) == 0\n         assert q.peek() is None\n@@ -111,7 +110,7 @@ class FifoQueueMixin(RequestQueueTestMixin):\n \n     def test_fifo_without_peek(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+            pytest.skip(\"The queuelib queues do not define peek\")\n         q = self.queue()\n         assert len(q) == 0\n         assert q.pop() is None\n@@ -140,7 +139,7 @@ class FifoQueueMixin(RequestQueueTestMixin):\n class LifoQueueMixin(RequestQueueTestMixin):\n     def test_lifo_with_peek(self):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+            pytest.skip(\"The queuelib queues do not define peek\")\n         q = self.queue()\n         assert len(q) == 0\n         assert q.peek() is None\n@@ -167,7 +166,7 @@ class LifoQueueMixin(RequestQueueTestMixin):\n \n     def test_lifo_without_peek(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"The queuelib queues do not define peek\")\n+            pytest.skip(\"The queuelib queues do not define peek\")\n         q = self.queue()\n         assert len(q) == 0\n         assert q.pop() is None\n\n@@ -4,7 +4,6 @@ import json\n import logging\n import re\n import sys\n-import unittest\n from io import StringIO\n from typing import TYPE_CHECKING, Any\n \n@@ -100,21 +99,19 @@ class TestLogCounterHandler:\n         assert self.crawler.stats.get_value(\"log_count/INFO\") is None\n \n \n-class StreamLoggerTest(unittest.TestCase):\n-    def setUp(self):\n-        self.stdout = sys.stdout\n+class TestStreamLogger:\n+    def test_redirect(self):\n         logger = logging.getLogger(\"test\")\n         logger.setLevel(logging.WARNING)\n+        old_stdout = sys.stdout\n         sys.stdout = StreamLogger(logger, logging.ERROR)\n \n-    def tearDown(self):\n-        sys.stdout = self.stdout\n-\n-    def test_redirect(self):\n         with LogCapture() as log:\n             print(\"test log msg\")\n         log.check((\"test\", \"ERROR\", \"test log msg\"))\n \n+        sys.stdout = old_stdout\n+\n \n @pytest.mark.parametrize(\n     (\"base_extra\", \"log_extra\", \"expected_extra\"),\n\n@@ -4,6 +4,7 @@ import pytest\n from pydispatch import dispatcher\n from testfixtures import LogCapture\n from twisted.internet import defer\n+from twisted.internet.defer import inlineCallbacks\n from twisted.python.failure import Failure\n from twisted.trial import unittest\n \n@@ -17,7 +18,7 @@ from scrapy.utils.test import get_from_asyncio_queue\n \n \n class TestSendCatchLog(unittest.TestCase):\n-    @defer.inlineCallbacks\n+    @inlineCallbacks\n     def test_send_catch_log(self):\n         test_signal = object()\n         handlers_called = set()\n\n@@ -2,7 +2,7 @@ from io import StringIO\n from time import sleep, time\n from unittest import mock\n \n-from twisted.trial.unittest import SkipTest\n+import pytest\n \n from scrapy.utils import trackref\n \n@@ -71,7 +71,7 @@ Foo                                 1   oldest: 0s ago\\n\\n\"\"\"\n             sleep(0.01)\n             o3_time = time()\n         if o3_time <= o1_time:\n-            raise SkipTest(\"time.time is not precise enough\")\n+            pytest.skip(\"time.time is not precise enough\")\n \n         o3 = Foo()  # noqa: F841\n         assert trackref.get_oldest(\"Foo\") is o1\n\n@@ -1,4 +1,3 @@\n-import unittest\n import warnings\n \n import pytest\n@@ -267,7 +266,7 @@ def create_guess_scheme_t(args):\n \n def create_skipped_scheme_t(args):\n     def do_expected(self):\n-        raise unittest.SkipTest(args[2])\n+        pytest.skip(args[2])\n \n     return do_expected\n \n\n@@ -290,17 +290,15 @@ class TestWebClient(unittest.TestCase):\n         d.addCallback(self.assertEqual, to_bytes(f\"127.0.0.1:{self.portno}\"))\n         return d\n \n+    @inlineCallbacks\n     def test_timeoutTriggering(self):\n         \"\"\"\n         When a non-zero timeout is passed to L{getPage} and that many\n         seconds elapse before the server responds to the request. the\n         L{Deferred} is errbacked with a L{error.TimeoutError}.\n         \"\"\"\n-        finished = self.assertFailure(\n-            getPage(self.getURL(\"wait\"), timeout=0.000001), defer.TimeoutError\n-        )\n-\n-        def cleanup(passthrough):\n+        with pytest.raises(defer.TimeoutError):\n+            yield getPage(self.getURL(\"wait\"), timeout=0.000001)\n         # Clean up the server which is hanging around not doing\n         # anything.\n         connected = list(self.wrapper.protocols.keys())\n@@ -308,10 +306,6 @@ class TestWebClient(unittest.TestCase):\n         # that the connection was lost.\n         if connected:\n             connected[0].transport.loseConnection()\n-            return passthrough\n-\n-        finished.addBoth(cleanup)\n-        return finished\n \n     def testNotFound(self):\n         return getPage(self.getURL(\"notsuchfile\")).addCallback(self._cbNoSuchFile)\n@@ -384,6 +378,7 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n             self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n         ).addCallback(self.assertEqual, to_bytes(s))\n \n+    @inlineCallbacks\n     def testPayloadDisabledCipher(self):\n         s = \"0123456789\" * 10\n         crawler = get_crawler(\n@@ -392,7 +387,7 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n             }\n         )\n         client_context_factory = build_from_crawler(ScrapyClientContextFactory, crawler)\n-        d = getPage(\n+        with pytest.raises(OpenSSL.SSL.Error):\n+            yield getPage(\n                 self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n             )\n-        return self.assertFailure(d, OpenSSL.SSL.Error)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
