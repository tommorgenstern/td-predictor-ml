{"custom_id": "keras#2d1086447a25d281f9428832d046c473d80ad761", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 46 | Files Changed: 2 | Hunks: 27 | Methods Changed: 16 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 100 | Churn Cumulative: 4878 | Contributors (this commit): 4 | Commits (past 90d): 19 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -55,7 +55,6 @@ WEIGHTS_HASHES = {\n      \"de3f8a54174130e0cecdc71583354753d557fcf1f4487331558e2a16ba0cfe05\"),\n }\n \n-\n MODEL_CONFIGS = {\n     \"tiny\": {\n         \"depths\": [3, 3, 9, 3],\n@@ -148,6 +147,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     A `keras.Model` instance.\n \"\"\"\n \n+\n class StochasticDepth(layers.Layer):\n     \"\"\"Stochastic Depth module.\n \n@@ -204,24 +204,25 @@ class LayerScale(layers.Layer):\n         self.projection_dim = projection_dim\n \n     def build(self, input_shape):\n-    self.gamma = tf.Variable(self.init_values * tf.ones((self.projection_dim,)))\n+        self.gamma = tf.Variable(self.init_values * tf.ones(\n+            (self.projection_dim, )))\n \n     def call(self, x):\n         return x * self.gamma\n \n     def get_config(self):\n         config = super().get_config()\n-    config.update(\n-      {\"init_values\": self.init_values, \"projection_dim\": self.projection_dim}\n-    )\n+        config.update({\n+            \"init_values\": self.init_values,\n+            \"projection_dim\": self.projection_dim\n+        })\n         return config\n \n-def ConvNeXtBlock(\n-    projection_dim,\n+\n+def ConvNeXtBlock(projection_dim,\n                   drop_path_rate=0.0,\n                   layer_scale_init_value=1e-6,\n-    name=None\n-    ):\n+                  name=None):\n     \"\"\"ConvNeXt block.\n \n   References:\n@@ -251,23 +252,30 @@ def ConvNeXtBlock(\n     def apply(inputs):\n         x = inputs\n \n-    x = layers.Conv2D(\n-      filters=projection_dim, kernel_size=7, padding=\"same\",\n-      groups=projection_dim, name=name + \"_depthwise_conv\")(x)\n-    x = layers.LayerNormalization(epsilon=1e-6, name=name + \"_layernorm\")(x)\n-    x = layers.Dense(4 * projection_dim, name=name + \"_pointwise_conv_1\")(x)\n+        x = layers.Conv2D(filters=projection_dim,\n+                          kernel_size=7,\n+                          padding=\"same\",\n+                          groups=projection_dim,\n+                          name=name + \"_depthwise_conv\")(x)\n+        x = layers.LayerNormalization(epsilon=1e-6,\n+                                      name=name + \"_layernorm\")(x)\n+        x = layers.Dense(4 * projection_dim,\n+                         name=name + \"_pointwise_conv_1\")(x)\n         x = layers.Activation(\"gelu\", name=name + \"_gelu\")(x)\n         x = layers.Dense(projection_dim, name=name + \"_pointwise_conv_2\")(x)\n \n         if layer_scale_init_value is not None:\n-      x = LayerScale(layer_scale_init_value, projection_dim,\n+            x = LayerScale(layer_scale_init_value,\n+                           projection_dim,\n                            name=name + \"_layer_scale\")(x)\n         if drop_path_rate:\n-      layer = StochasticDepth(drop_path_rate, name=name + \"_stochastic_depth\")\n+            layer = StochasticDepth(drop_path_rate,\n+                                    name=name + \"_stochastic_depth\")\n         else:\n             layer = layers.Activation(\"linear\", name=name + \"_identity\")\n \n         return inputs + layer(x)\n+\n     return apply\n \n \n@@ -284,11 +292,10 @@ def PreStem(name=None):\n         name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n \n     def apply(x):\n-    x = layers.Normalization(\n-      mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n-      variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n-      name=name + \"_prestem_normalization\"\n-    )(x)\n+        x = layers.Normalization(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n+                                 variance=[(0.229 * 255)**2, (0.224 * 255)**2,\n+                                           (0.225 * 255)**2],\n+                                 name=name + \"_prestem_normalization\")(x)\n         return x\n \n     return apply\n@@ -309,8 +316,8 @@ def Head(num_classes=1000, name=None):\n \n     def apply(x):\n         x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n-    x = layers.LayerNormalization(\n-      epsilon=1e-6, name=name + \"_head_layernorm\")(x)\n+        x = layers.LayerNormalization(epsilon=1e-6,\n+                                      name=name + \"_head_layernorm\")(x)\n         x = layers.Dense(num_classes, name=name + \"_head_dense\")(x)\n         return x\n \n@@ -386,7 +393,8 @@ def ConvNeXt(depths,\n                          \"or the path to the weights file to be loaded.\")\n \n     if weights == \"imagenet\" and include_top and classes != 1000:\n-    raise ValueError(\"If using `weights` as `'imagenet'` with `include_top`\"\n+        raise ValueError(\n+            \"If using `weights` as `'imagenet'` with `include_top`\"\n             \" as true, `classes` should be 1000\")\n \n     # Determine proper input shape.\n@@ -413,7 +421,8 @@ def ConvNeXt(depths,\n \n     x = inputs\n     if include_preprocessing:\n-    channel_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n+        channel_axis = 3 if backend.image_data_format(\n+        ) == \"channels_last\" else 1\n         num_channels = input_shape[channel_axis - 1]\n         if num_channels == 3:\n             x = PreStem(name=model_name)(x)\n@@ -421,12 +430,12 @@ def ConvNeXt(depths,\n     # Stem block.\n     stem = sequential.Sequential(\n         [\n-      layers.Conv2D(projection_dims[0], kernel_size=4, strides=4,\n+            layers.Conv2D(projection_dims[0],\n+                          kernel_size=4,\n+                          strides=4,\n                           name=model_name + \"_stem_conv\"),\n-      layers.LayerNormalization(\n-              epsilon=1e-6,\n-              name=model_name + \"_stem_layernorm\"\n-      ),\n+            layers.LayerNormalization(epsilon=1e-6,\n+                                      name=model_name + \"_stem_layernorm\"),\n         ],\n         name=model_name + \"_stem\",\n     )\n@@ -439,9 +448,13 @@ def ConvNeXt(depths,\n     for i in range(num_downsample_layers):\n         downsample_layer = sequential.Sequential(\n             [\n-        layers.LayerNormalization(epsilon=1e-6,\n+                layers.LayerNormalization(\n+                    epsilon=1e-6,\n                     name=model_name + \"_downsampling_layernorm_\" + str(i)),\n-        layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2,\n+                layers.Conv2D(\n+                    projection_dims[i + 1],\n+                    kernel_size=2,\n+                    strides=2,\n                     name=model_name + \"_downsampling_conv_\" + str(i)),\n             ],\n             name=model_name + \"_downsampling_block_\" + str(i),\n@@ -492,8 +505,7 @@ def ConvNeXt(depths,\n             file_suffix = \"_notop.h5\"\n             file_hash = WEIGHTS_HASHES[model_name][1]\n         file_name = model_name + file_suffix\n-    weights_path = utils.data_utils.get_file(\n-      file_name,\n+        weights_path = utils.data_utils.get_file(file_name,\n                                                  BASE_WEIGHTS_PATH + file_name,\n                                                  cache_subdir=\"models\",\n                                                  file_hash=file_hash)\n@@ -506,6 +518,7 @@ def ConvNeXt(depths,\n \n ## Instantiating variants ##\n \n+\n @keras_export(\"keras.applications.convnext.ConvNeXtTiny\",\n               \"keras.applications.ConvNeXtTiny\")\n def ConvNeXtTiny(model_name=\"convnext_tiny\",\n@@ -517,8 +530,7 @@ def ConvNeXtTiny(model_name=\"convnext_tiny\",\n                  pooling=None,\n                  classes=1000,\n                  classifier_activation=\"softmax\"):\n-  return ConvNeXt(\n-    depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n+    return ConvNeXt(depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n                     projection_dims=MODEL_CONFIGS[\"tiny\"][\"projection_dims\"],\n                     drop_path_rate=0.0,\n                     layer_scale_init_value=1e-6,\n@@ -545,8 +557,7 @@ def ConvNeXtSmall(model_name=\"convnext_small\",\n                   pooling=None,\n                   classes=1000,\n                   classifier_activation=\"softmax\"):\n-  return ConvNeXt(\n-    depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n+    return ConvNeXt(depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n                     projection_dims=MODEL_CONFIGS[\"small\"][\"projection_dims\"],\n                     drop_path_rate=0.0,\n                     layer_scale_init_value=1e-6,\n@@ -573,8 +584,7 @@ def ConvNeXtBase(model_name=\"convnext_base\",\n                  pooling=None,\n                  classes=1000,\n                  classifier_activation=\"softmax\"):\n-  return ConvNeXt(\n-    depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n+    return ConvNeXt(depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n                     projection_dims=MODEL_CONFIGS[\"base\"][\"projection_dims\"],\n                     drop_path_rate=0.0,\n                     layer_scale_init_value=1e-6,\n@@ -601,8 +611,7 @@ def ConvNeXtLarge(model_name=\"convnext_large\",\n                   pooling=None,\n                   classes=1000,\n                   classifier_activation=\"softmax\"):\n-  return ConvNeXt(\n-    depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n+    return ConvNeXt(depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n                     projection_dims=MODEL_CONFIGS[\"large\"][\"projection_dims\"],\n                     drop_path_rate=0.0,\n                     layer_scale_init_value=1e-6,\n@@ -629,8 +638,7 @@ def ConvNeXtXLarge(model_name=\"convnext_xlarge\",\n                    pooling=None,\n                    classes=1000,\n                    classifier_activation=\"softmax\"):\n-  return ConvNeXt(\n-    depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n+    return ConvNeXt(depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n                     projection_dims=MODEL_CONFIGS[\"xlarge\"][\"projection_dims\"],\n                     drop_path_rate=0.0,\n                     layer_scale_init_value=1e-6,\n@@ -657,7 +665,7 @@ ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     \"\"\"A placeholder method for backward compatibility.\n \n-  The preprocessing logic has been included in the efficientnet model\n+  The preprocessing logic has been included in the convnext model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n\n@@ -1609,7 +1609,7 @@ RegNetY320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY320\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n \n-  The preprocessing logic has been included in the efficientnet model\n+  The preprocessing logic has been included in the regnet model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3fa82006746757e21298a36e26513d09a743893", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 54 | Files Changed: 2 | Hunks: 27 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 100 | Churn Cumulative: 4978 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,6 +55,7 @@ WEIGHTS_HASHES = {\n       \"de3f8a54174130e0cecdc71583354753d557fcf1f4487331558e2a16ba0cfe05\"),\n }\n \n+\n MODEL_CONFIGS = {\n   \"tiny\": {\n     \"depths\": [3, 3, 9, 3],\n@@ -147,7 +148,6 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     A `keras.Model` instance.\n \"\"\"\n \n-\n class StochasticDepth(layers.Layer):\n   \"\"\"Stochastic Depth module.\n \n@@ -204,25 +204,24 @@ class LayerScale(layers.Layer):\n     self.projection_dim = projection_dim\n \n   def build(self, input_shape):\n-        self.gamma = tf.Variable(self.init_values * tf.ones(\n-            (self.projection_dim, )))\n+    self.gamma = tf.Variable(self.init_values * tf.ones((self.projection_dim,)))\n \n   def call(self, x):\n     return x * self.gamma\n \n   def get_config(self):\n     config = super().get_config()\n-        config.update({\n-            \"init_values\": self.init_values,\n-            \"projection_dim\": self.projection_dim\n-        })\n+    config.update(\n+      {\"init_values\": self.init_values, \"projection_dim\": self.projection_dim}\n+    )\n     return config\n \n-\n-def ConvNeXtBlock(projection_dim,\n+def ConvNeXtBlock(\n+    projection_dim,\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n-                  name=None):\n+    name=None\n+    ):\n   \"\"\"ConvNeXt block.\n \n   References:\n@@ -252,30 +251,23 @@ def ConvNeXtBlock(projection_dim,\n   def apply(inputs):\n     x = inputs\n \n-        x = layers.Conv2D(filters=projection_dim,\n-                          kernel_size=7,\n-                          padding=\"same\",\n-                          groups=projection_dim,\n-                          name=name + \"_depthwise_conv\")(x)\n-        x = layers.LayerNormalization(epsilon=1e-6,\n-                                      name=name + \"_layernorm\")(x)\n-        x = layers.Dense(4 * projection_dim,\n-                         name=name + \"_pointwise_conv_1\")(x)\n+    x = layers.Conv2D(\n+      filters=projection_dim, kernel_size=7, padding=\"same\",\n+      groups=projection_dim, name=name + \"_depthwise_conv\")(x)\n+    x = layers.LayerNormalization(epsilon=1e-6, name=name + \"_layernorm\")(x)\n+    x = layers.Dense(4 * projection_dim, name=name + \"_pointwise_conv_1\")(x)\n     x = layers.Activation(\"gelu\", name=name + \"_gelu\")(x)\n     x = layers.Dense(projection_dim, name=name + \"_pointwise_conv_2\")(x)\n \n     if layer_scale_init_value is not None:\n-            x = LayerScale(layer_scale_init_value,\n-                           projection_dim,\n+      x = LayerScale(layer_scale_init_value, projection_dim,\n         name=name + \"_layer_scale\")(x)\n     if drop_path_rate:\n-            layer = StochasticDepth(drop_path_rate,\n-                                    name=name + \"_stochastic_depth\")\n+      layer = StochasticDepth(drop_path_rate, name=name + \"_stochastic_depth\")\n     else:\n       layer = layers.Activation(\"linear\", name=name + \"_identity\")\n \n     return inputs + layer(x)\n-\n   return apply\n \n \n@@ -292,10 +284,11 @@ def PreStem(name=None):\n     name = \"prestem\" + str(backend.get_uid(\"prestem\"))\n \n   def apply(x):\n-        x = layers.Normalization(mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n-                                 variance=[(0.229 * 255)**2, (0.224 * 255)**2,\n-                                           (0.225 * 255)**2],\n-                                 name=name + \"_prestem_normalization\")(x)\n+    x = layers.Normalization(\n+      mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n+      variance=[(0.229 * 255) ** 2, (0.224 * 255) ** 2, (0.225 * 255) ** 2],\n+      name=name + \"_prestem_normalization\"\n+    )(x)\n     return x\n \n   return apply\n@@ -316,8 +309,8 @@ def Head(num_classes=1000, name=None):\n \n   def apply(x):\n     x = layers.GlobalAveragePooling2D(name=name + \"_head_gap\")(x)\n-        x = layers.LayerNormalization(epsilon=1e-6,\n-                                      name=name + \"_head_layernorm\")(x)\n+    x = layers.LayerNormalization(\n+      epsilon=1e-6, name=name + \"_head_layernorm\")(x)\n     x = layers.Dense(num_classes, name=name + \"_head_dense\")(x)\n     return x\n \n@@ -393,8 +386,7 @@ def ConvNeXt(depths,\n                      \"or the path to the weights file to be loaded.\")\n \n   if weights == \"imagenet\" and include_top and classes != 1000:\n-        raise ValueError(\n-            \"If using `weights` as `'imagenet'` with `include_top`\"\n+    raise ValueError(\"If using `weights` as `'imagenet'` with `include_top`\"\n                      \" as true, `classes` should be 1000\")\n \n   # Determine proper input shape.\n@@ -421,8 +413,7 @@ def ConvNeXt(depths,\n \n   x = inputs\n   if include_preprocessing:\n-        channel_axis = 3 if backend.image_data_format(\n-        ) == \"channels_last\" else 1\n+    channel_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n     num_channels = input_shape[channel_axis - 1]\n     if num_channels == 3:\n       x = PreStem(name=model_name)(x)\n@@ -430,12 +421,12 @@ def ConvNeXt(depths,\n   # Stem block.\n   stem = sequential.Sequential(\n     [\n-            layers.Conv2D(projection_dims[0],\n-                          kernel_size=4,\n-                          strides=4,\n+      layers.Conv2D(projection_dims[0], kernel_size=4, strides=4,\n         name=model_name + \"_stem_conv\"),\n-            layers.LayerNormalization(epsilon=1e-6,\n-                                      name=model_name + \"_stem_layernorm\"),\n+      layers.LayerNormalization(\n+              epsilon=1e-6,\n+              name=model_name + \"_stem_layernorm\"\n+      ),\n     ],\n     name=model_name + \"_stem\",\n   )\n@@ -448,13 +439,9 @@ def ConvNeXt(depths,\n   for i in range(num_downsample_layers):\n     downsample_layer = sequential.Sequential(\n       [\n-                layers.LayerNormalization(\n-                    epsilon=1e-6,\n+        layers.LayerNormalization(epsilon=1e-6,\n           name=model_name + \"_downsampling_layernorm_\" + str(i)),\n-                layers.Conv2D(\n-                    projection_dims[i + 1],\n-                    kernel_size=2,\n-                    strides=2,\n+        layers.Conv2D(projection_dims[i + 1], kernel_size=2, strides=2,\n           name=model_name + \"_downsampling_conv_\" + str(i)),\n       ],\n       name=model_name + \"_downsampling_block_\" + str(i),\n@@ -505,7 +492,8 @@ def ConvNeXt(depths,\n       file_suffix = \"_notop.h5\"\n       file_hash = WEIGHTS_HASHES[model_name][1]\n     file_name = model_name + file_suffix\n-        weights_path = utils.data_utils.get_file(file_name,\n+    weights_path = utils.data_utils.get_file(\n+      file_name,\n       BASE_WEIGHTS_PATH + file_name,\n       cache_subdir=\"models\",\n       file_hash=file_hash)\n@@ -518,7 +506,6 @@ def ConvNeXt(depths,\n \n ## Instantiating variants ##\n \n-\n @keras_export(\"keras.applications.convnext.ConvNeXtTiny\",\n               \"keras.applications.ConvNeXtTiny\")\n def ConvNeXtTiny(model_name=\"convnext_tiny\",\n@@ -530,7 +517,8 @@ def ConvNeXtTiny(model_name=\"convnext_tiny\",\n   pooling=None,\n   classes=1000,\n   classifier_activation=\"softmax\"):\n-    return ConvNeXt(depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n     projection_dims=MODEL_CONFIGS[\"tiny\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n@@ -557,7 +545,8 @@ def ConvNeXtSmall(model_name=\"convnext_small\",\n   pooling=None,\n   classes=1000,\n   classifier_activation=\"softmax\"):\n-    return ConvNeXt(depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n     projection_dims=MODEL_CONFIGS[\"small\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n@@ -584,7 +573,8 @@ def ConvNeXtBase(model_name=\"convnext_base\",\n   pooling=None,\n   classes=1000,\n   classifier_activation=\"softmax\"):\n-    return ConvNeXt(depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n     projection_dims=MODEL_CONFIGS[\"base\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n@@ -611,7 +601,8 @@ def ConvNeXtLarge(model_name=\"convnext_large\",\n   pooling=None,\n   classes=1000,\n   classifier_activation=\"softmax\"):\n-    return ConvNeXt(depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n     projection_dims=MODEL_CONFIGS[\"large\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n@@ -638,7 +629,8 @@ def ConvNeXtXLarge(model_name=\"convnext_xlarge\",\n   pooling=None,\n   classes=1000,\n   classifier_activation=\"softmax\"):\n-    return ConvNeXt(depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n+  return ConvNeXt(\n+    depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n     projection_dims=MODEL_CONFIGS[\"xlarge\"][\"projection_dims\"],\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n@@ -665,7 +657,7 @@ ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n \n-  The preprocessing logic has been included in the convnext model\n+  The preprocessing logic has been included in the efficientnet model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n\n@@ -1609,7 +1609,7 @@ RegNetY320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY320\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n \n-  The preprocessing logic has been included in the regnet model\n+  The preprocessing logic has been included in the efficientnet model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38254c44ab6128220935716004b8e5aa9f7b837d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2835 | Contributors (this commit): 3 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -657,7 +657,7 @@ ConvNeXtXLarge.__doc__ = BASE_DOCSTRING.format(name=\"ConvNeXtXLarge\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n \n-  The preprocessing logic has been included in the efficientnet model\n+  The preprocessing logic has been included in the convnext model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ac6e91c1b32282cd96d1efe116cc1ff5693cb29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2147 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1609,7 +1609,7 @@ RegNetY320.__doc__ = BASE_DOCSTRING.format(name=\"RegNetY320\")\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n   \"\"\"A placeholder method for backward compatibility.\n \n-  The preprocessing logic has been included in the efficientnet model\n+  The preprocessing logic has been included in the regnet model\n   implementation. Users are no longer required to call this method to normalize\n   the input data. This method does nothing and only kept as a placeholder to\n   align the API surface between old and new version of model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f54af1c18c0b46f3ab0d29cd96427352e023b09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 2845 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -38,19 +38,19 @@ from tensorflow.python.util.tf_export import keras_export\n BASE_WEIGHTS_PATH = \"https://storage.googleapis.com/tensorflow/keras-applications/convnext/\"\n \n WEIGHTS_HASHES = {\n-  \"tiny\":\n+  \"convnext_tiny\":\n     (\"8ae6e78ce2933352b1ef4008e6dd2f17bc40771563877d156bc6426c7cf503ff\",\n       \"d547c096cabd03329d7be5562c5e14798aa39ed24b474157cef5e85ab9e49ef1\"),\n-  \"small\":\n+  \"convnext_small\":\n     (\"ce1277d8f1ee5a0ef0e171469089c18f5233860ceaf9b168049cb9263fd7483c\",\n       \"6fc8009faa2f00c1c1dfce59feea9b0745eb260a7dd11bee65c8e20843da6eab\"),\n-  \"base\":\n+  \"convnext_base\":\n     (\"52cbb006d3dadd03f6e095a8ca1aca47aecdd75acb4bc74bce1f5c695d0086e6\",\n       \"40a20c5548a5e9202f69735ecc06c990e6b7c9d2de39f0361e27baeb24cb7c45\"),\n-  \"large\":\n+  \"convnext_large\":\n     (\"070c5ed9ed289581e477741d3b34beffa920db8cf590899d6d2c67fba2a198a6\",\n       \"96f02b6f0753d4f543261bc9d09bed650f24dd6bc02ddde3066135b63d23a1cd\"),\n-  \"xlarge\":\n+  \"convnext_xlarge\":\n     (\"c1f5ccab661354fc3a79a10fa99af82f0fbf10ec65cb894a3ae0815f17a889ee\",\n       \"de3f8a54174130e0cecdc71583354753d557fcf1f4487331558e2a16ba0cfe05\"),\n }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f3dbf93e61403adb8a055b053f016984c802c87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 2848 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -96,6 +96,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   For transfer learning use cases, make sure to read the\n   [guide to transfer learning & fine-tuning](\n     https://keras.io/guides/transfer_learning/).\n+  \n   The `base`, `large`, and `xlarge` models were first pre-trained on the\n   ImageNet-21k dataset and then fine-tuned on the ImageNet-1k dataset. The\n   pre-trained parameters of the models were assembled from the\n@@ -103,10 +104,12 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   sense of how these parameters were converted to Keras compatible parameters,\n   please refer to\n   [this repository](https://github.com/sayakpaul/keras-convnext-conversion).\n+  \n   Note: Each Keras Application expects a specific kind of input preprocessing.\n   For ConvNeXt, preprocessing is included in the model using a `Normalization`\n   layer.  ConvNeXt models expect their inputs to be float or uint8 tensors of\n   pixels with values in the [0-255] range.\n+  \n   When calling the `summary()` method after instantiating a ConvNeXt model,\n   prefer setting the `expand_nested` argument `summary()` to `True` to better\n   investigate the instantiated model.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c62e53f28a15d4766548cbe1eaff2cfbe8b877a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 865 | Lines Deleted: 874 | Files Changed: 260 | Hunks: 865 | Methods Changed: 855 | Complexity Δ (Sum/Max): 97/50 | Churn Δ: 1739 | Churn Cumulative: 175230 | Contributors (this commit): 315 | Commits (past 90d): 719 | Contributors (cumulative): 1372 | DMM Complexity: 0.0\n\nDIFF:\n@@ -167,7 +167,7 @@ def _FilterGoldenProtoDict(golden_proto_dict, omit_golden_symbols_map):\n class ApiCompatibilityTest(tf.test.TestCase):\n \n   def __init__(self, *args, **kwargs):\n-    super(ApiCompatibilityTest, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n \n     self._update_golden_warning = file_io.read_file_to_string(\n         _UPDATE_WARNING_FILE)\n\n@@ -119,7 +119,7 @@ class _DummyEagerGraph(threading.local):\n     # Constructors for classes subclassing threading.local run once\n     # per thread accessing something in the class. Thus, each thread will\n     # get a different key.\n-    super(_DummyEagerGraph, self).__init__()\n+    super().__init__()\n     self.key = _DummyEagerGraph._WeakReferencableClass()\n     self.learning_phase_is_set = False\n \n\n@@ -26,7 +26,7 @@ class AntirectifierBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for Antirectifier using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(AntirectifierBenchmark, self).__init__()\n+    super().__init__()\n     (self.x_train, self.y_train), _ = tf.keras.datasets.mnist.load_data()\n     self.x_train = self.x_train.reshape(-1, 784)\n     self.x_train = self.x_train.astype(\"float32\") / 255\n@@ -131,7 +131,7 @@ class Antirectifier(tf.keras.layers.Layer):\n   \"\"\"Build simple custom layer.\"\"\"\n \n   def __init__(self, initializer=\"he_normal\", **kwargs):\n-    super(Antirectifier, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.initializer = tf.keras.initializers.get(initializer)\n \n   def build(self, input_shape):\n@@ -153,7 +153,7 @@ class Antirectifier(tf.keras.layers.Layer):\n \n   def get_config(self):\n     # Implement get_config to enable serialization. This is optional.\n-    base_config = super(Antirectifier, self).get_config()\n+    base_config = super().get_config()\n     config = {\"initializer\": tf.keras.initializers.serialize(self.initializer)}\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -26,7 +26,7 @@ class BidirectionalLSTMBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for Bidirectional LSTM using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(BidirectionalLSTMBenchmark, self).__init__()\n+    super().__init__()\n     self.max_feature = 20000\n     self.max_len = 200\n     (self.imdb_x, self.imdb_y), _ = tf.keras.datasets.imdb.load_data(\n\n@@ -26,7 +26,7 @@ class Cifar10CNNBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for CNN using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(Cifar10CNNBenchmark, self).__init__()\n+    super().__init__()\n     self.num_classes = 10\n     (self.x_train, self.y_train), _ = tf.keras.datasets.cifar10.load_data()\n     self.x_train = self.x_train.astype('float32') / 255\n\n@@ -28,7 +28,7 @@ class ConvMnistBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for Convnet using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(ConvMnistBenchmark, self).__init__()\n+    super().__init__()\n     self.num_classes = 10\n     self.input_shape = (28, 28, 1)\n     (self.x_train, self.y_train), _ = tf.keras.datasets.mnist.load_data()\n\n@@ -30,7 +30,7 @@ class CustomMnistBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for custom training loop using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(CustomMnistBenchmark, self).__init__()\n+    super().__init__()\n     self.num_classes = 10\n     self.input_shape = (28, 28, 1)\n     self.epochs = 15\n\n@@ -26,7 +26,7 @@ class HierarchicalRNNBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for Hierarchical RNN using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(HierarchicalRNNBenchmark, self).__init__()\n+    super().__init__()\n     self.num_classes = 10\n     self.row_hidden, self.col_hidden = 128, 128\n     (self.x_train, self.y_train), _ = tf.keras.datasets.mnist.load_data()\n\n@@ -26,7 +26,7 @@ class IRNNMnistBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for IRNN using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(IRNNMnistBenchmark, self).__init__()\n+    super().__init__()\n     self.num_classes = 10\n     self.hidden_units = 100\n     self.learning_rate = 1e-6\n\n@@ -28,7 +28,7 @@ class MLPReutersBenchmark(tf.test.Benchmark):\n   \"\"\"Benchmarks for MLP using `tf.test.Benchmark`.\"\"\"\n \n   def __init__(self):\n-    super(MLPReutersBenchmark, self).__init__()\n+    super().__init__()\n     self.max_words = 1000\n     (self.x_train, self.y_train), _ = tf.keras.datasets.reuters.load_data(\n         num_words=self.max_words)\n\n@@ -28,7 +28,7 @@ class TextWithTransformerBenchmark(tf.test.Benchmark):\n   \"\"\"\n \n   def __init__(self):\n-    super(TextWithTransformerBenchmark, self).__init__()\n+    super().__init__()\n     self.max_feature = 20000\n     self.max_len = 200\n     (self.imdb_x, self.imdb_y), _ = tf.keras.datasets.imdb.load_data(\n@@ -143,7 +143,7 @@ class MultiHeadSelfAttention(tf.keras.layers.Layer):\n   \"\"\"Implement multi head self attention as a Keras layer.\"\"\"\n \n   def __init__(self, embed_dim, num_heads=8):\n-    super(MultiHeadSelfAttention, self).__init__()\n+    super().__init__()\n     self.embed_dim = embed_dim\n     self.num_heads = num_heads\n     if embed_dim % num_heads != 0:\n@@ -195,7 +195,7 @@ class TransformerBlock(tf.keras.layers.Layer):\n   \"\"\"Implement a Transformer block as a layer.\"\"\"\n \n   def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n-    super(TransformerBlock, self).__init__()\n+    super().__init__()\n     self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n     self.ffn = tf.keras.Sequential([\n         tf.keras.layers.Dense(ff_dim, activation='relu'),\n@@ -219,7 +219,7 @@ class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n   \"\"\"Implement embedding layer.\"\"\"\n \n   def __init__(self, maxlen, vocab_size, embed_dim):\n-    super(TokenAndPositionEmbedding, self).__init__()\n+    super().__init__()\n     self.token_emb = tf.keras.layers.Embedding(\n         input_dim=vocab_size, output_dim=embed_dim)\n     self.pos_emb = tf.keras.layers.Embedding(\n\n@@ -27,7 +27,7 @@ from tensorflow.python.eager.context import get_executor\n class SubclassedKerasModel(tf.keras.Model):\n \n   def __init__(self, initializer=\"ones\"):\n-    super(SubclassedKerasModel, self).__init__()\n+    super().__init__()\n     self.layer_a = tf.keras.layers.Dense(\n         64, kernel_initializer=initializer, bias_initializer=\"zeros\")\n     self.layer_b = tf.keras.layers.Dense(\n\n@@ -895,7 +895,7 @@ class BaseLogger(Callback):\n   \"\"\"\n \n   def __init__(self, stateful_metrics=None):\n-    super(BaseLogger, self).__init__()\n+    super().__init__()\n     self.stateful_metrics = set(stateful_metrics or [])\n \n   def on_epoch_begin(self, epoch, logs=None):\n@@ -936,7 +936,7 @@ class TerminateOnNaN(Callback):\n   \"\"\"\n \n   def __init__(self):\n-    super(TerminateOnNaN, self).__init__()\n+    super().__init__()\n     self._supports_tf_logs = True\n \n   def on_batch_end(self, batch, logs=None):\n@@ -968,7 +968,7 @@ class ProgbarLogger(Callback):\n   \"\"\"\n \n   def __init__(self, count_mode='samples', stateful_metrics=None):\n-    super(ProgbarLogger, self).__init__()\n+    super().__init__()\n     self._supports_tf_logs = True\n     if count_mode == 'samples':\n       self.use_steps = False\n@@ -1141,7 +1141,7 @@ class History(Callback):\n   \"\"\"\n \n   def __init__(self):\n-    super(History, self).__init__()\n+    super().__init__()\n     self.history = {}\n \n   def on_train_begin(self, logs=None):\n@@ -1275,7 +1275,7 @@ class ModelCheckpoint(Callback):\n                options=None,\n                initial_value_threshold=None,\n                **kwargs):\n-    super(ModelCheckpoint, self).__init__()\n+    super().__init__()\n     self._supports_tf_logs = True\n     self.monitor = monitor\n     self.verbose = verbose\n@@ -1667,7 +1667,7 @@ class BackupAndRestore(Callback):\n   \"\"\"\n \n   def __init__(self, backup_dir):\n-    super(BackupAndRestore, self).__init__()\n+    super().__init__()\n     self.backup_dir = backup_dir\n     self._supports_tf_logs = True\n     self._supported_strategies = (\n@@ -1738,7 +1738,7 @@ class BackupAndRestoreExperimental(BackupAndRestore):\n         '`tf.keras.callbacks.experimental.BackupAndRestore` endpoint is '\n         'deprecated and will be removed in a future release. Please use '\n         '`tf.keras.callbacks.BackupAndRestore`.')\n-    super(BackupAndRestoreExperimental, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n \n \n @keras_export('keras.callbacks.EarlyStopping')\n@@ -1805,7 +1805,7 @@ class EarlyStopping(Callback):\n                mode='auto',\n                baseline=None,\n                restore_best_weights=False):\n-    super(EarlyStopping, self).__init__()\n+    super().__init__()\n \n     self.monitor = monitor\n     self.patience = patience\n@@ -1922,7 +1922,7 @@ class RemoteMonitor(Callback):\n                field='data',\n                headers=None,\n                send_as_json=False):\n-    super(RemoteMonitor, self).__init__()\n+    super().__init__()\n \n     self.root = root\n     self.path = path\n@@ -1995,7 +1995,7 @@ class LearningRateScheduler(Callback):\n   \"\"\"\n \n   def __init__(self, schedule, verbose=0):\n-    super(LearningRateScheduler, self).__init__()\n+    super().__init__()\n     self.schedule = schedule\n     self.verbose = verbose\n \n@@ -2210,7 +2210,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                embeddings_freq=0,\n                embeddings_metadata=None,\n                **kwargs):\n-    super(TensorBoard, self).__init__()\n+    super().__init__()\n     self._supports_tf_logs = True\n     self._validate_kwargs(kwargs)\n \n@@ -2690,7 +2690,7 @@ class ReduceLROnPlateau(Callback):\n                cooldown=0,\n                min_lr=0,\n                **kwargs):\n-    super(ReduceLROnPlateau, self).__init__()\n+    super().__init__()\n \n     self.monitor = monitor\n     if factor >= 1.0:\n@@ -2797,7 +2797,7 @@ class CSVLogger(Callback):\n     self.writer = None\n     self.keys = None\n     self.append_header = True\n-    super(CSVLogger, self).__init__()\n+    super().__init__()\n \n   def on_train_begin(self, logs=None):\n     if self.append:\n@@ -2913,7 +2913,7 @@ class LambdaCallback(Callback):\n                on_train_begin=None,\n                on_train_end=None,\n                **kwargs):\n-    super(LambdaCallback, self).__init__()\n+    super().__init__()\n     self.__dict__.update(kwargs)\n     if on_epoch_begin is not None:\n       self.on_epoch_begin = on_epoch_begin\n\n@@ -288,7 +288,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n       \"\"\"A simple metric that adds all the one's in `y_true`.\"\"\"\n \n       def __init__(self, name='add_all_ones', **kwargs):\n-        super(AddAllOnes, self).__init__(name=name, **kwargs)\n+        super().__init__(name=name, **kwargs)\n         self.total = self.add_weight(name='total', initializer='zeros')\n \n       def update_state(self, y_true, y_pred, sample_weight=None):\n@@ -1993,7 +1993,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n     class MyCallbackWithTFBatchHooks(keras.callbacks.Callback):\n \n       def __init__(self):\n-        super(MyCallbackWithTFBatchHooks, self).__init__()\n+        super().__init__()\n         self._supports_tf_logs = True\n \n     class MyCallbackWithoutBatchHooks(keras.callbacks.Callback):\n@@ -2051,7 +2051,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n     class MutateTensorFlowLogs(CallAllHooks):\n \n       def __init__(self):\n-        super(MutateTensorFlowLogs, self).__init__()\n+        super().__init__()\n         self._supports_tf_logs = True\n \n       def _run(self, *args, logs=None):\n@@ -2067,7 +2067,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n     class AssertTensorFlowLogs(AssertNumpyLogs):\n \n       def __init__(self):\n-        super(AssertTensorFlowLogs, self).__init__()\n+        super().__init__()\n         self._supports_tf_logs = True\n \n     cb_list = keras.callbacks.CallbackList([\n@@ -2202,7 +2202,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n     class MyCallback(keras.callbacks.Callback):\n \n       def __init__(self):\n-        super(MyCallback, self).__init__()\n+        super().__init__()\n         self.batch_counter = 0\n \n       def on_train_batch_end(self, batch, logs=None):\n@@ -2227,7 +2227,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n     class TestingCallbackList(keras.callbacks.CallbackList):\n \n       def __init__(self, *args, **kwargs):\n-        super(TestingCallbackList, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         if ((not isinstance(self.callbacks[0], CustomCallback)) or\n             (not isinstance(self.callbacks[1], keras.callbacks.History)) or\n             (not isinstance(self.callbacks[2], keras.callbacks.ProgbarLogger))):\n@@ -3207,7 +3207,7 @@ class SummaryOpsTest(tf.test.TestCase):\n     class SimpleSubclass(keras.Model):\n \n       def __init__(self):\n-        super(SimpleSubclass, self).__init__(name='subclass')\n+        super().__init__(name='subclass')\n         self.dense = Dense(10, input_shape=(100,))\n         self.activation = Activation('relu', name='my_relu')\n \n\n@@ -223,7 +223,7 @@ class TestDistributionStrategyDnnCorrectness(tf.test.TestCase,\n   \"\"\"Test custom training loop correctness with a simple DNN model.\"\"\"\n \n   def setUp(self):\n-    super(TestDistributionStrategyDnnCorrectness, self).setUp()\n+    super().setUp()\n     np.random.seed(_RANDOM_SEED)\n     tf.compat.v1.set_random_seed(_RANDOM_SEED)\n \n\n@@ -30,7 +30,7 @@ from keras.optimizers.optimizer_v2 import gradient_descent\n class CustomModel(tf.Module):\n \n   def __init__(self, name=None):\n-    super(CustomModel, self).__init__(name=name)\n+    super().__init__(name=name)\n     with self.name_scope:\n       self._layers = [\n           keras.layers.Dense(4, name=\"dense\"),\n@@ -108,7 +108,7 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n       class KerasSubclassModel(keras.Model):\n \n         def __init__(self):\n-          super(KerasSubclassModel, self).__init__()\n+          super().__init__()\n           self.l = keras.layers.Dense(4, name=\"dense\")\n \n         def call(self, x):\n@@ -486,7 +486,7 @@ class KerasModelsXLATest(tf.test.TestCase, parameterized.TestCase):\n     class CustomDense(keras.layers.Layer):\n \n       def __init__(self, num_outputs):\n-        super(CustomDense, self).__init__()\n+        super().__init__()\n         self.num_outputs = num_outputs\n \n       def build(self, input_shape):\n\n@@ -73,7 +73,7 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     class MyCallback(callbacks_lib.Callback):\n \n       def __init__(self):\n-        super(MyCallback, self).__init__()\n+        super().__init__()\n         # Fetches the RemoteValues if necessary.\n         self._supports_tf_logs = True\n \n\n@@ -65,7 +65,7 @@ def simple_subclassed_model(num_labels=_NUM_CLASS):\n   class _SimpleMLP(keras.Model):\n \n     def __init__(self, num_labels):\n-      super(_SimpleMLP, self).__init__()\n+      super().__init__()\n       self.dense = keras.layers.Dense(num_labels)\n \n     def call(self, inputs):\n@@ -302,7 +302,7 @@ def strategy_and_optimizer_combinations():\n class BatchCountingCB(keras.callbacks.Callback):\n \n   def __init__(self):\n-    super(BatchCountingCB, self).__init__()\n+    super().__init__()\n     self.train_begin_batches = []\n     self.train_end_batches = []\n     self.test_begin_batches = []\n@@ -830,7 +830,7 @@ class TestDistributionStrategyWithNumpyArrays(tf.test.TestCase,\n         # Gradients w.r.t. extra_weights are None\n         self.extra_weight_1 = self.add_weight('extra_weight_1', shape=(),\n                                               initializer='ones')\n-        super(DenseWithExtraWeight, self).build(input_shape)\n+        super().build(input_shape)\n         self.extra_weight_2 = self.add_weight('extra_weight_2', shape=(),\n                                               initializer='ones')\n \n@@ -1572,7 +1572,7 @@ class TestDistributionStrategyWithDatasetsFile(tf.test.TestCase,\n                                                parameterized.TestCase):\n \n   def setUp(self):\n-    super(TestDistributionStrategyWithDatasetsFile, self).setUp()\n+    super().setUp()\n     self.input_file_name = os.path.join(self.get_temp_dir(), 'input.tfrecord')\n     inputs = np.zeros((20, 3), dtype=np.float32)\n     input_dataset = tf.data.Dataset.from_tensor_slices(inputs)\n@@ -2308,7 +2308,7 @@ class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n       \"\"\"Create a ragged tensor based on a given dense tensor.\"\"\"\n \n       def __init__(self, padding, ragged_rank=1, **kwargs):\n-        super(ToRagged, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self._padding = padding\n         self._ragged_rank = ragged_rank\n \n@@ -2561,7 +2561,7 @@ class DeterministicModel(keras.Model):\n   \"\"\"\n \n   def __init__(self, strategy):\n-    super(DeterministicModel, self).__init__()\n+    super().__init__()\n     self.x = None\n     self.strategy = strategy\n \n\n@@ -225,7 +225,7 @@ class TestDistributionStrategyDnnMetricEvalCorrectness(\n class SubclassedModel(keras.Model):\n \n   def __init__(self, initial_weights, input_shapes):\n-    super(SubclassedModel, self).__init__()\n+    super().__init__()\n     self.dense1 = keras.layers.Dense(10, activation='relu', input_shape=(1,))\n     self.dense2 = keras.layers.Dense(\n         10, activation='relu', kernel_regularizer=keras.regularizers.l2(1e-4))\n\n@@ -123,13 +123,11 @@ class DistributionStrategySiameseEmbeddingModelCorrectnessTest(\n                max_word_id=19,\n                num_classes=2):\n     features_a, labels_a, _ = (\n-        super(DistributionStrategySiameseEmbeddingModelCorrectnessTest,\n-              self).get_data(count, min_words, max_words, max_word_id,\n+        super().get_data(count, min_words, max_words, max_word_id,\n                              num_classes))\n \n     features_b, labels_b, _ = (\n-        super(DistributionStrategySiameseEmbeddingModelCorrectnessTest,\n-              self).get_data(count, min_words, max_words, max_word_id,\n+        super().get_data(count, min_words, max_words, max_word_id,\n                              num_classes))\n \n     y_train = np.zeros((count, 1), dtype=np.float32)\n\n@@ -142,7 +142,7 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n     class MetricLayer(base_layer.Layer):\n \n       def __init__(self):\n-        super(MetricLayer, self).__init__(name=\"metric_layer\")\n+        super().__init__(name=\"metric_layer\")\n         self.sum = metrics.Sum(name=\"sum\")\n         # Using aggregation for jit_compile results in failure. Thus only set\n         # aggregation for PS Strategy for multi-gpu tests.\n\n@@ -26,7 +26,7 @@ class KerasSaveLoadTest(test_base.TestSavedModelBase):\n \n   def setUp(self):\n     self._root_dir = 'keras_save_load'\n-    super(KerasSaveLoadTest, self).setUp()\n+    super().setUp()\n \n   def _save_model(self, model, saved_dir):\n     model.save(saved_dir, save_format='tf')\n\n@@ -297,7 +297,7 @@ class TestDistributionStrategyErrorCases(tf.test.TestCase, parameterized.TestCas\n       class _SimpleMLP(keras.Model):\n \n         def __init__(self, num_labels):\n-          super(_SimpleMLP, self).__init__()\n+          super().__init__()\n           self.dense = keras.layers.Dense(num_labels)\n \n         def call(self, inputs):\n\n@@ -35,7 +35,7 @@ class MiniModel(keras_training.Model):\n   \"\"\"\n \n   def __init__(self):\n-    super(MiniModel, self).__init__(name=\"\")\n+    super().__init__(name=\"\")\n     self.fc = keras_core.Dense(1, name=\"fc\", kernel_initializer=\"ones\",\n                                bias_initializer=\"ones\")\n \n\n@@ -113,7 +113,7 @@ class MultiWorkerVerificationCallback(callbacks.Callback):\n       num_epoch: Number of epochs this Callback is expected to be called for.\n       num_worker: Number of workers this Callback is expected to be called from.\n     \"\"\"\n-    super(MultiWorkerVerificationCallback, self).__init__()\n+    super().__init__()\n     self._num_epoch = num_epoch\n     self._num_worker = num_worker\n     self._task_dict = {\n\n@@ -34,7 +34,7 @@ class SavedModelSaveAndLoadTest(test_base.TestSavedModelBase):\n \n   def setUp(self):\n     self._root_dir = 'saved_model_save_load'\n-    super(SavedModelSaveAndLoadTest, self).setUp()\n+    super().setUp()\n \n   def _save_model(self, model, saved_dir):\n     save.save_model(model, saved_dir, save_format='tf')\n\n@@ -29,7 +29,7 @@ class SavedModelKerasModelTest(test_base.TestSavedModelBase):\n \n   def setUp(self):\n     self._root_dir = 'saved_model_save_load'\n-    super(SavedModelKerasModelTest, self).setUp()\n+    super().setUp()\n \n   def _save_model(self, model, saved_dir):\n     tf.saved_model.save(model, saved_dir)\n@@ -89,7 +89,7 @@ class SavedModelTFModuleTest(test_base.TestSavedModelBase):\n \n   def setUp(self):\n     self._root_dir = 'saved_model_save_load'\n-    super(SavedModelTFModuleTest, self).setUp()\n+    super().setUp()\n \n   def _train_model(self, model, x_train, y_train, batch_size):\n     pass\n\n@@ -112,7 +112,7 @@ class TestSavedModelBase(tf.test.TestCase, parameterized.TestCase):\n     np.random.seed(_RANDOM_SEED)\n     tf.compat.v1.set_random_seed(_RANDOM_SEED)\n     self._root_dir = 'base'\n-    super(TestSavedModelBase, self).setUp()\n+    super().setUp()\n \n   def _save_model(self, model, saved_dir):\n     \"\"\"Save the given model to the given saved_dir.\n\n@@ -275,4 +275,4 @@ class SidecarEvaluatorExperimental(SidecarEvaluator):\n         '`tf.keras.experimental.SidecarEvaluator` endpoint is '\n         'deprecated and will be removed in a future release. Please use '\n         '`tf.keras.utils.SidecarEvaluator`.')\n-    super(SidecarEvaluatorExperimental, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n\n@@ -87,7 +87,7 @@ class SimpleSequentialModel(model_collection_base.ModelAndInput):\n class _SimpleModel(keras.Model):\n \n   def __init__(self):\n-    super(_SimpleModel, self).__init__()\n+    super().__init__()\n     self._dense_layer = keras.layers.Dense(5, dtype=tf.float32)\n \n   def call(self, inputs):\n\n@@ -27,7 +27,7 @@ import tensorflow.compat.v2 as tf\n class InitializersTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(InitializersTest, self).setUp()\n+    super().setUp()\n     global_ids = test_util.create_device_ids_array((2, 2))\n     local_device_ids = np.ravel(global_ids).tolist()\n     mesh_dict = {\n\n@@ -27,7 +27,7 @@ import tensorflow.compat.v2 as tf\n class LayersTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(LayersTest, self).setUp()\n+    super().setUp()\n     backend.enable_tf_random_generator()\n     tf_utils.set_random_seed(1337)\n     global_ids = test_util.create_device_ids_array((2, 2))\n\n@@ -29,7 +29,7 @@ from keras.dtensor.tests import test_util\n class LayoutMapTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(LayoutMapTest, self).setUp()\n+    super().setUp()\n     backend.enable_tf_random_generator()\n     tf_utils.set_random_seed(1337)\n     global_ids = test_util.create_device_ids_array((2, 2))\n@@ -149,7 +149,7 @@ class SubclassModel(tf.keras.Model):\n class ObjectPathMappingTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(ObjectPathMappingTest, self).setUp()\n+    super().setUp()\n     backend.enable_tf_random_generator()\n     tf_utils.set_random_seed(1337)\n     global_ids = test_util.create_device_ids_array((2, 2))\n\n@@ -131,7 +131,7 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n      unique_id) = _infer_shape_dtype_and_create_handle(initial_value, shape,\n                                                        dtype, name)\n \n-    super(LazyInitVariable, self).__init__(\n+    super().__init__(\n         distribute_strategy=distribute_strategy,\n         initial_value=initial_value,\n         shape=shape,\n@@ -175,7 +175,7 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n            initial_value, self._shape, self._dtype, self._name)\n       self.initialize()\n \n-    super(LazyInitVariable, self).__init__(\n+    super().__init__(\n         trainable=self._trainable,\n         shape=shape,\n         dtype=dtype,\n\n@@ -26,7 +26,7 @@ import tensorflow.compat.v2 as tf\n class MetricsTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(MetricsTest, self).setUp()\n+    super().setUp()\n     global_ids = test_util.create_device_ids_array((2, 2))\n     local_device_ids = np.ravel(global_ids).tolist()\n     mesh_dict = {\n\n@@ -25,7 +25,7 @@ import tensorflow.compat.v2 as tf\n class OptimizersTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(OptimizersTest, self).setUp()\n+    super().setUp()\n     global_ids = test_util.create_device_ids_array((2, 2))\n     local_device_ids = np.ravel(global_ids).tolist()\n     mesh_dict = {\n\n@@ -27,7 +27,7 @@ import tensorflow.compat.v2 as tf\n class UtilsTest(test_util.DTensorBaseTest):\n \n   def setUp(self):\n-    super(UtilsTest, self).setUp()\n+    super().setUp()\n     global_ids = test_util.create_device_ids_array((2, 2))\n     local_device_ids = np.ravel(global_ids).tolist()\n     mesh_dict = {\n\n@@ -37,7 +37,7 @@ from keras.utils import control_flow_util\n class DynamicLayer(base_layer.Layer):\n \n   def __init__(self, dynamic=False, **kwargs):\n-    super(DynamicLayer, self).__init__(dynamic=dynamic, **kwargs)\n+    super().__init__(dynamic=dynamic, **kwargs)\n \n   def call(self, inputs):\n     samples = tf.TensorArray(\n@@ -120,7 +120,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class BuildCounter(base_layer.Layer):\n \n       def __init__(self, *args, **kwargs):  # pylint: disable=redefined-outer-name\n-        super(BuildCounter, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.build_counter = 0\n \n       def build(self, input_shape):\n@@ -173,7 +173,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyModel(training_lib.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__(dynamic=True)\n+        super().__init__(dynamic=True)\n         self.layer1 = layers.Dense(3)\n         self.layer2 = layers.Dense(3)\n \n@@ -195,7 +195,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyModel(training_lib.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__(dynamic=True)\n+        super().__init__(dynamic=True)\n         self.layer1 = layers.Dense(3)\n         self.layer2 = layers.Dense(3)\n \n@@ -287,7 +287,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class TestLayer(base_layer.Layer):\n \n       def __init__(self):\n-        super(TestLayer, self).__init__()\n+        super().__init__()\n         self.default_weight = self.add_weight()\n         self.weight_without_name = self.add_weight(shape=(3, 4))\n         self.regularized_weight_without_name = self.add_weight(\n@@ -350,7 +350,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class ComputeSum(base_layer.Layer):\n \n       def __init__(self):\n-        super(ComputeSum, self).__init__()\n+        super().__init__()\n         self.total = tf.Variable(\n             initial_value=tf.zeros((1, 1)), trainable=False)\n         if not tf.executing_eagerly():\n@@ -415,7 +415,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class RawVariableLayer(base_layer.Layer):\n \n       def __init__(self, **kwargs):\n-        super(RawVariableLayer, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         # Test variables in nested structure.\n         self.var_list = [tf.Variable(1.), {'a': tf.Variable(2.)}]\n \n@@ -612,7 +612,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self, my_kwarg='default', **kwargs):\n-        super(MyLayer, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.my_kwarg = my_kwarg\n \n     # `__init__` includes kwargs but `get_config` is not overridden, so\n@@ -623,11 +623,11 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyLayerNew(base_layer.Layer):\n \n       def __init__(self, my_kwarg='default', **kwargs):\n-        super(MyLayerNew, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.my_kwarg = my_kwarg\n \n       def get_config(self):\n-        config = super(MyLayerNew, self).get_config()\n+        config = super().get_config()\n         config['my_kwarg'] = self.my_kwarg\n         return config\n \n@@ -638,7 +638,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyLayerNew2(base_layer.Layer):\n \n       def __init__(self, name='MyLayerName', dtype=None, **kwargs):  # pylint:disable=redefined-outer-name\n-        super(MyLayerNew2, self).__init__(name=name, dtype=dtype, **kwargs)\n+        super().__init__(name=name, dtype=dtype, **kwargs)\n \n     # Check that if the kwargs in `__init__` are base layer constructor\n     # arguments, no error is thrown:\n@@ -718,7 +718,7 @@ class BaseLayerTest(test_combinations.TestCase):\n \n       def build(self, input_shape):\n         self.add_weight('w', shape=input_shape[1:])\n-        super(CustomLayer, self).build(input_shape)\n+        super().build(input_shape)\n \n     layer = CustomLayer()\n     self.assertFalse(layer.built)\n@@ -763,7 +763,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerNoTrainingArg(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerNoTrainingArg, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs):\n@@ -772,7 +772,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingMissing(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingMissing, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, training):\n@@ -784,7 +784,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingNone(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingNone, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, training=None):\n@@ -796,7 +796,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingFalse(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingFalse, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, training=False):\n@@ -808,7 +808,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingTrue(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingTrue, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, training=True):\n@@ -829,7 +829,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerNoTrainingArg(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerNoTrainingArg, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs):\n@@ -838,7 +838,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingMissing(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingMissing, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, *, training):\n@@ -850,7 +850,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingNone(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingNone, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, *, training=None):\n@@ -862,7 +862,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingFalse(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingFalse, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, *, training=False):\n@@ -874,7 +874,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class CustomLayerDefaultTrainingTrue(base_layer.Layer):\n \n       def __init__(self, nested_layer=None):\n-        super(CustomLayerDefaultTrainingTrue, self).__init__()\n+        super().__init__()\n         self._nested_layer = nested_layer or tf.identity\n \n       def call(self, inputs, *, training=True):\n@@ -968,7 +968,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyModule(tf.Module):\n \n       def __init__(self):\n-        super(MyModule, self).__init__()\n+        super().__init__()\n         self.v1 = tf.Variable(1., trainable=True, name='v1')\n         self.v2 = tf.Variable(2., trainable=False, name='v2')\n \n@@ -978,7 +978,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self, **kwargs):\n-        super(MyLayer, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.my_modules = {}\n         self.my_modules['a'] = MyModule()\n \n@@ -998,7 +998,7 @@ class BaseLayerTest(test_combinations.TestCase):\n     class MyModel(training_lib.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.my_modules = []\n         self.my_modules.append(MyModule())\n \n@@ -1158,7 +1158,7 @@ class NestedTrackingTest(tf.test.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self):\n-        super(MyLayer, self).__init__()\n+        super().__init__()\n         self.dense1 = layers.Dense(1)\n         self.dense2 = layers.BatchNormalization()\n \n@@ -1214,7 +1214,7 @@ class NestedTrackingTest(tf.test.TestCase):\n         self.v1 = self.add_weight('v1', shape=())\n \n       def __init__(self):\n-        super(MyLayer, self).__init__()\n+        super().__init__()\n         self.ul1 = UpdateAndLossLayer()\n         self.ul2 = UpdateAndLossLayer()\n \n@@ -1264,7 +1264,7 @@ class NestedTrackingTest(tf.test.TestCase):\n     class LayerWithClassAttribute(base_layer.Layer):\n \n       def __init__(self):\n-        super(LayerWithClassAttribute, self).__init__()\n+        super().__init__()\n         self.layer_fn = layers.Dense\n \n     layer = LayerWithClassAttribute()\n@@ -1302,7 +1302,7 @@ class NestedTrackingTest(tf.test.TestCase):\n     class Sequential(training_lib.Model):\n \n       def __init__(self):\n-        super(Sequential, self).__init__()\n+        super().__init__()\n         self.dense_layers = [layers.Dense(10), layers.Dense(5)]\n \n       def call(self, inputs):\n@@ -1342,7 +1342,7 @@ class NameScopingTest(test_combinations.TestCase):\n     class NestedLayer(base_layer.Layer):\n \n       def __init__(self, name='OuterName'):\n-        super(NestedLayer, self).__init__(name=name)\n+        super().__init__(name=name)\n         self.dense = layers.Dense(10, name='InnerName')\n \n       def call(self, inputs):\n@@ -1639,8 +1639,7 @@ class AutographControlFlowTest(test_combinations.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self):\n-        super(MyLayer,\n-              self).__init__(dynamic=test_utils.should_run_eagerly())\n+        super().__init__(dynamic=test_utils.should_run_eagerly())\n \n       def call(self, inputs, training=None):\n         if training:\n@@ -1684,8 +1683,7 @@ class AutographControlFlowTest(test_combinations.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self):\n-        super(MyLayer,\n-              self).__init__(dynamic=test_utils.should_run_eagerly())\n+        super().__init__(dynamic=test_utils.should_run_eagerly())\n \n       def call(self, inputs, training=None):\n         if training:\n@@ -1710,7 +1708,7 @@ class AutographControlFlowTest(test_combinations.TestCase):\n     class TestModel(training_lib.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(\n+        super().__init__(\n             name='test_model', dynamic=test_utils.should_run_eagerly())\n         self.layer = layers.Dense(2, activity_regularizer='l2')\n \n@@ -1740,7 +1738,7 @@ class AutographControlFlowTest(test_combinations.TestCase):\n     class TestModel(training_lib.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(\n+        super().__init__(\n             name='test_model', dynamic=test_utils.should_run_eagerly())\n         self.layer = layers.TimeDistributed(\n             layers.Dense(2, activity_regularizer='l2'), input_shape=(3, 4))\n@@ -1904,7 +1902,7 @@ class DTypeTest(test_combinations.TestCase):\n \n       def __init__(self, *args, **kwargs):\n         kwargs['autocast'] = False\n-        super(IdentityLayerWithoutAutocast, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n \n     layer = IdentityLayerWithoutAutocast(dtype='float64')\n     self.assertEqual(layer(self._const('float32')).dtype, 'float32')\n\n@@ -50,7 +50,7 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n   _must_restore_from_config = True\n \n   def __init__(self, **kwargs):\n-    super(PreprocessingLayer, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self._is_compiled = False\n     self._is_adapted = False\n \n\n@@ -29,7 +29,7 @@ import tensorflow.compat.v2 as tf\n class AddingPreprocessingLayer(base_preprocessing_layer.PreprocessingLayer):\n \n   def build(self, input_shape):\n-    super(AddingPreprocessingLayer, self).build(input_shape)\n+    super().build(input_shape)\n     self.sum = tf.Variable(0., dtype=tf.float32)\n \n   def update_state(self, data):\n\n@@ -53,7 +53,7 @@ class NestedControlFlowLayer(base_layer.Layer):\n   \"\"\"Layer nested with a control flow layer.\"\"\"\n \n   def __init__(self, **kwargs):\n-    super(NestedControlFlowLayer, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.layer = ControlFlowLayer1()\n \n   def call(self, inputs):\n@@ -74,7 +74,7 @@ class NestedControlFlowModel(keras.Model):\n   \"\"\"Model with an `if` condition in call using a control flow layer.\"\"\"\n \n   def __init__(self, **kwargs):\n-    super(NestedControlFlowModel, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.layer = NestedControlFlowLayer()\n \n   def call(self, inputs):\n\n@@ -28,7 +28,7 @@ class MultiInputSubclassed(keras.Model):\n   \"\"\"Subclassed Model that adds its inputs and then adds a bias.\"\"\"\n \n   def __init__(self):\n-    super(MultiInputSubclassed, self).__init__()\n+    super().__init__()\n     self.add = keras.layers.Add()\n     self.bias = test_utils.Bias()\n \n\n@@ -233,7 +233,7 @@ class TensorLikeDataAdapter(DataAdapter):\n                steps=None,\n                shuffle=False,\n                **kwargs):\n-    super(TensorLikeDataAdapter, self).__init__(x, y, **kwargs)\n+    super().__init__(x, y, **kwargs)\n     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n     sample_weight_modes = broadcast_sample_weight_modes(\n         sample_weights, sample_weight_modes)\n@@ -438,7 +438,7 @@ class GenericArrayLikeDataAdapter(TensorLikeDataAdapter):\n         \"supported by TensorFlow I/O (https://github.com/tensorflow/io) we \"\n         \"recommend using that to load a Dataset instead.\")\n \n-    super(GenericArrayLikeDataAdapter, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n \n   def slice_inputs(self, indices_dataset, inputs):\n     \"\"\"Slice inputs into a Dataset of batches.\n@@ -492,7 +492,7 @@ class DatasetCreatorAdapter(DataAdapter):\n   \"\"\"Adapter that handles dataset functions.\"\"\"\n \n   def __init__(self, x, y, steps=None, distribution_strategy=None, **kwargs):\n-    super(DatasetCreatorAdapter, self).__init__(x, **kwargs)\n+    super().__init__(x, **kwargs)\n \n     if not isinstance(x, dataset_creator.DatasetCreator):\n       raise TypeError(\"The input of a `DatasetCreatorAdapter` should be a \"\n@@ -574,7 +574,7 @@ class CompositeTensorDataAdapter(DataAdapter):\n                steps=None,\n                shuffle=False,\n                **kwargs):\n-    super(CompositeTensorDataAdapter, self).__init__(x, y, **kwargs)\n+    super().__init__(x, y, **kwargs)\n     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n     sample_weight_modes = broadcast_sample_weight_modes(\n         sample_weights, sample_weight_modes)\n@@ -653,7 +653,7 @@ class ListsOfScalarsDataAdapter(DataAdapter):\n                batch_size=None,\n                shuffle=False,\n                **kwargs):\n-    super(ListsOfScalarsDataAdapter, self).__init__(x, y, **kwargs)\n+    super().__init__(x, y, **kwargs)\n     x = np.asarray(x)\n     if y is not None:\n       y = np.asarray(y)\n@@ -704,7 +704,7 @@ class DatasetAdapter(DataAdapter):\n                sample_weights=None,\n                steps=None,\n                **kwargs):\n-    super(DatasetAdapter, self).__init__(x, y, **kwargs)\n+    super().__init__(x, y, **kwargs)\n     # Note that the dataset instance is immutable, its fine to reuse the user\n     # provided dataset.\n     self._dataset = x\n@@ -794,7 +794,7 @@ class GeneratorDataAdapter(DataAdapter):\n       raise ValueError(\"`sample_weight` argument is not supported when using \"\n                        \"python generator as input.\")\n \n-    super(GeneratorDataAdapter, self).__init__(x, y, **kwargs)\n+    super().__init__(x, y, **kwargs)\n \n     # Since we have to know the dtype of the python generator when we build the\n     # dataset, we have to look at a batch to infer the structure.\n@@ -923,7 +923,7 @@ class KerasSequenceAdapter(GeneratorDataAdapter):\n     self._shuffle_sequence = shuffle\n     self._keras_sequence = x\n     self._enqueuer = None\n-    super(KerasSequenceAdapter, self).__init__(\n+    super().__init__(\n         x,\n         shuffle=False,  # Shuffle is handed in the _make_callable override.\n         workers=workers,\n\n@@ -60,7 +60,7 @@ tf.register_tensor_conversion_function(DummyArrayLike, fail_on_convert)\n class DataAdapterTestBase(test_combinations.TestCase):\n \n   def setUp(self):\n-    super(DataAdapterTestBase, self).setUp()\n+    super().setUp()\n     self.batch_size = 5\n     self.numpy_input = np.zeros((50, 10))\n     self.numpy_target = np.ones(50)\n@@ -193,7 +193,7 @@ class TestBatchSequence(data_utils.Sequence):\n class TensorLikeDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n-    super(TensorLikeDataAdapterTest, self).setUp()\n+    super().setUp()\n     self.adapter_cls = data_adapter.TensorLikeDataAdapter\n \n   def test_can_handle_numpy(self):\n@@ -554,7 +554,7 @@ class IncreasingBatchSizeAdapterTest(test_combinations.TestCase):\n class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n-    super(GenericArrayLikeDataAdapterTest, self).setUp()\n+    super().setUp()\n     self.adapter_cls = data_adapter.GenericArrayLikeDataAdapter\n \n   def test_can_handle_some_numpy(self):\n@@ -787,7 +787,7 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n class DatasetAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n-    super(DatasetAdapterTest, self).setUp()\n+    super().setUp()\n     self.adapter_cls = data_adapter.DatasetAdapter\n \n   def test_can_handle(self):\n@@ -830,7 +830,7 @@ class DatasetAdapterTest(DataAdapterTestBase):\n class GeneratorDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n-    super(GeneratorDataAdapterTest, self).setUp()\n+    super().setUp()\n     self.adapter_cls = data_adapter.GeneratorDataAdapter\n \n   def test_can_handle(self):\n@@ -915,7 +915,7 @@ class GeneratorDataAdapterTest(DataAdapterTestBase):\n class KerasSequenceAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n-    super(KerasSequenceAdapterTest, self).setUp()\n+    super().setUp()\n     self.adapter_cls = data_adapter.KerasSequenceAdapter\n \n   def test_can_handle(self):\n@@ -973,14 +973,14 @@ class KerasSequenceAdapterTest(DataAdapterTestBase):\n class KerasSequenceAdapterSparseTest(KerasSequenceAdapterTest):\n \n   def setUp(self):\n-    super(KerasSequenceAdapterSparseTest, self).setUp()\n+    super().setUp()\n     self.sequence_input = TestSparseSequence(self.batch_size, 10)\n \n \n class KerasSequenceAdapterRaggedTest(KerasSequenceAdapterTest):\n \n   def setUp(self):\n-    super(KerasSequenceAdapterRaggedTest, self).setUp()\n+    super().setUp()\n     self.sequence_input = TestRaggedSequence(self.batch_size, 10)\n \n     self.model = keras.models.Sequential([\n@@ -1286,7 +1286,7 @@ class TestValidationSplit(test_combinations.TestCase):\n class ListsOfScalarsDataAdapterTest(DataAdapterTestBase):\n \n   def setUp(self):\n-    super(ListsOfScalarsDataAdapterTest, self).setUp()\n+    super().setUp()\n     self.adapter_cls = data_adapter.ListsOfScalarsDataAdapter\n \n   def test_can_list_inputs(self):\n\n@@ -29,7 +29,7 @@ from keras.utils import np_utils\n class TestDNNModel(keras.models.Model):\n \n   def __init__(self, feature_columns, units, name=None, **kwargs):\n-    super(TestDNNModel, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     self._input_layer = df.DenseFeatures(feature_columns, name='input_layer')\n     self._dense_layer = keras.layers.Dense(units, name='dense_layer')\n \n\n@@ -136,7 +136,7 @@ class Functional(training_lib.Model):\n     if skip_init:\n       return\n     generic_utils.validate_kwargs(kwargs, {})\n-    super(Functional, self).__init__(name=name, trainable=trainable)\n+    super().__init__(name=name, trainable=trainable)\n     # Check if the inputs contain any intermediate `KerasTensor` (not created\n     # by tf.keras.Input()). In this case we need to clone the `Node` and\n     # `KerasTensor` objects to mimic rebuilding a new model from new inputs.\n@@ -406,14 +406,14 @@ class Functional(training_lib.Model):\n   def _trackable_children(self, save_type='checkpoint', **kwargs):\n     dependencies = self._layer_checkpoint_dependencies\n     dependencies.update(\n-        super(Functional, self)._trackable_children(save_type, **kwargs))\n+        super()._trackable_children(save_type, **kwargs))\n     return dependencies\n \n   def _lookup_dependency(self, name):\n     layer_dependencies = self._layer_checkpoint_dependencies\n     if name in layer_dependencies:\n       return layer_dependencies[name]\n-    return super(Functional, self)._lookup_dependency(name)\n+    return super()._lookup_dependency(name)\n \n   def _handle_deferred_layer_dependencies(self, layers):\n     \"\"\"Handles layer checkpoint dependencies that are added after init.\"\"\"\n@@ -887,7 +887,7 @@ class Functional(training_lib.Model):\n       # Functional models and Sequential models that have an explicit input\n       # shape should use the batch size set by the input layer.\n       dynamic_batch = False\n-    return super(Functional, self)._get_save_spec(dynamic_batch, inputs_only)\n+    return super()._get_save_spec(dynamic_batch, inputs_only)\n \n \n def _make_node_key(layer_name, node_index):\n@@ -1447,7 +1447,7 @@ class ModuleWrapper(base_layer.Layer):\n     Raises:\n       ValueError: If `method` is not defined on `module`.\n     \"\"\"\n-    super(ModuleWrapper, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if method_name is None:\n       if hasattr(module, '__call__'):\n         method_name = '__call__'\n\n@@ -1644,7 +1644,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n     class Model(training_lib.Model):\n \n       def __init__(self):\n-        super(Model, self).__init__()\n+        super().__init__()\n         self.conv1 = layers.Conv2D(8, 3)\n         self.pool = layers.GlobalAveragePooling2D()\n         self.fc = layers.Dense(3)\n@@ -1672,7 +1672,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n     class BasicBlock(training_lib.Model):\n \n       def __init__(self):\n-        super(BasicBlock, self).__init__()\n+        super().__init__()\n         self.conv1 = layers.Conv2D(8, 3)\n         self.pool = layers.GlobalAveragePooling2D()\n         self.dense = layers.Dense(3)\n@@ -1686,7 +1686,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n     class CompoundModel(training_lib.Model):\n \n       def __init__(self):\n-        super(CompoundModel, self).__init__()\n+        super().__init__()\n         self.block = BasicBlock()\n \n       def call(self, x):\n@@ -1712,7 +1712,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n       # inside a model created using functional API.\n \n       def __init__(self):\n-        super(BasicBlock, self).__init__()\n+        super().__init__()\n         self.conv1 = layers.Conv2D(8, 3)\n \n       def call(self, x):\n@@ -2178,7 +2178,7 @@ class WeightAccessTest(test_combinations.TestCase):\n     class SubclassModel(models.Model):\n \n       def __init__(self):\n-        super(SubclassModel, self).__init__()\n+        super().__init__()\n         self.w = self.add_weight(shape=(), initializer='ones')\n \n       def call(self, inputs):\n@@ -2227,17 +2227,17 @@ class AttrTrackingLayer(base_layer.Layer):\n   def __init__(self, *args, **kwargs):\n     self.stateful_count = 0\n     self.dynamic_count = 0\n-    super(AttrTrackingLayer, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n \n   @base_layer.Layer.stateful.getter\n   def stateful(self):\n     self.stateful_count += 1\n-    return super(AttrTrackingLayer, self).stateful\n+    return super().stateful\n \n   @property\n   def dynamic(self):\n     self.dynamic_count += 1\n-    return super(AttrTrackingLayer, self).dynamic\n+    return super().dynamic\n \n \n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n\n@@ -153,7 +153,7 @@ class InputLayer(base_layer.Layer):\n           '`input_tensor.dtype` differs from `dtype`. Received: '\n           f'input_tensor.dtype={input_tensor.dtype} '\n           f'but expected dtype={dtype}')\n-    super(InputLayer, self).__init__(dtype=dtype, name=name)\n+    super().__init__(dtype=dtype, name=name)\n     self.built = True\n     self.sparse = True if sparse else False\n     self.ragged = True if ragged else False\n\n@@ -442,7 +442,7 @@ class RaggedKerasTensor(KerasTensor):\n   def _to_placeholder(self):\n     ragged_spec = self.type_spec\n     if ragged_spec.ragged_rank == 0 or ragged_spec.shape.rank is None:\n-      return super(RaggedKerasTensor, self)._to_placeholder()\n+      return super()._to_placeholder()\n \n     flat_shape = ragged_spec.shape[ragged_spec.ragged_rank:]\n     result = tf.compat.v1.placeholder(ragged_spec.dtype, flat_shape)\n@@ -533,7 +533,7 @@ class UserRegisteredTypeKerasTensor(KerasTensor):\n     type_spec = UserRegisteredSpec(x.shape, x.dtype)\n     name = getattr(x, 'name', None)\n \n-    super(UserRegisteredTypeKerasTensor, self).__init__(type_spec, name)\n+    super().__init__(type_spec, name)\n \n   @classmethod\n   def from_tensor(cls, tensor):\n\n@@ -142,7 +142,7 @@ class Sequential(functional.Functional):\n     # bottom of the stack.\n     # `Trackable` manages the `_layers` attributes and does filtering\n     # over it.\n-    layers = super(Sequential, self).layers\n+    layers = super().layers\n     if layers and isinstance(layers[0], input_layer.InputLayer):\n       return layers[1:]\n     return layers[:]\n@@ -346,7 +346,7 @@ class Sequential(functional.Functional):\n       if not self.built:\n         input_shape = tuple(input_shape)\n         self._build_input_shape = input_shape\n-        super(Sequential, self).build(input_shape)\n+        super().build(input_shape)\n     self.built = True\n \n   def call(self, inputs, training=None, mask=None):  # pylint: disable=redefined-outer-name\n@@ -371,7 +371,7 @@ class Sequential(functional.Functional):\n     if self._graph_initialized:\n       if not self.built:\n         self._init_graph_network(self.inputs, self.outputs)\n-      return super(Sequential, self).call(inputs, training=training, mask=mask)\n+      return super().call(inputs, training=training, mask=mask)\n \n     outputs = inputs  # handle the corner case where self.layers is empty\n     for layer in self.layers:\n@@ -409,7 +409,7 @@ class Sequential(functional.Functional):\n \n   def get_config(self):\n     layer_configs = []\n-    for layer in super(Sequential, self).layers:\n+    for layer in super().layers:\n       # `super().layers` include the InputLayer if available (it is filtered out\n       # of `self.layers`). Note that `self._self_tracked_trackables` is managed\n       # by the tracking infrastructure and should not be used.\n\n@@ -508,7 +508,7 @@ class TestSequentialEagerIntegration(test_combinations.TestCase):\n     class MySequential(keras.Sequential):\n \n       def __init__(self, name=None):\n-        super(MySequential, self).__init__(name=name)\n+        super().__init__(name=name)\n         self.call = tf.function(self.call)\n \n     model = MySequential()\n\n@@ -233,7 +233,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     generic_utils.validate_kwargs(kwargs, {\n         'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'\n     })\n-    super(Model, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     # By default, Model is a subclass model, which is not in graph network.\n     self._is_graph_network = False\n \n@@ -303,7 +303,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n   def __setattr__(self, name, value):\n     if not getattr(self, '_self_setattr_tracking', True):\n-      super(Model, self).__setattr__(name, value)\n+      super().__setattr__(name, value)\n       return\n \n     if all(\n@@ -317,7 +317,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             'forgot to call `super().__init__()`.'\n             ' Always start with this line.')\n \n-    super(Model, self).__setattr__(name, value)\n+    super().__setattr__(name, value)\n \n   def __reduce__(self):\n     if self.built:\n@@ -331,7 +331,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       # can be serialized as plain Python objects.\n       # Thus we call up the superclass hierarchy to get an implementation of\n       # __reduce__ that can pickle this Model as a plain Python object.\n-      return super(Model, self).__reduce__()\n+      return super().__reduce__()\n \n   def __deepcopy__(self, memo):\n     if self.built:\n@@ -340,7 +340,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       memo[id(self)] = new\n     else:\n       # See comment in __reduce__ for explanation\n-      deserializer, serialized, *rest = super(Model, self).__reduce__()\n+      deserializer, serialized, *rest = super().__reduce__()\n       new = deserializer(*serialized)\n       memo[id(self)] = new\n       if rest:\n@@ -380,7 +380,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       on real tensor data.\n     \"\"\"\n     if self._is_graph_network:\n-      super(Model, self).build(input_shape)\n+      super().build(input_shape)\n       return\n \n     if input_shape is None:\n@@ -455,7 +455,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                            'model, call your model on real tensor data (of '\n                            'the correct dtype).\\n\\nThe actual error from '\n                            f'`call` is: {e}.')\n-    super(Model, self).build(input_shape)\n+    super().build(input_shape)\n \n   @traceback_utils.filter_traceback\n   def __call__(self, *args, **kwargs):\n@@ -2393,7 +2393,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         A flat list of Numpy arrays.\n     \"\"\"\n     with self.distribute_strategy.scope():\n-      return super(Model, self).get_weights()\n+      return super().get_weights()\n \n   @traceback_utils.filter_traceback\n   def save(self,\n@@ -3000,7 +3000,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       inputs_spec.append(\n           tf_utils.get_tensor_spec(tensor, dynamic_batch=False, name=name))\n     inputs_spec = tf.nest.pack_sequence_as(inputs, inputs_spec)\n-    super(Model, self)._set_save_spec(inputs_spec, args, kwargs)\n+    super()._set_save_spec(inputs_spec, args, kwargs)\n \n     # Store the input shapes\n     if (self.__class__.__name__ == 'Sequential' and\n@@ -3244,7 +3244,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       self.predict_function = None\n       self.train_tf_function = None\n \n-    children = super(Model, self)._trackable_children(save_type, **kwargs)\n+    children = super()._trackable_children(save_type, **kwargs)\n \n     if save_type == 'savedmodel':\n       self.train_function = train_function\n\n@@ -168,7 +168,7 @@ class PrintTrainingInfoTest(test_combinations.TestCase,\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__(self)\n+        super().__init__(self)\n         self.dense1 = keras.layers.Dense(10, activation=\"relu\")\n         self.dense2 = keras.layers.Dense(10, activation=\"relu\")\n         self.concat = keras.layers.Concatenate()\n@@ -208,7 +208,7 @@ class PrintTrainingInfoTest(test_combinations.TestCase,\n     class my_model(keras.Model):\n \n       def __init__(self):\n-        super(my_model, self).__init__(self)\n+        super().__init__(self)\n         self.hidden_layer_0 = keras.layers.Dense(100, activation=\"relu\")\n         self.hidden_layer_1 = keras.layers.Dense(100, activation=\"relu\")\n         self.concat = keras.layers.Concatenate()\n\n@@ -37,7 +37,7 @@ class TrainingTest(test_combinations.TestCase):\n     class DynamicModel(keras.Model):\n \n       def __init__(self):\n-        super(DynamicModel, self).__init__(dynamic=True)\n+        super().__init__(dynamic=True)\n         self.dense = keras.layers.Dense(\n             1, kernel_initializer='zeros', bias_initializer='ones')\n \n\n@@ -375,7 +375,7 @@ class TrainingTest(test_combinations.TestCase):\n     class ReturnTraining(layers_module.Layer):\n \n       def __init__(self, input_shape=None, **kwargs):\n-        super(ReturnTraining, self).__init__(input_shape=input_shape, **kwargs)\n+        super().__init__(input_shape=input_shape, **kwargs)\n         self._nested_layer = None\n \n       def build(self, input_shape):\n@@ -739,7 +739,7 @@ class TrainingTest(test_combinations.TestCase):\n \n         def __init__(self, use_namedtuple):\n           self._use_namedtuple = use_namedtuple\n-          super(XYSequence, self).__init__()\n+          super().__init__()\n \n         def __getitem__(self, idx):\n           x, y = np.ones((4, 1)), np.ones((4, 1))\n@@ -754,7 +754,7 @@ class TrainingTest(test_combinations.TestCase):\n \n         def __init__(self, use_namedtuple):\n           self._use_namedtuple = use_namedtuple\n-          super(XSequence, self).__init__()\n+          super().__init__()\n \n         def __getitem__(self, idx):\n           x = np.ones((4, 1))\n@@ -929,7 +929,7 @@ class TrainingTest(test_combinations.TestCase):\n         # doubling the learning rate if weights are not deduped.\n         self._kernel = dense_to_track.kernel\n         self._bias = dense_to_track.bias\n-        super(WatchingLayer, self).__init__()\n+        super().__init__()\n \n     inp = layers_module.Input(shape=(1,))\n     dense_layer = layers_module.Dense(1)\n@@ -966,7 +966,7 @@ class TrainingTest(test_combinations.TestCase):\n       def __init__(self, trainable_var, non_trainable_var):\n         self.trainable_var = trainable_var\n         self.non_trainable_var = non_trainable_var\n-        super(AddWeightLayer, self).__init__()\n+        super().__init__()\n \n       def call(self, inputs):\n         return inputs + self.trainable_var\n@@ -974,7 +974,7 @@ class TrainingTest(test_combinations.TestCase):\n     class LayerWithWeightSharedLayers(layers_module.Layer):\n \n       def __init__(self):\n-        super(LayerWithWeightSharedLayers, self).__init__()\n+        super().__init__()\n         shared_trainable_var = tf.Variable(1.)\n         shared_non_trainable_var = tf.Variable(\n             1., trainable=False)\n@@ -1062,7 +1062,7 @@ class TrainingTest(test_combinations.TestCase):\n     class TestCallback(Callback):\n \n       def __init__(self):\n-        super(TestCallback, self).__init__()\n+        super().__init__()\n         self.epoch_end_logs = None\n         self.batch_end_logs = None\n         self.epoch_end_call_count = 0\n@@ -1470,7 +1470,7 @@ class TrainingTest(test_combinations.TestCase):\n     class ModelWithTrainingArg(training_module.Model):\n \n       def __init__(self):\n-        super(ModelWithTrainingArg, self).__init__()\n+        super().__init__()\n         self.l1 = LayerWithTrainingArg()\n \n       def call(self, inputs, training=None):\n@@ -1615,11 +1615,11 @@ class TrainingTest(test_combinations.TestCase):\n \n       def __init__(self):\n         self.aggregate_gradients_called = False\n-        super(_Optimizer, self).__init__(name='MyOptimizer')\n+        super().__init__(name='MyOptimizer')\n \n       def _aggregate_gradients(self, grads):\n         self.aggregate_gradients_called = True\n-        return super(_Optimizer, self)._aggregate_gradients(grads)\n+        return super()._aggregate_gradients(grads)\n \n     mock_optimizer = _Optimizer()\n \n@@ -1642,8 +1642,7 @@ class TrainingTest(test_combinations.TestCase):\n       _HAS_AGGREGATE_GRAD = False\n \n       def apply_gradients(self, grads_and_vars, name=None):  # pylint: disable=useless-super-delegation\n-        return super(_OptimizerOverrideApplyGradients,\n-                     self).apply_gradients(grads_and_vars, name)\n+        return super().apply_gradients(grads_and_vars, name)\n \n     mock_optimizer = _OptimizerOverrideApplyGradients()\n     model.compile(mock_optimizer, 'mse',\n@@ -1661,7 +1660,7 @@ class TrainingTest(test_combinations.TestCase):\n         # Gradients w.r.t. extra_weights are None\n         self.extra_weight_1 = self.add_weight('extra_weight_1', shape=(),\n                                               initializer='ones')\n-        super(DenseWithExtraWeight, self).build(input_shape)\n+        super().build(input_shape)\n         self.extra_weight_2 = self.add_weight('extra_weight_2', shape=(),\n                                               initializer='ones')\n \n@@ -1685,7 +1684,7 @@ class TrainingTest(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self, name):\n-        super(MyModel, self).__init__(name=name)\n+        super().__init__(name=name)\n \n         self.weight = tf.Variable(0, name=name)\n \n@@ -1708,7 +1707,7 @@ class TrainingTest(test_combinations.TestCase):\n     class UpdateLayer(layers_module.Layer):\n \n       def __init__(self):\n-        super(UpdateLayer, self).__init__()\n+        super().__init__()\n         self.v = tf.Variable(0., trainable=False)\n \n       def call(self, x):\n@@ -1764,15 +1763,15 @@ class TrainingTest(test_combinations.TestCase):\n       def train_step(self, data):\n         # No tuple wrapping for single x input and no targets.\n         test_case.assertIsInstance(data, expected_data_type)\n-        return super(MyModel, self).train_step(data)\n+        return super().train_step(data)\n \n       def test_step(self, data):\n         test_case.assertIsInstance(data, expected_data_type)\n-        return super(MyModel, self).test_step(data)\n+        return super().test_step(data)\n \n       def predict_step(self, data):\n         test_case.assertIsInstance(data, expected_data_type)\n-        return super(MyModel, self).predict_step(data)\n+        return super().predict_step(data)\n \n     inputs = layers_module.Input(shape=(1,), name='my_input')\n     outputs = layers_module.Dense(1)(inputs)\n@@ -1885,13 +1884,12 @@ class TrainingTest(test_combinations.TestCase):\n \n       def update_state(self, x, y_true, y_pred, sample_weight=None):\n         matches = self.sq_diff_plus_x(x, y_true, y_pred)\n-        return super(CustomMetric, self).update_state(matches)\n+        return super().update_state(matches)\n \n     class MyModel(sequential.Sequential):\n \n       def compute_metrics(self, x, y, y_pred, sample_weight):\n-        metric_results = super(MyModel,\n-                               self).compute_metrics(x, y, y_pred,\n+        metric_results = super().compute_metrics(x, y, y_pred,\n                                                      sample_weight)\n         self.custom_metric.update_state(x, y, y_pred, sample_weight)\n         metric_results['custom_metric_name'] = self.custom_metric.result()\n@@ -1917,7 +1915,7 @@ class TrainingTest(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self, *args, **kwargs):\n-        super(MyModel, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.loss_metric = metrics_module.Mean(name='loss')\n \n       def compute_loss(self, x, y, y_pred, sample_weight):\n@@ -2448,7 +2446,7 @@ class MaskingTest(test_combinations.TestCase):\n     class CustomMaskedLayer(layers_module.Layer):\n \n       def __init__(self):\n-        super(CustomMaskedLayer, self).__init__()\n+        super().__init__()\n         self.supports_masking = True\n \n       def call(self, inputs, mask=None):\n@@ -3418,7 +3416,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n         self.mean = metrics_module.Mean(name='metric_1')\n \n@@ -3490,7 +3488,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class LayerWithAddMetric(layers_module.Layer):\n \n       def __init__(self):\n-        super(LayerWithAddMetric, self).__init__()\n+        super().__init__()\n         self.dense = layers_module.Dense(1, kernel_initializer='ones')\n \n       def __call__(self, inputs):\n@@ -3502,7 +3500,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class LayerWithNestedAddMetricLayer(layers_module.Layer):\n \n       def __init__(self):\n-        super(LayerWithNestedAddMetricLayer, self).__init__()\n+        super().__init__()\n         self.layer = LayerWithAddMetric()\n \n       def call(self, inputs):\n@@ -3550,7 +3548,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n \n       def call(self, x):\n@@ -3577,7 +3575,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n         self.mean1 = metrics_module.Mean(name='metric_1')\n         self.mean2 = metrics_module.Mean(name='metric_2')\n@@ -3617,7 +3615,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestLayer(layers_module.Layer):\n \n       def __init__(self):\n-        super(TestLayer, self).__init__(name='test_layer')\n+        super().__init__(name='test_layer')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n         self.m1 = metrics_module.Mean(name='m_1')\n         self.m2 = [\n@@ -3652,7 +3650,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n         self.mean = metrics_module.Mean(name='metric_1')\n         self.mean2 = metrics_module.Mean(name='metric_1')\n@@ -3681,7 +3679,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n \n       def call(self, x):\n@@ -3752,7 +3750,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self, **kwargs):\n-        super(MyModel, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self._sampler = MyLayer(name='sampler')\n \n       def call(self, inputs, training=None, mask=None):\n@@ -3787,7 +3785,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n \n       def call(self, x):\n@@ -3806,7 +3804,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class TestModel(training_module.Model):\n \n       def __init__(self):\n-        super(TestModel, self).__init__(name='test_model')\n+        super().__init__(name='test_model')\n         self.dense1 = layers_module.Dense(2, kernel_initializer='ones')\n         self.mean = metrics_module.Mean(name='metric_1')\n \n@@ -3876,7 +3874,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class LayerWithAddMetric(layers_module.Layer):\n \n       def __init__(self):\n-        super(LayerWithAddMetric, self).__init__()\n+        super().__init__()\n         self.dense = layers_module.Dense(1, kernel_initializer='ones')\n \n       def call(self, inputs):\n@@ -3926,7 +3924,7 @@ class TestTrainingWithMetrics(test_combinations.TestCase):\n     class DictMetric(metrics_module.Metric):\n \n       def __init__(self):\n-        super(DictMetric, self).__init__()\n+        super().__init__()\n         self.sample_count = tf.Variable(0)\n         self.l2_sum = tf.Variable(0.)\n \n@@ -4179,7 +4177,7 @@ class TestBuildCustomModel(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.l1 = layers_module.Dense(1)\n         self.l2 = layers_module.Dense(2)\n \n@@ -4204,7 +4202,7 @@ class TestBuildCustomModel(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.l1 = layers_module.Dense(1)\n \n       def call(self, x):\n@@ -4223,7 +4221,7 @@ class TestBuildCustomModel(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.l1 = layers_module.Dense(1)\n \n       def call(self, inputs):\n@@ -4238,7 +4236,7 @@ class TestBuildCustomModel(test_combinations.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.class_token = self.add_weight(shape=(1,), name='class_token')\n         self.inner_layer = layers_module.Dense(1)\n \n\n@@ -104,7 +104,7 @@ class MetricsAggregator(Aggregator):\n   \"\"\"\n \n   def __init__(self, use_steps, num_samples=None, steps=None):\n-    super(MetricsAggregator, self).__init__(\n+    super().__init__(\n         use_steps=use_steps,\n         num_samples=num_samples,\n         steps=steps,\n@@ -250,7 +250,7 @@ class ConcatAggregator(Aggregator):\n \n   def __init__(self, batch_size):\n     self.composite = None\n-    super(ConcatAggregator, self).__init__(\n+    super().__init__(\n         use_steps=True, num_samples=None, steps=None, batch_size=batch_size)\n \n   def create(self, batch_element):\n@@ -333,7 +333,7 @@ class SliceAggregator(Aggregator):\n     self._async_copies = []\n     self._pool = get_copy_pool()\n     self._errors = []\n-    super(SliceAggregator, self).__init__(\n+    super().__init__(\n         use_steps=False,\n         num_samples=num_samples,\n         steps=None,\n\n@@ -201,13 +201,13 @@ class MonitoredPool(multiprocessing.pool.ThreadPool):\n   def __init__(self, *args, **kwargs):\n     self._apply_counter = 0\n     self._func_wrapper = None\n-    super(MonitoredPool, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n \n   def apply_async(self, func, *args, **kwargs):\n     self._apply_counter += 1\n     if self._func_wrapper:\n       func = self._func_wrapper(func)  # pylint: disable=not-callable\n-    return super(MonitoredPool, self).apply_async(func, *args, **kwargs)\n+    return super().apply_async(func, *args, **kwargs)\n \n \n def add_sleep(f):\n@@ -238,7 +238,7 @@ _TEST_DATA = np.array((\n class AggregationTest(test_combinations.TestCase):\n \n   def setUp(self):\n-    super(AggregationTest, self).setUp()\n+    super().setUp()\n     self._old_pool = training_utils_v1._COPY_POOL\n     self._old_threshold = (\n         training_utils_v1.SliceAggregator._BINARY_SIZE_THRESHOLD)\n@@ -247,7 +247,7 @@ class AggregationTest(test_combinations.TestCase):\n         training_utils_v1._COPY_THREADS)\n \n   def tearDown(self):\n-    super(AggregationTest, self).tearDown()\n+    super().tearDown()\n     training_utils_v1._COPY_POOL = self._old_pool\n     training_utils_v1.SliceAggregator._BINARY_SIZE_THRESHOLD = (\n         self._old_threshold)\n\n@@ -118,7 +118,7 @@ class Model(training_lib.Model):\n   \"\"\"\n \n   def __init__(self, *args, **kwargs):\n-    super(Model, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n     # initializing _distribution_strategy here since it is possible to call\n     # predict on a model without compiling it.\n     self._distribution_strategy = None\n@@ -210,7 +210,7 @@ class Model(training_lib.Model):\n           (not saving_utils.is_hdf5_filepath(filepath))):  # pylint: disable=protected-access\n         raise ValueError('Load weights is not yet supported with TPUStrategy '\n                          'with steps_per_run greater than 1.')\n-    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)\n+    return super().load_weights(filepath, by_name, skip_mismatch)\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def compile(self,\n@@ -480,7 +480,7 @@ class Model(training_lib.Model):\n         # See b/155687393 for more details, the model is created as a v2\n         # instance but converted to v1. Fallback to use base Model to retrieve\n         # the metrics.\n-        return super(Model, self).metrics\n+        return super().metrics\n       metrics += self._compile_metric_functions\n     metrics.extend(self._metrics)\n     metrics.extend(\n@@ -500,7 +500,7 @@ class Model(training_lib.Model):\n         # See b/155687393 for more details, the model is created as a v2\n         # instance but converted to v1. Fallback to use base Model to retrieve\n         # the metrics name\n-        return super(Model, self).metrics_names\n+        return super().metrics_names\n \n       # Add output loss metric names to the metric names list.\n       if len(self._training_endpoints) > 1:\n@@ -2825,7 +2825,7 @@ class DistributedCallbackModel(Model):\n   \"\"\"Model that is used for callbacks with tf.distribute.Strategy.\"\"\"\n \n   def __init__(self, model):\n-    super(DistributedCallbackModel, self).__init__()\n+    super().__init__()\n     self.optimizer = model.optimizer\n \n   def set_original_model(self, orig_model):\n@@ -2858,7 +2858,7 @@ class DistributedCallbackModel(Model):\n       logging.warning('You are accessing attribute ' + item + ' of the '\n                       'DistributedCallbackModel that may not have been set '\n                       'correctly.')\n-    return super(DistributedCallbackModel, self).__getattr__(item)\n+    return super().__getattr__(item)\n \n \n class _TrainingEndpoint:\n\n@@ -55,7 +55,7 @@ class _BaseFeaturesLayer(Layer):\n                name,\n                partitioner=None,\n                **kwargs):\n-    super(_BaseFeaturesLayer, self).__init__(\n+    super().__init__(\n         name=name, trainable=trainable, **kwargs)\n     self._feature_columns = _normalize_feature_columns(\n         feature_columns)\n@@ -77,7 +77,7 @@ class _BaseFeaturesLayer(Layer):\n         with tf.compat.v1.variable_scope(\n             _sanitize_column_name_for_variable_scope(column.name)):\n           column.create_state(self._state_manager)\n-    super(_BaseFeaturesLayer, self).build(None)\n+    super().build(None)\n \n   def _output_shape(self, input_shape, num_elements):\n     \"\"\"Computes expected output shape of the layer or a column's dense tensor.\n@@ -123,8 +123,7 @@ class _BaseFeaturesLayer(Layer):\n     config['partitioner'] = generic_utils.serialize_keras_object(\n         self._partitioner)\n \n-    base_config = super(  # pylint: disable=bad-super-call\n-        _BaseFeaturesLayer, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n\n@@ -90,7 +90,7 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n     Raises:\n       ValueError: if an item in `feature_columns` is not a `DenseColumn`.\n     \"\"\"\n-    super(DenseFeatures, self).__init__(\n+    super().__init__(\n         feature_columns=feature_columns,\n         trainable=trainable,\n         name=name,\n@@ -109,7 +109,7 @@ class DenseFeatures(kfc._BaseFeaturesLayer):  # pylint: disable=protected-access\n     Returns:\n       A serialized JSON storing information necessary for recreating this layer.\n     \"\"\"\n-    metadata = json.loads(super(DenseFeatures, self)._tracking_metadata)\n+    metadata = json.loads(super()._tracking_metadata)\n     metadata['_is_feature_layer'] = True\n     return json.dumps(metadata, default=json_utils.get_json_type)\n \n\n@@ -80,7 +80,7 @@ class DenseFeatures(dense_features.DenseFeatures):\n     Raises:\n       ValueError: if an item in `feature_columns` is not a `DenseColumn`.\n     \"\"\"\n-    super(DenseFeatures, self).__init__(\n+    super().__init__(\n         feature_columns=feature_columns,\n         trainable=trainable,\n         name=name,\n\n@@ -99,7 +99,7 @@ class SequenceFeatures(kfc._BaseFeaturesLayer):\n       ValueError: If any of the `feature_columns` is not a\n         `SequenceDenseColumn`.\n     \"\"\"\n-    super(SequenceFeatures, self).__init__(\n+    super().__init__(\n         feature_columns=feature_columns,\n         trainable=trainable,\n         name=name,\n\n@@ -156,7 +156,7 @@ class RandomNormal(tf.compat.v1.random_normal_initializer):\n   \"\"\"\n \n   def __init__(self, mean=0.0, stddev=0.05, seed=None, dtype=tf.float32):\n-    super(RandomNormal, self).__init__(\n+    super().__init__(\n         mean=mean, stddev=stddev, seed=seed, dtype=dtype)\n \n \n@@ -274,7 +274,7 @@ class RandomUniform(tf.compat.v1.random_uniform_initializer):\n \n   def __init__(self, minval=-0.05, maxval=0.05, seed=None,\n                dtype=tf.float32):\n-    super(RandomUniform, self).__init__(\n+    super().__init__(\n         minval=minval, maxval=maxval, seed=seed, dtype=dtype)\n \n \n@@ -405,7 +405,7 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n       dtype: Default data type, used if no `dtype` argument is provided when\n         calling the initializer. Only floating point types are supported.\n     \"\"\"\n-    super(TruncatedNormal, self).__init__(\n+    super().__init__(\n         mean=mean, stddev=stddev, seed=seed, dtype=dtype)\n \n \n@@ -413,7 +413,7 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n class LecunNormal(tf.compat.v1.variance_scaling_initializer):\n \n   def __init__(self, seed=None):\n-    super(LecunNormal, self).__init__(\n+    super().__init__(\n         scale=1., mode='fan_in', distribution='truncated_normal', seed=seed)\n \n   def get_config(self):\n@@ -424,7 +424,7 @@ class LecunNormal(tf.compat.v1.variance_scaling_initializer):\n class LecunUniform(tf.compat.v1.variance_scaling_initializer):\n \n   def __init__(self, seed=None):\n-    super(LecunUniform, self).__init__(\n+    super().__init__(\n         scale=1., mode='fan_in', distribution='uniform', seed=seed)\n \n   def get_config(self):\n@@ -435,7 +435,7 @@ class LecunUniform(tf.compat.v1.variance_scaling_initializer):\n class HeNormal(tf.compat.v1.variance_scaling_initializer):\n \n   def __init__(self, seed=None):\n-    super(HeNormal, self).__init__(\n+    super().__init__(\n         scale=2., mode='fan_in', distribution='truncated_normal', seed=seed)\n \n   def get_config(self):\n@@ -446,7 +446,7 @@ class HeNormal(tf.compat.v1.variance_scaling_initializer):\n class HeUniform(tf.compat.v1.variance_scaling_initializer):\n \n   def __init__(self, seed=None):\n-    super(HeUniform, self).__init__(\n+    super().__init__(\n         scale=2., mode='fan_in', distribution='uniform', seed=seed)\n \n   def get_config(self):\n\n@@ -793,7 +793,7 @@ class GlorotUniform(VarianceScaling):\n   \"\"\"\n \n   def __init__(self, seed=None):\n-    super(GlorotUniform, self).__init__(\n+    super().__init__(\n         scale=1.0,\n         mode='fan_avg',\n         distribution='uniform',\n@@ -839,7 +839,7 @@ class GlorotNormal(VarianceScaling):\n   \"\"\"\n \n   def __init__(self, seed=None):\n-    super(GlorotNormal, self).__init__(\n+    super().__init__(\n         scale=1.0,\n         mode='fan_avg',\n         distribution='truncated_normal',\n@@ -888,7 +888,7 @@ class LecunNormal(VarianceScaling):\n   \"\"\"\n \n   def __init__(self, seed=None):\n-    super(LecunNormal, self).__init__(\n+    super().__init__(\n         scale=1., mode='fan_in', distribution='truncated_normal', seed=seed)\n \n   def get_config(self):\n@@ -930,7 +930,7 @@ class LecunUniform(VarianceScaling):\n   \"\"\"\n \n   def __init__(self, seed=None):\n-    super(LecunUniform, self).__init__(\n+    super().__init__(\n         scale=1., mode='fan_in', distribution='uniform', seed=seed)\n \n   def get_config(self):\n@@ -972,7 +972,7 @@ class HeNormal(VarianceScaling):\n   \"\"\"\n \n   def __init__(self, seed=None):\n-    super(HeNormal, self).__init__(\n+    super().__init__(\n         scale=2., mode='fan_in', distribution='truncated_normal', seed=seed)\n \n   def get_config(self):\n@@ -1014,7 +1014,7 @@ class HeUniform(VarianceScaling):\n   \"\"\"\n \n   def __init__(self, seed=None):\n-    super(HeUniform, self).__init__(\n+    super().__init__(\n         scale=2., mode='fan_in', distribution='uniform', seed=seed)\n \n   def get_config(self):\n\n@@ -266,7 +266,7 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n     class M(tf.keras.Model):\n \n       def __init__(self):\n-        super(M, self).__init__()\n+        super().__init__()\n         self.embed = tf.keras.layers.Embedding(5, 1)\n         self.proj = tf.keras.layers.Dense(1)\n \n\n@@ -25,7 +25,7 @@ class MiniModel(tf.keras.Model):\n   \"\"\"\n \n   def __init__(self):\n-    super(MiniModel, self).__init__(name='')\n+    super().__init__(name='')\n     self.fc = tf.keras.layers.Dense(1, name='fc', kernel_initializer='ones',\n                                     bias_initializer='ones')\n \n@@ -43,7 +43,7 @@ class DefunnedMiniModel(MiniModel):\n class ModelWithOptimizer(tf.keras.Model):\n \n   def __init__(self):\n-    super(ModelWithOptimizer, self).__init__()\n+    super().__init__()\n     self.dense = tf.keras.layers.Dense(1)\n     self.optimizer = tf.keras.optimizers.Adam(0.01)\n \n\n@@ -164,7 +164,7 @@ class GradientCheckpointTest(tf.test.TestCase):\n     self.assertLen(losses, n_step)\n \n   def tearDown(self):\n-    super(GradientCheckpointTest, self).tearDown()\n+    super().tearDown()\n     # Make sure all the models created in keras has been deleted and cleared\n     # from the global keras grpah, also do a force GC to recycle the GPU memory.\n     tf.keras.backend.clear_session()\n\n@@ -21,7 +21,7 @@ class TestKerasModelClass(tf.keras.Model):\n   \"\"\"A simple tensorflow keras Model class definition.\"\"\"\n \n   def __init__(self, width):\n-    super(TestKerasModelClass, self).__init__()\n+    super().__init__()\n     self.width = width\n \n   def build(self, input_shape):\n@@ -80,7 +80,7 @@ class GradientsTest(tf.test.TestCase):\n     class HasLSTM(tf.keras.Model):\n \n       def __init__(self):\n-        super(HasLSTM, self).__init__()\n+        super().__init__()\n         self.lstm = tf.keras.layers.LSTM(units=5)\n         self.dense = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n \n\n@@ -22,7 +22,7 @@ tf.disable_eager_execution()\n class KerasNetworkTFRNNs(tf.keras.Model):\n \n   def __init__(self, name=None):\n-    super(KerasNetworkTFRNNs, self).__init__(name=name)\n+    super().__init__(name=name)\n     self._cell = tf.nn.rnn_cell.MultiRNNCell(\n         [tf.nn.rnn_cell.LSTMCell(1) for _ in range(2)])\n \n@@ -33,7 +33,7 @@ class KerasNetworkTFRNNs(tf.keras.Model):\n class KerasNetworkKerasRNNs(tf.keras.Model):\n \n   def __init__(self, name=None):\n-    super(KerasNetworkKerasRNNs, self).__init__(name=name)\n+    super().__init__(name=name)\n     self._cell = tf.keras.layers.StackedRNNCells(\n         [tf.keras.layers.LSTMCell(1) for _ in range(2)])\n \n@@ -44,7 +44,7 @@ class KerasNetworkKerasRNNs(tf.keras.Model):\n class LegacyRNNTest(tf.test.TestCase):\n \n   def setUp(self):\n-    super(LegacyRNNTest, self).setUp()\n+    super().setUp()\n     self._seed = 23489\n     np.random.seed(self._seed)\n \n\n@@ -61,7 +61,7 @@ class ParameterServerCustomTrainingLoopTest(tf.test.TestCase):\n     return cluster_spec\n \n   def setUp(self):\n-    super(ParameterServerCustomTrainingLoopTest, self).setUp()\n+    super().setUp()\n \n     cluster_spec = self.create_in_process_cluster(num_workers=3, num_ps=2)\n     cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n\n@@ -70,7 +70,7 @@ def create_in_process_cluster(num_workers, num_ps):\n class KPLTest(tf.test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n-    super(KPLTest, self).setUp()\n+    super().setUp()\n \n     cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)\n     cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n@@ -269,7 +269,7 @@ class KPLCreatedInDatasetsFromFunctionTest(tf.test.TestCase,\n                                            parameterized.TestCase):\n \n   def setUp(self):\n-    super(KPLCreatedInDatasetsFromFunctionTest, self).setUp()\n+    super().setUp()\n \n     cluster_spec = create_in_process_cluster(num_workers=3, num_ps=2)\n     cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n\n@@ -122,7 +122,7 @@ class LoadTest(tf.test.TestCase, parameterized.TestCase):\n     class _HasOptimizer(tf.Module):\n \n       def __init__(self):\n-        super(_HasOptimizer, self).__init__()\n+        super().__init__()\n         self.layer = tf.keras.layers.Dense(1)\n         self.optimizer = tf.keras.optimizers.Adam(0.01)\n \n\n@@ -267,4 +267,4 @@ class VersionAwareLayers:\n     serialization.populate_deserializable_objects()\n     if name in serialization.LOCAL.ALL_OBJECTS:\n       return serialization.LOCAL.ALL_OBJECTS[name]\n-    return super(VersionAwareLayers, self).__getattr__(name)\n+    return super().__getattr__(name)\n\n@@ -46,7 +46,7 @@ class ELU(Layer):\n   \"\"\"\n \n   def __init__(self, alpha=1.0, **kwargs):\n-    super(ELU, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if alpha is None:\n       raise ValueError(\n           'Alpha of an ELU layer cannot be None, expecting a float. '\n@@ -59,7 +59,7 @@ class ELU(Layer):\n \n   def get_config(self):\n     config = {'alpha': float(self.alpha)}\n-    base_config = super(ELU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -58,7 +58,7 @@ class LeakyReLU(Layer):\n   \"\"\"\n \n   def __init__(self, alpha=0.3, **kwargs):\n-    super(LeakyReLU, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if alpha is None:\n       raise ValueError(\n           'The alpha value of a Leaky ReLU layer cannot be None, '\n@@ -71,7 +71,7 @@ class LeakyReLU(Layer):\n \n   def get_config(self):\n     config = {'alpha': float(self.alpha)}\n-    base_config = super(LeakyReLU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -67,7 +67,7 @@ class PReLU(Layer):\n                alpha_constraint=None,\n                shared_axes=None,\n                **kwargs):\n-    super(PReLU, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.supports_masking = True\n     self.alpha_initializer = initializers.get(alpha_initializer)\n     self.alpha_regularizer = regularizers.get(alpha_regularizer)\n@@ -112,7 +112,7 @@ class PReLU(Layer):\n         'alpha_constraint': constraints.serialize(self.alpha_constraint),\n         'shared_axes': self.shared_axes\n     }\n-    base_config = super(PReLU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -72,7 +72,7 @@ class ReLU(Layer):\n   \"\"\"\n \n   def __init__(self, max_value=None, negative_slope=0., threshold=0., **kwargs):\n-    super(ReLU, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if max_value is not None and max_value < 0.:\n       raise ValueError('max_value of a ReLU layer cannot be a negative '\n                        f'value. Received: {max_value}')\n@@ -104,7 +104,7 @@ class ReLU(Layer):\n         'negative_slope': self.negative_slope,\n         'threshold': self.threshold\n     }\n-    base_config = super(ReLU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -75,7 +75,7 @@ class Softmax(Layer):\n   \"\"\"\n \n   def __init__(self, axis=-1, **kwargs):\n-    super(Softmax, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.supports_masking = True\n     self.axis = axis\n \n@@ -100,7 +100,7 @@ class Softmax(Layer):\n \n   def get_config(self):\n     config = {'axis': self.axis}\n-    base_config = super(Softmax, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -47,7 +47,7 @@ class ThresholdedReLU(Layer):\n   \"\"\"\n \n   def __init__(self, theta=1.0, **kwargs):\n-    super(ThresholdedReLU, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if theta is None:\n       raise ValueError(\n           'Theta of a Thresholded ReLU layer cannot be None, expecting a float.'\n@@ -64,7 +64,7 @@ class ThresholdedReLU(Layer):\n \n   def get_config(self):\n     config = {'theta': float(self.theta)}\n-    base_config = super(ThresholdedReLU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -130,7 +130,7 @@ class AdditiveAttention(BaseDenseAttention):\n   \"\"\"\n \n   def __init__(self, use_scale=True, **kwargs):\n-    super(AdditiveAttention, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.use_scale = use_scale\n \n   def build(self, input_shape):\n@@ -146,7 +146,7 @@ class AdditiveAttention(BaseDenseAttention):\n           trainable=True)\n     else:\n       self.scale = None\n-    super(AdditiveAttention, self).build(input_shape)\n+    super().build(input_shape)\n \n   def _calculate_scores(self, query, key):\n     \"\"\"Calculates attention scores as a nonlinear sum of query and key.\n@@ -171,5 +171,5 @@ class AdditiveAttention(BaseDenseAttention):\n \n   def get_config(self):\n     config = {'use_scale': self.use_scale}\n-    base_config = super(AdditiveAttention, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -132,7 +132,7 @@ class Attention(BaseDenseAttention):\n   \"\"\"\n \n   def __init__(self, use_scale=False, score_mode='dot', **kwargs):\n-    super(Attention, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.use_scale = use_scale\n     self.score_mode = score_mode\n     if self.score_mode not in ['dot', 'concat']:\n@@ -159,7 +159,7 @@ class Attention(BaseDenseAttention):\n           trainable=True)\n     else:\n       self.concat_score_weight = None\n-    super(Attention, self).build(input_shape)\n+    super().build(input_shape)\n \n   def _calculate_scores(self, query, key):\n     \"\"\"Calculates attention scores as a query-key dot product.\n@@ -191,5 +191,5 @@ class Attention(BaseDenseAttention):\n \n   def get_config(self):\n     config = {'use_scale': self.use_scale, 'score_mode': self.score_mode}\n-    base_config = super(Attention, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -68,7 +68,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n   \"\"\"\n \n   def __init__(self, causal=False, dropout=0.0, **kwargs):\n-    super(BaseDenseAttention, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.causal = causal\n     self.dropout = dropout\n     self.supports_masking = True\n@@ -212,7 +212,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         'causal': self.causal,\n         'dropout': self.dropout,\n     }\n-    base_config = super(BaseDenseAttention, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -232,7 +232,7 @@ class MultiHeadAttention(Layer):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(MultiHeadAttention, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self._num_heads = num_heads\n     self._key_dim = key_dim\n     self._value_dim = value_dim if value_dim else key_dim\n@@ -281,7 +281,7 @@ class MultiHeadAttention(Layer):\n         \"key_shape\": self._key_shape,\n         \"value_shape\": self._value_shape,\n     }\n-    base_config = super(MultiHeadAttention, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n\n@@ -304,7 +304,7 @@ class AttentionSubclassTest(test_combinations.TestCase):\n class TestModel(keras.Model):\n \n   def __init__(self):\n-    super(TestModel, self).__init__()\n+    super().__init__()\n     self.attention = keras.layers.MultiHeadAttention(\n         num_heads=3,\n         key_dim=4,\n\n@@ -109,7 +109,7 @@ class Conv(Layer):\n                name=None,\n                conv_op=None,\n                **kwargs):\n-    super(Conv, self).__init__(\n+    super().__init__(\n         trainable=trainable,\n         name=name,\n         activity_regularizer=regularizers.get(activity_regularizer),\n@@ -350,7 +350,7 @@ class Conv(Layer):\n         'bias_constraint':\n             constraints.serialize(self.bias_constraint)\n     }\n-    base_config = super(Conv, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def _compute_causal_padding(self, inputs):\n\n@@ -132,7 +132,7 @@ class DepthwiseConv(Conv):\n                depthwise_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(DepthwiseConv, self).__init__(\n+    super().__init__(\n         rank,\n         filters=None,\n         kernel_size=kernel_size,\n@@ -193,7 +193,7 @@ class DepthwiseConv(Conv):\n     raise NotImplementedError\n \n   def get_config(self):\n-    config = super(DepthwiseConv, self).get_config()\n+    config = super().get_config()\n     config.pop('filters')\n     config.pop('kernel_initializer')\n     config.pop('kernel_regularizer')\n\n@@ -119,7 +119,7 @@ class SeparableConv(Conv):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(SeparableConv, self).__init__(\n+    super().__init__(\n         rank=rank,\n         filters=filters,\n         kernel_size=kernel_size,\n@@ -233,5 +233,5 @@ class SeparableConv(Conv):\n         'bias_constraint':\n             constraints.serialize(self.bias_constraint)\n     }\n-    base_config = super(SeparableConv, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -146,7 +146,7 @@ class Conv1D(Conv):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Conv1D, self).__init__(\n+    super().__init__(\n         rank=1,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -136,7 +136,7 @@ class Conv1DTranspose(Conv1D):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Conv1DTranspose, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -274,7 +274,7 @@ class Conv1DTranspose(Conv1D):\n     return tf.TensorShape(output_shape)\n \n   def get_config(self):\n-    config = super(Conv1DTranspose, self).get_config()\n+    config = super().get_config()\n     config['output_padding'] = self.output_padding\n     return config\n \n\n@@ -167,7 +167,7 @@ class Conv2D(Conv):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Conv2D, self).__init__(\n+    super().__init__(\n         rank=2,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -160,7 +160,7 @@ class Conv2DTranspose(Conv2D):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Conv2DTranspose, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -330,7 +330,7 @@ class Conv2DTranspose(Conv2D):\n     return tf.TensorShape(output_shape)\n \n   def get_config(self):\n-    config = super(Conv2DTranspose, self).get_config()\n+    config = super().get_config()\n     config['output_padding'] = self.output_padding\n     return config\n \n\n@@ -154,7 +154,7 @@ class Conv3D(Conv):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Conv3D, self).__init__(\n+    super().__init__(\n         rank=3,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -167,7 +167,7 @@ class Conv3DTranspose(Conv3D):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Conv3DTranspose, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -339,7 +339,7 @@ class Conv3DTranspose(Conv3D):\n     return tf.TensorShape(output_shape)\n \n   def get_config(self):\n-    config = super(Conv3DTranspose, self).get_config()\n+    config = super().get_config()\n     config.pop('dilation_rate')\n     config['output_padding'] = self.output_padding\n     return config\n\n@@ -131,7 +131,7 @@ class DepthwiseConv1D(DepthwiseConv):\n                depthwise_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(DepthwiseConv1D, self).__init__(\n+    super().__init__(\n         1,\n         kernel_size=kernel_size,\n         strides=strides,\n\n@@ -132,7 +132,7 @@ class DepthwiseConv2D(DepthwiseConv):\n                depthwise_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(DepthwiseConv2D, self).__init__(\n+    super().__init__(\n         2,\n         kernel_size=kernel_size,\n         strides=strides,\n\n@@ -136,7 +136,7 @@ class SeparableConv1D(SeparableConv):\n                pointwise_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(SeparableConv1D, self).__init__(\n+    super().__init__(\n         rank=1,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -148,7 +148,7 @@ class SeparableConv2D(SeparableConv):\n                pointwise_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(SeparableConv2D, self).__init__(\n+    super().__init__(\n         rank=2,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -49,7 +49,7 @@ class Activation(Layer):\n   \"\"\"\n \n   def __init__(self, activation, **kwargs):\n-    super(Activation, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.supports_masking = True\n     self.activation = activations.get(activation)\n \n@@ -61,6 +61,6 @@ class Activation(Layer):\n \n   def get_config(self):\n     config = {'activation': activations.serialize(self.activation)}\n-    base_config = super(Activation, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -110,7 +110,7 @@ class Dense(Layer):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(Dense, self).__init__(\n+    super().__init__(\n         activity_regularizer=activity_regularizer, **kwargs)\n \n     self.units = int(units) if not isinstance(units, int) else units\n@@ -249,7 +249,7 @@ class Dense(Layer):\n     return input_shape[:-1].concatenate(self.units)\n \n   def get_config(self):\n-    config = super(Dense, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'units': self.units,\n         'activation': activations.serialize(self.activation),\n\n@@ -124,7 +124,7 @@ class EinsumDense(Layer):\n                kernel_constraint=None,\n                bias_constraint=None,\n                **kwargs):\n-    super(EinsumDense, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.equation = equation\n     if isinstance(output_shape, int):\n       self.partial_output_shape = [output_shape]\n@@ -166,7 +166,7 @@ class EinsumDense(Layer):\n           trainable=True)\n     else:\n       self.bias = None\n-    super(EinsumDense, self).build(input_shape)\n+    super().build(input_shape)\n \n   def compute_output_shape(self, _):\n     return tf.TensorShape(self.full_output_shape)\n@@ -186,7 +186,7 @@ class EinsumDense(Layer):\n         \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n         \"bias_constraint\": constraints.serialize(self.bias_constraint),\n     }\n-    base_config = super(EinsumDense, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def call(self, inputs):\n\n@@ -140,7 +140,7 @@ class Embedding(Layer):\n     # before casting to int32 might cause the int32 values to be different due\n     # to a loss of precision.\n     kwargs['autocast'] = False\n-    super(Embedding, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n     self.input_dim = input_dim\n     self.output_dim = output_dim\n@@ -218,5 +218,5 @@ class Embedding(Layer):\n         'mask_zero': self.mask_zero,\n         'input_length': self.input_length\n     }\n-    base_config = super(Embedding, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -129,7 +129,7 @@ class Lambda(Layer):\n                mask=None,\n                arguments=None,\n                **kwargs):\n-    super(Lambda, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n     self.arguments = arguments or {}\n     self.function = function\n@@ -155,7 +155,7 @@ class Lambda(Layer):\n       # `add_loss`.\n       with tf.__internal__.eager_context.eager_mode():\n         try:\n-          return super(Lambda, self).compute_output_shape(input_shape)\n+          return super().compute_output_shape(input_shape)\n         except NotImplementedError:\n           raise NotImplementedError(\n               'We could not automatically infer the shape of the Lambda\\'s '\n@@ -275,7 +275,7 @@ class Lambda(Layer):\n       })\n     config['arguments'] = self.arguments\n \n-    base_config = super(Lambda, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def _serialize_function_to_config(self, inputs, allow_raw=False):\n\n@@ -62,7 +62,7 @@ class Masking(Layer):\n   \"\"\"\n \n   def __init__(self, mask_value=0., **kwargs):\n-    super(Masking, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.supports_masking = True\n     self.mask_value = mask_value\n     self._compute_output_and_mask_jointly = True\n@@ -83,5 +83,5 @@ class Masking(Layer):\n \n   def get_config(self):\n     config = {'mask_value': self.mask_value}\n-    base_config = super(Masking, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -57,7 +57,7 @@ class ClassMethod(Layer):\n     # Do not individually trace op layers in the SavedModel.\n     self._must_restore_from_config = True\n \n-    super(ClassMethod, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n     # Preserve all argument data structures when saving/loading a config\n     # (e.g., don't unnest lists that contain one element)\n@@ -80,7 +80,7 @@ class ClassMethod(Layer):\n           'public TensorFlow API symbols can be serialized.')\n \n     config = {'cls_symbol': self.cls_symbol, 'method_name': self.method_name}\n-    base_config = super(ClassMethod, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -141,7 +141,7 @@ class InstanceProperty(Layer):\n     # Do not individually trace op layers in the SavedModel.\n     self._must_restore_from_config = True\n \n-    super(InstanceProperty, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n     # Preserve all argument data structures when saving/loading a config\n     # (e.g., don't unnest lists that contain one element)\n@@ -152,7 +152,7 @@ class InstanceProperty(Layer):\n \n   def get_config(self):\n     config = {'attr_name': self.attr_name}\n-    base_config = super(InstanceProperty, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -231,7 +231,7 @@ class TFOpLambda(Layer):\n     # Do not individually trace op layers in the SavedModel.\n     self._must_restore_from_config = True\n \n-    super(TFOpLambda, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n     # Preserve all argument data structures when saving/loading a config\n     # (e.g., don't unnest lists that contain one element)\n@@ -314,7 +314,7 @@ class TFOpLambda(Layer):\n           'public TensorFlow API symbols can be serialized.')\n     config = {'function': self.symbol}\n \n-    base_config = super(TFOpLambda, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -465,7 +465,7 @@ class SlicingOpLambda(TFOpLambda):\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, function, **kwargs):\n-    super(SlicingOpLambda, self).__init__(function, **kwargs)\n+    super().__init__(function, **kwargs)\n \n     original_call = self.call\n \n\n@@ -155,7 +155,7 @@ class RandomFourierFeatures(base_layer.Layer):\n     if scale is not None and scale <= 0.0:\n       raise ValueError('When provided, `scale` should be a positive float. '\n                        f'Received: {scale}')\n-    super(RandomFourierFeatures, self).__init__(\n+    super().__init__(\n         trainable=trainable, name=name, **kwargs)\n     self.output_dim = output_dim\n     self.kernel_initializer = kernel_initializer\n@@ -204,7 +204,7 @@ class RandomFourierFeatures(base_layer.Layer):\n         initializer=tf.compat.v1.constant_initializer(self.scale),\n         trainable=True,\n         constraint='NonNeg')\n-    super(RandomFourierFeatures, self).build(input_shape)\n+    super().build(input_shape)\n \n   def call(self, inputs):\n     inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n@@ -232,7 +232,7 @@ class RandomFourierFeatures(base_layer.Layer):\n         'kernel_initializer': kernel_initializer,\n         'scale': self.scale,\n     }\n-    base_config = super(RandomFourierFeatures, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -128,7 +128,7 @@ class LocallyConnected1D(Layer):\n                bias_constraint=None,\n                implementation=1,\n                **kwargs):\n-    super(LocallyConnected1D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.filters = filters\n     self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')\n     self.strides = conv_utils.normalize_tuple(\n@@ -329,5 +329,5 @@ class LocallyConnected1D(Layer):\n         'implementation':\n             self.implementation\n     }\n-    base_config = super(LocallyConnected1D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -142,7 +142,7 @@ class LocallyConnected2D(Layer):\n                bias_constraint=None,\n                implementation=1,\n                **kwargs):\n-    super(LocallyConnected2D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.filters = filters\n     self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n     self.strides = conv_utils.normalize_tuple(\n@@ -351,5 +351,5 @@ class LocallyConnected2D(Layer):\n         'implementation':\n             self.implementation\n     }\n-    base_config = super(LocallyConnected2D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -32,7 +32,7 @@ class _Merge(Layer):\n     Args:\n       **kwargs: standard layer keyword arguments.\n     \"\"\"\n-    super(_Merge, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.supports_masking = True\n \n   def _merge_function(self, inputs):\n\n@@ -84,7 +84,7 @@ class Concatenate(_Merge):\n       axis: Axis along which to concatenate.\n       **kwargs: standard layer keyword arguments.\n     \"\"\"\n-    super(Concatenate, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.axis = axis\n     self.supports_masking = True\n     self._reshape_required = False\n@@ -178,7 +178,7 @@ class Concatenate(_Merge):\n     config = {\n         'axis': self.axis,\n     }\n-    base_config = super(Concatenate, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -89,7 +89,7 @@ class Dot(_Merge):\n         is the cosine proximity between the two samples.\n       **kwargs: Standard layer keyword arguments.\n     \"\"\"\n-    super(Dot, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if not isinstance(axes, int):\n       if not isinstance(axes, (list, tuple)):\n         raise TypeError(\n@@ -190,7 +190,7 @@ class Dot(_Merge):\n         'axes': self.axes,\n         'normalize': self.normalize,\n     }\n-    base_config = super(Dot, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -48,7 +48,7 @@ class Subtract(_Merge):\n \n   @tf_utils.shape_type_conversion\n   def build(self, input_shape):\n-    super(Subtract, self).build(input_shape)\n+    super().build(input_shape)\n     if len(input_shape) != 2:\n       raise ValueError(\n           'A `Subtract` layer should be called on exactly 2 inputs. '\n\n@@ -172,7 +172,7 @@ class BatchNormalizationBase(Layer):\n                adjustment=None,\n                name=None,\n                **kwargs):\n-    super(BatchNormalizationBase, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     if isinstance(axis, (list, tuple)):\n       self.axis = axis[:]\n     elif isinstance(axis, int):\n@@ -938,7 +938,7 @@ class BatchNormalizationBase(Layer):\n                       'layer cannot be serialized and has been omitted from '\n                       'the layer config. It will not be included when '\n                       're-creating the layer from the saved config.')\n-    base_config = super(BatchNormalizationBase, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1032,7 +1032,7 @@ class SyncBatchNormalization(BatchNormalizationBase):\n           '`fused` argument cannot be True for SyncBatchNormalization.')\n \n     # Currently we only support aggregating over the global batch size.\n-    super(SyncBatchNormalization, self).__init__(\n+    super().__init__(\n         axis=axis,\n         momentum=momentum,\n         epsilon=epsilon,\n@@ -1232,7 +1232,7 @@ class BatchNormalization(BatchNormalizationBase):\n                beta_constraint=None,\n                gamma_constraint=None,\n                **kwargs):\n-    super(BatchNormalization, self).__init__(\n+    super().__init__(\n         axis=axis,\n         momentum=momentum,\n         epsilon=epsilon,\n\n@@ -217,7 +217,7 @@ class BatchNormalizationTest(test_combinations.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.bn = keras.layers.BatchNormalization()\n \n       @tf.function()\n\n@@ -163,7 +163,7 @@ class LayerNormalization(Layer):\n                beta_constraint=None,\n                gamma_constraint=None,\n                **kwargs):\n-    super(LayerNormalization, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     if isinstance(axis, (list, tuple)):\n       self.axis = list(axis)\n     elif isinstance(axis, int):\n@@ -351,5 +351,5 @@ class LayerNormalization(Layer):\n         'beta_constraint': constraints.serialize(self.beta_constraint),\n         'gamma_constraint': constraints.serialize(self.gamma_constraint)\n     }\n-    base_config = super(LayerNormalization, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -72,6 +72,6 @@ class UnitNormalization(base_layer.Layer):\n     return input_shape\n \n   def get_config(self):\n-    config = super(UnitNormalization, self).get_config()\n+    config = super().get_config()\n     config.update({'axis': self.axis})\n     return config\n\n@@ -126,7 +126,7 @@ class AveragePooling1D(Pooling1D):\n \n   def __init__(self, pool_size=2, strides=None,\n                padding='valid', data_format='channels_last', **kwargs):\n-    super(AveragePooling1D, self).__init__(\n+    super().__init__(\n         functools.partial(backend.pool2d, pool_mode='avg'),\n         pool_size=pool_size,\n         strides=strides,\n\n@@ -129,7 +129,7 @@ class AveragePooling2D(Pooling2D):\n                padding='valid',\n                data_format=None,\n                **kwargs):\n-    super(AveragePooling2D, self).__init__(\n+    super().__init__(\n         tf.nn.avg_pool,\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, **kwargs)\n\n@@ -86,7 +86,7 @@ class AveragePooling3D(Pooling3D):\n                padding='valid',\n                data_format=None,\n                **kwargs):\n-    super(AveragePooling3D, self).__init__(\n+    super().__init__(\n         tf.nn.avg_pool3d,\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, **kwargs)\n\n@@ -25,7 +25,7 @@ class GlobalPooling1D(Layer):\n   \"\"\"Abstract class for different global pooling 1D layers.\"\"\"\n \n   def __init__(self, data_format='channels_last', keepdims=False, **kwargs):\n-    super(GlobalPooling1D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.input_spec = InputSpec(ndim=3)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.keepdims = keepdims\n@@ -48,6 +48,6 @@ class GlobalPooling1D(Layer):\n \n   def get_config(self):\n     config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n-    base_config = super(GlobalPooling1D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -25,7 +25,7 @@ class GlobalPooling2D(Layer):\n   \"\"\"Abstract class for different global pooling 2D layers.\"\"\"\n \n   def __init__(self, data_format=None, keepdims=False, **kwargs):\n-    super(GlobalPooling2D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.input_spec = InputSpec(ndim=4)\n     self.keepdims = keepdims\n@@ -48,5 +48,5 @@ class GlobalPooling2D(Layer):\n \n   def get_config(self):\n     config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n-    base_config = super(GlobalPooling2D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -25,7 +25,7 @@ class GlobalPooling3D(Layer):\n   \"\"\"Abstract class for different global pooling 3D layers.\"\"\"\n \n   def __init__(self, data_format=None, keepdims=False, **kwargs):\n-    super(GlobalPooling3D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.input_spec = InputSpec(ndim=5)\n     self.keepdims = keepdims\n@@ -50,5 +50,5 @@ class GlobalPooling3D(Layer):\n \n   def get_config(self):\n     config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n-    base_config = super(GlobalPooling3D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -48,7 +48,7 @@ class Pooling1D(Layer):\n   def __init__(self, pool_function, pool_size, strides,\n                padding='valid', data_format='channels_last',\n                name=None, **kwargs):\n-    super(Pooling1D, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     if data_format is None:\n       data_format = backend.image_data_format()\n     if strides is None:\n@@ -96,5 +96,5 @@ class Pooling1D(Layer):\n         'padding': self.padding,\n         'data_format': self.data_format,\n     }\n-    base_config = super(Pooling1D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -50,7 +50,7 @@ class Pooling2D(Layer):\n   def __init__(self, pool_function, pool_size, strides,\n                padding='valid', data_format=None,\n                name=None, **kwargs):\n-    super(Pooling2D, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     if data_format is None:\n       data_format = backend.image_data_format()\n     if strides is None:\n@@ -104,5 +104,5 @@ class Pooling2D(Layer):\n         'strides': self.strides,\n         'data_format': self.data_format\n     }\n-    base_config = super(Pooling2D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -52,7 +52,7 @@ class Pooling3D(Layer):\n   def __init__(self, pool_function, pool_size, strides,\n                padding='valid', data_format='channels_last',\n                name=None, **kwargs):\n-    super(Pooling3D, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     if data_format is None:\n       data_format = backend.image_data_format()\n     if strides is None:\n@@ -115,5 +115,5 @@ class Pooling3D(Layer):\n         'strides': self.strides,\n         'data_format': self.data_format\n     }\n-    base_config = super(Pooling3D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -74,7 +74,7 @@ class GlobalAveragePooling1D(GlobalPooling1D):\n   \"\"\"\n \n   def __init__(self, data_format='channels_last', **kwargs):\n-    super(GlobalAveragePooling1D, self).__init__(data_format=data_format,\n+    super().__init__(data_format=data_format,\n                                                  **kwargs)\n     self.supports_masking = True\n \n\n@@ -106,7 +106,7 @@ class MaxPooling1D(Pooling1D):\n   def __init__(self, pool_size=2, strides=None,\n                padding='valid', data_format='channels_last', **kwargs):\n \n-    super(MaxPooling1D, self).__init__(\n+    super().__init__(\n         functools.partial(backend.pool2d, pool_mode='max'),\n         pool_size=pool_size,\n         strides=strides,\n\n@@ -152,7 +152,7 @@ class MaxPooling2D(Pooling2D):\n                padding='valid',\n                data_format=None,\n                **kwargs):\n-    super(MaxPooling2D, self).__init__(\n+    super().__init__(\n         tf.compat.v1.nn.max_pool,\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, **kwargs)\n\n@@ -86,7 +86,7 @@ class MaxPooling3D(Pooling3D):\n                padding='valid',\n                data_format=None,\n                **kwargs):\n-    super(MaxPooling3D, self).__init__(\n+    super().__init__(\n         tf.nn.max_pool3d,\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, **kwargs)\n\n@@ -130,7 +130,7 @@ class CategoryEncoding(base_layer.Layer):\n     if \"dtype\" not in kwargs:\n       kwargs[\"dtype\"] = backend.floatx()\n \n-    super(CategoryEncoding, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"CategoryEncoding\").set(\n         True)\n \n@@ -178,7 +178,7 @@ class CategoryEncoding(base_layer.Layer):\n         \"output_mode\": self.output_mode,\n         \"sparse\": self.sparse,\n     }\n-    base_config = super(CategoryEncoding, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def call(self, inputs, count_weights=None):\n\n@@ -90,7 +90,7 @@ class Resizing(base_layer.Layer):\n     self.interpolation = interpolation\n     self.crop_to_aspect_ratio = crop_to_aspect_ratio\n     self._interpolation_method = image_utils.get_interpolation(interpolation)\n-    super(Resizing, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Resizing').set(True)\n \n   def call(self, inputs):\n@@ -139,7 +139,7 @@ class Resizing(base_layer.Layer):\n         'interpolation': self.interpolation,\n         'crop_to_aspect_ratio': self.crop_to_aspect_ratio,\n     }\n-    base_config = super(Resizing, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -178,7 +178,7 @@ class CenterCrop(base_layer.Layer):\n   def __init__(self, height, width, **kwargs):\n     self.height = height\n     self.width = width\n-    super(CenterCrop, self).__init__(**kwargs, autocast=False)\n+    super().__init__(**kwargs, autocast=False)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('CenterCrop').set(True)\n \n   def call(self, inputs):\n@@ -212,7 +212,7 @@ class CenterCrop(base_layer.Layer):\n         'height': self.height,\n         'width': self.width,\n     }\n-    base_config = super(CenterCrop, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -508,7 +508,7 @@ class RandomCrop(BaseImageAugmentationLayer):\n \n   def __init__(self, height, width, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n-    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed,\n+    super().__init__(**kwargs, autocast=False, seed=seed,\n                                      force_generator=True)\n     self.height = height\n     self.width = width\n@@ -570,7 +570,7 @@ class RandomCrop(BaseImageAugmentationLayer):\n         'width': self.width,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomCrop, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -611,7 +611,7 @@ class Rescaling(base_layer.Layer):\n   def __init__(self, scale, offset=0., **kwargs):\n     self.scale = scale\n     self.offset = offset\n-    super(Rescaling, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Rescaling').set(True)\n \n   def call(self, inputs):\n@@ -628,7 +628,7 @@ class Rescaling(base_layer.Layer):\n         'scale': self.scale,\n         'offset': self.offset,\n     }\n-    base_config = super(Rescaling, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -673,7 +673,7 @@ class RandomFlip(BaseImageAugmentationLayer):\n                mode=HORIZONTAL_AND_VERTICAL,\n                seed=None,\n                **kwargs):\n-    super(RandomFlip, self).__init__(seed=seed, force_generator=True, **kwargs)\n+    super().__init__(seed=seed, force_generator=True, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomFlip').set(True)\n     self.mode = mode\n     if mode == HORIZONTAL:\n@@ -742,7 +742,7 @@ class RandomFlip(BaseImageAugmentationLayer):\n     config = {\n         'mode': self.mode,\n     }\n-    base_config = super(RandomFlip, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -815,7 +815,7 @@ class RandomTranslation(BaseImageAugmentationLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n-    super(RandomTranslation, self).__init__(seed=seed, force_generator=True,\n+    super().__init__(seed=seed, force_generator=True,\n                                             **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n@@ -918,7 +918,7 @@ class RandomTranslation(BaseImageAugmentationLayer):\n         'interpolation': self.interpolation,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomTranslation, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1147,7 +1147,7 @@ class RandomRotation(BaseImageAugmentationLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n-    super(RandomRotation, self).__init__(seed=seed, force_generator=True,\n+    super().__init__(seed=seed, force_generator=True,\n                                          **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n@@ -1248,7 +1248,7 @@ class RandomRotation(BaseImageAugmentationLayer):\n         'interpolation': self.interpolation,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomRotation, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1325,7 +1325,7 @@ class RandomZoom(BaseImageAugmentationLayer):\n                fill_value=0.0,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n-    super(RandomZoom, self).__init__(seed=seed, force_generator=True, **kwargs)\n+    super().__init__(seed=seed, force_generator=True, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -1413,7 +1413,7 @@ class RandomZoom(BaseImageAugmentationLayer):\n         'interpolation': self.interpolation,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomZoom, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1502,7 +1502,7 @@ class RandomContrast(BaseImageAugmentationLayer):\n   def __init__(self, factor, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n-    super(RandomContrast, self).__init__(seed=seed, force_generator=True,\n+    super().__init__(seed=seed, force_generator=True,\n                                          **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n@@ -1544,7 +1544,7 @@ class RandomContrast(BaseImageAugmentationLayer):\n         'factor': self.factor,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomContrast, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1735,7 +1735,7 @@ class RandomHeight(BaseImageAugmentationLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomHeight').set(True)\n-    super(RandomHeight, self).__init__(seed=seed, force_generator=True,\n+    super().__init__(seed=seed, force_generator=True,\n                                        **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n@@ -1803,7 +1803,7 @@ class RandomHeight(BaseImageAugmentationLayer):\n         'interpolation': self.interpolation,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomHeight, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1853,7 +1853,7 @@ class RandomWidth(BaseImageAugmentationLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomWidth').set(True)\n-    super(RandomWidth, self).__init__(seed=seed, force_generator=True, **kwargs)\n+    super().__init__(seed=seed, force_generator=True, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.width_lower = factor[0]\n@@ -1921,5 +1921,5 @@ class RandomWidth(BaseImageAugmentationLayer):\n         'interpolation': self.interpolation,\n         'seed': self.seed,\n     }\n-    base_config = super(RandomWidth, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -366,7 +366,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n     mask_token = None if mask_token is None else np.int64(mask_token)\n     oov_token = None if oov_token is None else np.int64(oov_token)\n \n-    super(IntegerLookup, self).__init__(\n+    super().__init__(\n         max_tokens=max_tokens,\n         num_oov_indices=num_oov_indices,\n         mask_token=mask_token,\n\n@@ -36,7 +36,7 @@ class PL(base_preprocessing_layer.PreprocessingLayer):\n   def __init__(self, **kwargs):\n     self.adapt_time = None\n     self.adapt_count = 0\n-    super(PL, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def adapt(self, data, reset_state=True):\n     self.adapt_time = time.time()\n\n@@ -37,7 +37,7 @@ class PreprocessingStageTest(\n       def __init__(self, **kwargs):\n         self.adapt_time = None\n         self.adapt_count = 0\n-        super(PL, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n       def adapt(self, data, reset_state=True):\n         self.adapt_time = time.time()\n\n@@ -323,7 +323,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n     self.encoding = encoding\n \n-    super(StringLookup, self).__init__(\n+    super().__init__(\n         max_tokens=max_tokens,\n         num_oov_indices=num_oov_indices,\n         mask_token=mask_token,\n@@ -340,7 +340,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   def get_config(self):\n     config = {\"encoding\": self.encoding}\n-    base_config = super(StringLookup, self).get_config()\n+    base_config = super().get_config()\n     # There is only one valid dtype for strings, so we don't expose this.\n     del base_config[\"vocabulary_dtype\"]\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -472,7 +472,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         \"vocabulary\": utils.listify_tensors(vocab),\n         \"idf_weights\": utils.listify_tensors(idf_weights),\n     }\n-    base_config = super(TextVectorization, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def set_vocabulary(self, vocabulary, idf_weights=None):\n\n@@ -38,7 +38,7 @@ class ActivityRegularization(Layer):\n   \"\"\"\n \n   def __init__(self, l1=0., l2=0., **kwargs):\n-    super(ActivityRegularization, self).__init__(\n+    super().__init__(\n         activity_regularizer=regularizers.L1L2(l1=l1, l2=l2), **kwargs)\n     self.supports_masking = True\n     self.l1 = l1\n@@ -49,5 +49,5 @@ class ActivityRegularization(Layer):\n \n   def get_config(self):\n     config = {'l1': self.l1, 'l2': self.l2}\n-    base_config = super(ActivityRegularization, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -55,7 +55,7 @@ class AlphaDropout(base_layer.BaseRandomLayer):\n   \"\"\"\n \n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n-    super(AlphaDropout, self).__init__(seed=seed, **kwargs)\n+    super().__init__(seed=seed, **kwargs)\n     self.rate = rate\n     self.noise_shape = noise_shape\n     self.seed = seed\n@@ -92,7 +92,7 @@ class AlphaDropout(base_layer.BaseRandomLayer):\n \n   def get_config(self):\n     config = {'rate': self.rate, 'seed': self.seed}\n-    base_config = super(AlphaDropout, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -75,7 +75,7 @@ class Dropout(base_layer.BaseRandomLayer):\n   \"\"\"\n \n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n-    super(Dropout, self).__init__(seed=seed, **kwargs)\n+    super().__init__(seed=seed, **kwargs)\n     if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n       raise ValueError(f'Invalid value {rate} received for '\n                        f'`rate`, expected a value between 0 and 1.')\n@@ -121,5 +121,5 @@ class Dropout(base_layer.BaseRandomLayer):\n         'noise_shape': self.noise_shape,\n         'seed': self.seed\n     }\n-    base_config = super(Dropout, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -52,7 +52,7 @@ class GaussianDropout(base_layer.BaseRandomLayer):\n   \"\"\"\n \n   def __init__(self, rate, seed=None, **kwargs):\n-    super(GaussianDropout, self).__init__(seed=seed, **kwargs)\n+    super().__init__(seed=seed, **kwargs)\n     self.supports_masking = True\n     self.rate = rate\n     self.seed = seed\n@@ -73,7 +73,7 @@ class GaussianDropout(base_layer.BaseRandomLayer):\n \n   def get_config(self):\n     config = {'rate': self.rate, 'seed': self.seed}\n-    base_config = super(GaussianDropout, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -54,7 +54,7 @@ class GaussianNoise(base_layer.BaseRandomLayer):\n   \"\"\"\n \n   def __init__(self, stddev, seed=None, **kwargs):\n-    super(GaussianNoise, self).__init__(seed=seed, **kwargs)\n+    super().__init__(seed=seed, **kwargs)\n     self.supports_masking = True\n     self.stddev = stddev\n     self.seed = seed\n@@ -72,7 +72,7 @@ class GaussianNoise(base_layer.BaseRandomLayer):\n \n   def get_config(self):\n     config = {'stddev': self.stddev, 'seed': self.seed}\n-    base_config = super(GaussianNoise, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @tf_utils.shape_type_conversion\n\n@@ -48,7 +48,7 @@ class SpatialDropout1D(Dropout):\n   \"\"\"\n \n   def __init__(self, rate, **kwargs):\n-    super(SpatialDropout1D, self).__init__(rate, **kwargs)\n+    super().__init__(rate, **kwargs)\n     self.input_spec = InputSpec(ndim=3)\n \n   def _get_noise_shape(self, inputs):\n\n@@ -57,7 +57,7 @@ class SpatialDropout2D(Dropout):\n   \"\"\"\n \n   def __init__(self, rate, data_format=None, **kwargs):\n-    super(SpatialDropout2D, self).__init__(rate, **kwargs)\n+    super().__init__(rate, **kwargs)\n     if data_format is None:\n       data_format = backend.image_data_format()\n     if data_format not in {'channels_last', 'channels_first'}:\n\n@@ -57,7 +57,7 @@ class SpatialDropout3D(Dropout):\n   \"\"\"\n \n   def __init__(self, rate, data_format=None, **kwargs):\n-    super(SpatialDropout3D, self).__init__(rate, **kwargs)\n+    super().__init__(rate, **kwargs)\n     if data_format is None:\n       data_format = backend.image_data_format()\n     if data_format not in {'channels_last', 'channels_first'}:\n\n@@ -60,7 +60,7 @@ class Cropping1D(Layer):\n   \"\"\"\n \n   def __init__(self, cropping=(1, 1), **kwargs):\n-    super(Cropping1D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.cropping = conv_utils.normalize_tuple(\n         cropping, 2, 'cropping', allow_zero=True)\n     self.input_spec = InputSpec(ndim=3)\n@@ -85,5 +85,5 @@ class Cropping1D(Layer):\n \n   def get_config(self):\n     config = {'cropping': self.cropping}\n-    base_config = super(Cropping1D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -75,7 +75,7 @@ class Cropping2D(Layer):\n   \"\"\"\n \n   def __init__(self, cropping=((0, 0), (0, 0)), data_format=None, **kwargs):\n-    super(Cropping2D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     if isinstance(cropping, int):\n       self.cropping = ((cropping, cropping), (cropping, cropping))\n@@ -160,5 +160,5 @@ class Cropping2D(Layer):\n \n   def get_config(self):\n     config = {'cropping': self.cropping, 'data_format': self.data_format}\n-    base_config = super(Cropping2D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -79,7 +79,7 @@ class Cropping3D(Layer):\n                cropping=((1, 1), (1, 1), (1, 1)),\n                data_format=None,\n                **kwargs):\n-    super(Cropping3D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     if isinstance(cropping, int):\n       self.cropping = ((cropping, cropping), (cropping, cropping), (cropping,\n@@ -201,5 +201,5 @@ class Cropping3D(Layer):\n \n   def get_config(self):\n     config = {'cropping': self.cropping, 'data_format': self.data_format}\n-    base_config = super(Cropping3D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -59,7 +59,7 @@ class Flatten(Layer):\n   \"\"\"\n \n   def __init__(self, data_format=None, **kwargs):\n-    super(Flatten, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.input_spec = InputSpec(min_ndim=1)\n     self._channels_first = self.data_format == 'channels_first'\n@@ -110,6 +110,6 @@ class Flatten(Layer):\n     return tf.TensorShape(output_shape)\n \n   def get_config(self):\n-    config = super(Flatten, self).get_config()\n+    config = super().get_config()\n     config.update({'data_format': self.data_format})\n     return config\n\n@@ -56,7 +56,7 @@ class Permute(Layer):\n   \"\"\"\n \n   def __init__(self, dims, **kwargs):\n-    super(Permute, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.dims = tuple(dims)\n     if sorted(dims) != list(range(1, len(dims) + 1)):\n       raise ValueError(\n@@ -78,5 +78,5 @@ class Permute(Layer):\n \n   def get_config(self):\n     config = {'dims': self.dims}\n-    base_config = super(Permute, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -46,7 +46,7 @@ class RepeatVector(Layer):\n   \"\"\"\n \n   def __init__(self, n, **kwargs):\n-    super(RepeatVector, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.n = n\n     if not isinstance(n, int):\n       raise TypeError(f'Expected an integer value for `n`, got {type(n)}.')\n@@ -61,5 +61,5 @@ class RepeatVector(Layer):\n \n   def get_config(self):\n     config = {'n': self.n}\n-    base_config = super(RepeatVector, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -63,7 +63,7 @@ class Reshape(Layer):\n         samples dimension (batch size).\n       **kwargs: Any additional layer keyword arguments.\n     \"\"\"\n-    super(Reshape, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.target_shape = tuple(target_shape)\n \n   def _fix_unknown_dimension(self, input_shape, output_shape):\n@@ -133,5 +133,5 @@ class Reshape(Layer):\n \n   def get_config(self):\n     config = {'target_shape': self.target_shape}\n-    base_config = super(Reshape, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -61,7 +61,7 @@ class UpSampling1D(Layer):\n   \"\"\"\n \n   def __init__(self, size=2, **kwargs):\n-    super(UpSampling1D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.size = int(size)\n     self.input_spec = InputSpec(ndim=3)\n \n@@ -76,5 +76,5 @@ class UpSampling1D(Layer):\n \n   def get_config(self):\n     config = {'size': self.size}\n-    base_config = super(UpSampling1D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -88,7 +88,7 @@ class UpSampling2D(Layer):\n                data_format=None,\n                interpolation='nearest',\n                **kwargs):\n-    super(UpSampling2D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.size = conv_utils.normalize_tuple(size, 2, 'size')\n     interpolations = {\n@@ -136,5 +136,5 @@ class UpSampling2D(Layer):\n         'data_format': self.data_format,\n         'interpolation': self.interpolation\n     }\n-    base_config = super(UpSampling2D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -72,7 +72,7 @@ class UpSampling3D(Layer):\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.size = conv_utils.normalize_tuple(size, 3, 'size')\n     self.input_spec = InputSpec(ndim=5)\n-    super(UpSampling3D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n@@ -101,5 +101,5 @@ class UpSampling3D(Layer):\n \n   def get_config(self):\n     config = {'size': self.size, 'data_format': self.data_format}\n-    base_config = super(UpSampling3D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -70,7 +70,7 @@ class ZeroPadding1D(Layer):\n   \"\"\"\n \n   def __init__(self, padding=1, **kwargs):\n-    super(ZeroPadding1D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.padding = conv_utils.normalize_tuple(\n         padding, 2, 'padding', allow_zero=True)\n     self.input_spec = InputSpec(ndim=3)\n@@ -87,5 +87,5 @@ class ZeroPadding1D(Layer):\n \n   def get_config(self):\n     config = {'padding': self.padding}\n-    base_config = super(ZeroPadding1D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -92,7 +92,7 @@ class ZeroPadding2D(Layer):\n   \"\"\"\n \n   def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n-    super(ZeroPadding2D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     if isinstance(padding, int):\n       self.padding = ((padding, padding), (padding, padding))\n@@ -145,5 +145,5 @@ class ZeroPadding2D(Layer):\n \n   def get_config(self):\n     config = {'padding': self.padding, 'data_format': self.data_format}\n-    base_config = super(ZeroPadding2D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -79,7 +79,7 @@ class ZeroPadding3D(Layer):\n   \"\"\"\n \n   def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):\n-    super(ZeroPadding3D, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     if isinstance(padding, int):\n       self.padding = ((padding, padding), (padding, padding), (padding,\n@@ -146,5 +146,5 @@ class ZeroPadding3D(Layer):\n \n   def get_config(self):\n     config = {'padding': self.padding, 'data_format': self.data_format}\n-    base_config = super(ZeroPadding3D, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -111,7 +111,7 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n                dropout=0.0,\n                recurrent_dropout=0.0,\n                **kwargs):\n-    super(ConvLSTMCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.rank = rank\n     if self.rank > 3:\n       raise ValueError(f'Rank {rank} convolutions are not currently '\n@@ -326,7 +326,7 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         'recurrent_dropout':\n             self.recurrent_dropout,\n     }\n-    base_config = super(ConvLSTMCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -462,7 +462,7 @@ class ConvLSTM(ConvRNN):\n         dropout=dropout,\n         recurrent_dropout=recurrent_dropout,\n         dtype=kwargs.get('dtype'))\n-    super(ConvLSTM, self).__init__(\n+    super().__init__(\n         rank,\n         cell,\n         return_sequences=return_sequences,\n@@ -473,7 +473,7 @@ class ConvLSTM(ConvRNN):\n     self.activity_regularizer = regularizers.get(activity_regularizer)\n \n   def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(ConvLSTM, self).call(\n+    return super().call(\n         inputs, mask=mask, training=training, initial_state=initial_state)\n \n   @property\n@@ -591,7 +591,7 @@ class ConvLSTM(ConvRNN):\n               'bias_constraint': constraints.serialize(self.bias_constraint),\n               'dropout': self.dropout,\n               'recurrent_dropout': self.recurrent_dropout}\n-    base_config = super(ConvLSTM, self).get_config()\n+    base_config = super().get_config()\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -137,7 +137,7 @@ class ConvRNN(RNN):\n                       'stack convolutional cells. Only pass a single cell '\n                       'instance as the `cell` argument. Received: '\n                       f'cell={cell}')\n-    super(ConvRNN, self).__init__(cell, return_sequences, return_state,\n+    super().__init__(cell, return_sequences, return_state,\n                                   go_backwards, stateful, unroll, **kwargs)\n     self.rank = rank\n     self.input_spec = [InputSpec(ndim=rank + 3)]\n\n@@ -138,7 +138,7 @@ class _CuDNNRNN(RNN):\n \n   @property\n   def losses(self):\n-    return super(RNN, self).losses\n+    return super(RNN, self).losses  # pylint: disable=bad-super-call\n \n   def get_losses_for(self, inputs=None):\n     return super(  # pylint: disable=bad-super-call\n\n@@ -253,7 +253,7 @@ class RNN(base_layer.Layer):\n                      kwargs.pop('input_dim', None))\n       kwargs['input_shape'] = input_shape\n \n-    super(RNN, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.cell = cell\n     self.return_sequences = return_sequences\n     self.return_state = return_state\n@@ -285,7 +285,7 @@ class RNN(base_layer.Layer):\n       # called with any time step value, as long as it is not None), so it\n       # cannot be used as the call function signature when saving to SavedModel.\n       return False\n-    return super(RNN, self)._use_input_spec_as_call_signature\n+    return super()._use_input_spec_as_call_signature\n \n   @property\n   def states(self):\n@@ -512,7 +512,7 @@ class RNN(base_layer.Layer):\n         inputs, initial_state, constants, self._num_constants)\n \n     if initial_state is None and constants is None:\n-      return super(RNN, self).__call__(inputs, **kwargs)\n+      return super().__call__(inputs, **kwargs)\n \n     # If any of `initial_state` or `constants` are specified and are Keras\n     # tensors, then add them to the inputs and temporarily modify the\n@@ -559,7 +559,7 @@ class RNN(base_layer.Layer):\n             tf.nest.map_structure(lambda _: None, inputs)) + additional_specs\n       # Perform the call with temporarily replaced input_spec\n       self.input_spec = full_input_spec\n-      output = super(RNN, self).__call__(full_input, **kwargs)\n+      output = super().__call__(full_input, **kwargs)\n       # Remove the additional_specs from input spec and keep the rest. It is\n       # important to keep since the input spec was populated by build(), and\n       # will be reused in the stateful=True.\n@@ -570,7 +570,7 @@ class RNN(base_layer.Layer):\n         kwargs['initial_state'] = initial_state\n       if constants is not None:\n         kwargs['constants'] = constants\n-      return super(RNN, self).__call__(inputs, **kwargs)\n+      return super().__call__(inputs, **kwargs)\n \n   def call(self,\n            inputs,\n@@ -845,7 +845,7 @@ class RNN(base_layer.Layer):\n       config['zero_output_for_mask'] = self.zero_output_for_mask\n \n     config['cell'] = generic_utils.serialize_keras_object(self.cell)\n-    base_config = super(RNN, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n\n@@ -136,7 +136,7 @@ class RNNTest(test_combinations.TestCase):\n       def __init__(self, units, **kwargs):\n         self.units = units\n         self.state_size = units\n-        super(MinimalRNNCell, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n       def build(self, input_shape):\n         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n@@ -156,7 +156,7 @@ class RNNTest(test_combinations.TestCase):\n \n       def get_config(self):\n         config = {'units': self.units}\n-        base_config = super(MinimalRNNCell, self).get_config()\n+        base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n     # Test basic case.\n@@ -216,7 +216,7 @@ class RNNTest(test_combinations.TestCase):\n \n       def __init__(self, units, **kwargs):\n         self.units = units\n-        super(MinimalRNNCell, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n       @property\n       def state_size(self):\n@@ -661,7 +661,7 @@ class RNNTest(test_combinations.TestCase):\n       def __init__(self, units, **kwargs):\n         self.units = units\n         self.state_size = units\n-        super(CustomRNNCell, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n \n       def build(self, input_shape):\n         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n@@ -681,7 +681,7 @@ class RNNTest(test_combinations.TestCase):\n \n       def get_config(self):\n         config = {'units': self.units}\n-        base_config = super(CustomRNNCell, self).get_config()\n+        base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n     for cell_class in [keras.layers.SimpleRNNCell,\n@@ -874,7 +874,7 @@ class RNNTest(test_combinations.TestCase):\n     class CellWrapper(keras.layers.AbstractRNNCell):\n \n       def __init__(self, cell):\n-        super(CellWrapper, self).__init__()\n+        super().__init__()\n         self.cell = cell\n \n       @property\n@@ -1325,7 +1325,7 @@ class RNNTest(test_combinations.TestCase):\n       def __init__(self):\n         self.state_size = None\n         self.output_size = None\n-        super(Cell, self).__init__()\n+        super().__init__()\n \n       def build(self, input_shape):\n         self.state_size = input_shape[-1]\n@@ -1741,7 +1741,7 @@ class RNNTest(test_combinations.TestCase):\n       def __init__(self):\n         self.state_size = ((), [], ())\n         self.output_size = None\n-        super(StatelessCell, self).__init__()\n+        super().__init__()\n \n       def build(self, input_shape):\n         self.output_size = input_shape[-1]\n@@ -1803,7 +1803,7 @@ class RNNCellWithConstants(keras.layers.Layer):\n     self.units = units\n     self.state_size = units\n     self.constant_size = constant_size\n-    super(RNNCellWithConstants, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def build(self, input_shape):\n     self.input_kernel = self.add_weight(\n@@ -1831,7 +1831,7 @@ class RNNCellWithConstants(keras.layers.Layer):\n \n   def get_config(self):\n     config = {'units': self.units, 'constant_size': self.constant_size}\n-    base_config = super(RNNCellWithConstants, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1847,7 +1847,7 @@ class Minimal2DRNNCell(keras.layers.Layer):\n     self.unit_b = unit_b\n     self.state_size = tf.TensorShape([unit_a, unit_b])\n     self.output_size = tf.TensorShape([unit_a, unit_b])\n-    super(Minimal2DRNNCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def build(self, input_shape):\n     input_a = input_shape[-2]\n@@ -1880,7 +1880,7 @@ class PlusOneRNNCell(keras.layers.Layer):\n \n   def __init__(self, num_unit, **kwargs):\n     self.state_size = num_unit\n-    super(PlusOneRNNCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def build(self, input_shape):\n     self.output_size = input_shape[-1]\n@@ -1896,7 +1896,7 @@ class NestedCell(keras.layers.Layer):\n     self.unit_2 = unit_2\n     self.unit_3 = unit_3\n     self.use_tuple = use_tuple\n-    super(NestedCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     # A nested state.\n     if use_tuple:\n       self.state_size = NestedState(\n\n@@ -41,7 +41,7 @@ class Wrapper(Layer):\n   def __init__(self, layer, **kwargs):\n     assert isinstance(layer, Layer)\n     self.layer = layer\n-    super(Wrapper, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def build(self, input_shape=None):\n     if not self.layer.built:\n@@ -58,7 +58,7 @@ class Wrapper(Layer):\n \n   def get_config(self):\n     config = {'layer': generic_utils.serialize_keras_object(self.layer)}\n-    base_config = super(Wrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n\n@@ -123,7 +123,7 @@ class Bidirectional(Wrapper):\n     # We don't want to track `layer` since we're already tracking the two copies\n     # of it we actually run.\n     self._setattr_tracking = False\n-    super(Bidirectional, self).__init__(layer, **kwargs)\n+    super().__init__(layer, **kwargs)\n     self._setattr_tracking = True\n \n     # Recreate the forward layer from the original layer config, so that it will\n@@ -246,7 +246,7 @@ class Bidirectional(Wrapper):\n       inputs = inputs[0]\n \n     if initial_state is None and constants is None:\n-      return super(Bidirectional, self).__call__(inputs, **kwargs)\n+      return super().__call__(inputs, **kwargs)\n \n     # Applies the same workaround as in `RNN.__call__`\n     additional_inputs = []\n@@ -306,11 +306,11 @@ class Bidirectional(Wrapper):\n       # Perform the call with temporarily replaced input_spec\n       original_input_spec = self.input_spec\n       self.input_spec = full_input_spec\n-      output = super(Bidirectional, self).__call__(full_input, **kwargs)\n+      output = super().__call__(full_input, **kwargs)\n       self.input_spec = original_input_spec\n       return output\n     else:\n-      return super(Bidirectional, self).__call__(inputs, **kwargs)\n+      return super().__call__(inputs, **kwargs)\n \n   def call(self,\n            inputs,\n@@ -443,7 +443,7 @@ class Bidirectional(Wrapper):\n \n     if hasattr(self, '_backward_layer_config'):\n       config['backward_layer'] = self._backward_layer_config\n-    base_config = super(Bidirectional, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n\n@@ -38,7 +38,7 @@ class _RNNCellWithConstants(keras.layers.Layer):\n     self.units = units\n     self.state_size = units\n     self.constant_size = constant_size\n-    super(_RNNCellWithConstants, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def build(self, input_shape):\n     self.input_kernel = self.add_weight(\n@@ -66,14 +66,14 @@ class _RNNCellWithConstants(keras.layers.Layer):\n \n   def get_config(self):\n     config = {'units': self.units, 'constant_size': self.constant_size}\n-    base_config = super(_RNNCellWithConstants, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n class _ResidualLSTMCell(keras.layers.LSTMCell):\n \n   def call(self, inputs, states, training=None):\n-    output, states = super(_ResidualLSTMCell, self).call(inputs, states)\n+    output, states = super().call(inputs, states)\n     return output + inputs, states\n \n \n@@ -579,13 +579,13 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     class TestListLayer(TestLayer):\n \n       def compute_output_shape(self, input_shape):\n-        shape = super(TestListLayer, self).compute_output_shape(input_shape)\n+        shape = super().compute_output_shape(input_shape)\n         return shape.as_list()\n \n     class TestTupleLayer(TestLayer):\n \n       def compute_output_shape(self, input_shape):\n-        shape = super(TestTupleLayer, self).compute_output_shape(input_shape)\n+        shape = super().compute_output_shape(input_shape)\n         return tuple(shape.as_list())\n \n     # Layers can specify output shape as list/tuple/TensorShape\n\n@@ -45,7 +45,7 @@ class _RNNCellWrapper(AbstractRNNCell):\n   \"\"\"\n \n   def __init__(self, cell, *args, **kwargs):\n-    super(_RNNCellWrapper, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n     self.cell = cell\n     cell_call_spec = tf_inspect.getfullargspec(cell.call)\n     self._call_spec.expects_training_arg = ((\"training\"\n@@ -123,7 +123,7 @@ class _RNNCellWrapper(AbstractRNNCell):\n             \"config\": self.cell.get_config()\n         },\n     }\n-    base_config = super(_RNNCellWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -210,7 +210,7 @@ class DropoutWrapper(_RNNCellWrapper):\n       raise ValueError(\"keras LSTM cell does not work with DropoutWrapper. \"\n                        \"Please use LSTMCell(dropout=x, recurrent_dropout=y) \"\n                        \"instead.\")\n-    super(DropoutWrapper, self).__init__(cell, dtype=dtype, **kwargs)\n+    super().__init__(cell, dtype=dtype, **kwargs)\n \n     if (dropout_state_filter_visitor is not None and\n         not callable(dropout_state_filter_visitor)):\n@@ -389,7 +389,7 @@ class DropoutWrapper(_RNNCellWrapper):\n       config.update({\"dropout_fn\": function,\n                      \"dropout_fn_type\": function_type,\n                      \"dropout_fn_module\": function_module})\n-    base_config = super(DropoutWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -420,7 +420,7 @@ class ResidualWrapper(_RNNCellWrapper):\n           and outputs.\n       **kwargs: dict of keyword arguments for base layer.\n     \"\"\"\n-    super(ResidualWrapper, self).__init__(cell, **kwargs)\n+    super().__init__(cell, **kwargs)\n     self._residual_fn = residual_fn\n \n   def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n@@ -466,7 +466,7 @@ class ResidualWrapper(_RNNCellWrapper):\n       }\n     else:\n       config = {}\n-    base_config = super(ResidualWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -496,7 +496,7 @@ class DeviceWrapper(_RNNCellWrapper):\n       device: A device string or function, for passing to `tf.device`.\n       **kwargs: dict of keyword arguments for base layer.\n     \"\"\"\n-    super(DeviceWrapper, self).__init__(cell, **kwargs)\n+    super().__init__(cell, **kwargs)\n     self._device = device\n \n   def zero_state(self, batch_size, dtype):\n@@ -511,7 +511,7 @@ class DeviceWrapper(_RNNCellWrapper):\n \n   def get_config(self):\n     config = {\"device\": self._device}\n-    base_config = super(DeviceWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -153,7 +153,7 @@ class ConvLSTM1D(ConvLSTM):\n                dropout=0.0,\n                recurrent_dropout=0.0,\n                **kwargs):\n-    super(ConvLSTM1D, self).__init__(\n+    super().__init__(\n         rank=1,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -155,7 +155,7 @@ class ConvLSTM2D(ConvLSTM):\n                dropout=0.0,\n                recurrent_dropout=0.0,\n                **kwargs):\n-    super(ConvLSTM2D, self).__init__(\n+    super().__init__(\n         rank=2,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -155,7 +155,7 @@ class ConvLSTM3D(ConvLSTM):\n                dropout=0.0,\n                recurrent_dropout=0.0,\n                **kwargs):\n-    super(ConvLSTM3D, self).__init__(\n+    super().__init__(\n         rank=3,\n         filters=filters,\n         kernel_size=kernel_size,\n\n@@ -85,7 +85,7 @@ class CuDNNGRU(_CuDNNRNN):\n     self.units = units\n     cell_spec = collections.namedtuple('cell', 'state_size')\n     self._cell = cell_spec(state_size=self.units)\n-    super(CuDNNGRU, self).__init__(\n+    super().__init__(\n         return_sequences=return_sequences,\n         return_state=return_state,\n         go_backwards=go_backwards,\n@@ -110,7 +110,7 @@ class CuDNNGRU(_CuDNNRNN):\n     return self._cell\n \n   def build(self, input_shape):\n-    super(CuDNNGRU, self).build(input_shape)\n+    super().build(input_shape)\n     if isinstance(input_shape, list):\n       input_shape = input_shape[0]\n     input_dim = int(input_shape[-1])\n@@ -203,5 +203,5 @@ class CuDNNGRU(_CuDNNRNN):\n             constraints.serialize(self.recurrent_constraint),\n         'bias_constraint': constraints.serialize(self.bias_constraint)\n     }\n-    base_config = super(CuDNNGRU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -90,7 +90,7 @@ class CuDNNLSTM(_CuDNNRNN):\n     self.units = units\n     cell_spec = collections.namedtuple('cell', 'state_size')\n     self._cell = cell_spec(state_size=(self.units, self.units))\n-    super(CuDNNLSTM, self).__init__(\n+    super().__init__(\n         return_sequences=return_sequences,\n         return_state=return_state,\n         go_backwards=go_backwards,\n@@ -116,7 +116,7 @@ class CuDNNLSTM(_CuDNNRNN):\n     return self._cell\n \n   def build(self, input_shape):\n-    super(CuDNNLSTM, self).build(input_shape)\n+    super().build(input_shape)\n     if isinstance(input_shape, list):\n       input_shape = input_shape[0]\n     input_dim = int(input_shape[-1])\n@@ -226,5 +226,5 @@ class CuDNNLSTM(_CuDNNRNN):\n             constraints.serialize(self.recurrent_constraint),\n         'bias_constraint': constraints.serialize(self.bias_constraint)\n     }\n-    base_config = super(CuDNNLSTM, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -40,7 +40,7 @@ class DropoutRNNCellMixin:\n \n   def __init__(self, *args, **kwargs):\n     self._create_non_trackable_mask_cache()\n-    super(DropoutRNNCellMixin, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _create_non_trackable_mask_cache(self):\n@@ -148,7 +148,7 @@ class DropoutRNNCellMixin:\n   def __getstate__(self):\n     # Used for deepcopy. The caching can't be pickled by python, since it will\n     # contain tensor and graph.\n-    state = super(DropoutRNNCellMixin, self).__getstate__()\n+    state = super().__getstate__()\n     state.pop('_dropout_mask_cache', None)\n     state.pop('_recurrent_dropout_mask_cache', None)\n     return state\n@@ -158,7 +158,7 @@ class DropoutRNNCellMixin:\n         self._create_dropout_mask)\n     state['_recurrent_dropout_mask_cache'] = backend.ContextValueCache(\n         self._create_recurrent_dropout_mask)\n-    super(DropoutRNNCellMixin, self).__setstate__(state)\n+    super().__setstate__(state)\n \n \n def _generate_dropout_mask(generator, ones, rate, training=None, count=1):\n\n@@ -139,7 +139,7 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n     else:\n       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(GRUCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.units = units\n     self.activation = activations.get(activation)\n     self.recurrent_activation = activations.get(recurrent_activation)\n@@ -339,7 +339,7 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         'reset_after': self.reset_after\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self))\n-    base_config = super(GRUCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n@@ -535,7 +535,7 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n         dtype=kwargs.get('dtype'),\n         trainable=kwargs.get('trainable', True),\n         **cell_kwargs)\n-    super(GRU, self).__init__(\n+    super().__init__(\n         cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n@@ -732,7 +732,7 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             self.reset_after\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n-    base_config = super(GRU, self).get_config()\n+    base_config = super().get_config()\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -95,7 +95,7 @@ class GRUCell(gru.GRUCell):\n                recurrent_dropout=0.,\n                reset_after=False,\n                **kwargs):\n-    super(GRUCell, self).__init__(\n+    super().__init__(\n         units,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n@@ -261,7 +261,7 @@ class GRU(RNN):\n         dtype=kwargs.get('dtype'),\n         trainable=kwargs.get('trainable', True),\n         **cell_kwargs)\n-    super(GRU, self).__init__(\n+    super().__init__(\n         cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n@@ -273,7 +273,7 @@ class GRU(RNN):\n     self.input_spec = [InputSpec(ndim=3)]\n \n   def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(GRU, self).call(\n+    return super().call(\n         inputs, mask=mask, training=training, initial_state=initial_state)\n \n   @property\n@@ -384,7 +384,7 @@ class GRU(RNN):\n             self.reset_after\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n-    base_config = super(GRU, self).get_config()\n+    base_config = super().get_config()\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -87,7 +87,7 @@ class _RNNCellWrapperV1(RNNCell):\n   \"\"\"\n \n   def __init__(self, cell, *args, **kwargs):\n-    super(_RNNCellWrapperV1, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n     assert_like_rnncell(\"cell\", cell)\n     self.cell = cell\n     if isinstance(cell, tf.__internal__.tracking.Trackable):\n@@ -156,7 +156,7 @@ class _RNNCellWrapperV1(RNNCell):\n             \"config\": self.cell.get_config()\n         },\n     }\n-    base_config = super(_RNNCellWrapperV1, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -245,7 +245,7 @@ class DropoutWrapper(_RNNCellWrapperV1):\n         but not `callable`.\n       ValueError: if any of the keep_probs are not between 0 and 1.\n     \"\"\"\n-    super(DropoutWrapper, self).__init__(cell, dtype=dtype, **kwargs)\n+    super().__init__(cell, dtype=dtype, **kwargs)\n \n     if (dropout_state_filter_visitor is not None and\n         not callable(dropout_state_filter_visitor)):\n@@ -432,7 +432,7 @@ class DropoutWrapper(_RNNCellWrapperV1):\n       config.update({\"dropout_fn\": function,\n                      \"dropout_fn_type\": function_type,\n                      \"dropout_fn_module\": function_module})\n-    base_config = super(DropoutWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -464,7 +464,7 @@ class ResidualWrapper(_RNNCellWrapperV1):\n           and outputs.\n       **kwargs: dict of keyword arguments for base layer.\n     \"\"\"\n-    super(ResidualWrapper, self).__init__(cell, **kwargs)\n+    super().__init__(cell, **kwargs)\n     self._residual_fn = residual_fn\n \n   def _call_wrapped_cell(self, inputs, state, cell_call_fn, **kwargs):\n@@ -510,7 +510,7 @@ class ResidualWrapper(_RNNCellWrapperV1):\n       }\n     else:\n       config = {}\n-    base_config = super(ResidualWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -541,7 +541,7 @@ class DeviceWrapper(_RNNCellWrapperV1):\n       device: A device string or function, for passing to `tf.device`.\n       **kwargs: dict of keyword arguments for base layer.\n     \"\"\"\n-    super(DeviceWrapper, self).__init__(cell, **kwargs)\n+    super().__init__(cell, **kwargs)\n     self._device = device\n \n   def zero_state(self, batch_size, dtype):\n@@ -556,7 +556,7 @@ class DeviceWrapper(_RNNCellWrapperV1):\n \n   def get_config(self):\n     config = {\"device\": self._device}\n-    base_config = super(DeviceWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -160,7 +160,7 @@ class RNNCell(base_layer.Layer):\n   \"\"\"\n \n   def __init__(self, trainable=True, name=None, dtype=None, **kwargs):\n-    super(RNNCell, self).__init__(\n+    super().__init__(\n         trainable=trainable, name=name, dtype=dtype, **kwargs)\n     # Attribute that indicates whether the cell is a TF RNN cell, due the slight\n     # difference between TF and Keras RNN cell. Notably the state is not wrapped\n@@ -189,7 +189,7 @@ class RNNCell(base_layer.Layer):\n     if scope is not None:\n       with tf.compat.v1.variable_scope(\n           scope, custom_getter=self._rnn_get_variable) as scope:\n-        return super(RNNCell, self).__call__(inputs, state, scope=scope)\n+        return super().__call__(inputs, state, scope=scope)\n     else:\n       scope_attrname = \"rnncell_scope\"\n       scope = getattr(self, scope_attrname, None)\n@@ -199,7 +199,7 @@ class RNNCell(base_layer.Layer):\n             custom_getter=self._rnn_get_variable)\n         setattr(self, scope_attrname, scope)\n       with scope:\n-        return super(RNNCell, self).__call__(inputs, state)\n+        return super().__call__(inputs, state)\n \n   def _rnn_get_variable(self, getter, *args, **kwargs):\n     variable = getter(*args, **kwargs)\n@@ -298,7 +298,7 @@ class RNNCell(base_layer.Layer):\n \n   # TODO(b/134773139): Remove when contrib RNN cells implement `get_config`\n   def get_config(self):  # pylint: disable=useless-super-delegation\n-    return super(RNNCell, self).get_config()\n+    return super().get_config()\n \n   @property\n   def _use_input_spec_as_call_signature(self):\n@@ -385,7 +385,7 @@ class BasicRNNCell(LayerRNNCell):\n         \"is equivalent as `tf.keras.layers.SimpleRNNCell`, \"\n         \"and will be replaced by that in Tensorflow 2.0.\",\n         stacklevel=2)\n-    super(BasicRNNCell, self).__init__(\n+    super().__init__(\n         _reuse=reuse, name=name, dtype=dtype, **kwargs)\n     _check_supported_dtypes(self.dtype)\n     if tf.executing_eagerly() and tf.config.list_logical_devices(\"GPU\"):\n@@ -445,7 +445,7 @@ class BasicRNNCell(LayerRNNCell):\n         \"activation\": activations.serialize(self._activation),\n         \"reuse\": self._reuse,\n     }\n-    base_config = super(BasicRNNCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -495,7 +495,7 @@ class GRUCell(LayerRNNCell):\n         \"is equivalent as `tf.keras.layers.GRUCell`, \"\n         \"and will be replaced by that in Tensorflow 2.0.\",\n         stacklevel=2)\n-    super(GRUCell, self).__init__(\n+    super().__init__(\n         _reuse=reuse, name=name, dtype=dtype, **kwargs)\n     _check_supported_dtypes(self.dtype)\n \n@@ -583,7 +583,7 @@ class GRUCell(LayerRNNCell):\n         \"activation\": activations.serialize(self._activation),\n         \"reuse\": self._reuse,\n     }\n-    base_config = super(GRUCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -673,7 +673,7 @@ class BasicLSTMCell(LayerRNNCell):\n         \"is equivalent as `tf.keras.layers.LSTMCell`, \"\n         \"and will be replaced by that in Tensorflow 2.0.\",\n         stacklevel=2)\n-    super(BasicLSTMCell, self).__init__(\n+    super().__init__(\n         _reuse=reuse, name=name, dtype=dtype, **kwargs)\n     _check_supported_dtypes(self.dtype)\n     if not state_is_tuple:\n@@ -781,7 +781,7 @@ class BasicLSTMCell(LayerRNNCell):\n         \"activation\": activations.serialize(self._activation),\n         \"reuse\": self._reuse,\n     }\n-    base_config = super(BasicLSTMCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -878,7 +878,7 @@ class LSTMCell(LayerRNNCell):\n         \"is equivalent as `tf.keras.layers.LSTMCell`, \"\n         \"and will be replaced by that in Tensorflow 2.0.\",\n         stacklevel=2)\n-    super(LSTMCell, self).__init__(\n+    super().__init__(\n         _reuse=reuse, name=name, dtype=dtype, **kwargs)\n     _check_supported_dtypes(self.dtype)\n     if not state_is_tuple:\n@@ -1072,7 +1072,7 @@ class LSTMCell(LayerRNNCell):\n         \"activation\": activations.serialize(self._activation),\n         \"reuse\": self._reuse,\n     }\n-    base_config = super(LSTMCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1106,7 +1106,7 @@ class MultiRNNCell(RNNCell):\n     logging.warning(\"`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class \"\n                     \"is equivalent as `tf.keras.layers.StackedRNNCells`, \"\n                     \"and will be replaced by that in Tensorflow 2.0.\")\n-    super(MultiRNNCell, self).__init__()\n+    super().__init__()\n     if not cells:\n       raise ValueError(\"Must specify at least one cell for MultiRNNCell.\")\n     if not tf.nest.is_nested(cells):\n@@ -1150,7 +1150,7 @@ class MultiRNNCell(RNNCell):\n       else:\n         # We know here that state_size of each cell is not a tuple and\n         # presumably does not contain TensorArrays or anything else fancy\n-        return super(MultiRNNCell, self).zero_state(batch_size, dtype)\n+        return super().zero_state(batch_size, dtype)\n \n   @property\n   def trainable_weights(self):\n\n@@ -143,7 +143,7 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n     else:\n       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(LSTMCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.units = units\n     self.activation = activations.get(activation)\n     self.recurrent_activation = activations.get(recurrent_activation)\n@@ -335,7 +335,7 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n             self.implementation\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self))\n-    base_config = super(LSTMCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n@@ -516,7 +516,7 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n         dtype=kwargs.get('dtype'),\n         trainable=kwargs.get('trainable', True),\n         **cell_kwargs)\n-    super(LSTM, self).__init__(\n+    super().__init__(\n         cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n@@ -802,7 +802,7 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             self.implementation\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n-    base_config = super(LSTM, self).get_config()\n+    base_config = super().get_config()\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -98,7 +98,7 @@ class LSTMCell(lstm.LSTMCell):\n                dropout=0.,\n                recurrent_dropout=0.,\n                **kwargs):\n-    super(LSTMCell, self).__init__(\n+    super().__init__(\n         units,\n         activation=activation,\n         recurrent_activation=recurrent_activation,\n@@ -261,7 +261,7 @@ class LSTM(RNN):\n         dtype=kwargs.get('dtype'),\n         trainable=kwargs.get('trainable', True),\n         **cell_kwargs)\n-    super(LSTM, self).__init__(\n+    super().__init__(\n         cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n@@ -273,7 +273,7 @@ class LSTM(RNN):\n     self.input_spec = [InputSpec(ndim=3)]\n \n   def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(LSTM, self).call(\n+    return super().call(\n         inputs, mask=mask, training=training, initial_state=initial_state)\n \n   @property\n@@ -384,7 +384,7 @@ class LSTM(RNN):\n             self.implementation\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n-    base_config = super(LSTM, self).get_config()\n+    base_config = super().get_config()\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -125,7 +125,7 @@ class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n     else:\n       self._enable_caching_device = kwargs.pop('enable_caching_device', False)\n-    super(SimpleRNNCell, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.units = units\n     self.activation = activations.get(activation)\n     self.use_bias = use_bias\n@@ -234,7 +234,7 @@ class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n             self.recurrent_dropout\n     }\n     config.update(rnn_utils.config_for_enable_caching_device(self))\n-    base_config = super(SimpleRNNCell, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -376,7 +376,7 @@ class SimpleRNN(RNN):\n         dtype=kwargs.get('dtype'),\n         trainable=kwargs.get('trainable', True),\n         **cell_kwargs)\n-    super(SimpleRNN, self).__init__(\n+    super().__init__(\n         cell,\n         return_sequences=return_sequences,\n         return_state=return_state,\n@@ -388,7 +388,7 @@ class SimpleRNN(RNN):\n     self.input_spec = [InputSpec(ndim=3)]\n \n   def call(self, inputs, mask=None, training=None, initial_state=None):\n-    return super(SimpleRNN, self).call(\n+    return super().call(\n         inputs, mask=mask, training=training, initial_state=initial_state)\n \n   @property\n@@ -480,7 +480,7 @@ class SimpleRNN(RNN):\n         'recurrent_dropout':\n             self.recurrent_dropout\n     }\n-    base_config = super(SimpleRNN, self).get_config()\n+    base_config = super().get_config()\n     config.update(rnn_utils.config_for_enable_caching_device(self.cell))\n     del base_config['cell']\n     return dict(list(base_config.items()) + list(config.items()))\n\n@@ -73,7 +73,7 @@ class StackedRNNCells(base_layer.Layer):\n                       'be deprecated. Please update the code to work with the '\n                       'natural order of states if you rely on the RNN states, '\n                       'eg RNN(return_state=True).')\n-    super(StackedRNNCells, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   @property\n   def state_size(self):\n@@ -167,7 +167,7 @@ class StackedRNNCells(base_layer.Layer):\n     for cell in self.cells:\n       cells.append(generic_utils.serialize_keras_object(cell))\n     config = {'cells': cells}\n-    base_config = super(StackedRNNCells, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n\n@@ -72,7 +72,7 @@ class TimeDistributed(Wrapper):\n       raise ValueError(\n           'Please initialize `TimeDistributed` layer with a '\n           f'`tf.keras.layers.Layer` instance. Received: {layer}')\n-    super(TimeDistributed, self).__init__(layer, **kwargs)\n+    super().__init__(layer, **kwargs)\n     self.supports_masking = True\n \n     # It is safe to use the fast, reshape-based approach with all of our\n@@ -132,7 +132,7 @@ class TimeDistributed(Wrapper):\n     child_input_shape = tf.nest.map_structure(self._remove_timesteps,\n                                               input_shape)\n     child_input_shape = tf_utils.convert_shapes(child_input_shape)\n-    super(TimeDistributed, self).build(tuple(child_input_shape))\n+    super().build(tuple(child_input_shape))\n     self.built = True\n \n   def compute_output_shape(self, input_shape):\n\n@@ -274,13 +274,13 @@ class TimeDistributedTest(test_combinations.TestCase):\n     class TestListLayer(TestLayer):\n \n       def compute_output_shape(self, input_shape):\n-        shape = super(TestListLayer, self).compute_output_shape(input_shape)\n+        shape = super().compute_output_shape(input_shape)\n         return shape.as_list()\n \n     class TestTupleLayer(TestLayer):\n \n       def compute_output_shape(self, input_shape):\n-        shape = super(TestTupleLayer, self).compute_output_shape(input_shape)\n+        shape = super().compute_output_shape(input_shape)\n         return tuple(shape.as_list())\n \n     # Layers can specify output shape as list/tuple/TensorShape\n@@ -399,7 +399,7 @@ class TimeDistributedTest(test_combinations.TestCase):\n     class TestLayer(keras.layers.Layer):\n \n       def __init__(self):\n-        super(TestLayer, self).__init__()\n+        super().__init__()\n         self.dense_1 = dense_1\n         self.dense_2 = dense_2\n \n\n@@ -212,7 +212,7 @@ class Layer(base_layer.Layer):\n     # Mark that legacy layers should not be instrumented as Keras usage\n     self._disable_keras_instrumentation = True\n \n-    super(Layer, self).__init__(trainable=trainable, name=name, dtype=dtype,\n+    super().__init__(trainable=trainable, name=name, dtype=dtype,\n                                 **kwargs)\n \n     if _is_in_keras_style_scope():\n@@ -289,7 +289,7 @@ class Layer(base_layer.Layer):\n   def add_loss(self, losses, inputs=None):\n     previous_losses_length = len(self._losses)\n     previous_callable_losses_length = len(self._callable_losses)\n-    super(Layer, self).add_loss(losses, inputs=inputs)\n+    super().add_loss(losses, inputs=inputs)\n     if not tf.executing_eagerly():\n       # TODO(fchollet): deprecate collection below.\n       new_losses = self._losses[previous_losses_length:]\n@@ -306,7 +306,7 @@ class Layer(base_layer.Layer):\n   def _name_scope(self):  # pylint: disable=method-hidden\n     \"\"\"Determines op naming for the Layer.\"\"\"\n     if self._keras_style:\n-      return super(Layer, self)._name_scope()\n+      return super()._name_scope()\n     return self._current_scope.original_name_scope\n \n   def _set_scope(self, scope=None):\n@@ -385,7 +385,7 @@ class Layer(base_layer.Layer):\n       if kwarg != 'experimental_autocast':\n         raise TypeError('Unknown keyword argument:', kwarg)\n     if self._keras_style:\n-      return super(Layer, self).add_weight(\n+      return super().add_weight(\n           name=name,\n           shape=shape,\n           dtype=dtype,\n@@ -453,7 +453,7 @@ class Layer(base_layer.Layer):\n                         scope.use_resource)\n         if initializer is None:\n           initializer = scope.initializer\n-        variable = super(Layer, self).add_weight(\n+        variable = super().add_weight(\n             name,\n             shape,\n             dtype=tf.as_dtype(dtype),\n@@ -525,7 +525,7 @@ class Layer(base_layer.Layer):\n         raise ValueError(\n             'scope argument not allowed when keras style layers are enabled, '\n             'but saw: {}'.format(scope))\n-      return super(Layer, self).__call__(inputs, *args, **kwargs)\n+      return super().__call__(inputs, *args, **kwargs)\n \n     self._set_scope(scope)\n \n@@ -566,7 +566,7 @@ class Layer(base_layer.Layer):\n         kwargs['scope'] = scope\n \n       # Actually call layer\n-      outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n+      outputs = super().__call__(inputs, *args, **kwargs)\n \n     if not tf.executing_eagerly():\n       # Update global default collections.\n\n@@ -278,7 +278,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomerLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = input_spec.InputSpec(ndim=2)\n \n       def call(self, inputs):\n@@ -301,7 +301,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = input_spec.InputSpec(min_ndim=2)\n \n       def call(self, inputs):\n@@ -325,7 +325,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomerLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = input_spec.InputSpec(max_ndim=2)\n \n       def call(self, inputs):\n@@ -349,7 +349,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomerLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = input_spec.InputSpec(dtype='float32')\n \n       def call(self, inputs):\n@@ -370,7 +370,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomerLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = input_spec.InputSpec(axes={-1: 2})\n \n       def call(self, inputs):\n@@ -393,7 +393,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomerLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = input_spec.InputSpec(shape=(None, 3))\n \n       def call(self, inputs):\n@@ -414,7 +414,7 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     class CustomerLayer(base_tf_layers.Layer):\n \n       def __init__(self):\n-        super(CustomerLayer, self).__init__()\n+        super().__init__()\n         self.input_spec = None\n \n       def call(self, inputs):\n\n@@ -130,7 +130,7 @@ class Conv1D(keras_layers.Conv1D, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(Conv1D, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -404,7 +404,7 @@ class Conv2D(keras_layers.Conv2D, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(Conv2D, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -686,7 +686,7 @@ class Conv3D(keras_layers.Conv3D, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(Conv3D, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -975,7 +975,7 @@ class SeparableConv1D(keras_layers.SeparableConv1D, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(SeparableConv1D, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -1122,7 +1122,7 @@ class SeparableConv2D(keras_layers.SeparableConv2D, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(SeparableConv2D, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -1576,7 +1576,7 @@ class Conv2DTranspose(keras_layers.Conv2DTranspose, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(Conv2DTranspose, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n@@ -1833,7 +1833,7 @@ class Conv3DTranspose(keras_layers.Conv3DTranspose, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(Conv3DTranspose, self).__init__(\n+    super().__init__(\n         filters=filters,\n         kernel_size=kernel_size,\n         strides=strides,\n\n@@ -129,7 +129,7 @@ class Dense(keras_layers.Dense, base.Layer):\n                trainable=True,\n                name=None,\n                **kwargs):\n-    super(Dense, self).__init__(units=units,\n+    super().__init__(units=units,\n                                 activation=activation,\n                                 use_bias=use_bias,\n                                 kernel_initializer=kernel_initializer,\n@@ -321,14 +321,14 @@ class Dropout(keras_layers.Dropout, base.Layer):\n                seed=None,\n                name=None,\n                **kwargs):\n-    super(Dropout, self).__init__(rate=rate,\n+    super().__init__(rate=rate,\n                                   noise_shape=noise_shape,\n                                   seed=seed,\n                                   name=name,\n                                   **kwargs)\n \n   def call(self, inputs, training=False):\n-    return super(Dropout, self).call(inputs, training=training)\n+    return super().call(inputs, training=training)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.dropout'])\n\n@@ -203,7 +203,7 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n                adjustment=None,\n                name=None,\n                **kwargs):\n-    super(BatchNormalization, self).__init__(\n+    super().__init__(\n         axis=axis,\n         momentum=momentum,\n         epsilon=epsilon,\n@@ -228,7 +228,7 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n         **kwargs)\n \n   def call(self, inputs, training=False):\n-    return super(BatchNormalization, self).call(inputs, training=training)\n+    return super().call(inputs, training=training)\n \n \n @keras_export(v1=['keras.__internal__.legacy.layers.batch_normalization'])\n\n@@ -82,7 +82,7 @@ class AveragePooling1D(keras_layers.AveragePooling1D, base.Layer):\n                name=None, **kwargs):\n     if strides is None:\n       raise ValueError('Argument `strides` must not be None.')\n-    super(AveragePooling1D, self).__init__(\n+    super().__init__(\n         pool_size=pool_size,\n         strides=strides,\n         padding=padding,\n@@ -224,7 +224,7 @@ class MaxPooling1D(keras_layers.MaxPooling1D, base.Layer):\n                name=None, **kwargs):\n     if strides is None:\n       raise ValueError('Argument `strides` must not be None.')\n-    super(MaxPooling1D, self).__init__(\n+    super().__init__(\n         pool_size=pool_size,\n         strides=strides,\n         padding=padding,\n@@ -370,7 +370,7 @@ class AveragePooling2D(keras_layers.AveragePooling2D, base.Layer):\n                name=None, **kwargs):\n     if strides is None:\n       raise ValueError('Argument `strides` must not be None.')\n-    super(AveragePooling2D, self).__init__(\n+    super().__init__(\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, name=name, **kwargs)\n \n@@ -515,7 +515,7 @@ class MaxPooling2D(keras_layers.MaxPooling2D, base.Layer):\n                name=None, **kwargs):\n     if strides is None:\n       raise ValueError('Argument `strides` must not be None.')\n-    super(MaxPooling2D, self).__init__(\n+    super().__init__(\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, name=name, **kwargs)\n \n@@ -662,7 +662,7 @@ class AveragePooling3D(keras_layers.AveragePooling3D, base.Layer):\n                name=None, **kwargs):\n     if strides is None:\n       raise ValueError('Argument `strides` must not be None.')\n-    super(AveragePooling3D, self).__init__(\n+    super().__init__(\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, name=name, **kwargs)\n \n@@ -811,7 +811,7 @@ class MaxPooling3D(keras_layers.MaxPooling3D, base.Layer):\n                name=None, **kwargs):\n     if strides is None:\n       raise ValueError('Argument `strides` must not be None.')\n-    super(MaxPooling3D, self).__init__(\n+    super().__init__(\n         pool_size=pool_size, strides=strides,\n         padding=padding, data_format=data_format, name=name, **kwargs)\n \n\n@@ -734,7 +734,7 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n     config = {\n         'gamma': self.gamma,\n     }\n-    base_config = super(BinaryFocalCrossentropy, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -113,7 +113,7 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n   \"\"\"\n \n   def __init__(self, name=None, dtype=None, **kwargs):\n-    super(Metric, self).__init__(name=name, dtype=dtype, **kwargs)\n+    super().__init__(name=name, dtype=dtype, **kwargs)\n     self.stateful = True  # All metric layers are stateful.\n     self.built = True\n     if not base_layer_utils.v2_dtype_behavior_enabled():\n@@ -348,7 +348,7 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n       additional_kwargs = {}\n \n     with tf.init_scope():\n-      return super(Metric, self).add_weight(\n+      return super().add_weight(\n           name=name,\n           shape=shape,\n           dtype=self._dtype if dtype is None else dtype,\n@@ -408,7 +408,7 @@ class Reduce(Metric):\n   \"\"\"\n \n   def __init__(self, reduction, name, dtype=None):\n-    super(Reduce, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self.reduction = reduction\n     self.total = self.add_weight(\n         'total', initializer='zeros')\n@@ -532,7 +532,7 @@ class Sum(Reduce):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='sum', dtype=None):\n-    super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n+    super().__init__(reduction=metrics_utils.Reduction.SUM,\n                               name=name, dtype=dtype)\n \n \n@@ -575,7 +575,7 @@ class Mean(Reduce):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='mean', dtype=None):\n-    super(Mean, self).__init__(\n+    super().__init__(\n         reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n \n \n@@ -609,7 +609,7 @@ class MeanMetricWrapper(Mean):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, fn, name=None, dtype=None, **kwargs):\n-    super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self._fn = fn\n     self._fn_kwargs = kwargs\n \n@@ -644,7 +644,7 @@ class MeanMetricWrapper(Mean):\n \n     ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n-    return super(MeanMetricWrapper, self).update_state(\n+    return super().update_state(\n         matches, sample_weight=sample_weight)\n \n   def get_config(self):\n@@ -657,7 +657,7 @@ class MeanMetricWrapper(Mean):\n \n     for k, v in self._fn_kwargs.items():\n       config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n-    base_config = super(MeanMetricWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   @classmethod\n@@ -710,7 +710,7 @@ class MeanTensor(Metric):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_tensor', dtype=None, shape=None):\n-    super(MeanTensor, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self._shape = None\n     self._total = None\n     self._count = None\n@@ -814,7 +814,7 @@ class SumOverBatchSize(Reduce):\n   \"\"\"\n \n   def __init__(self, name='sum_over_batch_size', dtype=None):\n-    super(SumOverBatchSize, self).__init__(\n+    super().__init__(\n         reduction=metrics_utils.Reduction.SUM_OVER_BATCH_SIZE,\n         name=name,\n         dtype=dtype)\n@@ -833,7 +833,7 @@ class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n       dtype: (Optional) data type of the metric result.\n       **kwargs: The keyword arguments that are passed on to `fn`.\n     \"\"\"\n-    super(SumOverBatchSizeMetricWrapper, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self._fn = fn\n     self._fn_kwargs = kwargs\n \n@@ -845,14 +845,14 @@ class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n \n     ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n-    return super(SumOverBatchSizeMetricWrapper, self).update_state(\n+    return super().update_state(\n         matches, sample_weight=sample_weight)\n \n   def get_config(self):\n     config = {}\n     for k, v in self._fn_kwargs.items():\n       config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n-    base_config = super(SumOverBatchSizeMetricWrapper, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -481,7 +481,7 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n     class ModelWithMetric(Model):\n \n       def __init__(self):\n-        super(ModelWithMetric, self).__init__()\n+        super().__init__()\n         self.dense1 = layers.Dense(\n             3, activation='relu', kernel_initializer='ones')\n         self.dense2 = layers.Dense(\n@@ -522,7 +522,7 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n class BinaryTruePositives(metrics.Metric):\n \n   def __init__(self, name='binary_true_positives', **kwargs):\n-    super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     self.true_positives = self.add_weight(name='tp', initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n@@ -546,7 +546,7 @@ class BinaryTruePositives(metrics.Metric):\n class BinaryTruePositivesViaControlFlow(metrics.Metric):\n \n   def __init__(self, name='binary_true_positives', **kwargs):\n-    super(BinaryTruePositivesViaControlFlow, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     self.true_positives = self.add_weight(name='tp', initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n@@ -658,7 +658,7 @@ class CustomMetricsTest(tf.test.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def __init__(self, **kwargs):\n-        super(MyLayer, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.mean_obj = metrics.Mean(name='my_mean_obj')\n \n       def call(self, x):\n@@ -678,7 +678,7 @@ class CustomMetricsTest(tf.test.TestCase):\n     class MyModel(training_module.Model):\n \n       def __init__(self, **kwargs):\n-        super(MyModel, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.mean_obj = metrics.Mean(name='my_mean_obj')\n \n       def call(self, x):\n\n@@ -86,7 +86,7 @@ class MeanRelativeError(base_metric.Mean):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, normalizer, name=None, dtype=None):\n-    super(MeanRelativeError, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     normalizer = tf.cast(normalizer, self._dtype)\n     self.normalizer = normalizer\n \n@@ -117,13 +117,13 @@ class MeanRelativeError(base_metric.Mean):\n     relative_errors = tf.math.divide_no_nan(\n         tf.abs(y_true - y_pred), self.normalizer)\n \n-    return super(MeanRelativeError, self).update_state(\n+    return super().update_state(\n         relative_errors, sample_weight=sample_weight)\n \n   def get_config(self):\n     n = self.normalizer\n     config = {'normalizer': backend.eval(n) if is_tensor_or_variable(n) else n}\n-    base_config = super(MeanRelativeError, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -167,7 +167,7 @@ class Accuracy(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='accuracy', dtype=None):\n-    super(Accuracy, self).__init__(accuracy, name, dtype=dtype)\n+    super().__init__(accuracy, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.BinaryAccuracy')\n@@ -212,7 +212,7 @@ class BinaryAccuracy(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n-    super(BinaryAccuracy, self).__init__(\n+    super().__init__(\n         metrics_utils.binary_matches, name, dtype=dtype, threshold=threshold)\n \n \n@@ -265,7 +265,7 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_accuracy', dtype=None):\n-    super(CategoricalAccuracy, self).__init__(\n+    super().__init__(\n         lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(  # pylint: disable=g-long-lambda\n             tf.math.argmax(y_true, axis=-1), y_pred),\n         name,\n@@ -320,7 +320,7 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n-    super(SparseCategoricalAccuracy, self).__init__(\n+    super().__init__(\n         metrics_utils.sparse_categorical_matches, name, dtype=dtype)\n \n \n@@ -386,7 +386,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n-    super(TopKCategoricalAccuracy, self).__init__(\n+    super().__init__(\n         lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(  # pylint: disable=g-long-lambda\n             tf.math.argmax(yt, axis=-1), yp, k),\n         name,\n@@ -429,7 +429,7 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n-    super(SparseTopKCategoricalAccuracy, self).__init__(\n+    super().__init__(\n         metrics_utils.sparse_top_k_categorical_matches, name, dtype=dtype, k=k)\n \n \n@@ -455,7 +455,7 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n                thresholds=None,\n                name=None,\n                dtype=None):\n-    super(_ConfusionMatrixConditionCount, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self._confusion_matrix_cond = confusion_matrix_cond\n     self.init_thresholds = thresholds\n     self.thresholds = metrics_utils.parse_init_thresholds(\n@@ -502,7 +502,7 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n \n   def get_config(self):\n     config = {'thresholds': self.init_thresholds}\n-    base_config = super(_ConfusionMatrixConditionCount, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -559,7 +559,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n-    super(FalsePositives, self).__init__(\n+    super().__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n         thresholds=thresholds,\n         name=name,\n@@ -619,7 +619,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n-    super(FalseNegatives, self).__init__(\n+    super().__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n         thresholds=thresholds,\n         name=name,\n@@ -679,7 +679,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n-    super(TrueNegatives, self).__init__(\n+    super().__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n         thresholds=thresholds,\n         name=name,\n@@ -739,7 +739,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n-    super(TruePositives, self).__init__(\n+    super().__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n         thresholds=thresholds,\n         name=name,\n@@ -832,7 +832,7 @@ class Precision(base_metric.Metric):\n                class_id=None,\n                name=None,\n                dtype=None):\n-    super(Precision, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self.init_thresholds = thresholds\n     self.top_k = top_k\n     self.class_id = class_id\n@@ -896,7 +896,7 @@ class Precision(base_metric.Metric):\n         'top_k': self.top_k,\n         'class_id': self.class_id\n     }\n-    base_config = super(Precision, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -973,7 +973,7 @@ class Recall(base_metric.Metric):\n                class_id=None,\n                name=None,\n                dtype=None):\n-    super(Recall, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self.init_thresholds = thresholds\n     self.top_k = top_k\n     self.class_id = class_id\n@@ -1037,7 +1037,7 @@ class Recall(base_metric.Metric):\n         'top_k': self.top_k,\n         'class_id': self.class_id\n     }\n-    base_config = super(Recall, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1054,7 +1054,7 @@ class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n                class_id=None,\n                name=None,\n                dtype=None):\n-    super(SensitivitySpecificityBase, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     if num_thresholds <= 0:\n       raise ValueError(\n           'Argument `num_thresholds` must be an integer > 0. '\n@@ -1125,7 +1125,7 @@ class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n \n   def get_config(self):\n     config = {'class_id': self.class_id}\n-    base_config = super(SensitivitySpecificityBase, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n   def _find_max_under_constraint(self, constrained, dependent, predicate):\n@@ -1223,7 +1223,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n           f'Received: specificity={specificity}')\n     self.specificity = specificity\n     self.num_thresholds = num_thresholds\n-    super(SensitivityAtSpecificity, self).__init__(\n+    super().__init__(\n         specificity,\n         num_thresholds=num_thresholds,\n         class_id=class_id,\n@@ -1245,7 +1245,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n         'num_thresholds': self.num_thresholds,\n         'specificity': self.specificity\n     }\n-    base_config = super(SensitivityAtSpecificity, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1320,7 +1320,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n           f'Received: sensitivity={sensitivity}')\n     self.sensitivity = sensitivity\n     self.num_thresholds = num_thresholds\n-    super(SpecificityAtSensitivity, self).__init__(\n+    super().__init__(\n         sensitivity,\n         num_thresholds=num_thresholds,\n         class_id=class_id,\n@@ -1342,7 +1342,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n         'num_thresholds': self.num_thresholds,\n         'sensitivity': self.sensitivity\n     }\n-    base_config = super(SpecificityAtSensitivity, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1409,7 +1409,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n           f'Received: recall={recall}')\n     self.recall = recall\n     self.num_thresholds = num_thresholds\n-    super(PrecisionAtRecall, self).__init__(\n+    super().__init__(\n         value=recall,\n         num_thresholds=num_thresholds,\n         class_id=class_id,\n@@ -1428,7 +1428,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n \n   def get_config(self):\n     config = {'num_thresholds': self.num_thresholds, 'recall': self.recall}\n-    base_config = super(PrecisionAtRecall, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1498,7 +1498,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n           f'Received: precision={precision}')\n     self.precision = precision\n     self.num_thresholds = num_thresholds\n-    super(RecallAtPrecision, self).__init__(\n+    super().__init__(\n         value=precision,\n         num_thresholds=num_thresholds,\n         class_id=class_id,\n@@ -1518,7 +1518,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   def get_config(self):\n     config = {'num_thresholds': self.num_thresholds,\n               'precision': self.precision}\n-    base_config = super(RecallAtPrecision, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -1700,7 +1700,7 @@ class AUC(base_metric.Metric):\n     else:\n       self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n           summation_method)\n-    super(AUC, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n \n     # Handle multilabel arguments.\n     self.multi_label = multi_label\n@@ -2000,7 +2000,7 @@ class AUC(base_metric.Metric):\n       # were initialized. This ensures that a metric initialized from this\n       # config has the same thresholds.\n       config['thresholds'] = self.thresholds[1:-1]\n-    base_config = super(AUC, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -2051,7 +2051,7 @@ class CosineSimilarity(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='cosine_similarity', dtype=None, axis=-1):\n-    super(CosineSimilarity, self).__init__(\n+    super().__init__(\n         cosine_similarity, name, dtype=dtype, axis=axis)\n \n \n@@ -2088,7 +2088,7 @@ class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_error', dtype=None):\n-    super(MeanAbsoluteError, self).__init__(\n+    super().__init__(\n         mean_absolute_error, name, dtype=dtype)\n \n \n@@ -2125,7 +2125,7 @@ class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_percentage_error', dtype=None):\n-    super(MeanAbsolutePercentageError, self).__init__(\n+    super().__init__(\n         mean_absolute_percentage_error, name, dtype=dtype)\n \n \n@@ -2162,7 +2162,7 @@ class MeanSquaredError(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_error', dtype=None):\n-    super(MeanSquaredError, self).__init__(\n+    super().__init__(\n         mean_squared_error, name, dtype=dtype)\n \n \n@@ -2199,7 +2199,7 @@ class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_logarithmic_error', dtype=None):\n-    super(MeanSquaredLogarithmicError, self).__init__(\n+    super().__init__(\n         mean_squared_logarithmic_error, name, dtype=dtype)\n \n \n@@ -2236,7 +2236,7 @@ class Hinge(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='hinge', dtype=None):\n-    super(Hinge, self).__init__(hinge, name, dtype=dtype)\n+    super().__init__(hinge, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.SquaredHinge')\n@@ -2275,7 +2275,7 @@ class SquaredHinge(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='squared_hinge', dtype=None):\n-    super(SquaredHinge, self).__init__(squared_hinge, name, dtype=dtype)\n+    super().__init__(squared_hinge, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.CategoricalHinge')\n@@ -2311,7 +2311,7 @@ class CategoricalHinge(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_hinge', dtype=None):\n-    super(CategoricalHinge, self).__init__(categorical_hinge, name, dtype=dtype)\n+    super().__init__(categorical_hinge, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.RootMeanSquaredError')\n@@ -2343,7 +2343,7 @@ class RootMeanSquaredError(base_metric.Mean):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='root_mean_squared_error', dtype=None):\n-    super(RootMeanSquaredError, self).__init__(name, dtype=dtype)\n+    super().__init__(name, dtype=dtype)\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n     \"\"\"Accumulates root mean squared error statistics.\n@@ -2363,7 +2363,7 @@ class RootMeanSquaredError(base_metric.Mean):\n     y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n         y_pred, y_true)\n     error_sq = tf.math.squared_difference(y_pred, y_true)\n-    return super(RootMeanSquaredError, self).update_state(\n+    return super().update_state(\n         error_sq, sample_weight=sample_weight)\n \n   def result(self):\n@@ -2404,7 +2404,7 @@ class LogCoshError(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='logcosh', dtype=None):\n-    super(LogCoshError, self).__init__(logcosh, name, dtype=dtype)\n+    super().__init__(logcosh, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.Poisson')\n@@ -2441,7 +2441,7 @@ class Poisson(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='poisson', dtype=None):\n-    super(Poisson, self).__init__(poisson, name, dtype=dtype)\n+    super().__init__(poisson, name, dtype=dtype)\n \n \n @keras_export('keras.metrics.KLDivergence')\n@@ -2478,7 +2478,7 @@ class KLDivergence(base_metric.MeanMetricWrapper):\n \n   @dtensor_utils.inject_mesh\n   def __init__(self, name='kullback_leibler_divergence', dtype=None):\n-    super(KLDivergence, self).__init__(\n+    super().__init__(\n         kullback_leibler_divergence, name, dtype=dtype)\n \n \n@@ -2512,7 +2512,7 @@ class _IoUBase(base_metric.Metric):\n   \"\"\"\n \n   def __init__(self, num_classes, name=None, dtype=None):\n-    super(_IoUBase, self).__init__(name=name, dtype=dtype)\n+    super().__init__(name=name, dtype=dtype)\n     self.num_classes = num_classes\n \n     # Variable to accumulate the predictions in the confusion matrix.\n@@ -2640,7 +2640,7 @@ class IoU(_IoUBase):\n       name=None,\n       dtype=None,\n   ):\n-    super(IoU, self).__init__(\n+    super().__init__(\n         name=name,\n         num_classes=num_classes,\n         dtype=dtype,\n@@ -2682,7 +2682,7 @@ class IoU(_IoUBase):\n         'num_classes': self.num_classes,\n         'target_class_ids': self.target_class_ids,\n     }\n-    base_config = super(IoU, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -2764,7 +2764,7 @@ class BinaryIoU(IoU):\n       dtype=None,\n   ):\n \n-    super(BinaryIoU, self).__init__(\n+    super().__init__(\n         num_classes=2,\n         target_class_ids=target_class_ids,\n         name=name,\n@@ -2865,7 +2865,7 @@ class MeanIoU(IoU):\n   @dtensor_utils.inject_mesh\n   def __init__(self, num_classes, name=None, dtype=None):\n     target_class_ids = list(range(num_classes))\n-    super(MeanIoU, self).__init__(\n+    super().__init__(\n         name=name,\n         num_classes=num_classes,\n         target_class_ids=target_class_ids,\n@@ -2961,7 +2961,7 @@ class OneHotIoU(IoU):\n       name=None,\n       dtype=None,\n   ):\n-    super(OneHotIoU, self).__init__(\n+    super().__init__(\n         num_classes=num_classes,\n         target_class_ids=target_class_ids,\n         name=name,\n@@ -3065,7 +3065,7 @@ class OneHotMeanIoU(MeanIoU):\n       name=None,\n       dtype=None,\n   ):\n-    super(OneHotMeanIoU, self).__init__(\n+    super().__init__(\n         num_classes=num_classes,\n         name=name,\n         dtype=dtype,\n@@ -3137,7 +3137,7 @@ class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n                dtype=None,\n                from_logits=False,\n                label_smoothing=0):\n-    super(BinaryCrossentropy, self).__init__(\n+    super().__init__(\n         binary_crossentropy,\n         name,\n         dtype=dtype,\n@@ -3202,7 +3202,7 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n                dtype=None,\n                from_logits=False,\n                label_smoothing=0):\n-    super(CategoricalCrossentropy, self).__init__(\n+    super().__init__(\n         categorical_crossentropy,\n         name,\n         dtype=dtype,\n@@ -3274,7 +3274,7 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n                dtype=None,\n                from_logits=False,\n                axis=-1):\n-    super(SparseCategoricalCrossentropy, self).__init__(\n+    super().__init__(\n         sparse_categorical_crossentropy,\n         name,\n         dtype=dtype,\n\n@@ -1758,7 +1758,7 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n class BinaryTruePositives(metrics.Metric):\n \n   def __init__(self, name='binary_true_positives', **kwargs):\n-    super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     self.true_positives = self.add_weight(name='tp', initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n@@ -1782,7 +1782,7 @@ class BinaryTruePositives(metrics.Metric):\n class BinaryTruePositivesViaControlFlow(metrics.Metric):\n \n   def __init__(self, name='binary_true_positives', **kwargs):\n-    super(BinaryTruePositivesViaControlFlow, self).__init__(name=name, **kwargs)\n+    super().__init__(name=name, **kwargs)\n     self.true_positives = self.add_weight(name='tp', initializer='zeros')\n \n   def update_state(self, y_true, y_pred, sample_weight=None):\n\n@@ -65,7 +65,7 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n     set_cpu_logical_devices_to_at_least(3)\n-    super(AutoCastVariableTest, self).setUp()\n+    super().setUp()\n \n   @tf.__internal__.distribute.combinations.generate(maybe_distribute)\n   def test_read(self, distribution):\n\n@@ -69,7 +69,7 @@ def _create_normalization_layer_without_adapt():\n class LayerCorrectnessTest(test_combinations.TestCase):\n \n   def setUp(self):\n-    super(LayerCorrectnessTest, self).setUp()\n+    super().setUp()\n     # Set two virtual CPUs to test MirroredStrategy with multiple devices\n     cpus = tf.config.list_physical_devices('CPU')\n     tf.config.set_logical_device_configuration(cpus[0], [\n\n@@ -36,7 +36,7 @@ class MultiplyLayerWithFunction(mp_test_util.MultiplyLayer):\n \n   @tf.function\n   def _multiply(self, x, y):\n-    return super(MultiplyLayerWithFunction, self)._multiply(x, y)\n+    return super()._multiply(x, y)\n \n \n # If called outside any strategy.scope() calls, this will return the default\n\n@@ -129,7 +129,7 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n                growth_steps,\n                multiplier):\n     \"\"\"Creates the dynamic loss scale.\"\"\"\n-    super(_DynamicLossScaleState, self).__init__()\n+    super().__init__()\n     self._initial_loss_scale = float(initial_loss_scale)\n     self._growth_steps = int(growth_steps)\n     self._multiplier = float(multiplier)\n@@ -192,13 +192,12 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n       if g == graph_key:\n         weights[name] = v\n     weights.update(\n-        super(_DynamicLossScaleState,\n-              self)._trackable_children(save_type, **kwargs))\n+        super()._trackable_children(save_type, **kwargs))\n     return weights\n \n   def _lookup_dependency(self, name):\n     \"\"\"From Trackable. Find a weight in the current graph.\"\"\"\n-    unconditional = super(_DynamicLossScaleState, self)._lookup_dependency(name)\n+    unconditional = super()._lookup_dependency(name)\n     if unconditional is not None:\n       return unconditional\n     if tf.executing_eagerly():\n@@ -936,7 +935,7 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n       raise e\n \n   def __dir__(self):\n-    result = set(super(LossScaleOptimizer, self).__dir__())\n+    result = set(super().__dir__())\n     if '_optimizer' in result:\n       result |= self._optimizer._hyper.keys()\n       if 'learning_rate' in self._optimizer._hyper.keys():\n@@ -960,7 +959,7 @@ class LossScaleOptimizer(tf.__internal__.tracking.DelegatingTrackableMixin,\n         and not has_attribute):\n       self._optimizer._set_hyper(name, value)\n     else:\n-      super(LossScaleOptimizer, self).__setattr__(name, value)\n+      super().__setattr__(name, value)\n \n   # Explicitly delegate learning_rate. Normally hyperparameters are delegated in\n   # __getattribute__, but if a hyperparameter is not in self._optimizer._hyper\n\n@@ -636,8 +636,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                           experimental_aggregate_gradients=True):\n         for grad, _ in grads_and_vars:\n           outer_self.assertIsInstance(grad, tf.Tensor)\n-        return super(MyOptimizer,\n-                     self).apply_gradients(grads_and_vars, name,\n+        return super().apply_gradients(grads_and_vars, name,\n                                            experimental_aggregate_gradients)\n \n     with create_mirrored_strategy().scope() as strategy:\n\n@@ -29,7 +29,7 @@ class MixedPrecisionTest(test_combinations.TestCase):\n   IGNORE_PERF_VAR = 'TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'\n \n   def setUp(self):\n-    super(MixedPrecisionTest, self).setUp()\n+    super().setUp()\n     # Enable the tests to be run on pre-Volta GPUs by telling the grappler pass\n     # to ignore performance and always transform the graph.\n     self._original_ignore_perf_value = os.getenv(self.IGNORE_PERF_VAR)\n@@ -43,7 +43,7 @@ class MixedPrecisionTest(test_combinations.TestCase):\n       del os.environ[self.IGNORE_PERF_VAR]\n \n     tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite()\n-    super(MixedPrecisionTest, self).tearDown()\n+    super().tearDown()\n \n   @test_combinations.generate(\n       test_combinations.combine(mode=['graph', 'eager']))\n\n@@ -103,7 +103,7 @@ class AssertTypeLayer(base_layer.Layer):\n   def __init__(self, assert_type=None, **kwargs):\n     self._assert_type = (tf.as_dtype(assert_type).name if assert_type\n                          else None)\n-    super(AssertTypeLayer, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n \n   def assert_input_types(self, inputs):\n     \"\"\"Asserts `inputs` are of the correct type. Should be called in call().\"\"\"\n@@ -147,7 +147,7 @@ class MultiplyLayer(AssertTypeLayer):\n \n     self._use_operator = use_operator\n     self._var_name = var_name\n-    super(MultiplyLayer, self).__init__(\n+    super().__init__(\n         activity_regularizer=self._activity_regularizer, **kwargs)\n \n   def build(self, _):\n@@ -166,7 +166,7 @@ class MultiplyLayer(AssertTypeLayer):\n       return tf.multiply(x, y)\n \n   def get_config(self):\n-    config = super(MultiplyLayer, self).get_config()\n+    config = super().get_config()\n     config['regularizer'] = regularizers.serialize(self._regularizer)\n     config['activity_regularizer'] = regularizers.serialize(\n         self._activity_regularizer)\n\n@@ -36,7 +36,7 @@ class TestModel(keras.Model):\n \n   def __init__(self, n_outputs=4, trainable=True):\n     \"\"\"A test class with one dense layer and number of outputs as a variable.\"\"\"\n-    super(TestModel, self).__init__()\n+    super().__init__()\n     self.layer1 = keras.layers.Dense(n_outputs)\n     self.n_outputs = tf.Variable(n_outputs, trainable=trainable)\n \n\n@@ -70,7 +70,7 @@ class Adadelta(optimizer.Optimizer):\n                jit_compile=True,\n                name='Adadelta',\n                **kwargs):\n-    super(Adadelta, self).__init__(\n+    super().__init__(\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n         global_clipnorm=global_clipnorm,\n@@ -127,7 +127,7 @@ class Adadelta(optimizer.Optimizer):\n     variable.assign_add(lr * delta_var)\n \n   def get_config(self):\n-    config = super(Adadelta, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -65,7 +65,7 @@ class Adagrad(optimizer.Optimizer):\n                jit_compile=True,\n                name='Adagrad',\n                **kwargs):\n-    super(Adagrad, self).__init__(\n+    super().__init__(\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n         global_clipnorm=global_clipnorm,\n@@ -110,7 +110,7 @@ class Adagrad(optimizer.Optimizer):\n     variable.assign_sub(lr * grad / tf.sqrt(accumulator + self.epsilon))\n \n   def get_config(self):\n-    config = super(Adagrad, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -95,7 +95,7 @@ class Adam(optimizer.Optimizer):\n                jit_compile=True,\n                name='Adam',\n                **kwargs):\n-    super(Adam, self).__init__(\n+    super().__init__(\n         name=name,\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n@@ -181,7 +181,7 @@ class Adam(optimizer.Optimizer):\n       variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n \n   def get_config(self):\n-    config = super(Adam, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -83,7 +83,7 @@ class Adamax(optimizer.Optimizer):\n                jit_compile=True,\n                name='Adamax',\n                **kwargs):\n-    super(Adamax, self).__init__(\n+    super().__init__(\n         name=name,\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n@@ -151,7 +151,7 @@ class Adamax(optimizer.Optimizer):\n       variable.assign_sub((lr * m) / ((1 - beta_1_power) * (u + self.epsilon)))\n \n   def get_config(self):\n-    config = super(Adamax, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -102,7 +102,7 @@ class AdamW(optimizer.Optimizer):\n                jit_compile=True,\n                name='AdamW',\n                **kwargs):\n-    super(AdamW, self).__init__(\n+    super().__init__(\n         name=name,\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n@@ -203,7 +203,7 @@ class AdamW(optimizer.Optimizer):\n       variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n \n   def get_config(self):\n-    config = super(AdamW, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -772,7 +772,7 @@ class Optimizer(_BaseOptimizer):\n     # TODO(b/197554203): replace _distributed_container() with a public api.\n     if hasattr(variable, \"_distributed_container\"):\n       variable = variable._distributed_container()\n-    return super(Optimizer, self)._var_key(variable)\n+    return super()._var_key(variable)\n \n   def aggregate_gradients(self, grads_and_vars):\n     \"\"\"Aggregate gradients on all devices.\n@@ -870,7 +870,7 @@ class Optimizer(_BaseOptimizer):\n class RestoredOptimizer(Optimizer):\n \n   def __init__(self):\n-    super(RestoredOptimizer, self).__init__(\"RestoredOptimizer\")\n+    super().__init__(\"RestoredOptimizer\")\n \n   def get_config(self):\n     raise NotImplementedError(\n\n@@ -85,7 +85,7 @@ class RMSprop(optimizer.Optimizer):\n                jit_compile=True,\n                name='RMSprop',\n                **kwargs):\n-    super(RMSprop, self).__init__(\n+    super().__init__(\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n         global_clipnorm=global_clipnorm,\n@@ -178,7 +178,7 @@ class RMSprop(optimizer.Optimizer):\n         variable.assign_add(-lr * transformed_grad)\n \n   def get_config(self):\n-    config = super(RMSprop, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -104,7 +104,7 @@ class SGD(optimizer.Optimizer):\n                jit_compile=True,\n                name='SGD',\n                **kwargs):\n-    super(SGD, self).__init__(\n+    super().__init__(\n         name=name,\n         clipnorm=clipnorm,\n         clipvalue=clipvalue,\n@@ -175,7 +175,7 @@ class SGD(optimizer.Optimizer):\n         variable.assign_add(-gradient * lr)\n \n   def get_config(self):\n-    config = super(SGD, self).get_config()\n+    config = super().get_config()\n \n     config.update({\n         'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n\n@@ -165,7 +165,7 @@ class SGD(Optimizer):\n   \"\"\"\n \n   def __init__(self, lr=0.01, momentum=0., decay=0., nesterov=False, **kwargs):\n-    super(SGD, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.iterations = backend.variable(0, dtype='int64', name='iterations')\n       self.lr = backend.variable(lr, name='lr')\n@@ -216,7 +216,7 @@ class SGD(Optimizer):\n         'decay': float(backend.get_value(self.decay)),\n         'nesterov': self.nesterov\n     }\n-    base_config = super(SGD, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -236,7 +236,7 @@ class RMSprop(Optimizer):\n   \"\"\"\n \n   def __init__(self, lr=0.001, rho=0.9, epsilon=None, decay=0., **kwargs):\n-    super(RMSprop, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.lr = backend.variable(lr, name='lr')\n       self.rho = backend.variable(rho, name='rho')\n@@ -287,7 +287,7 @@ class RMSprop(Optimizer):\n         'decay': float(backend.get_value(self.decay)),\n         'epsilon': self.epsilon\n     }\n-    base_config = super(RMSprop, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -313,7 +313,7 @@ class Adagrad(Optimizer):\n   \"\"\"\n \n   def __init__(self, lr=0.01, epsilon=None, decay=0., **kwargs):\n-    super(Adagrad, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.lr = backend.variable(lr, name='lr')\n       self.decay = backend.variable(decay, name='decay')\n@@ -361,7 +361,7 @@ class Adagrad(Optimizer):\n         'decay': float(backend.get_value(self.decay)),\n         'epsilon': self.epsilon\n     }\n-    base_config = super(Adagrad, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -394,7 +394,7 @@ class Adadelta(Optimizer):\n   \"\"\"\n \n   def __init__(self, lr=1.0, rho=0.95, epsilon=None, decay=0., **kwargs):\n-    super(Adadelta, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.lr = backend.variable(lr, name='lr')\n       self.decay = backend.variable(decay, name='decay')\n@@ -453,7 +453,7 @@ class Adadelta(Optimizer):\n         'decay': float(backend.get_value(self.decay)),\n         'epsilon': self.epsilon\n     }\n-    base_config = super(Adadelta, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -481,7 +481,7 @@ class Adam(Optimizer):\n                decay=0.,\n                amsgrad=False,\n                **kwargs):\n-    super(Adam, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.iterations = backend.variable(0, dtype='int64', name='iterations')\n       self.lr = backend.variable(lr, name='lr')\n@@ -559,7 +559,7 @@ class Adam(Optimizer):\n         'epsilon': self.epsilon,\n         'amsgrad': self.amsgrad\n     }\n-    base_config = super(Adam, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -584,7 +584,7 @@ class Adamax(Optimizer):\n                epsilon=None,\n                decay=0.,\n                **kwargs):\n-    super(Adamax, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.iterations = backend.variable(0, dtype='int64', name='iterations')\n       self.lr = backend.variable(lr, name='lr')\n@@ -649,7 +649,7 @@ class Adamax(Optimizer):\n         'decay': float(backend.get_value(self.decay)),\n         'epsilon': self.epsilon\n     }\n-    base_config = super(Adamax, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n@@ -677,7 +677,7 @@ class Nadam(Optimizer):\n                epsilon=None,\n                schedule_decay=0.004,\n                **kwargs):\n-    super(Nadam, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     with backend.name_scope(self.__class__.__name__):\n       self.iterations = backend.variable(0, dtype='int64', name='iterations')\n       self.m_schedule = backend.variable(1., name='m_schedule')\n@@ -749,7 +749,7 @@ class Nadam(Optimizer):\n         'epsilon': self.epsilon,\n         'schedule_decay': self.schedule_decay\n     }\n-    base_config = super(Nadam, self).get_config()\n+    base_config = super().get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n\n@@ -74,7 +74,7 @@ class Adadelta(optimizer_v2.OptimizerV2):\n                epsilon=1e-7,\n                name='Adadelta',\n                **kwargs):\n-    super(Adadelta, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n     self._set_hyper('decay', self._initial_decay)\n     self._set_hyper('rho', rho)\n@@ -88,7 +88,7 @@ class Adadelta(optimizer_v2.OptimizerV2):\n       self.add_slot(v, 'accum_var')\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(Adadelta, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n     apply_state[(var_device, var_dtype)].update(\n         dict(\n             epsilon=tf.convert_to_tensor(\n@@ -102,7 +102,7 @@ class Adadelta(optimizer_v2.OptimizerV2):\n     # iteration to 0.\n     if len(params) == len(weights) + 1:\n       weights = [np.array(0)] + weights\n-    super(Adadelta, self).set_weights(weights)\n+    super().set_weights(weights)\n \n   def _resource_apply_dense(self, grad, var, apply_state=None):\n     var_device, var_dtype = var.device, var.dtype.base_dtype\n@@ -140,7 +140,7 @@ class Adadelta(optimizer_v2.OptimizerV2):\n         use_locking=self._use_locking)\n \n   def get_config(self):\n-    config = super(Adadelta, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n         'decay': self._initial_decay,\n\n@@ -74,7 +74,7 @@ class Adagrad(optimizer_v2.OptimizerV2):\n                        initial_accumulator_value)\n     if epsilon is None:\n       epsilon = backend_config.epsilon()\n-    super(Adagrad, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n     self._set_hyper('decay', self._initial_decay)\n     self._initial_accumulator_value = initial_accumulator_value\n@@ -88,7 +88,7 @@ class Adagrad(optimizer_v2.OptimizerV2):\n       self.add_slot(var, 'accumulator', init)\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(Adagrad, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n     apply_state[(var_device, var_dtype)].update(\n         dict(\n             epsilon=tf.convert_to_tensor(\n@@ -103,7 +103,7 @@ class Adagrad(optimizer_v2.OptimizerV2):\n     # iteration to 0.\n     if len(params) == len(weights) + 1:\n       weights = [np.array(0)] + weights\n-    super(Adagrad, self).set_weights(weights)\n+    super().set_weights(weights)\n \n   @classmethod\n   def from_config(cls, config, custom_objects=None):\n@@ -158,7 +158,7 @@ class Adagrad(optimizer_v2.OptimizerV2):\n         use_locking=self._use_locking)\n \n   def get_config(self):\n-    config = super(Adagrad, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n         'decay': self._initial_decay,\n\n@@ -107,7 +107,7 @@ class Adam(optimizer_v2.OptimizerV2):\n                amsgrad=False,\n                name='Adam',\n                **kwargs):\n-    super(Adam, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n     self._set_hyper('decay', self._initial_decay)\n     self._set_hyper('beta_1', beta_1)\n@@ -127,7 +127,7 @@ class Adam(optimizer_v2.OptimizerV2):\n         self.add_slot(var, 'vhat')\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(Adam, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n \n     local_step = tf.cast(self.iterations + 1, var_dtype)\n     beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))\n@@ -156,7 +156,7 @@ class Adam(optimizer_v2.OptimizerV2):\n     num_vars = int((len(params) - 1) / 2)\n     if len(weights) == 3 * num_vars + 1:\n       weights = weights[:len(params)]\n-    super(Adam, self).set_weights(weights)\n+    super().set_weights(weights)\n \n   def _resource_apply_dense(self, grad, var, apply_state=None):\n     var_device, var_dtype = var.device, var.dtype.base_dtype\n@@ -236,7 +236,7 @@ class Adam(optimizer_v2.OptimizerV2):\n       return tf.group(*[var_update, m_t, v_t, v_hat_t])\n \n   def get_config(self):\n-    config = super(Adam, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n         'decay': self._initial_decay,\n@@ -359,7 +359,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n         compatibility, recommended to use `learning_rate` instead.\n     \"\"\"\n \n-    super(NonFusedAdam, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n     self._set_hyper('decay', self._initial_decay)\n     self._set_hyper('beta_1', beta_1)\n@@ -379,7 +379,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n         self.add_slot(var, 'vhat')\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(NonFusedAdam, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n \n     local_step = tf.cast(self.iterations + 1, var_dtype)\n     beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))\n@@ -409,7 +409,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n     num_vars = int((len(params) - 1) / 2)\n     if len(weights) == 3 * num_vars + 1:\n       weights = weights[:len(params)]\n-    super(NonFusedAdam, self).set_weights(weights)\n+    super().set_weights(weights)\n \n   @tf.function(jit_compile=True)\n   def _resource_apply_dense(self, grad, var, apply_state=None):\n@@ -460,7 +460,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n                      (tf.sqrt(v_hat) + coefficients['epsilon']))\n \n   def get_config(self):\n-    config = super(NonFusedAdam, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n         'decay': self._initial_decay,\n\n@@ -92,7 +92,7 @@ class Adamax(optimizer_v2.OptimizerV2):\n                epsilon=1e-7,\n                name='Adamax',\n                **kwargs):\n-    super(Adamax, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n     self._set_hyper('decay', self._initial_decay)\n     self._set_hyper('beta_1', beta_1)\n@@ -107,7 +107,7 @@ class Adamax(optimizer_v2.OptimizerV2):\n       self.add_slot(var, 'v')  # Create slots for the second moments.\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(Adamax, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n \n     local_step = tf.cast(self.iterations + 1, var_dtype)\n     beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))\n@@ -173,7 +173,7 @@ class Adamax(optimizer_v2.OptimizerV2):\n     return tf.group(*[var_update, m_t, v_t])\n \n   def get_config(self):\n-    config = super(Adamax, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n         'decay': self._initial_decay,\n\n@@ -112,7 +112,7 @@ class Ftrl(optimizer_v2.OptimizerV2):\n                l2_shrinkage_regularization_strength=0.0,\n                beta=0.0,\n                **kwargs):\n-    super(Ftrl, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n \n     if initial_accumulator_value < 0.0:\n       raise ValueError(\n@@ -156,7 +156,7 @@ class Ftrl(optimizer_v2.OptimizerV2):\n       self.add_slot(var, 'linear')\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(Ftrl, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n     apply_state[(var_device, var_dtype)].update(\n         dict(\n             learning_rate_power=tf.identity(\n@@ -248,7 +248,7 @@ class Ftrl(optimizer_v2.OptimizerV2):\n           use_locking=self._use_locking)\n \n   def get_config(self):\n-    config = super(Ftrl, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate':\n             self._serialize_hyperparameter('learning_rate'),\n\n@@ -105,7 +105,7 @@ class SGD(optimizer_v2.OptimizerV2):\n                nesterov=False,\n                name=\"SGD\",\n                **kwargs):\n-    super(SGD, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n     self._set_hyper(\"decay\", self._initial_decay)\n \n@@ -125,7 +125,7 @@ class SGD(optimizer_v2.OptimizerV2):\n         self.add_slot(var, \"momentum\")\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(SGD, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n     apply_state[(var_device, var_dtype)][\"momentum\"] = tf.identity(\n         self._get_hyper(\"momentum\", var_dtype))\n \n@@ -154,7 +154,7 @@ class SGD(optimizer_v2.OptimizerV2):\n   def _resource_apply_sparse_duplicate_indices(self, grad, var, indices,\n                                                **kwargs):\n     if self._momentum:\n-      return super(SGD, self)._resource_apply_sparse_duplicate_indices(\n+      return super()._resource_apply_sparse_duplicate_indices(\n           grad, var, indices, **kwargs)\n     else:\n       var_device, var_dtype = var.device, var.dtype.base_dtype\n@@ -184,7 +184,7 @@ class SGD(optimizer_v2.OptimizerV2):\n         use_nesterov=self.nesterov)\n \n   def get_config(self):\n-    config = super(SGD, self).get_config()\n+    config = super().get_config()\n     config.update({\n         \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n         \"decay\": self._initial_decay,\n\n@@ -75,7 +75,7 @@ class Nadam(optimizer_v2.OptimizerV2):\n                        'tf.keras.optimizers.LearningRateSchedules as the '\n                        'learning rate.')\n \n-    super(Nadam, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n     self._set_hyper('decay', self._initial_decay)\n     self._set_hyper('beta_1', beta_1)\n@@ -141,7 +141,7 @@ class Nadam(optimizer_v2.OptimizerV2):\n   def _prepare(self, var_list):\n     # Get the value of the momentum cache before starting to apply gradients.\n     self._m_cache_read = tf.identity(self._m_cache)\n-    return super(Nadam, self)._prepare(var_list)\n+    return super()._prepare(var_list)\n \n   def _resource_apply_dense(self, grad, var, apply_state=None):\n     var_device, var_dtype = var.device, var.dtype.base_dtype\n@@ -207,7 +207,7 @@ class Nadam(optimizer_v2.OptimizerV2):\n     return tf.group(*[var_update, m_t_bar, v_t])\n \n   def get_config(self):\n-    config = super(Nadam, self).get_config()\n+    config = super().get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n         'decay': self._initial_decay,\n\n@@ -863,7 +863,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   def __getattribute__(self, name):\n     \"\"\"Overridden to support hyperparameter access.\"\"\"\n     try:\n-      return super(OptimizerV2, self).__getattribute__(name)\n+      return super().__getattribute__(name)\n     except AttributeError as e:\n       # Needed to avoid infinite recursion with __setattr__.\n       if name == \"_hyper\":\n@@ -876,7 +876,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       raise e\n \n   def __dir__(self):\n-    result = set(super(OptimizerV2, self).__dir__())\n+    result = set(super().__dir__())\n     if \"_hyper\" in result:\n       result |= self._hyper.keys()\n       if \"learning_rate\" in self._hyper.keys():\n@@ -891,7 +891,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     if hasattr(self, \"_hyper\") and name in self._hyper:\n       self._set_hyper(name, value)\n     else:\n-      super(OptimizerV2, self).__setattr__(name, value)\n+      super().__setattr__(name, value)\n \n   def get_slot_names(self):\n     \"\"\"A list of names for this optimizer's slots.\"\"\"\n@@ -1520,7 +1520,7 @@ class RestoredOptimizer(OptimizerV2):\n   # methods.\n \n   def __init__(self):\n-    super(RestoredOptimizer, self).__init__(\"RestoredOptimizer\")\n+    super().__init__(\"RestoredOptimizer\")\n     self._hypers_created = True\n \n   def get_config(self):\n\n@@ -1297,10 +1297,10 @@ class OptimizerCoefficientTest(test_combinations.TestCase):\n     class SubclassedOptimizer(optimizer_class):\n \n       def _resource_apply_dense(self, grad, var):  # pylint: disable=useless-super-delegation\n-        return super(SubclassedOptimizer, self)._resource_apply_dense(grad, var)\n+        return super()._resource_apply_dense(grad, var)\n \n       def _resource_apply_sparse(self, grad, var, indices):  # pylint: disable=useless-super-delegation\n-        return super(SubclassedOptimizer, self)._resource_apply_sparse(\n+        return super()._resource_apply_sparse(\n             grad, var, indices)\n \n     init_kwargs = init_kwargs or {}\n\n@@ -132,7 +132,7 @@ class RMSprop(optimizer_v2.OptimizerV2):\n     different invocations of optimizer functions.\n     @end_compatibility\n     \"\"\"\n-    super(RMSprop, self).__init__(name, **kwargs)\n+    super().__init__(name, **kwargs)\n     self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n     self._set_hyper(\"decay\", self._initial_decay)\n     self._set_hyper(\"rho\", rho)\n@@ -159,7 +159,7 @@ class RMSprop(optimizer_v2.OptimizerV2):\n         self.add_slot(var, \"mg\")\n \n   def _prepare_local(self, var_device, var_dtype, apply_state):\n-    super(RMSprop, self)._prepare_local(var_device, var_dtype, apply_state)\n+    super()._prepare_local(var_device, var_dtype, apply_state)\n \n     rho = tf.identity(self._get_hyper(\"rho\", var_dtype))\n     apply_state[(var_device, var_dtype)].update(\n@@ -282,10 +282,10 @@ class RMSprop(optimizer_v2.OptimizerV2):\n     # iteration to 0.\n     if len(params) == len(weights) + 1:\n       weights = [np.array(0)] + weights\n-    super(RMSprop, self).set_weights(weights)\n+    super().set_weights(weights)\n \n   def get_config(self):\n-    config = super(RMSprop, self).get_config()\n+    config = super().get_config()\n     config.update({\n         \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n         \"decay\": self._initial_decay,\n\n@@ -161,7 +161,7 @@ class ExponentialDecay(LearningRateSchedule):\n       name: String.  Optional name of the operation.  Defaults to\n         'ExponentialDecay'.\n     \"\"\"\n-    super(ExponentialDecay, self).__init__()\n+    super().__init__()\n     self.initial_learning_rate = initial_learning_rate\n     self.decay_steps = decay_steps\n     self.decay_rate = decay_rate\n@@ -252,7 +252,7 @@ class PiecewiseConstantDecay(LearningRateSchedule):\n     Raises:\n       ValueError: if the number of elements in the lists do not match.\n     \"\"\"\n-    super(PiecewiseConstantDecay, self).__init__()\n+    super().__init__()\n \n     if len(boundaries) != len(values) - 1:\n       raise ValueError(\n@@ -392,7 +392,7 @@ class PolynomialDecay(LearningRateSchedule):\n       name: String.  Optional name of the operation. Defaults to\n         'PolynomialDecay'.\n     \"\"\"\n-    super(PolynomialDecay, self).__init__()\n+    super().__init__()\n \n     self.initial_learning_rate = initial_learning_rate\n     self.decay_steps = decay_steps\n@@ -513,7 +513,7 @@ class InverseTimeDecay(LearningRateSchedule):\n       name: String.  Optional name of the operation.  Defaults to\n         'InverseTimeDecay'.\n     \"\"\"\n-    super(InverseTimeDecay, self).__init__()\n+    super().__init__()\n \n     self.initial_learning_rate = initial_learning_rate\n     self.decay_steps = decay_steps\n@@ -609,7 +609,7 @@ class CosineDecay(LearningRateSchedule):\n         Minimum learning rate value as a fraction of initial_learning_rate.\n       name: String. Optional name of the operation.  Defaults to 'CosineDecay'.\n     \"\"\"\n-    super(CosineDecay, self).__init__()\n+    super().__init__()\n \n     self.initial_learning_rate = initial_learning_rate\n     self.decay_steps = decay_steps\n@@ -707,7 +707,7 @@ class CosineDecayRestarts(LearningRateSchedule):\n         Minimum learning rate value as a fraction of the initial_learning_rate.\n       name: String. Optional name of the operation.  Defaults to 'SGDRDecay'.\n     \"\"\"\n-    super(CosineDecayRestarts, self).__init__()\n+    super().__init__()\n \n     self.initial_learning_rate = initial_learning_rate\n     self.first_decay_steps = first_decay_steps\n@@ -844,7 +844,7 @@ class LinearCosineDecay(LearningRateSchedule):\n       name: String.  Optional name of the operation.  Defaults to\n         'LinearCosineDecay'.\n     \"\"\"\n-    super(LinearCosineDecay, self).__init__()\n+    super().__init__()\n \n     self.initial_learning_rate = initial_learning_rate\n     self.decay_steps = decay_steps\n@@ -970,7 +970,7 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n       name: String.  Optional name of the operation.  Defaults to\n         'NoisyLinearCosineDecay'.\n     \"\"\"\n-    super(NoisyLinearCosineDecay, self).__init__()\n+    super().__init__()\n \n     self.initial_learning_rate = initial_learning_rate\n     self.decay_steps = decay_steps\n\n@@ -93,7 +93,7 @@ class LinearModel(training.Model):\n     self.bias_initializer = initializers.get(bias_initializer)\n     self.kernel_regularizer = regularizers.get(kernel_regularizer)\n     self.bias_regularizer = regularizers.get(bias_regularizer)\n-    super(LinearModel, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     base_layer.keras_premade_model_gauge.get_cell('Linear').set(True)\n \n   def build(self, input_shape):\n\n@@ -83,7 +83,7 @@ class WideDeepModel(keras_training.Model):\n       **kwargs: The keyword arguments that are passed on to BaseLayer.__init__.\n         Allowed keyword arguments include `name`.\n     \"\"\"\n-    super(WideDeepModel, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     base_layer.keras_premade_model_gauge.get_cell('WideDeep').set(True)\n     self.linear_model = linear_model\n     self.dnn_model = dnn_model\n\n@@ -43,7 +43,7 @@ class MyMeanAbsoluteError(losses.LossFunctionWrapper):\n   def __init__(self,\n                reduction=losses_utils.ReductionV2.AUTO,\n                name='mean_absolute_error'):\n-    super(MyMeanAbsoluteError, self).__init__(\n+    super().__init__(\n         my_mae, name=name, reduction=reduction)\n \n \n\n@@ -40,7 +40,7 @@ except ImportError:\n class MyMeanAbsoluteError(metrics.MeanMetricWrapper):\n \n   def __init__(self, name='my_mae', dtype=None):\n-    super(MyMeanAbsoluteError, self).__init__(_my_mae, name, dtype=dtype)\n+    super().__init__(_my_mae, name, dtype=dtype)\n \n \n # Custom metric function\n\n@@ -52,7 +52,7 @@ except ImportError:\n class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n-    super(TestSaveModel, self).setUp()\n+    super().setUp()\n     self.model = test_utils.get_small_sequential_mlp(1, 2, 3)\n     self.subclassed_model = test_utils.get_small_subclass_mlp(1, 2)\n \n@@ -164,7 +164,7 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.layer = keras.layers.Dense(1)\n \n       def call(self, x):\n@@ -240,7 +240,7 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n     class Sequential(keras.Model):\n \n       def __init__(self):\n-        super(Sequential, self).__init__()\n+        super().__init__()\n         self.layer = keras.layers.Dense(1)\n \n       def call(self, x):\n@@ -384,11 +384,11 @@ class TestJson(test_combinations.TestCase):\n     class MyLayer(keras.layers.Layer):\n \n       def __init__(self, sublayers, **kwargs):\n-        super(MyLayer, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.sublayers = sublayers\n \n       def get_config(self):\n-        config = super(MyLayer, self).get_config()\n+        config = super().get_config()\n         config['sublayers'] = self.sublayers\n         return config\n \n@@ -1019,7 +1019,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n     class OuterLayer(keras.layers.Layer):\n \n       def __init__(self, inner_layer):\n-        super(OuterLayer, self).__init__()\n+        super().__init__()\n         self.inner_layer = inner_layer\n \n       def call(self, inputs):\n@@ -1039,7 +1039,7 @@ class TestWholeModelSaving(test_combinations.TestCase):\n     class InnerLayer(keras.layers.Layer):\n \n       def __init__(self):\n-        super(InnerLayer, self).__init__()\n+        super().__init__()\n         self.v = self.add_weight(name='v', shape=[], dtype=tf.float32)\n \n       def call(self, inputs):\n@@ -1313,7 +1313,7 @@ def _make_sequential_input_shape(input_size, output_size):\n class _make_subclassed(keras.Model):  # pylint: disable=invalid-name\n \n   def __init__(self, input_size, output_size):\n-    super(_make_subclassed, self).__init__()\n+    super().__init__()\n     self._config = {'input_size': input_size, 'output_size': output_size}\n     self._hidden_layer = keras.layers.Dense(8, activation='relu', name='hidden')\n     self._logits_layer = keras.layers.Dense(output_size, name='logits')\n@@ -1333,7 +1333,7 @@ class _make_subclassed(keras.Model):  # pylint: disable=invalid-name\n class _make_subclassed_built(_make_subclassed):  # pylint: disable=invalid-name\n \n   def __init__(self, input_size, output_size):\n-    super(_make_subclassed_built, self).__init__(input_size, output_size)\n+    super().__init__(input_size, output_size)\n     self.build((None, input_size))\n \n \n\n@@ -376,7 +376,7 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n class SubclassedModel(training.Model):\n \n   def __init__(self):\n-    super(SubclassedModel, self).__init__()\n+    super().__init__()\n     self.x_layer = keras.layers.Dense(3)\n     self.b_layer = keras.layers.Dense(1)\n \n@@ -585,7 +585,7 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n     class SubclassedModelRestore(training.Model):\n \n       def __init__(self):\n-        super(SubclassedModelRestore, self).__init__()\n+        super().__init__()\n         self.x_layer = keras.layers.Dense(3)\n         self.y_layer = keras.layers.Dense(3)\n         self.b_layer = keras.layers.Dense(1)\n\n@@ -50,7 +50,7 @@ class Encoder(json.JSONEncoder):\n     return get_json_type(obj)\n \n   def encode(self, obj):\n-    return super(Encoder, self).encode(_encode_tuple(obj))\n+    return super().encode(_encode_tuple(obj))\n \n \n def _encode_tuple(x):\n\n@@ -147,7 +147,7 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n \n   def _get_serialized_attributes_internal(self, serialization_cache):\n     objects, functions = (\n-        super(RNNSavedModelSaver, self)._get_serialized_attributes_internal(\n+        super()._get_serialized_attributes_internal(\n             serialization_cache))\n     states = tf.__internal__.tracking.wrap(self.obj.states)\n     # SaveModel require all the objects to be Trackable when saving.\n\n@@ -22,7 +22,7 @@ class LoadContext(threading.local):\n   \"\"\"A context for loading a model.\"\"\"\n \n   def __init__(self):\n-    super(LoadContext, self).__init__()\n+    super().__init__()\n     self._entered_load_context = []\n     self._load_options = None\n \n\n@@ -28,7 +28,7 @@ class ModelSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n     return constants.MODEL_IDENTIFIER\n \n   def _python_properties_internal(self):\n-    metadata = super(ModelSavedModelSaver, self)._python_properties_internal()\n+    metadata = super()._python_properties_internal()\n     # Network stateful property is dependent on the child layers.\n     metadata.pop('stateful')\n     metadata['is_graph_network'] = self.obj._is_graph_network  # pylint: disable=protected-access\n@@ -53,7 +53,7 @@ class ModelSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n     # Other than the default signature function, all other attributes match with\n     # the ones serialized by Layer.\n     objects, functions = (\n-        super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(\n+        super()._get_serialized_attributes_internal(\n             serialization_cache))\n     functions['_default_save_signature'] = default_signature\n     return objects, functions\n\n@@ -37,7 +37,7 @@ from keras.utils import generic_utils\n class SubclassedModelNoConfig(keras.Model):\n \n   def __init__(self, a, b):\n-    super(SubclassedModelNoConfig, self).__init__()\n+    super().__init__()\n \n     self.a = a\n     self.b = b\n@@ -53,7 +53,7 @@ class SubclassedModelNoConfig(keras.Model):\n             # TODO(b/145029112): Bug with losses when there are shared layers.\n             # self.shared,  <-- Enable when bug is fixed.\n             CustomLayerNoConfig(self.a + 5, self.b + 6)])])\n-    super(SubclassedModelNoConfig, self).build(input_shape)\n+    super().build(input_shape)\n \n   def call(self, inputs):\n     x = inputs\n@@ -80,7 +80,7 @@ class SparseDense(keras.layers.Dense):\n class SubclassedSparseModelNoConfig(keras.Model):\n \n   def __init__(self, a, b):\n-    super(SubclassedSparseModelNoConfig, self).__init__()\n+    super().__init__()\n     self.a = a\n     self.shared = CustomLayerNoConfig(a, b)\n     self.all_layers = [SparseDense(4)]\n@@ -106,7 +106,7 @@ class SubclassedModelWithConfig(SubclassedModelNoConfig):\n class CustomLayerNoConfig(keras.layers.Layer):\n \n   def __init__(self, a, b, name=None):\n-    super(CustomLayerNoConfig, self).__init__(name=name)\n+    super().__init__(name=name)\n     self.a = tf.Variable(a, name='a')\n     self.b = b\n     def a_regularizer():\n@@ -141,13 +141,13 @@ class CustomNetworkDefaultConfig(keras.Model):\n     inputs = keras.Input((2, 3), name='inputs')\n     x = keras.layers.Flatten(name='flatten')(inputs)\n     y = keras.layers.Dense(num_classes, name='outputs')(x)\n-    super(CustomNetworkDefaultConfig, self).__init__(inputs, y, name=name)\n+    super().__init__(inputs, y, name=name)\n \n \n class CustomNetworkWithConfig(CustomNetworkDefaultConfig):\n \n   def __init__(self, num_classes, name=None):\n-    super(CustomNetworkWithConfig, self).__init__(num_classes, name=name)\n+    super().__init__(num_classes, name=name)\n     self._config_dict = dict(num_classes=num_classes)\n \n   def get_config(self):\n@@ -161,7 +161,7 @@ class CustomNetworkWithConfig(CustomNetworkDefaultConfig):\n class CustomNetworkWithConfigName(CustomNetworkWithConfig):\n \n   def __init__(self, num_classes, name=None):\n-    super(CustomNetworkWithConfigName, self).__init__(num_classes, name=name)\n+    super().__init__(num_classes, name=name)\n     self._config_dict['name'] = self.name\n \n \n@@ -169,7 +169,7 @@ class UnregisteredCustomSequentialModel(keras.Sequential):\n   # This class is *not* registered in the CustomObjectScope.\n \n   def __init__(self, **kwargs):\n-    super(UnregisteredCustomSequentialModel, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self.add(keras.layers.InputLayer(input_shape=(2, 3)))\n \n \n@@ -202,7 +202,7 @@ class WideDeepModel(SubclassedModelWithConfig):\n class ReviveTestBase(test_combinations.TestCase):\n \n   def setUp(self):\n-    super(ReviveTestBase, self).setUp()\n+    super().setUp()\n     self.path = self.get_temp_dir()\n     self.addCleanup(shutil.rmtree, self.path, ignore_errors=True)\n \n@@ -287,7 +287,7 @@ class TestBigModelRevive(ReviveTestBase):\n     class SubclassedModel(keras.Model):\n \n       def __init__(self):\n-        super(SubclassedModel, self).__init__()\n+        super().__init__()\n         self.all_layers = [CustomLayerWithConfig(1., 2),\n                            CustomLayerNoConfig(3., 4),\n                            SubclassedModelWithConfig(4., 6.),\n\n@@ -345,7 +345,7 @@ def _restore_layer_losses(losses_dict):\n class LayerTracingContext(threading.local):\n \n   def __init__(self):\n-    super(LayerTracingContext, self).__init__()\n+    super().__init__()\n     self.enable_call_tracing = False\n     self.trace_queue = []\n \n\n@@ -346,7 +346,7 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n     class LayerWithNestedSpec(keras.layers.Layer):\n \n       def __init__(self):\n-        super(LayerWithNestedSpec, self).__init__()\n+        super().__init__()\n         self.input_spec = {\n             'a': keras.layers.InputSpec(max_ndim=3, axes={-1: 2}),\n             'b': keras.layers.InputSpec(shape=(None, 2, 3), dtype='int32')}\n@@ -590,7 +590,7 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n     class Model(keras.models.Model):\n \n       def __init__(self):\n-        super(Model, self).__init__()\n+        super().__init__()\n         self.layer_with_training_default_none = LayerWithLearningPhase()\n         self.layer_with_training_default_true = LayerWithTrainingDefaultTrue()\n         self.layer_with_required_training_arg = LayerWithTrainingRequiredArg()\n@@ -714,10 +714,10 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n \n       def build(self, input_shape):\n         self.w = self.add_weight('w', shape=[])\n-        super(CustomAdd, self).build(input_shape)\n+        super().build(input_shape)\n \n       def call(self, inputs):\n-        outputs = super(CustomAdd, self).call(inputs)\n+        outputs = super().call(inputs)\n         return outputs * self.w\n \n     input1 = keras.layers.Input(shape=(None, 3), name='input_1')\n@@ -967,7 +967,7 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n     class Custom(keras.models.Model):\n \n       def __init__(self):\n-        super(Custom, self).__init__()\n+        super().__init__()\n         self.layer = LayerWithLearningPhase()\n \n       def call(self, inputs):\n@@ -1009,7 +1009,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n     class Model(keras.models.Model):\n \n       def __init__(self):\n-        super(Model, self).__init__()\n+        super().__init__()\n         self.layer = CustomLayer()\n \n       @tf.function(\n@@ -1055,7 +1055,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n     class DoNotTrace(keras.layers.Layer):\n \n       def __init__(self):\n-        super(DoNotTrace, self).__init__()\n+        super().__init__()\n         self.input_spec = keras.layers.InputSpec(shape=[None])\n         self.built = True\n \n@@ -1181,7 +1181,7 @@ class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n \n       def __init__(self):\n         self.child = LayerWithKwargs()\n-        super(LayerWithChildLayer, self).__init__()\n+        super().__init__()\n \n       def call(self, inputs):\n         return self.child(inputs)\n@@ -1210,7 +1210,7 @@ class CustomMeanMetric(keras.metrics.Mean):\n     # Sometimes built-in metrics return an op in update_state. Custom\n     # metrics don't support returning ops, so wrap the update_state method\n     # while returning nothing.\n-    super(CustomMeanMetric, self).update_state(*args)\n+    super().update_state(*args)\n \n \n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n@@ -1300,7 +1300,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n         # Sometimes built-in metrics return an op in update_state. Custom\n         # metrics don't support returning ops, so wrap the update_state method\n         # while returning nothing.\n-        super(CustomMetric, self).update_state(*args)\n+        super().update_state(*args)\n \n     with self.cached_session():\n       metric = CustomMetric()\n@@ -1352,7 +1352,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n       @tf.function(\n           input_signature=[tf.TensorSpec(None, tf.float32)])\n       def update_state(self, value):\n-        super(NegativeMean, self).update_state(-value)\n+        super().update_state(-value)\n \n     metric = NegativeMean()\n     self.evaluate([v.initializer for v in metric.variables])\n\n@@ -207,7 +207,7 @@ def set_training_arg_spec(arg_spec, default_training_value):\n class SaveOptionsContext(threading.local):\n \n   def __init__(self):\n-    super(SaveOptionsContext, self).__init__()\n+    super().__init__()\n     self.save_traces = True\n \n \n\n@@ -176,7 +176,7 @@ class TestModelSavingandLoading(parameterized.TestCase, tf.test.TestCase):\n     class SubclassedModel(model_lib.Model):\n \n       def __init__(self):\n-        super(SubclassedModel, self).__init__()\n+        super().__init__()\n         self.layer1 = keras.layers.Dense(3)\n         self.layer2 = keras.layers.Dense(1)\n \n@@ -239,7 +239,7 @@ def sequential_model_without_input_shape(uses_learning_phase=True):\n class Subclassed(keras.models.Model):\n \n   def __init__(self):\n-    super(Subclassed, self).__init__()\n+    super().__init__()\n     self.dense1 = keras.layers.Dense(2)\n     self.dense2 = keras.layers.Dense(3)\n \n\n@@ -187,7 +187,7 @@ class TraceModelCallTest(test_combinations.TestCase):\n     class Model(keras.Model):\n \n       def __init__(self):\n-        super(Model, self).__init__()\n+        super().__init__()\n         self.dense = keras.layers.Dense(3, name='dense')\n \n       @tf.function(\n@@ -296,7 +296,7 @@ class BasicAutographedMetricLayer(keras.layers.Layer):\n class BasicAutographedMetricModel(keras.models.Model):\n \n   def __init__(self):\n-    super(BasicAutographedMetricModel, self).__init__(name='test_model')\n+    super().__init__(name='test_model')\n     self._layer = BasicAutographedMetricLayer()\n \n   def call(self, inputs, **kwargs):\n@@ -399,7 +399,7 @@ class ExtractModelMetricsTest(test_combinations.TestCase):\n class UnbuiltModelSavingErrorMessageTest(test_combinations.TestCase):\n \n   def setUp(self):\n-    super(UnbuiltModelSavingErrorMessageTest, self).setUp()\n+    super().setUp()\n     if not tf.__internal__.tf2.enabled():\n       self.skipTest('The test does not intend to cover TF1.')\n \n\n@@ -39,7 +39,7 @@ class TestCase(tf.test.TestCase, parameterized.TestCase):\n \n   def tearDown(self):\n     keras.backend.clear_session()\n-    super(TestCase, self).tearDown()\n+    super().tearDown()\n \n \n def run_with_all_saved_model_formats(\n\n@@ -442,7 +442,7 @@ class SmallSubclassMLP(models.Model):\n                use_bn=False,\n                use_dp=False,\n                **kwargs):\n-    super(SmallSubclassMLP, self).__init__(name='test_model', **kwargs)\n+    super().__init__(name='test_model', **kwargs)\n     self.use_bn = use_bn\n     self.use_dp = use_dp\n \n@@ -467,7 +467,7 @@ class _SmallSubclassMLPCustomBuild(models.Model):\n   \"\"\"A subclass model small MLP that uses a custom build method.\"\"\"\n \n   def __init__(self, num_hidden, num_classes):\n-    super(_SmallSubclassMLPCustomBuild, self).__init__()\n+    super().__init__()\n     self.layer_a = None\n     self.layer_b = None\n     self.num_hidden = num_hidden\n@@ -519,7 +519,7 @@ class _SubclassModel(models.Model):\n     \"\"\"\n \n     inputs = kwargs.pop('input_tensor', None)\n-    super(_SubclassModel, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n     # Note that clone and build doesn't support lists of layers in subclassed\n     # models. Adding each layer directly here.\n     for i, layer in enumerate(model_layers):\n@@ -550,7 +550,7 @@ class _SubclassModelCustomBuild(models.Model):\n   \"\"\"A Keras subclass model that uses a custom build method.\"\"\"\n \n   def __init__(self, layer_generating_func, *args, **kwargs):\n-    super(_SubclassModelCustomBuild, self).__init__(*args, **kwargs)\n+    super().__init__(*args, **kwargs)\n     self.all_layers = None\n     self._layer_generating_func = layer_generating_func\n \n@@ -649,7 +649,7 @@ class _MultiIOSubclassModel(models.Model):\n \n   def __init__(self, branch_a, branch_b, shared_input_branch=None,\n                shared_output_branch=None, name=None):\n-    super(_MultiIOSubclassModel, self).__init__(name=name)\n+    super().__init__(name=name)\n     self._shared_input_branch = shared_input_branch\n     self._branch_a = branch_a\n     self._branch_b = branch_b\n@@ -686,7 +686,7 @@ class _MultiIOSubclassModelCustomBuild(models.Model):\n   def __init__(self, branch_a_func, branch_b_func,\n                shared_input_branch_func=None,\n                shared_output_branch_func=None):\n-    super(_MultiIOSubclassModelCustomBuild, self).__init__()\n+    super().__init__()\n     self._shared_input_branch_func = shared_input_branch_func\n     self._branch_a_func = branch_a_func\n     self._branch_b_func = branch_b_func\n\n@@ -56,7 +56,7 @@ def get_ctl_train_step(model):\n class TestAddLossCorrectness(test_combinations.TestCase):\n \n   def setUp(self):\n-    super(TestAddLossCorrectness, self).setUp()\n+    super().setUp()\n     self.x = np.array([[0.], [1.], [2.]], dtype='float32')\n     self.y = np.array([[0.5], [2.], [3.5]], dtype='float32')\n     self.w = np.array([[1.25], [0.5], [1.25]], dtype='float32')\n@@ -176,7 +176,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n     class MyModel(Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.bias = test_utils.Bias()\n \n       def call(self, inputs):\n@@ -204,7 +204,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n     class MyLayer(layers.Layer):\n \n       def __init__(self):\n-        super(MyLayer, self).__init__()\n+        super().__init__()\n         self.bias = test_utils.Bias()\n \n       def call(self, inputs):\n@@ -352,7 +352,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n     class LayerWithNestedLayerWithLoss(layers.Layer):\n \n       def __init__(self):\n-        super(LayerWithNestedLayerWithLoss, self).__init__()\n+        super().__init__()\n         self.loss_layer = LayerWithLoss()\n \n       def call(self, inputs):\n@@ -372,7 +372,7 @@ class TestAddLossCorrectness(test_combinations.TestCase):\n     class LayerWithSharedNestedLossLayer(layers.Layer):\n \n       def __init__(self):\n-        super(LayerWithSharedNestedLossLayer, self).__init__()\n+        super().__init__()\n         self.loss_layer = layers.ActivityRegularization(l2=0.001)\n         self.add_weight(shape=(1,), regularizer='l2')\n \n\n@@ -95,7 +95,7 @@ class CustomModel(training.Model):\n   \"\"\"Custom model with summary ops in model call definition.\"\"\"\n \n   def __init__(self, name=None, enable_histograms=True):\n-    super(CustomModel, self).__init__()\n+    super().__init__()\n     self._my_layers = [\n         layer_lib.Dense(\n             4096,\n@@ -158,7 +158,7 @@ def mnist_model(input_shape, enable_histograms=True):\n class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n \n   def setUp(self):\n-    super(AutoOutsideCompilationWithKerasTest, self).setUp()\n+    super().setUp()\n     set_soft_device_placement(True)\n     self.summary_dir = self.get_temp_dir()\n \n\n@@ -147,7 +147,7 @@ class VariablesToConstantsTest(tf.test.TestCase):\n     class EmbeddingModel(keras.Model):\n \n       def __init__(self):\n-        super(EmbeddingModel, self).__init__()\n+        super().__init__()\n         self.shared_weights = self.add_weight(\n             \"weights\",\n             shape=(2000, 300),\n\n@@ -166,7 +166,7 @@ class CustomTrainingLoopTest(test_combinations.TestCase):\n     class MyModel(keras.layers.Layer):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.layer = LayerWithTrainingArg()\n \n       def call(self, inputs):\n@@ -203,7 +203,7 @@ class CustomTrainingLoopTest(test_combinations.TestCase):\n     class MyModel(keras.layers.Layer):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.layer = LayerWithTrainingArg()\n \n       def call(self, inputs, training=False):\n\n@@ -30,7 +30,7 @@ class SingleLayerNet(keras.Model):\n   \"\"\"Simple keras model used to ensure that there are no leaks.\"\"\"\n \n   def __init__(self):\n-    super(SingleLayerNet, self).__init__()\n+    super().__init__()\n     self.fc1 = keras.layers.Dense(5)\n \n   def call(self, x):\n\n@@ -154,7 +154,7 @@ class MySubclassModel(keras.Model):\n   \"\"\"A subclass model.\"\"\"\n \n   def __init__(self, input_dim=3):\n-    super(MySubclassModel, self).__init__(name='my_subclass_model')\n+    super().__init__(name='my_subclass_model')\n     self._config = {'input_dim': input_dim}\n     self.dense1 = keras.layers.Dense(8, activation='relu')\n     self.dense2 = keras.layers.Dense(2, activation='softmax')\n@@ -182,7 +182,7 @@ def nested_subclassed_model():\n     \"\"\"A nested subclass model.\"\"\"\n \n     def __init__(self):\n-      super(NestedSubclassModel, self).__init__()\n+      super().__init__()\n       self.dense1 = keras.layers.Dense(4, activation='relu')\n       self.dense2 = keras.layers.Dense(2, activation='relu')\n       self.bn = keras.layers.BatchNormalization()\n@@ -221,7 +221,7 @@ def nested_functional_in_subclassed_model():\n     \"\"\"A functional nested in subclass model.\"\"\"\n \n     def __init__(self):\n-      super(NestedFunctionalInSubclassModel, self).__init__(\n+      super().__init__(\n           name='nested_functional_in_subclassed_model')\n       self.dense1 = keras.layers.Dense(4, activation='relu')\n       self.dense2 = keras.layers.Dense(2, activation='relu')\n@@ -241,7 +241,7 @@ def shared_layer_subclassed_model():\n     \"\"\"A subclass model with shared layers.\"\"\"\n \n     def __init__(self):\n-      super(SharedLayerSubclassModel, self).__init__(\n+      super().__init__(\n           name='shared_layer_subclass_model')\n       self.dense = keras.layers.Dense(3, activation='relu')\n       self.dp = keras.layers.Dropout(0.5)\n\n@@ -135,7 +135,7 @@ class ModelSubclassCompiledTest(test_combinations.TestCase):\n     class BNNet(keras.Model):\n \n       def __init__(self):\n-        super(BNNet, self).__init__()\n+        super().__init__()\n         self.bn = keras.layers.BatchNormalization(beta_initializer='ones',\n                                                   gamma_initializer='ones')\n \n@@ -165,7 +165,7 @@ class ModelSubclassCompiledTest(test_combinations.TestCase):\n     class DPNet(keras.Model):\n \n       def __init__(self):\n-        super(DPNet, self).__init__()\n+        super().__init__()\n         self.dp = keras.layers.Dropout(0.5)\n         self.dense = keras.layers.Dense(1,\n                                         use_bias=False,\n@@ -375,7 +375,7 @@ class ModelSubclassCompiledTest(test_combinations.TestCase):\n     class Inner(keras.Model):\n \n       def __init__(self):\n-        super(Inner, self).__init__()\n+        super().__init__()\n         self.dense1 = keras.layers.Dense(32, activation='relu')\n         self.dense2 = keras.layers.Dense(num_classes, activation='relu')\n         self.bn = keras.layers.BatchNormalization()\n@@ -412,7 +412,7 @@ class ModelSubclassCompiledTest(test_combinations.TestCase):\n     class DPNet(keras.Model):\n \n       def __init__(self):\n-        super(DPNet, self).__init__()\n+        super().__init__()\n         self.dp = keras.layers.Dropout(0.5)\n         self.dense = keras.layers.Dense(1,\n                                         use_bias=False,\n\n@@ -42,7 +42,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class DummyModel(keras.Model):\n \n       def __init__(self):\n-        super(DummyModel, self).__init__()\n+        super().__init__()\n         self.dense1 = keras.layers.Dense(32, activation='relu')\n         self.uses_custom_build = False\n \n@@ -75,7 +75,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class DummyModel(keras.Model):\n \n       def __init__(self):\n-        super(DummyModel, self).__init__()\n+        super().__init__()\n         self.layer1 = keras.layers.Dense(10, activation='relu')\n \n       def build(self, input_shape):\n@@ -98,7 +98,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.dense1 = keras.layers.Dense(1)\n         self.dense2 = keras.layers.Dense(1)\n         self.add = keras.layers.Add()\n@@ -137,7 +137,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n       \"\"\"An Embedding layer.\"\"\"\n \n       def __init__(self, vocab_size, embedding_dim, **kwargs):\n-        super(Embedding, self).__init__(**kwargs)\n+        super().__init__(**kwargs)\n         self.vocab_size = vocab_size\n         self.embedding_dim = embedding_dim\n \n@@ -155,7 +155,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class EmbedModel(keras.Model):\n \n       def __init__(self, vocab_size, embed_size):\n-        super(EmbedModel, self).__init__()\n+        super().__init__()\n         self.embed1 = Embedding(vocab_size, embed_size)\n \n       def call(self, inputs):\n@@ -178,7 +178,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class SimpleRNNModel(keras.Model):\n \n       def __init__(self):\n-        super(SimpleRNNModel, self).__init__()\n+        super().__init__()\n         self.lstm = keras.layers.LSTM(units)\n \n       def call(self, inputs):\n@@ -354,7 +354,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class Foo(keras.Model):\n \n       def __init__(self):\n-        super(Foo, self).__init__()\n+        super().__init__()\n         self.isdep = keras.layers.Dense(1)\n         self.notdep = data_structures.NoDependency(keras.layers.Dense(2))\n         self.notdep_var = data_structures.NoDependency(\n@@ -371,7 +371,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class ExtraVar(keras.Model):\n \n       def __init__(self):\n-        super(ExtraVar, self).__init__()\n+        super().__init__()\n         self.dense = keras.layers.Dense(1)\n         self.var = tf.Variable(1.)\n         self.not_trainable_var = tf.Variable(2., trainable=False)\n@@ -419,7 +419,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.b = self.add_weight('bias', (10,))\n         self.c = self.add_weight('bias2', (10,), trainable=False)\n \n@@ -454,7 +454,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.b = self.add_weight('bias', (10,))\n         self.c = self.add_weight('bias2', (10,))\n \n@@ -517,7 +517,7 @@ class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n     class TestModel1(keras.Model):\n \n       def __init__(self):\n-        super(TestModel1, self).__init__()\n+        super().__init__()\n         self.fc = keras.layers.Dense(10, input_shape=(784,),\n                                      activity_regularizer='l1')\n         self.bn = keras.Sequential([keras.layers.BatchNormalization(axis=1)])\n@@ -537,7 +537,7 @@ class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n     class TestModel2(keras.Model):\n \n       def __init__(self):\n-        super(TestModel2, self).__init__()\n+        super().__init__()\n         self.fc = keras.layers.Dense(10, input_shape=(784,),\n                                      activity_regularizer='l1')\n         self.bn = keras.Sequential(\n@@ -563,7 +563,7 @@ class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n       class TestModel3(keras.Model):\n \n         def __init__(self):\n-          super(TestModel3, self).__init__()\n+          super().__init__()\n           self.fc = keras.layers.Dense(10, input_shape=(784,),\n                                        activity_regularizer='l1')\n           self.bn = bn\n@@ -710,7 +710,7 @@ class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.my_variable = tf.Variable(0.0, trainable=False)\n         self.layer = keras.layers.Dense(4)\n \n@@ -737,7 +737,7 @@ class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n     class MyModel(keras.Model):\n \n       def __init__(self):\n-        super(MyModel, self).__init__()\n+        super().__init__()\n         self.layer = keras.layers.Dense(4)\n \n       def call(self, obs):\n\n@@ -22,7 +22,7 @@ from keras.testing_infra import test_utils\n class SimpleConvTestModel(keras.Model):\n \n   def __init__(self, num_classes=10):\n-    super(SimpleConvTestModel, self).__init__(name='test_model')\n+    super().__init__(name='test_model')\n     self.num_classes = num_classes\n \n     self.conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu')\n@@ -59,7 +59,7 @@ class NestedTestModel1(keras.Model):\n   \"\"\"\n \n   def __init__(self, num_classes=2):\n-    super(NestedTestModel1, self).__init__(name='nested_model_1')\n+    super().__init__(name='nested_model_1')\n     self.num_classes = num_classes\n     self.dense1 = keras.layers.Dense(32, activation='relu')\n     self.dense2 = keras.layers.Dense(num_classes, activation='relu')\n@@ -79,7 +79,7 @@ class NestedTestModel2(keras.Model):\n   \"\"\"\n \n   def __init__(self, num_classes=2):\n-    super(NestedTestModel2, self).__init__(name='nested_model_2')\n+    super().__init__(name='nested_model_2')\n     self.num_classes = num_classes\n     self.dense1 = keras.layers.Dense(32, activation='relu')\n     self.dense2 = keras.layers.Dense(num_classes, activation='relu')\n@@ -113,7 +113,7 @@ def get_nested_model_3(input_dim, num_classes):\n   class Inner(keras.Model):\n \n     def __init__(self):\n-      super(Inner, self).__init__()\n+      super().__init__()\n       self.dense1 = keras.layers.Dense(32, activation='relu')\n       self.dense2 = keras.layers.Dense(5, activation='relu')\n       self.bn = keras.layers.BatchNormalization()\n@@ -132,7 +132,7 @@ def get_nested_model_3(input_dim, num_classes):\n class CustomCallModel(keras.Model):\n \n   def __init__(self):\n-    super(CustomCallModel, self).__init__()\n+    super().__init__()\n     self.dense1 = keras.layers.Dense(1, activation='relu')\n     self.dense2 = keras.layers.Dense(1, activation='softmax')\n \n@@ -147,7 +147,7 @@ class CustomCallModel(keras.Model):\n class TrainingNoDefaultModel(keras.Model):\n \n   def __init__(self):\n-    super(TrainingNoDefaultModel, self).__init__()\n+    super().__init__()\n     self.dense1 = keras.layers.Dense(1)\n \n   def call(self, x, training):\n@@ -157,7 +157,7 @@ class TrainingNoDefaultModel(keras.Model):\n class TrainingMaskingModel(keras.Model):\n \n   def __init__(self):\n-    super(TrainingMaskingModel, self).__init__()\n+    super().__init__()\n     self.dense1 = keras.layers.Dense(1)\n \n   def call(self, x, training=False, mask=None):\n\n@@ -44,7 +44,7 @@ class _ModelWithOptimizerUsingDefun(tf.train.Checkpoint):\n class MemoryTests(tf.test.TestCase):\n \n   def setUp(self):\n-    super(MemoryTests, self).setUp()\n+    super().setUp()\n     self._model = _ModelWithOptimizerUsingDefun()\n \n   @tf_test_utils.assert_no_garbage_created\n\n@@ -26,7 +26,7 @@ from tensorflow.python.training.tracking import util as trackable_utils\n class NonLayerTrackable(tf.Module):\n \n   def __init__(self):\n-    super(NonLayerTrackable, self).__init__()\n+    super().__init__()\n     self.a_variable = trackable_utils.add_variable(\n         self, name=\"a_variable\", shape=[])\n \n@@ -35,7 +35,7 @@ class MyModel(training.Model):\n   \"\"\"A concrete Model for testing.\"\"\"\n \n   def __init__(self):\n-    super(MyModel, self).__init__()\n+    super().__init__()\n     self._named_dense = core.Dense(1, use_bias=True)\n     self._second = core.Dense(1, use_bias=False)\n     # We can still track Trackables which aren't Layers.\n\n@@ -31,7 +31,7 @@ from tensorflow.python.training.tracking import util\n class HasList(training.Model):\n \n   def __init__(self):\n-    super(HasList, self).__init__()\n+    super().__init__()\n     self.layer_list = tf.__internal__.tracking.wrap([core.Dense(3)])\n     self.layer_list.append(core.Dense(4))\n     self.layer_list.extend(\n@@ -111,7 +111,7 @@ class ListTests(test_combinations.TestCase):\n     class _Subclassed(training.Model):\n \n       def __init__(self, wrapped):\n-        super(_Subclassed, self).__init__()\n+        super().__init__()\n         self._wrapped = wrapped\n \n       def call(self, x):\n@@ -129,7 +129,7 @@ class ListTests(test_combinations.TestCase):\n     class AttrDict(dict):\n \n       def __init__(self, *args, **kwargs):\n-        super(AttrDict, self).__init__(*args, **kwargs)\n+        super().__init__(*args, **kwargs)\n         self.__dict__ = self\n \n     def ffnet(layer_sizes, name):\n@@ -143,7 +143,7 @@ class ListTests(test_combinations.TestCase):\n     class MyModel2(training.Model):\n \n       def __init__(self, config, name=\"my_model_2\"):\n-        super(MyModel2, self).__init__(name=name)\n+        super().__init__(name=name)\n         self._num_tokens = config.num_tokens\n \n         # list of sub-models\n@@ -188,7 +188,7 @@ class ListTests(test_combinations.TestCase):\n     class HasEqualContainers(training.Model):\n \n       def __init__(self):\n-        super(HasEqualContainers, self).__init__()\n+        super().__init__()\n         self.l1 = []\n         self.l2 = []\n \n@@ -206,7 +206,7 @@ class ListTests(test_combinations.TestCase):\n     class ListToTensor(training.Model):\n \n       def __init__(self):\n-        super(ListToTensor, self).__init__()\n+        super().__init__()\n         self.l = [1., 2., 3.]\n \n     self.assertAllEqual(\n@@ -231,7 +231,7 @@ class ListWrapperTest(tf.test.TestCase):\n class HasMapping(training.Model):\n \n   def __init__(self):\n-    super(HasMapping, self).__init__()\n+    super().__init__()\n     self.layer_dict = tf.__internal__.tracking.wrap(dict(output=core.Dense(7)))\n     self.layer_dict[\"norm\"] = tf.__internal__.tracking.wrap([])\n     self.layer_dict[\"dense\"] = tf.__internal__.tracking.wrap([])\n@@ -387,7 +387,7 @@ class MappingTests(test_combinations.TestCase):\n class HasTuple(training.Model):\n \n   def __init__(self):\n-    super(HasTuple, self).__init__()\n+    super().__init__()\n     self.layer_list = (\n         core.Dense(3), core.Dense(4),\n         core.Dense(5, kernel_regularizer=tf.reduce_sum))\n@@ -458,7 +458,7 @@ class TupleTests(test_combinations.TestCase):\n     class _Subclassed(training.Model):\n \n       def __init__(self, wrapped):\n-        super(_Subclassed, self).__init__()\n+        super().__init__()\n         self._wrapped = wrapped\n \n       def call(self, x):\n@@ -498,7 +498,7 @@ class TupleTests(test_combinations.TestCase):\n     class HasEqualContainers(training.Model):\n \n       def __init__(self):\n-        super(HasEqualContainers, self).__init__()\n+        super().__init__()\n         self.l1 = ()\n         self.l2 = ()\n \n@@ -522,7 +522,7 @@ class TupleTests(test_combinations.TestCase):\n     class TupleToTensor(training.Model):\n \n       def __init__(self):\n-        super(TupleToTensor, self).__init__()\n+        super().__init__()\n         self.l = (1., 2., 3.)\n \n     self.assertAllEqual(\n@@ -551,7 +551,7 @@ class InterfaceTests(test_combinations.TestCase):\n \n       @tf.__internal__.tracking.no_automatic_dependency_tracking\n       def __init__(self):\n-        super(NoDependencyModel, self).__init__()\n+        super().__init__()\n         self.a = []\n         self.b = tf.Module()\n \n\n@@ -37,7 +37,7 @@ class MyModel(training.Model):\n   \"\"\"A concrete Model for testing.\"\"\"\n \n   def __init__(self):\n-    super(MyModel, self).__init__()\n+    super().__init__()\n     self._named_dense = core.Dense(1, use_bias=True)\n     self._second = core.Dense(1, use_bias=False)\n     # We can still track Trackables which aren't Layers.\n@@ -51,7 +51,7 @@ class MyModel(training.Model):\n class NonLayerTrackable(tf.Module):\n \n   def __init__(self):\n-    super(NonLayerTrackable, self).__init__()\n+    super().__init__()\n     self.a_variable = trackable_utils.add_variable(\n         self, name=\"a_variable\", shape=[])\n \n@@ -420,7 +420,7 @@ class CheckpointingTests(test_combinations.TestCase):\n     class Model(training.Model):\n \n       def __init__(self):\n-        super(Model, self).__init__()\n+        super().__init__()\n         self.w = tf.Variable(0.0)\n         self.b = tf.Variable(0.0)\n         self.vars = [self.w, self.b]\n\n@@ -30,7 +30,7 @@ from tensorflow.python.training.tracking import util as trackable_utils\n class NonLayerTrackable(tf.Module):\n \n   def __init__(self):\n-    super(NonLayerTrackable, self).__init__()\n+    super().__init__()\n     self.a_variable = trackable_utils.add_variable(\n         self, name=\"a_variable\", shape=[])\n \n@@ -40,7 +40,7 @@ class MyModel(training.Model):\n   \"\"\"A concrete Model for testing.\"\"\"\n \n   def __init__(self):\n-    super(MyModel, self).__init__()\n+    super().__init__()\n     self._named_dense = core.Dense(1, use_bias=True)\n     self._second = core.Dense(1, use_bias=False)\n     # We can still track Trackables which aren't Layers.\n@@ -449,7 +449,7 @@ class CheckpointingTests(test_combinations.TestCase):\n     class Model(training.Model):\n \n       def __init__(self):\n-        super(Model, self).__init__()\n+        super().__init__()\n         self.w = tf.Variable(0.0)\n         self.b = tf.Variable(0.0)\n         self.vars = [self.w, self.b]\n\n@@ -25,7 +25,7 @@ from tensorflow.python.training.tracking import util as trackable_utils\n class NonLayerTrackable(tf.Module):\n \n   def __init__(self):\n-    super(NonLayerTrackable, self).__init__()\n+    super().__init__()\n     self.a_variable = trackable_utils.add_variable(\n         self, name=\"a_variable\", shape=[])\n \n@@ -34,7 +34,7 @@ class Subclassed(training.Model):\n   \"\"\"A concrete Model for testing.\"\"\"\n \n   def __init__(self):\n-    super(Subclassed, self).__init__()\n+    super().__init__()\n     self._named_dense = core.Dense(1, use_bias=True)\n     self._second = core.Dense(1, use_bias=False)\n     # We can still track Trackables which aren't Layers.\n\n@@ -37,7 +37,7 @@ class ToDense(Layer):\n   \"\"\"Create a dense (standard) tensor from the given input tensor.\"\"\"\n \n   def __init__(self, default_value, **kwargs):\n-    super(ToDense, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self._default_value = default_value\n \n   def call(self, inputs):\n@@ -63,7 +63,7 @@ class ToRagged(Layer):\n   \"\"\"Create a ragged tensor based on a given dense tensor.\"\"\"\n \n   def __init__(self, padding, ragged_rank=1, **kwargs):\n-    super(ToRagged, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     self._padding = padding\n     self._ragged_rank = ragged_rank\n \n@@ -86,7 +86,7 @@ class _SubclassModel(keras.Model):\n   \"\"\"A Keras subclass model.\"\"\"\n \n   def __init__(self, layers, i_layer=None):\n-    super(_SubclassModel, self).__init__()\n+    super().__init__()\n     # Note that clone and build doesn't support lists of layers in subclassed\n     # models. Adding each layer directly here.\n     for i, layer in enumerate(layers):\n\n@@ -719,7 +719,7 @@ class OrderedEnqueuer(SequenceEnqueuer):\n   \"\"\"\n \n   def __init__(self, sequence, use_multiprocessing=False, shuffle=False):\n-    super(OrderedEnqueuer, self).__init__(sequence, use_multiprocessing)\n+    super().__init__(sequence, use_multiprocessing)\n     self.shuffle = shuffle\n \n   def _get_executor_init(self, workers):\n@@ -858,7 +858,7 @@ class GeneratorEnqueuer(SequenceEnqueuer):\n   def __init__(self, generator,\n                use_multiprocessing=False,\n                random_seed=None):\n-    super(GeneratorEnqueuer, self).__init__(generator, use_multiprocessing)\n+    super().__init__(generator, use_multiprocessing)\n     self.random_seed = random_seed\n \n   def _get_executor_init(self, workers):\n\n@@ -242,7 +242,7 @@ class SharedObjectConfig(dict):\n   def __init__(self, base_config, object_id, **kwargs):\n     self.ref_count = 1\n     self.object_id = object_id\n-    super(SharedObjectConfig, self).__init__(base_config, **kwargs)\n+    super().__init__(base_config, **kwargs)\n \n   def increment_ref_count(self):\n     # As soon as we've seen the object more than once, we want to attach the\n@@ -1221,7 +1221,7 @@ class LazyLoader(python_types.ModuleType):\n   def __init__(self, local_name, parent_module_globals, name):\n     self._local_name = local_name\n     self._parent_module_globals = parent_module_globals\n-    super(LazyLoader, self).__init__(name)\n+    super().__init__(name)\n \n   def _load(self):\n     \"\"\"Load the module and insert it into the parent's globals.\"\"\"\n\n@@ -161,7 +161,7 @@ class LayerUtilsTest(tf.test.TestCase):\n     class Sequential(keras.Model):\n \n       def __init__(self, *args):\n-        super(Sequential, self).__init__()\n+        super().__init__()\n         self.module_list = list(args) if args else []\n \n       def call(self, x):\n@@ -172,7 +172,7 @@ class LayerUtilsTest(tf.test.TestCase):\n     class Block(keras.Model):\n \n       def __init__(self):\n-        super(Block, self).__init__()\n+        super().__init__()\n         self.module = Sequential(\n             keras.layers.Dense(10),\n             keras.layers.Dense(10),\n@@ -185,7 +185,7 @@ class LayerUtilsTest(tf.test.TestCase):\n     class Base(keras.Model):\n \n       def __init__(self):\n-        super(Base, self).__init__()\n+        super().__init__()\n         self.module = Sequential(Block(), Block())\n \n       def call(self, input_tensor):\n@@ -196,7 +196,7 @@ class LayerUtilsTest(tf.test.TestCase):\n     class Network(keras.Model):\n \n       def __init__(self):\n-        super(Network, self).__init__()\n+        super().__init__()\n         self.child = Base()\n \n       def call(self, inputs):\n@@ -376,14 +376,14 @@ class LayerUtilsTest(tf.test.TestCase):\n     class MyObject(tf.__internal__.tracking.AutoTrackable):\n \n       def __init__(self):\n-        super(MyObject, self).__init__()\n+        super().__init__()\n         self._frozen = True\n \n       def __setattr__(self, key, value):\n         \"\"\"Enforce that cache does not set attribute on MyObject.\"\"\"\n         if getattr(self, '_frozen', False):\n           raise ValueError('Cannot mutate when frozen.')\n-        return super(MyObject, self).__setattr__(key, value)\n+        return super().__setattr__(key, value)\n \n       @property\n       @layer_utils.cached_per_instance\n\n@@ -74,7 +74,7 @@ class _WeakObjectIdentityWrapper(_ObjectIdentityWrapper):\n   __slots__ = ()\n \n   def __init__(self, wrapped):\n-    super(_WeakObjectIdentityWrapper, self).__init__(weakref.ref(wrapped))\n+    super().__init__(weakref.ref(wrapped))\n \n   @property\n   def unwrapped(self):\n\n@@ -111,17 +111,17 @@ class TestIsSymbolicTensor(tf.test.TestCase, parameterized.TestCase):\n           d.shape = x.shape\n           d.get_shape = x.get_shape\n           return d, x\n-        super(PlumbingLayer, self).__init__(_fn, **kwargs)\n+        super().__init__(_fn, **kwargs)\n         self._enter_dunder_call = False\n \n       def __call__(self, inputs, *args, **kwargs):\n         self._enter_dunder_call = True\n-        d, _ = super(PlumbingLayer, self).__call__(inputs, *args, **kwargs)\n+        d, _ = super().__call__(inputs, *args, **kwargs)\n         self._enter_dunder_call = False\n         return d\n \n       def call(self, inputs, *args, **kwargs):\n-        d, v = super(PlumbingLayer, self).call(inputs, *args, **kwargs)\n+        d, v = super().call(inputs, *args, **kwargs)\n         if self._enter_dunder_call:\n           return d, v\n         return d\n\n@@ -233,7 +233,7 @@ class KerasClassifier(BaseWrapper):\n     else:\n       raise ValueError('Invalid shape for y: ' + str(y.shape))\n     self.n_classes_ = len(self.classes_)\n-    return super(KerasClassifier, self).fit(x, y, **kwargs)\n+    return super().fit(x, y, **kwargs)\n \n   def predict(self, x, **kwargs):\n     \"\"\"Returns the class predictions for the given test data.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
