{"custom_id": "keras#bd8c905537f34f8df35a55f9108ebff24cb23d6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 6445 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1899,7 +1899,9 @@ class KerasCallbacksTest(test_combinations.TestCase):\n             )\n             # Test 'patience' argument functions correctly when used\n             # in conjunction with 'start_from_epoch'.\n-            assert len(history.epoch) >= patience + start_from_epoch\n+            self.assertGreaterEqual(\n+                len(history.epoch), patience + start_from_epoch\n+            )\n \n             start_from_epoch = 2\n             patience = 0\n@@ -1912,7 +1914,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n                 data, labels, callbacks=[stopper], verbose=0, epochs=20\n             )\n             # Test for boundary condition when 'patience' = 0.\n-            assert len(history.epoch) >= start_from_epoch\n+            self.assertGreaterEqual(len(history.epoch), start_from_epoch)\n \n     def test_RemoteMonitor(self):\n         if requests is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46cd7586872d9c3cdd65a55b3226d2109e6254ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 55 | Files Changed: 27 | Hunks: 37 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 121 | Churn Cumulative: 6016 | Contributors (this commit): 12 | Commits (past 90d): 101 | Contributors (cumulative): 139 | DMM Complexity: None\n\nDIFF:\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import adadelta\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Adadelta\")\n class Adadelta(adadelta.Adadelta):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import adagrad\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Adagrad\")\n class Adagrad(adagrad.Adagrad):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import adam\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Adam\")\n class Adam(adam.Adam):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import adamax\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Adamax\")\n class Adamax(adamax.Adamax):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import ftrl\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Ftrl\")\n class Ftrl(ftrl.Ftrl):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import nadam\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Nadam\")\n class Nadam(nadam.Nadam):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import optimizer_v2\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.Optimizer\")\n class Optimizer(optimizer_v2.OptimizerV2):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import rmsprop\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.RMSprop\")\n class RMSprop(rmsprop.RMSprop):\n     pass\n\n@@ -16,10 +16,6 @@\n \n from keras.optimizers.optimizer_v2 import gradient_descent\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n \n-\n-@keras_export(\"keras.optimizers.legacy.SGD\")\n class SGD(gradient_descent.SGD):\n     pass\n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Adadelta\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.Adadelta\", \"keras.optimizers.Adadelta\", v1=[]\n+)\n class Adadelta(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n\n@@ -25,7 +25,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Adagrad\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.Adagrad\", \"keras.optimizers.Adagrad\", v1=[]\n+)\n class Adagrad(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adagrad algorithm.\n \n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Adam\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.Adam\", \"keras.optimizers.experimental.Adam\", v1=[]\n+)\n class Adam(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Adam algorithm.\n \n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Adamax\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.Adamax\", \"keras.optimizers.Adamax\", v1=[]\n+)\n class Adamax(optimizer.Optimizer):\n     \"\"\"Optimizer that implements the Adamax algorithm.\n \n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Ftrl\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.Ftrl\", \"keras.optimizers.Ftrl\", v1=[]\n+)\n class Ftrl(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n \n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.Nadam\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.Nadam\", \"keras.optimizers.Nadam\", v1=[]\n+)\n class Nadam(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the Nadam algorithm.\n \n\n@@ -859,7 +859,11 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n       **kwargs: keyword arguments only used for backward compatibility.\"\"\"\n \n \n-@keras_export(\"keras.optimizers.experimental.Optimizer\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.Optimizer\",\n+    \"keras.optimizers.experimental.Optimizer\",\n+    v1=[],\n+)\n class Optimizer(_BaseOptimizer):\n     \"\"\"Abstract optimizer base class.\n \n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.RMSprop\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.RMSprop\", \"keras.optimizers.RMSprop\", v1=[]\n+)\n class RMSprop(optimizer.Optimizer):\n     r\"\"\"Optimizer that implements the RMSprop algorithm.\n \n\n@@ -24,7 +24,9 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @register_keras_serializable()\n-@keras_export(\"keras.optimizers.experimental.SGD\", v1=[])\n+@keras_export(\n+    \"keras.optimizers.experimental.SGD\", \"keras.optimizers.SGD\", v1=[]\n+)\n class SGD(optimizer.Optimizer):\n     r\"\"\"Gradient descent (with momentum) optimizer.\n \n\n@@ -24,7 +24,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.Adadelta\")\n+@keras_export(\n+    \"keras.optimizers.legacy.Adadelta\",\n+    v1=[\"keras.optimizers.Adadelta\", \"keras.optimizers.legacy.Adadelta\"],\n+)\n class Adadelta(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n\n@@ -24,7 +24,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.Adagrad\")\n+@keras_export(\n+    \"keras.optimizers.legacy.Adagrad\",\n+    v1=[\"keras.optimizers.Adagrad\", \"keras.optimizers.legacy.Adagrad\"],\n+)\n class Adagrad(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adagrad algorithm.\n \n\n@@ -23,7 +23,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.Adam\")\n+@keras_export(\n+    \"keras.optimizers.legacy.Adam\",\n+    v1=[\"keras.optimizers.Adam\", \"keras.optimizers.legacy.Adam\"],\n+)\n class Adam(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the Adam algorithm.\n \n@@ -334,7 +337,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n \n     Usage:\n \n-    >>> opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n+    >>> opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)\n     >>> var1 = tf.Variable(10.0)\n     >>> loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n     >>> step_count = opt.minimize(loss, [var1]).numpy()\n\n@@ -23,7 +23,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.Adamax\")\n+@keras_export(\n+    \"keras.optimizers.legacy.Adamax\",\n+    v1=[\"keras.optimizers.Adamax\", \"keras.optimizers.legacy.Adamax\"],\n+)\n class Adamax(optimizer_v2.OptimizerV2):\n     \"\"\"Optimizer that implements the Adamax algorithm.\n \n\n@@ -23,7 +23,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.Ftrl\")\n+@keras_export(\n+    \"keras.optimizers.legacy.Ftrl\",\n+    v1=[\"keras.optimizers.Ftrl\", \"keras.optimizers.legacy.Ftrl\"],\n+)\n class Ftrl(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the FTRL algorithm.\n \n\n@@ -23,7 +23,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.SGD\")\n+@keras_export(\n+    \"keras.optimizers.legacy.SGD\",\n+    v1=[\"keras.optimizers.SGD\", \"keras.optimizers.legacy.SGD\"],\n+)\n class SGD(optimizer_v2.OptimizerV2):\n     r\"\"\"Gradient descent (with momentum) optimizer.\n \n\n@@ -24,7 +24,10 @@ from keras.optimizers.schedules import learning_rate_schedule\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.Nadam\")\n+@keras_export(\n+    \"keras.optimizers.legacy.Nadam\",\n+    v1=[\"keras.optimizers.Nadam\", \"keras.optimizers.legacy.Nadam\"],\n+)\n class Nadam(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the NAdam algorithm.\n     Much like Adam is essentially RMSprop with momentum, Nadam is Adam with\n\n@@ -100,7 +100,10 @@ def name_scope_only_in_function_or_graph(name):\n         return NullContextmanager()\n \n \n-@keras_export(\"keras.optimizers.Optimizer\", metaclass=abc.ABCMeta)\n+@keras_export(\n+    \"keras.optimizers.legacy.Optimizer\",\n+    v1=[\"keras.optimizers.Optimizer\", \"keras.optimizers.legacy.Optimizer\"],\n+)\n class OptimizerV2(tf.__internal__.tracking.Trackable):\n     \"\"\"Base class for Keras optimizers.\n \n\n@@ -24,7 +24,10 @@ from keras.optimizers.optimizer_v2 import optimizer_v2\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.optimizers.RMSprop\")\n+@keras_export(\n+    \"keras.optimizers.legacy.RMSprop\",\n+    v1=[\"keras.optimizers.RMSprop\", \"keras.optimizers.legacy.RMSprop\"],\n+)\n class RMSprop(optimizer_v2.OptimizerV2):\n     r\"\"\"Optimizer that implements the RMSprop algorithm.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d469b0f217fcc5bd03881cbac77d041a9243014", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 28 | Churn Cumulative: 681 | Contributors (this commit): 11 | Commits (past 90d): 21 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -77,6 +77,7 @@ from keras.optimizers.optimizer_v2.ftrl import Ftrl\n from keras.optimizers.optimizer_v2.gradient_descent import SGD\n from keras.optimizers.optimizer_v2.nadam import Nadam\n from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n+from keras.optimizers.schedules import learning_rate_schedule\n from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import serialize_keras_object\n \n@@ -221,6 +222,12 @@ def convert_to_legacy_optimizer(optimizer):\n     ]\n     for key in keys_to_remove:\n         config.pop(key, None)\n+    # Learning rate can be a custom LearningRateSchedule, which is stored as\n+    # a dict in config, and cannot be deserialized.\n+    if isinstance(\n+        optimizer._learning_rate, learning_rate_schedule.LearningRateSchedule\n+    ):\n+        config[\"learning_rate\"] = optimizer._learning_rate\n     legacy_optimizer_config = {\n         \"class_name\": optimizer_name,\n         \"config\": config,\n\n@@ -350,6 +350,27 @@ class KerasOptimizersTest(test_combinations.TestCase):\n             legacy_optimizer.get_config()[\"learning_rate\"],\n         )\n \n+        class CustomLRSchedule(learning_rate_schedule.LearningRateSchedule):\n+            def __init__(self, initial_learning_rate):\n+                self.initial_learning_rate = initial_learning_rate\n+\n+            def __call__(self, step):\n+                step = tf.cast(step, tf.float32)\n+                return self.initial_learning_rate / (step + 1)\n+\n+            def get_config(self):\n+                return {\"initial_learning_rate\": self.initial_learning_rate}\n+\n+        lr_schedule = CustomLRSchedule(0.001)\n+        optimizer = adam_experimental.Adam(learning_rate=lr_schedule)\n+        legacy_optimizer = keras.optimizers.convert_to_legacy_optimizer(\n+            optimizer\n+        )\n+        self.assertDictEqual(\n+            optimizer.get_config()[\"learning_rate\"],\n+            legacy_optimizer.get_config()[\"learning_rate\"],\n+        )\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ca8f0dd81506c4dbc75da0e36917a36ee7c351e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 2 | Churn Cumulative: 9358 | Contributors (this commit): 112 | Commits (past 90d): 14 | Contributors (cumulative): 112 | DMM Complexity: None\n\nDIFF:\n@@ -1947,7 +1947,7 @@ class EarlyStopping(Callback):\n           improves on `baseline`, training will run for `patience`\n           epochs and restore weights from the best epoch in that set.\n       start_from_epoch: Number of epochs to wait before starting\n-          to monitor improvement. This allows a warm-up period in which\n+          to monitor improvement. This allows for a warm-up period in which\n           no improvement is expected and thus training will not be stopped.\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67c428f7e0431b1a8197dcf939936653d0a3a059", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 4 | Churn Cumulative: 15164 | Contributors (this commit): 126 | Commits (past 90d): 25 | Contributors (cumulative): 142 | DMM Complexity: None\n\nDIFF:\n@@ -1776,7 +1776,7 @@ class BackupAndRestore(Callback):\n     >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),\n     ...                     epochs=10, batch_size=1, callbacks=[callback],\n     ...                     verbose=0)\n-    >>> # Only 6 more epochs are run, since first trainning got interrupted at\n+    >>> # Only 6 more epochs are run, since first training got interrupted at\n     >>> # zero-indexed epoch 4, second training will continue from 4 to 9.\n     >>> len(history.history['loss'])\n     6\n\n@@ -1503,7 +1503,7 @@ def _ragged_tensor_apply_loss(loss_fn, y_true, y_pred, y_pred_extra_dim=False):\n     \"\"\"\n \n     def rt_is_equiv_dense(rt):\n-        \"\"\"Returns true if this RaggedTensor has the same row_lenghts across\n+        \"\"\"Returns true if this RaggedTensor has the same row_lengths across\n \n            all ragged dimensions and thus can be converted to a dense tensor\n            without loss of information.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d0684c736567b2471c8ee95bac8b75d9b902cee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 115 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -115,6 +115,13 @@ class Adagrad(optimizer.Optimizer):\n             accumulator.scatter_add(\n                 tf.IndexedSlices(grad.values * grad.values, grad.indices)\n             )\n+            denominator = tf.sqrt(accumulator + self.epsilon)\n+            sparse_denominator = tf.gather(denominator, indices=grad.indices)\n+            variable.scatter_add(\n+                tf.IndexedSlices(\n+                    -lr * grad.values / sparse_denominator, grad.indices\n+                )\n+            )\n         else:\n             # Dense gradients.\n             accumulator.assign_add(grad * grad)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bef4ea30440af7af41b0319f245c11d8c40c1aaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 41 | Churn Cumulative: 624 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -149,16 +149,44 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n         tf.__internal__.test.combinations.combine(\n             mode=[\"eager\"],\n             use_adapt=[True, False],\n-            # TODO(b/1949359300): `load_under_strategy=True` flakily times out.\n-            load_under_strategy=[False],\n+            test_training_with_loaded=[True, False],\n+            # TODO(b/1949359300): `load_for_serving_under_strategy=True` flakily\n+            # times out.\n+            load_for_serving_under_strategy=[False],\n         )\n     )\n-    def testTrainAndServe(self, use_adapt, load_under_strategy):\n+    def testTrainAndLoadAndServe(\n+        self,\n+        use_adapt,\n+        test_training_with_loaded,\n+        load_for_serving_under_strategy,\n+    ):\n+\n+        # test_training_with_loaded=False tests distributed training with newly\n+        # constructed KPL, while test_training_with_loaded=True tests\n+        # distributed training with a loaded KPL which was created under\n+        # strategy scope as well.\n+        #\n+        # load_for_serving_under_strategy test serving with a model loaded\n+        # under distribution strategy or not.\n \n         with self.coordinator.strategy.scope():\n \n             feature_ps, label_ps = self.define_kpls_for_training(use_adapt)\n \n+            if test_training_with_loaded:\n+                saved_kpl_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n+                feature_ps_dir = os.path.join(saved_kpl_dir, \"feature\")\n+                label_ps_dir = os.path.join(saved_kpl_dir, \"label\")\n+\n+                feature_ps.save(feature_ps_dir)\n+                label_ps.save(label_ps_dir)\n+\n+                del feature_ps, label_ps\n+\n+                feature_ps = tf.keras.models.load_model(feature_ps_dir)\n+                label_ps = tf.keras.models.load_model(label_ps_dir)\n+\n             def dataset_fn():\n                 def feature_and_label_gen():\n                     while True:\n@@ -266,7 +294,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n         saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n         model.save(saved_model_dir, signatures={\"serving_default\": serving_fn})\n \n-        if load_under_strategy:\n+        if load_for_serving_under_strategy:\n             with self.coordinator.strategy.scope():\n \n                 loaded_serving_fn = tf.keras.models.load_model(\n\n@@ -17,6 +17,8 @@\n import contextlib\n import threading\n \n+import tensorflow.compat.v2 as tf\n+\n \n class LoadContext(threading.local):\n     \"\"\"A context for loading a model.\"\"\"\n@@ -61,3 +63,6 @@ def get_load_options():\n def in_load_context():\n     \"\"\"Returns whether under a load context.\"\"\"\n     return _load_context.in_load_context()\n+\n+\n+tf.__internal__.register_load_context_function(in_load_context)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2da87321552ee8305f4d438f799c5e46a4e7735e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2281 | Contributors (this commit): 12 | Commits (past 90d): 43 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -12,10 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Base class of optimizer.\n-\n-This is under development, and subject to interface/implementation changes.\n-\"\"\"\n+\"\"\"Base class of optimizer.\"\"\"\n \n import abc\n import re\n@@ -104,6 +101,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         self._variables.append(self._iterations)\n \n     def _process_kwargs(self, kwargs):\n+        # Remove the `is_legacy_optimizer` arg, which is for serialization only.\n         kwargs.pop(\"is_legacy_optimizer\", None)\n         legacy_kwargs = {\n             \"lr\",\n\n@@ -105,12 +105,18 @@ def name_scope_only_in_function_or_graph(name):\n     v1=[\"keras.optimizers.Optimizer\", \"keras.optimizers.legacy.Optimizer\"],\n )\n class OptimizerV2(tf.__internal__.tracking.Trackable):\n-    \"\"\"Base class for Keras optimizers.\n+    \"\"\"Base class for legacy Keras optimizers.\n \n     You should not use this class directly, but instead instantiate one of its\n     subclasses such as `tf.keras.optimizers.legacy.SGD`,\n     `tf.keras.optimizers.legacy.Adam`, etc.\n \n+    This is the default Keras optimizer base class until v2.10 (included).\n+    In v2.11 and later, `tf.keras.optimizers.Optimizer`\n+    points to a new base class implementation. The legacy class won't be\n+    deleted in the future and will continue to be available at\n+    `tf.keras.optimizers.legacy.Optimizer`.\n+\n     ### Usage\n \n     ```python\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2a79077b38452c181dd559a96b6d15c67315f621", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 21115 | Contributors (this commit): 137 | Commits (past 90d): 41 | Contributors (cumulative): 145 | DMM Complexity: 1.0\n\nDIFF:\n@@ -300,11 +300,12 @@ class KerasMultiWorkerTestIndependentWorker(\n         EPOCHS = 1\n         STEPS = 2\n \n-        # Dataset's targets are [0, 1, 2, 3]:\n+        # Dataset's targets are [0, 1, 2, 3, 4, 5, 6, 7]:\n         train_ds, _ = multi_worker_testing_utils.mnist_synthetic_dataset(\n             BATCH, STEPS, target_values=\"increasing\"\n         )\n \n+        # A model that has loss=sum(targets) / BATCH:\n         class MyModel(keras.Model):\n             def train_step(self, data):\n                 _, y = data\n@@ -344,7 +345,6 @@ class KerasMultiWorkerTestIndependentWorker(\n         #   loss_e0_s1_r0 = [4+5]/BATCH =  9/4\n         #   loss_e0_s2_r1 = [6+7]/BATCH = 13/4\n         #   loss_e0_s1    = 9/4 + 13/4   = 5.5\n-        #\n         #   loss_e0       = last([1.5, 5.5])\n         history = model.fit(train_ds, epochs=EPOCHS, steps_per_epoch=STEPS)\n         self.assertAllClose([5.5], history.history[\"loss\"])\n\n@@ -3879,13 +3879,10 @@ def reduce_per_replica(values, strategy, reduction):\n \n     def _reduce(v):\n         \"\"\"Reduce a single `PerReplica` object.\"\"\"\n-        if reduction in (\n-            \"concat\",\n-            \"sum\",\n-        ) and _collective_all_reduce_multi_worker(strategy):\n+        if _collective_all_reduce_multi_worker(strategy):\n             if reduction == \"concat\":\n                 return _multi_worker_concat(v, strategy)\n-            else:\n+            elif reduction == \"sum\":\n                 return strategy.reduce(\"SUM\", v, axis=None)\n \n         if not _is_per_replica_instance(v):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9feb902638167be811c3d7daa51eb67b6abd781d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 11 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 6467 | Contributors (this commit): 23 | Commits (past 90d): 8 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -3340,14 +3340,14 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n         model.compile(opt, \"mse\", run_eagerly=test_utils.should_run_eagerly())\n         return model\n \n-    def _count_trace_file(self, logdir):\n+    def _count_xplane_file(self, logdir):\n         profile_dir = os.path.join(logdir, \"plugins\", \"profile\")\n         count = 0\n         for dirpath, dirnames, filenames in os.walk(profile_dir):\n             del dirpath  # unused\n             del dirnames  # unused\n             for filename in filenames:\n-                if filename.endswith(\".trace.json.gz\"):\n+                if filename.endswith(\".xplane.pb\"):\n                     count += 1\n         return count\n \n@@ -3436,7 +3436,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n                 _ObservedSummary(logdir=self.train_dir, tag=\"batch_1\"),\n             },\n         )\n-        self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n+        self.assertEqual(1, self._count_xplane_file(logdir=self.logdir))\n \n     def test_TensorBoard_autoTrace_outerProfiler(self):\n         \"\"\"Runs a profiler session that interferes with the one from the callback.\n@@ -3468,7 +3468,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n                 _ObservedSummary(logdir=self.train_dir, tag=\"batch_1\"),\n             },\n         )\n-        self.assertEqual(0, self._count_trace_file(logdir=self.train_dir))\n+        self.assertEqual(0, self._count_xplane_file(logdir=self.train_dir))\n \n     def test_TensorBoard_autoTrace_tagNameWithBatchNum(self):\n         model = self._get_seq_model()\n@@ -3493,7 +3493,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n                 _ObservedSummary(logdir=self.train_dir, tag=\"batch_2\"),\n             },\n         )\n-        self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n+        self.assertEqual(1, self._count_xplane_file(logdir=self.logdir))\n \n     def test_TensorBoard_autoTrace_profileBatchRangeSingle(self):\n         model = self._get_seq_model()\n@@ -3522,7 +3522,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n                 _ObservedSummary(logdir=self.train_dir, tag=\"batch_2\"),\n             },\n         )\n-        self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n+        self.assertEqual(1, self._count_xplane_file(logdir=self.logdir))\n \n     def test_TensorBoard_autoTrace_profileBatchRangeTwice(self):\n         model = self._get_seq_model()\n@@ -3553,7 +3553,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n             validation_data=(x, y),\n             callbacks=[tb_cbk],\n         )\n-        self.assertEqual(2, self._count_trace_file(logdir=self.logdir))\n+        self.assertEqual(2, self._count_xplane_file(logdir=self.logdir))\n \n     # Test case that replicates a GitHub issue.\n     # https://github.com/tensorflow/tensorflow/issues/37543\n@@ -3573,7 +3573,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n             callbacks=[keras.callbacks.TensorBoard(logdir, profile_batch=1)],\n         )\n         # Verifies trace exists in the first logdir.\n-        self.assertEqual(1, self._count_trace_file(logdir=logdir))\n+        self.assertEqual(1, self._count_xplane_file(logdir=logdir))\n         logdir = os.path.join(self.get_temp_dir(), \"tb2\")\n         model.fit(\n             np.zeros((64, 1)),\n@@ -3582,7 +3582,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n             callbacks=[keras.callbacks.TensorBoard(logdir, profile_batch=2)],\n         )\n         # Verifies trace exists in the second logdir.\n-        self.assertEqual(1, self._count_trace_file(logdir=logdir))\n+        self.assertEqual(1, self._count_xplane_file(logdir=logdir))\n \n     def test_TensorBoard_autoTrace_profileBatchRange(self):\n         model = self._get_seq_model()\n@@ -3611,7 +3611,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n                 _ObservedSummary(logdir=self.train_dir, tag=\"batch_3\"),\n             },\n         )\n-        self.assertEqual(1, self._count_trace_file(logdir=self.logdir))\n+        self.assertEqual(1, self._count_xplane_file(logdir=self.logdir))\n \n     def test_TensorBoard_autoTrace_profileInvalidBatchRange(self):\n         with self.assertRaises(ValueError):\n@@ -3668,7 +3668,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n \n         # Enabled trace only on the 10000th batch, thus it should be empty.\n         self.assertEmpty(summary_file.tensors)\n-        self.assertEqual(0, self._count_trace_file(logdir=self.train_dir))\n+        self.assertEqual(0, self._count_xplane_file(logdir=self.train_dir))\n \n \n class MostRecentlyModifiedFileMatchingPatternTest(tf.test.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#376520287278895de9b1eb73bf888254f564ab7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 163 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -15,8 +15,6 @@\n \"\"\"E2E Tests for mnist_model.\"\"\"\n \n import tensorflow.compat.v2 as tf\n-from tensorflow.dtensor.python import mesh_util\n-from tensorflow.dtensor.python import tpu_util\n \n from keras import backend\n from keras.dtensor import dtensor_api as dtensor\n@@ -37,7 +35,7 @@ class MnistTest(test_util.DTensorBaseTest):\n             * 8,\n         )\n \n-        mesh = mesh_util.create_mesh(\n+        mesh = dtensor.create_mesh(\n             devices=[\"CPU:%d\" % i for i in range(8)], mesh_dims=[(\"batch\", 8)]\n         )\n \n@@ -66,10 +64,10 @@ class MnistTest(test_util.DTensorBaseTest):\n     def DISABLED_test_mnist_training_tpu(self):\n         # TODO(scottzhu): Enable TPU test once the dtensor_test rule is migrated\n         # out of learning/brain\n-        tpu_util.dtensor_initialize_tpu_system()\n+        dtensor.initialize_accelerator_system()\n         total_tpu_device_count = dtensor.num_global_devices(\"TPU\")\n         mesh_shape = [total_tpu_device_count]\n-        mesh = tpu_util.create_tpu_mesh([\"batch\"], mesh_shape, \"tpu_mesh\")\n+        mesh = dtensor.create_tpu_mesh([\"batch\"], mesh_shape, \"tpu_mesh\")\n \n         # Needed by keras initializers.\n         tf_utils.set_random_seed(1337)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d989ea3dee8cbee0fd8b7abe34e66ee5faf221ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 21/21 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -207,7 +207,9 @@ class SidecarEvaluator:\n         if self.model.optimizer and isinstance(\n             self.model.optimizer, optimizer_experimental.Optimizer\n         ):\n-            checkpoint = tf.train.Checkpoint(model=self.model)\n+            checkpoint = tf.train.Checkpoint(\n+                model=self.model, optimizer=self.model.optimizer\n+            )\n         else:\n             optimizer_checkpoint = tf.train.Checkpoint(iter=self._iterations)\n             checkpoint = tf.train.Checkpoint(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ee8945025c3c6611413385126abe637b618fa83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 278 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 20/11 | Churn Δ: 281 | Churn Cumulative: 4643 | Contributors (this commit): 43 | Commits (past 90d): 11 | Contributors (cumulative): 54 | DMM Complexity: 0.8770949720670391\n\nDIFF:\n@@ -23,6 +23,7 @@ import weakref\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n+from keras import initializers\n from keras.utils import io_utils\n \n # isort: off\n@@ -809,3 +810,141 @@ class CallFunctionSpec:\n                 \"The first argument to `Layer.call` must always be passed.\"\n             )\n         return inputs, args, kwargs\n+\n+\n+@keras_export(\"keras.utils.warmstart_embedding_matrix\")\n+def warmstart_embedding_matrix(\n+    base_vocabulary,\n+    new_vocabulary,\n+    base_embeddings,\n+    new_embeddings_initializer=\"uniform\",\n+):\n+    \"\"\"Warm start embedding matrix with changing vocab.\n+\n+    This util can be used to warmstart the embedding layer matrix when\n+    vocabulary changes between previously saved checkpoint and model.\n+    Vocabulary change could mean, the size of the new vocab is different or the\n+    vocabulary is reshuffled or new vocabulary has been added to old vocabulary.\n+    If the vocabulary size changes, size of the embedding layer matrix also\n+    changes. This util remaps the old vocabulary embeddings to the new embedding\n+    layer matrix.\n+\n+    Example:\n+    Here is an example that demonstrates how to use the\n+    `warmstart_embedding_matrix` util.\n+    >>> import keras\n+    >>> vocab_base = tf.convert_to_tensor([\"unk\", \"a\", \"b\", \"c\"])\n+    >>> vocab_new = tf.convert_to_tensor(\n+    ...        [\"unk\", \"unk\", \"a\", \"b\", \"c\", \"d\", \"e\"])\n+    >>> vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)\n+    >>> vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)\n+    >>> warmstarted_embedding_matrix = warmstart_embedding_matrix(\n+    ...       base_vocabulary=vocab_base,\n+    ...       new_vocabulary=vocab_new,\n+    ...       base_embeddings=vectorized_vocab_base,\n+    ...       new_embeddings_initializer=keras.initializers.Constant(\n+    ...         vectorized_vocab_new))\n+\n+    Here is an example that demonstrates how to get vocabulary and embedding\n+    weights from layers, use the `warmstart_embedding_matrix` util to remap the\n+    layer embeddings and continue with model training.\n+    ```\n+    # get old and new vocabulary by using layer.get_vocabulary()\n+    # for example assume TextVectorization layer is used\n+    base_vocabulary = old_text_vectorization_layer.get_vocabulary()\n+    new_vocabulary = new_text_vectorization_layer.get_vocabulary()\n+    # get previous embedding layer weights\n+    embedding_weights_base = model.get_layer('embedding').get_weights()[0]\n+    warmstarted_embedding = keras.utils.warmstart_embedding_matrix(\n+                                  base_vocabulary,\n+                                  new_vocabulary,\n+                                  base_embeddings=embedding_weights_base,\n+                                  new_embeddings_initializer=\"uniform\")\n+    updated_embedding_variable = tf.Variable(warmstarted_embedding)\n+\n+    # update embedding layer weights\n+    model.layers[1].embeddings = updated_embedding_variable\n+    model.fit(..)\n+    # continue with model training\n+\n+    ```\n+\n+    Args:\n+        base_vocabulary: The list of vocabulary terms that\n+          the preexisting embedding matrix `base_embeddings` represents.\n+          It can be either a 1D array/tensor or a tuple/list of vocabulary\n+          terms (strings), or a path to a vocabulary text file. If passing a\n+           file path, the file should contain one line per term in the\n+           vocabulary.\n+        new_vocabulary: The list of vocabulary terms for the new vocabulary\n+           (same format as above).\n+        base_embeddings: NumPy array or tensor representing the preexisting\n+          embedding matrix.\n+        new_embeddings_initializer: Initializer for embedding vectors for\n+          previously unseen terms to be added to the new embedding matrix (see\n+          `keras.initializers`). Defaults to \"uniform\". new_embedding matrix\n+          needs to be specified with \"constant\" initializer.\n+          matrix. Default value is None.\n+\n+    Returns:\n+      tf.tensor of remapped embedding layer matrix\n+\n+    \"\"\"\n+    # convert vocab to list\n+    base_vocabulary = convert_vocab_to_list(base_vocabulary)\n+    new_vocabulary = convert_vocab_to_list(new_vocabulary)\n+\n+    # Initialize the new embedding layer matrix\n+    new_embeddings_initializer = initializers.get(new_embeddings_initializer)\n+    new_embedding = new_embeddings_initializer(\n+        shape=(len(new_vocabulary), base_embeddings.shape[1]),\n+        dtype=base_embeddings.dtype,\n+    )\n+\n+    # create mapping dict {vocab:index}\n+    base_vocabulary_dict = dict(\n+        zip(base_vocabulary, range(len(base_vocabulary)))\n+    )\n+\n+    indices_base_vocabulary = []\n+    indices_new_vocabulary = []\n+    for index, key in enumerate(new_vocabulary):\n+        if key in base_vocabulary_dict:\n+            indices_base_vocabulary.append(base_vocabulary_dict[key])\n+            indices_new_vocabulary.append(int(index))\n+\n+    # update embedding matrix\n+    values_to_update = tf.gather(base_embeddings, indices_base_vocabulary)\n+    warmstarted_embedding_matrix = tf.tensor_scatter_nd_update(\n+        new_embedding,\n+        tf.expand_dims(indices_new_vocabulary, axis=1),\n+        values_to_update,\n+    )\n+    return warmstarted_embedding_matrix\n+\n+\n+def convert_vocab_to_list(vocab):\n+    \"\"\"Convert input vacabulary to list.\"\"\"\n+    vocab_list = []\n+    if tf.is_tensor(vocab):\n+        vocab_list = list(vocab.numpy())\n+    elif isinstance(vocab, (np.ndarray, tuple, list)):\n+        vocab_list = list(vocab)\n+    elif isinstance(vocab, str):\n+        if not tf.io.gfile.exists(vocab):\n+            raise ValueError(f\"Vocabulary file {vocab} does not exist.\")\n+        with tf.io.gfile.GFile(vocab, \"r\") as vocabulary_file:\n+            vocab_list = vocabulary_file.read().splitlines()\n+    else:\n+        raise ValueError(\n+            \"Vocabulary is expected to be either a NumPy array, \"\n+            \"list, 1D tensor or a vocabulary text file. Instead type \"\n+            f\"{type(vocab)} was received.\"\n+        )\n+    if len(vocab_list) == 0:\n+        raise ValueError(\n+            \"Vocabulary is expected to be either a NumPy array, \"\n+            \"list, 1D tensor or a vocabulary text file with at least one token.\"\n+            \" Received 0 instead.\"\n+        )\n+    return vocab_list\n\n@@ -21,6 +21,7 @@ import os\n import pickle\n import shutil\n import sys\n+import tempfile\n import time\n import timeit\n \n@@ -28,6 +29,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n import keras\n+from keras.testing_infra import test_utils\n from keras.utils import io_utils\n from keras.utils import layer_utils\n \n@@ -593,6 +595,142 @@ class LayerUtilsTest(tf.test.TestCase):\n         _ = size_check_instance.my_id\n         self.assertEqual(expected_size, len(pickle.dumps(size_check_instance)))\n \n+    def test_warmstart_embedding_matrix_with_list(self):\n+        vocab_base = [\"unk\", \"a\", \"b\", \"c\"]\n+        vocab_new = [\"unk\", \"unk\", \"a\", \"b\", \"c\", \"d\", \"e\"]\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        vectorized_vocab_new = np.random.rand(len(vocab_new), 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=keras.initializers.Constant(\n+                vectorized_vocab_new\n+            ),\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[2],\n+            vectorized_vocab_base[1],\n+        )\n+\n+    def test_warmstart_embedding_matrix_with_nparray(self):\n+        vocab_base = np.array([\"unk\", \"a\", \"b\", \"c\"])\n+        vocab_new = np.array([\"unk\", \"unk\", \"a\", \"b\", \"c\", \"d\", \"e\"])\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        vectorized_vocab_new = np.random.rand(len(vocab_new), 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=keras.initializers.Constant(\n+                vectorized_vocab_new\n+            ),\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[2],\n+            vectorized_vocab_base[1],\n+        )\n+\n+    @test_utils.run_v2_only\n+    def test_warmstart_embedding_matrix_with_tensor(self):\n+        vocab_base = tf.convert_to_tensor([\"unk\", \"a\", \"b\", \"c\"])\n+        vocab_new = tf.convert_to_tensor(\n+            [\"unk\", \"unk\", \"a\", \"b\", \"c\", \"d\", \"e\"]\n+        )\n+        vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)\n+        vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=keras.initializers.Constant(\n+                vectorized_vocab_new\n+            ),\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[2],\n+            vectorized_vocab_base[1],\n+        )\n+\n+    def test_warmstart_embedding_matrix_with_file_name(self):\n+        def _write_list_to_file(filename, content_list):\n+            with tf.io.gfile.GFile(filename, \"w\") as output_file:\n+                for line in content_list:\n+                    output_file.write(line + \"\\n\")\n+\n+        vocab_base = [\"UNK\", \"a\", \"b\", \"c\"]\n+        vocab_base_file = tempfile.mktemp(\".tsv\")\n+        _write_list_to_file(vocab_base_file, vocab_base)\n+        vocab_new = [\"UNK\", \"UNK\", \"a\", \"b\", \"c\", \"d\", \"e\"]\n+        vocab_new_file = tempfile.mktemp(\".tsv\")\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        vectorized_vocab_new = np.random.rand(len(vocab_new), 3)\n+        _write_list_to_file(vocab_new_file, vocab_new)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base_file,\n+            new_vocabulary=vocab_new_file,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=keras.initializers.Constant(\n+                vectorized_vocab_new\n+            ),\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[3],\n+            vectorized_vocab_base[2],\n+        )\n+\n+    def test_warmstart_default_initialization(self):\n+        def _write_list_to_file(filename, content_list):\n+            with tf.io.gfile.GFile(filename, \"w\") as output_file:\n+                for line in content_list:\n+                    output_file.write(line + \"\\n\")\n+\n+        vocab_base = [\"UNK\", \"a\", \"b\", \"c\"]\n+        vocab_base_file = tempfile.mktemp(\".tsv\")\n+        _write_list_to_file(vocab_base_file, vocab_base)\n+        vocab_new = [\"UNK\", \"UNK\", \"a\", \"b\", \"c\", \"d\", \"e\"]\n+        vocab_new_file = tempfile.mktemp(\".tsv\")\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        _write_list_to_file(vocab_new_file, vocab_new)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base_file,\n+            new_vocabulary=vocab_new_file,\n+            base_embeddings=vectorized_vocab_base,\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[3],\n+            vectorized_vocab_base[2],\n+        )\n+\n+    def test_warmstart_default_value(self):\n+        vocab_base = np.array([\"unk\", \"a\", \"b\", \"c\"])\n+        vocab_new = np.array([\"unk\", \"unk\", \"a\", \"b\", \"c\", \"d\", \"e\"])\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[2],\n+            vectorized_vocab_base[1],\n+        )\n+\n+    def test_warmstart_with_randomuniform_initializer(self):\n+        vocab_base = np.array([\"unk\", \"a\", \"b\", \"c\"])\n+        vocab_new = np.array([\"unk\", \"unk\", \"a\", \"b\", \"c\", \"d\", \"e\"])\n+        vectorized_vocab_base = np.random.rand(len(vocab_base), 3)\n+        warmstarted_embedding_matrix = layer_utils.warmstart_embedding_matrix(\n+            base_vocabulary=vocab_base,\n+            new_vocabulary=vocab_new,\n+            base_embeddings=vectorized_vocab_base,\n+            new_embeddings_initializer=\"RandomUniform\",\n+        )\n+        self.assertAllEqual(\n+            warmstarted_embedding_matrix[2],\n+            vectorized_vocab_base[1],\n+        )\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -207,9 +207,7 @@ class SidecarEvaluator:\n         if self.model.optimizer and isinstance(\n             self.model.optimizer, optimizer_experimental.Optimizer\n         ):\n-            checkpoint = tf.train.Checkpoint(\n-                model=self.model, optimizer=self.model.optimizer\n-            )\n+            checkpoint = tf.train.Checkpoint(model=self.model)\n         else:\n             optimizer_checkpoint = tf.train.Checkpoint(iter=self._iterations)\n             checkpoint = tf.train.Checkpoint(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
