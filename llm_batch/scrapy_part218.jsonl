{"custom_id": "scrapy#a087d2593a36dcacfeb9353d9ca1e6737478ddd9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 209 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n from OpenSSL import SSL\n from twisted.internet.ssl import ClientContextFactory\n \n+\n try:\n \n     from zope.interface.declarations import implementer\n@@ -11,6 +12,12 @@ try:\n     from twisted.web.client import BrowserLikePolicyForHTTPS\n     from twisted.web.iweb import IPolicyForHTTPS\n \n+\n+    class ScrapyClientTLSOptions(ClientTLSOptions):\n+        def _identityVerifyingInfoCallback(self, connection, where, ret):\n+            pass\n+\n+\n     @implementer(IPolicyForHTTPS)\n     class ScrapyClientContextFactory(BrowserLikePolicyForHTTPS):\n         \"\"\"\n@@ -49,7 +56,7 @@ try:\n             return self.getCertificateOptions().getContext()\n \n         def creatorForNetloc(self, hostname, port):\n-            return ClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())\n+            return ScrapyClientTLSOptions(hostname.decode(\"ascii\"), self.getContext())\n \n \n     @implementer(IPolicyForHTTPS)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25ee023561a59401ec932d2afae8ee62e7b5e7d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 48 | Churn Cumulative: 273 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,21 +1,19 @@\n from OpenSSL import SSL\n from twisted.internet.ssl import ClientContextFactory\n \n-\n try:\n \n     from zope.interface.declarations import implementer\n \n     # the following should be available from Twisted 14.0.0\n-    from twisted.internet.ssl import optionsForClientTLS, CertificateOptions, platformTrust\n-    from twisted.internet._sslverify import ClientTLSOptions\n+    from twisted.internet.ssl import (optionsForClientTLS,\n+                                      CertificateOptions,\n+                                      platformTrust)\n+\n     from twisted.web.client import BrowserLikePolicyForHTTPS\n     from twisted.web.iweb import IPolicyForHTTPS\n \n-\n-    class ScrapyClientTLSOptions(ClientTLSOptions):\n-        def _identityVerifyingInfoCallback(self, connection, where, ret):\n-            pass\n+    from scrapy.core.downloader.tls import ScrapyClientTLSOptions\n \n \n     @implementer(IPolicyForHTTPS)\n\n@@ -1,6 +1,9 @@\n+import logging\n from OpenSSL import SSL\n \n \n+logger = logging.getLogger(__name__)\n+\n METHOD_SSLv3 = 'SSLv3'\n METHOD_TLS = 'TLS'\n METHOD_TLSv10 = 'TLSv1.0'\n@@ -14,3 +17,36 @@ openssl_methods = {\n     METHOD_TLSv11: getattr(SSL, 'TLSv1_1_METHOD', 5),   # TLS 1.1 only\n     METHOD_TLSv12: getattr(SSL, 'TLSv1_2_METHOD', 6),   # TLS 1.2 only\n }\n+\n+# ClientTLSOptions requires a recent-enough version of Twisted\n+try:\n+\n+    # taken from twisted/twisted/internet/_sslverify.py\n+    try:\n+        from OpenSSL.SSL import SSL_CB_HANDSHAKE_DONE, SSL_CB_HANDSHAKE_START\n+    except ImportError:\n+        SSL_CB_HANDSHAKE_START = 0x10\n+        SSL_CB_HANDSHAKE_DONE = 0x20\n+\n+    from twisted.internet._sslverify import (ClientTLSOptions,\n+                                             _maybeSetHostNameIndication,\n+                                             verifyHostname,\n+                                             VerificationError)\n+\n+    class ScrapyClientTLSOptions(ClientTLSOptions):\n+        # same as Twisted's ClientTLSOptions,\n+        # except that VerificationError is caught\n+        # and doesn't close the connection\n+        def _identityVerifyingInfoCallback(self, connection, where, ret):\n+            if where & SSL_CB_HANDSHAKE_START:\n+                _maybeSetHostNameIndication(connection, self._hostnameBytes)\n+            elif where & SSL_CB_HANDSHAKE_DONE:\n+                try:\n+                    verifyHostname(connection, self._hostnameASCII)\n+                except VerificationError as e:\n+                    logger.warning(e)\n+\n+except ImportError:\n+    # ImportError should not matter for older Twisted versions\n+    # as the above is not used in the fallback ScrapyClientContextFactory\n+    pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6760dbaac0f1de1938abec7207ba9b52f12c8cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 1371 | Contributors (this commit): 13 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,7 +122,10 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         \"\"\"\n         self._protocol.dataReceived = self._protocolDataReceived\n         if  TunnelingTCP4ClientEndpoint._responseMatcher.match(bytes):\n-            self._protocol.transport.startTLS(self._contextFactory,\n+            # this set proper Server Name Indication extension\n+            sslOptions = self._contextFactory.creatorForNetloc(\n+                self._tunneledHost, self._tunneledPort)\n+            self._protocol.transport.startTLS(sslOptions,\n                                               self._protocolFactory)\n             self._tunnelReadyDeferred.callback(self._protocol)\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcea11a70c52e3fb03e185cda7065267c2e4b869", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 1378 | Contributors (this commit): 13 | Commits (past 90d): 13 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,9 +122,14 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         \"\"\"\n         self._protocol.dataReceived = self._protocolDataReceived\n         if  TunnelingTCP4ClientEndpoint._responseMatcher.match(bytes):\n-            # this set proper Server Name Indication extension\n+            try:\n+                # this sets proper Server Name Indication extension\n+                # but is only available for Twisted>=14.0\n                 sslOptions = self._contextFactory.creatorForNetloc(\n                     self._tunneledHost, self._tunneledPort)\n+            except AttributeError:\n+                # fall back to non-SNI SSL context factory\n+                sslOptions = self._contextFactory\n             self._protocol.transport.startTLS(sslOptions,\n                                               self._protocolFactory)\n             self._tunnelReadyDeferred.callback(self._protocol)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd979ace40f26e8d921aaa4cf2d603b434fa2064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 701 | Contributors (this commit): 9 | Commits (past 90d): 15 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -204,10 +204,10 @@ class MockServer():\n         time.sleep(0.2)\n \n \n-def ssl_context_factory():\n+def ssl_context_factory(keyfile='keys/cert.pem', certfile='keys/cert.pem'):\n     return ssl.DefaultOpenSSLContextFactory(\n-         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n-         os.path.join(os.path.dirname(__file__), 'keys/cert.pem'),\n+         os.path.join(os.path.dirname(__file__), keyfile),\n+         os.path.join(os.path.dirname(__file__), certfile),\n          )\n \n \n\n@@ -119,6 +119,10 @@ class HttpTestCase(unittest.TestCase):\n     scheme = 'http'\n     download_handler_cls = HTTPDownloadHandler\n \n+    # only used for HTTPS tests\n+    keyfile = 'keys/cert.pem'\n+    certfile = 'keys/cert.pem'\n+\n     def setUp(self):\n         name = self.mktemp()\n         os.mkdir(name)\n@@ -137,7 +141,8 @@ class HttpTestCase(unittest.TestCase):\n         self.host = 'localhost'\n         if self.scheme == 'https':\n             self.port = reactor.listenSSL(\n-                0, self.wrapper, ssl_context_factory(), interface=self.host)\n+                0, self.wrapper, ssl_context_factory(self.keyfile, self.certfile),\n+                interface=self.host)\n         else:\n             self.port = reactor.listenTCP(0, self.wrapper, interface=self.host)\n         self.portno = self.port.getHost().port\n@@ -318,6 +323,18 @@ class Https11TestCase(Http11TestCase):\n     scheme = 'https'\n \n \n+class Https11WrongHostnameTestCase(Http11TestCase):\n+    scheme = 'https'\n+\n+    # above tests use a server certificate for \"localhost\",\n+    # client connection to \"localhost\" too.\n+    # here we test that even if the server certificate is for another domain,\n+    # \"www.example.com\" in this case,\n+    # the tests still pass\n+    keyfile = 'keys/example-com.key.pem'\n+    certfile = 'keys/example-com.cert.pem'\n+\n+\n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n     if twisted_version < (11, 1, 0):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68dedf54cb27847f6d035099b61aa06226549fad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 215 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 23 | Methods Changed: 14 | Complexity Δ (Sum/Max): 22/16 | Churn Δ: 237 | Churn Cumulative: 1123 | Contributors (this commit): 16 | Commits (past 90d): 6 | Contributors (cumulative): 20 | DMM Complexity: 0.5\n\nDIFF:\n@@ -7,15 +7,18 @@ to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n import re\n+import six\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n-                                    unquote)\n+                                    quote, unquote)\n+if six.PY3:\n+    from urllib.parse import unquote_to_bytes\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n # move doesn't break old code\n from w3lib.url import *\n from w3lib.url import _safe_chars\n-from scrapy.utils.python import to_native_str\n+from scrapy.utils.python import to_bytes, to_native_str, to_unicode\n \n \n def url_is_from_any_domain(url, domains):\n@@ -37,41 +40,113 @@ def url_has_any_extension(url, extensions):\n     return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n \n \n+def _safe_ParseResult(parts, encoding='utf8', path_encoding='utf8'):\n+    return (\n+        to_native_str(parts.scheme),\n+        to_native_str(parts.netloc.encode('idna')),\n+\n+        # default encoding for path component SHOULD be UTF-8\n+        quote(to_bytes(parts.path, path_encoding), _safe_chars),\n+        quote(to_bytes(parts.params, path_encoding), _safe_chars),\n+\n+        # encoding of query and fragment follows page encoding\n+        # or form-charset (if known and passed)\n+        quote(to_bytes(parts.query, encoding), _safe_chars),\n+        quote(to_bytes(parts.fragment, encoding), _safe_chars)\n+    )\n+\n+\n def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n                      encoding=None):\n     \"\"\"Canonicalize the given url by applying the following procedures:\n \n     - sort query arguments, first by key, then by value\n-    - percent encode paths and query arguments. non-ASCII characters are\n-      percent-encoded using UTF-8 (RFC-3986)\n+    - percent encode paths ; non-ASCII characters are percent-encoded\n+      using UTF-8 (RFC-3986)\n+    - percent encode query arguments ; non-ASCII characters are percent-encoded\n+      using passed `encoding` (UTF-8 by default)\n     - normalize all spaces (in query arguments) '+' (plus symbol)\n     - normalize percent encodings case (%2f -> %2F)\n-    - remove query arguments with blank values (unless keep_blank_values is True)\n-    - remove fragments (unless keep_fragments is True)\n+    - remove query arguments with blank values (unless `keep_blank_values` is True)\n+    - remove fragments (unless `keep_fragments` is True)\n \n-    The url passed can be a str or unicode, while the url returned is always a\n-    str.\n+    The url passed can be bytes or unicode, while the url returned is\n+    always a native str (bytes in Python 2, unicode in Python 3).\n \n     For examples see the tests in tests/test_utils_url.py\n     \"\"\"\n+    # If supplied `encoding` is not compatible with all characters in `url`,\n+    # fallback to UTF-8 as safety net.\n+    # UTF-8 can handle all Unicode characters,\n+    # so we should be covered regarding URL normalization,\n+    # if not for proper URL expected by remote website.\n+    try:\n+        scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n+            parse_url(url), encoding=encoding)\n+    except UnicodeError as e:\n+        if encoding != 'utf8':\n+            scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n+                parse_url(url), encoding='utf8')\n+        else:\n+            raise\n \n-    scheme, netloc, path, params, query, fragment = parse_url(url)\n+    # 1. decode query-string as UTF-8 (or keep raw bytes),\n+    #    sort values,\n+    #    and percent-encode them back\n+    if not six.PY2:\n+        # Python3's urllib.parse.parse_qsl does not work as wanted\n+        # for percent-encoded characters that do not match passed encoding,\n+        # they get lost.\n+        #\n+        # e.g., 'q=b%a3' becomes [('q', 'b\\ufffd')]\n+        # (ie. with 'REPLACEMENT CHARACTER' (U+FFFD),\n+        #      instead of \\xa3 that you get with Python2's parse_qsl)\n+        #\n+        # what we want here is to keep raw bytes, and percent encode them\n+        # so as to preserve whatever encoding what originally used.\n+        #\n+        # See https://tools.ietf.org/html/rfc3987#section-6.4:\n+        #\n+        # For example, it is possible to have a URI reference of\n+        # \"http://www.example.org/r%E9sum%E9.xml#r%C3%A9sum%C3%A9\", where the\n+        # document name is encoded in iso-8859-1 based on server settings, but\n+        # where the fragment identifier is encoded in UTF-8 according to\n+        # [XPointer]. The IRI corresponding to the above URI would be (in XML\n+        # notation)\n+        # \"http://www.example.org/r%E9sum%E9.xml#r&#xE9;sum&#xE9;\".\n+        # Similar considerations apply to query parts.  The functionality of\n+        # IRIs (namely, to be able to include non-ASCII characters) can only be\n+        # used if the query part is encoded in UTF-8.\n+        keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n+    else:\n         keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n \n-    # XXX: copied from w3lib.url.safe_url_string to add encoding argument\n-    # path = to_native_str(path, encoding)\n-    # path = moves.urllib.parse.quote(path, _safe_chars, encoding='latin1') or '/'\n+    # 2. decode percent-encoded sequences in path as UTF-8 (or keep raw bytes)\n+    #    and percent-encode path again (this normalizes to upper-case %XX)\n+    uqp = _unquotepath(path)\n+    path = quote(uqp, _safe_chars) or '/'\n \n-    path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n+\n+    # every part should be safe already\n     return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n \n \n def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n+\n+    if six.PY3:\n+        # standard lib's unquote() does not work in Python 3\n+        # for non-UTF-8 percent-escaped characters, they get lost.\n+        # e.g., '%a3' becomes 'REPLACEMENT CHARACTER' (U+FFFD)\n+        #\n+        # unquote_to_bytes() returns raw bytes instead\n+        return unquote_to_bytes(path)\n+    else:\n+        # in Python 2, '%a3' becomes '\\xa3', which is what we want\n         return unquote(path)\n \n \n@@ -81,7 +156,60 @@ def parse_url(url, encoding=None):\n     \"\"\"\n     if isinstance(url, ParseResult):\n         return url\n-    return urlparse(to_native_str(url, encoding))\n+    return urlparse(to_unicode(url, encoding))\n+\n+\n+if six.PY3:\n+    from urllib.parse import _coerce_args, unquote_to_bytes\n+\n+    def parse_qsl_to_bytes(qs, keep_blank_values=False, strict_parsing=False):\n+        \"\"\"Parse a query given as a string argument.\n+\n+        Data are returned as a list of name, value pairs as bytes.\n+\n+        Arguments:\n+\n+        qs: percent-encoded query string to be parsed\n+\n+        keep_blank_values: flag indicating whether blank values in\n+            percent-encoded queries should be treated as blank strings.  A\n+            true value indicates that blanks should be retained as blank\n+            strings.  The default false value indicates that blank values\n+            are to be ignored and treated as if they were  not included.\n+\n+        strict_parsing: flag indicating what to do with parsing errors. If\n+            false (the default), errors are silently ignored. If true,\n+            errors raise a ValueError exception.\n+\n+        \"\"\"\n+        # This code is the same as Python3's parse_qsl()\n+        # (at https://hg.python.org/cpython/rev/c38ac7ab8d9a)\n+        # except for the unquote(s, encoding, errors) calls replaced\n+        # with unquote_to_bytes(s)\n+        qs, _coerce_result = _coerce_args(qs)\n+        pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n+        r = []\n+        for name_value in pairs:\n+            if not name_value and not strict_parsing:\n+                continue\n+            nv = name_value.split('=', 1)\n+            if len(nv) != 2:\n+                if strict_parsing:\n+                    raise ValueError(\"bad query field: %r\" % (name_value,))\n+                # Handle case of a control-name with no equal sign\n+                if keep_blank_values:\n+                    nv.append('')\n+                else:\n+                    continue\n+            if len(nv[1]) or keep_blank_values:\n+                name = nv[0].replace('+', ' ')\n+                name = unquote_to_bytes(name)\n+                name = _coerce_result(name)\n+                value = nv[1].replace('+', ' ')\n+                value = unquote_to_bytes(value)\n+                value = _coerce_result(value)\n+                r.append((name, value))\n+        return r\n \n \n def escape_ajax(url):\n\n@@ -2,10 +2,12 @@\n import unittest\n \n import six\n+from six.moves.urllib.parse import urlparse\n+\n from scrapy.spiders import Spider\n from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n                               canonicalize_url, add_http_if_no_scheme,\n-                              guess_scheme)\n+                              guess_scheme, parse_url)\n \n __doctests__ = ['scrapy.utils.url']\n \n@@ -123,16 +125,55 @@ class CanonicalizeUrlTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(\"http://www.example.com/do?q=a%20space&a=1\"),\n                                           \"http://www.example.com/do?a=1&q=a+space\")\n \n-    @unittest.skipUnless(six.PY2, \"TODO\")\n-    def test_normalize_percent_encoding_in_paths(self):\n-        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do\"),\n-                                          \"http://www.example.com/a%A3do\"),\n+    def test_canonicalize_url_unicode_path(self):\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé\"),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9\")\n+\n+    def test_canonicalize_url_unicode_query_string(self):\n+        # default encoding for path and query is UTF-8\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?q=résumé\"),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n+\n+        # passed encoding will affect query string\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?q=résumé\", encoding='latin1'),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%E9sum%E9\")\n+\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?country=Россия\", encoding='cp1251'),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?country=%D0%EE%F1%F1%E8%FF\")\n+\n+    def test_canonicalize_url_unicode_query_string_wrong_encoding(self):\n+        # trying to encode with wrong encoding\n+        # fallback to UTF-8\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?currency=€\", encoding='latin1'),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?currency=%E2%82%AC\")\n+\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/résumé?country=Россия\", encoding='latin1'),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?country=%D0%A0%D0%BE%D1%81%D1%81%D0%B8%D1%8F\")\n+\n+    def test_normalize_percent_encoding_in_paths(self):\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/r%c3%a9sum%c3%a9\"),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9\")\n+\n+        # non-UTF8 encoded sequences: they should be kept untouched, only upper-cased\n+        # 'latin1'-encoded sequence in path\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do\"),\n+                                          \"http://www.example.com/a%A3do\")\n+\n+        # 'latin1'-encoded path, UTF-8 encoded query string\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\"),\n+                                          \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n+\n+        # 'latin1'-encoded path and query string\n+        self.assertEqual(canonicalize_url(\"http://www.example.com/a%a3do?q=r%e9sum%e9\"),\n+                                          \"http://www.example.com/a%A3do?q=r%E9sum%E9\")\n \n-    @unittest.skipUnless(six.PY2, \"TODO\")\n     def test_normalize_percent_encoding_in_query_arguments(self):\n         self.assertEqual(canonicalize_url(\"http://www.example.com/do?k=b%a3\"),\n                                           \"http://www.example.com/do?k=b%A3\")\n \n+        self.assertEqual(canonicalize_url(\"http://www.example.com/do?k=r%c3%a9sum%c3%a9\"),\n+                                          \"http://www.example.com/do?k=r%C3%A9sum%C3%A9\")\n+\n     def test_non_ascii_percent_encoding_in_paths(self):\n         self.assertEqual(canonicalize_url(\"http://www.example.com/a do?a=1\"),\n                                           \"http://www.example.com/a%20do?a=1\"),\n@@ -144,7 +185,7 @@ class CanonicalizeUrlTest(unittest.TestCase):\n                                           \"http://www.example.com/a%20do%C2%A3.html?a=1\")\n \n     def test_non_ascii_percent_encoding_in_query_arguments(self):\n-        self.assertEqual(canonicalize_url(u\"http://www.example.com/do?price=\\xa3500&a=5&z=3\"),\n+        self.assertEqual(canonicalize_url(u\"http://www.example.com/do?price=£500&a=5&z=3\"),\n                                           u\"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n         self.assertEqual(canonicalize_url(b\"http://www.example.com/do?price=\\xc2\\xa3500&a=5&z=3\"),\n                                           \"http://www.example.com/do?a=5&price=%C2%A3500&z=3\")\n@@ -167,7 +208,6 @@ class CanonicalizeUrlTest(unittest.TestCase):\n             \"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html\"),\n             \"http://www.simplybedrooms.com/White-Bedroom-Furniture/Bedroom-Mirror:-Josephine-Cheval-Mirror.html\")\n \n-    @unittest.skipUnless(six.PY2, \"TODO\")\n     def test_safe_characters_unicode(self):\n         # urllib.quote uses a mapping cache of encoded characters. when parsing\n         # an already percent-encoded url, it will fail if that url was not\n@@ -181,12 +221,37 @@ class CanonicalizeUrlTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(\"http://www.EXAMPLE.com/\"),\n                                           \"http://www.example.com/\")\n \n+    def test_canonicalize_idns(self):\n+        self.assertEqual(canonicalize_url(u'http://www.bücher.de?q=bücher'),\n+                                           'http://www.xn--bcher-kva.de/?q=b%C3%BCcher')\n+        # Japanese (+ reordering query parameters)\n+        self.assertEqual(canonicalize_url(u'http://はじめよう.みんな/?query=サ&maxResults=5'),\n+                                           'http://xn--p8j9a0d9c9a.xn--q9jyb4c/?maxResults=5&query=%E3%82%B5')\n+\n     def test_quoted_slash_and_question_sign(self):\n         self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC+rocks%3f/?yeah=1\"),\n                          \"http://foo.com/AC%2FDC+rocks%3F/?yeah=1\")\n         self.assertEqual(canonicalize_url(\"http://foo.com/AC%2FDC/\"),\n                          \"http://foo.com/AC%2FDC/\")\n \n+    def test_canonicalize_urlparsed(self):\n+        # canonicalize_url() can be passed an already urlparse'd URL\n+        self.assertEqual(canonicalize_url(urlparse(u\"http://www.example.com/résumé?q=résumé\")),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n+        self.assertEqual(canonicalize_url(urlparse('http://www.example.com/caf%e9-con-leche.htm')),\n+                                          'http://www.example.com/caf%E9-con-leche.htm')\n+        self.assertEqual(canonicalize_url(urlparse(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\")),\n+                                          \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n+\n+    def test_canonicalize_parse_url(self):\n+        # parse_url() wraps urlparse and is used in link extractors\n+        self.assertEqual(canonicalize_url(parse_url(u\"http://www.example.com/résumé?q=résumé\")),\n+                                          \"http://www.example.com/r%C3%A9sum%C3%A9?q=r%C3%A9sum%C3%A9\")\n+        self.assertEqual(canonicalize_url(parse_url('http://www.example.com/caf%e9-con-leche.htm')),\n+                                          'http://www.example.com/caf%E9-con-leche.htm')\n+        self.assertEqual(canonicalize_url(parse_url(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\")),\n+                                          \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n+\n \n class AddHttpIfNoScheme(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19a4a0adf953d2a3d428411b6b0306997276238c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 119 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -100,10 +100,10 @@ class JsonItemExporter(BaseItemExporter):\n         self.first_item = True\n \n     def start_exporting(self):\n-        self.file.write(b\"[\")\n+        self.file.write(b\"[\\n\")\n \n     def finish_exporting(self):\n-        self.file.write(b\"]\")\n+        self.file.write(b\"\\n]\")\n \n     def export_item(self, item):\n         if self.first_item:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25401fd322b6c8f74d618e998fa675ea5eb494b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 24 | Churn Cumulative: 739 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,17 +83,16 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n     try:\n         scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n             parse_url(url), encoding=encoding)\n-    except UnicodeError as e:\n-        if encoding != 'utf8':\n+    except UnicodeEncodeError as e:\n         scheme, netloc, path, params, query, fragment = _safe_ParseResult(\n             parse_url(url), encoding='utf8')\n-        else:\n-            raise\n \n     # 1. decode query-string as UTF-8 (or keep raw bytes),\n     #    sort values,\n     #    and percent-encode them back\n-    if not six.PY2:\n+    if six.PY2:\n+        keyvals = parse_qsl(query, keep_blank_values)\n+    else:\n         # Python3's urllib.parse.parse_qsl does not work as wanted\n         # for percent-encoded characters that do not match passed encoding,\n         # they get lost.\n@@ -118,8 +117,6 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n         # IRIs (namely, to be able to include non-ASCII characters) can only be\n         # used if the query part is encoded in UTF-8.\n         keyvals = parse_qsl_to_bytes(query, keep_blank_values)\n-    else:\n-        keyvals = parse_qsl(query, keep_blank_values)\n     keyvals.sort()\n     query = urlencode(keyvals)\n \n@@ -138,16 +135,17 @@ def _unquotepath(path):\n     for reserved in ('2f', '2F', '3f', '3F'):\n         path = path.replace('%' + reserved, '%25' + reserved.upper())\n \n-    if six.PY3:\n-        # standard lib's unquote() does not work in Python 3\n-        # for non-UTF-8 percent-escaped characters, they get lost.\n+    if six.PY2:\n+        # in Python 2, '%a3' becomes '\\xa3', which is what we want\n+        return unquote(path)\n+    else:\n+        # in Python 3,\n+        # standard lib's unquote() does not work for non-UTF-8\n+        # percent-escaped characters, they get lost.\n         # e.g., '%a3' becomes 'REPLACEMENT CHARACTER' (U+FFFD)\n         #\n         # unquote_to_bytes() returns raw bytes instead\n         return unquote_to_bytes(path)\n-    else:\n-        # in Python 2, '%a3' becomes '\\xa3', which is what we want\n-        return unquote(path)\n \n \n def parse_url(url, encoding=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#efbe75ea5ef862e44ccee6b9410a8317f9fad329", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 743 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ import six\n from six.moves.urllib.parse import (ParseResult, urlunparse, urldefrag,\n                                     urlparse, parse_qsl, urlencode,\n                                     quote, unquote)\n-if six.PY3:\n+if not six.PY2:\n     from urllib.parse import unquote_to_bytes\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n@@ -157,7 +157,7 @@ def parse_url(url, encoding=None):\n     return urlparse(to_unicode(url, encoding))\n \n \n-if six.PY3:\n+if not six.PY2:\n     from urllib.parse import _coerce_args, unquote_to_bytes\n \n     def parse_qsl_to_bytes(qs, keep_blank_values=False, strict_parsing=False):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0e11b3e6f021b96dd97f84b465583a32a8663420", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 421 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -252,6 +252,19 @@ class CanonicalizeUrlTest(unittest.TestCase):\n         self.assertEqual(canonicalize_url(parse_url(\"http://www.example.com/a%a3do?q=r%c3%a9sum%c3%a9\")),\n                                           \"http://www.example.com/a%A3do?q=r%C3%A9sum%C3%A9\")\n \n+    def test_canonicalize_url_idempotence(self):\n+        for url, enc in [(u'http://www.bücher.de/résumé?q=résumé', 'utf8'),\n+                         (u'http://www.example.com/résumé?q=résumé', 'latin1'),\n+                         (u'http://www.example.com/résumé?country=Россия', 'cp1251'),\n+                         (u'http://はじめよう.みんな/?query=サ&maxResults=5', 'iso2022jp')]:\n+            canonicalized = canonicalize_url(url, encoding=enc)\n+\n+            # if we canonicalize again, we ge the same result\n+            self.assertEqual(canonicalize_url(canonicalized, encoding=enc), canonicalized)\n+\n+            # without encoding, already canonicalized URL is canonicalized identically\n+            self.assertEqual(canonicalize_url(canonicalized), canonicalized)\n+\n \n class AddHttpIfNoScheme(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c8bff2375161c4934212b6cedbfb866c5d909fc8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 416 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ setup(\n     ],\n     install_requires=[\n         'Twisted>=10.0.0',\n-        'w3lib>=1.13.0',\n+        'w3lib>=1.14.2',\n         'queuelib',\n         'lxml',\n         'pyOpenSSL',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#334ee40e3de7f93b9e178ecc4310224d0f4fe022", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 9 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -21,6 +21,7 @@ from twisted.internet.error import TimeoutError, DNSLookupError, \\\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.response import response_status_message\n from scrapy.xlib.tx import ResponseFailed\n+from scrapy.core.downloader.handlers.http11 import TunnelError\n \n logger = logging.getLogger(__name__)\n \n@@ -32,7 +33,7 @@ class RetryMiddleware(object):\n     EXCEPTIONS_TO_RETRY = (defer.TimeoutError, TimeoutError, DNSLookupError,\n                            ConnectionRefusedError, ConnectionDone, ConnectError,\n                            ConnectionLost, TCPTimedOutError, ResponseFailed,\n-                           IOError)\n+                           IOError, TunnelError)\n \n     def __init__(self, settings):\n         if not settings.getbool('RETRY_ENABLED'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#137197b98b1d3e94a1be88db48abbf537d90f6e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 56 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,7 +44,9 @@ try:\n                 try:\n                     verifyHostname(connection, self._hostnameASCII)\n                 except VerificationError as e:\n-                    logger.warning(e)\n+                    logger.warning(\n+                        'Remote certificate is not valid for hostname \"{}\"; {}'.format(\n+                            self._hostnameASCII, e))\n \n except ImportError:\n     # ImportError should not matter for older Twisted versions\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cef1a1d0060148716ffec4ad6ce9bf709ad1ea2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 21 | Files Changed: 4 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 820 | Contributors (this commit): 28 | Commits (past 90d): 21 | Contributors (cumulative): 44 | DMM Complexity: 1.0\n\nDIFF:\n@@ -214,11 +214,14 @@ class FilesPipeline(MediaPipeline):\n     \"\"\"\n \n     MEDIA_NAME = \"file\"\n+    EXPIRES = 90\n     STORE_SCHEMES = {\n         '': FSFilesStore,\n         'file': FSFilesStore,\n         's3': S3FilesStore,\n     }\n+    DEFAULT_FILES_URLS_FIELD = 'file_urls'\n+    DEFAULT_FILES_RESULT_FIELD = 'files'\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n         if not store_uri:\n@@ -228,9 +231,9 @@ class FilesPipeline(MediaPipeline):\n             settings = Settings(settings)\n         \n         self.store = self._get_store(store_uri)\n-        self.expires = settings.getint('FILES_EXPIRES')\n-        self.files_urls_field = settings.get('FILES_URLS_FIELD')\n-        self.files_result_field = settings.get('FILES_RESULT_FIELD')\n+        self.expires = settings.getint('FILES_EXPIRES', self.EXPIRES)\n+        self.files_urls_field = settings.get('FILES_URLS_FIELD', self.DEFAULT_FILES_URLS_FIELD)\n+        self.files_result_field = settings.get('FILES_RESULT_FIELD', self.DEFAULT_FILES_RESULT_FIELD)\n \n         super(FilesPipeline, self).__init__(download_func=download_func)\n \n\n@@ -37,6 +37,12 @@ class ImagesPipeline(FilesPipeline):\n     \"\"\"\n \n     MEDIA_NAME = 'image'\n+    MIN_WIDTH = 0\n+    MIN_HEIGHT = 0\n+    EXPIRES = 0\n+    THUMBS = {}\n+    IMAGES_URLS_FIELD = 'image_urls'\n+    IMAGES_RESULT_FIELD = 'images'\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n         super(ImagesPipeline, self).__init__(store_uri, settings=settings, download_func=download_func)\n@@ -44,19 +50,18 @@ class ImagesPipeline(FilesPipeline):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n-        self.expires = settings.getint('IMAGES_EXPIRES')\n-        self.images_urls_field = settings.get('IMAGES_URLS_FIELD')\n-        self.images_result_field = settings.get('IMAGES_RESULT_FIELD')\n-        self.min_width = settings.getint('IMAGES_MIN_WIDTH')\n-        self.min_height = settings.getint('IMAGES_MIN_HEIGHT')\n-        self.thumbs = settings.get('IMAGES_THUMBS')\n+        self.expires = settings.getint('IMAGES_EXPIRES', self.EXPIRES)\n+        self.images_urls_field = settings.get('IMAGES_URLS_FIELD', self.IMAGES_URLS_FIELD)\n+        self.images_result_field = settings.get('IMAGES_RESULT_FIELD', self.IMAGES_RESULT_FIELD)\n+        self.min_width = settings.getint('IMAGES_MIN_WIDTH', self.MIN_WIDTH)\n+        self.min_height = settings.getint('IMAGES_MIN_HEIGHT', self.MIN_HEIGHT)\n+        self.thumbs = settings.get('IMAGES_THUMBS', self.THUMBS)\n \n     @classmethod\n     def from_settings(cls, settings):\n         s3store = cls.STORE_SCHEMES['s3']\n         s3store.AWS_ACCESS_KEY_ID = settings['AWS_ACCESS_KEY_ID']\n         s3store.AWS_SECRET_ACCESS_KEY = settings['AWS_SECRET_ACCESS_KEY']\n-\n         store_uri = settings['IMAGES_STORE']\n         return cls(store_uri, settings=settings)\n \n\n@@ -179,13 +179,6 @@ HTTPCACHE_GZIP = False\n \n HTTPPROXY_AUTH_ENCODING = 'latin-1'\n \n-IMAGES_MIN_WIDTH = 0\n-IMAGES_MIN_HEIGHT = 0\n-IMAGES_EXPIRES = 90\n-IMAGES_THUMBS = {}\n-IMAGES_URLS_FIELD = 'image_urls'\n-IMAGES_RESULT_FIELD = 'images'\n-\n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n ITEM_PIPELINES = {}\n\n@@ -224,19 +224,22 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n     def test_images_urls_field(self):\n         another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n                                                                 'IMAGES_URLS_FIELD': 'funny_field'}))\n-        self.assertEqual(self.pipeline.images_urls_field, self.default_settings.get('IMAGES_URLS_FIELD'))\n+        default = self.pipeline.IMAGES_URLS_FIELD\n+        self.assertEqual(self.pipeline.images_urls_field, self.default_settings.get('IMAGES_URLS_FIELD', default))\n         self.assertEqual(another_pipeline.images_urls_field, 'funny_field')\n \n     def test_images_result_field(self):\n         another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n                                                                 'IMAGES_RESULT_FIELD': 'funny_field'}))\n-        self.assertEqual(self.pipeline.images_result_field, self.default_settings.get('IMAGES_RESULT_FIELD'))\n+        default = self.pipeline.IMAGES_RESULT_FIELD\n+        self.assertEqual(self.pipeline.images_result_field, self.default_settings.get('IMAGES_RESULT_FIELD', default))\n         self.assertEqual(another_pipeline.images_result_field, 'funny_field')\n \n     def test_min_width(self):\n         another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n                                                                 'IMAGES_MIN_WIDTH': 42}))\n-        self.assertEqual(self.pipeline.min_width, self.default_settings.getint('IMAGES_MIN_WIDTH'))\n+        default = self.pipeline.MIN_WIDTH\n+        self.assertEqual(self.pipeline.min_width, self.default_settings.getint('IMAGES_MIN_WIDTH', default))\n         self.assertEqual(another_pipeline.min_width, 42)\n \n     def test_min_height(self):\n@@ -249,7 +252,8 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         custom_thumbs = {'small': (50, 50), 'big': (270, 270)}\n         another_pipeline = ImagesPipeline.from_settings(Settings({'IMAGES_STORE': self.tempdir,\n                                                                 'IMAGES_THUMBS': custom_thumbs}))\n-        self.assertEqual(self.pipeline.thumbs, self.default_settings.get('IMAGES_THUMBS'))\n+        default = self.pipeline.THUMBS\n+        self.assertEqual(self.pipeline.thumbs, self.default_settings.get('IMAGES_THUMBS', default))\n         self.assertEqual(another_pipeline.thumbs, custom_thumbs)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c67db3917dd07761b113f3e3b37ebf5a9558f00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 23 | Churn Cumulative: 107 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -256,6 +256,29 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n         self.assertEqual(self.pipeline.thumbs, self.default_settings.get('IMAGES_THUMBS', default))\n         self.assertEqual(another_pipeline.thumbs, custom_thumbs)\n \n+    def test_class_attrs_preserved(self):\n+\n+        class UserDefinedImagePipeline(ImagesPipeline):\n+            MIN_WIDTH = 1000\n+\n+        # If image settings are not defined values are taken from class attributes.\n+        pipeline = UserDefinedImagePipeline.from_settings(Settings({\"IMAGES_STORE\": self.tempdir}))\n+        self.assertEqual(pipeline.min_width, 1000)\n+\n+    def test_class_attrs_not_preserved_if_settings_defined(self):\n+\n+        class UserDefinedImagePipeline(ImagesPipeline):\n+            MIN_WIDTH = 1000\n+\n+        settings = {\n+            \"IMAGES_STORE\": self.tempdir,\n+            \"IMAGES_MIN_WIDTH\": 90\n+        }\n+\n+        # If image settings are defined they override class attributes.\n+        pipeline = UserDefinedImagePipeline.from_settings(Settings(settings))\n+        self.assertEqual(pipeline.min_width, 90)\n+\n \n def _create_image(format, *a, **kw):\n     buf = TemporaryFile()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2c8036a97f0c3b18432638f50538019888ec1be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 92 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -45,7 +45,7 @@ master_doc = 'index'\n \n # General information about the project.\n project = u'Scrapy'\n-copyright = u'2008-2015, Scrapy developers'\n+copyright = u'2008-2016, Scrapy developers'\n \n # The version info for the project you're documenting, acts as replacement for\n # |version| and |release|, also used in various other places throughout the\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1bc9d35a878a4028b4dd802707c7b513d4c3ef86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1380 | Contributors (this commit): 14 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -343,7 +343,7 @@ class ScrapyAgent(object):\n         txresponse, body, flags = result\n         status = int(txresponse.code)\n         headers = Headers(txresponse.headers.getAllRawHeaders())\n-        respcls = responsetypes.from_args(headers=headers, url=url)\n+        respcls = responsetypes.from_args(headers=headers, url=url, body=body)\n         return respcls(url=url, status=status, headers=headers, body=body, flags=flags)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce48bae533cfaf093778481a5a599943afa15846", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 40 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -144,3 +144,13 @@ class HttpCompressionTest(TestCase):\n         self.assertIs(newresponse, response)\n         self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n         self.assertEqual(response.headers['Content-Type'], b'application/gzip')\n+\n+    def test_process_response_head_request_no_decode_required(self):\n+        response = self._getresponse('gzip')\n+        response.headers['Content-Type'] = 'application/gzip'\n+        request = response.request\n+        request.method = 'HEAD'\n+        response = response.replace(body = None)\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        self.assertIs(newresponse, response)\n+        self.assertEquals(response.body, '')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
