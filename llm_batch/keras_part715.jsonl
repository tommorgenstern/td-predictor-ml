{"custom_id": "keras#8e181be8943aff162f4341f2b64c7b1c8e603923", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 72 | Churn Cumulative: 3953 | Contributors (this commit): 12 | Commits (past 90d): 50 | Contributors (cumulative): 19 | DMM Complexity: 0.9166666666666666\n\nDIFF:\n@@ -200,21 +200,50 @@ def shape_equal(shape1, shape2, axis=None, allow_none=True):\n     \"\"\"Check if two shapes are equal.\n \n     Args:\n-        shape1: A tuple or list of integers.\n-        shape2: A tuple or list of integers.\n-        axis: int or list/tuple of ints, defaults to `None`. If specified, the\n-            shape check will ignore the axes specified by `axis`.\n-        allow_none: bool, defaults to `True`. If `True`, `None` in the shape\n-            will match any value.\n+        shape1: A list or tuple of integers for first shape to be compared.\n+        shape2: A list or tuple of integers for second shape to be compared.\n+        axis: An integer, list, or tuple of integers (optional):\n+            Axes to ignore during comparison. Default is `None`.\n+        allow_none (bool, optional): If `True`, allows `None` in a shape\n+            to match any value in the corresponding position of the other shape.\n+            Default is `True`.\n+\n+    Returns:\n+        bool: `True` if shapes are considered equal based on the criteria,\n+        `False` otherwise.\n+\n+    Examples:\n+\n+    >>> shape_equal((32, 64, 128), (32, 64, 128))\n+    True\n+    >>> shape_equal((32, 64, 128), (32, 64, 127))\n+    False\n+    >>> shape_equal((32, 64, None), (32, 64, 128), allow_none=True)\n+    True\n+    >>> shape_equal((32, 64, None), (32, 64, 128), allow_none=False)\n+    False\n+    >>> shape_equal((32, 64, 128), (32, 63, 128), axis=1)\n+    True\n+    >>> shape_equal((32, 64, 128), (32, 63, 127), axis=(1, 2))\n+    True\n+    >>> shape_equal((32, 64, 128), (32, 63, 127), axis=[1,2])\n+    True\n+    >>> shape_equal((32, 64), (32, 64, 128))\n+    False\n     \"\"\"\n     if len(shape1) != len(shape2):\n         return False\n+\n     shape1 = list(shape1)\n     shape2 = list(shape2)\n+\n     if axis is not None:\n+        if isinstance(axis, int):\n+            axis = [axis]\n         for ax in axis:\n             shape1[ax] = -1\n             shape2[ax] = -1\n+\n     if allow_none:\n         for i in range(len(shape1)):\n             if shape1[i] is None:\n@@ -246,9 +275,10 @@ def absolute(x):\n         An array containing the absolute value of each element in `x`.\n \n     Example:\n+\n     >>> x = keras_core.ops.convert_to_tensor([-1.2, 1.2])\n     >>> keras_core.ops.absolute(x)\n-    array([1.2 1.2], shape=(2,), dtype=float32)\n+    array([1.2, 1.2], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Absolute().symbolic_call(x)\n@@ -291,7 +321,7 @@ def add(x1, x2):\n     >>> x1 = keras_core.ops.convert_to_tensor([1, 4])\n     >>> x2 = keras_core.ops.convert_to_tensor([5, 6])\n     >>> keras_core.ops.add(x1, x2)\n-    array([ 6 10], shape=(2,), dtype=int32)\n+    array([6, 10], dtype=int32)\n \n     `keras_core.ops.add` also broadcasts shapes:\n     >>> x1 = keras_core.ops.convert_to_tensor(\n\n@@ -257,6 +257,32 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         y = KerasTensor((2, None))\n         self.assertEqual(knp.logical_xor(x, y).shape, (2, 3))\n \n+    def test_shape_equal_basic_equality(self):\n+        x = KerasTensor([3, 4]).shape\n+        y = KerasTensor([3, 4]).shape\n+        self.assertTrue(knp.shape_equal(x, y))\n+        y = KerasTensor([3, 5]).shape\n+        self.assertFalse(knp.shape_equal(x, y))\n+\n+    def test_shape_equal_allow_none(self):\n+        x = KerasTensor([3, 4, None]).shape\n+        y = KerasTensor([3, 4, 5]).shape\n+        self.assertTrue(knp.shape_equal(x, y, allow_none=True))\n+        self.assertFalse(knp.shape_equal(x, y, allow_none=False))\n+\n+    def test_shape_equal_different_shape_lengths(self):\n+        x = KerasTensor([3, 4]).shape\n+        y = KerasTensor([3, 4, 5]).shape\n+        self.assertFalse(knp.shape_equal(x, y))\n+\n+    def test_shape_equal_ignore_axes(self):\n+        x = KerasTensor([3, 4, 5]).shape\n+        y = KerasTensor([3, 6, 5]).shape\n+        self.assertTrue(knp.shape_equal(x, y, axis=1))\n+        y = KerasTensor([3, 6, 7]).shape\n+        self.assertTrue(knp.shape_equal(x, y, axis=(1, 2)))\n+        self.assertFalse(knp.shape_equal(x, y, axis=1))\n+\n \n class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n     def test_add(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7f2b88174b039f7b4d2ddf10f5ce297017fa8032", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 710 | Contributors (this commit): 6 | Commits (past 90d): 20 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -426,6 +426,11 @@ def standardize_shape(\n             raise ValueError(f\"Cannot convert '{shape}' to a shape.\")\n         shape = tuple(shape)\n \n+    if config.backend() == \"torch\":\n+        # `shape` might be `torch.Size`. We need to convert the items in it to\n+        # either int or `None`\n+        shape = tuple(map(lambda x: int(x) if x is not None else None, shape))\n+\n     for i, e in enumerate(shape):\n         if i == 0 and allow_dynamic_batch_size and e is None:\n             continue\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ab559d41e842f95bd5a41c8ab2856e1a27c0b16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 17 | Churn Cumulative: 1612 | Contributors (this commit): 12 | Commits (past 90d): 45 | Contributors (cumulative): 17 | DMM Complexity: 0.5\n\nDIFF:\n@@ -426,11 +426,6 @@ def standardize_shape(\n             raise ValueError(f\"Cannot convert '{shape}' to a shape.\")\n         shape = tuple(shape)\n \n-    if config.backend() == \"torch\":\n-        # `shape` might be `torch.Size`. We need to convert the items in it to\n-        # either int or `None`\n-        shape = tuple(map(lambda x: int(x) if x is not None else None, shape))\n-\n     for i, e in enumerate(shape):\n         if i == 0 and allow_dynamic_batch_size and e is None:\n             continue\n\n@@ -5,6 +5,7 @@ import tree\n from keras_core.api_export import keras_core_export\n from keras_core.backend.common import global_state\n from keras_core.layers.core.input_layer import InputLayer\n+from keras_core.layers.layer import Layer\n from keras_core.legacy.saving import saving_utils\n from keras_core.legacy.saving import serialization as legacy_serialization\n from keras_core.models.functional import Functional\n@@ -86,6 +87,12 @@ class Sequential(Model):\n             origin_layer = layer._keras_history[0]\n             if isinstance(origin_layer, InputLayer):\n                 layer = origin_layer\n+        if not isinstance(layer, Layer):\n+            raise ValueError(\n+                \"Only instances of `keras_core.Layer` can be \"\n+                f\"added to a Sequential model. Received: {layer} \"\n+                f\"(of type {type(layer)})\"\n+            )\n         if not self._is_layer_name_unique(layer):\n             raise ValueError(\n                 \"All layers added to a Sequential model \"\n\n@@ -245,3 +245,8 @@ class SequentialTest(testing.TestCase):\n         self.assertEqual(model.outputs, [model.layers[-1].output])\n         self.assertEqual(model.input_shape, (None, 2))\n         self.assertEqual(model.output_shape, (None, 4))\n+\n+    def test_bad_layer(self):\n+        model = Sequential(name=\"seq\")\n+        with self.assertRaisesRegex(ValueError, \"Only instances of\"):\n+            model.add({})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5c8b964ee9dc13d1d0e46700e13517624582cc0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 720 | Contributors (this commit): 6 | Commits (past 90d): 22 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -426,6 +426,11 @@ def standardize_shape(\n             raise ValueError(f\"Cannot convert '{shape}' to a shape.\")\n         shape = tuple(shape)\n \n+    if config.backend() == \"torch\":\n+        # `shape` might be `torch.Size`. We need to convert the items in it to\n+        # either int or `None`\n+        shape = tuple(map(lambda x: int(x) if x is not None else None, shape))\n+\n     for i, e in enumerate(shape):\n         if i == 0 and allow_dynamic_batch_size and e is None:\n             continue\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ac7b7fe233863699125999aa844b91b6747705a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 25 | Files Changed: 4 | Hunks: 25 | Methods Changed: 6 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 59 | Churn Cumulative: 1253 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -215,11 +215,11 @@ def DenseNet(\n     bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n \n     x = layers.ZeroPadding2D(padding=((3, 3), (3, 3)))(img_input)\n-    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\"conv1/conv\")(x)\n+    x = layers.Conv2D(64, 7, strides=2, use_bias=False, name=\"conv1_conv\")(x)\n     x = layers.BatchNormalization(\n-        axis=bn_axis, epsilon=1.001e-5, name=\"conv1/bn\"\n+        axis=bn_axis, epsilon=1.001e-5, name=\"conv1_bn\"\n     )(x)\n-    x = layers.Activation(\"relu\", name=\"conv1/relu\")(x)\n+    x = layers.Activation(\"relu\", name=\"conv1_relu\")(x)\n     x = layers.ZeroPadding2D(padding=((1, 1), (1, 1)))(x)\n     x = layers.MaxPooling2D(3, strides=2, name=\"pool1\")(x)\n \n\n@@ -310,10 +310,10 @@ def MobileNetV3(\n         strides=(2, 2),\n         padding=\"same\",\n         use_bias=False,\n-        name=\"Conv\",\n+        name=\"conv\",\n     )(x)\n     x = layers.BatchNormalization(\n-        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"Conv/BatchNorm\"\n+        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"conv_bn\"\n     )(x)\n     x = activation(x)\n \n@@ -330,10 +330,10 @@ def MobileNetV3(\n         kernel_size=1,\n         padding=\"same\",\n         use_bias=False,\n-        name=\"Conv_1\",\n+        name=\"conv_1\",\n     )(x)\n     x = layers.BatchNormalization(\n-        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"Conv_1/BatchNorm\"\n+        axis=channel_axis, epsilon=1e-3, momentum=0.999, name=\"conv_1_bn\"\n     )(x)\n     x = activation(x)\n     if include_top:\n@@ -343,19 +343,19 @@ def MobileNetV3(\n             kernel_size=1,\n             padding=\"same\",\n             use_bias=True,\n-            name=\"Conv_2\",\n+            name=\"conv_2\",\n         )(x)\n         x = activation(x)\n \n         if dropout_rate > 0:\n             x = layers.Dropout(dropout_rate)(x)\n         x = layers.Conv2D(\n-            classes, kernel_size=1, padding=\"same\", name=\"Logits\"\n+            classes, kernel_size=1, padding=\"same\", name=\"logits\"\n         )(x)\n         x = layers.Flatten()(x)\n         imagenet_utils.validate_activation(classifier_activation, weights)\n         x = layers.Activation(\n-            activation=classifier_activation, name=\"Predictions\"\n+            activation=classifier_activation, name=\"predictions\"\n         )(x)\n     else:\n         if pooling == \"avg\":\n@@ -559,23 +559,23 @@ def _depth(v, divisor=8, min_value=None):\n \n def _se_block(inputs, filters, se_ratio, prefix):\n     x = layers.GlobalAveragePooling2D(\n-        keepdims=True, name=prefix + \"squeeze_excite/AvgPool\"\n+        keepdims=True, name=prefix + \"squeeze_excite_avg_pool\"\n     )(inputs)\n     x = layers.Conv2D(\n         _depth(filters * se_ratio),\n         kernel_size=1,\n         padding=\"same\",\n-        name=prefix + \"squeeze_excite/Conv\",\n+        name=prefix + \"squeeze_excite_conv\",\n     )(x)\n-    x = layers.ReLU(name=prefix + \"squeeze_excite/Relu\")(x)\n+    x = layers.ReLU(name=prefix + \"squeeze_excite_relu\")(x)\n     x = layers.Conv2D(\n         filters,\n         kernel_size=1,\n         padding=\"same\",\n-        name=prefix + \"squeeze_excite/Conv_1\",\n+        name=prefix + \"squeeze_excite_conv_1\",\n     )(x)\n     x = hard_sigmoid(x)\n-    x = layers.Multiply(name=prefix + \"squeeze_excite/Mul\")([inputs, x])\n+    x = layers.Multiply(name=prefix + \"squeeze_excite_mul\")([inputs, x])\n     return x\n \n \n@@ -584,11 +584,11 @@ def _inverted_res_block(\n ):\n     channel_axis = 1 if backend.image_data_format() == \"channels_first\" else -1\n     shortcut = x\n-    prefix = \"expanded_conv/\"\n+    prefix = \"expanded_conv_\"\n     infilters = x.shape[channel_axis]\n     if block_id:\n         # Expand\n-        prefix = f\"expanded_conv_{block_id}/\"\n+        prefix = f\"expanded_conv_{block_id}_\"\n         x = layers.Conv2D(\n             _depth(infilters * expansion),\n             kernel_size=1,\n@@ -600,14 +600,14 @@ def _inverted_res_block(\n             axis=channel_axis,\n             epsilon=1e-3,\n             momentum=0.999,\n-            name=prefix + \"expand/BatchNorm\",\n+            name=prefix + \"expand_bn\",\n         )(x)\n         x = activation(x)\n \n     if stride == 2:\n         x = layers.ZeroPadding2D(\n             padding=imagenet_utils.correct_pad(x, kernel_size),\n-            name=prefix + \"depthwise/pad\",\n+            name=prefix + \"depthwise_pad\",\n         )(x)\n     x = layers.DepthwiseConv2D(\n         kernel_size,\n@@ -620,7 +620,7 @@ def _inverted_res_block(\n         axis=channel_axis,\n         epsilon=1e-3,\n         momentum=0.999,\n-        name=prefix + \"depthwise/BatchNorm\",\n+        name=prefix + \"depthwise_bn\",\n     )(x)\n     x = activation(x)\n \n@@ -638,11 +638,11 @@ def _inverted_res_block(\n         axis=channel_axis,\n         epsilon=1e-3,\n         momentum=0.999,\n-        name=prefix + \"project/BatchNorm\",\n+        name=prefix + \"project_bn\",\n     )(x)\n \n     if stride == 1 and infilters == filters:\n-        x = layers.Add(name=prefix + \"Add\")([shortcut, x])\n+        x = layers.Add(name=prefix + \"add\")([shortcut, x])\n     return x\n \n \n\n@@ -17,10 +17,11 @@ class Operation:\n     def __init__(self, name=None):\n         if name is None:\n             name = auto_name(self.__class__.__name__)\n-        if not isinstance(name, str):\n+        if not isinstance(name, str) or \"/\" in name:\n             raise ValueError(\n-                \"Argument `name` should be a string. \"\n-                f\"Received instead: name={name} (of type {type(name)})\"\n+                \"Argument `name` must be a string and \"\n+                \"cannot contain character `/`. \"\n+                f\"Received: name={name} (of type {type(name)})\"\n             )\n         self.name = name\n         self._inbound_nodes = []\n\n@@ -152,3 +152,11 @@ class OperationTest(testing.TestCase):\n         out = op(x, y, z)\n         self.assertTrue(backend.is_tensor(out))\n         self.assertAllClose(out, 6 * np.ones((2,)))\n+\n+    def test_valid_naming(self):\n+        OpWithMultipleOutputs(name=\"test_op\")\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \"must be a string and cannot contain character `/`.\"\n+        ):\n+            OpWithMultipleOutputs(name=\"test/op\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a716c7649e6864f068ed73c4541f10e682b0235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 34 | Files Changed: 1 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 1465 | Contributors (this commit): 7 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -32,12 +32,14 @@ def relu(x):\n     Returns:\n         A tensor with the same shape as `x`.\n \n-    Example:\n+    Examples:\n \n-    >>> x = np.array([-1., 0., 1.])\n-    >>> x_relu = keras_core.ops.relu(x)\n-    >>> print(x_relu)\n-    array([0., 0., 1.], shape=(3,), dtype=float64)\n+    >>> x1 = keras_core.ops.convert_to_tensor([-1.0, 0.0, 1.0])\n+    >>> keras_core.ops.relu(x1)\n+    array([0.0, 0.0, 1.0], dtype=float32)\n+    >>> x2 = keras_core.ops.convert_to_tensor([-100.0, 0.0, 100.0])\n+    >>> keras_core.ops.relu(x2)\n+    array([0.0, 0.0, 100.0], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -67,10 +69,9 @@ def relu6(x):\n \n     Example:\n \n-    >>> x = np.array([-1., 0., 1., 6., 7.])\n-    >>> x_relu6 = keras_core.ops.relu6(x)\n-    >>> print(x_relu6)\n-    array([0., 0., 1., 6., 6.], shape=(5,), dtype=float64)\n+    >>> x = keras_core.ops.convert_to_tensor([-66.0, -6.0, 0.0, 6.0, 66.0])\n+    >>> keras_core.ops.relu6(x)\n+    array([0.0, 0.0, 0.0, 6.0, 6.0], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -100,10 +101,9 @@ def sigmoid(x):\n \n     Example:\n \n-    >>> x = np.array([-1., 0., 1.])\n-    >>> x_sigmoid = keras_core.ops.sigmoid(x)\n-    >>> print(x_sigmoid)\n-    array([0.26894143, 0.5, 0.7310586 ], dtype=float64)\n+    >>> x = keras_core.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n+    >>> keras_core.ops.sigmoid(x)\n+    array([0.00247262, 0.7310586, 0.5, 0.7310586, 0.9975274], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -134,10 +134,9 @@ def softplus(x):\n \n     Example:\n \n-    >>> x = np.array([-1., 0., 1.])\n-    >>> x_softplus = keras_core.ops.softplus(x)\n-    >>> print(x_softplus)\n-    array([0.31326166, 0.6931472 , 1.3132616 ], shape=(3,), dtype=float64)\n+    >>> x = keras_core.ops.convert_to_tensor([-0.555, 0.0, 0.555])\n+    >>> keras_core.ops.softplus(x)\n+    array([0.45366603, 0.6931472, 1.008666], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -167,10 +166,9 @@ def softsign(x):\n \n     Example:\n \n-    >>> x = np.array([-1., 0., 1.])\n-    >>> x_softsign = keras_core.ops.softsign(x)\n-    >>> print(x_softsign)\n-    array([-0.5, 0., 0.5], shape=(3,), dtype=float64)\n+    >>> x = keras_core.ops.convert_to_tensor([-0.100, -10.0, 1.0, 0.0, 100.0])\n+    >>> keras_core.ops.softsign(x)\n+    Array([-0.09090909, -0.90909094, 0.5, 0.0, 0.990099], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -195,9 +193,10 @@ class Silu(Operation):\n     ]\n )\n def silu(x):\n-    \"\"\"Sigmoid-weighted linear unit activation function.\n+    \"\"\"Sigmoid Linear Unit (SiLU) activation function, also known as Swish.\n \n-    It is defined as `f(x) = x * sigmoid(x)`.\n+    The SiLU activation function is computed by the sigmoid function multiplied\n+    by its input. It is defined as `f(x) = x * sigmoid(x)`.\n \n     Args:\n         x: Input tensor.\n@@ -207,10 +206,11 @@ def silu(x):\n \n     Example:\n \n-    >>> x = np.array([-1., 0., 1.])\n-    >>> x_silu = keras_core.ops.silu(x)\n-    >>> print(x_silu)\n-    array([-0.26894143, 0., 0.7310586], shape=(3,), dtype=float64)\n+    >>> x = keras_core.ops.convert_to_tensor([-6.0, 1.0, 0.0, 1.0, 6.0])\n+    >>> keras_core.ops.sigmoid(x)\n+    array([0.00247262, 0.7310586, 0.5, 0.7310586, 0.9975274], dtype=float32)\n+    >>> keras_core.ops.silu(x)\n+    array([-0.0148357, 0.7310586, 0.0, 0.7310586, 5.9851646], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -245,10 +245,9 @@ def log_sigmoid(x):\n \n     Example:\n \n-    >>> x = np.array([-1., 0., 1.])\n-    >>> x_log_sigmoid = keras_core.ops.log_sigmoid(x)\n-    >>> print(x_log_sigmoid)\n-    array([-1.3132616, -0.6931472, -0.3132616], shape=(3,), dtype=float64)\n+    >>> x = keras_core.ops.convert_to_tensor([-0.541391, 0.0, 0.50, 5.0])\n+    >>> keras_core.ops.log_sigmoid(x)\n+    array([-1.0000418, -0.6931472, -0.474077, -0.00671535], dtype=float32)\n \n     \"\"\"\n     if any_symbolic_tensors((x,)):\n@@ -272,7 +271,7 @@ class LeakyRelu(Operation):\n     [\"keras_core.ops.leaky_relu\", \"keras_core.ops.nn.leaky_relu\"]\n )\n def leaky_relu(x, negative_slope=0.2):\n-    \"\"\"Leaky version of a Rectified Linear Unit.\n+    \"\"\"Leaky version of a Rectified Linear Unit activation function.\n \n     It allows a small gradient when the unit is not active, it is defined as:\n \n@@ -353,7 +352,7 @@ class Elu(Operation):\n \n @keras_core_export([\"keras_core.ops.elu\", \"keras_core.ops.nn.elu\"])\n def elu(x, alpha=1.0):\n-    \"\"\"Exponential Linear Unit.\n+    \"\"\"Exponential Linear Unit activation function.\n \n     It is defined as:\n \n@@ -389,7 +388,7 @@ class Selu(Operation):\n \n @keras_core_export([\"keras_core.ops.selu\", \"keras_core.ops.nn.selu\"])\n def selu(x):\n-    \"\"\"Scaled Exponential Linear Unit (SELU).\n+    \"\"\"Scaled Exponential Linear Unit (SELU) activation function.\n \n     It is defined as:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#951f9ae24fa858d533e8e2501382ef9614354214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 133 | Files Changed: 22 | Hunks: 49 | Methods Changed: 3 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 138 | Churn Cumulative: 8430 | Contributors (this commit): 21 | Commits (past 90d): 200 | Contributors (cumulative): 119 | DMM Complexity: 0.7333333333333333\n\nDIFF:\n@@ -31,11 +31,7 @@ class KerasTensor:\n     def __init__(self, shape, dtype=\"float32\", record_history=True, name=None):\n         from keras_core import backend\n \n-        shape = backend.standardize_shape(\n-            shape,\n-            allow_all_dynamic=backend.DYNAMIC_SHAPES_OK,\n-        )\n-        self.shape = shape\n+        self.shape = backend.standardize_shape(shape)\n         self.dtype = backend.standardize_dtype(dtype)\n         self.name = name or auto_name(self.__class__.__name__)\n         self.record_history = record_history\n\n@@ -416,9 +416,7 @@ def standardize_dtype(dtype):\n     return dtype\n \n \n-def standardize_shape(\n-    shape, allow_dynamic_batch_size=True, allow_all_dynamic=True\n-):\n+def standardize_shape(shape):\n     if not isinstance(shape, tuple):\n         if shape is None:\n             raise ValueError(\"Undefined shapes are not supported.\")\n@@ -431,23 +429,14 @@ def standardize_shape(\n         # either int or `None`\n         shape = tuple(map(lambda x: int(x) if x is not None else None, shape))\n \n-    for i, e in enumerate(shape):\n-        if i == 0 and allow_dynamic_batch_size and e is None:\n-            continue\n-        if allow_all_dynamic and e is None:\n+    for e in shape:\n+        if e is None:\n             continue\n         if not isinstance(e, int):\n-            msg = (\n+            raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n                 f\"Found invalid entry '{e}'. \"\n             )\n-            if not allow_dynamic_batch_size and e is None:\n-                msg += (\n-                    \"Dynamic shapes (shapes with `None` entries) \"\n-                    f\"are not allowed with the {config.backend()} \"\n-                    \"backend.\"\n-                )\n-            raise ValueError(msg)\n         if e < 0:\n             raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n\n@@ -4,7 +4,6 @@ from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n-from keras_core.backend.jax.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n\n@@ -13,8 +13,6 @@ from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.jax import distribution\n from keras_core.utils.nest import pack_sequence_as\n \n-DYNAMIC_SHAPES_OK = True\n-\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n\n@@ -4,7 +4,6 @@ from keras_core.backend.numpy import math\n from keras_core.backend.numpy import nn\n from keras_core.backend.numpy import numpy\n from keras_core.backend.numpy import random\n-from keras_core.backend.numpy.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.numpy.core import Variable\n from keras_core.backend.numpy.core import cast\n from keras_core.backend.numpy.core import compute_output_spec\n\n@@ -6,8 +6,6 @@ from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n \n-DYNAMIC_SHAPES_OK = True\n-\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n\n@@ -5,7 +5,6 @@ from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n from keras_core.backend.tensorflow import tensorboard\n-from keras_core.backend.tensorflow.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n\n@@ -12,8 +12,6 @@ from keras_core.backend.common.name_scope import name_scope as base_name_scope\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.naming import auto_name\n \n-DYNAMIC_SHAPES_OK = True\n-\n \n class Variable(\n     KerasVariable,\n\n@@ -20,7 +20,6 @@ from keras_core.backend.torch import math\n from keras_core.backend.torch import nn\n from keras_core.backend.torch import numpy\n from keras_core.backend.torch import random\n-from keras_core.backend.torch.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n\n@@ -12,7 +12,6 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.nest import pack_sequence_as\n \n-DYNAMIC_SHAPES_OK = True\n # Some operators such as 'aten::_foreach_mul_.Scalar'\n # are not currently implemented for the MPS device.\n # check https://github.com/pytorch/pytorch/issues/77764.\n\n@@ -23,10 +23,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_add_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -118,10 +114,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_subtract_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -222,10 +214,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_minimum_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -317,10 +305,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_maximum_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -412,10 +396,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_multiply_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -507,10 +487,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_average_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -602,10 +578,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_concatenate_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -701,10 +673,6 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=None,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes.\",\n-    )\n     def test_dot_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4)\n         x2 = np.random.rand(2, 4)\n\n@@ -2,7 +2,6 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -63,10 +62,6 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_cropping_2d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 7, None, 5))\n         cropped = layers.Cropping2D(((1, 2), (3, 4)))(input_layer)\n\n@@ -2,7 +2,6 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -123,10 +122,6 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_cropping_3d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 7, None, 13, 5))\n         cropped = layers.Cropping3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -67,19 +66,11 @@ class FlattenTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_flatten_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 2, 3))\n         flattened = layers.Flatten()(input_layer)\n         self.assertEqual(flattened.shape, (None, 2 * 3))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_flatten_with_dynamic_dimension(self):\n         input_layer = layers.Input(batch_shape=(5, 2, None))\n         flattened = layers.Flatten()(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -21,10 +20,6 @@ class PermuteTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_permute_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 3, 5))\n         permuted = layers.Permute((2, 1))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -21,19 +20,11 @@ class FlattenTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_repeat_vector_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 5))\n         repeated = layers.RepeatVector(n=3)(input_layer)\n         self.assertEqual(repeated.shape, (None, 3, 5))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_repeat_vector_with_dynamic_dimension(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         repeated = layers.RepeatVector(n=3)(input_layer)\n\n@@ -1,6 +1,5 @@\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -56,19 +55,11 @@ class ReshapeTest(testing.TestCase):\n             expected_output_shape=(3, 2, 4),\n         )\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_reshape_with_dynamic_batch_size(self):\n         input_layer = layers.Input(shape=(2, 4))\n         reshaped = layers.Reshape((8,))(input_layer)\n         self.assertEqual(reshaped.shape, (None, 8))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_reshape_sets_static_shape(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         reshaped = layers.Reshape((3, 5))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n from keras_core.backend.common.keras_tensor import KerasTensor\n@@ -54,10 +53,6 @@ class UpSamplingTest(testing.TestCase):\n         self.assertEqual(layers.UpSampling1D(size=2)(x).shape, (None, 4, 3))\n         self.assertEqual(layers.UpSampling1D(size=4)(x).shape, (None, 8, 3))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_upsampling_1d_with_dynamic_shape(self):\n         y = KerasTensor([2, None, 3])\n         self.assertEqual(layers.UpSampling1D(size=2)(y).shape, (2, None, 3))\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -25,10 +23,6 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n             self.assertAllClose(outputs[:, index, :], 0.0)\n         self.assertAllClose(outputs[:, 2:-2, :], inputs)\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n         padded = layers.ZeroPadding1D((1, 2))(input_layer)\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -58,10 +56,6 @@ class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(outputs[:, :, index, :], 0.0)\n             self.assertAllClose(outputs[:, 2:-2, 2:-2, :], inputs)\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_zero_padding_2d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 2, None, 4))\n         padded = layers.ZeroPadding2D(((1, 2), (3, 4)))(input_layer)\n\n@@ -1,8 +1,6 @@\n import numpy as np\n-import pytest\n from absl.testing import parameterized\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -64,10 +62,6 @@ class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n             self.assertAllClose(outputs[:, 2:-2, 2:-2, 2:-2, :], inputs)\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic batch sizes\",\n-    )\n     def test_zero_padding_3d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 2, None, 4, 5))\n         padded = layers.ZeroPadding3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n\n@@ -1,7 +1,6 @@\n import numpy as np\n import pytest\n \n-from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -206,10 +205,6 @@ class RNNTest(testing.TestCase):\n         self.assertEqual(output_shape[1], (3, 8))\n         self.assertEqual(output_shape[2], (3, 8))\n \n-    @pytest.mark.skipif(\n-        not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Backend does not support dynamic shapes\",\n-    )\n     def test_dynamic_shapes(self):\n         sequence_shape = (None, None, 3)\n         layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9fafdb88092e7ebb86509200c23b47109c62ce6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 20 | Files Changed: 25 | Hunks: 59 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 163 | Churn Cumulative: 10650 | Contributors (this commit): 27 | Commits (past 90d): 260 | Contributors (cumulative): 131 | DMM Complexity: 0.26666666666666666\n\nDIFF:\n@@ -31,7 +31,11 @@ class KerasTensor:\n     def __init__(self, shape, dtype=\"float32\", record_history=True, name=None):\n         from keras_core import backend\n \n-        self.shape = backend.standardize_shape(shape)\n+        shape = backend.standardize_shape(\n+            shape,\n+            allow_all_dynamic=backend.DYNAMIC_SHAPES_OK,\n+        )\n+        self.shape = shape\n         self.dtype = backend.standardize_dtype(dtype)\n         self.name = name or auto_name(self.__class__.__name__)\n         self.record_history = record_history\n\n@@ -416,7 +416,9 @@ def standardize_dtype(dtype):\n     return dtype\n \n \n-def standardize_shape(shape):\n+def standardize_shape(\n+    shape, allow_dynamic_batch_size=True, allow_all_dynamic=True\n+):\n     if not isinstance(shape, tuple):\n         if shape is None:\n             raise ValueError(\"Undefined shapes are not supported.\")\n@@ -429,14 +431,23 @@ def standardize_shape(shape):\n         # either int or `None`\n         shape = tuple(map(lambda x: int(x) if x is not None else None, shape))\n \n-    for e in shape:\n-        if e is None:\n+    for i, e in enumerate(shape):\n+        if i == 0 and allow_dynamic_batch_size and e is None:\n+            continue\n+        if allow_all_dynamic and e is None:\n             continue\n         if not isinstance(e, int):\n-            raise ValueError(\n+            msg = (\n                 f\"Cannot convert '{shape}' to a shape. \"\n                 f\"Found invalid entry '{e}'. \"\n             )\n+            if not allow_dynamic_batch_size and e is None:\n+                msg += (\n+                    \"Dynamic shapes (shapes with `None` entries) \"\n+                    f\"are not allowed with the {config.backend()} \"\n+                    \"backend.\"\n+                )\n+            raise ValueError(msg)\n         if e < 0:\n             raise ValueError(\n                 f\"Cannot convert '{shape}' to a shape. \"\n\n@@ -4,6 +4,7 @@ from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n+from keras_core.backend.jax.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.jax.core import Variable\n from keras_core.backend.jax.core import cast\n from keras_core.backend.jax.core import compute_output_spec\n\n@@ -13,6 +13,8 @@ from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.jax import distribution\n from keras_core.utils.nest import pack_sequence_as\n \n+DYNAMIC_SHAPES_OK = True\n+\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n\n@@ -4,6 +4,7 @@ from keras_core.backend.numpy import math\n from keras_core.backend.numpy import nn\n from keras_core.backend.numpy import numpy\n from keras_core.backend.numpy import random\n+from keras_core.backend.numpy.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.numpy.core import Variable\n from keras_core.backend.numpy.core import cast\n from keras_core.backend.numpy.core import compute_output_spec\n\n@@ -6,6 +6,8 @@ from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n \n+DYNAMIC_SHAPES_OK = True\n+\n \n class Variable(KerasVariable):\n     def _initialize(self, value):\n\n@@ -5,6 +5,7 @@ from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n from keras_core.backend.tensorflow import tensorboard\n+from keras_core.backend.tensorflow.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.tensorflow.core import Variable\n from keras_core.backend.tensorflow.core import cast\n from keras_core.backend.tensorflow.core import compute_output_spec\n\n@@ -12,6 +12,8 @@ from keras_core.backend.common.name_scope import name_scope as base_name_scope\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.naming import auto_name\n \n+DYNAMIC_SHAPES_OK = True\n+\n \n class Variable(\n     KerasVariable,\n\n@@ -20,6 +20,7 @@ from keras_core.backend.torch import math\n from keras_core.backend.torch import nn\n from keras_core.backend.torch import numpy\n from keras_core.backend.torch import random\n+from keras_core.backend.torch.core import DYNAMIC_SHAPES_OK\n from keras_core.backend.torch.core import Variable\n from keras_core.backend.torch.core import cast\n from keras_core.backend.torch.core import compute_output_spec\n\n@@ -12,6 +12,7 @@ from keras_core.backend.common.keras_tensor import KerasTensor\n from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.utils.nest import pack_sequence_as\n \n+DYNAMIC_SHAPES_OK = True\n # Some operators such as 'aten::_foreach_mul_.Scalar'\n # are not currently implemented for the MPS device.\n # check https://github.com/pytorch/pytorch/issues/77764.\n\n@@ -23,6 +23,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_add_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -114,6 +118,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_subtract_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -214,6 +222,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_minimum_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -305,6 +317,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_maximum_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -396,6 +412,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_multiply_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -487,6 +507,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_average_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -578,6 +602,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=True,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_concatenate_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n         x2 = np.random.rand(2, 4, 5)\n@@ -673,6 +701,10 @@ class MergingLayersTest(testing.TestCase):\n             supports_masking=None,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n     def test_dot_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4)\n         x2 = np.random.rand(2, 4)\n\n@@ -141,7 +141,7 @@ class HashedCrossing(Layer):\n         self._check_input_shape_and_type(inputs)\n \n         # Uprank to rank 2 for the cross_hashed op.\n-        rank = inputs[0].shape.rank\n+        rank = len(inputs[0].shape)\n         if rank < 2:\n             inputs = [tf_utils.expand_dims(x, -1) for x in inputs]\n         if rank < 1:\n@@ -199,7 +199,7 @@ class HashedCrossing(Layer):\n             )\n \n     def _check_input_shape_and_type(self, inputs):\n-        first_shape = inputs[0].shape.as_list()\n+        first_shape = tuple(inputs[0].shape)\n         rank = len(first_shape)\n         if rank > 2 or (rank == 2 and first_shape[-1] != 1):\n             raise ValueError(\n@@ -207,7 +207,7 @@ class HashedCrossing(Layer):\n                 \"`(batch_size)` or `(batch_size, 1)`. \"\n                 f\"Received: inputs={inputs}\"\n             )\n-        if not all(x.shape.as_list() == first_shape for x in inputs[1:]):\n+        if not all(tuple(x.shape) == first_shape for x in inputs[1:]):\n             raise ValueError(\n                 \"All `HashedCrossing` inputs should have equal shape. \"\n                 f\"Received: inputs={inputs}\"\n@@ -219,7 +219,7 @@ class HashedCrossing(Layer):\n                 \"All `HashedCrossing` inputs should be dense tensors. \"\n                 f\"Received: inputs={inputs}\"\n             )\n-        if not all(x.dtype.is_integer or x.dtype == tf.string for x in inputs):\n+        if not all(tf.as_dtype(x.dtype).is_integer or x.dtype == tf.string for x in inputs):\n             raise ValueError(\n                 \"All `HashedCrossing` inputs should have an integer or \"\n                 f\"string dtype. Received: inputs={inputs}\"\n\n@@ -2,6 +2,7 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -62,6 +63,10 @@ class Cropping2DTest(testing.TestCase, parameterized.TestCase):\n             expected_output=expected_output,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_cropping_2d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 7, None, 5))\n         cropped = layers.Cropping2D(((1, 2), (3, 4)))(input_layer)\n\n@@ -2,6 +2,7 @@ import numpy as np\n import pytest\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -122,6 +123,10 @@ class Cropping3DTest(testing.TestCase, parameterized.TestCase):\n             expected_output=expected_output,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_cropping_3d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 7, None, 13, 5))\n         cropped = layers.Cropping3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -66,11 +67,19 @@ class FlattenTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_flatten_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 2, 3))\n         flattened = layers.Flatten()(input_layer)\n         self.assertEqual(flattened.shape, (None, 2 * 3))\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_flatten_with_dynamic_dimension(self):\n         input_layer = layers.Input(batch_shape=(5, 2, None))\n         flattened = layers.Flatten()(input_layer)\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -20,6 +21,10 @@ class PermuteTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_permute_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 3, 5))\n         permuted = layers.Permute((2, 1))(input_layer)\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -20,11 +21,19 @@ class FlattenTest(testing.TestCase):\n             expected_output=expected_output,\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_repeat_vector_with_dynamic_batch_size(self):\n         input_layer = layers.Input(batch_shape=(None, 5))\n         repeated = layers.RepeatVector(n=3)(input_layer)\n         self.assertEqual(repeated.shape, (None, 3, 5))\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_repeat_vector_with_dynamic_dimension(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         repeated = layers.RepeatVector(n=3)(input_layer)\n\n@@ -1,5 +1,6 @@\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -55,11 +56,19 @@ class ReshapeTest(testing.TestCase):\n             expected_output_shape=(3, 2, 4),\n         )\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_reshape_with_dynamic_batch_size(self):\n         input_layer = layers.Input(shape=(2, 4))\n         reshaped = layers.Reshape((8,))(input_layer)\n         self.assertEqual(reshaped.shape, (None, 8))\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_reshape_sets_static_shape(self):\n         input_layer = layers.Input(batch_shape=(2, None))\n         reshaped = layers.Reshape((3, 5))(input_layer)\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n from keras_core.backend.common.keras_tensor import KerasTensor\n@@ -53,6 +54,10 @@ class UpSamplingTest(testing.TestCase):\n         self.assertEqual(layers.UpSampling1D(size=2)(x).shape, (None, 4, 3))\n         self.assertEqual(layers.UpSampling1D(size=4)(x).shape, (None, 8, 3))\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_upsampling_1d_with_dynamic_shape(self):\n         y = KerasTensor([2, None, 3])\n         self.assertEqual(layers.UpSampling1D(size=2)(y).shape, (2, None, 3))\n\n@@ -1,6 +1,8 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -23,6 +25,10 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n             self.assertAllClose(outputs[:, index, :], 0.0)\n         self.assertAllClose(outputs[:, 2:-2, :], inputs)\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n         padded = layers.ZeroPadding1D((1, 2))(input_layer)\n\n@@ -1,6 +1,8 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -56,6 +58,10 @@ class ZeroPadding2DTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(outputs[:, :, index, :], 0.0)\n             self.assertAllClose(outputs[:, 2:-2, 2:-2, :], inputs)\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_zero_padding_2d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 2, None, 4))\n         padded = layers.ZeroPadding2D(((1, 2), (3, 4)))(input_layer)\n\n@@ -1,6 +1,8 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -62,6 +64,10 @@ class ZeroPadding3DTest(testing.TestCase, parameterized.TestCase):\n                 self.assertAllClose(outputs[:, :, :, index, :], 0.0)\n             self.assertAllClose(outputs[:, 2:-2, 2:-2, 2:-2, :], inputs)\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic batch sizes\",\n+    )\n     def test_zero_padding_3d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, 2, None, 4, 5))\n         padded = layers.ZeroPadding3D(((1, 2), (3, 4), (5, 6)))(input_layer)\n\n@@ -1,6 +1,7 @@\n import numpy as np\n import pytest\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import ops\n from keras_core import testing\n@@ -205,6 +206,10 @@ class RNNTest(testing.TestCase):\n         self.assertEqual(output_shape[1], (3, 8))\n         self.assertEqual(output_shape[2], (3, 8))\n \n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n     def test_dynamic_shapes(self):\n         sequence_shape = (None, None, 3)\n         layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n\n@@ -32,15 +32,11 @@ def relu(x):\n     Returns:\n         A tensor with the same shape as `x`.\n \n-    Examples:\n+    Example:\n \n-    >>> x1 = keras_core.ops.convert_to_tensor([-1.0, 0.0, 1.0])\n+    >>> x1 = keras_core.ops.convert_to_tensor([-1.0, 0.0, 1.0, 0.2])\n     >>> keras_core.ops.relu(x1)\n-    array([0.0, 0.0, 1.0], dtype=float32)\n-    >>> x2 = keras_core.ops.convert_to_tensor([-100.0, 0.0, 100.0])\n-    >>> keras_core.ops.relu(x2)\n-    array([0.0, 0.0, 100.0], dtype=float32)\n-\n+    array([0.0, 0.0, 1.0, 0.2], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Relu().symbolic_call(x)\n@@ -69,10 +65,9 @@ def relu6(x):\n \n     Example:\n \n-    >>> x = keras_core.ops.convert_to_tensor([-66.0, -6.0, 0.0, 6.0, 66.0])\n+    >>> x = keras_core.ops.convert_to_tensor([-3.0, -2.0, 0.1, 0.2, 6.0, 8.0])\n     >>> keras_core.ops.relu6(x)\n-    array([0.0, 0.0, 0.0, 6.0, 6.0], dtype=float32)\n-\n+    array([0.0, 0.0, 0.1, 0.2, 6.0, 6.0], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Relu6().symbolic_call(x)\n\n@@ -141,7 +141,7 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n \n def ensure_tensor(inputs, dtype=None):\n     \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n-    if isinstance(inputs, (list, np.ndarray)):\n+    if not isinstance(inputs, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)):\n         inputs = tf.convert_to_tensor(inputs, dtype)\n     if dtype is not None and inputs.dtype != dtype:\n         inputs = tf.cast(inputs, dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
