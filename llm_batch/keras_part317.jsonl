{"custom_id": "keras#2fde2bec570a12bd2c8e5a8a7bf301b21e58dc05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 7026 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -276,6 +276,8 @@ def clear_session():\n   _GRAPH.graph = None\n   tf.compat.v1.reset_default_graph()\n   reset_uids()\n+  if _SESSION.session is not None:\n+    _SESSION.session.close()\n     _SESSION.session = None\n   graph = get_graph()\n   with graph.as_default():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#488bda8fc8e699471263bc0bf5f5574326559894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 16 | Files Changed: 9 | Hunks: 15 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/1 | Churn Δ: 100 | Churn Cumulative: 17703 | Contributors (this commit): 116 | Commits (past 90d): 38 | Contributors (cumulative): 142 | DMM Complexity: 1.0\n\nDIFF:\n@@ -653,7 +653,8 @@ class Callback(object):\n         logs: Dict, metric results for this training epoch, and for the\n           validation epoch if validation is performed. Validation result keys\n           are prefixed with `val_`. For training epoch, the values of the\n-         `Model`'s metrics are returned. Example : `{'loss': 0.2, 'acc': 0.7}`.\n+         `Model`'s metrics are returned. Example : `{'loss': 0.2, 'accuracy':\n+           0.7}`.\n     \"\"\"\n \n   @doc_controls.for_subclass_implementers\n\n@@ -318,7 +318,7 @@ class Multiply(_Merge):\n   def _merge_function(self, inputs):\n     output = inputs[0]\n     for i in range(1, len(inputs)):\n-      output *= inputs[i]\n+      output = output * inputs[i]\n     return output\n \n \n\n@@ -46,7 +46,7 @@ class BatchNormalizationBase(Layer):\n   with the argument `training=True`), the layer normalizes its output using\n   the mean and standard deviation of the current batch of inputs. That is to\n   say, for each channel being normalized, the layer returns\n-  `(batch - mean(batch)) / (var(batch) + epsilon) * gamma + beta`, where:\n+  `gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta`, where:\n \n   - `epsilon` is small constant (configurable as part of the constructor\n   arguments)\n@@ -60,7 +60,7 @@ class BatchNormalizationBase(Layer):\n   default), the layer normalizes its output using a moving average of the\n   mean and standard deviation of the batches it has seen during training. That\n   is to say, it returns\n-  `(batch - self.moving_mean) / (self.moving_var + epsilon) * gamma + beta`.\n+  `gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta`.\n \n   `self.moving_mean` and `self.moving_var` are non-trainable variables that\n   are updated each time the layer in called in training mode, as such:\n\n@@ -301,11 +301,10 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n           (self.mask_token, self.oov_token, self.oov_token,\n            oov_start, oov_end, self.mask_token, vocab[0]))\n \n-    insert_special_tokens = not has_oov and not has_mask\n-\n     special_tokens = [] if self.mask_token is None else [self.mask_token]\n     special_tokens.extend([self.oov_token] * self.num_oov_indices)\n \n+    insert_special_tokens = special_tokens and not has_oov and not has_mask\n     num_special_tokens = len(special_tokens)\n     tokens = vocab if insert_special_tokens else vocab[num_special_tokens:]\n     if self.mask_token in tokens:\n@@ -321,25 +320,24 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n                        \"OOV token for this layer.\" %\n                        (self.oov_token, tokens.index(self.oov_token)))\n \n-    if insert_special_tokens:\n-      total_vocab_size = len(vocab) + num_special_tokens\n-    else:\n-      total_vocab_size = len(vocab)\n+    total_vocab_size = len(tokens) + num_special_tokens\n     if self.max_tokens is not None and total_vocab_size > self.max_tokens:\n       raise ValueError(\n           \"Attempted to set a vocabulary larger than the maximum vocab size. \"\n           \"Passed vocab size is %s, max vocab size is %s.\" %\n           (total_vocab_size, self.max_tokens))\n \n-    start_index = num_special_tokens\n-    values = np.arange(start_index, len(vocab) + start_index, dtype=np.int64)\n-\n     self._table_handler.clear()\n-    self._table_handler.insert(vocab, values)\n-\n-    if insert_special_tokens and num_special_tokens > 0:\n+    if insert_special_tokens:\n+      start_index = num_special_tokens\n+      values = np.arange(start_index, len(tokens) + start_index, dtype=np.int64)\n+      self._table_handler.insert(tokens, values)\n       special_token_values = np.arange(num_special_tokens, dtype=np.int64)\n       self._table_handler.insert(special_tokens, special_token_values)\n+    else:\n+      values = np.arange(len(vocab), dtype=np.int64)\n+      self._table_handler.insert(vocab, values)\n+\n     return total_vocab_size\n \n   def _set_inverse_vocabulary(self, vocab):\n\n@@ -685,6 +685,25 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_int_output_explicit_vocab_with_special_tokens(self):\n+    vocab_data = [\"\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        vocabulary=vocab_data,\n+        max_tokens=None,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        dtype=tf.string)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n   def test_vocab_with_max_cap(self):\n     vocab_data = [\"\", \"[OOV]\", \"wind\", \"and\", \"fire\"]\n     layer = get_layer_class()(\n\n@@ -339,6 +339,21 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_int_output_explicit_vocab_with_special_tokens(self):\n+    vocab_data = [0, -1, 42, 1138, 725, 1729]\n+    input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n+    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.int64)\n+    layer = get_layer_class()(\n+        vocabulary=vocab_data,\n+        max_values=None,\n+    )\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n   def test_inverse_output(self):\n     vocab_data = [0, -1, 42, 1138, 725, 1729]\n     input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])\n\n@@ -148,6 +148,19 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     output_data = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_data)\n \n+  def test_int_output_explicit_vocab_with_special_tokens(self):\n+    vocab_data = [\"\", \"[UNK]\", \"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(vocabulary=vocab_data)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+    output_data = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_data)\n+\n   def test_no_vocab(self):\n     with self.assertRaisesRegex(\n         ValueError, \"You must set the layer's vocabulary\"):\n\n@@ -722,6 +722,26 @@ class TextVectorizationPreprocessingTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_vocab_setting_with_oov_via_setter(self):\n+    vocab_data = [\"\", \"[UNK]\", \"earth\", \"wind\", \"and\", \"fire\"]\n+    input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n+                            [\"fire\", \"and\", \"earth\", \"michigan\"]])\n+    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+\n+    vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_data)\n+    input_data = keras.Input(shape=(None,), dtype=tf.string)\n+    layer = get_layer_class()(\n+        max_tokens=None,\n+        standardize=None,\n+        split=None,\n+        output_mode=text_vectorization.INT)\n+    layer.set_vocabulary(vocab_path)\n+    int_data = layer(input_data)\n+    model = keras.Model(inputs=input_data, outputs=int_data)\n+\n+    output_dataset = model.predict(input_array)\n+    self.assertAllEqual(expected_output, output_dataset)\n+\n \n @keras_parameterized.run_all_keras_modes\n class TextVectorizationDistributionTest(\n\n@@ -1104,6 +1104,8 @@ def infer_inputs_from_restored_call_function(fn):\n     common_shape = get_common_shape(x.shape, y.shape)\n     if isinstance(x, tf.SparseTensorSpec):\n       return tf.SparseTensorSpec(common_shape, x.dtype)\n+    elif isinstance(x, tf.RaggedTensorSpec):\n+      return tf.RaggedTensorSpec(common_shape, x.dtype)\n     return tf.TensorSpec(common_shape, x.dtype, x.name)\n \n   spec = fn.concrete_functions[0].structured_input_signature[0][0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94968bafb725aaf7a51ee0f5692c1715c152b925", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 58 | Churn Cumulative: 475 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,8 +74,11 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n           num_oov_indices=0, mask_token=None)\n       label_lookup_layer.adapt(LABEL_VOCAB)\n     else:\n+      # Do vocab shuffling.\n+      shuffled_vocab = FEATURE_VOCAB.copy()\n+      random.shuffle(shuffled_vocab)\n       feature_lookup_layer = string_lookup.StringLookup(\n-          vocabulary=FEATURE_VOCAB, num_oov_indices=1)\n+          vocabulary=shuffled_vocab, num_oov_indices=1)\n       label_lookup_layer = string_lookup.StringLookup(\n           vocabulary=LABEL_VOCAB, num_oov_indices=0, mask_token=None)\n \n\n@@ -19,13 +19,12 @@ from __future__ import division\n from __future__ import print_function\n \n import random\n+import tempfile\n \n from absl import flags\n \n import tensorflow as tf\n \n-preproc_layer = tf.keras.layers.experimental.preprocessing\n-\n FLAGS = flags.FLAGS\n flags.DEFINE_string(\"tpu\", \"\", \"Name of TPU to connect to.\")\n flags.DEFINE_string(\"project\", None, \"Name of GCP project with TPU.\")\n@@ -88,6 +87,16 @@ class TpuStrategyTest(tf.test.TestCase):\n \n     return feature_mapper, label_mapper\n \n+  def define_inverse_lookup_layer(self):\n+    # Only needed for serving.\n+    label_inverse_lookup_layer = (\n+        tf.keras.layers.experimental.preprocessing.StringLookup(\n+            num_oov_indices=1,\n+            mask_token=None,\n+            vocabulary=LABEL_VOCAB,\n+            invert=True))\n+    return label_inverse_lookup_layer\n+\n   def test_keras_metric_outside_strategy_scope_per_replica(self):\n     strategy = get_tpu_strategy()\n     metric = tf.keras.metrics.Mean(\"test_metric\", dtype=tf.float32)\n@@ -184,7 +193,45 @@ class TpuStrategyTest(tf.test.TestCase):\n       self.assertGreater(accuracy.result().numpy(), 0.5)\n       self.assertEqual(optimizer.iterations.numpy(), num_epochs * num_steps)\n \n-      # TODO(b/178495959): Add tests that cover the serving phase.\n+      # Create a saved model.\n+      model.feature_mapper = feature_mapper\n+      model.label_mapper = label_mapper\n+      model.label_inverse_lookup_layer = self.define_inverse_lookup_layer()\n+\n+      def create_serving_signature(model):\n+\n+        @tf.function\n+        def serve_fn(raw_features):\n+          raw_features = tf.expand_dims(raw_features, axis=0)\n+          transformed_features = model.feature_mapper(raw_features)\n+          outputs = model(transformed_features)\n+          outputs = tf.squeeze(outputs, axis=0)\n+          outputs = tf.cast(tf.math.greater(outputs, 0.5), tf.dtypes.int64)\n+          decoded_outputs = model.label_inverse_lookup_layer(outputs)\n+          return tf.squeeze(decoded_outputs, axis=0)\n+\n+        # Serving does NOT have batch dimension\n+        return serve_fn.get_concrete_function(\n+            tf.TensorSpec(shape=(3), dtype=tf.dtypes.string, name=\"example\"))\n+\n+      serving_fn = create_serving_signature(model)\n+\n+      saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n+      tf.saved_model.save(\n+          model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+\n+    # Test the saved_model.\n+    loaded_serving_fn = tf.keras.models.load_model(\n+        saved_model_dir).signatures[\"serving_default\"]\n+\n+    # Check model calling with serving signature.\n+    prediction1 = loaded_serving_fn(\n+        tf.constant([\"avenger\", \"ironman\", \"avenger\"]))[\"output_0\"]\n+    self.assertIn(prediction1, (\"yes\", \"no\"))\n+\n+    prediction2 = loaded_serving_fn(\n+        tf.constant([\"ironman\", \"ironman\", \"unkonwn\"]))[\"output_0\"]\n+    self.assertIn(prediction2, (\"yes\", \"no\"))\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9daa18100c5f11e90c55609602d9ba75673a7da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 600 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -21,8 +21,8 @@ import tensorflow as tf\n \n import threading\n \n+from tensorflow.python.distribute import distribute_utils\n from tensorflow.python.distribute import ps_values as ps_distribute_values\n-from tensorflow.python.distribute import values as distribute_values\n from tensorflow.python.types import core\n \n \n@@ -510,8 +510,8 @@ def create_autocast_variable(variable):\n   Returns:\n     An AutoCastVariable that wraps the variable.\n   \"\"\"\n-  if not isinstance(variable, (distribute_values.DistributedVariable,\n-                               ps_distribute_values.AggregatingVariable)):\n+  if (not distribute_utils.is_distributed_variable(variable) and\n+      not isinstance(variable, ps_distribute_values.AggregatingVariable)):\n     return AutoCastVariable(variable)\n \n   class AutoCastDistributedVariable(AutoCastVariable, variable.__class__):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c3dee9c89091bd2aadd4fe2177b3e7f211bd75a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 25 | Churn Cumulative: 4072 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.7391304347826086\n\nDIFF:\n@@ -617,10 +617,16 @@ class Functional(training_lib.Model):\n         # Should squeeze last dimension.\n         # True if tensor is (BATCH, ..., 1) and reference is (BATCH, ...).\n         if (t_rank == ref_rank + 1 and t_shape[-1] == 1):\n+          if t_rank >= 2 and (\n+              t_shape[-2] is None or ref_shape[-1] is None or\n+              t_shape[-2] == ref_shape[-1]):\n             tensor = tf.squeeze(tensor, axis=-1)\n         # Should expand last_dimension.\n         # True if tensor is (BATCH, ...) and reference is (BATCH, ..., 1).\n         elif (t_rank == ref_rank - 1 and ref_shape[-1] == 1):\n+          if ref_rank >= 2 and (\n+              ref_shape[-2] is None or t_shape[-1] is None or\n+              ref_shape[-2] == t_shape[-1]):\n             tensor = tf.expand_dims(tensor, axis=-1)\n       if keras_history is not None:  # Restore keras history.\n         tensor._keras_history = keras_history\n\n@@ -48,6 +48,25 @@ except ImportError:\n \n class NetworkConstructionTest(keras_parameterized.TestCase):\n \n+  def test_last_1_dim_squeeze_behavior(self):\n+    inputs = input_layer_lib.Input(shape=(3, 1))\n+    outputs = layers.Dense(2)(inputs)\n+    model = training_lib.Model(inputs=inputs, outputs=outputs)\n+    x = np.zeros((4, 3))  # Missing last \"1\" dimension\n+    self.assertEqual(list(model(x).shape), [4, 3, 2]) # Works\n+    with self.assertRaisesRegex(ValueError, 'incompatible with layer'):\n+      x = np.zeros((4, 5))  # Missing last \"1\" dimension, but bad shape\n+      model(x)\n+\n+    inputs = input_layer_lib.Input(shape=(3,))\n+    outputs = layers.Dense(2)(inputs)\n+    model = training_lib.Model(inputs=inputs, outputs=outputs)\n+    x = np.zeros((4, 3, 1))  # Added last \"1\" dimension\n+    self.assertEqual(list(model(x).shape), [4, 2]) # Works\n+    with self.assertRaisesRegex(ValueError, 'incompatible with layer'):\n+      x = np.zeros((4, 5, 1))  # Added last \"1\" dimension, but bad shape\n+      model(x)\n+\n   def test_default_model_name(self):\n     inputs = input_layer_lib.Input(shape=(1,))\n     outputs = layers.Dense(1, activation='relu')(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc41fb9ed228480b337904d8d21edaceb1f1b5ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 149 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -23,20 +23,9 @@ import fnmatch\n import os\n \n PIP_EXCLUDED_FILES = frozenset([\n-    'keras/combinations.py',\n     'keras/api/create_python_api_wrapper.py',\n     'keras/applications/efficientnet_weight_update_util.py',\n-    'keras/distribute/keras_correctness_test_base.py',\n-    'keras/distribute/model_collection_base.py',\n-    'keras/distribute/model_combinations.py',\n-    'keras/distribute/multi_worker_testing_utils.py',\n-    'keras/distribute/optimizer_combinations.py',\n-    'keras/distribute/saved_model_test_base.py',\n-    'keras/distribute/simple_models.py',\n-    'keras/distribute/strategy_combinations.py',\n     'keras/distribute/tpu_strategy_test_utils.py',\n-    'keras/distribute/test_example.py',\n-    'keras/mixed_precision/test_util.py',\n     'keras/tools/pip_package/setup.py',\n     'keras/tools/pip_package/create_pip_helper.py',\n ])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f8c18b9aa804c8b099eb5714aebd9620d8f5a27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 583 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -351,9 +351,7 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n       # mode. Variable.assign(...).op is None in Eager mode and an op in Graph\n       # mode or a tf.function. We test this is also true of AutoCastVariable.\n       if tf.executing_eagerly():\n-        with self.assertRaisesRegex(\n-            AttributeError,\n-            'Tensor.op is meaningless when eager execution is enabled'):\n+        with self.assertRaises(AttributeError):\n           x.op  # pylint: disable=pointless-statement\n         self.assertIsNone(x.assign(1.0).op)\n         self.assertIsNone(x.assign_add(1.0).op)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb87e95b4916784e1727f1c4cd3fec4914a9a59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 629 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -309,7 +309,7 @@ def compare_results(results_with_ds,\n     default_tolerance = 1e-3\n     relaxed_tolerance = 1e-3\n   else:\n-    default_tolerance = 1e-5\n+    default_tolerance = 4e-5\n     relaxed_tolerance = 1e-4\n \n   def _get_compare_result_tolerance(key):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c1de6aed0daeadcdf417f181f9f81cf482e677a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 505 | Lines Deleted: 339 | Files Changed: 16 | Hunks: 74 | Methods Changed: 57 | Complexity Δ (Sum/Max): 49/49 | Churn Δ: 844 | Churn Cumulative: 54291 | Contributors (this commit): 113 | Commits (past 90d): 70 | Contributors (cumulative): 163 | DMM Complexity: 0.40625\n\nDIFF:\n@@ -38,7 +38,6 @@ from tensorflow.core.protobuf import config_pb2\n from tensorflow.python.distribute import distribute_coordinator as dc\n from tensorflow.python.distribute import distribute_coordinator_context as dc_context\n from tensorflow.python.eager import function as eager_function\n-from tensorflow.python.eager import lift_to_graph\n from tensorflow.python.eager.context import get_config\n from tensorflow.python.framework import config\n from tensorflow.python.framework import ops\n@@ -4090,7 +4089,7 @@ def eval_in_eager_or_function(outputs):\n \n     if source_graph is global_graph and exec_graph is not global_graph:\n       init_tensors = outputs\n-      lifted_map = lift_to_graph.lift_to_graph(\n+      lifted_map = tf.__internal__.lift_to_graph(\n           tensors=init_tensors,\n           graph=exec_graph,\n           sources=[],\n\n@@ -33,10 +33,7 @@ import six\n from six.moves import zip  # pylint: disable=redefined-builtin\n \n from google.protobuf import json_format\n-from tensorflow.python.autograph.core import ag_ctx\n-from tensorflow.python.autograph.impl import api as autograph\n from tensorflow.python.eager import execute\n-from tensorflow.python.eager import monitoring\n from keras import backend\n from keras import constraints\n from keras import initializers\n@@ -79,13 +76,13 @@ _TF_OP_LAYER_NAME_PREFIX = 'tf_op_layer_'\n _AUTOCAST_TYPES = (tf.Tensor, tf.SparseTensor,\n                    tf.RaggedTensor)\n \n-keras_layers_gauge = monitoring.BoolGauge('/tensorflow/api/oss-keras/layers',\n+keras_layers_gauge = tf.__internal__.monitoring.BoolGauge('/tensorflow/api/oss-keras/layers',\n                                           'keras layers usage', 'method')\n-keras_models_gauge = monitoring.BoolGauge(\n+keras_models_gauge = tf.__internal__.monitoring.BoolGauge(\n     '/tensorflow/api/oss-keras/models', 'keras model usage', 'method')\n-keras_api_gauge = monitoring.BoolGauge('/tensorflow/api/oss-keras',\n+keras_api_gauge = tf.__internal__.monitoring.BoolGauge('/tensorflow/api/oss-keras',\n                                        'keras api usage', 'method')\n-keras_premade_model_gauge = monitoring.BoolGauge(\n+keras_premade_model_gauge = tf.__internal__.monitoring.BoolGauge(\n     '/tensorflow/api/oss-keras/premade_models', 'premade keras model usage', 'type')\n \n \n@@ -831,7 +828,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # enclosing tf.function, if any.\n     if (base_layer_utils.is_subclassed(self) and\n         not base_layer_utils.from_saved_model(self)):\n-      call_fn = autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\n+      call_fn = tf.__internal__.autograph.tf_convert(self.call, tf.__internal__.autograph.control_status_ctx())\n \n     # We enter a scratch graph and build placeholder inputs inside of it that\n     # match the input args.\n@@ -955,7 +952,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     # Accept NumPy and scalar inputs by converting to Tensors.\n     if any(isinstance(x, (\n-        tf.experimental.numpy.ndarray, np.ndarray, float, int)) for x in input_list):\n+        tf.Tensor, np.ndarray, float, int)) for x in input_list):\n       inputs = tf.nest.map_structure(_convert_numpy_or_python_types, inputs)\n       input_list = tf.nest.flatten(inputs)\n \n@@ -1024,12 +1021,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     # Accept NumPy and scalar inputs by converting to Tensors.\n     if any(isinstance(x, (\n-        tf.experimental.numpy.ndarray, np.ndarray, float, int)) for x in input_list):\n+        tf.Tensor, np.ndarray, float, int)) for x in input_list):\n \n       def _convert_non_tensor(x):\n         # Don't call `ops.convert_to_tensor` on all `inputs` because\n         # `SparseTensors` can't be converted to `Tensor`.\n-        if isinstance(x, (tf.experimental.numpy.ndarray, np.ndarray, float, int)):\n+        if isinstance(x, (tf.Tensor, np.ndarray, float, int)):\n           return tf.convert_to_tensor(x)\n         return x\n \n@@ -1134,8 +1131,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n           # enclosing tf.function, if any.\n           if (base_layer_utils.is_subclassed(self) and\n               not base_layer_utils.from_saved_model(self)):\n-            call_fn = autograph.tf_convert(self.call,\n-                                           ag_ctx.control_status_ctx())\n+            call_fn = tf.__internal__.autograph.tf_convert(self.call,\n+                                           tf.__internal__.autograph.control_status_ctx())\n           else:\n             call_fn = self.call\n \n@@ -1234,7 +1231,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     # enclosing tf.function, if any.\n     if (base_layer_utils.is_subclassed(self) and\n         not base_layer_utils.from_saved_model(self)):\n-      return autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\n+      return tf.__internal__.autograph.tf_convert(self.call, tf.__internal__.autograph.control_status_ctx())\n     else:\n       return self.call\n \n@@ -3313,7 +3310,7 @@ def _in_functional_construction_mode(layer, inputs, args, kwargs, input_list):\n \n \n def _convert_numpy_or_python_types(x):\n-  if isinstance(x, (tf.experimental.numpy.ndarray, np.ndarray, float, int)):\n+  if isinstance(x, (tf.Tensor, np.ndarray, float, int)):\n     return tf.convert_to_tensor(x)\n   return x\n \n\n@@ -29,9 +29,6 @@ import warnings\n import numpy as np\n import six\n from six.moves import zip  # pylint: disable=redefined-builtin\n-\n-from tensorflow.python.autograph.core import ag_ctx\n-from tensorflow.python.autograph.impl import api as autograph\n from keras import backend\n from keras import constraints\n from keras import initializers\n@@ -758,8 +755,8 @@ class Layer(base_layer.Layer):\n           # enclosing tf.function, if any.\n           if (base_layer_utils.is_subclassed(self) and\n               not base_layer_utils.from_saved_model(self)):\n-            call_fn = autograph.tf_convert(\n-                self.call, ag_ctx.control_status_ctx())\n+            call_fn = tf.__internal__.autograph.tf_convert(\n+                self.call, tf.__internal__.autograph.control_status_ctx())\n           else:\n             call_fn = self.call\n \n\n@@ -26,7 +26,6 @@ import numpy as np\n import six\n \n from tensorflow.python.eager import context\n-from tensorflow.python.eager import monitoring\n from keras import backend as K\n from keras.engine import data_adapter\n from keras.engine.base_layer import Layer\n@@ -35,7 +34,7 @@ from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n \n-keras_kpl_gauge = monitoring.BoolGauge(\n+keras_kpl_gauge = tf.__internal__.monitoring.BoolGauge(\n     '/tensorflow/api/oss-keras/layers/preprocessing',\n     'keras preprocessing layers usage', 'method')\n \n\n@@ -30,7 +30,6 @@ import random\n import numpy as np\n import six\n from tensorflow.python.eager import context\n-from tensorflow.python.eager import monitoring\n from tensorflow.python.framework import smart_cond\n from keras import backend\n from keras.engine import training_utils\n@@ -39,7 +38,7 @@ from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n-keras_data_adapter_gauge = monitoring.BoolGauge(\n+keras_data_adapter_gauge = tf.__internal__.monitoring.BoolGauge(\n     \"/tensorflow/api/oss-keras/data_adapters\", \"keras data adapter usage\", \"method\")\n \n try:\n\n@@ -359,7 +359,7 @@ class Sequential(functional.Functional):\n     # If applicable, update the static input shape of the model.\n     if not self._has_explicit_input_shape:\n       if not tf.is_tensor(inputs) and not isinstance(\n-          inputs, tf.experimental.numpy.ndarray):\n+          inputs, tf.Tensor):\n         # This is a Sequential with mutiple inputs. This is technically an\n         # invalid use case of Sequential, but we tolerate it for backwards\n         # compatibility.\n\n@@ -18,14 +18,15 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from tensorflow.python.eager import monitoring\n+import tensorflow as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n # Keras has undeclared dependency on tensorflow/estimator:estimator_py.\n # As long as you depend //third_party/py/tensorflow:tensorflow target\n # everything will work as normal.\n \n-_model_to_estimator_usage_gauge = monitoring.BoolGauge(\n+_model_to_estimator_usage_gauge = tf.__internal__.monitoring.BoolGauge(\n     '/tensorflow/api/oss-keras/model_to_estimator',\n     'Whether tf.keras.estimator.model_to_estimator() is called.', 'version')\n \n\n@@ -29,7 +29,6 @@ import types as python_types\n import warnings\n \n import numpy as np\n-from tensorflow.python.eager import monitoring\n from keras import activations\n from keras import backend as K\n from keras import constraints\n@@ -52,7 +51,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n # TODO(b/168039935): track dropout rate to decide whether/how to make a\n # dropout rate fastpath.\n-keras_temporary_dropout_rate = monitoring.BoolGauge(\n+keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(\n     '/tensorflow/api/oss-keras/dropout/temp_rate_is_zero',\n     'Temporarily record if Keras dropout layer was created w/'\n     'constant rate = 0')\n\n@@ -45,6 +45,7 @@ _graph_options = tf.compat.v1.GraphOptions(rewrite_options=_rewrites)\n _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n \n \n+@testing_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n @keras_parameterized.run_all_keras_modes(config=_config)\n class GRUV2Test(keras_parameterized.TestCase):\n \n@@ -653,6 +654,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))\n \n \n+@testing_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n class GRULayerGradientTapeTest(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['eager']))\n@@ -680,6 +682,7 @@ class GRULayerGradientTapeTest(keras_parameterized.TestCase):\n       tape.gradient(loss, gru.variables)\n \n \n+@testing_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n @keras_parameterized.run_all_keras_modes(config=_config)\n class GRUGraphRewriteTest(keras_parameterized.TestCase):\n \n\n@@ -20,29 +20,14 @@ from __future__ import print_function\n \n import tensorflow as tf\n \n-import json\n-\n import numpy as np\n from keras import backend as K\n from keras.engine import base_preprocessing_layer\n from tensorflow.python.util.tf_export import keras_export\n \n-_COUNT_NAME = 'count'\n-_MEAN_NAME = 'mean'\n-_VARIANCE_NAME = 'variance'\n-\n-\n-def convert_to_ndarray(values):\n-  if isinstance(values, np.ndarray):\n-    return values\n-  elif isinstance(values, tf.Tensor):\n-    return K.get_value(values)\n-  else:\n-    return np.array(values)\n-\n \n @keras_export('keras.layers.experimental.preprocessing.Normalization', v1=[])\n-class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n+class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   \"\"\"Feature-wise normalization of the data.\n \n   This layer will coerce its inputs into a distribution centered around\n@@ -97,6 +82,9 @@ class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   \"\"\"\n \n   def __init__(self, axis=-1, mean=None, variance=None, **kwargs):\n+    super(Normalization, self).__init__(stateful=True, streaming=True, **kwargs)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('Normalization').set(True)\n+\n     # Standardize `axis` to a tuple.\n     if axis is None:\n       axis = ()\n@@ -104,23 +92,17 @@ class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       axis = (axis,)\n     else:\n       axis = tuple(axis)\n-\n-    super(Normalization, self).__init__(\n-        combiner=_NormalizingCombiner(axis), **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell('Normalization').set(True)\n-\n     if 0 in axis:\n       raise ValueError('The argument \\'axis\\' may not be 0.')\n-\n     self.axis = axis\n \n+    # Set `mean` and `variance` if passed.\n     if isinstance(mean, tf.Variable):\n       raise ValueError('Normalization does not support passing a Variable '\n                        'for the `mean` init arg.')\n     if isinstance(variance, tf.Variable):\n       raise ValueError('Normalization does not support passing a Variable '\n                        'for the `variance` init arg.')\n-\n     if mean is not None and variance is not None:\n       mean = convert_to_ndarray(mean)\n       variance = convert_to_ndarray(variance)\n@@ -128,7 +110,6 @@ class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       raise ValueError(\n           'When setting values directly, both `mean` and `variance` '\n           'must be set. Got mean: {} and variance: {}'.format(mean, variance))\n-\n     self.mean_val = mean\n     self.variance_val = variance\n \n@@ -136,62 +117,121 @@ class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     input_shape = tf.TensorShape(input_shape).as_list()\n     if len(input_shape) == 1:\n       input_shape = input_shape + [1]\n-\n     ndim = len(input_shape)\n \n-    # Sort `self.axis` to avoid transposing `mean_and_var_shape`.\n-    # Negative axes are not sortable until you know the number of dimensions.\n-    original_axis = self.axis\n-    self.axis = tuple(sorted(self.axis,\n-                             key=lambda a: a if a >= 0 else ndim + a))\n+    if any(a < 1 - ndim or a >= ndim for a in self.axis):\n+      raise ValueError('All `axis` values must be in the range '\n+                       '[1 - ndim, ndim - 1]. Found '\n+                       'ndim: `{}`, axis: {}'.format(ndim, self.axis))\n \n-    if any(a < 1-ndim for a in self.axis) or any(a >= ndim for a in self.axis):\n-      raise ValueError('All `axis` values must be in '\n-                       'the range [1-ndim, ndim-1].\\n'\n-                       'Got:\\n'\n-                       '    ndim: {}\\n'\n-                       '    axis: {}'.format(ndim, original_axis))\n+    # Axes to be kept, replacing negative values with positive equivalents.\n+    # Sorted to avoid transposing axes.\n+    self._keep_axis = sorted([d if d >= 0 else d + ndim for d in self.axis])\n+    # Axes to be reduced.\n+    self._reduce_axis = [d for d in range(ndim) if d not in self._keep_axis]\n+    # 1 if an axis should be reduced, 0 otherwise.\n+    self._reduce_axis_mask = [\n+        0 if d in self._keep_axis else 1 for d in range(ndim)\n+    ]\n+    # Broadcast any reduced axes.\n+    self._broadcast_shape = [\n+        input_shape[d] if d in self._keep_axis else 1 for d in range(ndim)\n+    ]\n+    # Create variables without keeping reduced axes.\n+    mean_and_var_shape = tuple(input_shape[d] for d in self._keep_axis)\n \n-    self._broadcast_shape = [1 for _ in range(len(input_shape))]\n-    mean_and_var_shape = []\n-    for i in self.axis:\n-      mean_and_var_shape.append(input_shape[i])\n-      self._broadcast_shape[i] = input_shape[i]\n-\n-    # count is not used in this class's call() method, but is used to re-create\n-    # the accumulator during multiple calls to 'adapt'.\n-    # TODO(omalleyt): should mean and variance be set to self.dtype?\n-    self.mean = self._add_state_variable(\n-        name=_MEAN_NAME,\n+    self.mean = self.add_weight(\n+        name='mean',\n         shape=mean_and_var_shape,\n-        dtype=K.floatx(),\n-        initializer=tf.compat.v1.zeros_initializer)\n-    self.variance = self._add_state_variable(\n-        name=_VARIANCE_NAME,\n+        dtype=self.dtype,\n+        initializer=tf.compat.v1.zeros_initializer,\n+        trainable=False)\n+    self.variance = self.add_weight(\n+        name='variance',\n         shape=mean_and_var_shape,\n-        dtype=K.floatx(),\n-        initializer=tf.compat.v1.ones_initializer)\n-    self.count = self._add_state_variable(\n-        name=_COUNT_NAME,\n+        dtype=self.dtype,\n+        initializer=tf.compat.v1.ones_initializer,\n+        trainable=False)\n+    self.count = self.add_weight(\n+        name='count',\n         shape=(),\n         dtype=tf.int64,\n-        initializer=tf.compat.v1.zeros_initializer)\n+        initializer=tf.compat.v1.zeros_initializer,\n+        trainable=False)\n \n     super(Normalization, self).build(input_shape)\n \n     if (self.mean_val is not None and self.variance_val is not None):\n       mean_val = self.mean_val * np.ones(mean_and_var_shape)\n       variance_val = self.variance_val * np.ones(mean_and_var_shape)\n-      self.set_weights([mean_val, variance_val])\n+      self.mean.assign(mean_val)\n+      self.variance.assign(variance_val)\n+\n+    self.built = True\n+\n+  def update_state(self, data):\n+    if not self.built:\n+      raise RuntimeError('`build` must be called before `update_state`.')\n+\n+    data = self._standardize_inputs(data)\n+    batch_mean, batch_variance = tf.nn.moments(\n+        data, axes=self._reduce_axis)\n+    batch_shape = tf.compat.v1.shape(data, out_type=self.count.dtype)\n+    batch_reduce_shape = tf.compat.v1.gather(batch_shape, self._reduce_axis)\n+    batch_count = tf.reduce_prod(batch_reduce_shape)\n+\n+    total_count = batch_count + self.count\n+    batch_weight = (\n+        tf.cast(batch_count, dtype=self.dtype) /\n+        tf.cast(total_count, dtype=self.dtype))\n+    existing_weight = 1. - batch_weight\n+\n+    total_mean = self.mean * existing_weight + batch_mean * batch_weight\n+    # The variance is computed using the lack-of-fit sum of squares\n+    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n+    total_variance = ((self.variance +\n+                       (self.mean - total_mean)**2) * existing_weight +\n+                      (batch_variance +\n+                       (batch_mean - total_mean)**2) * batch_weight)\n+    self.mean.assign(total_mean)\n+    self.variance.assign(total_variance)\n+    self.count.assign(total_count)\n+\n+  def merge_state(self, layers):\n+    layers = layers + [self]\n+    if any(not l.built for l in layers):\n+      raise ValueError(\n+          'All layers to be merged must have been adapted to some inputs '\n+          'first (otherwise they have no state).')\n+\n+    layer_counts = [l.count for l in layers]\n+    layer_means = [l.mean for l in layers]\n+    layer_variances = [l.variance for l in layers]\n+\n+    total_count = tf.reduce_sum(layer_counts)\n+    layer_weightings = (\n+        tf.cast(layer_counts, self.dtype) /\n+        tf.cast(total_count, self.dtype))\n+    layer_weightings = tf.reshape(\n+        layer_weightings, shape=[len(layers)] + [1] * self.mean.shape.rank)\n+\n+    total_mean = tf.reduce_sum(layer_means * layer_weightings, axis=0)\n+    inter_layer_variances = (layer_means - total_mean)**2\n+    total_variance = tf.reduce_sum(\n+        ((layer_variances + inter_layer_variances) * layer_weightings), axis=0)\n+\n+    self.mean.assign(total_mean)\n+    self.variance.assign(total_variance)\n+    self.count.assign(total_count)\n+\n+  def reset_state(self):  # pylint: disable=method-hidden\n+    if self.built:\n+      self.mean.assign(tf.compat.v1.zeros_like(self.mean))\n+      self.variance.assign(tf.compat.v1.ones_like(self.variance))\n+      self.count.assign(tf.compat.v1.zeros_like(self.count))\n \n   def call(self, inputs):\n-    inputs = tf.convert_to_tensor(inputs)\n-    if inputs.shape.rank == 1:\n-      inputs = tf.compat.v1.expand_dims(inputs, 1)\n-    # If the inputs are not floats, cast them to floats. This avoids issues\n-    # with int-float multiplication and division below.\n-    if inputs.dtype != K.floatx():\n-      inputs = tf.cast(inputs, K.floatx())\n+    inputs = self._standardize_inputs(inputs)\n     # We need to reshape the mean and variance data to ensure that Tensorflow\n     # broadcasts the data correctly.\n     mean = tf.reshape(self.mean, self._broadcast_shape)\n@@ -206,9 +246,9 @@ class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     return input_spec\n \n   def get_config(self):\n-    config = {'axis': self.axis}\n-    base_config = super(Normalization, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n+    config = super(Normalization, self).get_config()\n+    config.update({'axis': self.axis})\n+    return config\n \n   def set_weights(self, weights):\n     \"\"\"Override for set_weights to ensure we can set just mean/var weights.\"\"\"\n@@ -216,149 +256,22 @@ class Normalization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       weights.append(np.array(0))\n     super(Normalization, self).set_weights(weights)\n \n+  def _standardize_inputs(self, inputs):\n+    inputs = tf.convert_to_tensor(inputs)\n+    if inputs.shape.rank == 0:\n+      inputs = tf.reshape(inputs, [1, 1])\n+    elif inputs.shape.rank == 1:\n+      inputs = tf.compat.v1.expand_dims(inputs, 1)\n \n-class _NormalizingCombiner(base_preprocessing_layer.Combiner):\n-  \"\"\"Combiner for the Normalization preprocessing layer.\n+    if inputs.dtype != self.dtype:\n+      inputs = tf.cast(inputs, self.dtype)\n+    return inputs\n \n-  This class encapsulates the computations for finding the mean and variance\n-  of a set of data in a stable and numerically correct way. Its associated\n-  accumulator is a namedtuple('count', 'mean', 'variance').\n \n-  Attributes:\n-    axis: The axis to compute mean and var over.\n-  \"\"\"\n-  COUNT_IDX = 0\n-  MEAN_IDX = 1\n-  VAR_IDX = 2\n-\n-  def __init__(self, axis):\n-    self.axis = axis\n-\n-  def compute(self, values, accumulator=None):\n-    \"\"\"Compute a step in this computation, returning a new accumulator.\"\"\"\n-    values = np.array(values)\n-    if values.ndim == 1:\n-      values = np.expand_dims(values, 1)\n-\n-    # `np.delete` ignores negative indexes, so use a mask to delete items.\n-    axis_mask = np.ones([values.ndim], dtype=bool)\n-    axis_mask[np.array(self.axis, dtype=np.int32)] = False\n-\n-    # This is the shape of all reduced axes (not specified in 'axis').\n-\n-    reduction_counts = np.array(values.shape)[axis_mask]\n-    # We get the number of elements that will be reduced by multiplying all\n-    # values of 'shape' corresponding to the reduced axes.\n-    count = np.prod(reduction_counts, dtype=np.int64)\n-\n-    # We want to reduce across dimensions except those specified in 'axis'\n-    # when using np.mean or np.variance; create the tuple of axes to reduce\n-    # over here.\n-    reduction_axes = tuple(np.arange(values.ndim)[axis_mask])\n-\n-    mean = np.mean(values, axis=reduction_axes, dtype=np.float64)\n-    variance = np.var(values, axis=reduction_axes, dtype=np.float64)\n-\n-    # Create an accumulator with our new data and either return it or combine\n-    # it with the passed accumulator.\n-    if accumulator is None:\n-      return self._create_accumulator(count, mean, variance)\n+def convert_to_ndarray(values):\n+  if isinstance(values, np.ndarray):\n+    return values\n+  elif isinstance(values, tf.Tensor):\n+    return K.get_value(values)\n   else:\n-      return self.add_data_to_accumulator(count, mean, variance, accumulator)\n-\n-  def add_data_to_accumulator(self, count, mean, variance, accumulator):\n-    \"\"\"Add new data to the totals in an accumulator.\"\"\"\n-    # Combine accumulators and return the result.\n-    combined_count = count + accumulator[self.COUNT_IDX]\n-\n-    # To combine accumulator means, we weight each accumulator's mean by the\n-    # number of elements that were accumulated, and then divide by the\n-    # total number of elements.\n-    combined_mean = (mean * count + accumulator[self.MEAN_IDX] *\n-                     accumulator[self.COUNT_IDX]) / combined_count\n-\n-    # The variance is computed using the lack-of-fit sum of squares\n-    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n-    accumulator_var_contribution = accumulator[self.COUNT_IDX] * (\n-        accumulator[self.VAR_IDX] +\n-        np.square(accumulator[self.MEAN_IDX] - combined_mean))\n-    data_var_contribution = count * (variance + np.square(mean - combined_mean))\n-    combined_variance = (accumulator_var_contribution +\n-                         data_var_contribution) / combined_count\n-\n-    accumulator[self.COUNT_IDX] = combined_count\n-    accumulator[self.MEAN_IDX] = np.nan_to_num(combined_mean)\n-    accumulator[self.VAR_IDX] = np.nan_to_num(combined_variance)\n-    return accumulator\n-\n-  def merge(self, accumulators):\n-    \"\"\"Merge several accumulators to a single accumulator.\"\"\"\n-    # Combine accumulators and return the result.\n-    combined_count = np.sum(\n-        [accumulator[self.COUNT_IDX] for accumulator in accumulators])\n-\n-    # To combine accumulator means, we weight each accumulator's mean by the\n-    # number of elements that were accumulated, and then divide by the\n-    # total number of elements.\n-    combined_mean = np.add.reduce([\n-        accumulator[self.MEAN_IDX] * accumulator[self.COUNT_IDX]\n-        for accumulator in accumulators\n-    ]) / combined_count\n-\n-    # The variance is computed using the lack-of-fit sum of squares\n-    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n-    def variance_contribution(accumulator):\n-      return accumulator[self.COUNT_IDX] * (\n-          accumulator[self.VAR_IDX] +\n-          np.square(accumulator[self.MEAN_IDX] - combined_mean))\n-\n-    combined_variance = np.add.reduce([\n-        variance_contribution(accumulator) for accumulator in accumulators\n-    ]) / combined_count\n-\n-    return self._create_accumulator(combined_count, combined_mean,\n-                                    combined_variance)\n-\n-  def extract(self, accumulator):\n-    \"\"\"Convert an accumulator into a dict of output values.\"\"\"\n-    return {\n-        _COUNT_NAME: accumulator[self.COUNT_IDX],\n-        _MEAN_NAME: accumulator[self.MEAN_IDX],\n-        _VARIANCE_NAME: accumulator[self.VAR_IDX]\n-    }\n-\n-  def restore(self, output):\n-    \"\"\"Create an accumulator based on 'output'.\"\"\"\n-    # There is no special internal state here, so we just return the relevant\n-    # internal value.\n-    count = output[_COUNT_NAME]\n-    mean = output[_MEAN_NAME]\n-    var = output[_VARIANCE_NAME]\n-    if (count == 0 and (mean.any() != 0.0 or var.any() != 0.0)):\n-      raise RuntimeError(\n-          'The mean and/or variance of a Normalization preprocessing layer '\n-          \"were set without also setting 'count'. If 'count' is not also set, \"\n-          \" or was set to 0, 'adapt' cannot be called unless the 'reset_state'\"\n-          'arg is True.')\n-    return self._create_accumulator(output[_COUNT_NAME], output[_MEAN_NAME],\n-                                    output[_VARIANCE_NAME])\n-\n-  def serialize(self, accumulator):\n-    \"\"\"Serialize an accumulator for a remote call.\"\"\"\n-    output_dict = {\n-        _COUNT_NAME: accumulator[self.COUNT_IDX].tolist(),\n-        _MEAN_NAME: accumulator[self.MEAN_IDX].tolist(),\n-        _VARIANCE_NAME: accumulator[self.VAR_IDX].tolist()\n-    }\n-    return tf.compat.as_bytes(json.dumps(output_dict))\n-\n-  def deserialize(self, encoded_accumulator):\n-    \"\"\"Deserialize an accumulator received from 'serialize()'.\"\"\"\n-    value_dict = json.loads(tf.compat.as_text(encoded_accumulator))\n-    return self._create_accumulator(\n-        np.array(value_dict[_COUNT_NAME]), np.array(value_dict[_MEAN_NAME]),\n-        np.array(value_dict[_VARIANCE_NAME]))\n-\n-  def _create_accumulator(self, count, mean, variance):\n-    \"\"\"Convert any 'nan' values in the given accumulator to numeric values.\"\"\"\n-    return [count, mean, variance]\n+    return np.array(values)\n\n@@ -138,80 +138,6 @@ class NormalizationTest(keras_parameterized.TestCase,\n     expected = np.array([[3., -3., -0.33333333], [9., 5., 1.]])\n     self.assertAllClose(expected, output_data)\n \n-  def test_combiner_api_compatibility(self):\n-    data = np.array([[1], [2], [3], [4], [5]])\n-    combiner = normalization._NormalizingCombiner(axis=-1)\n-    expected = {\n-        \"count\": np.array(5.0),\n-        \"variance\": np.array([2.]),\n-        \"mean\": np.array([3.])\n-    }\n-    expected_accumulator = combiner._create_accumulator(expected[\"count\"],\n-                                                        expected[\"mean\"],\n-                                                        expected[\"variance\"])\n-    self.validate_accumulator_serialize_and_deserialize(combiner, data,\n-                                                        expected_accumulator)\n-    self.validate_accumulator_uniqueness(combiner, data)\n-    self.validate_accumulator_extract(combiner, data, expected)\n-    self.validate_accumulator_extract_and_restore(combiner, data,\n-                                                  expected)\n-\n-  @parameterized.named_parameters(\n-      {\n-          \"data\": np.array([[1], [2], [3], [4], [5]]),\n-          \"axis\": -1,\n-          \"expected\": {\n-              \"count\": np.array(5.0),\n-              \"variance\": np.array([2.]),\n-              \"mean\": np.array([3.])\n-          },\n-          \"testcase_name\": \"2d_single_element\"\n-      }, {\n-          \"data\": np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]),\n-          \"axis\": -1,\n-          \"expected\": {\n-              \"count\": np.array(5.0),\n-              \"mean\": np.array([3., 4.]),\n-              \"variance\": np.array([2., 2.])\n-          },\n-          \"testcase_name\": \"2d_multi_element\"\n-      }, {\n-          \"data\": np.array([[[1, 2]], [[2, 3]], [[3, 4]], [[4, 5]], [[5, 6]]]),\n-          \"axis\": 2,\n-          \"expected\": {\n-              \"count\": np.array(5.0),\n-              \"mean\": np.array([3., 4.]),\n-              \"variance\": np.array([2., 2.])\n-          },\n-          \"testcase_name\": \"3d_multi_element\"\n-      }, {\n-          \"data\": np.array([[[1, 2]], [[2, 3]], [[3, 4]], [[4, 5]], [[5, 6]]]),\n-          \"axis\": (1, 2),\n-          \"expected\": {\n-              \"count\": np.array(5.0),\n-              \"mean\": np.array([[3., 4.]]),\n-              \"variance\": np.array([[2., 2.]])\n-          },\n-          \"testcase_name\": \"3d_multi_element_multi_axis\"\n-      }, {\n-          \"data\":\n-              np.array([[[1, 2], [2, 3]], [[3, 4], [4, 5]], [[1, 2], [2, 3]],\n-                        [[3, 4], [4, 5]]]),\n-          \"axis\":\n-              1,\n-          \"expected\": {\n-              \"count\": np.array(8.0),\n-              \"mean\": np.array([2.5, 3.5]),\n-              \"variance\": np.array([1.25, 1.25])\n-          },\n-          \"testcase_name\":\n-              \"3d_multi_element_internal_axis\"\n-      })\n-  def test_combiner_computation_multi_value_axis(self, data, axis, expected):\n-    combiner = normalization._NormalizingCombiner(axis=axis)\n-    expected_accumulator = combiner._create_accumulator(**expected)\n-    self.validate_accumulator_computation(combiner, data, expected_accumulator)\n-\n   @parameterized.named_parameters(*_get_layer_computation_test_cases())\n   def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,\n                              expected):\n@@ -283,6 +209,18 @@ class NormalizationTest(keras_parameterized.TestCase,\n     if tf.executing_eagerly():\n       self.assertAllClose(output.numpy(), [[-1], [1], [-1], [1]])\n \n+  def test_0d_data(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest(\"Only supported in TF2.\")\n+\n+    data = [0, 2, 0, 2]\n+    cls = get_layer_class()\n+    layer = cls(axis=-1)\n+    layer.adapt(data)\n+    output = layer(0.)\n+    self.assertListEqual(output.shape.as_list(), [1, 1])\n+    self.assertAllClose(output.numpy(), [[-1]])\n+\n   @parameterized.parameters(\n       {\"axis\": 0},\n       {\"axis\": (-1, 0)},\n@@ -304,8 +242,7 @@ class NormalizationTest(keras_parameterized.TestCase,\n   def test_bad_axis_fail_build(self, axis):\n     cls = get_layer_class()\n     layer = cls(axis=axis)\n-    with self.assertRaisesRegex(ValueError,\n-                                r\"in the range \\[1-ndim, ndim-1\\].\"):\n+    with self.assertRaisesRegex(ValueError, r\"in the range\"):\n       layer.build([None, 2, 3])\n \n   @parameterized.parameters(\n@@ -338,6 +275,33 @@ class NormalizationTest(keras_parameterized.TestCase,\n          keras.layers.Dense(1)])\n     model.summary()\n \n+  def test_merge_state(self):\n+    if not tf.executing_eagerly():\n+      self.skipTest(\"`merge_state` only supported in TF2\")\n+\n+    cls = get_layer_class()\n+    data = np.random.rand(30, 10, 2)\n+    ds = tf.data.Dataset.from_tensor_slices(data).batch(2)\n+    norm = cls(axis=(1, 2))\n+    norm.adapt(ds)\n+\n+    partial_ds_1 = ds.shard(3, 0)\n+    partial_ds_2 = ds.shard(3, 1)\n+    partial_ds_3 = ds.shard(3, 2)\n+\n+    norm_1 = cls(axis=(1, 2))\n+    norm_2 = cls(axis=(1, 2))\n+    norm_3 = cls(axis=(1, 2))\n+\n+    norm_1.adapt(partial_ds_1)\n+    norm_2.adapt(partial_ds_2)\n+    norm_3.adapt(partial_ds_3)\n+\n+    norm_1.merge_state([norm_2, norm_3])\n+    merged_norm = norm_1\n+\n+    self.assertAllClose(norm(data), merged_norm(data))\n+\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n@@ -18,16 +18,319 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+import tensorflow as tf\n+\n+import json\n+\n+import numpy as np\n+from keras import backend as K\n from keras.engine import base_preprocessing_layer\n from keras.engine.base_preprocessing_layer_v1 import CombinerPreprocessingLayer\n-from keras.layers.preprocessing import normalization\n from tensorflow.python.util.tf_export import keras_export\n \n+_COUNT_NAME = 'count'\n+_MEAN_NAME = 'mean'\n+_VARIANCE_NAME = 'variance'\n+\n+\n+def convert_to_ndarray(values):\n+  if isinstance(values, np.ndarray):\n+    return values\n+  elif isinstance(values, tf.Tensor):\n+    return K.get_value(values)\n+  else:\n+    return np.array(values)\n+\n \n @keras_export(v1=['keras.layers.experimental.preprocessing.Normalization'])\n-class Normalization(normalization.Normalization, CombinerPreprocessingLayer):\n+class Normalization(CombinerPreprocessingLayer):\n+  \"\"\"Feature-wise normalization of the data.\n+\n+  This layer will coerce its inputs into a distribution centered around\n+  0 with standard deviation 1. It accomplishes this by precomputing the mean and\n+  variance of the data, and calling (input-mean)/sqrt(var) at runtime.\n+\n+  What happens in `adapt`: Compute mean and variance of the data and store them\n+    as the layer's weights. `adapt` should be called before `fit`, `evaluate`,\n+    or `predict`.\n+\n+  Attributes:\n+      axis: Integer or tuple of integers, the axis or axes that should be\n+        \"kept\". These axes are not be summed over when calculating the\n+        normalization statistics. By default the last axis, the `features` axis\n+        is kept and any `space` or `time` axes are summed. Each element in the\n+        the axes that are kept is normalized independently. If `axis` is set to\n+        'None', the layer will perform scalar normalization (dividing the input\n+        by a single scalar value). The `batch` axis, 0, is always summed over\n+        (`axis=0` is not allowed).\n+  \"\"\"\n \n   def __init__(self, axis=-1, **kwargs):\n-    super(Normalization, self).__init__(axis, **kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell(\n-        'Normalization v1').set(True)\n+    # Standardize `axis` to a tuple.\n+    if axis is None:\n+      axis = ()\n+    elif isinstance(axis, int):\n+      axis = (axis,)\n+    else:\n+      axis = tuple(axis)\n+\n+    mean = kwargs.pop('mean', None)\n+    variance = kwargs.pop('variance', None)\n+\n+    super(Normalization, self).__init__(\n+        combiner=_NormalizingCombiner(axis), **kwargs)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell('Normalization').set(True)\n+\n+    if 0 in axis:\n+      raise ValueError('The argument \\'axis\\' may not be 0.')\n+\n+    self.axis = axis\n+\n+    if isinstance(mean, tf.Variable):\n+      raise ValueError('Normalization does not support passing a Variable '\n+                       'for the `mean` init arg.')\n+    if isinstance(variance, tf.Variable):\n+      raise ValueError('Normalization does not support passing a Variable '\n+                       'for the `variance` init arg.')\n+    if mean is not None and variance is not None:\n+      mean = convert_to_ndarray(mean)\n+      variance = convert_to_ndarray(variance)\n+    elif mean is not None or variance is not None:\n+      raise ValueError(\n+          'When setting values directly, both `mean` and `variance` '\n+          'must be set. Got mean: {} and variance: {}'.format(mean, variance))\n+\n+    self.mean_val = mean\n+    self.variance_val = variance\n+\n+  def build(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if len(input_shape) == 1:\n+      input_shape = input_shape + [1]\n+\n+    ndim = len(input_shape)\n+\n+    # Sort `self.axis` to avoid transposing `mean_and_var_shape`.\n+    # Negative axes are not sortable until you know the number of dimensions.\n+    original_axis = self.axis\n+    self.axis = tuple(\n+        sorted(self.axis, key=lambda a: a if a >= 0 else ndim + a))\n+\n+    if any(a < 1 - ndim for a in self.axis) or any(\n+        a >= ndim for a in self.axis):\n+      raise ValueError('All `axis` values must be in '\n+                       'the range [1-ndim, ndim-1].\\n'\n+                       'Got:\\n'\n+                       '    ndim: {}\\n'\n+                       '    axis: {}'.format(ndim, original_axis))\n+\n+    self._broadcast_shape = [1 for _ in range(len(input_shape))]\n+    mean_and_var_shape = []\n+    for i in self.axis:\n+      mean_and_var_shape.append(input_shape[i])\n+      self._broadcast_shape[i] = input_shape[i]\n+\n+    # count is not used in this class's call() method, but is used to re-create\n+    # the accumulator during multiple calls to 'adapt'.\n+    # TODO(omalleyt): should mean and variance be set to self.dtype?\n+    self.mean = self._add_state_variable(\n+        name=_MEAN_NAME,\n+        shape=mean_and_var_shape,\n+        dtype=K.floatx(),\n+        initializer=tf.compat.v1.zeros_initializer)\n+    self.variance = self._add_state_variable(\n+        name=_VARIANCE_NAME,\n+        shape=mean_and_var_shape,\n+        dtype=K.floatx(),\n+        initializer=tf.compat.v1.ones_initializer)\n+    self.count = self._add_state_variable(\n+        name=_COUNT_NAME,\n+        shape=(),\n+        dtype=tf.int64,\n+        initializer=tf.compat.v1.zeros_initializer)\n+\n+    super(Normalization, self).build(input_shape)\n+\n+    if (self.mean_val is not None and self.variance_val is not None):\n+      mean_val = self.mean_val * np.ones(mean_and_var_shape)\n+      variance_val = self.variance_val * np.ones(mean_and_var_shape)\n+      self.set_weights([mean_val, variance_val])\n+\n+  def call(self, inputs):\n+    inputs = tf.convert_to_tensor(inputs)\n+    if inputs.shape.rank == 1:\n+      inputs = tf.compat.v1.expand_dims(inputs, 1)\n+    # If the inputs are not floats, cast them to floats. This avoids issues\n+    # with int-float multiplication and division below.\n+    if inputs.dtype != K.floatx():\n+      inputs = tf.cast(inputs, K.floatx())\n+    # We need to reshape the mean and variance data to ensure that Tensorflow\n+    # broadcasts the data correctly.\n+    mean = tf.reshape(self.mean, self._broadcast_shape)\n+    variance = tf.reshape(self.variance, self._broadcast_shape)\n+    return ((inputs - mean) /\n+            tf.maximum(tf.sqrt(variance), K.epsilon()))\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def compute_output_signature(self, input_spec):\n+    return input_spec\n+\n+  def get_config(self):\n+    config = {'axis': self.axis}\n+    base_config = super(Normalization, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n+  def set_weights(self, weights):\n+    \"\"\"Override for set_weights to ensure we can set just mean/var weights.\"\"\"\n+    if len(weights) == 2:\n+      weights.append(np.array(0))\n+    super(Normalization, self).set_weights(weights)\n+\n+\n+class _NormalizingCombiner(base_preprocessing_layer.Combiner):\n+  \"\"\"Combiner for the Normalization preprocessing layer.\n+\n+  This class encapsulates the computations for finding the mean and variance\n+  of a set of data in a stable and numerically correct way. Its associated\n+  accumulator is a namedtuple('count', 'mean', 'variance').\n+\n+  Attributes:\n+    axis: The axis to compute mean and var over.\n+  \"\"\"\n+  COUNT_IDX = 0\n+  MEAN_IDX = 1\n+  VAR_IDX = 2\n+\n+  def __init__(self, axis):\n+    self.axis = axis\n+\n+  def compute(self, values, accumulator=None):\n+    \"\"\"Compute a step in this computation, returning a new accumulator.\"\"\"\n+    values = np.array(values)\n+    if values.ndim == 1:\n+      values = np.expand_dims(values, 1)\n+\n+    # `np.delete` ignores negative indexes, so use a mask to delete items.\n+    axis_mask = np.ones([values.ndim], dtype=bool)\n+    axis_mask[np.array(self.axis, dtype=np.int32)] = False\n+\n+    # This is the shape of all reduced axes (not specified in 'axis').\n+\n+    reduction_counts = np.array(values.shape)[axis_mask]\n+    # We get the number of elements that will be reduced by multiplying all\n+    # values of 'shape' corresponding to the reduced axes.\n+    count = np.prod(reduction_counts, dtype=np.int64)\n+\n+    # We want to reduce across dimensions except those specified in 'axis'\n+    # when using np.mean or np.variance; create the tuple of axes to reduce\n+    # over here.\n+    reduction_axes = tuple(np.arange(values.ndim)[axis_mask])\n+\n+    mean = np.mean(values, axis=reduction_axes, dtype=np.float64)\n+    variance = np.var(values, axis=reduction_axes, dtype=np.float64)\n+\n+    # Create an accumulator with our new data and either return it or combine\n+    # it with the passed accumulator.\n+    if accumulator is None:\n+      return self._create_accumulator(count, mean, variance)\n+    else:\n+      return self.add_data_to_accumulator(count, mean, variance, accumulator)\n+\n+  def add_data_to_accumulator(self, count, mean, variance, accumulator):\n+    \"\"\"Add new data to the totals in an accumulator.\"\"\"\n+    # Combine accumulators and return the result.\n+    combined_count = count + accumulator[self.COUNT_IDX]\n+\n+    # To combine accumulator means, we weight each accumulator's mean by the\n+    # number of elements that were accumulated, and then divide by the\n+    # total number of elements.\n+    combined_mean = (mean * count + accumulator[self.MEAN_IDX] *\n+                     accumulator[self.COUNT_IDX]) / combined_count\n+\n+    # The variance is computed using the lack-of-fit sum of squares\n+    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n+    accumulator_var_contribution = accumulator[self.COUNT_IDX] * (\n+        accumulator[self.VAR_IDX] +\n+        np.square(accumulator[self.MEAN_IDX] - combined_mean))\n+    data_var_contribution = count * (variance + np.square(mean - combined_mean))\n+    combined_variance = (accumulator_var_contribution +\n+                         data_var_contribution) / combined_count\n+\n+    accumulator[self.COUNT_IDX] = combined_count\n+    accumulator[self.MEAN_IDX] = np.nan_to_num(combined_mean)\n+    accumulator[self.VAR_IDX] = np.nan_to_num(combined_variance)\n+    return accumulator\n+\n+  def merge(self, accumulators):\n+    \"\"\"Merge several accumulators to a single accumulator.\"\"\"\n+    # Combine accumulators and return the result.\n+    combined_count = np.sum(\n+        [accumulator[self.COUNT_IDX] for accumulator in accumulators])\n+\n+    # To combine accumulator means, we weight each accumulator's mean by the\n+    # number of elements that were accumulated, and then divide by the\n+    # total number of elements.\n+    combined_mean = np.add.reduce([\n+        accumulator[self.MEAN_IDX] * accumulator[self.COUNT_IDX]\n+        for accumulator in accumulators\n+    ]) / combined_count\n+\n+    # The variance is computed using the lack-of-fit sum of squares\n+    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n+    def variance_contribution(accumulator):\n+      return accumulator[self.COUNT_IDX] * (\n+          accumulator[self.VAR_IDX] +\n+          np.square(accumulator[self.MEAN_IDX] - combined_mean))\n+\n+    combined_variance = np.add.reduce([\n+        variance_contribution(accumulator) for accumulator in accumulators\n+    ]) / combined_count\n+\n+    return self._create_accumulator(combined_count, combined_mean,\n+                                    combined_variance)\n+\n+  def extract(self, accumulator):\n+    \"\"\"Convert an accumulator into a dict of output values.\"\"\"\n+    return {\n+        _COUNT_NAME: accumulator[self.COUNT_IDX],\n+        _MEAN_NAME: accumulator[self.MEAN_IDX],\n+        _VARIANCE_NAME: accumulator[self.VAR_IDX]\n+    }\n+\n+  def restore(self, output):\n+    \"\"\"Create an accumulator based on 'output'.\"\"\"\n+    # There is no special internal state here, so we just return the relevant\n+    # internal value.\n+    count = output[_COUNT_NAME]\n+    mean = output[_MEAN_NAME]\n+    var = output[_VARIANCE_NAME]\n+    if (count == 0 and (mean.any() != 0.0 or var.any() != 0.0)):\n+      raise RuntimeError(\n+          'The mean and/or variance of a Normalization preprocessing layer '\n+          \"were set without also setting 'count'. If 'count' is not also set, \"\n+          \" or was set to 0, 'adapt' cannot be called unless the 'reset_state'\"\n+          'arg is True.')\n+    return self._create_accumulator(output[_COUNT_NAME], output[_MEAN_NAME],\n+                                    output[_VARIANCE_NAME])\n+\n+  def serialize(self, accumulator):\n+    \"\"\"Serialize an accumulator for a remote call.\"\"\"\n+    output_dict = {\n+        _COUNT_NAME: accumulator[self.COUNT_IDX].tolist(),\n+        _MEAN_NAME: accumulator[self.MEAN_IDX].tolist(),\n+        _VARIANCE_NAME: accumulator[self.VAR_IDX].tolist()\n+    }\n+    return tf.compat.as_bytes(json.dumps(output_dict))\n+\n+  def deserialize(self, encoded_accumulator):\n+    \"\"\"Deserialize an accumulator received from 'serialize()'.\"\"\"\n+    value_dict = json.loads(tf.compat.as_text(encoded_accumulator))\n+    return self._create_accumulator(\n+        np.array(value_dict[_COUNT_NAME]), np.array(value_dict[_MEAN_NAME]),\n+        np.array(value_dict[_VARIANCE_NAME]))\n+\n+  def _create_accumulator(self, count, mean, variance):\n+    \"\"\"Convert any 'nan' values in the given accumulator to numeric values.\"\"\"\n+    return [count, mean, variance]\n\n@@ -22,9 +22,6 @@ import tensorflow as tf\n import abc\n \n import six\n-\n-from tensorflow.python.autograph.core import ag_ctx\n-from tensorflow.python.autograph.impl import api as autograph\n from tensorflow.python.framework import smart_cond\n from keras import backend as K\n from keras.utils import losses_utils\n@@ -143,7 +140,7 @@ class Loss(object):\n       if tf.executing_eagerly():\n         call_fn = self.call\n       else:\n-        call_fn = autograph.tf_convert(self.call, ag_ctx.control_status_ctx())\n+        call_fn = tf.__internal__.autograph.tf_convert(self.call, tf.__internal__.autograph.control_status_ctx())\n       losses = call_fn(y_true, y_pred)\n       return losses_utils.compute_weighted_loss(\n           losses, sample_weight, reduction=self._get_reduction())\n@@ -247,7 +244,7 @@ class LossFunctionWrapper(Loss):\n     if tf.is_tensor(y_pred) and tf.is_tensor(y_true):\n       y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(y_pred, y_true)\n \n-    ag_fn = autograph.tf_convert(self.fn, ag_ctx.control_status_ctx())\n+    ag_fn = tf.__internal__.autograph.tf_convert(self.fn, tf.__internal__.autograph.control_status_ctx())\n     return ag_fn(y_true, y_pred, **self._fn_kwargs)\n \n   def get_config(self):\n\n@@ -27,9 +27,6 @@ import types\n \n import numpy as np\n import six\n-\n-from tensorflow.python.autograph.core import ag_ctx\n-from tensorflow.python.autograph.impl import api as autograph\n from tensorflow.python.eager import def_function\n from keras import activations\n from keras import backend as K\n@@ -156,8 +153,8 @@ class Metric(base_layer.Layer):\n       obj_update_state = obj.update_state\n \n       def update_state_fn(*args, **kwargs):\n-        control_status = ag_ctx.control_status_ctx()\n-        ag_update_state = autograph.tf_convert(obj_update_state, control_status)\n+        control_status = tf.__internal__.autograph.control_status_ctx()\n+        ag_update_state = tf.__internal__.autograph.tf_convert(obj_update_state, control_status)\n         return ag_update_state(*args, **kwargs)\n     else:\n       if isinstance(obj.update_state, def_function.Function):\n@@ -171,8 +168,8 @@ class Metric(base_layer.Layer):\n     obj_result = obj.result\n \n     def result_fn(*args, **kwargs):\n-      control_status = ag_ctx.control_status_ctx()\n-      ag_result = autograph.tf_convert(obj_result, control_status)\n+      control_status = tf.__internal__.autograph.control_status_ctx()\n+      ag_result = tf.__internal__.autograph.tf_convert(obj_result, control_status)\n       return ag_result(*args, **kwargs)\n \n     obj.result = types.MethodType(metrics_utils.result_wrapper(result_fn), obj)\n@@ -621,7 +618,7 @@ class MeanMetricWrapper(Mean):\n     y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n         y_pred, y_true)\n \n-    ag_fn = autograph.tf_convert(self._fn, ag_ctx.control_status_ctx())\n+    ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n     return super(MeanMetricWrapper, self).update_state(\n         matches, sample_weight=sample_weight)\n@@ -3250,7 +3247,7 @@ class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n     y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n         y_pred, y_true)\n \n-    ag_fn = autograph.tf_convert(self._fn, ag_ctx.control_status_ctx())\n+    ag_fn = tf.__internal__.autograph.tf_convert(self._fn, tf.__internal__.autograph.control_status_ctx())\n     matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n     return super(SumOverBatchSizeMetricWrapper, self).update_state(\n         matches, sample_weight=sample_weight)\n\n@@ -23,7 +23,6 @@ import threading\n \n from tensorflow.python.distribute import distribute_utils\n from tensorflow.python.distribute import ps_values as ps_distribute_values\n-from tensorflow.python.types import core\n \n \n # _autocast_dtype.dtype is the dtype AutoCastVariables should be cast to, or\n@@ -44,7 +43,7 @@ def numpy_text(tensor, is_repr=False):\n   return text\n \n \n-class AutoCastVariable(tf.Variable, core.Tensor):\n+class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n   \"\"\"Variable that will cast itself to a different dtype in applicable contexts.\n \n   This class wraps a floating-point `tf.Variable`. It emulates the variable\n\n@@ -27,7 +27,6 @@ import functools\n \n import six\n from tensorflow.python.distribute import values as ds_values\n-from tensorflow.python.eager import monitoring\n from keras import backend\n from keras import initializers\n from keras.engine import base_layer_utils\n@@ -42,7 +41,7 @@ from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n \n-keras_optimizers_gauge = monitoring.BoolGauge(\n+keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n     \"/tensorflow/api/oss-keras/optimizers\", \"keras optimizer usage\", \"method\")\n \n _DEFAULT_VALID_DTYPES = frozenset([\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
