{"custom_id": "keras#b1105dca17670dcac229271e63d5073fe445b84c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 645 | Lines Deleted: 548 | Files Changed: 30 | Hunks: 225 | Methods Changed: 142 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1193 | Churn Cumulative: 22072 | Contributors (this commit): 23 | Commits (past 90d): 121 | Contributors (cumulative): 211 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,8 +46,8 @@ class MultiWorkerMirroredStrategyTest(tf.test.TestCase, parameterized.TestCase):\n         def _get_dataset():\n             inputs = tf.expand_dims(tf.constant(range(10)), axis=1)\n             targets = tf.expand_dims(tf.constant(range(10)), axis=1)\n-            # Make global batch size 12 for 2 replicas and a non-repeated dataset\n-            # with 10 elements so that we have partial batch\n+            # Make global batch size 12 for 2 replicas and a non-repeated\n+            # dataset with 10 elements so that we have partial batch\n             dataset = tf.data.Dataset.from_tensor_slices(\n                 (inputs, targets)\n             ).batch(12, drop_remainder=False)\n\n@@ -293,7 +293,8 @@ class TestDistributionStrategyDnnCorrectness(\n         sync_batchnorm,\n         jit_compile,\n     ):\n-        # TODO(anjs): Identify why this particular V1 optimizer needs a higher tol.\n+        # TODO(anjs): Identify why this particular V1 optimizer needs a higher\n+        # tol.\n         if (\n             \"FtrlV1\" in optimizer_fn._name\n             and \"TPU\" in type(distribution).__name__\n@@ -358,15 +359,16 @@ class TestDistributionStrategyDnnCorrectness(\n     def test_fused_batch_norm_uneven_batch(self, distribution):\n         \"\"\"Test that fused batch norm works when the last device may get empty data.\n \n-        Adapted from https://www.tensorflow.org/tutorials/distribute/custom_training\n+        Adapted from\n+        https://www.tensorflow.org/tutorials/distribute/custom_training\n         but using ResNet, which uses fused batchnorm, as the model.\n \n         Arguments:\n           distribution: distribute test configuration\n         \"\"\"\n         (train_images, train_labels), _ = fashion_mnist.load_data()\n-        # add channel dimension to make 2D data into 3D, since some ops of the model\n-        # require it.\n+        # add channel dimension to make 2D data into 3D, since some ops of the\n+        # model require it.\n         train_images = train_images[..., None]\n         train_images = train_images / np.float32(255)\n \n@@ -394,7 +396,8 @@ class TestDistributionStrategyDnnCorrectness(\n \n         epochs = 2\n \n-        # Keep only the first images, so that the last GPU receives an empty batch\n+        # Keep only the first images, so that the last GPU receives an empty\n+        # batch\n         padded_train_images = padded_train_images[:num_samples]\n         train_labels = train_labels[:num_samples]\n \n@@ -423,8 +426,8 @@ class TestDistributionStrategyDnnCorrectness(\n             return keras.Model(inputs, features)\n \n         with distribution.scope():\n-            # Set reduction to `none` so we can do the reduction afterwards and divide\n-            # by global batch size.\n+            # Set reduction to `none` so we can do the reduction afterwards and\n+            # divide by global batch size.\n             loss_object = keras.losses.SparseCategoricalCrossentropy(\n                 from_logits=True, reduction=losses_impl.Reduction.NONE\n             )\n\n@@ -74,8 +74,8 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n         for i in dataset:\n             distribution.run(step_fn, args=(i,))\n \n-        # This should be the mean of integers 0-9 which has a sum of 45 and a count\n-        # of 10 resulting in mean of 4.5.\n+        # This should be the mean of integers 0-9 which has a sum of 45 and a\n+        # count of 10 resulting in mean of 4.5.\n         self.assertEqual(metric.result().numpy(), 4.5)\n \n     @tf.__internal__.distribute.combinations.generate(\n@@ -92,8 +92,8 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n             for i in range(10):\n                 metric.update_state(i)\n \n-        # This should be the mean of integers 0-9 which has a sum of 45 and a count\n-        # of 10 resulting in mean of 4.5.\n+        # This should be the mean of integers 0-9 which has a sum of 45 and a\n+        # count of 10 resulting in mean of 4.5.\n         self.assertEqual(metric.result().numpy(), 4.5)\n \n     @tf.__internal__.distribute.combinations.generate(\n@@ -122,8 +122,8 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n \n         train_fn(dataset)\n \n-        # This should be the mean of integers 0-9 which has a sum of 45 and a count\n-        # of 10 resulting in mean of 4.5.\n+        # This should be the mean of integers 0-9 which has a sum of 45 and a\n+        # count of 10 resulting in mean of 4.5.\n         self.assertEqual(metric.result().numpy(), 4.5)\n \n \n\n@@ -210,8 +210,8 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n         def create_lstm_model():\n             model = keras.models.Sequential()\n-            # We only have LSTM variables so we can detect no gradient issues more\n-            # easily.\n+            # We only have LSTM variables so we can detect no gradient issues\n+            # more easily.\n             model.add(\n                 keras.layers.LSTM(\n                     1, return_sequences=False, input_shape=(10, 1)\n@@ -262,8 +262,8 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n     def test_nested_tf_functions(self, distribution):\n         # The test builds two computations with keras layers, one with nested\n         # tf.function, and the other without nested tf.function. We run these\n-        # computations independently on the model with same weights, and make sure\n-        # the variables are still the same after one training step.\n+        # computations independently on the model with same weights, and make\n+        # sure the variables are still the same after one training step.\n \n         inputs = np.random.random((10, 3)).astype(np.float32)\n         targets = np.ones((10, 4), dtype=np.float32)\n@@ -470,8 +470,8 @@ class KerasModelsTest(tf.test.TestCase, parameterized.TestCase):\n         loss = distribution.reduce(tf.distribute.ReduceOp.MEAN, loss, axis=0)\n \n     def test_variable_run_argument(self, distribution):\n-        # Test that variables passed to run() remain variables. Previous behavior\n-        # in TPUStrategy was to cast to Tensor.\n+        # Test that variables passed to run() remain variables. Previous\n+        # behavior in TPUStrategy was to cast to Tensor.\n \n         with distribution.scope():\n             optimizer = gradient_descent.SGD(0.1)\n\n@@ -167,9 +167,9 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n \n     def testModelPredict(self, strategy):\n         _, predictions = self._model_predict(strategy, steps=3)\n-        # Check the first (0th index), fourth (3rd index) and the last predictions\n-        # because the first, fourth and the last input are the same in\n-        # `model.predict` so there predictions should match.\n+        # Check the first (0th index), fourth (3rd index) and the last\n+        # predictions because the first, fourth and the last input are the same\n+        # in `model.predict` so there predictions should match.\n         self.assertTrue(\n             all(predictions[0] == predictions[i] for i in [0, 3, 5])\n         )\n@@ -203,9 +203,9 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n         _, predictions = self._model_predict(\n             strategy, with_normalization_layer=True, steps=3\n         )\n-        # Check the first (0th index), fourth (3rd index) and the last predictions\n-        # because the first, fourth and the last input is the same in\n-        # `model.predict` so there predictions should match.\n+        # Check the first (0th index), fourth (3rd index) and the last\n+        # predictions because the first, fourth and the last input is the same\n+        # in `model.predict` so there predictions should match.\n         self.assertTrue(\n             all(predictions[0] == predictions[i] for i in [0, 3, 5])\n         )\n@@ -219,9 +219,9 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n             strategy, steps_per_execution=3, steps=3\n         )\n \n-        # Check the first (0th index), fourth (3rd index) and the last predictions\n-        # because the first, fourth and the last input is the same in\n-        # `model.predict` so there predictions should match.\n+        # Check the first (0th index), fourth (3rd index) and the last\n+        # predictions because the first, fourth and the last input is the same\n+        # in `model.predict` so there predictions should match.\n         self.assertTrue(\n             all(predictions[0] == predictions[i] for i in [0, 3, 5])\n         )\n@@ -248,9 +248,9 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n         model = self._model_fit(strategy, x=x, validation_data=validation_data)\n         _, predictions = self._model_predict(strategy, model, steps=3)\n \n-        # Check the first (0th index), fourth (3rd index) and the last predictions\n-        # because the first, fourth and the last input is the same in\n-        # `model.predict` so there predictions should match.\n+        # Check the first (0th index), fourth (3rd index) and the last\n+        # predictions because the first, fourth and the last input is the same\n+        # in `model.predict` so there predictions should match.\n         self.assertTrue(\n             all(predictions[0] == predictions[i] for i in [0, 3, 5])\n         )\n@@ -274,9 +274,9 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n             test_data=dataset_creator.DatasetCreator(_dataset_fn),\n         )\n \n-        # Check the first (0th index), fourth (3rd index) and the last predictions\n-        # because the first, fourth and the last input is the same in\n-        # `model.predict` so there predictions should match.\n+        # Check the first (0th index), fourth (3rd index) and the last\n+        # predictions because the first, fourth and the last input is the same\n+        # in `model.predict` so there predictions should match.\n         self.assertTrue(\n             all(predictions[0] == predictions[i] for i in [0, 3, 5])\n         )\n\n@@ -68,9 +68,9 @@ class _WorkerContext:\n     \"\"\"The worker context class.\n \n     This context object provides configuration information for each task. One\n-    context manager with a worker context object will be created per\n-    invocation to the `worker_fn` where `get_current_worker_context` can be called\n-    to access the worker context object.\n+    context manager with a worker context object will be created per invocation\n+    to the `worker_fn` where `get_current_worker_context` can be called to\n+    access the worker context object.\n     \"\"\"\n \n     def __init__(\n@@ -87,18 +87,19 @@ class _WorkerContext:\n \n         Args:\n           strategy: a `DistributionStrategy` object.\n-          cluster_spec: a ClusterSpec object. It can be empty or None in the local\n-            training case.\n-          task_type: a string indicating the role of the corresponding task, such as\n-            \"worker\" or \"ps\". It can be None if it is local training or in-graph\n-            replicated training.\n+          cluster_spec: a ClusterSpec object. It can be empty or None in the\n+            local training case.\n+          task_type: a string indicating the role of the corresponding task,\n+            such as \"worker\" or \"ps\". It can be None if it is local training or\n+            in-graph replicated training.\n           task_id: an integer indicating id of the corresponding task. It can be\n             None if it is local training or in-graph replicated training.\n           session_config: an optional `tf.compat.v1.ConfigProto` object.\n-          rpc_layer: optional string specifying the RPC protocol for communication\n-            with worker masters. If None or empty, hosts in the `cluster_spec` will\n-            be used directly.\n-          worker_barrier: optional, the barrier object for worker synchronization.\n+          rpc_layer: optional string specifying the RPC protocol for\n+            communication with worker masters. If None or empty, hosts in the\n+            `cluster_spec` will be used directly.\n+          worker_barrier: optional, the barrier object for worker\n+            synchronization.\n         \"\"\"\n         self._strategy = strategy\n         self._cluster_spec = cluster_spec\n@@ -171,8 +172,8 @@ class _WorkerContext:\n         ]:\n             return True\n \n-        # If not local and chief not in the cluster_spec, use the first worker as\n-        # chief.\n+        # If not local and chief not in the cluster_spec, use the first worker\n+        # as chief.\n         if (\n             _TaskType.CHIEF not in self._cluster_spec.jobs\n             and self._task_type == _TaskType.WORKER\n@@ -188,7 +189,8 @@ class _WorkerContext:\n           ValueError: if `worker_barrier` is not passed to the __init__ method.\n         \"\"\"\n         if not self._worker_barrier:\n-            # TODO(yuefengz): we should throw an error in independent worker mode.\n+            # TODO(yuefengz): we should throw an error in independent worker\n+            # mode.\n             return\n         self._worker_barrier.wait()\n \n@@ -203,19 +205,22 @@ class _WorkerContext:\n         \"\"\"Returns a session creator.\n \n         The returned session creator will be configured with the correct master\n-        target and session configs. It will also run either init ops or ready ops\n-        by querying the `strategy` object when `create_session` is called on it.\n+        target and session configs. It will also run either init ops or ready\n+        ops by querying the `strategy` object when `create_session` is called on\n+        it.\n \n         Args:\n-          scaffold: A `Scaffold` used for gathering or building supportive ops. If\n-            not specified a default one is created. It's used to finalize the graph.\n+          scaffold: A `Scaffold` used for gathering or building supportive ops.\n+            If not specified a default one is created. It's used to finalize the\n+            graph.\n           config: `ConfigProto` proto used to configure the session.\n-          checkpoint_dir: A string. Optional path to a directory where to restore\n-            variables.\n-          checkpoint_filename_with_path: Full file name path to the checkpoint file.\n-            Only one of `checkpoint_dir` or `checkpoint_filename_with_path` can be\n-            specified.\n-          max_wait_secs: Maximum time to wait for the session to become available.\n+          checkpoint_dir: A string. Optional path to a directory where to\n+            restore variables.\n+          checkpoint_filename_with_path: Full file name path to the checkpoint\n+            file. Only one of `checkpoint_dir` or\n+            `checkpoint_filename_with_path` can be specified.\n+          max_wait_secs: Maximum time to wait for the session to become\n+            available.\n \n         Returns:\n           a descendant of SessionCreator.\n@@ -284,7 +289,8 @@ class _WorkerContext:\n \n     @property\n     def master_target(self):\n-        \"\"\"Returns the session master for the corresponding task to connect to.\"\"\"\n+        \"\"\"Returns the session master for the corresponding task to connect\n+        to.\"\"\"\n         return self._master_target\n \n     @property\n@@ -355,8 +361,8 @@ def _run_single_worker(\n \n def _split_cluster_for_evaluator(cluster_spec, task_type):\n     \"\"\"Split the cluster for evaluator since it needn't talk to other tasks.\"\"\"\n-    # Splitting the cluster is important to prevent the evaluator from talking to\n-    # other tasks in the cluster. Since we allow evaluator not to use\n+    # Splitting the cluster is important to prevent the evaluator from talking\n+    # to other tasks in the cluster. Since we allow evaluator not to use\n     # distribution strategies and as a result ops in the evaluator task may have\n     # unspecified devices. Those ops may end up on other tasks if we don't split\n     # the cluster.\n@@ -383,9 +389,9 @@ def _run_std_server(\n     environment=None,\n ):\n     \"\"\"Runs a standard server.\"\"\"\n-    # Check if the Server is already running. If so, assert that no configuration\n-    # options have changed, and return the existing Server. This allows us to\n-    # call `run_distribute_coordinator` multiple times.\n+    # Check if the Server is already running. If so, assert that no\n+    # configuration options have changed, and return the existing Server. This\n+    # allows us to call `run_distribute_coordinator` multiple times.\n     if getattr(_thread_local, \"server\", None) is not None:\n         assert _thread_local.cluster_spec == cluster_spec\n         assert _thread_local.task_type == task_type\n@@ -431,8 +437,8 @@ def _run_std_server(\n     else:\n         if session_config:\n             logging.info(\n-                \"Starting standard TensorFlow server, target = %r, session_config= \"\n-                \"%r\",\n+                \"Starting standard TensorFlow server, target = %r, \"\n+                \"session_config = %r\",\n                 target,\n                 session_config,\n             )\n@@ -502,43 +508,44 @@ def run_distribute_coordinator(\n     default mode, i.e the STANDALONE_CLIENT mode. Given a `cluster_spec`\n     specifying server addresses and their roles in a cluster, this coordinator\n     will figure out how to set them up, give the underlying function the right\n-    targets for master sessions via a scope object and coordinate their training.\n-    The cluster consisting of standard servers needs to be brought up either with\n-    the standard server binary or with a binary running distribute coordinator\n-    with `task_type` set to non-client type which will then turn into standard\n-    servers.\n+    targets for master sessions via a scope object and coordinate their\n+    training.  The cluster consisting of standard servers needs to be brought up\n+    either with the standard server binary or with a binary running distribute\n+    coordinator with `task_type` set to non-client type which will then turn\n+    into standard servers.\n \n     In addition to be the distribute coordinator, this is also the source of\n-    configurations for each job in the distributed training. As there are multiple\n-    ways to configure a distributed TensorFlow cluster, its context object\n-    provides these configurations so that users or higher-level APIs don't have to\n-    figure out the configuration for each job by themselves.\n+    configurations for each job in the distributed training. As there are\n+    multiple ways to configure a distributed TensorFlow cluster, its context\n+    object provides these configurations so that users or higher-level APIs\n+    don't have to figure out the configuration for each job by themselves.\n \n     In the between-graph replicated training, this coordinator will create\n     multiple threads and each calls the `worker_fn` which is supposed to create\n-    its own graph and connect to one worker master given by its context object. In\n-    the in-graph replicated training, it has only one thread calling this\n+    its own graph and connect to one worker master given by its context object.\n+    In the in-graph replicated training, it has only one thread calling this\n     `worker_fn`.\n \n     Another mode is the INDEPENDENT_WORKER mode where each server runs a\n-    distribute coordinator which will start a standard server and optionally runs\n-    `worker_fn` depending whether it is between-graph training or in-graph\n+    distribute coordinator which will start a standard server and optionally\n+    runs `worker_fn` depending whether it is between-graph training or in-graph\n     replicated training.\n \n     The `strategy` object is expected to be a DistributionStrategy object which\n     has implemented methods needed by distributed coordinator such as\n-    `configure(session_config, cluster_spec, task_type, task_id)` which configures\n-    the strategy object for a specific task and `experimental_should_init`\n-    property which instructs the distribute coordinator whether to run init ops\n-    for a task. The distribute coordinator will make a copy of the `strategy`\n-    object, call its `configure` method and pass it to `worker_fn` as an argument.\n+    `configure(session_config, cluster_spec, task_type, task_id)` which\n+    configures the strategy object for a specific task and\n+    `experimental_should_init` property which instructs the distribute\n+    coordinator whether to run init ops for a task. The distribute coordinator\n+    will make a copy of the `strategy` object, call its `configure` method and\n+    pass it to `worker_fn` as an argument.\n \n     The `worker_fn` defines the training logic and is called under its own\n     worker context which can be accessed to via `get_current_worker_context`. A\n     worker context provides access to configurations for each task, e.g. the\n-    task_type, task_id, master target and so on. Since `worker_fn` will be called\n-    in a thread and possibly multiple times, caller should be careful when it\n-    accesses global data. For example, it is unsafe to define flags in a\n+    task_type, task_id, master target and so on. Since `worker_fn` will be\n+    called in a thread and possibly multiple times, caller should be careful\n+    when it accesses global data. For example, it is unsafe to define flags in a\n     `worker_fn` or to define different environment variables for different\n     `worker_fn`s.\n \n@@ -547,16 +554,16 @@ def run_distribute_coordinator(\n     example, when training with parameter servers, it assigns variables to\n     parameter servers and all other operations to that worker. In the in-graph\n     replication case, the `worker_fn` has to define operations for all worker\n-    jobs. Using a distribution strategy can simplify the `worker_fn` by not having\n-    to worry about the replication and device assignment of variables and\n+    jobs. Using a distribution strategy can simplify the `worker_fn` by not\n+    having to worry about the replication and device assignment of variables and\n     operations.\n \n     This method is intended to be invoked by high-level APIs so that users don't\n     have to explicitly call it to run this coordinator. For those who don't use\n-    high-level APIs, to change a program to use this coordinator, wrap everything\n-    in a the program after global data definitions such as commandline flag\n-    definition into the `worker_fn` and get task-specific configurations from\n-    the worker context.\n+    high-level APIs, to change a program to use this coordinator, wrap\n+    everything in a the program after global data definitions such as\n+    commandline flag definition into the `worker_fn` and get task-specific\n+    configurations from the worker context.\n \n     The `cluster_spec` can be either passed by the argument or parsed from the\n     \"TF_CONFIG\" environment variable. Example of a TF_CONFIG:\n@@ -571,8 +578,8 @@ def run_distribute_coordinator(\n     this coordinator will connect to a local session.\n \n     For evaluation, if \"evaluator\" exists in the cluster_spec, a separate thread\n-    will be created to call `eval_fn` with its `task_type` set to \"evaluator\". If\n-    `eval_fn` is not defined, fall back to `worker_fn`. This implies that\n+    will be created to call `eval_fn` with its `task_type` set to \"evaluator\".\n+    If `eval_fn` is not defined, fall back to `worker_fn`. This implies that\n     evaluation will be done on a single machine if there is an \"evaluator\" task.\n     If \"evaluator\" doesn't exist in the cluster_spec, it entirely depends on the\n     `worker_fn` for how to do evaluation.\n@@ -585,21 +592,22 @@ def run_distribute_coordinator(\n         between-graph replicated training or not, whether to run init ops, etc.\n         This object will also be configured given `session_config`,\n         `cluster_spec`, `task_type` and `task_id`.\n-      eval_fn: optional function for \"evaluator\" task. If `eval_fn` is not passed\n-        in but a \"evaluator\" task is found in the `cluster_spec`, the `worker_fn`\n-        will be used for this task.\n+      eval_fn: optional function for \"evaluator\" task. If `eval_fn` is not\n+        passed in but a \"evaluator\" task is found in the `cluster_spec`, the\n+        `worker_fn` will be used for this task.\n       eval_strategy: optional DistributionStrategy object for \"evaluator\" task.\n-      cluster_spec: a dict, ClusterDef or ClusterSpec specifying servers and roles\n-        in a cluster. If not set or empty, fall back to local training.\n+      cluster_spec: a dict, ClusterDef or ClusterSpec specifying servers and\n+        roles in a cluster. If not set or empty, fall back to local training.\n       task_type: the current task type, optional if this is a client.\n       task_id: the current task id, optional if this is a client.\n-      session_config: an optional `tf.compat.v1.ConfigProto` object which will be\n-        passed to `strategy`'s `configure` method and used to create a session.\n+      session_config: an optional `tf.compat.v1.ConfigProto` object which will\n+        be passed to `strategy`'s `configure` method and used to create a\n+        session.\n       rpc_layer: optional string, the protocol for RPC, e.g. \"grpc\".\n \n     Raises:\n-      ValueError: if `cluster_spec` is supplied but not a dict or a ClusterDef or\n-        a ClusterSpec.\n+      ValueError: if `cluster_spec` is supplied but not a dict or a ClusterDef\n+        or a ClusterSpec.\n \n     Returns:\n       In the client job, return the value returned by `worker_fn` if\n@@ -680,8 +688,8 @@ def run_distribute_coordinator(\n                 \"strategy will be used for evaluation.\"\n             )\n \n-        # Every one starts a standard server, get session config from `configure`\n-        # method.\n+        # Every one starts a standard server, get session config from\n+        # `configure` method.\n         _configure_session_config_for_std_servers(\n             strategy,\n             eval_strategy,\n@@ -694,9 +702,10 @@ def run_distribute_coordinator(\n         if task_type != _TaskType.EVALUATOR and not getattr(\n             strategy.extended, \"_std_server_started\", False\n         ):\n-            # Right now, with eager mode, context is configured with a std server at\n-            # the very beginning while with graph mode the std server is started when\n-            # distribute coordinator is called. We should consolidate these two paths.\n+            # Right now, with eager mode, context is configured with a std\n+            # server at the very beginning while with graph mode the std server\n+            # is started when distribute coordinator is called. We should\n+            # consolidate these two paths.\n             server = _run_std_server(\n                 cluster_spec=cluster_spec,\n                 task_type=task_type,\n\n@@ -382,14 +382,16 @@ class TestDistributionStrategyWithNumpyArrays(\n             replica_scale_factor = distribution.num_replicas_in_sync\n \n         with self.cached_session():\n-            # Default global batch size 32 for input with 64 samples run in 2 steps\n+            # Default global batch size 32 for input with 64 samples run in 2\n+            # steps\n             steps, batch_size = distributed_training_utils_v1.get_input_params(\n                 distribution, 64, steps=None, batch_size=None\n             )\n             self.assertEqual(batch_size, 32 // replica_scale_factor)\n             self.assertEqual(steps, 2)\n \n-            # Computed global batch size 20 is lower than 32 if we pass less samples.\n+            # Computed global batch size 20 is lower than 32 if we pass less\n+            # samples.\n             steps, batch_size = distributed_training_utils_v1.get_input_params(\n                 distribution, 20, steps=None, batch_size=None\n             )\n@@ -411,14 +413,16 @@ class TestDistributionStrategyWithNumpyArrays(\n             replica_scale_factor = distribution.num_replicas_in_sync\n \n         with self.cached_session():\n-            # Computed global batch size is correct for number of specified 1 step\n+            # Computed global batch size is correct for number of specified 1\n+            # step\n             steps, batch_size = distributed_training_utils_v1.get_input_params(\n                 distribution, 64, steps=1, batch_size=None\n             )\n             self.assertEqual(batch_size, 64 // replica_scale_factor)\n             self.assertEqual(steps, 1)\n \n-            # Computed global batch size is correct for number of specified 2 steps\n+            # Computed global batch size is correct for number of specified 2\n+            # steps\n             steps, batch_size = distributed_training_utils_v1.get_input_params(\n                 distribution, 64, steps=2, batch_size=None\n             )\n@@ -530,8 +534,9 @@ class TestDistributionStrategyWithNumpyArrays(\n                     validation_data=(inputs, targets),\n                 )\n \n-                # TODO(anjalisridhar): We need tests for when the batch size and steps\n-                # are smaller and results in a 0 batch_size and steps value.\n+                # TODO(anjalisridhar): We need tests for when the batch size and\n+                # steps are smaller and results in a 0 batch_size and steps\n+                # value.\n                 model.evaluate(inputs, targets)\n                 model.evaluate(inputs, targets, batch_size=8)\n \n@@ -569,9 +574,9 @@ class TestDistributionStrategyWithNumpyArrays(\n             metrics = [\"mae\"]\n             model.compile(optimizer, loss, metrics=metrics)\n \n-            # We need to pass float32 since TPUs do not support float64, even though\n-            # these arrays will immediately be casted to bfloat16 on TPUs. We also\n-            # cannot pass bfloat16, as Numpy does not support it.\n+            # We need to pass float32 since TPUs do not support float64, even\n+            # though these arrays will immediately be casted to bfloat16 on\n+            # TPUs. We also cannot pass bfloat16, as Numpy does not support it.\n             inputs = np.zeros((64, 3), dtype=\"float32\")\n             targets = np.zeros((64, 4), dtype=\"float32\")\n \n@@ -595,9 +600,9 @@ class TestDistributionStrategyWithNumpyArrays(\n     )\n     def test_operator_overload_mixed_precision(self, distribution):\n         # Regression test that tests a fixed bug does not reoccur. Adding an\n-        # AutoCastVariable to a tensor on a TPU, where the variable was the LHS of\n-        # the '+' operator, used to cause the gradient w.r.t. the variable to be\n-        # None.\n+        # AutoCastVariable to a tensor on a TPU, where the variable was the LHS\n+        # of the '+' operator, used to cause the gradient w.r.t. the variable to\n+        # be None.\n         if isinstance(\n             distribution,\n             (\n@@ -694,8 +699,8 @@ class TestDistributionStrategyWithNumpyArrays(\n             # Call fit with validation data\n             model.fit(inputs, targets, epochs=1, batch_size=8, verbose=0)\n \n-            # TODO(anjalisridhar): We need tests for when the batch size and steps are\n-            # smaller and results in a 0 batch_size and steps value.\n+            # TODO(anjalisridhar): We need tests for when the batch size and\n+            # steps are smaller and results in a 0 batch_size and steps value.\n             model.evaluate(inputs, targets)\n             model.evaluate(inputs, targets, batch_size=8)\n \n@@ -729,16 +734,18 @@ class TestDistributionStrategyWithNumpyArrays(\n                 verbose=1,\n             )\n \n-            # The per sample loss is multiplied by the corresponding sample weight.\n-            # The average of these weighted losses is the return value of the\n-            # `evaluate` call. For example, in the test above the average weighted\n-            # loss is calculated in the following manner:\n+            # The per sample loss is multiplied by the corresponding sample\n+            # weight.  The average of these weighted losses is the return value\n+            # of the `evaluate` call. For example, in the test above the average\n+            # weighted loss is calculated in the following manner:\n \n-            # batch_1 = (((2-0)^2) * 0.25 + ((4-1)^2) * 0.5) / 2 = 5.5 / 2 = 2.75\n+            # batch_1 = (((2-0)^2) * 0.25 + ((4-1)^2) * 0.5) / 2 = 5.5 / 2 =\n+            # 2.75\n             # batch_2 = (((6-2)^2 * 0.75) + ((8-3)^2 * 1)) / 2 = 37 / 2 = 18.5\n             # final result = (batch_1 + batch_2) / 2 = 10.625.\n-            # The first time we divide by number of input samples and the second time\n-            # we divide by number of steps/batches that the loss is aggregated over.\n+            # The first time we divide by number of input samples and the second\n+            # time we divide by number of steps/batches that the loss is\n+            # aggregated over.\n             self.assertAllClose(result, 10.625)\n \n             # We now test without passing sample_weights:\n@@ -760,18 +767,20 @@ class TestDistributionStrategyWithNumpyArrays(\n                 loss = \"mse\"\n                 model.compile(optimizer, loss)\n \n-            # We take 6 input samples with each input having a dimension of 3 or 5.\n+            # We take 6 input samples with each input having a dimension of 3 or\n+            # 5.\n             input_a_np = np.asarray(np.random.random((6, 3)), dtype=np.float32)\n             input_b_np = np.asarray(np.random.random((6, 5)), dtype=np.float32)\n             inputs = [input_a_np, input_b_np]\n \n             outs = model.predict(inputs)\n-            # `predict` a list that is equal in length to the number of model outputs.\n-            # In this test our model has two outputs and each element of `outs`\n-            # corresponds to all the samples of one of the model outputs.\n+            # `predict` a list that is equal in length to the number of model\n+            # outputs.  In this test our model has two outputs and each element\n+            # of `outs` corresponds to all the samples of one of the model\n+            # outputs.\n             self.assertLen(outs, 2)\n-            # Each of the output samples have a dimension of 7. We should process all\n-            # the available input samples(6).\n+            # Each of the output samples have a dimension of 7. We should\n+            # process all the available input samples(6).\n             self.assertAllEqual([6, 7], outs[0].shape)\n             self.assertAllEqual([6, 7], outs[1].shape)\n \n@@ -797,16 +806,16 @@ class TestDistributionStrategyWithNumpyArrays(\n             x = np.random.random((10, 3)).astype(\"float32\")\n             y = np.random.random((10, 4)).astype(\"float32\")\n \n-            # As sample size is 10, we batch by 4 so that the last batch is\n-            # a partial batch. Also `evaluate()` using numpy array as inputs without\n-            # distribution strategy uses entire sample as a single batch. As so,\n-            # we remove parameters `batch_size` and `steps`.\n+            # As sample size is 10, we batch by 4 so that the last batch is a\n+            # partial batch. Also `evaluate()` using numpy array as inputs\n+            # without distribution strategy uses entire sample as a single\n+            # batch. As so, we remove parameters `batch_size` and `steps`.\n             cpu_model.set_weights(model_with_ds_strategy.get_weights())\n             evaluate_ground_truth = cpu_model.evaluate(x, y)\n \n-            # We don't compare the loss as loss is currently not computed as metric\n-            # in Keras, the loss value is inaccurate for last partial batch due to\n-            # more weights for the last batch samples.\n+            # We don't compare the loss as loss is currently not computed as\n+            # metric in Keras, the loss value is inaccurate for last partial\n+            # batch due to more weights for the last batch samples.\n             steps = np.ceil(10.0 / batch_size)\n             self.assertAllClose(\n                 model_with_ds_strategy.evaluate(\n@@ -816,7 +825,8 @@ class TestDistributionStrategyWithNumpyArrays(\n                 atol=1e-5,\n                 rtol=1e-5,\n             )\n-            # Test that `steps` is inferred correctly when final partial batch exists.\n+            # Test that `steps` is inferred correctly when final partial batch\n+            # exists.\n             self.assertAllClose(\n                 model_with_ds_strategy.evaluate(x, y, batch_size=batch_size)[\n                     1:\n@@ -846,9 +856,9 @@ class TestDistributionStrategyWithNumpyArrays(\n             inputs = np.random.random((10, 3)).astype(np.float32)\n \n             # As sample size is 10, we batch by 4 so that the last batch is\n-            # a partial batch. Also `predict()` using numpy array as inputs without\n-            # distribution strategy uses entire sample as a single batch. As so,\n-            # we remove parameters `batch_size` and `steps`.\n+            # a partial batch. Also `predict()` using numpy array as inputs\n+            # without distribution strategy uses entire sample as a single\n+            # batch. As so, we remove parameters `batch_size` and `steps`.\n             cpu_model.set_weights(model_with_ds_strategy.get_weights())\n             predict_ground_truth = cpu_model.predict(inputs)\n             self.assertAllClose(\n@@ -857,7 +867,8 @@ class TestDistributionStrategyWithNumpyArrays(\n                 atol=1e-5,\n                 rtol=1e-5,\n             )\n-            # Test that `steps` is inferred correctly when final partial batch exists.\n+            # Test that `steps` is inferred correctly when final partial batch\n+            # exists.\n             self.assertAllClose(\n                 model_with_ds_strategy.predict(inputs, batch_size=4),\n                 predict_ground_truth,\n@@ -1256,8 +1267,8 @@ class TestDistributionStrategyWithDatasets(\n     def test_on_dataset_with_unknown_cardinality_without_steps(\n         self, distribution, mode\n     ):\n-        # TODO(b/155867206): Investigate why this test occasionally segfaults on TPU\n-        # in eager mode.\n+        # TODO(b/155867206): Investigate why this test occasionally segfaults on\n+        # TPU in eager mode.\n         if mode == \"eager\" and backend.is_tpu_strategy(distribution):\n             self.skipTest(\"caused segfault with TPU in eager mode.\")\n \n@@ -1488,9 +1499,9 @@ class TestDistributionStrategyWithDatasets(\n         )\n     )\n     def test_learning_phase_value(self, distribution):\n-        # TODO(anjalisridhar): Modify this test to use Lambdas since we can compare\n-        # meaningful values. Currently we don't pass the learning phase if the\n-        # Lambda layer uses the learning phase.\n+        # TODO(anjalisridhar): Modify this test to use Lambdas since we can\n+        # compare meaningful values. Currently we don't pass the learning phase\n+        # if the Lambda layer uses the learning phase.\n         with self.cached_session():\n             with distribution.scope():\n                 x = keras.layers.Input(shape=(1,), name=\"input\")\n@@ -1525,8 +1536,8 @@ class TestDistributionStrategyWithDatasets(\n \n             with distribution.scope():\n                 model.set_weights(initial_weights)\n-            # TODO(psv/anjalisridhar): Enable these lines after we fix b/117431185.\n-            # evaluate_output = model.evaluate(dataset, steps=20)\n+            # TODO(psv/anjalisridhar): Enable these lines after we fix\n+            # b/117431185.  evaluate_output = model.evaluate(dataset, steps=20)\n             # self.assertAlmostEqual(evaluate_output[1], 1, 0)\n \n             inputs = np.ones((10, 1), dtype=np.float32)\n@@ -1594,9 +1605,9 @@ class TestDistributionStrategyWithDatasets(\n             cpu_model.set_weights(model_with_ds_strategy.get_weights())\n             dataset_with_partial_batch = dataset.batch(batch_size)\n \n-            # We don't compare the loss as loss is currently not computed as metric\n-            # in Keras, the loss value is inaccurate for last partial batch due to\n-            # more weights for the last batch samples.\n+            # We don't compare the loss as loss is currently not computed as\n+            # metric in Keras, the loss value is inaccurate for last partial\n+            # batch due to more weights for the last batch samples.\n             steps = np.ceil(10.0 / batch_size)\n             self.assertAllClose(\n                 model_with_ds_strategy.evaluate(\n@@ -1718,12 +1729,12 @@ class TestDistributionStrategyWithDatasets(\n         with self.cached_session():\n             with distribution.scope():\n                 input_a, input_b, output = _create_model_input_output_tensors()\n-                # `input_a`, which has input name that comes last in alphanumeric\n-                # order, is the first input of the model input layers. If tensors\n-                # from `input_dict` is blindly flattened and passed to model\n-                # inputs incorrectly, this would result in `input_a` input layer\n-                # matching with tensor `a_input_sorted_first` and would result in\n-                # shape mismatch.\n+                # `input_a`, which has input name that comes last in\n+                # alphanumeric order, is the first input of the model input\n+                # layers. If tensors from `input_dict` is blindly flattened and\n+                # passed to model inputs incorrectly, this would result in\n+                # `input_a` input layer matching with tensor\n+                # `a_input_sorted_first` and would result in shape mismatch.\n                 model_with_array_input = keras.models.Model(\n                     inputs=[input_a, input_b], outputs=output\n                 )\n@@ -1776,15 +1787,17 @@ class TestDistributionStrategyWithDatasets(\n             ).batch(2)\n             result = model.evaluate(ds, verbose=1)\n \n-            # The per sample loss is multiplied by the corresponding sample weight.\n-            # The average of these weighted losses is the return value of the\n-            # `evaluate` call. For example, in the test above the average weighted\n-            # loss is calculated in the following manner:\n-            # batch_1 = (((2-0)^2) * 0.25 + ((4-1)^2) * 0.5) / 2 = 5.5 / 2 = 2.75\n+            # The per sample loss is multiplied by the corresponding sample\n+            # weight.  The average of these weighted losses is the return value\n+            # of the `evaluate` call. For example, in the test above the average\n+            # weighted loss is calculated in the following manner:\n+            # batch_1 = (((2-0)^2) * 0.25 + ((4-1)^2) * 0.5) / 2 = 5.5 / 2 =\n+            # 2.75\n             # batch_2 = (((6-2)^2 * 0.75) + ((8-3)^2 * 1)) / 2 = 37 / 2 = 18.5\n             # final result = (batch_1 + batch_2) / 2 = 10.625.\n-            # The first time we divide by number of input samples and the second time\n-            # we divide by number of steps/batches that the loss is aggregated over.\n+            # The first time we divide by number of input samples and the second\n+            # time we divide by number of steps/batches that the loss is\n+            # aggregated over.\n             self.assertAllClose(result, 10.625)\n \n             # We now test without passing sample_weights:\n@@ -1817,12 +1830,12 @@ class TestDistributionStrategyWithDatasetsFile(\n     def test_predict_on_dataset_shard_options_file_multi_worker_mirrored(\n         self, distribution, mode\n     ):\n-        # This test is to verify if we successfully switch auto_shard_policy of a\n-        # input dataset inside model.predict with MultiWorkerMirroredStrategy to\n-        # AutoShardPolicy.DATA. Since there is only one input file for multiple\n-        # workers, AutoShardPolicy.AUTO or AutoShardPolicy.FILE will lead to an\n-        # error. However, since we switch to AutoShardPolicy.DATA in model.predict,\n-        # no error is raised.\n+        # This test is to verify if we successfully switch auto_shard_policy of\n+        # a input dataset inside model.predict with MultiWorkerMirroredStrategy\n+        # to AutoShardPolicy.DATA. Since there is only one input file for\n+        # multiple workers, AutoShardPolicy.AUTO or AutoShardPolicy.FILE will\n+        # lead to an error. However, since we switch to AutoShardPolicy.DATA in\n+        # model.predict, no error is raised.\n         del mode\n         with distribution.scope():\n             optimizer_fn = gradient_descent_keras.SGD\n@@ -1880,15 +1893,16 @@ class TestRegularizerLoss(tf.test.TestCase, parameterized.TestCase):\n         ):\n             batch_size //= distribution.num_replicas_in_sync\n \n-            # Given an input x, which is always 1, and variable v, this model computes\n-            # Loss=x+v+regularizer_loss, where regularizer_loss=v and the variable is\n-            # initialized to 1. Therefore, this model computes Loss=1+2v, and so the\n-            # gradient dLoss/dv = 2. This gradient of 2 is averaged over all examples\n-            # in a batch and then multiplied by the learning rate of 1. As a result,\n-            # the model update for one batch should subtract 2 from v, resulting in v\n-            # being -1. If the regularizer loss is not scaled correctly by number of\n-            # replicas, the variable value will be incorrect when number of replicas\n-            # >1. For e.g. it will be -2 if num replicas = 2.\n+            # Given an input x, which is always 1, and variable v, this model\n+            # computes Loss=x+v+regularizer_loss, where regularizer_loss=v and\n+            # the variable is initialized to 1. Therefore, this model computes\n+            # Loss=1+2v, and so the gradient dLoss/dv = 2. This gradient of 2 is\n+            # averaged over all examples in a batch and then multiplied by the\n+            # learning rate of 1. As a result, the model update for one batch\n+            # should subtract 2 from v, resulting in v being -1. If the\n+            # regularizer loss is not scaled correctly by number of replicas,\n+            # the variable value will be incorrect when number of replicas >1.\n+            # For e.g. it will be -2 if num replicas = 2.\n         with distribution.scope():\n             x = keras.layers.Input(shape=(1,), batch_size=batch_size)\n             y = TestRegularizerLoss.AddLayer()(x)\n@@ -2674,8 +2688,9 @@ class TestDistributionStrategyWithKerasModels(\n                             loss, global_batch_size=batch_size\n                         )\n \n-                        # Verify that the loss computed in this loop is equivalent to the\n-                        # loss from the model that was added via add_loss.\n+                        # Verify that the loss computed in this loop is\n+                        # equivalent to the loss from the model that was added\n+                        # via add_loss.\n                         tf.compat.v1.assert_equal(loss, loss_from_model)\n \n                     grads = tape.gradient(loss, model.trainable_variables)\n@@ -2739,8 +2754,8 @@ def _functional_with_add_loss_and_metric(input_shape, num_classes, l1, l2):\n     x = keras.layers.MaxPooling2D(pool_size=2)(x)\n     x = keras.layers.Conv2D(64, kernel_size=5, activation=\"relu\")(x)\n     x = keras.layers.MaxPooling2D(pool_size=2)(x)\n-    # Apply L2 regularization to embedding. Use a mix of TensorFlow ops and layers\n-    # to exercise all code paths.\n+    # Apply L2 regularization to embedding. Use a mix of TensorFlow ops and\n+    # layers to exercise all code paths.\n     x = keras.layers.Flatten(name=\"embedding\")(x)\n     l2_loss = tf.reduce_mean(tf.reduce_sum(tf.square(x), -1))\n     # Apply L1 regularization to next layer.\n@@ -2855,7 +2870,8 @@ class TestDistributionStrategyWithMultipleAddLossAndMetricCalls(\n         dataset = dataset.shuffle(64).batch(\n             8 * distribution.num_replicas_in_sync, drop_remainder=True\n         )\n-        # Make model with distribution strategy and initialize with dataset shape.\n+        # Make model with distribution strategy and initialize with dataset\n+        # shape.\n         input_shape = tf.data.experimental.get_structure(dataset)[0].shape[1:]\n         with distribution.scope():\n             model = model_fn(input_shape, 10, l1, l2)\n@@ -2930,7 +2946,8 @@ class TestModelCapturesStrategy(tf.test.TestCase, parameterized.TestCase):\n             model = DeterministicModel(distribution)\n             optimizer = keras.optimizers.adam_v2.Adam(1e-4)\n \n-        # Compile & evaluate the model outside of the distribution strategy scope\n+        # Compile & evaluate the model outside of the distribution strategy\n+        # scope\n         model.compile(\n             optimizer=optimizer,\n             loss=keras.losses.MeanSquaredError(),\n\n@@ -83,8 +83,8 @@ def write_dirpath(dirpath, strategy):\n         # Infer strategy from `distribution_strategy_context` if not given.\n         strategy = tf.distribute.get_strategy()\n     if strategy is None:\n-        # If strategy is still not available, this is not in distributed training.\n-        # Fallback to original dirpath.\n+        # If strategy is still not available, this is not in distributed\n+        # training.  Fallback to original dirpath.\n         return dirpath\n     if (\n         not strategy.extended._in_multi_worker_mode()\n@@ -108,14 +108,14 @@ def remove_temp_dirpath(dirpath, strategy):\n         # Infer strategy from `distribution_strategy_context` if not given.\n         strategy = tf.distribute.get_strategy()\n     if strategy is None:\n-        # If strategy is still not available, this is not in distributed training.\n-        # Fallback to no-op.\n+        # If strategy is still not available, this is not in distributed\n+        # training.  Fallback to no-op.\n         return\n-    # TODO(anjalisridhar): Consider removing the check for multi worker mode since\n-    # it is redundant when used with the should_checkpoint property.\n+    # TODO(anjalisridhar): Consider removing the check for multi worker mode\n+    # since it is redundant when used with the should_checkpoint property.\n     if (\n         strategy.extended._in_multi_worker_mode()\n-        and not strategy.extended.should_checkpoint  # pylint: disable=protected-access\n+        and not strategy.extended.should_checkpoint\n     ):\n         # If this worker is not chief and hence should not save file, remove\n         # the temporary directory.\n\n@@ -74,22 +74,22 @@ def unwrap_values(\n \n     This function calls `flatten_per_replica_values` to parse each of the input\n     parameters into a list of values on the different devices. If we set\n-    `with_loss_tensor` to be True, we also call `reduce` on the list of losses on\n-    the different devices to give us one loss tensor.\n+    `with_loss_tensor` to be True, we also call `reduce` on the list of losses\n+    on the different devices to give us one loss tensor.\n \n     Args:\n-      distribution_strategy: DistributionStrategy used to distribute training and\n-          validation.\n+      distribution_strategy: DistributionStrategy used to distribute training\n+          and validation.\n       grouped_inputs: PerReplica inputs returned from the train or test function\n           that we ran on each device.\n-      grouped_outputs: PerReplica outputs returned from the train or test function\n-          that we ran on each device.\n-      grouped_updates: PerReplica updates returned from the train or test function\n-          that we ran on each device.\n+      grouped_outputs: PerReplica outputs returned from the train or test\n+          function that we ran on each device.\n+      grouped_updates: PerReplica updates returned from the train or test\n+          function that we ran on each device.\n       grouped_session_args: PerReplica session args returned from the train or\n           test function that we ran on each device.\n-      with_loss_tensor: Boolean that indicates if we need to add the reduced loss\n-          tensor as one of the outputs.\n+      with_loss_tensor: Boolean that indicates if we need to add the reduced\n+          loss tensor as one of the outputs.\n \n     Returns:\n       Values of each of the PerReplica parameters.\n@@ -134,9 +134,9 @@ def unwrap_output_dict(strategy, grouped_outputs, mode):\n     if mode == ModeKeys.PREDICT:\n         return flatten_per_replica_values(strategy, grouped_outputs)\n \n-    # In the case of fit/eval, the grouped_outputs is a dict, whereas in predict,\n-    # the output is as same structure as model output. They need to be treated\n-    # differently\n+    # In the case of fit/eval, the grouped_outputs is a dict, whereas in\n+    # predict, the output is as same structure as model output. They need to be\n+    # treated differently\n     total_loss = strategy.reduce(\n         tf.distribute.ReduceOp.SUM, grouped_outputs[\"total_loss\"][0], axis=None\n     )\n@@ -151,12 +151,12 @@ def unwrap_output_dict(strategy, grouped_outputs, mode):\n         backend.is_tpu_strategy(strategy)\n         and tf.compat.v1.executing_eagerly_outside_functions()\n     ):\n-        # Choose 1 value per replica in the TPU case since all replicas produce the\n-        # same output.\n+        # Choose 1 value per replica in the TPU case since all replicas produce\n+        # the same output.\n         # We only do this in eager mode for now since this function is used in\n         # both graph and eager mode and in the graph case we currently don't use\n-        # experimental_run so would need to be removed when we converge the graph\n-        # code path as well.\n+        # experimental_run so would need to be removed when we converge the\n+        # graph code path as well.\n         output_losses = output_losses[:: strategy.num_replicas_in_sync]\n         metrics = metrics[:: strategy.num_replicas_in_sync]\n     return {\n@@ -174,16 +174,16 @@ def unwrap_outputs(\n \n     This function calls `flatten_per_replica_values` to parse each of the input\n     parameters into a list of outputs on the different devices. If we set\n-    `with_loss_tensor` to be True, we also call `reduce` on the list of losses on\n-    the different devices to give us one loss tensor.\n+    `with_loss_tensor` to be True, we also call `reduce` on the list of losses\n+    on the different devices to give us one loss tensor.\n \n     Args:\n-      distribution_strategy: DistributionStrategy used to distribute training and\n-          validation.\n-      grouped_outputs: PerReplica outputs returned from the train or test function\n-          that we ran on each device.\n-      with_loss_tensor: Boolean that indicates if we need to add the reduced loss\n-          tensor as one of the outputs.\n+      distribution_strategy: DistributionStrategy used to distribute training\n+          and validation.\n+      grouped_outputs: PerReplica outputs returned from the train or test\n+          function that we ran on each device.\n+      with_loss_tensor: Boolean that indicates if we need to add the reduced\n+          loss tensor as one of the outputs.\n \n     Returns:\n       Values of each of the PerReplica outputs.\n@@ -207,12 +207,12 @@ def unwrap_outputs(\n         backend.is_tpu_strategy(distribution_strategy)\n         and tf.compat.v1.executing_eagerly_outside_functions()\n     ):\n-        # Choose 1 value per replica in the TPU case since all replicas produce the\n-        # same output.\n+        # Choose 1 value per replica in the TPU case since all replicas produce\n+        # the same output.\n         # We only do this in eager mode for now since this function is used in\n         # both graph and eager mode and in the graph case we currently don't use\n-        # experimental_run so would need to be removed when we converge the graph\n-        # code path as well.\n+        # experimental_run so would need to be removed when we converge the\n+        # graph code path as well.\n         all_outputs = all_outputs[:: distribution_strategy.num_replicas_in_sync]\n     return [loss] + all_outputs\n \n@@ -226,17 +226,18 @@ def flatten_per_replica_values(distribution_strategy, per_replica_values):\n     of PerReplica values and return all the values in the PerReplica dict.\n \n     Args:\n-      distribution_strategy: DistributionStrategy used to distribute training and\n-        validation.\n-      per_replica_values: List of PerReplica object or a single PerReplica object.\n+      distribution_strategy: DistributionStrategy used to distribute training\n+        and validation.\n+      per_replica_values: List of PerReplica object or a single PerReplica\n+        object.\n \n     Returns:\n       List of values of all the PerReplica objects.\n \n     \"\"\"\n     # pylint: disable=g-complex-comprehension\n-    # This function takes a PerReplica object or a list of PerReplica objects and\n-    # returns all the values associated with it.\n+    # This function takes a PerReplica object or a list of PerReplica objects\n+    # and returns all the values associated with it.\n     return [\n         e\n         for flattened in tf.nest.flatten(per_replica_values)\n@@ -252,10 +253,10 @@ def validate_callbacks(input_callbacks, optimizer):\n       optimizer: Optimizer instance used to train the model.\n \n     Raises:\n-      ValueError: If `LearningRateScheduler` or `ReduceLROnPlateau` is one of the\n-          callbacks passed.\n-      ValueError: If `write_grads` is one of the parameters passed as part of the\n-          TensorBoard callback.\n+      ValueError: If `LearningRateScheduler` or `ReduceLROnPlateau` is one of\n+          the callbacks passed.\n+      ValueError: If `write_grads` is one of the parameters passed as part of\n+          the TensorBoard callback.\n     \"\"\"\n     if input_callbacks:\n         for callback in input_callbacks:\n@@ -270,16 +271,16 @@ def validate_callbacks(input_callbacks, optimizer):\n                         \"%s callback with DistributionStrategy.\" % callback\n                     )\n \n-            # If users want to use the TensorBoard callback they cannot use certain\n-            # features of the callback that involve accessing model attributes and\n-            # running ops.\n+            # If users want to use the TensorBoard callback they cannot use\n+            # certain features of the callback that involve accessing model\n+            # attributes and running ops.\n             if isinstance(callback, callbacks.TensorBoard):\n                 if getattr(callback, \"write_grads\", False):\n                     logging.warning(\n                         UserWarning(\n-                            \"`write_grads` in the TensorBoard callback is not supported \"\n-                            \"when using DistributionStrategy. Setting `write_grads` \"\n-                            \"to `False`.\"\n+                            \"`write_grads` in the TensorBoard callback is not \"\n+                            \"supported when using DistributionStrategy. \"\n+                            \"Setting `write_grads` to `False`.\"\n                         )\n                     )\n                     callback.write_grads = False\n@@ -301,9 +302,9 @@ def validate_distributed_dataset_inputs(\n           `MirroredStrategy` this is a PerReplica object with a tensor for each\n           device set in the dict. y can also be a tuple or dict. The keys of the\n           dict should match the names of the output layers of the model.\n-      sample_weights: Sample weights Dataset DistributedValue object. For example,\n-          when we use `MirroredStrategy` this is a PerReplica object with a tensor\n-          for each device set in the dict.\n+      sample_weights: Sample weights Dataset DistributedValue object. For\n+          example, when we use `MirroredStrategy` this is a PerReplica object\n+          with a tensor for each device set in the dict.\n \n     Returns:\n       The unwrapped values list of the x and y DistributedValues inputs.\n@@ -351,7 +352,8 @@ def validate_per_replica_inputs(distribution_strategy, x):\n       the input list.\n \n     Raises:\n-      ValueError: If any of the objects in the `per_replica_list` is not a tensor.\n+      ValueError: If any of the objects in the `per_replica_list` is not a\n+        tensor.\n \n     \"\"\"\n     # Convert the inputs and targets into a list of PerReplica objects.\n@@ -368,7 +370,8 @@ def validate_per_replica_inputs(distribution_strategy, x):\n                 )\n \n         if not tf.executing_eagerly():\n-            # Validate that the shape and dtype of all the elements in x are the same.\n+            # Validate that the shape and dtype of all the elements in x are the\n+            # same.\n             validate_all_tensor_shapes(x, x_values)\n         validate_all_tensor_types(x, x_values)\n \n@@ -424,7 +427,8 @@ def _wait_for_variable_initialization(session):\n \n \n def init_restore_or_wait_for_variables():\n-    \"\"\"Initialize or restore variables or wait for variables to be initialized.\"\"\"\n+    \"\"\"Initialize or restore variables or wait for variables to be\n+    initialized.\"\"\"\n     backend._initialize_variables(\n         backend._get_session()\n     )  # pylint: disable=protected-access\n@@ -508,8 +512,8 @@ def get_input_params(\n         distribution_strategy\n     )\n \n-    # TODO(b/128995245): In eager mode, uneven batch sizes are allowed except for\n-    # `fit()` on TPUStrategy.\n+    # TODO(b/128995245): In eager mode, uneven batch sizes are allowed except\n+    # for `fit()` on TPUStrategy.\n     # In graph mode, the zero batch case in batch norm is not handled due to\n     # XLA-GPU regression. Uneven batch sizes are not allowed except\n     # for `test()` and `predict()` on TPUStrategy.\n@@ -526,13 +530,14 @@ def get_input_params(\n \n     if steps is None:\n         if batch_size is None:\n-            # If neither the batch size or number of steps are set. We choose the\n-            # global batch size as the minimum of number of samples and 32. 32 is\n-            # chosen to provide backward compatibility.\n+            # If neither the batch size or number of steps are set. We choose\n+            # the global batch size as the minimum of number of samples and 32.\n+            # 32 is chosen to provide backward compatibility.\n             global_batch_size = min(num_samples, 32)\n         else:\n             # If the user provided the batch size we need to handle the case\n-            # between different strategies that use the global/per-replica batch size\n+            # between different strategies that use the global/per-replica batch\n+            # size\n             global_batch_size = batch_size\n             if use_per_replica_batch:\n                 global_batch_size *= distribution_strategy.num_replicas_in_sync\n@@ -558,7 +563,8 @@ def get_input_params(\n             global_batch_size = num_samples // steps\n         else:\n             # If the user provided the batch size we need to handle the case\n-            # between different strategies that use the global/per-replica batch size\n+            # between different strategies that use the global/per-replica batch\n+            # size\n             global_batch_size = batch_size\n             if use_per_replica_batch:\n                 global_batch_size *= distribution_strategy.num_replicas_in_sync\n@@ -576,12 +582,13 @@ def get_input_params(\n                     % (num_samples, global_batch_size, steps)\n                 )\n \n-    # We need to return the per replica or global batch size based on the strategy\n+    # We need to return the per replica or global batch size based on the\n+    # strategy\n     if use_per_replica_batch:\n         if global_batch_size % distribution_strategy.num_replicas_in_sync:\n             raise ValueError(\n-                \"The batch size (%s) could not be sharded evenly across the sync \"\n-                \"replicas (%s) in the distribution strategy.\"\n+                \"The batch size (%s) could not be sharded evenly across the \"\n+                \"sync replicas (%s) in the distribution strategy.\"\n                 % (\n                     global_batch_size,\n                     distribution_strategy.num_replicas_in_sync,\n@@ -624,8 +631,8 @@ def _get_input_from_iterator(iterator, model):\n \n     # `len(nest.flatten(x))` is going to not count empty elements such as {}.\n     # len(nest.flatten([[0,1,2], {}])) is 3 and not 4. The `next_element` is\n-    # going to get flattened in `_prepare_feed_values` to work around that. Empty\n-    # elements are going to get filtered out as part of the flattening.\n+    # going to get flattened in `_prepare_feed_values` to work around that.\n+    # Empty elements are going to get filtered out as part of the flattening.\n     if len(tf.nest.flatten(next_element)) == len(model.inputs):\n         x = next_element\n         y = None\n@@ -673,8 +680,8 @@ def _prepare_feed_values(model, inputs, targets, sample_weights, mode):\n         inputs = flatten_per_replica_values(strategy, inputs)\n         targets = flatten_per_replica_values(strategy, targets)\n         # Expand 1-dimensional inputs.\n-        # TODO(b/124535720): Remove once this standarize data logic is shared with\n-        # main flow.\n+        # TODO(b/124535720): Remove once this standarize data logic is shared\n+        # with main flow.\n         inputs, targets = tf.nest.map_structure(\n             training_utils_v1.standardize_single_array, (inputs, targets)\n         )\n@@ -723,8 +730,8 @@ def _custom_compile_for_predict(model):\n     \"\"\"Custom compile for TPU predict mode.\"\"\"\n     if not model.built:\n         # Model is not compilable because it does not know its number of inputs\n-        # and outputs, nor their shapes and names. We will compile after the first\n-        # time the model gets called on training data.\n+        # and outputs, nor their shapes and names. We will compile after the\n+        # first time the model gets called on training data.\n         return\n     model._is_compiled = True\n     model.total_loss = None\n@@ -741,12 +748,12 @@ def _build_network_on_replica(model, mode, inputs=None, targets=None):\n     placeholders for the input and the output that are not accessible till we\n     call iterator.get_next() inside the step_fn for `fit`/`evaluate`/`predict`.\n \n-    The sharing of weights and layers between the old and the new model guarantee\n-    that we're using Strategy variables and any updates on either model are\n-    reflected correctly in callbacks and loop iterations.\n+    The sharing of weights and layers between the old and the new model\n+    guarantee that we're using Strategy variables and any updates on either\n+    model are reflected correctly in callbacks and loop iterations.\n \n-    We need to make sure we share the optimizers between the old and the new model\n-    as well so that optimizer state is not lost if the user is running fit\n+    We need to make sure we share the optimizers between the old and the new\n+    model as well so that optimizer state is not lost if the user is running fit\n     multiple times.\n \n     Args:\n@@ -762,8 +769,8 @@ def _build_network_on_replica(model, mode, inputs=None, targets=None):\n     from keras import models  # pylint: disable=g-import-not-at-top\n     from keras.engine import sequential  # pylint: disable=g-import-not-at-top\n \n-    # We rely on the internal methods to avoid having share_weights weights in the\n-    # public API.\n+    # We rely on the internal methods to avoid having share_weights weights in\n+    # the public API.\n     if isinstance(model, sequential.Sequential):\n         updated_model = models._clone_sequential_model(\n             model, input_tensors=inputs, layer_fn=models.share_weights\n@@ -776,8 +783,8 @@ def _build_network_on_replica(model, mode, inputs=None, targets=None):\n         # here.\n         updated_model._callable_losses = model._callable_losses\n \n-    # Recast all low precision outputs back to float32 since we only casted\n-    # the inputs to bfloat16 and not targets. This is done so that we can preserve\n+    # Recast all low precision outputs back to float32 since we only casted the\n+    # inputs to bfloat16 and not targets. This is done so that we can preserve\n     # precision when calculating the loss value.\n     def _upcast_low_precision_outputs(output):\n         if output.dtype == tf.bfloat16:\n@@ -836,8 +843,8 @@ def _clone_and_build_model(model, mode, inputs=None, targets=None):\n         optimizer = model.optimizer.__class__.from_config(optimizer_config)\n \n     # Recast all low precision outputs back to float32 since we only casted\n-    # the inputs to bfloat16 and not targets. This is done so that we can preserve\n-    # precision when calculating the loss value.\n+    # the inputs to bfloat16 and not targets. This is done so that we can\n+    # preserve precision when calculating the loss value.\n     def _upcast_low_precision_outputs(output):\n         if output.dtype == tf.bfloat16:\n             return tf.cast(output, tf.float32)\n@@ -879,7 +886,8 @@ def clone_model_on_replicas(model, strategy, mode, inputs=None, targets=None):\n \n \n def _make_execution_function(model, mode):\n-    \"\"\"Makes or reuses function to run one step of distributed model execution.\"\"\"\n+    \"\"\"Makes or reuses function to run one step of distributed model\n+    execution.\"\"\"\n     if is_distributing_by_cloning(model):\n         return _make_execution_function_with_cloning(model, mode)\n \n@@ -904,10 +912,10 @@ def _make_execution_function_without_cloning(model, mode):\n         def distributed_function(input_fn):\n             \"\"\"A single step of the distributed execution across replicas.\"\"\"\n             x, y, sample_weights = input_fn()\n-            # Call `Model.{train,test,predict}_on_batch` on every replica passing\n-            # PerReplicas as arguments.  On every replica inside this call, each\n-            # PerReplica object will return the value for that replica.  The outputs\n-            # are PerReplicas too.\n+            # Call `Model.{train,test,predict}_on_batch` on every replica\n+            # passing PerReplicas as arguments.  On every replica inside this\n+            # call, each PerReplica object will return the value for that\n+            # replica. The outputs are PerReplicas too.\n             outputs = strategy.run(\n                 per_replica_function, args=(x, y, sample_weights)\n             )\n@@ -964,16 +972,17 @@ def _make_replicated_models_with_cloning(model, mode):\n \n \n def _make_execution_function_with_cloning(model, mode):\n-    \"\"\"Clones or re-uses models to run one step of distributed model execution.\"\"\"\n+    \"\"\"Clones or re-uses models to run one step of distributed model\n+    execution.\"\"\"\n     distributed_model = get_distributed_model(model, mode)\n     # TODO(b/134069401): Create a cache for the distributed model and exec\n-    # function that incorporates additional attributes to be part of the cache key\n-    # than just the mode.\n+    # function that incorporates additional attributes to be part of the cache\n+    # key than just the mode.\n     # If distributed model for a particular `mode` is already built, use the\n     # `_distribution_function` on that distributed model.\n-    # If you have updated the sample_weight_mode on the model, then you will need\n-    # to recompile metrics and recreate the execution function. This is indicated\n-    # by the `_recompile_exec_function` property.\n+    # If you have updated the sample_weight_mode on the model, then you will\n+    # need to recompile metrics and recreate the execution function. This is\n+    # indicated by the `_recompile_exec_function` property.\n     if (\n         distributed_model\n         and hasattr(distributed_model, \"_distribution_function\")\n@@ -1022,16 +1031,18 @@ def _make_graph_execution_function(model, mode):\n             _per_replica_function, args=(get_distributed_model(model, mode),)\n         )\n \n-        # Initialize the variables in the replicated model. This is necessary for\n-        # multi-worker training because on some workers, initialization is not\n-        # needed. This method does initialization or waiting for initialization\n-        # according to the context object of distribute coordinator.\n+        # Initialize the variables in the replicated model. This is necessary\n+        # for multi-worker training because on some workers, initialization is\n+        # not needed. This method does initialization or waiting for\n+        # initialization according to the context object of distribute\n+        # coordinator.\n         init_restore_or_wait_for_variables()\n \n-        # Unwrap all the per device values returned from `call_for_each_replica`.\n-        # Unwrapping per device values gives you a list of values that can be\n-        # used to construct a new train function that is composed of update ops on\n-        # all the devices over which the model is distributed.\n+        # Unwrap all the per device values returned from\n+        # `call_for_each_replica`.  Unwrapping per device values gives you a\n+        # list of values that can be used to construct a new train function that\n+        # is composed of update ops on all the devices over which the model is\n+        # distributed.\n         (\n             all_inputs,\n             all_outputs,\n@@ -1062,15 +1073,16 @@ def _make_eager_execution_function(model, mode):\n         f = model._make_execution_function(mode)\n         return (f.inputs, f.outputs)\n \n-    # NOTE(priyag): Try creating a new FuncGraph within DS scope instead of using\n-    # the global one.\n+    # NOTE(priyag): Try creating a new FuncGraph within DS scope instead of\n+    # using the global one.\n     strategy = model._distribution_strategy\n     global_graph = backend.get_graph()\n \n     with global_graph.as_default(), strategy.scope():\n-        # First we gather the relevant portions of the model across all replicas.\n-        # `backend._scratch_graph(global_graph)` signals to Keras that it should not\n-        # lift to a separate graph when creating the per-replica functions.\n+        # First we gather the relevant portions of the model across all\n+        # replicas.  `backend._scratch_graph(global_graph)` signals to Keras\n+        # that it should not lift to a separate graph when creating the\n+        # per-replica functions.\n         with backend._scratch_graph(global_graph):\n             # Create train ops on each of the devices when we call\n             # `_per_replica_fit_function`.\n@@ -1080,10 +1092,11 @@ def _make_eager_execution_function(model, mode):\n             )\n             grouped_inputs, grouped_outputs = grouped\n \n-            # Unwrap all the per device values returned from `call_for_each_replica`.\n-            # Unwrapping per device values gives you a list of values that can be\n-            # used to construct a new train function that is composed of\n-            # inputs/outputs on all the devices over which the model is distributed.\n+            # Unwrap all the per device values returned from\n+            # `call_for_each_replica`.  Unwrapping per device values gives you a\n+            # list of values that can be used to construct a new train function\n+            # that is composed of inputs/outputs on all the devices over which\n+            # the model is distributed.\n             (all_inputs, all_outputs, _, _) = unwrap_values(\n                 strategy,\n                 grouped_inputs,\n@@ -1091,8 +1104,8 @@ def _make_eager_execution_function(model, mode):\n                 with_loss_tensor=(mode != ModeKeys.PREDICT),\n             )\n \n-        # Finally, a joint Keras function is created; this one will be created in\n-        # a separate FuncGraph.\n+        # Finally, a joint Keras function is created; this one will be created\n+        # in a separate FuncGraph.\n         return backend.function(\n             all_inputs,\n             all_outputs,\n@@ -1123,7 +1136,8 @@ def _copy_weights_to_original_model(model, mode):\n \n \n def _per_replica_aggregate_batch(strategy, batch_outs, model, mode):\n-    \"\"\"Aggregates the per-replica batch-level outputs from a distributed step.\"\"\"\n+    \"\"\"Aggregates the per-replica batch-level outputs from a distributed\n+    step.\"\"\"\n     if strategy is not None and mode == ModeKeys.PREDICT:\n         total_batch_outs = []\n         for i in range(len(model.outputs)):\n@@ -1238,8 +1252,8 @@ def _update_sample_weight_modes(model, mode, sample_weights):\n             distributed_models = flatten_per_replica_values(\n                 model._distribution_strategy, distributed_model\n             )\n-            # sample_weights is a tuple of 1 list where the number of elements in the\n-            # list is equal to the number of replicas in sync.\n+            # sample_weights is a tuple of 1 list where the number of elements\n+            # in the list is equal to the number of replicas in sync.\n             sample_weights = sample_weights[0]\n             if sample_weights and None not in sample_weights:\n                 for m, sw in zip(distributed_models, sample_weights):\n\n@@ -340,12 +340,12 @@ def compare_results(\n         # We relax the tolerance a lot in the partial last batch case as\n         #   1. the examples in uneven batches may have different weights when\n         #      applying the gradients in the distributed case.\n-        #   2. TF Keras and TF Keras DS have different ways to handle the case when\n-        #      training with epochs > 1 with numpy inputs. In TF Keras, every epoch\n-        #      may have a partial batch. While in TF Keras DS, as we convert\n-        #      numpy inputs into dataset, it will do a repeat() first and calculate\n-        #      steps_per_epoch, so it will at most have one partial batch. This\n-        #      makes the 1-CPU result even different.\n+        #   2. TF Keras and TF Keras DS have different ways to handle the case\n+        #      when training with epochs > 1 with numpy inputs. In TF Keras,\n+        #      every epoch may have a partial batch. While in TF Keras DS, as we\n+        #      convert numpy inputs into dataset, it will do a repeat() first\n+        #      and calculate steps_per_epoch, so it will at most have one\n+        #      partial batch. This makes the 1-CPU result even different.\n         default_tolerance = 1e-3\n         relaxed_tolerance = 1e-3\n     else:\n@@ -458,11 +458,13 @@ class TestDistributionStrategyCorrectnessBase(\n     def get_input_for_correctness_test(self, **kwargs):\n         \"\"\"Generates inputs that are dictionaries.\n \n-        We only provide a default implementation of this method here. If you need\n-        more customized way of providing input to your model, overwrite this method.\n+        We only provide a default implementation of this method here. If you\n+        need more customized way of providing input to your model, overwrite\n+        this method.\n \n         Args:\n-          **kwargs: key word arguments about how to create the input dictionaries\n+          **kwargs: key word arguments about how to create the input\n+            dictionaries\n \n         Returns:\n           Three dictionaries representing the input for fit(), evaluate() and\n@@ -558,8 +560,8 @@ class TestDistributionStrategyCorrectnessBase(\n             )\n \n             # First, special case, for multi-replica distributed training, batch\n-            # norm is not aggregated globally. So it is expected to have different\n-            # weights.\n+            # norm is not aggregated globally. So it is expected to have\n+            # different weights.\n             if (\n                 self.with_batch_norm == \"regular\"\n                 and distribution.num_replicas_in_sync > 1\n@@ -584,11 +586,13 @@ class TestDistributionStrategyCorrectnessBase(\n     def get_input_for_dynamic_lr_test(self, **kwargs):\n         \"\"\"Generates inputs that are dictionaries.\n \n-        We only provide a default implementation of this method here. If you need\n-        more customized way of providing input to your model, overwrite this method.\n+        We only provide a default implementation of this method here. If you\n+        need more customized way of providing input to your model, overwrite\n+        this method.\n \n         Args:\n-          **kwargs: key word arguments about how to create the input dictionaries\n+          **kwargs: key word arguments about how to create the input\n+            dictionaries\n \n         Returns:\n           Three dictionaries representing the input for fit(), evaluate() and\n@@ -614,9 +618,9 @@ class TestDistributionStrategyCorrectnessBase(\n                 )\n                 and distribution.extended.steps_per_run > 1\n             ):\n-                # For TPUStrategy with steps_per_run > 1, the callback is not invoked\n-                # every step. So, to compare the CPU/TPU, we let the CPU to behave the\n-                # same as TPU.\n+                # For TPUStrategy with steps_per_run > 1, the callback is not\n+                # invoked every step. So, to compare the CPU/TPU, we let the CPU\n+                # to behave the same as TPU.\n                 update_freq = distribution.extended.steps_per_run\n \n             training_epochs = 2\n\n@@ -312,8 +312,8 @@ class TestDistributionStrategyDnnCorrectnessWithSubclassedModel(\n         ):\n             with self.assertRaisesRegex(\n                 ValueError,\n-                \"Expected `model` argument to be a functional `Model` instance, \"\n-                \"but got a subclassed model instead.\",\n+                \"Expected `model` argument to be a functional `Model` \"\n+                \"instance, but got a subclassed model instead.\",\n             ):\n                 self.run_correctness_test(\n                     distribution, use_numpy, use_validation_data\n@@ -340,8 +340,8 @@ class TestDistributionStrategyDnnCorrectnessWithSubclassedModel(\n         elif backend.is_tpu_strategy(distribution):\n             with self.assertRaisesRegex(\n                 ValueError,\n-                \"Expected `model` argument to be a functional `Model` instance, \"\n-                \"but got a subclassed model instead.\",\n+                \"Expected `model` argument to be a functional `Model` \"\n+                \"instance, but got a subclassed model instead.\",\n             ):\n                 self.run_dynamic_lr_test(distribution)\n         else:\n\n@@ -122,8 +122,8 @@ class DistributionStrategySiameseEmbeddingModelCorrectnessTest(\n             if initial_weights:\n                 model.set_weights(initial_weights)\n \n-            # TODO(b/130808953): Switch back to the V1 optimizer after global_step\n-            # is made mirrored.\n+            # TODO(b/130808953): Switch back to the V1 optimizer after\n+            # global_step is made mirrored.\n             model.compile(\n                 optimizer=gradient_descent_keras.SGD(learning_rate=0.1),\n                 loss=\"mse\",\n\n@@ -177,8 +177,8 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n             def __init__(self):\n                 super().__init__(name=\"metric_layer\")\n                 self.sum = metrics.Sum(name=\"sum\")\n-                # Using aggregation for jit_compile results in failure. Thus only set\n-                # aggregation for PS Strategy for multi-gpu tests.\n+                # Using aggregation for jit_compile results in failure. Thus\n+                # only set aggregation for PS Strategy for multi-gpu tests.\n                 if isinstance(\n                     distribution,\n                     tf.distribute.experimental.ParameterServerStrategy,\n\n@@ -74,10 +74,12 @@ class MirroredStrategyOptimizerV2Test(tf.test.TestCase, parameterized.TestCase):\n \n             # first step.\n             train_fn()\n-            # var(1) = var(0) - lr * m(1) * sqrt(1 - beta2) / sqrt(v(1)) / (1 - beta1)\n+            # var(1) = var(0) - lr * m(1) * sqrt(1 - beta2) / sqrt(v(1)) / (1 -\n+            # beta1)\n             #        = 2.0 - 0.01 * 1.2 * sqrt(0.8) / sqrt(1.8) / 0.8\n             self.assertAllClose(1.99, self.evaluate(all_vars[0]))\n-            # m(1) = beta1 * m(0) + (1-beta1) * grad = 0.2 * 0 + 0.8 * (1 + 2) / 2\n+            # m(1) = beta1 * m(0) + (1-beta1) * grad = 0.2 * 0 + 0.8 * (1 + 2) /\n+            # 2\n             self.assertAllClose(1.2, self.evaluate(all_vars[1]))\n             # v(1) = beta2 * v(0) + (1-beta2) * grad^2 = 0.2 * 0 + 0.8 * 2.25\n             self.assertAllClose(1.8, self.evaluate(all_vars[2]))\n\n@@ -93,8 +93,8 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n             distribution, tf.distribute.experimental.ParameterServerStrategy\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy requires dataset creator to be used in \"\n-                \"model.fit.\"\n+                \"Parameter Server strategy requires dataset creator to be used \"\n+                \"in model.fit.\"\n             )\n         if (\n             not tf.__internal__.tf2.enabled()\n@@ -104,8 +104,8 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n             )\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy with dataset creator needs to be run when \"\n-                \"eager execution is enabled.\"\n+                \"Parameter Server strategy with dataset creator needs to be \"\n+                \"run when eager execution is enabled.\"\n             )\n         with distribution.scope():\n             model = linear.LinearModel()\n@@ -130,8 +130,8 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n             distribution, tf.distribute.experimental.ParameterServerStrategy\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy requires dataset creator to be used in \"\n-                \"model.fit.\"\n+                \"Parameter Server strategy requires dataset creator to be used \"\n+                \"in model.fit.\"\n             )\n         if (\n             not tf.__internal__.tf2.enabled()\n@@ -141,8 +141,8 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n             )\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy with dataset creator needs to be run when \"\n-                \"eager execution is enabled.\"\n+                \"Parameter Server strategy with dataset creator needs to be \"\n+                \"run when eager execution is enabled.\"\n             )\n         with distribution.scope():\n             linear_model = linear.LinearModel(units=1)\n\n@@ -12,7 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for tf.keras models with callbacks, checkpointing with dist strategy.\"\"\"\n+\"\"\"Tests for tf.keras models with callbacks, checkpointing with dist\n+strategy.\"\"\"\n \n import collections\n import tempfile\n@@ -107,8 +108,9 @@ class TestDistributionStrategyWithCallbacks(\n             )\n             and not tf.executing_eagerly()\n         ):\n-            # TPU Strategy can have multi step training, from extended.steps_per_run\n-            # if steps_per_run = 1, then num_batch_call_per_epoch = steps_per_epoch\n+            # TPU Strategy can have multi step training, from\n+            # extended.steps_per_run if steps_per_run = 1, then\n+            # num_batch_call_per_epoch = steps_per_epoch\n             steps_per_run = distribution.extended.steps_per_run\n             num_batch_call_per_epoch = steps_per_epoch // steps_per_run\n             if steps_per_epoch % steps_per_run:\n@@ -215,9 +217,9 @@ class TestDistributionStrategyErrorCases(\n \n             x = distribution.run(run)\n \n-            # Removed device and input tensor shape details from the error message\n-            # since the order of the device and the corresponding input tensor shape\n-            # is not deterministic over different runs.\n+            # Removed device and input tensor shape details from the error\n+            # message since the order of the device and the corresponding input\n+            # tensor shape is not deterministic over different runs.\n             with self.assertRaisesRegex(\n                 ValueError,\n                 \"Input tensor shapes do not match for \"\n@@ -252,9 +254,9 @@ class TestDistributionStrategyErrorCases(\n \n             x = distribution.run(run)\n \n-            # Removed device and input tensor dtype details from the error message\n-            # since the order of the device and the corresponding input tensor dtype\n-            # is not deterministic over different runs.\n+            # Removed device and input tensor dtype details from the error\n+            # message since the order of the device and the corresponding input\n+            # tensor dtype is not deterministic over different runs.\n             with self.assertRaisesRegex(\n                 ValueError,\n                 \"Input tensor dtypes do not match for \"\n@@ -306,8 +308,8 @@ class TestDistributionStrategyErrorCases(\n                     sample_weight=sample_weight,\n                 )\n \n-            # Test with not specifying the `steps` argument for dataset with infinite\n-            # cardinality.\n+            # Test with not specifying the `steps` argument for dataset with\n+            # infinite cardinality.\n             dataset = dataset.repeat()\n             with self.assertRaises(ValueError):\n                 model.fit(dataset, epochs=1, verbose=0)\n\n@@ -221,8 +221,8 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n \n         def appending_creator(next_creator, **kwargs):\n             v = next_creator(**kwargs)\n-            # Skip the StateVar created in the tf.random.Generator, which is used by\n-            # keras initializers.\n+            # Skip the StateVar created in the tf.random.Generator, which is\n+            # used by keras initializers.\n             if \"StateVar\" in v.name:\n                 return v\n             created_variables.append(v.name)\n@@ -355,18 +355,18 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n             expected_moving_means = [0.0] * 8\n \n             def averaged_batch_mean(i):\n-                # Each batch has shape [16, 8] where the ith element in jth list is\n-                # (8 * j + i + replica_id * 100). So the batch mean in each replica is\n-                # (60 + i + replica_id * 100). So here comes its batch mean over all\n-                # replicas:\n+                # Each batch has shape [16, 8] where the ith element in jth list\n+                # is (8 * j + i + replica_id * 100). So the batch mean in each\n+                # replica is (60 + i + replica_id * 100). So here comes its\n+                # batch mean over all replicas:\n                 return 60.0 + i + (num_replicas - 1.0) / 2.0 * 100.0\n \n             for _ in range(10):\n                 run_step()\n                 moving_means = self.evaluate(batchnorm.moving_mean)\n \n-                # We make sure that the moving_mean is updated as if the sample mean is\n-                # calculated over all replicas.\n+                # We make sure that the moving_mean is updated as if the sample\n+                # mean is calculated over all replicas.\n                 for i, expected_moving_mean in enumerate(expected_moving_means):\n                     expected_moving_means[i] -= (\n                         expected_moving_mean - averaged_batch_mean(i)\n@@ -507,12 +507,12 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n             #   predict = [4, 14]\n             #   predict - y = [-2, -7]\n             #   dloss/dw = 2 <[2, 7], [-2, -7]> = - 2(4 + 49) = -106\n-            # So unreplicated the update to w with lr=0.001 is -0.2 * -106 = 0.106\n-            # with sum loss reduction, or 0.053 with mean.\n+            # So unreplicated the update to w with lr=0.001 is -0.2 * -106 =\n+            # 0.106 with sum loss reduction, or 0.053 with mean.\n             if loss_reduction == tf.compat.v1.losses.Reduction.SUM:\n-                # Note that the \"distribution.num_replicas_in_sync\" factor will go away\n-                # once we split the input across replicas, instead of pulling a complete\n-                # batch of input per replica.\n+                # Note that the \"distribution.num_replicas_in_sync\" factor will\n+                # go away once we split the input across replicas, instead of\n+                # pulling a complete batch of input per replica.\n                 self.assertNear(\n                     weight,\n                     2 + 0.106 * distribution.num_replicas_in_sync,\n@@ -540,8 +540,9 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n \n             def dataset_fn():\n                 dataset = tf.data.Dataset.from_tensors([[1.0]]).repeat()\n-                # TODO(priyag): batch with drop_remainder=True causes shapes to be\n-                # fully defined for TPU. Remove this when XLA supports dynamic shapes.\n+                # TODO(priyag): batch with drop_remainder=True causes shapes to\n+                # be fully defined for TPU. Remove this when XLA supports\n+                # dynamic shapes.\n                 return dataset.batch(batch_size=1, drop_remainder=True)\n \n             optimizer = optimizer_fn()\n@@ -592,10 +593,11 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n                 initial_loss = lambda: tf.constant(1e7)\n                 # Initial values corresponding to reduced losses are just single\n                 # tensors. But for non reduced losses, we need to have initial\n-                # values that are of the same structure as non reduced losses. In\n-                # MirroredStrategy, this will be a list of losses, in TPUStrategy\n-                # it will be single tensor. Using `call_for_each_replica` followed\n-                # by `experimental_local_results` gives us the desired initial\n+                # values that are of the same structure as non reduced losses.\n+                # In MirroredStrategy, this will be a list of losses, in\n+                # TPUStrategy it will be single tensor. Using\n+                # `call_for_each_replica` followed by\n+                # `experimental_local_results` gives us the desired initial\n                 # value structure.\n                 not_reduced = distribution.experimental_local_results(\n                     distribution.extended.call_for_each_replica(initial_loss)\n\n@@ -93,10 +93,10 @@ class MirroredVariableCreationTest(tf.test.TestCase):\n             layer1(features)\n             layer2 = core.Dense(1)\n             layer2(features)\n-            # We rely on names and orders to make sure replica references the same\n-            # MirroredVariable. Uniquifying names may involve global states,\n-            # merge_call switches threads so we need to test things work after\n-            # merge_call.\n+            # We rely on names and orders to make sure replica references the\n+            # same MirroredVariable. Uniquifying names may involve global\n+            # states, merge_call switches threads so we need to test things work\n+            # after merge_call.\n             tf.distribute.get_replica_context().merge_call(lambda _: _)\n             layer3 = core.Dense(1)\n             layer3(features)\n\n@@ -110,10 +110,11 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n             num_epoch = 2\n             extension = os.path.splitext(saving_filepath)[1]\n \n-            # Incorporate type/index information and thread id in saving_filepath to\n-            # ensure every worker has a unique path. Note that in normal use case the\n-            # saving_filepath will be the same for all workers, but we use different\n-            # ones here just to test out chief saves checkpoint but non-chief doesn't.\n+            # Incorporate type/index information and thread id in\n+            # saving_filepath to ensure every worker has a unique path. Note\n+            # that in normal use case the saving_filepath will be the same for\n+            # all workers, but we use different ones here just to test out chief\n+            # saves checkpoint but non-chief doesn't.\n             task_config = get_tf_config_task()\n             saving_filepath = os.path.join(\n                 test_obj.get_temp_dir(),\n@@ -121,7 +122,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n                 % (task_config[\"type\"], task_config[\"index\"], extension),\n             )\n \n-            # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+            # The saving_filepath shouldn't exist at the beginning (as it's\n+            # unique).\n             test_obj.assertFalse(checkpoint_exists(saving_filepath))\n \n             model.fit(\n@@ -138,13 +140,14 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n                 ],\n             )\n \n-            # If it's chief, the model should be saved; if not, the model shouldn't.\n+            # If it's chief, the model should be saved; if not, the model\n+            # shouldn't.\n             test_obj.assertEqual(checkpoint_exists(saving_filepath), is_chief())\n \n             # If it's chief, the model should be saved (`write_filepath` should\n-            # simply return `saving_filepath`); if not, i.e. for non-chief workers,\n-            # the temporary path generated by `write_filepath` should no longer\n-            # contain the checkpoint that has been deleted.\n+            # simply return `saving_filepath`); if not, i.e. for non-chief\n+            # workers, the temporary path generated by `write_filepath` should\n+            # no longer contain the checkpoint that has been deleted.\n             test_obj.assertEqual(\n                 checkpoint_exists(\n                     distributed_file_utils.write_filepath(\n@@ -172,7 +175,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n             model, _, train_ds, steps = _model_setup(test_obj, file_format=\"\")\n             num_epoch = 2\n \n-            # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+            # The saving_filepath shouldn't exist at the beginning (as it's\n+            # unique).\n             test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))\n \n             model.fit(\n@@ -206,8 +210,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n         class AssertCallback(callbacks.Callback):\n             def on_epoch_begin(self, epoch, logs=None):\n                 # the interruption happened on epoch 2 as specified in\n-                # InterruptingCallback, so the initial epoch after restart will begin\n-                # at 2.\n+                # InterruptingCallback, so the initial epoch after restart will\n+                # begin at 2.\n                 assert epoch > 1\n \n         def proc_model_checkpoint_works_with_same_file_path(\n@@ -216,7 +220,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n             model, _, train_ds, steps = _model_setup(test_obj, file_format=\"\")\n             num_epoch = 4\n \n-            # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+            # The saving_filepath shouldn't exist at the beginning (as it's\n+            # unique).\n             test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))\n             bar_dir = os.path.join(os.path.dirname(saving_filepath), \"backup\")\n \n@@ -278,7 +283,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n                 \"logfile_%s_%d\" % (task_config[\"type\"], task_config[\"index\"]),\n             )\n \n-            # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+            # The saving_filepath shouldn't exist at the beginning (as it's\n+            # unique).\n             test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))\n \n             model.fit(\n@@ -314,17 +320,19 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n             model, _, train_ds, steps = _model_setup(test_obj, file_format=\"\")\n             num_epoch = 2\n \n-            # Incorporate type/index information and thread id in saving_filepath to\n-            # ensure every worker has a unique path. Note that in normal use case the\n-            # saving_filepath will be the same for all workers, but we use different\n-            # ones here just to test out chief saves summaries but non-chief doesn't.\n+            # Incorporate type/index information and thread id in\n+            # saving_filepath to ensure every worker has a unique path. Note\n+            # that in normal use case the saving_filepath will be the same for\n+            # all workers, but we use different ones here just to test out chief\n+            # saves summaries but non-chief doesn't.\n             task_config = get_tf_config_task()\n             saving_filepath = os.path.join(\n                 test_obj.get_temp_dir(),\n                 \"logfile_%s_%d\" % (task_config[\"type\"], task_config[\"index\"]),\n             )\n \n-            # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+            # The saving_filepath shouldn't exist at the beginning (as it's\n+            # unique).\n             test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))\n \n             model.fit(\n@@ -339,10 +347,10 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n                 ],\n             )\n \n-            # If it's chief, the summaries should be saved in the filepath; if not,\n-            # the directory should be empty (although created). Using\n-            # `file_io.list_directory()` since the directory may be created at this\n-            # point.\n+            # If it's chief, the summaries should be saved in the filepath; if\n+            # not, the directory should be empty (although created). Using\n+            # `file_io.list_directory()` since the directory may be created at\n+            # this point.\n             test_obj.assertEqual(\n                 bool(tf.io.gfile.listdir(saving_filepath)), is_chief()\n             )\n@@ -374,8 +382,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n             os.mkdir(saving_filepath)\n             os.mkdir(saving_filepath_for_temp)\n \n-            # Verifies that even if `saving_filepath_for_temp` exists, tensorboard\n-            # can still save to temporary directory.\n+            # Verifies that even if `saving_filepath_for_temp` exists,\n+            # tensorboard can still save to temporary directory.\n             test_obj.assertTrue(tf.io.gfile.exists(saving_filepath_for_temp))\n \n             model.fit(\n@@ -403,7 +411,8 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n             model, _, train_ds, steps = _model_setup(test_obj, file_format=\"\")\n             num_epoch = 2\n \n-            # The saving_filepath shouldn't exist at the beginning (as it's unique).\n+            # The saving_filepath shouldn't exist at the beginning (as it's\n+            # unique).\n             test_obj.assertFalse(tf.io.gfile.exists(saving_filepath))\n \n             tf.__internal__.distribute.multi_process_runner.get_barrier().wait()\n@@ -447,9 +456,9 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n                 epoch_counter_cbk,\n             ]\n \n-            # Empirically, it is expected that `model.fit()` terminates around the\n-            # 22th epoch. Asserting that it should have been stopped before the 50th\n-            # epoch to avoid flakiness and be more predictable.\n+            # Empirically, it is expected that `model.fit()` terminates around\n+            # the 22th epoch. Asserting that it should have been stopped before\n+            # the 50th epoch to avoid flakiness and be more predictable.\n             model.fit(\n                 x=train_ds, epochs=100, steps_per_epoch=steps, callbacks=cbks\n             )\n\n@@ -68,15 +68,16 @@ def _clone_and_build_model(model, strategy):\n \n # TODO(b/123918215): Possibly merge this Callback with keras_test.Counter.\n class MultiWorkerVerificationCallback(callbacks.Callback):\n-    \"\"\"MultiWorkerVerificationCallback verifies the callbacks in multi-worker scheme.\n+    \"\"\"MultiWorkerVerificationCallback verifies the callbacks in multi-worker\n+    scheme.\n \n     This Callback is intended to be used for verifying the callback is indeed\n     called the correct number of times in various task types.\n \n     Attributes:\n       _task_dict: A nested dictionary storing the number of times a callback has\n-                  been called in specific task type, task index, and method name.\n-                  Look up structure is\n+                  been called in specific task type, task index, and method\n+                  name.  Look up structure is\n                   task_name -> task_id -> tracking_method_name -> invoke_count\n                   For example, a _task_dict of\n                   {\n@@ -97,8 +98,8 @@ class MultiWorkerVerificationCallback(callbacks.Callback):\n                            }\n                       }\n                   }\n-                  indicates the ps task has 'on_epoch_begin' called twice on each\n-                  of the two indices, and likewise for worker task.\n+                  indicates the ps task has 'on_epoch_begin' called twice on\n+                  each of the two indices, and likewise for worker task.\n     \"\"\"\n \n     # TODO(rchao): Add other method calls to verify.\n@@ -108,8 +109,10 @@ class MultiWorkerVerificationCallback(callbacks.Callback):\n         \"\"\"Initialize a MultiWorkerVerificationCallback.\n \n         Args:\n-          num_epoch: Number of epochs this Callback is expected to be called for.\n-          num_worker: Number of workers this Callback is expected to be called from.\n+          num_epoch: Number of epochs this Callback is expected to be called\n+            for.\n+          num_worker: Number of workers this Callback is expected to be called\n+            from.\n         \"\"\"\n         super().__init__()\n         self._num_epoch = num_epoch\n@@ -161,9 +164,9 @@ class MultiWorkerVerificationCallback(callbacks.Callback):\n         }\n         assert self._is_between_graph is not None\n         if self._is_between_graph:\n-            # TODO(b/124171024): In between-graph replication, by default only the\n-            # chief calls callback. Fix this test to cover that, as well as the rare\n-            # cases where all workers call.\n+            # TODO(b/124171024): In between-graph replication, by default only\n+            # the chief calls callback. Fix this test to cover that, as well as\n+            # the rare cases where all workers call.\n             worker_call_count = {\n                 i: method_count_dict for i in range(0, self._num_worker)\n             }\n@@ -297,8 +300,8 @@ class KPLMultiWorkerTest(tf.test.TestCase, parameterized.TestCase):\n \n \n if __name__ == \"__main__\":\n-    # Enable manual variable initialization to make sure variables are initialized\n-    # by `init_restore_or_wait_for_variables`.\n+    # Enable manual variable initialization to make sure variables are\n+    # initialized by `init_restore_or_wait_for_variables`.\n     backend.manual_variable_initialization(True)\n     with tf.compat.v1.test.mock.patch.object(sys, \"exit\", os._exit):\n         tf.__internal__.distribute.multi_process_runner.test_main()\n\n@@ -227,15 +227,15 @@ def create_in_process_cluster(\n     eval_config = tf.compat.v1.ConfigProto()\n     eval_config.experimental.collective_group_leader = \"\"\n \n-    # Create in-process servers. Once an in-process tensorflow server is created,\n-    # there is no way to terminate it. So we create one cluster per test process.\n-    # We could've started the server in another process, we could then kill that\n-    # process to terminate the server. The reasons why we don\"t want multiple\n-    # processes are\n+    # Create in-process servers. Once an in-process tensorflow server is\n+    # created, there is no way to terminate it. So we create one cluster per\n+    # test process.  We could've started the server in another process, we could\n+    # then kill that process to terminate the server. The reasons why we don\"t\n+    # want multiple processes are\n     # 1) it is more difficult to manage these processes;\n-    # 2) there is something global in CUDA such that if we initialize CUDA in the\n-    # parent process, the child process cannot initialize it again and thus cannot\n-    # use GPUs (https://stackoverflow.com/questions/22950047).\n+    # 2) there is something global in CUDA such that if we initialize CUDA in\n+    # the parent process, the child process cannot initialize it again and thus\n+    # cannot use GPUs (https://stackoverflow.com/questions/22950047).\n     cluster = None\n     try:\n         cluster = _create_cluster(\n\n@@ -95,7 +95,8 @@ optimizers_v1_and_v2 = optimizers_v1 + optimizers_v2\n \n \n def distributions_and_v1_optimizers():\n-    \"\"\"A common set of combination with DistributionStrategies and Optimizers.\"\"\"\n+    \"\"\"A common set of combination with DistributionStrategies and\n+    Optimizers.\"\"\"\n     return tf.__internal__.test.combinations.combine(\n         distribution=[\n             tf.__internal__.distribute.combinations.one_device_strategy,\n@@ -108,7 +109,8 @@ def distributions_and_v1_optimizers():\n \n \n def distributions_and_v2_optimizers():\n-    \"\"\"A common set of combination with DistributionStrategies and Optimizers.\"\"\"\n+    \"\"\"A common set of combination with DistributionStrategies and\n+    Optimizers.\"\"\"\n     return tf.__internal__.test.combinations.combine(\n         distribution=[\n             tf.__internal__.distribute.combinations.one_device_strategy,\n@@ -121,7 +123,8 @@ def distributions_and_v2_optimizers():\n \n \n def distributions_and_v1_and_v2_optimizers():\n-    \"\"\"A common set of combination with DistributionStrategies and Optimizers.\"\"\"\n+    \"\"\"A common set of combination with DistributionStrategies and\n+    Optimizers.\"\"\"\n     return tf.__internal__.test.combinations.combine(\n         distribution=[\n             tf.__internal__.distribute.combinations.one_device_strategy,\n\n@@ -142,8 +142,8 @@ class EvaluationTest(tf.test.TestCase):\n         def metric_fn():\n             return MeanMetricAsCompositeTensor()\n \n-        # TODO(yuefengz): make _create_per_worker_resources public and get rid of\n-        # the type_spec hack.\n+        # TODO(yuefengz): make _create_per_worker_resources public and get rid\n+        # of the type_spec hack.\n         per_worker_metric = self.cluster_coord._create_per_worker_resources(\n             metric_fn\n         )\n@@ -165,8 +165,8 @@ class EvaluationTest(tf.test.TestCase):\n             for i in dataset_shard:\n                 metric.update_state(i)\n \n-            # TODO(yuefengz): we should return the internal state of the metric and\n-            # then use the combiner API.\n+            # TODO(yuefengz): we should return the internal state of the metric\n+            # and then use the combiner API.\n             return metric.result()\n \n         total_shards = 128\n\n@@ -216,7 +216,8 @@ class SavedModelTFModuleTest(test_base.TestSavedModelBase):\n         load_options = tf.saved_model.LoadOptions(\n             experimental_io_device=\"/job:localhost\"\n         )\n-        # Check that the model can be loaded and training continued without error.\n+        # Check that the model can be loaded and training continued without\n+        # error.\n         with distribution.scope():\n             loaded_model = tf.saved_model.load(saved_dir, options=load_options)\n             self._train_model(loaded_model, x_train, y_train, batch_size)\n\n@@ -140,13 +140,13 @@ class TestSavedModelBase(tf.test.TestCase, parameterized.TestCase):\n         This method must be implemented by the subclasses.\n \n         Args:\n-          distribution: the distribution strategy used to load the model. None if no\n-            distribution strategy is used\n+          distribution: the distribution strategy used to load the model. None\n+            if no distribution strategy is used\n           saved_dir: the string representing the path where the model is saved.\n           predict_dataset: the data used to do the predict on the model for\n             cross_replica context.\n-          output_name: the string representing the name of the output layer of the\n-            model.\n+          output_name: the string representing the name of the output layer of\n+            the model.\n         \"\"\"\n \n         raise NotImplementedError(\"must be implemented in descendants\")\n@@ -237,7 +237,8 @@ class TestSavedModelBase(tf.test.TestCase, parameterized.TestCase):\n         distribution_for_restoring,\n         save_in_scope,\n     ):\n-        \"\"\"Save a model with DS, and restore it with potentially different DS.\"\"\"\n+        \"\"\"Save a model with DS, and restore it with potentially different\n+        DS.\"\"\"\n         saved_dir = os.path.join(self.get_temp_dir(), \"2\")\n \n         with distribution_for_saving.scope():\n\n@@ -158,10 +158,10 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n         \"\"\"Test saving and loading models with various fixed numbers of shards.\n \n         Args:\n-          shard_config: The number of shards to use per variable before and after\n-            loading. For example, [1, 3] means to create and save the model with 1\n-            shard (i.e., no variable partitioning), and load it into 3 shards per\n-            variable.\n+          shard_config: The number of shards to use per variable before and\n+            after loading. For example, [1, 3] means to create and save the\n+            model with 1 shard (i.e., no variable partitioning), and load it\n+            into 3 shards per variable.\n           model_type: Either 'dense' or 'embedding', which simple model to test.\n         \"\"\"\n \n@@ -203,7 +203,8 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n             )\n             expect = model(x)\n \n-        # Dense layers have two variables (kernel and bias), embedding layers have 1\n+        # Dense layers have two variables (kernel and bias), embedding layers\n+        # have 1\n         n_expected_variables = shard_config[0] * (\n             2 if model_type == \"dense\" else 1\n         )\n@@ -293,12 +294,14 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n     def test_slot_variable_checkpointing(self):\n \n         with self.strategy.scope():\n-            # Set a name so the ShardedVariable is well-named for slot var keying\n+            # Set a name so the ShardedVariable is well-named for slot var\n+            # keying\n             var = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"test\")\n \n         opt = keras.optimizers.optimizer_v2.adam.Adam()\n \n-        # Run once to trigger apply_gradients to populate optimizer slot variables.\n+        # Run once to trigger apply_gradients to populate optimizer slot\n+        # variables.\n         def train_step():\n             with tf.GradientTape() as tape:\n                 loss = sum(var)\n@@ -314,7 +317,8 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n \n         ckpt = tf.train.Checkpoint(var=var, opt=opt)\n \n-        # Assert that checkpoint has slots for each shard and the ShardedVariable\n+        # Assert that checkpoint has slots for each shard and the\n+        # ShardedVariable\n         self.assertLen(ckpt.opt._slots, 3)\n         for var_name in ckpt.opt._slots.keys():\n             self.assertLen(ckpt.opt._slots[var_name], 2)\n@@ -349,12 +353,14 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n     def test_slot_variable_checkpoint_load_with_diff_shards(self):\n \n         with self.strategy.scope():\n-            # Set a name so the ShardedVariable is well-named for slot var keying\n+            # Set a name so the ShardedVariable is well-named for slot var\n+            # keying\n             var = tf.Variable([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], name=\"test\")\n \n         opt = keras.optimizers.optimizer_v2.adam.Adam()\n \n-        # Run once to trigger apply_gradients to populate optimizer slot variables.\n+        # Run once to trigger apply_gradients to populate optimizer slot\n+        # variables.\n         def train_step():\n             with tf.GradientTape() as tape:\n                 loss = sum(var)\n@@ -388,7 +394,8 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n             var = tf.Variable([0.0, 1.0, 2.0, 3.0, 4.0, 5.0], name=\"test\")\n \n         opt = keras.optimizers.optimizer_v2.adam.Adam()\n-        # Run once to trigger apply_gradients to populate optimizer slot variables.\n+        # Run once to trigger apply_gradients to populate optimizer slot\n+        # variables.\n         strategy2.run(train_step)\n \n         new_ckpt = tf.train.Checkpoint(var=var, opt=opt)\n@@ -406,7 +413,8 @@ class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n class ShardedVariableMixedPartitioningTest(tf.test.TestCase):\n     def test_saved_model_min_size_partitioner(self):\n \n-        # set min_shard_bytes such that Dense kernel is split into 2 and bias into 1\n+        # set min_shard_bytes such that Dense kernel is split into 2 and bias\n+        # into 1\n         partitioner = (\n             tf.distribute.experimental.partitioners.MinSizePartitioner(\n                 min_shard_bytes=(6 * 6 * 4) // 2, max_shards=2\n@@ -438,7 +446,8 @@ class ShardedVariableMixedPartitioningTest(tf.test.TestCase):\n         saved_dir = self.get_temp_dir()\n         model.save(saved_dir)\n \n-        # set min_shard_bytes such that Dense kernel is split into 3 and bias into 1\n+        # set min_shard_bytes such that Dense kernel is split into 3 and bias\n+        # into 1\n         partitioner2 = (\n             tf.distribute.experimental.partitioners.MinSizePartitioner(\n                 min_shard_bytes=(6 * 6 * 4) // 3, max_shards=3\n\n@@ -53,23 +53,24 @@ class SidecarEvaluator:\n     evaluator, evaluating the metric results of a training cluster which has one\n     or more workers performing the training, and saving checkpoints.\n \n-    The `SidecarEvaluator` API is compatible with both Custom Training Loop (CTL),\n-    and Keras `Model.fit` to be used in the training cluster. Using the model\n-    (with compiled metrics) provided at `__init__`, `SidecarEvaluator` repeatedly\n-    performs evaluation \"epochs\" when it finds a checkpoint that has not yet been\n-    used. Depending on the `steps` argument, an eval epoch is evaluation over all\n-    eval data, or up to certain number of steps (batches). See examples below for\n-    how the training program should save the checkpoints in order to be recognized\n-    by `SidecarEvaluator`.\n+    The `SidecarEvaluator` API is compatible with both Custom Training Loop\n+    (CTL), and Keras `Model.fit` to be used in the training cluster. Using the\n+    model (with compiled metrics) provided at `__init__`, `SidecarEvaluator`\n+    repeatedly performs evaluation \"epochs\" when it finds a checkpoint that has\n+    not yet been used. Depending on the `steps` argument, an eval epoch is\n+    evaluation over all eval data, or up to certain number of steps (batches).\n+    See examples below for how the training program should save the checkpoints\n+    in order to be recognized by `SidecarEvaluator`.\n \n-    Since under the hood, `SidecarEvaluator` uses `model.evaluate` for evaluation,\n-    it also supports arbitrary Keras callbacks. That is, if one or more callbacks\n-    are provided, their `on_test_batch_begin` and `on_test_batch_end` methods are\n-    called at the start and end of a batch, and their `on_test_begin` and\n-    `on_test_end` are called at the start and end of an evaluation epoch. Note\n-    that `SidecarEvaluator` may skip some checkpoints because it always picks up\n-    the latest checkpoint available, and during an evaluation epoch, multiple\n-    checkpoints can be produced from the training side.\n+    Since under the hood, `SidecarEvaluator` uses `model.evaluate` for\n+    evaluation, it also supports arbitrary Keras callbacks. That is, if one or\n+    more callbacks are provided, their `on_test_batch_begin` and\n+    `on_test_batch_end` methods are called at the start and end of a batch, and\n+    their `on_test_begin` and `on_test_end` are called at the start and end of\n+    an evaluation epoch. Note that `SidecarEvaluator` may skip some checkpoints\n+    because it always picks up the latest checkpoint available, and during an\n+    evaluation epoch, multiple checkpoints can be produced from the training\n+    side.\n \n     Example:\n     ```python\n@@ -81,15 +82,16 @@ class SidecarEvaluator:\n     tf.keras.SidecarEvaluator(\n         model=model,\n         data=data,\n-        checkpoint_dir='/tmp/checkpoint_dir',  # dir for training-saved checkpoint\n+        # dir for training-saved checkpoint\n+        checkpoint_dir='/tmp/checkpoint_dir',\n         steps=None,  # Eval until dataset is exhausted\n         max_evaluations=None,  # The evaluation needs to be stopped manually\n         callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]\n     ).start()\n     ```\n \n-    `SidecarEvaluator.start` writes a series of summary\n-    files which can be visualized by tensorboard (which provides a webpage link):\n+    `SidecarEvaluator.start` writes a series of summary files which can be\n+    visualized by tensorboard (which provides a webpage link):\n \n     ```bash\n     $ tensorboard --logdir=/tmp/log_dir/validation\n@@ -103,7 +105,8 @@ class SidecarEvaluator:\n     `tf.train.Checkpoint` and a `tf.train.CheckpointManager`:\n \n     ```python\n-    checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.\n+    # Same `checkpoint_dir` supplied to `SidecarEvaluator`.\n+    checkpoint_dir = ...\n     checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n     checkpoint_manager = tf.train.CheckpointManager(\n         checkpoint, checkpoint_dir=..., max_to_keep=...)\n@@ -116,7 +119,8 @@ class SidecarEvaluator:\n     appended:\n \n     ```python\n-    checkpoint_dir = ...  # Same `checkpoint_dir` supplied to `SidecarEvaluator`.\n+    # Same `checkpoint_dir` supplied to `SidecarEvaluator`.\n+    checkpoint_dir = ...\n     model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n         filepath=os.path.join(checkpoint_dir, 'ckpt-{epoch}'),\n         save_weights_only=True)\n@@ -136,34 +140,37 @@ class SidecarEvaluator:\n         \"\"\"Initializes an `SidecarEvaluator` object.\n \n         Args:\n-          model: Model to use for evaluation. The model object used here should be a\n-            `tf.keras.Model`, and should be the same as the one that is used in\n-            training, where `tf.keras.Model`s are checkpointed. The model should\n-            have one or more metrics compiled before using `SidecarEvaluator`.\n-          data: The input data for evaluation. `SidecarEvaluator` supports all data\n-            types that Keras `model.evaluate` supports as the input data `x`, such\n-            as a `tf.data.Dataset`.\n+          model: Model to use for evaluation. The model object used here should\n+            be a `tf.keras.Model`, and should be the same as the one that is\n+            used in training, where `tf.keras.Model`s are checkpointed. The\n+            model should have one or more metrics compiled before using\n+            `SidecarEvaluator`.\n+          data: The input data for evaluation. `SidecarEvaluator` supports all\n+            data types that Keras `model.evaluate` supports as the input data\n+            `x`, such as a `tf.data.Dataset`.\n           checkpoint_dir: Directory where checkpoint files are saved.\n-          steps: Number of steps to perform evaluation for, when evaluating a single\n-            checkpoint file. If `None`, evaluation continues until the dataset is\n-            exhausted. For repeated evaluation dataset, user must specify `steps` to\n-            avoid infinite evaluation loop.\n-          max_evaluations: Maximum number of the checkpoint file to be evaluated,\n-            for `SidecarEvaluator` to know when to stop. The evaluator will stop\n-            after it evaluates a checkpoint filepath ending with\n-            '<ckpt_name>-<max_evaluations>'. If using\n-            `tf.train.CheckpointManager.save` for saving checkpoints, the kth saved\n-            checkpoint has the filepath suffix '<ckpt_name>-<k>' (k=1 for the first\n-            saved), and if checkpoints are saved every epoch after training, the\n-            filepath saved at the kth epoch would end with '<ckpt_name>-<k>. Thus,\n-            if training runs for n epochs, and the evaluator should end after the\n-            training finishes, use n for this parameter. Note that this is not\n-            necessarily equal to the number of total evaluations, since some\n-            checkpoints may be skipped if evaluation is slower than checkpoint\n-            creation. If `None`, `SidecarEvaluator` will evaluate indefinitely, and\n-            the user must terminate evaluator program themselves.\n-          callbacks: List of `keras.callbacks.Callback` instances to apply during\n-            evaluation. See [callbacks](/api_docs/python/tf/keras/callbacks).\n+          steps: Number of steps to perform evaluation for, when evaluating a\n+            single checkpoint file. If `None`, evaluation continues until the\n+            dataset is exhausted. For repeated evaluation dataset, user must\n+            specify `steps` to avoid infinite evaluation loop.\n+          max_evaluations: Maximum number of the checkpoint file to be\n+            evaluated, for `SidecarEvaluator` to know when to stop. The\n+            evaluator will stop after it evaluates a checkpoint filepath ending\n+            with '<ckpt_name>-<max_evaluations>'. If using\n+            `tf.train.CheckpointManager.save` for saving checkpoints, the kth\n+            saved checkpoint has the filepath suffix '<ckpt_name>-<k>' (k=1 for\n+            the first saved), and if checkpoints are saved every epoch after\n+            training, the filepath saved at the kth epoch would end with\n+            '<ckpt_name>-<k>. Thus, if training runs for n epochs, and the\n+            evaluator should end after the training finishes, use n for this\n+            parameter. Note that this is not necessarily equal to the number of\n+            total evaluations, since some checkpoints may be skipped if\n+            evaluation is slower than checkpoint creation. If `None`,\n+            `SidecarEvaluator` will evaluate indefinitely, and the user must\n+            terminate evaluator program themselves.\n+          callbacks: List of `keras.callbacks.Callback` instances to apply\n+            during evaluation. See\n+            [callbacks](/api_docs/python/tf/keras/callbacks).\n         \"\"\"\n         self.model = model\n         self.data = data\n@@ -179,11 +186,12 @@ class SidecarEvaluator:\n \n     def _timeout_fn(self):\n         logging.info(\n-            f\"No checkpoints appear to be found after {_CHECKPOINT_TIMEOUT_SEC} \"\n-            \"seconds. Please check if you are properly using a \"\n+            \"No checkpoints appear to be found after \"\n+            f\"{_CHECKPOINT_TIMEOUT_SEC} seconds. \"\n+            \"Please check if you are properly using a \"\n             \"`tf.train.Checkpoint/CheckpointManager` or \"\n-            \"`tf.keras.callbacks.ModelCheckpoint(save_weights_only=True)` to save \"\n-            \"checkpoints by the training. See \"\n+            \"`tf.keras.callbacks.ModelCheckpoint(save_weights_only=True)` to \"\n+            \"save checkpoints by the training. See \"\n             \"`tf.keras.SidecarEvaluator` doc for recommended flows \"\n             \"of saving checkpoints.\"\n         )\n@@ -202,34 +210,38 @@ class SidecarEvaluator:\n             timeout_fn=self._timeout_fn,\n         ):\n             try:\n-                # `expect_partial` because the checkpoint can have other `Trackable`s\n-                # such as `optimizer`.\n+                # `expect_partial` because the checkpoint can have other\n+                # `Trackable`s such as `optimizer`.\n                 checkpoint.restore(latest_checkpoint).expect_partial()\n                 checkpoint_attributes = list_checkpoint_attributes(\n                     latest_checkpoint\n                 )\n-                # The checkpoint should contain model and optimizer for SidecarEvaluator\n-                # to work. But the model weights saved by ModelCheckpoint callback does\n-                # not contain model as an attribute. To make SidecarEvaluator compatibly\n-                # work in this case, use model.load_weights to load the model's weights,\n-                # while self._iterations is still restored by checkpoint variable.\n+                # The checkpoint should contain model and optimizer for\n+                # SidecarEvaluator to work. But the model weights saved by\n+                # ModelCheckpoint callback does not contain model as an\n+                # attribute. To make SidecarEvaluator compatibly work in this\n+                # case, use model.load_weights to load the model's weights,\n+                # while self._iterations is still restored by checkpoint\n+                # variable.\n                 if \"model\" not in checkpoint_attributes:\n                     self.model.load_weights(latest_checkpoint)\n-                # The model checkpoint might not include optimizer in cases, e.g.\n-                # using a custom training loop. Directly assign the iterations\n-                # property to be used in callbacks.\n+                # The model checkpoint might not include optimizer in cases,\n+                # e.g.  using a custom training loop. Directly assign the\n+                # iterations property to be used in callbacks.\n                 if self.model.optimizer:\n                     self.model.optimizer.iterations.assign(self._iterations)\n             except (tf.errors.OpError,) as e:\n-                # A couple errors can happen here with the coordinator racing to write\n-                # checkpoint:\n-                # 1) OpError: open failed for <file path>: No such file or directory\n+                # A couple errors can happen here with the coordinator racing to\n+                # write checkpoint:\n+                # 1) OpError: open failed for <file path>: No such file or\n+                # directory\n                 # 2) NotFoundError (subclass of OpError): Unsuccessful\n                 # TensorSliceReader constructor.\n-                # TODO(rchao): Remove this except block once b/150954027 is resolved.\n+                # TODO(rchao): Remove this except block once b/150954027 is\n+                # resolved.\n                 logging.info(\n-                    \"SidecarEvaluator encountered an error when loading the checkpoint \"\n-                    f\"at {latest_checkpoint}. Retrying. \"\n+                    \"SidecarEvaluator encountered an error when loading the \"\n+                    f\"checkpoint at {latest_checkpoint}. Retrying. \"\n                     f\"Error: {e.__class__.__name__}: {e}\"\n                 )\n                 continue\n@@ -272,7 +284,8 @@ class SidecarEvaluator:\n             if self.max_evaluations and (\n                 self.max_evaluations <= int(latest_checkpoint.split(\"-\")[-1])\n             ):\n-                # Exit the loop because we have evaluated the final checkpoint file.\n+                # Exit the loop because we have evaluated the final checkpoint\n+                # file.\n                 logging.info(\n                     \"Last checkpoint evaluated. SidecarEvaluator stops.\"\n                 )\n\n@@ -214,8 +214,8 @@ class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n             callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir)],\n         )\n         sidecar_evaluator.start()\n-        # Eval model has been restored to the same state as the original model, so\n-        # their weights should match. If not, restoration of the model didn't\n+        # Eval model has been restored to the same state as the original model,\n+        # so their weights should match. If not, restoration of the model didn't\n         # work.\n         self.assertModelsSameVariables(model, eval_model)\n \n@@ -280,8 +280,8 @@ class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n         for metric_name in expected_logged_metrics:\n             self.assertRegex(metrics_logging[0], f\"{metric_name}=\")\n \n-        # Eval model has been restored to the same state as the original model, so\n-        # their weights should match. If not, restoration of the model didn't\n+        # Eval model has been restored to the same state as the original model,\n+        # so their weights should match. If not, restoration of the model didn't\n         # work.\n         self.assertModelsSameVariables(model, eval_model)\n \n\n@@ -58,10 +58,12 @@ def batchnorm_example(\n     renorm=False,\n     update_ops_in_replica_mode=False,\n ):\n-    \"\"\"Example of non-distribution-aware legacy code with batch normalization.\"\"\"\n+    \"\"\"Example of non-distribution-aware legacy code with batch\n+    normalization.\"\"\"\n \n     def dataset_fn():\n-        # input shape is [16, 8], input values are increasing in both dimensions.\n+        # input shape is [16, 8], input values are increasing in both\n+        # dimensions.\n         return tf.data.Dataset.from_tensor_slices(\n             [\n                 [\n@@ -91,7 +93,8 @@ def batchnorm_example(\n                 loss = tf.reduce_mean(\n                     tf.reduce_sum(layer(y)) - tf.constant(1.0)\n                 )\n-            # `x` and `y` will be fetched by the gradient computation, but not `loss`.\n+            # `x` and `y` will be fetched by the gradient computation, but not\n+            # `loss`.\n             return loss\n \n         if isinstance(optimizer, optimizer_v2.OptimizerV2):\n\n@@ -63,13 +63,13 @@ class WorkerTrainingState:\n         # If this is single-worker training, checkpoint_dir are the same for\n         # write_checkpoint_manager and read_checkpoint_manager.\n         #\n-        # If this is multi-worker training, and this worker should not\n-        # save checkpoint, we replace the write_checkpoint_manager's checkpoint_dir\n-        # with a temp filepath, so it writes to a file that will be removed at the\n-        # end of back_up() call. This is necessary because the SyncOnReadVariable\n-        # needs to be synced across all the workers in order to be read, and all\n-        # workers need to perform `save()`.\n-        # But all workers should restore from the same checkpoint_dir as passed in\n+        # If this is multi-worker training, and this worker should not save\n+        # checkpoint, we replace the write_checkpoint_manager's checkpoint_dir\n+        # with a temp filepath, so it writes to a file that will be removed at\n+        # the end of back_up() call. This is necessary because the\n+        # SyncOnReadVariable needs to be synced across all the workers in order\n+        # to be read, and all workers need to perform `save()`.  But all workers\n+        # should restore from the same checkpoint_dir as passed in\n         # read_checkpoint_manager.\n         self.read_checkpoint_manager = tf.train.CheckpointManager(\n             checkpoint,\n@@ -104,8 +104,9 @@ class WorkerTrainingState:\n         \"\"\"Restore the training state from the backed up checkpoint file.\n \n         Returns:\n-          True if the training state is successfully restored. False if the training\n-          state doesn't need to be restored, or error occurred so it can't.\n+          True if the training state is successfully restored. False if the\n+          training state doesn't need to be restored, or error occurred so it\n+          can't.\n         \"\"\"\n         self.read_checkpoint_manager.restore_or_initialize()\n \n@@ -125,10 +126,10 @@ class WorkerTrainingState:\n         \"\"\"Maybe load initial epoch from ckpt considering possible worker recovery.\n \n         When `_ckpt_saved_epoch` attribute exists and is not\n-        `CKPT_SAVED_EPOCH_UNUSED_VALUE`, this is under multi-worker training setting\n-        and indicates the worker is recovering from previous failure. In this case,\n-        infer `initial_epoch` from `self._ckpt_saved_epoch` to continue previous\n-        unfinished training from certain epoch.\n+        `CKPT_SAVED_EPOCH_UNUSED_VALUE`, this is under multi-worker training\n+        setting and indicates the worker is recovering from previous failure. In\n+        this case, infer `initial_epoch` from `self._ckpt_saved_epoch` to\n+        continue previous unfinished training from certain epoch.\n \n         Args:\n           initial_epoch: The original initial_epoch user passes in in `fit()`.\n@@ -136,13 +137,14 @@ class WorkerTrainingState:\n \n         Returns:\n           If the training is recovering from previous failure under multi-worker\n-          training setting, return the epoch the training is supposed to continue\n-          at. Otherwise, return the `initial_epoch` the user passes in.\n+          training setting, return the epoch the training is supposed to\n+          continue at. Otherwise, return the `initial_epoch` the user passes in.\n         \"\"\"\n \n         epoch = backend.eval(self._ckpt_saved_epoch)\n         if mode == mode_keys.ModeKeys.TRAIN and epoch >= 0:\n             # The most recently saved epoch is one epoch prior to the epoch it\n-            # failed at, so return the value of 'self._ckpt_saved_epoch' plus one.\n+            # failed at, so return the value of 'self._ckpt_saved_epoch' plus\n+            # one.\n             return epoch + 1\n         return initial_epoch\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
