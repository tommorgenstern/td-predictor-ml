{"custom_id": "keras#c5eb85b66bf44640114986c841b0839fdcc2dea3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 20 | Files Changed: 3 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 47 | Churn Cumulative: 455 | Contributors (this commit): 9 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,23 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Thin wrapper to call TensorFlow's API generation script.\n-\n-This file exists to provide a main function for the py_binary in the API\n-generation genrule. It just calls the main function for the actual API\n-generation script in TensorFlow.\n-\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import keras  # noqa: F401\n-\n-# isort: off\n-from tensorflow.python.tools.api.generator import (\n-    create_python_api,\n-)\n+\"\"\"Thin wrapper to call TensorFlow's API extractor script.\"\"\"\n+from absl import app\n+from tensorflow.python.tools.api.generator2.extractor import extractor\n \n if __name__ == \"__main__\":\n-    create_python_api.main()\n+    app.run(extractor.main)\n\n@@ -0,0 +1,20 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Thin wrapper to call TensorFlow's API generator script.\"\"\"\n+from absl import app\n+from tensorflow.python.tools.api.generator2.generator import generator\n+\n+if __name__ == \"__main__\":\n+    app.run(generator.main)\n\n@@ -24,7 +24,8 @@ import os\n \n PIP_EXCLUDED_FILES = frozenset(\n     [\n-        \"keras/api/create_python_api_wrapper.py\",\n+        \"keras/api/extractor_wrapper.py\",\n+        \"keras/api/generator_wrapper.py\",\n         \"keras/applications/efficientnet_weight_update_util.py\",\n         \"keras/distribute/tpu_strategy_test_utils.py\",\n         \"keras/saving/legacy/saved_model/create_test_saved_model.py\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0efae32da204dd407e1c583b647c61caf04711e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 19 | Complexity Δ (Sum/Max): 18/11 | Churn Δ: 135 | Churn Cumulative: 2069 | Contributors (this commit): 16 | Commits (past 90d): 33 | Contributors (cumulative): 22 | DMM Complexity: 0.99\n\nDIFF:\n@@ -74,6 +74,7 @@ class ReLU(ops.Operation):\n \n     @staticmethod\n     def static_call(x, negative_slope=0.0, max_value=None, threshold=0.0):\n+        x = backend.convert_to_tensor(x)\n         if negative_slope != 0.0:\n             if max_value is None and threshold == 0:\n                 return backend.nn.leaky_relu(x, negative_slope=negative_slope)\n\n@@ -333,6 +333,67 @@ class ActivationsTest(testing.TestCase):\n             result_positive_above_1, expected_positive_above_1, rtol=1e-05\n         )\n \n+    def test_relu_negative_slope(self):\n+        # Define the input tensor\n+        x = np.array([-10, -5, 0.0, 5, 10])\n+\n+        # Test with only negative_slope\n+        result_negative_slope = activations.relu(x, negative_slope=0.5)\n+        expected_negative_slope = np.array([-5.0, -2.5, 0.0, 5.0, 10.0])\n+        self.assertAllClose(\n+            result_negative_slope, expected_negative_slope, rtol=1e-05\n+        )\n+\n+    def test_relu_max_value(self):\n+        # Define the input tensor\n+        x = np.array([-10, -5, 0.0, 5, 10])\n+\n+        # Test with only max_value\n+        result_max_value = activations.relu(x, max_value=5.0)\n+        expected_max_value = np.array([0.0, 0.0, 0.0, 5.0, 5.0])\n+        self.assertAllClose(result_max_value, expected_max_value, rtol=1e-05)\n+\n+    def test_relu_threshold(self):\n+        # Define the input tensor\n+        x = np.array([-10, -5, 0.0, 5, 10])\n+\n+        # Test with only threshold\n+        result_threshold = activations.relu(x, threshold=5.0)\n+        expected_threshold = np.array([-0.0, -0.0, 0.0, 0.0, 10.0])\n+        self.assertAllClose(result_threshold, expected_threshold, rtol=1e-05)\n+\n+    def test_relu_combined_threshold_and_max_value(self):\n+        # Define the input tensor\n+        x = np.array([-10, -5, 0.0, 5, 10])\n+\n+        # Test with threshold and max_value\n+        result_combined = activations.relu(x, threshold=5.0, max_value=5.0)\n+        expected_combined = np.array([0.0, 0.0, 0.0, 0.0, 5.0])\n+        self.assertAllClose(result_combined, expected_combined, rtol=1e-05)\n+\n+    def test_relu_combined_all_parameters(self):\n+        # Define the input tensor\n+        x = np.array([-10, -5, 0.0, 5, 10])\n+\n+        # Test with negative_slope, max_value, and threshold\n+        result_combined = activations.relu(\n+            x, negative_slope=0.5, max_value=5.0, threshold=5.0\n+        )\n+        expected_combined = np.array([-7.5, -5.0, -2.5, 0.0, 5.0])\n+        self.assertAllClose(result_combined, expected_combined, rtol=1e-05)\n+\n+    def test_relu_to_trigger_relu6(self):\n+        x = np.array([-10, -5, 0.0, 5, 10, 12])\n+        result_relu6 = activations.relu(x, max_value=6.0)\n+        expected_relu6 = np.array([0.0, 0.0, 0.0, 5.0, 6.0, 6.0])\n+        self.assertAllClose(result_relu6, expected_relu6, rtol=1e-05)\n+\n+    def test_relu_to_trigger_leaky(self):\n+        x = np.array([-10, -5, 0.0, 5, 10])\n+        result_leaky = activations.relu(x, negative_slope=0.5)\n+        expected_leaky = np.array([-5.0, -2.5, 0.0, 5.0, 10.0])\n+        self.assertAllClose(result_leaky, expected_leaky, rtol=1e-05)\n+\n     def test_relu(self):\n         # Basic test for positive values\n         positive_values = np.random.uniform(0.1, 10, (2, 5))\n\n@@ -285,6 +285,79 @@ class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n         self.assertTrue(knp.shape_equal(x, y, axis=(1, 2)))\n         self.assertFalse(knp.shape_equal(x, y, axis=1))\n \n+    def test_shape_equal_only_none(self):\n+        x = KerasTensor([None, None]).shape\n+        y = KerasTensor([5, 6]).shape\n+        self.assertTrue(knp.shape_equal(x, y, allow_none=True))\n+\n+    def test_shape_equal_axis_as_list(self):\n+        x = KerasTensor([3, 4, 5]).shape\n+        y = KerasTensor([3, 6, 5]).shape\n+        self.assertTrue(knp.shape_equal(x, y, axis=[1]))\n+\n+    def test_shape_non_equal_with_negative_axis(self):\n+        x = KerasTensor([3, 4, 5]).shape\n+        y = KerasTensor([3, 4, 6]).shape\n+        self.assertFalse(knp.shape_equal(x, y, axis=-2))\n+\n+    def test_shape_equal_with_negative_axis(self):\n+        x = KerasTensor([3, 4, 5]).shape\n+        y = KerasTensor([3, 4, 5]).shape\n+        self.assertTrue(knp.shape_equal(x, y, axis=-1))\n+\n+    def test_shape_equal_zeros(self):\n+        x = KerasTensor([0, 4]).shape\n+        y = KerasTensor([0, 4]).shape\n+        self.assertTrue(knp.shape_equal(x, y))\n+        y = KerasTensor([0, 5]).shape\n+        self.assertFalse(knp.shape_equal(x, y))\n+\n+    def test_broadcast_shapes_conversion_to_list(self):\n+        shape1 = KerasTensor([1, 2]).shape\n+        shape2 = KerasTensor([3, 1]).shape\n+        expected_output = [3, 2]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n+    def test_broadcast_shapes_shape1_longer_than_shape2(self):\n+        shape1 = KerasTensor([5, 3, 2]).shape\n+        shape2 = KerasTensor([1, 3]).shape\n+        with self.assertRaisesRegex(ValueError, \"Cannot broadcast shape\"):\n+            knp.broadcast_shapes(shape1, shape2)\n+\n+    def test_broadcast_shapes_shape2_longer_than_shape1(self):\n+        shape1 = KerasTensor([5, 3]).shape\n+        shape2 = KerasTensor([2, 5, 3]).shape\n+        expected_output = [2, 5, 3]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n+    def test_broadcast_shapes_broadcasting_shape1_is_1(self):\n+        shape1 = KerasTensor([1, 3]).shape\n+        shape2 = KerasTensor([5, 1]).shape\n+        expected_output = [5, 3]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n+    def test_broadcast_shapes_broadcasting_shape1_is_none(self):\n+        shape1 = KerasTensor([None, 3]).shape\n+        shape2 = KerasTensor([5, 1]).shape\n+        expected_output = [5, 3]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n+        shape1 = KerasTensor([None, 3]).shape\n+        shape2 = KerasTensor([5, 3]).shape\n+        expected_output = [5, 3]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n+    def test_broadcast_shapes_broadcasting_shape2_conditions(self):\n+        shape1 = KerasTensor([5, 3, 2]).shape\n+        shape2 = KerasTensor([1, 3, 2]).shape\n+        expected_output = [5, 3, 2]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n+        shape1 = KerasTensor([5, 3, 2]).shape\n+        shape2 = KerasTensor([1, None, 2]).shape\n+        expected_output = [5, 3, 2]\n+        self.assertEqual(knp.broadcast_shapes(shape1, shape2), expected_output)\n+\n \n class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n     def test_add(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b0ecfc731ef8df23846297f92de893e24f97fda6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 501 | Lines Deleted: 24 | Files Changed: 16 | Hunks: 47 | Methods Changed: 57 | Complexity Δ (Sum/Max): 81/34 | Churn Δ: 525 | Churn Cumulative: 16015 | Contributors (this commit): 28 | Commits (past 90d): 189 | Contributors (cumulative): 111 | DMM Complexity: 0.8058823529411765\n\nDIFF:\n@@ -238,6 +238,8 @@ def empty(shape, dtype=\"float32\"):\n \n \n def equal(x1, x2):\n+    x1 = convert_to_tensor(x1)\n+    x2 = convert_to_tensor(x2)\n     return jnp.equal(x1, x2)\n \n \n\n@@ -29,6 +29,7 @@ class JAXTrainer(base_trainer.Trainer):\n         y,\n         sample_weight,\n         training=False,\n+        optimizer_variables=None,\n     ):\n         \"\"\"This method is stateless and is intended for use with jax.grad.\"\"\"\n         kwargs = {}\n@@ -44,7 +45,13 @@ class JAXTrainer(base_trainer.Trainer):\n         loss = self.compute_loss(x, y, y_pred, sample_weight, allow_empty=True)\n         if losses:\n             loss += ops.sum(losses)\n-        return loss, (y_pred, non_trainable_variables)\n+        unscaled_loss = loss\n+        if training and self.optimizer is not None:\n+            # Scale loss with a StatelessScope, to use an update scale variable.\n+            mapping = list(zip(self.optimizer.variables, optimizer_variables))\n+            with backend.StatelessScope(state_mapping=mapping):\n+                loss = self.optimizer.scale_loss(loss)\n+        return loss, (unscaled_loss, y_pred, non_trainable_variables)\n \n     def _eager_build(self, data_batch):\n         model_unbuilt = not all(layer.built for layer in self._flatten_layers())\n@@ -93,14 +100,16 @@ class JAXTrainer(base_trainer.Trainer):\n         grad_fn = jax.value_and_grad(\n             self.compute_loss_and_updates, has_aux=True\n         )\n-        (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n+        (loss, aux), grads = grad_fn(\n             trainable_variables,\n             non_trainable_variables,\n             x,\n             y,\n             sample_weight,\n             training=True,\n+            optimizer_variables=optimizer_variables,\n         )\n+        (unscaled_loss, y_pred, non_trainable_variables) = aux\n \n         (\n             trainable_variables,\n@@ -115,7 +124,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 for ref_v, v in zip(self.metrics_variables, metrics_variables)\n             ]\n         ) as scope:\n-            self._loss_tracker.update_state(loss)\n+            self._loss_tracker.update_state(unscaled_loss)\n             logs = self.compute_metrics(x, y, y_pred, sample_weight)\n \n         new_metrics_variables = []\n@@ -141,10 +150,7 @@ class JAXTrainer(base_trainer.Trainer):\n             metrics_variables,\n         ) = state\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        loss, (\n-            y_pred,\n-            non_trainable_variables,\n-        ) = self.compute_loss_and_updates(\n+        loss, aux = self.compute_loss_and_updates(\n             trainable_variables,\n             non_trainable_variables,\n             x,\n@@ -152,6 +158,7 @@ class JAXTrainer(base_trainer.Trainer):\n             sample_weight,\n             training=False,\n         )\n+        (unscaled_loss, y_pred, non_trainable_variables) = aux\n \n         with backend.StatelessScope(\n             state_mapping=[\n@@ -159,7 +166,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 for ref_v, v in zip(self.metrics_variables, metrics_variables)\n             ]\n         ) as scope:\n-            self._loss_tracker.update_state(loss)\n+            self._loss_tracker.update_state(unscaled_loss)\n             logs = self.compute_metrics(x, y, y_pred, sample_weight)\n \n         new_metrics_variables = []\n@@ -831,7 +838,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self._non_trainable_variable_shardings = [\n             v.value.sharding for v in self.non_trainable_variables\n         ]\n-        if hasattr(self, \"optimizer\"):\n+        if hasattr(self, \"optimizer\") and self.optimizer is not None:\n             self._optimizer_variable_shardings = [\n                 v.value.sharding for v in self.optimizer.variables\n             ]\n\n@@ -86,8 +86,6 @@ class TFOptimizer(base_optimizer.BaseOptimizer):\n             return self.update_step(grad, var, learning_rate)\n \n         for grad, var in grads_and_vars:\n-            if isinstance(var, backend.Variable):\n-                var = var.value\n             distribution.extended.update(\n                 var, apply_grad_to_update_var, args=(grad,), group=False\n             )\n\n@@ -60,6 +60,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n             )\n             self._loss_tracker.update_state(loss)\n+            if self.optimizer is not None:\n+                loss = self.optimizer.scale_loss(loss)\n \n         # Compute gradients\n         if self.trainable_weights:\n\n@@ -40,6 +40,8 @@ class TorchTrainer(base_trainer.Trainer):\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n         self._loss_tracker.update_state(loss)\n+        if self.optimizer is not None:\n+            loss = self.optimizer.scale_loss(loss)\n \n         # Compute gradients\n         if self.trainable_weights:\n\n@@ -12,7 +12,6 @@ from keras_core.api_export import keras_core_export\n from keras_core.callbacks.callback import Callback\n from keras_core.layers import Embedding\n from keras_core.optimizers import Optimizer\n-from keras_core.optimizers.schedules import learning_rate_schedule\n from keras_core.utils import file_utils\n \n \n@@ -503,12 +502,8 @@ class TensorBoard(Callback):\n \n     def _collect_learning_rate(self, logs):\n         if isinstance(self.model.optimizer, Optimizer):\n-            lr_schedule = getattr(self.model.optimizer, \"_learning_rate\", None)\n-        else:\n-            lr_schedule = getattr(self.model.optimizer, \"lr\", None)\n-        if isinstance(lr_schedule, learning_rate_schedule.LearningRateSchedule):\n             logs[\"learning_rate\"] = float(\n-                lr_schedule(self.model.optimizer.iterations)\n+                ops.convert_to_numpy(self.model.optimizer.learning_rate)\n             )\n         return logs\n \n\n@@ -165,6 +165,7 @@ class TestTensorBoardV2(testing.TestCase):\n             {\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(\n                     logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n@@ -196,6 +197,7 @@ class TestTensorBoardV2(testing.TestCase):\n             {\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(\n                     logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n@@ -240,6 +242,7 @@ class TestTensorBoardV2(testing.TestCase):\n                 _ObservedSummary(logdir=train_dir, tag=\"batch_loss\"),\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(\n                     logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n@@ -334,6 +337,7 @@ class TestTensorBoardV2(testing.TestCase):\n             {\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(\n                     logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n@@ -378,6 +382,7 @@ class TestTensorBoardV2(testing.TestCase):\n             {\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(\n                     logdir=validation_dir,\n                     tag=\"evaluation_loss_vs_iterations\",\n@@ -492,6 +497,7 @@ class TestTensorBoardV2(testing.TestCase):\n             {\n                 _ObservedSummary(logdir=train_dir, tag=\"batch_loss\"),\n                 _ObservedSummary(logdir=train_dir, tag=\"epoch_loss\"),\n+                _ObservedSummary(logdir=train_dir, tag=\"epoch_learning_rate\"),\n                 _ObservedSummary(logdir=validation_dir, tag=\"epoch_loss\"),\n                 _ObservedSummary(\n                     logdir=validation_dir,\n\n@@ -537,7 +537,10 @@ class Cond(Operation):\n         return true_fn_spec\n \n     def _check_output_spec(self, true_fn_spec, false_fn_spec):\n-        if isinstance(true_fn_spec, dict):\n+        if true_fn_spec is None:\n+            if false_fn_spec is not None:\n+                return False\n+        elif isinstance(true_fn_spec, dict):\n             if not isinstance(false_fn_spec, dict):\n                 return False\n             if true_fn_spec.keys() != false_fn_spec.keys():\n@@ -557,6 +560,16 @@ class Cond(Operation):\n                 for ti, fi in zip(true_fn_spec, false_fn_spec)\n             ):\n                 return False\n+        elif isinstance(true_fn_spec, tuple):\n+            if not isinstance(false_fn_spec, tuple):\n+                return False\n+            if len(true_fn_spec) != len(false_fn_spec):\n+                return False\n+            if any(\n+                (not self._check_output_spec(ti, fi))\n+                for ti, fi in zip(true_fn_spec, false_fn_spec)\n+            ):\n+                return False\n         else:\n             if true_fn_spec.dtype != false_fn_spec.dtype:\n                 return False\n\n@@ -328,6 +328,8 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertEqual(t, 0)\n         f = ops.cond(False, lambda: 0, lambda: 1)\n         self.assertEqual(f, 1)\n+        f = ops.cond(False, lambda: None, lambda: None)\n+        self.assertEqual(f, None)\n \n         for val in [True, False]:\n             out = ops.cond(\n\n@@ -7,6 +7,7 @@ from keras_core.optimizers.adamax import Adamax\n from keras_core.optimizers.adamw import AdamW\n from keras_core.optimizers.ftrl import Ftrl\n from keras_core.optimizers.lion import Lion\n+from keras_core.optimizers.loss_scale_optimizer import LossScaleOptimizer\n from keras_core.optimizers.nadam import Nadam\n from keras_core.optimizers.optimizer import Optimizer\n from keras_core.optimizers.rmsprop import RMSprop\n@@ -26,6 +27,7 @@ ALL_OBJECTS = {\n     Nadam,\n     Ftrl,\n     Lion,\n+    LossScaleOptimizer,\n }\n ALL_OBJECTS_DICT = {cls.__name__.lower(): cls for cls in ALL_OBJECTS}\n \n\n@@ -21,6 +21,7 @@ class BaseOptimizer:\n         use_ema=False,\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n+        loss_scale_factor=None,\n         name=None,\n         **kwargs,\n     ):\n@@ -41,6 +42,7 @@ class BaseOptimizer:\n         self.global_clipnorm = global_clipnorm\n         self.clipvalue = clipvalue\n         self.use_ema = use_ema\n+        self.loss_scale_factor = loss_scale_factor\n \n         if use_ema:\n             # Verify the arguments related to EMA.\n@@ -250,6 +252,11 @@ class BaseOptimizer:\n             if len(list(grads)) == 0:\n                 return\n \n+            # Unscale gradients.\n+            scale = self.loss_scale_factor\n+            if scale is not None:\n+                grads = [g if g is None else g / scale for g in grads]\n+\n             # Apply clipping and weight decay.\n             grads = self._clip_gradients(grads)\n             self._apply_weight_decay(trainable_variables)\n@@ -265,9 +272,8 @@ class BaseOptimizer:\n                     variable.assign(variable.constraint(variable))\n \n     def _internal_apply_gradients(self, grads_and_vars):\n-        learning_rate = self._get_current_learning_rate()\n         for grad, var in grads_and_vars:\n-            self.update_step(grad, var, learning_rate)\n+            self.update_step(grad, var, self.learning_rate)\n         self.iterations.assign(self.iterations + 1)\n \n     def stateless_apply(self, optimizer_variables, grads, trainable_variables):\n@@ -322,6 +328,17 @@ class BaseOptimizer:\n                 optimizer_variables.append(v)\n         return trainable_variables, optimizer_variables\n \n+    def scale_loss(self, loss):\n+        \"\"\"Scale the loss before computing gradients.\n+\n+        Scales the loss before gradients are computed in a `train_step`. This\n+        is primarily useful during mixed precision training to prevent numeric\n+        underflow.\n+        \"\"\"\n+        if self.loss_scale_factor is not None:\n+            return loss * self.loss_scale_factor\n+        return loss\n+\n     @property\n     def learning_rate(self):\n         return self._get_current_learning_rate()\n@@ -494,7 +511,7 @@ class BaseOptimizer:\n             return\n         for variable in variables:\n             if self._use_weight_decay(variable):\n-                lr = ops.cast(self._get_current_learning_rate(), variable.dtype)\n+                lr = ops.cast(self.learning_rate, variable.dtype)\n                 wd = ops.cast(self.weight_decay, variable.dtype)\n                 variable.assign(variable - variable * wd * lr)\n \n@@ -596,6 +613,7 @@ class BaseOptimizer:\n             \"use_ema\": self.use_ema,\n             \"ema_momentum\": self.ema_momentum,\n             \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n+            \"loss_scale_factor\": self.loss_scale_factor,\n         }\n         return config\n \n@@ -665,7 +683,14 @@ base_optimizer_keyword_args = \"\"\"name: String. The name to use\n           (which updates the model\n           variables in-place). When using the built-in `fit()` training loop,\n           this happens automatically after the last epoch,\n-          and you don't need to do anything.\"\"\"\n+          and you don't need to do anything.\n+      loss_scale_factor: Float or `None`. If a float, the scale factor will\n+          be multiplied the loss before computing gradients, and the inverse of\n+          the scale factor will be multiplied by the gradients before updating\n+          variables. Useful for preventing underflow during mixed precision\n+          training. Alternately, `keras_core.optimizers.LossScaleOptimizer` will\n+          automatically set a loss scale factor.\n+\"\"\"\n \n \n def clip_by_norm(values, clip_norm, axes=None):\n\n@@ -0,0 +1,231 @@\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+from keras_core.saving import serialization_lib\n+from keras_core.utils import tracking\n+\n+\n+@keras_core_export(\n+    [\n+        \"keras_core.optimizers.LossScaleOptimizer\",\n+        \"keras_core.mixed_precision.LossScaleOptimizer\",\n+    ]\n+)\n+class LossScaleOptimizer(optimizer.Optimizer):\n+    \"\"\"An optimizer that dynamically scales the loss to prevent underflow.\n+\n+    Loss scaling is a technique to prevent numeric underflow in intermediate\n+    gradients when float16 is used. To prevent underflow, the loss is multiplied\n+    (or \"scaled\") by a certain factor called the \"loss scale\", which causes\n+    intermediate gradients to be scaled by the loss scale as well. The final\n+    gradients are divided (or \"unscaled\") by the loss scale to bring them back\n+    to their original value.\n+\n+    `LossScaleOptimizer` wraps another optimizer and applies dynamic loss\n+    scaling to it. This loss scale is dynamically updated over time as follows:\n+    - On any train step, if a nonfinite gradient is encountered, the loss scale\n+      is halved, and the train step is skipped.\n+    - If `dynamic_growth_steps` have ocurred since the last time the loss scale\n+      was updated, and no nonfinite gradients have occurred, the loss scale\n+      is doubled.\n+\n+    Args:\n+        inner_optimizer: The `keras_core.optimizers.Optimizer` instance to wrap.\n+        initial_scale: Float. The initial loss scale. This scale will be updated\n+            during training. It is recommended for this to be a very high\n+            number, because a loss scale that is too high gets lowered far more\n+            quickly than a loss scale that is too low gets raised.\n+        dynamic_growth_steps: Int. How often to update the scale upwards. After\n+            every `dynamic_growth_steps` steps with finite gradients, the\n+            loss scale is doubled.\n+        {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        inner_optimizer,\n+        initial_scale=2.0**15,\n+        dynamic_growth_steps=2000,\n+        **kwargs,\n+    ):\n+        if not kwargs.pop(\"dynamic\", True):\n+            raise ValueError(\n+                \"LossScaleOptimizer no longer suports `dynamic=False`. \"\n+                \"Instead, simply set `loss_scale_factor` directly on the \"\n+                \"`inner_optimizer`.\"\n+            )\n+        super().__init__(learning_rate=0.0, **kwargs)\n+        self.inner_optimizer = inner_optimizer\n+        self.initial_scale = initial_scale\n+        self.dynamic_growth_steps = dynamic_growth_steps\n+\n+    @tracking.no_automatic_dependency_tracking\n+    def build(self, var_list):\n+        self.step_counter = self.add_variable(\n+            shape=(),\n+            dtype=\"int\",\n+            initializer=initializers.Zeros(),\n+            name=\"step_counter\",\n+        )\n+        self.dynamic_scale = self.add_variable(\n+            shape=(),\n+            dtype=\"float32\",\n+            initializer=initializers.Constant(self.initial_scale),\n+            name=\"dynamic_scale\",\n+        )\n+        self.inner_optimizer.build(var_list)\n+        self.built = True\n+\n+    @property\n+    def variables(self):\n+        return self._variables + self.inner_optimizer.variables\n+\n+    def stateless_apply(self, optimizer_variables, grads, trainable_variables):\n+        if not self.built:\n+            raise ValueError(\n+                f\"To call `stateless_apply`, {self.__class__.__name__} \"\n+                \"must be built (i.e. its variables must have been created). \"\n+                \"You can build it via `optimizer.build(trainable_variables)`.\"\n+            )\n+\n+        def handle_finite_grads():\n+            def upscale():\n+                mapping = list(zip(self.variables, optimizer_variables))\n+                with backend.StatelessScope(state_mapping=mapping) as scope:\n+                    self.step_counter.assign(0)\n+                    self.dynamic_scale.assign(self.dynamic_scale * 2.0)\n+                return [scope.get_current_value(v) for v in self._variables]\n+\n+            def increment():\n+                mapping = list(zip(self.variables, optimizer_variables))\n+                with backend.StatelessScope(state_mapping=mapping) as scope:\n+                    self.step_counter.assign_add(1)\n+                return [scope.get_current_value(v) for v in self._variables]\n+\n+            mapping = list(zip(self.variables, optimizer_variables))\n+            with backend.StatelessScope(state_mapping=mapping):\n+                # Potentially upscale loss and reset counter.\n+                own_variables = ops.cond(\n+                    ops.equal(self.step_counter, self.dynamic_growth_steps - 1),\n+                    upscale,\n+                    increment,\n+                )\n+\n+                # Unscale gradients.\n+                scale = self.dynamic_scale\n+                unscaled_grads = [\n+                    g if g is None else ops.divide(g, scale) for g in grads\n+                ]\n+                (\n+                    new_trainable_variables,\n+                    new_inner_variables,\n+                ) = self.inner_optimizer.stateless_apply(\n+                    self.inner_optimizer.variables,\n+                    unscaled_grads,\n+                    trainable_variables,\n+                )\n+\n+            new_optimizer_variables = own_variables + new_inner_variables\n+            return new_trainable_variables, new_optimizer_variables\n+\n+        def handle_non_finite_grads():\n+            mapping = list(zip(self.variables, optimizer_variables))\n+            with backend.StatelessScope(state_mapping=mapping) as scope:\n+                self.step_counter.assign(0)\n+                self.dynamic_scale.assign(self.dynamic_scale / 2.0)\n+            new_optimizer_variables = []\n+            for v in self.variables:\n+                new_optimizer_variables.append(scope.get_current_value(v))\n+            return trainable_variables, new_optimizer_variables\n+\n+        finite = self.check_finite(grads)\n+        return ops.cond(finite, handle_finite_grads, handle_non_finite_grads)\n+\n+    def apply(self, grads, trainable_variables=None):\n+        # Optionally build optimizer.\n+        if not self.built:\n+            with backend.name_scope(self.name, caller=self):\n+                self.build(trainable_variables)\n+            self.built = True\n+\n+        def handle_finite_grads():\n+            scale = self.dynamic_scale\n+            # Unscale gradients.\n+            unscaled_grads = [\n+                g if g is None else ops.divide(g, scale) for g in grads\n+            ]\n+            self.inner_optimizer.apply(\n+                unscaled_grads, trainable_variables=trainable_variables\n+            )\n+\n+            def upscale():\n+                self.step_counter.assign(0)\n+                self.dynamic_scale.assign(self.dynamic_scale * 2.0)\n+\n+            def increment():\n+                self.step_counter.assign_add(1)\n+\n+            # Potentially upscale loss and reset counter.\n+            ops.cond(\n+                ops.equal(self.step_counter, self.dynamic_growth_steps - 1),\n+                upscale,\n+                increment,\n+            )\n+\n+        def handle_non_finite_grads():\n+            # If any inf or nan in grads, downscale loss and reset counter.\n+            self.step_counter.assign(0)\n+            self.dynamic_scale.assign(self.dynamic_scale / 2.0)\n+\n+        finite = self.check_finite(grads)\n+        ops.cond(finite, handle_finite_grads, handle_non_finite_grads)\n+\n+    def check_finite(self, grads):\n+        tensor_grads = [g for g in grads if g is not None]\n+        finite_grads = [ops.all(ops.isfinite(g)) for g in tensor_grads]\n+        return ops.all(ops.convert_to_tensor(finite_grads))\n+\n+    @property\n+    def learning_rate(self):\n+        return self.inner_optimizer.learning_rate\n+\n+    @learning_rate.setter\n+    def learning_rate(self, learning_rate):\n+        self.inner_optimizer.learning_rate = learning_rate\n+\n+    def scale_loss(self, loss):\n+        scale = self.dynamic_scale if self.built else self.initial_scale\n+        return loss * scale\n+\n+    def finalize_variable_values(self, var_list):\n+        self.inner_optimizer.finalize_variable_values(var_list)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+        inner_optimizer_config = serialization_lib.serialize_keras_object(\n+            self.inner_optimizer\n+        )\n+        config.update(\n+            {\n+                \"inner_optimizer\": inner_optimizer_config,\n+                \"initial_scale\": self.initial_scale,\n+                \"dynamic_growth_steps\": self.dynamic_growth_steps,\n+            }\n+        )\n+        del config[\"learning_rate\"]\n+        return config\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        inner_optimizer = serialization_lib.deserialize_keras_object(\n+            config.pop(\"inner_optimizer\"),\n+            custom_objects=custom_objects,\n+        )\n+        return cls(inner_optimizer, **config)\n+\n+\n+LossScaleOptimizer.__doc__ = LossScaleOptimizer.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,98 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import ops\n+from keras_core import testing\n+from keras_core.optimizers.loss_scale_optimizer import LossScaleOptimizer\n+from keras_core.optimizers.sgd import SGD\n+\n+\n+class LossScaleOptimizerTest(testing.TestCase, parameterized.TestCase):\n+    def test_config(self):\n+        inner_optimizer = SGD(\n+            learning_rate=0.5,\n+            momentum=0.06,\n+            nesterov=True,\n+            weight_decay=0.004,\n+        )\n+        optimizer = LossScaleOptimizer(inner_optimizer)\n+        self.run_class_serialization_test(optimizer)\n+\n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_finite_step(self, stateless):\n+        if not stateless and backend.backend() == \"jax\":\n+            self.skipTest(\"LossScaleOptimizer must use stateless_apply on jax.\")\n+        inner_optimizer = SGD(learning_rate=0.5)\n+        optimizer = LossScaleOptimizer(inner_optimizer)\n+        grads = [ops.array([1.0, 6.0, 7.0, 2.0]) * optimizer.initial_scale]\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        if stateless:\n+            optimizer.build(vars)\n+            vars, _ = optimizer.stateless_apply(\n+                optimizer.variables, grads, vars\n+            )\n+        else:\n+            optimizer.apply(grads, vars)\n+        self.assertAllClose(\n+            vars, [[0.5, -1.0, -0.5, 3.0]], rtol=1e-4, atol=1e-4\n+        )\n+\n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_infinite_step(self, stateless):\n+        if not stateless and backend.backend() == \"jax\":\n+            self.skipTest(\"LossScaleOptimizer must use stateless_apply on jax.\")\n+        inner_optimizer = SGD(learning_rate=0.5)\n+        optimizer = LossScaleOptimizer(inner_optimizer)\n+        grads = [ops.array([np.inf, np.inf, np.inf, np.inf])]\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        if stateless:\n+            optimizer.build(vars)\n+            vars, _ = optimizer.stateless_apply(\n+                optimizer.variables, grads, vars\n+            )\n+        else:\n+            optimizer.apply(grads, vars)\n+        self.assertAllClose(vars, [[1.0, 2.0, 3.0, 4.0]], rtol=1e-4, atol=1e-4)\n+\n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_downscaling(self, stateless):\n+        if not stateless and backend.backend() == \"jax\":\n+            self.skipTest(\"LossScaleOptimizer must use stateless_apply on jax.\")\n+        inner_optimizer = SGD(learning_rate=0.5)\n+        optimizer = LossScaleOptimizer(inner_optimizer, initial_scale=400.0)\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        optimizer.build(vars)\n+        opt_vars = optimizer.variables\n+        grads = [ops.array([np.inf, np.inf, np.inf, np.inf])]\n+        for _ in range(4):\n+            if stateless:\n+                _, opt_vars = optimizer.stateless_apply(opt_vars, grads, vars)\n+                for ref_v, v in zip(optimizer.variables, opt_vars):\n+                    ref_v.assign(v)\n+            else:\n+                optimizer.apply(grads, vars)\n+        self.assertAllClose(optimizer.scale_loss(1.0), 25.0)\n+\n+    @parameterized.named_parameters((\"stateless\", True), (\"stateful\", False))\n+    def test_upscaling(self, stateless):\n+        if not stateless and backend.backend() == \"jax\":\n+            self.skipTest(\"LossScaleOptimizer must use stateless_apply on jax.\")\n+        inner_optimizer = SGD(learning_rate=0.5)\n+        optimizer = LossScaleOptimizer(\n+            inner_optimizer,\n+            initial_scale=2.0,\n+            dynamic_growth_steps=2,\n+        )\n+        vars = [backend.Variable([1.0, 2.0, 3.0, 4.0])]\n+        optimizer.build(vars)\n+        opt_vars = optimizer.variables\n+        grads = [ops.array([1.0, 6.0, 7.0, 2.0])]\n+        for _ in range(8):\n+            if stateless:\n+                _, opt_vars = optimizer.stateless_apply(opt_vars, grads, vars)\n+                for ref_v, v in zip(optimizer.variables, opt_vars):\n+                    ref_v.assign(v)\n+            else:\n+                optimizer.apply(grads, vars)\n+        self.assertAllClose(optimizer.scale_loss(1.0), 32.0)\n\n@@ -27,6 +27,14 @@ class OptimizerTest(testing.TestCase):\n         with self.assertRaises(ValueError):\n             optimizers.get(\"typo\")\n \n+    def test_static_loss_scaling(self):\n+        v = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n+        grads = backend.convert_to_tensor([[1.0, 2.0], [3.0, 4.0]]) * 1024.0\n+        optimizer = optimizers.SGD(learning_rate=1.0, loss_scale_factor=1024.0)\n+        optimizer.apply_gradients([(grads, v)])\n+        self.assertEqual(optimizer.scale_loss(1.0), 1024.0)\n+        self.assertAllClose(v, [[0.0, 0.0], [0.0, 0.0]])\n+\n     def test_set_weights(self):\n         x = backend.Variable([[1.0, 2.0], [3.0, 4.0]])\n         optimizer_1 = optimizers.Adam()\n\n@@ -5,6 +5,7 @@ from keras_core import backend\n from keras_core import metrics as metrics_module\n from keras_core import ops\n from keras_core import optimizers\n+from keras_core.optimizers.loss_scale_optimizer import LossScaleOptimizer\n from keras_core.saving import serialization_lib\n from keras_core.trainers.compile_utils import CompileLoss\n from keras_core.trainers.compile_utils import CompileMetrics\n@@ -33,6 +34,7 @@ class Trainer:\n         run_eagerly=False,\n         steps_per_execution=1,\n         jit_compile=\"auto\",\n+        auto_scale_loss=True,\n     ):\n         \"\"\"Configures the model for training.\n \n@@ -116,8 +118,21 @@ class Trainer:\n                 compiling a model. Not supported with the PyTorch backend.\n                 If `\"auto\"`, XLA compilation will be enabled if the\n                 the model supports it, and disabled otherwise.\n+            auto_scale_loss: Bool. If `True` and the model dtype policy is\n+                `\"mixed_float16\"`, the passed optimizer will be automatically\n+                wrapped in a `LossScaleOptimizer`, which will dynamically\n+                scale the loss to prevent underflow.\n         \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n+        if (\n+            auto_scale_loss\n+            and self.dtype_policy.name == \"mixed_float16\"\n+            and self.optimizer\n+            and not isinstance(self.optimizer, LossScaleOptimizer)\n+        ):\n+            self.optimizer = LossScaleOptimizer(\n+                self.optimizer, name=\"loss_scale_optimizer\"\n+            )\n         if hasattr(self, \"output_names\"):\n             output_names = self.output_names\n         else:\n\n@@ -12,6 +12,7 @@ from keras_core import ops\n from keras_core import optimizers\n from keras_core import testing\n from keras_core.callbacks.callback import Callback\n+from keras_core.optimizers.rmsprop import RMSprop\n \n if backend.backend() == \"jax\":\n     from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n@@ -82,8 +83,8 @@ class ListModel(layers.Layer, Trainer):\n \n \n class TrainingTestingLayer(layers.Layer, Trainer):\n-    def __init__(self):\n-        layers.Layer.__init__(self)\n+    def __init__(self, **kwargs):\n+        layers.Layer.__init__(self, **kwargs)\n         Trainer.__init__(self)\n \n     def call(self, x, training=False):\n@@ -331,6 +332,76 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         model.evaluate(x, y, batch_size=batch_size, callbacks=[step_count])\n         self.assertEqual(step_count.test_count, 3)\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_adds_loss_scaling_optimizer(self):\n+        model = TrainingTestingLayer(dtype=\"mixed_float16\")\n+        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+        x = np.ones((128, 1))\n+        y = np.zeros((128, 1))\n+        model.fit(x, y, batch_size=32)\n+        self.assertIsInstance(model.optimizer, optimizers.LossScaleOptimizer)\n+\n+        model = TrainingTestingLayer(dtype=\"mixed_float16\")\n+        model.compile(optimizer=\"rmsprop\", loss=\"mse\", auto_scale_loss=False)\n+        x = np.ones((128, 1))\n+        y = np.zeros((128, 1))\n+        model.fit(x, y, batch_size=32)\n+        self.assertIsInstance(model.optimizer, RMSprop)\n+\n+        model = TrainingTestingLayer(dtype=\"mixed_bfloat16\")\n+        model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+        x = np.ones((128, 1))\n+        y = np.zeros((128, 1))\n+        model.fit(x, y, batch_size=32)\n+        self.assertIsInstance(model.optimizer, RMSprop)\n+\n+    @pytest.mark.requires_trainable_backend\n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"half precision unsupported on torch CPU.\",\n+    )\n+    def test_loss_scaling_prevents_underflow(self):\n+        class DeepModel(layers.Layer, Trainer):\n+            def __init__(self):\n+                layers.Layer.__init__(self, dtype=\"mixed_float16\")\n+                Trainer.__init__(self)\n+                self.layers = []\n+                for _ in range(15):\n+                    # Sigmoid has a small gradient, will eventually underflow.\n+                    self.layers.append(\n+                        layers.Dense(\n+                            1,\n+                            use_bias=False,\n+                            kernel_initializer=\"ones\",\n+                            activation=\"sigmoid\",\n+                            dtype=\"mixed_float16\",\n+                        )\n+                    )\n+\n+            def call(self, x):\n+                for layer in self.layers:\n+                    x = layer(x)\n+                return x\n+\n+        loss = losses.MeanSquaredError()\n+        # Blow up any gradient updates, so underflow is obvious.\n+        optimizer = optimizers.SGD(learning_rate=1e9)\n+        model = DeepModel()\n+        model.compile(optimizer, loss=loss, auto_scale_loss=False)\n+        model.fit(np.ones((1, 1)), np.ones((1, 1)), batch_size=1)\n+        first_kernel = model.layers[0].kernel\n+        # Without autoscaling, the first dense will not update.\n+        self.assertEqual(first_kernel, np.ones_like(first_kernel))\n+\n+        # Blow up any gradient updates, so underflow is obvious.\n+        optimizer = optimizers.SGD(learning_rate=1e9)\n+        model = DeepModel()\n+        model.compile(optimizer, loss=loss, auto_scale_loss=True)\n+        model.fit(np.ones((1, 1)), np.ones((1, 1)), batch_size=1)\n+        first_kernel = model.layers[0].kernel\n+        # With autoscaling, the first dense will update.\n+        self.assertNotEqual(first_kernel, np.ones_like(first_kernel))\n+\n     @pytest.mark.requires_trainable_backend\n     def test_training_arg(self):\n         model = TrainingTestingLayer()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#66fe923b15959e4e628e09510283792ff4c6781b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 15 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 79 | Churn Cumulative: 101 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,6 @@\n+import base64\n+import marshal\n+\n from keras_core import testing\n from keras_core.utils import python_utils\n \n@@ -20,3 +23,79 @@ class PythonUtilsTest(testing.TestCase):\n         x = \"model.keras\"\n         self.assertEqual(python_utils.removeprefix(x, \"model\"), \".keras\")\n         self.assertEqual(python_utils.removeprefix(x, \".keras\"), x)\n+\n+    def test_func_load_defaults_as_tuple(self):\n+        # Using tuple as a default argument\n+        def dummy_function(x=(1, 2, 3)):\n+            pass\n+\n+        serialized = python_utils.func_dump(dummy_function)\n+        deserialized = python_utils.func_load(serialized)\n+        # Ensure that the defaults are still a tuple\n+        self.assertIsInstance(deserialized.__defaults__[0], tuple)\n+        # Ensure that the tuple default remains unchanged\n+        self.assertEqual(deserialized.__defaults__[0], (1, 2, 3))\n+\n+    def test_remove_long_seq_standard_case(self):\n+        sequences = [[1], [2, 2], [3, 3, 3], [4, 4, 4, 4]]\n+        labels = [1, 2, 3, 4]\n+        new_sequences, new_labels = python_utils.remove_long_seq(\n+            3, sequences, labels\n+        )\n+        self.assertEqual(new_sequences, [[1], [2, 2]])\n+        self.assertEqual(new_labels, [1, 2])\n+\n+    def test_func_load_with_closure(self):\n+        def outer_fn(x):\n+            def inner_fn(y):\n+                return x + y\n+\n+            return inner_fn\n+\n+        func_with_closure = outer_fn(10)\n+        serialized = python_utils.func_dump(func_with_closure)\n+        deserialized = python_utils.func_load(serialized)\n+        self.assertEqual(deserialized(5), 15)\n+\n+    def test_func_load_closure_conversion(self):\n+        def my_function_with_closure(x):\n+            return x + y\n+\n+        y = 5\n+        serialized = python_utils.func_dump(my_function_with_closure)\n+        deserialized = python_utils.func_load(serialized)\n+        self.assertEqual(deserialized(5), 10)\n+\n+    def test_ensure_value_to_cell(self):\n+        value_to_test = \"test_value\"\n+\n+        def dummy_fn():\n+            value_to_test\n+\n+        cell_value = dummy_fn.__closure__[0].cell_contents\n+        self.assertEqual(value_to_test, cell_value)\n+\n+    def test_closure_processing(self):\n+        def simple_function(x):\n+            return x + 10\n+\n+        serialized = python_utils.func_dump(simple_function)\n+        deserialized = python_utils.func_load(serialized)\n+        self.assertEqual(deserialized(5), 15)\n+\n+    def test_func_load_valid_encoded_code(self):\n+        def another_simple_function(x):\n+            return x * 2\n+\n+        raw_data = marshal.dumps(another_simple_function.__code__)\n+        valid_encoded_code = base64.b64encode(raw_data).decode(\"utf-8\")\n+\n+        try:\n+            python_utils.func_load(valid_encoded_code)\n+        except (UnicodeEncodeError, ValueError):\n+            self.fail(\"Expected no error for valid code, but got an error.\")\n+\n+    def test_func_load_bad_encoded_code(self):\n+        bad_encoded_code = \"This isn't valid base64!\"\n+        with self.assertRaises(AttributeError):\n+            python_utils.func_load(bad_encoded_code)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
