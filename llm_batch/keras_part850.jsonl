{"custom_id": "keras#d6dfba5e756b1b7d855396d0c3444223bc6755fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 252 | Lines Deleted: 47 | Files Changed: 7 | Hunks: 26 | Methods Changed: 34 | Complexity Δ (Sum/Max): 26/35 | Churn Δ: 299 | Churn Cumulative: 802 | Contributors (this commit): 7 | Commits (past 90d): 21 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,104 @@\n+import jax\n+import numpy as np\n+\n+import keras\n+\n+\n+def test_custom_fit():\n+    class CustomModel(keras.Model):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n+            self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n+            self.loss_fn = keras.losses.MeanSquaredError()\n+\n+        def compute_loss_and_updates(\n+            self,\n+            trainable_variables,\n+            non_trainable_variables,\n+            x,\n+            y,\n+            training=False,\n+        ):\n+            y_pred, non_trainable_variables = self.stateless_call(\n+                trainable_variables,\n+                non_trainable_variables,\n+                x,\n+                training=training,\n+            )\n+            loss = self.loss_fn(y, y_pred)\n+            return loss, (y_pred, non_trainable_variables)\n+\n+        def train_step(self, state, data):\n+            (\n+                trainable_variables,\n+                non_trainable_variables,\n+                optimizer_variables,\n+                metrics_variables,\n+            ) = state\n+            x, y = data\n+            grad_fn = jax.value_and_grad(\n+                self.compute_loss_and_updates, has_aux=True\n+            )\n+            (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n+                trainable_variables,\n+                non_trainable_variables,\n+                x,\n+                y,\n+                training=True,\n+            )\n+            (\n+                trainable_variables,\n+                optimizer_variables,\n+            ) = self.optimizer.stateless_apply(\n+                optimizer_variables, grads, trainable_variables\n+            )\n+            loss_tracker_vars = metrics_variables[\n+                : len(self.loss_tracker.variables)\n+            ]\n+            mae_metric_vars = metrics_variables[\n+                len(self.loss_tracker.variables) :\n+            ]\n+            loss_tracker_vars = self.loss_tracker.stateless_update_state(\n+                loss_tracker_vars, loss\n+            )\n+            mae_metric_vars = self.mae_metric.stateless_update_state(\n+                mae_metric_vars, y, y_pred\n+            )\n+            logs = {}\n+            logs[self.loss_tracker.name] = self.loss_tracker.stateless_result(\n+                loss_tracker_vars\n+            )\n+            logs[self.mae_metric.name] = self.mae_metric.stateless_result(\n+                mae_metric_vars\n+            )\n+            new_metrics_vars = loss_tracker_vars + mae_metric_vars\n+            state = (\n+                trainable_variables,\n+                non_trainable_variables,\n+                optimizer_variables,\n+                new_metrics_vars,\n+            )\n+            return logs, state\n+\n+        @property\n+        def metrics(self):\n+            return [self.loss_tracker, self.mae_metric]\n+\n+    inputs = keras.Input(shape=(32,))\n+    outputs = keras.layers.Dense(1)(inputs)\n+    model = CustomModel(inputs, outputs)\n+    model.compile(optimizer=\"adam\")\n+    x = np.random.random((64, 32))\n+    y = np.random.random((64, 1))\n+    history = model.fit(x, y, epochs=1)\n+\n+    assert \"loss\" in history.history\n+    assert \"mae\" in history.history\n+\n+    print(\"History:\")\n+    print(history.history)\n+\n+\n+if __name__ == \"__main__\":\n+    test_custom_fit()\n\n@@ -0,0 +1,50 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+import keras\n+\n+\n+def test_custom_fit():\n+    class CustomModel(keras.Model):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n+            self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n+            self.loss_fn = keras.losses.MeanSquaredError()\n+\n+        def train_step(self, data):\n+            x, y = data\n+            with tf.GradientTape() as tape:\n+                y_pred = self(x, training=True)\n+                loss = self.loss_fn(y, y_pred)\n+            trainable_vars = self.trainable_variables\n+            gradients = tape.gradient(loss, trainable_vars)\n+            self.optimizer.apply(gradients, trainable_vars)\n+            self.loss_tracker.update_state(loss)\n+            self.mae_metric.update_state(y, y_pred)\n+            return {\n+                \"loss\": self.loss_tracker.result(),\n+                \"mae\": self.mae_metric.result(),\n+            }\n+\n+        @property\n+        def metrics(self):\n+            return [self.loss_tracker, self.mae_metric]\n+\n+    inputs = keras.Input(shape=(32,))\n+    outputs = keras.layers.Dense(1)(inputs)\n+    model = CustomModel(inputs, outputs)\n+    model.compile(optimizer=\"adam\")\n+    x = np.random.random((64, 32))\n+    y = np.random.random((64, 1))\n+    history = model.fit(x, y, epochs=1)\n+\n+    assert \"loss\" in history.history\n+    assert \"mae\" in history.history\n+\n+    print(\"History:\")\n+    print(history.history)\n+\n+\n+if __name__ == \"__main__\":\n+    test_custom_fit()\n\n@@ -0,0 +1,52 @@\n+import numpy as np\n+import torch\n+\n+import keras\n+\n+\n+def test_custom_fit():\n+    class CustomModel(keras.Model):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n+            self.mae_metric = keras.metrics.MeanAbsoluteError(name=\"mae\")\n+            self.loss_fn = keras.losses.MeanSquaredError()\n+\n+        def train_step(self, data):\n+            x, y = data\n+            self.zero_grad()\n+            y_pred = self(x, training=True)\n+            loss = self.loss_fn(y, y_pred)\n+            loss.backward()\n+            trainable_weights = [v for v in self.trainable_weights]\n+            gradients = [v.value.grad for v in trainable_weights]\n+            with torch.no_grad():\n+                self.optimizer.apply(gradients, trainable_weights)\n+            self.loss_tracker.update_state(loss)\n+            self.mae_metric.update_state(y, y_pred)\n+            return {\n+                \"loss\": self.loss_tracker.result(),\n+                \"mae\": self.mae_metric.result(),\n+            }\n+\n+        @property\n+        def metrics(self):\n+            return [self.loss_tracker, self.mae_metric]\n+\n+    inputs = keras.Input(shape=(32,))\n+    outputs = keras.layers.Dense(1)(inputs)\n+    model = CustomModel(inputs, outputs)\n+    model.compile(optimizer=\"adam\")\n+    x = np.random.random((64, 32))\n+    y = np.random.random((64, 1))\n+    history = model.fit(x, y, epochs=1)\n+\n+    assert \"loss\" in history.history\n+    assert \"mae\" in history.history\n+\n+    print(\"History:\")\n+    print(history.history)\n+\n+\n+if __name__ == \"__main__\":\n+    test_custom_fit()\n\n@@ -281,7 +281,6 @@ class NumpyTrainer(base_trainer.Trainer):\n         for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n             logs = self.test_function(data)\n-            logs = self._pythonify_logs(logs)\n             callbacks.on_test_batch_end(step, logs)\n             if self.stop_evaluating:\n                 break\n\n@@ -6,6 +6,7 @@ from keras.src.api_export import keras_export\n from keras.src.callbacks.callback import Callback\n from keras.src.callbacks.history import History\n from keras.src.callbacks.progbar_logger import ProgbarLogger\n+from keras.src.utils import python_utils\n \n \n @keras_export(\"keras.callbacks.CallbackList\")\n@@ -114,31 +115,18 @@ class CallbackList(Callback):\n         future = self._executor.submit(fn, *args)\n         self._futures.append(future)\n \n-    def _pythonify_logs(self, logs):\n-        result = {}\n-        for key, value in sorted(logs.items()):\n-            if isinstance(value, dict):\n-                result.update(self._pythonify_logs(value))\n-            else:\n-                try:\n-                    value = float(value)\n-                except:\n-                    pass\n-                result[key] = value\n-        return result\n-\n     def _clear_futures(self):\n         for future in self._futures:\n             future.result()\n         self._futures = []\n \n     def on_batch_begin(self, batch, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_batch_begin(batch, logs=logs)\n \n     def on_epoch_begin(self, epoch, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_epoch_begin(epoch, logs)\n \n@@ -146,22 +134,22 @@ class CallbackList(Callback):\n         if self._async_train:\n             self._clear_futures()\n \n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_epoch_end(epoch, logs)\n \n     def on_train_batch_begin(self, batch, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_train_batch_begin(batch, logs=logs)\n \n     def on_test_batch_begin(self, batch, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_test_batch_begin(batch, logs=logs)\n \n     def on_predict_batch_begin(self, batch, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_predict_batch_begin(batch, logs=logs)\n \n@@ -190,30 +178,27 @@ class CallbackList(Callback):\n             self._on_predict_batch_end(batch, logs)\n \n     def _on_batch_end(self, batch, logs=None):\n-        logs = logs or {}\n-        logs = self._pythonify_logs(logs)\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_batch_end(batch, logs=logs)\n \n     def _on_train_batch_end(self, batch, logs=None):\n-        logs = logs or {}\n-        logs = self._pythonify_logs(logs)\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_train_batch_end(batch, logs=logs)\n \n     def _on_test_batch_end(self, batch, logs=None):\n-        logs = logs or {}\n-        logs = self._pythonify_logs(logs)\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_test_batch_end(batch, logs=logs)\n \n     def _on_predict_batch_end(self, batch, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_predict_batch_end(batch, logs=logs)\n \n     def on_train_begin(self, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_train_begin(logs)\n \n@@ -221,12 +206,12 @@ class CallbackList(Callback):\n         if self._async_train:\n             self._clear_futures()\n \n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_train_end(logs)\n \n     def on_test_begin(self, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_test_begin(logs)\n \n@@ -234,12 +219,12 @@ class CallbackList(Callback):\n         if self._async_test:\n             self._clear_futures()\n \n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_test_end(logs)\n \n     def on_predict_begin(self, logs=None):\n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_predict_begin(logs)\n \n@@ -247,6 +232,6 @@ class CallbackList(Callback):\n         if self._async_predict:\n             self._clear_futures()\n \n-        logs = logs or {}\n+        logs = python_utils.pythonify_logs(logs)\n         for callback in self.callbacks:\n             callback.on_predict_end(logs)\n\n@@ -12,6 +12,7 @@ from keras.src.saving import serialization_lib\n from keras.src.trainers.compile_utils import CompileLoss\n from keras.src.trainers.compile_utils import CompileMetrics\n from keras.src.trainers.data_adapters import data_adapter_utils\n+from keras.src.utils import python_utils\n from keras.src.utils import traceback_utils\n from keras.src.utils import tracking\n \n@@ -508,7 +509,7 @@ class Trainer:\n                 return_metrics.update(result)\n             else:\n                 return_metrics[metric.name] = result\n-        return self._pythonify_logs(return_metrics)\n+        return python_utils.pythonify_logs(return_metrics)\n \n     def fit(\n         self,\n@@ -965,19 +966,6 @@ class Trainer:\n                 f\"type {type(validation_freq)}.\"\n             )\n \n-    def _pythonify_logs(self, logs):\n-        result = {}\n-        for key, value in sorted(logs.items()):\n-            if isinstance(value, dict):\n-                result.update(self._pythonify_logs(value))\n-            else:\n-                try:\n-                    value = float(value)\n-                except:\n-                    pass\n-                result[key] = value\n-        return result\n-\n     def _get_metrics_result_or_logs(self, logs):\n         \"\"\"Returns model metrics as a dict if the keys match with input logs.\n \n\n@@ -148,3 +148,30 @@ def remove_by_id(lst, value):\n         if id(v) == id(value):\n             del lst[i]\n             return\n+\n+\n+def pythonify_logs(logs):\n+    \"\"\"Flatten and convert log values to Python-native types.\n+\n+    This function attempts to convert dict value by `float(value)` and skips\n+    the conversion if it fails.\n+\n+    Args:\n+        logs: A dict containing log values.\n+\n+    Returns:\n+        A flattened dict with values converted to Python-native types if\n+        possible.\n+    \"\"\"\n+    logs = logs or {}\n+    result = {}\n+    for key, value in sorted(logs.items()):\n+        if isinstance(value, dict):\n+            result.update(pythonify_logs(value))\n+        else:\n+            try:\n+                value = float(value)\n+            except:\n+                pass\n+            result[key] = value\n+    return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#27bcdce48e011092f7f64b1947ae1229685e118f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 43 | Churn Cumulative: 167 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.6190476190476191\n\nDIFF:\n@@ -128,28 +128,24 @@ class TestCase(parameterized.TestCase, unittest.TestCase):\n         # get_config roundtrip\n         cls = instance.__class__\n         config = instance.get_config()\n-        config_json = json.dumps(config, sort_keys=True, indent=4)\n+        config_json = to_json_with_tuples(config)\n         ref_dir = dir(instance)[:]\n         with custom_object_scope(custom_objects):\n             revived_instance = cls.from_config(config)\n         revived_config = revived_instance.get_config()\n-        revived_config_json = json.dumps(\n-            revived_config, sort_keys=True, indent=4\n-        )\n+        revived_config_json = to_json_with_tuples(revived_config)\n         self.assertEqual(config_json, revived_config_json)\n         self.assertEqual(set(ref_dir), set(dir(revived_instance)))\n \n         # serialization roundtrip\n         serialized = serialize_keras_object(instance)\n-        serialized_json = json.dumps(serialized, sort_keys=True, indent=4)\n+        serialized_json = to_json_with_tuples(serialized)\n         with custom_object_scope(custom_objects):\n             revived_instance = deserialize_keras_object(\n-                json.loads(serialized_json)\n+                from_json_with_tuples(serialized_json)\n             )\n         revived_config = revived_instance.get_config()\n-        revived_config_json = json.dumps(\n-            revived_config, sort_keys=True, indent=4\n-        )\n+        revived_config_json = to_json_with_tuples(revived_config)\n         self.assertEqual(config_json, revived_config_json)\n         new_dir = dir(revived_instance)[:]\n         for lst in [ref_dir, new_dir]:\n@@ -769,3 +765,32 @@ def get_seed_generators(layer):\n                 seed_generators.append(sg)\n                 seen_ids.add(id(sg))\n     return seed_generators\n+\n+\n+def to_json_with_tuples(value):\n+    def _tuple_encode(obj):\n+        if isinstance(obj, tuple):\n+            return {\"__class__\": \"tuple\", \"__value__\": list(obj)}\n+        if isinstance(obj, list):\n+            return [_tuple_encode(e) for e in obj]\n+        if isinstance(obj, dict):\n+            return {key: _tuple_encode(value) for key, value in obj.items()}\n+        return obj\n+\n+    class _PreserveTupleJsonEncoder(json.JSONEncoder):\n+        def encode(self, obj):\n+            obj = _tuple_encode(obj)\n+            return super().encode(obj)\n+\n+    return _PreserveTupleJsonEncoder(sort_keys=True, indent=4).encode(value)\n+\n+\n+def from_json_with_tuples(value):\n+    def _tuple_decode(obj):\n+        if not isinstance(obj, dict):\n+            return obj\n+        if \"__class__\" not in obj or \"__value__\" not in obj:\n+            return obj\n+        return tuple(obj[\"__value__\"])\n+\n+    return json.loads(value, object_hook=_tuple_decode)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a50fdf848ee3bf5c9d67a7ceaaa8f2ef76ed8777", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 13 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -25,6 +25,7 @@ class Softmax(Layer):\n     >>> softmax_layer = keras.layers.activations.Softmax()\n     >>> input = np.array([1.0, 2.0, 1.0])\n     >>> result = softmax_layer(input)\n+    >>> result\n     [0.21194157, 0.5761169, 0.21194157]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cdd58c758f32b4b9f75761317a693d220e19776", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 20 | Churn Cumulative: 1427 | Contributors (this commit): 2 | Commits (past 90d): 22 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -480,7 +480,7 @@ class KerasFileEditor:\n                         value, metadata=metadata, inner_path=inner_path\n                     )\n             else:\n-                result[key] = value[:]\n+                result[key] = value[()]\n         return result, metadata\n \n     def _generate_filepath_info(self, rich_style=False):\n\n@@ -89,3 +89,21 @@ class SavingTest(testing.TestCase):\n         editor.add_weights(\"dense_2\", {\"1\": np.zeros((3,))})\n         out = editor.compare(target_model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n+\n+    def test_scalar_weight(self):\n+        model = keras.Sequential(name=\"my_sequential\")\n+        model.add(keras.Input(shape=(1,), name=\"my_input\"))\n+        model.add(keras.layers.Dense(1, activation=\"sigmoid\", name=\"my_dense\"))\n+        model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n+        model.fit(np.array([[1]]), np.array([[1]]), verbose=0)\n+        model.save(\"model.keras\")\n+        model.save_weights(\"model.weights.h5\")\n+\n+        model_editor = KerasFileEditor(\"model.keras\")\n+        self.assertEqual(\n+            len(keras.src.tree.flatten(model_editor.weights_dict)), 8\n+        )\n+        model_weights_editor = KerasFileEditor(\"model.weights.h5\")\n+        self.assertEqual(\n+            len(keras.src.tree.flatten(model_weights_editor.weights_dict)), 8\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3544870b573a47b9e89ef1ac62ebde77ac4d61d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 135 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,7 @@\n import os\n \n import numpy as np\n+import pytest\n \n import keras\n from keras.src import testing\n@@ -90,6 +91,7 @@ class SavingTest(testing.TestCase):\n         out = editor.compare(target_model)  # Succeeds\n         self.assertEqual(out[\"status\"], \"success\")\n \n+    @pytest.mark.requires_trainable_backend\n     def test_scalar_weight(self):\n         model = keras.Sequential(name=\"my_sequential\")\n         model.add(keras.Input(shape=(1,), name=\"my_input\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b3e0b46323618cc1cf56715a9b9cf4baf2918a4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 43 | Churn Cumulative: 1059 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,7 +3,9 @@\n import datetime\n import io\n import json\n+import os\n import pathlib\n+import shutil\n import tempfile\n import warnings\n import zipfile\n@@ -513,14 +515,21 @@ def save_weights_only(model, filepath, objects_to_skip=None):\n \n     Note: only supports h5 for now.\n     \"\"\"\n-    # TODO: if h5 filepath is remote, create the file in a temporary directory\n-    # then upload it\n     filepath = str(filepath)\n+    tmp_dir = None\n+    remote_filepath = None\n     if not filepath.endswith(\".weights.h5\"):\n         raise ValueError(\n             \"Invalid `filepath` argument: expected a `.weights.h5` extension. \"\n             f\"Received: filepath={filepath}\"\n         )\n+    try:\n+        if file_utils.is_remote_path(filepath):\n+            tmp_dir = get_temp_dir()\n+            local_filepath = os.path.join(tmp_dir, os.path.basename(filepath))\n+            remote_filepath = filepath\n+            filepath = local_filepath\n+\n         weights_store = H5IOStore(filepath, mode=\"w\")\n         if objects_to_skip is not None:\n             visited_saveables = set(id(o) for o in objects_to_skip)\n@@ -534,6 +543,10 @@ def save_weights_only(model, filepath, objects_to_skip=None):\n             visited_saveables=visited_saveables,\n         )\n         weights_store.close()\n+    finally:\n+        if tmp_dir is not None:\n+            file_utils.copy(filepath, remote_filepath)\n+            shutil.rmtree(tmp_dir)\n \n \n def load_weights_only(\n@@ -544,9 +557,17 @@ def load_weights_only(\n     Note: only supports h5 for now.\n     \"\"\"\n     archive = None\n+    tmp_dir = None\n     filepath = str(filepath)\n+\n+    try:\n+        if file_utils.is_remote_path(filepath):\n+            tmp_dir = get_temp_dir()\n+            local_filepath = os.path.join(tmp_dir, os.path.basename(filepath))\n+            file_utils.copy(filepath, local_filepath)\n+            filepath = local_filepath\n+\n         if filepath.endswith(\".weights.h5\"):\n-        # TODO: download file if h5 filepath is remote\n             weights_store = H5IOStore(filepath, mode=\"r\")\n         elif filepath.endswith(\".keras\"):\n             archive = zipfile.ZipFile(filepath, \"r\")\n@@ -574,6 +595,9 @@ def load_weights_only(\n \n         if failed_saveables:\n             _raise_loading_failure(error_msgs, warn_only=skip_mismatch)\n+    finally:\n+        if tmp_dir is not None:\n+            shutil.rmtree(tmp_dir)\n \n \n def _raise_loading_failure(error_msgs, warn_only=False):\n\n@@ -1049,3 +1049,16 @@ class SavingBattleTest(testing.TestCase):\n         ref_out = model(x)\n         out = new_model(x)\n         self.assertAllClose(ref_out, out)\n+\n+    def test_remove_weights_only_saving_and_loading(self):\n+        def is_remote_path(path):\n+            return True\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"model.weights.h5\")\n+\n+        with mock.patch(\n+            \"keras.src.utils.file_utils.is_remote_path\", is_remote_path\n+        ):\n+            model = _get_subclassed_model()\n+            model.save_weights(temp_filepath)\n+            model.load_weights(temp_filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#132223fa20cfc684574a12c24e1d1715f9329be6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 775 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -511,9 +511,9 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n \n \n def save_weights_only(model, filepath, objects_to_skip=None):\n-    \"\"\"Save only the weights of a model to a target filepath (.weights.h5).\n+    \"\"\"Save only the weights of a model to a target filepath.\n     \n-    Note: only supports h5 for now.\n+    Supports both `.weights.h5` and `.keras`.\n     \"\"\"\n     filepath = str(filepath)\n     tmp_dir = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96158dcd0f40601611e42c137cf01305d544efcd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 92 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -73,6 +73,10 @@ class PipelineTest(testing.TestCase):\n             output = output.numpy()\n         self.assertEqual(tuple(output.shape), output_shape)\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"torch\",\n+        reason=\"Fails on CI, passes locally. TODO: debug\",\n+    )\n     def test_from_config(self):\n         pipeline = layers.Pipeline(\n             [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8029c78e8e160f67573250c29e7a75c273f13fd3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 44 | Churn Cumulative: 1400 | Contributors (this commit): 12 | Commits (past 90d): 33 | Contributors (cumulative): 53 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -0,0 +1,29 @@\n+import concurrent\n+\n+import numpy as np\n+\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src import testing\n+\n+\n+class TestThreadSafe(testing.TestCase):\n+    def test_is_thread_safe(self):\n+        if backend.IS_THREAD_SAFE:\n+            executor = concurrent.futures.ThreadPoolExecutor()\n+\n+            def sum(x, axis):\n+                return ops.sum(x, axis=axis)\n+\n+            futures = []\n+\n+            for i in range(10000):\n+                futures.clear()\n+                x = ops.convert_to_tensor(np.random.rand(100, 100))\n+                futures.append(executor.submit(sum, x, 1))\n+                x = ops.convert_to_tensor(np.random.rand(100))\n+                futures.append(executor.submit(sum, x, 0))\n+                concurrent.futures.wait(\n+                    futures, return_when=concurrent.futures.ALL_COMPLETED\n+                )\n+                [future.result() for future in futures]\n\n@@ -7,6 +7,7 @@ from keras.src.backend.jax import math\n from keras.src.backend.jax import nn\n from keras.src.backend.jax import numpy\n from keras.src.backend.jax import random\n+from keras.src.backend.jax.core import IS_THREAD_SAFE\n from keras.src.backend.jax.core import SUPPORTS_SPARSE_TENSORS\n from keras.src.backend.jax.core import Variable\n from keras.src.backend.jax.core import cast\n\n@@ -14,6 +14,7 @@ from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.jax import distribution_lib\n \n SUPPORTS_SPARSE_TENSORS = True\n+IS_THREAD_SAFE = True\n \n \n class Variable(KerasVariable):\n\n@@ -6,6 +6,7 @@ from keras.src.backend.numpy import math\n from keras.src.backend.numpy import nn\n from keras.src.backend.numpy import numpy\n from keras.src.backend.numpy import random\n+from keras.src.backend.numpy.core import IS_THREAD_SAFE\n from keras.src.backend.numpy.core import SUPPORTS_SPARSE_TENSORS\n from keras.src.backend.numpy.core import Variable\n from keras.src.backend.numpy.core import cast\n\n@@ -15,6 +15,7 @@ from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.symbolic_scope import SymbolicScope\n \n SUPPORTS_SPARSE_TENSORS = False\n+IS_THREAD_SAFE = True\n \n \n class Variable(KerasVariable):\n\n@@ -7,6 +7,7 @@ from keras.src.backend.tensorflow import nn\n from keras.src.backend.tensorflow import numpy\n from keras.src.backend.tensorflow import random\n from keras.src.backend.tensorflow import tensorboard\n+from keras.src.backend.tensorflow.core import IS_THREAD_SAFE\n from keras.src.backend.tensorflow.core import SUPPORTS_SPARSE_TENSORS\n from keras.src.backend.tensorflow.core import Variable\n from keras.src.backend.tensorflow.core import cast\n\n@@ -18,6 +18,8 @@ from keras.src.backend.tensorflow.sparse import sparse_to_dense\n from keras.src.utils.naming import auto_name\n \n SUPPORTS_SPARSE_TENSORS = True\n+# https://github.com/tensorflow/tensorflow/issues/78338\n+IS_THREAD_SAFE = False\n \n \n class Variable(\n\n@@ -22,6 +22,7 @@ from keras.src.backend.torch import math\n from keras.src.backend.torch import nn\n from keras.src.backend.torch import numpy\n from keras.src.backend.torch import random\n+from keras.src.backend.torch.core import IS_THREAD_SAFE\n from keras.src.backend.torch.core import SUPPORTS_SPARSE_TENSORS\n from keras.src.backend.torch.core import Variable\n from keras.src.backend.torch.core import cast\n\n@@ -20,6 +20,7 @@ from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.config import floatx\n \n SUPPORTS_SPARSE_TENSORS = False\n+IS_THREAD_SAFE = True\n \n # Some operators such as 'aten::_foreach_mul_.Scalar'\n # are not currently implemented for the MPS device.\n\n@@ -1,5 +1,6 @@\n import concurrent.futures\n \n+from keras.src import backend\n from keras.src import tree\n from keras.src import utils\n from keras.src.api_export import keras_export\n@@ -39,6 +40,9 @@ class CallbackList(Callback):\n         \"\"\"\n         self.callbacks = tree.flatten(callbacks) if callbacks else []\n         self._executor = None\n+        self._async_train = False\n+        self._async_test = False\n+        self._async_predict = False\n         self._futures = []\n         self._configure_async_dispatch(callbacks)\n         self._add_default_callbacks(add_history, add_progbar)\n@@ -53,6 +57,8 @@ class CallbackList(Callback):\n \n     def _configure_async_dispatch(self, callbacks):\n         # Determine whether callbacks can be dispatched asynchronously.\n+        if not backend.IS_THREAD_SAFE:\n+            return\n         async_train = True\n         async_test = True\n         async_predict = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2cddf9e44ae33631358e8bbbb524a0827b152ac9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 457 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -328,7 +328,7 @@ class TestTrainer(testing.TestCase):\n         self.assertAllClose(\n             history[\"mean_squared_error\"],\n             [14.5, 11.5, 8.5],\n-            atol=0.6,  # TODO: abnormal results for certain configs.\n+            atol=1.0,  # TODO: results vary across backends\n         )\n \n     @parameterized.named_parameters(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ff65a53d8fff2c6a9fca51552f2741423554c31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 666 | Lines Deleted: 184 | Files Changed: 5 | Hunks: 39 | Methods Changed: 29 | Complexity Δ (Sum/Max): 35/16 | Churn Δ: 850 | Churn Cumulative: 2048 | Contributors (this commit): 13 | Commits (past 90d): 22 | Contributors (cumulative): 26 | DMM Complexity: 0.9331742243436754\n\nDIFF:\n@@ -2,6 +2,7 @@ import warnings\n \n from keras.src import backend\n from keras.src import ops\n+from keras.src import tree\n from keras.src.api_export import keras_export\n from keras.src.losses.loss import Loss\n from keras.src.losses.loss import squeeze_or_expand_to_same_rank\n@@ -23,7 +24,11 @@ class LossFunctionWrapper(Loss):\n         self._fn_kwargs = kwargs\n \n     def call(self, y_true, y_pred):\n-        y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n+        y_true_y_pred = tree.map_structure(\n+            squeeze_or_expand_to_same_rank, y_true, y_pred\n+        )\n+        y_true = tree.map_structure_up_to(y_true, lambda x: x[0], y_true_y_pred)\n+        y_pred = tree.map_structure_up_to(y_pred, lambda x: x[1], y_true_y_pred)\n         return self.fn(y_true, y_pred, **self._fn_kwargs)\n \n     def get_config(self):\n\n@@ -1,4 +1,5 @@\n import pickle\n+from collections import namedtuple\n \n import numpy as np\n import pytest\n@@ -6,7 +7,9 @@ from absl.testing import parameterized\n \n from keras.src import backend\n from keras.src import layers\n+from keras.src import losses\n from keras.src import testing\n+from keras.src import tree\n from keras.src.layers.core.input_layer import Input\n from keras.src.models.functional import Functional\n from keras.src.models.model import Model\n@@ -68,6 +71,48 @@ def _get_model_multi_outputs_dict():\n     return model\n \n \n+def _get_model_multi_outputs_struct_list_like(_type):\n+    x = Input(shape=(3,), name=\"x\")\n+    y1 = layers.Dense(1, name=\"y1\", activation=\"sigmoid\")(x)\n+    y2 = layers.Dense(1, name=\"y2\", activation=\"sigmoid\")(x)\n+    model = Model(x, _type([y1, y2]))\n+    return model\n+\n+\n+def _get_model_multi_outputs_struct_namedtuple():\n+    Y = namedtuple(\"Y\", [\"y1\", \"y2\"])\n+    x = Input(shape=(3,), name=\"x\")\n+    y1 = layers.Dense(1, name=\"y1\", activation=\"sigmoid\")(x)\n+    y2 = layers.Dense(1, name=\"y2\", activation=\"sigmoid\")(x)\n+    model = Model(x, Y(y1, y2))\n+    return model, Y\n+\n+\n+def _get_model_multi_outputs_struct_dict():\n+    x = Input(shape=(3,), name=\"x\")\n+    y1 = layers.Dense(1, name=\"y1\", activation=\"sigmoid\")(x)\n+    y2 = layers.Dense(1, name=\"y2\", activation=\"sigmoid\")(x)\n+    model = Model(x, {\"a\": y1, \"b\": y2})\n+    return model\n+\n+\n+def _get_model_multi_outputs_struct():\n+    x = Input(shape=(3,), name=\"x\")\n+    y1 = layers.Dense(1, name=\"y1\", activation=\"sigmoid\")(x)\n+    y2 = layers.Dense(1, name=\"y2\", activation=\"sigmoid\")(x)\n+    y3 = layers.Dense(1, name=\"y3\", activation=\"sigmoid\")(x)\n+    model = Model(\n+        x,\n+        {\n+            \"a\": (y1, y2),\n+            \"b\": {\"b1\": y1, \"b2\": y2},\n+            \"c\": {\"c1\": (y1, y2), \"c2\": y2},\n+            \"d\": y3,\n+        },\n+    )\n+    return model\n+\n+\n def _get_model_multi_outputs_dict_with_single_tensor():\n     x = Input(shape=(3,), name=\"input_a\")\n     output = layers.Dense(1, name=\"output_a\")(x)\n@@ -121,6 +166,7 @@ def _get_variable_value_by_path(variables, path):\n \n @pytest.mark.requires_trainable_backend\n class ModelTest(testing.TestCase):\n+\n     def test_functional_rerouting(self):\n         model = _get_model()\n         self.assertIsInstance(model, Functional)\n@@ -620,8 +666,7 @@ class ModelTest(testing.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n             KeyError,\n-            \"in the `loss` argument, but they can't be found in the \"\n-            \"model's output\",\n+            \"in the `loss` argument, can't be found in the model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n@@ -638,8 +683,7 @@ class ModelTest(testing.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n             KeyError,\n-            \"in the `loss` argument, but they can't be found in the \"\n-            \"model's output\",\n+            \"in the `loss` argument, can't be found in the model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n@@ -683,8 +727,7 @@ class ModelTest(testing.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         with self.assertRaisesRegex(\n             KeyError,\n-            \"in the `loss` argument, but they can't be found in the \"\n-            \"model's output\",\n+            \"in the `loss` argument, can't be found in the model's output\",\n         ):\n             model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n \n@@ -725,13 +768,10 @@ class ModelTest(testing.TestCase):\n                 [\"mean_squared_error\", \"binary_crossentropy\"],\n             ],\n         )\n-        # Fit the model to make sure compile_metrics are built\n-        with self.assertRaisesRegex(\n-            ValueError,\n-            \"when providing the `loss` argument as a list, \"\n-            \"it should have as many entries as the model has outputs\",\n-        ):\n-            model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+        hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+        hist_keys = sorted(hist.history.keys())\n+        ref_keys = sorted([\"loss\", \"output_a_loss\", \"output_b_loss\"])\n+        self.assertListEqual(hist_keys, ref_keys)\n \n     @parameterized.named_parameters(\n         (\"int8\", \"int8\"),\n@@ -944,3 +984,237 @@ class ModelTest(testing.TestCase):\n             AttributeError, \"`Model.layers` attribute is reserved\"\n         ):\n             model.layers = [layers.Dense(4)]\n+\n+    def get_struct_loss(self, structure):\n+        def loss_fn(y_true, y_pred):\n+            tree.assert_same_structure(structure, y_true, check_types=False)\n+            tree.assert_same_structure(structure, y_pred, check_types=False)\n+            tree.map_structure(\n+                lambda spec, tensor: self.assertEqual(spec.ndim, tensor.ndim),\n+                structure,\n+                y_true,\n+            )\n+            tree.map_structure(\n+                lambda spec, tensor: self.assertEqual(spec.ndim, tensor.ndim),\n+                structure,\n+                y_pred,\n+            )\n+            flat_y_pred, flat_y_true = tree.flatten(y_pred), tree.flatten(\n+                y_true\n+            )\n+            diff = 0\n+            for y_p, y_t in zip(flat_y_pred, flat_y_true):\n+                diff += losses.mean_absolute_error(y_t, y_p)\n+            return diff\n+\n+        return loss_fn\n+\n+    @parameterized.product(\n+        _type=[tuple, list], other_type=[list, tuple], weighted=[False, True]\n+    )\n+    def test_functional_struct_outputs_struct_losses(\n+        self, _type, other_type, weighted\n+    ):\n+        model = _get_model_multi_outputs_struct_list_like(_type)\n+        self.assertIsInstance(model, Functional)\n+        x = np.random.rand(8, 3)\n+        y1 = np.random.rand(8, 1)\n+        y2 = np.random.rand(8, 1)\n+        y = _type([y1, y2])\n+        loss = other_type(\n+            [\n+                self.get_struct_loss(model.output),\n+                _type(\n+                    [\n+                        self.get_struct_loss(model.output[0]),\n+                        self.get_struct_loss(model.output[1]),\n+                    ]\n+                ),\n+            ]\n+        )\n+        if weighted:\n+            loss_weights = tree.map_structure(lambda _: np.random.rand(), loss)\n+        else:\n+            loss_weights = None\n+\n+        model.compile(\n+            optimizer=\"sgd\",\n+            loss=loss,\n+            loss_weights=loss_weights,\n+        )\n+\n+        if _type is other_type:\n+            with self.assertRaisesRegex(\n+                ValueError, \"don't have the same structure\"\n+            ):\n+                model.fit(x, y, batch_size=2, epochs=1, verbose=0)\n+        else:\n+            # Check dict outputs.\n+            outputs = model.predict(x)\n+            self.assertIsInstance(outputs, _type)\n+            # Fit the model to make sure compile_metrics are built\n+            hist = model.fit(\n+                x,\n+                y,\n+                batch_size=2,\n+                epochs=1,\n+                verbose=0,\n+            )\n+            hist_keys = sorted(hist.history.keys())\n+            ref_keys = sorted(\n+                [\n+                    \"loss\",\n+                    \"y1_loss\",\n+                    \"y2_loss\",\n+                    \"y1_y2_loss\",\n+                ]\n+            )\n+            self.assertListEqual(hist_keys, ref_keys)\n+\n+    @parameterized.named_parameters((\"weighted\", True), (\"not_weighted\", False))\n+    def test_functional_struct_outputs_dict_struct_losses(self, weighted):\n+        model = _get_model_multi_outputs_struct_dict()\n+        self.assertIsInstance(model, Functional)\n+        x = np.random.rand(8, 3)\n+        y1 = np.random.rand(8, 1)\n+        y2 = np.random.rand(8, 1)\n+\n+        y = {\"a\": y1, \"b\": y2}\n+        loss = [\n+            self.get_struct_loss(model.output),\n+            {\n+                \"a\": self.get_struct_loss(model.output[\"a\"]),\n+                \"b\": self.get_struct_loss(model.output[\"a\"]),\n+            },\n+        ]\n+        if weighted:\n+            loss_weights = tree.map_structure(lambda _: np.random.rand(), loss)\n+        else:\n+            loss_weights = None\n+\n+        model.compile(\n+            optimizer=\"sgd\",\n+            loss=loss,\n+            loss_weights=loss_weights,\n+        )\n+        # Check dict outputs.\n+        outputs = model.predict(x)\n+        self.assertIsInstance(outputs, dict)\n+\n+        # Fit the model to make sure compile_metrics are built\n+        hist = model.fit(\n+            x,\n+            y,\n+            batch_size=2,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        hist_keys = sorted(hist.history.keys())\n+        ref_keys = sorted(\n+            [\n+                \"loss\",\n+                \"a_loss\",\n+                \"b_loss\",\n+                \"a_b_loss\",\n+            ]\n+        )\n+        self.assertListEqual(hist_keys, ref_keys)\n+\n+    def test_functional_struct_outputs_namedtuple_struct_losses(self):\n+        model, Y = _get_model_multi_outputs_struct_namedtuple()\n+        self.assertIsInstance(model, Functional)\n+        x = np.random.rand(8, 3)\n+        y1 = np.random.rand(8, 1)\n+        y2 = np.random.rand(8, 1)\n+\n+        y = Y(y1, y2)\n+        model.compile(\n+            optimizer=\"sgd\",\n+            loss=[\n+                self.get_struct_loss(model.output),\n+                Y(\n+                    self.get_struct_loss(model.output.y1),\n+                    self.get_struct_loss(model.output.y2),\n+                ),\n+            ],\n+        )\n+        # Check dict outputs.\n+        outputs = model.predict(x)\n+        self.assertIsInstance(outputs, tuple)\n+\n+        # Fit the model to make sure compile_metrics are built\n+        hist = model.fit(\n+            x,\n+            y,\n+            batch_size=2,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        hist_keys = sorted(hist.history.keys())\n+        ref_keys = sorted(\n+            [\n+                \"loss\",\n+                \"y1_loss\",\n+                \"y2_loss\",\n+                \"y1_y2_loss\",\n+            ]\n+        )\n+        self.assertListEqual(hist_keys, ref_keys)\n+\n+    def test_functional_deeply_nested_outputs_struct_losses(self):\n+        model = _get_model_multi_outputs_struct()\n+        self.assertIsInstance(model, Functional)\n+        x = np.random.rand(8, 3)\n+        y1 = np.random.rand(8, 1)\n+        y2 = np.random.rand(8, 1)\n+        y3 = np.random.rand(8, 1)\n+        y = {\n+            \"a\": (y1, y2),\n+            \"b\": {\"b1\": y1, \"b2\": y2},\n+            \"c\": {\"c1\": (y1, y2), \"c2\": y2},\n+            \"d\": y3,\n+        }\n+        model.compile(\n+            optimizer=\"sgd\",\n+            loss={\n+                \"a\": [\n+                    self.get_struct_loss(model.output[\"a\"]),\n+                    (None, self.get_struct_loss(model.output[\"a\"][1])),\n+                ],\n+                \"b\": [\n+                    self.get_struct_loss(model.output[\"b\"]),\n+                    {\"b1\": self.get_struct_loss(model.output[\"b\"][\"b1\"])},\n+                ],\n+                \"c\": [\n+                    self.get_struct_loss(model.output[\"c\"]),\n+                    {\"c1\": self.get_struct_loss(model.output[\"c\"][\"c1\"])},\n+                ],\n+                \"d\": self.get_struct_loss(model.output[\"d\"]),\n+            },\n+        )\n+        # Check dict outputs.\n+        outputs = model.predict(x)\n+        self.assertIsInstance(outputs, dict)\n+\n+        # Fit the model to make sure compile_metrics are built\n+        hist = model.fit(\n+            x,\n+            y,\n+            batch_size=2,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        hist_keys = sorted(hist.history.keys())\n+        ref_keys = sorted(\n+            [\n+                \"a/y2_loss\",\n+                \"a_loss\",\n+                \"b/b1_loss\",\n+                \"b_loss\",\n+                \"c/c1_loss\",\n+                \"c_loss\",\n+                \"d_loss\",\n+                \"loss\",\n+            ]\n+        )\n+        self.assertListEqual(hist_keys, ref_keys)\n\n@@ -1,7 +1,10 @@\n+from collections import namedtuple\n+\n from keras.src import losses as losses_module\n from keras.src import metrics as metrics_module\n from keras.src import ops\n from keras.src import tree\n+from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.utils.naming import get_object_name\n from keras.src.utils.tracking import Tracker\n \n@@ -406,6 +409,8 @@ class CompileMetrics(metrics_module.Metric):\n \n \n class CompileLoss(losses_module.Loss):\n+    Loss = namedtuple(\"Loss\", [\"path\", \"loss\", \"loss_weights\", \"name\"])\n+\n     def __init__(\n         self,\n         loss,\n@@ -423,15 +428,13 @@ class CompileLoss(losses_module.Loss):\n                 f\"Received instead: loss_weights={loss_weights} \"\n                 f\"of type {type(loss_weights)}\"\n             )\n+\n         self._user_loss = loss\n         self._user_loss_weights = loss_weights\n         self.built = False\n         self.output_names = output_names\n         super().__init__(name=\"compile_loss\", reduction=reduction)\n \n-        # Inferred by `y_pred` and `output_names`\n-        self.inferred_output_names = None\n-\n         # Use `Tracker` to track metrics for individual losses.\n         self._metrics = []\n         self._tracker = Tracker(\n@@ -442,6 +445,8 @@ class CompileLoss(losses_module.Loss):\n                 )\n             }\n         )\n+        self._flat_losses = None\n+        self._y_pred_build_structure = None\n \n     @property\n     def metrics(self):\n@@ -454,209 +459,252 @@ class CompileLoss(losses_module.Loss):\n             vars.extend(m.variables)\n         return vars\n \n+    def _build_nested(self, y_true, y_pred, loss, output_names, current_path):\n+        flat_y_pred = tree.flatten(y_pred)\n+        if not tree.is_nested(loss):\n+            _loss = loss.loss\n+            if _loss is None:\n+                return\n+            loss_weight = loss.weight\n+            resolved_loss = get_loss(_loss, y_true, y_pred)\n+            name_path = current_path\n+            if not tree.is_nested(output_names):\n+                if output_names is not None:\n+                    output_name = output_names\n+                else:\n+                    output_name = resolved_loss.name\n+                if len(name_path) == 0:\n+                    name_path = (output_name,)\n+                elif isinstance(name_path[-1], int):\n+                    name_path = name_path[:-1] + (output_name,)\n+            name = \"/\".join([str(path) for path in name_path])\n+            if name == \"\":\n+                if isinstance(output_names, dict):\n+                    flat_output_names = list(output_names.keys())\n+                else:\n+                    flat_output_names = tree.flatten(output_names)\n+                name = \"_\".join(flat_output_names)\n+            self._flat_losses.append(\n+                CompileLoss.Loss(current_path, resolved_loss, loss_weight, name)\n+            )\n+            return\n+        elif (\n+            issubclass(type(loss), (list, tuple))\n+            and all([not tree.is_nested(_loss) for _loss in loss])\n+            and len(loss) == len(flat_y_pred)\n+        ):\n+            loss = tree.pack_sequence_as(y_pred, loss)\n+        elif issubclass(type(loss), (list, tuple)) and not isinstance(\n+            y_pred, type(loss)\n+        ):\n+            for _loss in loss:\n+                self._build_nested(\n+                    y_true,\n+                    y_pred,\n+                    _loss,\n+                    output_names,\n+                    current_path,\n+                )\n+            return\n+\n+        if not tree.is_nested(loss):\n+            return self._build_nested(\n+                y_true, y_pred, loss, output_names, current_path\n+            )\n+\n+        # At this point loss should match the structure of y_pred and y_true\n+        # We assume y_pred and y_true have the same structure and this\n+        # have been already asserted.\n+\n+        if not isinstance(loss, type(y_pred)):\n+            raise KeyError(\n+                f\"The path: {current_path} in \"\n+                \"the `loss` argument, can't be found in \"\n+                \"the model's output (`y_pred`).\"\n+            )\n+\n+        # shallow traverse the loss config\n+        if isinstance(loss, dict):\n+            iterator = loss.items()\n+\n+            def key_check_fn(key, objs):\n+                return all([key in obj for obj in objs])\n+\n+        elif issubclass(type(loss), (list, tuple)):\n+            iterator = enumerate(loss)\n+\n+            def key_check_fn(key, objs):\n+                return all([key < len(obj) for obj in objs])\n+\n+        else:\n+            raise TypeError(\n+                f\"Unsupported type {type(loss)} \"\n+                f\"in the `loss` configuration.\"\n+            )\n+\n+        for key, _loss in iterator:\n+            if not key_check_fn(key, (y_true, y_pred)):\n+                raise KeyError(\n+                    f\"The path: {current_path + (key,)} in \"\n+                    \"the `loss` argument, can't be found in \"\n+                    \"the model's output (`y_pred`).\"\n+                )\n+\n+            self._build_nested(\n+                y_true[key],\n+                y_pred[key],\n+                _loss,\n+                output_names[key],\n+                current_path + (key,),\n+            )\n+\n     def build(self, y_true, y_pred):\n         loss = self._user_loss\n         loss_weights = self._user_loss_weights\n-        output_names = self._get_y_pred_output_names(y_pred)\n-        inferred_output_names = output_names or self.output_names\n+        flat_output_names = self.output_names\n \n-        if is_function_like(loss) and tree.is_nested(y_pred):\n-            # The model has multiple outputs but only one loss fn\n-            # was provided. Broadcast loss to all outputs.\n+        # Pytree leaf container\n+        class WeightedLoss:\n+            def __init__(self, loss, weight):\n+                self.loss = loss\n+                self.weight = weight\n+\n+        # pack the losses and the weights together\n+        if loss_weights is not None:\n+            try:\n+                tree.assert_same_structure(\n+                    loss, loss_weights, check_types=False\n+                )\n+            except ValueError:\n+                flat_loss_weights = tree.flatten(loss_weights)\n+                if len(tree.flatten(loss)) != len(flat_loss_weights):\n+                    raise ValueError(\n+                        f\"`loss_weights` must match the number of losses, \"\n+                        f\"got {len(tree.flatten(loss))} losses \"\n+                        f\"and {len(loss_weights)} weigths.\"\n+                    )\n+                loss_weights = tree.pack_sequence_as(loss, flat_loss_weights)\n+            loss = tree.map_structure(\n+                lambda _loss, _weight: WeightedLoss(_loss, _weight),\n+                loss,\n+                loss_weights,\n+            )\n+        else:\n+            loss = tree.map_structure(\n+                lambda _loss: WeightedLoss(_loss, None), loss\n+            )\n+\n+        self._flat_losses = []\n+\n+        if (\n+            isinstance(loss, dict)\n+            and issubclass(type(y_pred), (list, tuple))\n+            and set(loss.keys()) == set(flat_output_names)\n+            and len(y_pred) == len(flat_output_names)\n+        ):\n+            y_pred = {name: y_p for name, y_p in zip(flat_output_names, y_pred)}\n+            y_true = {name: y_t for name, y_t in zip(flat_output_names, y_true)}\n+        elif (\n+            isinstance(loss, dict)\n+            and not tree.is_nested(y_pred)\n+            and set(loss.keys()) == set(flat_output_names)\n+            and len(flat_output_names) == 1\n+        ):\n+            y_pred = {\n+                name: y_p for name, y_p in zip(flat_output_names, [y_pred])\n+            }\n+            y_true = {\n+                name: y_t for name, y_t in zip(flat_output_names, [y_true])\n+            }\n+\n+        try:\n+            output_names = tree.pack_sequence_as(y_pred, flat_output_names)\n+        except:\n+            inferred_flat_output_names = self._get_y_pred_output_names(y_pred)\n+            output_names = tree.pack_sequence_as(\n+                y_pred, inferred_flat_output_names\n+            )\n+\n+        if not tree.is_nested(loss):\n             loss = tree.map_structure(lambda x: loss, y_pred)\n \n-        # Check and filter the keys.\n-        if isinstance(loss, dict):\n-            if inferred_output_names is None:\n-                raise ValueError(\n-                    \"Argument `loss` can only be provided as a dict \"\n-                    \"when the model also returns a dict of outputs. \"\n-                    f\"Received loss={loss}\"\n-                )\n-        filtered_y_pred_keys = []\n-        filtered_y_true_keys = []\n-        if isinstance(loss, dict):\n-            loss_keys = set(loss.keys())\n-            if inferred_output_names is not None:\n-                y_pred_keys = set(inferred_output_names)\n-                if len(loss_keys - y_pred_keys) > 0:\n-                    raise KeyError(\n-                        f\"There are keys: {list(loss_keys - y_pred_keys)} in \"\n-                        \"the `loss` argument, but they can't be found in \"\n-                        \"the model's output (`y_pred`).\"\n-                    )\n-                filtered_y_pred_keys.extend(list(y_pred_keys - loss_keys))\n-            if isinstance(y_true, dict):\n-                y_true_keys = set(y_true.keys())\n-                if len(loss_keys - y_true_keys) > 0:\n-                    raise KeyError(\n-                        f\"There are keys: {list(loss_keys - y_true_keys)} in \"\n-                        \"the `loss` argument, but they can't be found in \"\n-                        \"`y` (`y_true`).\"\n-                    )\n-                filtered_y_true_keys.extend(list(y_true_keys - loss_keys))\n-        filtered_y_pred_keys = set(filtered_y_pred_keys)\n-        filtered_y_true_keys = set(filtered_y_true_keys)\n-\n-        # Filter unused inputs.\n-        y_true, y_pred = self._filter_unused_inputs(\n-            y_true,\n-            y_pred,\n-            filtered_y_true_keys,\n-            filtered_y_pred_keys,\n-            self.inferred_output_names,\n-        )\n-\n-        # `loss` could be a plain function (or a `Loss` instance), a list, a\n-        # nested list, or a dict. However, in `call`, we want to iterate over\n-        # all losses, so we flatten them into a list regardless of their\n-        # original structure.\n-        flat_losses = tree.flatten(loss)\n-        if loss_weights is None:\n-            flat_loss_weights = [None] * len(flat_losses)\n-        else:\n-            flat_loss_weights = tree.flatten(loss_weights)\n-            for loss_weight in flat_loss_weights:\n-                if not isinstance(loss_weight, (int, float, type(None))):\n-                    raise TypeError(\n-                        \"When providing the `loss_weights` argument, each \"\n-                        \"element should be a Python int, float (the weighting \"\n-                        \"coefficient corresponding to the loss for that \"\n-                        \"output) or `None`.\"\n-                        f\"Received: loss_weights={loss_weights}\"\n-                    )\n-            if len(flat_loss_weights) != len(flat_losses):\n-                raise ValueError(\n-                    \"When providing the `loss_weights` argument, it should \"\n-                    \"have equal length of `loss` argument. \"\n-                    f\"Received: loss_weights length={len(flat_loss_weights)}, \"\n-                    f\"loss length={len(flat_losses)}\"\n-                )\n-\n-        y_true = tree.flatten(y_true)\n-        y_pred = tree.flatten(y_pred)\n-        if len(y_pred) != len(flat_losses):\n-            raise ValueError(\n-                \"For a model with multiple outputs, \"\n-                \"when providing the `loss` argument as a list, \"\n-                \"it should have as many entries as the model has outputs. \"\n-                f\"Received:\\nloss={loss}\\nof length {len(flat_losses)} \"\n-                f\"whereas the model has {len(y_pred)} outputs.\"\n-            )\n-\n-        # Get the real loss instances.\n-        flat_losses = [\n-            get_loss(identifier, _y_true, _y_pred)\n-            for identifier, _y_true, _y_pred in zip(flat_losses, y_true, y_pred)\n-        ]\n+        self._build_nested(y_true, y_pred, loss, output_names, ())\n \n         # Add `Mean` metric to the tracker for each loss.\n-        if len(flat_losses) > 1:\n-            for i, _loss in enumerate(flat_losses):\n-                if _loss is not None:\n-                    if inferred_output_names is not None and len(\n-                        inferred_output_names\n-                    ) == len(flat_losses):\n-                        name = inferred_output_names[i]\n-                    else:\n-                        name = _loss.name\n-                    name += \"_loss\"\n+        if len(self._flat_losses) > 1:\n+            for _loss in self._flat_losses:\n+                name = _loss.name + \"_loss\"\n                 self._tracker.add_to_store(\n                     \"metrics\", metrics_module.Mean(name=name)\n                 )\n \n-        self.flat_losses = flat_losses\n-        self.flat_loss_weights = flat_loss_weights\n-        self.filtered_y_true_keys = filtered_y_true_keys\n-        self.filtered_y_pred_keys = filtered_y_pred_keys\n-        self.inferred_output_names = inferred_output_names\n+        self._y_pred_build_structure = tree.map_structure(\n+            lambda x: None, y_pred\n+        )\n         self.built = True\n \n     def _get_y_pred_output_names(self, y_pred):\n-        if isinstance(y_pred, dict):\n-            output_names = sorted(y_pred.keys())\n+        flat_y_pred = tree.flatten(y_pred)\n+        if all((isinstance(x, KerasTensor) for x in flat_y_pred)):\n+            output_names = []\n+            for tensor in flat_y_pred:\n+                if hasattr(tensor, \"_keras_history\"):\n+                    output_names.append(tensor._keras_history.operation.name)\n                 else:\n-            y_pred = tree.flatten(y_pred)\n-            if all(hasattr(x, \"_keras_history\") for x in y_pred):\n-                output_names = [x._keras_history.operation.name for x in y_pred]\n+                    output_names.append(tensor.name)\n         else:\n-                output_names = None\n+            output_names = [None] * len(flat_y_pred)\n         return output_names\n \n-    def _filter_unused_inputs(\n-        self,\n-        y_true,\n-        y_pred,\n-        filtered_y_true_keys,\n-        filtered_y_pred_keys,\n-        output_names,\n-    ):\n-        if len(filtered_y_true_keys) > 0 and isinstance(y_true, dict):\n-            # Modifying data in-place can cause errors in TF's graph.\n-            filtered_y_true = {}\n-            for k, v in y_true.items():\n-                if k not in filtered_y_true_keys:\n-                    filtered_y_true[k] = v\n-            y_true = filtered_y_true\n-        if len(filtered_y_pred_keys) > 0:\n-            if isinstance(y_pred, dict):\n-                # Modifying data in-place can cause errors in TF's graph.\n-                filtered_y_pred = {}\n-                for k, v in y_pred.items():\n-                    if k not in filtered_y_pred_keys:\n-                        filtered_y_pred[k] = v\n-                y_pred = filtered_y_pred\n-            elif output_names is not None:\n-                y_pred = []\n-                for x, output_name in zip(tree.flatten(y_pred), output_names):\n-                    if output_name not in filtered_y_pred_keys:\n-                        y_pred.append(x)\n-        return y_true, y_pred\n-\n     def __call__(self, y_true, y_pred, sample_weight=None):\n         with ops.name_scope(self.name):\n             return self.call(y_true, y_pred, sample_weight)\n \n     def call(self, y_true, y_pred, sample_weight=None):\n+        try:\n+            tree.assert_same_structure(y_pred, y_true, check_types=False)\n+        except ValueError:\n+            # y_true is either flat or leaf\n+            if not tree.is_nested(y_true):\n+                y_true = [y_true]\n+            y_true = tree.pack_sequence_as(y_pred, y_true)\n+\n         if not self.built:\n             self.build(y_true, y_pred)\n \n-        # Filter unused inputs.\n-        y_true, y_pred = self._filter_unused_inputs(\n-            y_true,\n-            y_pred,\n-            self.filtered_y_true_keys,\n-            self.filtered_y_pred_keys,\n-            self.inferred_output_names,\n+        try:\n+            tree.assert_same_structure(\n+                self._y_pred_build_structure, y_pred, check_types=False\n+            )\n+        except ValueError:\n+            y_pred = tree.pack_sequence_as(\n+                self._y_pred_build_structure, tree.flatten(y_pred)\n+            )\n+            y_true = tree.pack_sequence_as(\n+                self._y_pred_build_structure, tree.flatten(y_true)\n             )\n-\n-        # Flatten the inputs.\n-        y_true = tree.flatten(y_true)\n-        y_pred = tree.flatten(y_pred)\n-        if sample_weight is not None:\n-            sample_weight = tree.flatten(sample_weight)\n-            # For multi-outputs, repeat sample weights for n outputs.\n-            if len(sample_weight) < len(y_true):\n-                sample_weight = [sample_weight[0] for _ in range(len(y_true))]\n-        else:\n-            sample_weight = [None for _ in y_true]\n \n         # We need to add a dummy `None` if the model has only a single output.\n         metrics = [None] if len(self.metrics) == 0 else self.metrics\n \n         # Iterate all losses in flat form.\n         loss_values = []\n-        for loss_fn, y_t, y_p, loss_weight, sample_weight, metric in zip(\n-            self.flat_losses,\n-            y_true,\n-            y_pred,\n-            self.flat_loss_weights,\n-            sample_weight,\n-            metrics,\n+\n+        def resolve_path(path, object):\n+            for _path in path:\n+                object = object[_path]\n+            return object\n+\n+        for (path, loss_fn, loss_weight, name), metric in zip(\n+            self._flat_losses, metrics\n         ):\n-            if loss_fn:\n+            y_t, y_p = resolve_path(path, y_true), resolve_path(path, y_pred)\n+            if sample_weight is not None and tree.is_nested(sample_weight):\n+                _sample_weight = resolve_path(path, sample_weight)\n+            else:\n+                _sample_weight = sample_weight\n             value = ops.cast(\n-                    loss_fn(y_t, y_p, sample_weight), dtype=self.dtype\n+                loss_fn(y_t, y_p, _sample_weight), dtype=self.dtype\n             )\n             if loss_weight is not None:\n                 value = ops.multiply(value, loss_weight)\n@@ -666,6 +714,7 @@ class CompileLoss(losses_module.Loss):\n                 metric.update_state(\n                     value, sample_weight=tree.flatten(y_p)[0].shape[0]\n                 )\n+\n         if loss_values:\n             total_loss = sum(loss_values)\n             return total_loss\n\n@@ -1,4 +1,7 @@\n+from collections import namedtuple\n+\n import numpy as np\n+import tree\n from absl.testing import parameterized\n \n from keras.src import backend\n@@ -347,3 +350,143 @@ class TestCompileLoss(testing.TestCase):\n         }\n         value = compile_loss(y_true, y_pred)\n         self.assertAllClose(value, 1.07666, atol=1e-5)\n+\n+    def test_struct_loss(self):\n+        y_true = {\n+            \"a\": {\n+                \"c\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+                \"d\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+            },\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": {\n+                \"c\": np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+                \"d\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+            },\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        loss = {\"a\": {\"c\": \"mse\", \"d\": \"mae\"}}\n+        compile_loss = CompileLoss(loss=loss, output_names=[\"c\", \"d\", \"b\"])\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_pred\n+        )\n+        compile_loss.build(y_true_symb, y_pred_symb)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 1.07666, atol=1e-5)\n+\n+    def test_struct_loss_invalid_weights(self):\n+        y_true = {\n+            \"a\": {\n+                \"c\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+                \"d\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+            },\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": {\n+                \"c\": np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+                \"d\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+            },\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        loss = {\"a\": {\"c\": \"mse\", \"d\": \"mae\"}}\n+        compile_loss = CompileLoss(\n+            loss=loss, output_names=[\"c\", \"d\", \"b\"], loss_weights=[1]\n+        )\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_pred\n+        )\n+        with self.assertRaisesRegex(\n+            ValueError, \"must match the number of losses\"\n+        ):\n+            compile_loss.build(y_true_symb, y_pred_symb)\n+\n+    def test_struct_loss_indice_path(self):\n+        y_true = {\n+            \"a\": (\n+                np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+                np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+            ),\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": (\n+                np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+                np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+            ),\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        loss = {\"a\": [\"mse\", \"mae\"]}\n+        compile_loss = CompileLoss(loss=loss, output_names=[\"c\", \"d\", \"b\"])\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_pred\n+        )\n+        compile_loss.build(y_true_symb, y_pred_symb)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 1.07666, atol=1e-5)\n+\n+    def test_struct_loss_namedtuple(self):\n+        Point = namedtuple(\"Point\", [\"x\", \"y\"])\n+        y_true = {\n+            \"a\": Point(\n+                np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+                np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+            ),\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": Point(\n+                np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+                np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+            ),\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        loss = {\"a\": Point(\"mse\", \"mae\")}\n+        compile_loss = CompileLoss(loss=loss, output_names=[\"c\", \"d\", \"b\"])\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_pred\n+        )\n+        compile_loss.build(y_true_symb, y_pred_symb)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 1.07666, atol=1e-5)\n+\n+    def test_struct_loss_invalid_path(self):\n+        y_true = {\n+            \"a\": {\n+                \"c\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+                \"d\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+            },\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": {\n+                \"c\": np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+                \"d\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+            },\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        loss = {\"a\": {\"c\": \"mse\"}, \"b\": {\"d\": \"mae\"}}\n+        compile_loss = CompileLoss(loss=loss, output_names=[\"c\", \"d\", \"b\"])\n+        y_true_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_true\n+        )\n+        y_pred_symb = tree.map_structure(\n+            lambda _: backend.KerasTensor((3, 4)), y_pred\n+        )\n+        with self.assertRaisesRegex(\n+            KeyError, \"can't be found in the model's output\"\n+        ):\n+            compile_loss.build(y_true_symb, y_pred_symb)\n\n@@ -23,6 +23,17 @@ if backend() == \"tensorflow\":\n         namespace=\"keras\",\n     )\n \n+    from tensorflow.python.trackable.data_structures import _DictWrapper\n+\n+    optree.register_pytree_node(\n+        _DictWrapper,\n+        lambda x: (list(x.values()), list(x.keys())),\n+        lambda metadata, children: _DictWrapper(\n+            {key: child for key, child in zip(metadata, children)}\n+        ),\n+        namespace=\"keras\",\n+    )\n+\n \n def is_nested(structure):\n     return not optree.tree_is_leaf(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
