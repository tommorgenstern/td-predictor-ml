{"custom_id": "keras#41c992105fdc0f0d00e7618752f36e0884f8e5c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 1430 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -193,6 +193,11 @@ class _BaseOptimizer(tf.Module):\n \n     return grads\n \n+  @property\n+  def use_ema(self):\n+    \"\"\"Returns whether the optimizer uses EMA of weights.\"\"\"\n+    return self._use_ema\n+\n   @property\n   def iterations(self):\n     \"\"\"The number of training steps this `optimizer` has run.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ec98823f6e07e1bf1991b091788bef90ee49c1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 678 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -160,3 +160,6 @@ class SeparableConv2DTest(keras_parameterized.TestCase):\n       self.assertEqual(layer.depthwise_kernel.constraint, d_constraint)\n       self.assertEqual(layer.pointwise_kernel.constraint, p_constraint)\n       self.assertEqual(layer.bias.constraint, b_constraint)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for reshaping layers.\"\"\"\n+\"\"\"Tests for cropping layers.\"\"\"\n \n import keras\n from keras import keras_parameterized\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for convolutional layers.\"\"\"\n+\"\"\"Tests for up-sampling layers.\"\"\"\n # pylint: disable=g-direct-tensorflow-import\n \n import keras\n\n@@ -33,6 +33,7 @@ from keras.layers import noise\n from keras.layers import pooling\n from keras.layers import recurrent\n from keras.layers import recurrent_v2\n+from keras.layers import reshaping\n from keras.layers import wrappers\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import layer_normalization\n@@ -96,9 +97,9 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n        (2, 2, 2, 1)),\n       ('DepthwiseConv2D', lambda: convolutional.DepthwiseConv2D(2, 2),\n        (2, 2, 2, 1)),\n-      ('UpSampling2D', convolutional.UpSampling2D, (2, 2, 2, 1)),\n-      ('ZeroPadding2D', convolutional.ZeroPadding2D, (2, 2, 2, 1)),\n-      ('Cropping2D', convolutional.Cropping2D, (2, 3, 3, 1)),\n+      ('UpSampling2D', reshaping.UpSampling2D, (2, 2, 2, 1)),\n+      ('ZeroPadding2D', reshaping.ZeroPadding2D, (2, 2, 2, 1)),\n+      ('Cropping2D', reshaping.Cropping2D, (2, 3, 3, 1)),\n       ('ConvLSTM2D',\n        lambda: convolutional_recurrent.ConvLSTM2D(4, kernel_size=(2, 2)),\n        (4, 4, 4, 4, 4)),\n@@ -190,8 +191,7 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n         will be randomly generated\n     \"\"\"\n \n-    if f32_layer_fn == convolutional.ZeroPadding2D and \\\n-       tf.test.is_built_with_rocm():\n+    if f32_layer_fn == reshaping.ZeroPadding2D and tf.test.is_built_with_rocm():\n       return\n     if isinstance(input_shape[0], int):\n       input_shapes = [input_shape]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#571b7965e79c8075c3d0f8b19ffe5cfcdb95f71f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 114 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 117 | Churn Cumulative: 1918 | Contributors (this commit): 8 | Commits (past 90d): 17 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -242,10 +242,6 @@ class Attention(BaseDenseAttention):\n       Defaults to `False`.\n     dropout: Float between 0 and 1. Fraction of the units to drop for the\n       attention scores. Defaults to 0.0.\n-    score_mode: Function to use to compute attention scores, one of\n-      `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the query\n-      and key vectors. `\"concat\"` refers to the hyperbolic tangent of the\n-      concatenation of the query and key vectors.\n \n   Call Args:\n \n@@ -323,16 +319,12 @@ class Attention(BaseDenseAttention):\n   ```\n   \"\"\"\n \n-  def __init__(self, use_scale=False, score_mode='dot', **kwargs):\n+  def __init__(self, use_scale=False, **kwargs):\n     super(Attention, self).__init__(**kwargs)\n     self.use_scale = use_scale\n-    self.score_mode = score_mode\n-    if self.score_mode not in ['dot', 'concat']:\n-      raise ValueError(f'Received: score_mode={score_mode}. Acceptable values '\n-                       'are: [\"dot\", \"concat\"]')\n \n   def build(self, input_shape):\n-    \"\"\"Creates variable when `use_scale` is True or `score_mode` is `concat`.\"\"\"\n+    \"\"\"Creates scale variable if use_scale==True.\"\"\"\n     if self.use_scale:\n       self.scale = self.add_weight(\n           name='scale',\n@@ -342,15 +334,6 @@ class Attention(BaseDenseAttention):\n           trainable=True)\n     else:\n       self.scale = None\n-    if self.score_mode == 'concat':\n-      self.concat_score_weight = self.add_weight(\n-          name='concat_score_weight',\n-          shape=(),\n-          initializer='ones',\n-          dtype=self.dtype,\n-          trainable=True)\n-    else:\n-      self.concat_score_weight = None\n     super(Attention, self).build(input_shape)\n \n   def _calculate_scores(self, query, key):\n@@ -362,27 +345,13 @@ class Attention(BaseDenseAttention):\n     Returns:\n       Tensor of shape `[batch_size, Tq, Tv]`.\n     \"\"\"\n-    if self.score_mode == 'dot':\n     scores = tf.matmul(query, key, transpose_b=True)\n     if self.scale is not None:\n       scores *= self.scale\n-    elif self.score_mode == 'concat':\n-      # Reshape tensors to enable broadcasting.\n-      # Reshape into [batch_size, Tq, 1, dim].\n-      q_reshaped = tf.expand_dims(query, axis=-2)\n-      # Reshape into [batch_size, 1, Tv, dim].\n-      k_reshaped = tf.expand_dims(key, axis=-3)\n-      if self.scale is not None:\n-        scores = self.concat_score_weight * tf.reduce_sum(\n-            tf.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1)\n-      else:\n-        scores = self.concat_score_weight * tf.reduce_sum(\n-            tf.tanh(q_reshaped + k_reshaped), axis=-1)\n-\n     return scores\n \n   def get_config(self):\n-    config = {'use_scale': self.use_scale, 'score_mode': self.score_mode}\n+    config = {'use_scale': self.use_scale}\n     base_config = super(Attention, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -204,31 +204,6 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n                         dtype=np.float32)\n     self.assertAllClose(expected, actual)\n \n-  def test_calculate_scores_multi_dim_concat(self):\n-    # Query tensor of shape [1, 2, 4]\n-    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n-    # Key tensor of shape [1, 3, 4]\n-    k = np.array(\n-        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n-        dtype=np.float32)\n-    attention_layer = dense_attention.Attention(score_mode='concat')\n-    attention_layer.concat_score_weight = 1\n-    attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))\n-    actual = keras.backend.get_value(\n-        attention_layer._calculate_scores(query=q, key=k))\n-\n-    # pylint:disable=line-too-long\n-    # expected000 = tanh(1.+1.5) + tanh(1.1+1.6) + tanh(1.2+1.7) + tanh(1.3+1.8) = 3.96753427840\n-    # expected001 = tanh(1.+2.5) + tanh(1.1+2.6) + tanh(1.2+2.7) + tanh(1.3+2.8) = 3.99558784825\n-    # expected002 = tanh(1.+3.5) + tanh(1.1+3.6) + tanh(1.2+3.7) + tanh(1.3+3.8) = 3.99940254147\n-    # expected010 = tanh(2.+1.5) + tanh(2.1+1.6) + tanh(2.2+1.7) + tanh(2.3+1.8) = 3.99558784825\n-    # expected011 = tanh(2.+2.5) + tanh(2.1+2.6) + tanh(2.2+2.7) + tanh(2.3+2.8) = 3.99940254147\n-    # expected012 = tanh(2.+3.5) + tanh(2.1+3.6) + tanh(2.2+3.7) + tanh(2.3+3.8) = 3.99991913657\n-    expected = np.array([[[3.96753427840, 3.99558784825, 3.99940254147],\n-                          [3.99558784825, 3.99940254147, 3.99991913657]]],\n-                        dtype=np.float32)\n-    self.assertAllClose(expected, actual)\n-\n   def test_calculate_scores_one_dim_batch_size_two(self):\n     # Query tensor of shape [2, 1, 1]\n     q = np.array([[[1.1]], [[2.1]]], dtype=np.float32)\n@@ -260,25 +235,6 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     expected = np.array([[[-3.52]]], dtype=np.float32)\n     self.assertAllClose(expected, actual)\n \n-  def test_calculate_scores_one_dim_with_scale_concat(self):\n-    \"\"\"Tests that scores are multiplied by scale.\"\"\"\n-    # Query tensor of shape [1, 1, 1]\n-    q = np.array([[[1.1]]], dtype=np.float32)\n-    # Key tensor of shape [1, 1, 1]\n-    k = np.array([[[1.6]]], dtype=np.float32)\n-    attention_layer = dense_attention.Attention(\n-        use_scale=True, score_mode='concat')\n-    attention_layer.concat_score_weight = 1\n-    attention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))\n-    attention_layer.scale = 2.\n-    actual = keras.backend.get_value(\n-        attention_layer._calculate_scores(query=q, key=k))\n-\n-    # Expected tensor of shape [1, 1, 1].\n-    # expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402\n-    expected = np.array([[[0.999959202]]], dtype=np.float32)\n-    self.assertAllClose(expected, actual)\n-\n   def test_shape(self):\n     # Query tensor of shape [1, 2, 4]\n     q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n@@ -294,22 +250,6 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     expected_shape = [1, 2, 4]\n     self.assertAllEqual(expected_shape, tf.shape(actual))\n \n-  def test_shape_concat(self):\n-    # Query tensor of shape [1, 2, 4]\n-    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n-    # Value tensor of shape [1, 3, 4]\n-    v = np.array(\n-        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n-        dtype=np.float32)\n-    # Value mask tensor of shape [1, 3]\n-    v_mask = np.array([[True, True, False]], dtype=np.bool_)\n-    attention_layer = dense_attention.Attention(score_mode='concat')\n-    attention_layer.concat_score_weight = 1\n-    actual = attention_layer([q, v], mask=[None, v_mask])\n-\n-    expected_shape = [1, 2, 4]\n-    self.assertAllEqual(expected_shape, tf.shape(actual))\n-\n   def test_shape_with_key(self):\n     # Query tensor of shape [1, 2, 4]\n     q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n@@ -329,26 +269,6 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     expected_shape = [1, 2, 4]\n     self.assertAllEqual(expected_shape, tf.shape(actual))\n \n-  def test_shape_with_key_concat(self):\n-    # Query tensor of shape [1, 2, 4]\n-    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n-    # Value tensor of shape [1, 3, 4]\n-    v = np.array(\n-        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n-        dtype=np.float32)\n-    # Key tensor of shape [1, 3, 4]\n-    k = np.array(\n-        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n-        dtype=np.float32)\n-    # Value mask tensor of shape [1, 3]\n-    v_mask = np.array([[True, True, False]], dtype=np.bool_)\n-    attention_layer = dense_attention.Attention(score_mode='concat')\n-    attention_layer.concat_score_weight = 1\n-    actual = attention_layer([q, v, k], mask=[None, v_mask])\n-\n-    expected_shape = [1, 2, 4]\n-    self.assertAllEqual(expected_shape, tf.shape(actual))\n-\n   def test_multi_dim(self):\n     # Query tensor of shape [1, 1, 1]\n     q = np.array([[[1.1]]], dtype=np.float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#914fa54fa027fdd037dbe90382e48071c89a2e6d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 36 | Churn Cumulative: 11861 | Contributors (this commit): 29 | Commits (past 90d): 20 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3132,8 +3132,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       flat_specs = [tf_utils.get_tensor_spec(x) for x in flat_kwarg]\n       if any(s is None for s in flat_specs):\n         continue\n-      kwargs[key] = args_spec.append(\n-          tf.nest.pack_sequence_as(kwarg, flat_specs))\n+      kwargs_spec[key] = tf.nest.pack_sequence_as(kwarg, flat_specs)\n \n     self._saved_model_inputs_spec = inputs_spec\n     self._saved_model_arg_spec = ([inputs_spec] + args_spec, kwargs_spec)\n\n@@ -1930,6 +1930,39 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertEqual(training_module._get_verbosity(\n         'auto', tf.distribute.MirroredStrategy()), 2)\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_save_spec(self):\n+\n+    class Model(training_module.Model):\n+\n+      def call(self, arg_input_1, arg_input_2, keyword_input, training=None):\n+        return 0\n+\n+    # Test subclassed model save specs.\n+    model = Model()\n+    model(tf.ones([1, 1]), tf.ones([2, 2]), keyword_input=tf.ones([3, 3]),\n+          training=False)\n+    spec = model.save_spec(dynamic_batch=False)\n+    self.assertEqual(spec[0][0].shape.as_list(), [1, 1])\n+    self.assertEqual(spec[0][1].shape.as_list(), [2, 2])\n+    self.assertEqual(spec[1]['keyword_input'].shape.as_list(), [3, 3])\n+    spec = model.save_spec(dynamic_batch=True)\n+    self.assertEqual(spec[0][0].shape.as_list(), [None, 1])\n+\n+    # Test functional model save specs.\n+    input_1 = layers_module.Input((1,), batch_size=1)\n+    input_2 = layers_module.Input((2,), batch_size=2)\n+    input_3 = layers_module.Input((3,), batch_size=3)\n+    output = model(input_1, input_2, keyword_input=input_3, training=True)\n+    functional = training_module.Model([input_1, input_2, input_3], output)\n+    # Functional models should ignore dynamic_batch if the input layers have a\n+    # known batch size.\n+    spec = functional.save_spec(dynamic_batch=True)\n+    input_specs = spec[0][0]\n+    self.assertEqual(input_specs[0].shape.as_list(), [1, 1])\n+    self.assertEqual(input_specs[1].shape.as_list(), [2, 2])\n+    self.assertEqual(input_specs[2].shape.as_list(), [3, 3])\n+\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2dc80bac1428e72421a90dd118a3659f2fe60695", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 218 | Lines Deleted: 91 | Files Changed: 15 | Hunks: 23 | Methods Changed: 20 | Complexity Δ (Sum/Max): 37/16 | Churn Δ: 309 | Churn Cumulative: 3228 | Contributors (this commit): 22 | Commits (past 90d): 25 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,6 +83,10 @@ from keras.layers.convolutional.separable_conv2d import SeparableConvolution2D\n from keras.layers.reshaping.cropping1d import Cropping1D\n from keras.layers.reshaping.cropping2d import Cropping2D\n from keras.layers.reshaping.cropping3d import Cropping3D\n+from keras.layers.reshaping.flatten import Flatten\n+from keras.layers.reshaping.permute import Permute\n+from keras.layers.reshaping.repeat_vector import RepeatVector\n+from keras.layers.reshaping.reshape import Reshape\n from keras.layers.reshaping.up_sampling1d import UpSampling1D\n from keras.layers.reshaping.up_sampling2d import UpSampling2D\n from keras.layers.reshaping.up_sampling3d import UpSampling3D\n@@ -95,12 +99,8 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.activity_regularization import ActivityRegularization\n from keras.layers.core.dense import Dense\n from keras.layers.core.dropout import Dropout\n-from keras.layers.core.flatten import Flatten\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n-from keras.layers.core.permute import Permute\n-from keras.layers.core.repeat_vector import RepeatVector\n-from keras.layers.core.reshape import Reshape\n from keras.layers.core.spatial_dropout import SpatialDropout1D\n from keras.layers.core.spatial_dropout import SpatialDropout2D\n from keras.layers.core.spatial_dropout import SpatialDropout3D\n\n@@ -18,12 +18,8 @@ from keras.layers.core.activation import Activation\n from keras.layers.core.activity_regularization import ActivityRegularization\n from keras.layers.core.dense import Dense\n from keras.layers.core.dropout import Dropout\n-from keras.layers.core.flatten import Flatten\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n-from keras.layers.core.permute import Permute\n-from keras.layers.core.repeat_vector import RepeatVector\n-from keras.layers.core.reshape import Reshape\n from keras.layers.core.spatial_dropout import SpatialDropout1D\n from keras.layers.core.spatial_dropout import SpatialDropout2D\n from keras.layers.core.spatial_dropout import SpatialDropout3D\n@@ -37,3 +33,8 @@ from keras.layers.core.tf_op_layer import InstanceProperty\n from keras.layers.core.tf_op_layer import SlicingOpLambda\n from keras.layers.core.tf_op_layer import TFOpLambda\n \n+# Reshaping layers imported for backwards namespace compatibility\n+from keras.layers.reshaping.flatten import Flatten\n+from keras.layers.reshaping.permute import Permute\n+from keras.layers.reshaping.repeat_vector import RepeatVector\n+from keras.layers.reshaping.reshape import Reshape\n\n@@ -481,81 +481,6 @@ class CoreLayersTest(keras_parameterized.TestCase):\n         kwargs={'activation': keras.backend.relu},\n         input_shape=(3, 2))\n \n-  def test_reshape(self):\n-    testing_utils.layer_test(\n-        keras.layers.Reshape,\n-        kwargs={'target_shape': (8, 1)},\n-        input_shape=(3, 2, 4))\n-\n-    testing_utils.layer_test(\n-        keras.layers.Reshape,\n-        kwargs={'target_shape': (-1, 1)},\n-        input_shape=(3, 2, 4))\n-\n-    testing_utils.layer_test(\n-        keras.layers.Reshape,\n-        kwargs={'target_shape': (1, -1)},\n-        input_shape=(3, 2, 4))\n-\n-    testing_utils.layer_test(\n-        keras.layers.Reshape,\n-        kwargs={'target_shape': (-1, 1)},\n-        input_shape=(None, None, 2))\n-\n-  def test_reshape_set_static_shape(self):\n-    input_layer = keras.Input(batch_shape=(1, None))\n-    reshaped = keras.layers.Reshape((1, 100))(input_layer)\n-    # Make sure the batch dim is not lost after array_ops.reshape.\n-    self.assertEqual(reshaped.shape, [1, 1, 100])\n-\n-  def test_permute(self):\n-    testing_utils.layer_test(\n-        keras.layers.Permute, kwargs={'dims': (2, 1)}, input_shape=(3, 2, 4))\n-\n-  def test_permute_errors_on_invalid_starting_dims_index(self):\n-    with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n-      testing_utils.layer_test(\n-          keras.layers.Permute,\n-          kwargs={'dims': (0, 1, 2)},\n-          input_shape=(3, 2, 4))\n-\n-  def test_permute_errors_on_invalid_set_of_dims_indices(self):\n-    with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n-      testing_utils.layer_test(\n-          keras.layers.Permute,\n-          kwargs={'dims': (1, 4, 2)},\n-          input_shape=(3, 2, 4))\n-\n-  def test_flatten(self):\n-    testing_utils.layer_test(\n-        keras.layers.Flatten, kwargs={}, input_shape=(3, 2, 4))\n-\n-    # Test channels_first\n-    inputs = np.random.random((10, 3, 5, 5)).astype('float32')\n-    outputs = testing_utils.layer_test(\n-        keras.layers.Flatten,\n-        kwargs={'data_format': 'channels_first'},\n-        input_data=inputs)\n-    target_outputs = np.reshape(\n-        np.transpose(inputs, (0, 2, 3, 1)), (-1, 5 * 5 * 3))\n-    self.assertAllClose(outputs, target_outputs)\n-\n-  def test_flatten_scalar_channels(self):\n-    testing_utils.layer_test(keras.layers.Flatten, kwargs={}, input_shape=(3,))\n-\n-    # Test channels_first\n-    inputs = np.random.random((10,)).astype('float32')\n-    outputs = testing_utils.layer_test(\n-        keras.layers.Flatten,\n-        kwargs={'data_format': 'channels_first'},\n-        input_data=inputs)\n-    target_outputs = np.expand_dims(inputs, -1)\n-    self.assertAllClose(outputs, target_outputs)\n-\n-  def test_repeat_vector(self):\n-    testing_utils.layer_test(\n-        keras.layers.RepeatVector, kwargs={'n': 3}, input_shape=(3, 2))\n-\n   def test_dense(self):\n     testing_utils.layer_test(\n         keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 2))\n@@ -708,10 +633,6 @@ class CoreLayersTest(keras_parameterized.TestCase):\n \n   def test_numpy_inputs(self):\n     if tf.executing_eagerly():\n-      layer = keras.layers.RepeatVector(2)\n-      x = np.ones((10, 10))\n-      self.assertAllEqual(np.ones((10, 2, 10)), layer(x))\n-\n       layer = keras.layers.Concatenate()\n       x, y = np.ones((10, 10)), np.ones((10, 10))\n       self.assertAllEqual(np.ones((10, 20)), layer([x, y]))\n\n@@ -17,6 +17,10 @@\n from keras.layers.reshaping.cropping1d import Cropping1D\n from keras.layers.reshaping.cropping2d import Cropping2D\n from keras.layers.reshaping.cropping3d import Cropping3D\n+from keras.layers.reshaping.flatten import Flatten\n+from keras.layers.reshaping.permute import Permute\n+from keras.layers.reshaping.repeat_vector import RepeatVector\n+from keras.layers.reshaping.reshape import Reshape\n from keras.layers.reshaping.up_sampling1d import UpSampling1D\n from keras.layers.reshaping.up_sampling2d import UpSampling2D\n from keras.layers.reshaping.up_sampling3d import UpSampling3D\n\n@@ -17,11 +17,13 @@\n \n import functools\n import operator\n+\n from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n from keras.utils import conv_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -0,0 +1,54 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for flatten layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class FlattenTest(keras_parameterized.TestCase):\n+\n+  def test_flatten(self):\n+    testing_utils.layer_test(\n+        keras.layers.Flatten, kwargs={}, input_shape=(3, 2, 4))\n+\n+    # Test channels_first\n+    inputs = np.random.random((10, 3, 5, 5)).astype('float32')\n+    outputs = testing_utils.layer_test(\n+        keras.layers.Flatten,\n+        kwargs={'data_format': 'channels_first'},\n+        input_data=inputs)\n+    target_outputs = np.reshape(\n+        np.transpose(inputs, (0, 2, 3, 1)), (-1, 5 * 5 * 3))\n+    self.assertAllClose(outputs, target_outputs)\n+\n+  def test_flatten_scalar_channels(self):\n+    testing_utils.layer_test(keras.layers.Flatten, kwargs={}, input_shape=(3,))\n+\n+    # Test channels_first\n+    inputs = np.random.random((10,)).astype('float32')\n+    outputs = testing_utils.layer_test(\n+        keras.layers.Flatten,\n+        kwargs={'data_format': 'channels_first'},\n+        input_data=inputs)\n+    target_outputs = np.expand_dims(inputs, -1)\n+    self.assertAllClose(outputs, target_outputs)\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -16,9 +16,11 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n import copy\n+\n from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -0,0 +1,46 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras permute layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class PermuteTest(keras_parameterized.TestCase):\n+\n+  def test_permute(self):\n+    testing_utils.layer_test(\n+        keras.layers.Permute, kwargs={'dims': (2, 1)}, input_shape=(3, 2, 4))\n+\n+  def test_permute_errors_on_invalid_starting_dims_index(self):\n+    with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n+      testing_utils.layer_test(\n+          keras.layers.Permute,\n+          kwargs={'dims': (0, 1, 2)},\n+          input_shape=(3, 2, 4))\n+\n+  def test_permute_errors_on_invalid_set_of_dims_indices(self):\n+    with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n+      testing_utils.layer_test(\n+          keras.layers.Permute,\n+          kwargs={'dims': (1, 4, 2)},\n+          input_shape=(3, 2, 4))\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n\n@@ -0,0 +1,39 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for repeat vector layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+import numpy as np\n+\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class RepeatVectorTest(keras_parameterized.TestCase):\n+\n+  def test_repeat_vector(self):\n+    testing_utils.layer_test(\n+        keras.layers.RepeatVector, kwargs={'n': 3}, input_shape=(3, 2))\n+\n+  def test_numpy_inputs(self):\n+    if tf.executing_eagerly():\n+      layer = keras.layers.RepeatVector(2)\n+      x = np.ones((10, 10))\n+      self.assertAllEqual(np.ones((10, 2, 10)), layer(x))\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -18,6 +18,7 @@\n from keras.engine.base_layer import Layer\n import numpy as np\n import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -0,0 +1,55 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for reshape layer.\"\"\"\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+\n+import tensorflow.compat.v2 as tf\n+\n+\n+@keras_parameterized.run_all_keras_modes\n+class ReshapeTest(keras_parameterized.TestCase):\n+\n+  def test_reshape(self):\n+    testing_utils.layer_test(\n+        keras.layers.Reshape,\n+        kwargs={'target_shape': (8, 1)},\n+        input_shape=(3, 2, 4))\n+\n+    testing_utils.layer_test(\n+        keras.layers.Reshape,\n+        kwargs={'target_shape': (-1, 1)},\n+        input_shape=(3, 2, 4))\n+\n+    testing_utils.layer_test(\n+        keras.layers.Reshape,\n+        kwargs={'target_shape': (1, -1)},\n+        input_shape=(3, 2, 4))\n+\n+    testing_utils.layer_test(\n+        keras.layers.Reshape,\n+        kwargs={'target_shape': (-1, 1)},\n+        input_shape=(None, None, 2))\n+\n+  def test_reshape_set_static_shape(self):\n+    input_layer = keras.Input(batch_shape=(1, None))\n+    reshaped = keras.layers.Reshape((1, 100))(input_layer)\n+    # Make sure the batch dim is not lost after array_ops.reshape.\n+    self.assertEqual(reshaped.shape, [1, 1, 100])\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -107,8 +107,8 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('Dropout', lambda: core.Dropout(0.5), (2, 2)),\n       ('SpatialDropout2D', lambda: core.SpatialDropout2D(0.5), (2, 2, 2, 2)),\n       ('Activation', lambda: core.Activation('sigmoid'), (2, 2)),\n-      ('Reshape', lambda: core.Reshape((1, 4, 1)), (2, 2, 2)),\n-      ('Permute', lambda: core.Permute((2, 1)), (2, 2, 2)),\n+      ('Reshape', lambda: reshaping.Reshape((1, 4, 1)), (2, 2, 2)),\n+      ('Permute', lambda: reshaping.Permute((2, 1)), (2, 2, 2)),\n       ('Attention', dense_attention.Attention, [(2, 2, 3), (2, 3, 3),\n                                                 (2, 3, 3)]),\n       ('AdditiveAttention', dense_attention.AdditiveAttention, [(2, 2, 3),\n\n@@ -26,6 +26,7 @@ from keras.engine import training\n from keras.layers import convolutional as conv_layer_lib\n from keras.layers import core as layer_lib\n from keras.layers import pooling as pool_layer_lib\n+from keras.layers import reshaping as reshaping_layer_lib\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -140,7 +141,7 @@ def mnist_model(input_shape, enable_histograms=True):\n   model.add(conv_layer_lib.Conv2D(64, (3, 3), activation='relu'))\n   model.add(pool_layer_lib.MaxPooling2D(pool_size=(2, 2)))\n   model.add(layer_lib.Dropout(0.25))\n-  model.add(layer_lib.Flatten())\n+  model.add(reshaping_layer_lib.Flatten())\n   model.add(layer_lib.Dense(128, activation='relu'))\n   model.add(layer_lib.Dropout(0.5))\n   model.add(layer_lib.Dense(NUM_CLASSES, activation='softmax'))\n\n@@ -27,6 +27,7 @@ from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n+from keras.layers import reshaping\n from keras.optimizer_v2 import adam\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.tracking import util as trackable_utils\n@@ -871,7 +872,7 @@ class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n       # Create and save a model using Saver() before using a Checkpoint. This\n       # generates a snapshot without the Checkpoint's `save_counter`.\n       model = sequential.Sequential()\n-      model.add(core.Flatten(input_shape=(1,)))\n+      model.add(reshaping.Flatten(input_shape=(1,)))\n       model.add(core.Dense(1))\n       name_saver = tf.compat.v1.train.Saver(model.trainable_variables)\n       save_path = name_saver.save(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa9e953c6fa165056110bdd598a8d8b1b9c4c807", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3486 | Contributors (this commit): 13 | Commits (past 90d): 11 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1790,11 +1790,9 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n       values = []\n       with open(fp) as f:\n-        for x in csv.reader(f):\n-          # In windows, due to \\r\\n line ends we may end up reading empty lines\n+        values = [x for x in csv.reader(f) if x]# In windows, due to \\r\\n line ends we may end up reading empty lines\n           # after each line. Skip empty lines.\n-          if x:\n-            values.append(x)\n+        \n       assert 'nan' in values[-1], 'The last epoch was not logged.'\n \n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#109219d3fe75bea515c2b30f4cf8961080defd86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 3489 | Contributors (this commit): 14 | Commits (past 90d): 12 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1790,8 +1790,9 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n       values = []\n       with open(fp) as f:\n-        values = [x for x in csv.reader(f) if x]# In windows, due to \\r\\n line ends we may end up reading empty lines\n+        # On Windows, due to \\r\\n line ends, we may end up reading empty lines\n         # after each line. Skip empty lines.\n+        values = [x for x in csv.reader(f) if x]\n         \n       assert 'nan' in values[-1], 'The last epoch was not logged.'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5059d2dcb682ccdb2d1a20970ac88a73f6aa7572", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1736 | Contributors (this commit): 11 | Commits (past 90d): 3 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -147,9 +147,10 @@ class Functional(training_lib.Model):\n \n   @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def _init_graph_network(self, inputs, outputs):\n-    base_layer.keras_api_gauge.get_cell('Functional').set(True)\n     # This method is needed for Sequential to reinitialize graph network when\n     # layer is added or removed.\n+\n+    base_layer.keras_api_gauge.get_cell('Functional').set(True)\n     self._is_graph_network = True\n \n     # Normalize and set self.inputs, self.outputs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f2d0d8c96ba74d71f6074ed8123be151b14fcea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 114 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 12 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 117 | Churn Cumulative: 2035 | Contributors (this commit): 8 | Commits (past 90d): 19 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -242,6 +242,10 @@ class Attention(BaseDenseAttention):\n       Defaults to `False`.\n     dropout: Float between 0 and 1. Fraction of the units to drop for the\n       attention scores. Defaults to 0.0.\n+    score_mode: Function to use to compute attention scores, one of\n+      `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the query\n+      and key vectors. `\"concat\"` refers to the hyperbolic tangent of the\n+      concatenation of the query and key vectors.\n \n   Call Args:\n \n@@ -319,12 +323,16 @@ class Attention(BaseDenseAttention):\n   ```\n   \"\"\"\n \n-  def __init__(self, use_scale=False, **kwargs):\n+  def __init__(self, use_scale=False, score_mode='dot', **kwargs):\n     super(Attention, self).__init__(**kwargs)\n     self.use_scale = use_scale\n+    self.score_mode = score_mode\n+    if self.score_mode not in ['dot', 'concat']:\n+      raise ValueError(f'Received: score_mode={score_mode}. Acceptable values '\n+                       'are: [\"dot\", \"concat\"]')\n \n   def build(self, input_shape):\n-    \"\"\"Creates scale variable if use_scale==True.\"\"\"\n+    \"\"\"Creates variable when `use_scale` is True or `score_mode` is `concat`.\"\"\"\n     if self.use_scale:\n       self.scale = self.add_weight(\n           name='scale',\n@@ -334,6 +342,15 @@ class Attention(BaseDenseAttention):\n           trainable=True)\n     else:\n       self.scale = None\n+    if self.score_mode == 'concat':\n+      self.concat_score_weight = self.add_weight(\n+          name='concat_score_weight',\n+          shape=(),\n+          initializer='ones',\n+          dtype=self.dtype,\n+          trainable=True)\n+    else:\n+      self.concat_score_weight = None\n     super(Attention, self).build(input_shape)\n \n   def _calculate_scores(self, query, key):\n@@ -345,13 +362,27 @@ class Attention(BaseDenseAttention):\n     Returns:\n       Tensor of shape `[batch_size, Tq, Tv]`.\n     \"\"\"\n+    if self.score_mode == 'dot':\n       scores = tf.matmul(query, key, transpose_b=True)\n       if self.scale is not None:\n         scores *= self.scale\n+    elif self.score_mode == 'concat':\n+      # Reshape tensors to enable broadcasting.\n+      # Reshape into [batch_size, Tq, 1, dim].\n+      q_reshaped = tf.expand_dims(query, axis=-2)\n+      # Reshape into [batch_size, 1, Tv, dim].\n+      k_reshaped = tf.expand_dims(key, axis=-3)\n+      if self.scale is not None:\n+        scores = self.concat_score_weight * tf.reduce_sum(\n+            tf.tanh(self.scale * (q_reshaped + k_reshaped)), axis=-1)\n+      else:\n+        scores = self.concat_score_weight * tf.reduce_sum(\n+            tf.tanh(q_reshaped + k_reshaped), axis=-1)\n+\n     return scores\n \n   def get_config(self):\n-    config = {'use_scale': self.use_scale}\n+    config = {'use_scale': self.use_scale, 'score_mode': self.score_mode}\n     base_config = super(Attention, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -204,6 +204,31 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n                         dtype=np.float32)\n     self.assertAllClose(expected, actual)\n \n+  def test_calculate_scores_multi_dim_concat(self):\n+    # Query tensor of shape [1, 2, 4]\n+    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n+    # Key tensor of shape [1, 3, 4]\n+    k = np.array(\n+        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n+        dtype=np.float32)\n+    attention_layer = dense_attention.Attention(score_mode='concat')\n+    attention_layer.concat_score_weight = 1\n+    attention_layer.build(input_shape=([1, 2, 4], [1, 3, 4]))\n+    actual = keras.backend.get_value(\n+        attention_layer._calculate_scores(query=q, key=k))\n+\n+    # pylint:disable=line-too-long\n+    # expected000 = tanh(1.+1.5) + tanh(1.1+1.6) + tanh(1.2+1.7) + tanh(1.3+1.8) = 3.96753427840\n+    # expected001 = tanh(1.+2.5) + tanh(1.1+2.6) + tanh(1.2+2.7) + tanh(1.3+2.8) = 3.99558784825\n+    # expected002 = tanh(1.+3.5) + tanh(1.1+3.6) + tanh(1.2+3.7) + tanh(1.3+3.8) = 3.99940254147\n+    # expected010 = tanh(2.+1.5) + tanh(2.1+1.6) + tanh(2.2+1.7) + tanh(2.3+1.8) = 3.99558784825\n+    # expected011 = tanh(2.+2.5) + tanh(2.1+2.6) + tanh(2.2+2.7) + tanh(2.3+2.8) = 3.99940254147\n+    # expected012 = tanh(2.+3.5) + tanh(2.1+3.6) + tanh(2.2+3.7) + tanh(2.3+3.8) = 3.99991913657\n+    expected = np.array([[[3.96753427840, 3.99558784825, 3.99940254147],\n+                          [3.99558784825, 3.99940254147, 3.99991913657]]],\n+                        dtype=np.float32)\n+    self.assertAllClose(expected, actual)\n+\n   def test_calculate_scores_one_dim_batch_size_two(self):\n     # Query tensor of shape [2, 1, 1]\n     q = np.array([[[1.1]], [[2.1]]], dtype=np.float32)\n@@ -235,6 +260,25 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     expected = np.array([[[-3.52]]], dtype=np.float32)\n     self.assertAllClose(expected, actual)\n \n+  def test_calculate_scores_one_dim_with_scale_concat(self):\n+    \"\"\"Tests that scores are multiplied by scale.\"\"\"\n+    # Query tensor of shape [1, 1, 1]\n+    q = np.array([[[1.1]]], dtype=np.float32)\n+    # Key tensor of shape [1, 1, 1]\n+    k = np.array([[[1.6]]], dtype=np.float32)\n+    attention_layer = dense_attention.Attention(\n+        use_scale=True, score_mode='concat')\n+    attention_layer.concat_score_weight = 1\n+    attention_layer.build(input_shape=([1, 1, 1], [1, 1, 1]))\n+    attention_layer.scale = 2.\n+    actual = keras.backend.get_value(\n+        attention_layer._calculate_scores(query=q, key=k))\n+\n+    # Expected tensor of shape [1, 1, 1].\n+    # expected000 = tanh(2*(1.1+1.6)) = 0.9999592018254402\n+    expected = np.array([[[0.999959202]]], dtype=np.float32)\n+    self.assertAllClose(expected, actual)\n+\n   def test_shape(self):\n     # Query tensor of shape [1, 2, 4]\n     q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n@@ -250,6 +294,22 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     expected_shape = [1, 2, 4]\n     self.assertAllEqual(expected_shape, tf.shape(actual))\n \n+  def test_shape_concat(self):\n+    # Query tensor of shape [1, 2, 4]\n+    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n+    # Value tensor of shape [1, 3, 4]\n+    v = np.array(\n+        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n+        dtype=np.float32)\n+    # Value mask tensor of shape [1, 3]\n+    v_mask = np.array([[True, True, False]], dtype=np.bool_)\n+    attention_layer = dense_attention.Attention(score_mode='concat')\n+    attention_layer.concat_score_weight = 1\n+    actual = attention_layer([q, v], mask=[None, v_mask])\n+\n+    expected_shape = [1, 2, 4]\n+    self.assertAllEqual(expected_shape, tf.shape(actual))\n+\n   def test_shape_with_key(self):\n     # Query tensor of shape [1, 2, 4]\n     q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n@@ -269,6 +329,26 @@ class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n     expected_shape = [1, 2, 4]\n     self.assertAllEqual(expected_shape, tf.shape(actual))\n \n+  def test_shape_with_key_concat(self):\n+    # Query tensor of shape [1, 2, 4]\n+    q = np.array([[[1., 1.1, 1.2, 1.3], [2., 2.1, 2.2, 2.3]]], dtype=np.float32)\n+    # Value tensor of shape [1, 3, 4]\n+    v = np.array(\n+        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n+        dtype=np.float32)\n+    # Key tensor of shape [1, 3, 4]\n+    k = np.array(\n+        [[[1.5, 1.6, 1.7, 1.8], [2.5, 2.6, 2.7, 2.8], [3.5, 3.6, 3.7, 3.8]]],\n+        dtype=np.float32)\n+    # Value mask tensor of shape [1, 3]\n+    v_mask = np.array([[True, True, False]], dtype=np.bool_)\n+    attention_layer = dense_attention.Attention(score_mode='concat')\n+    attention_layer.concat_score_weight = 1\n+    actual = attention_layer([q, v, k], mask=[None, v_mask])\n+\n+    expected_shape = [1, 2, 4]\n+    self.assertAllEqual(expected_shape, tf.shape(actual))\n+\n   def test_multi_dim(self):\n     # Query tensor of shape [1, 1, 1]\n     q = np.array([[[1.1]]], dtype=np.float32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80ca9f8fbbbb5e248143af02d639d0a947276331", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1444 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -704,6 +704,20 @@ class Optimizer(_BaseOptimizer):\n         skip_aggregate_gradients=True)\n   ```\n \n+  ### Creating a custom optimizer\n+\n+  If you intend to create your own optimization algorithm, please inherit from\n+  this class and override the following methods:\n+\n+    - `build`: Create your optimizer-related variables, such as `momentums` in\n+      SGD optimizer.\n+    - `update_step`: Implement your optimizer's updating logic.\n+    - `get_config`: serialization of the optimizer, include all hyper\n+      parameters.\n+\n+  Your optimizer would automatically be compatible with tensorflow distributed\n+  training if you subclass `optimizer_experimental.Optimizer`.\n+\n   \"\"\"\n \n   def __init__(self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8cdedbf695718e2afc7ab040c00e19f6d7feb25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2239 | Lines Deleted: 1723 | Files Changed: 27 | Hunks: 28 | Methods Changed: 92 | Complexity Δ (Sum/Max): 5/10 | Churn Δ: 3962 | Churn Cumulative: 9634 | Contributors (this commit): 33 | Commits (past 90d): 34 | Contributors (cumulative): 66 | DMM Complexity: 0.0\n\nDIFF:\n@@ -168,32 +168,32 @@ else:\n from keras.layers.kernelized import RandomFourierFeatures\n \n # Pooling layers.\n-from keras.layers.pooling import MaxPooling1D\n-from keras.layers.pooling import MaxPooling2D\n-from keras.layers.pooling import MaxPooling3D\n-from keras.layers.pooling import AveragePooling1D\n-from keras.layers.pooling import AveragePooling2D\n-from keras.layers.pooling import AveragePooling3D\n-from keras.layers.pooling import GlobalAveragePooling1D\n-from keras.layers.pooling import GlobalAveragePooling2D\n-from keras.layers.pooling import GlobalAveragePooling3D\n-from keras.layers.pooling import GlobalMaxPooling1D\n-from keras.layers.pooling import GlobalMaxPooling2D\n-from keras.layers.pooling import GlobalMaxPooling3D\n+from keras.layers.pooling.average_pooling1d import AveragePooling1D\n+from keras.layers.pooling.average_pooling2d import AveragePooling2D\n+from keras.layers.pooling.average_pooling3d import AveragePooling3D\n+from keras.layers.pooling.max_pooling1d import MaxPooling1D\n+from keras.layers.pooling.max_pooling2d import MaxPooling2D\n+from keras.layers.pooling.max_pooling3d import MaxPooling3D\n+from keras.layers.pooling.global_average_pooling1d import GlobalAveragePooling1D\n+from keras.layers.pooling.global_average_pooling2d import GlobalAveragePooling2D\n+from keras.layers.pooling.global_average_pooling3d import GlobalAveragePooling3D\n+from keras.layers.pooling.global_max_pooling1d import GlobalMaxPooling1D\n+from keras.layers.pooling.global_max_pooling2d import GlobalMaxPooling2D\n+from keras.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n \n # Pooling layer aliases.\n-from keras.layers.pooling import MaxPool1D\n-from keras.layers.pooling import MaxPool2D\n-from keras.layers.pooling import MaxPool3D\n-from keras.layers.pooling import AvgPool1D\n-from keras.layers.pooling import AvgPool2D\n-from keras.layers.pooling import AvgPool3D\n-from keras.layers.pooling import GlobalAvgPool1D\n-from keras.layers.pooling import GlobalAvgPool2D\n-from keras.layers.pooling import GlobalAvgPool3D\n-from keras.layers.pooling import GlobalMaxPool1D\n-from keras.layers.pooling import GlobalMaxPool2D\n-from keras.layers.pooling import GlobalMaxPool3D\n+from keras.layers.pooling.average_pooling1d import AvgPool1D\n+from keras.layers.pooling.average_pooling2d import AvgPool2D\n+from keras.layers.pooling.average_pooling3d import AvgPool3D\n+from keras.layers.pooling.max_pooling1d import MaxPool1D\n+from keras.layers.pooling.max_pooling2d import MaxPool2D\n+from keras.layers.pooling.max_pooling3d import MaxPool3D\n+from keras.layers.pooling.global_average_pooling1d import GlobalAvgPool1D\n+from keras.layers.pooling.global_average_pooling2d import GlobalAvgPool2D\n+from keras.layers.pooling.global_average_pooling3d import GlobalAvgPool3D\n+from keras.layers.pooling.global_max_pooling1d import GlobalMaxPool1D\n+from keras.layers.pooling.global_max_pooling2d import GlobalMaxPool2D\n+from keras.layers.pooling.global_max_pooling3d import GlobalMaxPool3D\n \n # Recurrent layers.\n from keras.layers.recurrent import RNN\n\n@@ -38,12 +38,12 @@ from keras.layers.convolutional.separable_conv1d import SeparableConvolution1D\n from keras.layers.convolutional.separable_conv2d import SeparableConvolution2D\n \n # Pooling layers imported for backwards namespace compatibility.\n-from keras.layers.pooling import AveragePooling1D\n-from keras.layers.pooling import AveragePooling2D\n-from keras.layers.pooling import AveragePooling3D\n-from keras.layers.pooling import MaxPooling1D\n-from keras.layers.pooling import MaxPooling2D\n-from keras.layers.pooling import MaxPooling3D\n+from keras.layers.pooling.average_pooling1d import AveragePooling1D\n+from keras.layers.pooling.average_pooling2d import AveragePooling2D\n+from keras.layers.pooling.average_pooling3d import AveragePooling3D\n+from keras.layers.pooling.max_pooling1d import MaxPooling1D\n+from keras.layers.pooling.max_pooling2d import MaxPooling2D\n+from keras.layers.pooling.max_pooling3d import MaxPooling3D\n \n # Reshaping layers imported for backwards namespace compatibility\n from keras.layers.reshaping.cropping1d import Cropping1D\n\n@@ -1,1331 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Pooling layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import functools\n-from keras import backend\n-from keras.engine.base_layer import Layer\n-from keras.engine.input_spec import InputSpec\n-from keras.utils import conv_utils\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-class Pooling1D(Layer):\n-  \"\"\"Pooling layer for arbitrary pooling functions, for 1D inputs.\n-\n-  This class only exists for code reuse. It will never be an exposed API.\n-\n-  Args:\n-    pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n-    pool_size: An integer or tuple/list of a single integer,\n-      representing the size of the pooling window.\n-    strides: An integer or tuple/list of a single integer, specifying the\n-      strides of the pooling operation.\n-    padding: A string. The padding method, either 'valid' or 'same'.\n-      Case-insensitive.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, steps, features)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, features, steps)`.\n-    name: A string, the name of the layer.\n-  \"\"\"\n-\n-  def __init__(self, pool_function, pool_size, strides,\n-               padding='valid', data_format='channels_last',\n-               name=None, **kwargs):\n-    super(Pooling1D, self).__init__(name=name, **kwargs)\n-    if data_format is None:\n-      data_format = backend.image_data_format()\n-    if strides is None:\n-      strides = pool_size\n-    self.pool_function = pool_function\n-    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')\n-    self.strides = conv_utils.normalize_tuple(\n-        strides, 1, 'strides', allow_zero=True)\n-    self.padding = conv_utils.normalize_padding(padding)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.input_spec = InputSpec(ndim=3)\n-\n-  def call(self, inputs):\n-    pad_axis = 2 if self.data_format == 'channels_last' else 3\n-    inputs = tf.expand_dims(inputs, pad_axis)\n-    outputs = self.pool_function(\n-        inputs,\n-        self.pool_size + (1,),\n-        strides=self.strides + (1,),\n-        padding=self.padding,\n-        data_format=self.data_format)\n-    return tf.squeeze(outputs, pad_axis)\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_first':\n-      steps = input_shape[2]\n-      features = input_shape[1]\n-    else:\n-      steps = input_shape[1]\n-      features = input_shape[2]\n-    length = conv_utils.conv_output_length(steps,\n-                                           self.pool_size[0],\n-                                           self.padding,\n-                                           self.strides[0])\n-    if self.data_format == 'channels_first':\n-      return tf.TensorShape([input_shape[0], features, length])\n-    else:\n-      return tf.TensorShape([input_shape[0], length, features])\n-\n-  def get_config(self):\n-    config = {\n-        'strides': self.strides,\n-        'pool_size': self.pool_size,\n-        'padding': self.padding,\n-        'data_format': self.data_format,\n-    }\n-    base_config = super(Pooling1D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.MaxPool1D', 'keras.layers.MaxPooling1D')\n-class MaxPooling1D(Pooling1D):\n-  \"\"\"Max pooling operation for 1D temporal data.\n-\n-  Downsamples the input representation by taking the maximum value over a\n-  spatial window of size `pool_size`. The window is shifted by `strides`.  The\n-  resulting output, when using the `\"valid\"` padding option, has a shape of:\n-  `output_shape = (input_shape - pool_size + 1) / strides)`\n-\n-  The resulting output shape when using the `\"same\"` padding option is:\n-  `output_shape = input_shape / strides`\n-\n-  For example, for `strides=1` and `padding=\"valid\"`:\n-\n-  >>> x = tf.constant([1., 2., 3., 4., 5.])\n-  >>> x = tf.reshape(x, [1, 5, 1])\n-  >>> max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,\n-  ...    strides=1, padding='valid')\n-  >>> max_pool_1d(x)\n-  <tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=\n-  array([[[2.],\n-          [3.],\n-          [4.],\n-          [5.]]], dtype=float32)>\n-\n-  For example, for `strides=2` and `padding=\"valid\"`:\n-\n-  >>> x = tf.constant([1., 2., 3., 4., 5.])\n-  >>> x = tf.reshape(x, [1, 5, 1])\n-  >>> max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,\n-  ...    strides=2, padding='valid')\n-  >>> max_pool_1d(x)\n-  <tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n-  array([[[2.],\n-          [4.]]], dtype=float32)>\n-\n-  For example, for `strides=1` and `padding=\"same\"`:\n-\n-  >>> x = tf.constant([1., 2., 3., 4., 5.])\n-  >>> x = tf.reshape(x, [1, 5, 1])\n-  >>> max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,\n-  ...    strides=1, padding='same')\n-  >>> max_pool_1d(x)\n-  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n-  array([[[2.],\n-          [3.],\n-          [4.],\n-          [5.],\n-          [5.]]], dtype=float32)>\n-\n-  Args:\n-    pool_size: Integer, size of the max pooling window.\n-    strides: Integer, or None. Specifies how much the pooling window moves\n-      for each pooling step.\n-      If None, it will default to `pool_size`.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, steps, features)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, features, steps)`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      3D tensor with shape `(batch_size, steps, features)`.\n-    - If `data_format='channels_first'`:\n-      3D tensor with shape `(batch_size, features, steps)`.\n-\n-  Output shape:\n-    - If `data_format='channels_last'`:\n-      3D tensor with shape `(batch_size, downsampled_steps, features)`.\n-    - If `data_format='channels_first'`:\n-      3D tensor with shape `(batch_size, features, downsampled_steps)`.\n-  \"\"\"\n-\n-  def __init__(self, pool_size=2, strides=None,\n-               padding='valid', data_format='channels_last', **kwargs):\n-\n-    super(MaxPooling1D, self).__init__(\n-        functools.partial(backend.pool2d, pool_mode='max'),\n-        pool_size=pool_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        **kwargs)\n-\n-\n-@keras_export('keras.layers.AveragePooling1D', 'keras.layers.AvgPool1D')\n-class AveragePooling1D(Pooling1D):\n-  \"\"\"Average pooling for temporal data.\n-\n-  Downsamples the input representation by taking the average value over the\n-  window defined by `pool_size`. The window is shifted by `strides`.  The\n-  resulting output when using \"valid\" padding option has a shape of:\n-  `output_shape = (input_shape - pool_size + 1) / strides)`\n-\n-  The resulting output shape when using the \"same\" padding option is:\n-  `output_shape = input_shape / strides`\n-\n-  For example, for strides=1 and padding=\"valid\":\n-\n-  >>> x = tf.constant([1., 2., 3., 4., 5.])\n-  >>> x = tf.reshape(x, [1, 5, 1])\n-  >>> x\n-  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n-    array([[[1.],\n-            [2.],\n-            [3.],\n-            [4.],\n-            [5.]], dtype=float32)>\n-  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n-  ...    strides=1, padding='valid')\n-  >>> avg_pool_1d(x)\n-  <tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=\n-  array([[[1.5],\n-          [2.5],\n-          [3.5],\n-          [4.5]]], dtype=float32)>\n-\n-  For example, for strides=2 and padding=\"valid\":\n-\n-  >>> x = tf.constant([1., 2., 3., 4., 5.])\n-  >>> x = tf.reshape(x, [1, 5, 1])\n-  >>> x\n-  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n-    array([[[1.],\n-            [2.],\n-            [3.],\n-            [4.],\n-            [5.]], dtype=float32)>\n-  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n-  ...    strides=2, padding='valid')\n-  >>> avg_pool_1d(x)\n-  <tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n-  array([[[1.5],\n-          [3.5]]], dtype=float32)>\n-\n-  For example, for strides=1 and padding=\"same\":\n-\n-  >>> x = tf.constant([1., 2., 3., 4., 5.])\n-  >>> x = tf.reshape(x, [1, 5, 1])\n-  >>> x\n-  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n-    array([[[1.],\n-            [2.],\n-            [3.],\n-            [4.],\n-            [5.]], dtype=float32)>\n-  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n-  ...    strides=1, padding='same')\n-  >>> avg_pool_1d(x)\n-  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n-  array([[[1.5],\n-          [2.5],\n-          [3.5],\n-          [4.5],\n-          [5.]]], dtype=float32)>\n-\n-  Args:\n-    pool_size: Integer, size of the average pooling windows.\n-    strides: Integer, or None. Factor by which to downscale.\n-      E.g. 2 will halve the input.\n-      If None, it will default to `pool_size`.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, steps, features)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, features, steps)`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      3D tensor with shape `(batch_size, steps, features)`.\n-    - If `data_format='channels_first'`:\n-      3D tensor with shape `(batch_size, features, steps)`.\n-\n-  Output shape:\n-    - If `data_format='channels_last'`:\n-      3D tensor with shape `(batch_size, downsampled_steps, features)`.\n-    - If `data_format='channels_first'`:\n-      3D tensor with shape `(batch_size, features, downsampled_steps)`.\n-  \"\"\"\n-\n-  def __init__(self, pool_size=2, strides=None,\n-               padding='valid', data_format='channels_last', **kwargs):\n-    super(AveragePooling1D, self).__init__(\n-        functools.partial(backend.pool2d, pool_mode='avg'),\n-        pool_size=pool_size,\n-        strides=strides,\n-        padding=padding,\n-        data_format=data_format,\n-        **kwargs)\n-\n-\n-class Pooling2D(Layer):\n-  \"\"\"Pooling layer for arbitrary pooling functions, for 2D inputs (e.g. images).\n-\n-  This class only exists for code reuse. It will never be an exposed API.\n-\n-  Args:\n-    pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n-    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n-      specifying the size of the pooling window.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    strides: An integer or tuple/list of 2 integers,\n-      specifying the strides of the pooling operation.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    padding: A string. The padding method, either 'valid' or 'same'.\n-      Case-insensitive.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, height, width, channels)` while `channels_first` corresponds to\n-      inputs with shape `(batch, channels, height, width)`.\n-    name: A string, the name of the layer.\n-  \"\"\"\n-\n-  def __init__(self, pool_function, pool_size, strides,\n-               padding='valid', data_format=None,\n-               name=None, **kwargs):\n-    super(Pooling2D, self).__init__(name=name, **kwargs)\n-    if data_format is None:\n-      data_format = backend.image_data_format()\n-    if strides is None:\n-      strides = pool_size\n-    self.pool_function = pool_function\n-    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n-    self.strides = conv_utils.normalize_tuple(\n-        strides, 2, 'strides', allow_zero=True)\n-    self.padding = conv_utils.normalize_padding(padding)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.input_spec = InputSpec(ndim=4)\n-\n-  def call(self, inputs):\n-    if self.data_format == 'channels_last':\n-      pool_shape = (1,) + self.pool_size + (1,)\n-      strides = (1,) + self.strides + (1,)\n-    else:\n-      pool_shape = (1, 1) + self.pool_size\n-      strides = (1, 1) + self.strides\n-    outputs = self.pool_function(\n-        inputs,\n-        ksize=pool_shape,\n-        strides=strides,\n-        padding=self.padding.upper(),\n-        data_format=conv_utils.convert_data_format(self.data_format, 4))\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_first':\n-      rows = input_shape[2]\n-      cols = input_shape[3]\n-    else:\n-      rows = input_shape[1]\n-      cols = input_shape[2]\n-    rows = conv_utils.conv_output_length(rows, self.pool_size[0], self.padding,\n-                                         self.strides[0])\n-    cols = conv_utils.conv_output_length(cols, self.pool_size[1], self.padding,\n-                                         self.strides[1])\n-    if self.data_format == 'channels_first':\n-      return tf.TensorShape(\n-          [input_shape[0], input_shape[1], rows, cols])\n-    else:\n-      return tf.TensorShape(\n-          [input_shape[0], rows, cols, input_shape[3]])\n-\n-  def get_config(self):\n-    config = {\n-        'pool_size': self.pool_size,\n-        'padding': self.padding,\n-        'strides': self.strides,\n-        'data_format': self.data_format\n-    }\n-    base_config = super(Pooling2D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.MaxPool2D', 'keras.layers.MaxPooling2D')\n-class MaxPooling2D(Pooling2D):\n-  \"\"\"Max pooling operation for 2D spatial data.\n-\n-  Downsamples the input along its spatial dimensions (height and width)\n-  by taking the maximum value over an input window\n-  (of size defined by `pool_size`) for each channel of the input.\n-  The window is shifted by `strides` along each dimension.\n-\n-  The resulting output,\n-  when using the `\"valid\"` padding option, has a spatial shape\n-  (number of rows or columns) of:\n-  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n-  (when `input_shape >= pool_size`)\n-\n-  The resulting output shape when using the `\"same\"` padding option is:\n-  `output_shape = math.floor((input_shape - 1) / strides) + 1`\n-\n-  For example, for `strides=(1, 1)` and `padding=\"valid\"`:\n-\n-  >>> x = tf.constant([[1., 2., 3.],\n-  ...                  [4., 5., 6.],\n-  ...                  [7., 8., 9.]])\n-  >>> x = tf.reshape(x, [1, 3, 3, 1])\n-  >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n-  ...    strides=(1, 1), padding='valid')\n-  >>> max_pool_2d(x)\n-  <tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n-    array([[[[5.],\n-             [6.]],\n-            [[8.],\n-             [9.]]]], dtype=float32)>\n-\n-  For example, for `strides=(2, 2)` and `padding=\"valid\"`:\n-\n-  >>> x = tf.constant([[1., 2., 3., 4.],\n-  ...                  [5., 6., 7., 8.],\n-  ...                  [9., 10., 11., 12.]])\n-  >>> x = tf.reshape(x, [1, 3, 4, 1])\n-  >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n-  ...    strides=(2, 2), padding='valid')\n-  >>> max_pool_2d(x)\n-  <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=\n-    array([[[[6.],\n-             [8.]]]], dtype=float32)>\n-\n-  Usage Example:\n-\n-  >>> input_image = tf.constant([[[[1.], [1.], [2.], [4.]],\n-  ...                            [[2.], [2.], [3.], [2.]],\n-  ...                            [[4.], [1.], [1.], [1.]],\n-  ...                            [[2.], [2.], [1.], [4.]]]])\n-  >>> output = tf.constant([[[[1], [0]],\n-  ...                       [[0], [1]]]])\n-  >>> model = tf.keras.models.Sequential()\n-  >>> model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n-  ...    input_shape=(4, 4, 1)))\n-  >>> model.compile('adam', 'mean_squared_error')\n-  >>> model.predict(input_image, steps=1)\n-  array([[[[2.],\n-           [4.]],\n-          [[4.],\n-           [4.]]]], dtype=float32)\n-\n-  For example, for stride=(1, 1) and padding=\"same\":\n-\n-  >>> x = tf.constant([[1., 2., 3.],\n-  ...                  [4., 5., 6.],\n-  ...                  [7., 8., 9.]])\n-  >>> x = tf.reshape(x, [1, 3, 3, 1])\n-  >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n-  ...    strides=(1, 1), padding='same')\n-  >>> max_pool_2d(x)\n-  <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n-    array([[[[5.],\n-             [6.],\n-             [6.]],\n-            [[8.],\n-             [9.],\n-             [9.]],\n-            [[8.],\n-             [9.],\n-             [9.]]]], dtype=float32)>\n-\n-  Args:\n-    pool_size: integer or tuple of 2 integers,\n-      window size over which to take the maximum.\n-      `(2, 2)` will take the max value over a 2x2 pooling window.\n-      If only one integer is specified, the same window length\n-      will be used for both dimensions.\n-    strides: Integer, tuple of 2 integers, or None.\n-      Strides values.  Specifies how far the pooling window moves\n-      for each pooling step. If None, it will default to `pool_size`.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, channels, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      4D tensor with shape `(batch_size, rows, cols, channels)`.\n-    - If `data_format='channels_first'`:\n-      4D tensor with shape `(batch_size, channels, rows, cols)`.\n-\n-  Output shape:\n-    - If `data_format='channels_last'`:\n-      4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n-    - If `data_format='channels_first'`:\n-      4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n-\n-  Returns:\n-    A tensor of rank 4 representing the maximum pooled values.  See above for\n-    output shape.\n-  \"\"\"\n-\n-  def __init__(self,\n-               pool_size=(2, 2),\n-               strides=None,\n-               padding='valid',\n-               data_format=None,\n-               **kwargs):\n-    super(MaxPooling2D, self).__init__(\n-        tf.compat.v1.nn.max_pool,\n-        pool_size=pool_size, strides=strides,\n-        padding=padding, data_format=data_format, **kwargs)\n-\n-\n-@keras_export('keras.layers.AveragePooling2D', 'keras.layers.AvgPool2D')\n-class AveragePooling2D(Pooling2D):\n-  \"\"\"Average pooling operation for spatial data.\n-\n-  Downsamples the input along its spatial dimensions (height and width)\n-  by taking the average value over an input window\n-  (of size defined by `pool_size`) for each channel of the input.\n-  The window is shifted by `strides` along each dimension.\n-\n-  The resulting output when using `\"valid\"` padding option has a shape\n-  (number of rows or columns) of:\n-  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n-  (when `input_shape >= pool_size`)\n-\n-  The resulting output shape when using the `\"same\"` padding option is:\n-  `output_shape = math.floor((input_shape - 1) / strides) + 1`\n-\n-  For example, for `strides=(1, 1)` and `padding=\"valid\"`:\n-\n-  >>> x = tf.constant([[1., 2., 3.],\n-  ...                  [4., 5., 6.],\n-  ...                  [7., 8., 9.]])\n-  >>> x = tf.reshape(x, [1, 3, 3, 1])\n-  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n-  ...    strides=(1, 1), padding='valid')\n-  >>> avg_pool_2d(x)\n-  <tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n-    array([[[[3.],\n-             [4.]],\n-            [[6.],\n-             [7.]]]], dtype=float32)>\n-\n-  For example, for `stride=(2, 2)` and `padding=\"valid\"`:\n-\n-  >>> x = tf.constant([[1., 2., 3., 4.],\n-  ...                  [5., 6., 7., 8.],\n-  ...                  [9., 10., 11., 12.]])\n-  >>> x = tf.reshape(x, [1, 3, 4, 1])\n-  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n-  ...    strides=(2, 2), padding='valid')\n-  >>> avg_pool_2d(x)\n-  <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=\n-    array([[[[3.5],\n-             [5.5]]]], dtype=float32)>\n-\n-  For example, for `strides=(1, 1)` and `padding=\"same\"`:\n-\n-  >>> x = tf.constant([[1., 2., 3.],\n-  ...                  [4., 5., 6.],\n-  ...                  [7., 8., 9.]])\n-  >>> x = tf.reshape(x, [1, 3, 3, 1])\n-  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n-  ...    strides=(1, 1), padding='same')\n-  >>> avg_pool_2d(x)\n-  <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n-    array([[[[3.],\n-             [4.],\n-             [4.5]],\n-            [[6.],\n-             [7.],\n-             [7.5]],\n-            [[7.5],\n-             [8.5],\n-             [9.]]]], dtype=float32)>\n-\n-  Args:\n-    pool_size: integer or tuple of 2 integers,\n-      factors by which to downscale (vertical, horizontal).\n-      `(2, 2)` will halve the input in both spatial dimension.\n-      If only one integer is specified, the same window length\n-      will be used for both dimensions.\n-    strides: Integer, tuple of 2 integers, or None.\n-      Strides values.\n-      If None, it will default to `pool_size`.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, channels, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      4D tensor with shape `(batch_size, rows, cols, channels)`.\n-    - If `data_format='channels_first'`:\n-      4D tensor with shape `(batch_size, channels, rows, cols)`.\n-\n-  Output shape:\n-    - If `data_format='channels_last'`:\n-      4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n-    - If `data_format='channels_first'`:\n-      4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               pool_size=(2, 2),\n-               strides=None,\n-               padding='valid',\n-               data_format=None,\n-               **kwargs):\n-    super(AveragePooling2D, self).__init__(\n-        tf.nn.avg_pool,\n-        pool_size=pool_size, strides=strides,\n-        padding=padding, data_format=data_format, **kwargs)\n-\n-\n-class Pooling3D(Layer):\n-  \"\"\"Pooling layer for arbitrary pooling functions, for 3D inputs.\n-\n-  This class only exists for code reuse. It will never be an exposed API.\n-\n-  Args:\n-    pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n-    pool_size: An integer or tuple/list of 3 integers:\n-      (pool_depth, pool_height, pool_width)\n-      specifying the size of the pooling window.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    strides: An integer or tuple/list of 3 integers,\n-      specifying the strides of the pooling operation.\n-      Can be a single integer to specify the same value for\n-      all spatial dimensions.\n-    padding: A string. The padding method, either 'valid' or 'same'.\n-      Case-insensitive.\n-    data_format: A string, one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, depth, height, width, channels)`\n-      while `channels_first` corresponds to\n-      inputs with shape `(batch, channels, depth, height, width)`.\n-    name: A string, the name of the layer.\n-  \"\"\"\n-\n-  def __init__(self, pool_function, pool_size, strides,\n-               padding='valid', data_format='channels_last',\n-               name=None, **kwargs):\n-    super(Pooling3D, self).__init__(name=name, **kwargs)\n-    if data_format is None:\n-      data_format = backend.image_data_format()\n-    if strides is None:\n-      strides = pool_size\n-    self.pool_function = pool_function\n-    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')\n-    self.strides = conv_utils.normalize_tuple(\n-        strides, 3, 'strides', allow_zero=True)\n-    self.padding = conv_utils.normalize_padding(padding)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.input_spec = InputSpec(ndim=5)\n-\n-  def call(self, inputs):\n-    pool_shape = (1,) + self.pool_size + (1,)\n-    strides = (1,) + self.strides + (1,)\n-\n-    if self.data_format == 'channels_first':\n-      # TF does not support `channels_first` with 3D pooling operations,\n-      # so we must handle this case manually.\n-      # TODO(fchollet): remove this when TF pooling is feature-complete.\n-      inputs = tf.transpose(inputs, (0, 2, 3, 4, 1))\n-\n-    outputs = self.pool_function(\n-        inputs,\n-        ksize=pool_shape,\n-        strides=strides,\n-        padding=self.padding.upper())\n-\n-    if self.data_format == 'channels_first':\n-      outputs = tf.transpose(outputs, (0, 4, 1, 2, 3))\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_first':\n-      len_dim1 = input_shape[2]\n-      len_dim2 = input_shape[3]\n-      len_dim3 = input_shape[4]\n-    else:\n-      len_dim1 = input_shape[1]\n-      len_dim2 = input_shape[2]\n-      len_dim3 = input_shape[3]\n-    len_dim1 = conv_utils.conv_output_length(len_dim1, self.pool_size[0],\n-                                             self.padding, self.strides[0])\n-    len_dim2 = conv_utils.conv_output_length(len_dim2, self.pool_size[1],\n-                                             self.padding, self.strides[1])\n-    len_dim3 = conv_utils.conv_output_length(len_dim3, self.pool_size[2],\n-                                             self.padding, self.strides[2])\n-    if self.data_format == 'channels_first':\n-      return tf.TensorShape(\n-          [input_shape[0], input_shape[1], len_dim1, len_dim2, len_dim3])\n-    else:\n-      return tf.TensorShape(\n-          [input_shape[0], len_dim1, len_dim2, len_dim3, input_shape[4]])\n-\n-  def get_config(self):\n-    config = {\n-        'pool_size': self.pool_size,\n-        'padding': self.padding,\n-        'strides': self.strides,\n-        'data_format': self.data_format\n-    }\n-    base_config = super(Pooling3D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.MaxPool3D', 'keras.layers.MaxPooling3D')\n-class MaxPooling3D(Pooling3D):\n-  \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n-\n-  Downsamples the input along its spatial dimensions (depth, height, and width)\n-  by taking the maximum value over an input window\n-  (of size defined by `pool_size`) for each channel of the input.\n-  The window is shifted by `strides` along each dimension.\n-\n-  Args:\n-    pool_size: Tuple of 3 integers,\n-      factors by which to downscale (dim1, dim2, dim3).\n-      `(2, 2, 2)` will halve the size of the 3D input in each dimension.\n-    strides: tuple of 3 integers, or None. Strides values.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-      while `channels_first` corresponds to inputs with shape\n-      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      5D tensor with shape:\n-      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-    - If `data_format='channels_first'`:\n-      5D tensor with shape:\n-      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n-\n-  Output shape:\n-    - If `data_format='channels_last'`:\n-      5D tensor with shape:\n-      `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n-    - If `data_format='channels_first'`:\n-      5D tensor with shape:\n-      `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n-\n-  Example:\n-\n-  ```python\n-  depth = 30\n-  height = 30\n-  width = 30\n-  input_channels = 3\n-\n-  inputs = tf.keras.Input(shape=(depth, height, width, input_channels))\n-  layer = tf.keras.layers.MaxPooling3D(pool_size=3)\n-  outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               pool_size=(2, 2, 2),\n-               strides=None,\n-               padding='valid',\n-               data_format=None,\n-               **kwargs):\n-    super(MaxPooling3D, self).__init__(\n-        tf.nn.max_pool3d,\n-        pool_size=pool_size, strides=strides,\n-        padding=padding, data_format=data_format, **kwargs)\n-\n-\n-@keras_export('keras.layers.AveragePooling3D', 'keras.layers.AvgPool3D')\n-class AveragePooling3D(Pooling3D):\n-  \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n-\n-  Downsamples the input along its spatial dimensions (depth, height, and width)\n-  by taking the average value over an input window\n-  (of size defined by `pool_size`) for each channel of the input.\n-  The window is shifted by `strides` along each dimension.\n-\n-  Args:\n-    pool_size: tuple of 3 integers,\n-      factors by which to downscale (dim1, dim2, dim3).\n-      `(2, 2, 2)` will halve the size of the 3D input in each dimension.\n-    strides: tuple of 3 integers, or None. Strides values.\n-    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n-      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n-      the left/right or up/down of the input such that output has the same\n-      height/width dimension as the input.\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-      while `channels_first` corresponds to inputs with shape\n-      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      5D tensor with shape:\n-      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-    - If `data_format='channels_first'`:\n-      5D tensor with shape:\n-      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n-\n-  Output shape:\n-    - If `data_format='channels_last'`:\n-      5D tensor with shape:\n-      `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n-    - If `data_format='channels_first'`:\n-      5D tensor with shape:\n-      `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n-\n-  Example:\n-\n-  ```python\n-  depth = 30\n-  height = 30\n-  width = 30\n-  input_channels = 3\n-\n-  inputs = tf.keras.Input(shape=(depth, height, width, input_channels))\n-  layer = tf.keras.layers.AveragePooling3D(pool_size=3)\n-  outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n-  ```\n-  \"\"\"\n-\n-  def __init__(self,\n-               pool_size=(2, 2, 2),\n-               strides=None,\n-               padding='valid',\n-               data_format=None,\n-               **kwargs):\n-    super(AveragePooling3D, self).__init__(\n-        tf.nn.avg_pool3d,\n-        pool_size=pool_size, strides=strides,\n-        padding=padding, data_format=data_format, **kwargs)\n-\n-\n-class GlobalPooling1D(Layer):\n-  \"\"\"Abstract class for different global pooling 1D layers.\"\"\"\n-\n-  def __init__(self, data_format='channels_last', keepdims=False, **kwargs):\n-    super(GlobalPooling1D, self).__init__(**kwargs)\n-    self.input_spec = InputSpec(ndim=3)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.keepdims = keepdims\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_first':\n-      if self.keepdims:\n-        return tf.TensorShape([input_shape[0], input_shape[1], 1])\n-      else:\n-        return tf.TensorShape([input_shape[0], input_shape[1]])\n-    else:\n-      if self.keepdims:\n-        return tf.TensorShape([input_shape[0], 1, input_shape[2]])\n-      else:\n-        return tf.TensorShape([input_shape[0], input_shape[2]])\n-\n-  def call(self, inputs):\n-    raise NotImplementedError\n-\n-  def get_config(self):\n-    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n-    base_config = super(GlobalPooling1D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.GlobalAveragePooling1D',\n-              'keras.layers.GlobalAvgPool1D')\n-class GlobalAveragePooling1D(GlobalPooling1D):\n-  \"\"\"Global average pooling operation for temporal data.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 3, 4)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.GlobalAveragePooling1D()(x)\n-  >>> print(y.shape)\n-  (2, 4)\n-\n-  Args:\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, steps, features)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, features, steps)`.\n-    keepdims: A boolean, whether to keep the temporal dimension or not.\n-      If `keepdims` is `False` (default), the rank of the tensor is reduced\n-      for spatial dimensions.\n-      If `keepdims` is `True`, the temporal dimension are retained with\n-      length 1.\n-      The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n-\n-  Call arguments:\n-    inputs: A 3D tensor.\n-    mask: Binary tensor of shape `(batch_size, steps)` indicating whether\n-      a given step should be masked (excluded from the average).\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      3D tensor with shape:\n-      `(batch_size, steps, features)`\n-    - If `data_format='channels_first'`:\n-      3D tensor with shape:\n-      `(batch_size, features, steps)`\n-\n-  Output shape:\n-    - If `keepdims`=False:\n-      2D tensor with shape `(batch_size, features)`.\n-    - If `keepdims`=True:\n-      - If `data_format='channels_last'`:\n-        3D tensor with shape `(batch_size, 1, features)`\n-      - If `data_format='channels_first'`:\n-        3D tensor with shape `(batch_size, features, 1)`\n-  \"\"\"\n-\n-  def __init__(self, data_format='channels_last', **kwargs):\n-    super(GlobalAveragePooling1D, self).__init__(data_format=data_format,\n-                                                 **kwargs)\n-    self.supports_masking = True\n-\n-  def call(self, inputs, mask=None):\n-    steps_axis = 1 if self.data_format == 'channels_last' else 2\n-    if mask is not None:\n-      mask = tf.cast(mask, inputs[0].dtype)\n-      mask = tf.expand_dims(\n-          mask, 2 if self.data_format == 'channels_last' else 1)\n-      inputs *= mask\n-      return backend.sum(\n-          inputs, axis=steps_axis,\n-          keepdims=self.keepdims) / tf.reduce_sum(\n-              mask, axis=steps_axis, keepdims=self.keepdims)\n-    else:\n-      return backend.mean(inputs, axis=steps_axis, keepdims=self.keepdims)\n-\n-  def compute_mask(self, inputs, mask=None):\n-    return None\n-\n-\n-@keras_export('keras.layers.GlobalMaxPool1D', 'keras.layers.GlobalMaxPooling1D')\n-class GlobalMaxPooling1D(GlobalPooling1D):\n-  \"\"\"Global max pooling operation for 1D temporal data.\n-\n-  Downsamples the input representation by taking the maximum value over\n-  the time dimension.\n-\n-  For example:\n-\n-  >>> x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n-  >>> x = tf.reshape(x, [3, 3, 1])\n-  >>> x\n-  <tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=\n-  array([[[1.], [2.], [3.]],\n-         [[4.], [5.], [6.]],\n-         [[7.], [8.], [9.]]], dtype=float32)>\n-  >>> max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()\n-  >>> max_pool_1d(x)\n-  <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n-  array([[3.],\n-         [6.],\n-         [9.], dtype=float32)>\n-\n-  Args:\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, steps, features)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, features, steps)`.\n-    keepdims: A boolean, whether to keep the temporal dimension or not.\n-      If `keepdims` is `False` (default), the rank of the tensor is reduced\n-      for spatial dimensions.\n-      If `keepdims` is `True`, the temporal dimension are retained with\n-      length 1.\n-      The behavior is the same as for `tf.reduce_max` or `np.max`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      3D tensor with shape:\n-      `(batch_size, steps, features)`\n-    - If `data_format='channels_first'`:\n-      3D tensor with shape:\n-      `(batch_size, features, steps)`\n-\n-  Output shape:\n-    - If `keepdims`=False:\n-      2D tensor with shape `(batch_size, features)`.\n-    - If `keepdims`=True:\n-      - If `data_format='channels_last'`:\n-        3D tensor with shape `(batch_size, 1, features)`\n-      - If `data_format='channels_first'`:\n-        3D tensor with shape `(batch_size, features, 1)`\n-  \"\"\"\n-\n-  def call(self, inputs):\n-    steps_axis = 1 if self.data_format == 'channels_last' else 2\n-    return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n-\n-\n-class GlobalPooling2D(Layer):\n-  \"\"\"Abstract class for different global pooling 2D layers.\n-  \"\"\"\n-\n-  def __init__(self, data_format=None, keepdims=False, **kwargs):\n-    super(GlobalPooling2D, self).__init__(**kwargs)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.input_spec = InputSpec(ndim=4)\n-    self.keepdims = keepdims\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_last':\n-      if self.keepdims:\n-        return tf.TensorShape([input_shape[0], 1, 1, input_shape[3]])\n-      else:\n-        return tf.TensorShape([input_shape[0], input_shape[3]])\n-    else:\n-      if self.keepdims:\n-        return tf.TensorShape([input_shape[0], input_shape[1], 1, 1])\n-      else:\n-        return tf.TensorShape([input_shape[0], input_shape[1]])\n-\n-  def call(self, inputs):\n-    raise NotImplementedError\n-\n-  def get_config(self):\n-    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n-    base_config = super(GlobalPooling2D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.GlobalAveragePooling2D',\n-              'keras.layers.GlobalAvgPool2D')\n-class GlobalAveragePooling2D(GlobalPooling2D):\n-  \"\"\"Global average pooling operation for spatial data.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 4, 5, 3)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.GlobalAveragePooling2D()(x)\n-  >>> print(y.shape)\n-  (2, 3)\n-\n-  Args:\n-      data_format: A string,\n-        one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n-        `channels_last` corresponds to inputs with shape\n-        `(batch, height, width, channels)` while `channels_first`\n-        corresponds to inputs with shape\n-        `(batch, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n-      keepdims: A boolean, whether to keep the spatial dimensions or not.\n-        If `keepdims` is `False` (default), the rank of the tensor is reduced\n-        for spatial dimensions.\n-        If `keepdims` is `True`, the spatial dimensions are retained with\n-        length 1.\n-        The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      4D tensor with shape `(batch_size, rows, cols, channels)`.\n-    - If `data_format='channels_first'`:\n-      4D tensor with shape `(batch_size, channels, rows, cols)`.\n-\n-  Output shape:\n-    - If `keepdims`=False:\n-      2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n-      - If `data_format='channels_last'`:\n-        4D tensor with shape `(batch_size, 1, 1, channels)`\n-      - If `data_format='channels_first'`:\n-        4D tensor with shape `(batch_size, channels, 1, 1)`\n-  \"\"\"\n-\n-  def call(self, inputs):\n-    if self.data_format == 'channels_last':\n-      return backend.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n-    else:\n-      return backend.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n-\n-\n-@keras_export('keras.layers.GlobalMaxPool2D', 'keras.layers.GlobalMaxPooling2D')\n-class GlobalMaxPooling2D(GlobalPooling2D):\n-  \"\"\"Global max pooling operation for spatial data.\n-\n-  Examples:\n-\n-  >>> input_shape = (2, 4, 5, 3)\n-  >>> x = tf.random.normal(input_shape)\n-  >>> y = tf.keras.layers.GlobalMaxPool2D()(x)\n-  >>> print(y.shape)\n-  (2, 3)\n-\n-  Args:\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, height, width, channels)` while `channels_first`\n-      corresponds to inputs with shape\n-      `(batch, channels, height, width)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    keepdims: A boolean, whether to keep the spatial dimensions or not.\n-      If `keepdims` is `False` (default), the rank of the tensor is reduced\n-      for spatial dimensions.\n-      If `keepdims` is `True`, the spatial dimensions are retained with\n-      length 1.\n-      The behavior is the same as for `tf.reduce_max` or `np.max`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      4D tensor with shape `(batch_size, rows, cols, channels)`.\n-    - If `data_format='channels_first'`:\n-      4D tensor with shape `(batch_size, channels, rows, cols)`.\n-\n-  Output shape:\n-    - If `keepdims`=False:\n-      2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n-      - If `data_format='channels_last'`:\n-        4D tensor with shape `(batch_size, 1, 1, channels)`\n-      - If `data_format='channels_first'`:\n-        4D tensor with shape `(batch_size, channels, 1, 1)`\n-  \"\"\"\n-\n-  def call(self, inputs):\n-    if self.data_format == 'channels_last':\n-      return backend.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n-    else:\n-      return backend.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n-\n-\n-class GlobalPooling3D(Layer):\n-  \"\"\"Abstract class for different global pooling 3D layers.\"\"\"\n-\n-  def __init__(self, data_format=None, keepdims=False, **kwargs):\n-    super(GlobalPooling3D, self).__init__(**kwargs)\n-    self.data_format = conv_utils.normalize_data_format(data_format)\n-    self.input_spec = InputSpec(ndim=5)\n-    self.keepdims = keepdims\n-\n-  def compute_output_shape(self, input_shape):\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    if self.data_format == 'channels_last':\n-      if self.keepdims:\n-        return tf.TensorShape(\n-            [input_shape[0], 1, 1, 1, input_shape[4]])\n-      else:\n-        return tf.TensorShape([input_shape[0], input_shape[4]])\n-    else:\n-      if self.keepdims:\n-        return tf.TensorShape(\n-            [input_shape[0], input_shape[1], 1, 1, 1])\n-      else:\n-        return tf.TensorShape([input_shape[0], input_shape[1]])\n-\n-  def call(self, inputs):\n-    raise NotImplementedError\n-\n-  def get_config(self):\n-    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n-    base_config = super(GlobalPooling3D, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export('keras.layers.GlobalAveragePooling3D',\n-              'keras.layers.GlobalAvgPool3D')\n-class GlobalAveragePooling3D(GlobalPooling3D):\n-  \"\"\"Global Average pooling operation for 3D data.\n-\n-  Args:\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-      while `channels_first` corresponds to inputs with shape\n-      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    keepdims: A boolean, whether to keep the spatial dimensions or not.\n-      If `keepdims` is `False` (default), the rank of the tensor is reduced\n-      for spatial dimensions.\n-      If `keepdims` is `True`, the spatial dimensions are retained with\n-      length 1.\n-      The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      5D tensor with shape:\n-      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-    - If `data_format='channels_first'`:\n-      5D tensor with shape:\n-      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n-\n-  Output shape:\n-    - If `keepdims`=False:\n-      2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n-      - If `data_format='channels_last'`:\n-        5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n-      - If `data_format='channels_first'`:\n-        5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n-  \"\"\"\n-\n-  def call(self, inputs):\n-    if self.data_format == 'channels_last':\n-      return backend.mean(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n-    else:\n-      return backend.mean(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n-\n-\n-@keras_export('keras.layers.GlobalMaxPool3D', 'keras.layers.GlobalMaxPooling3D')\n-class GlobalMaxPooling3D(GlobalPooling3D):\n-  \"\"\"Global Max pooling operation for 3D data.\n-\n-  Args:\n-    data_format: A string,\n-      one of `channels_last` (default) or `channels_first`.\n-      The ordering of the dimensions in the inputs.\n-      `channels_last` corresponds to inputs with shape\n-      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-      while `channels_first` corresponds to inputs with shape\n-      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-      It defaults to the `image_data_format` value found in your\n-      Keras config file at `~/.keras/keras.json`.\n-      If you never set it, then it will be \"channels_last\".\n-    keepdims: A boolean, whether to keep the spatial dimensions or not.\n-      If `keepdims` is `False` (default), the rank of the tensor is reduced\n-      for spatial dimensions.\n-      If `keepdims` is `True`, the spatial dimensions are retained with\n-      length 1.\n-      The behavior is the same as for `tf.reduce_max` or `np.max`.\n-\n-  Input shape:\n-    - If `data_format='channels_last'`:\n-      5D tensor with shape:\n-      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n-    - If `data_format='channels_first'`:\n-      5D tensor with shape:\n-      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n-\n-  Output shape:\n-    - If `keepdims`=False:\n-      2D tensor with shape `(batch_size, channels)`.\n-    - If `keepdims`=True:\n-      - If `data_format='channels_last'`:\n-        5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n-      - If `data_format='channels_first'`:\n-        5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n-  \"\"\"\n-\n-  def call(self, inputs):\n-    if self.data_format == 'channels_last':\n-      return backend.max(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n-    else:\n-      return backend.max(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n-\n-\n-# Aliases\n-\n-AvgPool1D = AveragePooling1D\n-MaxPool1D = MaxPooling1D\n-AvgPool2D = AveragePooling2D\n-MaxPool2D = MaxPooling2D\n-AvgPool3D = AveragePooling3D\n-MaxPool3D = MaxPooling3D\n-GlobalMaxPool1D = GlobalMaxPooling1D\n-GlobalMaxPool2D = GlobalMaxPooling2D\n-GlobalMaxPool3D = GlobalMaxPooling3D\n-GlobalAvgPool1D = GlobalAveragePooling1D\n-GlobalAvgPool2D = GlobalAveragePooling2D\n-GlobalAvgPool3D = GlobalAveragePooling3D\n\n@@ -0,0 +1,44 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Keras Pooling layers.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+# Pooling layers.\n+from keras.layers.pooling.average_pooling1d import AveragePooling1D\n+from keras.layers.pooling.average_pooling2d import AveragePooling2D\n+from keras.layers.pooling.average_pooling3d import AveragePooling3D\n+from keras.layers.pooling.max_pooling1d import MaxPooling1D\n+from keras.layers.pooling.max_pooling2d import MaxPooling2D\n+from keras.layers.pooling.max_pooling3d import MaxPooling3D\n+from keras.layers.pooling.global_average_pooling1d import GlobalAveragePooling1D\n+from keras.layers.pooling.global_average_pooling2d import GlobalAveragePooling2D\n+from keras.layers.pooling.global_average_pooling3d import GlobalAveragePooling3D\n+from keras.layers.pooling.global_max_pooling1d import GlobalMaxPooling1D\n+from keras.layers.pooling.global_max_pooling2d import GlobalMaxPooling2D\n+from keras.layers.pooling.global_max_pooling3d import GlobalMaxPooling3D\n+\n+# Pooling layer aliases.\n+from keras.layers.pooling.average_pooling1d import AvgPool1D\n+from keras.layers.pooling.average_pooling2d import AvgPool2D\n+from keras.layers.pooling.average_pooling3d import AvgPool3D\n+from keras.layers.pooling.max_pooling1d import MaxPool1D\n+from keras.layers.pooling.max_pooling2d import MaxPool2D\n+from keras.layers.pooling.max_pooling3d import MaxPool3D\n+from keras.layers.pooling.global_average_pooling1d import GlobalAvgPool1D\n+from keras.layers.pooling.global_average_pooling2d import GlobalAvgPool2D\n+from keras.layers.pooling.global_average_pooling3d import GlobalAvgPool3D\n+from keras.layers.pooling.global_max_pooling1d import GlobalMaxPool1D\n+from keras.layers.pooling.global_max_pooling2d import GlobalMaxPool2D\n+from keras.layers.pooling.global_max_pooling3d import GlobalMaxPool3D\n\n@@ -0,0 +1,140 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Average pooling 1D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import functools\n+\n+from keras import backend\n+from keras.layers.pooling.base_pooling1d import Pooling1D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.AveragePooling1D', 'keras.layers.AvgPool1D')\n+class AveragePooling1D(Pooling1D):\n+  \"\"\"Average pooling for temporal data.\n+\n+  Downsamples the input representation by taking the average value over the\n+  window defined by `pool_size`. The window is shifted by `strides`.  The\n+  resulting output when using \"valid\" padding option has a shape of:\n+  `output_shape = (input_shape - pool_size + 1) / strides)`\n+\n+  The resulting output shape when using the \"same\" padding option is:\n+  `output_shape = input_shape / strides`\n+\n+  For example, for strides=1 and padding=\"valid\":\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> x\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+    array([[[1.],\n+            [2.],\n+            [3.],\n+            [4.],\n+            [5.]], dtype=float32)>\n+  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n+  ...    strides=1, padding='valid')\n+  >>> avg_pool_1d(x)\n+  <tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=\n+  array([[[1.5],\n+          [2.5],\n+          [3.5],\n+          [4.5]]], dtype=float32)>\n+\n+  For example, for strides=2 and padding=\"valid\":\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> x\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+    array([[[1.],\n+            [2.],\n+            [3.],\n+            [4.],\n+            [5.]], dtype=float32)>\n+  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n+  ...    strides=2, padding='valid')\n+  >>> avg_pool_1d(x)\n+  <tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n+  array([[[1.5],\n+          [3.5]]], dtype=float32)>\n+\n+  For example, for strides=1 and padding=\"same\":\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> x\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+    array([[[1.],\n+            [2.],\n+            [3.],\n+            [4.],\n+            [5.]], dtype=float32)>\n+  >>> avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,\n+  ...    strides=1, padding='same')\n+  >>> avg_pool_1d(x)\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+  array([[[1.5],\n+          [2.5],\n+          [3.5],\n+          [4.5],\n+          [5.]]], dtype=float32)>\n+\n+  Args:\n+    pool_size: Integer, size of the average pooling windows.\n+    strides: Integer, or None. Factor by which to downscale.\n+      E.g. 2 will halve the input.\n+      If None, it will default to `pool_size`.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, steps, features)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, features, steps)`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      3D tensor with shape `(batch_size, steps, features)`.\n+    - If `data_format='channels_first'`:\n+      3D tensor with shape `(batch_size, features, steps)`.\n+\n+  Output shape:\n+    - If `data_format='channels_last'`:\n+      3D tensor with shape `(batch_size, downsampled_steps, features)`.\n+    - If `data_format='channels_first'`:\n+      3D tensor with shape `(batch_size, features, downsampled_steps)`.\n+  \"\"\"\n+\n+  def __init__(self, pool_size=2, strides=None,\n+               padding='valid', data_format='channels_last', **kwargs):\n+    super(AveragePooling1D, self).__init__(\n+        functools.partial(backend.pool2d, pool_mode='avg'),\n+        pool_size=pool_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        **kwargs)\n+\n+\n+# Alias\n+\n+AvgPool1D = AveragePooling1D\n\n@@ -0,0 +1,140 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Average pooling 2D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.layers.pooling.base_pooling2d import Pooling2D\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.AveragePooling2D', 'keras.layers.AvgPool2D')\n+class AveragePooling2D(Pooling2D):\n+  \"\"\"Average pooling operation for spatial data.\n+\n+  Downsamples the input along its spatial dimensions (height and width)\n+  by taking the average value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n+\n+  The resulting output when using `\"valid\"` padding option has a shape\n+  (number of rows or columns) of:\n+  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+  (when `input_shape >= pool_size`)\n+\n+  The resulting output shape when using the `\"same\"` padding option is:\n+  `output_shape = math.floor((input_shape - 1) / strides) + 1`\n+\n+  For example, for `strides=(1, 1)` and `padding=\"valid\"`:\n+\n+  >>> x = tf.constant([[1., 2., 3.],\n+  ...                  [4., 5., 6.],\n+  ...                  [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [1, 3, 3, 1])\n+  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n+  ...    strides=(1, 1), padding='valid')\n+  >>> avg_pool_2d(x)\n+  <tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n+    array([[[[3.],\n+             [4.]],\n+            [[6.],\n+             [7.]]]], dtype=float32)>\n+\n+  For example, for `stride=(2, 2)` and `padding=\"valid\"`:\n+\n+  >>> x = tf.constant([[1., 2., 3., 4.],\n+  ...                  [5., 6., 7., 8.],\n+  ...                  [9., 10., 11., 12.]])\n+  >>> x = tf.reshape(x, [1, 3, 4, 1])\n+  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n+  ...    strides=(2, 2), padding='valid')\n+  >>> avg_pool_2d(x)\n+  <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=\n+    array([[[[3.5],\n+             [5.5]]]], dtype=float32)>\n+\n+  For example, for `strides=(1, 1)` and `padding=\"same\"`:\n+\n+  >>> x = tf.constant([[1., 2., 3.],\n+  ...                  [4., 5., 6.],\n+  ...                  [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [1, 3, 3, 1])\n+  >>> avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),\n+  ...    strides=(1, 1), padding='same')\n+  >>> avg_pool_2d(x)\n+  <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n+    array([[[[3.],\n+             [4.],\n+             [4.5]],\n+            [[6.],\n+             [7.],\n+             [7.5]],\n+            [[7.5],\n+             [8.5],\n+             [9.]]]], dtype=float32)>\n+\n+  Args:\n+    pool_size: integer or tuple of 2 integers,\n+      factors by which to downscale (vertical, horizontal).\n+      `(2, 2)` will halve the input in both spatial dimension.\n+      If only one integer is specified, the same window length\n+      will be used for both dimensions.\n+    strides: Integer, tuple of 2 integers, or None.\n+      Strides values.\n+      If None, it will default to `pool_size`.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      4D tensor with shape `(batch_size, rows, cols, channels)`.\n+    - If `data_format='channels_first'`:\n+      4D tensor with shape `(batch_size, channels, rows, cols)`.\n+\n+  Output shape:\n+    - If `data_format='channels_last'`:\n+      4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n+    - If `data_format='channels_first'`:\n+      4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               pool_size=(2, 2),\n+               strides=None,\n+               padding='valid',\n+               data_format=None,\n+               **kwargs):\n+    super(AveragePooling2D, self).__init__(\n+        tf.nn.avg_pool,\n+        pool_size=pool_size, strides=strides,\n+        padding=padding, data_format=data_format, **kwargs)\n+\n+\n+# Alias\n+\n+AvgPool2D = AveragePooling2D\n\n@@ -0,0 +1,97 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Average pooling 3D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.layers.pooling.base_pooling3d import Pooling3D\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.AveragePooling3D', 'keras.layers.AvgPool3D')\n+class AveragePooling3D(Pooling3D):\n+  \"\"\"Average pooling operation for 3D data (spatial or spatio-temporal).\n+\n+  Downsamples the input along its spatial dimensions (depth, height, and width)\n+  by taking the average value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n+\n+  Args:\n+    pool_size: tuple of 3 integers,\n+      factors by which to downscale (dim1, dim2, dim3).\n+      `(2, 2, 2)` will halve the size of the 3D input in each dimension.\n+    strides: tuple of 3 integers, or None. Strides values.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      5D tensor with shape:\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+      5D tensor with shape:\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+  Output shape:\n+    - If `data_format='channels_last'`:\n+      5D tensor with shape:\n+      `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+      5D tensor with shape:\n+      `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n+\n+  Example:\n+\n+  ```python\n+  depth = 30\n+  height = 30\n+  width = 30\n+  input_channels = 3\n+\n+  inputs = tf.keras.Input(shape=(depth, height, width, input_channels))\n+  layer = tf.keras.layers.AveragePooling3D(pool_size=3)\n+  outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               pool_size=(2, 2, 2),\n+               strides=None,\n+               padding='valid',\n+               data_format=None,\n+               **kwargs):\n+    super(AveragePooling3D, self).__init__(\n+        tf.nn.avg_pool3d,\n+        pool_size=pool_size, strides=strides,\n+        padding=padding, data_format=data_format, **kwargs)\n+\n+\n+# Alias\n+\n+AvgPool3D = AveragePooling3D\n\n@@ -0,0 +1,98 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for average pooling layers.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import combinations\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  def test_average_pooling_1d(self):\n+    for padding in ['valid', 'same']:\n+      for stride in [1, 2]:\n+        testing_utils.layer_test(\n+            keras.layers.AveragePooling1D,\n+            kwargs={\n+                'strides': stride,\n+                'padding': padding\n+            },\n+            input_shape=(3, 5, 4))\n+\n+    testing_utils.layer_test(\n+        keras.layers.AveragePooling1D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 2, 6))\n+\n+  def test_average_pooling_2d(self):\n+    testing_utils.layer_test(\n+        keras.layers.AveragePooling2D,\n+        kwargs={\n+            'strides': (2, 2),\n+            'padding': 'same',\n+            'pool_size': (2, 2)\n+        },\n+        input_shape=(3, 5, 6, 4))\n+    testing_utils.layer_test(\n+        keras.layers.AveragePooling2D,\n+        kwargs={\n+            'strides': (2, 2),\n+            'padding': 'valid',\n+            'pool_size': (3, 3)\n+        },\n+        input_shape=(3, 5, 6, 4))\n+\n+    # This part of the test can only run on GPU but doesn't appear\n+    # to be properly assigned to a GPU when running in eager mode.\n+    if not tf.executing_eagerly():\n+      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n+      # TODO(b/62340061): Support channels_first on CPU.\n+      if tf.test.is_gpu_available(cuda_only=True):\n+        testing_utils.layer_test(\n+            keras.layers.AveragePooling2D,\n+            kwargs={\n+                'strides': (1, 1),\n+                'padding': 'valid',\n+                'pool_size': (2, 2),\n+                'data_format': 'channels_first'\n+            },\n+            input_shape=(3, 4, 5, 6))\n+\n+  def test_average_pooling_3d(self):\n+    pool_size = (3, 3, 3)\n+    testing_utils.layer_test(\n+        keras.layers.AveragePooling3D,\n+        kwargs={\n+            'strides': 2,\n+            'padding': 'valid',\n+            'pool_size': pool_size\n+        },\n+        input_shape=(3, 11, 12, 10, 4))\n+    testing_utils.layer_test(\n+        keras.layers.AveragePooling3D,\n+        kwargs={\n+            'strides': 3,\n+            'padding': 'valid',\n+            'data_format': 'channels_first',\n+            'pool_size': pool_size\n+        },\n+        input_shape=(3, 4, 11, 12, 10))\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,53 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for global pooling 1D layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class GlobalPooling1D(Layer):\n+  \"\"\"Abstract class for different global pooling 1D layers.\"\"\"\n+\n+  def __init__(self, data_format='channels_last', keepdims=False, **kwargs):\n+    super(GlobalPooling1D, self).__init__(**kwargs)\n+    self.input_spec = InputSpec(ndim=3)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.keepdims = keepdims\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], input_shape[1], 1])\n+      else:\n+        return tf.TensorShape([input_shape[0], input_shape[1]])\n+    else:\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], 1, input_shape[2]])\n+      else:\n+        return tf.TensorShape([input_shape[0], input_shape[2]])\n+\n+  def call(self, inputs):\n+    raise NotImplementedError\n+\n+  def get_config(self):\n+    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n+    base_config = super(GlobalPooling1D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n+\n\n@@ -0,0 +1,52 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for global pooling 2D layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class GlobalPooling2D(Layer):\n+  \"\"\"Abstract class for different global pooling 2D layers.\"\"\"\n+\n+  def __init__(self, data_format=None, keepdims=False, **kwargs):\n+    super(GlobalPooling2D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.input_spec = InputSpec(ndim=4)\n+    self.keepdims = keepdims\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_last':\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], 1, 1, input_shape[3]])\n+      else:\n+        return tf.TensorShape([input_shape[0], input_shape[3]])\n+    else:\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], input_shape[1], 1, 1])\n+      else:\n+        return tf.TensorShape([input_shape[0], input_shape[1]])\n+\n+  def call(self, inputs):\n+    raise NotImplementedError\n+\n+  def get_config(self):\n+    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n+    base_config = super(GlobalPooling2D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,54 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for global pooling 3D layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class GlobalPooling3D(Layer):\n+  \"\"\"Abstract class for different global pooling 3D layers.\"\"\"\n+\n+  def __init__(self, data_format=None, keepdims=False, **kwargs):\n+    super(GlobalPooling3D, self).__init__(**kwargs)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.input_spec = InputSpec(ndim=5)\n+    self.keepdims = keepdims\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_last':\n+      if self.keepdims:\n+        return tf.TensorShape(\n+            [input_shape[0], 1, 1, 1, input_shape[4]])\n+      else:\n+        return tf.TensorShape([input_shape[0], input_shape[4]])\n+    else:\n+      if self.keepdims:\n+        return tf.TensorShape(\n+            [input_shape[0], input_shape[1], 1, 1, 1])\n+      else:\n+        return tf.TensorShape([input_shape[0], input_shape[1]])\n+\n+  def call(self, inputs):\n+    raise NotImplementedError\n+\n+  def get_config(self):\n+    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n+    base_config = super(GlobalPooling3D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,100 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for pooling 1D layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class Pooling1D(Layer):\n+  \"\"\"Pooling layer for arbitrary pooling functions, for 1D inputs.\n+\n+  This class only exists for code reuse. It will never be an exposed API.\n+\n+  Args:\n+    pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n+    pool_size: An integer or tuple/list of a single integer,\n+      representing the size of the pooling window.\n+    strides: An integer or tuple/list of a single integer, specifying the\n+      strides of the pooling operation.\n+    padding: A string. The padding method, either 'valid' or 'same'.\n+      Case-insensitive.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, steps, features)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, features, steps)`.\n+    name: A string, the name of the layer.\n+  \"\"\"\n+\n+  def __init__(self, pool_function, pool_size, strides,\n+               padding='valid', data_format='channels_last',\n+               name=None, **kwargs):\n+    super(Pooling1D, self).__init__(name=name, **kwargs)\n+    if data_format is None:\n+      data_format = backend.image_data_format()\n+    if strides is None:\n+      strides = pool_size\n+    self.pool_function = pool_function\n+    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')\n+    self.strides = conv_utils.normalize_tuple(\n+        strides, 1, 'strides', allow_zero=True)\n+    self.padding = conv_utils.normalize_padding(padding)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.input_spec = InputSpec(ndim=3)\n+\n+  def call(self, inputs):\n+    pad_axis = 2 if self.data_format == 'channels_last' else 3\n+    inputs = tf.expand_dims(inputs, pad_axis)\n+    outputs = self.pool_function(\n+        inputs,\n+        self.pool_size + (1,),\n+        strides=self.strides + (1,),\n+        padding=self.padding,\n+        data_format=self.data_format)\n+    return tf.squeeze(outputs, pad_axis)\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      steps = input_shape[2]\n+      features = input_shape[1]\n+    else:\n+      steps = input_shape[1]\n+      features = input_shape[2]\n+    length = conv_utils.conv_output_length(steps,\n+                                           self.pool_size[0],\n+                                           self.padding,\n+                                           self.strides[0])\n+    if self.data_format == 'channels_first':\n+      return tf.TensorShape([input_shape[0], features, length])\n+    else:\n+      return tf.TensorShape([input_shape[0], length, features])\n+\n+  def get_config(self):\n+    config = {\n+        'strides': self.strides,\n+        'pool_size': self.pool_size,\n+        'padding': self.padding,\n+        'data_format': self.data_format,\n+    }\n+    base_config = super(Pooling1D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,108 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for pooling 2D layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class Pooling2D(Layer):\n+  \"\"\"Pooling layer for arbitrary pooling functions, for 2D inputs (e.g. images).\n+\n+  This class only exists for code reuse. It will never be an exposed API.\n+\n+  Args:\n+    pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n+    pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n+      specifying the size of the pooling window.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    strides: An integer or tuple/list of 2 integers,\n+      specifying the strides of the pooling operation.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    padding: A string. The padding method, either 'valid' or 'same'.\n+      Case-insensitive.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, height, width, channels)` while `channels_first` corresponds to\n+      inputs with shape `(batch, channels, height, width)`.\n+    name: A string, the name of the layer.\n+  \"\"\"\n+\n+  def __init__(self, pool_function, pool_size, strides,\n+               padding='valid', data_format=None,\n+               name=None, **kwargs):\n+    super(Pooling2D, self).__init__(name=name, **kwargs)\n+    if data_format is None:\n+      data_format = backend.image_data_format()\n+    if strides is None:\n+      strides = pool_size\n+    self.pool_function = pool_function\n+    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n+    self.strides = conv_utils.normalize_tuple(\n+        strides, 2, 'strides', allow_zero=True)\n+    self.padding = conv_utils.normalize_padding(padding)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.input_spec = InputSpec(ndim=4)\n+\n+  def call(self, inputs):\n+    if self.data_format == 'channels_last':\n+      pool_shape = (1,) + self.pool_size + (1,)\n+      strides = (1,) + self.strides + (1,)\n+    else:\n+      pool_shape = (1, 1) + self.pool_size\n+      strides = (1, 1) + self.strides\n+    outputs = self.pool_function(\n+        inputs,\n+        ksize=pool_shape,\n+        strides=strides,\n+        padding=self.padding.upper(),\n+        data_format=conv_utils.convert_data_format(self.data_format, 4))\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      rows = input_shape[2]\n+      cols = input_shape[3]\n+    else:\n+      rows = input_shape[1]\n+      cols = input_shape[2]\n+    rows = conv_utils.conv_output_length(rows, self.pool_size[0], self.padding,\n+                                         self.strides[0])\n+    cols = conv_utils.conv_output_length(cols, self.pool_size[1], self.padding,\n+                                         self.strides[1])\n+    if self.data_format == 'channels_first':\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], rows, cols])\n+    else:\n+      return tf.TensorShape(\n+          [input_shape[0], rows, cols, input_shape[3]])\n+\n+  def get_config(self):\n+    config = {\n+        'pool_size': self.pool_size,\n+        'padding': self.padding,\n+        'strides': self.strides,\n+        'data_format': self.data_format\n+    }\n+    base_config = super(Pooling2D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,119 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Private base class for pooling 3D layers.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import backend\n+from keras.engine.base_layer import Layer\n+from keras.engine.input_spec import InputSpec\n+from keras.utils import conv_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+class Pooling3D(Layer):\n+  \"\"\"Pooling layer for arbitrary pooling functions, for 3D inputs.\n+\n+  This class only exists for code reuse. It will never be an exposed API.\n+\n+  Args:\n+    pool_function: The pooling function to apply, e.g. `tf.nn.max_pool2d`.\n+    pool_size: An integer or tuple/list of 3 integers:\n+      (pool_depth, pool_height, pool_width)\n+      specifying the size of the pooling window.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    strides: An integer or tuple/list of 3 integers,\n+      specifying the strides of the pooling operation.\n+      Can be a single integer to specify the same value for\n+      all spatial dimensions.\n+    padding: A string. The padding method, either 'valid' or 'same'.\n+      Case-insensitive.\n+    data_format: A string, one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, depth, height, width, channels)`\n+      while `channels_first` corresponds to\n+      inputs with shape `(batch, channels, depth, height, width)`.\n+    name: A string, the name of the layer.\n+  \"\"\"\n+\n+  def __init__(self, pool_function, pool_size, strides,\n+               padding='valid', data_format='channels_last',\n+               name=None, **kwargs):\n+    super(Pooling3D, self).__init__(name=name, **kwargs)\n+    if data_format is None:\n+      data_format = backend.image_data_format()\n+    if strides is None:\n+      strides = pool_size\n+    self.pool_function = pool_function\n+    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')\n+    self.strides = conv_utils.normalize_tuple(\n+        strides, 3, 'strides', allow_zero=True)\n+    self.padding = conv_utils.normalize_padding(padding)\n+    self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.input_spec = InputSpec(ndim=5)\n+\n+  def call(self, inputs):\n+    pool_shape = (1,) + self.pool_size + (1,)\n+    strides = (1,) + self.strides + (1,)\n+\n+    if self.data_format == 'channels_first':\n+      # TF does not support `channels_first` with 3D pooling operations,\n+      # so we must handle this case manually.\n+      # TODO(fchollet): remove this when TF pooling is feature-complete.\n+      inputs = tf.transpose(inputs, (0, 2, 3, 4, 1))\n+\n+    outputs = self.pool_function(\n+        inputs,\n+        ksize=pool_shape,\n+        strides=strides,\n+        padding=self.padding.upper())\n+\n+    if self.data_format == 'channels_first':\n+      outputs = tf.transpose(outputs, (0, 4, 1, 2, 3))\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    input_shape = tf.TensorShape(input_shape).as_list()\n+    if self.data_format == 'channels_first':\n+      len_dim1 = input_shape[2]\n+      len_dim2 = input_shape[3]\n+      len_dim3 = input_shape[4]\n+    else:\n+      len_dim1 = input_shape[1]\n+      len_dim2 = input_shape[2]\n+      len_dim3 = input_shape[3]\n+    len_dim1 = conv_utils.conv_output_length(len_dim1, self.pool_size[0],\n+                                             self.padding, self.strides[0])\n+    len_dim2 = conv_utils.conv_output_length(len_dim2, self.pool_size[1],\n+                                             self.padding, self.strides[1])\n+    len_dim3 = conv_utils.conv_output_length(len_dim3, self.pool_size[2],\n+                                             self.padding, self.strides[2])\n+    if self.data_format == 'channels_first':\n+      return tf.TensorShape(\n+          [input_shape[0], input_shape[1], len_dim1, len_dim2, len_dim3])\n+    else:\n+      return tf.TensorShape(\n+          [input_shape[0], len_dim1, len_dim2, len_dim3, input_shape[4]])\n+\n+  def get_config(self):\n+    config = {\n+        'pool_size': self.pool_size,\n+        'padding': self.padding,\n+        'strides': self.strides,\n+        'data_format': self.data_format\n+    }\n+    base_config = super(Pooling3D, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,101 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Global average pooling 1D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.pooling.base_global_pooling1d import GlobalPooling1D\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GlobalAveragePooling1D',\n+              'keras.layers.GlobalAvgPool1D')\n+class GlobalAveragePooling1D(GlobalPooling1D):\n+  \"\"\"Global average pooling operation for temporal data.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 3, 4)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.GlobalAveragePooling1D()(x)\n+  >>> print(y.shape)\n+  (2, 4)\n+\n+  Args:\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, steps, features)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, features, steps)`.\n+    keepdims: A boolean, whether to keep the temporal dimension or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the temporal dimension are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+  Call arguments:\n+    inputs: A 3D tensor.\n+    mask: Binary tensor of shape `(batch_size, steps)` indicating whether\n+      a given step should be masked (excluded from the average).\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      3D tensor with shape:\n+      `(batch_size, steps, features)`\n+    - If `data_format='channels_first'`:\n+      3D tensor with shape:\n+      `(batch_size, features, steps)`\n+\n+  Output shape:\n+    - If `keepdims`=False:\n+      2D tensor with shape `(batch_size, features)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        3D tensor with shape `(batch_size, 1, features)`\n+      - If `data_format='channels_first'`:\n+        3D tensor with shape `(batch_size, features, 1)`\n+  \"\"\"\n+\n+  def __init__(self, data_format='channels_last', **kwargs):\n+    super(GlobalAveragePooling1D, self).__init__(data_format=data_format,\n+                                                 **kwargs)\n+    self.supports_masking = True\n+\n+  def call(self, inputs, mask=None):\n+    steps_axis = 1 if self.data_format == 'channels_last' else 2\n+    if mask is not None:\n+      mask = tf.cast(mask, inputs[0].dtype)\n+      mask = tf.expand_dims(\n+          mask, 2 if self.data_format == 'channels_last' else 1)\n+      inputs *= mask\n+      return backend.sum(\n+          inputs, axis=steps_axis,\n+          keepdims=self.keepdims) / tf.reduce_sum(\n+              mask, axis=steps_axis, keepdims=self.keepdims)\n+    else:\n+      return backend.mean(inputs, axis=steps_axis, keepdims=self.keepdims)\n+\n+  def compute_mask(self, inputs, mask=None):\n+    return None\n+\n+\n+# Alias\n+\n+GlobalAvgPool1D = GlobalAveragePooling1D\n\n@@ -0,0 +1,80 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Global average pooling 2D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.pooling.base_global_pooling2d import GlobalPooling2D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GlobalAveragePooling2D',\n+              'keras.layers.GlobalAvgPool2D')\n+class GlobalAveragePooling2D(GlobalPooling2D):\n+  \"\"\"Global average pooling operation for spatial data.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 4, 5, 3)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.GlobalAveragePooling2D()(x)\n+  >>> print(y.shape)\n+  (2, 3)\n+\n+  Args:\n+      data_format: A string,\n+        one of `channels_last` (default) or `channels_first`.\n+        The ordering of the dimensions in the inputs.\n+        `channels_last` corresponds to inputs with shape\n+        `(batch, height, width, channels)` while `channels_first`\n+        corresponds to inputs with shape\n+        `(batch, channels, height, width)`.\n+        It defaults to the `image_data_format` value found in your\n+        Keras config file at `~/.keras/keras.json`.\n+        If you never set it, then it will be \"channels_last\".\n+      keepdims: A boolean, whether to keep the spatial dimensions or not.\n+        If `keepdims` is `False` (default), the rank of the tensor is reduced\n+        for spatial dimensions.\n+        If `keepdims` is `True`, the spatial dimensions are retained with\n+        length 1.\n+        The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      4D tensor with shape `(batch_size, rows, cols, channels)`.\n+    - If `data_format='channels_first'`:\n+      4D tensor with shape `(batch_size, channels, rows, cols)`.\n+\n+  Output shape:\n+    - If `keepdims`=False:\n+      2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        4D tensor with shape `(batch_size, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        4D tensor with shape `(batch_size, channels, 1, 1)`\n+  \"\"\"\n+\n+  def call(self, inputs):\n+    if self.data_format == 'channels_last':\n+      return backend.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n+    else:\n+      return backend.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n+\n+\n+# Alias\n+\n+GlobalAvgPool2D = GlobalAveragePooling2D\n\n@@ -0,0 +1,74 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Global average pooling 3D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.pooling.base_global_pooling3d import GlobalPooling3D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GlobalAveragePooling3D',\n+              'keras.layers.GlobalAvgPool3D')\n+class GlobalAveragePooling3D(GlobalPooling3D):\n+  \"\"\"Global Average pooling operation for 3D data.\n+\n+  Args:\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    keepdims: A boolean, whether to keep the spatial dimensions or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the spatial dimensions are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      5D tensor with shape:\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+      5D tensor with shape:\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+  Output shape:\n+    - If `keepdims`=False:\n+      2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n+  \"\"\"\n+\n+  def call(self, inputs):\n+    if self.data_format == 'channels_last':\n+      return backend.mean(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n+    else:\n+      return backend.mean(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n+\n+\n+# Alias\n+\n+GlobalAvgPool3D = GlobalAveragePooling3D\n\n@@ -0,0 +1,144 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for global average pooling layers.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import combinations\n+from keras import testing_utils\n+from keras.mixed_precision import policy\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class GlobalAveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  @testing_utils.enable_v2_dtype_behavior\n+  def test_mixed_float16_policy(self):\n+    with policy.policy_scope('mixed_float16'):\n+      inputs1 = keras.Input(shape=(36, 512), dtype='float16')\n+      inputs2 = keras.Input(shape=(36,), dtype='bool')\n+      average_layer = keras.layers.GlobalAveragePooling1D()\n+      _ = average_layer(inputs1, inputs2)\n+\n+  def test_global_average_pooling_1d(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling1D, input_shape=(3, 4, 5))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling1D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 4, 5))\n+\n+  def test_global_average_pooling_1d_masking_support(self):\n+    model = keras.Sequential()\n+    model.add(keras.layers.Masking(mask_value=0., input_shape=(None, 4)))\n+    model.add(keras.layers.GlobalAveragePooling1D())\n+    model.compile(loss='mae', optimizer='rmsprop')\n+\n+    model_input = np.random.random((2, 3, 4))\n+    model_input[0, 1:, :] = 0\n+    output = model.predict(model_input)\n+    self.assertAllClose(output[0], model_input[0, 0, :])\n+\n+  def test_global_average_pooling_1d_with_ragged(self):\n+    ragged_data = tf.ragged.constant(\n+        [[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], [[1.0, 1.0], [2.0, 2.0]]],\n+        ragged_rank=1)\n+    dense_data = ragged_data.to_tensor()\n+\n+    inputs = keras.Input(shape=(None, 2), dtype='float32', ragged=True)\n+    out = keras.layers.GlobalAveragePooling1D()(inputs)\n+    model = keras.models.Model(inputs=inputs, outputs=out)\n+    output_ragged = model.predict(ragged_data, steps=1)\n+\n+    inputs = keras.Input(shape=(None, 2), dtype='float32')\n+    masking = keras.layers.Masking(mask_value=0., input_shape=(3, 2))(inputs)\n+    out = keras.layers.GlobalAveragePooling1D()(masking)\n+    model = keras.models.Model(inputs=inputs, outputs=out)\n+    output_dense = model.predict(dense_data, steps=1)\n+\n+    self.assertAllEqual(output_ragged, output_dense)\n+\n+  def test_global_average_pooling_2d(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling2D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 4, 5, 6))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling2D,\n+        kwargs={'data_format': 'channels_last'},\n+        input_shape=(3, 5, 6, 4))\n+\n+  def test_global_average_pooling_3d(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling3D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 4, 3, 4, 3))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling3D,\n+        kwargs={'data_format': 'channels_last'},\n+        input_shape=(3, 4, 3, 4, 3))\n+\n+  def test_global_average_pooling_1d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling1D,\n+        kwargs={'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 1, 5))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling1D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 4, 1))\n+\n+  def test_global_average_pooling_2d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling2D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 4, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling2D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 1, 1, 6))\n+\n+  def test_global_average_pooling_3d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling3D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 4, 1, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalAveragePooling3D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 1, 1, 1, 3))\n+\n+  def test_global_average_pooling_1d_keepdims_masking_support(self):\n+    model = keras.Sequential()\n+    model.add(keras.layers.Masking(mask_value=0., input_shape=(None, 4)))\n+    model.add(keras.layers.GlobalAveragePooling1D(keepdims=True))\n+    model.compile(loss='mae', optimizer='rmsprop')\n+\n+    model_input = np.random.random((2, 3, 4))\n+    model_input[0, 1:, :] = 0\n+    output = model.predict(model_input)\n+    self.assertAllEqual((2, 1, 4), output.shape)\n+    self.assertAllClose(output[0, 0], model_input[0, 0, :])\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,87 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Global max pooling 1D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.pooling.base_global_pooling1d import GlobalPooling1D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GlobalMaxPool1D', 'keras.layers.GlobalMaxPooling1D')\n+class GlobalMaxPooling1D(GlobalPooling1D):\n+  \"\"\"Global max pooling operation for 1D temporal data.\n+\n+  Downsamples the input representation by taking the maximum value over\n+  the time dimension.\n+\n+  For example:\n+\n+  >>> x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [3, 3, 1])\n+  >>> x\n+  <tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=\n+  array([[[1.], [2.], [3.]],\n+         [[4.], [5.], [6.]],\n+         [[7.], [8.], [9.]]], dtype=float32)>\n+  >>> max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()\n+  >>> max_pool_1d(x)\n+  <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n+  array([[3.],\n+         [6.],\n+         [9.], dtype=float32)>\n+\n+  Args:\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, steps, features)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, features, steps)`.\n+    keepdims: A boolean, whether to keep the temporal dimension or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the temporal dimension are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_max` or `np.max`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      3D tensor with shape:\n+      `(batch_size, steps, features)`\n+    - If `data_format='channels_first'`:\n+      3D tensor with shape:\n+      `(batch_size, features, steps)`\n+\n+  Output shape:\n+    - If `keepdims`=False:\n+      2D tensor with shape `(batch_size, features)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        3D tensor with shape `(batch_size, 1, features)`\n+      - If `data_format='channels_first'`:\n+        3D tensor with shape `(batch_size, features, 1)`\n+  \"\"\"\n+\n+  def call(self, inputs):\n+    steps_axis = 1 if self.data_format == 'channels_last' else 2\n+    return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n+\n+\n+# Alias\n+\n+GlobalMaxPool1D = GlobalMaxPooling1D\n\n@@ -0,0 +1,79 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Global max pooling 2D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.pooling.base_global_pooling2d import GlobalPooling2D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GlobalMaxPool2D', 'keras.layers.GlobalMaxPooling2D')\n+class GlobalMaxPooling2D(GlobalPooling2D):\n+  \"\"\"Global max pooling operation for spatial data.\n+\n+  Examples:\n+\n+  >>> input_shape = (2, 4, 5, 3)\n+  >>> x = tf.random.normal(input_shape)\n+  >>> y = tf.keras.layers.GlobalMaxPool2D()(x)\n+  >>> print(y.shape)\n+  (2, 3)\n+\n+  Args:\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    keepdims: A boolean, whether to keep the spatial dimensions or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the spatial dimensions are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_max` or `np.max`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      4D tensor with shape `(batch_size, rows, cols, channels)`.\n+    - If `data_format='channels_first'`:\n+      4D tensor with shape `(batch_size, channels, rows, cols)`.\n+\n+  Output shape:\n+    - If `keepdims`=False:\n+      2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        4D tensor with shape `(batch_size, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        4D tensor with shape `(batch_size, channels, 1, 1)`\n+  \"\"\"\n+\n+  def call(self, inputs):\n+    if self.data_format == 'channels_last':\n+      return backend.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n+    else:\n+      return backend.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n+\n+\n+# Alias\n+\n+GlobalMaxPool2D = GlobalMaxPooling2D\n\n@@ -0,0 +1,73 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Global max pooling 3D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras import backend\n+from keras.layers.pooling.base_global_pooling3d import GlobalPooling3D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.GlobalMaxPool3D', 'keras.layers.GlobalMaxPooling3D')\n+class GlobalMaxPooling3D(GlobalPooling3D):\n+  \"\"\"Global Max pooling operation for 3D data.\n+\n+  Args:\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+    keepdims: A boolean, whether to keep the spatial dimensions or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the spatial dimensions are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_max` or `np.max`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      5D tensor with shape:\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+      5D tensor with shape:\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+  Output shape:\n+    - If `keepdims`=False:\n+      2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n+  \"\"\"\n+\n+  def call(self, inputs):\n+    if self.data_format == 'channels_last':\n+      return backend.max(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n+    else:\n+      return backend.max(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n+\n+\n+# Alias\n+\n+GlobalMaxPool3D = GlobalMaxPooling3D\n\n@@ -0,0 +1,111 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for global max pooling layers.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import combinations\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class GlobalMaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  def test_global_max_pooling_1d(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling1D, input_shape=(3, 4, 5))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling1D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 4, 5))\n+\n+  def test_global_max_pooling_2d_with_ragged(self):\n+    ragged_data = tf.ragged.constant(\n+        [[[[1.0], [1.0]], [[2.0], [2.0]], [[3.0], [3.0]]],\n+         [[[1.0], [1.0]], [[2.0], [2.0]]]],\n+        ragged_rank=1)\n+    dense_data = ragged_data.to_tensor()\n+\n+    inputs = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)\n+    out = keras.layers.GlobalMaxPooling2D()(inputs)\n+    model = keras.models.Model(inputs=inputs, outputs=out)\n+    output_ragged = model.predict(ragged_data, steps=1)\n+\n+    inputs = keras.Input(shape=(None, 2, 1), dtype='float32')\n+    out = keras.layers.GlobalMaxPooling2D()(inputs)\n+    model = keras.models.Model(inputs=inputs, outputs=out)\n+    output_dense = model.predict(dense_data, steps=1)\n+\n+    self.assertAllEqual(output_ragged, output_dense)\n+\n+  def test_global_max_pooling_2d(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling2D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 4, 5, 6))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling2D,\n+        kwargs={'data_format': 'channels_last'},\n+        input_shape=(3, 5, 6, 4))\n+\n+  def test_global_maxpooling_3d(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling3D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 4, 3, 4, 3))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling3D,\n+        kwargs={'data_format': 'channels_last'},\n+        input_shape=(3, 4, 3, 4, 3))\n+\n+  def test_global_max_pooling_1d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling1D,\n+        kwargs={'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 1, 5))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling1D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 4, 1))\n+\n+  def test_global_max_pooling_2d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling2D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 4, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling2D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 1, 1, 6))\n+\n+  def test_global_max_pooling_3d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling3D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 4, 1, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.GlobalMaxPooling3D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 1, 1, 1, 3))\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -0,0 +1,120 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Max pooling 1D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+import functools\n+\n+from keras import backend\n+from keras.layers.pooling.base_pooling1d import Pooling1D\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.MaxPool1D', 'keras.layers.MaxPooling1D')\n+class MaxPooling1D(Pooling1D):\n+  \"\"\"Max pooling operation for 1D temporal data.\n+\n+  Downsamples the input representation by taking the maximum value over a\n+  spatial window of size `pool_size`. The window is shifted by `strides`.  The\n+  resulting output, when using the `\"valid\"` padding option, has a shape of:\n+  `output_shape = (input_shape - pool_size + 1) / strides)`\n+\n+  The resulting output shape when using the `\"same\"` padding option is:\n+  `output_shape = input_shape / strides`\n+\n+  For example, for `strides=1` and `padding=\"valid\"`:\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,\n+  ...    strides=1, padding='valid')\n+  >>> max_pool_1d(x)\n+  <tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=\n+  array([[[2.],\n+          [3.],\n+          [4.],\n+          [5.]]], dtype=float32)>\n+\n+  For example, for `strides=2` and `padding=\"valid\"`:\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,\n+  ...    strides=2, padding='valid')\n+  >>> max_pool_1d(x)\n+  <tf.Tensor: shape=(1, 2, 1), dtype=float32, numpy=\n+  array([[[2.],\n+          [4.]]], dtype=float32)>\n+\n+  For example, for `strides=1` and `padding=\"same\"`:\n+\n+  >>> x = tf.constant([1., 2., 3., 4., 5.])\n+  >>> x = tf.reshape(x, [1, 5, 1])\n+  >>> max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,\n+  ...    strides=1, padding='same')\n+  >>> max_pool_1d(x)\n+  <tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=\n+  array([[[2.],\n+          [3.],\n+          [4.],\n+          [5.],\n+          [5.]]], dtype=float32)>\n+\n+  Args:\n+    pool_size: Integer, size of the max pooling window.\n+    strides: Integer, or None. Specifies how much the pooling window moves\n+      for each pooling step.\n+      If None, it will default to `pool_size`.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, steps, features)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, features, steps)`.\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      3D tensor with shape `(batch_size, steps, features)`.\n+    - If `data_format='channels_first'`:\n+      3D tensor with shape `(batch_size, features, steps)`.\n+\n+  Output shape:\n+    - If `data_format='channels_last'`:\n+      3D tensor with shape `(batch_size, downsampled_steps, features)`.\n+    - If `data_format='channels_first'`:\n+      3D tensor with shape `(batch_size, features, downsampled_steps)`.\n+  \"\"\"\n+\n+  def __init__(self, pool_size=2, strides=None,\n+               padding='valid', data_format='channels_last', **kwargs):\n+\n+    super(MaxPooling1D, self).__init__(\n+        functools.partial(backend.pool2d, pool_mode='max'),\n+        pool_size=pool_size,\n+        strides=strides,\n+        padding=padding,\n+        data_format=data_format,\n+        **kwargs)\n+\n+\n+# Alias\n+\n+MaxPool1D = MaxPooling1D\n\n@@ -0,0 +1,163 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Max pooling 2D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.layers.pooling.base_pooling2d import Pooling2D\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.MaxPool2D', 'keras.layers.MaxPooling2D')\n+class MaxPooling2D(Pooling2D):\n+  \"\"\"Max pooling operation for 2D spatial data.\n+\n+  Downsamples the input along its spatial dimensions (height and width)\n+  by taking the maximum value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n+\n+  The resulting output,\n+  when using the `\"valid\"` padding option, has a spatial shape\n+  (number of rows or columns) of:\n+  `output_shape = math.floor((input_shape - pool_size) / strides) + 1`\n+  (when `input_shape >= pool_size`)\n+\n+  The resulting output shape when using the `\"same\"` padding option is:\n+  `output_shape = math.floor((input_shape - 1) / strides) + 1`\n+\n+  For example, for `strides=(1, 1)` and `padding=\"valid\"`:\n+\n+  >>> x = tf.constant([[1., 2., 3.],\n+  ...                  [4., 5., 6.],\n+  ...                  [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [1, 3, 3, 1])\n+  >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n+  ...    strides=(1, 1), padding='valid')\n+  >>> max_pool_2d(x)\n+  <tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=\n+    array([[[[5.],\n+             [6.]],\n+            [[8.],\n+             [9.]]]], dtype=float32)>\n+\n+  For example, for `strides=(2, 2)` and `padding=\"valid\"`:\n+\n+  >>> x = tf.constant([[1., 2., 3., 4.],\n+  ...                  [5., 6., 7., 8.],\n+  ...                  [9., 10., 11., 12.]])\n+  >>> x = tf.reshape(x, [1, 3, 4, 1])\n+  >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n+  ...    strides=(2, 2), padding='valid')\n+  >>> max_pool_2d(x)\n+  <tf.Tensor: shape=(1, 1, 2, 1), dtype=float32, numpy=\n+    array([[[[6.],\n+             [8.]]]], dtype=float32)>\n+\n+  Usage Example:\n+\n+  >>> input_image = tf.constant([[[[1.], [1.], [2.], [4.]],\n+  ...                            [[2.], [2.], [3.], [2.]],\n+  ...                            [[4.], [1.], [1.], [1.]],\n+  ...                            [[2.], [2.], [1.], [4.]]]])\n+  >>> output = tf.constant([[[[1], [0]],\n+  ...                       [[0], [1]]]])\n+  >>> model = tf.keras.models.Sequential()\n+  >>> model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n+  ...    input_shape=(4, 4, 1)))\n+  >>> model.compile('adam', 'mean_squared_error')\n+  >>> model.predict(input_image, steps=1)\n+  array([[[[2.],\n+           [4.]],\n+          [[4.],\n+           [4.]]]], dtype=float32)\n+\n+  For example, for stride=(1, 1) and padding=\"same\":\n+\n+  >>> x = tf.constant([[1., 2., 3.],\n+  ...                  [4., 5., 6.],\n+  ...                  [7., 8., 9.]])\n+  >>> x = tf.reshape(x, [1, 3, 3, 1])\n+  >>> max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),\n+  ...    strides=(1, 1), padding='same')\n+  >>> max_pool_2d(x)\n+  <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n+    array([[[[5.],\n+             [6.],\n+             [6.]],\n+            [[8.],\n+             [9.],\n+             [9.]],\n+            [[8.],\n+             [9.],\n+             [9.]]]], dtype=float32)>\n+\n+  Args:\n+    pool_size: integer or tuple of 2 integers,\n+      window size over which to take the maximum.\n+      `(2, 2)` will take the max value over a 2x2 pooling window.\n+      If only one integer is specified, the same window length\n+      will be used for both dimensions.\n+    strides: Integer, tuple of 2 integers, or None.\n+      Strides values.  Specifies how far the pooling window moves\n+      for each pooling step. If None, it will default to `pool_size`.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, height, width, channels)` while `channels_first`\n+      corresponds to inputs with shape\n+      `(batch, channels, height, width)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      4D tensor with shape `(batch_size, rows, cols, channels)`.\n+    - If `data_format='channels_first'`:\n+      4D tensor with shape `(batch_size, channels, rows, cols)`.\n+\n+  Output shape:\n+    - If `data_format='channels_last'`:\n+      4D tensor with shape `(batch_size, pooled_rows, pooled_cols, channels)`.\n+    - If `data_format='channels_first'`:\n+      4D tensor with shape `(batch_size, channels, pooled_rows, pooled_cols)`.\n+\n+  Returns:\n+    A tensor of rank 4 representing the maximum pooled values.  See above for\n+    output shape.\n+  \"\"\"\n+\n+  def __init__(self,\n+               pool_size=(2, 2),\n+               strides=None,\n+               padding='valid',\n+               data_format=None,\n+               **kwargs):\n+    super(MaxPooling2D, self).__init__(\n+        tf.compat.v1.nn.max_pool,\n+        pool_size=pool_size, strides=strides,\n+        padding=padding, data_format=data_format, **kwargs)\n+\n+\n+# Alias\n+\n+MaxPool2D = MaxPooling2D\n\n@@ -0,0 +1,97 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Max pooling 3D layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\n+from keras.layers.pooling.base_pooling3d import Pooling3D\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.MaxPool3D', 'keras.layers.MaxPooling3D')\n+class MaxPooling3D(Pooling3D):\n+  \"\"\"Max pooling operation for 3D data (spatial or spatio-temporal).\n+\n+  Downsamples the input along its spatial dimensions (depth, height, and width)\n+  by taking the maximum value over an input window\n+  (of size defined by `pool_size`) for each channel of the input.\n+  The window is shifted by `strides` along each dimension.\n+\n+  Args:\n+    pool_size: Tuple of 3 integers,\n+      factors by which to downscale (dim1, dim2, dim3).\n+      `(2, 2, 2)` will halve the size of the 3D input in each dimension.\n+    strides: tuple of 3 integers, or None. Strides values.\n+    padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n+      `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n+      the left/right or up/down of the input such that output has the same\n+      height/width dimension as the input.\n+    data_format: A string,\n+      one of `channels_last` (default) or `channels_first`.\n+      The ordering of the dimensions in the inputs.\n+      `channels_last` corresponds to inputs with shape\n+      `(batch, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+      while `channels_first` corresponds to inputs with shape\n+      `(batch, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n+      It defaults to the `image_data_format` value found in your\n+      Keras config file at `~/.keras/keras.json`.\n+      If you never set it, then it will be \"channels_last\".\n+\n+  Input shape:\n+    - If `data_format='channels_last'`:\n+      5D tensor with shape:\n+      `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+      5D tensor with shape:\n+      `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n+\n+  Output shape:\n+    - If `data_format='channels_last'`:\n+      5D tensor with shape:\n+      `(batch_size, pooled_dim1, pooled_dim2, pooled_dim3, channels)`\n+    - If `data_format='channels_first'`:\n+      5D tensor with shape:\n+      `(batch_size, channels, pooled_dim1, pooled_dim2, pooled_dim3)`\n+\n+  Example:\n+\n+  ```python\n+  depth = 30\n+  height = 30\n+  width = 30\n+  input_channels = 3\n+\n+  inputs = tf.keras.Input(shape=(depth, height, width, input_channels))\n+  layer = tf.keras.layers.MaxPooling3D(pool_size=3)\n+  outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)\n+  ```\n+  \"\"\"\n+\n+  def __init__(self,\n+               pool_size=(2, 2, 2),\n+               strides=None,\n+               padding='valid',\n+               data_format=None,\n+               **kwargs):\n+    super(MaxPooling3D, self).__init__(\n+        tf.nn.max_pool3d,\n+        pool_size=pool_size, strides=strides,\n+        padding=padding, data_format=data_format, **kwargs)\n+\n+\n+# Alias\n+\n+MaxPool3D = MaxPooling3D\n\n@@ -0,0 +1,75 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for max pooling layers.\"\"\"\n+\n+from absl.testing import parameterized\n+import keras\n+from keras import combinations\n+from keras import testing_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class MaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n+\n+  def test_max_pooling_1d(self):\n+    for padding in ['valid', 'same']:\n+      for stride in [1, 2]:\n+        testing_utils.layer_test(\n+            keras.layers.MaxPooling1D,\n+            kwargs={\n+                'strides': stride,\n+                'padding': padding\n+            },\n+            input_shape=(3, 5, 4))\n+    testing_utils.layer_test(\n+        keras.layers.MaxPooling1D,\n+        kwargs={'data_format': 'channels_first'},\n+        input_shape=(3, 2, 6))\n+\n+  def test_max_pooling_2d(self):\n+    pool_size = (3, 3)\n+    for strides in [(1, 1), (2, 2)]:\n+      testing_utils.layer_test(\n+          keras.layers.MaxPooling2D,\n+          kwargs={\n+              'strides': strides,\n+              'padding': 'valid',\n+              'pool_size': pool_size\n+          },\n+          input_shape=(3, 5, 6, 4))\n+\n+  def test_max_pooling_3d(self):\n+    pool_size = (3, 3, 3)\n+    testing_utils.layer_test(\n+        keras.layers.MaxPooling3D,\n+        kwargs={\n+            'strides': 2,\n+            'padding': 'valid',\n+            'pool_size': pool_size\n+        },\n+        input_shape=(3, 11, 12, 10, 4))\n+    testing_utils.layer_test(\n+        keras.layers.MaxPooling3D,\n+        kwargs={\n+            'strides': 3,\n+            'padding': 'valid',\n+            'data_format': 'channels_first',\n+            'pool_size': pool_size\n+        },\n+        input_shape=(3, 4, 11, 12, 10))\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -1,362 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for pooling layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-from absl.testing import parameterized\n-import numpy as np\n-\n-import keras\n-from keras import combinations\n-from keras import testing_utils\n-from keras.mixed_precision import policy\n-\n-\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  @testing_utils.enable_v2_dtype_behavior\n-  def test_mixed_float16_policy(self):\n-    with policy.policy_scope('mixed_float16'):\n-      inputs1 = keras.Input(shape=(36, 512), dtype='float16')\n-      inputs2 = keras.Input(shape=(36,), dtype='bool')\n-      average_layer = keras.layers.pooling.GlobalAveragePooling1D()\n-      _ = average_layer(inputs1, inputs2)\n-\n-  def test_globalpooling_1d(self):\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling1D, input_shape=(3, 4, 5))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling1D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 4, 5))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling1D, input_shape=(3, 4, 5))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling1D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 4, 5))\n-\n-  def test_globalpooling_1d_masking_support(self):\n-    model = keras.Sequential()\n-    model.add(keras.layers.Masking(mask_value=0., input_shape=(None, 4)))\n-    model.add(keras.layers.GlobalAveragePooling1D())\n-    model.compile(loss='mae', optimizer='rmsprop')\n-\n-    model_input = np.random.random((2, 3, 4))\n-    model_input[0, 1:, :] = 0\n-    output = model.predict(model_input)\n-    self.assertAllClose(output[0], model_input[0, 0, :])\n-\n-  def test_globalpooling_1d_with_ragged(self):\n-    ragged_data = tf.ragged.constant(\n-        [[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], [[1.0, 1.0], [2.0, 2.0]]],\n-        ragged_rank=1)\n-    dense_data = ragged_data.to_tensor()\n-\n-    inputs = keras.Input(shape=(None, 2), dtype='float32', ragged=True)\n-    out = keras.layers.GlobalAveragePooling1D()(inputs)\n-    model = keras.models.Model(inputs=inputs, outputs=out)\n-    output_ragged = model.predict(ragged_data, steps=1)\n-\n-    inputs = keras.Input(shape=(None, 2), dtype='float32')\n-    masking = keras.layers.Masking(mask_value=0., input_shape=(3, 2))(inputs)\n-    out = keras.layers.GlobalAveragePooling1D()(masking)\n-    model = keras.models.Model(inputs=inputs, outputs=out)\n-    output_dense = model.predict(dense_data, steps=1)\n-\n-    self.assertAllEqual(output_ragged, output_dense)\n-\n-  def test_globalpooling_2d_with_ragged(self):\n-    ragged_data = tf.ragged.constant(\n-        [[[[1.0], [1.0]], [[2.0], [2.0]], [[3.0], [3.0]]],\n-         [[[1.0], [1.0]], [[2.0], [2.0]]]],\n-        ragged_rank=1)\n-    dense_data = ragged_data.to_tensor()\n-\n-    inputs = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)\n-    out = keras.layers.GlobalMaxPooling2D()(inputs)\n-    model = keras.models.Model(inputs=inputs, outputs=out)\n-    output_ragged = model.predict(ragged_data, steps=1)\n-\n-    inputs = keras.Input(shape=(None, 2, 1), dtype='float32')\n-    out = keras.layers.GlobalMaxPooling2D()(inputs)\n-    model = keras.models.Model(inputs=inputs, outputs=out)\n-    output_dense = model.predict(dense_data, steps=1)\n-\n-    self.assertAllEqual(output_ragged, output_dense)\n-\n-  def test_globalpooling_3d_with_ragged(self):\n-    ragged_data = tf.ragged.constant(\n-        [[[[[1.0]], [[1.0]]], [[[2.0]], [[2.0]]], [[[3.0]], [[3.0]]]],\n-         [[[[1.0]], [[1.0]]], [[[2.0]], [[2.0]]]]],\n-        ragged_rank=1)\n-\n-    inputs = keras.Input(shape=(None, 2, 1, 1), dtype='float32', ragged=True)\n-    out = keras.layers.GlobalAveragePooling3D()(inputs)\n-    model = keras.models.Model(inputs=inputs, outputs=out)\n-    output_ragged = model.predict(ragged_data, steps=1)\n-    # Because GlobalAveragePooling3D doesn't support masking, the results\n-    # cannot be compared with its dense equivalent.\n-    expected_output = tf.constant([[2.0], [1.5]])\n-    self.assertAllEqual(output_ragged, expected_output)\n-\n-  def test_globalpooling_2d(self):\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling2D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 4, 5, 6))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling2D,\n-        kwargs={'data_format': 'channels_last'},\n-        input_shape=(3, 5, 6, 4))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling2D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 4, 5, 6))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling2D,\n-        kwargs={'data_format': 'channels_last'},\n-        input_shape=(3, 5, 6, 4))\n-\n-  def test_globalpooling_3d(self):\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling3D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 4, 3, 4, 3))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling3D,\n-        kwargs={'data_format': 'channels_last'},\n-        input_shape=(3, 4, 3, 4, 3))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling3D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 4, 3, 4, 3))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling3D,\n-        kwargs={'data_format': 'channels_last'},\n-        input_shape=(3, 4, 3, 4, 3))\n-\n-  def test_globalpooling_1d_keepdims(self):\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling1D,\n-        kwargs={'keepdims': True},\n-        input_shape=(3, 4, 5),\n-        expected_output_shape=(None, 1, 5))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling1D,\n-        kwargs={'data_format': 'channels_first', 'keepdims': True},\n-        input_shape=(3, 4, 5),\n-        expected_output_shape=(None, 4, 1))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling1D,\n-        kwargs={'keepdims': True},\n-        input_shape=(3, 4, 5),\n-        expected_output_shape=(None, 1, 5))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling1D,\n-        kwargs={'data_format': 'channels_first', 'keepdims': True},\n-        input_shape=(3, 4, 5),\n-        expected_output_shape=(None, 4, 1))\n-\n-  def test_globalpooling_2d_keepdims(self):\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling2D,\n-        kwargs={'data_format': 'channels_first', 'keepdims': True},\n-        input_shape=(3, 4, 5, 6),\n-        expected_output_shape=(None, 4, 1, 1))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling2D,\n-        kwargs={'data_format': 'channels_last', 'keepdims': True},\n-        input_shape=(3, 4, 5, 6),\n-        expected_output_shape=(None, 1, 1, 6))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling2D,\n-        kwargs={'data_format': 'channels_first', 'keepdims': True},\n-        input_shape=(3, 4, 5, 6),\n-        expected_output_shape=(None, 4, 1, 1))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling2D,\n-        kwargs={'data_format': 'channels_last', 'keepdims': True},\n-        input_shape=(3, 4, 5, 6),\n-        expected_output_shape=(None, 1, 1, 6))\n-\n-  def test_globalpooling_3d_keepdims(self):\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling3D,\n-        kwargs={'data_format': 'channels_first', 'keepdims': True},\n-        input_shape=(3, 4, 3, 4, 3),\n-        expected_output_shape=(None, 4, 1, 1, 1))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalMaxPooling3D,\n-        kwargs={'data_format': 'channels_last', 'keepdims': True},\n-        input_shape=(3, 4, 3, 4, 3),\n-        expected_output_shape=(None, 1, 1, 1, 3))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling3D,\n-        kwargs={'data_format': 'channels_first', 'keepdims': True},\n-        input_shape=(3, 4, 3, 4, 3),\n-        expected_output_shape=(None, 4, 1, 1, 1))\n-    testing_utils.layer_test(\n-        keras.layers.pooling.GlobalAveragePooling3D,\n-        kwargs={'data_format': 'channels_last', 'keepdims': True},\n-        input_shape=(3, 4, 3, 4, 3),\n-        expected_output_shape=(None, 1, 1, 1, 3))\n-\n-  def test_globalpooling_1d_keepdims_masking_support(self):\n-    model = keras.Sequential()\n-    model.add(keras.layers.Masking(mask_value=0., input_shape=(None, 4)))\n-    model.add(keras.layers.GlobalAveragePooling1D(keepdims=True))\n-    model.compile(loss='mae', optimizer='rmsprop')\n-\n-    model_input = np.random.random((2, 3, 4))\n-    model_input[0, 1:, :] = 0\n-    output = model.predict(model_input)\n-    self.assertAllEqual((2, 1, 4), output.shape)\n-    self.assertAllClose(output[0, 0], model_input[0, 0, :])\n-\n-\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class Pooling2DTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  def test_maxpooling_2d(self):\n-    pool_size = (3, 3)\n-    for strides in [(1, 1), (2, 2)]:\n-      testing_utils.layer_test(\n-          keras.layers.MaxPooling2D,\n-          kwargs={\n-              'strides': strides,\n-              'padding': 'valid',\n-              'pool_size': pool_size\n-          },\n-          input_shape=(3, 5, 6, 4))\n-\n-  def test_averagepooling_2d(self):\n-    testing_utils.layer_test(\n-        keras.layers.AveragePooling2D,\n-        kwargs={\n-            'strides': (2, 2),\n-            'padding': 'same',\n-            'pool_size': (2, 2)\n-        },\n-        input_shape=(3, 5, 6, 4))\n-    testing_utils.layer_test(\n-        keras.layers.AveragePooling2D,\n-        kwargs={\n-            'strides': (2, 2),\n-            'padding': 'valid',\n-            'pool_size': (3, 3)\n-        },\n-        input_shape=(3, 5, 6, 4))\n-\n-    # This part of the test can only run on GPU but doesn't appear\n-    # to be properly assigned to a GPU when running in eager mode.\n-    if not tf.executing_eagerly():\n-      # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n-      # TODO(b/62340061): Support channels_first on CPU.\n-      if tf.test.is_gpu_available(cuda_only=True):\n-        testing_utils.layer_test(\n-            keras.layers.AveragePooling2D,\n-            kwargs={\n-                'strides': (1, 1),\n-                'padding': 'valid',\n-                'pool_size': (2, 2),\n-                'data_format': 'channels_first'\n-            },\n-            input_shape=(3, 4, 5, 6))\n-\n-\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class Pooling3DTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  def test_maxpooling_3d(self):\n-    pool_size = (3, 3, 3)\n-    testing_utils.layer_test(\n-        keras.layers.MaxPooling3D,\n-        kwargs={\n-            'strides': 2,\n-            'padding': 'valid',\n-            'pool_size': pool_size\n-        },\n-        input_shape=(3, 11, 12, 10, 4))\n-    testing_utils.layer_test(\n-        keras.layers.MaxPooling3D,\n-        kwargs={\n-            'strides': 3,\n-            'padding': 'valid',\n-            'data_format': 'channels_first',\n-            'pool_size': pool_size\n-        },\n-        input_shape=(3, 4, 11, 12, 10))\n-\n-  def test_averagepooling_3d(self):\n-    pool_size = (3, 3, 3)\n-    testing_utils.layer_test(\n-        keras.layers.AveragePooling3D,\n-        kwargs={\n-            'strides': 2,\n-            'padding': 'valid',\n-            'pool_size': pool_size\n-        },\n-        input_shape=(3, 11, 12, 10, 4))\n-    testing_utils.layer_test(\n-        keras.layers.AveragePooling3D,\n-        kwargs={\n-            'strides': 3,\n-            'padding': 'valid',\n-            'data_format': 'channels_first',\n-            'pool_size': pool_size\n-        },\n-        input_shape=(3, 4, 11, 12, 10))\n-\n-\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class Pooling1DTest(tf.test.TestCase, parameterized.TestCase):\n-\n-  def test_maxpooling_1d(self):\n-    for padding in ['valid', 'same']:\n-      for stride in [1, 2]:\n-        testing_utils.layer_test(\n-            keras.layers.MaxPooling1D,\n-            kwargs={\n-                'strides': stride,\n-                'padding': padding\n-            },\n-            input_shape=(3, 5, 4))\n-    testing_utils.layer_test(\n-        keras.layers.MaxPooling1D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 2, 6))\n-\n-  def test_averagepooling_1d(self):\n-    for padding in ['valid', 'same']:\n-      for stride in [1, 2]:\n-        testing_utils.layer_test(\n-            keras.layers.AveragePooling1D,\n-            kwargs={\n-                'strides': stride,\n-                'padding': padding\n-            },\n-            input_shape=(3, 5, 4))\n-\n-    testing_utils.layer_test(\n-        keras.layers.AveragePooling1D,\n-        kwargs={'data_format': 'channels_first'},\n-        input_shape=(3, 2, 6))\n-\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
