{"custom_id": "keras#89e5669fabe5aec0cdf87c00323d37336330d548", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 165 | Lines Deleted: 53 | Files Changed: 5 | Hunks: 49 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 218 | Churn Cumulative: 7091 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -112,13 +112,21 @@ def split_data(images, labels, train_size=0.9, shuffle=True):\n     # 3. Get the size of training samples\n     train_samples = int(size * train_size)\n     # 4. Split data into training and validation sets\n-    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n-    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n+    x_train, y_train = (\n+        images[indices[:train_samples]],\n+        labels[indices[:train_samples]],\n+    )\n+    x_valid, y_valid = (\n+        images[indices[train_samples:]],\n+        labels[indices[train_samples:]],\n+    )\n     return x_train, x_valid, y_train, y_valid\n \n \n # Splitting data into training and validation sets\n-x_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n+x_train, x_valid, y_train, y_valid = split_data(\n+    np.array(images), np.array(labels)\n+)\n \n \n def encode_single_sample(img_path, label):\n@@ -153,7 +161,9 @@ train_dataset = (\n \n validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n validation_dataset = (\n-    validation_dataset.map(encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE)\n+    validation_dataset.map(\n+        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n+    )\n     .batch(batch_size)\n     .prefetch(buffer_size=tf.data.AUTOTUNE)\n )\n@@ -169,7 +179,11 @@ for batch in train_dataset.take(1):\n     labels = batch[\"label\"]\n     for i in range(16):\n         img = (images[i] * 255).numpy().astype(\"uint8\")\n-        label = tf.strings.reduce_join(num_to_char(labels[i])).numpy().decode(\"utf-8\")\n+        label = (\n+            tf.strings.reduce_join(num_to_char(labels[i]))\n+            .numpy()\n+            .decode(\"utf-8\")\n+        )\n         ax[i // 4, i % 4].imshow(img[:, :, 0].T, cmap=\"gray\")\n         ax[i // 4, i % 4].set_title(label)\n         ax[i // 4, i % 4].axis(\"off\")\n@@ -192,8 +206,12 @@ class CTCLayer(layers.Layer):\n         input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n         label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n \n-        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n-        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n+        input_length = input_length * tf.ones(\n+            shape=(batch_len, 1), dtype=\"int64\"\n+        )\n+        label_length = label_length * tf.ones(\n+            shape=(batch_len, 1), dtype=\"int64\"\n+        )\n \n         loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n         self.add_loss(loss)\n@@ -241,12 +259,18 @@ def build_model():\n     x = layers.Dropout(0.2)(x)\n \n     # RNNs\n-    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n-    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n+    x = layers.Bidirectional(\n+        layers.LSTM(128, return_sequences=True, dropout=0.25)\n+    )(x)\n+    x = layers.Bidirectional(\n+        layers.LSTM(64, return_sequences=True, dropout=0.25)\n+    )(x)\n \n     # Output layer\n     x = layers.Dense(\n-        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n+        len(char_to_num.get_vocabulary()) + 1,\n+        activation=\"softmax\",\n+        name=\"dense2\",\n     )(x)\n \n     # Add CTC layer for calculating CTC loss at each step\n@@ -276,7 +300,9 @@ epochs = 1\n early_stopping_patience = 10\n # Add early stopping\n early_stopping = keras.callbacks.EarlyStopping(\n-    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n+    monitor=\"val_loss\",\n+    patience=early_stopping_patience,\n+    restore_best_weights=True,\n )\n \n # Train the model\n@@ -307,9 +333,9 @@ prediction_model.summary()\n def decode_batch_predictions(pred):\n     input_len = np.ones(pred.shape[0]) * pred.shape[1]\n     # Use greedy search. For complex tasks, you can use beam search\n-    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n-        :, :max_length\n-    ]\n+    results = keras.backend.ctc_decode(\n+        pred, input_length=input_len, greedy=True\n+    )[0][0][:, :max_length]\n     # Iterate over the results and get back the text\n     output_text = []\n     for res in results:\n@@ -328,7 +354,9 @@ for batch in validation_dataset.take(1):\n \n     orig_texts = []\n     for label in batch_labels:\n-        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n+        label = (\n+            tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n+        )\n         orig_texts.append(label)\n \n     _, ax = plt.subplots(4, 4, figsize=(15, 5))\n\n@@ -58,15 +58,21 @@ path_to_downloaded_file = keras.utils.get_file(\n )\n \n # Extracting tar files found inside main zip file\n-shutil.unpack_archive(\"./datasets/caltech-101/101_ObjectCategories.tar.gz\", \"./\")\n+shutil.unpack_archive(\n+    \"./datasets/caltech-101/101_ObjectCategories.tar.gz\", \"./\"\n+)\n shutil.unpack_archive(\"./datasets/caltech-101/Annotations.tar\", \"./\")\n \n # list of paths to images and annotations\n image_paths = [\n-    f for f in os.listdir(path_images) if os.path.isfile(os.path.join(path_images, f))\n+    f\n+    for f in os.listdir(path_images)\n+    if os.path.isfile(os.path.join(path_images, f))\n ]\n annot_paths = [\n-    f for f in os.listdir(path_annot) if os.path.isfile(os.path.join(path_annot, f))\n+    f\n+    for f in os.listdir(path_annot)\n+    if os.path.isfile(os.path.join(path_annot, f))\n ]\n \n image_paths.sort()\n@@ -186,7 +192,9 @@ plt.axis(\"off\")\n patches = Patches(patch_size)(tf.convert_to_tensor([x_train[0]]))\n print(f\"Image size: {image_size} X {image_size}\")\n print(f\"Patch size: {patch_size} X {patch_size}\")\n-print(f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\")\n+print(\n+    f\"{patches.shape[1]} patches per image \\n{patches.shape[-1]} elements per patch\"\n+)\n \n \n n = int(np.sqrt(patches.shape[1]))\n@@ -289,7 +297,9 @@ def create_vit_object_detector(\n     representation = layers.Flatten()(representation)\n     representation = layers.Dropout(0.3)(representation)\n     # Add MLP.\n-    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.3)\n+    features = mlp(\n+        representation, hidden_units=mlp_head_units, dropout_rate=0.3\n+    )\n \n     bounding_box = layers.Dense(4)(\n         features\n@@ -461,7 +471,9 @@ for input_image in x_test[:10]:\n \n     top_left_x, top_left_y = int(y_test[i][0] * w), int(y_test[i][1] * h)\n \n-    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(y_test[i][3] * h)\n+    bottom_right_x, bottom_right_y = int(y_test[i][2] * w), int(\n+        y_test[i][3] * h\n+    )\n \n     box_truth = top_left_x, top_left_y, bottom_right_x, bottom_right_y\n \n\n@@ -68,6 +68,7 @@ check out\n \n # Make sure we are able to handle large datasets\n import resource\n+\n low, high = resource.getrlimit(resource.RLIMIT_NOFILE)\n resource.setrlimit(resource.RLIMIT_NOFILE, (high, high))\n \n@@ -95,7 +96,11 @@ width = 128\n temperature = 0.1\n # Stronger augmentations for contrastive, weaker ones for supervised training\n contrastive_augmentation = {\"min_area\": 0.25, \"brightness\": 0.6, \"jitter\": 0.2}\n-classification_augmentation = {\"min_area\": 0.75, \"brightness\": 0.3, \"jitter\": 0.1}\n+classification_augmentation = {\n+    \"min_area\": 0.75,\n+    \"brightness\": 0.3,\n+    \"jitter\": 0.1,\n+}\n \n \"\"\"\n ## Dataset\n@@ -108,7 +113,9 @@ smaller batch of labeled images.\n def prepare_dataset():\n     # Labeled and unlabeled samples are loaded synchronously\n     # with batch sizes selected accordingly\n-    steps_per_epoch = (unlabeled_dataset_size + labeled_dataset_size) // batch_size\n+    steps_per_epoch = (\n+        unlabeled_dataset_size + labeled_dataset_size\n+    ) // batch_size\n     unlabeled_batch_size = unlabeled_dataset_size // steps_per_epoch\n     labeled_batch_size = labeled_dataset_size // steps_per_epoch\n     print(\n@@ -117,12 +124,16 @@ def prepare_dataset():\n \n     # Turning off shuffle to lower resource usage\n     unlabeled_train_dataset = (\n-        tfds.load(\"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False)\n+        tfds.load(\n+            \"stl10\", split=\"unlabelled\", as_supervised=True, shuffle_files=False\n+        )\n         .shuffle(buffer_size=10 * unlabeled_batch_size)\n         .batch(unlabeled_batch_size)\n     )\n     labeled_train_dataset = (\n-        tfds.load(\"stl10\", split=\"train\", as_supervised=True, shuffle_files=False)\n+        tfds.load(\n+            \"stl10\", split=\"train\", as_supervised=True, shuffle_files=False\n+        )\n         .shuffle(buffer_size=10 * labeled_batch_size)\n         .batch(labeled_batch_size)\n     )\n@@ -193,7 +204,9 @@ class RandomColorAffine(layers.Layer):\n \n             # Same for all colors\n             brightness_scales = 1 + tf.random.uniform(\n-                (batch_size, 1, 1, 1), minval=-self.brightness, maxval=self.brightness\n+                (batch_size, 1, 1, 1),\n+                minval=-self.brightness,\n+                maxval=self.brightness,\n             )\n             # Different for all colors\n             jitter_matrices = tf.random.uniform(\n@@ -356,7 +369,9 @@ class ContrastiveModel(keras.Model):\n \n         self.temperature = temperature\n         self.contrastive_augmenter = get_augmenter(**contrastive_augmentation)\n-        self.classification_augmenter = get_augmenter(**classification_augmentation)\n+        self.classification_augmenter = get_augmenter(\n+            **classification_augmentation\n+        )\n         self.encoder = get_encoder()\n         # Non-linear MLP as projection head\n         self.projection_head = keras.Sequential(\n@@ -369,7 +384,8 @@ class ContrastiveModel(keras.Model):\n         )\n         # Single dense layer for linear probing\n         self.linear_probe = keras.Sequential(\n-            [layers.Input(shape=(width,)), layers.Dense(10)], name=\"linear_probe\"\n+            [layers.Input(shape=(width,)), layers.Dense(10)],\n+            name=\"linear_probe\",\n         )\n \n         self.encoder.summary()\n@@ -383,14 +399,18 @@ class ContrastiveModel(keras.Model):\n         self.probe_optimizer = probe_optimizer\n \n         # self.contrastive_loss will be defined as a method\n-        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n+        self.probe_loss = keras.losses.SparseCategoricalCrossentropy(\n+            from_logits=True\n+        )\n \n         self.contrastive_loss_tracker = keras.metrics.Mean(name=\"c_loss\")\n         self.contrastive_accuracy = keras.metrics.SparseCategoricalAccuracy(\n             name=\"c_acc\"\n         )\n         self.probe_loss_tracker = keras.metrics.Mean(name=\"p_loss\")\n-        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(name=\"p_acc\")\n+        self.probe_accuracy = keras.metrics.SparseCategoricalAccuracy(\n+            name=\"p_acc\"\n+        )\n \n     @property\n     def metrics(self):\n@@ -409,7 +429,8 @@ class ContrastiveModel(keras.Model):\n         projections_1 = tf.math.l2_normalize(projections_1, axis=1)\n         projections_2 = tf.math.l2_normalize(projections_2, axis=1)\n         similarities = (\n-            tf.matmul(projections_1, projections_2, transpose_b=True) / self.temperature\n+            tf.matmul(projections_1, projections_2, transpose_b=True)\n+            / self.temperature\n         )\n \n         # The similarity between the representations of two augmented views of the\n@@ -445,15 +466,19 @@ class ContrastiveModel(keras.Model):\n             # The representations are passed through a projection mlp\n             projections_1 = self.projection_head(features_1, training=True)\n             projections_2 = self.projection_head(features_2, training=True)\n-            contrastive_loss = self.contrastive_loss(projections_1, projections_2)\n+            contrastive_loss = self.contrastive_loss(\n+                projections_1, projections_2\n+            )\n         gradients = tape.gradient(\n             contrastive_loss,\n-            self.encoder.trainable_weights + self.projection_head.trainable_weights,\n+            self.encoder.trainable_weights\n+            + self.projection_head.trainable_weights,\n         )\n         self.contrastive_optimizer.apply_gradients(\n             zip(\n                 gradients,\n-                self.encoder.trainable_weights + self.projection_head.trainable_weights,\n+                self.encoder.trainable_weights\n+                + self.projection_head.trainable_weights,\n             )\n         )\n         self.contrastive_loss_tracker.update_state(contrastive_loss)\n@@ -468,7 +493,9 @@ class ContrastiveModel(keras.Model):\n             features = self.encoder(preprocessed_images, training=False)\n             class_logits = self.linear_probe(features, training=True)\n             probe_loss = self.probe_loss(labels, class_logits)\n-        gradients = tape.gradient(probe_loss, self.linear_probe.trainable_weights)\n+        gradients = tape.gradient(\n+            probe_loss, self.linear_probe.trainable_weights\n+        )\n         self.probe_optimizer.apply_gradients(\n             zip(gradients, self.linear_probe.trainable_weights)\n         )\n@@ -548,11 +575,14 @@ print(\n \n \n # The classification accuracies of the baseline and the pretraining + finetuning process:\n-def plot_training_curves(pretraining_history, finetuning_history, baseline_history):\n+def plot_training_curves(\n+    pretraining_history, finetuning_history, baseline_history\n+):\n     for metric_key, metric_name in zip([\"acc\", \"loss\"], [\"accuracy\", \"loss\"]):\n         plt.figure(figsize=(8, 5), dpi=100)\n         plt.plot(\n-            baseline_history.history[f\"val_{metric_key}\"], label=\"supervised baseline\"\n+            baseline_history.history[f\"val_{metric_key}\"],\n+            label=\"supervised baseline\",\n         )\n         plt.plot(\n             pretraining_history.history[f\"val_p_{metric_key}\"],\n\n@@ -100,7 +100,15 @@ def window_partition(x, window_size):\n     patch_num_y = height // window_size\n     patch_num_x = width // window_size\n     x = tf.reshape(\n-        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n+        x,\n+        shape=(\n+            -1,\n+            patch_num_y,\n+            window_size,\n+            patch_num_x,\n+            window_size,\n+            channels,\n+        ),\n     )\n     x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n     windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n@@ -112,7 +120,14 @@ def window_reverse(windows, window_size, height, width, channels):\n     patch_num_x = width // window_size\n     x = tf.reshape(\n         windows,\n-        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n+        shape=(\n+            -1,\n+            patch_num_y,\n+            patch_num_x,\n+            window_size,\n+            window_size,\n+            channels,\n+        ),\n     )\n     x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n     x = tf.reshape(x, shape=(-1, height, width, channels))\n@@ -129,7 +144,9 @@ class DropPath(layers.Layer):\n         batch_size = input_shape[0]\n         rank = x.shape.rank\n         shape = (batch_size,) + (1,) * (rank - 1)\n-        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n+        random_tensor = (1 - self.drop_prob) + tf.random.uniform(\n+            shape, dtype=x.dtype\n+        )\n         path_mask = tf.floor(random_tensor)\n         output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n         return output\n@@ -149,7 +166,13 @@ whereas window-based self-attention leads to linear complexity and is easily sca\n \n class WindowAttention(layers.Layer):\n     def __init__(\n-        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n+        self,\n+        dim,\n+        window_size,\n+        num_heads,\n+        qkv_bias=True,\n+        dropout_rate=0.0,\n+        **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self.dim = dim\n@@ -174,7 +197,9 @@ class WindowAttention(layers.Layer):\n         coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n         coords = np.stack(coords_matrix)\n         coords_flatten = coords.reshape(2, -1)\n-        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        relative_coords = (\n+            coords_flatten[:, :, None] - coords_flatten[:, None, :]\n+        )\n         relative_coords = relative_coords.transpose([1, 2, 0])\n         relative_coords[:, :, 0] += self.window_size[0] - 1\n         relative_coords[:, :, 1] += self.window_size[1] - 1\n@@ -182,7 +207,8 @@ class WindowAttention(layers.Layer):\n         relative_position_index = relative_coords.sum(-1)\n \n         self.relative_position_index = tf.Variable(\n-            initial_value=lambda: tf.convert_to_tensor(relative_position_index), trainable=False\n+            initial_value=lambda: tf.convert_to_tensor(relative_position_index),\n+            trainable=False,\n         )\n \n     def call(self, x, mask=None):\n@@ -204,9 +230,12 @@ class WindowAttention(layers.Layer):\n             self.relative_position_bias_table, relative_position_index_flat\n         )\n         relative_position_bias = tf.reshape(\n-            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n+            relative_position_bias,\n+            shape=(num_window_elements, num_window_elements, -1),\n+        )\n+        relative_position_bias = tf.transpose(\n+            relative_position_bias, perm=(2, 0, 1)\n         )\n-        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n         attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n \n         if mask is not None:\n@@ -329,7 +358,9 @@ class SwinTransformer(layers.Layer):\n             )\n             attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n             attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n-            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n+            self.attn_mask = tf.Variable(\n+                initial_value=attn_mask, trainable=False\n+            )\n \n     def call(self, x):\n         height, width = self.num_patch\n@@ -351,7 +382,8 @@ class SwinTransformer(layers.Layer):\n         attn_windows = self.attn(x_windows, mask=self.attn_mask)\n \n         attn_windows = tf.reshape(\n-            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n+            attn_windows,\n+            shape=(-1, self.window_size, self.window_size, channels),\n         )\n         shifted_x = window_reverse(\n             attn_windows, self.window_size, height, width, channels\n@@ -401,7 +433,9 @@ class PatchExtract(layers.Layer):\n         )\n         patch_dim = patches.shape[-1]\n         patch_num = patches.shape[1]\n-        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n+        return tf.reshape(\n+            patches, (batch_size, patch_num * patch_num, patch_dim)\n+        )\n \n \n class PatchEmbedding(layers.Layer):\n@@ -409,7 +443,9 @@ class PatchEmbedding(layers.Layer):\n         super().__init__(**kwargs)\n         self.num_patch = num_patch\n         self.proj = layers.Dense(embed_dim)\n-        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n+        self.pos_embed = layers.Embedding(\n+            input_dim=num_patch, output_dim=embed_dim\n+        )\n \n     def call(self, patch):\n         pos = tf.range(start=0, limit=self.num_patch, delta=1)\n\n@@ -136,15 +136,21 @@ via random search using [KerasTuner](https://github.com/keras-team/keras-tuner).\n def make_model(input_shape):\n     input_layer = keras.layers.Input(input_shape)\n \n-    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(input_layer)\n+    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(\n+        input_layer\n+    )\n     conv1 = keras.layers.BatchNormalization()(conv1)\n     conv1 = keras.layers.ReLU()(conv1)\n \n-    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv1)\n+    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(\n+        conv1\n+    )\n     conv2 = keras.layers.BatchNormalization()(conv2)\n     conv2 = keras.layers.ReLU()(conv2)\n \n-    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(conv2)\n+    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\")(\n+        conv2\n+    )\n     conv3 = keras.layers.BatchNormalization()(conv3)\n     conv3 = keras.layers.ReLU()(conv3)\n \n@@ -178,7 +184,7 @@ callbacks = [\n model.compile(\n     optimizer=\"adam\",\n     loss=\"sparse_categorical_crossentropy\",\n-    metrics=[\"sparse_categorical_accuracy\"]\n+    metrics=[\"sparse_categorical_accuracy\"],\n )\n history = model.fit(\n     x_train,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e65a8c29b15e64314bcbd66e2d125bd45f16d9f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 673 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 12 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 673 | Churn Cumulative: 673 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,673 @@\n+\"\"\"\n+Title: Traffic forecasting using graph neural networks and LSTM\n+Author: [Arash Khodadadi](https://www.linkedin.com/in/arash-khodadadi-08a02490/)\n+Date created: 2021/12/28\n+Last modified: 2021/12/28\n+Description: This example demonstrates how to do timeseries forecasting over graphs.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example shows how to forecast traffic condition using graph neural networks and LSTM.\n+Specifically, we are interested in predicting the future values of the traffic speed given\n+a history of the traffic speed for a collection of road segments.\n+\n+One popular method to\n+solve this problem is to consider each road segment's traffic speed as a separate\n+timeseries and predict the future values of each timeseries\n+using the past values of the same timeseries.\n+\n+This method, however, ignores the dependency of the traffic speed of one road segment on\n+the neighboring segments. To be able to take into account the complex interactions between\n+the traffic speed on a collection of neighboring roads, we can define the traffic network\n+as a graph and consider the traffic speed as a signal on this graph. In this example,\n+we implement a neural network architecture which can process timeseries data over a graph.\n+We first show how to process the data and create a\n+[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for\n+forecasting over graphs. Then, we implement a model which uses graph convolution and\n+LSTM layers to perform forecasting over a graph.\n+\n+The data processing and the model architecture are inspired by this paper:\n+\n+Yu, Bing, Haoteng Yin, and Zhanxing Zhu. \"Spatio-temporal graph convolutional networks:\n+a deep learning framework for traffic forecasting.\" Proceedings of the 27th International\n+Joint Conference on Artificial Intelligence, 2018.\n+([github](https://github.com/VeritasYin/STGCN_IJCAI-18))\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import pandas as pd\n+import numpy as np\n+import os\n+import typing\n+import matplotlib.pyplot as plt\n+\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core.utils import timeseries_dataset_from_array\n+\n+\"\"\"\n+## Data preparation\n+\"\"\"\n+\n+\"\"\"\n+### Data description\n+\n+We use a real-world traffic speed dataset named `PeMSD7`. We use the version\n+collected and prepared by [Yu et al., 2018](https://arxiv.org/abs/1709.04875)\n+and available\n+[here](https://github.com/VeritasYin/STGCN_IJCAI-18/tree/master/dataset).\n+\n+The data consists of two files:\n+\n+- `PeMSD7_W_228.csv` contains the distances between 228\n+stations across the District 7 of California.\n+- `PeMSD7_V_228.csv` contains traffic\n+speed collected for those stations in the weekdays of May and June of 2012.\n+\n+The full description of the dataset can be found in\n+[Yu et al., 2018](https://arxiv.org/abs/1709.04875).\n+\"\"\"\n+\n+\"\"\"\n+### Loading data\n+\"\"\"\n+\n+url = \"https://github.com/VeritasYin/STGCN_IJCAI-18/raw/master/dataset/PeMSD7_Full.zip\"\n+data_dir = keras.utils.get_file(origin=url, extract=True, archive_format=\"zip\")\n+data_dir = data_dir.rstrip(\"PeMSD7_Full.zip\")\n+\n+route_distances = pd.read_csv(\n+    os.path.join(data_dir, \"PeMSD7_W_228.csv\"), header=None\n+).to_numpy()\n+speeds_array = pd.read_csv(\n+    os.path.join(data_dir, \"PeMSD7_V_228.csv\"), header=None\n+).to_numpy()\n+\n+print(f\"route_distances shape={route_distances.shape}\")\n+print(f\"speeds_array shape={speeds_array.shape}\")\n+\n+\"\"\"\n+### sub-sampling roads\n+\n+To reduce the problem size and make the training faster, we will only\n+work with a sample of 26 roads out of the 228 roads in the dataset.\n+We have chosen the roads by starting from road 0, choosing the 5 closest\n+roads to it, and continuing this process until we get 25 roads. You can choose\n+any other subset of the roads. We chose the roads in this way to increase the likelihood\n+of having roads with correlated speed timeseries.\n+`sample_routes` contains the IDs of the selected roads.\n+\"\"\"\n+\n+sample_routes = [\n+    0,\n+    1,\n+    4,\n+    7,\n+    8,\n+    11,\n+    15,\n+    108,\n+    109,\n+    114,\n+    115,\n+    118,\n+    120,\n+    123,\n+    124,\n+    126,\n+    127,\n+    129,\n+    130,\n+    132,\n+    133,\n+    136,\n+    139,\n+    144,\n+    147,\n+    216,\n+]\n+route_distances = route_distances[np.ix_(sample_routes, sample_routes)]\n+speeds_array = speeds_array[:, sample_routes]\n+\n+print(f\"route_distances shape={route_distances.shape}\")\n+print(f\"speeds_array shape={speeds_array.shape}\")\n+\n+\"\"\"\n+### Data visualization\n+\n+Here are the timeseries of the traffic speed for two of the routes:\n+\"\"\"\n+\n+plt.figure(figsize=(18, 6))\n+plt.plot(speeds_array[:, [0, -1]])\n+plt.legend([\"route_0\", \"route_25\"])\n+\n+\"\"\"\n+We can also visualize the correlation between the timeseries in different routes.\n+\"\"\"\n+\n+plt.figure(figsize=(8, 8))\n+plt.matshow(np.corrcoef(speeds_array.T), 0)\n+plt.xlabel(\"road number\")\n+plt.ylabel(\"road number\")\n+\n+\"\"\"\n+Using this correlation heatmap, we can see that for example the speed in\n+routes 4, 5, 6 are highly correlated.\n+\"\"\"\n+\n+\"\"\"\n+### Splitting and normalizing data\n+\n+Next, we split the speed values array into train/validation/test sets,\n+and normalize the resulting arrays:\n+\"\"\"\n+\n+train_size, val_size = 0.5, 0.2\n+\n+\n+def preprocess(data_array: np.ndarray, train_size: float, val_size: float):\n+    \"\"\"Splits data into train/val/test sets and normalizes the data.\n+\n+    Args:\n+        data_array: ndarray of shape `(num_time_steps, num_routes)`\n+        train_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n+            to include in the train split.\n+        val_size: A float value between 0.0 and 1.0 that represent the proportion of the dataset\n+            to include in the validation split.\n+\n+    Returns:\n+        `train_array`, `val_array`, `test_array`\n+    \"\"\"\n+\n+    num_time_steps = data_array.shape[0]\n+    num_train, num_val = (\n+        int(num_time_steps * train_size),\n+        int(num_time_steps * val_size),\n+    )\n+    train_array = data_array[:num_train]\n+    mean, std = train_array.mean(axis=0), train_array.std(axis=0)\n+\n+    train_array = (train_array - mean) / std\n+    val_array = (data_array[num_train : (num_train + num_val)] - mean) / std\n+    test_array = (data_array[(num_train + num_val) :] - mean) / std\n+\n+    return train_array, val_array, test_array\n+\n+\n+train_array, val_array, test_array = preprocess(\n+    speeds_array, train_size, val_size\n+)\n+\n+print(f\"train set size: {train_array.shape}\")\n+print(f\"validation set size: {val_array.shape}\")\n+print(f\"test set size: {test_array.shape}\")\n+\n+\"\"\"\n+### Creating TensorFlow Datasets\n+\n+Next, we create the datasets for our forecasting problem. The forecasting problem\n+can be stated as follows: given a sequence of the\n+road speed values at times `t+1, t+2, ..., t+T`, we want to predict the future values of\n+the roads speed for times `t+T+1, ..., t+T+h`. So for each time `t` the inputs to our\n+model are `T` vectors each of size `N` and the targets are `h` vectors each of size `N`,\n+where `N` is the number of roads.\n+\"\"\"\n+\n+\"\"\"\n+We use the Keras built-in function\n+[`timeseries_dataset_from_array()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array).\n+The function `create_tf_dataset()` below takes as input a `numpy.ndarray` and returns a\n+`tf.data.Dataset`. In this function `input_sequence_length=T` and `forecast_horizon=h`.\n+\n+The argument `multi_horizon` needs more explanation. Assume `forecast_horizon=3`.\n+If `multi_horizon=True` then the model will make a forecast for time steps\n+`t+T+1, t+T+2, t+T+3`. So the target will have shape `(T,3)`. But if\n+`multi_horizon=False`, the model will make a forecast only for time step `t+T+3` and\n+so the target will have shape `(T, 1)`.\n+\n+You may notice that the input tensor in each batch has shape\n+`(batch_size, input_sequence_length, num_routes, 1)`. The last dimension is added to\n+make the model more general: at each time step, the input features for each raod may\n+contain multiple timeseries. For instance, one might want to use temperature timeseries\n+in addition to historical values of the speed as input features. In this example,\n+however, the last dimension of the input is always 1.\n+\n+We use the last 12 values of the speed in each road to forecast the speed for 3 time\n+steps ahead:\n+\"\"\"\n+\n+batch_size = 64\n+input_sequence_length = 12\n+forecast_horizon = 3\n+multi_horizon = False\n+\n+\n+def create_tf_dataset(\n+    data_array: np.ndarray,\n+    input_sequence_length: int,\n+    forecast_horizon: int,\n+    batch_size: int = 128,\n+    shuffle=True,\n+    multi_horizon=True,\n+):\n+    \"\"\"Creates tensorflow dataset from numpy array.\n+\n+    This function creates a dataset where each element is a tuple `(inputs, targets)`.\n+    `inputs` is a Tensor\n+    of shape `(batch_size, input_sequence_length, num_routes, 1)` containing\n+    the `input_sequence_length` past values of the timeseries for each node.\n+    `targets` is a Tensor of shape `(batch_size, forecast_horizon, num_routes)`\n+    containing the `forecast_horizon`\n+    future values of the timeseries for each node.\n+\n+    Args:\n+        data_array: np.ndarray with shape `(num_time_steps, num_routes)`\n+        input_sequence_length: Length of the input sequence (in number of timesteps).\n+        forecast_horizon: If `multi_horizon=True`, the target will be the values of the timeseries for 1 to\n+            `forecast_horizon` timesteps ahead. If `multi_horizon=False`, the target will be the value of the\n+            timeseries `forecast_horizon` steps ahead (only one value).\n+        batch_size: Number of timeseries samples in each batch.\n+        shuffle: Whether to shuffle output samples, or instead draw them in chronological order.\n+        multi_horizon: See `forecast_horizon`.\n+\n+    Returns:\n+        A tf.data.Dataset instance.\n+    \"\"\"\n+\n+    inputs = timeseries_dataset_from_array(\n+        np.expand_dims(data_array[:-forecast_horizon], axis=-1),\n+        None,\n+        sequence_length=input_sequence_length,\n+        shuffle=False,\n+        batch_size=batch_size,\n+    )\n+\n+    target_offset = (\n+        input_sequence_length\n+        if multi_horizon\n+        else input_sequence_length + forecast_horizon - 1\n+    )\n+    target_seq_length = forecast_horizon if multi_horizon else 1\n+    targets = timeseries_dataset_from_array(\n+        data_array[target_offset:],\n+        None,\n+        sequence_length=target_seq_length,\n+        shuffle=False,\n+        batch_size=batch_size,\n+    )\n+\n+    dataset = tf.data.Dataset.zip((inputs, targets))\n+    if shuffle:\n+        dataset = dataset.shuffle(100)\n+\n+    return dataset.prefetch(16).cache()\n+\n+\n+train_dataset, val_dataset = (\n+    create_tf_dataset(\n+        data_array, input_sequence_length, forecast_horizon, batch_size\n+    )\n+    for data_array in [train_array, val_array]\n+)\n+\n+test_dataset = create_tf_dataset(\n+    test_array,\n+    input_sequence_length,\n+    forecast_horizon,\n+    batch_size=test_array.shape[0],\n+    shuffle=False,\n+    multi_horizon=multi_horizon,\n+)\n+\n+\n+\"\"\"\n+### Roads Graph\n+\n+As mentioned before, we assume that the road segments form a graph.\n+The `PeMSD7` dataset has the road segments distance. The next step\n+is to create the graph adjacency matrix from these distances. Following\n+[Yu et al., 2018](https://arxiv.org/abs/1709.04875) (equation 10) we assume there\n+is an edge between two nodes in the graph if the distance between the corresponding roads\n+is less than a threshold.\n+\"\"\"\n+\n+\n+def compute_adjacency_matrix(\n+    route_distances: np.ndarray, sigma2: float, epsilon: float\n+):\n+    \"\"\"Computes the adjacency matrix from distances matrix.\n+\n+    It uses the formula in https://github.com/VeritasYin/STGCN_IJCAI-18#data-preprocessing to\n+    compute an adjacency matrix from the distance matrix.\n+    The implementation follows that paper.\n+\n+    Args:\n+        route_distances: np.ndarray of shape `(num_routes, num_routes)`. Entry `i,j` of this array is the\n+            distance between roads `i,j`.\n+        sigma2: Determines the width of the Gaussian kernel applied to the square distances matrix.\n+        epsilon: A threshold specifying if there is an edge between two nodes. Specifically, `A[i,j]=1`\n+            if `np.exp(-w2[i,j] / sigma2) >= epsilon` and `A[i,j]=0` otherwise, where `A` is the adjacency\n+            matrix and `w2=route_distances * route_distances`\n+\n+    Returns:\n+        A boolean graph adjacency matrix.\n+    \"\"\"\n+    num_routes = route_distances.shape[0]\n+    route_distances = route_distances / 10000.0\n+    w2, w_mask = (\n+        route_distances * route_distances,\n+        np.ones([num_routes, num_routes]) - np.identity(num_routes),\n+    )\n+    return (np.exp(-w2 / sigma2) >= epsilon) * w_mask\n+\n+\n+\"\"\"\n+The function `compute_adjacency_matrix()` returns a boolean adjacency matrix\n+where 1 means there is an edge between two nodes. We use the following class\n+to store the information about the graph.\n+\"\"\"\n+\n+\n+class GraphInfo:\n+    def __init__(self, edges: typing.Tuple[list, list], num_nodes: int):\n+        self.edges = edges\n+        self.num_nodes = num_nodes\n+\n+\n+sigma2 = 0.1\n+epsilon = 0.5\n+adjacency_matrix = compute_adjacency_matrix(route_distances, sigma2, epsilon)\n+node_indices, neighbor_indices = np.where(adjacency_matrix == 1)\n+graph = GraphInfo(\n+    edges=(node_indices.tolist(), neighbor_indices.tolist()),\n+    num_nodes=adjacency_matrix.shape[0],\n+)\n+print(\n+    f\"number of nodes: {graph.num_nodes}, number of edges: {len(graph.edges[0])}\"\n+)\n+\n+\"\"\"\n+## Network architecture\n+\n+Our model for forecasting over the graph consists of a graph convolution\n+layer and a LSTM layer.\n+\"\"\"\n+\n+\"\"\"\n+### Graph convolution layer\n+\n+Our implementation of the graph convolution layer resembles the implementation\n+in [this Keras example](https://keras.io/examples/graph/gnn_citations/). Note that\n+in that example input to the layer is a 2D tensor of shape `(num_nodes,in_feat)`\n+but in our example the input to the layer is a 4D tensor of shape\n+`(num_nodes, batch_size, input_seq_length, in_feat)`. The graph convolution layer\n+performs the following steps:\n+\n+- The nodes' representations are computed in `self.compute_nodes_representation()`\n+by multiplying the input features by `self.weight`\n+- The aggregated neighbors' messages are computed in `self.compute_aggregated_messages()`\n+by first aggregating the neighbors' representations and then multiplying the results by\n+`self.weight`\n+- The final output of the layer is computed in `self.update()` by combining the nodes\n+representations and the neighbors' aggregated messages\n+\"\"\"\n+\n+\n+class GraphConv(layers.Layer):\n+    def __init__(\n+        self,\n+        in_feat,\n+        out_feat,\n+        graph_info: GraphInfo,\n+        aggregation_type=\"mean\",\n+        combination_type=\"concat\",\n+        activation: typing.Optional[str] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.in_feat = in_feat\n+        self.out_feat = out_feat\n+        self.graph_info = graph_info\n+        self.aggregation_type = aggregation_type\n+        self.combination_type = combination_type\n+        self.weight = tf.Variable(\n+            initial_value=keras.initializers.GlorotUniform()(\n+                shape=(in_feat, out_feat), dtype=\"float32\"\n+            ),\n+            trainable=True,\n+        )\n+        self.activation = layers.Activation(activation)\n+\n+    def aggregate(self, neighbour_representations: tf.Tensor):\n+        aggregation_func = {\n+            \"sum\": tf.math.unsorted_segment_sum,\n+            \"mean\": tf.math.unsorted_segment_mean,\n+            \"max\": tf.math.unsorted_segment_max,\n+        }.get(self.aggregation_type)\n+\n+        if aggregation_func:\n+            return aggregation_func(\n+                neighbour_representations,\n+                self.graph_info.edges[0],\n+                num_segments=self.graph_info.num_nodes,\n+            )\n+\n+        raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}\")\n+\n+    def compute_nodes_representation(self, features: tf.Tensor):\n+        \"\"\"Computes each node's representation.\n+\n+        The nodes' representations are obtained by multiplying the features tensor with\n+        `self.weight`. Note that\n+        `self.weight` has shape `(in_feat, out_feat)`.\n+\n+        Args:\n+            features: Tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n+\n+        Returns:\n+            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n+        \"\"\"\n+        return tf.matmul(features, self.weight)\n+\n+    def compute_aggregated_messages(self, features: tf.Tensor):\n+        neighbour_representations = tf.gather(\n+            features, self.graph_info.edges[1]\n+        )\n+        aggregated_messages = self.aggregate(neighbour_representations)\n+        return tf.matmul(aggregated_messages, self.weight)\n+\n+    def update(\n+        self, nodes_representation: tf.Tensor, aggregated_messages: tf.Tensor\n+    ):\n+        if self.combination_type == \"concat\":\n+            h = tf.concat([nodes_representation, aggregated_messages], axis=-1)\n+        elif self.combination_type == \"add\":\n+            h = nodes_representation + aggregated_messages\n+        else:\n+            raise ValueError(\n+                f\"Invalid combination type: {self.combination_type}.\"\n+            )\n+\n+        return self.activation(h)\n+\n+    def call(self, features: tf.Tensor):\n+        \"\"\"Forward pass.\n+\n+        Args:\n+            features: tensor of shape `(num_nodes, batch_size, input_seq_len, in_feat)`\n+\n+        Returns:\n+            A tensor of shape `(num_nodes, batch_size, input_seq_len, out_feat)`\n+        \"\"\"\n+        nodes_representation = self.compute_nodes_representation(features)\n+        aggregated_messages = self.compute_aggregated_messages(features)\n+        return self.update(nodes_representation, aggregated_messages)\n+\n+\n+\"\"\"\n+### LSTM plus graph convolution\n+\n+By applying the graph convolution layer to the input tensor, we get another tensor\n+containing the nodes' representations over time (another 4D tensor). For each time\n+step, a node's representation is informed by the information from its neighbors.\n+\n+To make good forecasts, however, we need not only information from the neighbors\n+but also we need to process the information over time. To this end, we can pass each\n+node's tensor through a recurrent layer. The `LSTMGC` layer below, first applies\n+a graph convolution layer to the inputs and then passes the results through a\n+`LSTM` layer.\n+\"\"\"\n+\n+\n+class LSTMGC(layers.Layer):\n+    \"\"\"Layer comprising a convolution layer followed by LSTM and dense layers.\"\"\"\n+\n+    def __init__(\n+        self,\n+        in_feat,\n+        out_feat,\n+        lstm_units: int,\n+        input_seq_len: int,\n+        output_seq_len: int,\n+        graph_info: GraphInfo,\n+        graph_conv_params: typing.Optional[dict] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        # graph conv layer\n+        if graph_conv_params is None:\n+            graph_conv_params = {\n+                \"aggregation_type\": \"mean\",\n+                \"combination_type\": \"concat\",\n+                \"activation\": None,\n+            }\n+        self.graph_conv = GraphConv(\n+            in_feat, out_feat, graph_info, **graph_conv_params\n+        )\n+\n+        self.lstm = layers.LSTM(lstm_units, activation=\"relu\")\n+        self.dense = layers.Dense(output_seq_len)\n+\n+        self.input_seq_len, self.output_seq_len = input_seq_len, output_seq_len\n+\n+    def call(self, inputs):\n+        \"\"\"Forward pass.\n+\n+        Args:\n+            inputs: tf.Tensor of shape `(batch_size, input_seq_len, num_nodes, in_feat)`\n+\n+        Returns:\n+            A tensor of shape `(batch_size, output_seq_len, num_nodes)`.\n+        \"\"\"\n+\n+        # convert shape to  (num_nodes, batch_size, input_seq_len, in_feat)\n+        inputs = tf.transpose(inputs, [2, 0, 1, 3])\n+\n+        gcn_out = self.graph_conv(\n+            inputs\n+        )  # gcn_out has shape: (num_nodes, batch_size, input_seq_len, out_feat)\n+        shape = tf.shape(gcn_out)\n+        num_nodes, batch_size, input_seq_len, out_feat = (\n+            shape[0],\n+            shape[1],\n+            shape[2],\n+            shape[3],\n+        )\n+\n+        # LSTM takes only 3D tensors as input\n+        gcn_out = tf.reshape(\n+            gcn_out, (batch_size * num_nodes, input_seq_len, out_feat)\n+        )\n+        lstm_out = self.lstm(\n+            gcn_out\n+        )  # lstm_out has shape: (batch_size * num_nodes, lstm_units)\n+\n+        dense_output = self.dense(\n+            lstm_out\n+        )  # dense_output has shape: (batch_size * num_nodes, output_seq_len)\n+        output = tf.reshape(\n+            dense_output, (num_nodes, batch_size, self.output_seq_len)\n+        )\n+        return tf.transpose(\n+            output, [1, 2, 0]\n+        )  # returns Tensor of shape (batch_size, output_seq_len, num_nodes)\n+\n+\n+\"\"\"\n+## Model training\n+\"\"\"\n+\n+in_feat = 1\n+batch_size = 64\n+epochs = 20\n+input_sequence_length = 12\n+forecast_horizon = 3\n+multi_horizon = False\n+out_feat = 10\n+lstm_units = 64\n+graph_conv_params = {\n+    \"aggregation_type\": \"mean\",\n+    \"combination_type\": \"concat\",\n+    \"activation\": None,\n+}\n+\n+st_gcn = LSTMGC(\n+    in_feat,\n+    out_feat,\n+    lstm_units,\n+    input_sequence_length,\n+    forecast_horizon,\n+    graph,\n+    graph_conv_params,\n+)\n+inputs = layers.Input((input_sequence_length, graph.num_nodes, in_feat))\n+outputs = st_gcn(inputs)\n+\n+model = keras.models.Model(inputs, outputs)\n+model.compile(\n+    optimizer=keras.optimizers.RMSprop(learning_rate=0.0002),\n+    loss=keras.losses.MeanSquaredError(),\n+)\n+model.fit(\n+    train_dataset,\n+    validation_data=val_dataset,\n+    epochs=epochs,\n+    callbacks=[keras.callbacks.EarlyStopping(patience=10)],\n+)\n+\n+\"\"\"\n+## Making forecasts on test set\n+\n+Now we can use the trained model to make forecasts for the test set. Below, we\n+compute the MAE of the model and compare it to the MAE of naive forecasts.\n+The naive forecasts are the last value of the speed for each node.\n+\"\"\"\n+\n+x_test, y = next(test_dataset.as_numpy_iterator())\n+y_pred = model.predict(x_test)\n+plt.figure(figsize=(18, 6))\n+plt.plot(y[:, 0, 0])\n+plt.plot(y_pred[:, 0, 0])\n+plt.legend([\"actual\", \"forecast\"])\n+\n+naive_mse, model_mse = (\n+    np.square(x_test[:, -1, :, 0] - y[:, 0, :]).mean(),\n+    np.square(y_pred[:, 0, :] - y[:, 0, :]).mean(),\n+)\n+print(f\"naive MAE: {naive_mse}, model MAE: {model_mse}\")\n+\n+\"\"\"\n+Of course, the goal here is to demonstrate the method,\n+not to achieve the best performance. To improve the\n+model's accuracy, all model hyperparameters should be tuned carefully. In addition,\n+several of the `LSTMGC` blocks can be stacked to increase the representation power\n+of the model.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06c28d2bdb8b811413b66a7e0d3944dfb4b222e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 876 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/12 | Churn Δ: 877 | Churn Cumulative: 1758 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.6697247706422018\n\nDIFF:\n@@ -0,0 +1,564 @@\n+\"\"\"\n+Title: Electroencephalogram Signal Classification for action identification\n+Author: [Suvaditya Mukherjee](https://github.com/suvadityamuk)\n+Date created: 2022/11/03\n+Last modified: 2022/11/05\n+Description: Training a Convolutional model to classify EEG signals produced by exposure to certain stimuli.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+The following example explores how we can make a Convolution-based Neural Network to\n+perform classification on Electroencephalogram signals captured when subjects were\n+exposed to different stimuli.\n+We train a model from scratch since such signal-classification models are fairly scarce\n+in pre-trained format.\n+The data we use is sourced from the UC Berkeley-Biosense Lab where the data was collected\n+from 15 subjects at the same time.\n+Our process is as follows:\n+\n+- Load the [UC Berkeley-Biosense Synchronized Brainwave Dataset](https://www.kaggle.com/datasets/berkeley-biosense/synchronized-brainwave-dataset)\n+- Visualize random samples from the data\n+- Pre-process, collate and scale the data to finally make a `tf.data.Dataset`\n+- Prepare class weights in order to tackle major imbalances\n+- Create a Conv1D and Dense-based model to perform classification\n+- Define callbacks and hyperparameters\n+- Train the model\n+- Plot metrics from History and perform evaluation\n+\n+This example needs the following external dependencies (Gdown, Scikit-learn, Pandas,\n+Numpy, Matplotlib). You can install it via the following commands.\n+\n+Gdown is an external package used to download large files from Google Drive. To know\n+more, you can refer to its [PyPi page here](https://pypi.org/project/gdown)\n+\"\"\"\n+\n+\n+\"\"\"\n+## Setup and Data Downloads\n+\n+First, lets install our dependencies:\n+\"\"\"\n+\n+\"\"\"shell\n+pip install gdown -q\n+pip install sklearn -q\n+pip install pandas -q\n+pip install numpy -q\n+pip install matplotlib -q\n+\"\"\"\n+\n+\"\"\"\n+Next, lets download our dataset.\n+The gdown package makes it easy to download the data from Google Drive:\n+\"\"\"\n+\n+\"\"\"shell\n+gdown 1V5B7Bt6aJm0UHbR7cRKBEK8jx7lYPVuX\n+# gdown will download eeg-data.csv onto the local drive for use. Total size of\n+# eeg-data.csv is 105.7 MB\n+\"\"\"\n+\n+import pandas as pd\n+import matplotlib.pyplot as plt\n+import json\n+import numpy as np\n+import keras_core as keras\n+from keras_core import layers\n+import tensorflow as tf\n+from sklearn import preprocessing, model_selection\n+import random\n+\n+QUALITY_THRESHOLD = 128\n+BATCH_SIZE = 64\n+SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 2\n+\n+\"\"\"\n+## Read data from `eeg-data.csv`\n+\n+We use the Pandas library to read the `eeg-data.csv` file and display the first 5 rows\n+using the `.head()` command\n+\"\"\"\n+\n+eeg = pd.read_csv(\"eeg-data.csv\")\n+\n+\"\"\"\n+We remove unlabeled samples from our dataset as they do not contribute to the model. We\n+also perform a `.drop()` operation on the columns that are not required for training data\n+preparation\n+\"\"\"\n+\n+unlabeled_eeg = eeg[eeg[\"label\"] == \"unlabeled\"]\n+eeg = eeg.loc[eeg[\"label\"] != \"unlabeled\"]\n+eeg = eeg.loc[eeg[\"label\"] != \"everyone paired\"]\n+\n+eeg.drop(\n+    [\n+        \"indra_time\",\n+        \"Unnamed: 0\",\n+        \"browser_latency\",\n+        \"reading_time\",\n+        \"attention_esense\",\n+        \"meditation_esense\",\n+        \"updatedAt\",\n+        \"createdAt\",\n+    ],\n+    axis=1,\n+    inplace=True,\n+)\n+\n+eeg.reset_index(drop=True, inplace=True)\n+eeg.head()\n+\n+\"\"\"\n+In the data, the samples recorded are given a score from 0 to 128 based on how\n+well-calibrated the sensor was (0 being best, 200 being worst). We filter the values\n+based on an arbitrary cutoff limit of 128.\n+\"\"\"\n+\n+\n+def convert_string_data_to_values(value_string):\n+    str_list = json.loads(value_string)\n+    return str_list\n+\n+\n+eeg[\"raw_values\"] = eeg[\"raw_values\"].apply(convert_string_data_to_values)\n+\n+eeg = eeg.loc[eeg[\"signal_quality\"] < QUALITY_THRESHOLD]\n+print(eeg.shape)\n+eeg.head()\n+\n+\"\"\"\n+## Visualize one random sample from the data\n+\"\"\"\n+\n+\"\"\"\n+We visualize one sample from the data to understand how the stimulus-induced signal looks\n+like\n+\"\"\"\n+\n+\n+def view_eeg_plot(idx):\n+    data = eeg.loc[idx, \"raw_values\"]\n+    plt.plot(data)\n+    plt.title(f\"Sample random plot\")\n+    plt.show()\n+\n+\n+view_eeg_plot(7)\n+\n+\"\"\"\n+## Pre-process and collate data\n+\"\"\"\n+\n+\"\"\"\n+There are a total of 67 different labels present in the data, where there are numbered\n+sub-labels. We collate them under a single label as per their numbering and replace them\n+in the data itself. Following this process, we perform simple Label encoding to get them\n+in an integer format.\n+\"\"\"\n+\n+print(\"Before replacing labels\")\n+print(eeg[\"label\"].unique(), \"\\n\")\n+print(len(eeg[\"label\"].unique()), \"\\n\")\n+\n+\n+eeg.replace(\n+    {\n+        \"label\": {\n+            \"blink1\": \"blink\",\n+            \"blink2\": \"blink\",\n+            \"blink3\": \"blink\",\n+            \"blink4\": \"blink\",\n+            \"blink5\": \"blink\",\n+            \"math1\": \"math\",\n+            \"math2\": \"math\",\n+            \"math3\": \"math\",\n+            \"math4\": \"math\",\n+            \"math5\": \"math\",\n+            \"math6\": \"math\",\n+            \"math7\": \"math\",\n+            \"math8\": \"math\",\n+            \"math9\": \"math\",\n+            \"math10\": \"math\",\n+            \"math11\": \"math\",\n+            \"math12\": \"math\",\n+            \"thinkOfItems-ver1\": \"thinkOfItems\",\n+            \"thinkOfItems-ver2\": \"thinkOfItems\",\n+            \"video-ver1\": \"video\",\n+            \"video-ver2\": \"video\",\n+            \"thinkOfItemsInstruction-ver1\": \"thinkOfItemsInstruction\",\n+            \"thinkOfItemsInstruction-ver2\": \"thinkOfItemsInstruction\",\n+            \"colorRound1-1\": \"colorRound1\",\n+            \"colorRound1-2\": \"colorRound1\",\n+            \"colorRound1-3\": \"colorRound1\",\n+            \"colorRound1-4\": \"colorRound1\",\n+            \"colorRound1-5\": \"colorRound1\",\n+            \"colorRound1-6\": \"colorRound1\",\n+            \"colorRound2-1\": \"colorRound2\",\n+            \"colorRound2-2\": \"colorRound2\",\n+            \"colorRound2-3\": \"colorRound2\",\n+            \"colorRound2-4\": \"colorRound2\",\n+            \"colorRound2-5\": \"colorRound2\",\n+            \"colorRound2-6\": \"colorRound2\",\n+            \"colorRound3-1\": \"colorRound3\",\n+            \"colorRound3-2\": \"colorRound3\",\n+            \"colorRound3-3\": \"colorRound3\",\n+            \"colorRound3-4\": \"colorRound3\",\n+            \"colorRound3-5\": \"colorRound3\",\n+            \"colorRound3-6\": \"colorRound3\",\n+            \"colorRound4-1\": \"colorRound4\",\n+            \"colorRound4-2\": \"colorRound4\",\n+            \"colorRound4-3\": \"colorRound4\",\n+            \"colorRound4-4\": \"colorRound4\",\n+            \"colorRound4-5\": \"colorRound4\",\n+            \"colorRound4-6\": \"colorRound4\",\n+            \"colorRound5-1\": \"colorRound5\",\n+            \"colorRound5-2\": \"colorRound5\",\n+            \"colorRound5-3\": \"colorRound5\",\n+            \"colorRound5-4\": \"colorRound5\",\n+            \"colorRound5-5\": \"colorRound5\",\n+            \"colorRound5-6\": \"colorRound5\",\n+            \"colorInstruction1\": \"colorInstruction\",\n+            \"colorInstruction2\": \"colorInstruction\",\n+            \"readyRound1\": \"readyRound\",\n+            \"readyRound2\": \"readyRound\",\n+            \"readyRound3\": \"readyRound\",\n+            \"readyRound4\": \"readyRound\",\n+            \"readyRound5\": \"readyRound\",\n+            \"colorRound1\": \"colorRound\",\n+            \"colorRound2\": \"colorRound\",\n+            \"colorRound3\": \"colorRound\",\n+            \"colorRound4\": \"colorRound\",\n+            \"colorRound5\": \"colorRound\",\n+        }\n+    },\n+    inplace=True,\n+)\n+\n+print(\"After replacing labels\")\n+print(eeg[\"label\"].unique())\n+print(len(eeg[\"label\"].unique()))\n+\n+le = preprocessing.LabelEncoder()  # Generates a look-up table\n+le.fit(eeg[\"label\"])\n+eeg[\"label\"] = le.transform(eeg[\"label\"])\n+\n+\"\"\"\n+We extract the number of unique classes present in the data\n+\"\"\"\n+\n+num_classes = len(eeg[\"label\"].unique())\n+print(num_classes)\n+\n+\"\"\"\n+We now visualize the number of samples present in each class using a Bar plot.\n+\"\"\"\n+\n+plt.bar(range(num_classes), eeg[\"label\"].value_counts())\n+plt.title(\"Number of samples per class\")\n+plt.show()\n+\n+\"\"\"\n+## Scale and split data\n+\"\"\"\n+\n+\"\"\"\n+We perform a simple Min-Max scaling to bring the value-range between 0 and 1. We do not\n+use Standard Scaling as the data does not follow a Gaussian distribution.\n+\"\"\"\n+\n+scaler = preprocessing.MinMaxScaler()\n+series_list = [\n+    scaler.fit_transform(np.asarray(i).reshape(-1, 1))\n+    for i in eeg[\"raw_values\"]\n+]\n+\n+labels_list = [i for i in eeg[\"label\"]]\n+\n+\"\"\"\n+We now create a Train-test split with a 15% holdout set. Following this, we reshape the\n+data to create a sequence of length 512. We also convert the labels from their current\n+label-encoded form to a one-hot encoding to enable use of several different\n+`keras.metrics` functions.\n+\"\"\"\n+\n+x_train, x_test, y_train, y_test = model_selection.train_test_split(\n+    series_list, labels_list, test_size=0.15, random_state=42, shuffle=True\n+)\n+\n+print(\n+    f\"Length of x_train : {len(x_train)}\\nLength of x_test : {len(x_test)}\\nLength of y_train : {len(y_train)}\\nLength of y_test : {len(y_test)}\"\n+)\n+\n+x_train = np.asarray(x_train).astype(np.float32).reshape(-1, 512, 1)\n+y_train = np.asarray(y_train).astype(np.float32).reshape(-1, 1)\n+y_train = keras.utils.to_categorical(y_train)\n+\n+x_test = np.asarray(x_test).astype(np.float32).reshape(-1, 512, 1)\n+y_test = np.asarray(y_test).astype(np.float32).reshape(-1, 1)\n+y_test = keras.utils.to_categorical(y_test)\n+\n+\"\"\"\n+## Prepare `tf.data.Dataset`\n+\"\"\"\n+\n+\"\"\"\n+We now create a `tf.data.Dataset` from this data to prepare it for training. We also\n+shuffle and batch the data for use later.\n+\"\"\"\n+\n+train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+\n+train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n+test_dataset = test_dataset.batch(BATCH_SIZE)\n+\n+\"\"\"\n+## Make Class Weights using Naive method\n+\"\"\"\n+\n+\"\"\"\n+As we can see from the plot of number of samples per class, the dataset is imbalanced.\n+Hence, we **calculate weights for each class** to make sure that the model is trained in\n+a fair manner without preference to any specific class due to greater number of samples.\n+\n+We use a naive method to calculate these weights, finding an **inverse proportion** of\n+each class and using that as the weight.\n+\"\"\"\n+\n+vals_dict = {}\n+for i in eeg[\"label\"]:\n+    if i in vals_dict.keys():\n+        vals_dict[i] += 1\n+    else:\n+        vals_dict[i] = 1\n+total = sum(vals_dict.values())\n+\n+# Formula used - Naive method where\n+# weight = 1 - (no. of samples present / total no. of samples)\n+# So more the samples, lower the weight\n+\n+weight_dict = {k: (1 - (v / total)) for k, v in vals_dict.items()}\n+print(weight_dict)\n+\n+\"\"\"\n+## Define simple function to plot all the metrics present in a `keras.callbacks.History`\n+object\n+\"\"\"\n+\n+\n+def plot_history_metrics(history: keras.callbacks.History):\n+    total_plots = len(history.history)\n+    cols = total_plots // 2\n+\n+    rows = total_plots // cols\n+\n+    if total_plots % cols != 0:\n+        rows += 1\n+\n+    pos = range(1, total_plots + 1)\n+    plt.figure(figsize=(15, 10))\n+    for i, (key, value) in enumerate(history.history.items()):\n+        plt.subplot(rows, cols, pos[i])\n+        plt.plot(range(len(value)), value)\n+        plt.title(str(key))\n+    plt.show()\n+\n+\n+\"\"\"\n+## Define function to generate Convolutional model\n+\"\"\"\n+\n+\n+def create_model():\n+    input_layer = keras.Input(shape=(512, 1))\n+\n+    x = layers.Conv1D(\n+        filters=32, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n+    )(input_layer)\n+    x = layers.BatchNormalization()(x)\n+\n+    x = layers.Conv1D(\n+        filters=64, kernel_size=3, strides=2, activation=\"relu\", padding=\"same\"\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    x = layers.Conv1D(\n+        filters=128, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    x = layers.Conv1D(\n+        filters=256, kernel_size=5, strides=2, activation=\"relu\", padding=\"same\"\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    x = layers.Conv1D(\n+        filters=512, kernel_size=7, strides=2, activation=\"relu\", padding=\"same\"\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    x = layers.Conv1D(\n+        filters=1024,\n+        kernel_size=7,\n+        strides=2,\n+        activation=\"relu\",\n+        padding=\"same\",\n+    )(x)\n+    x = layers.BatchNormalization()(x)\n+\n+    x = layers.Dropout(0.2)(x)\n+\n+    x = layers.Flatten()(x)\n+\n+    x = layers.Dense(4096, activation=\"relu\")(x)\n+    x = layers.Dropout(0.2)(x)\n+\n+    x = layers.Dense(\n+        2048, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n+    )(x)\n+    x = layers.Dropout(0.2)(x)\n+\n+    x = layers.Dense(\n+        1024, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n+    )(x)\n+    x = layers.Dropout(0.2)(x)\n+    x = layers.Dense(\n+        128, activation=\"relu\", kernel_regularizer=keras.regularizers.L2()\n+    )(x)\n+    output_layer = layers.Dense(num_classes, activation=\"softmax\")(x)\n+\n+    return keras.Model(inputs=input_layer, outputs=output_layer)\n+\n+\n+\"\"\"\n+## Get Model summary\n+\"\"\"\n+\n+conv_model = create_model()\n+conv_model.summary()\n+\n+\"\"\"\n+## Define callbacks, optimizer, loss and metrics\n+\"\"\"\n+\n+\"\"\"\n+We set the number of epochs at 30 after performing extensive experimentation. It was seen\n+that this was the optimal number, after performing Early-Stopping analysis as well.\n+We define a Model Checkpoint callback to make sure that we only get the best model\n+weights.\n+We also define a ReduceLROnPlateau as there were several cases found during\n+experimentation where the loss stagnated after a certain point. On the other hand, a\n+direct LRScheduler was found to be too aggressive in its decay.\n+\"\"\"\n+\n+epochs = 30\n+\n+callbacks = [\n+    keras.callbacks.ModelCheckpoint(\n+        \"best_model.keras\", save_best_only=True, monitor=\"loss\"\n+    ),\n+    keras.callbacks.ReduceLROnPlateau(\n+        monitor=\"val_top_k_categorical_accuracy\",\n+        factor=0.2,\n+        patience=2,\n+        min_lr=0.000001,\n+    ),\n+]\n+\n+optimizer = keras.optimizers.Adam(amsgrad=True, learning_rate=0.001)\n+loss = keras.losses.CategoricalCrossentropy()\n+\n+\"\"\"\n+## Compile model and call `model.fit()`\n+\"\"\"\n+\n+\"\"\"\n+We use the `Adam` optimizer since it is commonly considered the best choice for\n+preliminary training, and was found to be the best optimizer.\n+We use `CategoricalCrossentropy` as the loss as our labels are in a one-hot-encoded form.\n+\n+We define the `TopKCategoricalAccuracy(k=3)`, `AUC`, `Precision` and `Recall` metrics to\n+further aid in understanding the model better.\n+\"\"\"\n+\n+conv_model.compile(\n+    optimizer=optimizer,\n+    loss=loss,\n+    metrics=[\n+        keras.metrics.TopKCategoricalAccuracy(k=3),\n+        keras.metrics.AUC(),\n+        keras.metrics.Precision(),\n+        keras.metrics.Recall(),\n+    ],\n+)\n+\n+conv_model_history = conv_model.fit(\n+    train_dataset,\n+    epochs=epochs,\n+    callbacks=callbacks,\n+    validation_data=test_dataset,\n+    class_weight=weight_dict,\n+)\n+\n+\"\"\"\n+## Visualize model metrics during training\n+\"\"\"\n+\n+\"\"\"\n+We use the function defined above to see model metrics during training.\n+\"\"\"\n+\n+plot_history_metrics(conv_model_history)\n+\n+\"\"\"\n+## Evaluate model on test data\n+\"\"\"\n+\n+loss, accuracy, auc, precision, recall = conv_model.evaluate(test_dataset)\n+print(f\"Loss : {loss}\")\n+print(f\"Top 3 Categorical Accuracy : {accuracy}\")\n+print(f\"Area under the Curve (ROC) : {auc}\")\n+print(f\"Precision : {precision}\")\n+print(f\"Recall : {recall}\")\n+\n+\n+def view_evaluated_eeg_plots(model):\n+    start_index = random.randint(10, len(eeg))\n+    end_index = start_index + 11\n+    data = eeg.loc[start_index:end_index, \"raw_values\"]\n+    data_array = [\n+        scaler.fit_transform(np.asarray(i).reshape(-1, 1)) for i in data\n+    ]\n+    data_array = [np.asarray(data_array).astype(np.float32).reshape(-1, 512, 1)]\n+    original_labels = eeg.loc[start_index:end_index, \"label\"]\n+    predicted_labels = np.argmax(model.predict(data_array, verbose=0), axis=1)\n+    original_labels = [\n+        le.inverse_transform(np.array(label).reshape(-1))[0]\n+        for label in original_labels\n+    ]\n+    predicted_labels = [\n+        le.inverse_transform(np.array(label).reshape(-1))[0]\n+        for label in predicted_labels\n+    ]\n+    total_plots = 12\n+    cols = total_plots // 3\n+    rows = total_plots // cols\n+    if total_plots % cols != 0:\n+        rows += 1\n+    pos = range(1, total_plots + 1)\n+    fig = plt.figure(figsize=(20, 10))\n+    for i, (plot_data, og_label, pred_label) in enumerate(\n+        zip(data, original_labels, predicted_labels)\n+    ):\n+        plt.subplot(rows, cols, pos[i])\n+        plt.plot(plot_data)\n+        plt.title(f\"Actual Label : {og_label}\\nPredicted Label : {pred_label}\")\n+        fig.subplots_adjust(hspace=0.5)\n+    plt.show()\n+\n+\n+view_evaluated_eeg_plots(conv_model)\n\n@@ -0,0 +1,308 @@\n+\"\"\"\n+Title: Timeseries anomaly detection using an Autoencoder\n+Author: [pavithrasv](https://github.com/pavithrasv)\n+Date created: 2020/05/31\n+Last modified: 2020/05/31\n+Description: Detect anomalies in a timeseries using an Autoencoder.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This script demonstrates how you can use a reconstruction convolutional\n+autoencoder model to detect anomalies in timeseries data.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import pandas as pd\n+import keras_core as keras\n+from keras_core import layers\n+from matplotlib import pyplot as plt\n+\n+\"\"\"\n+## Load the data\n+\n+We will use the [Numenta Anomaly Benchmark(NAB)](\n+https://www.kaggle.com/boltzmannbrain/nab) dataset. It provides artifical\n+timeseries data containing labeled anomalous periods of behavior. Data are\n+ordered, timestamped, single-valued metrics.\n+\n+We will use the `art_daily_small_noise.csv` file for training and the\n+`art_daily_jumpsup.csv` file for testing. The simplicity of this dataset\n+allows us to demonstrate anomaly detection effectively.\n+\"\"\"\n+\n+master_url_root = \"https://raw.githubusercontent.com/numenta/NAB/master/data/\"\n+\n+df_small_noise_url_suffix = \"artificialNoAnomaly/art_daily_small_noise.csv\"\n+df_small_noise_url = master_url_root + df_small_noise_url_suffix\n+df_small_noise = pd.read_csv(\n+    df_small_noise_url, parse_dates=True, index_col=\"timestamp\"\n+)\n+\n+df_daily_jumpsup_url_suffix = \"artificialWithAnomaly/art_daily_jumpsup.csv\"\n+df_daily_jumpsup_url = master_url_root + df_daily_jumpsup_url_suffix\n+df_daily_jumpsup = pd.read_csv(\n+    df_daily_jumpsup_url, parse_dates=True, index_col=\"timestamp\"\n+)\n+\n+\"\"\"\n+## Quick look at the data\n+\"\"\"\n+\n+print(df_small_noise.head())\n+\n+print(df_daily_jumpsup.head())\n+\n+\"\"\"\n+## Visualize the data\n+### Timeseries data without anomalies\n+\n+We will use the following data for training.\n+\"\"\"\n+fig, ax = plt.subplots()\n+df_small_noise.plot(legend=False, ax=ax)\n+plt.show()\n+\n+\"\"\"\n+### Timeseries data with anomalies\n+\n+We will use the following data for testing and see if the sudden jump up in the\n+data is detected as an anomaly.\n+\"\"\"\n+fig, ax = plt.subplots()\n+df_daily_jumpsup.plot(legend=False, ax=ax)\n+plt.show()\n+\n+\"\"\"\n+## Prepare training data\n+\n+Get data values from the training timeseries data file and normalize the\n+`value` data. We have a `value` for every 5 mins for 14 days.\n+\n+-   24 * 60 / 5 = **288 timesteps per day**\n+-   288 * 14 = **4032 data points** in total\n+\"\"\"\n+\n+\n+# Normalize and save the mean and std we get,\n+# for normalizing test data.\n+training_mean = df_small_noise.mean()\n+training_std = df_small_noise.std()\n+df_training_value = (df_small_noise - training_mean) / training_std\n+print(\"Number of training samples:\", len(df_training_value))\n+\n+\"\"\"\n+### Create sequences\n+Create sequences combining `TIME_STEPS` contiguous data values from the\n+training data.\n+\"\"\"\n+\n+TIME_STEPS = 288\n+\n+\n+# Generated training sequences for use in the model.\n+def create_sequences(values, time_steps=TIME_STEPS):\n+    output = []\n+    for i in range(len(values) - time_steps + 1):\n+        output.append(values[i : (i + time_steps)])\n+    return np.stack(output)\n+\n+\n+x_train = create_sequences(df_training_value.values)\n+print(\"Training input shape: \", x_train.shape)\n+\n+\"\"\"\n+## Build a model\n+\n+We will build a convolutional reconstruction autoencoder model. The model will\n+take input of shape `(batch_size, sequence_length, num_features)` and return\n+output of the same shape. In this case, `sequence_length` is 288 and\n+`num_features` is 1.\n+\"\"\"\n+\n+model = keras.Sequential(\n+    [\n+        layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n+        layers.Conv1D(\n+            filters=32,\n+            kernel_size=7,\n+            padding=\"same\",\n+            strides=2,\n+            activation=\"relu\",\n+        ),\n+        layers.Dropout(rate=0.2),\n+        layers.Conv1D(\n+            filters=16,\n+            kernel_size=7,\n+            padding=\"same\",\n+            strides=2,\n+            activation=\"relu\",\n+        ),\n+        layers.Conv1DTranspose(\n+            filters=16,\n+            kernel_size=7,\n+            padding=\"same\",\n+            strides=2,\n+            activation=\"relu\",\n+        ),\n+        layers.Dropout(rate=0.2),\n+        layers.Conv1DTranspose(\n+            filters=32,\n+            kernel_size=7,\n+            padding=\"same\",\n+            strides=2,\n+            activation=\"relu\",\n+        ),\n+        layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n+    ]\n+)\n+model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n+model.summary()\n+\n+\"\"\"\n+## Train the model\n+\n+Please note that we are using `x_train` as both the input and the target\n+since this is a reconstruction model.\n+\"\"\"\n+\n+history = model.fit(\n+    x_train,\n+    x_train,\n+    epochs=50,\n+    batch_size=128,\n+    validation_split=0.1,\n+    callbacks=[\n+        keras.callbacks.EarlyStopping(\n+            monitor=\"val_loss\", patience=5, mode=\"min\"\n+        )\n+    ],\n+)\n+\n+\"\"\"\n+Let's plot training and validation loss to see how the training went.\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n+plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n+plt.legend()\n+plt.show()\n+\n+\"\"\"\n+## Detecting anomalies\n+\n+We will detect anomalies by determining how well our model can reconstruct\n+the input data.\n+\n+\n+1.   Find MAE loss on training samples.\n+2.   Find max MAE loss value. This is the worst our model has performed trying\n+to reconstruct a sample. We will make this the `threshold` for anomaly\n+detection.\n+3.   If the reconstruction loss for a sample is greater than this `threshold`\n+value then we can infer that the model is seeing a pattern that it isn't\n+familiar with. We will label this sample as an `anomaly`.\n+\n+\n+\"\"\"\n+\n+# Get train MAE loss.\n+x_train_pred = model.predict(x_train)\n+train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n+\n+plt.hist(train_mae_loss, bins=50)\n+plt.xlabel(\"Train MAE loss\")\n+plt.ylabel(\"No of samples\")\n+plt.show()\n+\n+# Get reconstruction loss threshold.\n+threshold = np.max(train_mae_loss)\n+print(\"Reconstruction error threshold: \", threshold)\n+\n+\"\"\"\n+### Compare recontruction\n+\n+Just for fun, let's see how our model has recontructed the first sample.\n+This is the 288 timesteps from day 1 of our training dataset.\n+\"\"\"\n+\n+# Checking how the first sequence is learnt\n+plt.plot(x_train[0])\n+plt.plot(x_train_pred[0])\n+plt.show()\n+\n+\"\"\"\n+### Prepare test data\n+\"\"\"\n+\n+\n+df_test_value = (df_daily_jumpsup - training_mean) / training_std\n+fig, ax = plt.subplots()\n+df_test_value.plot(legend=False, ax=ax)\n+plt.show()\n+\n+# Create sequences from test values.\n+x_test = create_sequences(df_test_value.values)\n+print(\"Test input shape: \", x_test.shape)\n+\n+# Get test MAE loss.\n+x_test_pred = model.predict(x_test)\n+test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n+test_mae_loss = test_mae_loss.reshape((-1))\n+\n+plt.hist(test_mae_loss, bins=50)\n+plt.xlabel(\"test MAE loss\")\n+plt.ylabel(\"No of samples\")\n+plt.show()\n+\n+# Detect all the samples which are anomalies.\n+anomalies = test_mae_loss > threshold\n+print(\"Number of anomaly samples: \", np.sum(anomalies))\n+print(\"Indices of anomaly samples: \", np.where(anomalies))\n+\n+\"\"\"\n+## Plot anomalies\n+\n+We now know the samples of the data which are anomalies. With this, we will\n+find the corresponding `timestamps` from the original test data. We will be\n+using the following method to do that:\n+\n+Let's say time_steps = 3 and we have 10 training values. Our `x_train` will\n+look like this:\n+\n+- 0, 1, 2\n+- 1, 2, 3\n+- 2, 3, 4\n+- 3, 4, 5\n+- 4, 5, 6\n+- 5, 6, 7\n+- 6, 7, 8\n+- 7, 8, 9\n+\n+All except the initial and the final time_steps-1 data values, will appear in\n+`time_steps` number of samples. So, if we know that the samples\n+[(3, 4, 5), (4, 5, 6), (5, 6, 7)] are anomalies, we can say that the data point\n+5 is an anomaly.\n+\"\"\"\n+\n+# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n+anomalous_data_indices = []\n+for data_idx in range(TIME_STEPS - 1, len(df_test_value) - TIME_STEPS + 1):\n+    if np.all(anomalies[data_idx - TIME_STEPS + 1 : data_idx]):\n+        anomalous_data_indices.append(data_idx)\n+\n+\"\"\"\n+Let's overlay the anomalies on the original test data plot.\n+\"\"\"\n+\n+df_subset = df_daily_jumpsup.iloc[anomalous_data_indices]\n+fig, ax = plt.subplots()\n+df_daily_jumpsup.plot(legend=False, ax=ax)\n+df_subset.plot(legend=False, ax=ax, color=\"r\")\n+plt.show()\n\n@@ -226,7 +226,10 @@ def _update_confusion_matrix_variables_optimized(\n     # Since the predict value has to be strictly greater than the thresholds,\n     # eg, buckets like [0, 0.5], (0.5, 1], and 0.5 belongs to first bucket.\n     # We have to use math.ceil(val) - 1 for the bucket.\n-    bucket_indices = ops.ceil(y_pred * (num_thresholds - 1)) - 1\n+    bucket_indices = (\n+        ops.ceil(y_pred * (ops.cast(num_thresholds, dtype=y_pred.dtype) - 1))\n+        - 1\n+    )\n \n     if thresholds_with_epsilon:\n         # In this case, the first bucket should actually take into account since\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
