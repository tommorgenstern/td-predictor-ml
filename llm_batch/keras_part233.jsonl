{"custom_id": "keras#f1df4297c27f4a2dab675d253be67eb0372817af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 11 | Churn Cumulative: 6411 | Contributors (this commit): 83 | Commits (past 90d): 28 | Contributors (cumulative): 83 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1275,21 +1275,14 @@ class Model(Container):\n             or list of arrays of predictions\n             (if the model has multiple outputs).\n         \"\"\"\n-\n-        if hasattr(self, 'metrics'):\n-            for m in self.metrics:\n-                if isinstance(m, Layer):\n-                    m.reset_states()\n         num_samples = self._check_num_samples(ins, batch_size,\n                                               steps,\n                                               'steps')\n         if verbose == 1:\n             if steps is not None:\n-                progbar = Progbar(target=steps,\n-                                  stateful_metrics=self.stateful_metric_names)\n+                progbar = Progbar(target=steps)\n             else:\n-                progbar = Progbar(target=num_samples,\n-                                  stateful_metrics=self.stateful_metric_names)\n+                progbar = Progbar(target=num_samples)\n \n         indices_for_conversion_to_dense = []\n         for i in range(len(self._feed_inputs)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#173a1a545954bae38e40f4fb0bde228765a9b059", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 44 | Churn Cumulative: 640 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 20 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -169,20 +169,26 @@ class Tokenizer(object):\n         self.document_count = 0\n         self.char_level = char_level\n         self.oov_token = oov_token\n+        self.index_docs = {}\n \n     def fit_on_texts(self, texts):\n         \"\"\"Updates internal vocabulary based on a list of texts.\n+        In the case where texts contains lists, we assume each entry of the lists\n+        to be a token.\n \n         Required before using `texts_to_sequences` or `texts_to_matrix`.\n \n         # Arguments\n             texts: can be a list of strings,\n-                or a generator of strings (for memory-efficiency)\n+                a generator of strings (for memory-efficiency),\n+                or a list of list of strings.\n         \"\"\"\n-        self.document_count = 0\n         for text in texts:\n             self.document_count += 1\n-            seq = text if self.char_level else text_to_word_sequence(text,\n+            if self.char_level or isinstance(text, list):\n+                seq = text\n+            else:\n+                seq = text_to_word_sequence(text,\n                                             self.filters,\n                                             self.lower,\n                                             self.split)\n@@ -208,7 +214,6 @@ class Tokenizer(object):\n             if i is None:\n                 self.word_index[self.oov_token] = len(self.word_index) + 1\n \n-        self.index_docs = {}\n         for w, c in list(self.word_docs.items()):\n             self.index_docs[self.word_index[w]] = c\n \n@@ -222,8 +227,7 @@ class Tokenizer(object):\n             sequences: A list of sequence.\n                 A \"sequence\" is a list of integer word indices.\n         \"\"\"\n-        self.document_count = len(sequences)\n-        self.index_docs = {}\n+        self.document_count += len(sequences)\n         for seq in sequences:\n             seq = set(seq)\n             for i in seq:\n@@ -250,7 +254,9 @@ class Tokenizer(object):\n         return res\n \n     def texts_to_sequences_generator(self, texts):\n-        \"\"\"Transforms each text in texts in a sequence of integers.\n+        \"\"\"Transforms each text in `texts` in a sequence of integers.\n+        Each item in texts can also be a list, in which case we assume each item of that list\n+        to be a token.\n \n         Only top \"num_words\" most frequent words will be taken into account.\n         Only words known by the tokenizer will be taken into account.\n@@ -263,7 +269,10 @@ class Tokenizer(object):\n         \"\"\"\n         num_words = self.num_words\n         for text in texts:\n-            seq = text if self.char_level else text_to_word_sequence(text,\n+            if self.char_level or isinstance(text, list):\n+                seq = text\n+            else:\n+                seq = text_to_word_sequence(text,\n                                             self.filters,\n                                             self.lower,\n                                             self.split)\n\n@@ -49,6 +49,25 @@ def test_tokenizer():\n         matrix = tokenizer.texts_to_matrix(texts, mode)\n \n \n+def test_sequential_fit():\n+    texts = ['The cat sat on the mat.',\n+             'The dog sat on the log.',\n+             'Dogs and cats living together.']\n+    word_sequences = [\n+        ['The', 'cat', 'is', 'sitting'],\n+        ['The', 'dog', 'is', 'standing']\n+    ]\n+\n+    tokenizer = Tokenizer()\n+    tokenizer.fit_on_texts(texts)\n+    tokenizer.fit_on_texts(word_sequences)\n+\n+    assert tokenizer.document_count == 5\n+\n+    tokenizer.texts_to_matrix(texts)\n+    tokenizer.texts_to_matrix(word_sequences)\n+\n+\n def test_text_to_word_sequence():\n     text = 'hello! ? world!'\n     assert text_to_word_sequence(text) == ['hello', 'world']\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4eab0556d29f11ff41758d80c15d6457263f6a93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 161 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 16/8 | Churn Δ: 169 | Churn Cumulative: 9379 | Contributors (this commit): 63 | Commits (past 90d): 22 | Contributors (cumulative): 71 | DMM Complexity: 0.76\n\nDIFF:\n@@ -25,6 +25,7 @@ from ..legacy import interfaces\n \n try:\n     import h5py\n+    HDF5_OBJECT_HEADER_LIMIT = 64512\n except ImportError:\n     h5py = None\n \n@@ -2867,10 +2868,72 @@ def _collect_input_shape(input_tensors):\n     return shapes\n \n \n+def _save_attributes_to_hdf5_group(group, name, data):\n+    \"\"\"Saves attributes (data) of the specified name into the HDF5 group.\n+    This method deals with an inherent problem of HDF5 file which is not\n+    able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n+\n+    # Arguments\n+        group: A pointer to a HDF5 group.\n+        name: A name of the attributes to save.\n+        data: Attributes data to store.\n+    \"\"\"\n+    # Check that no item in `data` is larger than `HDF5_OBJECT_HEADER_LIMIT`\n+    # because in that case even chunking the array would not make the saving\n+    # possible.\n+    bad_attributes = [x for x in data if len(x) > HDF5_OBJECT_HEADER_LIMIT]\n+\n+    # Expecting this to never be true.\n+    if len(bad_attributes) > 0:\n+        raise RuntimeError('the following attributes cannot be saved to HDF5 file '\n+                           'because they are larger than %d bytes: %s'\n+                           % (HDF5_OBJECT_HEADER_LIMIT,\n+                              ', '.join([x for x in bad_attributes])))\n+\n+    data_npy = np.asarray(data)\n+\n+    num_chunks = 1\n+    chunked_data = np.array_split(data_npy, num_chunks)\n+\n+    # This will never loop forever thanks to the test above.\n+    while any(map(lambda x: x.nbytes > HDF5_OBJECT_HEADER_LIMIT, chunked_data)):\n+        num_chunks += 1\n+        chunked_data = np.array_split(data_npy, num_chunks)\n+\n+    if num_chunks > 1:\n+        for chunk_id, chunk_data in enumerate(chunked_data):\n+            group.attrs['%s%d' % (name, chunk_id)] = chunk_data\n+    else:\n+        group.attrs[name] = data\n+\n+\n+def _load_attributes_from_hdf5_group(group, name):\n+    \"\"\"Loads attributes of the specified name from the HDF5 group. This method\n+    deals with an inherent problem of HDF5 file which is not able to store\n+    data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n+\n+    # Arguments\n+        group: A pointer to a HDF5 group.\n+        name: A name of the attributes to load.\n+\n+    # Returns\n+        data: Attributes data.\n+    \"\"\"\n+    if name in group.attrs:\n+        data = [n.decode('utf8') for n in group.attrs[name]]\n+    else:\n+        data = []\n+        chunk_id = 0\n+        while ('%s%d' % (name, chunk_id)) in group.attrs:\n+            data.extend([n.decode('utf8') for n in group.attrs['%s%d' % (name, chunk_id)]])\n+            chunk_id += 1\n+    return data\n+\n+\n def save_weights_to_hdf5_group(f, layers):\n     from .. import __version__ as keras_version\n \n-    f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in layers]\n+    _save_attributes_to_hdf5_group(f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n     f.attrs['backend'] = K.backend().encode('utf8')\n     f.attrs['keras_version'] = str(keras_version).encode('utf8')\n \n@@ -2885,7 +2948,7 @@ def save_weights_to_hdf5_group(f, layers):\n             else:\n                 name = 'param_' + str(i)\n             weight_names.append(name.encode('utf8'))\n-        g.attrs['weight_names'] = weight_names\n+        _save_attributes_to_hdf5_group(g, 'weight_names', weight_names)\n         for name, val in zip(weight_names, weight_values):\n             param_dset = g.create_dataset(name, val.shape,\n                                           dtype=val.dtype)\n@@ -3146,11 +3209,11 @@ def load_weights_from_hdf5_group(f, layers, reshape=False):\n         if weights:\n             filtered_layers.append(layer)\n \n-    layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n+    layer_names = _load_attributes_from_hdf5_group(f, 'layer_names')\n     filtered_layer_names = []\n     for name in layer_names:\n         g = f[name]\n-        weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+        weight_names = _load_attributes_from_hdf5_group(g, 'weight_names')\n         if weight_names:\n             filtered_layer_names.append(name)\n     layer_names = filtered_layer_names\n@@ -3165,7 +3228,7 @@ def load_weights_from_hdf5_group(f, layers, reshape=False):\n     weight_value_tuples = []\n     for k, name in enumerate(layer_names):\n         g = f[name]\n-        weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+        weight_names = _load_attributes_from_hdf5_group(g, 'weight_names')\n         weight_values = [g[weight_name] for weight_name in weight_names]\n         layer = filtered_layers[k]\n         symbolic_weights = layer.weights\n@@ -3220,7 +3283,7 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n         original_backend = None\n \n     # New file format.\n-    layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n+    layer_names = _load_attributes_from_hdf5_group(f, 'layer_names')\n \n     # Reverse index of layer name to list of layers with name.\n     index = {}\n@@ -3233,7 +3296,7 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n     weight_value_tuples = []\n     for k, name in enumerate(layer_names):\n         g = f[name]\n-        weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+        weight_names = _load_attributes_from_hdf5_group(g, 'weight_names')\n         weight_values = [g[weight_name] for weight_name in weight_names]\n \n         for layer in index.get(name, []):\n\n@@ -1,8 +1,10 @@\n import pytest\n import os\n+import h5py\n import tempfile\n import numpy as np\n-from numpy.testing import assert_allclose, assert_raises\n+from numpy.testing import assert_allclose\n+from numpy.testing import assert_raises\n \n from keras import backend as K\n from keras.models import Model, Sequential\n@@ -408,6 +410,94 @@ def test_saving_custom_activation_function():\n     assert_allclose(out, out2, atol=1e-05)\n \n \n+@keras_test\n+def test_saving_model_with_long_layer_names():\n+    # This layer name will make the `layers_name` HDF5 attribute blow\n+    # out of proportion. Note that it fits into the internal HDF5\n+    # attribute memory limit on its own but because h5py converts\n+    # the list of layer names into numpy array, which uses the same\n+    # amout of memory for every item, it increases the memory\n+    # requirements substantially.\n+    x = Input(shape=(2,), name='input_' + ('x' * (2**15)))\n+    f = x\n+    for i in range(4):\n+        f = Dense(2, name='dense_%d' % (i,))(f)\n+\n+    model = Model(inputs=[x], outputs=[f])\n+\n+    model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n+\n+    x = np.random.random((1, 2))\n+    y = np.random.random((1, 2))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+\n+    _, fname = tempfile.mkstemp('.h5')\n+    save_model(model, fname)\n+\n+    model = load_model(fname)\n+\n+    # Check that the HDF5 files contains chunked array\n+    # of layer names.\n+    with h5py.File(fname, 'r') as h5file:\n+        n_layer_names_arrays = len([attr for attr in h5file['model_weights'].attrs\n+                                    if attr.startswith('layer_names')])\n+\n+    os.remove(fname)\n+\n+    # The chunking of layer names array should have happend.\n+    assert n_layer_names_arrays > 0\n+\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+\n+@keras_test\n+def test_saving_model_with_long_weights_names():\n+    x = Input(shape=(2,), name='nested_model_input')\n+    f = x\n+    for i in range(4):\n+        f = Dense(2, name='nested_model_dense_%d' % (i,))(f)\n+    # This layer name will make the `weights_name`\n+    # HDF5 attribute blow out of proportion.\n+    f = Dense(2, name='nested_model_output' + ('x' * (2**15)))(f)\n+    nested_model = Model(inputs=[x], outputs=[f], name='nested_model')\n+\n+    x = Input(shape=(2,), name='outer_model_input')\n+    f = nested_model(x)\n+    f = Dense(2, name='outer_model_output')(f)\n+\n+    model = Model(inputs=[x], outputs=[f])\n+\n+    model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n+\n+    x = np.random.random((1, 2))\n+    y = np.random.random((1, 2))\n+    model.train_on_batch(x, y)\n+\n+    out = model.predict(x)\n+\n+    _, fname = tempfile.mkstemp('.h5')\n+    save_model(model, fname)\n+\n+    model = load_model(fname)\n+\n+    # Check that the HDF5 files contains chunked array\n+    # of weight names.\n+    with h5py.File(fname, 'r') as h5file:\n+        n_weight_names_arrays = len([attr for attr in h5file['model_weights']['nested_model'].attrs\n+                                     if attr.startswith('weight_names')])\n+\n+    os.remove(fname)\n+\n+    # The chunking of layer names array should have happend.\n+    assert n_weight_names_arrays > 0\n+\n+    out2 = model.predict(x)\n+    assert_allclose(out, out2, atol=1e-05)\n+\n+\n @keras_test\n def test_saving_recurrent_layer_with_init_state():\n     vector_size = 8\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8193bae22a3aec7b9b2ad2962b62ab71ee5a6f2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 8650 | Contributors (this commit): 58 | Commits (past 90d): 18 | Contributors (cumulative): 58 | DMM Complexity: 0.5\n\nDIFF:\n@@ -2870,6 +2870,7 @@ def _collect_input_shape(input_tensors):\n \n def _save_attributes_to_hdf5_group(group, name, data):\n     \"\"\"Saves attributes (data) of the specified name into the HDF5 group.\n+\n     This method deals with an inherent problem of HDF5 file which is not\n     able to store data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n \n@@ -2885,8 +2886,8 @@ def _save_attributes_to_hdf5_group(group, name, data):\n \n     # Expecting this to never be true.\n     if len(bad_attributes) > 0:\n-        raise RuntimeError('the following attributes cannot be saved to HDF5 file '\n-                           'because they are larger than %d bytes: %s'\n+        raise RuntimeError('The following attributes cannot be saved to HDF5 '\n+                           'file because they are larger than %d bytes: %s'\n                            % (HDF5_OBJECT_HEADER_LIMIT,\n                               ', '.join([x for x in bad_attributes])))\n \n@@ -2908,8 +2909,10 @@ def _save_attributes_to_hdf5_group(group, name, data):\n \n \n def _load_attributes_from_hdf5_group(group, name):\n-    \"\"\"Loads attributes of the specified name from the HDF5 group. This method\n-    deals with an inherent problem of HDF5 file which is not able to store\n+    \"\"\"Loads attributes of the specified name from the HDF5 group.\n+\n+    This method deals with an inherent problem\n+    of HDF5 file which is not able to store\n     data larger than HDF5_OBJECT_HEADER_LIMIT bytes.\n \n     # Arguments\n@@ -2925,7 +2928,8 @@ def _load_attributes_from_hdf5_group(group, name):\n         data = []\n         chunk_id = 0\n         while ('%s%d' % (name, chunk_id)) in group.attrs:\n-            data.extend([n.decode('utf8') for n in group.attrs['%s%d' % (name, chunk_id)]])\n+            data.extend([n.decode('utf8')\n+                        for n in group.attrs['%s%d' % (name, chunk_id)]])\n             chunk_id += 1\n     return data\n \n@@ -2933,7 +2937,8 @@ def _load_attributes_from_hdf5_group(group, name):\n def save_weights_to_hdf5_group(f, layers):\n     from .. import __version__ as keras_version\n \n-    _save_attributes_to_hdf5_group(f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n+    _save_attributes_to_hdf5_group(\n+        f, 'layer_names', [layer.name.encode('utf8') for layer in layers])\n     f.attrs['backend'] = K.backend().encode('utf8')\n     f.attrs['keras_version'] = str(keras_version).encode('utf8')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2802c0820218e29d09382cbaf2235825a3ae703b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 354 | Lines Deleted: 73 | Files Changed: 4 | Hunks: 49 | Methods Changed: 23 | Complexity Δ (Sum/Max): 46/21 | Churn Δ: 427 | Churn Cumulative: 20548 | Contributors (this commit): 96 | Commits (past 90d): 48 | Contributors (cumulative): 126 | DMM Complexity: 0.2074074074074074\n\nDIFF:\n@@ -3134,26 +3134,148 @@ def preprocess_weights_for_loading(layer, weights,\n             if layer.__class__.__name__ == 'ConvLSTM2D':\n                 weights[1] = np.transpose(weights[1], (3, 2, 0, 1))\n \n-    # convert the weights of CuDNNLSTM so that they could be loaded into LSTM\n-    if layer.__class__.__name__ == 'LSTM' and len(weights) == 3:\n+    weights = _convert_rnn_weights(layer, weights)\n+\n+    return weights\n+\n+\n+def _convert_rnn_weights(layer, weights):\n+    \"\"\"Converts weights for RNN layers between native and CuDNN format.\n+\n+    Input kernels for each gate are transposed and converted between Fortran\n+    and C layout, recurrent kernels are transposed. For LSTM biases are summed/\n+    split in half, for GRU biases are reshaped.\n+\n+    Weights can be converted in both directions between `LSTM` and`CuDNNSLTM`\n+    and between `CuDNNGRU` and `GRU(reset_after=True)`. Default `GRU` is not\n+    compatible with `CuDNNGRU`.\n+\n+    For missing biases in `LSTM`/`GRU` (`use_bias=False`) no conversion is made.\n+\n+    # Arguments\n+        layer: Target layer instance.\n+        weights: List of source weights values (input kernels, recurrent\n+            kernels, [biases]) (Numpy arrays).\n+\n+    # Returns\n+        A list of converted weights values (Numpy arrays).\n+\n+    # Raises\n+        ValueError: for incompatible GRU layer/weights or incompatible biases\n+    \"\"\"\n+\n+    def transform_kernels(kernels, func, n_gates):\n+        \"\"\"Transforms kernel for each gate separately using given function.\n+\n+        # Arguments\n+            kernels: Stacked array of kernels for individual gates.\n+            func: Function applied to kernel of each gate.\n+            n_gates: Number of gates (4 for LSTM, 3 for GRU).\n+        # Returns\n+            Stacked array of transformed kernels.\n+        \"\"\"\n+        return np.hstack([func(k) for k in np.hsplit(kernels, n_gates)])\n+\n+    def transpose_input(from_cudnn):\n+        \"\"\"Makes a function that transforms input kernels from/to CuDNN format.\n+\n+        It keeps the shape, but changes between the layout (Fortran/C). Eg.:\n+\n+        ```\n+        Keras                 CuDNN\n+        [[0, 1, 2],  <--->  [[0, 2, 4],\n+         [3, 4, 5]]          [1, 3, 5]]\n+        ```\n+\n+        It can be passed to `transform_kernels()`.\n+\n+        # Arguments\n+            from_cudnn: `True` if source weights are in CuDNN format, `False`\n+                if they're in plain Keras format.\n+        # Returns\n+            Function that converts input kernel to the other format.\n+        \"\"\"\n+        order = 'F' if from_cudnn else 'C'\n+\n+        def transform(kernel):\n+            return kernel.T.reshape(kernel.shape, order=order)\n+\n+        return transform\n+\n+    target_class = layer.__class__.__name__\n+\n+    # convert the weights between CuDNNLSTM and LSTM\n+    if target_class in ['LSTM', 'CuDNNLSTM'] and len(weights) == 3:\n         # determine if we're loading a CuDNNLSTM layer from the number of bias weights:\n         # CuDNNLSTM has (units * 8) weights; while LSTM has (units * 4)\n         # if there's no bias weight in the file, skip this conversion\n         units = weights[1].shape[0]\n-        bias = weights[2]\n-        if len(bias) == units * 8:\n-            # reshape the kernels\n-            kernels = np.split(weights[0], 4, axis=1)\n-            kernels = [kernel.reshape(-1).reshape(kernel.shape, order='F') for kernel in kernels]\n-            weights[0] = np.concatenate(kernels, axis=1)\n+        bias_shape = weights[2].shape\n+        n_gates = 4\n \n-            # transpose the recurrent kernels\n-            recurrent_kernels = np.split(weights[1], 4, axis=1)\n-            recurrent_kernels = [kernel.T for kernel in recurrent_kernels]\n-            weights[1] = np.concatenate(recurrent_kernels, axis=1)\n+        if bias_shape == (2 * units * n_gates,):\n+            source = 'CuDNNLSTM'\n+        elif bias_shape == (units * n_gates,):\n+            source = 'LSTM'\n+        else:\n+            raise ValueError('Invalid bias shape: ' + str(bias_shape))\n \n-            # split the bias into half and merge\n-            weights[2] = bias[:units * 4] + bias[units * 4:]\n+        def convert_weights(weights, from_cudnn=True):\n+            # transpose (and reshape) input and recurrent kernels\n+            kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n+            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n+            if from_cudnn:\n+                # merge input and recurrent biases into a single set\n+                biases = np.sum(np.split(weights[2], 2, axis=0), axis=0)\n+            else:\n+                # Split single set of biases evenly to two sets. The way of\n+                # splitting doesn't matter as long as the two sets sum is kept.\n+                biases = np.tile(0.5 * weights[2], 2)\n+            return [kernels, recurrent_kernels, biases]\n+\n+        if source != target_class:\n+            weights = convert_weights(weights, from_cudnn=source == 'CuDNNLSTM')\n+\n+    # convert the weights between CuDNNGRU and GRU(reset_after=True)\n+    if target_class in ['GRU', 'CuDNNGRU'] and len(weights) == 3:\n+        # We can determine the source of the weights from the shape of the bias.\n+        # If there is no bias we skip the conversion since CuDNNGRU always has biases.\n+\n+        units = weights[1].shape[0]\n+        bias_shape = weights[2].shape\n+        n_gates = 3\n+\n+        def convert_weights(weights, from_cudnn=True):\n+            kernels = transform_kernels(weights[0], transpose_input(from_cudnn), n_gates)\n+            recurrent_kernels = transform_kernels(weights[1], lambda k: k.T, n_gates)\n+            biases = weights[2].reshape((2, -1) if from_cudnn else -1)\n+            return [kernels, recurrent_kernels, biases]\n+\n+        if bias_shape == (2 * units * n_gates,):\n+            source = 'CuDNNGRU'\n+        elif bias_shape == (2, units * n_gates):\n+            source = 'GRU(reset_after=True)'\n+        elif bias_shape == (units * n_gates,):\n+            source = 'GRU(reset_after=False)'\n+        else:\n+            raise ValueError('Invalid bias shape: ' + str(bias_shape))\n+\n+        if target_class == 'CuDNNGRU':\n+            target = 'CuDNNGRU'\n+        elif layer.reset_after:\n+            target = 'GRU(reset_after=True)'\n+        else:\n+            target = 'GRU(reset_after=False)'\n+\n+        # only convert between different types\n+        if source != target:\n+            types = (source, target)\n+            if 'GRU(reset_after=False)' in types:\n+                raise ValueError('%s is not compatible with %s' % types)\n+            if source == 'CuDNNGRU':\n+                weights = convert_weights(weights, from_cudnn=True)\n+            elif source == 'GRU(reset_after=True)':\n+                weights = convert_weights(weights, from_cudnn=False)\n \n     return weights\n \n\n@@ -1204,6 +1204,9 @@ class GRUCell(Layer):\n             batch them into fewer, larger operations. These modes will\n             have different performance profiles on different hardware and\n             for different applications.\n+        reset_after: GRU convention (whether to apply reset gate after or\n+            before matrix multiplication). False = \"before\" (default),\n+            True = \"after\" (CuDNN compatible).\n     \"\"\"\n \n     def __init__(self, units,\n@@ -1222,6 +1225,7 @@ class GRUCell(Layer):\n                  dropout=0.,\n                  recurrent_dropout=0.,\n                  implementation=1,\n+                 reset_after=False,\n                  **kwargs):\n         super(GRUCell, self).__init__(**kwargs)\n         self.units = units\n@@ -1244,6 +1248,7 @@ class GRUCell(Layer):\n         self.dropout = min(1., max(0., dropout))\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n         self.implementation = implementation\n+        self.reset_after = reset_after\n         self.state_size = self.units\n         self._dropout_mask = None\n         self._recurrent_dropout_mask = None\n@@ -1263,31 +1268,58 @@ class GRUCell(Layer):\n             constraint=self.recurrent_constraint)\n \n         if self.use_bias:\n-            self.bias = self.add_weight(shape=(self.units * 3,),\n+            if not self.reset_after:\n+                bias_shape = (3 * self.units,)\n+            else:\n+                # separate biases for input and recurrent kernels\n+                # Note: the shape is intentionally different from CuDNNGRU biases\n+                # `(2 * 3 * self.units,)`, so that we can distinguish the classes\n+                # when loading and converting saved weights.\n+                bias_shape = (2, 3 * self.units)\n+            self.bias = self.add_weight(shape=bias_shape,\n                                         name='bias',\n                                         initializer=self.bias_initializer,\n                                         regularizer=self.bias_regularizer,\n                                         constraint=self.bias_constraint)\n+            if not self.reset_after:\n+                self.input_bias, self.recurrent_bias = self.bias, None\n+            else:\n+                # NOTE: need to flatten, since slicing in CNTK gives 2D array\n+                self.input_bias = K.flatten(self.bias[0])\n+                self.recurrent_bias = K.flatten(self.bias[1])\n         else:\n             self.bias = None\n \n+        # update gate\n         self.kernel_z = self.kernel[:, :self.units]\n         self.recurrent_kernel_z = self.recurrent_kernel[:, :self.units]\n+        # reset gate\n         self.kernel_r = self.kernel[:, self.units: self.units * 2]\n         self.recurrent_kernel_r = self.recurrent_kernel[:,\n                                                         self.units:\n                                                         self.units * 2]\n+        # new gate\n         self.kernel_h = self.kernel[:, self.units * 2:]\n         self.recurrent_kernel_h = self.recurrent_kernel[:, self.units * 2:]\n \n         if self.use_bias:\n-            self.bias_z = self.bias[:self.units]\n-            self.bias_r = self.bias[self.units: self.units * 2]\n-            self.bias_h = self.bias[self.units * 2:]\n+            # bias for inputs\n+            self.input_bias_z = self.input_bias[:self.units]\n+            self.input_bias_r = self.input_bias[self.units: self.units * 2]\n+            self.input_bias_h = self.input_bias[self.units * 2:]\n+            # bias for hidden state - just for compatibility with CuDNN\n+            if self.reset_after:\n+                self.recurrent_bias_z = self.recurrent_bias[:self.units]\n+                self.recurrent_bias_r = self.recurrent_bias[self.units: self.units * 2]\n+                self.recurrent_bias_h = self.recurrent_bias[self.units * 2:]\n         else:\n-            self.bias_z = None\n-            self.bias_r = None\n-            self.bias_h = None\n+            self.input_bias_z = None\n+            self.input_bias_r = None\n+            self.input_bias_h = None\n+            if self.reset_after:\n+                self.recurrent_bias_z = None\n+                self.recurrent_bias_r = None\n+                self.recurrent_bias_h = None\n         self.built = True\n \n     def call(self, inputs, states, training=None):\n@@ -1321,13 +1353,14 @@ class GRUCell(Layer):\n                 inputs_z = inputs\n                 inputs_r = inputs\n                 inputs_h = inputs\n+\n             x_z = K.dot(inputs_z, self.kernel_z)\n             x_r = K.dot(inputs_r, self.kernel_r)\n             x_h = K.dot(inputs_h, self.kernel_h)\n             if self.use_bias:\n-                x_z = K.bias_add(x_z, self.bias_z)\n-                x_r = K.bias_add(x_r, self.bias_r)\n-                x_h = K.bias_add(x_h, self.bias_h)\n+                x_z = K.bias_add(x_z, self.input_bias_z)\n+                x_r = K.bias_add(x_r, self.input_bias_r)\n+                x_h = K.bias_add(x_h, self.input_bias_h)\n \n             if 0. < self.recurrent_dropout < 1.:\n                 h_tm1_z = h_tm1 * rec_dp_mask[0]\n@@ -1337,40 +1370,73 @@ class GRUCell(Layer):\n                 h_tm1_z = h_tm1\n                 h_tm1_r = h_tm1\n                 h_tm1_h = h_tm1\n-            z = self.recurrent_activation(x_z + K.dot(h_tm1_z,\n-                                                      self.recurrent_kernel_z))\n-            r = self.recurrent_activation(x_r + K.dot(h_tm1_r,\n-                                                      self.recurrent_kernel_r))\n \n-            hh = self.activation(x_h + K.dot(r * h_tm1_h,\n-                                             self.recurrent_kernel_h))\n+            recurrent_z = K.dot(h_tm1_z, self.recurrent_kernel_z)\n+            recurrent_r = K.dot(h_tm1_r, self.recurrent_kernel_r)\n+            if self.reset_after and self.use_bias:\n+                recurrent_z = K.bias_add(recurrent_z, self.recurrent_bias_z)\n+                recurrent_r = K.bias_add(recurrent_r, self.recurrent_bias_r)\n+\n+            z = self.recurrent_activation(x_z + recurrent_z)\n+            r = self.recurrent_activation(x_r + recurrent_r)\n+\n+            # reset gate applied after/before matrix multiplication\n+            if self.reset_after:\n+                recurrent_h = K.dot(h_tm1_h, self.recurrent_kernel_h)\n+                if self.use_bias:\n+                    recurrent_h = K.bias_add(recurrent_h, self.recurrent_bias_h)\n+                recurrent_h = r * recurrent_h\n+            else:\n+                recurrent_h = K.dot(r * h_tm1_h, self.recurrent_kernel_h)\n+\n+            hh = self.activation(x_h + recurrent_h)\n         else:\n             if 0. < self.dropout < 1.:\n                 inputs *= dp_mask[0]\n+\n+            # inputs projected by all gate matrices at once\n             matrix_x = K.dot(inputs, self.kernel)\n             if self.use_bias:\n-                matrix_x = K.bias_add(matrix_x, self.bias)\n+                # biases: bias_z_i, bias_r_i, bias_h_i\n+                matrix_x = K.bias_add(matrix_x, self.input_bias)\n+            x_z = matrix_x[:, :self.units]\n+            x_r = matrix_x[:, self.units: 2 * self.units]\n+            x_h = matrix_x[:, 2 * self.units:]\n+\n             if 0. < self.recurrent_dropout < 1.:\n                 h_tm1 *= rec_dp_mask[0]\n+\n+            if self.reset_after:\n+                # hidden state projected by all gate matrices at once\n+                matrix_inner = K.dot(h_tm1, self.recurrent_kernel)\n+                if self.use_bias:\n+                    matrix_inner = K.bias_add(matrix_inner, self.recurrent_bias)\n+            else:\n+                # hidden state projected separately for update/reset and new\n                 matrix_inner = K.dot(h_tm1,\n                                      self.recurrent_kernel[:, :2 * self.units])\n \n-            x_z = matrix_x[:, :self.units]\n-            x_r = matrix_x[:, self.units: 2 * self.units]\n             recurrent_z = matrix_inner[:, :self.units]\n             recurrent_r = matrix_inner[:, self.units: 2 * self.units]\n \n             z = self.recurrent_activation(x_z + recurrent_z)\n             r = self.recurrent_activation(x_r + recurrent_r)\n \n-            x_h = matrix_x[:, 2 * self.units:]\n+            if self.reset_after:\n+                recurrent_h = r * matrix_inner[:, 2 * self.units:]\n+            else:\n                 recurrent_h = K.dot(r * h_tm1,\n                                     self.recurrent_kernel[:, 2 * self.units:])\n+\n             hh = self.activation(x_h + recurrent_h)\n+\n+        # previous and candidate state mixed by update gate\n         h = z * h_tm1 + (1 - z) * hh\n+\n         if 0 < self.dropout + self.recurrent_dropout:\n             if training is None:\n                 h._uses_learning_phase = True\n+\n         return h, [h]\n \n     def get_config(self):\n@@ -1389,7 +1455,8 @@ class GRUCell(Layer):\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout,\n-                  'implementation': self.implementation}\n+                  'implementation': self.implementation,\n+                  'reset_after': self.reset_after}\n         base_config = super(GRUCell, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1397,6 +1464,15 @@ class GRUCell(Layer):\n class GRU(RNN):\n     \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n+    There are two variants. The default one is based on 1406.1078v3 and\n+    has reset gate applied to hidden state before matrix multiplication. The\n+    other one is based on original 1406.1078v1 and has the order reversed.\n+\n+    The second variant is compatible with CuDNNGRU (GPU-only) and allows\n+    inference on CPU. Thus it has separate biases for `kernel` and\n+    `recurrent_kernel`. Use `'reset_after'=True` and\n+    `recurrent_activation='sigmoid'`.\n+\n     # Arguments\n         units: Positive integer, dimensionality of the output space.\n         activation: Activation function to use\n@@ -1467,8 +1543,12 @@ class GRU(RNN):\n             Unrolling can speed-up a RNN,\n             although it tends to be more memory-intensive.\n             Unrolling is only suitable for short sequences.\n+        reset_after: GRU convention (whether to apply reset gate after or\n+            before matrix multiplication). False = \"before\" (default),\n+            True = \"after\" (CuDNN compatible).\n \n     # References\n+        - [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n         - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n         - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n@@ -1497,6 +1577,7 @@ class GRU(RNN):\n                  go_backwards=False,\n                  stateful=False,\n                  unroll=False,\n+                 reset_after=False,\n                  **kwargs):\n         if implementation == 0:\n             warnings.warn('`implementation=0` has been deprecated, '\n@@ -1526,7 +1607,8 @@ class GRU(RNN):\n                        bias_constraint=bias_constraint,\n                        dropout=dropout,\n                        recurrent_dropout=recurrent_dropout,\n-                       implementation=implementation)\n+                       implementation=implementation,\n+                       reset_after=reset_after)\n         super(GRU, self).__init__(cell,\n                                   return_sequences=return_sequences,\n                                   return_state=return_state,\n@@ -1608,6 +1690,10 @@ class GRU(RNN):\n     def implementation(self):\n         return self.cell.implementation\n \n+    @property\n+    def reset_after(self):\n+        return self.cell.reset_after\n+\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n@@ -1625,7 +1711,8 @@ class GRU(RNN):\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout,\n-                  'implementation': self.implementation}\n+                  'implementation': self.implementation,\n+                  'reset_after': self.reset_after}\n         base_config = super(GRU, self).get_config()\n         del base_config['cell']\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -601,9 +601,10 @@ def convert_weights(layer, weights):\n     layers.ConvLSTM2D(5, (3, 3),\n                       input_shape=[6, 6, 6, 6],\n                       data_format='channels_first'),\n-])\n+], ids=['GRU', 'LSTM', 'ConvLSTM2D'])\n def test_preprocess_weights_for_loading(layer):\n-    model = Sequential([layer])\n+    # A model is needed to initialize weights.\n+    _ = Sequential([layer])\n     weights1 = layer.get_weights()\n     weights2 = topology.preprocess_weights_for_loading(\n         layer, convert_weights(layer, weights1),\n@@ -618,7 +619,7 @@ def test_preprocess_weights_for_loading(layer):\n     layers.Conv2DTranspose(2, (5, 5),\n                            input_shape=[7, 7, 3],\n                            data_format='channels_first'),\n-])\n+], ids=['Conv2D', 'Conv2DTranspose'])\n def test_preprocess_weights_for_loading_for_model(layer):\n     model = Sequential([layer])\n     weights1 = model.get_weights()\n@@ -629,6 +630,36 @@ def test_preprocess_weights_for_loading_for_model(layer):\n                 for (x, y) in zip(weights1, weights2)])\n \n \n+@keras_test\n+@pytest.mark.parametrize('layer_class,layer_args', [\n+    (layers.GRU, {'units': 2, 'input_shape': [3, 5]}),\n+    (layers.GRU, {'units': 2, 'input_shape': [3, 5], 'reset_after': True}),\n+    (layers.LSTM, {'units': 2, 'input_shape': [3, 5]}),\n+])\n+def test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, layer_args):\n+    \"\"\"\n+    Loading weights from a RNN class to itself should not convert the weights.\n+    \"\"\"\n+    # layer can be instantiated only for supported backends\n+    layer = layer_class(**layer_args)\n+    # A model is needed to initialize weights.\n+    _ = Sequential([layer])\n+    weights1 = layer.get_weights()\n+    weights2 = topology.preprocess_weights_for_loading(layer, weights1)\n+    assert all([np.allclose(x, y, 1e-5) for (x, y) in zip(weights1, weights2)])\n+\n+\n+@keras_test\n+@pytest.mark.parametrize('layer_class,layer_args', [\n+    (layers.CuDNNGRU, {'units': 2, 'input_shape': [3, 5]}),\n+    (layers.CuDNNLSTM, {'units': 2, 'input_shape': [3, 5]}),\n+])\n+@pytest.mark.skipif((K.backend() != 'tensorflow'), reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not K.tensorflow_backend._get_available_gpus(), reason='Requires GPU')\n+def test_preprocess_weights_for_loading_cudnn_rnn_should_be_idempotent(layer_class, layer_args):\n+    test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, layer_args)\n+\n+\n @keras_test\n def test_recursion_with_bn_and_loss():\n     model1 = Sequential([\n\n@@ -358,52 +358,60 @@ def test_statefulness():\n \n \n @keras_test\n+@pytest.mark.parametrize('implementation', [1, 2], ids=['impl1', 'impl2'])\n+@pytest.mark.parametrize('bidirectional', [False, True], ids=['single', 'bidirectional'])\n+@pytest.mark.parametrize('to_cudnn', [False, True], ids=['from_cudnn', 'to_cudnn'])\n+@pytest.mark.parametrize('rnn_type', ['LSTM', 'GRU'], ids=['LSTM', 'GRU'])\n @pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n                     reason='Requires TensorFlow backend')\n @pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n                     reason='Requires GPU')\n-def test_load_weights_into_noncudnn_lstm():\n+def test_load_weights_between_noncudnn_rnn(rnn_type, to_cudnn, bidirectional, implementation):\n     input_size = 10\n     timesteps = 6\n+    input_shape = (timesteps, input_size)\n     units = 2\n     num_samples = 32\n-\n-    # basic case\n-    input_shape = (timesteps, input_size)\n-    rnn_layer = keras.layers.LSTM(units, input_shape=input_shape,\n-                                  recurrent_activation='sigmoid')\n-    cudnn_rnn_layer = keras.layers.CuDNNLSTM(units, input_shape=input_shape)\n-\n-    model = keras.models.Sequential([rnn_layer])\n-    cudnn_model = keras.models.Sequential([cudnn_rnn_layer])\n-\n-    weights = cudnn_rnn_layer.get_weights()\n-    weights = keras.engine.topology.preprocess_weights_for_loading(rnn_layer, weights)\n-    rnn_layer.set_weights(weights)\n-\n     inputs = np.random.random((num_samples, timesteps, input_size))\n-    out = model.predict(inputs)\n-    cudnn_out = cudnn_model.predict(inputs)\n-    assert_allclose(out, cudnn_out, atol=1e-4)\n \n-    # bidirectional case\n-    input_shape = (timesteps, input_size)\n-    rnn_layer = keras.layers.LSTM(units, recurrent_activation='sigmoid')\n-    rnn_layer = keras.layers.Bidirectional(rnn_layer, input_shape=input_shape)\n-    cudnn_rnn_layer = keras.layers.CuDNNLSTM(units)\n-    cudnn_rnn_layer = keras.layers.Bidirectional(cudnn_rnn_layer, input_shape=input_shape)\n+    rnn_layer_kwargs = {\n+        'recurrent_activation': 'sigmoid',\n+        # ensure biases are non-zero and properly converted\n+        'bias_initializer': 'random_uniform',\n+        'implementation': implementation\n+    }\n+    if rnn_type == 'LSTM':\n+        rnn_layer_class = keras.layers.LSTM\n+        cudnn_rnn_layer_class = keras.layers.CuDNNLSTM\n+    else:\n+        rnn_layer_class = keras.layers.GRU\n+        cudnn_rnn_layer_class = keras.layers.CuDNNGRU\n+        rnn_layer_kwargs['reset_after'] = True\n \n-    model = keras.models.Sequential([rnn_layer])\n-    cudnn_model = keras.models.Sequential([cudnn_rnn_layer])\n+    def convert_weights(source_layer, target_layer):\n+        weights = source_layer.get_weights()\n+        weights = keras.engine.topology.preprocess_weights_for_loading(target_layer, weights)\n+        target_layer.set_weights(weights)\n \n-    weights = cudnn_rnn_layer.get_weights()\n-    weights = keras.engine.topology.preprocess_weights_for_loading(rnn_layer, weights)\n-    rnn_layer.set_weights(weights)\n+    input_layer = keras.layers.InputLayer(input_shape)\n \n-    inputs = np.random.random((num_samples, timesteps, input_size))\n-    out = model.predict(inputs)\n-    cudnn_out = cudnn_model.predict(inputs)\n-    assert_allclose(out, cudnn_out, atol=1e-4)\n+    layer = rnn_layer_class(units, **rnn_layer_kwargs)\n+    if bidirectional:\n+        layer = keras.layers.Bidirectional(layer)\n+\n+    cudnn_layer = cudnn_rnn_layer_class(units)\n+    if bidirectional:\n+        cudnn_layer = keras.layers.Bidirectional(cudnn_layer)\n+\n+    model = keras.models.Sequential([input_layer, layer])\n+    cudnn_model = keras.models.Sequential([input_layer, cudnn_layer])\n+\n+    if to_cudnn:\n+        convert_weights(layer, cudnn_layer)\n+    else:\n+        convert_weights(cudnn_layer, layer)\n+\n+    assert_allclose(model.predict(inputs), cudnn_model.predict(inputs), atol=1e-4)\n \n \n @keras_test\n@@ -463,5 +471,38 @@ def test_cudnnrnn_bidirectional():\n     model.fit(x, y, epochs=1, batch_size=1)\n \n \n+@pytest.mark.skipif((keras.backend.backend() != 'tensorflow'),\n+                    reason='Requires TensorFlow backend')\n+@pytest.mark.skipif(not keras.backend.tensorflow_backend._get_available_gpus(),\n+                    reason='Requires GPU')\n+def test_preprocess_weights_for_loading_gru_incompatible():\n+    \"\"\"\n+    Loading weights between incompatible layers should fail fast with an exception.\n+    \"\"\"\n+    def gru(cudnn=False, **kwargs):\n+        layer_class = keras.layers.CuDNNGRU if cudnn else keras.layers.GRU\n+        return layer_class(2, input_shape=[3, 5], **kwargs)\n+\n+    def initialize_weights(layer):\n+        # A model is needed to initialize weights.\n+        _ = keras.models.Sequential([layer])\n+        return layer\n+\n+    def assert_not_compatible(src, dest, message):\n+        with pytest.raises(ValueError) as ex:\n+            keras.engine.topology.preprocess_weights_for_loading(\n+                dest, initialize_weights(src).get_weights())\n+        assert message in ex.value.message\n+\n+    assert_not_compatible(gru(), gru(cudnn=True),\n+                          'GRU(reset_after=False) is not compatible with CuDNNGRU')\n+    assert_not_compatible(gru(cudnn=True), gru(),\n+                          'CuDNNGRU is not compatible with GRU(reset_after=False)')\n+    assert_not_compatible(gru(), gru(reset_after=True),\n+                          'GRU(reset_after=False) is not compatible with GRU(reset_after=True)')\n+    assert_not_compatible(gru(reset_after=True), gru(),\n+                          'GRU(reset_after=True) is not compatible with GRU(reset_after=False)')\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6596a56992ea7902b255c61f46c1f5fa8ac5e98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9982 | Contributors (this commit): 84 | Commits (past 90d): 14 | Contributors (cumulative): 84 | DMM Complexity: None\n\nDIFF:\n@@ -1181,7 +1181,7 @@ class Sequential(Model):\n                 Note that in conjunction with initial_epoch, the parameter\n                 epochs is to be understood as \"final epoch\". The model is\n                 not trained for n steps given by epochs, but until the\n-                epoch epochs is reached.\n+                epoch of index `epochs` is reached.\n             verbose: Verbosity mode, 0, 1, or 2.\n             callbacks: List of callbacks to be called during training.\n             validation_data: This can be either\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ffb8b813818221a9b43d51e251d489a40b116607", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6413 | Contributors (this commit): 84 | Commits (past 90d): 28 | Contributors (cumulative): 84 | DMM Complexity: None\n\nDIFF:\n@@ -2034,7 +2034,7 @@ class Model(Container):\n                 If unspecified, `workers` will default to 1. If 0, will\n                 execute the generator on the main thread.\n             use_multiprocessing: Boolean. If True, use process based threading.\n-                If unspecified, `workers` will default to False.\n+                If unspecified, `use_multiprocessing` will default to False.\n                 Note that because\n                 this implementation relies on multiprocessing,\n                 you should not pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
