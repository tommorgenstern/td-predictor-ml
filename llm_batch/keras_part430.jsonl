{"custom_id": "keras#fab2a30a34a4c2f5032b7a496bf01d1ce8c9cd2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 187 | Churn Cumulative: 274 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,183 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Nadam optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_experimental import optimizer\n+from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+# pylint: disable=g-classes-have-attributes\n+@generic_utils.register_keras_serializable()\n+class Nadam(optimizer.Optimizer):\n+  r\"\"\"Optimizer that implements the Nadam algorithm.\n+\n+  Much like Adam is essentially RMSprop with momentum, Nadam is Adam with\n+  Nesterov momentum.\n+\n+  Args:\n+    learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n+      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+      that takes no arguments and returns the actual value to use. The\n+      learning rate. Defaults to 0.001.\n+    beta_1: A float value or a constant float tensor, or a callable\n+      that takes no arguments and returns the actual value to use. The\n+      exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n+    beta_2: A float value or a constant float tensor, or a callable\n+      that takes no arguments and returns the actual value to use. The\n+      exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n+    epsilon: A small constant for numerical stability. This epsilon is\n+      \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+      Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n+      1e-7.\n+    {{base_optimizer_keyword_args}}\n+\n+  Reference:\n+    - [Dozat, 2015](http://cs229.stanford.edu/proj2015/054_report.pdf).\n+\n+  \"\"\"\n+\n+  def __init__(self,\n+               learning_rate=0.001,\n+               beta_1=0.9,\n+               beta_2=0.999,\n+               epsilon=1e-7,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=None,\n+               jit_compile=False,\n+               name='Nadam',\n+               **kwargs):\n+    super().__init__(\n+        name=name,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n+        jit_compile=jit_compile,\n+        **kwargs)\n+    self._learning_rate = self._build_learning_rate(learning_rate)\n+    self.beta_1 = beta_1\n+    self.beta_2 = beta_2\n+    self.epsilon = epsilon\n+\n+  def build(self, var_list):\n+    \"\"\"Initialize optimizer variables.\n+\n+    Nadam optimizer has 2 types of variables: momentums and velocities.\n+\n+    Args:\n+      var_list: list of model variables to build Nadam variables on.\n+    \"\"\"\n+    super().build(var_list)\n+    if getattr(self, '_built', False):\n+      return\n+    self._built = True\n+    self._momentums = []\n+    self._velocities = []\n+    self._u_product = tf.Variable(1.0, dtype=var_list[0].dtype)\n+    # Keep a counter on how many times of _u_product has been computed to\n+    # avoid duplicated computations.\n+    self._u_product_counter = 1\n+\n+    for var in var_list:\n+      self._momentums.append(\n+          self.add_variable_from_reference(\n+              model_variable=var, variable_name='m'))\n+      self._velocities.append(\n+          self.add_variable_from_reference(\n+              model_variable=var, variable_name='v'))\n+\n+  def update_step(self, gradient, variable):\n+    \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n+    var_dtype = variable.dtype\n+    lr = tf.cast(self.learning_rate, var_dtype)\n+    local_step = tf.cast(self.iterations + 1, var_dtype)\n+    next_step = tf.cast(self.iterations + 2, var_dtype)\n+    decay = tf.cast(0.96, var_dtype)\n+    beta_1 = tf.cast(self.beta_1, var_dtype)\n+    beta_2 = tf.cast(self.beta_2, var_dtype)\n+    u_t = beta_1 * (1. - 0.5 * (tf.pow(decay, local_step)))\n+    u_t_1 = beta_1 * (1. - 0.5 * (tf.pow(decay, next_step)))\n+    def get_cached_u_product():\n+      return self._u_product\n+\n+    def compute_new_u_product():\n+      u_product_t = self._u_product * u_t\n+      self._u_product.assign(u_product_t)\n+      self._u_product_counter += 1\n+      return u_product_t\n+\n+    u_product_t = tf.cond(\n+        self._u_product_counter == (self.iterations + 2),\n+        true_fn=get_cached_u_product,\n+        false_fn=compute_new_u_product)\n+    u_product_t_1 = u_product_t * u_t_1\n+    beta_2_power = tf.pow(beta_2, local_step)\n+\n+    var_key = self._var_key(variable)\n+    m = self._momentums[self._index_dict[var_key]]\n+    v = self._velocities[self._index_dict[var_key]]\n+\n+    if isinstance(gradient, tf.IndexedSlices):\n+      # Sparse gradients.\n+      m.assign_add(-m * (1 - beta_1))\n+      m.scatter_add(\n+          tf.IndexedSlices(gradient.values * (1 - beta_1),\n+                           gradient.indices))\n+      v.assign_add(-v * (1 - beta_2))\n+      v.scatter_add(\n+          tf.IndexedSlices(\n+              tf.square(gradient.values) * (1 - beta_2), gradient.indices))\n+      m_hat = (\n+          u_t_1 * m / (1 - u_product_t_1) + (1 - u_t) * gradient /\n+          (1 - u_product_t))\n+      v_hat = v / (1 - beta_2_power)\n+\n+      variable.assign_sub((m_hat * lr) / (tf.sqrt(v_hat) + self.epsilon))\n+    else:\n+      # Dense gradients.\n+      m.assign_add((gradient - m) * (1 - beta_1))\n+      v.assign_add((tf.square(gradient) - v) * (1 - beta_2))\n+      m_hat = (\n+          u_t_1 * m / (1 - u_product_t_1) + (1 - u_t) * gradient /\n+          (1 - u_product_t))\n+      v_hat = v / (1 - beta_2_power)\n+\n+      variable.assign_sub((m_hat * lr) / (tf.sqrt(v_hat) + self.epsilon))\n+\n+  def get_config(self):\n+    config = super().get_config()\n+\n+    config.update({\n+        'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n+        'beta_1': self.beta_1,\n+        'beta_2': self.beta_2,\n+        'epsilon': self.epsilon,\n+    })\n+    return config\n+\n+Nadam.__doc__ = Nadam.__doc__.replace(\n+    '{{base_optimizer_keyword_args}}', optimizer.base_optimizer_keyword_args)\n\n@@ -16,6 +16,7 @@ from keras.optimizers.optimizer_experimental import adam as adam_new\n from keras.optimizers.optimizer_experimental import adamax as adamax_new\n from keras.optimizers.optimizer_experimental import adamw as adamw_new\n from keras.optimizers.optimizer_experimental import ftrl as ftrl_new\n+from keras.optimizers.optimizer_experimental import nadam as nadam_new\n from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_new\n from keras.optimizers.optimizer_experimental import sgd as sgd_new\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_old\n@@ -58,6 +59,8 @@ adamw_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladamw\", lambda: adamw_new.AdamW(0.002, weight_decay=0.004))\n ftrl_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalftrl\", lambda: ftrl_new.Ftrl(0.002))\n+nadam_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"experimentnadam\", lambda: nadam_new.Nadam(0.002))\n rmsprop_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalrmsprop\", lambda: rmsprop_new.RMSprop(0.002))\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n@@ -74,6 +77,7 @@ OPTIMIZER_FN = [\n     adamax_new_fn,\n     adamw_new_fn,\n     ftrl_new_fn,\n+    nadam_new_fn,\n     rmsprop_new_fn,\n     sgd_new_fn,\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d56b634f711802ae88c277926b6634465f346275", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 22 | Churn Cumulative: 330 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,7 +42,9 @@ class InitializersTest(test_util.DTensorBaseTest):\n       ('Zeros', initializers.Zeros, {}),\n       ('Ones', initializers.Ones, {}),\n       ('Constant', initializers.Constant, {'value': 3.}),\n-      ('Identity', initializers.Identity, {}),\n+      # TODO(b/222160686): Add Identity after after we have SPMD support for\n+      # tf.MatrixDiagV3\n+      # ('Identity', initializers.Identity, {}),\n   )\n   def test_static_value_initializer(self, initializer_cls, init_args):\n     layout = dtensor.Layout([dtensor.UNSHARDED, dtensor.UNSHARDED], self.mesh)\n\n@@ -106,14 +106,6 @@ def allow_initializer_layout(init_method):\n def call_with_layout(fn, layout, *args, **kwargs):\n   \"\"\"Invoke the function with inputs and relayout the result.\n \n-  When the layout is not None, a temporary tf.function will be created under the\n-  hood, so that the layout can be properly set for the result. The tf.function\n-  need to be called under the dtensor device scope, since the dtensor related\n-  kernel need to that as the indicator to work with tf.function. The tf.function\n-  will ensure the it only create the needed tensor (sharded for local device)\n-  by the layout propagation, and not fully materialize the output of fn, which\n-  could potentially be a large tensor.\n-\n   Args:\n     fn: the function to invoke.\n     layout: if not None, the output of the fn will be relayout with this.\n@@ -124,15 +116,7 @@ def call_with_layout(fn, layout, *args, **kwargs):\n     The output of fn, with potential relayout with the layout specified.\n   \"\"\"\n   if layout:\n-\n-    # TODO(b/222160686): Remove this tf.function after after we have SPMD\n-    # support for tf.MatrixDiagV3\n-    @tf.function\n-    def wrapper_func():\n+    with dtensor.run_on(layout):\n       result = fn(*args, **kwargs)\n       return dtensor.relayout(result, layout)\n-\n-    with dtensor.run_on(layout):\n-      return wrapper_func()\n-\n   return fn(*args, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#61edccb559c0daf243149c62a03f58c019db387d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 8407 | Contributors (this commit): 26 | Commits (past 90d): 14 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -3420,6 +3420,7 @@ def _apply_name_scope_on_model_declaration(enable):\n   _is_name_scope_on_model_declaration_enabled = enable\n \n \n+@keras_export('keras.__internal__.layers.BaseRandomLayer')\n class BaseRandomLayer(Layer):\n   \"\"\"A layer handle the random number creation and savemodel behavior.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d0379e270c90ce4556a2a98a39a10ca3ff8991b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 21 | Files Changed: 9 | Hunks: 18 | Methods Changed: 5 | Complexity Δ (Sum/Max): 32/24 | Churn Δ: 35 | Churn Cumulative: 12465 | Contributors (this commit): 118 | Commits (past 90d): 54 | Contributors (cumulative): 151 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2329,12 +2329,12 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     \"\"\"Configure the Projector for embeddings.\"\"\"\n     # TODO(omalleyt): Add integration tests.\n     from google.protobuf import text_format\n-    from keras.layers import embeddings\n+    from keras.layers import core\n     from keras.protobuf import projector_config_pb2\n \n     config = projector_config_pb2.ProjectorConfig()\n     for layer in self.model.layers:\n-      if isinstance(layer, embeddings.Embedding):\n+      if isinstance(layer, core.Embedding):\n         embedding = config.embeddings.add()\n         # Embeddings are always the first layer, so this naming should be\n         # consistent in any keras models checkpoints.\n\n@@ -112,6 +112,7 @@ from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n # Core layers.\n from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n+from keras.layers.core.embedding import Embedding\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n from keras.layers.core.tf_op_layer import ClassMethod\n@@ -120,9 +121,6 @@ from keras.layers.core.tf_op_layer import InstanceProperty\n from keras.layers.core.tf_op_layer import SlicingOpLambda\n from keras.layers.core.tf_op_layer import TFOpLambda\n \n-# Embedding layers.\n-from keras.layers.embeddings import Embedding\n-\n # Einsum-based dense layer/\n from keras.layers.einsum_dense import EinsumDense\n \n\n@@ -16,6 +16,7 @@\n \n from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n+from keras.layers.core.embedding import Embedding\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n # Required by third_party/py/tensorflow_gnn/graph/keras/keras_tensors.py\n\n@@ -13,9 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Embedding layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import backend\n from keras import constraints\n@@ -24,6 +22,8 @@ from keras import regularizers\n from keras.engine import base_layer_utils\n from keras.engine.base_layer import Layer\n from keras.utils import tf_utils\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n\n@@ -12,8 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for embedding layers.\"\"\"\n-# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n+\"\"\"Tests for embedding layer.\"\"\"\n \n import keras\n from keras.mixed_precision import policy\n@@ -114,8 +113,7 @@ class EmbeddingTest(test_combinations.TestCase):\n     model = keras.Model(inputs, outputs)\n     model.run_eagerly = test_utils.should_run_eagerly()\n     outputs = model.predict(\n-        tf.ragged.constant([[1., 2., 2.], [0.], [1., 2.]],\n-                                    ragged_rank=1))\n+        tf.ragged.constant([[1., 2., 2.], [0.], [1., 2.]], ragged_rank=1))\n     self.assertAllClose(\n         outputs,\n         tf.ragged.constant(\n\n@@ -28,7 +28,6 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.layers import convolutional\n from keras.layers import core\n-from keras.layers import embeddings\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import text_vectorization\n from keras.utils import generic_utils\n@@ -1523,7 +1522,7 @@ class TextVectorizationModelBuildingTest(\n         output_sequence_length=output_sequence_length)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n-    embedded_data = embeddings.Embedding(\n+    embedded_data = core.Embedding(\n         input_dim=max_tokens + 1, output_dim=32)(\n             int_data)\n     output_data = convolutional.Conv1D(\n\n@@ -21,7 +21,6 @@ import os\n \n from absl.testing import parameterized\n import keras\n-from keras.layers import embeddings\n from keras.layers.rnn import gru\n from keras.layers.rnn import lstm\n from keras.testing_infra import test_combinations\n@@ -115,7 +114,7 @@ class RNNV2Test(test_combinations.TestCase):\n     vocab_size = 100\n     inputs = tf.ragged.constant(\n         np.random.RandomState(0).randint(0, vocab_size, [128, 25]))\n-    embedder = embeddings.Embedding(input_dim=vocab_size, output_dim=16)\n+    embedder = keras.layers.Embedding(input_dim=vocab_size, output_dim=16)\n     embedded_inputs = embedder(inputs)\n     layer = layer(32)\n     layer(embedded_inputs)\n@@ -126,7 +125,7 @@ class RNNV2Test(test_combinations.TestCase):\n     vocab_size = 100\n     timestep = 20\n     units = 32\n-    embedder = embeddings.Embedding(input_dim=vocab_size, output_dim=units)\n+    embedder = keras.layers.Embedding(input_dim=vocab_size, output_dim=units)\n     layer = layer(units, return_sequences=True)\n     data = tf.constant(\n         np.random.RandomState(0).randint(0, vocab_size, [timestep, timestep]))\n\n@@ -26,7 +26,6 @@ from keras.layers import attention\n from keras.layers import convolutional\n from keras.layers import core\n from keras.layers import einsum_dense\n-from keras.layers import embeddings\n from keras.layers import locally_connected\n from keras.layers import merging\n from keras.layers import noise\n@@ -56,7 +55,7 @@ from keras.utils import tf_inspect as inspect\n from tensorflow.python.util.tf_export import keras_export\n \n ALL_MODULES = (base_layer, input_layer, activation, attention, convolutional,\n-               core, embeddings, einsum_dense, locally_connected, merging,\n+               core, einsum_dense, locally_connected, merging,\n                batch_normalization_v1, layer_normalization, unit_normalization,\n                pooling, image_preprocessing, regularization, reshaping, rnn,\n                hashing, hashed_crossing, category_encoding, discretization,\n\n@@ -26,7 +26,6 @@ from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n from keras.layers import core\n-from keras.layers import embeddings\n from keras.layers import locally_connected\n from keras.layers import merging\n from keras.layers import pooling\n@@ -119,7 +118,7 @@ class LayerCorrectnessTest(test_combinations.TestCase):\n       ('AdditiveAttention', attention.AdditiveAttention, [(2, 2, 3),\n                                                           (2, 3, 3),\n                                                           (2, 3, 3)]),\n-      ('Embedding', lambda: embeddings.Embedding(4, 4),\n+      ('Embedding', lambda: core.Embedding(4, 4),\n        (2, 4), 2e-3, 2e-3, np.random.randint(4, size=(2, 4))),\n       ('LocallyConnected1D', lambda: locally_connected.LocallyConnected1D(2, 2),\n        (2, 2, 1)),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a253a9d67f95db27f1dbf74284fb6fdad2353b43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 4676 | Contributors (this commit): 22 | Commits (past 90d): 3 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -29,6 +29,7 @@ from tensorflow.python.ops.ragged import ragged_util\n from tensorflow.python.util import dispatch\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n+from tensorflow.python.ops import check_ops\n \n \n @keras_export('keras.losses.Loss')\n@@ -1773,6 +1774,8 @@ def categorical_crossentropy(y_true,\n   Returns:\n     Categorical crossentropy loss value.\n   \"\"\"\n+  #axis assert\n+  check_ops.assert_integer_v2(axis, message=('`axis` must be of type `int`.'))\n   y_pred = tf.convert_to_tensor(y_pred)\n   y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b14e16b8cc93abcc21355115a7a18c34d385281", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 1636 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,7 +12,17 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Utilities for text input preprocessing.\"\"\"\n+\"\"\"Utilities for text input preprocessing.\n+\n+Warning: `tf.keras.preprocessing.text` APIs are not recommended for new code.\n+Prefer `tf.keras.utils.text_dataset_from_directory` and\n+`tf.keras.layers.TextVectorization` which provide a more efficient approach\n+for preprocessing text input. For an introduction to these APIs, see\n+the [text loading tutorial]\n+(https://www.tensorflow.org/tutorials/load_data/text)\n+and [preprocessing layer guide]\n+(https://www.tensorflow.org/guide/keras/preprocessing_layers).\n+\"\"\"\n # pylint: disable=invalid-name\n # pylint: disable=g-classes-have-attributes\n # pylint: disable=g-direct-tensorflow-import\n@@ -34,6 +44,13 @@ def text_to_word_sequence(input_text,\n                           split=' '):\n   r\"\"\"Converts a text to a sequence of words (or tokens).\n \n+  Warning: `tf.keras.preprocessing.text.text_to_word_sequence` does not operate\n+  on tensors and is not recommended for new code. Prefer\n+  `tf.strings.regex_replace` and `tf.strings.split` which provide equivalent\n+  functionality and accept `tf.Tensor` input. For an overview of text handling\n+  in Tensorflow, see the [text loading tutorial]\n+  (https://www.tensorflow.org/tutorials/load_data/text).\n+\n   This function transforms a string of text into a list of words\n   while ignoring `filters` which include punctuations by default.\n \n@@ -72,6 +89,13 @@ def one_hot(input_text,\n             analyzer=None):\n   r\"\"\"One-hot encodes a text into a list of word indexes of size `n`.\n \n+  Warning: `tf.keras.text.preprocessing.one_hot` does not operate on tensors\n+  and is not recommended for new code. Prefer `tf.keras.layers.Hashing` with\n+  `output_mode='one_hot'` which provides equivalent functionality through a\n+  layer which accepts `tf.Tensor` input. See the [preprocessing layer guide]\n+  (https://www.tensorflow.org/guide/keras/preprocessing_layers)\n+  for an overview of preprocessing layers.\n+\n   This function receives as input a string of text and returns a\n   list of encoded integers each corresponding to a word (or token)\n   in the given input string.\n@@ -113,6 +137,13 @@ def hashing_trick(text,\n                   analyzer=None):\n   r\"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n \n+  Warning: `tf.keras.text.preprocessing.hashing_trick` does not operate on\n+  tensors and is not recommended for new code. Prefer `tf.keras.layers.Hashing`\n+  which provides equivalent functionality through a layer which accepts\n+  `tf.Tensor` input. See the [preprocessing layer guide]\n+  (https://www.tensorflow.org/guide/keras/preprocessing_layers)\n+  for an overview of preprocessing layers.\n+\n   Args:\n       text: Input text (string).\n       n: Dimension of the hashing space.\n@@ -155,6 +186,13 @@ def hashing_trick(text,\n class Tokenizer(object):\n   \"\"\"Text tokenization utility class.\n \n+  Warning: `tf.keras.preprocessing.text.Tokenizer` does not operate on tensors\n+  and is not recommended for new code. Prefer\n+  `tf.keras.layers.TextVectorization` which provides equivalent functionality\n+  through a layer which accepts `tf.Tensor` input. See the\n+  [text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)\n+  for an overview of the layer and text handling in tensorflow.\n+\n   This class allows to vectorize a text corpus, by turning each\n   text into either a sequence of integers (each integer being the index\n   of a token in a dictionary) or into a vector where the coefficient\n@@ -510,6 +548,13 @@ class Tokenizer(object):\n def tokenizer_from_json(json_string):\n   \"\"\"Parses a JSON tokenizer configuration and returns a tokenizer instance.\n \n+  Warning: `tf.keras.preprocessing.text.Tokenizer` does not operate on tensors\n+  and is not recommended for new code. Prefer\n+  `tf.keras.layers.TextVectorization` which provides equivalent functionality\n+  through a layer which accepts `tf.Tensor` input. See the\n+  [text loading tutorial](https://www.tensorflow.org/tutorials/load_data/text)\n+  for an overview of the layer and text handling in tensorflow.\n+\n   Args:\n       json_string: JSON string encoding a tokenizer configuration.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#96148d86f3b2fe52ac99c898d2adcb5cca7ddde5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4678 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -1775,7 +1775,7 @@ def categorical_crossentropy(y_true,\n     Categorical crossentropy loss value.\n   \"\"\"\n   #axis assert\n-  check_ops.assert_integer_v2(axis, message=('`axis` must be of type `int`.'))\n+  check_ops.assert_integer_v2(int(axis), message=('`axis` must be of type `int`.'))\n   y_pred = tf.convert_to_tensor(y_pred)\n   y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f7f9b3f60d997f681a3e407b183dae56c11c6f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 187 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 48 | Methods Changed: 4 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 188 | Churn Cumulative: 2533 | Contributors (this commit): 4 | Commits (past 90d): 26 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,92 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for metrics.\"\"\"\n+\n+from absl.testing import parameterized\n+from keras import metrics\n+from keras.dtensor import dtensor_api as dtensor\n+from keras.utils import tf_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from keras.dtensor.tests import test_util\n+\n+\n+class MetricsTest(test_util.DTensorBaseTest):\n+\n+  def setUp(self):\n+    super(MetricsTest, self).setUp()\n+    global_ids = test_util.create_device_ids_array((2, 2))\n+    local_device_ids = np.ravel(global_ids).tolist()\n+    mesh_dict = {\n+        'CPU':\n+            dtensor.Mesh(['X', 'Y'], global_ids, local_device_ids,\n+                         test_util.create_device_list((2, 2), 'CPU'))\n+    }\n+    self.mesh = self.configTestMesh(mesh_dict)\n+    tf_utils.set_random_seed(1337)\n+\n+  @parameterized.parameters(\n+      (metrics.Accuracy, {}),\n+      (metrics.AUC, {}),\n+      (metrics.BinaryAccuracy, {}),\n+      (metrics.BinaryCrossentropy, {}),\n+      (metrics.BinaryIoU, {}),\n+      (metrics.CategoricalAccuracy, {}),\n+      (metrics.CategoricalCrossentropy, {}),\n+      (metrics.CategoricalHinge, {}),\n+      (metrics.CosineSimilarity, {}),\n+      (metrics.FalseNegatives, {}),\n+      (metrics.FalsePositives, {}),\n+      (metrics.Hinge, {}),\n+      (metrics.IoU, {'num_classes': 3, 'target_class_ids': [1]}),\n+      (metrics.KLDivergence, {}),\n+      (metrics.LogCoshError, {}),\n+      (metrics.Mean, {}),\n+      (metrics.MeanAbsoluteError, {}),\n+      (metrics.MeanAbsolutePercentageError, {}),\n+      (metrics.MeanIoU, {'num_classes': 3}),\n+      (metrics.MeanRelativeError, {'normalizer': [1, 3, 2, 3]}),\n+      (metrics.MeanSquaredError, {}),\n+      (metrics.MeanSquaredLogarithmicError, {}),\n+      (metrics.OneHotIoU, {'num_classes': 3, 'target_class_ids': [1]}),\n+      (metrics.OneHotMeanIoU, {'num_classes': 3}),\n+      (metrics.Poisson, {}),\n+      (metrics.Precision, {}),\n+      (metrics.PrecisionAtRecall, {'recall': 0.5}),\n+      (metrics.Recall, {}),\n+      (metrics.RecallAtPrecision, {'precision': 0.5}),\n+      (metrics.RootMeanSquaredError, {}),\n+      (metrics.SensitivityAtSpecificity, {'specificity': 0.5}),\n+      (metrics.SparseCategoricalAccuracy, {}),\n+      (metrics.SparseCategoricalCrossentropy, {}),\n+      (metrics.SparseTopKCategoricalAccuracy, {}),\n+      (metrics.SpecificityAtSensitivity, {'sensitivity': 0.5}),\n+      (metrics.SquaredHinge, {}),\n+      (metrics.Sum, {}),\n+      (metrics.TopKCategoricalAccuracy, {}),\n+      (metrics.TrueNegatives, {}),\n+      (metrics.TruePositives, {}),\n+  )\n+  def test_metric_layout(self, metric_cls, init_args):\n+    metric = metric_cls(**init_args, mesh=self.mesh)\n+\n+    for weight in metric.non_trainable_weights:\n+      self.assertIsInstance(weight, dtensor.DVariable)\n+      self.assertTrue(weight.layout.is_fully_replicated())\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -103,6 +103,47 @@ def allow_initializer_layout(init_method):\n       target=init_method, decorator_func=_wrap_function)\n \n \n+def inject_mesh(init_method):\n+  \"\"\"Inject DTensor mesh information to an object.\n+\n+  This is useful for keras object like `Metric` and `Optimizer` which need\n+  DTensor mesh to create the weights, but doesn't want to change the current\n+  public API interface.\n+\n+  This is for temporary usage and eventually the mesh/layout information will be\n+  public arguments in the `__init__` method\n+\n+  Sample usage:\n+  ```python\n+  class Accuracy(tf.keras.metrics.Metric):\n+\n+    @inject_mesh\n+    def __init__(self, name='accuracy', dtype=None):\n+       super().__init__(**kwargs)\n+\n+    acc = Accuracy(mesh=mesh)\n+    assert acc._mesh == mesh\n+  ```\n+\n+  Args:\n+    init_method: the `__init__` method of the Keras class to annotate.\n+\n+  Returns:\n+    the annotated __init__ method.\n+  \"\"\"\n+  def _wrap_function(instance, *args, **kwargs):\n+    mesh = kwargs.pop(\"mesh\", None)\n+    # Note that the injection of _mesh need to happen before the invocation of\n+    # __init__, since the class might need the mesh to create weights in the\n+    # __init__.\n+    if mesh is not None:\n+      instance._mesh = mesh  # pylint: disable=protected-access\n+    init_method(instance, *args, **kwargs)\n+\n+  return tf.__internal__.decorator.make_decorator(\n+      target=init_method, decorator_func=_wrap_function)\n+\n+\n def call_with_layout(fn, layout, *args, **kwargs):\n   \"\"\"Invoke the function with inputs and relayout the result.\n \n\n@@ -22,6 +22,8 @@ import types\n import warnings\n \n from keras import backend\n+from keras.dtensor import dtensor_api as dtensor\n+from keras.dtensor import utils as dtensor_utils\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import keras_tensor\n@@ -337,6 +339,13 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     # TODO(b/120571621): Make `ON_READ` work with Keras metrics on TPU.\n     if backend.is_tpu_strategy(strategy):\n       synchronization = tf.VariableSynchronization.ON_WRITE\n+    if getattr(self, '_mesh', None) is not None:\n+      # When self._mesh is set, it means this metric is used for DTensor.\n+      additional_kwargs = {\n+          'layout': dtensor.Layout.replicated(self._mesh,\n+                                              tf.TensorShape(shape).rank)}\n+    else:\n+      additional_kwargs = {}\n \n     with tf.init_scope():\n       return super(Metric, self).add_weight(\n@@ -347,7 +356,8 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n           initializer=initializer,\n           collections=[],\n           synchronization=synchronization,\n-          aggregation=aggregation)\n+          aggregation=aggregation,\n+          **additional_kwargs)\n \n   ### End: For use by subclasses ###\n \n@@ -520,6 +530,7 @@ class Sum(Reduce):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='sum', dtype=None):\n     super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n                               name=name, dtype=dtype)\n@@ -562,6 +573,7 @@ class Mean(Reduce):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean', dtype=None):\n     super(Mean, self).__init__(\n         reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n@@ -595,6 +607,7 @@ class MeanMetricWrapper(Mean):\n     **kwargs: Keyword arguments to pass on to `fn`.\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, fn, name=None, dtype=None, **kwargs):\n     super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n     self._fn = fn\n@@ -695,6 +708,7 @@ class MeanTensor(Metric):\n   array([[2., 3., 4., 5.]])\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_tensor', dtype=None, shape=None):\n     super(MeanTensor, self).__init__(name=name, dtype=dtype)\n     self._shape = None\n\n@@ -21,6 +21,7 @@ from typing import List, Tuple, Union\n \n from keras import activations\n from keras import backend\n+from keras.dtensor import utils as dtensor_utils\n from keras.losses import binary_crossentropy\n from keras.losses import categorical_crossentropy\n from keras.losses import categorical_hinge\n@@ -83,6 +84,7 @@ class MeanRelativeError(base_metric.Mean):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, normalizer, name=None, dtype=None):\n     super(MeanRelativeError, self).__init__(name=name, dtype=dtype)\n     normalizer = tf.cast(normalizer, self._dtype)\n@@ -163,6 +165,7 @@ class Accuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='accuracy', dtype=None):\n     super(Accuracy, self).__init__(accuracy, name, dtype=dtype)\n \n@@ -207,6 +210,7 @@ class BinaryAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n     super(BinaryAccuracy, self).__init__(\n         binary_accuracy, name, dtype=dtype, threshold=threshold)\n@@ -259,6 +263,7 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_accuracy', dtype=None):\n     super(CategoricalAccuracy, self).__init__(\n         categorical_accuracy, name, dtype=dtype)\n@@ -310,6 +315,7 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n     super(SparseCategoricalAccuracy, self).__init__(\n         sparse_categorical_accuracy, name, dtype=dtype)\n@@ -375,6 +381,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n     super(TopKCategoricalAccuracy, self).__init__(\n         top_k_categorical_accuracy, name, dtype=dtype, k=k)\n@@ -413,6 +420,7 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n     super(SparseTopKCategoricalAccuracy, self).__init__(\n         sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n@@ -532,6 +540,7 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(FalsePositives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n@@ -581,6 +590,7 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(FalseNegatives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n@@ -630,6 +640,7 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(TrueNegatives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n@@ -679,6 +690,7 @@ class TruePositives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(TruePositives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n@@ -756,6 +768,7 @@ class Precision(base_metric.Metric):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                thresholds=None,\n                top_k=None,\n@@ -886,6 +899,7 @@ class Recall(base_metric.Metric):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                thresholds=None,\n                top_k=None,\n@@ -1129,6 +1143,7 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                specificity,\n                num_thresholds=200,\n@@ -1225,6 +1240,7 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                sensitivity,\n                num_thresholds=200,\n@@ -1313,6 +1329,7 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                recall,\n                num_thresholds=200,\n@@ -1401,6 +1418,7 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                precision,\n                num_thresholds=200,\n@@ -1554,6 +1572,7 @@ class AUC(base_metric.Metric):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                num_thresholds=200,\n                curve='ROC',\n@@ -1963,6 +1982,7 @@ class CosineSimilarity(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='cosine_similarity', dtype=None, axis=-1):\n     super(CosineSimilarity, self).__init__(\n         cosine_similarity, name, dtype=dtype, axis=axis)\n@@ -1999,6 +2019,7 @@ class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_error', dtype=None):\n     super(MeanAbsoluteError, self).__init__(\n         mean_absolute_error, name, dtype=dtype)\n@@ -2035,6 +2056,7 @@ class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_percentage_error', dtype=None):\n     super(MeanAbsolutePercentageError, self).__init__(\n         mean_absolute_percentage_error, name, dtype=dtype)\n@@ -2071,6 +2093,7 @@ class MeanSquaredError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_error', dtype=None):\n     super(MeanSquaredError, self).__init__(\n         mean_squared_error, name, dtype=dtype)\n@@ -2107,6 +2130,7 @@ class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_logarithmic_error', dtype=None):\n     super(MeanSquaredLogarithmicError, self).__init__(\n         mean_squared_logarithmic_error, name, dtype=dtype)\n@@ -2143,6 +2167,7 @@ class Hinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='hinge', dtype=None):\n     super(Hinge, self).__init__(hinge, name, dtype=dtype)\n \n@@ -2181,6 +2206,7 @@ class SquaredHinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='squared_hinge', dtype=None):\n     super(SquaredHinge, self).__init__(squared_hinge, name, dtype=dtype)\n \n@@ -2216,6 +2242,7 @@ class CategoricalHinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_hinge', dtype=None):\n     super(CategoricalHinge, self).__init__(categorical_hinge, name, dtype=dtype)\n \n@@ -2247,6 +2274,7 @@ class RootMeanSquaredError(base_metric.Mean):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='root_mean_squared_error', dtype=None):\n     super(RootMeanSquaredError, self).__init__(name, dtype=dtype)\n \n@@ -2307,6 +2335,7 @@ class LogCoshError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='logcosh', dtype=None):\n     super(LogCoshError, self).__init__(logcosh, name, dtype=dtype)\n \n@@ -2343,6 +2372,7 @@ class Poisson(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='poisson', dtype=None):\n     super(Poisson, self).__init__(poisson, name, dtype=dtype)\n \n@@ -2379,6 +2409,7 @@ class KLDivergence(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, name='kullback_leibler_divergence', dtype=None):\n     super(KLDivergence, self).__init__(\n         kullback_leibler_divergence, name, dtype=dtype)\n@@ -2534,6 +2565,7 @@ class IoU(_IoUBase):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -2656,6 +2688,7 @@ class BinaryIoU(IoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n@@ -2762,6 +2795,7 @@ class MeanIoU(IoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self, num_classes, name=None, dtype=None):\n     target_class_ids = list(range(num_classes))\n     super(MeanIoU, self).__init__(\n@@ -2852,6 +2886,7 @@ class OneHotIoU(IoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -2956,6 +2991,7 @@ class OneHotMeanIoU(MeanIoU):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -3028,6 +3064,7 @@ class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='binary_crossentropy',\n                dtype=None,\n@@ -3092,6 +3129,7 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='categorical_crossentropy',\n                dtype=None,\n@@ -3163,6 +3201,7 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n+  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='sparse_categorical_crossentropy',\n                dtype=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e71391c46fc561746905733a3dc5c2a8a9844e29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 187 | Files Changed: 4 | Hunks: 48 | Methods Changed: 4 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 188 | Churn Cumulative: 2721 | Contributors (this commit): 4 | Commits (past 90d): 30 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,92 +0,0 @@\n-# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for metrics.\"\"\"\n-\n-from absl.testing import parameterized\n-from keras import metrics\n-from keras.dtensor import dtensor_api as dtensor\n-from keras.utils import tf_utils\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-\n-from keras.dtensor.tests import test_util\n-\n-\n-class MetricsTest(test_util.DTensorBaseTest):\n-\n-  def setUp(self):\n-    super(MetricsTest, self).setUp()\n-    global_ids = test_util.create_device_ids_array((2, 2))\n-    local_device_ids = np.ravel(global_ids).tolist()\n-    mesh_dict = {\n-        'CPU':\n-            dtensor.Mesh(['X', 'Y'], global_ids, local_device_ids,\n-                         test_util.create_device_list((2, 2), 'CPU'))\n-    }\n-    self.mesh = self.configTestMesh(mesh_dict)\n-    tf_utils.set_random_seed(1337)\n-\n-  @parameterized.parameters(\n-      (metrics.Accuracy, {}),\n-      (metrics.AUC, {}),\n-      (metrics.BinaryAccuracy, {}),\n-      (metrics.BinaryCrossentropy, {}),\n-      (metrics.BinaryIoU, {}),\n-      (metrics.CategoricalAccuracy, {}),\n-      (metrics.CategoricalCrossentropy, {}),\n-      (metrics.CategoricalHinge, {}),\n-      (metrics.CosineSimilarity, {}),\n-      (metrics.FalseNegatives, {}),\n-      (metrics.FalsePositives, {}),\n-      (metrics.Hinge, {}),\n-      (metrics.IoU, {'num_classes': 3, 'target_class_ids': [1]}),\n-      (metrics.KLDivergence, {}),\n-      (metrics.LogCoshError, {}),\n-      (metrics.Mean, {}),\n-      (metrics.MeanAbsoluteError, {}),\n-      (metrics.MeanAbsolutePercentageError, {}),\n-      (metrics.MeanIoU, {'num_classes': 3}),\n-      (metrics.MeanRelativeError, {'normalizer': [1, 3, 2, 3]}),\n-      (metrics.MeanSquaredError, {}),\n-      (metrics.MeanSquaredLogarithmicError, {}),\n-      (metrics.OneHotIoU, {'num_classes': 3, 'target_class_ids': [1]}),\n-      (metrics.OneHotMeanIoU, {'num_classes': 3}),\n-      (metrics.Poisson, {}),\n-      (metrics.Precision, {}),\n-      (metrics.PrecisionAtRecall, {'recall': 0.5}),\n-      (metrics.Recall, {}),\n-      (metrics.RecallAtPrecision, {'precision': 0.5}),\n-      (metrics.RootMeanSquaredError, {}),\n-      (metrics.SensitivityAtSpecificity, {'specificity': 0.5}),\n-      (metrics.SparseCategoricalAccuracy, {}),\n-      (metrics.SparseCategoricalCrossentropy, {}),\n-      (metrics.SparseTopKCategoricalAccuracy, {}),\n-      (metrics.SpecificityAtSensitivity, {'sensitivity': 0.5}),\n-      (metrics.SquaredHinge, {}),\n-      (metrics.Sum, {}),\n-      (metrics.TopKCategoricalAccuracy, {}),\n-      (metrics.TrueNegatives, {}),\n-      (metrics.TruePositives, {}),\n-  )\n-  def test_metric_layout(self, metric_cls, init_args):\n-    metric = metric_cls(**init_args, mesh=self.mesh)\n-\n-    for weight in metric.non_trainable_weights:\n-      self.assertIsInstance(weight, dtensor.DVariable)\n-      self.assertTrue(weight.layout.is_fully_replicated())\n-\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n@@ -103,47 +103,6 @@ def allow_initializer_layout(init_method):\n       target=init_method, decorator_func=_wrap_function)\n \n \n-def inject_mesh(init_method):\n-  \"\"\"Inject DTensor mesh information to an object.\n-\n-  This is useful for keras object like `Metric` and `Optimizer` which need\n-  DTensor mesh to create the weights, but doesn't want to change the current\n-  public API interface.\n-\n-  This is for temporary usage and eventually the mesh/layout information will be\n-  public arguments in the `__init__` method\n-\n-  Sample usage:\n-  ```python\n-  class Accuracy(tf.keras.metrics.Metric):\n-\n-    @inject_mesh\n-    def __init__(self, name='accuracy', dtype=None):\n-       super().__init__(**kwargs)\n-\n-    acc = Accuracy(mesh=mesh)\n-    assert acc._mesh == mesh\n-  ```\n-\n-  Args:\n-    init_method: the `__init__` method of the Keras class to annotate.\n-\n-  Returns:\n-    the annotated __init__ method.\n-  \"\"\"\n-  def _wrap_function(instance, *args, **kwargs):\n-    mesh = kwargs.pop(\"mesh\", None)\n-    # Note that the injection of _mesh need to happen before the invocation of\n-    # __init__, since the class might need the mesh to create weights in the\n-    # __init__.\n-    if mesh is not None:\n-      instance._mesh = mesh  # pylint: disable=protected-access\n-    init_method(instance, *args, **kwargs)\n-\n-  return tf.__internal__.decorator.make_decorator(\n-      target=init_method, decorator_func=_wrap_function)\n-\n-\n def call_with_layout(fn, layout, *args, **kwargs):\n   \"\"\"Invoke the function with inputs and relayout the result.\n \n\n@@ -22,8 +22,6 @@ import types\n import warnings\n \n from keras import backend\n-from keras.dtensor import dtensor_api as dtensor\n-from keras.dtensor import utils as dtensor_utils\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import keras_tensor\n@@ -339,13 +337,6 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     # TODO(b/120571621): Make `ON_READ` work with Keras metrics on TPU.\n     if backend.is_tpu_strategy(strategy):\n       synchronization = tf.VariableSynchronization.ON_WRITE\n-    if getattr(self, '_mesh', None) is not None:\n-      # When self._mesh is set, it means this metric is used for DTensor.\n-      additional_kwargs = {\n-          'layout': dtensor.Layout.replicated(self._mesh,\n-                                              tf.TensorShape(shape).rank)}\n-    else:\n-      additional_kwargs = {}\n \n     with tf.init_scope():\n       return super(Metric, self).add_weight(\n@@ -356,8 +347,7 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n           initializer=initializer,\n           collections=[],\n           synchronization=synchronization,\n-          aggregation=aggregation,\n-          **additional_kwargs)\n+          aggregation=aggregation)\n \n   ### End: For use by subclasses ###\n \n@@ -530,7 +520,6 @@ class Sum(Reduce):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='sum', dtype=None):\n     super(Sum, self).__init__(reduction=metrics_utils.Reduction.SUM,\n                               name=name, dtype=dtype)\n@@ -573,7 +562,6 @@ class Mean(Reduce):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean', dtype=None):\n     super(Mean, self).__init__(\n         reduction=metrics_utils.Reduction.WEIGHTED_MEAN, name=name, dtype=dtype)\n@@ -607,7 +595,6 @@ class MeanMetricWrapper(Mean):\n     **kwargs: Keyword arguments to pass on to `fn`.\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, fn, name=None, dtype=None, **kwargs):\n     super(MeanMetricWrapper, self).__init__(name=name, dtype=dtype)\n     self._fn = fn\n@@ -708,7 +695,6 @@ class MeanTensor(Metric):\n   array([[2., 3., 4., 5.]])\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_tensor', dtype=None, shape=None):\n     super(MeanTensor, self).__init__(name=name, dtype=dtype)\n     self._shape = None\n\n@@ -21,7 +21,6 @@ from typing import List, Tuple, Union\n \n from keras import activations\n from keras import backend\n-from keras.dtensor import utils as dtensor_utils\n from keras.losses import binary_crossentropy\n from keras.losses import categorical_crossentropy\n from keras.losses import categorical_hinge\n@@ -84,7 +83,6 @@ class MeanRelativeError(base_metric.Mean):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, normalizer, name=None, dtype=None):\n     super(MeanRelativeError, self).__init__(name=name, dtype=dtype)\n     normalizer = tf.cast(normalizer, self._dtype)\n@@ -165,7 +163,6 @@ class Accuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='accuracy', dtype=None):\n     super(Accuracy, self).__init__(accuracy, name, dtype=dtype)\n \n@@ -210,7 +207,6 @@ class BinaryAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n     super(BinaryAccuracy, self).__init__(\n         binary_accuracy, name, dtype=dtype, threshold=threshold)\n@@ -263,7 +259,6 @@ class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_accuracy', dtype=None):\n     super(CategoricalAccuracy, self).__init__(\n         categorical_accuracy, name, dtype=dtype)\n@@ -315,7 +310,6 @@ class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n     super(SparseCategoricalAccuracy, self).__init__(\n         sparse_categorical_accuracy, name, dtype=dtype)\n@@ -381,7 +375,6 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n     super(TopKCategoricalAccuracy, self).__init__(\n         top_k_categorical_accuracy, name, dtype=dtype, k=k)\n@@ -420,7 +413,6 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n     super(SparseTopKCategoricalAccuracy, self).__init__(\n         sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n@@ -540,7 +532,6 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(FalsePositives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n@@ -590,7 +581,6 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(FalseNegatives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n@@ -640,7 +630,6 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(TrueNegatives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n@@ -690,7 +679,6 @@ class TruePositives(_ConfusionMatrixConditionCount):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, thresholds=None, name=None, dtype=None):\n     super(TruePositives, self).__init__(\n         confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n@@ -768,7 +756,6 @@ class Precision(base_metric.Metric):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                thresholds=None,\n                top_k=None,\n@@ -899,7 +886,6 @@ class Recall(base_metric.Metric):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                thresholds=None,\n                top_k=None,\n@@ -1143,7 +1129,6 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                specificity,\n                num_thresholds=200,\n@@ -1240,7 +1225,6 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                sensitivity,\n                num_thresholds=200,\n@@ -1329,7 +1313,6 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                recall,\n                num_thresholds=200,\n@@ -1418,7 +1401,6 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                precision,\n                num_thresholds=200,\n@@ -1572,7 +1554,6 @@ class AUC(base_metric.Metric):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                num_thresholds=200,\n                curve='ROC',\n@@ -1982,7 +1963,6 @@ class CosineSimilarity(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='cosine_similarity', dtype=None, axis=-1):\n     super(CosineSimilarity, self).__init__(\n         cosine_similarity, name, dtype=dtype, axis=axis)\n@@ -2019,7 +1999,6 @@ class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_error', dtype=None):\n     super(MeanAbsoluteError, self).__init__(\n         mean_absolute_error, name, dtype=dtype)\n@@ -2056,7 +2035,6 @@ class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_absolute_percentage_error', dtype=None):\n     super(MeanAbsolutePercentageError, self).__init__(\n         mean_absolute_percentage_error, name, dtype=dtype)\n@@ -2093,7 +2071,6 @@ class MeanSquaredError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_error', dtype=None):\n     super(MeanSquaredError, self).__init__(\n         mean_squared_error, name, dtype=dtype)\n@@ -2130,7 +2107,6 @@ class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='mean_squared_logarithmic_error', dtype=None):\n     super(MeanSquaredLogarithmicError, self).__init__(\n         mean_squared_logarithmic_error, name, dtype=dtype)\n@@ -2167,7 +2143,6 @@ class Hinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='hinge', dtype=None):\n     super(Hinge, self).__init__(hinge, name, dtype=dtype)\n \n@@ -2206,7 +2181,6 @@ class SquaredHinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='squared_hinge', dtype=None):\n     super(SquaredHinge, self).__init__(squared_hinge, name, dtype=dtype)\n \n@@ -2242,7 +2216,6 @@ class CategoricalHinge(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='categorical_hinge', dtype=None):\n     super(CategoricalHinge, self).__init__(categorical_hinge, name, dtype=dtype)\n \n@@ -2274,7 +2247,6 @@ class RootMeanSquaredError(base_metric.Mean):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='root_mean_squared_error', dtype=None):\n     super(RootMeanSquaredError, self).__init__(name, dtype=dtype)\n \n@@ -2335,7 +2307,6 @@ class LogCoshError(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='logcosh', dtype=None):\n     super(LogCoshError, self).__init__(logcosh, name, dtype=dtype)\n \n@@ -2372,7 +2343,6 @@ class Poisson(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='poisson', dtype=None):\n     super(Poisson, self).__init__(poisson, name, dtype=dtype)\n \n@@ -2409,7 +2379,6 @@ class KLDivergence(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, name='kullback_leibler_divergence', dtype=None):\n     super(KLDivergence, self).__init__(\n         kullback_leibler_divergence, name, dtype=dtype)\n@@ -2565,7 +2534,6 @@ class IoU(_IoUBase):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -2688,7 +2656,6 @@ class BinaryIoU(IoU):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n@@ -2795,7 +2762,6 @@ class MeanIoU(IoU):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self, num_classes, name=None, dtype=None):\n     target_class_ids = list(range(num_classes))\n     super(MeanIoU, self).__init__(\n@@ -2886,7 +2852,6 @@ class OneHotIoU(IoU):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -2991,7 +2956,6 @@ class OneHotMeanIoU(MeanIoU):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(\n       self,\n       num_classes: int,\n@@ -3064,7 +3028,6 @@ class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='binary_crossentropy',\n                dtype=None,\n@@ -3129,7 +3092,6 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='categorical_crossentropy',\n                dtype=None,\n@@ -3201,7 +3163,6 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n   ```\n   \"\"\"\n \n-  @dtensor_utils.inject_mesh\n   def __init__(self,\n                name='sparse_categorical_crossentropy',\n                dtype=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
