{"custom_id": "scrapy#7daf735f45ab9f3d8760d7a59e5f72c277d0687d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 11 | Files Changed: 5 | Hunks: 13 | Methods Changed: 16 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 7571 | Contributors (this commit): 59 | Commits (past 90d): 13 | Contributors (cumulative): 100 | DMM Complexity: None\n\nDIFF:\n@@ -3,13 +3,14 @@ from os import PathLike\n from pathlib import Path\n from importlib import import_module\n from types import ModuleType\n+from typing import Union\n \n from scrapy.utils.spider import iter_spider_classes\n from scrapy.exceptions import UsageError\n from scrapy.commands import BaseRunSpiderCommand\n \n \n-def _import_file(filepath: str | PathLike[str]) -> ModuleType:\n+def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n     abspath = Path(filepath).resolve()\n     dirname = str(abspath.parent)\n     if abspath.suffix not in ('.py', '.pyw'):\n\n@@ -6,6 +6,7 @@ import marshal\n import pickle\n from os import PathLike\n from pathlib import Path\n+from typing import Union\n \n from queuelib import queue\n \n@@ -17,7 +18,7 @@ def _with_mkdir(queue_class):\n \n     class DirectoriesCreated(queue_class):\n \n-        def __init__(self, path: str | PathLike[str], *args, **kwargs):\n+        def __init__(self, path: Union[str, PathLike], *args, **kwargs):\n             dirname = Path(path).parent\n             if not dirname.exists():\n                 dirname.mkdir(parents=True, exist_ok=True)\n\n@@ -5,7 +5,7 @@ import warnings\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n-from typing import Optional\n+from typing import List, Optional, Union\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n \n@@ -67,7 +67,7 @@ def arglist_to_dict(arglist):\n     return dict(x.split('=', 1) for x in arglist)\n \n \n-def closest_scrapy_cfg(path: str | os.PathLike[str] = '.', prevpath: Optional[str | os.PathLike] = None) -> str:\n+def closest_scrapy_cfg(path: Union[str, os.PathLike] = '.', prevpath: Optional[Union[str, os.PathLike]] = None) -> str:\n     \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n     directory and its parents\n     \"\"\"\n@@ -103,7 +103,7 @@ def get_config(use_closest=True):\n     return cfg\n \n \n-def get_sources(use_closest=True) -> list[str]:\n+def get_sources(use_closest=True) -> List[str]:\n     xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or Path('~/.config').expanduser()\n     sources = [\n         '/etc/scrapy.cfg',\n@@ -131,8 +131,8 @@ def feed_complete_default_values_from_settings(feed, settings):\n     return out\n \n \n-def feed_process_params_from_cli(settings, output: list[str], output_format=None,\n-                                 overwrite_output: Optional[list[str]] = None):\n+def feed_process_params_from_cli(settings, output: List[str], output_format=None,\n+                                 overwrite_output: Optional[List[str]] = None):\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n     checks for inconsistencies in their quantities and returns a dictionary\n\n@@ -14,7 +14,7 @@ from shutil import rmtree, copytree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import mkdtemp\n from threading import Timer\n-from typing import Generator, Optional\n+from typing import Dict, Generator, Optional, Union\n from unittest import skipIf\n \n from pytest import mark\n@@ -104,7 +104,7 @@ class ProjectTest(unittest.TestCase):\n \n         return p, to_unicode(stdout), to_unicode(stderr)\n \n-    def find_in_file(self, filename: str | os.PathLike[str], regex) -> Optional[re.Match]:\n+    def find_in_file(self, filename: Union[str, os.PathLike], regex) -> Optional[re.Match]:\n         \"\"\"Find first pattern occurrence in file\"\"\"\n         pattern = re.compile(regex)\n         with Path(filename).open(\"r\") as f:\n@@ -175,7 +175,7 @@ class StartprojectTest(ProjectTest):\n         assert Path(project_path, project_name, 'spiders', '__init__.py').exists()\n \n \n-def get_permissions_dict(path: str | os.PathLike[str], renamings=None, ignore=None) -> dict[str, str]:\n+def get_permissions_dict(path: Union[str, os.PathLike], renamings=None, ignore=None) -> Dict[str, str]:\n \n     def get_permissions(path: Path) -> str:\n         return oct(path.stat().st_mode)\n\n@@ -17,6 +17,7 @@ from logging import getLogger\n from os import PathLike\n from pathlib import Path\n from string import ascii_letters, digits\n+from typing import Union\n from unittest import mock\n from urllib.parse import urljoin, quote\n from urllib.request import pathname2url\n@@ -63,7 +64,7 @@ def printf_escape(string):\n     return string.replace('%', '%%')\n \n \n-def build_url(path: str | PathLike[str]) -> str:\n+def build_url(path: Union[str, PathLike]) -> str:\n     path_str = str(path)\n     if path_str[0] != '/':\n         path_str = '/' + path_str\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb26e6b650005e63c7017b55ff7b9755938a6c11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1755 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -547,7 +547,7 @@ class GenspiderStandaloneCommandTest(ProjectTest):\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n-            self.assertIn(f\"{Path(self.temp_path, file_name + '.py')} already exists\", out)\n+            self.assertIn(f\"{Path(self.temp_path, file_name + '.py').resolve()} already exists\", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n             file_contents_after = file_path.read_text()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93d82648e59a1a211c114d0b8be2d76e0d79b582", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1757 | Contributors (this commit): 25 | Commits (past 90d): 5 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -492,7 +492,7 @@ class GenspiderCommandTest(CommandTest):\n             self.assertNotEqual(file_contents_after, file_contents_before)\n         else:\n             p, out, err = self.proc('genspider', file_name, 'example.com')\n-            self.assertIn(f\"{file_path} already exists\", out)\n+            self.assertIn(f\"{file_path.resolve()} already exists\", out)\n             modify_time_after = file_path.stat().st_mtime\n             self.assertEqual(modify_time_after, modify_time_before)\n             file_contents_after = file_path.read_text()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#065db7b56688ce5cbdc1508cfda9603d33b17a87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 7 | Files Changed: 6 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 4020 | Contributors (this commit): 49 | Commits (past 90d): 22 | Contributors (cumulative): 99 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,7 +15,7 @@ from scrapy.exceptions import UsageError\n class ScrapyCommand:\n \n     requires_project = False\n-    crawler_process = None\n+    crawler_process: Any = None\n \n     # default settings to be used for this command instead of global defaults\n     default_settings: Dict[str, Any] = {}\n@@ -23,7 +23,7 @@ class ScrapyCommand:\n     exitcode = 0\n \n     def __init__(self):\n-        self.settings = None  # set in scrapy.cmdline\n+        self.settings: Any = None  # set in scrapy.cmdline\n \n     def set_crawler(self, crawler):\n         if hasattr(self, '_crawler'):\n\n@@ -4,7 +4,7 @@ import string\n \n from pathlib import Path\n from importlib import import_module\n-from typing import Optional\n+from typing import Optional, cast\n from urllib.parse import urlparse\n \n import scrapy\n@@ -116,6 +116,7 @@ class Command(ScrapyCommand):\n             return template_file\n         print(f\"Unable to find template: {template}\\n\")\n         print('Use \"scrapy genspider --list\" to see all available templates.')\n+        return None\n \n     def _list_templates(self):\n         print(\"Available templates:\")\n@@ -144,7 +145,7 @@ class Command(ScrapyCommand):\n \n         # a file with the same name exists in the target directory\n         spiders_module = import_module(self.settings['NEWSPIDER_MODULE'])\n-        spiders_dir = Path(spiders_module.__file__).parent\n+        spiders_dir = Path(cast(str, spiders_module.__file__)).parent\n         spiders_dir_abs = spiders_dir.resolve()\n         path = spiders_dir_abs / (name + \".py\")\n         if path.exists():\n\n@@ -22,7 +22,7 @@ class Spider(object_ref):\n     class.\n     \"\"\"\n \n-    name: Optional[str] = None\n+    name: str\n     custom_settings: Optional[dict] = None\n \n     def __init__(self, name=None, **kwargs):\n\n@@ -5,7 +5,7 @@ import warnings\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n-from typing import List, Optional, Union\n+from typing import Any, Dict, List, Optional, Union\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n \n@@ -176,7 +176,7 @@ def feed_process_params_from_cli(settings, output: List[str], output_format=None\n                 'URIs are specified'\n             )\n \n-    result = {}\n+    result: Dict[str, Dict[str, Any]] = {}\n     for element in output:\n         try:\n             feed_uri, feed_format = element.rsplit(':', 1)\n\n@@ -112,6 +112,7 @@ class ProjectTest(unittest.TestCase):\n                 match = pattern.search(line)\n                 if match is not None:\n                     return match\n+        return None\n \n \n class StartprojectTest(ProjectTest):\n\n@@ -292,6 +292,8 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n \n \n class ScriptRunnerMixin:\n+    script_dir: Path\n+\n     def run_script(self, script_name: str, *script_args):\n         script_path = self.script_dir / script_name\n         args = [sys.executable, str(script_path)] + list(script_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c85de908193a534c60140492d5f793ab32ef3b48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 49 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ class ScrapyUtilsTest(unittest.TestCase):\n         if not os.environ.get('_SCRAPY_PINNED', None):\n             self.skipTest('Not in a pinned environment')\n \n-        tox_config_file_path = Path(__file__) / '..' / 'tox.ini'\n+        tox_config_file_path = Path(__file__).parent / '..' / 'tox.ini'\n         config_parser = ConfigParser()\n         config_parser.read(tox_config_file_path)\n         pattern = r'Twisted\\[http2\\]==([\\d.]+)'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#24d6ac1f529e1e2ea5b04cff30cf9015311ef420", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 788 | Contributors (this commit): 27 | Commits (past 90d): 2 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ from contextlib import suppress\n from ftplib import FTP\n from io import BytesIO\n from pathlib import Path\n-from typing import Optional\n+from typing import DefaultDict, Optional, Set\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n@@ -46,7 +46,7 @@ class FSFilesStore:\n             basedir = basedir.split('://', 1)[1]\n         self.basedir = basedir\n         self._mkdir(Path(self.basedir))\n-        self.created_directories: defaultdict[str, set[str]] = defaultdict(set)\n+        self.created_directories: DefaultDict[str, Set[str]] = defaultdict(set)\n \n     def persist_file(self, path: str, buf, info, meta=None, headers=None):\n         absolute_path = self._get_filesystem_path(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#12a26755ae601c9b3244a4a6b55d0e880fc21864", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 543 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -109,7 +109,7 @@ class Command(ScrapyCommand):\n         move(project_dir / 'module', project_dir / project_name)\n         for paths in TEMPLATES_TO_RENDER:\n             tplfile = Path(project_dir, *(string.Template(s).substitute(project_name=project_name) for s in paths))\n-            render_templatefile(str(tplfile), project_name=project_name, ProjectName=string_camelcase(project_name))\n+            render_templatefile(tplfile, project_name=project_name, ProjectName=string_camelcase(project_name))\n         print(f\"New Scrapy project '{project_name}', using template directory \"\n               f\"'{self.templates_dir}', created in:\")\n         print(f\"    {project_dir.resolve()}\\n\")\n\n@@ -1,21 +1,24 @@\n \"\"\"Helper functions for working with templates\"\"\"\n \n+from os import PathLike\n import re\n import string\n from pathlib import Path\n+from typing import Union\n \n \n-def render_templatefile(path: str, **kwargs):\n-    raw = Path(path).read_text('utf8')\n+def render_templatefile(path: Union[str, PathLike], **kwargs):\n+    path_obj = Path(path)\n+    raw = path_obj.read_text('utf8')\n \n     content = string.Template(raw).substitute(**kwargs)\n \n-    render_path = path[:-len('.tmpl')] if path.endswith('.tmpl') else path\n+    render_path = path_obj.with_suffix('') if path_obj.suffix == '.tmpl' else path_obj\n \n-    if path.endswith('.tmpl'):\n-        Path(path).rename(render_path)\n+    if path_obj.suffix == '.tmpl':\n+        path_obj.rename(render_path)\n \n-    Path(render_path).write_text(content, 'utf8')\n+    render_path.write_text(content, 'utf8')\n \n \n CAMELCASE_INVALID_CHARS = re.compile(r'[^a-zA-Z\\d]')\n\n@@ -28,7 +28,7 @@ class UtilsRenderTemplateFileTestCase(unittest.TestCase):\n         template_path.write_text(template, encoding='utf8')\n         assert template_path.is_file()  # Failure of test itself\n \n-        render_templatefile(str(template_path), **context)\n+        render_templatefile(template_path, **context)\n \n         self.assertFalse(template_path.exists())\n         self.assertEqual(render_path.read_text(encoding='utf8'), rendered)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#607eece72a91beb0e9a598a987e51a56bcffb434", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 275 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -12,9 +12,9 @@ from scrapy.commands import BaseRunSpiderCommand\n \n def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n     abspath = Path(filepath).resolve()\n-    dirname = str(abspath.parent)\n     if abspath.suffix not in ('.py', '.pyw'):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n+    dirname = str(abspath.parent)\n     if dirname:\n         sys.path = [dirname] + sys.path\n     try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b33244e2f0d877b8911f949308222db0b076d665", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 1121 | Contributors (this commit): 23 | Commits (past 90d): 1 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -34,6 +34,7 @@ def url_has_any_extension(url, extensions):\n     lowercase_path = parse_url(url).path.lower()\n     return any(lowercase_path.endswith(ext) for ext in extensions)\n \n+\n def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#69d1b8fc0832f1cf00727c73eff6fbde11824c20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 2 | Churn Cumulative: 277 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,12 +15,10 @@ def _import_file(filepath: Union[str, PathLike]) -> ModuleType:\n     if abspath.suffix not in ('.py', '.pyw'):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n     dirname = str(abspath.parent)\n-    if dirname:\n     sys.path = [dirname] + sys.path\n     try:\n         module = import_module(abspath.stem)\n     finally:\n-        if dirname:\n         sys.path.pop(0)\n     return module\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#28396c34977535ca59ed5d8481b50dd1e8171e49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 382 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -293,7 +293,7 @@ class FilesystemCacheStorage:\n         metadata = self._read_meta(spider, request)\n         if metadata is None:\n             return  # not cached\n-        rpath = self._get_request_path(spider, request)\n+        rpath = Path(self._get_request_path(spider, request))\n         with self._open(rpath / 'response_body', 'rb') as f:\n             body = f.read()\n         with self._open(rpath / 'response_headers', 'rb') as f:\n@@ -307,7 +307,7 @@ class FilesystemCacheStorage:\n \n     def store_response(self, spider: Spider, request: Request, response):\n         \"\"\"Store the given response in the cache.\"\"\"\n-        rpath = self._get_request_path(spider, request)\n+        rpath = Path(self._get_request_path(spider, request))\n         if not rpath.exists():\n             rpath.mkdir(parents=True)\n         metadata = {\n@@ -330,12 +330,12 @@ class FilesystemCacheStorage:\n         with self._open(rpath / 'request_body', 'wb') as f:\n             f.write(request.body)\n \n-    def _get_request_path(self, spider: Spider, request: Request) -> Path:\n+    def _get_request_path(self, spider: Spider, request: Request) -> str:\n         key = self._fingerprinter.fingerprint(request).hex()\n-        return Path(self.cachedir, spider.name, key[0:2], key)\n+        return str(Path(self.cachedir, spider.name, key[0:2], key))\n \n     def _read_meta(self, spider: Spider, request: Request):\n-        rpath = self._get_request_path(spider, request)\n+        rpath = Path(self._get_request_path(spider, request))\n         metapath = rpath / 'pickled_meta'\n         if not metapath.exists():\n             return  # not found\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#726680c7125ab3a6622b12e25d45dbfedc5a39b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 503 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,9 +4,10 @@ Base class for Scrapy commands\n import os\n import argparse\n from pathlib import Path\n-from typing import Any, Dict\n+from typing import Any, Dict, Optional\n \n from twisted.python import failure\n+from scrapy.crawler import CrawlerProcess\n \n from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n from scrapy.exceptions import UsageError\n@@ -15,7 +16,7 @@ from scrapy.exceptions import UsageError\n class ScrapyCommand:\n \n     requires_project = False\n-    crawler_process: Any = None\n+    crawler_process: Optional[CrawlerProcess] = None\n \n     # default settings to be used for this command instead of global defaults\n     default_settings: Dict[str, Any] = {}\n\n@@ -133,6 +133,10 @@ class Command(ScrapyCommand):\n                 return True\n             return False\n \n+        assert (\n+            self.crawler_process is not None\n+        ), \"crawler_process must be set before calling run\"\n+\n         try:\n             spidercls = self.crawler_process.spider_loader.load(name)\n         except KeyError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b61b71c6f015d45f6e98a4280bf4993517180045", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 647 | Contributors (this commit): 32 | Commits (past 90d): 2 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -226,7 +226,14 @@ def is_generator_with_return_value(callable):\n         return value is None or isinstance(value, ast.NameConstant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n-        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n+        pattern = r\"(^[\\t ]+)\"\n+        src = inspect.getsource(callable)\n+        match = re.match(pattern, src)  # Find indentation\n+        code = re.sub(pattern, \"\", src)\n+        if match:\n+            # Remove indentation\n+            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)\n+\n         tree = ast.parse(code)\n         for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f4e2a10ed6a44300738bd3701ea62b432c9c1a06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 530 | Contributors (this commit): 22 | Commits (past 90d): 1 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -160,7 +160,14 @@ class ImagesPipeline(FilesPipeline):\n \n         if size:\n             image = image.copy()\n-            image.thumbnail(size, self._Image.ANTIALIAS)\n+            try:\n+                # Image.Resampling.LANCZOS was added in Pillow 9.1.0\n+                # remove this try except block,\n+                # when updating the minimum requirements for Pillow.\n+                resampling_filter = self._Image.Resampling.LANCZOS\n+            except AttributeError:\n+                resampling_filter = self._Image.ANTIALIAS\n+            image.thumbnail(size, resampling_filter)\n \n         buf = BytesIO()\n         image.save(buf, 'JPEG')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#830e1c5dd85618a27749bbe41d35b11fb2bdd348", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 83 | Churn Cumulative: 306 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -165,6 +165,89 @@ https://example.org\n             warn_on_generator_with_return_value(None, l2)\n             self.assertEqual(len(w), 0)\n \n+    def test_generators_return_none_with_decorator(self):\n+        def decorator(func):\n+            def inner_func():\n+                func()\n+            return inner_func\n+\n+        @decorator\n+        def f3():\n+            yield 1\n+            return None\n+\n+        @decorator\n+        def g3():\n+            yield 1\n+            return\n+\n+        @decorator\n+        def h3():\n+            yield 1\n+\n+        @decorator\n+        def i3():\n+            yield 1\n+            yield from generator_that_returns_stuff()\n+\n+        @decorator\n+        def j3():\n+            yield 1\n+\n+            def helper():\n+                return 0\n+\n+            yield helper()\n+\n+        @decorator\n+        def k3():\n+            \"\"\"\n+docstring\n+            \"\"\"\n+            url = \"\"\"\n+https://example.org\n+        \"\"\"\n+            yield url\n+            return\n+\n+        @decorator\n+        def l3():\n+            return\n+\n+        assert not is_generator_with_return_value(top_level_return_none)\n+        assert not is_generator_with_return_value(f3)\n+        assert not is_generator_with_return_value(g3)\n+        assert not is_generator_with_return_value(h3)\n+        assert not is_generator_with_return_value(i3)\n+        assert not is_generator_with_return_value(j3)  # not recursive\n+        assert not is_generator_with_return_value(k3)  # not recursive\n+        assert not is_generator_with_return_value(l3)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, top_level_return_none)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, f3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, g3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, h3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, i3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, j3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, k3)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, l3)\n+            self.assertEqual(len(w), 0)\n+\n     @mock.patch(\"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error)\n     def test_indentation_error(self):\n         with warnings.catch_warnings(record=True) as w:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0ddffc47b9cee5e6146497b42de3787da76d2ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 657 | Contributors (this commit): 32 | Commits (past 90d): 3 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -226,13 +226,13 @@ def is_generator_with_return_value(callable):\n         return value is None or isinstance(value, ast.NameConstant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n-        pattern = r\"(^[\\t ]+)\"\n         src = inspect.getsource(callable)\n-        match = re.match(pattern, src)  # Find indentation\n-        code = re.sub(pattern, \"\", src)\n+        pattern = re.compile(r\"(^[\\t ]+)\")\n+        code = pattern.sub(\"\", src)\n+\n+        match = pattern.match(src)  # finds indentation\n         if match:\n-            # Remove indentation\n-            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)\n+            code = re.sub(f\"\\n{match.group(0)}\", \"\\n\", code)  # remove indentation\n \n         tree = ast.parse(code)\n         for node in walk_callable(tree):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2464939b7ee8381f904f7b0625aee6de0989b6c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 2800 | Contributors (this commit): 28 | Commits (past 90d): 1 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -257,9 +257,7 @@ class ExecutionEngine:\n \n     def download(self, request: Request, spider: Optional[Spider] = None) -> Deferred:\n         \"\"\"Return a Deferred which fires with a Response as result, only downloader middlewares are applied\"\"\"\n-        if spider is None:\n-            spider = self.spider\n-        else:\n+        if spider is not None:\n             warnings.warn(\n                 \"Passing a 'spider' argument to ExecutionEngine.download is deprecated\",\n                 category=ScrapyDeprecationWarning,\n@@ -267,7 +265,7 @@ class ExecutionEngine:\n             )\n             if spider is not self.spider:\n                 logger.warning(\"The spider '%s' does not match the open spider\", spider.name)\n-        if spider is None:\n+        if self.spider is None:\n             raise RuntimeError(f\"No open spider to crawl: {request}\")\n         return self._download(request, spider).addBoth(self._downloaded, request, spider)\n \n@@ -278,11 +276,14 @@ class ExecutionEngine:\n         self.slot.remove_request(request)\n         return self.download(result, spider) if isinstance(result, Request) else result\n \n-    def _download(self, request: Request, spider: Spider) -> Deferred:\n+    def _download(self, request: Request, spider: Optional[Spider]) -> Deferred:\n         assert self.slot is not None  # typing\n \n         self.slot.add_request(request)\n \n+        if spider is None:\n+            spider = self.spider\n+\n         def _on_success(result: Union[Response, Request]) -> Union[Response, Request]:\n             if not isinstance(result, (Response, Request)):\n                 raise TypeError(f\"Incorrect type: expected Response or Request, got {type(result)}: {result!r}\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca50af645390e38299082d9ef4682c4be482ae70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 1 | Churn Cumulative: 1091 | Contributors (this commit): 20 | Commits (past 90d): 1 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,6 @@ import io\n import random\n from shutil import rmtree\n from tempfile import mkdtemp\n-from unittest import skipIf\n from warnings import catch_warnings\n \n import attr\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3a34fa839938d7640e56f3ce12ba60aed31a6ecf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 17/16 | Churn Δ: 33 | Churn Cumulative: 962 | Contributors (this commit): 25 | Commits (past 90d): 4 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ from twisted.python import failure\n from twisted.python.failure import Failure\n \n from scrapy.exceptions import IgnoreRequest\n-from scrapy.utils.reactor import is_asyncio_reactor_installed\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, get_asyncio_event_loop_policy\n \n \n def defer_fail(_failure: Failure) -> Deferred:\n@@ -269,7 +269,8 @@ def deferred_from_coro(o) -> Any:\n             return ensureDeferred(o)\n         else:\n             # wrapping the coroutine into a Future and then into a Deferred, this requires AsyncioSelectorReactor\n-            return Deferred.fromFuture(asyncio.ensure_future(o))\n+            event_loop = get_asyncio_event_loop_policy().get_event_loop()\n+            return Deferred.fromFuture(asyncio.ensure_future(o, loop=event_loop))\n     return o\n \n \n@@ -320,7 +321,8 @@ def deferred_to_future(d: Deferred) -> Future:\n                 d = treq.get('https://example.com/additional')\n                 additional_response = await deferred_to_future(d)\n     \"\"\"\n-    return d.asFuture(asyncio.get_event_loop())\n+    policy = get_asyncio_event_loop_policy()\n+    return d.asFuture(policy.get_event_loop())\n \n \n def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n\n@@ -51,6 +51,19 @@ class CallLaterOnce:\n         return self._func(*self._a, **self._kw)\n \n \n+def get_asyncio_event_loop_policy():\n+    policy = asyncio.get_event_loop_policy()\n+    if (\n+        sys.version_info >= (3, 8)\n+        and sys.platform == \"win32\"\n+        and not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy)\n+    ):\n+        policy = asyncio.WindowsSelectorEventLoopPolicy()\n+        asyncio.set_event_loop_policy(policy)\n+\n+    return policy\n+\n+\n def install_reactor(reactor_path, event_loop_path=None):\n     \"\"\"Installs the :mod:`~twisted.internet.reactor` with the specified\n     import path. Also installs the asyncio event loop with the specified import\n@@ -58,16 +71,14 @@ def install_reactor(reactor_path, event_loop_path=None):\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n-            if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n-                policy = asyncio.get_event_loop_policy()\n-                if not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy):\n-                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n+            policy = get_asyncio_event_loop_policy()\n             if event_loop_path is not None:\n                 event_loop_class = load_object(event_loop_path)\n                 event_loop = event_loop_class()\n                 asyncio.set_event_loop(event_loop)\n             else:\n-                event_loop = asyncio.get_event_loop()\n+                event_loop = policy.get_event_loop()\n+\n             asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n\n@@ -1,3 +1,4 @@\n+import warnings\n from unittest import TestCase\n \n from pytest import mark\n@@ -13,5 +14,6 @@ class AsyncioTest(TestCase):\n         self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n     def test_install_asyncio_reactor(self):\n-        # this should do nothing\n+        with warnings.catch_warnings(record=True) as w:\n             install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n+            self.assertEqual(len(w), 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b71d0292d5ff85d652e2943ee1a727a128b55594", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 314 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import unittest\n import warnings\n+from functools import partial\n from unittest import mock\n \n from scrapy.utils.misc import is_generator_with_return_value, warn_on_generator_with_return_value\n@@ -254,3 +255,10 @@ https://example.org\n             warn_on_generator_with_return_value(None, top_level_return_none)\n             self.assertEqual(len(w), 1)\n             self.assertIn('Unable to determine', str(w[0].message))\n+\n+    def test_partial(self):\n+        def cb(arg1, arg2):\n+            yield {}\n+\n+        partial_cb = partial(cb, arg1=42)\n+        assert not is_generator_with_return_value(partial_cb)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd692f309105d917f5f46bd00a88c550d6cc7da3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 688 | Contributors (this commit): 24 | Commits (past 90d): 8 | Contributors (cumulative): 31 | DMM Complexity: 0.13333333333333333\n\nDIFF:\n@@ -115,9 +115,11 @@ class BaseRunSpiderCommand(ScrapyCommand):\n         parser.add_argument(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n                             help=\"set spider argument (may be repeated)\")\n         parser.add_argument(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                            help=\"append scraped items to the end of FILE (use - for stdout)\")\n+                            help=\"append scraped items to the end of FILE (use - for stdout),\"\n+                                 \" to define format set a colon at the end of the output URI (i.e. -o FILE:FORMAT)\")\n         parser.add_argument(\"-O\", \"--overwrite-output\", metavar=\"FILE\", action=\"append\",\n-                            help=\"dump scraped items into FILE, overwriting any existing file\")\n+                            help=\"dump scraped items into FILE, overwriting any existing file,\"\n+                                 \" to define format set a colon at the end of the output URI (i.e. -O FILE:FORMAT)\")\n         parser.add_argument(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n                             help=\"format to use for dumping items\")\n \n\n@@ -155,6 +155,15 @@ def feed_process_params_from_cli(settings, output, output_format=None,\n             raise UsageError(\n                 \"Please use only one of -o/--output and -O/--overwrite-output\"\n             )\n+        if output_format:\n+            raise UsageError(\n+                \"-t/--output-format is a deprecated command line option\"\n+                \" and does not work in combination with -O/--overwrite-output.\"\n+                \" To specify a format please specify it after a colon at the end of the\"\n+                \" output URI (i.e. -O <URI>:<FORMAT>).\"\n+                \" Example working in the tutorial: \"\n+                \"scrapy crawl quotes -O quotes.json:json\"\n+            )\n         output = overwrite_output\n         overwrite = True\n \n@@ -162,9 +171,13 @@ def feed_process_params_from_cli(settings, output, output_format=None,\n         if len(output) == 1:\n             check_valid_format(output_format)\n             message = (\n-                'The -t command line option is deprecated in favor of '\n-                'specifying the output format within the output URI. See the '\n-                'documentation of the -o and -O options for more information.'\n+                \"The -t/--output-format command line option is deprecated in favor of \"\n+                \"specifying the output format within the output URI using the -o/--output or the\"\n+                \" -O/--overwrite-output option (i.e. -o/-O <URI>:<FORMAT>). See the documentation\"\n+                \" of the -o or -O option or the following examples for more information. \"\n+                \"Examples working in the tutorial: \"\n+                \"scrapy crawl quotes -o quotes.csv:csv   or   \"\n+                \"scrapy crawl quotes -O quotes.json:json\"\n             )\n             warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n             return {output[0]: {'format': output_format}}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3259a4252566f4130f14ce2acd03531a735dae90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 288 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -102,9 +102,9 @@ class CrawlSpider(Spider):\n                 request = self._build_request(rule_index, link)\n                 yield rule.process_request(request, response)\n \n-    def _callback(self, response):\n+    def _callback(self, response, **cb_kwargs):\n         rule = self._rules[response.meta['rule']]\n-        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)\n+        return self._parse_response(response, rule.callback, {**rule.cb_kwargs, **cb_kwargs}, rule.follow)\n \n     def _errback(self, failure):\n         rule = self._rules[failure.request.meta['rule']]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b18560315bda057610ecda165694f9c0f7445c1f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 1741 | Contributors (this commit): 19 | Commits (past 90d): 7 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -419,6 +419,17 @@ class CrawlSpiderWithErrback(CrawlSpiderWithParseMethod):\n         self.logger.info('[errback] status %i', failure.value.response.status)\n \n \n+class CrawlSpiderWithProcessRequestCallbackKeywordArguments(CrawlSpiderWithParseMethod):\n+    name = 'crawl_spider_with_process_request_cb_kwargs'\n+    rules = (\n+        Rule(LinkExtractor(), callback='parse', follow=True, process_request=\"process_request\"),\n+    )\n+\n+    def process_request(self, request, response):\n+        request.cb_kwargs[\"foo\"] = \"process_request\"\n+        return request\n+\n+\n class BytesReceivedCallbackSpider(MetaSpider):\n \n     full_response_length = 2**18\n\n@@ -41,6 +41,7 @@ from tests.spiders import (\n     CrawlSpiderWithAsyncGeneratorCallback,\n     CrawlSpiderWithErrback,\n     CrawlSpiderWithParseMethod,\n+    CrawlSpiderWithProcessRequestCallbackKeywordArguments,\n     DelaySpider,\n     DuplicateStartRequestsSpider,\n     FollowAllSpider,\n@@ -426,6 +427,16 @@ class CrawlSpiderTestCase(TestCase):\n         self.assertIn(\"[errback] status 500\", str(log))\n         self.assertIn(\"[errback] status 501\", str(log))\n \n+    @defer.inlineCallbacks\n+    def test_crawlspider_process_request_cb_kwargs(self):\n+        crawler = get_crawler(CrawlSpiderWithProcessRequestCallbackKeywordArguments)\n+        with LogCapture() as log:\n+            yield crawler.crawl(mockserver=self.mockserver)\n+\n+        self.assertIn(\"[parse] status 200 (foo: process_request)\", str(log))\n+        self.assertIn(\"[parse] status 201 (foo: process_request)\", str(log))\n+        self.assertIn(\"[parse] status 202 (foo: bar)\", str(log))\n+\n     @defer.inlineCallbacks\n     def test_async_def_parse(self):\n         crawler = get_crawler(AsyncDefSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a34b929a40c12933f75db4665b71348444a5d603", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 462 | Contributors (this commit): 18 | Commits (past 90d): 1 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -247,6 +247,12 @@ class CsvItemExporter(BaseItemExporter):\n         values = list(self._build_row(x for _, x in fields))\n         self.csv_writer.writerow(values)\n \n+    def finish_exporting(self):\n+        # Detaching stream in order to avoid file closing.\n+        # The file will be closed with slot.storage.store\n+        # https://github.com/scrapy/scrapy/issues/5043\n+        self.stream.detach()\n+\n     def _build_row(self, values):\n         for s in values:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c0890ff54a8d49237415e5b7d7dfbf216e88577", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 37 | Churn Cumulative: 1128 | Contributors (this commit): 20 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,6 @@ from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import skipIf\n from unittest.mock import patch\n-from warnings import catch_warnings\n \n import attr\n from itemadapter import ItemAdapter\n@@ -91,6 +90,22 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                                     info=object()),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n+    def test_thumbnail_name_from_item(self):\n+        \"\"\"\n+        Custom thumbnail name based on item data, overriding default implementation\n+        \"\"\"\n+\n+        class CustomImagesPipeline(ImagesPipeline):\n+            def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n+                return f\"thumb/{thumb_id}/{item.get('path')}\"\n+\n+        thumb_path = CustomImagesPipeline.from_settings(Settings(\n+            {'IMAGES_STORE': self.tempdir}\n+        )).thumb_path\n+        item = dict(path='path-to-store-file')\n+        request = Request(\"http://example.com\")\n+        self.assertEqual(thumb_path(request, 'small', item=item), 'thumb/small/path-to-store-file')\n+\n     def test_get_images_exception(self):\n         self.pipeline.min_width = 100\n         self.pipeline.min_height = 100\n@@ -231,22 +246,6 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(converted.mode, 'RGB')\n         self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n-    def test_thumbnail_name_from_item(self):\n-        \"\"\"\n-        Custom thumbnail name based on item data, overriding default implementation\n-        \"\"\"\n-\n-        class CustomImagesPipeline(ImagesPipeline):\n-            def thumb_path(self, request, thumb_id, response=None, info=None, item=None):\n-                return f\"thumb/{thumb_id}/{item.get('path')}\"\n-\n-        thumb_path = CustomImagesPipeline.from_settings(Settings(\n-            {'IMAGES_STORE': self.tempdir}\n-        )).thumb_path\n-        item = dict(path='path-to-store-file')\n-        request = Request(\"http://example.com\")\n-        self.assertEqual(thumb_path(request, 'small', item=item), 'thumb/small/path-to-store-file')\n-\n \n class DeprecatedImagesPipeline(ImagesPipeline):\n     def file_key(self, url):\n@@ -536,11 +535,11 @@ class NoimagesDropTestCase(unittest.TestCase):\n \n     def test_deprecation_warning(self):\n         arg = str()\n-        with catch_warnings(record=True) as warnings:\n+        with warnings.catch_warnings(record=True) as warnings:\n             NoimagesDrop(arg)\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n-        with catch_warnings(record=True) as warnings:\n+        with warnings.catch_warnings(record=True) as warnings:\n             class SubclassedNoimagesDrop(NoimagesDrop):\n                 pass\n             SubclassedNoimagesDrop(arg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
