{"custom_id": "keras#1eb26066fd2839fe4a7e7c0889e4f083c6209762", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 139 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 27 | Methods Changed: 12 | Complexity Δ (Sum/Max): 37/35 | Churn Δ: 165 | Churn Cumulative: 4332 | Contributors (this commit): 13 | Commits (past 90d): 49 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -79,7 +79,6 @@ class JAXTrainer(base_trainer.Trainer):\n             metrics_variables,\n         ) = state\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        x, y, sample_weight = self._distribute_data((x, y, sample_weight))\n         grad_fn = jax.value_and_grad(\n             self.compute_loss_and_updates, has_aux=True\n         )\n@@ -116,7 +115,7 @@ class JAXTrainer(base_trainer.Trainer):\n             new_metrics_variables.append(new_v)\n         metrics_variables = new_metrics_variables\n \n-        state = (\n+        state = self._enforce_jax_state_sharding(\n             trainable_variables,\n             non_trainable_variables,\n             optimizer_variables,\n@@ -131,7 +130,6 @@ class JAXTrainer(base_trainer.Trainer):\n             metrics_variables,\n         ) = state\n         x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        x, y, sample_weight = self._distribute_data((x, y, sample_weight))\n         loss, (\n             y_pred,\n             non_trainable_variables,\n@@ -161,6 +159,17 @@ class JAXTrainer(base_trainer.Trainer):\n             new_metrics_variables.append(new_v)\n         metrics_variables = new_metrics_variables\n \n+        (\n+            trainable_variables,\n+            non_trainable_variables,\n+            _,\n+            metrics_variables,\n+        ) = self._enforce_jax_state_sharding(\n+            trainable_variables=trainable_variables,\n+            non_trainable_variables=non_trainable_variables,\n+            optimizer_variables=None,\n+            metrics_variables=metrics_variables,\n+        )\n         state = (\n             trainable_variables,\n             non_trainable_variables,\n@@ -175,10 +184,20 @@ class JAXTrainer(base_trainer.Trainer):\n             kwargs[\"training\"] = False\n \n         x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        x = self._distribute_data(x)\n         outputs, non_trainable_variables = self.stateless_call(\n             trainable_variables, non_trainable_variables, x, **kwargs\n         )\n+        (\n+            trainable_variables,\n+            non_trainable_variables,\n+            _,\n+            _,\n+        ) = self._enforce_jax_state_sharding(\n+            trainable_variables=trainable_variables,\n+            non_trainable_variables=non_trainable_variables,\n+            optimizer_variables=None,\n+            metrics_variables=None,\n+        )\n         return outputs, (trainable_variables, non_trainable_variables)\n \n     def make_train_function(self, force=False):\n@@ -356,6 +375,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 steps=epoch_iterator.num_batches,\n                 model=self,\n             )\n+        self._record_training_state_sharding_spec()\n \n         self.make_train_function()\n         self.stop_training = False\n@@ -365,10 +385,12 @@ class JAXTrainer(base_trainer.Trainer):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n \n-            trainable_variables = self.trainable_variables\n-            non_trainable_variables = self.non_trainable_variables\n-            optimizer_variables = self.optimizer.variables\n-            metrics_variables = self.metrics_variables\n+            trainable_variables = [v.value for v in self.trainable_variables]\n+            non_trainable_variables = [\n+                v.value for v in self.non_trainable_variables\n+            ]\n+            optimizer_variables = [v.value for v in self.optimizer.variables]\n+            metrics_variables = [v.value for v in self.metrics_variables]\n \n             for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Callbacks\n@@ -381,6 +403,7 @@ class JAXTrainer(base_trainer.Trainer):\n                     optimizer_variables,\n                     metrics_variables,\n                 )\n+                data = self._distribute_data(data)\n                 logs, state = self.train_function(state, data)\n                 (\n                     trainable_variables,\n@@ -490,7 +513,13 @@ class JAXTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n-        if not all(layer.built for layer in self._flatten_layers()):\n+        needs_building = not all(\n+            layer.built for layer in self._flatten_layers()\n+        ) or (\n+            self._compile_metrics is not None\n+            and not self._compile_metrics.built\n+        )\n+        if needs_building:\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 data_batch = data[0]\n@@ -508,15 +537,18 @@ class JAXTrainer(base_trainer.Trainer):\n                 steps=epoch_iterator.num_batches,\n                 model=self,\n             )\n+        self._record_training_state_sharding_spec()\n \n         self.make_test_function()\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n \n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n-        metrics_variables = self.metrics_variables\n+        trainable_variables = [v.value for v in self.trainable_variables]\n+        non_trainable_variables = [\n+            v.value for v in self.non_trainable_variables\n+        ]\n+        metrics_variables = [v.value for v in self.metrics_variables]\n \n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_test_batch_begin(step)\n@@ -526,6 +558,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 non_trainable_variables,\n                 metrics_variables,\n             )\n+            data = self._distribute_data(data)\n             logs, state = self.test_function(state, data)\n             # Note that trainable variables are not returned since they're\n             # immutable here.\n@@ -584,6 +617,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 steps=epoch_iterator.num_batches,\n                 model=self,\n             )\n+        self._record_training_state_sharding_spec()\n \n         self.make_predict_function()\n         callbacks.on_predict_begin()\n@@ -603,12 +637,15 @@ class JAXTrainer(base_trainer.Trainer):\n                 )\n             return outputs\n \n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n+        trainable_variables = [v.value for v in self.trainable_variables]\n+        non_trainable_variables = [\n+            v.value for v in self.non_trainable_variables\n+        ]\n         state = (trainable_variables, non_trainable_variables)\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n+            x = self._distribute_data(x)\n             batch_outputs, state = self.predict_function(state, x)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n@@ -636,16 +673,20 @@ class JAXTrainer(base_trainer.Trainer):\n                 y, class_weight\n             )\n         data = (x, y, sample_weight)\n+        data = self._distribute_data(data)\n \n         # Maybe build model\n         self._eager_build(data)\n+        self._record_training_state_sharding_spec()\n         self.make_train_function()\n \n         # Train step\n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n-        optimizer_variables = self.optimizer.variables\n-        metrics_variables = self.metrics_variables\n+        trainable_variables = [v.value for v in self.trainable_variables]\n+        non_trainable_variables = [\n+            v.value for v in self.non_trainable_variables\n+        ]\n+        optimizer_variables = [v.value for v in self.optimizer.variables]\n+        metrics_variables = [v.value for v in self.metrics_variables]\n         state = (\n             trainable_variables,\n             non_trainable_variables,\n@@ -685,14 +726,18 @@ class JAXTrainer(base_trainer.Trainer):\n         self._assert_compile_called(\"test_on_batch\")\n \n         data = (x, y, sample_weight)\n+        data = self._distribute_data(data)\n         # Maybe build model\n         self._eager_build(data)\n+        self._record_training_state_sharding_spec()\n         self.make_test_function()\n \n         # Test step\n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n-        metrics_variables = self.metrics_variables\n+        trainable_variables = [v.value for v in self.trainable_variables]\n+        non_trainable_variables = [\n+            v.value for v in self.non_trainable_variables\n+        ]\n+        metrics_variables = [v.value for v in self.metrics_variables]\n         state = (\n             trainable_variables,\n             non_trainable_variables,\n@@ -719,10 +764,13 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build model\n             with backend.StatelessScope():\n                 self(x)\n-\n+        self._record_training_state_sharding_spec()\n         self.make_predict_function()\n-        trainable_variables = self.trainable_variables\n-        non_trainable_variables = self.non_trainable_variables\n+\n+        trainable_variables = [v.value for v in self.trainable_variables]\n+        non_trainable_variables = [\n+            v.value for v in self.non_trainable_variables\n+        ]\n         state = (trainable_variables, non_trainable_variables)\n         batch_outputs, state = self.predict_function(state, [(x,)])\n         batch_outputs = tree.map_structure(lambda x: np.array(x), batch_outputs)\n@@ -764,3 +812,68 @@ class JAXTrainer(base_trainer.Trainer):\n             return jax.tree_util.tree_map(distribute_single_value, data)\n         else:\n             return data\n+\n+    def _record_training_state_sharding_spec(self):\n+        self._trainable_variable_shardings = [\n+            v.value.sharding for v in self.trainable_variables\n+        ]\n+        self._non_trainable_variable_shardings = [\n+            v.value.sharding for v in self.non_trainable_variables\n+        ]\n+        if hasattr(self, \"optimizer\"):\n+            self._optimizer_variable_shardings = [\n+                v.value.sharding for v in self.optimizer.variables\n+            ]\n+        else:\n+            self._optimizer_variable_shardings = []\n+        self._metrics_variable_shardings = [\n+            v.value.sharding for v in self.metrics_variables\n+        ]\n+\n+    def _enforce_jax_state_sharding(\n+        self,\n+        trainable_variables=None,\n+        non_trainable_variables=None,\n+        optimizer_variables=None,\n+        metrics_variables=None,\n+    ):\n+        \"\"\"Enforce the sharding spec constraint for all the training state.\n+\n+        Since the output of the train/eval step will be used as inputs to next\n+        step, we need to ensure that they have the same sharding spec, so that\n+        jax.jit won't have to recompile the train/eval function.\n+\n+        Note that this function will also rely on the recorded sharding spec\n+        for each of states.\n+\n+        This function is expected to be called within the jitted train/eval\n+        function, especially around the end of the function.\n+        \"\"\"\n+        trainable_variables = trainable_variables or []\n+        non_trainable_variables = non_trainable_variables or []\n+        optimizer_variables = optimizer_variables or []\n+        metrics_variables = metrics_variables or []\n+\n+        for i in range(len(trainable_variables)):\n+            trainable_variables[i] = jax.lax.with_sharding_constraint(\n+                trainable_variables[i], self._trainable_variable_shardings[i]\n+            )\n+        for i in range(len(non_trainable_variables)):\n+            non_trainable_variables[i] = jax.lax.with_sharding_constraint(\n+                non_trainable_variables[i],\n+                self._non_trainable_variable_shardings[i],\n+            )\n+        for i in range(len(optimizer_variables)):\n+            optimizer_variables[i] = jax.lax.with_sharding_constraint(\n+                optimizer_variables[i], self._optimizer_variable_shardings[i]\n+            )\n+        for i in range(len(metrics_variables)):\n+            metrics_variables[i] = jax.lax.with_sharding_constraint(\n+                metrics_variables[i], self._metrics_variable_shardings[i]\n+            )\n+        return (\n+            trainable_variables,\n+            non_trainable_variables,\n+            optimizer_variables,\n+            metrics_variables,\n+        )\n\n@@ -121,9 +121,9 @@ class Trainer:\n \n     @property\n     def metrics(self):\n-        metrics = [self._loss_tracker]\n+        metrics = [self._loss_tracker] if self.compiled else []\n         metrics.extend(self._metrics[:])\n-        if self._compile_metrics is not None:\n+        if self.compiled and self._compile_metrics is not None:\n             metrics += [self._compile_metrics]\n         return metrics\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f47f87a28b944a4521832e489b99218fd3d05491", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 309 | Lines Deleted: 75 | Files Changed: 4 | Hunks: 55 | Methods Changed: 21 | Complexity Δ (Sum/Max): 21/12 | Churn Δ: 384 | Churn Cumulative: 1977 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 0.6927083333333334\n\nDIFF:\n@@ -2,11 +2,12 @@ import tree\n \n from keras_core import backend\n from keras_core import layers\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.saving import saving_lib\n from keras_core.saving import serialization_lib\n+from keras_core.utils import backend_utils\n+from keras_core.utils.module_utils import tensorflow as tf\n from keras_core.utils.naming import auto_name\n \n \n@@ -387,11 +388,6 @@ class FeatureSpace(Layer):\n         num_discretization_bins=32,\n         name=None,\n     ):\n-        if backend.backend() != \"tensorflow\":\n-            raise RuntimeError(\n-                \"FeatureSpace is only available with the TensorFlow backend \"\n-                \"at this time.\"\n-            )\n         super().__init__(name=name)\n         if not features:\n             raise ValueError(\"The `features` argument cannot be None or empty.\")\n@@ -503,9 +499,7 @@ class FeatureSpace(Layer):\n         return adaptable_preprocessors\n \n     def adapt(self, dataset):\n-        from tensorflow import data as tf_data\n-\n-        if not isinstance(dataset, tf_data.Dataset):\n+        if not isinstance(dataset, tf.data.Dataset):\n             raise ValueError(\n                 \"`adapt()` can only be called on a tf.data.Dataset. \"\n                 f\"Received instead: {dataset} (of type {type(dataset)})\"\n@@ -525,15 +519,15 @@ class FeatureSpace(Layer):\n             # Sample 1 element to check the rank\n             for x in feature_dataset.take(1):\n                 pass\n-            if x.shape.rank == 0:\n+            if len(x.shape) == 0:\n                 # The dataset yields unbatched scalars; batch it.\n                 feature_dataset = feature_dataset.batch(32)\n-            if x.shape.rank in {0, 1}:\n+            if len(x.shape) in {0, 1}:\n                 # If the rank is 1, add a dimension\n                 # so we can reduce on axis=-1.\n                 # Note: if rank was previously 0, it is now 1.\n                 feature_dataset = feature_dataset.map(\n-                    lambda x: ops.expand_dims(x, -1)\n+                    lambda x: tf.expand_dims(x, -1)\n                 )\n             preprocessor.adapt(feature_dataset)\n         self._is_adapted = True\n@@ -696,6 +690,13 @@ class FeatureSpace(Layer):\n             self.get_encoded_features()\n             self._sublayers_built = True\n \n+    def _convert_input(self, x):\n+        if not isinstance(x, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)):\n+            if not isinstance(x, (list, tuple, int, float)):\n+                x = backend.convert_to_numpy(x)\n+            x = tf.convert_to_tensor(x)\n+        return x\n+\n     def __call__(self, data):\n         self._check_if_built()\n         if not isinstance(data, dict):\n@@ -704,28 +705,50 @@ class FeatureSpace(Layer):\n                 f\"Received: data={data} (of type {type(data)}\"\n             )\n \n-        data = {\n-            key: ops.convert_to_tensor(value) for key, value in data.items()\n-        }\n+        # Many preprocessing layers support all backends but many do not.\n+        # Switch to TF to make FeatureSpace work universally.\n+        data = {key: self._convert_input(value) for key, value in data.items()}\n         rebatched = False\n         for name, x in data.items():\n-            if x.shape.rank == 0:\n-                data[name] = ops.reshape(x, (1, 1))\n+            if len(x.shape) == 0:\n+                data[name] = tf.reshape(x, (1, 1))\n                 rebatched = True\n-            elif x.shape.rank == 1:\n-                data[name] = ops.expand_dims(x, -1)\n+            elif len(x.shape) == 1:\n+                data[name] = tf.expand_dims(x, -1)\n \n+        with backend_utils.TFGraphScope():\n+            # This scope is to make sure that inner TFDataLayers\n+            # will not convert outputs back to backend-native --\n+            # they should be TF tensors throughout\n             preprocessed_data = self._preprocess_features(data)\n+            preprocessed_data = tree.map_structure(\n+                lambda x: self._convert_input(x), preprocessed_data\n+            )\n+\n             crossed_data = self._cross_features(preprocessed_data)\n+            crossed_data = tree.map_structure(\n+                lambda x: self._convert_input(x), crossed_data\n+            )\n+\n             merged_data = self._merge_features(preprocessed_data, crossed_data)\n+\n         if rebatched:\n             if self.output_mode == \"concat\":\n                 assert merged_data.shape[0] == 1\n-                return ops.squeeze(merged_data, axis=0)\n+                merged_data = tf.squeeze(merged_data, axis=0)\n             else:\n                 for name, x in merged_data.items():\n-                    if x.shape.rank == 2 and x.shape[0] == 1:\n-                        merged_data[name] = ops.squeeze(x, axis=0)\n+                    if len(x.shape) == 2 and x.shape[0] == 1:\n+                        merged_data[name] = tf.squeeze(x, axis=0)\n+\n+        if (\n+            backend.backend() != \"tensorflow\"\n+            and not backend_utils.in_tf_graph()\n+        ):\n+            merged_data = tree.map_structure(\n+                lambda x: backend.convert_to_tensor(x, dtype=x.dtype),\n+                merged_data,\n+            )\n         return merged_data\n \n     def get_config(self):\n\n@@ -1,7 +1,6 @@\n import os\n \n import pytest\n-import tree\n from tensorflow import data as tf_data\n \n from keras_core import backend\n@@ -13,32 +12,195 @@ from keras_core.layers.preprocessing import feature_space\n from keras_core.saving import saving_api\n \n \n-@pytest.mark.skipif(\n-    backend.backend() != \"tensorflow\", reason=\"Only available for TF right now.\"\n-)\n class FeatureSpaceTest(testing.TestCase):\n     def _get_train_data_dict(\n-        self, as_dataset=False, as_tf_tensors=False, as_labeled_dataset=False\n+        self,\n+        as_dataset=False,\n+        as_tensors=False,\n+        as_labeled_dataset=False,\n+        include_strings=True,\n     ):\n         data = {\n             \"float_1\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n             \"float_2\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n             \"float_3\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n-            \"string_1\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n-            \"string_2\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n             \"int_1\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n             \"int_2\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n             \"int_3\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n         }\n+        if include_strings:\n+            data[\"string_1\"] = [\n+                \"0\",\n+                \"1\",\n+                \"2\",\n+                \"3\",\n+                \"4\",\n+                \"5\",\n+                \"6\",\n+                \"7\",\n+                \"8\",\n+                \"9\",\n+            ]\n+            data[\"string_2\"] = [\n+                \"0\",\n+                \"1\",\n+                \"2\",\n+                \"3\",\n+                \"4\",\n+                \"5\",\n+                \"6\",\n+                \"7\",\n+                \"8\",\n+                \"9\",\n+            ]\n+\n         if as_dataset:\n             return tf_data.Dataset.from_tensor_slices(data)\n-        elif as_tf_tensors:\n-            return tree.map_structure(ops.convert_to_tensor, data)\n+        elif as_tensors:\n+            return {\n+                key: ops.convert_to_tensor(value) for key, value in data.items()\n+            }\n         elif as_labeled_dataset:\n             labels = [0, 1, 0, 1, 0, 0, 1, 0, 1, 1]\n             return tf_data.Dataset.from_tensor_slices((data, labels))\n         return data\n \n+    def test_basic_usage_no_strings(self):\n+        fs = feature_space.FeatureSpace(\n+            features={\n+                \"float_1\": \"float\",\n+                \"float_2\": \"float_normalized\",\n+                \"float_3\": \"float_discretized\",\n+                \"int_1\": \"integer_categorical\",\n+                \"int_2\": \"integer_hashed\",\n+                \"int_3\": \"integer_categorical\",\n+            },\n+            crosses=[(\"int_1\", \"int_2\"), (\"int_2\", \"int_3\")],\n+            output_mode=\"concat\",\n+        )\n+        # Test unbatched adapt\n+        fs.adapt(\n+            self._get_train_data_dict(as_dataset=True, include_strings=False)\n+        )\n+        # Test batched adapt\n+        fs.adapt(\n+            self._get_train_data_dict(\n+                as_dataset=True, include_strings=False\n+            ).batch(4)\n+        )\n+\n+        # Test unbatched call on raw data\n+        data = {\n+            key: value[0]\n+            for key, value in self._get_train_data_dict(\n+                include_strings=False\n+            ).items()\n+        }\n+        out = fs(data)\n+        out_dim = 152\n+        self.assertEqual(out.shape, (out_dim,))\n+\n+        # Test unbatched call on backend tensors\n+        data = self._get_train_data_dict(as_tensors=True, include_strings=False)\n+        data = {key: value[0] for key, value in data.items()}\n+        out = fs(data)\n+        self.assertEqual(out.shape, (out_dim,))\n+\n+        # Test batched call on raw data\n+        out = fs(self._get_train_data_dict(include_strings=False))\n+        self.assertEqual(out.shape, (10, out_dim))\n+\n+        # Test batched call on backend tensors\n+        out = fs(\n+            self._get_train_data_dict(as_tensors=True, include_strings=False)\n+        )\n+        self.assertEqual(out.shape, (10, out_dim))\n+\n+    def test_output_mode_dict_no_strings(self):\n+        fs = feature_space.FeatureSpace(\n+            features={\n+                \"float_1\": \"float\",\n+                \"float_2\": \"float_normalized\",\n+                \"float_3\": \"float_discretized\",\n+                \"int_1\": \"integer_categorical\",\n+                \"int_2\": \"integer_hashed\",\n+                \"int_3\": \"integer_categorical\",\n+            },\n+            crosses=[(\"int_1\", \"int_2\")],\n+            output_mode=\"dict\",\n+        )\n+        fs.adapt(\n+            self._get_train_data_dict(as_dataset=True, include_strings=False)\n+        )\n+\n+        # Test unbatched call on raw data\n+        data = {\n+            key: value[0]\n+            for key, value in self._get_train_data_dict(\n+                include_strings=False\n+            ).items()\n+        }\n+        out = fs(data)\n+        self.assertIsInstance(out, dict)\n+        self.assertLen(out, 7)\n+        self.assertEqual(out[\"int_2\"].shape, (32,))\n+        self.assertEqual(out[\"int_1_X_int_2\"].shape, (32,))\n+\n+        # Test batched call on raw data\n+        out = fs(self._get_train_data_dict(include_strings=False))\n+        self.assertIsInstance(out, dict)\n+        self.assertLen(out, 7)\n+        self.assertEqual(out[\"int_2\"].shape, (10, 32))\n+\n+        # Test batched call on backend tensors\n+        out = fs(\n+            self._get_train_data_dict(as_tensors=True, include_strings=False)\n+        )\n+        self.assertIsInstance(out, dict)\n+        self.assertLen(out, 7)\n+        self.assertEqual(out[\"int_2\"].shape, (10, 32))\n+\n+    def test_output_mode_dict_of_ints_no_strings(self):\n+        cls = feature_space.FeatureSpace\n+        fs = feature_space.FeatureSpace(\n+            features={\n+                \"float_1\": \"float\",\n+                \"float_2\": \"float_normalized\",\n+                \"float_3\": \"float_discretized\",\n+                \"int_1\": cls.integer_categorical(output_mode=\"int\"),\n+                \"int_2\": cls.integer_hashed(num_bins=32, output_mode=\"int\"),\n+                \"int_3\": cls.integer_categorical(output_mode=\"int\"),\n+            },\n+            crosses=[\n+                cls.cross(\n+                    (\"int_1\", \"int_2\"), output_mode=\"int\", crossing_dim=32\n+                ),\n+            ],\n+            output_mode=\"dict\",\n+        )\n+        fs.adapt(\n+            self._get_train_data_dict(as_dataset=True, include_strings=False)\n+        )\n+        data = {\n+            key: value[0]\n+            for key, value in self._get_train_data_dict(\n+                include_strings=False\n+            ).items()\n+        }\n+        out = fs(data)\n+        self.assertIsInstance(out, dict)\n+        self.assertLen(out, 7)\n+        self.assertEqual(out[\"int_2\"].shape, (1,))\n+        self.assertTrue(\n+            backend.standardize_dtype(out[\"int_2\"].dtype).startswith(\"int\")\n+        )\n+        self.assertEqual(out[\"int_1_X_int_2\"].shape, (1,))\n+        self.assertTrue(\n+            backend.standardize_dtype(out[\"int_1_X_int_2\"].dtype).startswith(\n+                \"int\"\n+            )\n+        )\n+\n     def test_basic_usage(self):\n         fs = feature_space.FeatureSpace(\n             features={\n@@ -51,7 +213,7 @@ class FeatureSpaceTest(testing.TestCase):\n                 \"int_2\": \"integer_hashed\",\n                 \"int_3\": \"integer_categorical\",\n             },\n-            # crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n+            crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n             output_mode=\"concat\",\n         )\n         # Test unbatched adapt\n@@ -64,22 +226,24 @@ class FeatureSpaceTest(testing.TestCase):\n             key: value[0] for key, value in self._get_train_data_dict().items()\n         }\n         out = fs(data)\n-        out_dim = 131\n-        self.assertEqual(out.shape, [out_dim])\n+        out_dim = 195\n+        self.assertEqual(out.shape, (out_dim,))\n \n-        # Test unbatched call on TF tensors\n-        data = self._get_train_data_dict(as_tf_tensors=True)\n+        # Test unbatched call on tensors\n+        if backend.backend() == \"tensorflow\":\n+            data = self._get_train_data_dict(as_tensors=True)\n             data = {key: value[0] for key, value in data.items()}\n             out = fs(data)\n-        self.assertEqual(out.shape, [out_dim])\n+            self.assertEqual(out.shape, (out_dim,))\n \n         # Test batched call on raw data\n         out = fs(self._get_train_data_dict())\n-        self.assertEqual(out.shape, [10, out_dim])\n+        self.assertEqual(out.shape, (10, out_dim))\n \n-        # Test batched call on TF tensors\n-        out = fs(self._get_train_data_dict(as_tf_tensors=True))\n-        self.assertEqual(out.shape, [10, out_dim])\n+        # Test batched call on tensors\n+        if backend.backend() == \"tensorflow\":\n+            out = fs(self._get_train_data_dict(as_tensors=True))\n+            self.assertEqual(out.shape, (10, out_dim))\n \n     def test_output_mode_dict(self):\n         fs = feature_space.FeatureSpace(\n@@ -105,25 +269,26 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertIsInstance(out, dict)\n         self.assertLen(out, 10)\n-        self.assertEqual(out[\"string_1\"].shape, [11])\n-        self.assertEqual(out[\"int_2\"].shape, [32])\n-        self.assertEqual(out[\"string_2_X_int_2\"].shape, [32])\n+        self.assertEqual(out[\"string_1\"].shape, (11,))\n+        self.assertEqual(out[\"int_2\"].shape, (32,))\n+        self.assertEqual(out[\"string_2_X_int_2\"].shape, (32,))\n \n         # Test batched call on raw data\n         out = fs(self._get_train_data_dict())\n         self.assertIsInstance(out, dict)\n         self.assertLen(out, 10)\n-        self.assertEqual(out[\"string_1\"].shape, [10, 11])\n-        self.assertEqual(out[\"int_2\"].shape, [10, 32])\n-        self.assertEqual(out[\"string_2_X_int_2\"].shape, [10, 32])\n+        self.assertEqual(out[\"string_1\"].shape, (10, 11))\n+        self.assertEqual(out[\"int_2\"].shape, (10, 32))\n+        self.assertEqual(out[\"string_2_X_int_2\"].shape, (10, 32))\n \n-        # Test batched call on TF tensors\n-        out = fs(self._get_train_data_dict(as_tf_tensors=True))\n+        # Test batched call on tensors\n+        if backend.backend == \"tensorflow\":\n+            out = fs(self._get_train_data_dict(as_tensors=True))\n             self.assertIsInstance(out, dict)\n             self.assertLen(out, 10)\n-        self.assertEqual(out[\"string_1\"].shape, [10, 11])\n-        self.assertEqual(out[\"int_2\"].shape, [10, 32])\n-        self.assertEqual(out[\"string_2_X_int_2\"].shape, [10, 32])\n+            self.assertEqual(out[\"string_1\"].shape, (10, 11))\n+            self.assertEqual(out[\"int_2\"].shape, (10, 32))\n+            self.assertEqual(out[\"string_2_X_int_2\"].shape, (10, 32))\n \n     def test_output_mode_dict_of_ints(self):\n         cls = feature_space.FeatureSpace\n@@ -155,13 +320,24 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertIsInstance(out, dict)\n         self.assertLen(out, 10)\n-        self.assertEqual(out[\"string_1\"].shape, [1])\n-        self.assertEqual(out[\"string_1\"].dtype.name, \"int64\")\n-        self.assertEqual(out[\"int_2\"].shape, [1])\n-        self.assertEqual(out[\"int_2\"].dtype.name, \"int64\")\n-        self.assertEqual(out[\"string_2_X_int_2\"].shape, [1])\n-        self.assertEqual(out[\"string_2_X_int_2\"].dtype.name, \"int64\")\n+        self.assertEqual(out[\"string_1\"].shape, (1,))\n+        self.assertTrue(\n+            backend.standardize_dtype(out[\"string_1\"].dtype).startswith(\"int\")\n+        )\n+        self.assertEqual(out[\"int_2\"].shape, (1,))\n+        self.assertTrue(\n+            backend.standardize_dtype(out[\"int_2\"].dtype).startswith(\"int\")\n+        )\n+        self.assertEqual(out[\"string_2_X_int_2\"].shape, (1,))\n+        self.assertTrue(\n+            backend.standardize_dtype(out[\"string_2_X_int_2\"].dtype).startswith(\n+                \"int\"\n+            )\n+        )\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"Requires string dtype.\"\n+    )\n     def test_functional_api_sync_processing(self):\n         fs = feature_space.FeatureSpace(\n             features={\n@@ -189,36 +365,43 @@ class FeatureSpaceTest(testing.TestCase):\n         ds = self._get_train_data_dict(as_dataset=True)\n         model.predict(ds.batch(4))\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"TODO: debug it\"\n+    )\n     def test_tf_data_async_processing(self):\n         fs = feature_space.FeatureSpace(\n             features={\n                 \"float_1\": \"float\",\n                 \"float_2\": \"float_normalized\",\n                 \"float_3\": \"float_discretized\",\n-                \"string_1\": \"string_categorical\",\n-                \"string_2\": \"string_hashed\",\n                 \"int_1\": \"integer_categorical\",\n                 \"int_2\": \"integer_hashed\",\n                 \"int_3\": \"integer_categorical\",\n             },\n-            crosses=[(\"float_3\", \"string_1\"), (\"string_2\", \"int_2\")],\n+            crosses=[(\"float_3\", \"int_1\"), (\"int_1\", \"int_2\")],\n             output_mode=\"concat\",\n         )\n-        fs.adapt(self._get_train_data_dict(as_dataset=True))\n+        fs.adapt(\n+            self._get_train_data_dict(as_dataset=True, include_strings=False)\n+        )\n         features = fs.get_encoded_features()\n         outputs = layers.Dense(1)(features)\n         model = models.Model(inputs=features, outputs=outputs)\n         model.compile(\"adam\", \"mse\")\n-        ds = self._get_train_data_dict(as_labeled_dataset=True)\n+        ds = self._get_train_data_dict(\n+            as_labeled_dataset=True, include_strings=False\n+        )\n         # Try map before batch\n         ds = ds.map(lambda x, y: (fs(x), y))\n         model.fit(ds.batch(4))\n         # Try map after batch\n-        ds = self._get_train_data_dict(as_labeled_dataset=True)\n+        ds = self._get_train_data_dict(\n+            as_labeled_dataset=True, include_strings=False\n+        )\n         ds = ds.batch(4)\n         ds = ds.map(lambda x, y: (fs(x), y))\n         model.evaluate(ds)\n-        ds = self._get_train_data_dict(as_dataset=True)\n+        ds = self._get_train_data_dict(as_dataset=True, include_strings=False)\n         ds = ds.map(fs)\n         model.predict(ds.batch(4))\n \n@@ -248,8 +431,11 @@ class FeatureSpaceTest(testing.TestCase):\n             key: value[0] for key, value in self._get_train_data_dict().items()\n         }\n         out = fs(data)\n-        self.assertEqual(out.shape, [148])\n+        self.assertEqual(out.shape, (148,))\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\", reason=\"TODO: debug it\"\n+    )\n     def test_manual_kpl(self):\n         data = {\n             \"text\": [\"1st string\", \"2nd string\", \"3rd string\"],\n@@ -281,7 +467,7 @@ class FeatureSpaceTest(testing.TestCase):\n             output_mode=\"concat\",\n         )\n         out = fs(data)\n-        self.assertEqual(out.shape, [10, 32])\n+        self.assertEqual(tuple(out.shape), (10, 32))\n \n     def test_saving(self):\n         cls = feature_space.FeatureSpace\n@@ -290,8 +476,6 @@ class FeatureSpaceTest(testing.TestCase):\n                 \"float_1\": cls.float(),\n                 \"float_2\": cls.float_normalized(),\n                 \"float_3\": cls.float_discretized(num_bins=3),\n-                \"string_1\": cls.string_categorical(max_tokens=5),\n-                \"string_2\": cls.string_hashed(num_bins=32),\n                 \"int_1\": cls.integer_categorical(\n                     max_tokens=5, num_oov_indices=2\n                 ),\n@@ -299,14 +483,19 @@ class FeatureSpaceTest(testing.TestCase):\n                 \"int_3\": cls.integer_categorical(max_tokens=5),\n             },\n             crosses=[\n-                cls.cross((\"float_3\", \"string_1\"), crossing_dim=32),\n-                cls.cross((\"string_2\", \"int_2\"), crossing_dim=32),\n+                cls.cross((\"float_3\", \"int_1\"), crossing_dim=32),\n+                cls.cross((\"int_1\", \"int_2\"), crossing_dim=32),\n             ],\n             output_mode=\"concat\",\n         )\n-        fs.adapt(self._get_train_data_dict(as_dataset=True))\n+        fs.adapt(\n+            self._get_train_data_dict(as_dataset=True, include_strings=False)\n+        )\n         data = {\n-            key: value[0] for key, value in self._get_train_data_dict().items()\n+            key: value[0]\n+            for key, value in self._get_train_data_dict(\n+                include_strings=False\n+            ).items()\n         }\n         ref_out = fs(data)\n \n@@ -325,7 +514,7 @@ class FeatureSpaceTest(testing.TestCase):\n         inputs = fs.get_inputs()\n         outputs = fs.get_encoded_features()\n         model = models.Model(inputs=inputs, outputs=outputs)\n-        ds = self._get_train_data_dict(as_dataset=True)\n+        ds = self._get_train_data_dict(as_dataset=True, include_strings=False)\n         out = model.predict(ds.batch(4))\n         self.assertAllClose(out[0], ref_out)\n \n\n@@ -242,7 +242,10 @@ class Hashing(Layer):\n             hash_bins -= 1\n             mask = tf.equal(values, self.mask_value)\n         # Convert all values to strings before hashing.\n-        if values.dtype.is_integer:\n+        # Floats are first normalized to int64.\n+        if values.dtype.is_floating:\n+            values = tf.cast(values, dtype=\"int64\")\n+        if values.dtype != tf.string:\n             values = tf.as_string(values)\n         # Hash the strings.\n         if self.strong_hash:\n\n@@ -1,9 +1,13 @@\n import sys\n \n from keras_core import backend as backend_module\n+from keras_core.backend.common import global_state\n \n \n def in_tf_graph():\n+    if global_state.get_global_attribute(\"in_tf_graph_scope\", False):\n+        return True\n+\n     if \"tensorflow\" in sys.modules:\n         from keras_core.utils.module_utils import tensorflow as tf\n \n@@ -11,6 +15,21 @@ def in_tf_graph():\n     return False\n \n \n+class TFGraphScope:\n+    def __init__(self):\n+        self._original_value = global_state.get_global_attribute(\n+            \"in_tf_graph_scope\", False\n+        )\n+\n+    def __enter__(self):\n+        global_state.set_global_attribute(\"in_tf_graph_scope\", True)\n+\n+    def __exit__(self, *args, **kwargs):\n+        global_state.set_global_attribute(\n+            \"in_tf_graph_scope\", self._original_value\n+        )\n+\n+\n class DynamicBackend:\n     \"\"\"A class that can be used to switch from one backend to another.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd504d4e154f5f11a26dbf47983f73d602f05631", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 731 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -13,6 +13,7 @@ import warnings\n \n import numpy as np\n \n+from keras_core.api_export import keras_core_export\n from keras_core.backend import distribution_lib\n from keras_core.backend.common import global_state\n \n@@ -20,10 +21,11 @@ DEFAULT_BATCH_DIM_NAME = \"batch\"\n GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n \n \n+@keras_core_export(\"keras_core.distribution.list_devices\")\n def list_devices(device_type=None):\n     \"\"\"Return all the available devices based on the device type.\n \n-    Note in a distributed setting, global devices are returned.\n+    Note: in a distributed setting, global devices are returned.\n \n     Args:\n         device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n@@ -37,6 +39,7 @@ def list_devices(device_type=None):\n     return distribution_lib.list_devices(device_type)\n \n \n+@keras_core_export(\"keras_core.distribution.DeviceMesh\")\n class DeviceMesh:\n     \"\"\"A cluster of computation devices for distributed computation.\n \n@@ -58,7 +61,7 @@ class DeviceMesh:\n             match/create the `TensorLayout` when distribute the data and\n             variables.\n         devices: Optional list of devices. Default to all the available\n-            devices locally from `list_devices()`.\n+            devices locally from `keras_core.distribution.list_devices()`.\n     \"\"\"\n \n     def __init__(\n@@ -105,8 +108,9 @@ class DeviceMesh:\n         return self._devices\n \n \n+@keras_core_export(\"keras_core.distribution.TensorLayout\")\n class TensorLayout:\n-    \"\"\"The layout of a tensor.\n+    \"\"\"A layout to apply to a tensor.\n \n     This API is aligned with `jax.sharding.NamedSharding`\n     and `tf.dtensor.Layout`.\n@@ -118,7 +122,7 @@ class TensorLayout:\n \n     Args:\n         axes: list of strings that should map to the `axis_names` in\n-            `DeviceMesh`. For any dimentions that doesn't need any sharding,\n+            a `DeviceMesh`. For any dimentions that doesn't need any sharding,\n             A `None` can be used a placeholder.\n         device_mesh: Optional `DeviceMesh` that will be used to create\n             the layout. The actual mapping of tensor to physical device\n@@ -160,12 +164,12 @@ class TensorLayout:\n \n \n class Distribution:\n-    \"\"\"Base class for the distribution.\n+    \"\"\"Base class for variable distribution strategies.\n \n-    The `Distribution` has following key functionalities.\n+    A `Distribution` has following key functionalities:\n \n-    1. Distribute the model variables to the `DeviceMesh`.\n-    2. Distribute the input data to the `DeviceMesh`.\n+    1. Distribute the model variables to a `DeviceMesh`.\n+    2. Distribute the input data to a `DeviceMesh`.\n \n     It can create a context scope so that the framework to properly detect the\n     `Distribution` and distribute the variable/data accordingly.\n@@ -216,6 +220,7 @@ class Distribution:\n         return self._device_mesh\n \n \n+@keras_core_export(\"keras_core.distribution.DataParallel\")\n class DataParallel(Distribution):\n     \"\"\"Distribution for data parallelism.\n \n@@ -292,6 +297,7 @@ class DataParallel(Distribution):\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n+@keras_core_export(\"keras_core.distribution.ModelParallel\")\n class ModelParallel(Distribution):\n     \"\"\"Distribution that shards model variables.\n \n@@ -388,6 +394,7 @@ class ModelParallel(Distribution):\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n+@keras_core_export(\"keras_core.distribution.LayoutMap\")\n class LayoutMap(collections.abc.MutableMapping):\n     \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n \n@@ -484,11 +491,13 @@ class LayoutMap(collections.abc.MutableMapping):\n             layout.device_mesh = self.device_mesh\n \n \n+@keras_core_export(\"keras_core.distribution.distribution\")\n def distribution():\n     \"\"\"Retrieve the current distribution from global context.\"\"\"\n     return global_state.get_global_attribute(GLOBAL_ATTRIBUTE_NAME)\n \n \n+@keras_core_export(\"keras_core.distribution.set_distribution\")\n def set_distribution(value):\n     \"\"\"Set the distribution as the global distribution setting.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#251a055ad4e0e9db63133ee267b26309e1d06f76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 689 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -469,6 +469,9 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertEqual(tuple(out.shape), (10, 32))\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\", reason=\"TODO: debug it\"\n+    )\n     def test_saving(self):\n         cls = feature_space.FeatureSpace\n         fs = feature_space.FeatureSpace(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b144f297f107241cb0295914d1cb1ae5ed6f58cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 693 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -469,9 +469,7 @@ class FeatureSpaceTest(testing.TestCase):\n         out = fs(data)\n         self.assertEqual(tuple(out.shape), (10, 32))\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"numpy\", reason=\"TODO: debug it\"\n-    )\n+    @pytest.mark.skipif(backend.backend() == \"numpy\", reason=\"TODO: debug it\")\n     def test_saving(self):\n         cls = feature_space.FeatureSpace\n         fs = feature_space.FeatureSpace(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f22642937f19ffd3af0aced68a4028e9f4ff96ef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 18 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +1,2 @@\n # Unique source of truth for the version number.\n-__version__ = \"0.1.5\"\n+__version__ = \"0.1.6\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01c9f38515738cded1fd9204210c81f6c514e2a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 86 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 86 | Churn Cumulative: 1881 | Contributors (this commit): 10 | Commits (past 90d): 26 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -34,6 +34,92 @@ class Trainer:\n         steps_per_execution=1,\n         jit_compile=\"auto\",\n     ):\n+        \"\"\"Configures the model for training.\n+\n+        Example:\n+\n+        ```python\n+        model.compile(\n+            optimizer=keras_core.optimizers.Adam(learning_rate=1e-3),\n+            loss=keras_core.losses.BinaryCrossentropy(),\n+            metrics=[\n+                keras_core.metrics.BinaryAccuracy(),\n+                keras_core.metrics.FalseNegatives(),\n+            ],\n+        )\n+        ```\n+\n+        Args:\n+            optimizer: String (name of optimizer) or optimizer instance. See\n+                `keras_core.optimizers`.\n+            loss: Loss function. May be a string (name of loss function), or\n+                a `keras_core.losses.Loss` instance. See `keras_core.losses`. A\n+                loss function is any callable with the signature\n+                `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth\n+                values, and `y_pred` are the model's predictions.\n+                `y_true` should have shape `(batch_size, d0, .. dN)`\n+                (except in the case of sparse loss functions such as\n+                sparse categorical crossentropy which expects integer arrays of\n+                shape `(batch_size, d0, .. dN-1)`).\n+                `y_pred` should have shape `(batch_size, d0, .. dN)`.\n+                The loss function should return a float tensor.\n+            loss_weights: Optional list or dictionary specifying scalar\n+                coefficients (Python floats) to weight the loss contributions of\n+                different model outputs. The loss value that will be minimized\n+                by the model will then be the *weighted sum* of all individual\n+                losses, weighted by the `loss_weights` coefficients.  If a list,\n+                it is expected to have a 1:1 mapping to the model's outputs. If\n+                a dict, it is expected to map output names (strings) to scalar\n+                coefficients.\n+            metrics: List of metrics to be evaluated by the model during\n+                training and testing. Each of this can be a string (name of a\n+                built-in function), function or a `keras_core.metrics.Metric`\n+                instance. See `keras_core.metrics`. Typically you will use\n+                `metrics=['accuracy']`. A function is any callable with the\n+                signature `result = fn(y_true, _pred)`. To specify different\n+                metrics for different outputs of a multi-output model, you could\n+                also pass a dictionary, such as\n+                `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`.\n+                You can also pass a list to specify a metric or a list of\n+                metrics for each output, such as\n+                `metrics=[['accuracy'], ['accuracy', 'mse']]`\n+                or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass\n+                the strings 'accuracy' or 'acc', we convert this to one of\n+                `keras_core.metrics.BinaryAccuracy`,\n+                `keras_core.metrics.CategoricalAccuracy`,\n+                `keras_core.metrics.SparseCategoricalAccuracy` based on the\n+                shapes of the targets and of the model output. We do a similar\n+                conversion for the strings 'crossentropy' and 'ce' as well.\n+                The metrics passed here are evaluated without sample weighting;\n+                if you would like sample weighting to apply, you can specify\n+                your metrics via the `weighted_metrics` argument instead.\n+            weighted_metrics: List of metrics to be evaluated and weighted by\n+                `sample_weight` or `class_weight` during training and testing.\n+            run_eagerly: Bool. If `True`, this `Model`'s logic will never be\n+                compiled (e.g. with `tf.function` or `jax.jit`). Recommended to\n+                leave this as `False` when training for best performance, and\n+                `True` when debugging.\n+            steps_per_execution: Int. The number of batches to run\n+                during each a single compiled function call. Running multiple\n+                batches inside a single a single compiled function call can\n+                greatly improve performance on TPUs or small models with a large\n+                Python overhead. At most, one full epoch will be run each\n+                execution. If a number larger than the size of the epoch is\n+                passed, the execution will be truncated to the size of the\n+                epoch. Note that if `steps_per_execution` is set to `N`,\n+                `Callback.on_batch_begin` and `Callback.on_batch_end` methods\n+                will only be called every `N` batches (i.e. before/after\n+                each compiled function execution).\n+            jit_compile: Bool or `\"auto\"`. Whether to use XLA compilation when\n+                compiling a model. This value should currently never be `True`\n+                on the torch backed, and should always be `True` or `\"auto\"` on\n+                the jax backend. On tensorflow, this value can be `True` or\n+                `False`, and will toggle the `jit_compile` option for any\n+                `tf.function` owned by the model. See\n+                https://www.tensorflow.org/xla/tutorials/jit_compile for more\n+                details. If `\"auto\"`, XLA compilation will be enabled if the\n+                backend supports it, and disabled otherwise.\n+        \"\"\"\n         self.optimizer = optimizers.get(optimizer)\n         if hasattr(self, \"output_names\"):\n             output_names = self.output_names\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
