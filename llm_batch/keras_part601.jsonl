{"custom_id": "keras#1f3b54101b82782396bc930b9d41145ab77c21ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 14970 | Contributors (this commit): 40 | Commits (past 90d): 13 | Contributors (cumulative): 40 | DMM Complexity: None\n\nDIFF:\n@@ -1560,17 +1560,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n     @property\n     def metrics(self):\n-        \"\"\"List of metrics added using the `add_metric()` API.\n-\n-        Example:\n-\n-        >>> input = tf.keras.layers.Input(shape=(3,))\n-        >>> d = tf.keras.layers.Dense(2)\n-        >>> output = d(input)\n-        >>> d.add_metric(tf.reduce_max(output), name='max')\n-        >>> d.add_metric(tf.reduce_min(output), name='min')\n-        >>> [m.name for m in d.metrics]\n-        ['max', 'min']\n+        \"\"\"List of metrics attached to the layer.\n \n         Returns:\n             A list of `Metric` objects.\n@@ -1583,6 +1573,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                 collected_metrics.extend(layer._metrics)\n         return collected_metrics\n \n+    @doc_controls.do_not_generate_docs\n     def add_metric(self, value, name=None, **kwargs):\n         \"\"\"Adds metric tensor to the layer.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8c2982b523401586574833faf46455bc8bb5b6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 246 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -163,8 +163,6 @@ class AdamW(optimizer.Optimizer):\n \n     def update_step(self, gradient, variable):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        beta_1_power = None\n-        beta_2_power = None\n         lr = tf.cast(self.learning_rate, variable.dtype)\n         local_step = tf.cast(self.iterations + 1, variable.dtype)\n         beta_1_power = tf.pow(tf.cast(self.beta_1, variable.dtype), local_step)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0775e6bc2de09e6ba1c52e7dfe3232685cc24651", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 259 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -54,16 +54,19 @@ class AdamW(optimizer.Optimizer):\n             learning rate. Defaults to 0.001.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n+            exponential decay rate for the 1st moment estimates.\n+            Defaults to 0.9.\n         beta_2: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n+            exponential decay rate for the 2nd moment estimates.\n+            Defaults to 0.999.\n         epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        1e-7.\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n+            Defaults to 1e-7.\n         amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n-        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n+            the paper \"On the Convergence of Adam and beyond\".\n+            Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#799997544f84069d4abe96584b459bed482d3b03", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 9 | Churn Cumulative: 198 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -48,14 +48,15 @@ class Adadelta(optimizer.Optimizer):\n \n     Args:\n         learning_rate: Initial value for the learning rate: either a floating\n-        point value, or a `tf.keras.optimizers.schedules.LearningRateSchedule`\n-        instance. Defaults to 0.001. Note that `Adadelta` tends to benefit from\n+            point value, or a\n+            `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n+            Defaults to 0.001. Note that `Adadelta` tends to benefit from\n             higher initial learning rate values compared to other optimizers. To\n             match the exact form in the original paper, use 1.0.\n         rho: A `Tensor` or a floating point value. The decay rate. Defaults to\n             0.95.\n-      epsilon: Small floating point value used to maintain numerical stability.\n-        Defaults to 1e-7.\n+        epsilon: Small floating point value used to maintain numerical\n+            stability. Defaults to 1e-7.\n       {{base_optimizer_keyword_args}}\n \n     Reference:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a99442bf346829bf70c42bb4981c4907e8a53a2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 8 | Churn Cumulative: 247 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -47,13 +47,13 @@ class Adafactor(optimizer.Optimizer):\n             or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n             Defaults to 0.001.\n         beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n-      epsilon_1: float, defaults to 1e-30. A small offset to keep demoninator\n+        epsilon_1: float, defaults to 1e-30. A small offset to keep denominator\n             away from 0.\n         epsilon_2: float, defaults to 1e-3. A small offset to avoid learning\n             rate becoming too small by time.\n-      clip_threshold: float, defaults to 1.0. Clipping threshold. This is a part\n-        of Adafactor algorithm, independent from `clipnorm`, `clipvalue` and\n-        `global_clipnorm`.\n+        clip_threshold: float, defaults to 1.0. Clipping threshold. This is a\n+            part of Adafactor algorithm, independent from `clipnorm`,\n+            `clipvalue` and `global_clipnorm`.\n         relative_step: bool, defaults to True. If `learning_rate` is a\n             constant and `relative_step=True`, learning rate will be adjusted\n             based on current iterations. This is a default learning rate decay\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#070dd8834306324aa472b29406d2ef21aa8cb118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 10 | Churn Cumulative: 167 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -43,14 +43,14 @@ class Adagrad(optimizer.Optimizer):\n         learning_rate: Initial value for the learning rate:\n             either a floating point value,\n             or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n-        Defaults to 0.001.\n-        Note that `Adagrad` tends to benefit from higher initial learning rate\n-        values compared to other optimizers.\n-        To match the exact form in the original paper, use 1.0.\n+            Defaults to 0.001. Note that `Adagrad` tends to benefit from higher\n+            initial learning rate values compared to other optimizers. To match\n+            the exact form in the original paper, use 1.0.\n         initial_accumulator_value: Floating point value.\n             Starting value for the accumulators (per-parameter momentum values).\n             Must be non-negative.\n-      epsilon: Small floating point value used to maintain numerical stability.\n+        epsilon: Small floating point value used to maintain numerical\n+        stability.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#49e56890042bdfa1dfdb4db4dddae77cc7114abb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 2 | Churn Cumulative: 242 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -160,8 +160,6 @@ class Adam(optimizer.Optimizer):\n \n     def update_step(self, gradient, variable):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        beta_1_power = None\n-        beta_2_power = None\n         lr = tf.cast(self.learning_rate, variable.dtype)\n         local_step = tf.cast(self.iterations + 1, variable.dtype)\n         beta_1_power = tf.pow(tf.cast(self.beta_1, variable.dtype), local_step)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b0a4da93bb985c66e9afc758a6478773961d4a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 258 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -50,17 +50,19 @@ class Adam(optimizer.Optimizer):\n             learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n+            exponential decay rate for the 1st moment estimates.\n+            Defaults to `0.9`.\n         beta_2: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 2nd moment estimates. Defaults to\n-        `0.999`.\n+            exponential decay rate for the 2nd moment estimates.\n+            Defaults to `0.999`.\n         epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        `1e-7`.\n-      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n-        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n+            Defaults to `1e-7`.\n+        amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm\n+            from the paper \"On the Convergence of Adam and beyond\".\n+            Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53a87b975a325bc220a1cc51a36deb99b5379de6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 27 | Churn Cumulative: 292 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -75,24 +75,25 @@ class Ftrl(optimizer.Optimizer):\n \n     Args:\n         learning_rate: A `Tensor`, floating point value, a schedule that is a\n-        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that\n-        takes no arguments and returns the actual value to use. The learning\n-        rate.  Defaults to `0.001`.\n+            `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+             that takes no arguments and returns the actual value to use. The\n+             learning rate.  Defaults to `0.001`.\n         learning_rate_power: A float value, must be less or equal to zero.\n-        Controls how the learning rate decreases during training. Use zero for a\n-        fixed learning rate.\n-      initial_accumulator_value: The starting value for accumulators. Only zero\n-        or positive values are allowed.\n+            Controls how the learning rate decreases during training. Use zero\n+            for a fixed learning rate.\n+        initial_accumulator_value: The starting value for accumulators. Only\n+            zero or positive values are allowed.\n         l1_regularization_strength: A float value, must be greater than or equal\n             to zero. Defaults to `0.0`.\n         l2_regularization_strength: A float value, must be greater than or equal\n             to zero. Defaults to `0.0`.\n-      l2_shrinkage_regularization_strength: A float value, must be greater than\n-        or equal to zero. This differs from L2 above in that the L2 above is a\n-        stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n-        When input is sparse shrinkage will only happen on the active weights.\n-      beta: A float value, representing the beta value from the paper. Defaults\n-        to 0.0.\n+        l2_shrinkage_regularization_strength: A float value, must be greater\n+        than or equal to zero. This differs from L2 above in that the L2 above\n+        is a stabilization penalty, whereas this L2 shrinkage is a magnitude\n+        penalty. When input is sparse shrinkage will only happen on the active\n+        weights.\n+        beta: A float value, representing the beta value from the paper.\n+        Defaults to 0.0.\n         {{base_optimizer_keyword_args}}\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3811ee2e279072e82f235af6533deefc7acf008f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 4 | Churn Cumulative: 450 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -45,8 +45,8 @@ class Lion(optimizer.Optimizer):\n             that takes no arguments and returns the actual value to use. The\n             learning rate. Defaults to 0.0001.\n         beta_1: A float value or a constant float tensor, or a callable\n-        that takes no arguments and returns the actual value to use. The rate\n-        to combine the current gradient and the 1st moment estimate.\n+            that takes no arguments and returns the actual value to use. The\n+            rate to combine the current gradient and the 1st moment estimate.\n         beta_2: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimate.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#745237407054e1c19018dad266da9f09d88d91dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 7 | Churn Cumulative: 223 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -40,15 +40,16 @@ class Nadam(optimizer.Optimizer):\n             learning rate. Defaults to `0.001`.\n         beta_1: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n+            exponential decay rate for the 1st moment estimates.\n+            Defaults to `0.9`.\n         beta_2: A float value or a constant float tensor, or a callable\n             that takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 2nd moment estimates. Defaults to\n             `0.999`.\n         epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        `1e-7`.\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n+            Defaults to `1e-7`.\n         {{base_optimizer_keyword_args}}\n \n     Reference:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11d682ed6ef048b782df900198e630895dc5bc04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 236 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -53,8 +53,8 @@ class RMSprop(optimizer.Optimizer):\n             momentum value, with a decay rate equals to `1 - momentum`.\n         epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        1e-7.\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n+            Defaults to `1e-7`.\n         centered: Boolean. If `True`, gradients are normalized by the estimated\n             variance of the gradient; if False, by the uncentered second moment.\n             Setting this to `True` may help with training, but is slightly more\n@@ -71,8 +71,7 @@ class RMSprop(optimizer.Optimizer):\n     9.683772\n \n     Reference:\n-      - [Hinton, 2012](\n-        http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n+        - [Hinton, 2012](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) # noqa: E501\n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#69070b4b56748331665fcb25658298c437fdbcfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 263 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -64,8 +64,8 @@ class AdamW(optimizer.Optimizer):\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n             Section 2.1), not the epsilon in Algorithm 1 of the paper.\n             Defaults to 1e-7.\n-        amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n-            the paper \"On the Convergence of Adam and beyond\".\n+        amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm\n+            from the paper \"On the Convergence of Adam and beyond\".\n             Defaults to `False`.\n         {{base_optimizer_keyword_args}}\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9ddd2522d21c75eb360496cf68735dd8a2c9889", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 191 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from keras_core.backend.common.stateless_scope import StatelessScope\n from keras_core.backend.common.stateless_scope import get_stateless_scope\n from keras_core.backend.common.stateless_scope import in_stateless_scope\n \n-DYNAMIC_SHAPES_OK = True  # Dynamic shapes NG\n+DYNAMIC_SHAPES_OK = False  # Dynamic shapes NG\n \n \n class Variable(KerasVariable):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2f197f8ac2cb7ed05b41d8be4b32bf32cbcb275d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 926 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 13 | Complexity Δ (Sum/Max): 56/50 | Churn Δ: 926 | Churn Cumulative: 939 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 0.7811501597444089\n\nDIFF:\n@@ -5,6 +5,7 @@ from keras_core.callbacks.early_stopping import EarlyStopping\n from keras_core.callbacks.history import History\n from keras_core.callbacks.lambda_callback import LambdaCallback\n from keras_core.callbacks.learning_rate_scheduler import LearningRateScheduler\n+from keras_core.callbacks.model_checkpoint import ModelCheckpoint\n from keras_core.callbacks.progbar_logger import ProgbarLogger\n from keras_core.callbacks.reduce_lr_on_plateau import ReduceLROnPlateau\n from keras_core.callbacks.remote_monitor import RemoteMonitor\n\n@@ -0,0 +1,389 @@\n+import os\n+import re\n+import warnings\n+\n+import numpy as np\n+from tensorflow.io import gfile\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.callbacks.callback import Callback\n+from keras_core.utils import file_utils\n+from keras_core.utils import io_utils\n+\n+\n+@keras_core_export(\"keras_core.callbacks.ModelCheckpoint\")\n+class ModelCheckpoint(Callback):\n+    \"\"\"Callback to save the Keras model or model weights at some frequency.\n+\n+    `ModelCheckpoint` callback is used in conjunction with training using\n+    `model.fit()` to save a model or weights (in a checkpoint file) at some\n+    interval, so the model or weights can be loaded later to continue the\n+    training from the state saved.\n+\n+    A few options this callback provides include:\n+\n+    - Whether to only keep the model that has achieved the \"best performance\" so\n+      far, or whether to save the model at the end of every epoch regardless of\n+      performance.\n+    - Definition of \"best\"; which quantity to monitor and whether it should be\n+      maximized or minimized.\n+    - The frequency it should save at. Currently, the callback supports saving\n+      at the end of every epoch, or after a fixed number of training batches.\n+    - Whether only weights are saved, or the whole model is saved.\n+\n+    Example:\n+\n+    ```python\n+    model.compile(loss=..., optimizer=...,\n+                  metrics=['accuracy'])\n+\n+    EPOCHS = 10\n+    checkpoint_filepath = '/tmp/ckpt/checkpoint.model.keras'\n+    model_checkpoint_callback = keras_core.callbacks.ModelCheckpoint(\n+        filepath=checkpoint_filepath,\n+        monitor='val_accuracy',\n+        mode='max',\n+        save_best_only=True)\n+\n+    # Model is saved at the end of every epoch, if it's the best seen so far.\n+    model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n+\n+    # The model (that are considered the best) can be loaded as -\n+    keras_core.models.load_model(checkpoint_filepath)\n+\n+    # Alternatively, one could checkpoint just the model weights as -\n+    checkpoint_filepath = '/tmp/ckpt/checkpoint.weights.h5'\n+    model_checkpoint_callback = keras_core.callbacks.ModelCheckpoint(\n+        filepath=checkpoint_filepath,\n+        save_weights_only=True,\n+        monitor='val_accuracy',\n+        mode='max',\n+        save_best_only=True)\n+\n+    # Model weights are saved at the end of every epoch, if it's the best seen\n+    # so far.\n+    model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n+\n+    # The model weights (that are considered the best) can be loaded as -\n+    model.load_weights(checkpoint_filepath)\n+    ```\n+\n+    Args:\n+        filepath: string or `PathLike`, path to save the model file. e.g.\n+            filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n+            can contain named formatting options, which will be filled the\n+            value of `epoch` and keys in `logs` (passed in `on_epoch_end`).\n+            The `filepath` name needs to end with `\".weights.h5\"` when\n+            `save_weights_only=True` or should end with `\".keras\"` when\n+            checkpoint saving the whole model (default)\".\n+            For example:\n+            if `filepath` is `{epoch:02d}-{val_loss:.2f}.keras`, then the\n+            model checkpoints will be saved with the epoch number and the\n+            validation loss in the filename. The directory of the filepath\n+            should not be reused by any other callbacks to avoid conflicts.\n+        monitor: The metric name to monitor. Typically the metrics are set by\n+            the `Model.compile` method. Note:\n+            * Prefix the name with `\"val_\"` to monitor validation metrics.\n+            * Use `\"loss\"` or `\"val_loss\"` to monitor the model's total loss.\n+            * If you specify metrics as strings, like `\"accuracy\"`, pass the\n+                same string (with or without the `\"val_\"` prefix).\n+            * If you pass `metrics.Metric` objects, `monitor` should be set to\n+                `metric.name`\n+            * If you're not sure about the metric names you can check the\n+                contents of the `history.history` dictionary returned by\n+                `history = model.fit()`\n+            * Multi-output models set additional prefixes on the metric names.\n+        verbose: Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n+            displays messages when the callback takes an action.\n+        save_best_only: if `save_best_only=True`, it only saves when the model\n+            is considered the \"best\" and the latest best model according to the\n+            quantity monitored will not be overwritten. If `filepath` doesn't\n+            contain formatting options like `{epoch}` then `filepath` will be\n+            overwritten by each new better model.\n+        mode: one of {`\"auto\"`, `\"min\"`, `\"max\"`}. If `save_best_only=True`, the\n+            decision to overwrite the current save file is made based on either\n+            the maximization or the minimization of the monitored quantity.\n+            For `val_acc`, this should be `\"max\"`, for `val_loss` this should be\n+            `\"min\"`, etc. In `\"auto\"` mode, the mode is set to `\"max\"` if the\n+            quantities monitored are `\"acc\"` or start with `\"fmeasure\"` and are\n+            set to `\"min\"` for the rest of the quantities.\n+        save_weights_only: if True, then only the model's weights will be saved\n+            (`model.save_weights(filepath)`), else the full model is saved\n+            (`model.save(filepath)`).\n+        save_freq: `\"epoch\"` or integer. When using `\"epoch\"`, the callback\n+            saves the model after each epoch. When using integer, the callback\n+            saves the model at end of this many batches. If the `Model` is\n+            compiled with `steps_per_execution=N`, then the saving criteria will\n+            be checked every Nth batch. Note that if the saving isn't aligned to\n+            epochs, the monitored metric may potentially be less reliable (it\n+            could reflect as little as 1 batch, since the metrics get reset\n+            every epoch). Defaults to `\"epoch\"`.\n+        initial_value_threshold: Floating point initial \"best\" value of the\n+            metric to be monitored. Only applies if `save_best_value=True`. Only\n+            overwrites the model weights already saved if the performance of\n+            current model is better than this value.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        filepath,\n+        monitor=\"val_loss\",\n+        verbose=0,\n+        save_best_only=False,\n+        save_weights_only=False,\n+        mode=\"auto\",\n+        save_freq=\"epoch\",\n+        initial_value_threshold=None,\n+    ):\n+        super().__init__()\n+        self.monitor = monitor\n+        self.verbose = verbose\n+        self.filepath = file_utils.path_to_string(filepath)\n+        self.save_best_only = save_best_only\n+        self.save_weights_only = save_weights_only\n+        self.save_freq = save_freq\n+        self._batches_seen_since_last_saving = 0\n+        self._last_batch_seen = 0\n+        self.best = initial_value_threshold\n+\n+        if mode not in [\"auto\", \"min\", \"max\"]:\n+            warnings.warn(\n+                f\"ModelCheckpoint mode '{mode}' is unknown, \"\n+                \"fallback to auto mode.\",\n+                stacklevel=2,\n+            )\n+            mode = \"auto\"\n+\n+        if mode == \"min\":\n+            self.monitor_op = np.less\n+            if self.best is None:\n+                self.best = np.Inf\n+        elif mode == \"max\":\n+            self.monitor_op = np.greater\n+            if self.best is None:\n+                self.best = -np.Inf\n+        else:\n+            if \"acc\" in self.monitor or self.monitor.startswith(\"fmeasure\"):\n+                self.monitor_op = np.greater\n+                if self.best is None:\n+                    self.best = -np.Inf\n+            else:\n+                self.monitor_op = np.less\n+                if self.best is None:\n+                    self.best = np.Inf\n+\n+        if self.save_freq != \"epoch\" and not isinstance(self.save_freq, int):\n+            raise ValueError(\n+                f\"Unrecognized save_freq: {self.save_freq}. \"\n+                \"Expected save_freq are 'epoch' or integer values\"\n+            )\n+\n+    def on_train_batch_end(self, batch, logs=None):\n+        if self._should_save_on_batch(batch):\n+            self._save_model(epoch=self._current_epoch, batch=batch, logs=logs)\n+\n+    def on_epoch_begin(self, epoch, logs=None):\n+        self._current_epoch = epoch\n+\n+    def on_epoch_end(self, epoch, logs=None):\n+        if self.save_freq == \"epoch\":\n+            self._save_model(epoch=epoch, batch=None, logs=logs)\n+\n+    def _should_save_on_batch(self, batch):\n+        \"\"\"Handles batch-level saving logic, supports steps_per_execution.\"\"\"\n+        if self.save_freq == \"epoch\":\n+            return False\n+        if batch <= self._last_batch_seen:  # New epoch.\n+            add_batches = batch + 1  # batches are zero-indexed.\n+        else:\n+            add_batches = batch - self._last_batch_seen\n+        self._batches_seen_since_last_saving += add_batches\n+        self._last_batch_seen = batch\n+\n+        if self._batches_seen_since_last_saving >= self.save_freq:\n+            self._batches_seen_since_last_saving = 0\n+            return True\n+        return False\n+\n+    def _save_model(self, epoch, batch, logs):\n+        \"\"\"Saves the model.\n+\n+        Args:\n+            epoch: the epoch this iteration is in.\n+            batch: the batch this iteration is in. `None` if the `save_freq`\n+                is set to `\"epoch\"`.\n+            logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n+        \"\"\"\n+        logs = logs or {}\n+\n+        filepath = self._get_file_path(epoch, batch, logs)\n+        # Create host directory if it doesn't exist.\n+        dirname = os.path.dirname(filepath)\n+        if dirname and not gfile.exists(dirname):\n+            gfile.makedirs(dirname)\n+\n+        try:\n+            if self.save_best_only:\n+                current = logs.get(self.monitor)\n+                if current is None:\n+                    warnings.warn(\n+                        f\"Can save best model only with {self.monitor} \"\n+                        \"available, skipping.\",\n+                        stacklevel=2,\n+                    )\n+                else:\n+                    if self.monitor_op(current, self.best):\n+                        if self.verbose > 0:\n+                            io_utils.print_msg(\n+                                f\"\\nEpoch {epoch + 1}: {self.monitor} \"\n+                                \"improved \"\n+                                f\"from {self.best:.5f} to {current:.5f}, \"\n+                                f\"saving model to {filepath}\"\n+                            )\n+                        self.best = current\n+                        if self.save_weights_only:\n+                            self.model.save_weights(filepath, overwrite=True)\n+                        else:\n+                            self.model.save(filepath, overwrite=True)\n+                    else:\n+                        if self.verbose > 0:\n+                            io_utils.print_msg(\n+                                f\"\\nEpoch {epoch + 1}: \"\n+                                f\"{self.monitor} did not improve \"\n+                                f\"from {self.best:.5f}\"\n+                            )\n+            else:\n+                if self.verbose > 0:\n+                    io_utils.print_msg(\n+                        f\"\\nEpoch {epoch + 1}: saving model to {filepath}\"\n+                    )\n+                if self.save_weights_only:\n+                    self.model.save_weights(filepath, overwrite=True)\n+                else:\n+                    self.model.save(filepath, overwrite=True)\n+        except IsADirectoryError:  # h5py 3.x\n+            raise IOError(\n+                \"Please specify a non-directory filepath for \"\n+                \"ModelCheckpoint. Filepath used is an existing \"\n+                f\"directory: {filepath}\"\n+            )\n+        except IOError as e:  # h5py 2.x\n+            # `e.errno` appears to be `None` so checking the content of\n+            # `e.args[0]`.\n+            if \"is a directory\" in str(e.args[0]).lower():\n+                raise IOError(\n+                    \"Please specify a non-directory filepath for \"\n+                    \"ModelCheckpoint. Filepath used is an existing \"\n+                    f\"directory: f{filepath}\"\n+                )\n+            # Re-throw the error for any other causes.\n+            raise e\n+\n+    def _get_file_path(self, epoch, batch, logs):\n+        \"\"\"Returns the file path for checkpoint.\"\"\"\n+\n+        try:\n+            # `filepath` may contain placeholders such as\n+            # `{epoch:02d}`,`{batch:02d}` and `{mape:.2f}`. A mismatch between\n+            # logged metrics and the path's placeholders can cause formatting to\n+            # fail.\n+            if batch is None or \"batch\" in logs:\n+                file_path = self.filepath.format(epoch=epoch + 1, **logs)\n+            else:\n+                file_path = self.filepath.format(\n+                    epoch=epoch + 1, batch=batch + 1, **logs\n+                )\n+        except KeyError as e:\n+            raise KeyError(\n+                f'Failed to format this callback filepath: \"{self.filepath}\". '\n+                f\"Reason: {e}\"\n+            )\n+        return file_path\n+\n+    def _checkpoint_exists(self, filepath):\n+        \"\"\"Returns whether the checkpoint `filepath` refers to exists.\"\"\"\n+        return gfile.exists(filepath)\n+\n+    def _get_most_recently_modified_file_matching_pattern(self, pattern):\n+        \"\"\"Returns the most recently modified filepath matching pattern.\n+\n+        In the rare case where there are more than one pattern-matching file\n+        having the same modified time that is most recent among all, return the\n+        filepath that is largest (by `>` operator, lexicographically using the\n+        numeric equivalents). This provides a tie-breaker when multiple files\n+        are most recent. Note that a larger `filepath` can sometimes indicate a\n+        later time of modification (for instance, when epoch/batch is used as\n+        formatting option), but not necessarily (when accuracy or loss is used).\n+        The tie-breaker is put in the logic as best effort to return the most\n+        recent, and to avoid undeterministic result.\n+\n+        Modified time of a file is obtained with `os.path.getmtime()`.\n+\n+        This utility function is best demonstrated via an example:\n+\n+        ```python\n+        file_pattern = 'f.batch{batch:02d}epoch{epoch:02d}.keras'\n+        test_dir = self.get_temp_dir()\n+        path_pattern = os.path.join(test_dir, file_pattern)\n+        file_paths = [\n+            os.path.join(test_dir, file_name) for file_name in\n+            ['f.batch03epoch02.keras',\n+             'f.batch02epoch02.keras', 'f.batch01epoch01.keras']\n+        ]\n+        for file_path in file_paths:\n+            # Write something to each of the files\n+        self.assertEqual(\n+            _get_most_recently_modified_file_matching_pattern(path_pattern),\n+            file_paths[-1])\n+        ```\n+\n+        Args:\n+            pattern: The file pattern that may optionally contain python\n+                placeholder such as `{epoch:02d}`.\n+\n+        Returns:\n+            The most recently modified file's full filepath matching `pattern`.\n+            If `pattern` does not contain any placeholder, this returns the\n+            filepath that exactly matches `pattern`. Returns `None` if no match\n+            is found.\n+        \"\"\"\n+        dir_name = os.path.dirname(pattern)\n+        base_name = os.path.basename(pattern)\n+        base_name_regex = \"^\" + re.sub(r\"{.*}\", r\".*\", base_name) + \"$\"\n+\n+        latest_mod_time = 0\n+        file_path_with_latest_mod_time = None\n+        n_file_with_latest_mod_time = 0\n+        file_path_with_largest_file_name = None\n+\n+        if gfile.exists(dir_name):\n+            for file_name in os.listdir(dir_name):\n+                # Only consider if `file_name` matches the pattern.\n+                if re.match(base_name_regex, file_name):\n+                    file_path = os.path.join(dir_name, file_name)\n+                    mod_time = os.path.getmtime(file_path)\n+                    if (\n+                        file_path_with_largest_file_name is None\n+                        or file_path > file_path_with_largest_file_name\n+                    ):\n+                        file_path_with_largest_file_name = file_path\n+                    if mod_time > latest_mod_time:\n+                        latest_mod_time = mod_time\n+                        file_path_with_latest_mod_time = file_path\n+                        # In the case a file with later modified time is found,\n+                        # reset the counter for the number of files with latest\n+                        # modified time.\n+                        n_file_with_latest_mod_time = 1\n+                    elif mod_time == latest_mod_time:\n+                        # In the case a file has modified time tied with the\n+                        # most recent, increment the counter for the number of\n+                        # files with latest modified time by 1.\n+                        n_file_with_latest_mod_time += 1\n+\n+        if n_file_with_latest_mod_time == 1:\n+            # Return the sole file that has most recent modified time.\n+            return file_path_with_latest_mod_time\n+        else:\n+            # If there are more than one file having latest modified time,\n+            # return the file path with the largest file name.\n+            return file_path_with_largest_file_name\n\n@@ -0,0 +1,536 @@\n+import os\n+import warnings\n+\n+import pytest\n+\n+from keras_core import callbacks\n+from keras_core import layers\n+from keras_core import metrics\n+from keras_core import models\n+from keras_core import saving\n+from keras_core import testing\n+from keras_core.models import Sequential\n+from keras_core.testing import test_utils\n+from keras_core.utils import numerical_utils\n+\n+try:\n+    import h5py\n+except ImportError:\n+    h5py = None\n+\n+TRAIN_SAMPLES = 10\n+TEST_SAMPLES = 10\n+NUM_CLASSES = 2\n+INPUT_DIM = 3\n+NUM_HIDDEN = 5\n+BATCH_SIZE = 5\n+\n+\n+class ModelCheckpointTest(testing.TestCase):\n+    @pytest.mark.skipif(\n+        h5py is None,\n+        reason=\"`h5py` is a required dependency for `ModelCheckpoint` tests.\",\n+    )\n+    def test_model_checkpoint_options(self):\n+        def get_model():\n+            model = Sequential(\n+                [\n+                    layers.Dense(NUM_HIDDEN, activation=\"relu\"),\n+                    layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n+                ]\n+            )\n+            model.compile(\n+                loss=\"categorical_crossentropy\",\n+                optimizer=\"sgd\",\n+                metrics=[metrics.Accuracy(\"acc\")],\n+            )\n+            return model\n+\n+        model = get_model()\n+        temp_dir = self.get_temp_dir()\n+\n+        # Save model to a subdir inside the temp_dir so we can test\n+        # automatic directory creation.\n+        filepath = os.path.join(temp_dir, \"subdir\", \"checkpoint.keras\")\n+        (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n+            train_samples=TRAIN_SAMPLES,\n+            test_samples=TEST_SAMPLES,\n+            input_shape=(INPUT_DIM,),\n+            num_classes=NUM_CLASSES,\n+        )\n+        y_test = numerical_utils.to_categorical(y_test)\n+        y_train = numerical_utils.to_categorical(y_train)\n+\n+        # Case 1\n+        monitor = \"val_loss\"\n+        save_best_only = False\n+        mode = \"auto\"\n+\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertTrue(os.path.exists(filepath))\n+        os.remove(filepath)\n+\n+        # Case 2\n+        mode = \"min\"\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertTrue(os.path.exists(filepath))\n+        os.remove(filepath)\n+\n+        # Case 3\n+        mode = \"max\"\n+        monitor = \"val_acc\"\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertTrue(os.path.exists(filepath))\n+        os.remove(filepath)\n+\n+        # Case 4\n+        save_best_only = True\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertTrue(os.path.exists(filepath))\n+        os.remove(filepath)\n+\n+        # Case 5: metric not available.\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath, monitor=\"unknown\", save_best_only=True\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        # File won't be written.\n+        self.assertFalse(os.path.exists(filepath))\n+\n+        # Case 6\n+        with warnings.catch_warnings(record=True) as warning_logs:\n+            warnings.simplefilter(\"always\")\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=\"unknown\",\n+            )\n+            self.assertIn(\n+                \"ModelCheckpoint mode 'unknown' is unknown\",\n+                str(warning_logs[-1].message),\n+            )\n+\n+        # Case 8a: `ModelCheckpoint` with an integer `save_freq`\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"checkpoint.epoch{epoch:02d}.keras\")\n+        save_best_only = False\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+                save_freq=15,\n+            )\n+        ]\n+        self.assertFalse(os.path.exists(filepath.format(epoch=3)))\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=2,  # 5 batches / epoch, so should backup every 3 epochs\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=10,\n+            verbose=0,\n+        )\n+        self.assertFalse(os.path.exists(filepath.format(epoch=1)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=3)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=4)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=5)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=6)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=7)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=8)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=9)))\n+        os.remove(filepath.format(epoch=3))\n+        os.remove(filepath.format(epoch=6))\n+        os.remove(filepath.format(epoch=9))\n+\n+        # Case 8b: `ModelCheckpoint` with int `save_freq` & `save_weights_only`\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(\n+            temp_dir, \"checkpoint.epoch{epoch:02d}.weights.h5\"\n+        )\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath, monitor=monitor, save_freq=15, save_weights_only=True\n+            )\n+        ]\n+        self.assertFalse(os.path.exists(filepath.format(epoch=3)))\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=2,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=10,\n+            verbose=0,\n+        )\n+        self.assertFalse(os.path.exists(filepath.format(epoch=1)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=3)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=4)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=5)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=6)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=7)))\n+        self.assertFalse(os.path.exists(filepath.format(epoch=8)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=9)))\n+\n+        # Case 9: `ModelCheckpoint` with valid and invalid save_freq argument.\n+        with self.assertRaisesRegex(ValueError, \"Unrecognized save_freq\"):\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+                save_freq=\"invalid_save_freq\",\n+            )\n+        # The following should not raise ValueError.\n+        callbacks.ModelCheckpoint(\n+            filepath,\n+            monitor=monitor,\n+            save_best_only=save_best_only,\n+            mode=mode,\n+            save_freq=\"epoch\",\n+        )\n+        callbacks.ModelCheckpoint(\n+            filepath,\n+            monitor=monitor,\n+            save_best_only=save_best_only,\n+            mode=mode,\n+            save_freq=3,\n+        )\n+\n+        # Case 10a: `ModelCheckpoint` save with batch in filename.\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(\n+            temp_dir, \"checkpoint.epoch{epoch:02d}batch{batch:02d}.keras\"\n+        )\n+        cbks = [\n+            callbacks.ModelCheckpoint(filepath, monitor=monitor, save_freq=1)\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=5,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=5,\n+            verbose=1,\n+        )\n+        self.assertTrue(os.path.exists(filepath.format(epoch=1, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=1, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=2, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=2, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=3, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=3, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=4, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=4, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=5, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=5, batch=2)))\n+\n+        # Case 10b: `ModelCheckpoint` save weights with batch in filename.\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(\n+            temp_dir, \"checkpoint.epoch{epoch:02d}batch{batch:02d}.weights.h5\"\n+        )\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath, monitor=monitor, save_freq=1, save_weights_only=True\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=5,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=5,\n+            verbose=1,\n+        )\n+\n+        self.assertTrue(os.path.exists(filepath.format(epoch=1, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=1, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=2, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=2, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=3, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=3, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=4, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=4, batch=2)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=5, batch=1)))\n+        self.assertTrue(os.path.exists(filepath.format(epoch=5, batch=2)))\n+\n+        # Case 11: ModelCheckpoint saves model with initial_value_threshold\n+        # param\n+        mode = \"max\"\n+        monitor = \"val_acc\"\n+        initial_value_threshold = -0.01\n+        save_best_only = True\n+        filepath = os.path.join(temp_dir, \"checkpoint.keras\")\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                initial_value_threshold=initial_value_threshold,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertTrue(os.path.exists(filepath))\n+        os.remove(filepath)\n+\n+        # Case 12: ModelCheckpoint saves model with initial_value_threshold\n+        # param\n+        mode = \"auto\"\n+        monitor = \"val_loss\"\n+        initial_value_threshold = None\n+        save_best_only = True\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                initial_value_threshold=initial_value_threshold,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertTrue(os.path.exists(filepath))\n+        os.remove(filepath)\n+\n+        # Case 13: ModelCheckpoint doesnt save model if loss was minimum earlier\n+        mode = \"min\"\n+        monitor = \"val_loss\"\n+        initial_value_threshold = 0\n+        save_best_only = True\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                initial_value_threshold=initial_value_threshold,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertFalse(os.path.exists(filepath))\n+\n+        # Case 14: ModelCheckpoint doesnt save model if loss was min earlier in\n+        # auto mode\n+        mode = \"auto\"\n+        monitor = \"val_loss\"\n+        initial_value_threshold = 0\n+        save_best_only = True\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                initial_value_threshold=initial_value_threshold,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        self.assertFalse(os.path.exists(filepath))\n+\n+    @pytest.mark.skipif(\n+        h5py is None,\n+        reason=\"`h5py` is a required dependency for `ModelCheckpoint` tests.\",\n+    )\n+    def test_model_checkpoint_loading(self):\n+        def get_model():\n+            inputs = layers.Input(shape=(INPUT_DIM,), batch_size=2)\n+            x = layers.Dense(NUM_HIDDEN, activation=\"relu\")(inputs)\n+            outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n+            functional_model = models.Model(inputs, outputs)\n+            functional_model.compile(\n+                loss=\"categorical_crossentropy\",\n+                optimizer=\"sgd\",\n+                metrics=[metrics.Accuracy(\"acc\")],\n+            )\n+            return functional_model\n+\n+        (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n+            train_samples=TRAIN_SAMPLES,\n+            test_samples=TEST_SAMPLES,\n+            input_shape=(INPUT_DIM,),\n+            num_classes=NUM_CLASSES,\n+        )\n+        y_test = numerical_utils.to_categorical(y_test)\n+        y_train = numerical_utils.to_categorical(y_train)\n+\n+        # Model Checkpoint load model (default)\n+        model = get_model()\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"checkpoint.model.keras\")\n+        mode = \"auto\"\n+        monitor = \"val_loss\"\n+        save_best_only = True\n+\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        ref_weights = model.get_weights()\n+        self.assertTrue(os.path.exists(filepath))\n+        new_model = saving.load_model(filepath)\n+        new_weights = new_model.get_weights()\n+        self.assertEqual(len(ref_weights), len(new_weights))\n+        for ref_w, w in zip(ref_weights, new_weights):\n+            self.assertAllClose(ref_w, w)\n+\n+        # Model Checkpoint load model weights\n+        model = get_model()\n+        temp_dir = self.get_temp_dir()\n+        filepath = os.path.join(temp_dir, \"checkpoint.weights.h5\")\n+        mode = \"auto\"\n+        monitor = \"val_loss\"\n+        save_best_only = True\n+\n+        cbks = [\n+            callbacks.ModelCheckpoint(\n+                filepath,\n+                monitor=monitor,\n+                save_best_only=save_best_only,\n+                save_weights_only=True,\n+                mode=mode,\n+            )\n+        ]\n+        model.fit(\n+            x_train,\n+            y_train,\n+            batch_size=BATCH_SIZE,\n+            validation_data=(x_test, y_test),\n+            callbacks=cbks,\n+            epochs=1,\n+            verbose=0,\n+        )\n+        ref_weights = model.get_weights()\n+        self.assertTrue(os.path.exists(filepath))\n+        new_model = get_model()\n+        new_model.load_weights(filepath)\n+        new_weights = new_model.get_weights()\n+        self.assertEqual(len(ref_weights), len(new_weights))\n+        for ref_w, w in zip(ref_weights, new_weights):\n+            self.assertAllClose(ref_w, w)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
