{"custom_id": "keras#fe00f5ff6436a94a1212676916a9edd824d44338", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 23 | Files Changed: 5 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 29 | Churn Cumulative: 3268 | Contributors (this commit): 10 | Commits (past 90d): 22 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -83,7 +83,6 @@ class UnitNorm(Constraint):\n                 \"axis\": self.axis}\n \n \n-identity = Constraint\n maxnorm = MaxNorm\n nonneg = NonNeg\n unitnorm = UnitNorm\n\n@@ -1787,7 +1787,9 @@ class Container(Layer):\n     def uses_learning_phase(self):\n         '''True if any layer in the graph uses it.\n         '''\n-        return any([layer.uses_learning_phase for layer in self.layers])\n+        layers_learning_phase = any([layer.uses_learning_phase for layer in self.layers])\n+        regs_learning_phase = any([reg.uses_learning_phase for reg in self.regularizers])\n+        return layers_learning_phase or regs_learning_phase\n \n     def call(self, input, mask=None):\n         '''`call` just reapplies all ops in the graph to the new inputs\n\n@@ -20,6 +20,7 @@ class WeightRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n         self.l2 = K.cast_to_floatx(l2)\n+        self.uses_learning_phase = True\n \n     def set_param(self, p):\n         self.p = p\n@@ -48,10 +49,10 @@ class ActivityRegularizer(Regularizer):\n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n         self.l2 = K.cast_to_floatx(l2)\n+        self.uses_learning_phase = True\n \n     def set_layer(self, layer):\n         self.layer = layer\n-        layer.uses_learning_phase = True\n \n     def __call__(self, loss):\n         if not hasattr(self, 'layer'):\n@@ -92,7 +93,6 @@ def activity_l2(l=0.01):\n def activity_l1l2(l1=0.01, l2=0.01):\n     return ActivityRegularizer(l1=l1, l2=l2)\n \n-identity = Regularizer\n \n from .utils.generic_utils import get_from_module\n def get(identifier, kwargs=None):\n\n@@ -34,23 +34,6 @@ def test_nonneg():\n     assert(np.all(np.min(K.eval(normed), axis=1) == 0.))\n \n \n-def test_identity():\n-    identity_instance = constraints.identity()\n-    normed = identity_instance(example_array)\n-    assert(np.all(normed == example_array))\n-\n-\n-def test_identity_oddballs():\n-    \"\"\"\n-    test the identity constraint on some more exotic input.\n-    this does not need to pass for the desired real life behaviour,\n-    but it should in the current implementation.\n-    \"\"\"\n-    identity_instance = constraints.identity()\n-    oddball_examples = [\"Hello\", [1], -1, None]\n-    assert(oddball_examples == identity_instance(oddball_examples))\n-\n-\n def test_unitnorm():\n     unitnorm_instance = constraints.unitnorm()\n     normalized = unitnorm_instance(K.variable(example_array))\n\n@@ -48,8 +48,7 @@ def create_model(weight_reg=None, activity_reg=None):\n \n \n def test_W_reg():\n-    for reg in [regularizers.identity(),\n-                regularizers.l1(),\n+    for reg in [regularizers.l1(),\n                 regularizers.l2(),\n                 regularizers.l1l2()]:\n         model = create_model(weight_reg=reg)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9621bb5b8e645048ce6acd8548393cd95997f813", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 2447 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2183,7 +2183,9 @@ class Container(Layer):\n             flattened_layers = self.layers\n \n         f = h5py.File(filepath, 'w')\n-        f.attrs['layer_names'] = [layer.name for layer in flattened_layers]\n+        for layer in flattened_layers:\n+            print(layer.name)\n+        f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in flattened_layers]\n \n         for layer in flattened_layers:\n             g = f.create_group(layer.name)\n@@ -2195,7 +2197,7 @@ class Container(Layer):\n                     name = str(w.name)\n                 else:\n                     name = 'param_' + str(i)\n-                weight_names.append(name)\n+                weight_names.append(name.encode('utf8'))\n             g.attrs['weight_names'] = weight_names\n             for name, val in zip(weight_names, weight_values):\n                 param_dset = g.create_dataset(name, val.shape,\n@@ -2231,7 +2233,7 @@ class Container(Layer):\n                 flattened_layers[k].set_weights(weights)\n         else:\n             # new file format\n-            layer_names = f.attrs['layer_names']\n+            layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n             if len(layer_names) != len(flattened_layers):\n                 raise Exception('You are trying to load a weight file '\n                                 'containing ' + str(len(layer_names)) +\n@@ -2240,7 +2242,7 @@ class Container(Layer):\n \n             for k, name in enumerate(layer_names):\n                 g = f[name]\n-                weight_names = g.attrs['weight_names']\n+                weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n                 if len(weight_names):\n                     weights = [g[weight_name] for weight_name in weight_names]\n                     flattened_layers[k].set_weights(weights)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73817b8b77fa925ffa77d98449de17cf27f69d84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 89 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,9 +2,9 @@ import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n-from keras.layers import wrappers\n+from keras.layers import wrappers, Input\n from keras.layers import core, convolutional\n-from keras.models import Sequential, model_from_json\n+from keras.models import Sequential, Model, model_from_json\n \n \n def test_TimeDistributed():\n@@ -59,6 +59,21 @@ def test_TimeDistributed():\n \n     model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n \n+    # test wrapping Sequential model\n+    model = Sequential()\n+    model.add(core.Dense(3, input_dim=2))\n+    outer_model = Sequential()\n+    outer_model.add(wrappers.TimeDistributed(model, input_shape=(3, 2)))\n+    outer_model.compile(optimizer='rmsprop', loss='mse')\n+    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n+\n+    # test with functional API\n+    x = Input(shape=(3, 2))\n+    y = wrappers.TimeDistributed(model)(x)\n+    outer_model = Model(x, y)\n+    outer_model.compile(optimizer='rmsprop', loss='mse')\n+    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9c6f264127bfac5bb9d499b2e7f1308bf0cee3b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 7442 | Contributors (this commit): 43 | Commits (past 90d): 38 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -214,13 +214,13 @@ class Sequential(Model):\n     def _gather_list_attr(self, attr):\n         all_attrs = []\n         for layer in self.flattened_layers:\n-            all_attrs += getattr(layer, attr)\n+            all_attrs += getattr(layer, attr, [])\n         return all_attrs\n \n     def _gather_dict_attr(self, attr):\n         all_attrs = {}\n         for layer in self.flattened_layers:\n-            layer_dict = getattr(layer, attr)\n+            layer_dict = getattr(layer, attr, {})\n             all_attrs = dict(list(all_attrs.items()) +\n                              list(layer_dict.items()))\n         return all_attrs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d09e2a67bb9ce53f9318930296a9825dd82f7197", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 562 | Contributors (this commit): 11 | Commits (past 90d): 14 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ class TestBackend(object):\n         check_two_tensor_operation('dot', (4, 2), (5, 2, 3))\n \n         check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n-                                   axes=((2,), (2,)))\n+                                   axes=(2, 2))\n         check_single_tensor_operation('transpose', (4, 2))\n \n     def test_shape_operations(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3860e078a5854282aac5e7329f6a8dd6b20b49f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1064 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ class Optimizer(object):\n \n class SGD(Optimizer):\n     '''Stochastic gradient descent, with support for momentum,\n-    decay, and Nesterov momentum.\n+    learning rate decay, and Nesterov momentum.\n \n     # Arguments\n         lr: float >= 0. Learning rate.\n@@ -143,7 +143,8 @@ class RMSprop(Optimizer):\n     '''RMSProp optimizer.\n \n     It is recommended to leave the parameters of this optimizer\n-    at their default values.\n+    at their default values\n+    (except the learning rate, which can be freely tuned).\n \n     This optimizer is usually a good choice for recurrent\n     neural networks.\n@@ -230,7 +231,8 @@ class Adadelta(Optimizer):\n     at their default values.\n \n     # Arguments\n-        lr: float >= 0. Learning rate. It is recommended to leave it at the default value.\n+        lr: float >= 0. Learning rate.\n+            It is recommended to leave it at the default value.\n         rho: float >= 0.\n         epsilon: float >= 0. Fuzz factor.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#43187187692bdfed0bb5c27c730aa8506e83dcab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 37 | Files Changed: 4 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 18/12 | Churn Δ: 118 | Churn Cumulative: 5683 | Contributors (this commit): 14 | Commits (past 90d): 27 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -154,12 +154,23 @@ class Node(object):\n             output_masks = to_list(outbound_layer.compute_mask(input_tensors, input_masks))\n             output_shapes = to_list(outbound_layer.get_output_shape_for(input_shapes))\n \n-        assert output_tensors and output_tensors[0], ('The `call` method of layer \"' +\n-            outbound_layer.name + '\" should return a tensor. Found: ' + str(output_tensors[0]))\n-        assert len(output_tensors) == len(output_shapes), ('The `get_output_shape_for` method of layer \"' + outbound_layer.name +\n-            '\"\" should return one shape tuple per output tensor of the layer. Found: ' + str(output_shapes))\n-        assert len(output_tensors) == len(output_masks), ('The `compute_mask` method of layer \"' + outbound_layer.name +\n-            '\" should return one mask tensor per output tensor of the layer. Found: ' + str(output_masks))\n+        if not output_tensors or output_tensors[0] is None:\n+            raise Exception('The `call` method of layer \"' +\n+                            outbound_layer.name +\n+                            '\" should return a tensor. Found: ' +\n+                            str(output_tensors[0]))\n+        if len(output_tensors) != len(output_shapes):\n+            raise Exception('The `get_output_shape_for` method of layer \"' +\n+                            outbound_layer.name +\n+                            '\"\" should return one shape tuple per '\n+                            'output tensor of the layer. Found: ' +\n+                            str(output_shapes))\n+        if len(output_tensors) != len(output_masks):\n+            raise Exception('The `compute_mask` method of layer \"' +\n+                            outbound_layer.name +\n+                            '\" should return one mask tensor per '\n+                            'output tensor of the layer. Found: ' +\n+                            str(output_masks))\n \n         for i in range(len(output_tensors)):\n             output_tensors[i]._keras_shape = output_shapes[i]\n@@ -356,18 +367,26 @@ class Layer(object):\n                 if type(spec.ndim) is str:\n                     int_ndim = spec.ndim[:spec.ndim.find('+')]\n                     ndim = int(int_ndim)\n-                    assert K.ndim(x) >= ndim, ('Input ' + str(input_index) +\n-                            ' is incompatible with layer ' + self.name +\n-                            ': expected ndim >= ' + str(ndim), ' found ndim=' + str(K.ndim(x)))\n+                    if K.ndim(x) < ndim:\n+                        raise Exception('Input ' + str(input_index) +\n+                                        ' is incompatible with layer ' +\n+                                        self.name + ': expected ndim >= ' +\n+                                        str(ndim) + ' found ndim=' +\n+                                        str(K.ndim(x)))\n                 else:\n-                    assert K.ndim(x) == spec.ndim, ('Input ' + str(input_index) +\n-                            ' is incompatible with layer ' + self.name +\n-                            ': expected ndim=' + str(spec.ndim), ' found ndim=' + str(K.ndim(x)))\n+                    if K.ndim(x) != spec.ndim:\n+                        raise Exception('Input ' + str(input_index) +\n+                                        ' is incompatible with layer ' +\n+                                        self.name + ': expected ndim=' +\n+                                        str(spec.ndim), ' found ndim=' +\n+                                        str(K.ndim(x)))\n             if spec.dtype is not None:\n-                assert K.dtype(x) == spec.dtype, ('Input ' + str(input_index) +\n-                        ' is incompatible with layer ' + self.name +\n-                        ': expected dtype=' + str(spec.dtype), ' found dtype=' + str(K.dtype(x)))\n-\n+                if K.dtype(x) != spec.dtype:\n+                    raise Exception('Input ' + str(input_index) +\n+                                    ' is incompatible with layer ' +\n+                                    self.name + ': expected dtype=' +\n+                                    str(spec.dtype), ' found dtype=' +\n+                                    str(K.dtype(x)))\n             if spec.shape is not None:\n                 if hasattr(x, '_keras_shape'):\n                     x_shape = x._keras_shape\n@@ -378,9 +397,12 @@ class Layer(object):\n                     continue\n                 for spec_dim, dim in zip(spec.shape, x_shape):\n                     if spec_dim is not None:\n-                        assert spec_dim == dim, ('Input ' + str(input_index) +\n-                                ' is incompatible with layer ' + self.name +\n-                                ': expected shape=' + str(spec.shape) + ', found shape=' + str(x_shape))\n+                        if spec_dim != dim:\n+                            raise Exception('Input ' + str(input_index) +\n+                                            ' is incompatible with layer ' +\n+                                            self.name + ': expected shape=' +\n+                                            str(spec.shape) + ', found shape=' +\n+                                            str(x_shape))\n \n     def call(self, x, mask=None):\n         '''This is where the layer's logic lives.\n@@ -716,8 +738,12 @@ class Layer(object):\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n         '''\n-        assert len(self.inbound_nodes) == 1, ('Layer ' + self.name + ' has multiple inbound nodes, ' +\n-            'hence the notion of \"layer output\" is ill-defined. Use `get_output_at(node_index)` instead.')\n+        if len(self.inbound_nodes) != 1:\n+            raise Exception('Layer ' + self.name +\n+                            ' has multiple inbound nodes, ' +\n+                            'hence the notion of \"layer output\" '\n+                            'is ill-defined. '\n+                            'Use `get_output_at(node_index)` instead.')\n         return self._get_node_attribute_at_index(0, 'output_tensors',\n                                                  'output')\n \n@@ -727,8 +753,12 @@ class Layer(object):\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n         '''\n-        assert len(self.inbound_nodes) == 1, ('Layer ' + self.name + ' has multiple inbound nodes, ' +\n-            'hence the notion of \"layer input mask\" is ill-defined. Use `get_input_mask_at(node_index)` instead.')\n+        if len(self.inbound_nodes) != 1:\n+            raise Exception('Layer ' + self.name +\n+                            ' has multiple inbound nodes, ' +\n+                            'hence the notion of \"layer input mask\" '\n+                            'is ill-defined. '\n+                            'Use `get_input_mask_at(node_index)` instead.')\n         return self._get_node_attribute_at_index(0, 'input_masks',\n                                                  'input mask')\n \n@@ -738,8 +768,12 @@ class Layer(object):\n         the layer has exactly one inbound node, i.e. if it is connected\n         to one incoming layer).\n         '''\n-        assert len(self.inbound_nodes) == 1, ('Layer ' + self.name + ' has multiple inbound nodes, ' +\n-            'hence the notion of \"layer output mask\" is ill-defined. Use `get_output_mask_at(node_index)` instead.')\n+        if len(self.inbound_nodes) != 1:\n+            raise Exception('Layer ' + self.name +\n+                            ' has multiple inbound nodes, ' +\n+                            'hence the notion of \"layer output mask\" '\n+                            'is ill-defined. '\n+                            'Use `get_output_mask_at(node_index)` instead.')\n         return self._get_node_attribute_at_index(0, 'output_masks',\n                                                  'output mask')\n \n\n@@ -150,19 +150,27 @@ def check_array_lengths(X, Y, W):\n     y_lengths = [y.shape[0] for y in Y]\n     w_lengths = [w.shape[0] for w in W]\n     set_x = set(x_lengths)\n-    assert len(set_x) == 1, 'All input arrays (x) should have the same number of samples.'\n+    if len(set_x) != 1:\n+        raise Exception('All input arrays (x) should have '\n+                        'the same number of samples.')\n     set_y = set(y_lengths)\n-    assert len(set_x) == 1, 'All target arrays (y) should have the same number of samples.'\n+    if len(set_x) != 1:\n+        raise Exception('All target arrays (y) should have '\n+                        'the same number of samples.')\n     set_w = set(w_lengths)\n-    assert len(set_w) == 1, 'All sample_weight arrays should have the same number of samples.'\n-    assert list(set_x)[0] == list(set_y)[0], ('Input arrays should have '\n+    if len(set_w) != 1:\n+        raise Exception('All sample_weight arrays should have '\n+                        'the same number of samples.')\n+    if list(set_x)[0] != list(set_y)[0]:\n+        raise Exception('Input arrays should have '\n                         'the same number of samples as target arrays. Found ' +\n-            str(list(set_x)[0]) + ' input samples and ' + str(list(set_y)[0]) +\n-            ' target samples.')\n-    assert list(set_x)[0] == list(set_w)[0], ('Sample_weight arrays should have '\n+                        str(list(set_x)[0]) + ' input samples and ' +\n+                        str(list(set_y)[0]) + ' target samples.')\n+    if list(set_x)[0] != list(set_w)[0]:\n+        raise Exception('Sample_weight arrays should have '\n                         'the same number of samples as input arrays. Found ' +\n-            str(list(set_x)[0]) + ' input samples and ' + str(list(set_w)[0]) +\n-            ' target samples.')\n+                        str(list(set_x)[0]) + ' input samples and ' +\n+                        str(list(set_w)[0]) + ' target samples.')\n \n \n def collect_metrics(metrics, output_names):\n\n@@ -67,8 +67,10 @@ class Optimizer(object):\n                 output of `get_weights`).\n         '''\n         params = self.weights\n-        assert len(params) == len(weights), ('Provided weight array does not match  weights (' +\n-            str(len(params)) + ' optimizer params vs. ' + str(len(weights)) + ' provided weights)')\n+        if len(params) != len(weights):\n+            raise Exception('Provided weight array does not match  weights (' +\n+                            str(len(params)) + ' optimizer params vs. ' +\n+                            str(len(weights)) + ' provided weights)')\n         for p, w in zip(params, weights):\n             if K.get_value(p).shape != w.shape:\n                 raise Exception('Optimizer weight shape ' +\n\n@@ -6,7 +6,7 @@ import numpy as np\n np.random.seed(1337)\n \n from keras import backend as K\n-from keras.models import Graph, Sequential #, model_from_json, model_from_yaml\n+from keras.models import Graph, Sequential\n from keras.layers.core import Dense, Activation, Merge, Lambda\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c064963ef8553a96f6f487ad4c00ed5832cc1cf7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1040 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,8 +42,8 @@ def test_merge():\n     input_a = Input(shape=input_shapes[0][1:])\n     input_b = Input(shape=input_shapes[1][1:])\n     merged = merge([input_a, input_b],\n-                   mode=lambda (x, y): K.concatenate([x, y]),\n-                   output_shape=lambda (s1, s2): (s1[:-1],) + (s1[-1] + s2[-1],))\n+                   mode=lambda tup: K.concatenate([tup[0], tup[1]]),\n+                   output_shape=lambda tup: (tup[0][:-1],) + (tup[0][-1] + tup[1][-1],))\n     expected_output_shape = model.get_output_shape_for(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n@@ -53,10 +53,12 @@ def test_merge():\n     model.compile('rmsprop', 'mse')\n \n     # test function with output_shape function\n-    def fn_mode((x, y)):\n+    def fn_mode(tup):\n+        x, y = tup\n         return K.concatenate([x, y])\n \n-    def fn_output_shape((s1, s2)):\n+    def fn_output_shape(tup):\n+        s1, s2 = tup\n         return (s1[:-1],) + (s1[-1] + s2[-1],)\n \n     input_a = Input(shape=input_shapes[0][1:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0242ca59ac0412b44419de2b6b4d07bebf975610", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 162 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1 +1 @@\n-__version__ = '0.3.2'\r\n+__version__ = '1.0.0'\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='0.3.2',\n+      version='1.0.0',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/0.3.2',\n+      download_url='https://github.com/fchollet/keras/tarball/1.0.0',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ad6b19bd60cb343498b1ff5f7e746fbd187dbc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 3 | Churn Cumulative: 2534 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1270,6 +1270,9 @@ class Merge(Layer):\n         elif self.mode == 'concat':\n             output_shape = list(input_shapes[0])\n             for shape in input_shapes[1:]:\n+                if output_shape[self.concat_axis] is None or shape[self.concat_axis] is None:\n+                    output_shape[self.concat_axis] = None\n+                    break\n                 output_shape[self.concat_axis] += shape[self.concat_axis]\n             return tuple(output_shape)\n         elif self.mode == 'join':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#81233b3cd3189bcd62845bd2f56006a2e0417354", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 10154 | Contributors (this commit): 43 | Commits (past 90d): 44 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -394,9 +394,9 @@ class Lambda(Layer):\n \n         if isinstance(self.function, python_types.LambdaType):\n             if py3:\n-                function = marshal.dumps(self.function.__code__)\n+                function = marshal.dumps(self.function.__code__).decode('raw_unicode_escape')\n             else:\n-                function = marshal.dumps(self.function.func_code)\n+                function = marshal.dumps(self.function.func_code).decode('raw_unicode_escape')\n             function_type = 'lambda'\n         else:\n             function = self.function.__name__\n@@ -429,7 +429,7 @@ class Lambda(Layer):\n         if function_type == 'function':\n             function = globals()[config['function']]\n         elif function_type == 'lambda':\n-            function = marshal.loads(config['function'])\n+            function = marshal.loads(config['function'].encode('raw_unicode_escape'))\n             function = python_types.FunctionType(function, globals())\n         else:\n             raise Exception('Unknown function type: ' + function_type)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7cbdff79cdd9600c8650ecea5acdadcf2da3c19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 96 | Churn Cumulative: 213 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,14 +6,15 @@ from keras.engine.topology import merge, Input\n from keras.engine.training import Model\n from keras import backend as K\n \n-a = Input(shape=(32,), name='input_a')\n-b = Input(shape=(32,), name='input_b')\n \n-a_2 = Dense(16, name='dense_1')(a)\n+def test_model_methods():\n+    a = Input(shape=(3,), name='input_a')\n+    b = Input(shape=(3,), name='input_b')\n+\n+    a_2 = Dense(4, name='dense_1')(a)\n     dp = Dropout(0.5, name='dropout')\n     b_2 = dp(b)\n \n-# Test recursion\n     model = Model([a, b], [a_2, b_2])\n \n     optimizer = 'rmsprop'\n@@ -22,12 +23,13 @@ loss_weights = [1., 0.5]\n     model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n                   sample_weight_mode=None)\n \n-input_a_np = np.random.random((10, 32))\n-input_b_np = np.random.random((10, 32))\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n \n-output_a_np = np.random.random((10, 16))\n-output_b_np = np.random.random((10, 32))\n+    output_a_np = np.random.random((10, 4))\n+    output_b_np = np.random.random((10, 3))\n \n+    # test train_on_batch\n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n     out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n@@ -35,6 +37,41 @@ out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n     out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                                {'dense_1': output_a_np, 'dropout': output_b_np})\n \n+    # test fit\n+    out = model.fit([input_a_np, input_b_np],\n+                    [output_a_np, output_b_np], nb_epoch=1, batch_size=4)\n+    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n+                    [output_a_np, output_b_np], nb_epoch=1, batch_size=4)\n+    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n+                    {'dense_1': output_a_np, 'dropout': output_b_np},\n+                    nb_epoch=1, batch_size=4)\n+\n+    # test validation_split\n+    out = model.fit([input_a_np, input_b_np],\n+                    [output_a_np, output_b_np],\n+                    nb_epoch=1, batch_size=4, validation_split=0.5)\n+    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n+                    [output_a_np, output_b_np],\n+                    nb_epoch=1, batch_size=4, validation_split=0.5)\n+    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n+                    {'dense_1': output_a_np, 'dropout': output_b_np},\n+                    nb_epoch=1, batch_size=4, validation_split=0.5)\n+\n+    # test validation data\n+    out = model.fit([input_a_np, input_b_np],\n+                    [output_a_np, output_b_np],\n+                    nb_epoch=1, batch_size=4,\n+                    validation_data=([input_a_np, input_b_np], [output_a_np, output_b_np]))\n+    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n+                    [output_a_np, output_b_np],\n+                    nb_epoch=1, batch_size=4, validation_split=0.5,\n+                    validation_data=({'input_a': input_a_np, 'input_b': input_b_np}, [output_a_np, output_b_np]))\n+    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n+                    {'dense_1': output_a_np, 'dropout': output_b_np},\n+                    nb_epoch=1, batch_size=4, validation_split=0.5,\n+                    validation_data=({'input_a': input_a_np, 'input_b': input_b_np}, {'dense_1': output_a_np, 'dropout': output_b_np}))\n+\n+    # test_on_batch\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n     out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n@@ -42,27 +79,26 @@ out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n     out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n                               {'dense_1': output_a_np, 'dropout': output_b_np})\n \n+    # predict_on_batch\n     out = model.predict_on_batch([input_a_np, input_b_np])\n     out = model.predict_on_batch({'input_a': input_a_np, 'input_b': input_b_np})\n \n-input_a_np = np.random.random((100, 32))\n-input_b_np = np.random.random((100, 32))\n+    # predict, evaluate\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n \n-output_a_np = np.random.random((100, 16))\n-output_b_np = np.random.random((100, 32))\n-\n-out = model.fit([input_a_np, input_b_np], [output_a_np, output_b_np])\n-out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np])\n-out = model.predict([input_a_np, input_b_np])\n+    output_a_np = np.random.random((10, 4))\n+    output_b_np = np.random.random((10, 3))\n \n+    out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4)\n+    out = model.predict([input_a_np, input_b_np], batch_size=4)\n \n     # with sample_weight\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n \n-input_a_np = np.random.random((10, 32))\n-input_b_np = np.random.random((10, 32))\n-\n-output_a_np = np.random.random((10, 16))\n-output_b_np = np.random.random((10, 32))\n+    output_a_np = np.random.random((10, 4))\n+    output_b_np = np.random.random((10, 3))\n \n     sample_weight = [None, np.random.random((10,))]\n     out = model.train_on_batch([input_a_np, input_b_np],\n@@ -106,12 +142,16 @@ out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n     assert len(out) == 2\n \n-input_a_np = np.random.random((100, 32))\n-input_b_np = np.random.random((100, 32))\n+    input_a_np = np.random.random((10, 3))\n+    input_b_np = np.random.random((10, 3))\n \n-output_a_np = np.random.random((100, 16))\n-output_b_np = np.random.random((100, 32))\n+    output_a_np = np.random.random((10, 4))\n+    output_b_np = np.random.random((10, 3))\n \n-out = model.fit([input_a_np, input_b_np], [output_a_np, output_b_np])\n-out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np])\n-out = model.predict([input_a_np, input_b_np])\n+    out = model.fit([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4, nb_epoch=1)\n+    out = model.evaluate([input_a_np, input_b_np], [output_a_np, output_b_np], batch_size=4)\n+    out = model.predict([input_a_np, input_b_np], batch_size=4)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76da13dff6860e4cf8d48e27883926439e12ad54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 49 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 71 | Churn Cumulative: 1667 | Contributors (this commit): 18 | Commits (past 90d): 44 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,21 +18,6 @@ def learning_phase():\n     return _LEARNING_PHASE\n \n \n-def _on_gpu():\n-    '''Return whether the session is set to\n-    run on GPU or not (i.e. on CPU).\n-    '''\n-    return theano.config.device[:3] == 'gpu' or theano.sandbox.cuda.cuda_enabled\n-\n-\n-if _on_gpu():\n-    '''Import cuDNN only if running on GPU:\n-    not having Cuda installed should not\n-    prevent from running the present code.\n-    '''\n-    from theano.sandbox.cuda import dnn\n-\n-\n # VARIABLE MANIPULATION\n \n def variable(value, dtype=_FLOATX, name=None):\n@@ -774,7 +759,6 @@ def l2_normalize(x, axis):\n def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n            image_shape=None, filter_shape=None):\n     '''\n-    Run on cuDNN if available.\n     border_mode: string, \"same\" or \"valid\".\n     '''\n     if dim_ordering not in {'th', 'tf'}:\n@@ -796,30 +780,8 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n             filter_shape = (filter_shape[3], filter_shape[2],\n                             filter_shape[0], filter_shape[1])\n \n-    if _on_gpu() and dnn.dnn_available():\n     if border_mode == 'same':\n-            np_kernel = kernel.eval()\n-            assert strides[0] <= np_kernel.shape[2], 'strides should be smaller than the convolution window.'\n-            assert strides[1] <= np_kernel.shape[3], 'strides should be smaller than the convolution window.'\n-            conv_out = dnn.dnn_conv(img=x,\n-                                    kerns=kernel,\n-                                    border_mode='full')\n-            shift_x = (np_kernel.shape[2] - strides[0]) // 2\n-            shift_y = (np_kernel.shape[3] - strides[1]) // 2\n-            expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n-            expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n-\n-            conv_out = conv_out[:, :,\n-                                shift_x: shift_x + expected_width,\n-                                shift_y: shift_y + expected_height]\n-        else:\n-            conv_out = dnn.dnn_conv(img=x,\n-                                    kerns=kernel,\n-                                    border_mode=border_mode,\n-                                    subsample=strides)\n-    else:\n-        if border_mode == 'same':\n-            th_border_mode = 'full'\n+        th_border_mode = 'half'\n         np_kernel = kernel.eval()\n         assert strides[0] <= np_kernel.shape[2], 'strides should be smaller than the convolution window.'\n         assert strides[1] <= np_kernel.shape[3], 'strides should be smaller than the convolution window.'\n@@ -828,20 +790,31 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n     else:\n         raise Exception('Border mode not supported: ' + str(border_mode))\n \n-        conv_out = T.nnet.conv.conv2d(x, kernel,\n+    # Theano might not accept long type\n+    def int_or_none(value):\n+        try:\n+            return int(value)\n+        except TypeError:\n+            return None\n+\n+    if image_shape is not None:\n+        image_shape = tuple(int_or_none(v) for v in image_shape)\n+\n+    if filter_shape is not None:\n+        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n+\n+    conv_out = T.nnet.conv2d(x, kernel,\n                              border_mode=th_border_mode,\n                              subsample=strides,\n-                                      image_shape=image_shape,\n+                             input_shape=image_shape,\n                              filter_shape=filter_shape)\n-        if border_mode == 'same':\n-            shift_x = (np_kernel.shape[2] - strides[0]) // 2\n-            shift_y = (np_kernel.shape[3] - strides[1]) // 2\n-            expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n-            expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n \n-            conv_out = conv_out[:, :,\n-                                shift_x: shift_x + expected_width,\n-                                shift_y: shift_y + expected_height]\n+    if border_mode == 'same':\n+        if np_kernel.shape[2] % 2 == 0:\n+            conv_out = conv_out[:, :, :(x.shape[2] + strides[0] - 1) // strides[0], :]\n+        if np_kernel.shape[3] % 2 == 0:\n+            conv_out = conv_out[:, :, :, :(x.shape[3] + strides[1] - 1) // strides[1]]\n+\n     if dim_ordering == 'tf':\n         conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n     return conv_out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2902149f776e20623ec49cec485455c057bd26ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 13 | Files Changed: 6 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 32 | Churn Cumulative: 2318 | Contributors (this commit): 13 | Commits (past 90d): 37 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,7 +63,7 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 40\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(60000, 784)\n\n@@ -28,7 +28,7 @@ nb_pool = 2\n # convolution kernel size\n nb_conv = 3\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n\n@@ -20,7 +20,7 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 20\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(60000, 784)\n\n@@ -73,7 +73,7 @@ def compute_accuracy(predictions, labels):\n     return labels[predictions.ravel() < 0.5].mean()\n \n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n X_train = X_train.reshape(60000, 784)\n X_test = X_test.reshape(10000, 784)\n\n@@ -3,6 +3,7 @@\n from __future__ import print_function\n from __future__ import absolute_import\n \n+import warnings\n import copy\n import time\n import numpy as np\n@@ -1170,6 +1171,12 @@ class Model(Container):\n                 samples_seen += batch_size\n \n                 # epoch finished\n+                if samples_seen > samples_per_epoch:\n+                    warnings.warn('Epoch comprised more than '\n+                                  '`samples_per_epoch` samples, '\n+                                  'which might affect learning results. '\n+                                  'Set `samples_per_epoch` correctly '\n+                                  'to avoid this warning.')\n                 if samples_seen >= samples_per_epoch and do_validation:\n                     if val_gen:\n                         val_outs = self.evaluate_generator(validation_data,\n\n@@ -11,14 +11,13 @@ from scipy import linalg\n \n from os import listdir\n from os.path import isfile, join\n-import random\n import math\n from six.moves import range\n import threading\n \n \n def random_rotation(x, rg, fill_mode='nearest', cval=0.):\n-    angle = random.uniform(-rg, rg)\n+    angle = np.random.uniform(-rg, rg)\n     x = ndimage.interpolation.rotate(x, angle,\n                                      axes=(1, 2),\n                                      reshape=False,\n@@ -31,9 +30,9 @@ def random_shift(x, wrg, hrg, fill_mode='nearest', cval=0.):\n     shift_x = shift_y = 0\n \n     if wrg:\n-        shift_x = random.uniform(-wrg, wrg) * x.shape[2]\n+        shift_x = np.random.uniform(-wrg, wrg) * x.shape[2]\n     if hrg:\n-        shift_y = random.uniform(-hrg, hrg) * x.shape[1]\n+        shift_y = np.random.uniform(-hrg, hrg) * x.shape[1]\n     x = ndimage.interpolation.shift(x, (0, shift_y, shift_x),\n                                     order=0,\n                                     mode=fill_mode,\n@@ -59,7 +58,7 @@ def random_barrel_transform(x, intensity):\n \n \n def random_shear(x, intensity, fill_mode='nearest', cval=0.):\n-    shear = random.uniform(-intensity, intensity)\n+    shear = np.random.uniform(-intensity, intensity)\n     shear_matrix = np.array([[1.0, -math.sin(shear), 0.0],\n                             [0.0, math.cos(shear), 0.0],\n                             [0.0, 0.0, 1.0]])\n@@ -76,8 +75,8 @@ def random_channel_shift(x, rg):\n \n \n def random_zoom(x, rg, fill_mode='nearest', cval=0.):\n-    zoom_w = random.uniform(1.-rg, 1.)\n-    zoom_h = random.uniform(1.-rg, 1.)\n+    zoom_w = np.random.uniform(1.-rg, 1.)\n+    zoom_h = np.random.uniform(1.-rg, 1.)\n     x = ndimage.interpolation.zoom(x, zoom=(1., zoom_w, zoom_h),\n                                    mode=fill_mode,\n                                    cval=cval)\n@@ -253,10 +252,10 @@ class ImageDataGenerator(object):\n         if self.width_shift_range or self.height_shift_range:\n             x = random_shift(x, self.width_shift_range, self.height_shift_range)\n         if self.horizontal_flip:\n-            if random.random() < 0.5:\n+            if np.random.random() < 0.5:\n                 x = horizontal_flip(x)\n         if self.vertical_flip:\n-            if random.random() < 0.5:\n+            if np.random.random() < 0.5:\n                 x = vertical_flip(x)\n         if self.shear_range:\n             x = random_shear(x, self.shear_range)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30118bbab0a90f77acd7d366d04c49375fa85aae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 473 | Lines Deleted: 206 | Files Changed: 12 | Hunks: 83 | Methods Changed: 19 | Complexity Δ (Sum/Max): 3/6 | Churn Δ: 679 | Churn Cumulative: 31687 | Contributors (this commit): 90 | Commits (past 90d): 191 | Contributors (cumulative): 167 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ class Constraint(object):\n         return p\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__}\n+        return {'name': self.__class__.__name__}\n \n \n class MaxNorm(Constraint):\n@@ -42,9 +42,9 @@ class MaxNorm(Constraint):\n         return p\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"m\": self.m,\n-                \"axis\": self.axis}\n+        return {'name': self.__class__.__name__,\n+                'm': self.m,\n+                'axis': self.axis}\n \n \n class NonNeg(Constraint):\n@@ -79,8 +79,8 @@ class UnitNorm(Constraint):\n         return p / (K.epsilon() + K.sqrt(K.sum(K.square(p), axis=self.axis, keepdims=True)))\n \n     def get_config(self):\n-        return {\"name\": self.__class__.__name__,\n-                \"axis\": self.axis}\n+        return {'name': self.__class__.__name__,\n+                'axis': self.axis}\n \n \n maxnorm = MaxNorm\n\n@@ -371,21 +371,21 @@ class Layer(object):\n                         raise Exception('Input ' + str(input_index) +\n                                         ' is incompatible with layer ' +\n                                         self.name + ': expected ndim >= ' +\n-                                        str(ndim) + ' found ndim=' +\n+                                        str(ndim) + ', found ndim=' +\n                                         str(K.ndim(x)))\n                 else:\n                     if K.ndim(x) != spec.ndim:\n                         raise Exception('Input ' + str(input_index) +\n                                         ' is incompatible with layer ' +\n                                         self.name + ': expected ndim=' +\n-                                        str(spec.ndim), ' found ndim=' +\n+                                        str(spec.ndim) + ', found ndim=' +\n                                         str(K.ndim(x)))\n             if spec.dtype is not None:\n                 if K.dtype(x) != spec.dtype:\n                     raise Exception('Input ' + str(input_index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected dtype=' +\n-                                    str(spec.dtype), ' found dtype=' +\n+                                    str(spec.dtype) + ', found dtype=' +\n                                     str(K.dtype(x)))\n             if spec.shape is not None:\n                 if hasattr(x, '_keras_shape'):\n@@ -438,6 +438,10 @@ class Layer(object):\n             mask: tensor or list/tuple of tensors.\n         '''\n         if not self.built:\n+            # raise exceptions in case the input is not compatible\n+            # with the input_spec specified in the layer constructor\n+            self.assert_input_compatibility(x)\n+\n             # collect input shapes to build layer\n             input_shapes = []\n             for x_elem in to_list(x):\n@@ -458,7 +462,8 @@ class Layer(object):\n                 self.build(input_shapes)\n             self.built = True\n \n-        # raise exceptions in case the input is not compatible with the layer\n+        # raise exceptions in case the input is not compatible\n+        # with the input_spec set at build time\n         self.assert_input_compatibility(x)\n         # build and connect layer\n         input_added = False\n\n@@ -1,5 +1,3 @@\n-'''TODO: docstrings\n-'''\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -174,6 +172,38 @@ def check_array_lengths(X, Y, W):\n                         str(list(set_w)[0]) + ' target samples.')\n \n \n+def check_loss_and_target_compatibility(targets, losses, output_shapes):\n+    assert len(targets) == len(losses) == len(output_shapes)\n+    key_losses = {'mean_square_error',\n+                  'binary_crossentropy',\n+                  'categorical_crossentropy'}\n+    for y, loss, shape in zip(targets, losses, output_shapes):\n+        if loss.__name__ == 'categorical_crossentropy':\n+            if y.shape[1] == 1:\n+                raise Exception('You are passing a target array of shape ' + str(y.shape) +\n+                                ' while using as loss `categorical_crossentropy`. '\n+                                '`categorical_crossentropy` expects '\n+                                'targets to be binary matrices (1s and 0s) '\n+                                'of shape (samples, classes). '\n+                                'If your targets are integer classes, '\n+                                'you can convert them to the expected format via:\\n'\n+                                '```\\n'\n+                                'from keras.utils.np_utils import to_categorical\\n'\n+                                'y_binary = to_categorical(y_int)\\n'\n+                                '```\\n'\n+                                '\\n'\n+                                'Alternatively, you can use the loss function '\n+                                '`sparse_categorical_crossentropy` instead, '\n+                                'which does expect integer targets.')\n+        if loss.__name__ in key_losses and y.shape[1] != shape[1]:\n+                raise Exception('A target array with shape ' + str(y.shape) +\n+                                ' was passed for an output of shape ' + str(shape) +\n+                                ' while using as loss `' + loss.__name__ + '`. '\n+                                'This loss expects '\n+                                'targets to have the same shape '\n+                                'as the output.')\n+\n+\n def collect_metrics(metrics, output_names):\n     if not metrics:\n         return [[] for _ in output_names]\n@@ -281,8 +311,8 @@ def weighted_objective(fn):\n \n def standardize_weights(y, sample_weight=None, class_weight=None,\n                         sample_weight_mode=None):\n-    '''Weight input validation and standardization to a single sample-wise\n-    (or timestep-wise) weight array.\n+    '''Performs weight input validation and standardization\n+    to a single sample-wise (or timestep-wise) weight array.\n     '''\n     if sample_weight_mode is not None:\n         if sample_weight_mode != 'temporal':\n@@ -445,6 +475,7 @@ class Model(Container):\n         else:\n             loss_function = objectives.get(loss)\n             loss_functions = [loss_function for _ in range(len(self.outputs))]\n+        self.loss_functions = loss_functions\n         weighted_losses = [weighted_objective(fn) for fn in loss_functions]\n \n         # prepare output masks\n@@ -507,7 +538,9 @@ class Model(Container):\n \n         # prepare targets of model\n         self.targets = []\n-        for y, shape, name in zip(self.outputs, self.internal_output_shapes, self.output_names):\n+        for i in range(len(self.outputs)):\n+            shape = self.internal_output_shapes[i]\n+            name = self.output_names[i]\n             self.targets.append(K.placeholder(ndim=len(shape), name=name + '_target'))\n \n         # compute total loss\n@@ -826,6 +859,7 @@ class Model(Container):\n                           for (ref, sw, cw, mode)\n                           in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n         check_array_lengths(x, y, sample_weights)\n+        check_loss_and_target_compatibility(y, self.loss_functions, self.internal_output_shapes)\n         if self.stateful and batch_size:\n             if x[0].shape[0] % batch_size != 0:\n                 raise Exception('In a stateful network, '\n@@ -838,7 +872,47 @@ class Model(Container):\n     def fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True,\n             class_weight=None, sample_weight=None):\n-        '''\n+        '''Trains the model for a fixed number of epochs (iterations on a dataset).\n+\n+        # Arguments\n+            x: Numpy array of training data,\n+                or list of Numpy arrays if the model has multiple inputs.\n+                If all inputs in the model are named, you can also pass a dictionary\n+                mapping input names to Numpy arrays.\n+            y: Numpy array of target data,\n+                or list of Numpy arrays if the model has multiple outputs.\n+                If all outputs in the model are named, you can also pass a dictionary\n+                mapping output names to Numpy arrays.\n+            batch_size: integer. Number of samples per gradient update.\n+            nb_epoch: integer, the number of times to iterate over the training data arrays.\n+            verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = verbose, 2 = one log line per epoch.\n+            callbacks: list of callbacks to be called during training.\n+                See [callbacks](callbacks.md).\n+            validation_split: float between 0 and 1:\n+                fraction of the training data to be used as validation data.\n+                The model will set apart this fraction of the training data,\n+                will not train on it, and will evaluate the loss and any model metrics\n+                on this data at the end of each epoch.\n+            validation_data: data on which to evaluate the loss and any model metrics\n+                at the end of each epoch. The model will not be trained on this data.\n+                This could be a tuple (x_val, y_val) or a tuple (val_x, val_y, val_sample_weights).\n+            shuffle: boolean, whether to shuffle the training data before each epoch.\n+            class_weight: optional dictionary mapping classe indices (integers) to\n+                a weight (float) to apply to the model's loss for the samples\n+                from this class during training.\n+                This can be useful to tell the model to \"pay more attention\" to\n+                samples from an under-represented class.\n+            sample_weight: optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape (samples, sequence_length),\n+                to apply a different weight to every timestep of every sample.\n+                In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n+\n+\n+        # Returns\n+            A `History` instance. Its `history` attribute contains\n+            all information collected during training.\n         '''\n         # validate user data\n         x, y, sample_weights = self._standardize_user_data(x, y,\n@@ -908,7 +982,15 @@ class Model(Container):\n         in test mode. Computation in done in batches.\n \n         # Arguments\n-            TODO\n+            x: Numpy array of test data,\n+                or list of Numpy arrays if the model has multiple inputs.\n+                If all inputs in the model are named, you can also pass a dictionary\n+                mapping input names to Numpy arrays.\n+            y: Numpy array of target data,\n+                or list of Numpy arrays if the model has multiple outputs.\n+                If all outputs in the model are named, you can also pass a dictionary\n+                mapping output names to Numpy arrays.\n+            batch_size: integer. Number of samples per gradient update.\n \n         # Returns\n             Scalar test loss (if the model has a single output and no metrics)\n@@ -967,7 +1049,34 @@ class Model(Container):\n                                   batch_size=batch_size, verbose=verbose)\n \n     def train_on_batch(self, x, y, sample_weight=None, class_weight=None):\n-        '''\n+        '''Runs a single gradient update on a single batch of data.\n+\n+        # Arguments\n+            x: Numpy array of training data,\n+                or list of Numpy arrays if the model has multiple inputs.\n+                If all inputs in the model are named, you can also pass a dictionary\n+                mapping input names to Numpy arrays.\n+            y: Numpy array of target data,\n+                or list of Numpy arrays if the model has multiple outputs.\n+                If all outputs in the model are named, you can also pass a dictionary\n+                mapping output names to Numpy arrays.\n+            sample_weight: optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape (samples, sequence_length),\n+                to apply a different weight to every timestep of every sample.\n+                In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n+            class_weight: optional dictionary mapping classe indices (integers) to\n+                a weight (float) to apply to the model's loss for the samples\n+                from this class during training.\n+                This can be useful to tell the model to \"pay more attention\" to\n+                samples from an under-represented class.\n+\n+        # Returns\n+            Scalar training loss (if the model has a single output and no metrics)\n+            or list of scalars (if the model has multiple outputs\n+            and/or metrics). The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n         '''\n         x, y, sample_weights = self._standardize_user_data(x, y,\n                                                            sample_weight=sample_weight,\n@@ -983,7 +1092,29 @@ class Model(Container):\n         return outputs\n \n     def test_on_batch(self, x, y, sample_weight=None):\n-        '''\n+        '''Test the model on a single batch of samples.\n+\n+        # Arguments\n+            x: Numpy array of test data,\n+                or list of Numpy arrays if the model has multiple inputs.\n+                If all inputs in the model are named, you can also pass a dictionary\n+                mapping input names to Numpy arrays.\n+            y: Numpy array of target data,\n+                or list of Numpy arrays if the model has multiple outputs.\n+                If all outputs in the model are named, you can also pass a dictionary\n+                mapping output names to Numpy arrays.\n+            sample_weight: optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape (samples, sequence_length),\n+                to apply a different weight to every timestep of every sample.\n+                In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n+\n+        # Returns\n+            Scalar test loss (if the model has a single output and no metrics)\n+            or list of scalars (if the model has multiple outputs\n+            and/or metrics). The attribute `model.metrics_names` will give you\n+            the display labels for the scalar outputs.\n         '''\n         x, y, sample_weights = self._standardize_user_data(x, y,\n                                                            sample_weight=sample_weight,\n\n@@ -7,7 +7,8 @@ import numpy as np\n class LeakyReLU(Layer):\n     '''Special version of a Rectified Linear Unit\n     that allows a small gradient when the unit is not active:\n-    `f(x) = alpha*x for x < 0`.\n+    `f(x) = alpha * x for x < 0`,\n+    `f(x) = x for x >= 0`.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -35,7 +36,10 @@ class LeakyReLU(Layer):\n \n \n class PReLU(Layer):\n-    '''Parametric Rectified Linear Unit.\n+    '''Parametric Rectified Linear Unit:\n+    `f(x) = alphas * x for x < 0`,\n+    `f(x) = x for x >= 0`,\n+    where `alphas` is a learned array with the same shape as x.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -49,7 +53,7 @@ class PReLU(Layer):\n         init: initialization function for the weights.\n         weights: initial weights, as a list of a single numpy array.\n \n-    # References:\n+    # References\n         - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](http://arxiv.org/pdf/1502.01852v1.pdf)\n     '''\n     def __init__(self, init='zero', weights=None, **kwargs):\n@@ -79,7 +83,9 @@ class PReLU(Layer):\n \n \n class ELU(Layer):\n-    '''Exponential Linear Unit.\n+    '''Exponential Linear Unit:\n+    `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\n+    `f(x) = x for x >= 0`.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -112,7 +118,8 @@ class ELU(Layer):\n \n \n class ParametricSoftplus(Layer):\n-    '''Parametric Softplus of the form: alpha * log(1 + exp(beta * X))\n+    '''Parametric Softplus:\n+    `alpha * log(1 + exp(beta * x))`\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -127,7 +134,7 @@ class ParametricSoftplus(Layer):\n         beta_init: float. Initial values of the beta weights.\n         weights: initial weights, as a list of 2 numpy arrays.\n \n-    # References:\n+    # References\n         - [Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003143)\n     '''\n     def __init__(self, alpha_init=0.2, beta_init=5.0,\n@@ -160,39 +167,10 @@ class ParametricSoftplus(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n-class ThresholdedLinear(Layer):\n-    '''Thresholded Linear Activation.\n-\n-    # Input shape\n-        Arbitrary. Use the keyword argument `input_shape`\n-        (tuple of integers, does not include the samples axis)\n-        when using this layer as the first layer in a model.\n-\n-    # Output shape\n-        Same shape as the input.\n-\n-    # Arguments\n-        theta: float >= 0. Threshold location of activation.\n-\n-    # References\n-        [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/pdf/1402.3337.pdf)\n-    '''\n-    def __init__(self, theta=1.0, **kwargs):\n-        self.supports_masking = True\n-        self.theta = K.cast_to_floatx(theta)\n-        super(ThresholdedLinear, self).__init__(**kwargs)\n-\n-    def call(self, x, mask=None):\n-        return x * K.cast(x > self.theta, K.floatx())\n-\n-    def get_config(self):\n-        config = {'theta': self.theta}\n-        base_config = super(ThresholdedLinear, self).get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n class ThresholdedReLU(Layer):\n-    '''Thresholded Rectified Linear Unit.\n+    '''Thresholded Rectified Linear Unit:\n+    `f(x) = x for x > theta`\n+    `f(x) = 0 otherwise`.\n \n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n@@ -206,7 +184,7 @@ class ThresholdedReLU(Layer):\n         theta: float >= 0. Threshold location of activation.\n \n     # References\n-        [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/pdf/1402.3337.pdf)\n+        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/pdf/1402.3337.pdf)\n     '''\n     def __init__(self, theta=1.0, **kwargs):\n         self.supports_masking = True\n@@ -240,7 +218,7 @@ class SReLU(Layer):\n         a_right_init: initialization function for the right part slope\n \n     # References\n-        [Deep Learning with S-shaped Rectified Linear Activation Units](http://arxiv.org/abs/1512.07030)\n+        - [Deep Learning with S-shaped Rectified Linear Activation Units](http://arxiv.org/abs/1512.07030)\n     '''\n     def __init__(self, t_left_init='zero', a_left_init='glorot_uniform',\n                  t_right_init='glorot_uniform', a_right_init='one', **kwargs):\n\n@@ -25,12 +25,19 @@ class Convolution1D(Layer):\n     or `input_shape` (tuple of integers, e.g. (10, 128) for sequences\n     of 10 vectors of 128-dimensional vectors).\n \n-    # Input shape\n-        3D tensor with shape: `(samples, steps, input_dim)`.\n+    # Example\n \n-    # Output shape\n-        3D tensor with shape: `(samples, new_steps, nb_filter)`.\n-        `steps` value might have changed due to padding.\n+    ```python\n+        # apply a convolution 1d of length 3 to a sequence with 10 timesteps,\n+        # with 64 output filters\n+        model = Sequential()\n+        model.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n+        # now model.output_shape == (None, 10, 64)\n+\n+        # add a new conv1d on top\n+        model.add(Convolution1D(32, 3, border_mode='same'))\n+        # now model.output_shape == (None, 10, 32)\n+    ```\n \n     # Arguments\n         nb_filter: Number of convolution kernels to use\n@@ -65,6 +72,13 @@ class Convolution1D(Layer):\n             This argument is required if you are going to connect\n             `Flatten` then `Dense` layers upstream\n             (without it, the shape of the dense outputs cannot be computed).\n+\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, input_dim)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(samples, new_steps, nb_filter)`.\n+        `steps` value might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, filter_length,\n                  init='uniform', activation='linear', weights=None,\n@@ -175,19 +189,18 @@ class Convolution2D(Layer):\n     (tuple of integers, does not include the sample axis),\n     e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n \n-    # Input shape\n-        4D tensor with shape:\n-        `(samples, channels, rows, cols)` if dim_ordering='th'\n-        or 4D tensor with shape:\n-        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+    # Examples\n \n-    # Output shape\n-        4D tensor with shape:\n-        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n-        or 4D tensor with shape:\n-        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n-        `rows` and `cols` values might have changed due to padding.\n+    ```python\n+        # apply a 3x3 convolution with 64 output filters on a 256x256 image:\n+        model = Sequential()\n+        model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(3, 256, 256)))\n+        # now model.output_shape == (None, 64, 256, 256)\n \n+        # add a 3x3 convolution on top, with 32 output filters:\n+        model.add(Convolution2D(32, 3, 3, border_mode='same'))\n+        # now model.output_shape == (None, 32, 256, 256)\n+    ```\n \n     # Arguments\n         nb_filter: Number of convolution filters to use.\n@@ -219,6 +232,19 @@ class Convolution2D(Layer):\n             applied to the bias.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        4D tensor with shape:\n+        `(samples, nb_filter, new_rows, new_cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, new_rows, new_cols, nb_filter)` if dim_ordering='tf'.\n+        `rows` and `cols` values might have changed due to padding.\n     '''\n     def __init__(self, nb_filter, nb_row, nb_col,\n                  init='glorot_uniform', activation='linear', weights=None,\n@@ -349,19 +375,6 @@ class Convolution3D(Layer):\n \n     Note: this layer will only work with Theano for the time being.\n \n-    # Input shape\n-        5D tensor with shape:\n-        `(samples, channels, conv_dim1, conv_dim2, conv_dim3)` if dim_ordering='th'\n-        or 5D tensor with shape:\n-        `(samples, conv_dim1, conv_dim2, conv_dim3, channels)` if dim_ordering='tf'.\n-\n-    # Output shape\n-        5D tensor with shape:\n-        `(samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if dim_ordering='th'\n-        or 5D tensor with shape:\n-        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)` if dim_ordering='tf'.\n-        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n-\n     # Arguments\n         nb_filter: Number of convolution filters to use.\n         kernel_dim1: Length of the first dimension in the covolution kernel.\n@@ -394,6 +407,19 @@ class Convolution3D(Layer):\n             applied to the bias.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode is it at index 4.\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, conv_dim1, conv_dim2, conv_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, conv_dim1, conv_dim2, conv_dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        5D tensor with shape:\n+        `(samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)` if dim_ordering='tf'.\n+        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n     '''\n \n     def __init__(self, nb_filter, kernel_dim1, kernel_dim2, kernel_dim3,\n@@ -609,17 +635,17 @@ class MaxPooling1D(_Pooling1D):\n class AveragePooling1D(_Pooling1D):\n     '''Average pooling for temporal data.\n \n-    # Input shape\n-        3D tensor with shape: `(samples, steps, features)`.\n-\n-    # Output shape\n-        3D tensor with shape: `(samples, downsampled_steps, features)`.\n-\n     # Arguments\n         pool_length: factor by which to downscale. 2 will halve the input.\n         stride: integer or None. Stride value.\n         border_mode: 'valid' or 'same'.\n             Note: 'same' will only work with TensorFlow for the time being.\n+\n+    # Input shape\n+        3D tensor with shape: `(samples, steps, features)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(samples, downsampled_steps, features)`.\n     '''\n \n     def __init__(self, pool_length=2, stride=None,\n@@ -696,6 +722,16 @@ class _Pooling2D(Layer):\n class MaxPooling2D(_Pooling2D):\n     '''Max pooling operation for spatial data.\n \n+    # Arguments\n+        pool_size: tuple of 2 integers,\n+            factors by which to downscale (vertical, horizontal).\n+            (2, 2) will halve the image in each dimension.\n+        strides: tuple of 2 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+            Note: 'same' will only work with TensorFlow for the time being.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+\n     # Input shape\n         4D tensor with shape:\n         `(samples, channels, rows, cols)` if dim_ordering='th'\n@@ -707,16 +743,6 @@ class MaxPooling2D(_Pooling2D):\n         `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n-\n-    # Arguments\n-        pool_size: tuple of 2 integers,\n-            factors by which to downscale (vertical, horizontal).\n-            (2, 2) will halve the image in each dimension.\n-        strides: tuple of 2 integers, or None. Strides values.\n-        border_mode: 'valid' or 'same'.\n-            Note: 'same' will only work with TensorFlow for the time being.\n-        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n-            (the depth) is at index 1, in 'tf' mode is it at index 3.\n     '''\n \n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n@@ -734,6 +760,16 @@ class MaxPooling2D(_Pooling2D):\n class AveragePooling2D(_Pooling2D):\n     '''Average pooling operation for spatial data.\n \n+    # Arguments\n+        pool_size: tuple of 2 integers,\n+            factors by which to downscale (vertical, horizontal).\n+            (2, 2) will halve the image in each dimension.\n+        strides: tuple of 2 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+            Note: 'same' will only work with TensorFlow for the time being.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+\n     # Input shape\n         4D tensor with shape:\n         `(samples, channels, rows, cols)` if dim_ordering='th'\n@@ -745,16 +781,6 @@ class AveragePooling2D(_Pooling2D):\n         `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n-\n-    # Arguments\n-        pool_size: tuple of 2 integers,\n-            factors by which to downscale (vertical, horizontal).\n-            (2, 2) will halve the image in each dimension.\n-        strides: tuple of 2 integers, or None. Strides values.\n-        border_mode: 'valid' or 'same'.\n-            Note: 'same' will only work with TensorFlow for the time being.\n-        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n-            (the depth) is at index 1, in 'tf' mode is it at index 3.\n     '''\n \n     def __init__(self, pool_size=(2, 2), strides=None, border_mode='valid',\n@@ -837,6 +863,15 @@ class MaxPooling3D(_Pooling3D):\n \n     Note: this layer will only work with Theano for the time being.\n \n+    # Arguments\n+        pool_size: tuple of 3 integers,\n+            factors by which to downscale (dim1, dim2, dim3).\n+            (2, 2, 2) will halve the size of the 3D input in each dimension.\n+        strides: tuple of 3 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+\n     # Input shape\n         5D tensor with shape:\n         `(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)` if dim_ordering='th'\n@@ -848,15 +883,6 @@ class MaxPooling3D(_Pooling3D):\n         `(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)` if dim_ordering='th'\n         or 5D tensor with shape:\n         `(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)` if dim_ordering='tf'.\n-\n-    # Arguments\n-        pool_size: tuple of 3 integers,\n-            factors by which to downscale (dim1, dim2, dim3).\n-            (2, 2, 2) will halve the size of the 3D input in each dimension.\n-        strides: tuple of 3 integers, or None. Strides values.\n-        border_mode: 'valid' or 'same'.\n-        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n-            (the depth) is at index 1, in 'tf' mode is it at index 4.\n     '''\n \n     def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n@@ -879,6 +905,15 @@ class AveragePooling3D(_Pooling3D):\n \n     Note: this layer will only work with Theano for the time being.\n \n+    # Arguments\n+        pool_size: tuple of 3 integers,\n+            factors by which to downscale (dim1, dim2, dim3).\n+            (2, 2, 2) will halve the size of the 3D input in each dimension.\n+        strides: tuple of 3 integers, or None. Strides values.\n+        border_mode: 'valid' or 'same'.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 4.\n+\n     # Input shape\n         5D tensor with shape:\n         `(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)` if dim_ordering='th'\n@@ -890,15 +925,6 @@ class AveragePooling3D(_Pooling3D):\n         `(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)` if dim_ordering='th'\n         or 5D tensor with shape:\n         `(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)` if dim_ordering='tf'.\n-\n-    # Arguments\n-        pool_size: tuple of 3 integers,\n-            factors by which to downscale (dim1, dim2, dim3).\n-            (2, 2, 2) will halve the size of the 3D input in each dimension.\n-        strides: tuple of 3 integers, or None. Strides values.\n-        border_mode: 'valid' or 'same'.\n-        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n-            (the depth) is at index 1, in 'tf' mode is it at index 4.\n     '''\n \n     def __init__(self, pool_size=(2, 2, 2), strides=None, border_mode='valid',\n@@ -919,14 +945,14 @@ class AveragePooling3D(_Pooling3D):\n class UpSampling1D(Layer):\n     '''Repeat each temporal step `length` times along the time axis.\n \n+    # Arguments:\n+        length: integer. Upsampling factor.\n+\n     # Input shape\n         3D tensor with shape: `(samples, steps, features)`.\n \n     # Output shape\n         3D tensor with shape: `(samples, upsampled_steps, features)`.\n-\n-    # Arguments:\n-        length: integer. Upsampling factor.\n     '''\n \n     def __init__(self, length=2, **kwargs):\n@@ -951,6 +977,12 @@ class UpSampling2D(Layer):\n     '''Repeat the rows and columns of the data\n     by size[0] and size[1] respectively.\n \n+    # Arguments\n+        size: tuple of 2 integers. The upsampling factors for rows and columns.\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 3.\n+\n     # Input shape\n         4D tensor with shape:\n         `(samples, channels, rows, cols)` if dim_ordering='th'\n@@ -962,12 +994,6 @@ class UpSampling2D(Layer):\n         `(samples, channels, upsampled_rows, upsampled_cols)` if dim_ordering='th'\n         or 4D tensor with shape:\n         `(samples, upsampled_rows, upsampled_cols, channels)` if dim_ordering='tf'.\n-\n-    # Arguments\n-        size: tuple of 2 integers. The upsampling factors for rows and columns.\n-        dim_ordering: 'th' or 'tf'.\n-            In 'th' mode, the channels dimension (the depth)\n-            is at index 1, in 'tf' mode is it at index 3.\n     '''\n \n     def __init__(self, size=(2, 2), dim_ordering='th', **kwargs):\n@@ -1007,6 +1033,12 @@ class UpSampling3D(Layer):\n \n     Note: this layer will only work with Theano for the time being.\n \n+    # Arguments\n+        size: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 4.\n+\n     # Input shape\n         5D tensor with shape:\n         `(samples, channels, dim1, dim2, dim3)` if dim_ordering='th'\n@@ -1018,12 +1050,6 @@ class UpSampling3D(Layer):\n         `(samples, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)` if dim_ordering='th'\n         or 5D tensor with shape:\n         `(samples, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)` if dim_ordering='tf'.\n-\n-    # Arguments\n-        size: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n-        dim_ordering: 'th' or 'tf'.\n-            In 'th' mode, the channels dimension (the depth)\n-            is at index 1, in 'tf' mode is it at index 4.\n     '''\n \n     def __init__(self, size=(2, 2, 2), dim_ordering='th', **kwargs):\n@@ -1065,16 +1091,16 @@ class UpSampling3D(Layer):\n class ZeroPadding1D(Layer):\n     '''Zero-padding layer for 1D input (e.g. temporal sequence).\n \n+    # Arguments\n+        padding: int\n+            How many zeros to add at the beginning and end of\n+            the padding dimension (axis 1).\n+\n     # Input shape\n         3D tensor with shape (samples, axis_to_pad, features)\n \n     # Output shape\n         3D tensor with shape (samples, padded_axis, features)\n-\n-    # Arguments\n-        padding: int\n-            How many zeros to add at the beginning and end of\n-            the padding dimension (axis 1).\n     '''\n \n     def __init__(self, padding=1, **kwargs):\n@@ -1100,6 +1126,11 @@ class ZeroPadding1D(Layer):\n class ZeroPadding2D(Layer):\n     '''Zero-padding layer for 2D input (e.g. picture).\n \n+    # Arguments\n+        padding: tuple of int (length 2)\n+            How many zeros to add at the beginning and end of\n+            the 2 padding dimensions (axis 3 and 4).\n+\n     # Input shape\n         4D tensor with shape:\n         (samples, depth, first_axis_to_pad, second_axis_to_pad)\n@@ -1107,11 +1138,6 @@ class ZeroPadding2D(Layer):\n     # Output shape\n         4D tensor with shape:\n         (samples, depth, first_padded_axis, second_padded_axis)\n-\n-    # Arguments\n-        padding: tuple of int (length 2)\n-            How many zeros to add at the beginning and end of\n-            the 2 padding dimensions (axis 3 and 4).\n     '''\n \n     def __init__(self, padding=(1, 1), dim_ordering='th', **kwargs):\n@@ -1154,6 +1180,11 @@ class ZeroPadding3D(Layer):\n \n     Note: this layer will only work with Theano for the time being.\n \n+    # Arguments\n+        padding: tuple of int (length 3)\n+            How many zeros to add at the beginning and end of\n+            the 3 padding dimensions (axis 3, 4 and 5).\n+\n     # Input shape\n         5D tensor with shape:\n         (samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)\n@@ -1161,11 +1192,6 @@ class ZeroPadding3D(Layer):\n     # Output shape\n         5D tensor with shape:\n         (samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)\n-\n-    # Arguments\n-        padding: tuple of int (length 3)\n-            How many zeros to add at the beginning and end of\n-            the 3 padding dimensions (axis 3, 4 and 5).\n     '''\n \n     def __init__(self, padding=(1, 1, 1), dim_ordering='th', **kwargs):\n\n@@ -119,6 +119,20 @@ class Reshape(Layer):\n \n     # Output shape\n         `(batch_size,) + target_shape`\n+\n+    # Example\n+\n+    ```python\n+        # as first layer in a Sequential model\n+        model = Sequential()\n+        model.add(Reshape((3, 4), input_shape=(12,)))\n+        # now: model.output_shape == (None, 3, 4)\n+        # note: `None` is the batch dimension\n+\n+        # as intermediate layer in a Sequential model\n+        model.add(Reshape((6, 2)))\n+        # now: model.output_shape == (None, 6, 2)\n+    ```\n     '''\n     def __init__(self, target_shape, **kwargs):\n         super(Reshape, self).__init__(**kwargs)\n@@ -201,6 +215,15 @@ class Permute(Layer):\n \n     Useful for e.g. connecting RNNs and convnets together.\n \n+    # Example\n+\n+    ```python\n+        model = Sequential()\n+        model.add(Permute((2, 1), input_shape=(10, 64)))\n+        # now: model.output_shape == (None, 64, 10)\n+        # note: `None` is the batch dimension\n+    ```\n+\n     # Arguments\n         dims: Tuple of integers. Permutation pattern, does not include the\n             samples dimension. Indexing starts at 1.\n@@ -240,6 +263,17 @@ class Permute(Layer):\n class Flatten(Layer):\n     '''Flattens the input. Does not affect the batch size.\n \n+    # Example\n+\n+    ```python\n+        model = Sequential()\n+        model.add(Convolution2D(64, 3, 3, border_mode='same', input_shape=(3, 32, 32)))\n+        # now: model.output_shape == (None, 64, 32, 32)\n+\n+        model.add(Flatten())\n+        # now: model.output_shape == (None, 65536)\n+    ```\n+\n     # Input shape\n         Arbitrary, although all dimensions in the input shape must be fixed.\n         Use the keyword argument `input_shape`\n@@ -268,7 +302,19 @@ class Flatten(Layer):\n \n \n class RepeatVector(Layer):\n-    '''Repeat the input n times.\n+    '''Repeats the input n times.\n+\n+    # Example\n+\n+    ```python\n+        model = Sequential()\n+        model.add(Dense(32, input_dim=32))\n+        # now: model.output_shape == (None, 32)\n+        # note: `None` is the batch dimension\n+\n+        model.add(RepeatVector(3))\n+        # now: model.output_shape == (None, 3, 32)\n+    ```\n \n     # Arguments\n         n: integer, repetition factor.\n@@ -300,22 +346,6 @@ class Lambda(Layer):\n     '''Used for evaluating an arbitrary Theano / TensorFlow expression\n     on the output of the previous layer.\n \n-    # Arguments\n-        function: The function to be evaluated.\n-            Takes one argument: the output of previous layer\n-        output_shape: Expected output shape from function.\n-            Could be a tuple or a function of the shape of the input\n-        arguments: optional dictionary of keyword arguments to be passed\n-            to the function.\n-\n-    # Input shape\n-        Arbitrary. Use the keyword argument input_shape\n-        (tuple of integers, does not include the samples axis)\n-        when using this layer as the first layer in a model.\n-\n-    # Output shape\n-        Specified by `output_shape` argument.\n-\n     # Examples\n \n     ```python\n@@ -342,6 +372,22 @@ class Lambda(Layer):\n \n         model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape))\n     ```\n+\n+    # Arguments\n+        function: The function to be evaluated.\n+            Takes one argument: the output of previous layer\n+        output_shape: Expected output shape from function.\n+            Could be a tuple or a function of the shape of the input\n+        arguments: optional dictionary of keyword arguments to be passed\n+            to the function.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument input_shape\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Specified by `output_shape` argument.\n     '''\n     def __init__(self, function, output_shape=None, arguments={}, **kwargs):\n         self.function = function\n@@ -451,6 +497,22 @@ class Lambda(Layer):\n class Dense(Layer):\n     '''Just your regular fully connected NN layer.\n \n+    # Example\n+\n+    ```python\n+        # as first layer in a sequential model:\n+        model = Sequential(Dense(32, input_dim=16))\n+        # now the model will take as input arrays of shape (*, 16)\n+        # and output arrays of shape (*, 32)\n+\n+        # this is equivalent to the above:\n+        model = Sequential(Dense(32, input_shape=(16,)))\n+\n+        # after the first layer, you don't need to specify\n+        # the size of the input anymore:\n+        model.add(Dense(32))\n+    ```\n+\n     # Arguments\n         output_dim: int > 0.\n         init: name of initialization function for the weights of the layer\n@@ -859,6 +921,12 @@ class Highway(Layer):\n class TimeDistributedDense(Layer):\n     '''Apply a same Dense layer for each dimension[1] (time_dimension) input.\n     Especially useful after a recurrent network with 'return_sequence=True'.\n+\n+    Note: this layer is deprecated, prefer using the `TimeDistributed` wrapper:\n+    ```python\n+        model.add(TimeDistributed(Dense(32)))\n+    ```\n+\n     # Input shape\n         3D tensor with shape `(nb_sample, time_dimension, input_dim)`.\n     # Output shape\n\n@@ -11,11 +11,21 @@ class Embedding(Layer):\n \n     This layer can only be used as the first layer in a model.\n \n-    # Input shape\n-        2D tensor with shape: `(nb_samples, sequence_length)`.\n+    # Example\n \n-    # Output shape\n-        3D tensor with shape: `(nb_samples, sequence_length, output_dim)`.\n+    ```python\n+      model = Sequential()\n+      model.add(Embedding(1000, 64, input_length=10))\n+      # the model will take as input an integer matrix of size (batch, input_length).\n+      # the largest integer (i.e. word index) in the input should be no larger than 1000 (vocabulary size).\n+      # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n+\n+      input_array = np.random.randint(1000, size=(32, 10))\n+\n+      model.compile('rmsprop', 'mse')\n+      output_array = model.predict(input_array)\n+      assert output_array.shape == (32, 10, 64)\n+    ```\n \n     # Arguments\n       input_dim: int >= 0. Size of the vocabulary, ie.\n@@ -42,6 +52,12 @@ class Embedding(Layer):\n           (without it, the shape of the dense outputs cannot be computed).\n       dropout: float between 0 and 1. Fraction of the embeddings to drop.\n \n+    # Input shape\n+        2D tensor with shape: `(nb_samples, sequence_length)`.\n+\n+    # Output shape\n+        3D tensor with shape: `(nb_samples, sequence_length, output_dim)`.\n+\n     # References\n         - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     '''\n\n@@ -12,6 +12,9 @@ class GaussianNoise(Layer):\n \n     As it is a regularization layer, it is only active at training time.\n \n+    # Arguments\n+        sigma: float, standard deviation of the noise distribution.\n+\n     # Input shape\n         Arbitrary. Use the keyword argument `input_shape`\n         (tuple of integers, does not include the samples axis)\n@@ -19,9 +22,6 @@ class GaussianNoise(Layer):\n \n     # Output shape\n         Same shape as input.\n-\n-    # Arguments\n-        sigma: float, standard deviation of the noise distribution.\n     '''\n     def __init__(self, sigma, **kwargs):\n         self.supports_masking = True\n@@ -50,6 +50,14 @@ class GaussianDropout(Layer):\n     # Arguments\n         p: float, drop probability (as with `Dropout`).\n \n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n     # References:\n         [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     '''\n\n@@ -8,14 +8,6 @@ class BatchNormalization(Layer):\n     i.e. applies a transformation that maintains the mean activation\n     close to 0 and the activation standard deviation close to 1.\n \n-    # Input shape\n-        Arbitrary. Use the keyword argument `input_shape`\n-        (tuple of integers, does not include the samples axis)\n-        when using this layer as the first layer in a model.\n-\n-    # Output shape\n-        Same shape as input.\n-\n     # Arguments\n         epsilon: small float > 0. Fuzz parameter.\n         mode: integer, 0 or 1.\n@@ -45,6 +37,15 @@ class BatchNormalization(Layer):\n             [initializations](../initializations.md)), or alternatively,\n             Theano/TensorFlow function to use for weights initialization.\n             This parameter is only relevant if you don't pass a `weights` argument.\n+\n+    # Input shape\n+        Arbitrary. Use the keyword argument `input_shape`\n+        (tuple of integers, does not include the samples axis)\n+        when using this layer as the first layer in a model.\n+\n+    # Output shape\n+        Same shape as input.\n+\n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)\n     '''\n\n@@ -47,6 +47,23 @@ class Recurrent(Layer):\n     follow the specifications of this class and accept\n     the keyword arguments listed below.\n \n+    # Example\n+\n+    ```python\n+        # as the first layer in a Sequential model\n+        model = Sequential()\n+        model.add(LSTM(32, input_shape=(10, 64)))\n+        # now model.output_shape == (None, 10, 32)\n+        # note: `None` is the batch dimension.\n+\n+        # the following is identical:\n+        model = Sequential()\n+        model.add(LSTM(32, input_dim=64, input_length=10))\n+\n+        # for subsequent layers, not need to specify the input size:\n+        model.add(LSTM(16))\n+    ```\n+\n     # Arguments\n         weights: list of numpy arrays to set as initial weights.\n             The list should have 3 elements, of shapes:\n\n@@ -53,8 +53,14 @@ class TimeDistributed(Wrapper):\n \n     You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10 timesteps, independently:\n     ```python\n+        # as the first layer in a model\n         model = Sequential()\n         model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n+        # now model.output_shape == (None, 10, 8)\n+\n+        # subsequent layers: no need for input_shape\n+        model.add(TimeDistributed(Dense(32)))\n+        # now model.output_shape == (None, 10, 32)\n     ```\n \n     The output will then have shape `(32, 10, 8)`.\n\n@@ -256,6 +256,9 @@ class Sequential(Model):\n         return self._gather_dict_attr('constraints')\n \n     def get_weights(self):\n+        '''Returns the weights of the model,\n+        as a flat list of Numpy arrays.\n+        '''\n         # support for legacy behavior\n         weights = []\n         for layer in self.flattened_layers:\n@@ -263,6 +266,11 @@ class Sequential(Model):\n         return weights\n \n     def set_weights(self, weights):\n+        '''Sets the weights of the model.\n+        The `weights` argument should be a list\n+        of Numpy arrays with shapes and types matching\n+        the output of `model.get_weights()`.\n+        '''\n         # support for legacy behavior\n         for layer in self.flattened_layers:\n             nb_param = len(layer.get_weights())\n@@ -421,9 +429,22 @@ class Sequential(Model):\n                                    sample_weight=sample_weight)\n \n     def predict(self, x, batch_size=32, verbose=0):\n+        '''Generates output predictions for the input samples,\n+        processing the samples in a batched way.\n+\n+        # Arguments\n+            x: the input data, as a Numpy array.\n+            batch_size: integer.\n+            verbose: verbosity mode, 0 or 1.\n+\n+        # Returns\n+            A Numpy array of predictions.\n+        '''\n         return self.model.predict(x, batch_size=batch_size, verbose=verbose)\n \n     def predict_on_batch(self, x):\n+        '''Returns predictions for a single batch of samples.\n+        '''\n         return self.model.predict_on_batch(x)\n \n     def train_on_batch(self, x, y, class_weight=None,\n@@ -639,18 +660,8 @@ class Sequential(Model):\n                                              val_samples)\n \n     def get_config(self):\n-        '''\n-        how to handle Merge layers:\n-            encoding:\n-                if first layer is Merge:\n-                    - get merge config (no connectivity)\n-                    - get config of Merge input layers\n-                    - insert into merge config as 'layers' (for backwards compatibility)\n-            decoding:\n-                if first layer is Merge:\n-                    - get config['layers']\n-                    - instantiate input layers\n-                    - merge them\n+        '''Returns the model configuration\n+        as a Python dictionary.\n         '''\n         config = []\n         if self.layers[0].__class__.__name__ == 'Merge':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
