{"custom_id": "scrapy#d0edad4b0bd93ed34a680ddd6563387be7797128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 28 | Churn Cumulative: 206 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,12 +2,11 @@\n Base class for Scrapy commands\n \"\"\"\n import os\n-import warnings\n from optparse import OptionGroup\n from twisted.python import failure\n \n from scrapy.utils.conf import arglist_to_dict\n-from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n+from scrapy.exceptions import UsageError\n \n \n class ScrapyCommand(object):\n@@ -27,31 +26,6 @@ class ScrapyCommand(object):\n         assert not hasattr(self, '_crawler'), \"crawler already set\"\n         self._crawler = crawler\n \n-    @property\n-    def crawler(self):\n-        warnings.warn(\"Command's default `crawler` is deprecated and will be removed. \"\n-            \"Use `create_crawler` method to instatiate crawlers.\",\n-            ScrapyDeprecationWarning)\n-\n-        if not hasattr(self, '_crawler'):\n-            crawler = self.crawler_process.create_crawler()\n-\n-            old_start = crawler.start\n-            self.crawler_process.started = False\n-\n-            def wrapped_start():\n-                if self.crawler_process.started:\n-                    old_start()\n-                else:\n-                    self.crawler_process.started = True\n-                    self.crawler_process.start()\n-\n-            crawler.start = wrapped_start\n-\n-            self.set_crawler(crawler)\n-\n-        return self._crawler\n-\n     def syntax(self):\n         \"\"\"\n         Command syntax (preferably one-line). Do not include command name.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#89df18bd2bc6fdc7f2084454a2c69f4db03008ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 41 | Files Changed: 8 | Hunks: 17 | Methods Changed: 10 | Complexity Δ (Sum/Max): 2/7 | Churn Δ: 77 | Churn Cumulative: 1281 | Contributors (this commit): 12 | Commits (past 90d): 20 | Contributors (cumulative): 40 | DMM Complexity: 0.5\n\nDIFF:\n@@ -22,9 +22,7 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         with _BenchServer():\n-            spider = _BenchSpider(total=100000)\n-            crawler = self.crawler_process.create_crawler()\n-            crawler.crawl(spider)\n+            self.crawler_process.crawl(_BenchSpider, total=100000)\n             self.crawler_process.start()\n \n \n\n@@ -69,20 +69,18 @@ class Command(ScrapyCommand):\n         # contract requests\n         contract_reqs = defaultdict(list)\n \n-        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n-        spiders = spman_cls.from_settings(self.settings)\n+        spiders = self.crawler_process.spiders\n \n-        for spider in args or spiders.list():\n-            spider = spiders.create(spider)\n-            requests = self.get_requests(spider, conman, result)\n-            contract_reqs[spider.name] = []\n+        for spidername in args or spiders.list():\n+            spidercls = spiders.load(spidername)\n+            spidercls.start_requests = lambda s: conman.from_spider(s, result)\n \n+            tested_methods = conman.tested_methods_from_spidercls(spidercls)\n             if opts.list:\n-                for req in requests:\n-                    contract_reqs[spider.name].append(req.callback.__name__)\n-            elif requests:\n-                crawler = self.crawler_process.create_crawler(spider.name)\n-                crawler.crawl(spider, requests)\n+                for method in tested_methods:\n+                    contract_reqs[spidercls.name].append(method)\n+            elif tested_methods:\n+                self.crawler_process.crawl(spidercls)\n \n         # start checks\n         if opts.list:\n@@ -101,15 +99,3 @@ class Command(ScrapyCommand):\n             result.printSummary(start, stop)\n             self.exitcode = int(not result.wasSuccessful())\n \n-    def get_requests(self, spider, conman, result):\n-        requests = []\n-\n-        for key, value in vars(type(spider)).items():\n-            if callable(value) and value.__doc__:\n-                bound_method = value.__get__(spider, type(spider))\n-                request = conman.from_method(bound_method, result)\n-\n-                if request:\n-                    requests.append(request)\n-\n-        return requests\n\n@@ -54,7 +54,5 @@ class Command(ScrapyCommand):\n             raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n         spname = args[0]\n \n-        crawler = self.crawler_process.create_crawler()\n-        spider = crawler.spiders.create(spname, **opts.spargs)\n-        crawler.crawl(spider)\n+        self.crawler_process.crawl(spname, **opts.spargs)\n         self.crawler_process.start()\n\n@@ -25,13 +25,12 @@ class Command(ScrapyCommand):\n         if len(args) != 1:\n             raise UsageError()\n \n-        crawler = self.crawler_process.create_crawler()\n-        editor = crawler.settings['EDITOR']\n+        editor = self.settings['EDITOR']\n         try:\n-            spider = crawler.spiders.create(args[0])\n+            spidercls = self.crawler_process.spiders.load(args[0])\n         except KeyError:\n             return self._err(\"Spider not found: %s\" % args[0])\n \n-        sfile = sys.modules[spider.__module__].__file__\n+        sfile = sys.modules[spidercls.__module__].__file__\n         sfile = sfile.replace('.pyc', '.py')\n         self.exitcode = os.system('%s \"%s\"' % (editor, sfile))\n\n@@ -65,15 +65,14 @@ class Command(ScrapyCommand):\n             return\n \n         try:\n-            crawler = self.crawler_process.create_crawler()\n-            spider = crawler.spiders.create(name)\n+            spidercls = self.crawler_process.spiders.load(name)\n         except KeyError:\n             pass\n         else:\n             # if spider already exists and not --force then halt\n             if not opts.force:\n                 print(\"Spider %r already exists in module:\" % name)\n-                print(\"  %s\" % spider.__module__)\n+                print(\"  %s\" % spidercls.__module__)\n                 return\n         template_file = self._find_template(opts.template)\n         if template_file:\n\n@@ -10,6 +10,5 @@ class Command(ScrapyCommand):\n         return \"List available spiders\"\n \n     def run(self, args, opts):\n-        crawler = self.crawler_process.create_crawler()\n-        for s in sorted(crawler.spiders.list()):\n+        for s in sorted(self.crawler_process.spiders.list()):\n             print(s)\n\n@@ -83,8 +83,7 @@ class Command(ScrapyCommand):\n         spclasses = list(iter_spider_classes(module))\n         if not spclasses:\n             raise UsageError(\"No spider found in file: %s\\n\" % filename)\n-        spider = spclasses.pop()(**opts.spargs)\n+        spidercls = spclasses.pop()\n \n-        crawler = self.crawler_process.create_crawler()\n-        crawler.crawl(spider)\n+        self.crawler_process.crawl(spidercls, **opts.spargs)\n         self.crawler_process.start()\n\n@@ -15,6 +15,15 @@ class ContractsManager(object):\n         for contract in contracts:\n             self.contracts[contract.name] = contract\n \n+    def tested_methods_from_spidercls(self, spidercls):\n+        methods = []\n+        for key, value in vars(spidercls).items():\n+            if (callable(value) and value.__doc__ and\n+                    re.search(r'^\\s*@', value.__doc__, re.MULTILINE)):\n+                methods.append(key)\n+\n+        return methods\n+\n     def extract_contracts(self, method):\n         contracts = []\n         for line in method.__doc__.split('\\n'):\n@@ -28,6 +37,14 @@ class ContractsManager(object):\n \n         return contracts\n \n+    def from_spider(self, spider, results):\n+        requests = []\n+        for method in self.tested_methods_from_spidercls(type(spider)):\n+            bound_method = spider.__getattribute__(method)\n+            requests.append(self.from_method(bound_method, results))\n+\n+        return requests\n+\n     def from_method(self, method, results):\n         contracts = self.extract_contracts(method)\n         if contracts:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#900a487682b11696ccab5d18c9f13e0addd25f12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 15 | Methods Changed: 18 | Complexity Δ (Sum/Max): 11/6 | Churn Δ: 69 | Churn Cumulative: 1561 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 17 | DMM Complexity: 0.8275862068965517\n\nDIFF:\n@@ -128,6 +128,7 @@ class CrawlerProcess(CrawlerRunner):\n         signame = signal_names[signum]\n         log.msg(format='Received %(signame)s twice, forcing unclean shutdown',\n                 level=log.INFO, signame=signame)\n+        self._stop_logging()\n         reactor.callFromThread(self._stop_reactor)\n \n     def start(self, stop_after_crawl=True):\n@@ -135,6 +136,7 @@ class CrawlerProcess(CrawlerRunner):\n         self._start_reactor(stop_after_crawl)\n \n     def _start_logging(self):\n+        self.log_observer = log.start_from_settings(self.settings)\n         log.scrapy_info(self.settings)\n \n     def _start_reactor(self, stop_after_crawl=True):\n@@ -149,6 +151,9 @@ class CrawlerProcess(CrawlerRunner):\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n+    def _stop_logging(self):\n+        self.log_observer.stop()\n+\n     def _stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n\n@@ -35,15 +35,16 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n     def __init__(self, f, level=INFO, encoding='utf-8', crawler=None):\n         self.level = level\n         self.encoding = encoding\n-        if crawler:\n         self.crawler = crawler\n+        if crawler:\n             self.emit = self._emit_with_crawler\n         else:\n             self.emit = self._emit\n         log.FileLogObserver.__init__(self, f)\n \n     def _emit(self, eventDict):\n-        ev = _adapt_eventdict(eventDict, self.level, self.encoding)\n+        ev = _adapt_eventdict(eventDict, self.level, self.encoding,\n+                              self.crawler)\n         if ev is not None:\n             log.FileLogObserver.emit(self, ev)\n         return ev\n@@ -55,7 +56,8 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n             sname = 'log_count/%s' % level_names.get(level, level)\n             self.crawler.stats.inc_value(sname)\n \n-def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=True):\n+def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8',\n+                     crawler=None, prepend_level=True):\n     \"\"\"Adapt Twisted log eventDict making it suitable for logging with a Scrapy\n     log observer. It may return None to indicate that the event should be\n     ignored by a Scrapy log observer.\n@@ -78,6 +80,12 @@ def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=\n     spider = ev.get('spider')\n     if spider:\n         ev['system'] = unicode_to_str(spider.name, encoding)\n+    if crawler and (not spider or spider.crawler is not crawler):\n+        # ignore events not triggered by own spiders in crawlers' observers\n+        return\n+    if not crawler and spider:\n+        # ignore spiders' events in observers without crawler\n+        return\n \n     lvlname = level_names.get(level, 'NOLEVEL')\n     message = ev.get('message')\n@@ -140,8 +148,6 @@ def start_from_settings(settings, crawler=None):\n             settings['LOG_ENCODING'], crawler)\n \n def scrapy_info(settings):\n-    log_observer = start_from_settings(settings)\n-    if log_observer:\n     msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__,\n         settings['BOT_NAME']))\n \n@@ -151,7 +157,5 @@ def scrapy_info(settings):\n     d = dict(overridden_settings(settings))\n     msg(format=\"Overridden settings: %(settings)r\", settings=d, level=INFO)\n \n-        log_observer.stop()\n-\n def start_from_crawler(crawler):\n     return start_from_settings(crawler.settings, crawler)\n\n@@ -6,6 +6,7 @@ from twisted.trial import unittest\n from scrapy import log\n from scrapy.spider import Spider\n from scrapy.settings import default_settings\n+from scrapy.utils.test import get_crawler\n \n class LogTest(unittest.TestCase):\n \n@@ -40,10 +41,10 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n         log.msg(\"Hello\")\n         self.assertEqual(self.logged(), \"[scrapy] INFO: Hello\")\n \n-    def test_msg_spider(self):\n+    def test_msg_ignore_spider(self):\n         spider = Spider(\"myspider\")\n         log.msg(\"Hello\", spider=spider)\n-        self.assertEqual(self.logged(), \"[myspider] INFO: Hello\")\n+        self.failIf(self.logged())\n \n     def test_msg_level1(self):\n         log.msg(\"Hello\", level=log.WARNING)\n@@ -57,11 +58,6 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n         log.msg(\"Hello\", level=9999)\n         self.assertEqual(self.logged(), \"[scrapy] NOLEVEL: Hello\")\n \n-    def test_msg_level_spider(self):\n-        spider = Spider(\"myspider\")\n-        log.msg(\"Hello\", spider=spider, level=log.WARNING)\n-        self.assertEqual(self.logged(), \"[myspider] WARNING: Hello\")\n-\n     def test_msg_encoding(self):\n         log.msg(u\"Price: \\xa3100\")\n         self.assertEqual(self.logged(), \"[scrapy] INFO: Price: \\xc2\\xa3100\")\n@@ -133,5 +129,41 @@ class Latin1ScrapyFileLogObserverTest(ScrapyFileLogObserverTest):\n #        self.assertEqual(self.first_log_line(), \"[scrapy] ERROR: \\xa3\")\n \n \n+class CrawlerScrapyFileLogObserverTest(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.f = BytesIO()\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler.spider = self.crawler._create_spider('test')\n+        self.log_observer = log.ScrapyFileLogObserver(self.f, log.INFO,\n+                                                      'utf-8', self.crawler)\n+        self.log_observer.start()\n+\n+    def tearDown(self):\n+        self.flushLoggedErrors()\n+        self.log_observer.stop()\n+\n+    def logged(self):\n+        return self.f.getvalue().strip()[25:]\n+\n+    def test_msg_basic(self):\n+        log.msg(\"Hello\", spider=self.spider)\n+        self.assertEqual(self.logged(), \"[test] INFO: Hello\")\n+\n+    def test_msg_ignore_scrapy_channel(self):\n+        log.msg(\"Hello\")\n+        self.failIf(self.logged())\n+\n+    def test_msg_ignore_another_crawler(self):\n+        crawler = get_crawler(Spider)\n+        log.msg(\"Hello\", spider=crawler._create_spider('test'))\n+        self.failIf(self.logged())\n+\n+    def test_msg_stats_log(self):\n+        assert self.crawler.stats.get_value('log_count/INFO', 0) == 0\n+        log.msg(\"Hello\", spider=self.spider)\n+        self.assertEqual(self.crawler.stats.get_value('log_count/INFO'), 1)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cbbfd8b04835c40568b687ef8b13d901db988cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 50 | Files Changed: 5 | Hunks: 37 | Methods Changed: 15 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 114 | Churn Cumulative: 1513 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,9 +3,8 @@ from w3lib.url import is_url\n \n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n-from scrapy.spider import Spider\n from scrapy.exceptions import UsageError\n-from scrapy.utils.spider import create_spider_for_request\n+from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n class Command(ScrapyCommand):\n \n@@ -48,12 +47,11 @@ class Command(ScrapyCommand):\n         request = Request(args[0], callback=cb, dont_filter=True)\n         request.meta['handle_httpstatus_all'] = True\n \n-        crawler = self.crawler_process.create_crawler()\n-        spider = None\n+        spidercls = DefaultSpider\n+        spiders = self.crawler_process.spiders\n         if opts.spider:\n-            spider = crawler.spiders.create(opts.spider)\n+            spidercls = spiders.load(opts.spider)\n         else:\n-            spider = create_spider_for_request(crawler.spiders, request, \\\n-                default_spider=Spider('default'))\n-        crawler.crawl(spider, [request])\n+            spidercls = spidercls_for_request(spiders, request, spidercls)\n+        self.crawler_process.crawl(spidercls, start_requests=lambda: [request])\n         self.crawler_process.start()\n\n@@ -5,7 +5,7 @@ from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.utils import display\n from scrapy.utils.conf import arglist_to_dict\n-from scrapy.utils.spider import iterate_spider_output, create_spider_for_request\n+from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n from scrapy.exceptions import UsageError\n from scrapy import log\n \n@@ -113,41 +113,45 @@ class Command(ScrapyCommand):\n                 requests.append(x)\n         return items, requests\n \n-    def get_callback_from_rules(self, response):\n-        if getattr(self.spider, 'rules', None):\n-            for rule in self.spider.rules:\n+    def get_callback_from_rules(self, spider, response):\n+        if getattr(spider, 'rules', None):\n+            for rule in spider.rules:\n                 if rule.link_extractor.matches(response.url) and rule.callback:\n                     return rule.callback\n         else:\n             log.msg(format='No CrawlSpider rules found in spider %(spider)r, '\n                            'please specify a callback to use for parsing',\n-                    level=log.ERROR, spider=self.spider.name)\n+                    level=log.ERROR, spider=spider.name)\n \n-    def set_spider(self, url, opts):\n+    def set_spidercls(self, url, opts):\n+        spiders = self.crawler_process.spiders\n         if opts.spider:\n             try:\n-                self.spider = self.pcrawler.spiders.create(opts.spider, **opts.spargs)\n+                self.spidercls = spiders.load(opts.spider)\n             except KeyError:\n                 log.msg(format='Unable to find spider: %(spider)s',\n                         level=log.ERROR, spider=opts.spider)\n         else:\n-            self.spider = create_spider_for_request(self.pcrawler.spiders, Request(url), **opts.spargs)\n-            if not self.spider:\n+            self.spidercls = spidercls_for_request(spiders, Request(url))\n+            if not self.spidercls:\n                 log.msg(format='Unable to find spider for: %(url)s',\n                         level=log.ERROR, url=url)\n \n-    def start_parsing(self, url, opts):\n         request = Request(url, opts.callback)\n-        request = self.prepare_request(request, opts)\n+        _start_requests = lambda s: [self.prepare_request(s, request, opts)]\n+        self.spidercls.start_requests = _start_requests\n \n-        self.pcrawler.crawl(self.spider, [request])\n+\n+    def start_parsing(self, url, opts):\n+        self.crawler_process.crawl(self.spidercls, **opts.spargs)\n+        self.pcrawler = list(self.crawler_process.crawlers)[0]\n         self.crawler_process.start()\n \n         if not self.first_response:\n-            log.msg(format='No response downloaded for: %(request)s',\n-                    level=log.ERROR, request=request)\n+            log.msg(format='No response downloaded for: %(url)s',\n+                    level=log.ERROR, url=url)\n \n-    def prepare_request(self, request, opts):\n+    def prepare_request(self, spider, request, opts):\n         def callback(response):\n             # memorize first request\n             if not self.first_response:\n@@ -157,17 +161,17 @@ class Command(ScrapyCommand):\n             cb = response.meta['_callback']\n             if not cb:\n                 if opts.rules and self.first_response == response:\n-                    cb = self.get_callback_from_rules(response)\n+                    cb = self.get_callback_from_rules(spider, response)\n                 else:\n                     cb = 'parse'\n \n             if not callable(cb):\n-                cb_method = getattr(self.spider, cb, None)\n+                cb_method = getattr(spider, cb, None)\n                 if callable(cb_method):\n                     cb = cb_method\n                 else:\n                     log.msg(format='Cannot find callback %(callback)r in spider: %(spider)s',\n-                            callback=callback, spider=self.spider.name, level=log.ERROR)\n+                            callback=callback, spider=spider.name, level=log.ERROR)\n                     return\n \n             # parse items and requests\n@@ -177,7 +181,7 @@ class Command(ScrapyCommand):\n             if opts.pipelines:\n                 itemproc = self.pcrawler.engine.scraper.itemproc\n                 for item in items:\n-                    itemproc.process_item(item, self.spider)\n+                    itemproc.process_item(item, spider)\n             self.add_items(depth, items)\n             self.add_requests(depth, requests)\n \n@@ -207,10 +211,9 @@ class Command(ScrapyCommand):\n         else:\n             url = args[0]\n \n-        # prepare spider\n-        self.pcrawler = self.crawler_process.create_crawler()\n-        self.set_spider(url, opts)\n+        # prepare spidercls\n+        self.set_spidercls(url, opts)\n \n-        if self.spider and opts.depth > 0:\n+        if self.spidercls and opts.depth > 0:\n             self.start_parsing(url, opts)\n             self.print_results(opts)\n\n@@ -8,6 +8,9 @@ from threading import Thread\n \n from scrapy.command import ScrapyCommand\n from scrapy.shell import Shell\n+from scrapy.http import Request\n+from scrapy import log\n+from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n \n \n class Command(ScrapyCommand):\n@@ -38,18 +41,27 @@ class Command(ScrapyCommand):\n         pass\n \n     def run(self, args, opts):\n-        crawler = self.crawler_process.create_crawler()\n-\n         url = args[0] if args else None\n-        spider = crawler.spiders.create(opts.spider) if opts.spider else None\n+        spiders = self.crawler_process.spiders\n \n-        self.crawler_process.start_crawling()\n+        spidercls = DefaultSpider\n+        if opts.spider:\n+            spidercls = spiders.load(opts.spider)\n+        elif url:\n+            spidercls = spidercls_for_request(spiders, Request(url),\n+                                              spidercls, log_multiple=True)\n+        crawler = self.crawler_process._create_logged_crawler(spidercls)\n+        crawler.engine = crawler._create_engine()\n+        crawler.engine.start()\n+\n+        self.crawler_process._start_logging()\n         self._start_crawler_thread()\n \n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n-        shell.start(url=url, spider=spider)\n+        shell.start(url=url)\n \n     def _start_crawler_thread(self):\n-        t = Thread(target=self.crawler_process.start_reactor)\n+        t = Thread(target=self.crawler_process._start_reactor,\n+                   kwargs={'stop_after_crawl': False})\n         t.daemon = True\n         t.start()\n\n@@ -21,7 +21,6 @@ from scrapy.spider import Spider\n from scrapy.utils.console import start_python_console\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n-from scrapy.utils.spider import create_spider_for_request\n \n \n class Shell(object):\n@@ -67,11 +66,9 @@ class Shell(object):\n             return self.spider\n \n         if spider is None:\n-            spider = create_spider_for_request(self.crawler.spiders,\n-                                               request,\n-                                               Spider('default'),\n-                                               log_multiple=True)\n-        spider.set_crawler(self.crawler)\n+            spider = self.crawler.spider or self.crawler._create_spider()\n+\n+        self.crawler.spider = spider\n         self.crawler.engine.open_spider(spider, close_if_idle=False)\n         self.spider = spider\n         return spider\n\n@@ -4,6 +4,7 @@ import six\n \n from scrapy import log\n from scrapy.item import BaseItem\n+from scrapy.spider import Spider\n from scrapy.utils.misc import  arg_to_iter\n \n \n@@ -25,21 +26,21 @@ def iter_spider_classes(module):\n            getattr(obj, 'name', None):\n             yield obj\n \n-def create_spider_for_request(spidermanager, request, default_spider=None, \\\n-        log_none=False, log_multiple=False, **spider_kwargs):\n-    \"\"\"Create a spider to handle the given Request.\n+def spidercls_for_request(spidermanager, request, default_spidercls=None,\n+                          log_none=False, log_multiple=False):\n+    \"\"\"Return a spider class that handles the given Request.\n \n     This will look for the spiders that can handle the given request (using\n-    the spider manager) and return a (new) Spider if (and only if) there is\n+    the spider manager) and return a Spider class if (and only if) there is\n     only one Spider able to handle the Request.\n \n     If multiple spiders (or no spider) are found, it will return the\n-    default_spider passed. It can optionally log if multiple or no spiders\n+    default_spidercls passed. It can optionally log if multiple or no spiders\n     are found.\n     \"\"\"\n     snames = spidermanager.find_by_request(request)\n     if len(snames) == 1:\n-        return spidermanager.create(snames[0], **spider_kwargs)\n+        return spidermanager.load(snames[0])\n \n     if len(snames) > 1 and log_multiple:\n         log.msg(format='More than one spider can handle: %(request)s - %(snames)s',\n@@ -49,5 +50,8 @@ def create_spider_for_request(spidermanager, request, default_spider=None, \\\n         log.msg(format='Unable to find spider that handles: %(request)s',\n                 level=log.ERROR, request=request)\n \n-    return default_spider\n+    return default_spidercls\n \n+\n+class DefaultSpider(Spider):\n+    name = 'default'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c90977ca98dd51b93d91739115d843f44e6a8a94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 34 | Files Changed: 7 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 56 | Churn Cumulative: 1929 | Contributors (this commit): 12 | Commits (past 90d): 20 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,7 @@ from twisted.python import log\n \n from scrapy import optional_features\n \n-collect_ignore = [\"scrapy/stats.py\"]\n+collect_ignore = [\"scrapy/stats.py\", \"scrapy/project.py\"]\n if 'django' not in optional_features:\n     collect_ignore.append(\"tests/test_djangoitem/models.py\")\n \n\n@@ -31,18 +31,6 @@ class Crawler(object):\n         self.spider = None\n         self.engine = None\n \n-    def install(self):\n-        # TODO: remove together with scrapy.project.crawler usage\n-        import scrapy.project\n-        assert not hasattr(scrapy.project, 'crawler'), \"crawler already installed\"\n-        scrapy.project.crawler = self\n-\n-    def uninstall(self):\n-        # TODO: remove together with scrapy.project.crawler usage\n-        import scrapy.project\n-        assert hasattr(scrapy.project, 'crawler'), \"crawler not installed\"\n-        del scrapy.project.crawler\n-\n     @defer.inlineCallbacks\n     def crawl(self, *args, **kwargs):\n         assert not self.crawling, \"Crawling already taking place\"\n@@ -84,9 +72,6 @@ class CrawlerRunner(object):\n         crawler = self._create_logged_crawler(spidercls)\n         self.crawlers.add(crawler)\n \n-        crawler.install()\n-        crawler.signals.connect(crawler.uninstall, signals.engine_stopped)\n-\n         d = crawler.crawl(*args, **kwargs)\n         self.crawl_deferreds.add(d)\n         return d\n\n@@ -1,13 +1,17 @@\n-\"\"\"\n---------- WARNING: THIS MODULE IS DEPRECATED -----------\n \n-This module is deprecated. If you want to get the Scrapy crawler from your\n-extension, middleware or pipeline implement the `from_crawler` class method.\n+\"\"\"\n+Obsolete module, kept for giving a meaningful error message when trying to\n+import.\n+\"\"\"\n+\n+raise ImportError(\"\"\"scrapy.project usage has become obsolete.\n+\n+If you want to get the Scrapy crawler from your extension, middleware or\n+pipeline implement the `from_crawler` class method (or look up for extending\n+components that have already done it, such as spiders).\n \n For example:\n \n     @classmethod\n     def from_crawler(cls, crawler):\n-        return cls(crawler)\n-\n-\"\"\"\n+        return cls(crawler)\"\"\")\n\n@@ -123,10 +123,9 @@ class Shell(object):\n         return isinstance(value, self.relevant_classes)\n \n \n-def inspect_response(response, spider=None):\n+def inspect_response(response, spider):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n-    from scrapy.project import crawler\n-    Shell(crawler).start(response=response, spider=spider)\n+    Shell(spider.crawler).start(response=response)\n \n \n def _request_deferred(request):\n\n@@ -93,6 +93,6 @@ class ObsoleteClass(object):\n         raise AttributeError(self.message)\n \n spiders = ObsoleteClass(\"\"\"\n-\"from scrapy.spider import spiders\" no longer works - use \"from scrapy.project import crawler\" and then access crawler.spiders attribute\"\n+\"from scrapy.spider import spiders\" no longer works - use \"from scrapy.spidermanager import SpiderManager\" and instantiate it with your project settings\"\n \"\"\")\n \n\n@@ -1,7 +1,8 @@\n-from scrapy.project import crawler\n-stats = crawler.stats\n \n-import warnings\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-warnings.warn(\"Module `scrapy.stats` is deprecated, use `crawler.stats` attribute instead\",\n-    ScrapyDeprecationWarning, stacklevel=2)\n+\"\"\"\n+Obsolete module, kept for giving a meaningful error message when trying to\n+import.\n+\"\"\"\n+\n+raise ImportError(\"scrapy.stats usage has become obsolete, use \"\n+                  \"`crawler.stats` attribute instead\")\n\n@@ -93,7 +93,6 @@ class CrawlerRun(object):\n                 dispatcher.connect(self.record_signal, signal)\n \n         self.crawler = get_crawler(TestSpider)\n-        self.crawler.install()\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n@@ -109,7 +108,6 @@ class CrawlerRun(object):\n         for name, signal in vars(signals).items():\n             if not name.startswith('_'):\n                 disconnect_all(signal)\n-        self.crawler.uninstall()\n         self.deferred.callback(None)\n \n     def geturl(self, path):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#419026615f040d6277e311b9307a3950f8532468", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 42 | Churn Cumulative: 1032 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import six\n import signal\n+import warnings\n \n from twisted.internet import reactor, defer\n \n@@ -7,6 +8,7 @@ from scrapy.core.engine import ExecutionEngine\n from scrapy.resolver import CachingThreadedResolver\n from scrapy.extension import ExtensionManager\n from scrapy.signalmanager import SignalManager\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n from scrapy import log, signals\n@@ -23,14 +25,22 @@ class Crawler(object):\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n \n-        # Attribute kept for backward compatibility (Use CrawlerRunner.spiders)\n-        spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n-        self.spiders = spman_cls.from_settings(self.settings)\n-\n         self.crawling = False\n         self.spider = None\n         self.engine = None\n \n+    @property\n+    def spiders(self):\n+        if not hasattr(self, '_spiders'):\n+            warnings.warn(\"Crawler.spiders is deprecated, use \"\n+                          \"CrawlerRunner.spiders or instantiate \"\n+                          \"scrapy.spidermanager.SpiderManager with your \"\n+                          \"settings.\",\n+                          category=ScrapyDeprecationWarning, stacklevel=2)\n+            spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n+            self._spiders = spman_cls.from_settings(self.settings)\n+        return self._spiders\n+\n     @defer.inlineCallbacks\n     def crawl(self, *args, **kwargs):\n         assert not self.crawling, \"Crawling already taking place\"\n\n@@ -0,0 +1,24 @@\n+import warnings\n+import unittest\n+\n+from scrapy.crawler import Crawler\n+from scrapy.settings import Settings\n+from scrapy.utils.spider import DefaultSpider\n+from scrapy.utils.misc import load_object\n+\n+\n+class CrawlerTestCase(unittest.TestCase):\n+\n+    def setUp(self):\n+        self.crawler = Crawler(DefaultSpider, Settings())\n+\n+    def test_deprecated_attribute_spiders(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            spiders = self.crawler.spiders\n+            self.assertEqual(len(w), 1)\n+            self.assertIn(\"Crawler.spiders\", str(w[0].message))\n+            sm_cls = load_object(self.crawler.settings['SPIDER_MANAGER_CLASS'])\n+            self.assertIsInstance(spiders, sm_cls)\n+\n+            self.crawler.spiders\n+            self.assertEqual(len(w), 1, \"Warn deprecated access only once\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#70f2010db17b3796c3edce9c6ca3e00c092d6064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 384 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,7 +88,7 @@ class Settings(object):\n         return dict(value)\n \n     def set(self, name, value, priority='project'):\n-        assert not self.frozen, \"Trying to modify an immutable Settings object\"\n+        self._assert_mutability()\n         if isinstance(priority, six.string_types):\n             priority = SETTINGS_PRIORITIES[priority]\n         if name not in self.attributes:\n@@ -97,18 +97,22 @@ class Settings(object):\n             self.attributes[name].set(value, priority)\n \n     def setdict(self, values, priority='project'):\n-        assert not self.frozen, \"Trying to modify an immutable Settings object\"\n+        self._assert_mutability()\n         for name, value in six.iteritems(values):\n             self.set(name, value, priority)\n \n     def setmodule(self, module, priority='project'):\n-        assert not self.frozen, \"Trying to modify an immutable Settings object\"\n+        self._assert_mutability()\n         if isinstance(module, six.string_types):\n             module = import_module(module)\n         for key in dir(module):\n             if key.isupper():\n                 self.set(key, getattr(module, key), priority)\n \n+    def _assert_mutability(self):\n+        if self.frozen:\n+            raise TypeError(\"Trying to modify an immutable Settings object\")\n+\n     def copy(self):\n         return copy.deepcopy(self)\n \n\n@@ -213,7 +213,7 @@ class SettingsTest(unittest.TestCase):\n \n     def test_freeze(self):\n         self.settings.freeze()\n-        with self.assertRaises(AssertionError) as cm:\n+        with self.assertRaises(TypeError) as cm:\n             self.settings.set('TEST_BOOL', False)\n             self.assertEqual(str(cm.exception),\n                              \"Trying to modify an immutable Settings object\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6339864f95d4ecaed5e428b342277dce9457d954", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 1146 | Contributors (this commit): 7 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -50,11 +50,15 @@ class Command(ScrapyCommand):\n         elif url:\n             spidercls = spidercls_for_request(spiders, Request(url),\n                                               spidercls, log_multiple=True)\n+\n+        # The crawler is created this way since the Shell manually handles the\n+        # crawling engine, so the set up in the crawl method won't work\n         crawler = self.crawler_process._create_logged_crawler(spidercls)\n+        # The Shell class needs a persistent engine in the crawler\n         crawler.engine = crawler._create_engine()\n         crawler.engine.start()\n \n-        self.crawler_process._start_logging()\n+        self.crawler_process.start(start_reactor=False)\n         self._start_crawler_thread()\n \n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n\n@@ -126,13 +126,11 @@ class CrawlerProcess(CrawlerRunner):\n         self._stop_logging()\n         reactor.callFromThread(self._stop_reactor)\n \n-    def start(self, stop_after_crawl=True):\n-        self._start_logging()\n-        self._start_reactor(stop_after_crawl)\n-\n-    def _start_logging(self):\n+    def start(self, stop_after_crawl=True, start_reactor=True):\n         self.log_observer = log.start_from_settings(self.settings)\n         log.scrapy_info(self.settings)\n+        if start_reactor:\n+            self._start_reactor(stop_after_crawl)\n \n     def _start_reactor(self, stop_after_crawl=True):\n         if stop_after_crawl:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51b0bd281d693ecbca9f35e62df8a7804dbe94af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 6 | Files Changed: 6 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 35 | Churn Cumulative: 470 | Contributors (this commit): 12 | Commits (past 90d): 14 | Contributors (cumulative): 27 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,7 +22,7 @@ class CookiesMiddleware(object):\n         return cls(crawler.settings.getbool('COOKIES_DEBUG'))\n \n     def process_request(self, request, spider):\n-        if 'dont_merge_cookies' in request.meta:\n+        if request.meta.get('dont_merge_cookies', False):\n             return\n \n         cookiejarkey = request.meta.get(\"cookiejar\")\n@@ -37,7 +37,7 @@ class CookiesMiddleware(object):\n         self._debug_cookie(request, spider)\n \n     def process_response(self, request, response, spider):\n-        if 'dont_merge_cookies' in request.meta:\n+        if request.meta.get('dont_merge_cookies', False):\n             return response\n \n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n\n@@ -52,7 +52,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n     \"\"\"Handle redirection of requests based on response status and meta-refresh html tag\"\"\"\n \n     def process_response(self, request, response, spider):\n-        if 'dont_redirect' in request.meta:\n+        if request.meta.get('dont_redirect', False):\n             return response\n \n         if request.method == 'HEAD':\n@@ -86,7 +86,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n                                          settings.getint('METAREFRESH_MAXDELAY'))\n \n     def process_response(self, request, response, spider):\n-        if 'dont_redirect' in request.meta or request.method == 'HEAD' or \\\n+        if request.meta.get('dont_redirect', False) or request.method == 'HEAD' or \\\n                 not isinstance(response, HtmlResponse):\n             return response\n \n\n@@ -50,7 +50,7 @@ class RetryMiddleware(object):\n         return cls(crawler.settings)\n \n     def process_response(self, request, response, spider):\n-        if 'dont_retry' in request.meta:\n+        if request.meta.get('dont_retry', False):\n             return response\n         if response.status in self.retry_http_codes:\n             reason = response_status_message(response.status)\n@@ -59,7 +59,7 @@ class RetryMiddleware(object):\n \n     def process_exception(self, request, exception, spider):\n         if isinstance(exception, self.EXCEPTIONS_TO_RETRY) \\\n-                and 'dont_retry' not in request.meta:\n+                and not request.meta.get('dont_retry', False):\n              return self._retry(request, exception, spider)\n \n     def _retry(self, request, reason, spider):\n\n@@ -52,10 +52,16 @@ class CookiesMiddlewareTest(TestCase):\n         res = Response('http://scrapytest.org/dontmerge', headers={'Set-Cookie': 'dont=mergeme; path=/'})\n         assert self.mw.process_response(req, res, self.spider) is res\n \n+        # check that cookies are merged back\n         req = Request('http://scrapytest.org/mergeme')\n         assert self.mw.process_request(req, self.spider) is None\n         self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n \n+        # check that cookies are merged when dont_merge_cookies is passed as 0\n+        req = Request('http://scrapytest.org/mergeme', meta={'dont_merge_cookies': 0})\n+        assert self.mw.process_request(req, self.spider) is None\n+        self.assertEquals(req.headers.get('Cookie'), 'C1=value1')\n+\n     def test_complex_cookies(self):\n         # merge some cookies into jar\n         cookies = [{'name': 'C1', 'value': 'value1', 'path': '/foo', 'domain': 'scrapytest.org'},\n\n@@ -50,6 +50,15 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(r, Response)\n         assert r is rsp\n \n+        # Test that it redirects when dont_redirect is False\n+        req = Request(url, meta={'dont_redirect': False})\n+        rsp = Response(url2, status=200)\n+\n+        r = self.mw.process_response(req, rsp, self.spider)\n+        assert isinstance(r, Response)\n+        assert r is rsp\n+\n+\n     def test_redirect_302(self):\n         url = 'http://www.example.com/302'\n         url2 = 'http://www.example.com/redirected2'\n\n@@ -40,6 +40,14 @@ class RetryTest(unittest.TestCase):\n         r = self.mw.process_response(req, rsp, self.spider)\n         assert r is rsp\n \n+        # Test retry when dont_retry set to False\n+        req = Request('http://www.scrapytest.org/503', meta={'dont_retry': False})\n+        rsp = Response('http://www.scrapytest.org/503')\n+\n+        # first retry\n+        r = self.mw.process_response(req, rsp, self.spider)\n+        assert r is rsp\n+\n     def test_dont_retry_exc(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a9292cfab75015f7dc7f8c9ff722f609af695c6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 575 | Files Changed: 11 | Hunks: 25 | Methods Changed: 46 | Complexity Δ (Sum/Max): -78/6 | Churn Δ: 590 | Churn Cumulative: 1387 | Contributors (this commit): 16 | Commits (past 90d): 15 | Contributors (cumulative): 37 | DMM Complexity: 0.18487394957983194\n\nDIFF:\n\n@@ -1,8 +0,0 @@\n-from scrapy.webservice import JsonRpcResource\n-\n-class CrawlerResource(JsonRpcResource):\n-\n-    ws_name = 'crawler'\n-\n-    def __init__(self, crawler):\n-        JsonRpcResource.__init__(self, crawler, crawler)\n\n@@ -1,22 +0,0 @@\n-from scrapy.webservice import JsonResource\n-from scrapy.utils.engine import get_engine_status\n-\n-class EngineStatusResource(JsonResource):\n-\n-    ws_name = 'enginestatus'\n-\n-    def __init__(self, crawler, spider_name=None):\n-        JsonResource.__init__(self, crawler)\n-        self._spider_name = spider_name\n-        self.isLeaf = spider_name is not None\n-\n-    def render_GET(self, txrequest):\n-        status = get_engine_status(self.crawler.engine)\n-        if self._spider_name is None:\n-            return status\n-        for sp, st in status['spiders'].items():\n-            if sp.name == self._spider_name:\n-                return st\n-\n-    def getChild(self, name, txrequest):\n-        return EngineStatusResource(name, self.crawler)\n\n@@ -1,8 +0,0 @@\n-from scrapy.webservice import JsonRpcResource\n-\n-class StatsResource(JsonRpcResource):\n-\n-    ws_name = 'stats'\n-\n-    def __init__(self, crawler):\n-        JsonRpcResource.__init__(self, crawler, crawler.stats)\n\n@@ -109,7 +109,6 @@ EXTENSIONS = {}\n \n EXTENSIONS_BASE = {\n     'scrapy.contrib.corestats.CoreStats': 0,\n-    'scrapy.webservice.WebService': 0,\n     'scrapy.telnet.TelnetConsole': 0,\n     'scrapy.contrib.memusage.MemoryUsage': 0,\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n@@ -239,17 +238,6 @@ TELNETCONSOLE_ENABLED = 1\n TELNETCONSOLE_PORT = [6023, 6073]\n TELNETCONSOLE_HOST = '127.0.0.1'\n \n-WEBSERVICE_ENABLED = True\n-WEBSERVICE_LOGFILE = None\n-WEBSERVICE_PORT = [6080, 7030]\n-WEBSERVICE_HOST = '127.0.0.1'\n-WEBSERVICE_RESOURCES = {}\n-WEBSERVICE_RESOURCES_BASE = {\n-    'scrapy.contrib.webservice.crawler.CrawlerResource': 1,\n-    'scrapy.contrib.webservice.enginestatus.EngineStatusResource': 1,\n-    'scrapy.contrib.webservice.stats.StatsResource': 1,\n-}\n-\n SPIDER_CONTRACTS = {}\n SPIDER_CONTRACTS_BASE = {\n     'scrapy.contracts.default.UrlContract': 1,\n\n@@ -1,97 +0,0 @@\n-\"\"\"\n-This module implements the JSON-RPC 2.0 protocol, as defined in:\n-http://groups.google.com/group/json-rpc/web/json-rpc-2-0\n-\"\"\"\n-\n-import urllib\n-import json\n-import traceback\n-\n-from scrapy.utils.serialize import ScrapyJSONDecoder\n-\n-# JSON-RPC 2.0 errors, as defined in:\n-class jsonrpc_errors:\n-    PARSE_ERROR = -32700\n-    INVALID_REQUEST = -32600\n-    METHOD_NOT_FOUND = -32601\n-    INVALID_PARAMS = -32602\n-    INTERNAL_ERROR = -32603\n-\n-class JsonRpcError(Exception):\n-\n-    def __init__(self, code, message, data=None):\n-        super(JsonRpcError, self).__init__()\n-        self.code = code\n-        self.message = message\n-        self.data = data\n-\n-    def __str__(self):\n-        return \"JSON-RPC error (code %d): %s\" % (self.code, self.message)\n-\n-def jsonrpc_client_call(url, method, *args, **kwargs):\n-    \"\"\"Execute a JSON-RPC call on the given url\"\"\"\n-    _urllib = kwargs.pop('_urllib', urllib)\n-    if args and kwargs:\n-        raise ValueError(\"Pass *args or **kwargs but not both to jsonrpc_client_call\")\n-    req = {'jsonrpc': '2.0', 'method': method, 'params': args or kwargs, 'id': 1}\n-    res = json.loads(_urllib.urlopen(url, json.dumps(req)).read())\n-    if 'result' in res:\n-        return res['result']\n-    elif 'error' in res:\n-        er = res['error']\n-        raise JsonRpcError(er['code'], er['message'], er['data'])\n-    else:\n-        msg = \"JSON-RPC response must contain 'result' or 'error': %s\" % res\n-        raise ValueError(msg)\n-\n-def jsonrpc_server_call(target, jsonrpc_request, json_decoder=None):\n-    \"\"\"Execute the given JSON-RPC request (as JSON-encoded string) on the given\n-    target object and return the JSON-RPC response, as a dict\n-    \"\"\"\n-    if json_decoder is None:\n-        json_decoder = ScrapyJSONDecoder()\n-\n-    try:\n-        req = json_decoder.decode(jsonrpc_request)\n-    except Exception as e:\n-        return jsonrpc_error(None, jsonrpc_errors.PARSE_ERROR, 'Parse error', \\\n-            traceback.format_exc())\n-\n-    try:\n-        id, methname = req['id'], req['method']\n-    except KeyError:\n-        return jsonrpc_error(None, jsonrpc_errors.INVALID_REQUEST, 'Invalid Request')\n-\n-    try:\n-        method = getattr(target, methname)\n-    except AttributeError:\n-        return jsonrpc_error(id, jsonrpc_errors.METHOD_NOT_FOUND, 'Method not found')\n-\n-    params = req.get('params', [])\n-    a, kw = ([], params) if isinstance(params, dict) else (params, {})\n-    kw = dict([(str(k), v) for k, v in kw.items()]) # convert kw keys to str\n-    try:\n-        return jsonrpc_result(id, method(*a, **kw))\n-    except Exception as e:\n-        return jsonrpc_error(id, jsonrpc_errors.INTERNAL_ERROR, str(e), \\\n-            traceback.format_exc())\n-\n-def jsonrpc_error(id, code, message, data=None):\n-    \"\"\"Create JSON-RPC error response\"\"\"\n-    return {\n-        'jsonrpc': '2.0',\n-        'error': {\n-            'code': code,\n-            'message': message,\n-            'data': data,\n-        },\n-        'id': id,\n-    }\n-\n-def jsonrpc_result(id, result):\n-    \"\"\"Create JSON-RPC result response\"\"\"\n-    return {\n-        'jsonrpc': '2.0',\n-        'result': result,\n-        'id': id,\n-    }\n\n@@ -1,93 +1,18 @@\n-import re\n+import json\n import datetime\n import decimal\n-import json\n \n from twisted.internet import defer\n \n-from scrapy.spider import Spider\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n \n \n-class SpiderReferencer(object):\n-    \"\"\"Class to serialize (and deserialize) objects (typically dicts)\n-    containing references to running spiders (ie. Spider objects). This is\n-    required because json library fails to serialize dicts containing\n-    non-primitive types as keys, even when you override\n-    ScrapyJSONEncoder.default() with a custom encoding mechanism.\n-    \"\"\"\n-\n-    spider_ref_re = re.compile('^spider:([0-9a-f]+)?:?(.+)?$')\n-\n-    def __init__(self, crawler):\n-        self.crawler = crawler\n-\n-    def get_reference_from_spider(self, spider):\n-        return 'spider:%x:%s' % (id(spider), spider.name)\n-\n-    def get_spider_from_reference(self, ref):\n-        \"\"\"Returns the Spider referenced by text, if text is a spider\n-        reference. Otherwise it returns the text itself. If the text references\n-        a non-running spider it raises a RuntimeError.\n-        \"\"\"\n-        m = self.spider_ref_re.search(ref)\n-        if m:\n-            spid, spname = m.groups()\n-            for spider in self.crawler.engine.open_spiders:\n-                if \"%x\" % id(spider) == spid or spider.name == spname:\n-                    return spider\n-            raise RuntimeError(\"Spider not running: %s\" % ref)\n-        return ref\n-\n-    def encode_references(self, obj):\n-        \"\"\"Look for Spider objects and replace them with spider references\"\"\"\n-        if isinstance(obj, Spider):\n-            return self.get_reference_from_spider(obj)\n-        elif isinstance(obj, dict):\n-            d = {}\n-            for k, v in obj.items():\n-                k = self.encode_references(k)\n-                v = self.encode_references(v)\n-                d[k] = v\n-            return d\n-        elif isinstance(obj, (list, tuple)):\n-            return [self.encode_references(x) for x in obj]\n-        else:\n-            return obj\n-\n-    def decode_references(self, obj):\n-        \"\"\"Look for spider references and replace them with Spider objects\"\"\"\n-        if isinstance(obj, basestring):\n-            return self.get_spider_from_reference(obj)\n-        elif isinstance(obj, dict):\n-            d = {}\n-            for k, v in obj.items():\n-                k = self.decode_references(k)\n-                v = self.decode_references(v)\n-                d[k] = v\n-            return d\n-        elif isinstance(obj, (list, tuple)):\n-            return [self.decode_references(x) for x in obj]\n-        else:\n-            return obj\n-\n-\n class ScrapyJSONEncoder(json.JSONEncoder):\n \n     DATE_FORMAT = \"%Y-%m-%d\"\n     TIME_FORMAT = \"%H:%M:%S\"\n \n-    def __init__(self, *a, **kw):\n-        crawler = kw.pop('crawler', None)\n-        self.spref = kw.pop('spref', None) or SpiderReferencer(crawler)\n-        super(ScrapyJSONEncoder, self).__init__(*a, **kw)\n-\n-    def encode(self, o):\n-        if self.spref:\n-            o = self.spref.encode_references(o)\n-        return super(ScrapyJSONEncoder, self).encode(o)\n-\n     def default(self, o):\n         if isinstance(o, datetime.datetime):\n             return o.strftime(\"%s %s\" % (self.DATE_FORMAT, self.TIME_FORMAT))\n@@ -110,14 +35,4 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n \n \n class ScrapyJSONDecoder(json.JSONDecoder):\n-\n-    def __init__(self, *a, **kw):\n-        crawler = kw.pop('crawler', None)\n-        self.spref = kw.pop('spref', None) or SpiderReferencer(crawler)\n-        super(ScrapyJSONDecoder, self).__init__(*a, **kw)\n-\n-    def decode(self, s):\n-        o = super(ScrapyJSONDecoder, self).decode(s)\n-        if self.spref:\n-            o = self.spref.decode_references(o)\n-        return o\n+    pass\n\n@@ -1,20 +0,0 @@\n-import json\n-\n-from twisted.web import resource\n-\n-class JsonResource(resource.Resource):\n-\n-    json_encoder = json.JSONEncoder()\n-\n-    def render(self, txrequest):\n-        r = resource.Resource.render(self, txrequest)\n-        return self.render_object(r, txrequest)\n-\n-    def render_object(self, obj, txrequest):\n-        r = self.json_encoder.encode(obj) + \"\\n\"\n-        txrequest.setHeader('Content-Type', 'application/json')\n-        txrequest.setHeader('Access-Control-Allow-Origin', '*')\n-        txrequest.setHeader('Access-Control-Allow-Methods', 'GET, POST, PATCH, PUT, DELETE')\n-        txrequest.setHeader('Access-Control-Allow-Headers',' X-Requested-With')\n-        txrequest.setHeader('Content-Length', len(r))\n-        return r\n\\ No newline at end of file\n\n@@ -1,97 +0,0 @@\n-\"\"\"\n-Scrapy web services extension\n-\n-See docs/topics/webservice.rst\n-\"\"\"\n-\n-from twisted.web import server, resource\n-\n-from scrapy.exceptions import NotConfigured\n-from scrapy import log, signals\n-from scrapy.utils.jsonrpc import jsonrpc_server_call\n-from scrapy.utils.serialize import ScrapyJSONEncoder, ScrapyJSONDecoder\n-from scrapy.utils.misc import load_object\n-from scrapy.utils.txweb import JsonResource as JsonResource_\n-from scrapy.utils.reactor import listen_tcp\n-from scrapy.utils.conf import build_component_list\n-\n-\n-class JsonResource(JsonResource_):\n-\n-    def __init__(self, crawler, target=None):\n-        JsonResource_.__init__(self)\n-        self.crawler = crawler\n-        self.json_encoder = ScrapyJSONEncoder(crawler=crawler)\n-\n-class JsonRpcResource(JsonResource):\n-\n-    def __init__(self, crawler, target=None):\n-        JsonResource.__init__(self, crawler, target)\n-        self.json_decoder = ScrapyJSONDecoder(crawler=crawler)\n-        self.crawler = crawler\n-        self._target = target\n-\n-    def render_GET(self, txrequest):\n-        return self.get_target()\n-\n-    def render_POST(self, txrequest):\n-        reqstr = txrequest.content.getvalue()\n-        target = self.get_target()\n-        return jsonrpc_server_call(target, reqstr, self.json_decoder)\n-\n-    def getChild(self, name, txrequest):\n-        target = self.get_target()\n-        try:\n-            newtarget = getattr(target, name)\n-            return JsonRpcResource(self.crawler, newtarget)\n-        except AttributeError:\n-            return resource.ErrorPage(404, \"No Such Resource\", \"No such child resource.\")\n-\n-    def get_target(self):\n-        return self._target\n-\n-\n-class RootResource(JsonResource):\n-\n-    def render_GET(self, txrequest):\n-        return {'resources': self.children.keys()}\n-\n-    def getChild(self, name, txrequest):\n-        if name == '':\n-            return self\n-        return JsonResource.getChild(self, name, txrequest)\n-\n-\n-class WebService(server.Site):\n-\n-    def __init__(self, crawler):\n-        if not crawler.settings.getbool('WEBSERVICE_ENABLED'):\n-            raise NotConfigured\n-        self.crawler = crawler\n-        logfile = crawler.settings['WEBSERVICE_LOGFILE']\n-        self.portrange = [int(x) for x in crawler.settings.getlist('WEBSERVICE_PORT')]\n-        self.host = crawler.settings['WEBSERVICE_HOST']\n-        root = RootResource(crawler)\n-        reslist = build_component_list(crawler.settings['WEBSERVICE_RESOURCES_BASE'], \\\n-            crawler.settings['WEBSERVICE_RESOURCES'])\n-        for res_cls in map(load_object, reslist):\n-            res = res_cls(crawler)\n-            root.putChild(res.ws_name, res)\n-        server.Site.__init__(self, root, logPath=logfile)\n-        self.noisy = False\n-        crawler.signals.connect(self.start_listening, signals.engine_started)\n-        crawler.signals.connect(self.stop_listening, signals.engine_stopped)\n-\n-    @classmethod\n-    def from_crawler(cls, crawler):\n-        return cls(crawler)\n-\n-    def start_listening(self):\n-        self.port = listen_tcp(self.portrange, self.host, self)\n-        h = self.port.getHost()\n-        log.msg(format='Web service listening on %(host)s:%(port)d',\n-                level=log.DEBUG, host=h.host, port=h.port)\n-\n-    def stop_listening(self):\n-        self.port.stopListening()\n-\n\n@@ -1,122 +0,0 @@\n-import unittest, json\n-from io import BytesIO\n-\n-from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n-    JsonRpcError, jsonrpc_errors\n-from scrapy.utils.serialize import ScrapyJSONDecoder\n-from tests.test_utils_serialize import CrawlerMock\n-\n-class urllib_mock(object):\n-    def __init__(self, result=None, error=None):\n-        response = {}\n-        if result:\n-            response.update(result=result)\n-        if error:\n-            response.update(error=error)\n-        self.response = json.dumps(response)\n-        self.request = None\n-\n-    def urlopen(self, url, request):\n-        self.url = url\n-        self.request = request\n-        return BytesIO(self.response)\n-\n-class TestTarget(object):\n-\n-    def call(self, *args, **kwargs):\n-        return list(args), kwargs\n-\n-    def exception(self):\n-        raise Exception(\"testing-errors\")\n-\n-class JsonRpcUtilsTestCase(unittest.TestCase):\n-\n-    def setUp(self):\n-        crawler = CrawlerMock([])\n-        self.json_decoder = ScrapyJSONDecoder(crawler=crawler)\n-\n-    def test_jsonrpc_client_call_args_kwargs_raises(self):\n-        self.assertRaises(ValueError, jsonrpc_client_call, 'url', 'test', 'one', kw=123)\n-\n-    def test_jsonrpc_client_call_request(self):\n-        ul = urllib_mock(1)\n-        jsonrpc_client_call('url', 'test', 'one', 2, _urllib=ul)\n-        req = json.loads(ul.request)\n-        assert 'id' in req\n-        self.assertEqual(ul.url, 'url')\n-        self.assertEqual(req['jsonrpc'], '2.0')\n-        self.assertEqual(req['method'], 'test')\n-        self.assertEqual(req['params'], ['one', 2])\n-\n-    def test_jsonrpc_client_call_response(self):\n-        ul = urllib_mock()\n-        # must return result or error\n-        self.assertRaises(ValueError, jsonrpc_client_call, 'url', 'test', _urllib=ul)\n-        ul = urllib_mock(result={'one': 1})\n-        self.assertEquals(jsonrpc_client_call('url', 'test', _urllib=ul), {'one': 1})\n-        ul = urllib_mock(error={'code': 123, 'message': 'hello', 'data': 'some data'})\n-\n-        raised = False\n-        try:\n-            jsonrpc_client_call('url', 'test', _urllib=ul)\n-        except JsonRpcError as e:\n-            raised = True\n-            self.assertEqual(e.code, 123)\n-            self.assertEqual(e.message, 'hello')\n-            self.assertEqual(e.data, 'some data')\n-            assert '123' in str(e)\n-            assert 'hello' in str(e)\n-        assert raised, \"JsonRpcError not raised\"\n-\n-    def test_jsonrpc_server_call(self):\n-        t = TestTarget()\n-        r = jsonrpc_server_call(t, 'invalid json data', self.json_decoder)\n-        assert 'error' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] is None\n-        self.assertEqual(r['error']['code'], jsonrpc_errors.PARSE_ERROR)\n-        assert 'Traceback' in r['error']['data']\n-\n-        r = jsonrpc_server_call(t, '{\"test\": \"test\"}', self.json_decoder)\n-        assert 'error' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] is None\n-        self.assertEqual(r['error']['code'], jsonrpc_errors.INVALID_REQUEST)\n-\n-        r = jsonrpc_server_call(t, '{\"method\": \"notfound\", \"id\": 1}', self.json_decoder)\n-        assert 'error' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] == 1\n-        self.assertEqual(r['error']['code'], jsonrpc_errors.METHOD_NOT_FOUND)\n-\n-        r = jsonrpc_server_call(t, '{\"method\": \"exception\", \"id\": 1}', self.json_decoder)\n-        assert 'error' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] == 1\n-        self.assertEqual(r['error']['code'], jsonrpc_errors.INTERNAL_ERROR)\n-        assert 'testing-errors' in r['error']['message']\n-        assert 'Traceback' in r['error']['data']\n-\n-        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"id\": 2}', self.json_decoder)\n-        assert 'result' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] == 2\n-        self.assertEqual(r['result'], ([], {}))\n-\n-        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": [456, 123], \"id\": 3}', \\\n-            self.json_decoder)\n-        assert 'result' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] == 3\n-        self.assertEqual(r['result'], ([456, 123], {}))\n-\n-        r = jsonrpc_server_call(t, '{\"method\": \"call\", \"params\": {\"data\": 789}, \"id\": 3}', \\\n-            self.json_decoder)\n-        assert 'result' in r\n-        assert r['jsonrpc'] == '2.0'\n-        assert r['id'] == 3\n-        self.assertEqual(r['result'], ([], {'data': 789}))\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n\n@@ -1,88 +1,20 @@\n+import json\n import unittest\n import datetime\n-import json\n from decimal import Decimal\n \n from twisted.internet import defer\n \n-from scrapy.utils.serialize import SpiderReferencer, ScrapyJSONEncoder, ScrapyJSONDecoder\n-from scrapy.spider import Spider\n+from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.http import Request, Response\n \n \n-class _EngineMock(object):\n-    def __init__(self, open_spiders):\n-        self.open_spiders = open_spiders\n-\n-class CrawlerMock(object):\n-    def __init__(self, open_spiders):\n-        self.engine = _EngineMock(open_spiders)\n-\n-class BaseTestCase(unittest.TestCase):\n+class JsonEncoderTestCase(unittest.TestCase):\n \n     def setUp(self):\n-        self.spider1 = Spider('name1')\n-        self.spider2 = Spider('name2')\n-        open_spiders = set([self.spider1, self.spider2])\n-        crawler = CrawlerMock(open_spiders)\n-        self.spref = SpiderReferencer(crawler)\n-        self.encoder = ScrapyJSONEncoder(spref=self.spref)\n-        self.decoder = ScrapyJSONDecoder(spref=self.spref)\n-\n-class SpiderReferencerTestCase(BaseTestCase):\n-\n-    def test_spiders_and_references(self):\n-        ref1 = self.spref.get_reference_from_spider(self.spider1)\n-        assert isinstance(ref1, str)\n-        assert self.spider1.name in ref1\n-        ref2 = self.spref.get_reference_from_spider(self.spider2)\n-        ref1_ = self.spref.get_reference_from_spider(self.spider1)\n-        assert ref1 == ref1_\n-        assert ref1 != ref2\n-\n-        sp1 = self.spref.get_spider_from_reference(ref1)\n-        sp2 = self.spref.get_spider_from_reference(ref2)\n-        sp1_ = self.spref.get_spider_from_reference(ref1)\n-        assert isinstance(sp1, Spider)\n-        assert sp1 is not sp2\n-        assert sp1 is sp1_\n-\n-        # referring to spiders by name\n-        assert sp1 is self.spref.get_spider_from_reference('spider::name1')\n-        assert sp2 is self.spref.get_spider_from_reference('spider::name2')\n-\n-        # must return string as-is if spider id not found\n-        assert 'lala' == self.spref.get_spider_from_reference('lala')\n-        # must raise RuntimeError if spider id is not found and spider is not running\n-        self.assertRaises(RuntimeError, self.spref.get_spider_from_reference, 'spider:fffffff')\n+        self.encoder = ScrapyJSONEncoder()\n \n     def test_encode_decode(self):\n-        sr = self.spref\n-        sp1 = self.spider1\n-        sp2 = self.spider2\n-        ref1 = sr.get_reference_from_spider(sp1)\n-        ref2 = sr.get_reference_from_spider(sp2)\n-\n-        examples = [\n-            ('lala', 'lala'),\n-            (sp1, ref1),\n-            (['lala', sp1], ['lala', ref1]),\n-            ({'lala': sp1}, {'lala': ref1}),\n-            ({sp1: sp2}, {ref1: ref2}),\n-            ({sp1: {sp2: ['lala', sp1]}}, {ref1: {ref2: ['lala', ref1]}})\n-        ]\n-        for spiders, refs in examples:\n-            self.assertEqual(sr.encode_references(spiders), refs)\n-            self.assertEqual(sr.decode_references(refs), spiders)\n-\n-class JsonEncoderTestCase(BaseTestCase):\n-    \n-    def test_encode_decode(self):\n-        sr = self.spref\n-        sp1 = self.spider1\n-        sp2 = self.spider2\n-        ref1 = sr.get_reference_from_spider(sp1)\n-        ref2 = sr.get_reference_from_spider(sp2)\n         dt = datetime.datetime(2010, 1, 2, 10, 11, 12)\n         dts = \"2010-01-02 10:11:12\"\n         d = datetime.date(2010, 1, 2)\n@@ -92,42 +24,21 @@ class JsonEncoderTestCase(BaseTestCase):\n         dec = Decimal(\"1000.12\")\n         decs = \"1000.12\"\n \n-        examples_encode_decode = [\n-            ('lala', 'lala'),\n-            (sp1, ref1),\n-            (['lala', sp1], ['lala', ref1]),\n-            ({'lala': sp1}, {'lala': ref1}),\n-            ({sp1: sp2}, {ref1: ref2}),\n-            ({sp1: {sp2: ['lala', sp1]}}, {ref1: {ref2: ['lala', ref1]}})\n-        ]\n-        for spiders, refs in examples_encode_decode:\n-            self.assertEqual(self.encoder.encode(spiders), json.dumps(refs))\n-            self.assertEqual(self.decoder.decode(json.dumps(refs)), spiders)\n+        for input, output in [('foo', 'foo'), (d, ds), (t, ts), (dt, dts),\n+                              (dec, decs), (['foo', d], ['foo', ds])]:\n+            self.assertEqual(self.encoder.encode(input), json.dumps(output))\n \n-        examples_encode_only = [\n-            ({sp1: dt}, {ref1: dts}),\n-            ({sp1: d}, {ref1: ds}),\n-            ({sp1: t}, {ref1: ts}),\n-            ({sp1: dec}, {ref1: decs}),\n-        ]\n-        for spiders, refs in examples_encode_only:\n-            self.assertEqual(self.encoder.encode(spiders), json.dumps(refs))\n-\n-        assert 'Deferred' in self.encoder.encode(defer.Deferred())\n+    def test_encode_deferred(self):\n+        self.assertIn('Deferred', self.encoder.encode(defer.Deferred()))\n \n     def test_encode_request(self):\n         r = Request(\"http://www.example.com/lala\")\n         rs = self.encoder.encode(r)\n-        assert r.method in rs\n-        assert r.url in rs\n+        self.assertIn(r.method, rs)\n+        self.assertIn(r.url, rs)\n \n     def test_encode_response(self):\n         r = Response(\"http://www.example.com/lala\")\n         rs = self.encoder.encode(r)\n-        assert r.url in rs\n-        assert str(r.status) in rs\n-\n-\n-if __name__ == \"__main__\":\n-    unittest.main()\n-\n+        self.assertIn(r.url, rs)\n+        self.assertIn(str(r.status), rs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
