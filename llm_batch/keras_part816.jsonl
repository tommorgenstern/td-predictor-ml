{"custom_id": "keras#e620cb48ea4df28d9ae36c4c5a61e5920ec95507", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 70 | Files Changed: 8 | Hunks: 36 | Methods Changed: 21 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 242 | Churn Cumulative: 792 | Contributors (this commit): 3 | Commits (past 90d): 38 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -266,6 +266,8 @@ class QuantizedDTypePolicy(DTypePolicy):\n \n @keras_export(\"keras.dtype_policies.QuantizedFloat8DTypePolicy\")\n class QuantizedFloat8DTypePolicy(QuantizedDTypePolicy):\n+    default_amax_history_length = 1024\n+\n     def __init__(self, name, amax_history_length=1024):\n         super().__init__(name)\n         if not isinstance(amax_history_length, int):\n\n@@ -283,6 +283,11 @@ class QuantizedDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n         policy = QuantizedFloat8DTypePolicy(\"float8_from_mixed_bfloat16\", 512)\n         self.assertEqual(policy.amax_history_length, 512)\n \n+        # Test default_amax_history_length\n+        self.assertEqual(\n+            QuantizedFloat8DTypePolicy.default_amax_history_length, 1024\n+        )\n+\n     def test_invalid_properties_for_float8(self):\n         with self.assertRaisesRegex(TypeError, \"must be an integer.\"):\n             QuantizedFloat8DTypePolicy(\"float8_from_float32\", \"512\")\n\n@@ -218,7 +218,7 @@ class Dense(Layer):\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n                 )\n         for i, variable in enumerate(target_variables):\n             store[str(i)] = variable\n@@ -247,7 +247,7 @@ class Dense(Layer):\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n                 )\n         for i, variable in enumerate(target_variables):\n             variable.assign(store[str(i)])\n@@ -327,7 +327,7 @@ class Dense(Layer):\n             self._float8_build()\n         else:\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n \n     def _int8_build(\n@@ -353,14 +353,15 @@ class Dense(Layer):\n         self._is_quantized = True\n \n     def _float8_build(self):\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedFloat8DTypePolicy\n-        ):\n-            raise TypeError(\n-                \"`self.dtype_policy` must be the type of \"\n-                f\"QuantizedFloat8DTypePolicy. Received {self.dtype_policy}\"\n+        from keras.src.dtype_policies import QuantizedFloat8DTypePolicy\n+\n+        # If `self.dtype_policy` is not QuantizedFloat8DTypePolicy, then set\n+        # `amax_history_length` to its default value.\n+        amax_history_length = getattr(\n+            self.dtype_policy,\n+            \"amax_history_length\",\n+            QuantizedFloat8DTypePolicy.default_amax_history_length,\n         )\n-        amax_history_length = self.dtype_policy.amax_history_length\n         # We set `trainable=True` because we will use the gradients to overwrite\n         # these variables\n         scale_kwargs = {\n@@ -410,7 +411,7 @@ class Dense(Layer):\n         else:\n             mode = self.dtype_policy.quantization_mode\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n \n     def _int8_call(self, inputs):\n@@ -550,15 +551,6 @@ class Dense(Layer):\n             )\n         self._check_quantize_args(mode, self.compute_dtype)\n \n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            # We set the internal `self._dtype_policy` instead of using the\n-            # setter to avoid double `quantize` call\n-            self._dtype_policy = dtype_policies.get(quantized_dtype)\n-\n         self._tracker.unlock()\n         if mode == \"int8\":\n             # Quantize `self._kernel` to int8 and compute corresponding scale\n@@ -580,10 +572,19 @@ class Dense(Layer):\n             self._float8_build()\n         else:\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n         self._tracker.lock()\n \n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            # We set the internal `self._dtype_policy` instead of using the\n+            # setter to avoid double `quantize` call\n+            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+\n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n \n\n@@ -332,12 +332,9 @@ class DenseTest(testing.TestCase, parameterized.TestCase):\n         with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n             layer.enable_lora(rank=2)\n \n-    \"\"\"Test quantization-related (int8 and float8) methods\"\"\"\n+    # Test quantization-related (int8 and float8) methods\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"numpy\",\n-        reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n-    )\n+    @pytest.mark.requires_trainable_backend\n     def test_quantize_int8(self):\n         layer = layers.Dense(units=16)\n         layer.build((None, 8))\n@@ -451,6 +448,38 @@ class DenseTest(testing.TestCase, parameterized.TestCase):\n         layer.dtype_policy = policy\n         self.assertLen(layer.variables, expected_num_variables)\n \n+    @parameterized.named_parameters(\n+        (\"int7\", \"int7\"),\n+        (\"float7\", \"float7\"),\n+    )\n+    def test_quantize_invalid_mode(self, mode):\n+        layer = layers.Dense(units=2)\n+        layer.build((None, 2))\n+        x = np.random.random((1, 2))\n+        # dtype_policy should not be altered by failed quantization\n+        original_dtype_policy = layer.dtype_policy\n+\n+        # Test quantize\n+        with self.assertRaisesRegex(ValueError, \"Invalid quantization mode.\"):\n+            layer.quantize(mode)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n+        # Test quantized_build\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Invalid quantization mode.\"\n+        ):\n+            layer.quantized_build((None, 2), mode)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n+        # Test quantized_call\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Invalid quantization mode.\"\n+        ):\n+            # Explicitly set quantization_mode\n+            layer._dtype_policy.quantization_mode = mode\n+            layer.quantized_call(x)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_quantize_int8_dtype_argument(self):\n         self.run_layer_test(\n\n@@ -273,7 +273,7 @@ class EinsumDense(Layer):\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n                 )\n         for i, variable in enumerate(target_variables):\n             store[str(i)] = variable\n@@ -302,7 +302,7 @@ class EinsumDense(Layer):\n                 target_variables.append(self.outputs_grad_amax_history)\n             else:\n                 raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n                 )\n         for i, variable in enumerate(target_variables):\n             variable.assign(store[str(i)])\n@@ -391,7 +391,7 @@ class EinsumDense(Layer):\n             self._float8_build()\n         else:\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n \n     def _int8_build(\n@@ -439,14 +439,15 @@ class EinsumDense(Layer):\n         self._is_quantized = True\n \n     def _float8_build(self):\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedFloat8DTypePolicy\n-        ):\n-            raise TypeError(\n-                \"`self.dtype_policy` must be the type of \"\n-                f\"QuantizedFloat8DTypePolicy. Received {self.dtype_policy}\"\n+        from keras.src.dtype_policies import QuantizedFloat8DTypePolicy\n+\n+        # If `self.dtype_policy` is not QuantizedFloat8DTypePolicy, then set\n+        # `amax_history_length` to its default value.\n+        amax_history_length = getattr(\n+            self.dtype_policy,\n+            \"amax_history_length\",\n+            QuantizedFloat8DTypePolicy.default_amax_history_length,\n         )\n-        amax_history_length = self.dtype_policy.amax_history_length\n         # We set `trainable=True` because we will use the gradients to overwrite\n         # these variables\n         scale_kwargs = {\n@@ -496,7 +497,7 @@ class EinsumDense(Layer):\n         else:\n             mode = self.dtype_policy.quantization_mode\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n \n     def _int8_call(self, inputs):\n@@ -665,15 +666,6 @@ class EinsumDense(Layer):\n             )\n         self._check_quantize_args(mode, self.compute_dtype)\n \n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            # We set the internal `self._dtype_policy` instead of using the\n-            # setter to avoid double `quantize` call\n-            self._dtype_policy = dtype_policies.get(quantized_dtype)\n-\n         self._tracker.unlock()\n         if mode == \"int8\":\n             (\n@@ -717,10 +709,19 @@ class EinsumDense(Layer):\n             self._float8_build()\n         else:\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n         self._tracker.lock()\n \n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            # We set the internal `self._dtype_policy` instead of using the\n+            # setter to avoid double `quantize` call\n+            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+\n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n \n\n@@ -380,12 +380,9 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n         )\n \n-    \"\"\"Test quantization-related (int8 and float8) methods\"\"\"\n+    # Test quantization-related (int8 and float8) methods\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"numpy\",\n-        reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n-    )\n+    @pytest.mark.requires_trainable_backend\n     def test_quantize_int8(self):\n         layer = layers.EinsumDense(\n             equation=\"ab,bcd->acd\",\n@@ -474,10 +471,7 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         (\"btd,ndh->btnh\", \"btd,ndh->btnh\", (None, 2, 8), (1, 2, 4)),\n         (\"btd,df->btf\", \"btd,df->btf\", (None, 4), (1, 2, 4)),\n     )\n-    @pytest.mark.skipif(\n-        backend.backend() == \"numpy\",\n-        reason=f\"{backend.backend()} does not support ops.custom_gradient.\",\n-    )\n+    @pytest.mark.requires_trainable_backend\n     def test_quantize_int8_with_specific_equations(\n         self, equation, output_shape, input_shape\n     ):\n@@ -575,6 +569,42 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         layer.dtype_policy = policy\n         self.assertLen(layer.variables, expected_num_variables)\n \n+    @parameterized.named_parameters(\n+        (\"int7\", \"int7\"),\n+        (\"float7\", \"float7\"),\n+    )\n+    def test_quantize_invalid_mode(self, mode):\n+        layer = layers.EinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        layer.build((None, 3))\n+        x = np.random.random((1, 3))\n+        # dtype_policy should not be altered by failed quantization\n+        original_dtype_policy = layer.dtype_policy\n+\n+        # Test quantize\n+        with self.assertRaisesRegex(ValueError, \"Invalid quantization mode.\"):\n+            layer.quantize(mode)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n+        # Test quantized_build\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Invalid quantization mode.\"\n+        ):\n+            layer.quantized_build((None, 2), mode)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n+        # Test quantized_call\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Invalid quantization mode.\"\n+        ):\n+            # Explicitly set quantization_mode\n+            layer._dtype_policy.quantization_mode = mode\n+            layer.quantized_call(x)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_quantize_int8_dtype_argument(self):\n         self.run_layer_test(\n\n@@ -206,7 +206,7 @@ class Embedding(Layer):\n                 target_variables.append(embeddings_scale)\n             else:\n                 raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n                 )\n         for i, variable in enumerate(target_variables):\n             store[str(i)] = variable\n@@ -226,7 +226,7 @@ class Embedding(Layer):\n                 target_variables.append(self.embeddings_scale)\n             else:\n                 raise NotImplementedError(\n-                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                    self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n                 )\n         for i, variable in enumerate(target_variables):\n             variable.assign(store[str(i)])\n@@ -307,7 +307,7 @@ class Embedding(Layer):\n             self._int8_build()\n         else:\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n \n     def _int8_build(\n@@ -339,7 +339,7 @@ class Embedding(Layer):\n         else:\n             mode = self.dtype_policy.quantization_mode\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n \n     def _int8_call(self, inputs):\n@@ -371,15 +371,6 @@ class Embedding(Layer):\n             )\n         self._check_quantize_args(mode, self.compute_dtype)\n \n-        # Set new dtype policy\n-        if not isinstance(\n-            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n-        ):\n-            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n-            # We set the internal `self._dtype_policy` instead of using the\n-            # setter to avoid double `quantize` call\n-            self._dtype_policy = dtype_policies.get(quantized_dtype)\n-\n         self._tracker.unlock()\n         if mode == \"int8\":\n             # Quantize `self._embeddings` to int8 and compute corresponding\n@@ -398,10 +389,19 @@ class Embedding(Layer):\n             )\n         else:\n             raise NotImplementedError(\n-                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode)\n+                self.QUANTIZATION_MODE_ERROR_TEMPLATE.format(mode=mode)\n             )\n         self._tracker.lock()\n \n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            # We set the internal `self._dtype_policy` instead of using the\n+            # setter to avoid double `quantize` call\n+            self._dtype_policy = dtype_policies.get(quantized_dtype)\n+\n         # Release memory manually because sometimes the backend doesn't\n         gc.collect()\n \n\n@@ -212,6 +212,8 @@ class EmbeddingTest(test_case.TestCase, parameterized.TestCase):\n         with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n             layer.enable_lora(rank=2)\n \n+    # Test quantization-related (int8) methods\n+\n     def test_quantize_int8(self):\n         layer = layers.Embedding(10, 16)\n         layer.build()\n@@ -337,6 +339,38 @@ class EmbeddingTest(test_case.TestCase, parameterized.TestCase):\n         layer.dtype_policy = policy\n         self.assertLen(layer.variables, expected_num_variables)\n \n+    @parameterized.named_parameters(\n+        (\"int7\", \"int7\"),\n+        (\"float7\", \"float7\"),\n+    )\n+    def test_quantize_invalid_mode(self, mode):\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        x = np.random.randint(0, 9, size=(1, 3))\n+        # dtype_policy should not be altered by failed quantization\n+        original_dtype_policy = layer.dtype_policy\n+\n+        # Test quantize\n+        with self.assertRaisesRegex(ValueError, \"Invalid quantization mode.\"):\n+            layer.quantize(mode)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n+        # Test quantized_build\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Invalid quantization mode.\"\n+        ):\n+            layer.quantized_build((None, 2), mode)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n+        # Test quantized_call\n+        with self.assertRaisesRegex(\n+            NotImplementedError, \"Invalid quantization mode.\"\n+        ):\n+            # Explicitly set quantization_mode\n+            layer._dtype_policy.quantization_mode = mode\n+            layer.quantized_call(x)\n+        self.assertEqual(layer.dtype_policy, original_dtype_policy)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_quantize_when_lora_enabled(self):\n         layer = layers.Embedding(10, 16)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da83683f5e92fa24a0ad7bf5dc034ea596346d21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 26 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,6 +59,7 @@ class MultiHeadAttention(Layer):\n         activity_regularizer: Regularizer for dense layer activity.\n         kernel_constraint: Constraint for dense layer kernels.\n         bias_constraint: Constraint for dense layer kernels.\n+        seed: Optional integer to seed the dropout layer.\n \n     Call arguments:\n         query: Query tensor of shape `(B, T, dim)`, where `B` is the batch size,\n@@ -110,6 +111,7 @@ class MultiHeadAttention(Layer):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n+        seed=None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -137,6 +139,7 @@ class MultiHeadAttention(Layer):\n                 f\"Received: attention_axes={attention_axes}\"\n             )\n         self._attention_axes = attention_axes\n+        self.seed = seed\n \n     @property\n     def num_heads(self):\n@@ -189,6 +192,7 @@ class MultiHeadAttention(Layer):\n             ),\n             \"kernel_constraint\": constraints.serialize(self._kernel_constraint),\n             \"bias_constraint\": constraints.serialize(self._bias_constraint),\n+            \"seed\": self.seed,\n         }\n         return {**base_config, **config}\n \n@@ -359,7 +363,7 @@ class MultiHeadAttention(Layer):\n         )\n         self._softmax = Softmax(axis=norm_axes, dtype=self.dtype_policy)\n         self._dropout_layer = Dropout(\n-            rate=self._dropout, dtype=self.dtype_policy\n+            rate=self._dropout, dtype=self.dtype_policy, seed=self.seed\n         )\n         self._inverse_sqrt_key_dim = 1.0 / math.sqrt(float(self._key_dim))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b18bd4feca747a22f303ca6fa54f08e8183ad88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 157 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 6 | Methods Changed: 9 | Complexity Δ (Sum/Max): 22/9 | Churn Δ: 157 | Churn Cumulative: 349 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 14 | DMM Complexity: 0.6041666666666666\n\nDIFF:\n@@ -81,3 +81,9 @@ def solve_triangular(a, b, lower=False):\n \n def svd(x, full_matrices=True, compute_uv=True):\n     return jnp.linalg.svd(x, full_matrices=full_matrices, compute_uv=compute_uv)\n+\n+\n+def lstsq(a, b, rcond=None):\n+    a = convert_to_tensor(a)\n+    b = convert_to_tensor(b)\n+    return jnp.linalg.lstsq(a, b, rcond=rcond)[0]\n\n@@ -80,3 +80,9 @@ def solve_triangular(a, b, lower=False):\n \n def svd(x, full_matrices=True, compute_uv=True):\n     return np.linalg.svd(x, full_matrices=full_matrices, compute_uv=compute_uv)\n+\n+\n+def lstsq(a, b, rcond=None):\n+    a = convert_to_tensor(a)\n+    b = convert_to_tensor(b)\n+    return np.linalg.lstsq(a, b, rcond=rcond)[0]\n\n@@ -189,3 +189,44 @@ def svd(x, full_matrices=True, compute_uv=True):\n         x, full_matrices=full_matrices, compute_uv=compute_uv\n     )\n     return u, s, tf.linalg.adjoint(v)\n+\n+\n+def lstsq(a, b, rcond=None):\n+    a = convert_to_tensor(a)\n+    b = convert_to_tensor(b)\n+    if a.shape[0] != b.shape[0]:\n+        raise ValueError(\"Leading dimensions of input arrays must match\")\n+    b_orig_ndim = b.ndim\n+    if b_orig_ndim == 1:\n+        b = b[:, None]\n+    if a.ndim != 2:\n+        raise TypeError(\n+            f\"{a.ndim}-dimensional array given. \"\n+            \"Array must be two-dimensional\"\n+        )\n+    if b.ndim != 2:\n+        raise TypeError(\n+            f\"{b.ndim}-dimensional array given. \"\n+            \"Array must be one or two-dimensional\"\n+        )\n+    m, n = a.shape\n+    dtype = a.dtype\n+    eps = tf.experimental.numpy.finfo(dtype).eps\n+    if a.shape == ():\n+        s = tf.zeros(0, dtype=a.dtype)\n+        x = tf.zeros((n, *b.shape[1:]), dtype=a.dtype)\n+    else:\n+        if rcond is None:\n+            rcond = eps * max(n, m)\n+        else:\n+            rcond = tf.where(rcond < 0, eps, rcond)\n+        u, s, vt = svd(a, full_matrices=False)\n+        mask = s >= tf.convert_to_tensor(rcond, dtype=s.dtype) * s[0]\n+        safe_s = tf.cast(tf.where(mask, s, 1), dtype=a.dtype)\n+        s_inv = tf.where(mask, 1 / safe_s, 0)[:, tf.newaxis]\n+        u_t_b = tf.matmul(tf.transpose(tf.math.conj(u)), b)\n+        x = tf.matmul(tf.transpose(tf.math.conj(vt)), s_inv * u_t_b)\n+\n+    if b_orig_ndim == 1:\n+        x = tf.reshape(x, [-1])\n+    return x\n\n@@ -72,3 +72,9 @@ def svd(x, full_matrices=True, compute_uv=True):\n             \"`compute_uv=False` is not supported for torch backend.\"\n         )\n     return torch.linalg.svd(x, full_matrices=full_matrices)\n+\n+\n+def lstsq(a, b, rcond=None):\n+    a = convert_to_tensor(a)\n+    b = convert_to_tensor(b)\n+    return torch.linalg.lstsq(a, b, rcond=rcond)[0]\n\n@@ -593,6 +593,83 @@ def _svd(x, full_matrices=True, compute_uv=True):\n     return backend.linalg.svd(x, full_matrices, compute_uv)\n \n \n+class Lstsq(Operation):\n+    def __init__(self, rcond=None):\n+        super().__init__()\n+        self.rcond = rcond\n+\n+    def call(self, a, b):\n+        return backend.linalg.lstsq(a, b, rcond=self.rcond)\n+\n+    def compute_output_spec(self, a, b):\n+        if len(a.shape) != 2:\n+            raise ValueError(\n+                \"Expected a to have rank 2. \" f\"Received: a.shape={a.shape}\"\n+            )\n+        if len(b.shape) not in (1, 2):\n+            raise ValueError(\n+                \"Expected b to have rank 1 or 2. \"\n+                f\"Received: b.shape={b.shape}\"\n+            )\n+        m, n = a.shape\n+        if b.shape[0] != m:\n+            raise ValueError(\n+                \"Expected b.shape[0] to be equal to \"\n+                \"a.shape[0]. Received: \"\n+                f\"a.shape={a.shape}, b.shape={b.shape}\"\n+            )\n+        if len(b.shape) == 2:\n+            k = b.shape[1]\n+            x = KerasTensor((n, k), dtype=a.dtype)\n+        else:\n+            x = KerasTensor((n,), dtype=a.dtype)\n+        return x\n+\n+\n+@keras_export([\"keras.ops.lstsq\", \"keras.ops.linalg.lstsq\"])\n+def lstsq(a, b, rcond=None):\n+    \"\"\"Return the least-squares solution to a linear matrix equation.\n+\n+    Computes the vector x that approximately solves the equation\n+    `a @ x = b`. The equation may be under-, well-, or over-determined\n+    (i.e., the number of linearly independent rows of a can be less than,\n+    equal to, or greater than its number of linearly independent columns).\n+    If a is square and of full rank, then `x` (but for round-off error)\n+    is the exact solution of the equation. Else, `x` minimizes the\n+    L2 norm of `b - a * x`.\n+\n+    If there are multiple minimizing solutions,\n+    the one with the smallest L2 norm  is returned.\n+\n+    Args:\n+        a: \"Coefficient\" matrix of shape `(M, N)`.\n+        b: Ordinate or \"dependent variable\" values,\n+            of shape `(M,)` or `(M, K)`.\n+            If `b` is two-dimensional, the least-squares solution\n+            is calculated for each of the K columns of `b`.\n+        rcond: Cut-off ratio for small singular values of `a`.\n+            For the purposes of rank determination,\n+            singular values are treated as zero if they are\n+            smaller than rcond times the largest\n+            singular value of `a`.\n+\n+    Returns:\n+        Tensor with shape `(N,)` or `(N, K)` containing\n+        the least-squares solutions.\n+    \n+    **NOTE:** The output differs from `numpy.linalg.lstsq`.\n+    NumPy returns a tuple with four elements, the first of which\n+    being the least-squares solutions and the others\n+    being essentially never used.\n+    Keras only returns the first value. This is done both\n+    to ensure consistency across backends (which cannot be achieved\n+    for the other values) and to simplify the API.\n+    \"\"\"\n+    if any_symbolic_tensors((a, b)):\n+        return Lstsq(rcond=rcond).symbolic_call(a, b)\n+    return backend.linalg.lstsq(a, b, rcond=rcond)\n+\n+\n def _assert_1d(*arrays):\n     for a in arrays:\n         if a.ndim < 1:\n\n@@ -533,6 +533,27 @@ class LinalgOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ]\n         self.assertAllClose(x_reconstructed, x, atol=1e-4)\n \n+    @parameterized.named_parameters(\n+        (\"b_rank_1\", 1, None),\n+        (\"b_rank_2\", 2, None),\n+        (\"rcond\", 1, 1e-3),\n+    )\n+    def test_lstsq(self, b_rank, rcond):\n+        a = np.random.random((5, 7)).astype(\"float32\")\n+        a_symb = backend.KerasTensor((5, 7))\n+        if b_rank == 1:\n+            b = np.random.random((5,)).astype(\"float32\")\n+            b_symb = backend.KerasTensor((5,))\n+        else:\n+            b = np.random.random((5, 4)).astype(\"float32\")\n+            b_symb = backend.KerasTensor((5, 4))\n+        out = linalg.lstsq(a, b, rcond=rcond)\n+        ref_out = np.linalg.lstsq(a, b, rcond=rcond)[0]\n+        self.assertAllClose(out, ref_out, atol=1e-5)\n+\n+        out_symb = linalg.lstsq(a_symb, b_symb)\n+        self.assertEqual(out_symb.shape, out.shape)\n+\n \n class QrOpTest(testing.TestCase):\n     def test_qr_init_mode_reduced(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b7120b104c35945d13a04ce3fadb795b923b723", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 209 | Lines Deleted: 148 | Files Changed: 12 | Hunks: 161 | Methods Changed: 79 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 357 | Churn Cumulative: 531 | Contributors (this commit): 2 | Commits (past 90d): 24 | Contributors (cumulative): 24 | DMM Complexity: 0.9111111111111111\n\nDIFF:\n@@ -131,6 +131,7 @@ Args:\n         Defaults to `\"softmax\"`.\n         When loading pretrained weights, `classifier_activation` can only\n         be `None` or `\"softmax\"`.\n+    name: The name of the model (string).\n \n Returns:\n     A model instance.\n@@ -333,7 +334,7 @@ def ConvNeXt(\n     drop_path_rate=0.0,\n     layer_scale_init_value=1e-6,\n     default_size=224,\n-    model_name=\"convnext\",\n+    name=\"convnext\",\n     include_preprocessing=True,\n     include_top=True,\n     weights=None,\n@@ -342,6 +343,7 @@ def ConvNeXt(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    weights_name=None,\n ):\n     \"\"\"Instantiates ConvNeXt architecture given specific configuration.\n \n@@ -354,7 +356,7 @@ def ConvNeXt(\n         layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling\n             won't be used.\n         default_size: Default input image size.\n-        model_name: An optional name for the model.\n+        name: An optional name for the model.\n         include_preprocessing: boolean denoting whther to\n             include preprocessing in the model.\n             When `weights=\"imagenet\"` this should always be `True`.\n@@ -440,7 +442,7 @@ def ConvNeXt(\n         )\n         num_channels = input_shape[channel_axis - 1]\n         if num_channels == 3:\n-            x = PreStem(name=model_name)(x)\n+            x = PreStem(name=name)(x)\n \n     # Stem block.\n     stem = Sequential(\n@@ -449,13 +451,13 @@ def ConvNeXt(\n                 projection_dims[0],\n                 kernel_size=4,\n                 strides=4,\n-                name=model_name + \"_stem_conv\",\n+                name=name + \"_stem_conv\",\n             ),\n             layers.LayerNormalization(\n-                epsilon=1e-6, name=model_name + \"_stem_layernorm\"\n+                epsilon=1e-6, name=name + \"_stem_layernorm\"\n             ),\n         ],\n-        name=model_name + \"_stem\",\n+        name=name + \"_stem\",\n     )\n \n     # Downsampling blocks.\n@@ -468,16 +470,16 @@ def ConvNeXt(\n             [\n                 layers.LayerNormalization(\n                     epsilon=1e-6,\n-                    name=model_name + \"_downsampling_layernorm_\" + str(i),\n+                    name=name + \"_downsampling_layernorm_\" + str(i),\n                 ),\n                 layers.Conv2D(\n                     projection_dims[i + 1],\n                     kernel_size=2,\n                     strides=2,\n-                    name=model_name + \"_downsampling_conv_\" + str(i),\n+                    name=name + \"_downsampling_conv_\" + str(i),\n                 ),\n             ],\n-            name=model_name + \"_downsampling_block_\" + str(i),\n+            name=name + \"_downsampling_block_\" + str(i),\n         )\n         downsample_layers.append(downsample_layer)\n \n@@ -499,7 +501,7 @@ def ConvNeXt(\n                 projection_dim=projection_dims[i],\n                 drop_path_rate=depth_drop_rates[cur + j],\n                 layer_scale_init_value=layer_scale_init_value,\n-                name=model_name + f\"_stage_{i}_block_{j}\",\n+                name=name + f\"_stage_{i}_block_{j}\",\n             )(x)\n         cur += depths[i]\n \n@@ -508,7 +510,7 @@ def ConvNeXt(\n         x = Head(\n             num_classes=classes,\n             classifier_activation=classifier_activation,\n-            name=model_name,\n+            name=name,\n         )(x)\n \n     else:\n@@ -518,17 +520,17 @@ def ConvNeXt(\n             x = layers.GlobalMaxPooling2D()(x)\n         x = layers.LayerNormalization(epsilon=1e-6)(x)\n \n-    model = Functional(inputs=inputs, outputs=x, name=model_name)\n+    model = Functional(inputs=inputs, outputs=x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n         if include_top:\n             file_suffix = \".h5\"\n-            file_hash = WEIGHTS_HASHES[model_name][0]\n+            file_hash = WEIGHTS_HASHES[weights_name][0]\n         else:\n             file_suffix = \"_notop.h5\"\n-            file_hash = WEIGHTS_HASHES[model_name][1]\n-        file_name = model_name + file_suffix\n+            file_hash = WEIGHTS_HASHES[weights_name][1]\n+        file_name = name + file_suffix\n         weights_path = file_utils.get_file(\n             file_name,\n             BASE_WEIGHTS_PATH + file_name,\n@@ -552,7 +554,6 @@ def ConvNeXt(\n     ]\n )\n def ConvNeXtTiny(\n-    model_name=\"convnext_tiny\",\n     include_top=True,\n     include_preprocessing=True,\n     weights=\"imagenet\",\n@@ -561,14 +562,16 @@ def ConvNeXtTiny(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"convnext_tiny\",\n ):\n     return ConvNeXt(\n+        weights_name=\"convnext_tiny\",\n         depths=MODEL_CONFIGS[\"tiny\"][\"depths\"],\n         projection_dims=MODEL_CONFIGS[\"tiny\"][\"projection_dims\"],\n         drop_path_rate=0.0,\n         layer_scale_init_value=1e-6,\n         default_size=MODEL_CONFIGS[\"tiny\"][\"default_size\"],\n-        model_name=model_name,\n+        name=name,\n         include_top=include_top,\n         include_preprocessing=include_preprocessing,\n         weights=weights,\n@@ -587,7 +590,6 @@ def ConvNeXtTiny(\n     ]\n )\n def ConvNeXtSmall(\n-    model_name=\"convnext_small\",\n     include_top=True,\n     include_preprocessing=True,\n     weights=\"imagenet\",\n@@ -596,14 +598,16 @@ def ConvNeXtSmall(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"convnext_small\",\n ):\n     return ConvNeXt(\n+        weights_name=\"convnext_small\",\n         depths=MODEL_CONFIGS[\"small\"][\"depths\"],\n         projection_dims=MODEL_CONFIGS[\"small\"][\"projection_dims\"],\n         drop_path_rate=0.0,\n         layer_scale_init_value=1e-6,\n         default_size=MODEL_CONFIGS[\"small\"][\"default_size\"],\n-        model_name=model_name,\n+        name=name,\n         include_top=include_top,\n         include_preprocessing=include_preprocessing,\n         weights=weights,\n@@ -622,7 +626,6 @@ def ConvNeXtSmall(\n     ]\n )\n def ConvNeXtBase(\n-    model_name=\"convnext_base\",\n     include_top=True,\n     include_preprocessing=True,\n     weights=\"imagenet\",\n@@ -631,14 +634,16 @@ def ConvNeXtBase(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"convnext_base\",\n ):\n     return ConvNeXt(\n+        weights_name=\"convnext_base\",\n         depths=MODEL_CONFIGS[\"base\"][\"depths\"],\n         projection_dims=MODEL_CONFIGS[\"base\"][\"projection_dims\"],\n         drop_path_rate=0.0,\n         layer_scale_init_value=1e-6,\n         default_size=MODEL_CONFIGS[\"base\"][\"default_size\"],\n-        model_name=model_name,\n+        name=name,\n         include_top=include_top,\n         include_preprocessing=include_preprocessing,\n         weights=weights,\n@@ -657,7 +662,6 @@ def ConvNeXtBase(\n     ]\n )\n def ConvNeXtLarge(\n-    model_name=\"convnext_large\",\n     include_top=True,\n     include_preprocessing=True,\n     weights=\"imagenet\",\n@@ -666,14 +670,16 @@ def ConvNeXtLarge(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"convnext_large\",\n ):\n     return ConvNeXt(\n+        weights_name=\"convnext_large\",\n         depths=MODEL_CONFIGS[\"large\"][\"depths\"],\n         projection_dims=MODEL_CONFIGS[\"large\"][\"projection_dims\"],\n         drop_path_rate=0.0,\n         layer_scale_init_value=1e-6,\n         default_size=MODEL_CONFIGS[\"large\"][\"default_size\"],\n-        model_name=model_name,\n+        name=name,\n         include_top=include_top,\n         include_preprocessing=include_preprocessing,\n         weights=weights,\n@@ -692,7 +698,6 @@ def ConvNeXtLarge(\n     ]\n )\n def ConvNeXtXLarge(\n-    model_name=\"convnext_xlarge\",\n     include_top=True,\n     include_preprocessing=True,\n     weights=\"imagenet\",\n@@ -701,14 +706,16 @@ def ConvNeXtXLarge(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"convnext_xlarge\",\n ):\n     return ConvNeXt(\n+        weights_name=\"convnext_xlarge\",\n         depths=MODEL_CONFIGS[\"xlarge\"][\"depths\"],\n         projection_dims=MODEL_CONFIGS[\"xlarge\"][\"projection_dims\"],\n         drop_path_rate=0.0,\n         layer_scale_init_value=1e-6,\n         default_size=MODEL_CONFIGS[\"xlarge\"][\"default_size\"],\n-        model_name=model_name,\n+        name=name,\n         include_top=include_top,\n         include_preprocessing=include_preprocessing,\n         weights=weights,\n\n@@ -113,6 +113,7 @@ def DenseNet(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"densenet\",\n ):\n     \"\"\"Instantiates the DenseNet architecture.\n \n@@ -176,6 +177,7 @@ def DenseNet(\n             `classifier_activation=None` to return the logits of the \"top\"\n             layer. When loading pretrained weights, `classifier_activation`\n             can only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A model instance.\n@@ -261,14 +263,7 @@ def DenseNet(\n         inputs = img_input\n \n     # Create model.\n-    if blocks == [6, 12, 24, 16]:\n-        model = Functional(inputs, x, name=\"densenet121\")\n-    elif blocks == [6, 12, 32, 32]:\n-        model = Functional(inputs, x, name=\"densenet169\")\n-    elif blocks == [6, 12, 48, 32]:\n-        model = Functional(inputs, x, name=\"densenet201\")\n-    else:\n-        model = Functional(inputs, x, name=\"densenet\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n@@ -337,6 +332,7 @@ def DenseNet121(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"densenet121\",\n ):\n     \"\"\"Instantiates the Densenet121 architecture.\"\"\"\n     return DenseNet(\n@@ -348,6 +344,7 @@ def DenseNet121(\n         pooling,\n         classes,\n         classifier_activation,\n+        name=name,\n     )\n \n \n@@ -365,6 +362,7 @@ def DenseNet169(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"densenet169\",\n ):\n     \"\"\"Instantiates the Densenet169 architecture.\"\"\"\n     return DenseNet(\n@@ -376,6 +374,7 @@ def DenseNet169(\n         pooling,\n         classes,\n         classifier_activation,\n+        name=name,\n     )\n \n \n@@ -393,6 +392,7 @@ def DenseNet201(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"densenet201\",\n ):\n     \"\"\"Instantiates the Densenet201 architecture.\"\"\"\n     return DenseNet(\n@@ -404,6 +404,7 @@ def DenseNet201(\n         pooling,\n         classes,\n         classifier_activation,\n+        name=name,\n     )\n \n \n@@ -476,6 +477,7 @@ Args:\n         `classifier_activation=None` to return the logits\n         of the \"top\" layer. When loading pretrained weights,\n         `classifier_activation` can only be `None` or `\"softmax\"`.\n+    name: The name of the model (string).\n \n Returns:\n     A Keras model instance.\n\n@@ -195,6 +195,7 @@ Args:\n         Defaults to `'softmax'`.\n         When loading pretrained weights, `classifier_activation` can only\n         be `None` or `\"softmax\"`.\n+    name: The name of the model (string).\n \n Returns:\n     A model instance.\n@@ -213,7 +214,7 @@ def EfficientNet(\n     depth_divisor=8,\n     activation=\"swish\",\n     blocks_args=\"default\",\n-    model_name=\"efficientnet\",\n+    name=\"efficientnet\",\n     include_top=True,\n     weights=\"imagenet\",\n     input_tensor=None,\n@@ -221,6 +222,7 @@ def EfficientNet(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    weights_name=None,\n ):\n     \"\"\"Instantiates the EfficientNet architecture.\n \n@@ -233,7 +235,7 @@ def EfficientNet(\n       depth_divisor: integer, a unit of network width.\n       activation: activation function.\n       blocks_args: list of dicts, parameters to construct block modules.\n-      model_name: string, model name.\n+      name: string, model name.\n       include_top: whether to include the fully-connected\n           layer at the top of the network.\n       weights: one of `None` (random initialization),\n@@ -411,17 +413,17 @@ def EfficientNet(\n         inputs = img_input\n \n     # Create model.\n-    model = Functional(inputs, x, name=model_name)\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n         if include_top:\n             file_suffix = \".h5\"\n-            file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n+            file_hash = WEIGHTS_HASHES[weights_name][0]\n         else:\n             file_suffix = \"_notop.h5\"\n-            file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n-        file_name = model_name + file_suffix\n+            file_hash = WEIGHTS_HASHES[weights_name][1]\n+        file_name = name + file_suffix\n         weights_path = file_utils.get_file(\n             file_name,\n             BASE_WEIGHTS_PATH + file_name,\n@@ -563,14 +565,14 @@ def EfficientNetB0(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb0\",\n ):\n     return EfficientNet(\n         1.0,\n         1.0,\n         224,\n         0.2,\n-        model_name=\"efficientnetb0\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -578,7 +580,7 @@ def EfficientNetB0(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b0\",\n     )\n \n \n@@ -596,14 +598,14 @@ def EfficientNetB1(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb1\",\n ):\n     return EfficientNet(\n         1.0,\n         1.1,\n         240,\n         0.2,\n-        model_name=\"efficientnetb1\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -611,7 +613,7 @@ def EfficientNetB1(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b1\",\n     )\n \n \n@@ -629,14 +631,14 @@ def EfficientNetB2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb2\",\n ):\n     return EfficientNet(\n         1.1,\n         1.2,\n         260,\n         0.3,\n-        model_name=\"efficientnetb2\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -644,7 +646,7 @@ def EfficientNetB2(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b2\",\n     )\n \n \n@@ -662,14 +664,14 @@ def EfficientNetB3(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb3\",\n ):\n     return EfficientNet(\n         1.2,\n         1.4,\n         300,\n         0.3,\n-        model_name=\"efficientnetb3\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -677,7 +679,7 @@ def EfficientNetB3(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b3\",\n     )\n \n \n@@ -695,14 +697,14 @@ def EfficientNetB4(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb4\",\n ):\n     return EfficientNet(\n         1.4,\n         1.8,\n         380,\n         0.4,\n-        model_name=\"efficientnetb4\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -710,7 +712,7 @@ def EfficientNetB4(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b4\",\n     )\n \n \n@@ -728,14 +730,14 @@ def EfficientNetB5(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb5\",\n ):\n     return EfficientNet(\n         1.6,\n         2.2,\n         456,\n         0.4,\n-        model_name=\"efficientnetb5\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -743,7 +745,7 @@ def EfficientNetB5(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b5\",\n     )\n \n \n@@ -761,14 +763,14 @@ def EfficientNetB6(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb6\",\n ):\n     return EfficientNet(\n         1.8,\n         2.6,\n         528,\n         0.5,\n-        model_name=\"efficientnetb6\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -776,7 +778,7 @@ def EfficientNetB6(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b6\",\n     )\n \n \n@@ -794,14 +796,14 @@ def EfficientNetB7(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs,\n+    name=\"efficientnetb7\",\n ):\n     return EfficientNet(\n         2.0,\n         3.1,\n         600,\n         0.5,\n-        model_name=\"efficientnetb7\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -809,7 +811,7 @@ def EfficientNetB7(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs,\n+        weights_name=\"b7\",\n     )\n \n \n\n@@ -579,6 +579,7 @@ Args:\n         Defaults to `\"softmax\"`.\n         When loading pretrained weights, `classifier_activation` can only\n         be `None` or `\"softmax\"`.\n+    name: The name of the model (string).\n \n Returns:\n     A model instance.\n@@ -830,7 +831,7 @@ def EfficientNetV2(\n     bn_momentum=0.9,\n     activation=\"swish\",\n     blocks_args=\"default\",\n-    model_name=\"efficientnetv2\",\n+    name=\"efficientnetv2\",\n     include_top=True,\n     weights=\"imagenet\",\n     input_tensor=None,\n@@ -839,6 +840,7 @@ def EfficientNetV2(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    weights_name=None,\n ):\n     \"\"\"Instantiates the EfficientNetV2 architecture using given scaling\n     coefficients.\n@@ -854,7 +856,7 @@ def EfficientNetV2(\n         bn_momentum: float. Momentum parameter for Batch Normalization layers.\n         activation: activation function.\n         blocks_args: list of dicts, parameters to construct block modules.\n-        model_name: string, model name.\n+        name: string, model name.\n         include_top: whether to include the fully-connected layer at the top of\n             the network.\n         weights: one of `None` (random initialization), `\"imagenet\"`\n@@ -888,7 +890,7 @@ def EfficientNetV2(\n     \"\"\"\n \n     if blocks_args == \"default\":\n-        blocks_args = DEFAULT_BLOCKS_ARGS[model_name]\n+        blocks_args = DEFAULT_BLOCKS_ARGS[name]\n \n     if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n         raise ValueError(\n@@ -931,7 +933,7 @@ def EfficientNetV2(\n         # Apply original V1 preprocessing for Bx variants\n         # if number of channels allows it\n         num_channels = input_shape[bn_axis - 1]\n-        if model_name.split(\"-\")[-1].startswith(\"b\") and num_channels == 3:\n+        if name.split(\"-\")[-1].startswith(\"b\") and num_channels == 3:\n             x = layers.Rescaling(scale=1.0 / 255)(x)\n             x = layers.Normalization(\n                 mean=[0.485, 0.456, 0.406],\n@@ -1057,17 +1059,17 @@ def EfficientNetV2(\n         inputs = img_input\n \n     # Create model.\n-    model = Functional(inputs, x, name=model_name)\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n         if include_top:\n             file_suffix = \".h5\"\n-            file_hash = WEIGHTS_HASHES[model_name[-2:]][0]\n+            file_hash = WEIGHTS_HASHES[weights_name][0]\n         else:\n             file_suffix = \"_notop.h5\"\n-            file_hash = WEIGHTS_HASHES[model_name[-2:]][1]\n-        file_name = model_name + file_suffix\n+            file_hash = WEIGHTS_HASHES[weights_name][1]\n+        file_name = name + file_suffix\n         weights_path = file_utils.get_file(\n             file_name,\n             BASE_WEIGHTS_PATH + file_name,\n@@ -1096,12 +1098,13 @@ def EfficientNetV2B0(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-b0\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.0,\n         depth_coefficient=1.0,\n         default_size=224,\n-        model_name=\"efficientnetv2-b0\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1110,6 +1113,7 @@ def EfficientNetV2B0(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"b0\",\n     )\n \n \n@@ -1128,12 +1132,13 @@ def EfficientNetV2B1(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-b1\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.0,\n         depth_coefficient=1.1,\n         default_size=240,\n-        model_name=\"efficientnetv2-b1\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1142,6 +1147,7 @@ def EfficientNetV2B1(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"b1\",\n     )\n \n \n@@ -1160,12 +1166,13 @@ def EfficientNetV2B2(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-b2\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.1,\n         depth_coefficient=1.2,\n         default_size=260,\n-        model_name=\"efficientnetv2-b2\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1174,6 +1181,7 @@ def EfficientNetV2B2(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"b2\",\n     )\n \n \n@@ -1192,12 +1200,13 @@ def EfficientNetV2B3(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-b3\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.2,\n         depth_coefficient=1.4,\n         default_size=300,\n-        model_name=\"efficientnetv2-b3\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1206,6 +1215,7 @@ def EfficientNetV2B3(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"b3\",\n     )\n \n \n@@ -1224,12 +1234,13 @@ def EfficientNetV2S(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-s\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.0,\n         depth_coefficient=1.0,\n         default_size=384,\n-        model_name=\"efficientnetv2-s\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1238,6 +1249,7 @@ def EfficientNetV2S(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"-s\",\n     )\n \n \n@@ -1256,12 +1268,13 @@ def EfficientNetV2M(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-m\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.0,\n         depth_coefficient=1.0,\n         default_size=480,\n-        model_name=\"efficientnetv2-m\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1270,6 +1283,7 @@ def EfficientNetV2M(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"-m\",\n     )\n \n \n@@ -1288,12 +1302,13 @@ def EfficientNetV2L(\n     classes=1000,\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n+    name=\"efficientnetv2-l\",\n ):\n     return EfficientNetV2(\n         width_coefficient=1.0,\n         depth_coefficient=1.0,\n         default_size=480,\n-        model_name=\"efficientnetv2-l\",\n+        name=name,\n         include_top=include_top,\n         weights=weights,\n         input_tensor=input_tensor,\n@@ -1302,6 +1317,7 @@ def EfficientNetV2L(\n         classes=classes,\n         classifier_activation=classifier_activation,\n         include_preprocessing=include_preprocessing,\n+        weights_name=\"-l\",\n     )\n \n \n\n@@ -27,6 +27,7 @@ def InceptionResNetV2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"inception_resnet_v2\",\n ):\n     \"\"\"Instantiates the Inception-ResNet v2 architecture.\n \n@@ -88,6 +89,7 @@ def InceptionResNetV2(\n             Set `classifier_activation=None` to return the logits\n             of the \"top\" layer. When loading pretrained weights,\n             `classifier_activation` can only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A model instance.\n@@ -213,7 +215,7 @@ def InceptionResNetV2(\n         inputs = img_input\n \n     # Create model.\n-    model = Functional(inputs, x, name=\"inception_resnet_v2\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n\n@@ -30,6 +30,7 @@ def InceptionV3(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"inception_v3\",\n ):\n     \"\"\"Instantiates the Inception v3 architecture.\n \n@@ -91,6 +92,7 @@ def InceptionV3(\n             Set `classifier_activation=None` to return the logits of the \"top\"\n             layer. When loading pretrained weights, `classifier_activation`\n             can only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A model instance.\n@@ -353,7 +355,7 @@ def InceptionV3(\n     else:\n         inputs = img_input\n     # Create model.\n-    model = Functional(inputs, x, name=\"inception_v3\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n\n@@ -31,6 +31,7 @@ def NASNet(\n     classes=1000,\n     default_size=None,\n     classifier_activation=\"softmax\",\n+    name=\"NASNet\",\n ):\n     \"\"\"Instantiates a NASNet model.\n \n@@ -105,6 +106,7 @@ def NASNet(\n             Set `classifier_activation=None` to return the logits\n             of the \"top\" layer. When loading pretrained weights,\n             `classifier_activation` can only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A model instance.\n@@ -179,8 +181,8 @@ def NASNet(\n     if penultimate_filters % (24 * (filter_multiplier**2)) != 0:\n         raise ValueError(\n             \"For NASNet-A models, the `penultimate_filters` must be a multiple \"\n-            \"of 24 * (`filter_multiplier` ** 2). Current value: %d\"\n-            % penultimate_filters\n+            \"of 24 * (`filter_multiplier` ** 2). \"\n+            f\"Current value: {penultimate_filters}\"\n         )\n \n     channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n@@ -209,10 +211,10 @@ def NASNet(\n     )\n \n     for i in range(num_blocks):\n-        x, p = _normal_a_cell(x, p, filters, block_id=\"%d\" % (i))\n+        x, p = _normal_a_cell(x, p, filters, block_id=f\"{i}\")\n \n     x, p0 = _reduction_a_cell(\n-        x, p, filters * filter_multiplier, block_id=\"reduce_%d\" % (num_blocks)\n+        x, p, filters * filter_multiplier, block_id=f\"reduce_{num_blocks}\"\n     )\n \n     p = p0 if not skip_reduction else p\n@@ -222,14 +224,14 @@ def NASNet(\n             x,\n             p,\n             filters * filter_multiplier,\n-            block_id=\"%d\" % (num_blocks + i + 1),\n+            block_id=f\"{num_blocks + i + 1}\",\n         )\n \n     x, p0 = _reduction_a_cell(\n         x,\n         p,\n         filters * filter_multiplier**2,\n-        block_id=\"reduce_%d\" % (2 * num_blocks),\n+        block_id=f\"reduce_{2 * num_blocks}\",\n     )\n \n     p = p0 if not skip_reduction else p\n@@ -239,7 +241,7 @@ def NASNet(\n             x,\n             p,\n             filters * filter_multiplier**2,\n-            block_id=\"%d\" % (2 * num_blocks + i + 1),\n+            block_id=f\"{2 * num_blocks + i + 1}\",\n         )\n \n     x = layers.Activation(\"relu\")(x)\n@@ -263,7 +265,7 @@ def NASNet(\n     else:\n         inputs = img_input\n \n-    model = Functional(inputs, x, name=\"NASNet\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n@@ -324,6 +326,7 @@ def NASNetMobile(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"nasnet_mobile\",\n ):\n     \"\"\"Instantiates a Mobile NASNet model in ImageNet mode.\n \n@@ -374,6 +377,7 @@ def NASNetMobile(\n             `classifier_activation=None` to return the logits of the \"top\"\n             layer.  When loading pretrained weights, `classifier_activation` can\n             only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A Keras model instance.\n@@ -400,6 +404,7 @@ def NASNetMobile(\n         classes=classes,\n         default_size=224,\n         classifier_activation=classifier_activation,\n+        name=name,\n     )\n \n \n@@ -417,6 +422,7 @@ def NASNetLarge(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"nasnet_large\",\n ):\n     \"\"\"Instantiates a NASNet model in ImageNet mode.\n \n@@ -467,6 +473,7 @@ def NASNetLarge(\n             `classifier_activation=None` to return the logits of the \"top\"\n             layer.  When loading pretrained weights, `classifier_activation`\n             can only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A Keras model instance.\n@@ -485,6 +492,7 @@ def NASNetLarge(\n         classes=classes,\n         default_size=331,\n         classifier_activation=classifier_activation,\n+        name=name,\n     )\n \n \n\n@@ -49,7 +49,6 @@ def ResNet(\n     stack_fn,\n     preact,\n     use_bias,\n-    model_name=\"resnet\",\n     include_top=True,\n     weights=\"imagenet\",\n     input_tensor=None,\n@@ -57,6 +56,8 @@ def ResNet(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet\",\n+    weights_name=None,\n ):\n     \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\n \n@@ -67,7 +68,7 @@ def ResNet(\n             `False` for ResNet and ResNeXt.\n         use_bias: Whether to use biases for convolutional layers or not.\n             `True` for ResNet and ResNetV2, `False` for ResNeXt.\n-        model_name: Name of the model.\n+        name: Name of the model.\n         include_top: Whether to include the fully-connected\n             layer at the top of the network.\n         weights: One of `None` (random initialization),\n@@ -100,6 +101,7 @@ def ResNet(\n             return the logits of the \"top\" layer. When loading\n             pretrained weights, `classifier_activation` can only be\n             `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A Model instance.\n@@ -189,18 +191,18 @@ def ResNet(\n         inputs = img_input\n \n     # Create model.\n-    model = Functional(inputs, x, name=model_name)\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n-    if (weights == \"imagenet\") and (model_name in WEIGHTS_HASHES):\n+    if (weights == \"imagenet\") and (weights_name in WEIGHTS_HASHES):\n         if include_top:\n-            file_name = model_name + \"_weights_tf_dim_ordering_tf_kernels.h5\"\n-            file_hash = WEIGHTS_HASHES[model_name][0]\n+            file_name = weights_name + \"_weights_tf_dim_ordering_tf_kernels.h5\"\n+            file_hash = WEIGHTS_HASHES[weights_name][0]\n         else:\n             file_name = (\n-                model_name + \"_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n+                weights_name + \"_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n             )\n-            file_hash = WEIGHTS_HASHES[model_name][1]\n+            file_hash = WEIGHTS_HASHES[weights_name][1]\n         weights_path = file_utils.get_file(\n             file_name,\n             BASE_WEIGHTS_PATH + file_name,\n@@ -394,6 +396,7 @@ def ResNet50(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet50\",\n ):\n     \"\"\"Instantiates the ResNet50 architecture.\"\"\"\n \n@@ -405,15 +408,16 @@ def ResNet50(\n \n     return ResNet(\n         stack_fn,\n-        False,\n-        True,\n-        \"resnet50\",\n-        include_top,\n-        weights,\n-        input_tensor,\n-        input_shape,\n-        pooling,\n-        classes,\n+        preact=False,\n+        use_bias=True,\n+        weights_name=\"resnet50\",\n+        name=name,\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n         classifier_activation=classifier_activation,\n     )\n \n@@ -432,6 +436,7 @@ def ResNet101(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet101\",\n ):\n     \"\"\"Instantiates the ResNet101 architecture.\"\"\"\n \n@@ -443,15 +448,16 @@ def ResNet101(\n \n     return ResNet(\n         stack_fn,\n-        False,\n-        True,\n-        \"resnet101\",\n-        include_top,\n-        weights,\n-        input_tensor,\n-        input_shape,\n-        pooling,\n-        classes,\n+        preact=False,\n+        use_bias=True,\n+        name=name,\n+        weights_name=\"resnet101\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n         classifier_activation=classifier_activation,\n     )\n \n@@ -470,6 +476,7 @@ def ResNet152(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet152\",\n ):\n     \"\"\"Instantiates the ResNet152 architecture.\"\"\"\n \n@@ -481,15 +488,16 @@ def ResNet152(\n \n     return ResNet(\n         stack_fn,\n-        False,\n-        True,\n-        \"resnet152\",\n-        include_top,\n-        weights,\n-        input_tensor,\n-        input_shape,\n-        pooling,\n-        classes,\n+        preact=False,\n+        use_bias=True,\n+        name=name,\n+        weights_name=\"resnet152\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n         classifier_activation=classifier_activation,\n     )\n \n@@ -572,6 +580,7 @@ Args:\n         `classifier_activation=None` to return the logits of the \"top\" layer.\n         When loading pretrained weights, `classifier_activation` can only\n         be `None` or `\"softmax\"`.\n+    name: The name of the model (string).\n \n Returns:\n     A Model instance.\n\n@@ -17,6 +17,7 @@ def ResNet50V2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet50v2\",\n ):\n     \"\"\"Instantiates the ResNet50V2 architecture.\"\"\"\n \n@@ -32,13 +33,14 @@ def ResNet50V2(\n         stack_fn,\n         True,\n         True,\n-        \"resnet50v2\",\n-        include_top,\n-        weights,\n-        input_tensor,\n-        input_shape,\n-        pooling,\n-        classes,\n+        name=name,\n+        weights_name=\"resnet50v2\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n         classifier_activation=classifier_activation,\n     )\n \n@@ -57,6 +59,7 @@ def ResNet101V2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet101v2\",\n ):\n     \"\"\"Instantiates the ResNet101V2 architecture.\"\"\"\n \n@@ -72,13 +75,14 @@ def ResNet101V2(\n         stack_fn,\n         True,\n         True,\n-        \"resnet101v2\",\n-        include_top,\n-        weights,\n-        input_tensor,\n-        input_shape,\n-        pooling,\n-        classes,\n+        name=name,\n+        weights_name=\"resnet101v2\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n         classifier_activation=classifier_activation,\n     )\n \n@@ -97,6 +101,7 @@ def ResNet152V2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"resnet152v2\",\n ):\n     \"\"\"Instantiates the ResNet152V2 architecture.\"\"\"\n \n@@ -112,13 +117,14 @@ def ResNet152V2(\n         stack_fn,\n         True,\n         True,\n-        \"resnet152v2\",\n-        include_top,\n-        weights,\n-        input_tensor,\n-        input_shape,\n-        pooling,\n-        classes,\n+        name=name,\n+        weights_name=\"resnet152v2\",\n+        include_top=include_top,\n+        weights=weights,\n+        input_tensor=input_tensor,\n+        input_shape=input_shape,\n+        pooling=pooling,\n+        classes=classes,\n         classifier_activation=classifier_activation,\n     )\n \n@@ -191,6 +197,7 @@ Args:\n         `classifier_activation=None` to return the logits of the \"top\" layer.\n         When loading pretrained weights, `classifier_activation` can only\n         be `None` or `\"softmax\"`.\n+    name: The name of the model (string).\n \n Returns:\n     A Model instance.\n\n@@ -26,6 +26,7 @@ def VGG16(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"vgg16\",\n ):\n     \"\"\"Instantiates the VGG16 model.\n \n@@ -86,9 +87,10 @@ def VGG16(\n             `classifier_activation=None` to return the logits of the \"top\"\n             layer.  When loading pretrained weights, `classifier_activation`\n             can only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n-        A model instance.\n+        A `Model` instance.\n     \"\"\"\n     if not (weights in {\"imagenet\", None} or file_utils.exists(weights)):\n         raise ValueError(\n@@ -201,7 +203,7 @@ def VGG16(\n         inputs = img_input\n \n     # Create model.\n-    model = Functional(inputs, x, name=\"vgg16\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n\n@@ -26,6 +26,7 @@ def VGG19(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"vgg19\",\n ):\n     \"\"\"Instantiates the VGG19 model.\n \n@@ -86,6 +87,7 @@ def VGG19(\n             `classifier_activation=None` to return the logits of the \"top\"\n             layer.  When loading pretrained weights, `classifier_activation` can\n             only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A model instance.\n@@ -209,7 +211,7 @@ def VGG19(\n         inputs = img_input\n \n     # Create model.\n-    model = Functional(inputs, x, name=\"vgg19\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n\n@@ -30,6 +30,7 @@ def Xception(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n+    name=\"xception\",\n ):\n     \"\"\"Instantiates the Xception architecture.\n \n@@ -86,6 +87,7 @@ def Xception(\n             `classifier_activation=None` to return the logits of the \"top\"\n             layer.  When loading pretrained weights, `classifier_activation` can\n             only be `None` or `\"softmax\"`.\n+        name: The name of the model (string).\n \n     Returns:\n         A model instance.\n@@ -308,7 +310,7 @@ def Xception(\n     else:\n         inputs = img_input\n     # Create model.\n-    model = Functional(inputs, x, name=\"xception\")\n+    model = Functional(inputs, x, name=name)\n \n     # Load weights.\n     if weights == \"imagenet\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
