{"custom_id": "keras#0ba72929ebb79a9875757cd6cb8cf156078761ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 77 | Files Changed: 2 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 105 | Churn Cumulative: 1806 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.1282051282051282\n\nDIFF:\n@@ -134,6 +134,9 @@ class _EagerVariableStore(object):\n   interaction between `tf.function` `FuncGraph` internals, Keras\n   Functional Models, and TPUStrategy variable initialization.\n \n+  Also, it always acts as if reuse is set to either \"TRUE\" or\n+  tf.compat.v1.AUTO_REUSE\n+\n   Attributes:\n     vars: a dictionary with string names (same as passed in GetVar) as keys and\n       the corresponding TensorFlow Variables as values.\n@@ -171,9 +174,9 @@ class _EagerVariableStore(object):\n     variable. Otherwise, we create a new one.\n \n     Set `reuse` to `True` when you only want to reuse existing Variables.\n-    Set `reuse` to `False` when you only want to create new Variables.\n     Set `reuse` to None (the default) or tf.compat.v1.AUTO_REUSE when you want\n     variables to be created if they don't exist or returned if they do.\n+    In this shim, `reuse` of `False` will be treated as auto-reuse.\n \n     If initializer is `None` (the default), the default initializer passed in\n     the constructor is used. If that one is `None` too, we use a new\n@@ -272,7 +275,7 @@ class _EagerVariableStore(object):\n     # lifted from a function-building graph into the eager context (that's why\n     # the following clause is not wrapped in an `init_scope`); lifted variables\n     # are tracked by the graph's `VariableStore`.\n-    if tf.executing_eagerly():\n+    if not reuse:\n       reuse = tf.compat.v1.AUTO_REUSE\n \n     # If a *_ref type is passed in an error would be triggered further down the\n@@ -437,12 +440,6 @@ class _EagerVariableStore(object):\n \n     if name in self._vars:\n       # Here we handle the case when returning an existing variable.\n-      if reuse is False:  # pylint: disable=g-bool-id-comparison\n-        err_msg = (\"Variable %s already exists, disallowed.\"\n-                   \" Did you mean to set reuse=True or \"\n-                   \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n-        # ResourceVariables don't have an op associated with so no traceback\n-        raise ValueError(err_msg)\n       found_var = self._vars[name]\n       if not shape.is_compatible_with(found_var.get_shape()):\n         raise ValueError(\"Trying to share variable %s, but specified shape %s\"\n\n@@ -246,6 +246,29 @@ class VariableScopeTest(tf.test.TestCase):\n       test_value(13.)  # Variable is reused hereafter.\n       test_value(17.)\n \n+  @test_util.run_in_graph_and_eager_modes\n+  @run_inside_wrap_function_in_eager_mode\n+  def testVarScopeGetOrCreateReuseIgnoreFalse(self):\n+    with self.cached_session():\n+\n+      def test_value(value):\n+        x = tf.constant(value)\n+        with tf.compat.v1.variable_scope(\n+            \"testVarScopeGetOrCreateReuse_bar\",\n+            reuse=False):\n+          _ = tf.compat.v1.assign(tf.compat.v1.get_variable(\"var\", []), x)\n+        # We need to ignore reuse=False in the shim, because the\n+        # code is expected to get rerun each time the user calls the shim.\n+        with tf.compat.v1.variable_scope(\n+            \"testVarScopeGetOrCreateReuse_bar\",\n+            reuse=False):\n+          _ = tf.compat.v1.get_variable(\"var\", [])\n+        self.assertEqual(value, self.evaluate(x))\n+\n+      test_value(42.)  # Variable is created.\n+      test_value(13.)  # Variable is reused hereafter.\n+      test_value(17.)\n+\n   @test_util.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScope(self):\n@@ -750,75 +773,6 @@ class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n \n class VariableScopeMultithreadedTest(tf.test.TestCase):\n \n-  @test_util.run_in_graph_and_eager_modes\n-  @run_inside_wrap_function_in_eager_mode\n-  def testTwoThreadsDisjointScopeEntry(self):\n-\n-    def thread_fn(i, graph):\n-      with graph.as_default():\n-        with tf.compat.v1.variable_scope(\"foo\"):\n-          if i == 0:\n-            v = tf.compat.v1.get_variable(\"v\", [])\n-            self.assertEqual(\"foo/v:0\", v.name)\n-          else:\n-            # Any thread after the first one should fail to create variable\n-            # with the same name.\n-            with self.assertRaises(ValueError):\n-              tf.compat.v1.get_variable(\"v\", [])\n-\n-    graph = tf.compat.v1.get_default_graph()\n-    threads = [\n-        threading.Thread(target=thread_fn, args=(\n-            i,\n-            graph,\n-        )) for i in range(2)\n-    ]\n-\n-    threads[0].start()\n-    # Allow thread 0 to finish before starting thread 1.\n-    threads[0].join()\n-    threads[1].start()\n-    threads[1].join()\n-\n-  @test_util.run_in_graph_and_eager_modes\n-  @run_inside_wrap_function_in_eager_mode\n-  def testTwoThreadsNestedScopeEntry(self):\n-\n-    def thread_fn(i, graph, run_event, pause_event):\n-      with graph.as_default():\n-        with tf.compat.v1.variable_scope(\"foo\"):\n-          if i == 0:\n-            v = tf.compat.v1.get_variable(\"v\", [])\n-            self.assertEqual(\"foo/v:0\", v.name)\n-          else:\n-            # Any thread after the first one should fail to create variable\n-            # with the same name.\n-            with self.assertRaises(ValueError):\n-              tf.compat.v1.get_variable(\"v\", [])\n-          pause_event.set()\n-          run_event.wait()\n-\n-    graph = tf.compat.v1.get_default_graph()\n-    run_events = [threading.Event() for _ in range(2)]\n-    pause_events = [threading.Event() for _ in range(2)]\n-    threads = [\n-        threading.Thread(\n-            target=thread_fn, args=(i, graph, run_events[i], pause_events[i]))\n-        for i in range(2)\n-    ]\n-\n-    # Start first thread.\n-    threads[0].start()\n-    pause_events[0].wait()\n-    # Start next thread once the first thread has paused.\n-    threads[1].start()\n-    pause_events[1].wait()\n-    # Resume both threads.\n-    run_events[0].set()\n-    run_events[1].set()\n-    threads[0].join()\n-    threads[1].join()\n-\n   @test_util.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testReenterMainScope(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f3f23fb6f5ab84a80b3fdfcc18e6c81ddfc7cdc3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 707 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -473,8 +473,7 @@ class Layer(base_layer.Layer):\n             # When the shim to get variable scope working in TF2 is used,\n             # We need to explicitly make the shim track the regularization\n             # losses as the collections will not be accessible.\n-            if isinstance(var_store,\n-                          variable_scope_shim._EagerVariableStore):  # pylint: disable=protected-access\n+            if hasattr(var_store, 'add_regularizer'):\n               var_store.add_regularizer(variable, regularizer)\n \n         if init_graph is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc48a179d9cbb730bd5910efdf9d53cb1a9b2120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 20 | Methods Changed: 10 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 66 | Churn Cumulative: 3537 | Contributors (this commit): 5 | Commits (past 90d): 22 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -335,10 +335,6 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n   def update_state(self, data):\n-    if isinstance(data, (list, tuple, np.ndarray)):\n-      data = tf.convert_to_tensor(data)\n-    if data.shape.rank == 1:\n-      data = tf.expand_dims(data, axis=-1)\n     self._index_lookup_layer.update_state(self._preprocess(data))\n \n   def finalize_state(self):\n@@ -485,23 +481,29 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n       return inputs\n \n     lookup_data = self._index_lookup_layer(inputs)\n-    if self._output_mode == INT:\n \n-      # Maybe trim the output (NOOP if self._output_sequence_length is None).\n-      output_tensor = lookup_data[..., :self._output_sequence_length]\n+    # For any non-int output, we can return directly from the underlying layer.\n+    if self._output_mode is not INT:\n+      return lookup_data\n \n-      output_shape = output_tensor.shape.as_list()\n-      output_shape[-1] = self._output_sequence_length\n+    # If we have a ragged tensor, we can pad during the conversion to dense.\n+    if tf_utils.is_ragged(lookup_data):\n+      shape = lookup_data.shape.as_list()\n+      # If output sequence length is None, to_tensor will pad the last dimension\n+      # to the bounding shape of the ragged dimension.\n+      shape[-1] = self._output_sequence_length\n+      return lookup_data.to_tensor(default_value=0, shape=shape)\n \n-      # If it is a ragged tensor, convert it to dense with correct shape.\n-      if tf_utils.is_ragged(output_tensor):\n-        return output_tensor.to_tensor(default_value=0, shape=output_shape)\n+    # If we have a dense tensor, we need to pad/trim directly.\n+    if self._output_sequence_length is not None:\n+      # Maybe trim the output.\n+      lookup_data = lookup_data[..., :self._output_sequence_length]\n \n-      if self._output_sequence_length is None:\n-        return output_tensor\n-\n-      padding, _ = tf.required_space_to_batch_paddings(\n-          output_tensor.shape, output_shape)\n-      return tf.pad(output_tensor, padding)\n+      # Maybe pad the output. We need to be careful to use dynamic shape here as\n+      # required_space_to_batch_paddings requires a fully known shape.\n+      shape = tf.shape(lookup_data)\n+      padded_shape = tf.concat((shape[:-1], [self._output_sequence_length]), 0)\n+      padding, _ = tf.required_space_to_batch_paddings(shape, padded_shape)\n+      return tf.pad(lookup_data, padding)\n \n     return lookup_data\n\n@@ -318,11 +318,9 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1])\n     layer.set_vocabulary([\"earth\", \"wind\", \"and\", \"fire\"])\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1])\n \n   def test_scalar_input_int_mode_trim_to_len_limit(self):\n@@ -333,11 +331,9 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     layer = text_vectorization.TextVectorization(output_sequence_length=3)\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [2, 3, 4])\n     layer.set_vocabulary([\"earth\", \"wind\", \"and\", \"fire\"])\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [2, 3, 4])\n \n   def test_scalar_input_int_pad_to_len_limit(self):\n@@ -348,11 +344,9 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     layer = text_vectorization.TextVectorization(output_sequence_length=10)\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])\n     layer.set_vocabulary([\"earth\", \"wind\", \"and\", \"fire\"])\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [2, 3, 4, 5, 5, 4, 2, 1, 0, 0])\n \n   def test_list_inputs_1d(self):\n@@ -361,11 +355,9 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n     layer.set_vocabulary([\"two\", \"three\", \"four\", \"five\"])\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n \n   def test_tensor_inputs(self):\n@@ -375,11 +367,9 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n     layer.set_vocabulary([\"two\", \"three\", \"four\", \"five\"])\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n \n   def test_list_inputs_2d(self):\n@@ -389,22 +379,30 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_data)\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n     layer.set_vocabulary([\"two\", \"three\", \"four\", \"five\"])\n     out = layer(input_data)\n-    if tf.executing_eagerly():\n     self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n \n   def test_dataset_of_single_strings(self):\n     vocab_data = [\"two two two\", \"two three three\", \"three four four five\"]\n     input_data = [\"two three\", \"four five\"]\n     vocab_ds = tf.data.Dataset.from_tensor_slices(vocab_data)  # unbatched\n+    input_ds = tf.data.Dataset.from_tensor_slices(input_data)  # unbatched\n     layer = text_vectorization.TextVectorization()\n     layer.adapt(vocab_ds)\n-    out = layer(input_data)\n-    if tf.executing_eagerly():\n-      self.assertAllClose(out.numpy(), [[2, 3], [4, 5]])\n+    out = input_ds.map(layer)\n+    self.assertAllClose(list(out.as_numpy_iterator()), [[2, 3], [4, 5]])\n+\n+  def test_dataset_of_single_strings_with_output_sequence(self):\n+    vocab_data = [\"two two two\", \"two three three\", \"three four four five\"]\n+    input_data = [\"two three\", \"four five\"]\n+    vocab_ds = tf.data.Dataset.from_tensor_slices(vocab_data)  # unbatched\n+    input_ds = tf.data.Dataset.from_tensor_slices(input_data)  # unbatched\n+    layer = text_vectorization.TextVectorization(output_sequence_length=3)\n+    layer.adapt(vocab_ds)\n+    out = input_ds.map(layer)\n+    self.assertAllClose(list(out.as_numpy_iterator()), [[2, 3, 0], [4, 5, 0]])\n \n   @parameterized.named_parameters(\n       {\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7cfd5ae75797924b1ab09072e2de06f56aaa250d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 33 | Churn Cumulative: 2520 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.45454545454545453\n\nDIFF:\n@@ -14,14 +14,16 @@\n # ==============================================================================\n \"\"\"Contains the loss scaling optimizer class.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n from keras import backend\n from keras import optimizers\n from keras.mixed_precision import loss_scale as keras_loss_scale_module\n from keras.optimizer_v2 import optimizer_v2\n from keras.optimizer_v2 import utils as optimizer_utils\n+\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.keras.optimizer_v2 import optimizer_v2 as legacy_optimizer\n from tensorflow.python.platform import tf_logging\n-from tensorflow.python.training.tracking import base as trackable\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -498,8 +500,14 @@ class LossScaleOptimizer(_DelegatingTrackableMixin, optimizer_v2.OptimizerV2):\n   def __init__(self, inner_optimizer, dynamic=True, initial_scale=None,\n                dynamic_growth_steps=None):\n     if not isinstance(inner_optimizer, optimizer_v2.OptimizerV2):\n-      raise TypeError('\"inner_optimizer\" must be an instance of OptimizerV2, '\n-                      'but got: %s' % inner_optimizer)\n+      msg = ('\"inner_optimizer\" must be an instance of '\n+             '`tf.keras.optimizers.Optimizer`, but got: %s. ' % inner_optimizer)\n+      if isinstance(inner_optimizer, legacy_optimizer.OptimizerV2):\n+        msg += ('Please make sure \"inner_optimizer\" is not an instance of '\n+                '`tensorflow.python.keras.optimizers`, which is '\n+                'the legacy keras code and will be removed in future release. '\n+                'Please use the tf.keras public API instead.')\n+      raise TypeError(msg)\n     if not isinstance(dynamic, bool):\n       # Catch errors if a user incorrectly passes a string or float to the\n       # second argument argument, as this is commonly done for\n\n@@ -14,13 +14,10 @@\n # ==============================================================================\n \"\"\"Tests for LossScaleOptimizer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import os\n \n from absl.testing import parameterized\n-import numpy as np\n-from tensorflow.python.framework import test_util\n+\n from keras import combinations\n from keras import optimizers\n from keras.mixed_precision import loss_scale_optimizer\n@@ -29,6 +26,12 @@ from keras.optimizer_v2 import adam\n from keras.optimizer_v2 import gradient_descent\n from keras.optimizer_v2 import optimizer_v2\n \n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.framework import test_util\n+from tensorflow.python.keras.optimizer_v2 import gradient_descent as legacy_sgd\n+\n # Disable not-callable lint error, as the linter is unable to detect that\n # LossScale instances are callable.\n # pylint: disable=not-callable\n@@ -1077,6 +1080,12 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         '\"inner_optimizer\" is already wrapped by a LossScaleOptimizer.'):\n       loss_scale_optimizer.LossScaleOptimizer(inner_opt)\n \n+  def testErrorWhenWrappingLegacyKerasOptimizers(self):\n+    sgd = legacy_sgd.SGD()\n+    with self.assertRaisesRegex(\n+        TypeError, 'not an instance of `tensorflow.python.keras.optimizers`'):\n+      loss_scale_optimizer.LossScaleOptimizer(sgd)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d42537dff5a7fb288f3de7aef79e7a72b6d44a8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 54 | Churn Cumulative: 1860 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,6 +24,8 @@ import functools\n from keras.engine import base_layer\n from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n+from keras.utils import layer_utils\n+\n from tensorflow.python.ops import variable_scope as vs\n from tensorflow.python.platform import tf_logging as logging\n \n@@ -697,17 +699,21 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n     checkpoints.\n \n   Training Arg in `forward_pass`:\n-    Keras will pass a `training` arg to this layer similarly to how it\n-    passes `training` to other layers in TF2. See more details in the docs\n+    Keras will pass a `training` arg to this layer if `forward_pass` contains\n+    a `training` arg or a `**kwargs` varargs in its call signature,\n+    similarly to how keras passes `training` to other layers in TF2 that have\n+    similar signatures in their `call` implementations.\n+    See more details in the docs\n     on `tf.keras.layers.Layer` to understand what will be passed and when.\n     Note: tf.compat.v1.layers are usually not called with `training=None`,\n     so the training arg to `forward_pass` might not feed through to them\n     unless you pass it to their calls explicitly.\n \n   Call signature of the forward pass:\n-    The semantics of the forward pass signature roughly match the standard\n-    Keras layer `call` signature, except that a `training` arg will *always*\n-    be passed, so your `forward_pass` must accept either.\n+    The semantics of the forward pass signature match the standard\n+    Keras layer `call` signature, including how Keras decides when\n+    to pass in a `training` arg., and the semantics applied to\n+    the first positional arg in the call signature.\n \n   Limitations:\n     * TF2 will not prune unused variable updates (or unused outputs). You may\n@@ -730,6 +736,13 @@ class VariableScopeWrapperLayer(base_layer.Layer):\n     self.tracker = VariableAndLossTracker()\n     # May need to inspect func to see if it should pass a `training` arg or not\n \n+  @property\n+  @layer_utils.cached_per_instance\n+  def _call_full_argspec(self):\n+    # Argspec inspection is expensive and the call spec is used often, so it\n+    # makes sense to cache the result.\n+    return tf_inspect.getfullargspec(self.forward_pass)\n+\n   def forward_pass(self, *args, **kwargs):\n     raise NotImplementedError\n \n\n@@ -901,6 +901,37 @@ class TF1VariableScopeWrapperLayerTest(tf.test.TestCase, parameterized.TestCase)\n     self.assertAllEqual(out, tf.ones(shape=(5, 10)) * 200)\n     self.assertAllEqual(tf.add_n(layer.losses), 6)\n \n+  def test_training_arg(self):\n+    # Test the shim when using `compat.v1` layers\n+\n+    class TrainingCheckLayer(variable_scope_shim.VariableScopeWrapperLayer):\n+\n+      def __init__(self, units, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.units = units\n+\n+      def forward_pass(self, inputs, training=None):\n+        if training:\n+          out = core_layers.dense(inputs, self.units, name=\"dense_training\")\n+        else:\n+          out = core_layers.dense(inputs, self.units, name=\"dense_no_training\")\n+        return out\n+\n+    layer = TrainingCheckLayer(10)\n+    layer(tf.ones(shape=(5, 5)), training=True)\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct variables were made\n+    self.assertEqual(weights.keys(),\n+                     {\"dense_training/bias:0\", \"dense_training/kernel:0\"})\n+\n+    layer = TrainingCheckLayer(10)\n+    layer(tf.ones(shape=(5, 5)))\n+    weights = {x.name: x for x in layer.variables}\n+\n+    # Verify the correct variables were made\n+    self.assertEqual(weights.keys(),\n+                     {\"dense_no_training/bias:0\", \"dense_no_training/kernel:0\"})\n \n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#967817c22657b31d4bb08f41e118e4118012284b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 76 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 76 | Churn Cumulative: 2049 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 0.875\n\nDIFF:\n@@ -16,12 +16,9 @@\n \n import abc\n \n-from keras import backend\n from keras.engine import data_adapter\n from keras.engine.base_layer import Layer\n-from keras.utils import tf_utils\n from keras.utils import version_utils\n-import numpy as np\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.eager import context\n@@ -284,43 +281,6 @@ class PreprocessingLayer(Layer, metaclass=abc.ABCMeta):\n       self.built = True\n \n \n-def convert_to_list(values, sparse_default_value=None):\n-  \"\"\"Convert a TensorLike, CompositeTensor, or ndarray into a Python list.\"\"\"\n-  if tf_utils.is_ragged(values):\n-    # There is a corner case when dealing with ragged tensors: if you get an\n-    # actual RaggedTensor (not a RaggedTensorValue) passed in non-eager mode,\n-    # you can't call to_list() on it without evaluating it first. However,\n-    # because we don't yet fully support composite tensors across Keras,\n-    # backend.get_value() won't evaluate the tensor.\n-    # TODO(momernick): Get Keras to recognize composite tensors as Tensors\n-    # and then replace this with a call to backend.get_value.\n-    if (isinstance(values, tf.RaggedTensor) and\n-        not tf.executing_eagerly()):\n-      values = backend.get_session(values).run(values)\n-    values = values.to_list()\n-\n-  if isinstance(values,\n-                (tf.SparseTensor, tf.compat.v1.SparseTensorValue)):\n-    if sparse_default_value is None:\n-      if tf.as_dtype(values.values.dtype) == tf.string:\n-        sparse_default_value = ''\n-      else:\n-        sparse_default_value = -1\n-    dense_tensor = tf.sparse.to_dense(\n-        values, default_value=sparse_default_value)\n-    values = backend.get_value(dense_tensor)\n-\n-  if isinstance(values, tf.Tensor):\n-    values = backend.get_value(values)\n-\n-  # We may get passed a ndarray or the code above may give us a ndarray.\n-  # In either case, we want to force it into a standard python list.\n-  if isinstance(values, np.ndarray):\n-    values = values.tolist()\n-\n-  return values\n-\n-\n def _disallow_inside_tf_function(method_name):\n   \"\"\"Disallow calling a method inside a `tf.function`.\"\"\"\n   if tf.inside_function():\n\n@@ -15,8 +15,6 @@\n \"\"\"Tests for Keras' base preprocessing layer.\"\"\"\n \n import os\n-from absl.testing import parameterized\n-\n \n import keras\n from keras import keras_parameterized\n@@ -235,39 +233,5 @@ class PreprocessingLayerV1Test(keras_parameterized.TestCase):\n         layer.adapt(input_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class ConvertToListTest(keras_parameterized.TestCase):\n-\n-  # Note: We need the inputs to be lambdas below to avoid some strangeness with\n-  # TF1.x graph mode - specifically, if the inputs are created outside the test\n-  # function body, the graph inside the test body will not contain the tensors\n-  # that were created in the parameters.\n-  @parameterized.named_parameters(\n-      {\n-          \"testcase_name\": \"ndarray\",\n-          \"inputs\": lambda: np.array([[1, 2, 3], [4, 5, 6]]),\n-          \"expected\": [[1, 2, 3], [4, 5, 6]]\n-      }, {\n-          \"testcase_name\": \"list\",\n-          \"inputs\": lambda: [[1, 2, 3], [4, 5, 6]],\n-          \"expected\": [[1, 2, 3], [4, 5, 6]]\n-      }, {\n-          \"testcase_name\": \"tensor\",\n-          \"inputs\": lambda: tf.constant([[1, 2, 3], [4, 5, 6]]),\n-          \"expected\": [[1, 2, 3], [4, 5, 6]]\n-      }, {\n-          \"testcase_name\": \"ragged_tensor\",\n-          \"inputs\": lambda: tf.ragged.constant([[1, 2, 3, 4], [4, 5, 6]]),\n-          \"expected\": [[1, 2, 3, 4], [4, 5, 6]]\n-      }, {\n-          \"testcase_name\": \"sparse_tensor\",\n-          \"inputs\": lambda: tf.sparse.from_dense([[1, 2, 0, 4], [4, 5, 6, 0]]),\n-          \"expected\": [[1, 2, -1, 4], [4, 5, 6, -1]]\n-      })\n-  def test_conversion(self, inputs, expected):\n-    values = base_preprocessing_layer.convert_to_list(inputs())\n-    self.assertAllEqual(expected, values)\n-\n-\n if __name__ == \"__main__\":\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c368c21feae8b9e9548c21e0aa83a9b6ff860528", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 18446 | Contributors (this commit): 74 | Commits (past 90d): 5 | Contributors (cumulative): 74 | DMM Complexity: None\n\nDIFF:\n@@ -2259,16 +2259,16 @@ class DepthwiseConv2D(Conv2D):\n   \"\"\"Depthwise 2D convolution.\n \n   Depthwise convolution is a type of convolution in which each input channel is\n-  convolved with a different filter of depth 1 (called a depthwise kernel). You\n+  convolved with a different filter (called a depthwise kernel). You\n   can understand depthwise convolution as the first step in a depthwise\n   separable convolution.\n \n   It could be implemented via the following steps:\n \t\n   - Split the input into individual channels.\n-  - Repeat each input channel `depth_multiplier` many times.\n-  - Convolve each channel with an individual depthwise kernel.\n-  - Stack the convolved outputs together (along the channels axis).\n+  - Convolve each channel with an individual depthwise kernel having 1 input\n+    and `depth_multiplier` many output channels.\n+  - Concatenate the convolved outputs (along the channels axis).\n \n   Unlike a regular 2D convolution, depthwise convolution does not mix\n   information across different input channels.\n@@ -2292,10 +2292,9 @@ class DepthwiseConv2D(Conv2D):\n       `\"valid\"` means no padding. `\"same\"` results in padding with zeros evenly\n       to the left/right or up/down of the input such that output has the same\n       height/width dimension as the input.\n-    depth_multiplier: The number of depthwise kernels that are applied to one\n-      input channel.\n-      The total number of depthwise convolution output channels per input\n-      channel will be equal to `filters_in * depth_multiplier`.\n+    depth_multiplier: The number of output channels for the depthwise kernels.\n+      It can be understood as the amount of filters that are applied to\n+      each input channel.\n     data_format: A string,\n       one of `channels_last` (default) or `channels_first`.\n       The ordering of the dimensions in the inputs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63e0f95f0b25c54b5fb327ffc29b1ddd50ddbf88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 378 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -179,8 +179,7 @@ class SidecarEvaluator(object):\n     checkpoint = tf.train.Checkpoint(\n         model=self.model, optimizer=optimizer_checkpoint)\n \n-    for latest_checkpoint in tf.train.checkpoints_iterator(\n-        self.checkpoint_dir):\n+    for latest_checkpoint in tf.train.checkpoints_iterator(self.checkpoint_dir):\n       try:\n         # `expect_partial` because the checkpoint can have other `Trackable`s\n         # such as `optimizer`.\n@@ -238,10 +237,8 @@ class SidecarEvaluator(object):\n               for name, value in return_metrics.items()\n           ]))\n \n-      # TODO(rchao): Make the max evaluation robust in case users save the\n-      # checkpoints with epoch format {epoch:03d}.\n       if (self.max_evaluations and\n-          latest_checkpoint.endswith('-{}'.format(self.max_evaluations))):\n+          (self.max_evaluations == int(latest_checkpoint.split('-')[-1]))):\n         # Exit the loop because we have evaluated the final checkpoint file.\n         logging.info('Last checkpoint evaluated. SidecarEvaluator stops.')\n         return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db99a30ecec6b2b8035faaf153323bc549a41976", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 242 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -144,6 +144,13 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n       def __init__(self):\n         super(MetricLayer, self).__init__(name=\"metric_layer\")\n         self.sum = metrics.Sum(name=\"sum\")\n+        # Using aggregation for jit_compile results in failure. Thus only set\n+        # aggregation for PS Strategy for multi-gpu tests.\n+        if isinstance(distribution,\n+                      tf.distribute.experimental.ParameterServerStrategy):\n+          self.sum_var = tf.Variable(\n+              1.0, aggregation=tf.VariableAggregation.ONLY_FIRST_REPLICA)\n+        else:\n           self.sum_var = tf.Variable(1.0)\n \n       def call(self, inputs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6abfaed1326e3c5e1d233e35c97bde177b562fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 2176 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -256,13 +256,7 @@ def MobileNet(input_shape=None,\n   x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n \n   if include_top:\n-    if backend.image_data_format() == 'channels_first':\n-      shape = (int(1024 * alpha), 1, 1)\n-    else:\n-      shape = (1, 1, int(1024 * alpha))\n-\n-    x = layers.GlobalAveragePooling2D()(x)\n-    x = layers.Reshape(shape, name='reshape_1')(x)\n+    x = layers.GlobalAveragePooling2D(keepdims=True)(x)\n     x = layers.Dropout(dropout, name='dropout')(x)\n     x = layers.Conv2D(classes, (1, 1), padding='same', name='conv_preds')(x)\n     x = layers.Reshape((classes,), name='reshape_2')(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d8e84b3d96635b1930654db4bc5d7ed5a6681367", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 43 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 73 | Churn Cumulative: 15481 | Contributors (this commit): 105 | Commits (past 90d): 24 | Contributors (cumulative): 106 | DMM Complexity: 0.0\n\nDIFF:\n@@ -323,27 +323,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     super(Model, self).__setattr__(name, value)\n \n   def __reduce__(self):\n-    if self.built:\n     return (deserialize_model_from_bytecode, serialize_model_as_bytecode(self))\n-    else:\n-      # SavedModel (and hence serialize_model_as_bytecode) only support built models\n-      # but if the model is not built, it can be serialized as a plain Python object\n-      # so call up the MRO to get an implementation of __reduce__,\n-      # possibly landing at object.__reduce__\n-      return super(Model, self).__reduce__()\n \n   def __deepcopy__(self, memo):\n-    if self.built:\n     new = deserialize_model_from_bytecode(*serialize_model_as_bytecode(self))\n     memo[id(self)] = new\n-    else:\n-      # See comment in __reduce__ for explanation \n-      deserializer, serialized, *rest = super(Model, self).__reduce__()\n-      new = deserializer(*serialized)\n-      memo[id(self)] = new\n-      if rest:\n-        state = copy.deepcopy(rest[0], memo=memo)\n-        new.__setstate__(state)\n     return new\n \n   def __copy__(self):\n\n@@ -25,47 +25,50 @@ from keras import keras_parameterized\n from keras import testing_utils\n \n \n+SERIALIZERS = (\n+  (\"copy\", copy.copy),\n+  (\"deepcopy\", copy.deepcopy),\n+  *(\n+    (f\"pickle-{protocol}\", lambda model: pickle.loads(pickle.dumps(model, protocol=protocol)))\n+    for protocol in range(pickle.HIGHEST_PROTOCOL+1)\n+  )\n+)\n+\n class TestPickleProtocol(keras_parameterized.TestCase):\n   \"\"\"Tests pickle protoocol support.\n   \"\"\"\n \n   @keras_parameterized.run_with_all_model_types\n-  def test_pickle_model(self):\n-    \"\"\"Test copy.copy, copy.deepcopy and pickle on Functional Model.\"\"\"\n-\n-    def roundtrip(model):\n-      model = copy.copy(model)\n-      model = copy.deepcopy(model)\n-      for protocol in range(pickle.HIGHEST_PROTOCOL+1):\n-        model = pickle.loads(pickle.dumps(model, protocol=protocol))\n-      return model\n+  @keras_parameterized.parameterized.named_parameters(*SERIALIZERS)\n+  def test_pickle_model_fitted(self, serializer):\n+    \"\"\"Fitted models should be copyable/picklable.\"\"\"\n \n     # create model\n     original_model = testing_utils.get_small_mlp(\n       num_hidden=1, num_classes=2, input_dim=3\n     )\n-    original_weights = original_model.get_weights()\n+    original_model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n \n-    # roundtrip without compiling\n-    model = roundtrip(original_model)\n-    # compile\n-    model.compile(optimizer='sgd', loss='mse')\n-    # roundtrip compiled but not trained\n-    model = roundtrip(model)\n     # train\n     x = np.random.random((1000, 3))\n-    y = np.random.random((1000, 2))\n-    model.fit(x, y)\n-    y1 = model.predict(x)\n-    # roundtrip with training\n-    model = roundtrip(model)\n-    y2 = model.predict(x)\n+    y = np.random.randint(low=0, high=2, size=(1000, ))\n+    original_model.fit(x, y)\n+    y1 = original_model.predict(x)\n+    original_weights = original_model.get_weights()\n+\n+    # roundtrip and check that the predictions are the same\n+    new_model = serializer(original_model)\n+\n+    y2 = new_model.predict(x)\n     self.assertAllClose(y1, y2)\n-    # check that the original model has not been changed\n-    final_weights = original_model.get_weights()\n-    self.assertAllClose(original_weights, final_weights)\n-    # but that the weights on the trained model are different\n-    self.assertNotAllClose(original_weights, model.get_weights())\n+\n+    # make sure we can keep training\n+    new_model.fit(x, y)\n+    new_weights = new_model.get_weights()\n+    # the weights on the new model should have changed\n+    self.assertNotAllClose(new_weights, original_weights)\n+    # but the weights on the original model have not been touched\n+    self.assertAllClose(original_weights, original_model.get_weights())\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#194bf736c6480cda52f711e5ea097d1cd5d3e498", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1427 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1165,7 +1165,12 @@ def infer_inputs_from_restored_call_function(fn):\n     if isinstance(x, tf.SparseTensorSpec):\n       return tf.SparseTensorSpec(common_shape, x.dtype)\n     elif isinstance(x, tf.RaggedTensorSpec):\n-      return tf.RaggedTensorSpec(common_shape, x.dtype)\n+      return tf.RaggedTensorSpec(\n+          common_shape,\n+          x.dtype,\n+          ragged_rank=x.ragged_rank,\n+          row_splits_dtype=x.row_splits_dtype,\n+          flat_values_spec=x.flat_values_spec)\n     return tf.TensorSpec(common_shape, x.dtype, x.name)\n \n   spec = fn.concrete_functions[0].structured_input_signature\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7b0c36e16502dd7f632f032277dfeebefb7a57d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 30 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 54 | Churn Cumulative: 18500 | Contributors (this commit): 75 | Commits (past 90d): 6 | Contributors (cumulative): 75 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,10 +13,9 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras convolution layers and image transformation layers.\"\"\"\n-\n+# pylint:disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n-import functools\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -24,7 +23,6 @@ from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n-# imports for backwards namespace compatibility\n # pylint: disable=unused-import\n from keras.layers.pooling import AveragePooling1D\n from keras.layers.pooling import AveragePooling2D\n@@ -35,6 +33,7 @@ from keras.layers.pooling import MaxPooling3D\n # pylint: enable=unused-import\n from keras.utils import conv_utils\n from keras.utils import tf_utils\n+\n from tensorflow.python.util.tf_export import keras_export\n # pylint: disable=g-classes-have-attributes\n \n@@ -157,7 +156,6 @@ class Conv(Layer):\n     self.input_spec = InputSpec(min_ndim=self.rank + 2)\n \n     self._validate_init()\n-    self._is_causal = self.padding == 'causal'\n     self._channels_first = self.data_format == 'channels_first'\n     self._tf_data_format = conv_utils.convert_data_format(\n         self.data_format, self.rank + 2)\n@@ -214,39 +212,35 @@ class Conv(Layer):\n     else:\n       self.bias = None\n     channel_axis = self._get_channel_axis()\n-    self.input_spec = InputSpec(min_ndim=self.rank + 2,\n-                                axes={channel_axis: input_channel})\n-\n-    # Convert Keras formats to TF native formats.\n-    if self.padding == 'causal':\n-      tf_padding = 'VALID'  # Causal padding handled in `call`.\n-    elif isinstance(self.padding, str):\n-      tf_padding = self.padding.upper()\n-    else:\n-      tf_padding = self.padding\n-    tf_dilations = list(self.dilation_rate)\n-    tf_strides = list(self.strides)\n-\n-    tf_op_name = self.__class__.__name__\n-    if tf_op_name == 'Conv1D':\n-      tf_op_name = 'conv1d'  # Backwards compat.\n-\n-    self._convolution_op = functools.partial(\n-        tf.nn.convolution,\n-        strides=tf_strides,\n-        padding=tf_padding,\n-        dilations=tf_dilations,\n-        data_format=self._tf_data_format,\n-        name=tf_op_name)\n+    self.input_spec = InputSpec(\n+        min_ndim=self.rank + 2, axes={channel_axis: input_channel})\n     self.built = True\n \n   def call(self, inputs):\n     input_shape = inputs.shape\n \n-    if self._is_causal:  # Apply causal padding to inputs for Conv1D.\n+    # Convert Keras formats to TF native formats.\n+    if self.padding == 'causal':\n+      # Apply causal padding to inputs for Conv1D.\n       inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n+      tf_padding = 'VALID'  # Causal padding handled in `call`.\n+    elif isinstance(self.padding, str):\n+      tf_padding = self.padding.upper()\n+    else:\n+      tf_padding = self.padding\n \n-    outputs = self._convolution_op(inputs, self.kernel)\n+    tf_op_name = self.__class__.__name__\n+    if tf_op_name == 'Conv1D':\n+      tf_op_name = 'conv1d'  # Backwards compat.\n+\n+    outputs = tf.nn.convolution(\n+        inputs,\n+        self.kernel,\n+        padding=tf_padding,\n+        strides=list(self.strides),\n+        dilations=list(self.dilation_rate),\n+        data_format=self._tf_data_format,\n+        name=tf_op_name)\n \n     if self.use_bias:\n       output_rank = outputs.shape.rank\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7556a0f5bd6a75b7aeb962e237bbae1bb1588580", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 1486 | Contributors (this commit): 4 | Commits (past 90d): 14 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -151,18 +151,18 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n   >>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])\n   >>> layer(input)\n-  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n+  <tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n   array([[0, 2, 3, 1],\n-         [1, 3, 2, 1]], dtype=int32)>\n+         [1, 3, 2, 1]], dtype=int64)>\n \n   Bucketize float values based on a number of buckets to compute.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n   >>> layer = tf.keras.layers.Discretization(num_bins=4, epsilon=0.01)\n   >>> layer.adapt(input)\n   >>> layer(input)\n-  <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n+  <tf.Tensor: shape=(2, 4), dtype=int64, numpy=\n   array([[0, 2, 3, 2],\n-         [1, 3, 3, 1]], dtype=int32)>\n+         [1, 3, 3, 1]], dtype=int64)>\n   \"\"\"\n \n   def __init__(self,\n@@ -263,8 +263,11 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n \n   def call(self, inputs):\n     def bucketize(inputs):\n-      return tf.raw_ops.Bucketize(\n+      outputs = tf.raw_ops.Bucketize(\n           input=inputs, boundaries=self.bin_boundaries)\n+      # All other preprocessing layers use int64 for int output, so we conform\n+      # here. Sadly the underlying op only supports int32, so we need to cast.\n+      return tf.cast(outputs, tf.int64)\n \n     if tf_utils.is_ragged(inputs):\n       integer_buckets = tf.ragged.map_flat_values(bucketize, inputs)\n\n@@ -128,6 +128,12 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     output = layer(input_data)\n     self.assertAllEqual(output.shape.as_list(), [16, 4])\n \n+  def test_output_dtype(self):\n+    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.int64)\n+    layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n+    output = layer(input_data)\n+    self.assertAllEqual(output.dtype, tf.int64)\n+\n   def test_num_bins_negative_fails(self):\n     with self.assertRaisesRegex(ValueError, \"`num_bins` must be.*num_bins=-7\"):\n       _ = discretization.Discretization(num_bins=-7)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a5770601eba163cefbd97de89993ff07cf9cf430", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 58 | Churn Cumulative: 1090 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,9 +23,6 @@ from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from tensorflow.python.util.tf_export import keras_export\n \n-# Default key from tf.sparse.cross_hashed\n-_DEFAULT_SALT_KEY = [0xDECAFCAFFE, 0xDECAFCAFFE]\n-\n \n @keras_export('keras.layers.Hashing',\n               'keras.layers.experimental.preprocessing.Hashing')\n@@ -128,11 +125,12 @@ class Hashing(base_layer.Layer):\n   def __init__(self, num_bins, mask_value=None, salt=None, **kwargs):\n     if num_bins is None or num_bins <= 0:\n       raise ValueError('`num_bins` cannot be `None` or non-positive values.')\n-    super(Hashing, self).__init__(**kwargs)\n+    super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Hashing').set(True)\n     self.num_bins = num_bins\n     self.mask_value = mask_value\n     self.strong_hash = True if salt is not None else False\n+    self.salt = None\n     if salt is not None:\n       if isinstance(salt, (tuple, list)) and len(salt) == 2:\n         self.salt = salt\n@@ -141,16 +139,10 @@ class Hashing(base_layer.Layer):\n       else:\n         raise ValueError('`salt can only be a tuple of size 2 integers, or a '\n                          'single integer, given {}'.format(salt))\n-    else:\n-      self.salt = _DEFAULT_SALT_KEY\n-\n-  def _preprocess_input(self, inp):\n-    if isinstance(inp, (list, tuple, np.ndarray)):\n-      inp = tf.convert_to_tensor(inp)\n-    return inp\n \n   def call(self, inputs):\n-    inputs = self._preprocess_input(inputs)\n+    if isinstance(inputs, (list, tuple, np.ndarray)):\n+      inputs = tf.convert_to_tensor(inputs)\n     if isinstance(inputs, tf.SparseTensor):\n       return tf.SparseTensor(\n           indices=inputs.indices,\n@@ -199,10 +191,10 @@ class Hashing(base_layer.Layer):\n       return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n   def get_config(self):\n-    config = {\n+    config = super().get_config()\n+    config.update({\n         'num_bins': self.num_bins,\n         'salt': self.salt,\n         'mask_value': self.mask_value,\n-    }\n-    base_config = super(Hashing, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n+    })\n+    return config\n\n@@ -14,16 +14,17 @@\n # ==============================================================================\n \"\"\"Tests for hashing layer.\"\"\"\n \n+import os\n from absl.testing import parameterized\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n+import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.layers.preprocessing import hashing\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -193,8 +194,7 @@ class HashingTest(keras_parameterized.TestCase):\n \n   def test_hash_ragged_int_input_farmhash(self):\n     layer = hashing.Hashing(num_bins=3)\n-    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]],\n-                                           dtype=tf.int64)\n+    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]], dtype=tf.int64)\n     out_data = layer(inp_data)\n     # Same hashed output as test_hash_sparse_input_farmhash\n     expected_output = [[1, 0, 0, 2], [1, 0, 1]]\n@@ -231,8 +231,7 @@ class HashingTest(keras_parameterized.TestCase):\n \n   def test_hash_ragged_int_input_siphash(self):\n     layer = hashing.Hashing(num_bins=3, salt=[133, 137])\n-    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]],\n-                                           dtype=tf.int64)\n+    inp_data = tf.ragged.constant([[0, 1, 3, 4], [2, 1, 0]], dtype=tf.int64)\n     out_data = layer(inp_data)\n     # Same hashed output as test_hash_sparse_input_farmhash\n     expected_output = [[1, 1, 0, 1], [2, 1, 1]]\n@@ -270,6 +269,27 @@ class HashingTest(keras_parameterized.TestCase):\n     layer_1 = hashing.Hashing.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_saved_model(self):\n+    input_data = np.array(['omar', 'stringer', 'marlo', 'wire', 'skywalker'])\n+\n+    inputs = keras.Input(shape=(None,), dtype=tf.string)\n+    outputs = hashing.Hashing(num_bins=100)(inputs)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+\n+    original_output_data = model(input_data)\n+\n+    # Save the model to disk.\n+    output_path = os.path.join(self.get_temp_dir(), 'tf_keras_saved_model')\n+    model.save(output_path, save_format='tf')\n+    loaded_model = keras.models.load_model(output_path)\n+\n+    # Ensure that the loaded model is unique (so that the save/load is real)\n+    self.assertIsNot(model, loaded_model)\n+\n+    # Validate correctness of the new model.\n+    new_output_data = loaded_model(input_data)\n+    self.assertAllClose(new_output_data, original_output_data)\n+\n   @parameterized.named_parameters(\n       (\n           'list_input',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
