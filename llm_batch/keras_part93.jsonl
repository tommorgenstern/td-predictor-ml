{"custom_id": "keras#bfa5ca553d4117ba5764e5a554222354609f0df1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 10329 | Contributors (this commit): 44 | Commits (past 90d): 45 | Contributors (cumulative): 44 | DMM Complexity: None\n\nDIFF:\n@@ -511,12 +511,14 @@ class Dense(Layer):\n \n     ```python\n         # as first layer in a sequential model:\n-        model = Sequential(Dense(32, input_dim=16))\n+        model = Sequential()\n+        model.add(Dense(32, input_dim=16))\n         # now the model will take as input arrays of shape (*, 16)\n         # and output arrays of shape (*, 32)\n \n         # this is equivalent to the above:\n-        model = Sequential(Dense(32, input_shape=(16,)))\n+        model = Sequential()\n+        model.add(Dense(32, input_shape=(16,)))\n \n         # after the first layer, you don't need to specify\n         # the size of the input anymore:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ec5e48969a8ee3d813d1e2fef8272efc260f7d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 832 | Contributors (this commit): 11 | Commits (past 90d): 15 | Contributors (cumulative): 11 | DMM Complexity: 0.75\n\nDIFF:\n@@ -56,7 +56,7 @@ def random_shear(x, intensity, row_index=1, col_index=2, channel_index=0,\n def random_zoom(x, zoom_range, row_index=1, col_index=2, channel_index=0,\n                 fill_mode='nearest', cval=0.):\n     if len(zoom_range) != 2:\n-        raise Exception('zoom_range should be a sequence of two. '\n+        raise Exception('zoom_range should be a tuple or list of two floats. '\n                         'Received arg: ', zoom_range)\n \n     if zoom_range[0] == 1 and zoom_range[1] == 1:\n@@ -237,8 +237,9 @@ class ImageDataGenerator(object):\n         elif len(zoom_range) == 2:\n             self.zoom_range = [zoom_range[0], zoom_range[1]]\n         else:\n-            raise Exception('zoom_range should be scalar or sequence of two. '\n-                            'Receive arg: ', zoom_range)\n+            raise Exception('zoom_range should be a float or '\n+                            'a tuple or list of two floats. '\n+                            'Received arg: ', zoom_range)\n \n         self.batch_index = 0\n         self.total_batches_seen = 0\n@@ -301,7 +302,10 @@ class ImageDataGenerator(object):\n         if self.save_to_dir:\n             for i in range(current_batch_size):\n                 img = array_to_img(bX[i], self.dim_ordering, scale=True)\n-                img.save(os.path.join(self.save_to_dir, self.save_prefix + '_' + str(current_index + i) + '.' + self.save_format))\n+                fname = '{prefix}_{index}.{format}'.format(prefix=self.save_prefix,\n+                                                           index=current_index + i,\n+                                                           format=self.save_format)\n+                img.save(os.path.join(self.save_to_dir, fname))\n         bY = self.y[index_array]\n         return bX, bY\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3779b8a008d199fe1f2ea0b05254a7d775500e5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 38 | Churn Cumulative: 361 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -4,37 +4,36 @@ from PIL import Image\n import numpy as np\n import os\n import shutil\n+import tempfile\n \n \n-def setup_function(func):\n-    paths = ['test_images', 'test_images/rgb', 'test_images/gsc']\n-    for path in paths:\n-        if not os.path.exists(path):\n-            os.mkdir(path)\n+class TestImage:\n \n+    def setup_class(cls):\n         img_w = img_h = 20\n+        rgb_images = []\n+        gray_images = []\n         for n in range(8):\n             bias = np.random.rand(img_w, img_h, 1) * 64\n             variance = np.random.rand(img_w, img_h, 1) * (255-64)\n             imarray = np.random.rand(img_w, img_h, 3) * variance + bias\n-        im = Image.fromarray(imarray.astype('uint8')).convert('RGBA')\n-        im.save('test_images/rgb/rgb_test_image_'+str(n)+'.png')\n+            im = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n+            rgb_images.append(im)\n \n             imarray = np.random.rand(img_w, img_h, 1) * variance + bias\n             im = Image.fromarray(imarray.astype('uint8').squeeze()).convert('L')\n-        im.save('test_images/gsc/gsc_test_image_'+str(n)+'.png')\n+            gray_images.append(im)\n \n+        cls.all_test_images = [rgb_images, gray_images]\n \n-def teardown_function(func):\n-    shutil.rmtree('test_images')\n+    def teardown_class(cls):\n+        del cls.all_test_images\n \n-\n-def test_image_data_generator():\n-    for color_mode in ['gsc', 'rgb']:\n-        file_list = list_pictures('test_images/' + color_mode)\n+    def test_image_data_generator(self):\n+        for test_images in self.all_test_images:\n             img_list = []\n-        for f in file_list:\n-            img_list.append(img_to_array(load_img(f))[None, ...])\n+            for im in test_images:\n+                img_list.append(img_to_array(im)[None, ...])\n \n             images = np.vstack(img_list)\n             generator = ImageDataGenerator(\n@@ -55,13 +54,14 @@ def test_image_data_generator():\n                 vertical_flip=True)\n             generator.fit(images, augment=True)\n \n+            tmp_folder = tempfile.mkdtemp(prefix='test_images')\n             for x, y in generator.flow(images, np.arange(images.shape[0]),\n-                                   shuffle=True, save_to_dir='test_images'):\n+                                       shuffle=True, save_to_dir=tmp_folder):\n                 assert x.shape[1:] == images.shape[1:]\n                 break\n+            shutil.rmtree(tmp_folder)\n \n-\n-def test_img_flip():\n+    def test_img_flip(self):\n         x = np.array(range(4)).reshape([1, 1, 2, 2])\n         assert (flip_axis(x, 0) == x).all()\n         assert (flip_axis(x, 1) == x).all()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d62f4da6c15cfee28ddfe9c05e78fb11f41b5bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 1823 | Contributors (this commit): 7 | Commits (past 90d): 28 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -642,6 +642,8 @@ class Model(Container):\n         self.predict_function = None\n \n     def _make_train_function(self):\n+        if not hasattr(self, 'train_function'):\n+            raise Exception('You must compile your model before using it.')\n         if self.train_function is None:\n             if self.uses_learning_phase:\n                 inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n@@ -663,6 +665,8 @@ class Model(Container):\n                                              **self._function_kwargs)\n \n     def _make_test_function(self):\n+        if not hasattr(self, 'test_function'):\n+            raise Exception('You must compile your model before using it.')\n         if self.test_function is None:\n             if self.uses_learning_phase:\n                 inputs = self.inputs + self.targets + self.sample_weights + [K.learning_phase()]\n@@ -676,6 +680,8 @@ class Model(Container):\n                                             **self._function_kwargs)\n \n     def _make_predict_function(self):\n+        if not hasattr(self, 'predict_function'):\n+            raise Exception('You must compile your model before using it.')\n         if self.predict_function is None:\n             if self.uses_learning_phase:\n                 inputs = self.inputs + [K.learning_phase()]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad3107073bb063fd2115642a0d1a27d4e399d120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1928 | Contributors (this commit): 8 | Commits (past 90d): 31 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1377,7 +1377,7 @@ class Model(Container):\n                                                class_weight=class_weight)\n                 except Exception as e:\n                     _stop.set()\n-                    raise e\n+                    raise\n \n                 if type(outs) != list:\n                     outs = [outs]\n@@ -1479,7 +1479,7 @@ class Model(Container):\n                 outs = self.test_on_batch(x, y, sample_weight=sample_weight)\n             except Exception as e:\n                 _stop.set()\n-                raise e\n+                raise\n \n             if type(x) is list:\n                 nb_samples = len(x[0])\n@@ -1551,7 +1551,7 @@ class Model(Container):\n                 outs = self.predict_on_batch(x)\n             except Exception as e:\n                 _stop.set()\n-                raise e\n+                raise\n \n             if type(x) is list:\n                 nb_samples = len(x[0])\n\n@@ -73,7 +73,7 @@ def get_file(fname, origin, untar=False):\n         except (Exception, KeyboardInterrupt) as e:\n             if os.path.exists(fpath):\n                 os.remove(fpath)\n-            raise e\n+            raise\n         progbar = None\n \n     if untar:\n@@ -88,7 +88,7 @@ def get_file(fname, origin, untar=False):\n                         os.remove(untar_fpath)\n                     else:\n                         shutil.rmtree(untar_fpath)\n-                raise e\n+                raise\n             tfile.close()\n         return untar_fpath\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5467107fc910d516c516b2b838624da376c1ba0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 190 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n from __future__ import absolute_import\r\n-__version__ = '1.0.1'\r\n+__version__ = '1.0.2'\r\n from . import backend\r\n from . import datasets\r\n from . import engine\r\n\n@@ -3,12 +3,12 @@ from setuptools import find_packages\n \n \n setup(name='Keras',\n-      version='1.0.1',\n+      version='1.0.2',\n       description='Deep Learning for Python',\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/fchollet/keras',\n-      download_url='https://github.com/fchollet/keras/tarball/1.0.1',\n+      download_url='https://github.com/fchollet/keras/tarball/1.0.2',\n       license='MIT',\n       install_requires=['theano', 'pyyaml', 'six'],\n       extras_require={\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a981a8c42c316831183cac7598266d577a1ea96a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 108 | Lines Deleted: 60 | Files Changed: 2 | Hunks: 72 | Methods Changed: 18 | Complexity Δ (Sum/Max): 25/13 | Churn Δ: 168 | Churn Cumulative: 13887 | Contributors (this commit): 54 | Commits (past 90d): 58 | Contributors (cumulative): 61 | DMM Complexity: 0.5111111111111111\n\nDIFF:\n@@ -65,6 +65,7 @@ class Convolution1D(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n         input_dim: Number of channels/dimensions in the input.\n             Either this argument or the keyword argument `input_shape`must be\n             provided when using this layer as the first layer in a model.\n@@ -85,7 +86,7 @@ class Convolution1D(Layer):\n                  border_mode='valid', subsample_length=1,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n-                 input_dim=None, input_length=None, **kwargs):\n+                 bias=True, input_dim=None, input_length=None, **kwargs):\n \n         if border_mode not in {'valid', 'same'}:\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n@@ -106,6 +107,7 @@ class Convolution1D(Layer):\n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n \n+        self.bias = bias\n         self.input_spec = [InputSpec(ndim=3)]\n         self.initial_weights = weights\n         self.input_dim = input_dim\n@@ -118,15 +120,18 @@ class Convolution1D(Layer):\n         input_dim = input_shape[2]\n         self.W_shape = (self.nb_filter, input_dim, self.filter_length, 1)\n         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        if self.bias:\n             self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n             self.trainable_weights = [self.W, self.b]\n+        else:\n+            self.trainable_weights = [self.W]\n         self.regularizers = []\n \n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -137,7 +142,7 @@ class Convolution1D(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -154,11 +159,11 @@ class Convolution1D(Layer):\n     def call(self, x, mask=None):\n         x = K.expand_dims(x, -1)  # add a dimension of the right\n         x = K.permute_dimensions(x, (0, 2, 1, 3))\n-        conv_out = K.conv2d(x, self.W, strides=self.subsample,\n+        output = K.conv2d(x, self.W, strides=self.subsample,\n                           border_mode=self.border_mode,\n                           dim_ordering='th')\n-\n-        output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+        if self.bias:\n+            output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n         output = self.activation(output)\n         output = K.squeeze(output, 3)  # remove the dummy 3rd dimension\n         output = K.permute_dimensions(output, (0, 2, 1))\n@@ -176,6 +181,7 @@ class Convolution1D(Layer):\n                   'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                   'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias,\n                   'input_dim': self.input_dim,\n                   'input_length': self.input_length}\n         base_config = super(Convolution1D, self).get_config()\n@@ -232,6 +238,7 @@ class Convolution2D(Layer):\n             applied to the bias.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode is it at index 3.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n \n     # Input shape\n         4D tensor with shape:\n@@ -250,7 +257,8 @@ class Convolution2D(Layer):\n                  init='glorot_uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample=(1, 1), dim_ordering='th',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, **kwargs):\n+                 W_constraint=None, b_constraint=None,\n+                 bias=True, **kwargs):\n \n         if border_mode not in {'valid', 'same'}:\n             raise Exception('Invalid border mode for Convolution2D:', border_mode)\n@@ -272,6 +280,7 @@ class Convolution2D(Layer):\n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n \n+        self.bias = bias\n         self.input_spec = [InputSpec(ndim=4)]\n         self.initial_weights = weights\n         super(Convolution2D, self).__init__(**kwargs)\n@@ -286,15 +295,18 @@ class Convolution2D(Layer):\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        if self.bias:\n             self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n             self.trainable_weights = [self.W, self.b]\n+        else:\n+            self.trainable_weights = [self.W]\n         self.regularizers = []\n \n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -305,7 +317,7 @@ class Convolution2D(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -335,14 +347,15 @@ class Convolution2D(Layer):\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def call(self, x, mask=None):\n-        conv_out = K.conv2d(x, self.W, strides=self.subsample,\n+        output = K.conv2d(x, self.W, strides=self.subsample,\n                           border_mode=self.border_mode,\n                           dim_ordering=self.dim_ordering,\n                           filter_shape=self.W_shape)\n+        if self.bias:\n             if self.dim_ordering == 'th':\n-            output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1))\n+                output += K.reshape(self.b, (1, self.nb_filter, 1, 1))\n             elif self.dim_ordering == 'tf':\n-            output = conv_out + K.reshape(self.b, (1, 1, 1, self.nb_filter))\n+                output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n             else:\n                 raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         output = self.activation(output)\n@@ -361,7 +374,8 @@ class Convolution2D(Layer):\n                   'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n                   'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n-                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None}\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias}\n         base_config = super(Convolution2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -407,6 +421,7 @@ class Convolution3D(Layer):\n             applied to the bias.\n         dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n             (the depth) is at index 1, in 'tf' mode is it at index 4.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n \n     # Input shape\n         5D tensor with shape:\n@@ -426,7 +441,8 @@ class Convolution3D(Layer):\n                  init='glorot_uniform', activation='linear', weights=None,\n                  border_mode='valid', subsample=(1, 1, 1), dim_ordering='th',\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, **kwargs):\n+                 W_constraint=None, b_constraint=None,\n+                 bias=True, **kwargs):\n         if K._BACKEND != 'theano':\n             raise Exception(self.__class__.__name__ +\n                             ' is currently only working with Theano backend.')\n@@ -451,6 +467,7 @@ class Convolution3D(Layer):\n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n \n+        self.bias = bias\n         self.input_spec = [InputSpec(ndim=5)]\n         self.initial_weights = weights\n         super(Convolution3D, self).__init__(**kwargs)\n@@ -471,15 +488,18 @@ class Convolution3D(Layer):\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        if self.bias:\n             self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n             self.trainable_weights = [self.W, self.b]\n-        self.regularizers = []\n+        else:\n+            self.trainable_weights = [self.W]\n \n+        self.regularizers = []\n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -490,7 +510,7 @@ class Convolution3D(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -525,36 +545,37 @@ class Convolution3D(Layer):\n \n     def call(self, x, mask=None):\n         input_shape = self.input_spec[0].shape\n-        conv_out = K.conv3d(x, self.W, strides=self.subsample,\n+        output = K.conv3d(x, self.W, strides=self.subsample,\n                           border_mode=self.border_mode,\n                           dim_ordering=self.dim_ordering,\n                           volume_shape=input_shape,\n                           filter_shape=self.W_shape)\n-\n+        if self.bias:\n             if self.dim_ordering == 'th':\n-            output = conv_out + K.reshape(self.b, (1, self.nb_filter, 1, 1, 1))\n+                output += K.reshape(self.b, (1, self.nb_filter, 1, 1, 1))\n             elif self.dim_ordering == 'tf':\n-            output = conv_out + K.reshape(self.b, (1, 1, 1, 1, self.nb_filter))\n+                output += K.reshape(self.b, (1, 1, 1, 1, self.nb_filter))\n             else:\n                 raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n     def get_config(self):\n-        config = {\"nb_filter\": self.nb_filter,\n-                  \"kernel_dim1\": self.kernel_dim1,\n-                  \"kernel_dim2\": self.kernel_dim2,\n-                  \"kernel_dim3\": self.kernel_dim3,\n-                  \"dim_ordering\": self.dim_ordering,\n-                  \"init\": self.init.__name__,\n-                  \"activation\": self.activation.__name__,\n-                  \"border_mode\": self.border_mode,\n-                  \"subsample\": self.subsample,\n-                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n-                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n-                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None}\n+        config = {'nb_filter': self.nb_filter,\n+                  'kernel_dim1': self.kernel_dim1,\n+                  'kernel_dim2': self.kernel_dim2,\n+                  'kernel_dim3': self.kernel_dim3,\n+                  'dim_ordering': self.dim_ordering,\n+                  'init': self.init.__name__,\n+                  'activation': self.activation.__name__,\n+                  'border_mode': self.border_mode,\n+                  'subsample': self.subsample,\n+                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  'b_regularizer': self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n+                  'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias}\n         base_config = super(Convolution3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -550,12 +550,10 @@ class Dense(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n-        bias: boolean\n-            Default True;\n-            Setting it to False will remove the bias (b) from all calculations.\n \n     # Input shape\n         2D tensor with shape: `(nb_samples, input_dim)`.\n@@ -565,7 +563,8 @@ class Dense(Layer):\n     '''\n     def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, bias=True, **kwargs):\n+                 W_constraint=None, b_constraint=None,\n+                 bias=True, input_dim=None, **kwargs):\n         self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.output_dim = output_dim\n@@ -606,7 +605,7 @@ class Dense(Layer):\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer and self.bias:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -617,7 +616,7 @@ class Dense(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint and self.bias:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -643,8 +642,8 @@ class Dense(Layer):\n                   'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                   'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n-                  'input_dim': self.input_dim,\n-                  'bias': self.bias}\n+                  'bias': self.bias,\n+                  'input_dim': self.input_dim}\n         base_config = super(Dense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -721,6 +720,7 @@ class MaxoutDense(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n@@ -737,7 +737,8 @@ class MaxoutDense(Layer):\n     def __init__(self, output_dim, nb_feature=4,\n                  init='glorot_uniform', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n+                 W_constraint=None, b_constraint=None,\n+                 bias=True, input_dim=None, **kwargs):\n         self.output_dim = output_dim\n         self.nb_feature = nb_feature\n         self.init = initializations.get(init)\n@@ -749,6 +750,7 @@ class MaxoutDense(Layer):\n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n \n+        self.bias = bias\n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=2)]\n \n@@ -764,17 +766,19 @@ class MaxoutDense(Layer):\n \n         self.W = self.init((self.nb_feature, input_dim, self.output_dim),\n                            name='{}_W'.format(self.name))\n+        if self.bias:\n             self.b = K.zeros((self.nb_feature, self.output_dim),\n                              name='{}_b'.format(self.name))\n-\n             self.trainable_weights = [self.W, self.b]\n-        self.regularizers = []\n+        else:\n+            self.trainable_weights = [self.W]\n \n+        self.regularizers = []\n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -785,7 +789,7 @@ class MaxoutDense(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -798,7 +802,10 @@ class MaxoutDense(Layer):\n \n     def call(self, x, mask=None):\n         # no activation, this layer is only linear.\n-        output = K.max(K.dot(x, self.W) + self.b, axis=1)\n+        output = K.dot(x, self.W)\n+        if self.bias:\n+            output += self.b\n+        output = K.max(output, axis=1)\n         return output\n \n     def get_config(self):\n@@ -810,6 +817,7 @@ class MaxoutDense(Layer):\n                   'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                   'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias,\n                   'input_dim': self.input_dim}\n         base_config = super(MaxoutDense, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -844,6 +852,7 @@ class Highway(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n@@ -860,7 +869,8 @@ class Highway(Layer):\n     def __init__(self, init='glorot_uniform', transform_bias=-2,\n                  activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n-                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n+                 W_constraint=None, b_constraint=None,\n+                 bias=True, input_dim=None, **kwargs):\n         self.init = initializations.get(init)\n         self.transform_bias = transform_bias\n         self.activation = activations.get(activation)\n@@ -872,6 +882,7 @@ class Highway(Layer):\n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n \n+        self.bias = bias\n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=2)]\n \n@@ -890,19 +901,21 @@ class Highway(Layer):\n         self.W_carry = self.init((input_dim, input_dim),\n                                  name='{}_W_carry'.format(self.name))\n \n+        if self.bias:\n             self.b = K.zeros((input_dim,), name='{}_b'.format(self.name))\n             # initialize with a vector of values `transform_bias`\n             self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias,\n                                       name='{}_b_carry'.format(self.name))\n-\n             self.trainable_weights = [self.W, self.b, self.W_carry, self.b_carry]\n+        else:\n+            self.trainable_weights = [self.W, self.W_carry]\n \n         self.regularizers = []\n         if self.W_regularizer:\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -913,7 +926,7 @@ class Highway(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -921,8 +934,14 @@ class Highway(Layer):\n             del self.initial_weights\n \n     def call(self, x, mask=None):\n-        transform_weight = activations.sigmoid(K.dot(x, self.W_carry) + self.b_carry)\n-        act = self.activation(K.dot(x, self.W) + self.b)\n+        y = K.dot(x, self.W_carry)\n+        if self.bias:\n+            y += self.b_carry\n+        transform_weight = activations.sigmoid(y)\n+        y = K.dot(x, self.W)\n+        if self.bias:\n+            y += self.b\n+        act = self.activation(y)\n         act *= transform_weight\n         output = act + (1 - transform_weight) * x\n         return output\n@@ -936,6 +955,7 @@ class Highway(Layer):\n                   'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                   'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias,\n                   'input_dim': self.input_dim}\n         base_config = super(Highway, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -979,16 +999,19 @@ class TimeDistributedDense(Layer):\n             (eg. maxnorm, nonneg), applied to the main weights matrix.\n         b_constraint: instance of the [constraints](../constraints.md) module,\n             applied to the bias.\n+        bias: whether to include a bias (i.e. make the layer affine rather than linear).\n         input_dim: dimensionality of the input (integer).\n             This argument (or alternatively, the keyword argument `input_shape`)\n             is required when using this layer as the first layer in a model.\n+        input_length: length of inputs sequences\n+            (integer, or None for variable-length sequences).\n     '''\n \n     def __init__(self, output_dim,\n                  init='glorot_uniform', activation='linear', weights=None,\n                  W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n                  W_constraint=None, b_constraint=None,\n-                 input_dim=None, input_length=None, **kwargs):\n+                 bias=True, input_dim=None, input_length=None, **kwargs):\n         warnings.warn('TimeDistributedDense is deprecated, '\n                       'please use TimeDistributed(Dense(...)) instead.')\n         self.output_dim = output_dim\n@@ -1002,6 +1025,7 @@ class TimeDistributedDense(Layer):\n         self.W_constraint = constraints.get(W_constraint)\n         self.b_constraint = constraints.get(b_constraint)\n \n+        self.bias = bias\n         self.initial_weights = weights\n         self.input_spec = [InputSpec(ndim=3)]\n         self.supports_masking = True\n@@ -1019,9 +1043,9 @@ class TimeDistributedDense(Layer):\n \n         self.W = self.init((input_dim, self.output_dim),\n                            name='{}_W'.format(self.name))\n+        if self.bias:\n             self.b = K.zeros((self.output_dim,),\n                              name='{}_b'.format(self.name))\n-\n             self.trainable_weights = [self.W, self.b]\n         self.regularizers = []\n \n@@ -1029,7 +1053,7 @@ class TimeDistributedDense(Layer):\n             self.W_regularizer.set_param(self.W)\n             self.regularizers.append(self.W_regularizer)\n \n-        if self.b_regularizer:\n+        if self.bias and self.b_regularizer:\n             self.b_regularizer.set_param(self.b)\n             self.regularizers.append(self.b_regularizer)\n \n@@ -1040,7 +1064,7 @@ class TimeDistributedDense(Layer):\n         self.constraints = {}\n         if self.W_constraint:\n             self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n+        if self.bias and self.b_constraint:\n             self.constraints[self.b] = self.b_constraint\n \n         if self.initial_weights is not None:\n@@ -1070,7 +1094,9 @@ class TimeDistributedDense(Layer):\n \n         # Squash samples and timesteps into a single axis\n         x = K.reshape(x, (-1, input_shape[-1]))  # (samples * timesteps, input_dim)\n-        y = K.dot(x, self.W) + self.b  # (samples * timesteps, output_dim)\n+        y = K.dot(x, self.W)  # (samples * timesteps, output_dim)\n+        if self.bias:\n+            y += self.b\n         # We have to reshape Y to (samples, timesteps, output_dim)\n         y = K.reshape(y, (-1, input_length, self.output_dim))  # (samples, timesteps, output_dim)\n         y = self.activation(y)\n@@ -1085,6 +1111,7 @@ class TimeDistributedDense(Layer):\n                   'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n                   'b_constraint': self.b_constraint.get_config() if self.b_constraint else None,\n+                  'bias': self.bias,\n                   'input_dim': self.input_dim,\n                   'input_length': self.input_length}\n         base_config = super(TimeDistributedDense, self).get_config()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f1701394993cdf9c5d0cd048e1b97ad10d049c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 16 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 2113 | Contributors (this commit): 8 | Commits (past 90d): 36 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -424,7 +424,7 @@ def generator_queue(generator, max_q_size=10,\n class Model(Container):\n \n     def compile(self, optimizer, loss, metrics=[], loss_weights=None,\n-                sample_weight_mode=None, multi_target_loss=False, **kwargs):\n+                sample_weight_mode=None, **kwargs):\n         '''Configures the model for training.\n \n         # Arguments\n@@ -446,7 +446,6 @@ class Model(Container):\n                 If the model has multiple outputs, you can use a different\n                 `sample_weight_mode` on each output by passing a\n                 dictionary or a list of modes.\n-            multi_target_loss: add each target's loss function as a metric\n             kwargs: when using the Theano backend, these arguments\n                 are passed into K.function. Ignored for Tensorflow backend.\n         '''\n@@ -573,6 +572,9 @@ class Model(Container):\n             self.targets.append(K.placeholder(ndim=len(shape), name=name + '_target'))\n \n         # prepare metrics\n+        if len(self.outputs) > 1:\n+            self.metrics_names = ['total_loss']\n+        else:\n             self.metrics_names = ['loss']\n         self.metrics = []\n \n@@ -585,15 +587,15 @@ class Model(Container):\n             sample_weight = sample_weights[i]\n             mask = masks[i]\n             loss_weight = loss_weights_list[i]\n-            output_loss = loss_weight * weighted_loss(y_true, y_pred,\n+            output_loss = weighted_loss(y_true, y_pred,\n                                         sample_weight, mask)\n-            if multi_target_loss:\n+            if len(self.outputs) > 1:\n                 self.metrics.append(output_loss)\n-                self.metrics_names.append('loss_'+self.output_names[i])\n+                self.metrics_names.append(self.output_names[i] + '_loss')\n             if total_loss is None:\n-                total_loss = output_loss\n+                total_loss = loss_weight * output_loss\n             else:\n-                total_loss += output_loss\n+                total_loss += loss_weight * output_loss\n \n         # add regularization penalties to the loss\n         for r in self.regularizers:\n\n@@ -117,10 +117,10 @@ def test_model_methods():\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n-    assert len(out) == 3\n+    assert len(out) == 5\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n-    assert len(out) == 3\n+    assert len(out) == 5\n \n     # this should also work\n     model.compile(optimizer, loss, metrics={'dense_1': 'acc'},\n@@ -128,10 +128,10 @@ def test_model_methods():\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n-    assert len(out) == 2\n+    assert len(out) == 4\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n-    assert len(out) == 2\n+    assert len(out) == 4\n \n     # and this as well\n     model.compile(optimizer, loss, metrics={'dense_1': ['acc']},\n@@ -139,10 +139,10 @@ def test_model_methods():\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n-    assert len(out) == 2\n+    assert len(out) == 4\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n-    assert len(out) == 2\n+    assert len(out) == 4\n \n     # test with a custom metric function\n     mse = lambda y_true, y_pred: K.mean(K.pow(y_true - y_pred, 2))\n@@ -151,10 +151,10 @@ def test_model_methods():\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n-    assert len(out) == 3\n+    assert len(out) == 5\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n-    assert len(out) == 3\n+    assert len(out) == 5\n \n     input_a_np = np.random.random((10, 3))\n     input_b_np = np.random.random((10, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#da57a530f985e1eccd8dd3899614ac4dd5c78928", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 1848 | Contributors (this commit): 8 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -572,9 +572,6 @@ class Model(Container):\n             self.targets.append(K.placeholder(ndim=len(shape), name=name + '_target'))\n \n         # prepare metrics\n-        if len(self.outputs) > 1:\n-            self.metrics_names = ['total_loss']\n-        else:\n         self.metrics_names = ['loss']\n         self.metrics = []\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#844420425e0588fff82488027ccfaa2ac99094f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 29 | Churn Cumulative: 3691 | Contributors (this commit): 30 | Commits (past 90d): 85 | Contributors (cumulative): 51 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,10 @@ def softplus(x):\n     return K.softplus(x)\n \n \n+def softsign(x):\n+    return K.softsign(x)\n+\n+\n def relu(x, alpha=0., max_value=None):\n     return K.relu(x, alpha=alpha, max_value=max_value)\n \n\n@@ -852,6 +852,10 @@ def softplus(x):\n     return tf.nn.softplus(x)\n \n \n+def softsign(x):\n+    return tf.nn.softsign(x)\n+\n+\n def categorical_crossentropy(output, target, from_logits=False):\n     '''Categorical crossentropy between an output tensor\n     and a target tensor, where the target is a tensor of the same\n\n@@ -3,6 +3,7 @@ from theano import tensor as T\n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from theano.tensor.signal import pool\n from theano.tensor.nnet import conv3d2d\n+from theano.sandbox import softsign as T_softsign\n import inspect\n import numpy as np\n from .common import _FLOATX, _EPSILON\n@@ -725,6 +726,10 @@ def softplus(x):\n     return T.nnet.softplus(x)\n \n \n+def softsign(x):\n+    return T_softsign.softsign(x)\n+\n+\n def categorical_crossentropy(output, target, from_logits=False):\n     if from_logits:\n         output = T.nnet.softmax(output)\n\n@@ -56,6 +56,22 @@ def test_softplus():\n     assert_allclose(result, expected, rtol=1e-05)\n \n \n+def test_softsign():\n+    '''\n+    Test using a reference softsign implementation\n+    '''\n+    def softsign(x):\n+        return np.divide(x, np.ones_like(x) + np.absolute(x))\n+\n+    x = K.placeholder(ndim=2)\n+    f = K.function([x],  [activations.softsign(x)])\n+    test_values = get_standard_values()\n+\n+    result = f([test_values])[0]\n+    expected = softsign(test_values)\n+    assert_allclose(result, expected, rtol=1e-05)\n+\n+\n def test_sigmoid():\n     '''\n     Test using a numerically stable reference sigmoid implementation\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33af75aa391940ab0f70c58ccf4ec326134fcf84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 506 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -59,8 +59,10 @@ class ActivityRegularizer(Regularizer):\n             raise Exception('Need to call `set_layer` on '\n                             'ActivityRegularizer instance '\n                             'before calling the instance.')\n-        output = self.layer.output\n-        regularized_loss = loss + self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n+        regularized_loss = loss\n+        for i in range(len(self.layer.inbound_nodes)):\n+            output = self.layer.get_output_at(i)\n+            regularized_loss += self.l1 * K.sum(K.mean(K.abs(output), axis=0))\n             regularized_loss += self.l2 * K.sum(K.mean(K.square(output), axis=0))\n         return K.in_train_phase(regularized_loss, loss)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
