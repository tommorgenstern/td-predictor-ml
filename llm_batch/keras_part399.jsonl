{"custom_id": "keras#ea0927b2523e1e684f7064e8f0289d6e81456a67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 46 | Churn Cumulative: 12930 | Contributors (this commit): 28 | Commits (past 90d): 15 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,6 +17,7 @@\n \"\"\"Built-in metrics.\"\"\"\n \n import abc\n+import copy\n import types\n from typing import List, Tuple, Union\n import warnings\n@@ -241,6 +242,29 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n     args = ','.join(f'{k}={v}' for k, v in self.get_config().items())\n     return f'{self.__class__.__name__}({args})'\n \n+  def __deepcopy__(self, memo):\n+    result = type(self)(name=self.name, dtype=self.dtype)\n+    memo[id(self)] = result\n+\n+    for k, v in self.__dict__.items():\n+      if k in ['update_state', 'result']:\n+        # `update_state` keeps a closure of `update_state_fn`, and deep\n+        # copying it would result in copying that old reference. Avoid that.\n+        # Likewise for `result`.\n+        continue\n+      if k in ['_obj_reference_counts_dict']:\n+        # `Layer.__setattr__` attempts to flatten the\n+        # `ObjectIdentityDictionary`, which can't be done since it stores\n+        # heterogeneous instances.\n+        tf.Module.__setattr__(result, k, copy.deepcopy(v, memo))\n+      elif k in ['_thread_local', '_metrics_lock']:\n+        # Can't pickle _thread.lock objects.\n+        setattr(result, k, v)\n+      else:\n+        setattr(result, k, copy.deepcopy(v, memo))\n+\n+    return result\n+\n   @property\n   def dtype(self):\n     return self._dtype\n\n@@ -14,14 +14,13 @@\n # ==============================================================================\n \"\"\"Tests for Keras metrics functions.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n+import copy\n import json\n import math\n import os\n \n from absl.testing import parameterized\n-import numpy as np\n from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n@@ -31,6 +30,8 @@ from keras import Model\n from keras import testing_utils\n from keras.engine import base_layer\n from keras.engine import training as training_module\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -345,6 +346,23 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(m.total), 100)\n     self.assertEqual(self.evaluate(m.count), 1)\n \n+  @testing_utils.run_v2_only\n+  def test_deepcopy_of_metrics(self):\n+    m = metrics.Mean(name='my_mean')\n+\n+    m.reset_state()\n+    m.update_state(100)\n+    m_copied = copy.deepcopy(m)\n+    m_copied.update_state(200)\n+\n+    self.assertEqual(self.evaluate(m.result()), 100)\n+    self.assertEqual(self.evaluate(m_copied.result()), 150)\n+\n+    m.reset_state()\n+\n+    self.assertEqual(self.evaluate(m.result()), 0)\n+    self.assertEqual(self.evaluate(m_copied.result()), 150)\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class KerasAccuracyTest(tf.test.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba99979596edd7b710c5fe3240a73793e2f5006f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/1 | Churn Δ: 39 | Churn Cumulative: 22583 | Contributors (this commit): 121 | Commits (past 90d): 57 | Contributors (cumulative): 134 | DMM Complexity: 0.9\n\nDIFF:\n@@ -34,6 +34,7 @@ from keras.engine import data_adapter\n from keras.engine import training_utils\n from keras.mixed_precision import loss_scale_optimizer as lso\n from keras.mixed_precision import policy\n+from keras.optimizer_experimental import optimizer as optimizer_experimental\n from keras.saving import hdf5_format\n from keras.saving import save\n from keras.saving import saving_utils\n@@ -1437,6 +1438,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         if self.stop_training:\n           break\n \n+      if isinstance(self.optimizer, optimizer_experimental.Optimizer):\n+        self.optimizer.finalize_variable_values(self.trainable_variables)\n+\n       # If eval data_handler exists, delete it after all epochs are done.\n       if getattr(self, '_eval_data_handler', None) is not None:\n         del self._eval_data_handler\n\n@@ -26,6 +26,7 @@ from keras import keras_parameterized\n from keras import layers as layers_module\n from keras import losses\n from keras import metrics as metrics_module\n+from keras import optimizer_experimental\n from keras import optimizer_v2\n from keras import testing_utils\n from keras.callbacks import Callback\n@@ -1836,6 +1837,23 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertLen(history.history['loss'], 2)\n     self.assertAllClose(history.history['loss'][1], model.loss_metric.result())\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_ema_overwrite(self):\n+    tf.autograph.set_verbosity(10)\n+    model = sequential.Sequential()\n+    model.add(input_layer.Input(shape=(4,)))\n+    model.add(layers_module.Dense(1, activation='relu'))\n+\n+    tensors = tf.random.uniform((4, 4)), tf.random.uniform((4,))\n+    dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n+\n+    optimizer = optimizer_experimental.sgd.SGD(use_ema=True, ema_momentum=1)\n+    model.compile(optimizer, loss='mse', steps_per_execution=10)\n+    initial_value = tf.Variable(model.trainable_variables[0])\n+    history = model.fit(dataset, epochs=2, steps_per_epoch=10)\n+    self.assertLen(history.history['loss'], 2)\n+    self.assertAllClose(initial_value, model.trainable_variables[0])\n+\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n\n@@ -71,7 +71,8 @@ class _BaseOptimizer(tf.Module):\n     \"\"\"Create the iterations counter variable.\"\"\"\n     with tf.init_scope():\n       # Lift the variable creation to init scope to avoid environment issue.\n-      self._iterations = tf.Variable(0, name=\"iteration\", dtype=tf.int64)\n+      self._iterations = tf.Variable(\n+          0, name=\"iteration\", dtype=tf.int64, trainable=False)\n \n   def _process_kwargs(self, kwargs):\n     legacy_kwargs = {\n@@ -198,10 +199,14 @@ class _BaseOptimizer(tf.Module):\n       self._current_learning_rate = tf.Variable(\n           learning_rate(self.iterations),\n           name=\"learning_rate\",\n-          dtype=tf.float32)\n+          dtype=tf.float32,\n+          trainable=False)\n       return learning_rate\n     return tf.Variable(\n-        learning_rate, name=\"learning_rate\", dtype=backend.floatx())\n+        learning_rate,\n+        name=\"learning_rate\",\n+        dtype=backend.floatx(),\n+        trainable=False)\n \n   @abc.abstractmethod\n   def build(self, var_list):\n@@ -392,6 +397,9 @@ class _BaseOptimizer(tf.Module):\n     Args:\n       var_list: list of model variables.\n     \"\"\"\n+    if self._use_ema:\n+      # If the optimizer uses EMA, then when finalizing, we replace the model\n+      # variable value with its moving average stored inside optimizer.\n       self._overwrite_model_variables_with_average_value(var_list)\n \n   def _serialize_hyperparameter(self, hyperparameter):\n@@ -627,7 +635,7 @@ class Optimizer(_BaseOptimizer):\n         should_overwrite_model_vars = (\n             self.iterations % self._ema_overwrite_frequency == 0)\n         tf.cond(\n-            should_overwrite_model_vars,\n+            tf.cast(should_overwrite_model_vars, tf.bool),\n             true_fn=lambda: self._overwrite_model_variables_with_average_value(  # pylint: disable=g-long-lambda\n                 var_list),\n             false_fn=lambda: None)\n\n@@ -262,7 +262,6 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n   \"\"\"Test optimizer outputs the same numerical results as optimizer_v2.\"\"\"\n \n   def _compare_numerical(self, old_optimizer, new_optimizer):\n-    tf.config.run_functions_eagerly(True)\n     x1 = tf.Variable(np.ones([10]), dtype=tf.float64)\n     x2 = tf.Variable(np.ones([10]), dtype=tf.float64)\n     grads = tf.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2db93975c4d994ab52f918d0b78d63bc176ed6f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 23 | Churn Cumulative: 3650 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -394,12 +394,10 @@ class Functional(training_lib.Model):\n       dependencies['layer-%d' % layer_index] = layer\n     return dependencies\n \n-  @property\n-  def _checkpoint_dependencies(self):\n-    dependencies = [\n-        tf.__internal__.tracking.TrackableReference(name=name, ref=layer)\n-        for name, layer in self._layer_checkpoint_dependencies.items()]\n-    dependencies.extend(super(Functional, self)._checkpoint_dependencies)\n+  def _trackable_children(self, save_type='checkpoint', **kwargs):\n+    dependencies = self._layer_checkpoint_dependencies\n+    dependencies.update(\n+        super(Functional, self)._trackable_children(save_type, **kwargs))\n     return dependencies\n \n   def _lookup_dependency(self, name):\n\n@@ -137,20 +137,21 @@ class _DynamicLossScaleState(tf.__internal__.tracking.Trackable):\n     backend.track_variable(variable)\n     return variable\n \n-  @property\n-  def _checkpoint_dependencies(self):\n+  def _trackable_children(self, save_type='checkpoint', **kwargs):\n     \"\"\"From Trackable. Gather graph-specific weights to save.\"\"\"\n     if tf.executing_eagerly():\n       graph_key = None\n     else:\n       graph = tf.compat.v1.get_default_graph()\n       graph_key = graph._graph_key  # pylint: disable=protected-access\n-    weights = []\n+    weights = {}\n     for (name, g), v in sorted(self._weights.items(), key=lambda i: i[0][0]):\n       if g == graph_key:\n-        weights.append(tf.__internal__.tracking.TrackableReference(name=name, ref=v))\n-    return (super(_DynamicLossScaleState, self)._checkpoint_dependencies +\n-            weights)\n+        weights[name] = v\n+    weights.update(\n+        super(_DynamicLossScaleState,\n+              self)._trackable_children(save_type, **kwargs))\n+    return weights\n \n   def _lookup_dependency(self, name):\n     \"\"\"From Trackable. Find a weight in the current graph.\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa479cf5f6931e2284e9625bd9954c36e4aa83a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4379 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1839,7 +1839,7 @@ class TrainingTest(keras_parameterized.TestCase):\n \n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def test_ema_overwrite(self):\n-    tf.autograph.set_verbosity(10)\n+\n     model = sequential.Sequential()\n     model.add(input_layer.Input(shape=(4,)))\n     model.add(layers_module.Dense(1, activation='relu'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af08bf431177c6b63204ce52b28276f2e94be642", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 24 | Churn Cumulative: 158 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,6 +119,30 @@ class RNNV2Test(keras_parameterized.TestCase):\n     lstm = layer(32)\n     lstm(embedded_inputs)\n \n+  @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  def test_compare_ragged_with_masks(self, layer):\n+    vocab_size = 100\n+    timestep = 20\n+    units = 32\n+    embedder = embeddings.Embedding(input_dim=vocab_size, output_dim=units)\n+    layer = layer(units, return_sequences=True)\n+    data = tf.constant(\n+      np.random.RandomState(0).randint(0, vocab_size, [timestep, timestep]))\n+    mask = tf.sequence_mask(tf.range(1, timestep + 1))\n+    data_ragged = tf.ragged.boolean_mask(data, mask)\n+\n+    outputs = []\n+    devices = [testing_utils.device(should_use_gpu=False)]\n+    if tf.test.is_gpu_available():\n+      devices.append(testing_utils.device(should_use_gpu=True))\n+    for device in devices:\n+      with device:\n+        outputs.append(tf.boolean_mask(layer(embedder(data), mask=mask), mask))\n+        outputs.append(layer(embedder(data_ragged)).values)\n+\n+    for i in range(len(outputs) - 1):\n+      self.assertAllClose(outputs[i], outputs[i + 1], atol=1e-4)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c333b2baa938ebce4a4944056f3f3e527b980086", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 2352 | Contributors (this commit): 4 | Commits (past 90d): 53 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -17,9 +17,12 @@\n from keras.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n @generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.Adadelta', v1=[])\n class Adadelta(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Adadelta algorithm.\n \n\n@@ -18,9 +18,12 @@ from keras import initializers\n from keras.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n @generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.Adagrad', v1=[])\n class Adagrad(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Adagrad algorithm.\n \n\n@@ -17,9 +17,12 @@\n from keras.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n @generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.Adam', v1=[])\n class Adam(optimizer.Optimizer):\n   r\"\"\"Optimizer that implements the Adam algorithm.\n \n\n@@ -25,6 +25,8 @@ from keras import initializers\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.optimizer_v2 import utils as optimizer_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n class _BaseOptimizer(tf.Module):\n@@ -457,6 +459,7 @@ class _BaseOptimizer(tf.Module):\n     return cls(**config)\n \n \n+@keras_export(\"keras.optimizers.experimental.Optimizer\", v1=[])\n class Optimizer(_BaseOptimizer):\n   \"\"\"Abstract optimizer base class.\n \n\n@@ -17,9 +17,12 @@\n from keras.optimizer_experimental import optimizer\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n @generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.SGD', v1=[])\n class SGD(optimizer.Optimizer):\n   r\"\"\"Gradient descent (with momentum) optimizer.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4dbe9e55f2fd28ec7ff9ba8b5958141c5ffef2f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 173 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 189 | Churn Cumulative: 1983 | Contributors (this commit): 4 | Commits (past 90d): 37 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -120,12 +120,16 @@ class _BaseOptimizer(tf.Module):\n         and return the value to minimize.\n       var_list: list or tuple of `Variable` objects to update to minimize\n         `loss`.\n-      tape: (Optional) `tf.GradientTape`.\n+      tape: (Optional) `tf.GradientTape`. If `loss` is provided as a `Tensor`,\n+        the tape that computed the `loss` must be provided.\n \n     Returns:\n       A list of (gradient, variable) pairs. Variable is always present, but\n       gradient can be `None`.\n     \"\"\"\n+    if not callable(loss) and tape is None:\n+      raise ValueError(\"`tape` is required when a `Tensor` loss is passed. \"\n+                       f\"Received: loss={loss}, tape={tape}.\")\n     if tape is None:\n       tape = tf.GradientTape()\n     if callable(loss):\n@@ -489,13 +493,159 @@ class Optimizer(_BaseOptimizer):\n       overwrite the model variable by its stored moving average. If None, we\n       do not overwrite model variables in the middle of training, and users\n       need to explicitly overwrite the model variable by calling\n-      `finalize_variable_update()`.\n+      `finalize_variable_values()`.\n     jit_compile: bool, default to False. If True, the optimizer will use XLA\n       acceleration. `jit_compile` can only be False when using Parameter\n       Server Strategy.\n     **kwargs: keyword arguments only used for backward compatibility with\n       `optimizer_v2.OptimizerV2`. Any new code using\n       `optimizer_experimental.Optimizer` should leave this parameter empty.\n+\n+  ### Usage\n+\n+  ```python\n+  # Create an optimizer with the desired parameters.\n+  opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)\n+  var1, var2 = tf.Variable(1.0), tf.Variable(2.0)\n+  # `loss` is a callable that takes no argument and returns the value\n+  # to minimize.\n+  loss = lambda: 3 * var1 * var1 + 2 * var2 * var2\n+  # Call minimize to update the list of variables.\n+  opt.minimize(loss, var_list=[var1, var2])\n+  ```\n+\n+  ### Processing gradients before applying them\n+\n+  Calling `minimize()` takes care of both computing the gradients and\n+  applying them to the variables. If you want to process the gradients\n+  before applying them you can instead use the optimizer in three steps:\n+\n+  1.  Compute the gradients with `tf.GradientTape`.\n+  2.  Process the gradients as you wish.\n+  3.  Apply the processed gradients with `apply_gradients()`.\n+\n+  Example:\n+\n+  ```python\n+  # Create an optimizer.\n+  opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)\n+  var1, var2 = tf.Variable(1.0), tf.Variable(2.0)\n+\n+  # Compute the gradients for a list of variables.\n+  with tf.GradientTape() as tape:\n+    loss = 3 * var1 * var1 + 2 * var2 * var2\n+  grads = tape.gradient(loss, [var1, var2])\n+\n+  # Process the gradients.\n+  grads[0] = grads[0] + 1\n+\n+  # Ask the optimizer to apply the gradients on variables.\n+  opt.apply_gradients(zip(grads, [var1, var2]))\n+  ```\n+\n+  ### Dynamic learning rate\n+\n+  Dynamic learning rate can be achieved by setting learning rate as a built-in\n+  or customized `tf.keras.optimizers.schedules.LearningRateSchedule`.\n+\n+  Example:\n+\n+  >>> var = tf.Variable(np.random.random(size=(1,)))\n+  >>> learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n+  ... initial_learning_rate=.01, decay_steps=20, decay_rate=.1)\n+  >>> opt = tf.keras.optimizers.experimental.SGD(learning_rate=learning_rate)\n+  >>> loss = lambda: 3 * var\n+  >>> opt.minimize(loss, var_list=[var])\n+  <tf.Variable...\n+\n+  ### Gradients clipping\n+\n+  Users can clip the gradients before applying to variables by setting\n+  `clipnorm`, `clipvalue` and `global_clipnorm`. Notice that `clipnorm` and\n+  `global_clipnorm` can only have one being set.\n+\n+  Example:\n+\n+  >>> opt = tf.keras.optimizers.experimental.SGD(learning_rate=1, clipvalue=1)\n+  >>> var1, var2 = tf.Variable(2.0), tf.Variable(2.0)\n+  >>> with tf.GradientTape() as tape:\n+    ... loss = 2 * var1 + 2 * var2\n+  >>> grads = tape.gradient(loss, [var1, var2])\n+  >>> print([grads[0].numpy(), grads[1].numpy()])\n+  [2.0., 2.0]\n+  >>> opt.apply_gradients(zip(grads, [var1, var2]))\n+  >>> # Without clipping, we should get [0, 0], but as gradients are clipped to\n+  >>> # have max value 1, we get [1.0, 1.0].\n+  >>> print([var1.numpy(), var2.numpy()])\n+  [1.0, 1.0]\n+\n+  ### Using exponential moving average.\n+\n+  Empirically it has been found that using the exponential moving average (EMA)\n+  of the trained parameters of a deep network achieves a better performance than\n+  using its trained parameters directly. Keras optimizers allows users to\n+  compute this moving average and overwrite the model variables at desired time.\n+\n+  Example:\n+\n+  ```python\n+  # Create an SGD optimizer with EMA on. `ema_momentum` controls the decay rate\n+  # of the moving average. `ema_momentum=1` means no decay and the stored moving\n+  # average is always model variable's initial value before training. Reversely,\n+  # `ema_momentum=0` is equivalent to not using EMA. `ema_overwrite_frequency=3`\n+  # means every 3 iterations, we overwrite the trainable variables with their\n+  # moving average values.\n+  opt = tf.keras.optimizers.experimental.SGD(\n+      learning_rate=1,\n+      use_ema=True,\n+      ema_momentum=0.5,\n+      ema_overwrite_frequency=3)\n+  var1, var2 = tf.Variable(2.0), tf.Variable(2.0)\n+  with tf.GradientTape() as tape:\n+    loss = var1 + var2\n+  grads = tape.gradient(loss, [var1, var2])\n+  # First iteration: [var1, var2] = [1.0, 1.0]\n+  opt.apply_gradients(zip(grads, [var1, var2]))\n+  print([var1, var2])\n+\n+  # Second iteration: [var1, var2] = [0.0, 0.0]\n+  opt.apply_gradients(zip(grads, [var1, var2]))\n+  print([var1, var2])\n+\n+  # Third iteration, without EMA, we should see [var1, var2] = [-1.0, -1.0],\n+  # but overwriting results in [var1, var2] = [-0.125, -0.125]. The full\n+  # calculation for the moving average of var1 is:\n+  # var1=2*0.5**3+1*(1-0.5)*0.5**2+0*(1-0.5)*0.5**1+(-1)*(1-0.5)=-0.125.\n+  opt.apply_gradients(zip(grads, [var1, var2]))\n+  print([var1, var2])\n+\n+  ```\n+  When optimizer is constructed with `use_ema=True`, in custom training loop,\n+  users can explicitly call `finalize_variable_values()` to overwrite trainable\n+  variables with their EMA values. `finalize_variable_values()` is by default\n+  called at the end of `model.fit()`.\n+\n+  ### Use with `tf.distribute.Strategy`\n+\n+  This optimizer class is `tf.distribute.Strategy` aware, which means it\n+  automatically sums gradients across all replicas. To aggregate gradients\n+  yourself, call `apply_gradients` with `skip_aggregate_gradients` set to True.\n+  This is useful if you need to process aggregated gradients.\n+\n+  ```python\n+  # This example is not runnable, it consists of dummy code for simple tutorial.\n+  strategy = tf.distribute.experimental.TPUStrategy()\n+\n+  with strategy.scope():\n+    opt = tf.keras.optimizers.experimental.SGD()\n+    model = magic_function_that_returns_model()\n+    gradients = magic_function_that_returns_gradients()\n+    # Custom logic to aggregate gradients.\n+    gradients = strategy.reduce(\"SUM\", gradients, axis=None)\n+    opt.apply_gradients(zip(gradients, model.trainable_variables),\n+        skip_aggregate_gradients=True)\n+  ```\n+\n   \"\"\"\n \n   def __init__(self,\n\n@@ -148,21 +148,28 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n       optimizer.learning_rate = 2.0\n \n   def testMovingAverageOptimizer(self):\n-    # We set polyak averaging with ema_momentum = 1 so that the\n-    #  moving average is always the original value of the variables.\n-    optimizer = adam_new.Adam(\n-        use_ema=True, ema_momentum=1, ema_overwrite_frequency=2)\n-    x = tf.Variable([1.0, 2.0], dtype=tf.float32)\n-    x_origin = tf.Variable(x)\n-    grads = tf.convert_to_tensor([1.0, 2.0])\n-    # First iteration, we store the moving average, and do not do overriding.\n-    optimizer.apply_gradients(zip([grads], [x]))\n-    self.assertAllEqual(optimizer._model_variables_moving_average[0], x_origin)\n-    self.assertNotAllEqual(x, x_origin)\n+    optimizer = sgd_new.SGD(\n+        learning_rate=1,\n+        use_ema=True,\n+        ema_momentum=0.5,\n+        ema_overwrite_frequency=3)\n \n-    # Second iteration, we store the moving average, and override model vars.\n-    optimizer.apply_gradients(zip([grads], [x]))\n-    self.assertAllEqual(x, x_origin)\n+    var1, var2 = tf.Variable(2.0), tf.Variable(2.0)\n+    with tf.GradientTape() as tape:\n+      loss = var1 + var2\n+    grads = tape.gradient(loss, [var1, var2])\n+    # First iteration: [var1, var2] = [1.0, 1.0]\n+    optimizer.apply_gradients(zip(grads, [var1, var2]))\n+    self.assertAllEqual([var1.numpy(), var2.numpy()], [1.0, 1.0])\n+\n+    # Second iteration: [var1, var2] = [0.0, 0.0]\n+    optimizer.apply_gradients(zip(grads, [var1, var2]))\n+    self.assertAllEqual([var1.numpy(), var2.numpy()], [0.0, 0.0])\n+\n+    # Third iteration, without EMA, we should see [var1, var2] = [-1.0, -1.0],\n+    # but overwriting results in [var1, var2] = [-0.125, -0.125].\n+    optimizer.apply_gradients(zip(grads, [var1, var2]))\n+    self.assertAllEqual([var1.numpy(), var2.numpy()], [-0.125, -0.125])\n \n   def testGetAndFromConfig(self):\n     optimizer = adam_new.Adam(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1e9c76675981ee6683f54a3ce569212d551d12d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1556 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -253,7 +253,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     with previous_losses[0].graph.as_default():\n       # If we try to compare symbolic Tensors in eager mode assertAllEqual will\n       # return False even if they are the same Tensor.\n-      self.assertAllEqual(previous_losses, model.losses)\n+      self.assertEqual(previous_losses, model.losses)\n \n     if tf.executing_eagerly():\n       # Test that eager losses are maintained.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#803c5837d0e65038f1b1cb03c577af0a0868eb89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 7439 | Contributors (this commit): 101 | Commits (past 90d): 14 | Contributors (cumulative): 101 | DMM Complexity: None\n\nDIFF:\n@@ -1228,7 +1228,8 @@ class ModelCheckpoint(Callback):\n           `history = model.fit()`\n         * Multi-output models set additional prefixes on the metric names.\n \n-      verbose: verbosity mode, 0 or 1.\n+      verbose: Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n+        displays messages when the callback takes an action.\n       save_best_only: if `save_best_only=True`, it only saves when the model\n         is considered the \"best\" and the latest best model according to the\n         quantity monitored will not be overwritten. If `filepath` doesn't\n@@ -1622,6 +1623,7 @@ class BackupAndRestore(Callback):\n   invalid.\n \n   Note:\n+\n   1. This callback is not compatible with eager execution disabled.\n   2. A checkpoint is saved at the end of each epoch. After restoring,\n   `Model.fit` redoes any partial work during the unfinished epoch in which the\n@@ -1762,8 +1764,8 @@ class EarlyStopping(Callback):\n         improvement.\n     patience: Number of epochs with no improvement\n         after which training will be stopped.\n-    verbose: int. 0: quiet, 1: message when the training is stopped\n-        and if weights got restored.\n+    verbose: Verbosity mode, 0 or 1. Mode 0 is silent, and mode 1\n+        displays messages when the callback takes an action.\n     mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n         training will stop when the quantity\n         monitored has stopped decreasing; in `\"max\"`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#218e20dae073548e21d7d118112d777b4f0d3ce4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7336 | Contributors (this commit): 23 | Commits (past 90d): 4 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -2197,7 +2197,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       return self._inbound_nodes[0].input_shapes\n     else:\n       raise AttributeError('The layer \"' + str(self.name) +\n-                           ' has multiple inbound nodes, '\n+                           '\" has multiple inbound nodes, '\n                            'with different input shapes. Hence '\n                            'the notion of \"input shape\" is '\n                            'ill-defined for the layer. '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab5b8a771cfff14413fb8d01a8e5c24f653fb4ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 159 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -120,6 +120,7 @@ class RNNV2Test(keras_parameterized.TestCase):\n     lstm(embedded_inputs)\n \n   @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n+  @testing_utils.run_v2_only\n   def test_compare_ragged_with_masks(self, layer):\n     vocab_size = 100\n     timestep = 20\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a726d9844b4dce3efc5077718bdfb905e1750fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 22 | Churn Cumulative: 2005 | Contributors (this commit): 4 | Commits (past 90d): 39 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -175,6 +175,17 @@ class _BaseOptimizer(tf.Module):\n     \"\"\"\n     return self._iterations\n \n+  @iterations.setter\n+  def iterations(self, variable):\n+    if getattr(self, \"_built\", False):\n+      raise RuntimeError(\"Cannot set `iterations` to a new Variable after \"\n+                         \"the Optimizer weights have been created. Here it is \"\n+                         f\"attempting to set `iterations` to {variable}.\"\n+                         \"Usually this means you are trying to set `iterations`\"\n+                         \" after calling `apply_gradients()`. Please set \"\n+                         \"`iterations` before calling `apply_gradients()`.\")\n+    self._iterations = variable\n+\n   @property\n   def learning_rate(self):\n     if not hasattr(self, \"_learning_rate\") or self._learning_rate is None:\n\n@@ -147,6 +147,17 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(TypeError, \"This optimizer was created with*\"):\n       optimizer.learning_rate = 2.0\n \n+  def testSetIterations(self):\n+    optimizer = adam_new.Adam()\n+    optimizer.iterations = tf.Variable(2, dtype=tf.int32)\n+    self.assertEqual(optimizer.iterations, 2)\n+    var_list = [tf.Variable(2.0), tf.Variable(2.0)]\n+    grads = tf.convert_to_tensor([1.0, 1.0])\n+    optimizer.apply_gradients(zip(grads, var_list))\n+    self.assertEqual(optimizer.iterations, 3)\n+    with self.assertRaisesRegex(RuntimeError, \"Cannot set*\"):\n+      optimizer.iterations = 2\n+\n   def testMovingAverageOptimizer(self):\n     optimizer = sgd_new.SGD(\n         learning_rate=1,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71d4d67dd74e82227cad4ee0a4a59b91d2758419", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 2067 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -93,11 +93,15 @@ class BaseLayerTest(keras_parameterized.TestCase):\n       model.compile(rmsprop.RMSprop(0.001), loss='mse')\n       model.train_on_batch(np.random.random((2, 3)), np.random.random((2, 3)))\n     except tf.errors.OperatorNotAllowedInGraphError as e:\n-      if 'iterating over `tf.Tensor` is not allowed' in str(e):\n+      if 'iterating over `tf.Tensor`' in str(e):\n+        raised_error = True\n+      elif 'Iterating over a symbolic `tf.Tensor`' in str(e):\n         raised_error = True\n     except TypeError as e:\n       if 'attempting to use Python control flow' in str(e):\n         raised_error = True\n+      elif 'Attempting to use Python control flow' in str(e):\n+        raised_error = True\n     self.assertTrue(raised_error)\n \n   @combinations.generate(combinations.keras_model_type_combinations())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e53ff1cab1b8b99b0dc41e082613dbc0081276bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 229 | Lines Deleted: 73 | Files Changed: 9 | Hunks: 33 | Methods Changed: 16 | Complexity Δ (Sum/Max): 3/9 | Churn Δ: 302 | Churn Cumulative: 3612 | Contributors (this commit): 22 | Commits (past 90d): 17 | Contributors (cumulative): 45 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,6 +152,7 @@ from keras.layers.noise import GaussianDropout\n # Normalization layers.\n from keras.layers.normalization.layer_normalization import LayerNormalization\n from keras.layers.normalization.batch_normalization import SyncBatchNormalization\n+from keras.layers.normalization.unit_normalization import UnitNormalization\n \n if tf.__internal__.tf2.enabled():\n   from keras.layers.normalization.batch_normalization import BatchNormalization\n\n@@ -1 +0,0 @@\n-\n\n@@ -22,6 +22,7 @@ from keras import regularizers\n from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n from keras.utils import control_flow_util\n+from keras.utils import tf_utils\n from tensorflow.python.ops.control_flow_ops import get_enclosing_xla_context\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n@@ -292,43 +293,24 @@ class BatchNormalizationBase(Layer):\n                 False))\n \n   def build(self, input_shape):\n+    self.axis = tf_utils.validate_axis(self.axis, input_shape)\n     input_shape = tf.TensorShape(input_shape)\n-    if not input_shape.ndims:\n-      raise ValueError(\n-          f'Input has undefined rank. Received: input_shape={input_shape}.')\n-    ndims = len(input_shape)\n-\n-    # Convert axis to list and resolve negatives\n-    if isinstance(self.axis, int):\n-      self.axis = [self.axis]\n-\n-    for idx, x in enumerate(self.axis):\n-      if x < 0:\n-        self.axis[idx] = ndims + x\n-\n-    # Validate axes\n-    for x in self.axis:\n-      if x < 0 or x >= ndims:\n-        raise ValueError(\n-            f'Invalid axis. Expected 0 <= axis < inputs.rank (with '\n-            f'inputs.rank={ndims}). Received: layer.axis={self.axis}')\n-    if len(self.axis) != len(set(self.axis)):\n-      raise ValueError('Duplicate axis: %s' % (self.axis,))\n+    rank = input_shape.rank\n \n     if self.virtual_batch_size is not None:\n       if self.virtual_batch_size <= 0:\n         raise ValueError(\n-            f'virtual_batch_size must be a positive integer that divides the '\n+            f'`virtual_batch_size` must be a positive integer that divides the '\n             f'true batch size of the input tensor. Received: '\n             f'virtual_batch_size={self.virtual_batch_size}')\n       # If using virtual batches, the first dimension must be the batch\n       # dimension and cannot be the batch norm axis\n       if 0 in self.axis:\n-        raise ValueError('When using virtual_batch_size, the batch dimension '\n+        raise ValueError('When using `virtual_batch_size`, the batch dimension '\n                          'must be 0 and thus axis cannot include 0. '\n-                         'Received axis=%s' % (self.axis,))\n+                         f'Received axis={self.axis}')\n       if self.adjustment is not None:\n-        raise ValueError('When using virtual_batch_size, adjustment cannot '\n+        raise ValueError('When using `virtual_batch_size`, adjustment cannot '\n                          'be specified')\n \n     if self.fused in (None, True):\n@@ -336,15 +318,14 @@ class BatchNormalizationBase(Layer):\n       # output back to its original shape accordingly.\n       if self._USE_V2_BEHAVIOR:\n         if self.fused is None:\n-          self.fused = ndims in (4, 5)\n-        elif self.fused and ndims not in (4, 5):\n+          self.fused = rank in (4, 5)\n+        elif self.fused and rank not in (4, 5):\n           raise ValueError('Batch normalization layers with `fused=True` only '\n                            'support 4D or 5D input tensors. '\n-                           'Received tensor with shape: %s' %\n-                           (tuple(input_shape),))\n+                           f'Received tensor with shape: {tuple(input_shape)}')\n       else:\n         assert self.fused is not None\n-        self.fused = (ndims in (4, 5) and self._fused_can_be_used())\n+        self.fused = (rank in (4, 5) and self._fused_can_be_used())\n       # TODO(chrisying): fused batch norm is currently not supported for\n       # multi-axis batch norm and by extension virtual batches. In some cases,\n       # it might be possible to use fused batch norm but would require reshaping\n@@ -353,37 +334,37 @@ class BatchNormalizationBase(Layer):\n       # common use case (turning 5D w/ virtual batch to NCHW)\n \n     if self.fused:\n-      if self.axis == [1] and ndims == 4:\n+      if self.axis == [1] and rank == 4:\n         self._data_format = 'NCHW'\n-      elif self.axis == [1] and ndims == 5:\n+      elif self.axis == [1] and rank == 5:\n         self._data_format = 'NCDHW'\n-      elif self.axis == [3] and ndims == 4:\n+      elif self.axis == [3] and rank == 4:\n         self._data_format = 'NHWC'\n-      elif self.axis == [4] and ndims == 5:\n+      elif self.axis == [4] and rank == 5:\n         self._data_format = 'NDHWC'\n-      elif ndims == 5:\n+      elif rank == 5:\n         # 5D tensors that can be passed in but should not use fused batch norm\n         # due to unsupported axis.\n         self.fused = False\n       else:\n-        if ndims == 4:\n+        if rank == 4:\n           raise ValueError(\n               'Unsupported axis. The use of `fused=True` is only possible with '\n-              '`axis=1` or `axis=3` for 4D input tensors. Received '\n-              'axis=%s' % (self.axis,))\n+              '`axis=1` or `axis=3` for 4D input tensors. Received: '\n+              f'axis={tuple(self.axis)}')\n         else:\n           raise ValueError(\n               'Unsupported axis. The use of `fused=True` is only possible with '\n-              '`axis=1` or `axis=4` for 5D input tensors. Received '\n-              'axis=%s' % (self.axis,))\n+              '`axis=1` or `axis=4` for 5D input tensors. Received: '\n+              f'axis={tuple(self.axis)}')\n \n     axis_to_dim = {x: input_shape.dims[x].value for x in self.axis}\n     for x in axis_to_dim:\n       if axis_to_dim[x] is None:\n         raise ValueError('Input has undefined `axis` dimension. Received input '\n-                         'with shape %s. Axis value: %s' %\n-                         (tuple(input_shape), self.axis))\n-    self.input_spec = InputSpec(ndim=ndims, axes=axis_to_dim)\n+                         f'with shape {tuple(input_shape)} '\n+                         f'and axis={tuple(self.axis)}')\n+    self.input_spec = InputSpec(ndim=rank, axes=axis_to_dim)\n \n     if len(axis_to_dim) == 1 and self.virtual_batch_size is None:\n       # Single axis batch norm (most common/default use-case)\n@@ -391,7 +372,7 @@ class BatchNormalizationBase(Layer):\n     else:\n       # Parameter shape is the original shape but with 1 in all non-axis dims\n       param_shape = [\n-          axis_to_dim[i] if i in axis_to_dim else 1 for i in range(ndims)\n+          axis_to_dim[i] if i in axis_to_dim else 1 for i in range(rank)\n       ]\n       if self.virtual_batch_size is not None:\n         # When using virtual batches, add an extra dim at index 1\n@@ -486,7 +467,7 @@ class BatchNormalizationBase(Layer):\n         # device and ignore any devices that may be set by the custom getter.\n         def _renorm_variable(name,\n                              shape,\n-                             initializer=tf.compat.v1.zeros_initializer()):\n+                             initializer='zeros'):\n           \"\"\"Create a renorm variable.\"\"\"\n           var = self.add_weight(\n               name=name,\n\n@@ -21,6 +21,7 @@ from keras import constraints\n from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n+from keras.utils import tf_utils\n \n from tensorflow.python.util.tf_export import keras_export\n \n@@ -162,7 +163,7 @@ class LayerNormalization(Layer):\n                **kwargs):\n     super(LayerNormalization, self).__init__(**kwargs)\n     if isinstance(axis, (list, tuple)):\n-      self.axis = axis[:]\n+      self.axis = list(axis)\n     elif isinstance(axis, int):\n       self.axis = axis\n     else:\n@@ -207,27 +208,9 @@ class LayerNormalization(Layer):\n     return can_use_fused\n \n   def build(self, input_shape):\n-    ndims = len(input_shape)\n-    if ndims is None:\n-      raise ValueError('Input shape %s has undefined rank.' % input_shape)\n-\n-    # Convert axis to list and resolve negatives\n-    if isinstance(self.axis, int):\n-      self.axis = [self.axis]\n-    elif isinstance(self.axis, tuple):\n-      self.axis = list(self.axis)\n-    for idx, x in enumerate(self.axis):\n-      if x < 0:\n-        self.axis[idx] = ndims + x\n-\n-    # Validate axes\n-    for x in self.axis:\n-      if x < 0 or x >= ndims:\n-        raise ValueError(\n-            f'Invalid axis. Expected 0 <= axis < inputs.rank (with '\n-            f'inputs.rank={ndims}). Received: layer.axis={self.axis}')\n-    if len(self.axis) != len(set(self.axis)):\n-      raise ValueError('Duplicate axis: {}'.format(tuple(self.axis)))\n+    self.axis = tf_utils.validate_axis(self.axis, input_shape)\n+    input_shape = tf.TensorShape(input_shape)\n+    rank = input_shape.rank\n \n     param_shape = [input_shape[dim] for dim in self.axis]\n     if self.scale:\n@@ -254,8 +237,7 @@ class LayerNormalization(Layer):\n     else:\n       self.beta = None\n \n-    self._fused = self._fused_can_be_used(ndims)\n-\n+    self._fused = self._fused_can_be_used(rank)\n     self.built = True\n \n   def call(self, inputs):\n\n@@ -158,7 +158,7 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n   def testInvalidAxis(self):\n     with self.assertRaisesRegex(\n         ValueError,\n-        r'Invalid axis. Expected 0 <= axis < inputs.rank'):\n+        r'Invalid value for `axis` argument. Expected 0 <= axis < inputs.rank'):\n       layer_norm = layer_normalization.LayerNormalization(axis=3)\n       layer_norm.build(input_shape=(2, 2, 2))\n \n\n@@ -0,0 +1,76 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Unit Normalization layer.\"\"\"\n+# pylint: disable=g-bad-import-order\n+# pylint: disable=g-direct-tensorflow-import\n+# pylint: disable=g-classes-have-attributes\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.engine import base_layer\n+from keras.utils import tf_utils\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.UnitNormalization', v1=[])\n+class UnitNormalization(base_layer.Layer):\n+  \"\"\"Unit normalization layer.\n+\n+  Normalize a batch of inputs so that each input in the batch has a L2 norm\n+  equal to 1 (across the axes specified in `axis`).\n+\n+  Example:\n+\n+  >>> data = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.float32)\n+  >>> normalized_data = tf.keras.layers.UnitNormalization()(data)\n+  >>> print(tf.reduce_sum(normalized_data[0, :] ** 2).numpy())\n+  1.0\n+\n+  Args:\n+    axis: Integer or list/tuple. The axis or axes to normalize across. Typically\n+      this is the features axis or axes. The left-out axes are typically the\n+      batch axis or axes. Defaults to `-1`, the last dimension in\n+      the input.\n+  \"\"\"\n+\n+  def __init__(self,\n+               axis=-1,\n+               **kwargs):\n+    super().__init__(**kwargs)\n+    if isinstance(axis, (list, tuple)):\n+      self.axis = list(axis)\n+    elif isinstance(axis, int):\n+      self.axis = axis\n+    else:\n+      raise TypeError(\n+          'Invalid value for `axis` argument: '\n+          'expected an int or a list/tuple of ints. '\n+          f'Received: axis={axis}')\n+    self.supports_masking = True\n+\n+  def build(self, input_shape):\n+    self.axis = tf_utils.validate_axis(self.axis, input_shape)\n+\n+  def call(self, inputs):\n+    return tf.linalg.l2_normalize(inputs, axis=self.axis)\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def get_config(self):\n+    config = super(UnitNormalization, self).get_config()\n+    config.update({'axis': self.axis})\n+    return config\n\n@@ -0,0 +1,78 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Unit Normalization layer.\"\"\"\n+# pylint: disable=g-bad-import-order\n+\n+import tensorflow.compat.v2 as tf\n+\n+import keras\n+from keras import keras_parameterized\n+from keras import testing_utils\n+\n+\n+def squared_l2_norm(x):\n+  return tf.reduce_sum(x ** 2)\n+\n+\n+class UnitNormalizationTest(keras_parameterized.TestCase):\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_basics(self):\n+    testing_utils.layer_test(\n+        keras.layers.UnitNormalization,\n+        kwargs={'axis': -1},\n+        input_shape=(2, 3))\n+    testing_utils.layer_test(\n+        keras.layers.UnitNormalization,\n+        kwargs={'axis': (1, 2)},\n+        input_shape=(1, 3, 3))\n+\n+  def test_correctness(self):\n+    layer = keras.layers.UnitNormalization(axis=-1)\n+    inputs = tf.random.normal(shape=(2, 3))\n+    outputs = layer(inputs).numpy()\n+    self.assertAllClose(squared_l2_norm(outputs[0, :]), 1.)\n+    self.assertAllClose(squared_l2_norm(outputs[1, :]), 1.)\n+\n+    layer = keras.layers.UnitNormalization(axis=(1, 2))\n+    inputs = tf.random.normal(shape=(2, 3, 3))\n+    outputs = layer(inputs).numpy()\n+    self.assertAllClose(squared_l2_norm(outputs[0, :, :]), 1.)\n+    self.assertAllClose(squared_l2_norm(outputs[1, :, :]), 1.)\n+\n+    layer = keras.layers.UnitNormalization(axis=1)\n+    inputs = tf.random.normal(shape=(2, 3, 2))\n+    outputs = layer(inputs).numpy()\n+    self.assertAllClose(squared_l2_norm(outputs[0, :, 0]), 1.)\n+    self.assertAllClose(squared_l2_norm(outputs[1, :, 0]), 1.)\n+    self.assertAllClose(squared_l2_norm(outputs[0, :, 1]), 1.)\n+    self.assertAllClose(squared_l2_norm(outputs[1, :, 1]), 1.)\n+\n+  def testInvalidAxis(self):\n+    with self.assertRaisesRegex(\n+        TypeError,\n+        r'Invalid value for `axis` argument'):\n+      layer = keras.layers.UnitNormalization(axis=None)\n+\n+    with self.assertRaisesRegex(\n+        ValueError,\n+        r'Invalid value for `axis` argument'):\n+      layer = keras.layers.UnitNormalization(axis=3)\n+      layer.build(input_shape=(2, 2, 2))\n+\n+\n+if __name__ == '__main__':\n+  tf.compat.v1.enable_v2_behavior()\n+  tf.test.main()\n\n@@ -42,6 +42,7 @@ from keras.layers import wrappers\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import batch_normalization_v1\n from keras.layers.normalization import layer_normalization\n+from keras.layers.normalization import unit_normalization\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import hashing\n@@ -58,8 +59,8 @@ from tensorflow.python.util.tf_export import keras_export\n ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n                embeddings, einsum_dense, local, merge, noise,\n-               batch_normalization_v1, layer_normalization, pooling,\n-               image_preprocessing, recurrent, wrappers, hashing,\n+               batch_normalization_v1, layer_normalization, unit_normalization,\n+               pooling, image_preprocessing, recurrent, wrappers, hashing,\n                hashed_crossing, category_encoding, discretization,\n                multi_head_attention, integer_lookup,\n                preprocessing_normalization, string_lookup, text_vectorization)\n\n@@ -224,6 +224,44 @@ def convert_shapes(input_shape, to_tuples=True):\n                                    input_shape)\n \n \n+def validate_axis(axis, input_shape):\n+  \"\"\"Validate an axis value and returns its standardized form.\n+\n+  Args:\n+    axis: Value to validate. Can be an integer or a list/tuple of integers.\n+      Integers may be negative.\n+    input_shape: Reference input shape that the axis/axes refer to.\n+\n+  Returns:\n+    Normalized form of `axis`, i.e. a list with all-positive values.\n+  \"\"\"\n+  input_shape = tf.TensorShape(input_shape)\n+  rank = input_shape.rank\n+  if not rank:\n+    raise ValueError(\n+        f'Input has undefined rank. Received: input_shape={input_shape}')\n+\n+  # Convert axis to list and resolve negatives\n+  if isinstance(axis, int):\n+    axis = [axis]\n+  else:\n+    axis = list(axis)\n+  for idx, x in enumerate(axis):\n+    if x < 0:\n+      axis[idx] = rank + x\n+\n+  # Validate axes\n+  for x in axis:\n+    if x < 0 or x >= rank:\n+      raise ValueError(\n+          'Invalid value for `axis` argument. '\n+          'Expected 0 <= axis < inputs.rank (with '\n+          f'inputs.rank={rank}). Received: axis={tuple(axis)}')\n+  if len(axis) != len(set(axis)):\n+    raise ValueError(f'Duplicate axis: {tuple(axis)}')\n+  return axis\n+\n+\n class ListWrapper:\n   \"\"\"A wrapper for lists to be treated as elements for `nest`.\"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
