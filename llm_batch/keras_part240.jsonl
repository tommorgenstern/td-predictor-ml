{"custom_id": "keras#e73199d413395d7ede0dd86c06f7a9ad2d1fee17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 22 | Churn Cumulative: 9387 | Contributors (this commit): 55 | Commits (past 90d): 12 | Contributors (cumulative): 55 | DMM Complexity: 0.0\n\nDIFF:\n@@ -888,13 +888,13 @@ class SimpleRNNCell(Layer):\n         prev_output = states[0]\n         if 0 < self.dropout < 1 and self._dropout_mask is None:\n             self._dropout_mask = _generate_dropout_mask(\n-                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n+                K.ones_like(inputs),\n                 self.dropout,\n                 training=training)\n         if (0 < self.recurrent_dropout < 1 and\n                 self._recurrent_dropout_mask is None):\n             self._recurrent_dropout_mask = _generate_dropout_mask(\n-                _generate_dropout_ones(inputs, self.units),\n+                K.ones_like(prev_output),\n                 self.recurrent_dropout,\n                 training=training)\n \n@@ -1329,14 +1329,14 @@ class GRUCell(Layer):\n \n         if 0 < self.dropout < 1 and self._dropout_mask is None:\n             self._dropout_mask = _generate_dropout_mask(\n-                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n+                K.ones_like(inputs),\n                 self.dropout,\n                 training=training,\n                 count=3)\n         if (0 < self.recurrent_dropout < 1 and\n                 self._recurrent_dropout_mask is None):\n             self._recurrent_dropout_mask = _generate_dropout_mask(\n-                _generate_dropout_ones(inputs, self.units),\n+                K.ones_like(h_tm1),\n                 self.recurrent_dropout,\n                 training=training,\n                 count=3)\n@@ -1887,14 +1887,14 @@ class LSTMCell(Layer):\n     def call(self, inputs, states, training=None):\n         if 0 < self.dropout < 1 and self._dropout_mask is None:\n             self._dropout_mask = _generate_dropout_mask(\n-                _generate_dropout_ones(inputs, K.shape(inputs)[-1]),\n+                K.ones_like(inputs),\n                 self.dropout,\n                 training=training,\n                 count=4)\n         if (0 < self.recurrent_dropout < 1 and\n                 self._recurrent_dropout_mask is None):\n             self._recurrent_dropout_mask = _generate_dropout_mask(\n-                _generate_dropout_ones(inputs, self.units),\n+                K.ones_like(states[0]),\n                 self.recurrent_dropout,\n                 training=training,\n                 count=4)\n@@ -2249,16 +2249,6 @@ class LSTM(RNN):\n         return cls(**config)\n \n \n-def _generate_dropout_ones(inputs, dims):\n-    # Currently, CNTK can't instantiate `ones` with symbolic shapes.\n-    # Will update workaround once CNTK supports it.\n-    if K.backend() == 'cntk':\n-        ones = K.ones_like(K.reshape(inputs[:, 0], (-1, 1)))\n-        return K.tile(ones, (1, dims))\n-    else:\n-        return K.ones((K.shape(inputs)[0], dims))\n-\n-\n def _generate_dropout_mask(ones, rate, training=None, count=1):\n     def dropped_inputs():\n         return K.dropout(ones, rate)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6332819c7fb1b01730817a2d46e4fff5a60436b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/2 | Churn Δ: 20 | Churn Cumulative: 6827 | Contributors (this commit): 32 | Commits (past 90d): 29 | Contributors (cumulative): 42 | DMM Complexity: 1.0\n\nDIFF:\n@@ -88,15 +88,8 @@ def in_train_phase(x, alt, training=None):\n         return result\n \n \n-def in_test_phase(x, alt):\n-    global _LEARNING_PHASE\n-    # Similar as in_train_phase, use element_select as workaround.\n-    if callable(x) and isinstance(x, C.cntk_py.Function) is False:\n-        x = x()\n-    if callable(alt) and isinstance(alt, C.cntk_py.Function) is False:\n-        alt = alt()\n-\n-    return C.element_select(learning_phase(), x, alt)\n+def in_test_phase(x, alt, training=None):\n+    return in_train_phase(alt, x, training=training)\n \n \n def _convert_string_dtype(dtype):\n\n@@ -1763,6 +1763,15 @@ class TestBackend(object):\n         for training in [True, False]:\n             check_two_tensor_operation('in_train_phase', (3, 3), (2, 2), [KTH, KTF],\n                                        training=training)\n+            check_two_tensor_operation('in_train_phase', (2, 3), (2, 3), BACKENDS,\n+                                       training=training)\n+\n+    def test_in_test_phase(self):\n+        for training in [True, False]:\n+            check_two_tensor_operation('in_test_phase', (3, 3), (2, 2), [KTH, KTF],\n+                                       training=training)\n+            check_two_tensor_operation('in_test_phase', (2, 3), (2, 3), BACKENDS,\n+                                       training=training)\n \n     def test_setfloatx_incorrect_values(self):\n         # Keep track of the old value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#946a3b371567af143e13c159f00051353f161c56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 29 | Churn Cumulative: 406 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,15 +5,8 @@ from __future__ import print_function\n \n import os\n \n-try:\n-    # pydot-ng is a fork of pydot that is better maintained.\n-    import pydot_ng as pydot\n-except ImportError:\n-    # pydotplus is an improved version of pydot\n-    try:\n-        import pydotplus as pydot\n-    except ImportError:\n-        # Fall back on pydot if necessary.\n+# `pydot` is an optional dependency,\n+# see `extras_require` in `setup.py`.\n try:\n     import pydot\n except ImportError:\n@@ -21,15 +14,21 @@ except ImportError:\n \n \n def _check_pydot():\n+    \"\"\"Raise errors if `pydot` or GraphViz unavailable.\"\"\"\n+    if pydot is None:\n+        raise ImportError(\n+            'Failed to import `pydot`. '\n+            'Please install `pydot`. '\n+            'For example with `pip install pydot`.')\n     try:\n         # Attempt to create an image of a blank graph\n         # to check the pydot/graphviz installation.\n         pydot.Dot.create(pydot.Dot())\n-    except Exception:\n-        # pydot raises a generic Exception here,\n-        # so no specific class can be caught.\n-        raise ImportError('Failed to import pydot. You must install pydot'\n-                          ' and graphviz for `pydotprint` to work.')\n+    except OSError:\n+        raise OSError(\n+            '`pydot` failed to call GraphViz.'\n+            'Please install GraphViz (https://www.graphviz.org/) '\n+            'and ensure that its executables are in the $PATH.')\n \n \n def model_to_dot(model,\n\n@@ -16,7 +16,7 @@ setup(name='Keras',\n                         'pyyaml',\n                         'h5py'],\n       extras_require={\n-          'visualize': ['pydot>=1.2.0'],\n+          'visualize': ['pydot>=1.2.4'],\n           'tests': ['pytest',\n                     'pytest-pep8',\n                     'pytest-xdist',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a053416a310868971a5ec743e714961e28d78c79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 3269 | Contributors (this commit): 53 | Commits (past 90d): 15 | Contributors (cumulative): 53 | DMM Complexity: None\n\nDIFF:\n@@ -718,17 +718,17 @@ class ImageDataGenerator(object):\n                  The dimensions to which all images found will be resized.\n                 color_mode: one of \"grayscale\", \"rbg\". Default: \"rgb\".\n                  Whether the images will be converted to have 1 or 3 color channels.\n-                classes: optional list of class subdirectories (e.g. `['dogs', 'cats']`).\n-                 Default: None. If not provided, the list of classes will\n-                 be automatically inferred from the subdirectory names/structure under `directory`,\n+                classes: optional list of class subdirectories (e.g. `['dogs', 'cats']`). Default: None.\n+                 If not provided, the list of classes will be automatically\n+                 inferred from the subdirectory names/structure under `directory`,\n                  where each subdirectory will be treated as a different class\n                  (and the order of the classes, which will map to the label indices, will be alphanumeric).\n                  The dictionary containing the mapping from class names to class\n                  indices can be obtained via the attribute `class_indices`.\n-                class_mode: one of \"categorical\", \"binary\", \"sparse\", \"input\" or None.\n-                 Default: \"categorical\". Determines the type of label arrays that are\n-                 returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels,\n-                 \"sparse\" will be 1D integer labels, \"input\" will be images identical to input images (mainly used to work with autoencoders).\n+                class_mode: one of \"categorical\", \"binary\", \"sparse\", \"input\" or None. Default: \"categorical\".\n+                 Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels,\n+                 \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels, \"input\" will be images identical\n+                 to input images (mainly used to work with autoencoders).\n                  If None, no labels are returned (the generator will only yield batches of image data, which is useful to use\n                  `model.predict_generator()`, `model.evaluate_generator()`, etc.).\n                   Please note that in case of class_mode None,\n@@ -751,8 +751,8 @@ class ImageDataGenerator(object):\n                  `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n \n         # Returns\n-            A DirectoryIterator yielding tuples of `(x, y)` where `x` is a numpy array of image data and\n-             `y` is a numpy array of corresponding labels.\n+            A DirectoryIterator yielding tuples of `(x, y)` where `x` is a numpy array containing a batch\n+            of images with shape `(batch_size, *target_size, channels)` and `y` is a numpy array of corresponding labels.\n         \"\"\"\n         return DirectoryIterator(\n             directory, self,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10533e08e78c1675ef30f9b38b974735247a4507", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2941 | Contributors (this commit): 69 | Commits (past 90d): 8 | Contributors (cumulative): 69 | DMM Complexity: None\n\nDIFF:\n@@ -447,8 +447,8 @@ class ModelCheckpoint(Callback):\n                             self.model.save(filepath, overwrite=True)\n                     else:\n                         if self.verbose > 0:\n-                            print('\\nEpoch %05d: %s did not improve' %\n-                                  (epoch + 1, self.monitor))\n+                            print('\\nEpoch %05d: %s did not improve from %0.5f' %\n+                                  (epoch + 1, self.monitor, self.best))\n             else:\n                 if self.verbose > 0:\n                     print('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#388a9ab4cd47d76e6feb592a24d76503a692a286", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 25/25 | Churn Δ: 54 | Churn Cumulative: 54 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from ..layers.merge import concatenate\n from .. import backend as K\n from ..layers.core import Lambda\n from ..engine.training import Model\n+from ..models import clone_model\n \n \n def _get_available_devices():\n@@ -19,7 +20,7 @@ def _normalize_device_name(name):\n     return name\n \n \n-def multi_gpu_model(model, gpus=None):\n+def multi_gpu_model(model, gpus=None, cpu_merge=True, cpu_relocation=False):\n     \"\"\"Replicates a model on different GPUs.\n \n     Specifically, this function implements single-machine\n@@ -46,12 +47,18 @@ def multi_gpu_model(model, gpus=None):\n             (see usage example below).\n         gpus: Integer >= 2 or list of integers, number of GPUs or\n             list of GPU IDs on which to create model replicas.\n+        cpu_merge: A boolean value to identify whether to force\n+            merging model weights under the scope of the CPU or not.\n+        cpu_relocation: A boolean value to identify whether to\n+            create the model's weights under the scope of the CPU.\n+            If the model is not defined under any preceding device\n+            scope, you can still rescue it by activating this option.\n \n     # Returns\n         A Keras `Model` instance which can be used just like the initial\n         `model` argument, but which distributes its workload on multiple GPUs.\n \n-    # Example\n+    # Example 1 - Training models with weights merge on CPU\n \n     ```python\n         import tensorflow as tf\n@@ -92,6 +99,40 @@ def multi_gpu_model(model, gpus=None):\n         model.save('my_model.h5')\n     ```\n \n+    # Example 2 - Training models with weights merge on CPU using cpu_relocation\n+\n+    ```python\n+         ..\n+         # Not needed to change the device scope for model definition:\n+         model = Xception(weights=None, ..)\n+\n+         try:\n+             model = multi_gpu_model(model, cpu_relocation=True)\n+             print(\"Training using multiple GPUs..\")\n+         except:\n+             print(\"Training using single GPU or CPU..\")\n+\n+         model.compile(..)\n+         ..\n+    ```\n+\n+    # Example 3 - Training models with weights merge on GPU (recommended for NV-link)\n+\n+    ```python\n+         ..\n+         # Not needed to change the device scope for model definition:\n+         model = Xception(weights=None, ..)\n+\n+         try:\n+             model = multi_gpu_model(model, cpu_merge=False)\n+             print(\"Training using multiple GPUs..\")\n+         except:\n+             print(\"Training using single GPU or CPU..\")\n+\n+         model.compile(..)\n+         ..\n+    ```\n+\n     # On model saving\n \n     To save the multi-gpu model, use `.save(fname)` or `.save_weights(fname)`\n@@ -151,6 +192,11 @@ def multi_gpu_model(model, gpus=None):\n         start = stride * i\n         return tf.slice(data, start, size)\n \n+    # Relocate the model definition under CPU device scope if needed\n+    if cpu_relocation:\n+        with tf.device('/cpu:0'):\n+            model = clone_model(model)\n+\n     all_outputs = []\n     for i in range(len(model.outputs)):\n         all_outputs.append([])\n@@ -180,8 +226,8 @@ def multi_gpu_model(model, gpus=None):\n                 for o in range(len(outputs)):\n                     all_outputs[o].append(outputs[o])\n \n-    # Merge outputs on CPU.\n-    with tf.device('/cpu:0'):\n+    # Merge outputs under expected scope.\n+    with tf.device('/cpu:0' if cpu_merge else '/gpu:%d' % target_gpu_ids[0]):\n         merged = []\n         for name, outputs in zip(model.output_names, all_outputs):\n             merged.append(concatenate(outputs,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0bcd9061856d699d9054f9475b5e59c51348319e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 2912 | Contributors (this commit): 19 | Commits (past 90d): 11 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -532,7 +532,11 @@ def batch_dot(x, y, axes=None):\n         axes = [len(x_shape) - 1, len(y_shape) - 2]\n \n     if len(x_shape) == 2 and len(y_shape) == 2:\n-        return sum(x * y, axis=1, keepdims=True)\n+        if axes[0] == axes[1]:\n+            result = sum(x * y, axis=axes[0], keepdims=True)\n+            return result if axes[0] == 1 else transpose(result)\n+        else:\n+            return sum(x * transpose(y), axis=axes[0], keepdims=True)\n     else:\n         if len(y_shape) == 2:\n             y = expand_dims(y)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a341c014412cbfc86a9dd9816ae228e398dff3a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 9391 | Contributors (this commit): 110 | Commits (past 90d): 13 | Contributors (cumulative): 110 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1132,6 +1132,9 @@ def batch_dot(x, y, axes=None):\n         axes = (axes, axes)\n     x_ndim = ndim(x)\n     y_ndim = ndim(y)\n+    if axes is None:\n+        # behaves like tf.batch_matmul as default\n+        axes = [x_ndim - 1, y_ndim - 2]\n     if x_ndim > y_ndim:\n         diff = x_ndim - y_ndim\n         y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#adc321b4d7a4e22f6bdb00b404dfe5e23d4887aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): -6/2 | Churn Δ: 25 | Churn Cumulative: 6835 | Contributors (this commit): 90 | Commits (past 90d): 23 | Contributors (cumulative): 93 | DMM Complexity: 1.0\n\nDIFF:\n@@ -853,6 +853,7 @@ class Model(Container):\n         nested_weighted_metrics = _collect_metrics(weighted_metrics, self.output_names)\n         self.metrics_updates = []\n         self.stateful_metric_names = []\n+        self.stateful_metric_functions = []\n         with K.name_scope('metrics'):\n             for i in range(len(self.outputs)):\n                 if i in skip_target_indices:\n@@ -929,6 +930,7 @@ class Model(Container):\n                         # stateful metrics (i.e. metrics layers).\n                         if isinstance(metric_fn, Layer) and metric_fn.stateful:\n                             self.stateful_metric_names.append(metric_name)\n+                            self.stateful_metric_functions.append(metric_fn)\n                             self.metrics_updates += metric_fn.updates\n \n                 handle_metrics(output_metrics)\n@@ -1174,8 +1176,7 @@ class Model(Container):\n \n         for epoch in range(initial_epoch, epochs):\n             # Reset stateful metrics\n-            for m in self.metrics:\n-                if isinstance(m, Layer) and m.stateful:\n+            for m in self.stateful_metric_functions:\n                 m.reset_states()\n             callbacks.on_epoch_begin(epoch)\n             epoch_logs = {}\n@@ -1363,8 +1364,7 @@ class Model(Container):\n         \"\"\"\n \n         if hasattr(self, 'metrics'):\n-            for m in self.metrics:\n-                if isinstance(m, Layer) and m.stateful:\n+            for m in self.stateful_metric_functions:\n                 m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n@@ -2185,8 +2185,7 @@ class Model(Container):\n             # Construct epoch logs.\n             epoch_logs = {}\n             while epoch < epochs:\n-                for m in self.metrics:\n-                    if isinstance(m, Layer) and m.stateful:\n+                for m in self.stateful_metric_functions:\n                     m.reset_states()\n                 callbacks.on_epoch_begin(epoch)\n                 steps_done = 0\n@@ -2331,8 +2330,7 @@ class Model(Container):\n \n         stateful_metric_indices = []\n         if hasattr(self, 'metrics'):\n-            for i, m in enumerate(self.metrics):\n-                if isinstance(m, Layer) and m.stateful:\n+            for m in self.stateful_metric_functions:\n                 m.reset_states()\n             stateful_metric_indices = [\n                 i for i, name in enumerate(self.metrics_names)\n\n@@ -107,7 +107,8 @@ def test_sparse_top_k_categorical_accuracy():\n \n \n @keras_test\n-def test_stateful_metrics():\n+@pytest.mark.parametrize('metrics_mode', ['list', 'dict'])\n+def test_stateful_metrics(metrics_mode):\n     np.random.seed(1334)\n \n     class BinaryTruePositives(keras.layers.Layer):\n@@ -155,11 +156,17 @@ def test_stateful_metrics():\n \n     # Test on simple model\n     inputs = keras.Input(shape=(2,))\n-    outputs = keras.layers.Dense(1, activation='sigmoid')(inputs)\n+    outputs = keras.layers.Dense(1, activation='sigmoid', name='out')(inputs)\n     model = keras.Model(inputs, outputs)\n+\n+    if metrics_mode == 'list':\n         model.compile(optimizer='sgd',\n                       loss='binary_crossentropy',\n                       metrics=['acc', metric_fn])\n+    elif metrics_mode == 'dict':\n+        model.compile(optimizer='sgd',\n+                      loss='binary_crossentropy',\n+                      metrics={'out': ['acc', metric_fn]})\n \n     samples = 1000\n     x = np.random.random((samples, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#083a41cc6be7b1796e3817df198d1557bb8557b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 10108 | Contributors (this commit): 86 | Commits (past 90d): 8 | Contributors (cumulative): 86 | DMM Complexity: None\n\nDIFF:\n@@ -35,6 +35,12 @@ except ImportError:\n def save_model(model, filepath, overwrite=True, include_optimizer=True):\n     \"\"\"Save a model to a HDF5 file.\n \n+    Note: Please also see\n+    [How can I install HDF5 or h5py to save my models in Keras?](\n+        /getting-started/faq/\n+        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n+    in the FAQ for instructions on how to install `h5py`.\n+\n     The saved model contains:\n         - the model's configuration (topology)\n         - the model's weights\n@@ -762,6 +768,15 @@ class Sequential(Model):\n                 topology.load_weights_from_hdf5_group(f, layers, reshape=reshape)\n \n     def save_weights(self, filepath, overwrite=True):\n+        \"\"\"Saves the weights of a model.\n+\n+        Note: Please also see\n+        [How can I install HDF5 or h5py to save my models in Keras?](\n+        /getting-started/faq/\n+        #how-can-i-install-HDF5-or-h5py-to-save-my-models-in-Keras)\n+        in the FAQ for instructions on how to install `h5py`.\n+        \"\"\"\n+\n         if h5py is None:\n             raise ImportError('`save_weights` requires h5py.')\n         # If file exists and should not be overwritten:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e24625095a33a5c9a2d016018203938e9bb2ccbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 334 | Lines Deleted: 84 | Files Changed: 5 | Hunks: 34 | Methods Changed: 26 | Complexity Δ (Sum/Max): 23/15 | Churn Δ: 418 | Churn Cumulative: 16383 | Contributors (this commit): 83 | Commits (past 90d): 34 | Contributors (cumulative): 115 | DMM Complexity: 0.8095238095238095\n\nDIFF:\n@@ -1353,31 +1353,37 @@ def rnn(step_function, inputs, initial_states,\n                                   'variable-length sequences. Please specify a '\n                                   'static length for your sequences.')\n \n+    rnn_inputs = inputs\n     if need_convert:\n         if go_backwards:\n-            inputs = reverse(inputs, 1)\n+            rnn_inputs = reverse(rnn_inputs, 1)\n \n-        inputs = C.to_sequence(inputs)\n+        rnn_inputs = C.to_sequence(rnn_inputs)\n \n-        j = 0\n-        while j < len(constants):\n-            if isinstance(constants[j], list):\n-                i = 0\n-                while i < len(constants[j]):\n-                    if _get_dynamic_axis_num(constants[j][i]) == 1:\n-                        constants[j][i] = C.sequence.broadcast_as(constants[j][i], inputs)\n-                    i += 1\n+        rnn_constants = []\n+        for constant in constants:\n+            if isinstance(constant, list):\n+                new_c = []\n+                for c in constant:\n+                    if _get_dynamic_axis_num(c) == 1:\n+                        new_c.append(C.sequence.broadcast_as(c, rnn_inputs))\n                     else:\n-                if _get_dynamic_axis_num(constants[j]) == 1:\n-                    constants[j] = C.sequence.broadcast_as(constants[j], inputs)\n-            j += 1\n+                        new_c.append(c)\n+                rnn_constants.append(new_c)\n+            else:\n+                if _get_dynamic_axis_num(constant) == 1:\n+                    rnn_constants.append(C.sequence.broadcast_as(constant, rnn_inputs))\n+                else:\n+                    rnn_constants.append(constant)\n+    else:\n+        rnn_constants = constants\n \n     if mask is not None and not has_seq_axis(mask):\n         if go_backwards:\n             mask = reverse(mask, 1)\n         if len(int_shape(mask)) == 2:\n             mask = expand_dims(mask)\n-        mask = C.to_sequence_like(mask, inputs)\n+        mask = C.to_sequence_like(mask, rnn_inputs)\n \n     states = tuple(initial)\n \n@@ -1389,7 +1395,7 @@ def rnn(step_function, inputs, initial_states,\n             for s, p in zip(states, place_holders):\n                 past_values.append(C.sequence.past_value(p, s))\n             new_output, new_states = step_function(\n-                x, tuple(past_values) + tuple(constants))\n+                x, tuple(past_values) + tuple(rnn_constants))\n \n             if getattr(new_output, '_uses_learning_phase', False):\n                 global uses_learning_phase\n@@ -1404,7 +1410,7 @@ def rnn(step_function, inputs, initial_states,\n                 new_output = n_s[0]\n             return new_output, n_s\n \n-        final_output, final_states = _recurrence(inputs, states, mask)\n+        final_output, final_states = _recurrence(rnn_inputs, states, mask)\n         last_output = C.sequence.last(final_output)\n         last_states = [C.sequence.last(s) for s in final_states]\n \n\n@@ -11,6 +11,7 @@ from .. import initializers\n from .. import regularizers\n from .. import constraints\n from .recurrent import _generate_dropout_mask\n+from .recurrent import _standardize_args\n \n import numpy as np\n import warnings\n@@ -270,8 +271,8 @@ class ConvRNN2D(RNN):\n             return [initial_state]\n \n     def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n-        inputs, initial_state, constants = self._standardize_args(\n-            inputs, initial_state, constants)\n+        inputs, initial_state, constants = _standardize_args(\n+            inputs, initial_state, constants, self._num_constants)\n \n         if initial_state is None and constants is None:\n             return super(ConvRNN2D, self).__call__(inputs, **kwargs)\n\n@@ -493,8 +493,8 @@ class RNN(Layer):\n             return [K.tile(initial_state, [1, self.cell.state_size])]\n \n     def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n-        inputs, initial_state, constants = self._standardize_args(\n-            inputs, initial_state, constants)\n+        inputs, initial_state, constants = _standardize_args(\n+            inputs, initial_state, constants, self._num_constants)\n \n         if initial_state is None and constants is None:\n             return super(RNN, self).__call__(inputs, **kwargs)\n@@ -633,46 +633,6 @@ class RNN(Layer):\n         else:\n             return output\n \n-    def _standardize_args(self, inputs, initial_state, constants):\n-        \"\"\"Standardize `__call__` to a single list of tensor inputs.\n-\n-        When running a model loaded from file, the input tensors\n-        `initial_state` and `constants` can be passed to `RNN.__call__` as part\n-        of `inputs` instead of by the dedicated keyword arguments. This method\n-        makes sure the arguments are separated and that `initial_state` and\n-        `constants` are lists of tensors (or None).\n-\n-        # Arguments\n-            inputs: tensor or list/tuple of tensors\n-            initial_state: tensor or list of tensors or None\n-            constants: tensor or list of tensors or None\n-\n-        # Returns\n-            inputs: tensor\n-            initial_state: list of tensors or None\n-            constants: list of tensors or None\n-        \"\"\"\n-        if isinstance(inputs, list):\n-            assert initial_state is None and constants is None\n-            if self._num_constants is not None:\n-                constants = inputs[-self._num_constants:]\n-                inputs = inputs[:-self._num_constants]\n-            if len(inputs) > 1:\n-                initial_state = inputs[1:]\n-            inputs = inputs[0]\n-\n-        def to_list_or_none(x):\n-            if x is None or isinstance(x, list):\n-                return x\n-            if isinstance(x, tuple):\n-                return list(x)\n-            return [x]\n-\n-        initial_state = to_list_or_none(initial_state)\n-        constants = to_list_or_none(constants)\n-\n-        return inputs, initial_state, constants\n-\n     def reset_states(self, states=None):\n         if not self.stateful:\n             raise AttributeError('Layer must be stateful.')\n@@ -2262,3 +2222,44 @@ def _generate_dropout_mask(ones, rate, training=None, count=1):\n         dropped_inputs,\n         ones,\n         training=training)\n+\n+\n+def _standardize_args(inputs, initial_state, constants, num_constants):\n+    \"\"\"Standardize `__call__` to a single list of tensor inputs.\n+\n+    When running a model loaded from file, the input tensors\n+    `initial_state` and `constants` can be passed to `RNN.__call__` as part\n+    of `inputs` instead of by the dedicated keyword arguments. This method\n+    makes sure the arguments are separated and that `initial_state` and\n+    `constants` are lists of tensors (or None).\n+\n+    # Arguments\n+        inputs: tensor or list/tuple of tensors\n+        initial_state: tensor or list of tensors or None\n+        constants: tensor or list of tensors or None\n+\n+    # Returns\n+        inputs: tensor\n+        initial_state: list of tensors or None\n+        constants: list of tensors or None\n+    \"\"\"\n+    if isinstance(inputs, list):\n+        assert initial_state is None and constants is None\n+        if num_constants is not None:\n+            constants = inputs[-num_constants:]\n+            inputs = inputs[:-num_constants]\n+        if len(inputs) > 1:\n+            initial_state = inputs[1:]\n+        inputs = inputs[0]\n+\n+    def to_list_or_none(x):\n+        if x is None or isinstance(x, list):\n+            return x\n+        if isinstance(x, tuple):\n+            return list(x)\n+        return [x]\n+\n+    initial_state = to_list_or_none(initial_state)\n+    constants = to_list_or_none(constants)\n+\n+    return inputs, initial_state, constants\n\n@@ -12,6 +12,8 @@ from ..engine.topology import _object_list_uid\n from ..utils.generic_utils import has_arg\n from .. import backend as K\n \n+from . import recurrent\n+\n \n class Wrapper(Layer):\n     \"\"\"Abstract wrapper base class.\n@@ -276,6 +278,7 @@ class Bidirectional(Wrapper):\n         self._trainable = True\n         super(Bidirectional, self).__init__(layer, **kwargs)\n         self.input_spec = layer.input_spec\n+        self._num_constants = None\n \n     @property\n     def trainable(self):\n@@ -314,36 +317,45 @@ class Bidirectional(Wrapper):\n             return [output_shape] + state_shape + copy.copy(state_shape)\n         return output_shape\n \n-    def __call__(self, inputs, initial_state=None, **kwargs):\n-        if isinstance(inputs, list):\n-            if len(inputs) > 1:\n-                initial_state = inputs[1:]\n-            inputs = inputs[0]\n+    def __call__(self, inputs, initial_state=None, constants=None, **kwargs):\n+        inputs, initial_state, constants = recurrent._standardize_args(\n+            inputs, initial_state, constants, self._num_constants)\n \n-        if initial_state is None:\n+        if initial_state is None and constants is None:\n             return super(Bidirectional, self).__call__(inputs, **kwargs)\n \n-        # Standardize `initial_state` into list\n-        if isinstance(initial_state, tuple):\n-            initial_state = list(initial_state)\n-        elif not isinstance(initial_state, list):\n-            initial_state = [initial_state]\n-\n+        # Applies the same workaround as in `RNN.__call__`\n+        additional_inputs = []\n+        additional_specs = []\n+        if initial_state is not None:\n             # Check if `initial_state` can be splitted into half\n             num_states = len(initial_state)\n             if num_states % 2 > 0:\n                 raise ValueError(\n-                'When passing `initial_state` to a Bidirectional RNN, the state '\n-                'should be a list containing the states of the underlying RNNs. '\n+                    'When passing `initial_state` to a Bidirectional RNN, '\n+                    'the state should be a list containing the states of '\n+                    'the underlying RNNs. '\n                     'Found: ' + str(initial_state))\n \n-        # Applies the same workaround as in `RNN.__call__`, without handling constants\n             kwargs['initial_state'] = initial_state\n-        additional_inputs = initial_state\n-        additional_specs = [InputSpec(shape=K.int_shape(state))\n+            additional_inputs += initial_state\n+            state_specs = [InputSpec(shape=K.int_shape(state))\n                            for state in initial_state]\n-        self.forward_layer.state_spec = additional_specs[:num_states // 2]\n-        self.backward_layer.state_spec = additional_specs[num_states // 2:]\n+            self.forward_layer.state_spec = state_specs[:num_states // 2]\n+            self.backward_layer.state_spec = state_specs[num_states // 2:]\n+            additional_specs += state_specs\n+        if constants is not None:\n+            kwargs['constants'] = constants\n+            additional_inputs += constants\n+            constants_spec = [InputSpec(shape=K.int_shape(constant))\n+                              for constant in constants]\n+            self.forward_layer.constants_spec = constants_spec\n+            self.backward_layer.constants_spec = constants_spec\n+            additional_specs += constants_spec\n+\n+            self._num_constants = len(constants)\n+            self.forward_layer._num_constants = self._num_constants\n+            self.backward_layer._num_constants = self._num_constants\n \n         is_keras_tensor = K.is_keras_tensor(additional_inputs[0])\n         for tensor in additional_inputs:\n@@ -368,12 +380,19 @@ class Bidirectional(Wrapper):\n         else:\n             return super(Bidirectional, self).__call__(inputs, **kwargs)\n \n-    def call(self, inputs, training=None, mask=None, initial_state=None):\n+    def call(self,\n+             inputs,\n+             mask=None,\n+             training=None,\n+             initial_state=None,\n+             constants=None):\n         kwargs = {}\n         if has_arg(self.layer.call, 'training'):\n             kwargs['training'] = training\n         if has_arg(self.layer.call, 'mask'):\n             kwargs['mask'] = mask\n+        if has_arg(self.layer.call, 'constants'):\n+            kwargs['constants'] = constants\n \n         if initial_state is not None and has_arg(self.layer.call, 'initial_state'):\n             forward_state = initial_state[:len(initial_state) // 2]\n@@ -429,13 +448,24 @@ class Bidirectional(Wrapper):\n         self.built = True\n \n     def compute_mask(self, inputs, mask):\n+        if isinstance(mask, list):\n+            mask = mask[0]\n         if self.return_sequences:\n             if not self.merge_mode:\n-                return [mask, mask]\n+                output_mask = [mask, mask]\n             else:\n-                return mask\n+                output_mask = mask\n         else:\n-            return None\n+            output_mask = [None, None] if not self.merge_mode else None\n+\n+        if self.return_state:\n+            states = self.forward_layer.states\n+            state_mask = [None for _ in states]\n+            if isinstance(output_mask, list):\n+                return output_mask + state_mask * 2\n+            return [output_mask] + state_mask * 2\n+\n+        return output_mask\n \n     @property\n     def trainable_weights(self):\n@@ -473,5 +503,18 @@ class Bidirectional(Wrapper):\n \n     def get_config(self):\n         config = {'merge_mode': self.merge_mode}\n+        if self._num_constants is not None:\n+            config['num_constants'] = self._num_constants\n+\n         base_config = super(Bidirectional, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n+\n+    @classmethod\n+    def from_config(cls, config, custom_objects=None):\n+        from . import deserialize as deserialize_layer\n+        rnn_layer = deserialize_layer(config.pop('layer'),\n+                                      custom_objects=custom_objects)\n+        num_constants = config.pop('num_constants', None)\n+        layer = cls(rnn_layer, **config)\n+        layer._num_constants = num_constants\n+        return layer\n\n@@ -1,8 +1,11 @@\n import pytest\n import numpy as np\n+import copy\n from numpy.testing import assert_allclose\n from keras.utils.test_utils import keras_test\n-from keras.layers import wrappers, Input\n+from keras.utils import CustomObjectScope\n+from keras.layers import wrappers, Input, Layer\n+from keras.layers import RNN\n from keras import layers\n from keras.models import Sequential, Model, model_from_json\n from keras import backend as K\n@@ -228,6 +231,31 @@ def test_Bidirectional():\n         model.fit(x, y, epochs=1, batch_size=1)\n \n \n+@keras_test\n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Unknown timestamps not supported in CNTK.')\n+def test_Bidirectional_unkown_timespamps():\n+    # test with functional API with unknown length\n+    rnn = layers.SimpleRNN\n+    samples = 2\n+    dim = 2\n+    timesteps = 2\n+    output_dim = 2\n+    dropout_rate = 0.2\n+    for mode in ['sum', 'concat']:\n+        x = np.random.random((samples, timesteps, dim))\n+        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n+        y = np.random.random((samples, target_dim))\n+\n+        inputs = Input((None, dim))\n+        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n+                                             recurrent_dropout=dropout_rate),\n+                                         merge_mode=mode)(inputs)\n+        model = Model(inputs, outputs)\n+        model.compile(loss='mse', optimizer='sgd')\n+        model.fit(x, y, epochs=1, batch_size=1)\n+\n+\n @keras_test\n @pytest.mark.parametrize('merge_mode', ['sum', 'mul', 'ave', 'concat', None])\n def test_Bidirectional_merged_value(merge_mode):\n@@ -344,6 +372,177 @@ def test_Bidirectional_state_reuse():\n     outputs = model.predict(inputs)\n \n \n+@keras_test\n+def test_Bidirectional_with_constants():\n+    class RNNCellWithConstants(Layer):\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(RNNCellWithConstants, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            if not isinstance(input_shape, list):\n+                raise TypeError('expects constants shape')\n+            [input_shape, constant_shape] = input_shape\n+            # will (and should) raise if more than one constant passed\n+\n+            self.input_kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer='uniform',\n+                name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.constant_kernel = self.add_weight(\n+                shape=(constant_shape[-1], self.units),\n+                initializer='uniform',\n+                name='constant_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states, constants):\n+            [prev_output] = states\n+            [constant] = constants\n+            h_input = K.dot(inputs, self.input_kernel)\n+            h_state = K.dot(prev_output, self.recurrent_kernel)\n+            h_const = K.dot(constant, self.constant_kernel)\n+            output = h_input + h_state + h_const\n+            return output, [output]\n+\n+        def get_config(self):\n+            config = {'units': self.units}\n+            base_config = super(RNNCellWithConstants, self).get_config()\n+            return dict(list(base_config.items()) + list(config.items()))\n+\n+    # Test basic case.\n+    x = Input((5, 5))\n+    c = Input((3,))\n+    cell = RNNCellWithConstants(32)\n+    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n+    with CustomObjectScope(custom_objects):\n+        layer = wrappers.Bidirectional(RNN(cell))\n+    y = layer(x, constants=c)\n+    model = Model([x, c], y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n+        np.zeros((6, 64))\n+    )\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    c_np = np.random.random((6, 3))\n+    y_np = model.predict([x_np, c_np])\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    with CustomObjectScope(custom_objects):\n+        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n+    y = layer(x, constants=c)\n+    model = Model([x, c], y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict([x_np, c_np])\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # test flat list inputs\n+    with CustomObjectScope(custom_objects):\n+        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n+    y = layer([x, c])\n+    model = Model([x, c], y)\n+    model.set_weights(weights)\n+    y_np_3 = model.predict([x_np, c_np])\n+    assert_allclose(y_np, y_np_3, atol=1e-4)\n+\n+\n+@keras_test\n+def test_Bidirectional_with_constants_layer_passing_initial_state():\n+    class RNNCellWithConstants(Layer):\n+        def __init__(self, units, **kwargs):\n+            self.units = units\n+            self.state_size = units\n+            super(RNNCellWithConstants, self).__init__(**kwargs)\n+\n+        def build(self, input_shape):\n+            if not isinstance(input_shape, list):\n+                raise TypeError('expects constants shape')\n+            [input_shape, constant_shape] = input_shape\n+            # will (and should) raise if more than one constant passed\n+\n+            self.input_kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer='uniform',\n+                name='kernel')\n+            self.recurrent_kernel = self.add_weight(\n+                shape=(self.units, self.units),\n+                initializer='uniform',\n+                name='recurrent_kernel')\n+            self.constant_kernel = self.add_weight(\n+                shape=(constant_shape[-1], self.units),\n+                initializer='uniform',\n+                name='constant_kernel')\n+            self.built = True\n+\n+        def call(self, inputs, states, constants):\n+            [prev_output] = states\n+            [constant] = constants\n+            h_input = K.dot(inputs, self.input_kernel)\n+            h_state = K.dot(prev_output, self.recurrent_kernel)\n+            h_const = K.dot(constant, self.constant_kernel)\n+            output = h_input + h_state + h_const\n+            return output, [output]\n+\n+        def get_config(self):\n+            config = {'units': self.units}\n+            base_config = super(RNNCellWithConstants, self).get_config()\n+            return dict(list(base_config.items()) + list(config.items()))\n+\n+    # Test basic case.\n+    x = Input((5, 5))\n+    c = Input((3,))\n+    s_for = Input((32,))\n+    s_bac = Input((32,))\n+    cell = RNNCellWithConstants(32)\n+    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n+    with CustomObjectScope(custom_objects):\n+        layer = wrappers.Bidirectional(RNN(cell))\n+    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n+    model = Model([x, s_for, s_bac, c], y)\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    model.train_on_batch(\n+        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 32)), np.zeros((6, 3))],\n+        np.zeros((6, 64))\n+    )\n+\n+    # Test basic case serialization.\n+    x_np = np.random.random((6, 5, 5))\n+    s_fw_np = np.random.random((6, 32))\n+    s_bk_np = np.random.random((6, 32))\n+    c_np = np.random.random((6, 3))\n+    y_np = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n+    weights = model.get_weights()\n+    config = layer.get_config()\n+    with CustomObjectScope(custom_objects):\n+        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n+    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n+    model = Model([x, s_for, s_bac, c], y)\n+    model.set_weights(weights)\n+    y_np_2 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n+    assert_allclose(y_np, y_np_2, atol=1e-4)\n+\n+    # verify that state is used\n+    y_np_2_different_s = model.predict([x_np, s_fw_np + 10., s_bk_np + 10., c_np])\n+    with pytest.raises(AssertionError):\n+        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n+\n+    # test flat list inputs\n+    with CustomObjectScope(custom_objects):\n+        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n+    y = layer([x, s_for, s_bac, c])\n+    model = Model([x, s_for, s_bac, c], y)\n+    model.set_weights(weights)\n+    y_np_3 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n+    assert_allclose(y_np, y_np_3, atol=1e-4)\n+\n+\n @keras_test\n def test_Bidirectional_trainable():\n     # test layers that need learning_phase to be set\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6171b3656ebd9b6038f709ba83f7475de284ba4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 131 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,9 +13,6 @@ to its get_next() method. Its components are then provided\n to the network's Input layer and the Model.compile() method,\n respectively.\n \n-Note that from TensorFlow 1.4, tf.contrib.data is deprecated\n-and tf.data is preferred. See the release notes for details.\n-\n This example is intended to closely follow the\n mnist_tfrecord.py example.\n '''\n@@ -29,7 +26,6 @@ from keras import layers\n from keras.datasets import mnist\n \n import tensorflow as tf\n-from tensorflow.contrib.data import Dataset\n \n \n if K.backend() != 'tensorflow':\n@@ -65,7 +61,7 @@ x_train = np.expand_dims(x_train, -1)\n y_train = tf.one_hot(y_train, num_classes)\n \n # Create the dataset and its associated one-shot iterator.\n-dataset = Dataset.from_tensor_slices((x_train, y_train))\n+dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n dataset = dataset.repeat()\n dataset = dataset.shuffle(buffer_size)\n dataset = dataset.batch(batch_size)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
