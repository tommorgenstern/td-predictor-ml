{"custom_id": "keras#a8836b48ddb4af9a778d48ba6aacd0a79eeffbb8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 500 | Contributors (this commit): 6 | Commits (past 90d): 22 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class InputLayer(Layer):\n         if shape is None and batch_shape is None:\n             raise ValueError(\"You must pass a `shape` argument.\")\n \n-        if shape:\n+        if shape is not None:\n             shape = backend.standardize_shape(shape)\n             batch_shape = (batch_size,) + shape\n         self.batch_shape = tuple(batch_shape)\n\n@@ -34,6 +34,17 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n+    def test_scalar_input(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        input_b = Input(shape=(), batch_size=2, name=\"input_b\")\n+        outputs = input_a + input_b[:, None]\n+        model = Functional([input_a, input_b], outputs)\n+        model.summary()\n+\n+        in_val = [np.zeros((2, 3)), np.ones((2,))]\n+        out_val = model(in_val)\n+        self.assertAllClose(out_val, np.ones((2, 3)))\n+\n     def test_basic_flow_multi_output(self):\n         inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n         x = layers.Dense(5)(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b720bba5f5914969f3b78b8741a986c5764de4df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 32 | Files Changed: 5 | Hunks: 17 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 65 | Churn Cumulative: 3329 | Contributors (this commit): 13 | Commits (past 90d): 139 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class InputLayer(Layer):\n         if shape is None and batch_shape is None:\n             raise ValueError(\"You must pass a `shape` argument.\")\n \n-        if shape is not None:\n+        if shape:\n             shape = backend.standardize_shape(shape)\n             batch_shape = (batch_size,) + shape\n         self.batch_shape = tuple(batch_shape)\n\n@@ -35,8 +35,8 @@ from keras_core.metrics.metric import Metric\n from keras_core.operations.operation import Operation\n from keras_core.utils import summary_utils\n from keras_core.utils import traceback_utils\n+from keras_core.utils import tracking\n from keras_core.utils.shape_utils import map_shape_structure\n-from keras_core.utils.tracking import Tracker\n \n if backend.backend() == \"tensorflow\":\n     from keras_core.backend.tensorflow.layer import TFLayer as BackendLayer\n@@ -257,40 +257,46 @@ class Layer(BackendLayer, Operation):\n         self._call_signature_parameters = [\n             p.name for p in inspect.signature(self.call).parameters.values()\n         ]\n-        self._configure_tracker()\n+        self._initializer_tracker()\n \n-    def _configure_tracker(self):\n+    @tracking.no_automatic_dependency_tracking\n+    def _initializer_tracker(self):\n         if hasattr(self, \"_tracker\"):\n             return\n \n-        self._trainable_variables = []\n-        self._non_trainable_variables = []\n-        self._layers = []\n-        self._metrics = []\n-        self._seed_generators = []\n-        self._tracker = Tracker(\n+        trainable_variables = []\n+        non_trainable_variables = []\n+        layers = []\n+        metrics = []\n+        seed_generators = []\n+        self._tracker = tracking.Tracker(\n             {\n                 \"trainable_variables\": (\n                     lambda x: isinstance(x, backend.Variable) and x.trainable,\n-                    self._trainable_variables,\n+                    trainable_variables,\n                 ),\n                 \"non_trainable_variables\": (\n                     lambda x: isinstance(x, backend.Variable)\n                     and not x.trainable,\n-                    self._non_trainable_variables,\n+                    non_trainable_variables,\n                 ),\n-                \"metrics\": (lambda x: isinstance(x, Metric), self._metrics),\n+                \"metrics\": (lambda x: isinstance(x, Metric), metrics),\n                 \"layers\": (\n                     lambda x: isinstance(x, Layer)\n                     and not isinstance(x, Metric),\n-                    self._layers,\n+                    layers,\n                 ),\n                 \"seed_generators\": (\n                     lambda x: isinstance(x, backend.random.SeedGenerator),\n-                    self._seed_generators,\n+                    seed_generators,\n                 ),\n             }\n         )\n+        self._trainable_variables = trainable_variables\n+        self._non_trainable_variables = non_trainable_variables\n+        self._layers = layers\n+        self._metrics = metrics\n+        self._seed_generators = seed_generators\n \n     @utils.default\n     def build(self, input_shape):\n@@ -1032,14 +1038,11 @@ class Layer(BackendLayer, Operation):\n         )\n \n     def __setattr__(self, name, value):\n-        # Prevent users from attaching state to the\n-        # layer before `super()` is called -- since that\n-        # state would silently not be tracked.\n-        if name != \"_lock\":\n-            self._check_super_called()\n         # Track Variables, Layers, Metrics, SeedGenerators.\n         if hasattr(self, \"_tracker\"):\n             value = self._tracker.track(value)\n+        elif name != \"_tracker\":\n+            self._initializer_tracker()\n         return super().__setattr__(name, value)\n \n     def _check_super_called(self):\n\n@@ -639,3 +639,13 @@ class LayerTest(testing.TestCase):\n             \"cannot add new elements of state\",\n         ):\n             layer(np.random.random((3, 2)))\n+\n+    def test_init_after_state_tracking(self):\n+        class MyLayer(layers.Layer):\n+            def __init__(self):\n+                self.some_attr = True\n+                self.w = backend.Variable(np.random.random((2,)))\n+                super().__init__()\n+\n+        layer = MyLayer()\n+        self.assertEqual(len(layer.weights), 1)\n\n@@ -34,17 +34,6 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n-    def test_scalar_input(self):\n-        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n-        input_b = Input(shape=(), batch_size=2, name=\"input_b\")\n-        outputs = input_a + input_b[:, None]\n-        model = Functional([input_a, input_b], outputs)\n-        model.summary()\n-\n-        in_val = [np.zeros((2, 3)), np.ones((2,))]\n-        out_val = model(in_val)\n-        self.assertAllClose(out_val, np.ones((2, 3)))\n-\n     def test_basic_flow_multi_output(self):\n         inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n         x = layers.Dense(5)(inputs)\n\n@@ -7,7 +7,6 @@ import warnings\n import numpy as np\n \n from keras_core import backend\n-from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n \n try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#68178e9cb222f69fc8e643f638b5bbe5ab426634", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 526 | Contributors (this commit): 6 | Commits (past 90d): 26 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,7 +30,7 @@ class InputLayer(Layer):\n         if shape is None and batch_shape is None:\n             raise ValueError(\"You must pass a `shape` argument.\")\n \n-        if shape:\n+        if shape is not None:\n             shape = backend.standardize_shape(shape)\n             batch_shape = (batch_size,) + shape\n         self.batch_shape = tuple(batch_shape)\n\n@@ -34,6 +34,17 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(in_val)\n         self.assertEqual(out_val.shape, (2, 4))\n \n+    def test_scalar_input(self):\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        input_b = Input(shape=(), batch_size=2, name=\"input_b\")\n+        outputs = input_a + input_b[:, None]\n+        model = Functional([input_a, input_b], outputs)\n+        model.summary()\n+\n+        in_val = [np.zeros((2, 3)), np.ones((2,))]\n+        out_val = model(in_val)\n+        self.assertAllClose(out_val, np.ones((2, 3)))\n+\n     def test_basic_flow_multi_output(self):\n         inputs = Input(shape=(3,), batch_size=2, name=\"input\")\n         x = layers.Dense(5)(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7e8cd5f6e8eca1ce24cf5676bfda7c86c4034d5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,9 +1,6 @@\n import sys\n \n from keras_core import backend as backend_module\n-from keras_core.backend import jax as jax_backend\n-from keras_core.backend import tensorflow as tf_backend\n-from keras_core.backend import torch as torch_backend\n \n \n def in_tf_graph():\n@@ -41,8 +38,14 @@ class DynamicBackend:\n \n     def __getattr__(self, name):\n         if self._backend == \"tensorflow\":\n+            from keras_core.backend import tensorflow as tf_backend\n+\n             return getattr(tf_backend, name)\n         if self._backend == \"jax\":\n+            from keras_core.backend import jax as jax_backend\n+\n             return getattr(jax_backend, name)\n         if self._backend == \"torch\":\n+            from keras_core.backend import torch as torch_backend\n+\n             return getattr(torch_backend, name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#325e5286cac2c73d0a6161c6851c80dc01091f41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 428 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,7 +103,7 @@ class KerasTensor:\n             \"class MyLayer(Layer):\\n\"\n             \"    def call(self, x):\\n\"\n             \"        return tf_fn(x)\\n\\n\"\n-            \"x = MyLayer()(x)\"\n+            \"x = MyLayer()(x)\\n\"\n             \"```\\n\"\n         )\n \n\n@@ -192,7 +192,9 @@ class BatchNormalization(Layer):\n         broadcast_shape[self.axis] = inputs.shape[self.axis]\n         if training and self.trainable:\n             mean = ops.mean(inputs, axis=self._reduction_axes, keepdims=True)\n-            variance = ops.var(inputs, axis=self._reduction_axes, keepdims=True)\n+            variance = ops.mean(\n+                ops.square(inputs), axis=self._reduction_axes, keepdims=True\n+            ) - ops.square(mean)\n             outputs = (inputs - mean) / ops.sqrt(variance + self.epsilon)\n             mean = ops.squeeze(mean, self._reduction_axes)\n             variance = ops.squeeze(variance, self._reduction_axes)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab030e3f188d32edf8fe13226cc0e5911971ab60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 40 | Churn Cumulative: 210 | Contributors (this commit): 11 | Commits (past 90d): 89 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras_core.api_export import keras_core_export\n from keras_core.layers.activations.activation import Activation\n from keras_core.layers.activations.elu import ELU\n from keras_core.layers.activations.leaky_relu import LeakyReLU\n@@ -124,3 +125,42 @@ from keras_core.layers.rnn.rnn import RNN\n from keras_core.layers.rnn.simple_rnn import SimpleRNN\n from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras_core.layers.rnn.time_distributed import TimeDistributed\n+from keras_core.saving import serialization_lib\n+\n+\n+@keras_core_export(\"keras_core.layers.serialize\")\n+def serialize(layer):\n+    \"\"\"Returns the layer configuration as a Python dict.\n+\n+    Args:\n+        layer: A `keras.layers.Layer` instance to serialize.\n+\n+    Returns:\n+        Python dict which contains the configuration of the layer.\n+    \"\"\"\n+    return serialization_lib.serialize_keras_object(layer)\n+\n+\n+@keras_core_export(\"keras_core.layers.deserialize\")\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Returns a Keras layer object via its configuration.\n+\n+    Args:\n+        config: A python dict containing a serialized layer configuration.\n+        custom_objects: Optional dictionary mapping names (strings) to custom\n+            objects (classes and functions) to be considered during\n+            deserialization.\n+\n+    Returns:\n+        A Keras layer instance.\n+    \"\"\"\n+    obj = serialization_lib.deserialize_keras_object(\n+        config,\n+        custom_objects=custom_objects,\n+    )\n+    if not isinstance(obj, Layer):\n+        raise ValueError(\n+            \"`keras.layers.deserialize` was passed a `config` object that is \"\n+            f\"not a `keras.layers.Layer`. Received: {config}\"\n+        )\n+    return obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ea98b399961fc85442ef297de5b66420cdfe6d82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 9 | Files Changed: 5 | Hunks: 7 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 35 | Churn Cumulative: 2414 | Contributors (this commit): 10 | Commits (past 90d): 98 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,6 +33,7 @@ from keras_core.backend.common import global_state\n from keras_core.layers import input_spec\n from keras_core.metrics.metric import Metric\n from keras_core.operations.operation import Operation\n+from keras_core.utils import python_utils\n from keras_core.utils import summary_utils\n from keras_core.utils import traceback_utils\n from keras_core.utils import tracking\n@@ -1121,6 +1122,15 @@ class Layer(BackendLayer, Operation):\n                     # It's a C type.\n                     pass\n \n+    @python_utils.default\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            \"trainable\": self.trainable,\n+            \"dtype\": self.dtype_policy.name,\n+        }\n+        return {**base_config, **config}\n+\n \n def is_backend_tensor_or_symbolic(x):\n     return backend.is_tensor(x) or isinstance(x, backend.KerasTensor)\n\n@@ -38,10 +38,8 @@ class Dropout(Layer):\n             training mode (adding dropout) or in inference mode (doing nothing).\n     \"\"\"\n \n-    def __init__(\n-        self, rate, noise_shape=None, seed=None, name=None, dtype=None\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n+        super().__init__(**kwargs)\n         if not 0 <= rate <= 1:\n             raise ValueError(\n                 f\"Invalid value received for argument \"\n\n@@ -24,8 +24,8 @@ class GaussianDropout(layers.Layer):\n             training mode (adding dropout) or in inference mode (doing nothing).\n     \"\"\"\n \n-    def __init__(self, rate, seed=None, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, rate, seed=None, **kwargs):\n+        super().__init__(**kwargs)\n         if not 0 <= rate <= 1:\n             raise ValueError(\n                 f\"Invalid value received for argument \"\n\n@@ -25,8 +25,8 @@ class GaussianNoise(layers.Layer):\n             training mode (adding noise) or in inference mode (doing nothing).\n     \"\"\"\n \n-    def __init__(self, stddev, seed=None, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, stddev, seed=None, **kwargs):\n+        super().__init__(**kwargs)\n         if not 0 <= stddev <= 1:\n             raise ValueError(\n                 f\"Invalid value received for argument \"\n\n@@ -87,8 +87,17 @@ class SerializationLibTest(testing.TestCase):\n             self.assertEqual(serialized, reserialized)\n \n     def test_builtin_layers(self):\n-        serialized, _, reserialized = self.roundtrip(keras_core.layers.Dense(3))\n+        layer = keras_core.layers.Dense(\n+            3,\n+            name=\"foo\",\n+            trainable=False,\n+            dtype=\"float16\",\n+        )\n+        serialized, restored, reserialized = self.roundtrip(layer)\n         self.assertEqual(serialized, reserialized)\n+        self.assertEqual(layer.name, restored.name)\n+        self.assertEqual(layer.trainable, restored.trainable)\n+        self.assertEqual(layer.compute_dtype, restored.compute_dtype)\n \n     def test_tensors_and_shapes(self):\n         x = ops.random.normal((2, 2), dtype=\"float64\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94831885d160f1107b43ce24121ed520bc159a1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 20 | Churn Cumulative: 745 | Contributors (this commit): 5 | Commits (past 90d): 36 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -117,6 +117,12 @@ class KerasVariable:\n         else:\n             self._direct_assign(value)\n \n+    def assign_add(self, value):\n+        self.assign(self + value)\n+\n+    def assign_sub(self, value):\n+        self.assign(self - value)\n+\n     @property\n     def dtype(self):\n         autocast_scope = get_autocast_scope()\n\n@@ -1,3 +1,5 @@\n+import numpy as np\n+\n from keras_core import backend\n from keras_core import initializers\n from keras_core.backend.common.variables import AutocastScope\n@@ -19,6 +21,18 @@ class VariablesTest(test_case.TestCase):\n             with backend.StatelessScope():\n                 v = backend.Variable(initializer=0)\n \n+    def test_deferred_assignment(self):\n+        with backend.StatelessScope() as scope:\n+            v = backend.Variable(\n+                initializer=initializers.RandomNormal(), shape=(2, 2)\n+            )\n+            self.assertEqual(v._value, None)\n+            v.assign(np.zeros((2, 2)))\n+            v.assign_add(2 * np.ones((2, 2)))\n+            v.assign_sub(np.ones((2, 2)))\n+        out = scope.get_current_value(v)\n+        self.assertAllClose(out, np.ones((2, 2)))\n+\n     def test_autocasting(self):\n         v = backend.Variable(\n             initializer=initializers.RandomNormal(),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6066bb3616b7420925a96887ffaec0666acbdb8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 14975 | Contributors (this commit): 40 | Commits (past 90d): 11 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1113,7 +1113,6 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             build_graph=not eager,\n             training=training_mode,\n         ):\n-\n             input_spec.assert_input_compatibility(\n                 self.input_spec, inputs, self.name\n             )\n@@ -2862,7 +2861,9 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n                         tf.shape(output)[0], activity_loss.dtype\n                     )\n                     # Make activity regularization strength batch-agnostic.\n-                    mean_activity_loss = activity_loss / batch_size\n+                    mean_activity_loss = tf.math.divide_no_nan(\n+                        activity_loss, batch_size\n+                    )\n                     self.add_loss(mean_activity_loss)\n \n     def _set_mask_metadata(self, inputs, outputs, previous_mask, build_graph):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d99e949b537dcfcde3edcb12ec4507b60bc3a6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 14 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import typing\n-\n from tensorflow import nest\n \n from keras_core.backend import KerasTensor\n@@ -43,7 +41,7 @@ class SymbolicArguments:\n             return (tensor_dict[self._single_positional_tensor],), {}\n \n         def switch_fn(x):\n-            if isinstance(x, typing.Hashable):\n+            if isinstance(x, KerasTensor):\n                 val = tensor_dict.get(x, None)\n                 if val is not None:\n                     return val\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e6988d0a238dcaddf209ab9a4b50569e59c6894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 710 | Contributors (this commit): 4 | Commits (past 90d): 35 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -258,7 +258,18 @@ class Model(Trainer, Layer):\n         )\n \n     @traceback_utils.filter_traceback\n-    def save(self, filepath, overwrite=True):\n+    def save(self, filepath, overwrite=True, save_format=\"keras\"):\n+        if save_format in [\"h5\", \"tf\"]:\n+            raise ValueError(\n+                \"`'h5'` and `'t5'` formats are no longer supported via the \"\n+                \"`save_format` option. Please use the new `'keras'` format. \"\n+                f\"Received: save_format={save_format}\"\n+            )\n+        if save_format not in [\"keras\", \"keras_v3\"]:\n+            raise ValueError(\n+                \"Unknown `save_format` value. Only the `'keras'` format is \"\n+                f\"currently supported. Received: save_format={save_format}\"\n+            )\n         if not str(filepath).endswith(\".keras\"):\n             raise ValueError(\n                 \"The filename must end in `.keras`. \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d7cbcc173fa47524ca6aac5cfe302d8c3c96a5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 315 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 330 | Churn Cumulative: 792 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ import numpy as np\n \n from keras_core import activations\n from keras_core import backend\n+from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.utils import file_utils\n \n@@ -190,7 +191,7 @@ def _preprocess_numpy_input(x, data_format, mode):\n     else:\n         if data_format == \"channels_first\":\n             # 'RGB'->'BGR'\n-            if x.ndim == 3:\n+            if len(x.shape) == 3:\n                 x = x[::-1, ...]\n             else:\n                 x = x[:, ::-1, ...]\n@@ -202,7 +203,7 @@ def _preprocess_numpy_input(x, data_format, mode):\n \n     # Zero-center by mean pixel\n     if data_format == \"channels_first\":\n-        if x.ndim == 3:\n+        if len(x.shape) == 3:\n             x[0, :, :] -= mean[0]\n             x[1, :, :] -= mean[1]\n             x[2, :, :] -= mean[2]\n@@ -249,6 +250,8 @@ def _preprocess_symbolic_input(x, data_format, mode):\n     Returns:\n         Preprocessed tensor.\n     \"\"\"\n+    ndim = len(x.shape)\n+\n     if mode == \"tf\":\n         x /= 127.5\n         x -= 1.0\n@@ -260,7 +263,7 @@ def _preprocess_symbolic_input(x, data_format, mode):\n     else:\n         if data_format == \"channels_first\":\n             # 'RGB'->'BGR'\n-            if backend.ndim(x) == 3:\n+            if ndim == 3:\n                 x = x[::-1, ...]\n             else:\n                 x = x[:, ::-1, ...]\n@@ -270,21 +273,18 @@ def _preprocess_symbolic_input(x, data_format, mode):\n         mean = [103.939, 116.779, 123.68]\n         std = None\n \n-    mean_tensor = backend.constant(-np.array(mean))\n+    mean_tensor = ops.convert_to_tensor(-np.array(mean), dtype=x.dtype)\n \n     # Zero-center by mean pixel\n-    if backend.dtype(x) != backend.dtype(mean_tensor):\n-        x = backend.bias_add(\n-            x,\n-            backend.cast(mean_tensor, backend.dtype(x)),\n-            data_format=data_format,\n-        )\n-    else:\n-        x = backend.bias_add(x, mean_tensor, data_format)\n-    if std is not None:\n-        std_tensor = backend.constant(np.array(std), dtype=backend.dtype(x))\n     if data_format == \"channels_first\":\n-            std_tensor = backend.reshape(std_tensor, (-1, 1, 1))\n+        mean_tensor = ops.reshape(mean_tensor, (1, 3) + (1,) * (ndim - 2))\n+    else:\n+        mean_tensor = ops.reshape(mean_tensor, (1,) * (ndim - 1) + (3,))\n+    x += mean_tensor\n+    if std is not None:\n+        std_tensor = ops.convert_to_tensor(np.array(std), dtype=x.dtype)\n+        if data_format == \"channels_first\":\n+            std_tensor = ops.reshape(std_tensor, (-1, 1, 1))\n         x /= std_tensor\n     return x\n \n\n@@ -0,0 +1,300 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+import keras_core as keras\n+from keras_core.applications import imagenet_utils as utils\n+from keras_core.mixed_precision import set_dtype_policy\n+from keras_core import testing\n+\n+\n+class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n+    def test_preprocess_input(self):\n+        # Test invalid mode check\n+        x = np.random.uniform(0, 255, (10, 10, 3))\n+        with self.assertRaises(ValueError):\n+            utils.preprocess_input(x, mode=\"some_unknown_mode\")\n+\n+        # Test image batch with float and int image input\n+        x = np.random.uniform(0, 255, (2, 10, 10, 3))\n+        xint = x.astype(\"int32\")\n+        self.assertEqual(utils.preprocess_input(x).shape, x.shape)\n+        self.assertEqual(utils.preprocess_input(xint).shape, xint.shape)\n+\n+        out1 = utils.preprocess_input(x, \"channels_last\")\n+        out1int = utils.preprocess_input(xint, \"channels_last\")\n+        out2 = utils.preprocess_input(\n+            np.transpose(x, (0, 3, 1, 2)), \"channels_first\"\n+        )\n+        out2int = utils.preprocess_input(\n+            np.transpose(xint, (0, 3, 1, 2)), \"channels_first\"\n+        )\n+        self.assertAllClose(out1, out2.transpose(0, 2, 3, 1))\n+        self.assertAllClose(out1int, out2int.transpose(0, 2, 3, 1))\n+\n+        # Test single image\n+        x = np.random.uniform(0, 255, (10, 10, 3))\n+        xint = x.astype(\"int32\")\n+        self.assertEqual(utils.preprocess_input(x).shape, x.shape)\n+        self.assertEqual(utils.preprocess_input(xint).shape, xint.shape)\n+\n+        out1 = utils.preprocess_input(x, \"channels_last\")\n+        out1int = utils.preprocess_input(xint, \"channels_last\")\n+        out2 = utils.preprocess_input(\n+            np.transpose(x, (2, 0, 1)), \"channels_first\"\n+        )\n+        out2int = utils.preprocess_input(\n+            np.transpose(xint, (2, 0, 1)), \"channels_first\"\n+        )\n+        self.assertAllClose(out1, out2.transpose(1, 2, 0))\n+        self.assertAllClose(out1int, out2int.transpose(1, 2, 0))\n+\n+        # Test that writing over the input data works predictably\n+        for mode in [\"torch\", \"tf\"]:\n+            x = np.random.uniform(0, 255, (2, 10, 10, 3))\n+            xint = x.astype(\"int\")\n+            x2 = utils.preprocess_input(x, mode=mode)\n+            xint2 = utils.preprocess_input(xint)\n+            self.assertAllClose(x, x2)\n+            self.assertNotEqual(xint.astype(\"float\").max(), xint2.max())\n+\n+        # Caffe mode works differently from the others\n+        x = np.random.uniform(0, 255, (2, 10, 10, 3))\n+        xint = x.astype(\"int\")\n+        x2 = utils.preprocess_input(\n+            x, data_format=\"channels_last\", mode=\"caffe\"\n+        )\n+        xint2 = utils.preprocess_input(xint)\n+        self.assertAllClose(x, x2[..., ::-1])\n+        self.assertNotEqual(xint.astype(\"float\").max(), xint2.max())\n+\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"mode_torch\", \"mode\": \"torch\"},\n+            {\"testcase_name\": \"mode_tf\", \"mode\": \"tf\"},\n+            {\"testcase_name\": \"mode_caffe\", \"mode\": \"caffe\"},\n+        ]\n+    )\n+    def test_preprocess_input_symbolic(self, mode):\n+        # Test image batch\n+        x = np.random.uniform(0, 255, (2, 10, 10, 3))\n+        inputs = keras.layers.Input(shape=x.shape[1:])\n+        outputs = keras.layers.Lambda(\n+            lambda x: utils.preprocess_input(x, mode=mode),\n+            output_shape=x.shape[1:],\n+        )(inputs)\n+        model = keras.Model(inputs, outputs)\n+        self.assertEqual(model.predict(x).shape, x.shape)\n+\n+        outputs1 = keras.layers.Lambda(\n+            lambda x: utils.preprocess_input(x, \"channels_last\", mode=mode),\n+            output_shape=x.shape[1:],\n+        )(inputs)\n+        model1 = keras.Model(inputs, outputs1)\n+        out1 = model1.predict(x)\n+        x2 = np.transpose(x, (0, 3, 1, 2))\n+        inputs2 = keras.layers.Input(shape=x2.shape[1:])\n+        outputs2 = keras.layers.Lambda(\n+            lambda x: utils.preprocess_input(x, \"channels_first\", mode=mode),\n+            output_shape=x2.shape[1:],\n+        )(inputs2)\n+        model2 = keras.Model(inputs2, outputs2)\n+        out2 = model2.predict(x2)\n+        self.assertAllClose(out1, out2.transpose(0, 2, 3, 1))\n+\n+        # Test single image\n+        x = np.random.uniform(0, 255, (10, 10, 3))\n+        inputs = keras.layers.Input(shape=x.shape)\n+        outputs = keras.layers.Lambda(\n+            lambda x: utils.preprocess_input(x, mode=mode), output_shape=x.shape\n+        )(inputs)\n+        model = keras.Model(inputs, outputs)\n+        self.assertEqual(model.predict(x[np.newaxis])[0].shape, x.shape)\n+\n+        outputs1 = keras.layers.Lambda(\n+            lambda x: utils.preprocess_input(x, \"channels_last\", mode=mode),\n+            output_shape=x.shape,\n+        )(inputs)\n+        model1 = keras.Model(inputs, outputs1)\n+        out1 = model1.predict(x[np.newaxis])[0]\n+        x2 = np.transpose(x, (2, 0, 1))\n+        inputs2 = keras.layers.Input(shape=x2.shape)\n+        outputs2 = keras.layers.Lambda(\n+            lambda x: utils.preprocess_input(x, \"channels_first\", mode=mode),\n+            output_shape=x2.shape,\n+        )(inputs2)\n+        model2 = keras.Model(inputs2, outputs2)\n+        out2 = model2.predict(x2[np.newaxis])[0]\n+        self.assertAllClose(out1, out2.transpose(1, 2, 0))\n+\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"mode_torch\", \"mode\": \"torch\"},\n+            {\"testcase_name\": \"mode_tf\", \"mode\": \"tf\"},\n+            {\"testcase_name\": \"mode_caffe\", \"mode\": \"caffe\"},\n+        ]\n+    )\n+    def test_preprocess_input_symbolic_mixed_precision(self, mode):\n+        set_dtype_policy(\"mixed_float16\")\n+        shape = (20, 20, 3)\n+        inputs = keras.layers.Input(shape=shape)\n+        try:\n+            keras.layers.Lambda(\n+                lambda x: utils.preprocess_input(x, mode=mode),\n+                output_shape=shape,\n+            )(inputs)\n+        finally:\n+            set_dtype_policy(\"float32\")\n+\n+    @parameterized.named_parameters(\n+        [\n+            {\n+                \"testcase_name\": \"channels_last_format\",\n+                \"data_format\": \"channels_last\",\n+            },\n+            {\n+                \"testcase_name\": \"channels_first_format\",\n+                \"data_format\": \"channels_first\",\n+            },\n+        ]\n+    )\n+    def test_obtain_input_shape(self, data_format):\n+        # input_shape and default_size are not identical.\n+        with self.assertRaises(ValueError):\n+            utils.obtain_input_shape(\n+                input_shape=(224, 224, 3),\n+                default_size=299,\n+                min_size=139,\n+                data_format=\"channels_last\",\n+                require_flatten=True,\n+                weights=\"imagenet\",\n+            )\n+\n+        # Test invalid use cases\n+\n+        shape = (139, 139)\n+        if data_format == \"channels_last\":\n+            input_shape = shape + (99,)\n+        else:\n+            input_shape = (99,) + shape\n+\n+        # input_shape is smaller than min_size.\n+        shape = (100, 100)\n+        if data_format == \"channels_last\":\n+            input_shape = shape + (3,)\n+        else:\n+            input_shape = (3,) + shape\n+        with self.assertRaises(ValueError):\n+            utils.obtain_input_shape(\n+                input_shape=input_shape,\n+                default_size=None,\n+                min_size=139,\n+                data_format=data_format,\n+                require_flatten=False,\n+            )\n+\n+        # shape is 1D.\n+        shape = (100,)\n+        if data_format == \"channels_last\":\n+            input_shape = shape + (3,)\n+        else:\n+            input_shape = (3,) + shape\n+        with self.assertRaises(ValueError):\n+            utils.obtain_input_shape(\n+                input_shape=input_shape,\n+                default_size=None,\n+                min_size=139,\n+                data_format=data_format,\n+                require_flatten=False,\n+            )\n+\n+        # the number of channels is 5 not 3.\n+        shape = (100, 100)\n+        if data_format == \"channels_last\":\n+            input_shape = shape + (5,)\n+        else:\n+            input_shape = (5,) + shape\n+        with self.assertRaises(ValueError):\n+            utils.obtain_input_shape(\n+                input_shape=input_shape,\n+                default_size=None,\n+                min_size=139,\n+                data_format=data_format,\n+                require_flatten=False,\n+            )\n+\n+        # require_flatten=True with dynamic input shape.\n+        with self.assertRaises(ValueError):\n+            utils.obtain_input_shape(\n+                input_shape=None,\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_first\",\n+                require_flatten=True,\n+            )\n+\n+        # test include top\n+        self.assertEqual(\n+            utils.obtain_input_shape(\n+                input_shape=(3, 200, 200),\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_first\",\n+                require_flatten=True,\n+            ),\n+            (3, 200, 200),\n+        )\n+\n+        self.assertEqual(\n+            utils.obtain_input_shape(\n+                input_shape=None,\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_last\",\n+                require_flatten=False,\n+            ),\n+            (None, None, 3),\n+        )\n+\n+        self.assertEqual(\n+            utils.obtain_input_shape(\n+                input_shape=None,\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_first\",\n+                require_flatten=False,\n+            ),\n+            (3, None, None),\n+        )\n+\n+        self.assertEqual(\n+            utils.obtain_input_shape(\n+                input_shape=None,\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_last\",\n+                require_flatten=False,\n+            ),\n+            (None, None, 3),\n+        )\n+\n+        self.assertEqual(\n+            utils.obtain_input_shape(\n+                input_shape=(150, 150, 3),\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_last\",\n+                require_flatten=False,\n+            ),\n+            (150, 150, 3),\n+        )\n+\n+        self.assertEqual(\n+            utils.obtain_input_shape(\n+                input_shape=(3, None, None),\n+                default_size=None,\n+                min_size=139,\n+                data_format=\"channels_first\",\n+                require_flatten=False,\n+            ),\n+            (3, None, None),\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#012109fa878c17e31a44a724e4f9a23aa2e5fa95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 508 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -53,8 +53,7 @@ class Attention(BaseDenseAttention):\n             query and key vectors. `\"concat\"` refers to the hyperbolic tangent\n             of the concatenation of the query and key vectors.\n \n-    Call Args:\n-\n+    Call arguments:\n         inputs: List of the following tensors:\n             * query: Query `Tensor` of shape `[batch_size, Tq, dim]`.\n             * value: Value `Tensor` of shape `[batch_size, Tv, dim]`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#311b0ac6967bb41ca925fe38c4ad50e9f298239d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 433 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -51,8 +51,7 @@ class AdditiveAttention(BaseDenseAttention):\n         dropout: Float between 0 and 1. Fraction of the units to drop for the\n             attention scores. Defaults to `0.0`.\n \n-    Call Args:\n-\n+    Call arguments:\n         inputs: List of the following tensors:\n             * query: Query `Tensor` of shape `[batch_size, Tq, dim]`.\n             * value: Value `Tensor` of shape `[batch_size, Tv, dim]`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b518168db71ad4f369aec491aa35c697a197dc7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 679 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -42,7 +42,7 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         dropout: Float between 0 and 1. Fraction of the units to drop for the\n             attention scores.\n \n-    Call Args:\n+    Call arguments:\n         inputs: List of the following tensors:\n             * query: Query `Tensor` of shape `[batch_size, Tq, dim]`.\n             * value: Value `Tensor` of shape `[batch_size, Tv, dim]`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08523bf1c533501330f3173ad6eb64cccf23a65c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 494 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n+import keras_core  # isort: skip, keep it on top for torch test\n+\n import numpy as np\n from tensorflow import keras\n \n-import keras_core\n-\n NUM_CLASSES = 10\n \n \n\n@@ -27,7 +27,11 @@ class TorchTrainer(base_trainer.Trainer):\n             y_pred = self(x, training=True)\n         else:\n             y_pred = self(x)\n+\n+        # Call torch.nn.Module.zero_grad() to clear the leftover gradients for\n+        # the weights from the previous train step.\n         self.zero_grad()\n+\n         loss = self.compute_loss(\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n@@ -37,7 +41,11 @@ class TorchTrainer(base_trainer.Trainer):\n         if self.trainable_weights:\n             # Backpropagation\n             trainable_weights = [v for v in self.trainable_weights]\n+\n+            # Call torch.Tensor.backward() on the loss to compute gradients for\n+            # the weights.\n             loss.backward()\n+\n             gradients = [v.value.grad for v in trainable_weights]\n \n             # Update weights\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#514babab17a2ba39b022dd280cab3c3730ffd75b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 625 | Lines Deleted: 274 | Files Changed: 5 | Hunks: 40 | Methods Changed: 58 | Complexity Δ (Sum/Max): 25/22 | Churn Δ: 899 | Churn Cumulative: 5760 | Contributors (this commit): 12 | Commits (past 90d): 145 | Contributors (cumulative): 26 | DMM Complexity: 0.9491525423728814\n\nDIFF:\n@@ -2,9 +2,9 @@ import numpy as np\n from absl.testing import parameterized\n \n import keras_core as keras\n+from keras_core import testing\n from keras_core.applications import imagenet_utils as utils\n from keras_core.mixed_precision import set_dtype_policy\n-from keras_core import testing\n \n \n class TestImageNetUtils(testing.TestCase, parameterized.TestCase):\n\n@@ -37,74 +37,18 @@ class JAXTrainer(base_trainer.Trainer):\n         loss = self.compute_loss(x, y, y_pred, sample_weight)\n         return loss, (y_pred, non_trainable_variables)\n \n-    def fit(\n-        self,\n-        x=None,\n-        y=None,\n-        batch_size=None,\n-        epochs=1,\n-        verbose=\"auto\",\n-        callbacks=None,\n-        validation_split=0.0,\n-        validation_data=None,\n-        shuffle=True,\n-        class_weight=None,\n-        sample_weight=None,\n-        initial_epoch=0,\n-        steps_per_epoch=None,\n-        validation_steps=None,\n-        validation_batch_size=None,\n-        validation_freq=1,\n-    ):\n-        if not self.compiled:\n-            raise ValueError(\n-                \"You must call `compile()` before calling `fit()`.\"\n-            )\n-\n-        # TODO: respect compiled trainable state\n-        if validation_split and validation_data is None:\n-            # Create the validation data using the training data. Only supported\n-            # for TF/numpy/jax arrays.\n-            (\n-                x,\n-                y,\n-                sample_weight,\n-            ), validation_data = data_adapter_utils.train_validation_split(\n-                (x, y, sample_weight), validation_split=validation_split\n-            )\n-\n-        if validation_data:\n-            (\n-                val_x,\n-                val_y,\n-                val_sample_weight,\n-            ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n-\n-        # Create an iterator that yields batches for one epoch.\n-        epoch_iterator = EpochIterator(\n-            x=x,\n-            y=y,\n-            sample_weight=sample_weight,\n-            batch_size=batch_size,\n-            steps_per_epoch=steps_per_epoch,\n-            shuffle=shuffle,\n-            class_weight=class_weight,\n-            steps_per_execution=self.steps_per_execution,\n-        )\n-\n+    def _eager_build(self, data_batch):\n         compile_metrics_unbuilt = (\n             self._compile_metrics is not None\n             and not self._compile_metrics.built\n         )\n         if not self.built or compile_metrics_unbuilt:\n             # Build the model on one batch of data.\n-            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-                data = data[0]\n             (\n                 x,\n                 y,\n                 sample_weight,\n-                ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n+            ) = data_adapter_utils.unpack_x_y_sample_weight(data_batch)\n             # Build model\n             with backend.StatelessScope():\n                 y_pred = self(x)\n@@ -113,28 +57,19 @@ class JAXTrainer(base_trainer.Trainer):\n                     self.compute_metrics(\n                         x, y, y_pred, sample_weight=sample_weight\n                     )\n-                break\n-        if not self.optimizer.built:\n+        if self.optimizer is not None and not self.optimizer.built:\n             # Build optimizer\n             self.optimizer.build(self.trainable_variables)\n \n-        # Container that configures and calls callbacks.\n-        if not isinstance(callbacks, callbacks_module.CallbackList):\n-            callbacks = callbacks_module.CallbackList(\n-                callbacks,\n-                add_history=True,\n-                add_progbar=verbose != 0,\n-                verbose=verbose,\n-                epochs=epochs,\n-                steps=epoch_iterator.num_batches,\n-                model=self,\n-            )\n+    def make_train_function(self, force=False):\n+        if self.train_function is not None and not force:\n+            return self.train_function\n \n         grad_fn = jax.value_and_grad(\n             self.compute_loss_and_updates, has_aux=True\n         )\n \n-        def _train_step(state, data):\n+        def one_train_step(state, data):\n             data = data[0]\n             (\n                 trainable_variables,\n@@ -169,8 +104,8 @@ class JAXTrainer(base_trainer.Trainer):\n                     )\n                 ]\n             ) as scope:\n-                logs = self.compute_metrics(x, y, y_pred, sample_weight)\n                 self._loss_tracker.update_state(loss)\n+                logs = self.compute_metrics(x, y, y_pred, sample_weight)\n \n             new_metrics_variables = []\n             for ref_v in self.metrics_variables:\n@@ -188,27 +123,233 @@ class JAXTrainer(base_trainer.Trainer):\n             )\n             return logs, state\n \n-        def _train_multi_step(state, data):\n+        def multi_train_steps(state, data):\n             for single_step_data in data:\n-                logs, state = _train_step(state, [single_step_data])\n+                logs, state = one_train_step(state, [single_step_data])\n             return logs, state\n \n         if self.steps_per_execution > 1:\n-            _train_function = _train_multi_step\n+            train_step = multi_train_steps\n         else:\n-            _train_function = _train_step\n-\n-        self.train_function = _train_function\n+            train_step = one_train_step\n \n         if not self.run_eagerly and self.jit_compile:\n \n             @jax.jit\n-            def train_step(state, data):\n-                return self.train_function(state, data)\n+            def compiled_train_step(state, data):\n+                return train_step(state, data)\n+\n+            self.train_function = compiled_train_step\n \n         else:\n-            train_step = self.train_function\n+            self.train_function = train_step\n \n+    def make_test_function(self, force=False):\n+        if self.test_function is not None and not force:\n+            return self.test_function\n+\n+        def one_test_step(state, data):\n+            data = data[0]\n+            (\n+                trainable_variables,\n+                non_trainable_variables,\n+                metrics_variables,\n+            ) = state\n+            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n+                data\n+            )\n+            loss, (\n+                y_pred,\n+                non_trainable_variables,\n+            ) = self.compute_loss_and_updates(\n+                trainable_variables,\n+                non_trainable_variables,\n+                x,\n+                y,\n+                sample_weight,\n+                training=False,\n+            )\n+\n+            with backend.StatelessScope(\n+                state_mapping=[\n+                    (ref_v, v)\n+                    for ref_v, v in zip(\n+                        self.metrics_variables, metrics_variables\n+                    )\n+                ]\n+            ) as scope:\n+                self._loss_tracker.update_state(loss)\n+                logs = self.compute_metrics(x, y, y_pred, sample_weight)\n+\n+            new_metrics_variables = []\n+            for ref_v in self.metrics_variables:\n+                new_v = scope.get_current_value(ref_v)\n+                if new_v is None:\n+                    new_v = ref_v.value\n+                new_metrics_variables.append(new_v)\n+            metrics_variables = new_metrics_variables\n+\n+            state = (\n+                trainable_variables,\n+                non_trainable_variables,\n+                metrics_variables,\n+            )\n+            return logs, state\n+\n+        def multi_test_steps(state, data):\n+            for single_step_data in data:\n+                logs, state = one_test_step(state, [single_step_data])\n+            return logs, state\n+\n+        if self.steps_per_execution > 1:\n+            test_step = multi_test_steps\n+        else:\n+            test_step = one_test_step\n+\n+        if not self.run_eagerly and self.jit_compile:\n+\n+            @jax.jit\n+            def compiled_test_step(state, data):\n+                return test_step(state, data)\n+\n+            self.test_function = compiled_test_step\n+\n+        else:\n+            self.test_function = test_step\n+\n+    def make_predict_function(self, force=False):\n+        if self.predict_function is not None and not force:\n+            return self.predict_function\n+\n+        def one_predict_step(\n+            trainable_variables, non_trainable_variables, data\n+        ):\n+            kwargs = {}\n+            if self._call_has_training_arg():\n+                kwargs[\"training\"] = False\n+            outputs, _ = self.stateless_call(\n+                trainable_variables, non_trainable_variables, data[0], **kwargs\n+            )\n+            return outputs\n+\n+        def multi_predict_steps(\n+            trainable_variables, non_trainable_variables, data\n+        ):\n+            outputs = one_predict_step(\n+                trainable_variables, non_trainable_variables, data[:1]\n+            )\n+            for single_step_data in data[1:]:\n+                step_outputs = one_predict_step(\n+                    trainable_variables,\n+                    non_trainable_variables,\n+                    [single_step_data],\n+                )\n+                outputs = tf.nest.map_structure(\n+                    lambda t1, t2: jax.numpy.concatenate([t1, t2]),\n+                    outputs,\n+                    step_outputs,\n+                )\n+            return outputs\n+\n+        if self.steps_per_execution > 1:\n+            predict_step = multi_predict_steps\n+        else:\n+            predict_step = one_predict_step\n+\n+        if not self.run_eagerly and self.jit_compile:\n+\n+            @jax.jit\n+            def compiled_predict_step(\n+                trainable_variables, non_trainable_variables, data\n+            ):\n+                return predict_step(\n+                    trainable_variables, non_trainable_variables, data\n+                )\n+\n+            self.predict_function = compiled_predict_step\n+\n+        else:\n+            self.predict_function = predict_step\n+\n+    def fit(\n+        self,\n+        x=None,\n+        y=None,\n+        batch_size=None,\n+        epochs=1,\n+        verbose=\"auto\",\n+        callbacks=None,\n+        validation_split=0.0,\n+        validation_data=None,\n+        shuffle=True,\n+        class_weight=None,\n+        sample_weight=None,\n+        initial_epoch=0,\n+        steps_per_epoch=None,\n+        validation_steps=None,\n+        validation_batch_size=None,\n+        validation_freq=1,\n+    ):\n+        self._assert_compile_called(\"fit\")\n+        # TODO: respect compiled trainable state\n+        if validation_split and validation_data is None:\n+            # Create the validation data using the training data. Only supported\n+            # for TF/numpy/jax arrays.\n+            (\n+                x,\n+                y,\n+                sample_weight,\n+            ), validation_data = data_adapter_utils.train_validation_split(\n+                (x, y, sample_weight), validation_split=validation_split\n+            )\n+\n+        if validation_data:\n+            (\n+                val_x,\n+                val_y,\n+                val_sample_weight,\n+            ) = data_adapter_utils.unpack_x_y_sample_weight(validation_data)\n+\n+        # Create an iterator that yields batches for one epoch.\n+        epoch_iterator = EpochIterator(\n+            x=x,\n+            y=y,\n+            sample_weight=sample_weight,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps_per_epoch,\n+            shuffle=shuffle,\n+            class_weight=class_weight,\n+            steps_per_execution=self.steps_per_execution,\n+        )\n+\n+        needs_building = (\n+            not self.built\n+            or not self.optimizer.built\n+            or (\n+                self._compile_metrics is not None\n+                and not self._compile_metrics.built\n+            )\n+        )\n+        if needs_building:\n+            # Build the model on one batch of data.\n+            for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n+                data_batch = data[0]\n+                self._eager_build(data_batch)\n+                break\n+\n+        # Container that configures and calls callbacks.\n+        if not isinstance(callbacks, callbacks_module.CallbackList):\n+            callbacks = callbacks_module.CallbackList(\n+                callbacks,\n+                add_history=True,\n+                add_progbar=verbose != 0,\n+                verbose=verbose,\n+                epochs=epochs,\n+                steps=epoch_iterator.num_batches,\n+                model=self,\n+            )\n+\n+        self.make_train_function()\n         self.stop_training = False\n         callbacks.on_train_begin()\n \n@@ -232,7 +373,7 @@ class JAXTrainer(base_trainer.Trainer):\n                     optimizer_variables,\n                     metrics_variables,\n                 )\n-                logs, state = train_step(state, data)\n+                logs, state = self.train_function(state, data)\n                 (\n                     trainable_variables,\n                     non_trainable_variables,\n@@ -318,6 +459,7 @@ class JAXTrainer(base_trainer.Trainer):\n         return_dict=False,\n         **kwargs,\n     ):\n+        self._assert_compile_called(\"evaluate\")\n         # TODO: respect compiled trainable state\n         use_cached_eval_dataset = kwargs.pop(\"_use_cached_eval_dataset\", False)\n         if kwargs:\n@@ -340,18 +482,8 @@ class JAXTrainer(base_trainer.Trainer):\n         if not self.built:\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-                data = data[0]\n-                (\n-                    x,\n-                    y,\n-                    sample_weight,\n-                ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n-                # Build model\n-                with backend.StatelessScope():\n-                    y_pred = self(x)\n-                    # Build metrics\n-                    self.compute_metrics(x, y, y_pred, sample_weight)\n-                self.reset_metrics()\n+                data_batch = data[0]\n+                self._eager_build(data_batch)\n                 break\n \n         # Container that configures and calls callbacks.\n@@ -366,73 +498,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        def _test_step(state, data):\n-            data = data[0]\n-            (\n-                trainable_variables,\n-                non_trainable_variables,\n-                metrics_variables,\n-            ) = state\n-            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n-                data\n-            )\n-            loss, (\n-                y_pred,\n-                non_trainable_variables,\n-            ) = self.compute_loss_and_updates(\n-                trainable_variables,\n-                non_trainable_variables,\n-                x,\n-                y,\n-                sample_weight,\n-                training=False,\n-            )\n-\n-            with backend.StatelessScope(\n-                state_mapping=[\n-                    (ref_v, v)\n-                    for ref_v, v in zip(\n-                        self.metrics_variables, metrics_variables\n-                    )\n-                ]\n-            ) as scope:\n-                logs = self.compute_metrics(x, y, y_pred, sample_weight)\n-                self._loss_tracker.update_state(loss)\n-\n-            new_metrics_variables = []\n-            for ref_v in self.metrics_variables:\n-                new_v = scope.get_current_value(ref_v)\n-                if new_v is None:\n-                    new_v = ref_v.value\n-                new_metrics_variables.append(new_v)\n-            metrics_variables = new_metrics_variables\n-\n-            state = (\n-                trainable_variables,\n-                non_trainable_variables,\n-                metrics_variables,\n-            )\n-            return logs, state\n-\n-        def _test_multi_step(state, data):\n-            for single_step_data in data:\n-                logs, state = _test_step(state, [single_step_data])\n-            return logs, state\n-\n-        if self.steps_per_execution > 1:\n-            _test_function = _test_multi_step\n-        else:\n-            _test_function = _test_step\n-\n-        if not self.run_eagerly and self.jit_compile:\n-\n-            @jax.jit\n-            def test_step(state, data):\n-                return _test_function(state, data)\n-\n-        else:\n-            test_step = _test_function\n-\n+        self.make_test_function()\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n@@ -449,7 +515,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 non_trainable_variables,\n                 metrics_variables,\n             )\n-            logs, state = test_step(state, data)\n+            logs, state = self.test_function(state, data)\n             # Note that trainable variables are not returned since they're\n             # immutable here.\n             _, non_trainable_variables, metrics_variables = state\n@@ -506,52 +572,7 @@ class JAXTrainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        def _predict_step(trainable_variables, non_trainable_variables, data):\n-            kwargs = {}\n-            if self._call_has_training_arg():\n-                kwargs[\"training\"] = False\n-            outputs, _ = self.stateless_call(\n-                trainable_variables, non_trainable_variables, data[0], **kwargs\n-            )\n-            return outputs\n-\n-        def _predict_multi_step(\n-            trainable_variables, non_trainable_variables, data\n-        ):\n-            outputs = _predict_step(\n-                trainable_variables, non_trainable_variables, data[:1]\n-            )\n-            for single_step_data in data[1:]:\n-                step_outputs = _predict_step(\n-                    trainable_variables,\n-                    non_trainable_variables,\n-                    [single_step_data],\n-                )\n-                outputs = tf.nest.map_structure(\n-                    lambda t1, t2: jax.numpy.concatenate([t1, t2]),\n-                    outputs,\n-                    step_outputs,\n-                )\n-            return outputs\n-\n-        if self.steps_per_execution > 1:\n-            _predict_function = _predict_multi_step\n-        else:\n-            _predict_function = _predict_step\n-\n-        if not self.run_eagerly and self.jit_compile:\n-\n-            @jax.jit\n-            def predict_step(\n-                trainable_variables, non_trainable_variables, data\n-            ):\n-                return _predict_function(\n-                    trainable_variables, non_trainable_variables, data\n-                )\n-\n-        else:\n-            predict_step = _predict_function\n-\n+        self.make_predict_function()\n         callbacks.on_predict_begin()\n \n         def append_to_outputs(batch_outputs, outputs):\n@@ -574,7 +595,7 @@ class JAXTrainer(base_trainer.Trainer):\n         outputs = None\n         for step, x in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n-            batch_outputs = predict_step(\n+            batch_outputs = self.predict_function(\n                 trainable_variables, non_trainable_variables, x\n             )\n             outputs = append_to_outputs(batch_outputs, outputs)\n@@ -584,6 +605,178 @@ class JAXTrainer(base_trainer.Trainer):\n             batch_outputs, np.concatenate, outputs\n         )\n \n+    def train_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        class_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Runs a single gradient update on a single batch of data.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) to apply to the model's loss for the samples\n+                from this class during training. This can be useful to tell the\n+                model to \"pay more attention\" to samples from an\n+                under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must\n+                be one-hot encoded, or an explicit final dimension of 1\n+                must be included for sparse class labels.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"train_on_batch\")\n+        if class_weight is not None:\n+            if sample_weight is not None:\n+                raise ValueError(\n+                    \"Arguments `sample_weight` and `class_weight` \"\n+                    \"cannot be specified at the same time. \"\n+                    f\"Received: sample_weight={sample_weight}, \"\n+                    f\"class_weight={class_weight}\"\n+                )\n+            sample_weight = data_adapter_utils.class_weight_to_sample_weights(\n+                y, class_weight\n+            )\n+        data = (x, y, sample_weight)\n+\n+        # Maybe build model\n+        self._eager_build(data)\n+        self.make_train_function()\n+\n+        # Train step\n+        trainable_variables = self.trainable_variables\n+        non_trainable_variables = self.non_trainable_variables\n+        optimizer_variables = self.optimizer.variables\n+        metrics_variables = self.metrics_variables\n+        state = (\n+            trainable_variables,\n+            non_trainable_variables,\n+            optimizer_variables,\n+            metrics_variables,\n+        )\n+        logs, state = self.train_function(state, [data])\n+\n+        # State sync\n+        (\n+            trainable_variables,\n+            non_trainable_variables,\n+            optimizer_variables,\n+            metrics_variables,\n+        ) = state\n+        self._jax_state = {\n+            \"trainable_variables\": trainable_variables,\n+            \"non_trainable_variables\": non_trainable_variables,\n+            \"optimizer_variables\": optimizer_variables,\n+            \"metrics_variables\": metrics_variables,\n+        }\n+        self.jax_state_sync()\n+\n+        # Format return values\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def test_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Test the model on a single batch of samples.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"test_on_batch\")\n+\n+        data = (x, y, sample_weight)\n+        # Maybe build model\n+        self._eager_build(data)\n+        self.make_test_function()\n+\n+        # Test step\n+        trainable_variables = self.trainable_variables\n+        non_trainable_variables = self.non_trainable_variables\n+        metrics_variables = self.metrics_variables\n+        state = (\n+            trainable_variables,\n+            non_trainable_variables,\n+            metrics_variables,\n+        )\n+        logs, state = self.test_function(state, [data])\n+\n+        # State sync\n+        _, non_trainable_variables, metrics_variables = state\n+        self._jax_state = {\n+            \"non_trainable_variables\": non_trainable_variables,\n+            \"metrics_variables\": metrics_variables,\n+        }\n+        self.jax_state_sync()\n+\n+        # Format return values.\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        Args:\n+            x: Input data. It must be array-like.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n+        if not self.built:\n+            # Build model\n+            with backend.StatelessScope():\n+                self(x)\n+\n+        self.make_predict_function()\n+        trainable_variables = self.trainable_variables\n+        non_trainable_variables = self.non_trainable_variables\n+        batch_outputs = self.predict_function(\n+            trainable_variables, non_trainable_variables, [x]\n+        )\n+        batch_outputs = tf.nest.map_structure(\n+            lambda x: np.array(x), batch_outputs\n+        )\n+        return batch_outputs\n+\n     def jax_state_sync(self):\n         if not getattr(self, \"_jax_state\", None):\n             return\n\n@@ -97,9 +97,11 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             \"\"\"Runs a single training step on a batch of data.\"\"\"\n             return self.train_step(data)\n \n-        if not self.run_eagerly and self.jit_compile:\n+        if not self.run_eagerly:\n             one_step_on_data = tf.function(\n-                one_step_on_data, jit_compile=True, reduce_retracing=True\n+                one_step_on_data,\n+                jit_compile=self.jit_compile,\n+                reduce_retracing=True,\n             )\n \n         @tf.autograph.experimental.do_not_convert\n@@ -245,10 +247,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         validation_batch_size=None,\n         validation_freq=1,\n     ):\n-        if not self.compiled:\n-            raise ValueError(\n-                \"You must call `compile()` before calling `fit()`.\"\n-            )\n+        self._assert_compile_called(\"fit\")\n         # TODO: respect compiled trainable state\n         if validation_split and validation_data is None:\n             # Create the validation data using the training data. Only supported\n@@ -368,6 +367,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n         **kwargs,\n     ):\n+        self._assert_compile_called(\"evaluate\")\n         # TODO: respect compiled trainable state\n         use_cached_eval_dataset = kwargs.pop(\"_use_cached_eval_dataset\", False)\n         if kwargs:\n@@ -462,7 +462,7 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 try:\n                     single_step_data = next(iterator)\n                 except (StopIteration, tf.errors.OutOfRangeError) as e:\n-                    if len(data) > 0:\n+                    if hasattr(data, \"__len__\") and len(data) > 0:\n                         # Suppress the error when still have remaining data.\n                         return data\n                     else:\n@@ -488,6 +488,120 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             batch_outputs, np.concatenate, outputs\n         )\n \n+    def train_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        class_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Runs a single gradient update on a single batch of data.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) to apply to the model's loss for the samples\n+                from this class during training. This can be useful to tell the\n+                model to \"pay more attention\" to samples from an\n+                under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must\n+                be one-hot encoded, or an explicit final dimension of 1\n+                must be included for sparse class labels.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"train_on_batch\")\n+        self.make_train_function()\n+        if class_weight is not None:\n+            if sample_weight is not None:\n+                raise ValueError(\n+                    \"Arguments `sample_weight` and `class_weight` \"\n+                    \"cannot be specified at the same time. \"\n+                    f\"Received: sample_weight={sample_weight}, \"\n+                    f\"class_weight={class_weight}\"\n+                )\n+            sample_weight = data_adapter_utils.class_weight_to_sample_weights(\n+                y, class_weight\n+            )\n+\n+        def data():\n+            yield (x, y, sample_weight)\n+\n+        logs = self.train_function(data())\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def test_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Test the model on a single batch of samples.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"test_on_batch\")\n+        self.make_test_function()\n+\n+        def data():\n+            yield (x, y, sample_weight)\n+\n+        logs = self.test_function(data())\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        Args:\n+            x: Input data. It must be array-like.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n+        self.make_predict_function()\n+        batch_outputs = self.predict_function((x,))\n+        batch_outputs = tf.nest.map_structure(\n+            lambda x: np.array(x), batch_outputs\n+        )\n+        return batch_outputs\n+\n     # Backwards compatibility shims.\n     @property\n     def compiled_metrics(self):\n\n@@ -348,3 +348,12 @@ class Trainer:\n         if len(results) == 1:\n             return results[0]\n         return results\n+\n+    def _assert_compile_called(self, method_name=None):\n+        if not self.compiled:\n+            msg = \"You must call `compile()` before \"\n+            if metrics_module:\n+                msg += \"using the model.\"\n+            else:\n+                msg += f\"calling `{method_name}()`.\"\n+            raise ValueError(msg)\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import initializers\n@@ -62,7 +63,7 @@ class TrainingTestingLayer(layers.Layer, Trainer):\n         return x * 0\n \n \n-class TestTrainer(testing.TestCase):\n+class TestTrainer(testing.TestCase, parameterized.TestCase):\n     def test_metric_tracking(self):\n         class ModelWithMetric(layers.Dense, Trainer):\n             def __init__(self, units):\n@@ -101,7 +102,24 @@ class TestTrainer(testing.TestCase):\n         # And those weights are tracked at the model level\n         self.assertEqual(len(model.metrics_variables), 6)\n \n-    def _test_fit_flow(self, run_eagerly, jit_compile, use_steps_per_epoch):\n+    @parameterized.named_parameters(\n+        [\n+            (\"eager\", True, False, False),\n+            (\"graph_fn\", False, False, False),\n+            (\"jit\", False, True, False),\n+            (\"steps_per_epoch_eager\", True, False, True),\n+            (\"steps_per_epoch_graph_fn\", False, False, True),\n+            (\"steps_per_epoch_jit\", False, True, True),\n+        ]\n+    )\n+    def test_fit_flow(self, run_eagerly, jit_compile, use_steps_per_epoch):\n+        if not run_eagerly and not jit_compile and use_steps_per_epoch:\n+            if backend.backend() == \"tensorflow\":\n+                self.skipTest(\n+                    \"TODO: Graph mode without XLA in TF backend leads to \"\n+                    \"unexpected logs, need further checks.\"\n+                )\n+\n         model = ExampleModel(units=3)\n         epochs = 3\n         batch_size = 20\n@@ -133,42 +151,14 @@ class TestTrainer(testing.TestCase):\n             atol=6.1051628e-1,\n         )\n \n-    def test_fit_flow_eager(self):\n-        self._test_fit_flow(\n-            run_eagerly=True, jit_compile=False, use_steps_per_epoch=False\n+    @parameterized.named_parameters(\n+        [\n+            (\"eager\", True, False),\n+            (\"graph_fn\", False, False),\n+            (\"jit\", False, True),\n+        ]\n     )\n-\n-    def test_fit_flow_graph_fn(self):\n-        self._test_fit_flow(\n-            run_eagerly=False, jit_compile=False, use_steps_per_epoch=False\n-        )\n-\n-    def test_fit_flow_jit(self):\n-        self._test_fit_flow(\n-            run_eagerly=False, jit_compile=True, use_steps_per_epoch=False\n-        )\n-\n-    def test_fit_steps_per_epoch_flow_eager(self):\n-        self._test_fit_flow(\n-            run_eagerly=True, jit_compile=False, use_steps_per_epoch=True\n-        )\n-\n-    def test_fit_steps_per_epoch_flow_graph_fn(self):\n-        if backend.backend() == \"tensorflow\":\n-            self.skipTest(\n-                \"TODO: Graph mode without XLA in TF backend leads to \"\n-                \"unexpected logs, need further checks.\"\n-            )\n-        self._test_fit_flow(\n-            run_eagerly=False, jit_compile=False, use_steps_per_epoch=True\n-        )\n-\n-    def test_fit_steps_per_epoch_flow_jit(self):\n-        self._test_fit_flow(\n-            run_eagerly=False, jit_compile=True, use_steps_per_epoch=True\n-        )\n-\n-    def _test_evaluate_flow(self, run_eagerly, jit_compile):\n+    def test_evaluate_flow(self, run_eagerly, jit_compile):\n         model = ExampleModel(units=3)\n         x = np.ones((100, 4))\n         y = np.zeros((100, 3))\n@@ -189,16 +179,14 @@ class TestTrainer(testing.TestCase):\n         self.assertIn(\"mean_squared_error\", output)\n         self.assertAllClose(output[\"mean_squared_error\"], 16.0)\n \n-    def test_evaluate_flow_eager(self):\n-        self._test_evaluate_flow(run_eagerly=True, jit_compile=False)\n-\n-    def test_evaluate_flow_graph_fn(self):\n-        self._test_evaluate_flow(run_eagerly=False, jit_compile=False)\n-\n-    def test_evaluate_flow_jit(self):\n-        self._test_evaluate_flow(run_eagerly=False, jit_compile=True)\n-\n-    def _test_predict_flow(self, run_eagerly, jit_compile):\n+    @parameterized.named_parameters(\n+        [\n+            (\"eager\", True, False),\n+            (\"graph_fn\", False, False),\n+            (\"jit\", False, True),\n+        ]\n+    )\n+    def test_predict_flow(self, run_eagerly, jit_compile):\n         # Test basic example\n         model = ExampleModel(units=3)\n         model.run_eagerly = run_eagerly\n@@ -225,15 +213,6 @@ class TestTrainer(testing.TestCase):\n         self.assertAllClose(outputs[\"y_one\"], 4 * np.ones((100, 3)))\n         self.assertAllClose(outputs[\"y_two\"], 4 * np.ones((100, 3)))\n \n-    def test_predict_flow_eager(self):\n-        self._test_predict_flow(run_eagerly=True, jit_compile=False)\n-\n-    def test_predict_flow_graph_fn(self):\n-        self._test_predict_flow(run_eagerly=False, jit_compile=False)\n-\n-    def test_predict_flow_jit(self):\n-        self._test_predict_flow(run_eagerly=False, jit_compile=True)\n-\n     def test_steps_per_execution_steps_count(self):\n         class StepCount(Callback):\n             def __init__(self):\n@@ -280,3 +259,59 @@ class TestTrainer(testing.TestCase):\n         self.assertAllClose(val_loss, 0.0)\n         preds = model.predict(x)\n         self.assertAllClose(preds, np.zeros((128, 1)))\n+\n+    @parameterized.named_parameters(\n+        [\n+            (\"eager\", True, False),\n+            (\"graph_fn\", False, False),\n+            (\"jit\", False, True),\n+        ]\n+    )\n+    def test_on_batch_methods(self, run_eagerly, jit_compile):\n+        model = ExampleModel(units=3)\n+        x = np.ones((100, 4))\n+        y = np.zeros((100, 3))\n+        sw = np.arange(100).reshape((100,)).astype(\"float32\") / 50.0\n+\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+            run_eagerly=run_eagerly,\n+            jit_compile=jit_compile,\n+        )\n+        logs = model.train_on_batch(x, y)\n+        self.assertTrue(isinstance(logs, list))\n+        self.assertEqual(len(logs), 2)\n+        self.assertAlmostEqual(logs[0], 16.0)\n+\n+        logs = model.train_on_batch(x, y, return_dict=True)\n+        self.assertTrue(isinstance(logs, dict))\n+        self.assertEqual(len(logs), 2)\n+        self.assertAlmostEqual(logs[\"loss\"], 15.579)\n+\n+        logs = model.test_on_batch(x, y)\n+        self.assertTrue(isinstance(logs, list))\n+        self.assertEqual(len(logs), 2)\n+        self.assertAlmostEqual(logs[0], 15.173)\n+\n+        logs = model.test_on_batch(x, y, return_dict=True)\n+        self.assertTrue(isinstance(logs, dict))\n+        self.assertEqual(len(logs), 2)\n+        self.assertAlmostEqual(logs[\"loss\"], 14.97)\n+\n+        output = model.predict_on_batch(x)\n+        self.assertTrue(isinstance(output, np.ndarray))\n+        self.assertAllClose(output[0], np.array([3.789511, 3.789511, 3.789511]))\n+\n+        # With sample weights\n+        logs = model.train_on_batch(x, y, sw)\n+        self.assertAlmostEqual(logs[0], 14.819)\n+        logs = model.test_on_batch(x, y, sw)\n+        self.assertAlmostEqual(logs[0], 14.595)\n+        output = model.predict_on_batch(x)\n+        self.assertAllClose(output[0], np.array([3.689468, 3.689468, 3.689468]))\n+\n+        # With class weights\n+        logs = model.train_on_batch(x, y, class_weight={1: 0.3, 0: 0.2})\n+        self.assertAlmostEqual(logs[0], 12.899)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
