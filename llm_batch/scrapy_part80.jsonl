{"custom_id": "scrapy#79851aefa645f6687d4d8f1d094c1fa38b388e6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a50af05d74a9e22e5d1476e7312209c27be8372", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 20 | Churn Cumulative: 240 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -81,7 +81,6 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.webconsole.spiderctl.Spiderctl': 0,\n     'scrapy.contrib.webconsole.enginestatus.EngineStatus': 0,\n     'scrapy.contrib.webconsole.stats.StatsDump': 0,\n-    'scrapy.contrib.spider.reloader.SpiderReloader': 0,\n     'scrapy.contrib.memusage.MemoryUsage': 0,\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n     'scrapy.contrib.closedomain.CloseDomain': 0,\n\n@@ -1,19 +0,0 @@\n-\"\"\"\n-Reload spider modules once they are finished scraping\n-\n-This is to release any resources held on to by scraping spiders.\n-\"\"\"\n-import sys\n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n-from scrapy import log\n-\n-class SpiderReloader(object):\n-\n-    def __init__(self):\n-        dispatcher.connect(self.domain_closed, signal=signals.domain_closed)\n-\n-    def domain_closed(self, domain, spider):\n-        module = spider.__module__\n-        log.msg(\"reloading module %s\" % module, domain=domain)\n-        reload(sys.modules[module])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34d559003ddb430f35263d016253499a6396549d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ce2b66c562b6ea511064e1579ee916530d4f920", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 122 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,10 @@ import sys\n import urlparse\n \n from twisted.plugin import getCache\n+from twisted.python.rebuild import rebuild\n \n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.core import signals\n from scrapy.spider.models import ISpider\n from scrapy import log\n from scrapy.conf import settings\n@@ -22,6 +25,7 @@ class TwistedPluginSpiderManager(object):\n         self.default_domain = None\n         self.force_domain = None\n         self.spider_modules = None\n+        dispatcher.connect(self._domain_closed, signal=signals.domain_closed)\n \n     def fromdomain(self, domain_name):\n         return self.asdict().get(domain_name)\n@@ -107,3 +111,12 @@ class TwistedPluginSpiderManager(object):\n         except KeyboardInterrupt:\n             sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n             sys.exit(2)\n+\n+    def _domain_closed(self, domain, spider):\n+        \"\"\"Reload spider module to release any resources held on to by the\n+        spider\n+        \"\"\"\n+        module_name = spider.__module__\n+        log.msg(\"reloading module %s\" % module_name, domain=domain)\n+        new_module = rebuild(sys.modules[module_name])\n+        self._spiders[domain] = new_module.SPIDER\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#596d2c447959b51d95c42c0bb6d2f1c23f813ce0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 221 | Contributors (this commit): 3 | Commits (past 90d): 34 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -73,7 +73,7 @@ DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n EXTENSIONS = {}\n \n EXTENSIONS_BASE = {\n-    'scrapy.stats.corestats.CoreStats': 0,\n+    'scrapy.contrib.corestats.CoreStats': 0,\n     'scrapy.management.web.WebConsole': 0,\n     'scrapy.management.telnet.TelnetConsole': 0,\n     'scrapy.contrib.webconsole.scheduler.SchedulerQueue': 0,\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db5420f8d57a48bd2fbb0d9fdfd2fb028719fe4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 127 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -117,6 +117,7 @@ class TwistedPluginSpiderManager(object):\n         spider\n         \"\"\"\n         module_name = spider.__module__\n-        log.msg(\"reloading module %s\" % module_name, domain=domain)\n-        new_module = rebuild(sys.modules[module_name])\n+        log.msg(\"Reloading module %s\" % module_name, domain=domain, \\\n+            level=log.DEBUG)\n+        new_module = rebuild(sys.modules[module_name], doLog=0)\n         self._spiders[domain] = new_module.SPIDER\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91ff9c9d50d8f360f9a36ed9580f8db3b13245c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 59 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,5 +1,5 @@\n from scrapy.xlib.pydispatch import dispatcher\n-from twisted.internet import defer\n+from twisted.internet.defer import Deferred, DeferredList\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n@@ -7,6 +7,7 @@ from scrapy.core import signals\n from scrapy.core.engine import scrapyengine\n from scrapy.utils.request import request_fingerprint\n from scrapy.spider import spiders\n+from scrapy.utils.misc import arg_to_iter\n \n \n class MediaPipeline(object):\n@@ -33,39 +34,29 @@ class MediaPipeline(object):\n \n     def process_item(self, domain, item):\n         info = self.domaininfo[domain]\n-        requests = self.get_media_requests(item, info)\n-        assert requests is None or hasattr(requests, '__iter__'), \\\n-                'get_media_requests should return None or iterable'\n-\n-        def _bugtrap(_failure, request):\n-            log.msg('Unhandled ERROR in MediaPipeline.item_media_{downloaded,failed} for %s: %s' \\\n-                    % (request, _failure), log.ERROR, domain=domain)\n-\n-        lst = []\n-        for request in requests or ():\n+        requests = arg_to_iter(self.get_media_requests(item, info))\n+        dlist = []\n+        for request in requests:\n             dfd = self._enqueue(request, info)\n             dfd.addCallbacks(\n                     callback=self.item_media_downloaded,\n                     callbackArgs=(item, request, info),\n                     errback=self.item_media_failed,\n-                    errbackArgs=(item, request, info),\n-                    )\n-            dfd.addErrback(_bugtrap, request)\n-            lst.append(dfd)\n+                    errbackArgs=(item, request, info),)\n+            dfd.addErrback(log.err, \\\n+                    'Unhandled ERROR in %s.item_media_{downloaded,failed} for %s' \\\n+                    % (type(self).__name__, request), domain=domain)\n+            dlist.append(dfd)\n \n-        dlst = defer.DeferredList(lst, consumeErrors=False)\n-        dlst.addBoth(self.item_completed, item, info)\n-        return dlst\n+        return DeferredList(dlist).addCallback(self.item_completed, item, info)\n \n     def _enqueue(self, request, info):\n-        wad = request.deferred or defer.Deferred()\n+        wad = request.deferred or Deferred()\n         fp = request_fingerprint(request)\n \n         # if already downloaded, return cached result.\n         if fp in info.downloaded:\n-            cached = info.downloaded[fp]\n-            defer_result(cached).chainDeferred(wad)\n-            return wad # break\n+            return defer_result(info.downloaded[fp]).chainDeferred(wad)\n \n         # add to pending list for this request, and wait for result like the others.\n         info.waiting.setdefault(fp, []).append(wad)\n@@ -77,19 +68,13 @@ class MediaPipeline(object):\n         return wad\n \n     def _download(self, request, info, fp):\n-        def _bugtrap(_failure):\n-            log.msg('Unhandled ERROR in MediaPipeline._downloaded: %s' \\\n-                    % (_failure), log.ERROR, domain=info.domain)\n-\n         def _downloaded(result):\n-            info.downloaded[fp] = result # cache result\n-            waiting = info.waiting[fp] # client list\n-            del info.waiting[fp]\n-            del info.downloading[fp]\n-            for wad in waiting: # call each waiting client with result\n+            info.downloading.pop(fp)\n+            info.downloaded[fp] = result\n+            for wad in info.waiting.pop(fp): # pass result to each waiting client\n                 defer_result(result).chainDeferred(wad)\n \n-        def _evaluated(result):\n+        def _post_media_to_download(result):\n             if result is None: # continue with download\n                 dwld = mustbe_deferred(self.download, request, info)\n                 dwld.addCallbacks(\n@@ -102,14 +87,14 @@ class MediaPipeline(object):\n \n             info.downloading[fp] = (request, dwld) # fill downloading state data\n             dwld.addBoth(_downloaded) # append post-download hook\n-            dwld.addErrback(_bugtrap) # catch media_downloaded and media_failed unhandled errors\n+            dwld.addErrback(log.err, domain=info.domain)\n \n         # declare request in downloading state (None is used as place holder)\n         info.downloading[fp] = None\n \n         # defer pre-download request processing\n         dfd = mustbe_deferred(self.media_to_download, request, info)\n-        dfd.addCallback(_evaluated)\n+        dfd.addCallback(_post_media_to_download)\n \n     ### Overradiable Interface\n     def download(self, request, info):\n@@ -166,8 +151,9 @@ class MediaPipeline(object):\n         result is the return value of `media_downloaded` hook, or the non-Failure instance\n         returned by `media_failed` hook.\n \n-        return value of this method isn't important and is recommended to return None.\n+        return value of this method is used for results parameter of item_completed hook\n         \"\"\"\n+        return result\n \n     def item_media_failed(self, failure, item, request, info):\n         \"\"\" Method to handle failed result of requested media for item.\n@@ -175,8 +161,9 @@ class MediaPipeline(object):\n         result is the returned Failure instance of `media_failed` hook, or Failure instance\n         of an exception raised by `media_downloaded` hook.\n \n-        return value of this method isn't important and is recommended to return None.\n+        return value of this method is used for results parameter of item_completed hook\n         \"\"\"\n+        return failure\n \n     def item_completed(self, results, item, info):\n         return item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b05fd806406ff8113f56f315eaae863cb6a6dfcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 4 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -72,7 +72,7 @@ class BaseImagesPipeline(MediaPipeline):\n             log.msg(str(ex), level=log.WARNING, domain=info.domain)\n             raise ex\n \n-        return '%s#%s' % (key, checksum) if checksum else key\n+        return {'scraped_url': request.url, 'path': key, 'checksum': checksum}\n \n     def media_failed(self, failure, request, info):\n         referer = request.headers.get('Referer')\n@@ -101,7 +101,7 @@ class BaseImagesPipeline(MediaPipeline):\n             self.inc_stats(info.domain, 'uptodate')\n \n             checksum = result.get('checksum', None)\n-            return '%s#%s' % (key, checksum) if checksum else key\n+            return {'scraped_url': request.url, 'path': key, 'checksum': checksum}\n \n         key = self.image_key(request.url)\n         dfd = defer.maybeDeferred(self.stat_key, key, info)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6adc141debde3d01ab8f003ba9b6d612e9ef828", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 163 | Lines Deleted: 73 | Files Changed: 2 | Hunks: 16 | Methods Changed: 22 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 236 | Churn Cumulative: 345 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,17 +8,24 @@ from __future__ import with_statement\n import os\n import time\n import hashlib\n+import urlparse\n+import rfc822\n import Image\n from cStringIO import StringIO\n+from collections import defaultdict\n \n from twisted.internet import defer\n \n+from scrapy.xlib.pydispatch import dispatcher\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.utils.misc import md5sum\n+from scrapy.core import signals\n+from scrapy.core.engine import scrapyengine\n from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.conf import settings\n from scrapy.contrib.pipeline.media import MediaPipeline\n+from scrapy.http import Request\n+from scrapy.conf import settings\n \n \n class NoimagesDrop(DropItem):\n@@ -28,17 +35,159 @@ class ImageException(Exception):\n     \"\"\"General image error exception\"\"\"\n \n \n-class BaseImagesPipeline(MediaPipeline):\n+class FSImagesStore(object):\n \n+    def __init__(self, basedir):\n+        if '://' in basedir:\n+            basedir = basedir.split('://', 1)[1]\n+        self.basedir = basedir\n+        self._mkdir(self.basedir)\n+        self.created_directories = defaultdict(set)\n+        dispatcher.connect(self.domain_closed, signals.domain_closed)\n+\n+    def domain_closed(self, domain):\n+        self.created_directories.pop(domain, None)\n+\n+    def persist_image(self, key, image, buf, info):\n+        absolute_path = self._get_filesystem_path(key)\n+        self._mkdir(os.path.dirname(absolute_path), info)\n+        image.save(absolute_path)\n+\n+    def stat_image(self, key, info):\n+        absolute_path = self._get_filesystem_path(key)\n+        try:\n+            last_modified = os.path.getmtime(absolute_path)\n+        except: # FIXME: catching everything!\n+            return {}\n+\n+        with open(absolute_path, 'rb') as imagefile:\n+            checksum = md5sum(imagefile)\n+\n+        return {'last_modified': last_modified, 'checksum': checksum}\n+\n+    def _get_filesystem_path(self, key):\n+        return os.path.join(self.basedir, key)\n+\n+    def _mkdir(self, dirname, domain=None):\n+        seen = self.created_directories[domain] if domain else set()\n+        if dirname not in seen:\n+            if not os.path.exists(dirname):\n+                os.makedirs(dirname)\n+            seen.add(dirname)\n+\n+\n+class S3ImagesStore(object):\n+\n+    request_priority = 1000\n+\n+    def __init__(self, uri):\n+        assert uri.startswith('s3://')\n+        self.bucket, self.prefix = uri[5:].split('/', 1)\n+        self._set_custom_spider()\n+\n+    def _set_custom_spider(self):\n+        domain = settings['IMAGES_S3STORE_SPIDER']\n+        if domain:\n+            from scrapy.spider import spiders\n+            self.s3_spider = spiders.fromdomain(domain)\n+        else:\n+            self.s3_spider = None\n+\n+    def stat_image(self, key, info):\n+        def _onsuccess(response):\n+            if response.status == 200:\n+                checksum = response.headers['Etag'].strip('\"')\n+                last_modified = response.headers['Last-Modified']\n+                modified_tuple = rfc822.parsedate_tz(last_modified)\n+                modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n+                return {'checksum': checksum, 'last_modified': modified_stamp}\n+\n+        req = self._build_request(key, method='HEAD')\n+        return self._download_request(req, info).addCallback(_onsuccess)\n+\n+    def persist_image(self, key, image, buf, info):\n+        \"\"\"Upload image to S3 storage\"\"\"\n+        width, height = image.size\n+        headers = {\n+                'Content-Type': 'image/jpeg',\n+                'X-Amz-Acl': 'public-read',\n+                'X-Amz-Meta-Width': str(width),\n+                'X-Amz-Meta-Height': str(height),\n+                'Cache-Control': 'max-age=172800',\n+                }\n+\n+        buf.seek(0)\n+        req = self._build_request(key, method='PUT', body=buf.read(), headers=headers)\n+        return self._download_request(req, info)\n+\n+    def _build_request(self, key, method, body=None, headers=None):\n+        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket, self.prefix, key)\n+        return Request(url, method=method, body=body, headers=headers, \\\n+                priority=self.request_priority)\n+\n+    def _download_request(self, request, info):\n+        \"\"\"This method is used for HEAD and PUT requests sent to amazon S3\n+\n+        It tries to use a specific spider domain for uploads, or defaults\n+        to current domain spider.\n+        \"\"\"\n+        if self.s3_spider:\n+            # need to use schedule to auto-open domain\n+            return scrapyengine.schedule(request, self.s3_spider)\n+        return scrapyengine.download(request, info.spider)\n+\n+\n+class ImagesPipeline(MediaPipeline):\n+    \"\"\"Abstract pipeline that implement the image downloading and thumbnail generation logic\n+\n+    This pipeline tries to minimize network transfers and image processing,\n+    doing stat of the images and determining if image is new, uptodate or\n+    expired.\n+\n+    `new` images are those that pipeline never processed and needs to be\n+        downloaded from supplier site the first time.\n+\n+    `uptodate` images are the ones that the pipeline processed and are still\n+        valid images.\n+\n+    `expired` images are those that pipeline already processed but the last\n+        modification was made long time ago, so a reprocessing is recommended to\n+        refresh it in case of change.\n+\n+    IMAGES_EXPIRES setting controls the maximun days since an image was modified\n+    to consider it uptodate.\n+\n+    THUMBS is a tuple of tuples, each sub-tuple is a pair of thumb_id string\n+    and a compatible python image library size (a tuple).\n+    See thumbnail method at http://www.pythonware.com/library/pil/handbook/image.htm\n+\n+    Downloaded images are skipped if sizes aren't greater than MIN_WIDTH and\n+    MIN_HEIGHT limit. A proper log messages will be printed.\n+\n+    \"\"\"\n+\n+    MEDIA_NAME = 'image'\n     MIN_WIDTH = settings.getint('IMAGES_MIN_WIDTH', 0)\n     MIN_HEIGHT = settings.getint('IMAGES_MIN_HEIGHT', 0)\n-    IMAGES_EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n-    MEDIA_NAME = 'image'\n-    THUMBS = (\n-#             ('50', (50, 50)),\n-#             ('110', (110, 110)),\n-#             ('270', (270, 270))\n-     )\n+    EXPIRES = settings.getint('IMAGES_EXPIRES', 90)\n+    THUMBS = settings.getlist('IMAGES_THUMBS')\n+    STORE_SCHEMES = {\n+            '': FSImagesStore,\n+            'file': FSImagesStore,\n+            's3': S3ImagesStore,\n+            }\n+\n+    def __init__(self):\n+        store_uri = settings['IMAGES_STORE']\n+        if not store_uri:\n+            raise NotConfigured\n+        self.store = self._get_store(store_uri)\n+        super(ImagesPipeline, self).__init__()\n+\n+    def _get_store(self, uri):\n+        scheme = urlparse.urlparse(uri).scheme\n+        store_cls = self.STORE_SCHEMES[scheme]\n+        return store_cls(uri)\n \n     def media_downloaded(self, response, request, info):\n         mtype = self.MEDIA_NAME\n@@ -92,7 +241,7 @@ class BaseImagesPipeline(MediaPipeline):\n \n             age_seconds = time.time() - last_modified\n             age_days = age_seconds / 60 / 60 / 24\n-            if age_days > self.IMAGES_EXPIRES:\n+            if age_days > self.EXPIRES:\n                 return # returning None force download\n \n             referer = request.headers.get('Referer')\n@@ -104,15 +253,15 @@ class BaseImagesPipeline(MediaPipeline):\n             return {'scraped_url': request.url, 'path': key, 'checksum': checksum}\n \n         key = self.image_key(request.url)\n-        dfd = defer.maybeDeferred(self.stat_key, key, info)\n+        dfd = defer.maybeDeferred(self.store.stat_image, key, info)\n         dfd.addCallbacks(_onsuccess, lambda _:None)\n-        dfd.addErrback(log.err, self.__class__.__name__ + '.stat_key')\n+        dfd.addErrback(log.err, self.__class__.__name__ + '.store.stat_image')\n         return dfd\n \n     def image_downloaded(self, response, request, info):\n         first_buf = None\n         for key, image, buf in self.get_images(response, request, info):\n-            self.store_image(key, image, buf, info)\n+            self.store.persist_image(key, image, buf, info)\n             if first_buf is None:\n                 first_buf = buf\n         first_buf.seek(0)\n@@ -162,59 +311,3 @@ class BaseImagesPipeline(MediaPipeline):\n     def thumb_key(self, url, thumb_id):\n         image_guid = hashlib.sha1(url).hexdigest()\n         return 'thumbs/%s/%s.jpg' % (thumb_id, image_guid)\n-\n-    # Required overradiable interface\n-    def store_image(self, key, image, buf, info):\n-        raise NotImplementedError\n-\n-    def stat_key(self, key, info):\n-        raise NotImplementedError\n-\n-\n-class ImagesPipeline(BaseImagesPipeline):\n-    \"\"\"Images pipeline with filesystem support as image's store backend\n-\n-    If IMAGES_DIR setting has a valid value, this pipeline is enabled and use\n-    path defined at setting as dirname for storing images.\n-\n-    \"\"\"\n-\n-    class DomainInfo(BaseImagesPipeline.DomainInfo):\n-        def __init__(self, domain):\n-            self.created_directories = set()\n-            super(ImagesPipeline.DomainInfo, self).__init__(domain)\n-\n-    def __init__(self):\n-        if not settings['IMAGES_DIR']:\n-            raise NotConfigured\n-\n-        self.BASEDIRNAME = settings['IMAGES_DIR']\n-        self.mkdir(self.BASEDIRNAME)\n-        super(ImagesPipeline, self).__init__()\n-\n-    def store_image(self, key, image, buf, info):\n-        absolute_path = self.get_filesystem_path(key)\n-        self.mkdir(os.path.dirname(absolute_path), info)\n-        image.save(absolute_path)\n-\n-    def stat_key(self, key, info):\n-        absolute_path = self.get_filesystem_path(key)\n-        try:\n-            last_modified = os.path.getmtime(absolute_path)\n-        except:\n-            return {}\n-\n-        with open(absolute_path, 'rb') as imagefile:\n-            checksum = md5sum(imagefile)\n-\n-        return {'last_modified': last_modified, 'checksum': checksum}\n-\n-    def get_filesystem_path(self, key):\n-        return os.path.join(self.BASEDIRNAME, key)\n-\n-    def mkdir(self, dirname, info=None):\n-        already_created = info.created_directories if info else set()\n-        if dirname not in already_created:\n-            if not os.path.exists(dirname):\n-                os.makedirs(dirname)\n-            already_created.add(dirname)\n\n@@ -54,8 +54,7 @@ class S3ImagesPipeline(BaseImagesPipeline):\n \n     def s3_request(self, key, method, body=None, headers=None):\n         url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.key_prefix, key)\n-        req = Request(url, method=method, body=body, headers=headers)\n-        return req\n+        return Request(url, method=method, body=body, headers=headers)\n \n     def stat_key(self, key, info):\n         def _onsuccess(response):\n@@ -97,5 +96,3 @@ class S3ImagesPipeline(BaseImagesPipeline):\n             # need to use schedule to auto-open domain\n             return scrapyengine.schedule(request, self.AmazonS3Spider)\n         return self.download(request, info)\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11c820232b772110c1545316d458bfea8da038e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 345 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -190,24 +190,23 @@ class ImagesPipeline(MediaPipeline):\n         return store_cls(uri)\n \n     def media_downloaded(self, response, request, info):\n-        mtype = self.MEDIA_NAME\n         referer = request.headers.get('Referer')\n \n         if response.status != 200:\n-            msg = 'Image (http-error): Error downloading %s from %s referred in <%s>' \\\n-                    % (mtype, request, referer)\n+            msg = 'Image (http-error): Error downloading image from %s referred in <%s>' \\\n+                    % (request, referer)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n             raise ImageException(msg)\n \n         if not response.body:\n-            msg = 'Image (empty-content): Empty %s from %s referred in <%s>: no-content' \\\n-                    % (mtype, request, referer)\n+            msg = 'Image (empty-content): Empty image from %s referred in <%s>: no-content' \\\n+                    % (request, referer)\n             log.msg(msg, level=log.WARNING, domain=info.domain)\n             raise ImageException(msg)\n \n         status = 'cached' if 'cached' in response.flags else 'downloaded'\n-        msg = 'Image (%s): Downloaded %s from %s referred in <%s>' % \\\n-                (status, mtype, request, referer)\n+        msg = 'Image (%s): Downloaded image from %s referred in <%s>' % \\\n+                (status, request, referer)\n         log.msg(msg, level=log.DEBUG, domain=info.domain)\n         self.inc_stats(info.domain, status)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19c0093e508923ff0d682fc4eb5764e4c93bfe8e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 14 | Churn Cumulative: 14 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,9 @@\n from twisted.trial import unittest\n+from scrapy.conf import settings\n+from tempfile import mkdtemp\n+from shutil import rmtree\n+\n+SETTINGS_DISABLED = settings.disabled\n \n class ImagesPipelineTestCase(unittest.TestCase):\n     def setUp(self):\n@@ -7,11 +12,16 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         except ImportError, e:\n             raise unittest.SkipTest(e)\n \n-        from scrapy.contrib.pipeline.images import BaseImagesPipeline\n-        self.pipeline = BaseImagesPipeline()\n+        from scrapy.contrib.pipeline.images import ImagesPipeline\n+        self.tempdir = mkdtemp()\n+        settings.disabled = False\n+        settings.overrides['IMAGES_STORE'] = self.tempdir\n+        self.pipeline = ImagesPipeline()\n \n     def tearDown(self):\n         del self.pipeline\n+        rmtree(self.tempdir)\n+        settings.disabled = SETTINGS_DISABLED\n \n     def test_image_path(self):\n         image_path = self.pipeline.image_key\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45bc12e26814c822e98a87c6e8621f22948dff6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 45 | Churn Cumulative: 241 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ See documentation in docs/topics/logging.rst\n \"\"\"\n import sys\n from traceback import format_exc\n+from contextlib import contextmanager\n \n from twisted.python import log\n from scrapy.xlib.pydispatch import dispatcher\n@@ -34,8 +35,6 @@ log_level = DEBUG\n \n started = False\n \n-_prev_descriptors = (sys.stdout, sys.stderr)\n-\n def start(logfile=None, loglevel=None, logstdout=None):\n     \"\"\"Initialize and start logging facility\"\"\"\n     global log_level, started\n@@ -56,13 +55,6 @@ def start(logfile=None, loglevel=None, logstdout=None):\n         file = open(logfile, 'a') if logfile else sys.stderr\n         log.startLogging(file, setStdout=logstdout)\n \n-def _switch_descriptors():\n-    global _prev_descriptors\n-\n-    cur = (sys.stdout, sys.stderr)\n-    sys.stdout, sys.stderr = _prev_descriptors\n-    _prev_descriptors = cur\n-\n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n@@ -81,3 +73,13 @@ def err(*args, **kwargs):\n     component = kwargs.pop('component', BOT_NAME)\n     kwargs['system'] = domain if domain else component\n     log.err(*args, **kwargs)\n+\n+_std_descriptors_backup = (sys.stdout, sys.stderr)\n+\n+@contextmanager\n+def _std_descriptors():\n+    cur = (sys.stdout, sys.stderr)\n+    sys.stdout, sys.stderr = _std_descriptors_backup\n+    yield\n+    sys.stdout, sys.stderr = cur\n+\n\n@@ -4,6 +4,8 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n+from __future__ import with_statement\n+\n import os\n import urllib\n import urlparse\n@@ -149,7 +151,6 @@ class Shell(object):\n \n def inspect_response(response):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n+    with log._std_descriptors():\n         shell = Shell(nofetch=True)\n-    log._switch_descriptors()\n         shell.inspect_response(response)\n-    log._switch_descriptors()\n\n@@ -0,0 +1,20 @@\n+\"\"\"\n+Function for invoking the Python Debugger from Scrapy\n+\"\"\"\n+\n+from __future__ import absolute_import, with_statement\n+\n+from pdb import Pdb\n+\n+from scrapy import log\n+\n+class ScrapyPdb(Pdb):\n+\n+    def setup(self, f, t):\n+        Pdb.setup(self, f, t)\n+        self.curindex -= 2\n+\n+def set_trace():\n+    \"\"\"Like pdb.set_trace() but works nice with the Scrapy log\"\"\"\n+    with log._std_descriptors():\n+        ScrapyPdb().set_trace()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ed743f26531e2e7b9a14361779d6201cbb1466c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 98 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 98 | Churn Cumulative: 111 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,98 +0,0 @@\n-import rfc822\n-\n-from scrapy.http import Request\n-from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.contrib.pipeline.images import BaseImagesPipeline\n-from scrapy.conf import settings\n-\n-\n-class S3ImagesPipeline(BaseImagesPipeline):\n-    \"\"\"Images pipeline with amazon S3 support as image's store backend\n-\n-    This pipeline tries to minimize the PUT requests made to amazon doing a\n-    HEAD per full image, if HEAD returns a successfully response, then the\n-    Last-Modified header is compared to current timestamp and if the difference\n-    in days are greater that IMAGE_EXPIRES setting, then the image is\n-    downloaded, reprocessed and uploaded to S3 again including its thumbnails.\n-\n-    It is recommended to add an spider with domain_name 's3.amazonaws.com',\n-    doing that you will overcome the limit of request per spider. The following\n-    is the minimal code for this spider:\n-\n-        from scrapy.spider import BaseSpider\n-\n-        class S3AmazonAWSSpider(BaseSpider):\n-            domain_name = \"s3.amazonaws.com\"\n-            max_concurrent_requests = 100\n-            start_urls = ('http://s3.amazonaws.com/',)\n-\n-        SPIDER = S3AmazonAWSSpider()\n-\n-    Commonly uploading images to S3 requires requests to be signed, the\n-    recommended way is to enable scrapy.contrib.aws.AWSMiddleware downloader\n-    middleware and configure AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY\n-    settings\n-\n-    More info about amazon S3 at http://docs.amazonwebservices.com/AmazonS3/2006-03-01/\n-\n-    \"\"\"\n-\n-    # amazon s3 bucket name to put images\n-    bucket_name = settings.get('S3_BUCKET')\n-\n-    # prefix to prepend to image keys\n-    key_prefix = settings.get('S3_PREFIX', '')\n-\n-    # Optional spider to use for image uploading\n-    AmazonS3Spider = None\n-\n-    def __init__(self):\n-        if not settings['S3_IMAGES']:\n-            raise NotConfigured\n-        super(S3ImagesPipeline, self).__init__()\n-\n-    def s3_request(self, key, method, body=None, headers=None):\n-        url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket_name, self.key_prefix, key)\n-        return Request(url, method=method, body=body, headers=headers)\n-\n-    def stat_key(self, key, info):\n-        def _onsuccess(response):\n-            if response.status == 200:\n-                checksum = response.headers['Etag'].strip('\"')\n-                last_modified = response.headers['Last-Modified']\n-                modified_tuple = rfc822.parsedate_tz(last_modified)\n-                modified_stamp = int(rfc822.mktime_tz(modified_tuple))\n-                return {'checksum': checksum, 'last_modified': modified_stamp}\n-\n-        req = self.s3_request(key, method='HEAD')\n-        dfd = self.s3_download(req, info)\n-        dfd.addCallback(_onsuccess)\n-        return dfd\n-\n-    def store_image(self, key, image, buf, info):\n-        \"\"\"Upload image to S3 storage\"\"\"\n-        width, height = image.size\n-        headers = {\n-                'Content-Type': 'image/jpeg',\n-                'X-Amz-Acl': 'public-read',\n-                'X-Amz-Meta-Width': str(width),\n-                'X-Amz-Meta-Height': str(height),\n-                'Cache-Control': 'max-age=172800',\n-                }\n-\n-        buf.seek(0)\n-        req = self.s3_request(key, method='PUT', body=buf.read(), headers=headers)\n-        self.s3_download(req, info)\n-\n-    def s3_download(self, request, info):\n-        \"\"\"This method is used for HEAD and PUT requests sent to amazon S3\n-\n-        It tries to use a specific spider domain for uploads, or defaults\n-        to current domain spider.\n-\n-        \"\"\"\n-        if self.AmazonS3Spider:\n-            # need to use schedule to auto-open domain\n-            return scrapyengine.schedule(request, self.AmazonS3Spider)\n-        return self.download(request, info)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11f296b7536780d8b736c0a4f32a376c1dfc4a05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 38 | Files Changed: 4 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 39 | Churn Cumulative: 280 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1 +0,0 @@\n-LOG_STDOUT = True\n\n@@ -5,7 +5,6 @@ See documentation in docs/topics/logging.rst\n \"\"\"\n import sys\n from traceback import format_exc\n-from contextlib import contextmanager\n \n from twisted.python import log\n from scrapy.xlib.pydispatch import dispatcher\n@@ -73,13 +72,3 @@ def err(*args, **kwargs):\n     component = kwargs.pop('component', BOT_NAME)\n     kwargs['system'] = domain if domain else component\n     log.err(*args, **kwargs)\n-\n-_std_descriptors_backup = (sys.stdout, sys.stderr)\n-\n-@contextmanager\n-def _std_descriptors():\n-    cur = (sys.stdout, sys.stderr)\n-    sys.stdout, sys.stderr = _std_descriptors_backup\n-    yield\n-    sys.stdout, sys.stderr = cur\n-\n\n@@ -4,8 +4,6 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-from __future__ import with_statement\n-\n import os\n import urllib\n import urlparse\n@@ -22,7 +20,6 @@ from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n from scrapy.http import Request\n from scrapy.fetcher import get_or_create_spider\n-from scrapy import log\n \n def relevant_var(varname):\n     return varname not in ['shelp', 'fetch', 'view', '__builtins__', 'In', \\\n@@ -151,6 +148,4 @@ class Shell(object):\n \n def inspect_response(response):\n     \"\"\"Open a shell to inspect the given response\"\"\"\n-    with log._std_descriptors():\n-        shell = Shell(nofetch=True)\n-        shell.inspect_response(response)\n+    Shell(nofetch=True).inspect_response(response)\n\n@@ -1,20 +0,0 @@\n-\"\"\"\n-Function for invoking the Python Debugger from Scrapy\n-\"\"\"\n-\n-from __future__ import absolute_import, with_statement\n-\n-from pdb import Pdb\n-\n-from scrapy import log\n-\n-class ScrapyPdb(Pdb):\n-\n-    def setup(self, f, t):\n-        Pdb.setup(self, f, t)\n-        self.curindex -= 2\n-\n-def set_trace():\n-    \"\"\"Like pdb.set_trace() but works nice with the Scrapy log\"\"\"\n-    with log._std_descriptors():\n-        ScrapyPdb().set_trace()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37929f42b3a5eba06b8cceaf2b7893dc154750b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 131 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -117,7 +117,9 @@ class TwistedPluginSpiderManager(object):\n         spider\n         \"\"\"\n         module_name = spider.__module__\n+        module = sys.modules[module_name]\n+        if hasattr(module, 'SPIDER'):\n             log.msg(\"Reloading module %s\" % module_name, domain=domain, \\\n                 level=log.DEBUG)\n-        new_module = rebuild(sys.modules[module_name], doLog=0)\n+            new_module = rebuild(module, doLog=0)\n             self._spiders[domain] = new_module.SPIDER\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51eb641fc772c1fb5d4719561ce7e72936a162be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 182 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -74,7 +74,6 @@ class Shell(object):\n             self.vars['response'] = response\n             self.vars['request'] = request\n             self.vars['spider'] = spiders.fromurl(url)\n-\n         if not self.nofetch:\n             self.vars['fetch'] = self.fetch\n         self.vars['view'] = open_in_browser\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a7f5c6f8785d99e44254127ec2b6fdaa70ed8e15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 207 | Lines Deleted: 150 | Files Changed: 8 | Hunks: 53 | Methods Changed: 50 | Complexity Δ (Sum/Max): 10/4 | Churn Δ: 357 | Churn Cumulative: 2130 | Contributors (this commit): 4 | Commits (past 90d): 103 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -129,7 +129,7 @@ def execute(argv=None):\n     del args[0]  # remove command name from args\n     _save_command_executed(cmdname, cmd, args, opts)\n     from scrapy.core.manager import scrapymanager\n-    scrapymanager.configure()\n+    scrapymanager.configure(control_reactor=True)\n     ret = _run_command(cmd, args, opts)\n     if ret is False:\n         parser.print_help()\n\n@@ -9,8 +9,6 @@ import urlparse\n from twisted.plugin import getCache\n from twisted.python.rebuild import rebuild\n \n-from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.core import signals\n from scrapy.spider.models import ISpider\n from scrapy import log\n from scrapy.conf import settings\n@@ -25,10 +23,9 @@ class TwistedPluginSpiderManager(object):\n         self.default_domain = None\n         self.force_domain = None\n         self.spider_modules = None\n-        dispatcher.connect(self._domain_closed, signal=signals.domain_closed)\n \n-    def fromdomain(self, domain_name):\n-        return self.asdict().get(domain_name)\n+    def fromdomain(self, domain):\n+        return self.asdict().get(domain)\n \n     def fromurl(self, url):\n         if self.force_domain:\n@@ -112,10 +109,11 @@ class TwistedPluginSpiderManager(object):\n             sys.stderr.write(\"Interrupted while loading Scrapy spiders\\n\")\n             sys.exit(2)\n \n-    def _domain_closed(self, domain, spider):\n+    def close_domain(self, domain):\n         \"\"\"Reload spider module to release any resources held on to by the\n         spider\n         \"\"\"\n+        spider = self._spiders[domain]\n         module_name = spider.__module__\n         module = sys.modules[module_name]\n         if hasattr(module, 'SPIDER'):\n\n@@ -5,6 +5,7 @@ Download web pages using asynchronous IO\n from time import time\n \n from twisted.internet import reactor, defer\n+from twisted.python.failure import Failure\n \n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.spider import spiders\n@@ -31,8 +32,8 @@ class SiteInfo(object):\n         else:\n             self.max_concurrent_requests =  max_concurrent_requests\n \n-        self.queue = []\n         self.active = set()\n+        self.queue = []\n         self.transferring = set()\n         self.closing = False\n         self.lastseen = 0\n@@ -64,7 +65,7 @@ class Downloader(object):\n         \"\"\"\n         site = self.sites[spider.domain_name]\n         if site.closing:\n-            raise IgnoreRequest('Can\\'t fetch on a closing domain')\n+            raise IgnoreRequest('Cannot fetch on a closing domain')\n \n         site.active.add(request)\n         def _deactivate(_):\n@@ -72,25 +73,21 @@ class Downloader(object):\n             self._close_if_idle(spider.domain_name)\n             return _\n \n-        return self.middleware.download(self.enqueue, request, spider).addBoth(_deactivate)\n+        dfd = self.middleware.download(self.enqueue, request, spider)\n+        return dfd.addBoth(_deactivate)\n \n     def enqueue(self, request, spider):\n         \"\"\"Enqueue a Request for a effective download from site\"\"\"\n-        deferred = defer.Deferred()\n         site = self.sites[spider.domain_name]\n+        if site.closing:\n+            raise IgnoreRequest\n+        deferred = defer.Deferred()\n         site.queue.append((request, deferred))\n-        self.process_queue(spider)\n+        self._process_queue(spider)\n         return deferred\n \n-    def process_queue(self, spider):\n-        try:\n-            self._process_queue(spider)\n-        except:\n-            log.exc('Downloader process queue bug')\n-\n     def _process_queue(self, spider):\n-        \"\"\" Effective download requests from site queue\n-        \"\"\"\n+        \"\"\"Effective download requests from site queue\"\"\"\n         domain = spider.domain_name\n         site = self.sites.get(domain)\n         if not site:\n@@ -101,14 +98,18 @@ class Downloader(object):\n         if site.download_delay:\n             penalty = site.download_delay - now + site.lastseen\n             if penalty > 0:\n-                reactor.callLater(penalty, self.process_queue, spider=spider)\n+                reactor.callLater(penalty, self._process_queue, spider=spider)\n                 return\n         site.lastseen = now\n \n-        # Process requests in queue if there are free slots to transfer for this site\n+        # Process enqueued requests if there are free slots to transfer for this site\n         while site.queue and site.free_transfer_slots() > 0:\n             request, deferred = site.queue.pop(0)\n-            self._download(site, request, spider).chainDeferred(deferred)\n+            if site.closing:\n+                dfd = defer.fail(Failure(IgnoreRequest()))\n+            else:\n+                dfd = self._download(site, request, spider)\n+            dfd.chainDeferred(deferred)\n \n         self._close_if_idle(domain)\n \n@@ -130,7 +131,13 @@ class Downloader(object):\n         site.transferring.add(request)\n         def finish_transferring(_):\n             site.transferring.remove(request)\n-            self.process_queue(spider)\n+            self._process_queue(spider)\n+            # avoid partially downloaded responses from propagating to the\n+            # downloader middleware, to speed-up the closing process\n+            if site.closing:\n+                log.msg(\"Crawled while closing domain: %s\" % request, \\\n+                    level=log.DEBUG)\n+                raise IgnoreRequest\n             return _\n         return dfd.addBoth(finish_transferring)\n \n@@ -153,7 +160,7 @@ class Downloader(object):\n \n         site.closing = True\n         spider = spiders.fromdomain(domain)\n-        self.process_queue(spider)\n+        self._process_queue(spider)\n \n     def has_capacity(self):\n         \"\"\"Does the downloader have capacity to handle more domains\"\"\"\n\n@@ -6,7 +6,7 @@ For more information see docs/topics/architecture.rst\n \"\"\"\n from datetime import datetime\n \n-from twisted.internet import reactor, task\n+from twisted.internet import reactor, task, defer\n from twisted.python.failure import Failure\n from scrapy.xlib.pydispatch import dispatcher\n \n@@ -30,8 +30,8 @@ class ExecutionEngine(object):\n         self.keep_alive = False\n         self.closing = {} # dict (domain -> reason) of spiders being closed\n         self.running = False\n+        self.killed = False\n         self.paused = False\n-        self.control_reactor = True\n         self._next_request_pending = set()\n         self._mainloop_task = task.LoopingCall(self._mainloop)\n \n@@ -45,34 +45,34 @@ class ExecutionEngine(object):\n         self.scraper = Scraper(self)\n         self.configured = True\n \n-    def start(self, control_reactor=True):\n+    def start(self):\n         \"\"\"Start the execution engine\"\"\"\n         if self.running:\n             return\n-        self.control_reactor = control_reactor\n         self.start_time = datetime.utcnow()\n         send_catch_log(signal=signals.engine_started, sender=self.__class__)\n         self._mainloop_task.start(5.0, now=True)\n         reactor.callWhenRunning(self._mainloop)\n         self.running = True\n-        if control_reactor:\n-            reactor.run() # blocking call\n \n     def stop(self):\n-        \"\"\"Stop the execution engine\"\"\"\n+        \"\"\"Stop the execution engine gracefully\"\"\"\n         if not self.running:\n             return\n         self.running = False\n         for domain in self.open_domains:\n-            spider = spiders.fromdomain(domain)\n-            send_catch_log(signal=signals.domain_closed, sender=self.__class__, \\\n-                domain=domain, spider=spider, reason='shutdown')\n-            stats.close_domain(domain, reason='shutdown')\n+            reactor.addSystemEventTrigger('before', 'shutdown', \\\n+                self.close_domain, domain, reason='shutdown')\n         if self._mainloop_task.running:\n             self._mainloop_task.stop()\n-        if self.control_reactor and reactor.running:\n-            reactor.stop()\n-        send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n+\n+    def kill(self):\n+        \"\"\"Forces shutdown without waiting for pending transfers to finish.\n+        stop() must have been called first\n+        \"\"\"\n+        if self.running:\n+            return\n+        self.killed = True\n \n     def pause(self):\n         \"\"\"Pause the execution engine\"\"\"\n@@ -83,7 +83,8 @@ class ExecutionEngine(object):\n         self.paused = False\n \n     def is_idle(self):\n-        return self.scheduler.is_idle() and self.downloader.is_idle() and self.scraper.is_idle()\n+        return self.scheduler.is_idle() and self.downloader.is_idle() and \\\n+            self.scraper.is_idle()\n \n     def next_domain(self):\n         domain = self.domain_scheduler.next_domain()\n@@ -130,12 +131,15 @@ class ExecutionEngine(object):\n             spider = spiders.fromdomain(domain)\n             dwld = mustbe_deferred(self.download, request, spider)\n             dwld.chainDeferred(deferred).addBoth(lambda _: deferred)\n-            return dwld.addErrback(log.err)\n+            dwld.addErrback(log.err, \"Unhandled error on engine._next_request\")\n+            return dwld\n \n     def domain_is_idle(self, domain):\n-        scraper_idle = domain in self.scraper.sites and self.scraper.sites[domain].is_idle()\n+        scraper_idle = domain in self.scraper.sites \\\n+            and self.scraper.sites[domain].is_idle()\n         pending = self.scheduler.domain_has_pending_requests(domain)\n-        downloading = domain in self.downloader.sites and self.downloader.sites[domain].active\n+        downloading = domain in self.downloader.sites \\\n+            and self.downloader.sites[domain].active\n         return scraper_idle and not (pending or downloading)\n \n     def domain_is_closed(self, domain):\n@@ -155,7 +159,7 @@ class ExecutionEngine(object):\n     def crawl(self, request, spider):\n         schd = mustbe_deferred(self.schedule, request, spider)\n         schd.addBoth(self.scraper.enqueue_scrape, request, spider)\n-        schd.addErrback(log.err)\n+        schd.addErrback(log.err, \"Unhandled error on engine.crawl()\")\n         schd.addBoth(lambda _: self.next_request(spider.domain_name))\n \n     def schedule(self, request, spider):\n@@ -190,8 +194,8 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(\"Crawled %s (referer: <%s>)\" % (response, referer), level=log.DEBUG, \\\n-                    domain=domain)\n+                log.msg(\"Crawled %s (referer: <%s>)\" % (response, referer), \\\n+                    level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 newrequest = response\n@@ -201,12 +205,17 @@ class ExecutionEngine(object):\n \n         def _on_error(_failure):\n             \"\"\"handle an error processing a page\"\"\"\n-            ex = _failure.value\n-            errmsg = str(_failure) if not isinstance(ex, IgnoreRequest) \\\n-                else _failure.getErrorMessage()\n-            log.msg(\"Downloading <%s> (referer: <%s>): %s\" % (request.url, referer, errmsg), \\\n-                log.ERROR, domain=domain)\n-            return Failure(IgnoreRequest(str(ex)))\n+            exc = _failure.value\n+            if isinstance(exc, IgnoreRequest):\n+                errmsg = _failure.getErrorMessage()\n+                level = exc.level\n+            else:\n+                errmsg = str(_failure)\n+                level = log.ERROR\n+            if errmsg:\n+                log.msg(\"Downloading <%s> (referer: <%s>): %s\" % (request.url, \\\n+                    referer, errmsg), level=level, domain=domain)\n+            return Failure(IgnoreRequest(str(exc)))\n \n         def _on_complete(_):\n             self.next_request(domain)\n@@ -249,7 +258,7 @@ class ExecutionEngine(object):\n             self.next_request(domain)\n             return\n         except:\n-            log.err(_why=\"Exception catched on domain_idle signal dispatch\")\n+            log.err(\"Exception catched on domain_idle signal dispatch\")\n         if self.domain_is_idle(domain):\n             self.close_domain(domain, reason='finished')\n \n@@ -265,14 +274,19 @@ class ExecutionEngine(object):\n             self.closing[domain] = reason\n             self.downloader.close_domain(domain)\n             self.scheduler.clear_pending_requests(domain)\n-            self._finish_closing_domain_if_idle(domain)\n+            return self._finish_closing_domain_if_idle(domain)\n+        return defer.succeed(None)\n \n     def _finish_closing_domain_if_idle(self, domain):\n         \"\"\"Call _finish_closing_domain if domain is idle\"\"\"\n-        if self.domain_is_idle(domain):\n+        if self.domain_is_idle(domain) or self.killed:\n             self._finish_closing_domain(domain)\n         else:\n-            reactor.callLater(5, self._finish_closing_domain_if_idle, domain)\n+            dfd = defer.Deferred()\n+            dfd.addCallback(self._finish_closing_domain_if_idle)\n+            delay = 5 if self.running else 1\n+            reactor.callLater(delay, dfd.callback, domain)\n+            return dfd\n \n     def _finish_closing_domain(self, domain):\n         \"\"\"This function is called after the domain has been closed\"\"\"\n@@ -284,6 +298,10 @@ class ExecutionEngine(object):\n             domain=domain, spider=spider, reason=reason)\n         stats.close_domain(domain, reason=reason)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n+        spiders.close_domain(domain)\n+        if self.running:\n             self._mainloop()\n+        elif not self.open_domains:\n+            send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n \n scrapyengine = ExecutionEngine()\n\n@@ -5,6 +5,8 @@ These exceptions are documented in docs/topics/exceptions.rst. Please don't add\n new exceptions here without documenting them there.\n \"\"\"\n \n+from scrapy import log\n+\n # Internal\n \n class NotConfigured(Exception):\n@@ -15,7 +17,13 @@ class NotConfigured(Exception):\n \n class IgnoreRequest(Exception):\n     \"\"\"Indicates a decision was made not to process a request\"\"\"\n-    pass\n+\n+    def __init__(self, msg='', level=log.ERROR):\n+        self.msg = msg\n+        self.level = level\n+\n+    def __str__(self):\n+        return self.msg\n \n class DontCloseDomain(Exception):\n     \"\"\"Request the domain not to be closed yet\"\"\"\n\n@@ -9,97 +9,10 @@ from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import is_url\n-from scrapy.conf import settings\n+from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n \n-class ExecutionManager(object):\n-    \"\"\"Process a list of sites or urls.\n-\n-    This class should be used in a main for process a list of sites/urls.\n-\n-    It extracts products and could be used to store results in a database or\n-    just for testing spiders.\n-    \"\"\"\n-    def __init__(self):\n-        self.interrupted = False\n-        self.configured = False\n-        self.control_reactor = True\n-\n-    def configure(self, control_reactor=True):\n-        self.control_reactor = control_reactor\n-        if control_reactor:\n-            self._install_signals()\n-        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n-\n-        if not log.started:\n-            log.start()\n-        if not extensions.loaded:\n-            extensions.load()\n-        if not spiders.loaded:\n-            spiders.load()\n-        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n-            level=log.DEBUG)\n-\n-        scrapyengine.configure()\n-        self.configured = True\n-        \n-    def crawl(self, *args):\n-        \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n-\n-        requests = self._parse_args(args)\n-        # schedule initial requests to be scraped at engine start\n-        for domain in requests or ():\n-            spider = spiders.fromdomain(domain) \n-            for request in requests[domain]:\n-                scrapyengine.crawl(request, spider)\n-\n-    def runonce(self, *args):\n-        \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n-        if not self.configured:\n-            self.configure()\n-        self.crawl(*args)\n-        scrapyengine.start()\n-\n-    def start(self, control_reactor=True):\n-        \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n-        self.configure(control_reactor)\n-        scrapyengine.keep_alive = True\n-        scrapyengine.start(control_reactor=control_reactor)\n-        if control_reactor:\n-            self.stop()\n-\n-    def stop(self):\n-        \"\"\"Stop the scrapy server, shutting down the execution engine\"\"\"\n-        self.interrupted = True\n-        scrapyengine.stop()\n-        log.log_level = -999 # disable logging\n-        if self.control_reactor:\n-            signal.signal(signal.SIGTERM, signal.SIG_IGN)\n-            signal.signal(signal.SIGINT, signal.SIG_IGN)\n-            if hasattr(signal, \"SIGBREAK\"):\n-                signal.signal(signal.SIGBREAK, signal.SIG_IGN)\n-\n-    def reload_spiders(self):\n-        \"\"\"Reload all enabled spiders except for the ones that are currently\n-        running.\n-        \"\"\"\n-        spiders.reload(skip_domains=scrapyengine.open_domains)\n-\n-    def _install_signals(self):\n-        def sig_handler_terminate(signalinfo, param):\n-            log.msg('Received shutdown request, waiting for deferreds to finish...', log.INFO)\n-            self.stop()\n-\n-        signal.signal(signal.SIGTERM, sig_handler_terminate)\n-        # only handle SIGINT if there isn't already a handler (e.g. for Pdb)\n-        if signal.getsignal(signal.SIGINT) == signal.default_int_handler:\n-            signal.signal(signal.SIGINT, sig_handler_terminate)\n-        # Catch Ctrl-Break in windows\n-        if hasattr(signal, \"SIGBREAK\"):\n-            signal.signal(signal.SIGBREAK, sig_handler_terminate)\n-\n-    def _parse_args(self, args):\n-        \"\"\"Parse crawl arguments and return a dict of domains -> list of \n-        requests\"\"\"\n+def _parse_args(args):\n+    \"\"\"Parse crawl arguments and return a dict of domains -> list of requests\"\"\"\n     requests, urls, sites = set(), set(), set()\n     for a in args:\n         if isinstance(a, Request):\n@@ -138,4 +51,84 @@ class ExecutionManager(object):\n         perdomain.setdefault(spider.domain_name, []).append(request)\n     return perdomain\n \n+\n+class ExecutionManager(object):\n+    \"\"\"Process a list of sites or urls.\n+\n+    This class should be used in a main for process a list of sites/urls.\n+\n+    It extracts products and could be used to store results in a database or\n+    just for testing spiders.\n+    \"\"\"\n+    def __init__(self):\n+        self.interrupted = False\n+        self.configured = False\n+        self.control_reactor = True\n+\n+    def configure(self, control_reactor=True):\n+        self.control_reactor = control_reactor\n+        if control_reactor:\n+            install_shutdown_handlers(self._signal_shutdown)\n+        reactor.addSystemEventTrigger('before', 'shutdown', scrapyengine.stop)\n+\n+        if not log.started:\n+            log.start()\n+        if not extensions.loaded:\n+            extensions.load()\n+        if not spiders.loaded:\n+            spiders.load()\n+        log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()),\n+            level=log.DEBUG)\n+\n+        scrapyengine.configure()\n+        self.configured = True\n+        \n+    def crawl(self, *args):\n+        \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n+\n+        requests = _parse_args(args)\n+        # schedule initial requests to be scraped at engine start\n+        for domain in requests or ():\n+            spider = spiders.fromdomain(domain) \n+            for request in requests[domain]:\n+                scrapyengine.crawl(request, spider)\n+\n+    def runonce(self, *args):\n+        \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n+        assert self.configured, \"Scrapy Manger not yet configured\"\n+        self.crawl(*args)\n+        scrapyengine.start()\n+        if self.control_reactor:\n+            reactor.run(installSignalHandlers=False)\n+\n+    def start(self):\n+        \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n+        assert self.configured, \"Scrapy Manger not yet configured\"\n+        scrapyengine.keep_alive = True\n+        scrapyengine.start()\n+        if self.control_reactor:\n+            reactor.run(installSignalHandlers=False)\n+\n+    def stop(self):\n+        \"\"\"Stop the scrapy server, shutting down the execution engine\"\"\"\n+        self.interrupted = True\n+        scrapyengine.stop()\n+        if self.control_reactor and reactor.running:\n+            reactor.stop()\n+\n+    def _signal_shutdown(self, signum, _):\n+        signame = signal_names[signum]\n+        log.msg(\"Received %s, shutting down gracefully. Send again to force \" \\\n+            \"unclean shutdown\" % signame, level=log.INFO)\n+        reactor.callFromThread(self.stop)\n+        install_shutdown_handlers(self._signal_kill)\n+\n+    def _signal_kill(self, signum, _):\n+        signame = signal_names[signum]\n+        log.msg('Received %s twice, forcing unclean shutdown' % signame, \\\n+            level=log.INFO)\n+        log.log_level = log.SILENT # disable logging of confusing tracebacks\n+        reactor.callFromThread(scrapyengine.kill)\n+        install_shutdown_handlers(signal.SIG_IGN)\n+\n scrapymanager = ExecutionManager()\n\n@@ -56,10 +56,11 @@ def start(logfile=None, loglevel=None, logstdout=None):\n \n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n+    if level > log_level:\n+        return\n     dispatcher.send(signal=logmessage_received, message=message, level=level, \\\n         domain=domain)\n     system = domain if domain else component\n-    if level <= log_level:\n     msg_txt = unicode_to_str(\"%s: %s\" % (level_names[level], message))\n     log.msg(msg_txt, system=system)\n \n@@ -67,8 +68,12 @@ def exc(message, level=ERROR, component=BOT_NAME, domain=None):\n     message = message + '\\n' + format_exc()\n     msg(message, level, component, domain)\n \n-def err(*args, **kwargs):\n+def err(_stuff=None, _why=None, **kwargs):\n+    if ERROR > log_level:\n+        return\n     domain = kwargs.pop('domain', None)\n     component = kwargs.pop('component', BOT_NAME)\n     kwargs['system'] = domain if domain else component\n-    log.err(*args, **kwargs)\n+    if _why:\n+        _why = unicode_to_str(\"ERROR: %s\" % _why)\n+    log.err(_stuff, _why, **kwargs)\n\n@@ -0,0 +1,28 @@\n+\n+from __future__ import absolute_import\n+\n+from twisted.internet import reactor\n+\n+import signal\n+\n+signal_names = {}\n+for signame in dir(signal):\n+    if signame.startswith(\"SIG\"):\n+        signum = getattr(signal, signame)\n+        if isinstance(signum, int):\n+            signal_names[signum] = signame\n+\n+def install_shutdown_handlers(function, override_sigint=True):\n+    \"\"\"Install the given function as a signal handler for all common shutdown\n+    signals (such as SIGINT, SIGTERM, etc). If override_sigint is ``False`` the\n+    SIGINT handler won't be install if there is already a handler in place\n+    (e.g.  Pdb)\n+    \"\"\"\n+    reactor._handleSignals()\n+    signal.signal(signal.SIGTERM, function)\n+    if signal.getsignal(signal.SIGINT) == signal.default_int_handler or \\\n+            override_sigint:\n+        signal.signal(signal.SIGINT, function)\n+    # Catch Ctrl-Break in windows\n+    if hasattr(signal, \"SIGBREAK\"):\n+        signal.signal(signal.SIGBREAK, function)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
