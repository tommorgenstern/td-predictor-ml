{"custom_id": "scrapy#d4b2b612551918647148013893da4cfa83fa2e7a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 20 | Churn Cumulative: 93 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,17 +14,15 @@ logger = logging.getLogger(__name__)\n \n class UrlLengthMiddleware:\n \n-    def __init__(self, maxlength, stats):\n+    def __init__(self, maxlength):\n         self.maxlength = maxlength\n-        self.stats = stats\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n-        settings = crawler.settings\n+    def from_settings(cls, settings):\n         maxlength = settings.getint('URLLENGTH_LIMIT')\n         if not maxlength:\n             raise NotConfigured\n-        return cls(maxlength, crawler.stats)\n+        return cls(maxlength)\n \n     def process_spider_output(self, response, result, spider):\n         def _filter(request):\n@@ -34,7 +32,7 @@ class UrlLengthMiddleware:\n                     {'maxlength': self.maxlength, 'url': request.url},\n                     extra={'spider': spider}\n                 )\n-                self.stats.inc_value('urllength/request_ignored_count', spider=spider)\n+                spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n                 return False\n             else:\n                 return True\n\n@@ -5,7 +5,6 @@ from testfixtures import LogCapture\n from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n-from scrapy.statscollectors import StatsCollector\n from scrapy.utils.test import get_crawler\n \n \n@@ -14,21 +13,16 @@ class TestUrlLengthMiddleware(TestCase):\n     def setUp(self):\n         crawler = get_crawler(Spider)\n         self.spider = crawler._create_spider('foo')\n-\n-        self.stats = StatsCollector(crawler)\n-        self.stats.open_spider(self.spider)\n+        self.stats = self.spider.crawler.stats\n \n         self.maxlength = 25\n-        self.mw = UrlLengthMiddleware(maxlength=self.maxlength, stats=self.stats)\n+        self.mw = UrlLengthMiddleware(maxlength=self.maxlength)\n \n         self.response = Response('http://scrapytest.org')\n         self.short_url_req = Request('http://scrapytest.org/')\n         self.long_url_req = Request('http://scrapytest.org/this_is_a_long_url')\n         self.reqs = [self.short_url_req, self.long_url_req]\n \n-    def tearDown(self):\n-        self.stats.close_spider(self.spider, '')\n-\n     def process_spider_output(self):\n         return list(self.mw.process_spider_output(self.response, self.reqs, self.spider))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f254a6afbc3ad4a42048ea67acafdd035ba690a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 68 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,17 +6,19 @@ from scrapy.spidermiddlewares.urllength import UrlLengthMiddleware\n from scrapy.http import Response, Request\n from scrapy.spiders import Spider\n from scrapy.utils.test import get_crawler\n+from scrapy.settings import Settings\n \n \n class TestUrlLengthMiddleware(TestCase):\n \n     def setUp(self):\n+        self.maxlength = 25\n+        settings = Settings({'URLLENGTH_LIMIT': self.maxlength})\n+\n         crawler = get_crawler(Spider)\n         self.spider = crawler._create_spider('foo')\n         self.stats = self.spider.crawler.stats\n-\n-        self.maxlength = 25\n-        self.mw = UrlLengthMiddleware(maxlength=self.maxlength)\n+        self.mw = UrlLengthMiddleware.from_settings(settings)\n \n         self.response = Response('http://scrapytest.org')\n         self.short_url_req = Request('http://scrapytest.org/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0f3ca193873cd4dbf4de730dcceb38967efcc16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 385 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -9,8 +9,8 @@ RETRY_HTTP_CODES - which HTTP response codes to retry\n Failed pages are collected on the scraping process and rescheduled at the end,\n once the spider has finished crawling all regular (non failed) pages.\n \"\"\"\n-import logging\n-from inspect import isclass\n+from logging import getLogger, Logger\n+from typing import Optional, Union\n \n from twisted.internet import defer\n from twisted.internet.error import (\n@@ -24,24 +24,26 @@ from twisted.internet.error import (\n )\n from twisted.web.client import ResponseFailed\n \n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.response import response_status_message\n from scrapy.core.downloader.handlers.http11 import TunnelError\n+from scrapy.exceptions import NotConfigured\n+from scrapy.http.request import Request\n+from scrapy.spiders import Spider\n from scrapy.utils.python import global_object_name\n+from scrapy.utils.response import response_status_message\n \n \n-retry_logger = logging.getLogger(__name__)\n+retry_logger = getLogger(__name__)\n \n \n def get_retry_request(\n-    request,\n+    request: Request,\n     *,\n-    spider,\n-    reason='unspecified',\n-    max_retry_times=None,\n-    priority_adjust=None,\n-    logger=retry_logger,\n-    stats_base_key='retry',\n+    spider: Spider,\n+    reason: Union[str, Exception] = 'unspecified',\n+    max_retry_times: Optional[int] = None,\n+    priority_adjust: Union[int, float, None] = None,\n+    logger: Logger = retry_logger,\n+    stats_base_key: str = 'retry',\n ):\n     \"\"\"\n     Returns a new :class:`~scrapy.Request` object to retry the specified\n@@ -105,7 +107,7 @@ def get_retry_request(\n             priority_adjust = settings.getint('RETRY_PRIORITY_ADJUST')\n         new_request.priority = request.priority + priority_adjust\n \n-        if isclass(reason):\n+        if callable(reason):\n             reason = reason()\n         if isinstance(reason, Exception):\n             reason = global_object_name(reason.__class__)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9cc4513bd60dcebcbfc53035482eff82d2b7acc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 70 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class TestUrlLengthMiddleware(TestCase):\n \n         crawler = get_crawler(Spider)\n         self.spider = crawler._create_spider('foo')\n-        self.stats = self.spider.crawler.stats\n+        self.stats = crawler.stats\n         self.mw = UrlLengthMiddleware.from_settings(settings)\n \n         self.response = Response('http://scrapytest.org')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2f61d7cc034e6ba6ba6ae9ccfc3cf2f1021b857e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 315 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -70,8 +70,6 @@ class H2ConnectionPool:\n             d = pending_requests.popleft()\n             d.callback(conn)\n \n-        del pending_requests\n-\n         return conn\n \n     def _remove_connection(self, errors: List[BaseException], key: Tuple) -> None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#42e4dbb23de238e1252eb50b059666494418588f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 602 | Contributors (this commit): 28 | Commits (past 90d): 2 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -85,6 +85,7 @@ setup(\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n         'Programming Language :: Python :: 3.8',\n+        'Programming Language :: Python :: 3.9',\n         'Programming Language :: Python :: Implementation :: CPython',\n         'Programming Language :: Python :: Implementation :: PyPy',\n         'Topic :: Internet :: WWW/HTTP',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94201612bcad7b74c60a2a7ab70f40ba87714ca8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 391 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -55,14 +55,12 @@ def get_retry_request(\n \n         def parse(self, response):\n             if not response.text:\n-                new_request = get_retry_request(\n+                new_request_or_none = get_retry_request(\n                     response.request,\n                     spider=self,\n                     reason='empty',\n                 )\n-                if new_request:\n-                    yield new_request\n-                return\n+                return new_request_or_none\n \n     *spider* is the :class:`~scrapy.Spider` instance which is asking for the\n     retry request. It is used to access the :ref:`settings <topics-settings>`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#308a58aa275aa514397351caa263e08de2f89adc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 10 | Files Changed: 6 | Hunks: 24 | Methods Changed: 3 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 62 | Churn Cumulative: 4569 | Contributors (this commit): 39 | Commits (past 90d): 6 | Contributors (cumulative): 90 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,9 +9,8 @@ from contextlib import suppress\n from io import BytesIO\n \n from itemadapter import ItemAdapter\n-from PIL import Image\n \n-from scrapy.exceptions import DropItem\n+from scrapy.exceptions import DropItem, NotConfigured\n from scrapy.http import Request\n from scrapy.pipelines.files import FileException, FilesPipeline\n # TODO: from scrapy.pipelines.media import MediaPipeline\n@@ -45,6 +44,14 @@ class ImagesPipeline(FilesPipeline):\n     DEFAULT_IMAGES_RESULT_FIELD = 'images'\n \n     def __init__(self, store_uri, download_func=None, settings=None):\n+        try:\n+            from PIL import Image\n+            self._Image = Image\n+        except ImportError:\n+            raise NotConfigured(\n+                'ImagesPipeline requires installing Pillow 4.0.0 or later'\n+            )\n+\n         super().__init__(store_uri, settings=settings, download_func=download_func)\n \n         if isinstance(settings, dict) or settings is None:\n@@ -121,7 +128,7 @@ class ImagesPipeline(FilesPipeline):\n \n     def get_images(self, response, request, info, *, item=None):\n         path = self.file_path(request, response=response, info=info, item=item)\n-        orig_image = Image.open(BytesIO(response.body))\n+        orig_image = self._Image.open(BytesIO(response.body))\n \n         width, height = orig_image.size\n         if width < self.min_width or height < self.min_height:\n@@ -139,12 +146,12 @@ class ImagesPipeline(FilesPipeline):\n \n     def convert_image(self, image, size=None):\n         if image.format == 'PNG' and image.mode == 'RGBA':\n-            background = Image.new('RGBA', image.size, (255, 255, 255))\n+            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode == 'P':\n             image = image.convert(\"RGBA\")\n-            background = Image.new('RGBA', image.size, (255, 255, 255))\n+            background = self._Image.new('RGBA', image.size, (255, 255, 255))\n             background.paste(image, image)\n             image = background.convert('RGB')\n         elif image.mode != 'RGB':\n@@ -152,7 +159,7 @@ class ImagesPipeline(FilesPipeline):\n \n         if size:\n             image = image.copy()\n-            image.thumbnail(size, Image.ANTIALIAS)\n+            image.thumbnail(size, self._Image.ANTIALIAS)\n \n         buf = BytesIO()\n         image.save(buf, 'JPEG')\n\n@@ -17,6 +17,8 @@ from threading import Timer\n from unittest import skipIf\n \n from pytest import mark\n+from twisted import version as twisted_version\n+from twisted.python.versions import Version\n from twisted.trial import unittest\n \n import scrapy\n@@ -630,6 +632,7 @@ class MySpider(scrapy.Spider):\n \n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n     @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n+    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n     def test_custom_asyncio_loop_enabled_true(self):\n         log = self.get_log(self.debug_log_spider, args=[\n             '-s',\n\n@@ -8,7 +8,9 @@ from unittest import skipIf\n \n from pytest import raises, mark\n from testfixtures import LogCapture\n+from twisted import version as twisted_version\n from twisted.internet import defer\n+from twisted.python.versions import Version\n from twisted.trial import unittest\n \n import scrapy\n@@ -358,6 +360,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n     @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n+    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n     def test_custom_loop_asyncio(self):\n         log = self.run_script(\"asyncio_custom_loop.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -366,6 +369,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     @mark.skipif(sys.implementation.name == \"pypy\", reason=\"uvloop does not support pypy properly\")\n     @mark.skipif(platform.system() == \"Windows\", reason=\"uvloop does not support Windows\")\n+    @mark.skipif(twisted_version == Version('twisted', 21, 2, 0), reason='https://twistedmatrix.com/trac/ticket/10106')\n     def test_custom_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\", \"uvloop.Loop\")\n         self.assertIn(\"Spider closed (finished)\", log)\n\n@@ -180,7 +180,18 @@ class FileDownloadCrawlTestCase(TestCase):\n         self.assertEqual(crawler.stats.get_value('downloader/response_status_count/302'), 3)\n \n \n+try:\n+    from PIL import Image  # noqa: imported just to check for the import error\n+except ImportError:\n+    skip_pillow = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n+else:\n+    skip_pillow = None\n+\n+\n class ImageDownloadCrawlTestCase(FileDownloadCrawlTestCase):\n+\n+    skip = skip_pillow\n+\n     pipeline_class = 'scrapy.pipelines.images.ImagesPipeline'\n     store_setting_key = 'IMAGES_STORE'\n     media_key = 'images'\n\n@@ -23,15 +23,16 @@ except ImportError:\n     dataclass_field = None\n \n \n-skip = False\n try:\n     from PIL import Image\n except ImportError:\n-    skip = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n+    skip_pillow = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n else:\n     encoders = {'jpeg_encoder', 'jpeg_decoder'}\n     if not encoders.issubset(set(Image.core.__dict__)):\n-        skip = 'Missing JPEG encoders'\n+        skip_pillow = 'Missing JPEG encoders'\n+    else:\n+        skip_pillow = None\n \n \n def _mocked_download_func(request, info):\n@@ -41,7 +42,7 @@ def _mocked_download_func(request, info):\n \n class ImagesPipelineTestCase(unittest.TestCase):\n \n-    skip = skip\n+    skip = skip_pillow\n \n     def setUp(self):\n         self.tempdir = mkdtemp()\n@@ -137,6 +138,8 @@ class DeprecatedImagesPipeline(ImagesPipeline):\n \n class ImagesPipelineTestCaseFieldsMixin:\n \n+    skip = skip_pillow\n+\n     def test_item_fields_default(self):\n         url = 'http://www.example.com/images/1.jpg'\n         item = self.item_class(name='item1', image_urls=[url])\n@@ -221,6 +224,9 @@ class ImagesPipelineTestCaseFieldsAttrsItem(ImagesPipelineTestCaseFieldsMixin, u\n \n \n class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n+\n+    skip = skip_pillow\n+\n     img_cls_attribute_names = [\n         # Pipeline attribute names with corresponding setting names.\n         (\"EXPIRES\", \"IMAGES_EXPIRES\"),\n\n@@ -1,3 +1,5 @@\n+from typing import Optional\n+\n from testfixtures import LogCapture\n from twisted.trial import unittest\n from twisted.python.failure import Failure\n@@ -17,6 +19,14 @@ from scrapy.utils.signal import disconnect_all\n from scrapy import signals\n \n \n+try:\n+    from PIL import Image  # noqa: imported just to check for the import error\n+except ImportError:\n+    skip_pillow: Optional[str] = 'Missing Python Imaging Library, install https://pypi.python.org/pypi/Pillow'\n+else:\n+    skip_pillow = None\n+\n+\n def _mocked_download_func(request, info):\n     response = request.meta.get('response')\n     return response() if callable(response) else response\n@@ -379,6 +389,7 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n \n \n class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n+    skip = skip_pillow\n \n     def setUp(self):\n         self.pipe = MockedMediaPipelineDeprecatedMethods(store_uri='store-uri', download_func=_mocked_download_func)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2973d8d51abbda4839981c192a366e907e2ad39e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 670 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,7 +69,7 @@ class Selector(_ParselSelector, object_ref):\n             raise ValueError(f'{self.__class__.__name__}.__init__() received '\n                              'both response and text')\n \n-        st = _st(response, type or self._default_type)\n+        st = _st(response, type)\n \n         if text is not None:\n             response = _response_from_text(text, st)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec5a7918ec2d8888ba356f9e3295dcd1ee935884", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 20 | Churn Cumulative: 4319 | Contributors (this commit): 37 | Commits (past 90d): 4 | Contributors (cumulative): 56 | DMM Complexity: 1.0\n\nDIFF:\n@@ -358,10 +358,19 @@ class ScrapyAgent:\n         request.meta['download_latency'] = time() - start_time\n         return result\n \n+    @staticmethod\n+    def _headers_from_twisted_response(response):\n+        headers = Headers()\n+        if response.length is not None:\n+            headers[b'Content-Length'] = str(response.length).encode()\n+        for key, value in response.headers.getAllRawHeaders():\n+            headers[key] = value\n+        return headers\n+\n     def _cb_bodyready(self, txresponse, request):\n         headers_received_result = self._crawler.signals.send_catch_log(\n             signal=signals.headers_received,\n-            headers=Headers(txresponse.headers.getAllRawHeaders()),\n+            headers=self._headers_from_twisted_response(txresponse),\n             body_length=txresponse.length,\n             request=request,\n             spider=self._crawler.spider,\n@@ -435,7 +444,7 @@ class ScrapyAgent:\n         return d\n \n     def _cb_bodydone(self, result, request, url):\n-        headers = Headers(result[\"txresponse\"].headers.getAllRawHeaders())\n+        headers = self._headers_from_twisted_response(result[\"txresponse\"])\n         respcls = responsetypes.from_args(headers=headers, url=url, body=result[\"body\"])\n         try:\n             version = result[\"txresponse\"].version\n\n@@ -364,6 +364,13 @@ class HttpTestCase(unittest.TestCase):\n         d.addCallback(self.assertEqual, body)\n         return d\n \n+    def test_response_header_content_length(self):\n+        request = Request(self.getURL(\"file\"), method=b\"GET\")\n+        d = self.download_request(request, Spider(\"foo\"))\n+        d.addCallback(lambda r: r.headers[b'content-length'])\n+        d.addCallback(self.assertEqual, b'159')\n+        return d\n+\n \n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#72e8cea8afb85f6704fcf6f5648b0a01f1abaab3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 155 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 19 | Methods Changed: 31 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 182 | Churn Cumulative: 846 | Contributors (this commit): 28 | Commits (past 90d): 2 | Contributors (cumulative): 31 | DMM Complexity: 0.9901960784313726\n\nDIFF:\n@@ -9,7 +9,6 @@ from collections import deque\n from contextlib import contextmanager\n from importlib import import_module\n from pkgutil import iter_modules\n-from textwrap import dedent\n \n from w3lib.html import replace_entities\n \n@@ -227,7 +226,8 @@ def is_generator_with_return_value(callable):\n         return value is None or isinstance(value, ast.NameConstant) and value.value is None\n \n     if inspect.isgeneratorfunction(callable):\n-        tree = ast.parse(dedent(inspect.getsource(callable)))\n+        code = re.sub(r\"^[\\t ]+\", \"\", inspect.getsource(callable))\n+        tree = ast.parse(code)\n         for node in walk_callable(tree):\n             if isinstance(node, ast.Return) and not returns_none(node):\n                 _generator_callbacks_cache[callable] = True\n@@ -242,6 +242,7 @@ def warn_on_generator_with_return_value(spider, callable):\n     Logs a warning if a callable is a generator function and includes\n     a 'return' statement with a value different than None\n     \"\"\"\n+    try:\n         if is_generator_with_return_value(callable):\n             warnings.warn(\n                 f'The \"{spider.__class__.__name__}.{callable.__name__}\" method is '\n@@ -251,3 +252,13 @@ def warn_on_generator_with_return_value(spider, callable):\n                 'for details about the semantics of the \"return\" statement within generators',\n                 stacklevel=2,\n             )\n+    except IndentationError:\n+        callable_name = spider.__class__.__name__ + \".\" + callable.__name__\n+        warnings.warn(\n+            f'Unable to determine whether or not \"{callable_name}\" is a generator with a return value. '\n+            'This will not prevent your code from working, but it prevents Scrapy from detecting '\n+            f'potential issues in your implementation of \"{callable_name}\". Please, report this in the '\n+            'Scrapy issue tracker (https://github.com/scrapy/scrapy/issues), '\n+            f'including the code of \"{callable_name}\"',\n+            stacklevel=2,\n+        )\n\n@@ -1,35 +1,116 @@\n import unittest\n+import warnings\n+from unittest import mock\n \n-from scrapy.utils.misc import is_generator_with_return_value\n+from scrapy.utils.misc import is_generator_with_return_value, warn_on_generator_with_return_value\n+\n+\n+def _indentation_error(*args, **kwargs):\n+    raise IndentationError()\n+\n+\n+def top_level_return_something():\n+    \"\"\"\n+docstring\n+    \"\"\"\n+    url = \"\"\"\n+https://example.org\n+\"\"\"\n+    yield url\n+    return 1\n+\n+\n+def top_level_return_none():\n+    \"\"\"\n+docstring\n+    \"\"\"\n+    url = \"\"\"\n+https://example.org\n+\"\"\"\n+    yield url\n+    return\n+\n+\n+def generator_that_returns_stuff():\n+    yield 1\n+    yield 2\n+    return 3\n \n \n class UtilsMiscPy3TestCase(unittest.TestCase):\n \n-    def test_generators_with_return_statements(self):\n-        def f():\n+    def test_generators_return_something(self):\n+        def f1():\n             yield 1\n             return 2\n \n-        def g():\n+        def g1():\n             yield 1\n-            return 'asdf'\n+            return \"asdf\"\n \n-        def h():\n+        def h1():\n+            yield 1\n+\n+            def helper():\n+                return 0\n+\n+            yield helper()\n+            return 2\n+\n+        def i1():\n+            \"\"\"\n+docstring\n+            \"\"\"\n+            url = \"\"\"\n+https://example.org\n+        \"\"\"\n+            yield url\n+            return 1\n+\n+        assert is_generator_with_return_value(top_level_return_something)\n+        assert is_generator_with_return_value(f1)\n+        assert is_generator_with_return_value(g1)\n+        assert is_generator_with_return_value(h1)\n+        assert is_generator_with_return_value(i1)\n+\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, top_level_return_something)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('The \"NoneType.top_level_return_something\" method is a generator', str(w[0].message))\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, f1)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('The \"NoneType.f1\" method is a generator', str(w[0].message))\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, g1)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('The \"NoneType.g1\" method is a generator', str(w[0].message))\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, h1)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('The \"NoneType.h1\" method is a generator', str(w[0].message))\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, i1)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('The \"NoneType.i1\" method is a generator', str(w[0].message))\n+\n+    def test_generators_return_none(self):\n+        def f2():\n             yield 1\n             return None\n \n-        def i():\n+        def g2():\n             yield 1\n             return\n \n-        def j():\n+        def h2():\n             yield 1\n \n-        def k():\n+        def i2():\n             yield 1\n-            yield from g()\n+            yield from generator_that_returns_stuff()\n \n-        def m():\n+        def j2():\n             yield 1\n \n             def helper():\n@@ -37,20 +118,56 @@ class UtilsMiscPy3TestCase(unittest.TestCase):\n \n             yield helper()\n \n-        def n():\n-            yield 1\n+        def k2():\n+            \"\"\"\n+docstring\n+            \"\"\"\n+            url = \"\"\"\n+https://example.org\n+        \"\"\"\n+            yield url\n+            return\n \n-            def helper():\n-                return 0\n+        def l2():\n+            return\n \n-            yield helper()\n-            return 2\n+        assert not is_generator_with_return_value(top_level_return_none)\n+        assert not is_generator_with_return_value(f2)\n+        assert not is_generator_with_return_value(g2)\n+        assert not is_generator_with_return_value(h2)\n+        assert not is_generator_with_return_value(i2)\n+        assert not is_generator_with_return_value(j2)  # not recursive\n+        assert not is_generator_with_return_value(k2)  # not recursive\n+        assert not is_generator_with_return_value(l2)\n \n-        assert is_generator_with_return_value(f)\n-        assert is_generator_with_return_value(g)\n-        assert not is_generator_with_return_value(h)\n-        assert not is_generator_with_return_value(i)\n-        assert not is_generator_with_return_value(j)\n-        assert not is_generator_with_return_value(k)  # not recursive\n-        assert not is_generator_with_return_value(m)\n-        assert is_generator_with_return_value(n)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, top_level_return_none)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, f2)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, g2)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, h2)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, i2)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, j2)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, k2)\n+            self.assertEqual(len(w), 0)\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, l2)\n+            self.assertEqual(len(w), 0)\n+\n+    @mock.patch(\"scrapy.utils.misc.is_generator_with_return_value\", new=_indentation_error)\n+    def test_indentation_error(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            warn_on_generator_with_return_value(None, top_level_return_none)\n+            self.assertEqual(len(w), 1)\n+            self.assertIn('Unable to determine', str(w[0].message))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64d4ae1a19eda9c6bb0b6dc85e5dfff6187d0ca8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 299 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -16,7 +16,7 @@ class Command(BaseRunSpiderCommand):\n         if len(args) < 1:\n             raise UsageError()\n         elif len(args) > 1:\n-            raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n+            raise UsageError(\"running 'scrapy crawl' with more than one spider is not supported\")\n         spname = args[0]\n \n         crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c51ec1ae1cef3fd68fc512f0636b75d0f3a2da13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 146 | Files Changed: 9 | Hunks: 20 | Methods Changed: 32 | Complexity Δ (Sum/Max): -15/9 | Churn Δ: 247 | Churn Cumulative: 2159 | Contributors (this commit): 20 | Commits (past 90d): 26 | Contributors (cumulative): 41 | DMM Complexity: 0.5483870967741935\n\nDIFF:\n@@ -10,6 +10,7 @@ from twisted.python.failure import Failure\n \n from scrapy.exceptions import _InvalidOutput\n from scrapy.middleware import MiddlewareManager\n+from scrapy.utils.asyncgen import _process_iterable_universal\n from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.python import MutableAsyncChain, MutableChain\n@@ -57,31 +58,18 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         return scrape_func(response, request, spider)\n \n     def _evaluate_iterable(self, response, spider, iterable, exception_processor_index, recover_to):\n-        def _process_exception(ex):\n-            exception_result = self._process_spider_exception(response, spider, Failure(ex),\n-                                                              exception_processor_index)\n-            if isinstance(exception_result, Failure):\n-                raise  # pylint: disable=E0704\n-            recover_to.extend(exception_result)\n-\n-        def _evaluate_normal_iterable(iterable):\n-            try:\n-                for r in iterable:\n-                    yield r\n-            except Exception as ex:\n-                _process_exception(ex)\n-\n+        @_process_iterable_universal\n         async def _evaluate_async_iterable(iterable):\n             try:\n                 async for r in iterable:\n                     yield r\n             except Exception as ex:\n-                _process_exception(ex)\n-\n-        if inspect.isasyncgen(iterable):\n+                exception_result = self._process_spider_exception(response, spider, Failure(ex),\n+                                                                  exception_processor_index)\n+                if isinstance(exception_result, Failure):\n+                    raise\n+                recover_to.extend(exception_result)\n         return _evaluate_async_iterable(iterable)\n-        else:\n-            return _evaluate_normal_iterable(iterable)\n \n     def _process_spider_exception(self, response, spider, _failure, start_index=0):\n         exception = _failure.value\n\n@@ -7,6 +7,7 @@ See documentation in docs/topics/spider-middleware.rst\n import logging\n \n from scrapy.http import Request\n+from scrapy.utils.asyncgen import _process_iterable_universal\n \n logger = logging.getLogger(__name__)\n \n@@ -49,10 +50,15 @@ class DepthMiddleware:\n                                          spider=spider)\n             return True\n \n+        @_process_iterable_universal\n+        async def process(result):\n             # base case (depth=0)\n             if 'depth' not in response.meta:\n                 response.meta['depth'] = 0\n                 if self.verbose_stats:\n                     self.stats.inc_value('request_depth_count/0', spider=spider)\n \n-        return (r for r in result or () if _filter(r))\n+            async for r in result or ():\n+                if _filter(r):\n+                    yield r\n+        return process(result)\n\n@@ -9,6 +9,7 @@ import warnings\n \n from scrapy import signals\n from scrapy.http import Request\n+from scrapy.utils.asyncgen import _process_iterable_universal\n from scrapy.utils.httpobj import urlparse_cached\n \n logger = logging.getLogger(__name__)\n@@ -26,7 +27,9 @@ class OffsiteMiddleware:\n         return o\n \n     def process_spider_output(self, response, result, spider):\n-        for x in result:\n+        @_process_iterable_universal\n+        async def process(result):\n+            async for x in result:\n                 if isinstance(x, Request):\n                     if x.dont_filter or self.should_follow(x, spider):\n                         yield x\n@@ -41,6 +44,7 @@ class OffsiteMiddleware:\n                         self.stats.inc_value('offsite/filtered', spider=spider)\n                 else:\n                     yield x\n+        return process(result)\n \n     def should_follow(self, request, spider):\n         regex = self.host_regex\n\n@@ -10,6 +10,7 @@ from w3lib.url import safe_url_string\n from scrapy.http import Request, Response\n from scrapy.exceptions import NotConfigured\n from scrapy import signals\n+from scrapy.utils.asyncgen import _process_iterable_universal\n from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n from scrapy.utils.url import strip_url\n@@ -337,7 +338,13 @@ class RefererMiddleware:\n                 if referrer is not None:\n                     r.headers.setdefault('Referer', referrer)\n             return r\n-        return (_set_referer(r) for r in result or ())\n+\n+        @_process_iterable_universal\n+        async def process(result):\n+            async for r in result or ():\n+                yield _set_referer(r)\n+\n+        return process(result)\n \n     def request_scheduled(self, request, spider):\n         # check redirected request to patch \"Referer\" header if necessary\n\n@@ -8,6 +8,7 @@ import logging\n \n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.asyncgen import _process_iterable_universal\n \n logger = logging.getLogger(__name__)\n \n@@ -34,4 +35,10 @@ class UrlLengthMiddleware:\n             else:\n                 return True\n \n-        return (r for r in result or () if _filter(r))\n+        @_process_iterable_universal\n+        async def process(result):\n+            async for r in result or ():\n+                if _filter(r):\n+                    yield r\n+\n+        return process(result)\n\n@@ -1,4 +1,6 @@\n import collections\n+import functools\n+import inspect\n \n \n async def collect_asyncgen(result):\n@@ -15,3 +17,46 @@ async def as_async_generator(it):\n     else:\n         for r in it:\n             yield r\n+\n+\n+# https://stackoverflow.com/a/66170760/113586\n+def _process_iterable_universal(process_async):\n+    \"\"\" Takes a function that takes an async iterable, args and kwargs. Returns\n+    a function that takes any iterable, args and kwargs.\n+\n+    Requires that process_async only awaits on the iterable and synchronous functions,\n+    so it's better to use this only in the Scrapy code itself.\n+    \"\"\"\n+\n+    # If this stops working, all internal uses can be just replaced with manually-written\n+    # process_sync functions.\n+\n+    def process_sync(iterable, *args, **kwargs):\n+        agen = process_async(as_async_generator(iterable), *args, **kwargs)\n+        if not inspect.isasyncgen(agen):\n+            raise ValueError(f\"process_async returned wrong type {type(agen)}\")\n+        sent = None\n+        while True:\n+            try:\n+                gen = agen.asend(sent)\n+                gen.send(None)\n+            except StopIteration as e:\n+                sent = yield e.value\n+            except StopAsyncIteration:\n+                return\n+            else:\n+                gen.throw(RuntimeError,\n+                          f\"Synchronously-called function '{process_async.__name__}' has blocked, \"\n+                          f\"you can't use {_process_iterable_universal.__name__} with it.\")\n+\n+    @functools.wraps(process_async)\n+    def process(iterable, *args, **kwargs):\n+        if inspect.isasyncgen(iterable):\n+            # call process_async directly\n+            return process_async(iterable, *args, **kwargs)\n+        if hasattr(iterable, '__iter__'):\n+            # convert process_async to process_sync\n+            return process_sync(iterable, *args, **kwargs)\n+        raise ValueError(f\"Wrong iterable type {type(iterable)}\")\n+\n+    return process\n\n@@ -1,35 +0,0 @@\n-# coding: utf-8\n-import inspect\n-\n-\n-def process_normal_iterable_helper(it, in_predicate=None, out_predicate=None, processor=None):\n-    for o in it:\n-        if in_predicate and not in_predicate(o):\n-            continue\n-        if processor is not None:\n-            o = processor(o)\n-        if out_predicate and not out_predicate(o):\n-            continue\n-        yield o\n-\n-\n-async def process_async_iterable_helper(it, in_predicate=None, out_predicate=None, processor=None):\n-    async for o in it:\n-        if in_predicate and not in_predicate(o):\n-            continue\n-        if processor is not None:\n-            o = processor(o)\n-        if out_predicate and not out_predicate(o):\n-            continue\n-        yield o\n-\n-\n-def process_iterable_helper(it, in_predicate=None, out_predicate=None, processor=None):\n-    \"\"\"\n-    For each item in the iterable: skips it if in_predicate is False, applies processor,\n-    skips the result if out_predicate is False, else yields it.\n-    \"\"\"\n-    if inspect.isasyncgen(it):\n-        return process_async_iterable_helper(it, in_predicate, out_predicate, processor)\n-    else:\n-        return process_normal_iterable_helper(it, in_predicate, out_predicate, processor)\n\n@@ -1,6 +1,6 @@\n from twisted.trial import unittest\n \n-from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen, _process_iterable_universal\n from scrapy.utils.defer import deferred_f_from_coro_f\n \n \n@@ -18,3 +18,23 @@ class AsyncgenUtilsTest(unittest.TestCase):\n         ag = as_async_generator(range(42))\n         results = await collect_asyncgen(ag)\n         self.assertEqual(results, list(range(42)))\n+\n+\n+@_process_iterable_universal\n+async def process_iterable(iterable):\n+    async for i in iterable:\n+        yield i * 2\n+\n+\n+class ProcessIterableUniversalTest(unittest.TestCase):\n+\n+    def test_normal(self):\n+        iterable = iter([1, 2, 3])\n+        results = list(process_iterable(iterable))\n+        self.assertEqual(results, [2, 4, 6])\n+\n+    @deferred_f_from_coro_f\n+    async def test_async(self):\n+        iterable = as_async_generator([1, 2, 3])\n+        results = await collect_asyncgen(process_iterable(iterable))\n+        self.assertEqual(results, [2, 4, 6])\n\n@@ -1,87 +0,0 @@\n-import collections\n-\n-from twisted.trial import unittest\n-\n-from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n-from scrapy.utils.defer import deferred_f_from_coro_f\n-from scrapy.utils.middlewares import process_iterable_helper\n-\n-\n-def predicate1(o):\n-    return bool(o % 2)\n-\n-\n-def predicate2(o):\n-    return o < 10\n-\n-\n-def processor(o):\n-    return o * 2\n-\n-\n-class ProcessIterableHelperNormalTest(unittest.TestCase):\n-\n-    def test_normal_in_predicate(self):\n-        iterable1 = iter([1, 2, 3])\n-        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1)\n-        self.assertIsInstance(iterable2, collections.abc.Iterable)\n-        list2 = list(iterable2)\n-        self.assertEqual(list2, [1, 3])\n-\n-    def test_normal_out_predicate(self):\n-        iterable1 = iter([1, 2, 10, 3, 15])\n-        iterable2 = process_iterable_helper(iterable1, out_predicate=predicate2)\n-        self.assertIsInstance(iterable2, collections.abc.Iterable)\n-        list2 = list(iterable2)\n-        self.assertEqual(list2, [1, 2, 3])\n-\n-    def test_normal_processor(self):\n-        iterable1 = iter([1, 2, 3])\n-        iterable2 = process_iterable_helper(iterable1, processor=processor)\n-        self.assertIsInstance(iterable2, collections.abc.Iterable)\n-        list2 = list(iterable2)\n-        self.assertEqual(list2, [2, 4, 6])\n-\n-    def test_normal_combined(self):\n-        iterable1 = iter([1, 2, 10, 3, 6, 18, 5, 15])\n-        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1,\n-                                            out_predicate=predicate2, processor=processor)\n-        self.assertIsInstance(iterable2, collections.abc.Iterable)\n-        list2 = list(iterable2)\n-        self.assertEqual(list2, [2, 6])\n-\n-\n-class ProcessIterableHelperAsyncTest(unittest.TestCase):\n-\n-    @deferred_f_from_coro_f\n-    async def test_async_in_predicate(self):\n-        iterable1 = as_async_generator([1, 2, 3])\n-        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1)\n-        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n-        list2 = await collect_asyncgen(iterable2)\n-        self.assertEqual(list2, [1, 3])\n-\n-    @deferred_f_from_coro_f\n-    async def test_async_out_predicate(self):\n-        iterable1 = as_async_generator([1, 2, 10, 3, 15])\n-        iterable2 = process_iterable_helper(iterable1, out_predicate=predicate2)\n-        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n-        list2 = await collect_asyncgen(iterable2)\n-        self.assertEqual(list2, [1, 2, 3])\n-\n-    @deferred_f_from_coro_f\n-    async def test_async_processor(self):\n-        iterable1 = as_async_generator([1, 2, 3])\n-        iterable2 = process_iterable_helper(iterable1, processor=processor)\n-        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n-        list2 = await collect_asyncgen(iterable2)\n-        self.assertEqual(list2, [2, 4, 6])\n-\n-    @deferred_f_from_coro_f\n-    async def test_async_combined(self):\n-        iterable1 = as_async_generator([1, 2, 10, 3, 6, 18, 5, 15])\n-        iterable2 = process_iterable_helper(iterable1, in_predicate=predicate1,\n-                                            out_predicate=predicate2, processor=processor)\n-        self.assertIsInstance(iterable2, collections.abc.AsyncIterable)\n-        list2 = await collect_asyncgen(iterable2)\n-        self.assertEqual(list2, [2, 6])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6034f942b034946538855cc53644b5a89ba081a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 68 | Churn Cumulative: 174 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,14 +1,18 @@\n+import random\n+\n from pytest import mark\n from twisted.trial import unittest\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n-from scrapy.utils.asyncgen import collect_asyncgen\n+from scrapy.utils.asyncgen import collect_asyncgen, as_async_generator\n from scrapy.utils.defer import (\n     aiter_errback,\n     deferred_f_from_coro_f,\n     iter_errback,\n+    maybe_deferred_to_future,\n     mustbe_deferred,\n+    parallel_async,\n     process_chain,\n     process_chain_both,\n     process_parallel,\n@@ -164,3 +168,65 @@ class AsyncDefTestsuiteTest(unittest.TestCase):\n     @deferred_f_from_coro_f\n     async def test_deferred_f_from_coro_f_xfail(self):\n         raise Exception(\"This is expected to be raised\")\n+\n+\n+class AsyncCooperatorTest(unittest.TestCase):\n+    \"\"\" This tests _AsyncCooperatorAdapter by testing parallel_async which is its only usage.\n+\n+    parallel_async is called with the results of a callback (so an iterable of items, requests and None,\n+    with arbitrary delays between values), and it uses Scraper._process_spidermw_output as the callable\n+    (so a callable that returns a Deferred for an item, which will fire after pipelines process it, and\n+    None for everything else). The concurrent task count is the CONCURRENT_ITEMS setting.\n+\n+    We want to test different concurrency values compared to the iterable length.\n+    We also want to simulate the real usage, with arbitrary delays between getting the values\n+    from the iterable. We also want to simulate sync and async results from the callable.\n+    \"\"\"\n+    CONCURRENT_ITEMS = 50\n+\n+    @staticmethod\n+    def callable(o, results):\n+        if random.random() < 0.4:\n+            # simulate async processing\n+            dfd = defer.Deferred()\n+            dfd.addCallback(lambda _: results.append(o))\n+            delay = random.random() / 8\n+            reactor.callLater(delay, dfd.callback, None)\n+            return dfd\n+        else:\n+            # simulate trivial sync processing\n+            results.append(o)\n+\n+    @staticmethod\n+    def get_async_iterable(length):\n+        # simulate a simple callback without delays between results\n+        return as_async_generator(range(length))\n+\n+    @staticmethod\n+    async def get_async_iterable_with_delays(length):\n+        # simulate a callback with delays between some of the results\n+        for i in range(length):\n+            if random.random() < 0.1:\n+                dfd = defer.Deferred()\n+                delay = random.random() / 20\n+                reactor.callLater(delay, dfd.callback, None)\n+                await maybe_deferred_to_future(dfd)\n+            yield i\n+\n+    @defer.inlineCallbacks\n+    def test_simple(self):\n+        for length in [20, 50, 100]:\n+            results = []\n+            ait = self.get_async_iterable(length)\n+            dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n+            yield dl\n+            self.assertEqual(list(range(length)), sorted(results))\n+\n+    @defer.inlineCallbacks\n+    def test_delays(self):\n+        for length in [20, 50, 100]:\n+            results = []\n+            ait = self.get_async_iterable_with_delays(length)\n+            dl = parallel_async(ait, self.CONCURRENT_ITEMS, self.callable, results)\n+            yield dl\n+            self.assertEqual(list(range(length)), sorted(results))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
