{"custom_id": "scrapy#70c82d33c00538228314dd6cef0253b70f8627e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 48 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -37,6 +37,8 @@ class HttpCompressionMiddleware:\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n+                spider.crawler.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n+                spider.crawler.stats.inc_value('httpcompression/response_count', spider=spider)\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#008cf1c75ebe72a607149d7efd8f902f8763bc52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 23 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 23 | Churn Cumulative: 2012 | Contributors (this commit): 26 | Commits (past 90d): 16 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -868,29 +868,6 @@ class S3TestCase(unittest.TestCase):\n         self.assertEqual(httpreq.headers['Authorization'],\n                          b'AWS 0PN5J17HBGZHT7JJ3X82:thdUi9VAkzhkniLj96JIrOPGi0g=')\n \n-    def test_request_signing5(self):\n-        try:\n-            import botocore  # noqa: F401\n-        except ImportError:\n-            pass\n-        else:\n-            raise unittest.SkipTest(\n-                'botocore does not support overriding date with x-amz-date')\n-        # deletes an object from the 'johnsmith' bucket using the\n-        # path-style and Date alternative.\n-        date = 'Tue, 27 Mar 2007 21:20:27 +0000'\n-        req = Request(\n-            's3://johnsmith/photos/puppy.jpg', method='DELETE', headers={\n-                'Date': date,\n-                'x-amz-date': 'Tue, 27 Mar 2007 21:20:26 +0000',\n-            })\n-        with self._mocked_date(date):\n-            httpreq = self.download_request(req, self.spider)\n-        # botocore does not override Date with x-amz-date\n-        self.assertEqual(\n-            httpreq.headers['Authorization'],\n-            b'AWS 0PN5J17HBGZHT7JJ3X82:k3nL7gH3+PadhTEVn5Ip83xlYzk=')\n-\n     def test_request_signing6(self):\n         # uploads an object to a CNAME style virtual hosted bucket with metadata.\n         date = 'Tue, 27 Mar 2007 21:06:08 +0000'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#56f05fb16476b40f773edc9e93d41c8893a349c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 48 | Churn Cumulative: 3193 | Contributors (this commit): 26 | Commits (past 90d): 18 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,7 @@ from logging import getLogger\n from pathlib import Path\n from string import ascii_letters, digits\n from unittest import mock\n+from unittest.mock import call\n from urllib.parse import urljoin, urlparse, quote\n from urllib.request import pathname2url\n \n@@ -254,21 +255,42 @@ class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n-        assert_aws_environ()\n-        uri = os.environ.get('S3_TEST_FILE_URI')\n-        if not uri:\n-            raise unittest.SkipTest(\"No S3 URI available for testing\")\n-        access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n-        secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n-        storage = S3FeedStorage(uri, access_key, secret_key)\n+        skip_if_no_boto()\n+\n+        settings = {\n+            'AWS_ACCESS_KEY_ID': 'access_key',\n+            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        bucket = 'mybucket'\n+        key = 'export.csv'\n+        storage = S3FeedStorage.from_crawler(crawler, f's3://{bucket}/{key}')\n         verifyObject(IFeedStorage, storage)\n-        file = storage.open(scrapy.Spider(\"default\"))\n-        expected_content = b\"content: \\xe2\\x98\\x83\"\n-        file.write(expected_content)\n+\n+        file = mock.MagicMock()\n+        from botocore.stub import Stubber\n+        with Stubber(storage.s3_client) as stub:\n+            stub.add_response(\n+                'put_object',\n+                expected_params={\n+                    'Body': file,\n+                    'Bucket': bucket,\n+                    'Key': key,\n+                },\n+                service_response={},\n+            )\n+\n             yield storage.store(file)\n-        u = urlparse(uri)\n-        content = get_s3_content_and_delete(u.hostname, u.path[1:])\n-        self.assertEqual(content, expected_content)\n+\n+            stub.assert_no_pending_responses()\n+            self.assertEqual(\n+                file.method_calls,\n+                [\n+                    call.seek(0),\n+                    # The call to read does not happen with Stubber\n+                    call.close(),\n+                ]\n+            )\n \n     def test_init_without_acl(self):\n         storage = S3FeedStorage(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#17e135377a564152e82cd2ad177a6a2749c15832", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 48 | Lines Deleted: 51 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 99 | Churn Cumulative: 3292 | Contributors (this commit): 26 | Commits (past 90d): 19 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,6 @@ from logging import getLogger\n from pathlib import Path\n from string import ascii_letters, digits\n from unittest import mock\n-from unittest.mock import call\n from urllib.parse import urljoin, urlparse, quote\n from urllib.request import pathname2url\n \n@@ -42,7 +41,6 @@ from scrapy.extensions.feedexport import (\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import (\n-    assert_aws_environ,\n     get_s3_content_and_delete,\n     get_crawler,\n     mock_google_cloud_storage,\n@@ -286,9 +284,9 @@ class S3FeedStorageTest(unittest.TestCase):\n             self.assertEqual(\n                 file.method_calls,\n                 [\n-                    call.seek(0),\n+                    mock.call.seek(0),\n                     # The call to read does not happen with Stubber\n-                    call.close(),\n+                    mock.call.close(),\n                 ]\n             )\n \n@@ -1518,46 +1516,53 @@ class BatchDeliveriesTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_s3_export(self):\n-        \"\"\"\n-        Test export of items into s3 bucket.\n-        S3_TEST_BUCKET_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY must be specified in tox.ini\n-        to perform this test:\n-        [testenv]\n-        setenv =\n-            AWS_SECRET_ACCESS_KEY = ABCD\n-            AWS_ACCESS_KEY_ID = EFGH\n-            S3_TEST_BUCKET_NAME = IJKL\n-        \"\"\"\n-        try:\n-            import boto3\n-        except ImportError:\n-            raise unittest.SkipTest(\"S3FeedStorage requires boto3\")\n+        skip_if_no_boto()\n \n-        assert_aws_environ()\n-        s3_test_bucket_name = os.environ.get('S3_TEST_BUCKET_NAME')\n-        access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n-        secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n-        if not s3_test_bucket_name:\n-            raise unittest.SkipTest(\"No S3 BUCKET available for testing\")\n-\n-        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n-        filename = ''.join(chars)\n-        prefix = f'tmp/{filename}'\n-        s3_test_file_uri = f's3://{s3_test_bucket_name}/{prefix}/%(batch_time)s.json'\n-        storage = S3FeedStorage(s3_test_bucket_name, access_key, secret_key)\n-        settings = Settings({\n-            'FEEDS': {\n-                s3_test_file_uri: {\n-                    'format': 'json',\n-                },\n-            },\n-            'FEED_EXPORT_BATCH_ITEM_COUNT': 1,\n-        })\n+        bucket = 'mybucket'\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n             self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n             self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n         ]\n+\n+        class CustomS3FeedStorage(S3FeedStorage):\n+\n+            stubs = []\n+\n+            def open(self, *args, **kwargs):\n+                from botocore.stub import ANY, Stubber\n+                stub = Stubber(self.s3_client)\n+                stub.activate()\n+                CustomS3FeedStorage.stubs.append(stub)\n+                stub.add_response(\n+                    'put_object',\n+                    expected_params={\n+                        'Body': ANY,\n+                        'Bucket': bucket,\n+                        'Key': ANY,\n+                    },\n+                    service_response={},\n+                )\n+                return super().open(*args, **kwargs)\n+\n+        key = 'export.csv'\n+        uri = f's3://{bucket}/{key}/%(batch_time)s.json'\n+        batch_item_count = 1\n+        settings = {\n+            'AWS_ACCESS_KEY_ID': 'access_key',\n+            'AWS_SECRET_ACCESS_KEY': 'secret_key',\n+            'FEED_EXPORT_BATCH_ITEM_COUNT': batch_item_count,\n+            'FEED_STORAGES': {\n+                's3': CustomS3FeedStorage,\n+            },\n+            'FEEDS': {\n+                uri: {\n+                    'format': 'json',\n+                },\n+            },\n+        }\n+        crawler = get_crawler(settings_dict=settings)\n+        storage = S3FeedStorage.from_crawler(crawler, uri)\n         verifyObject(IFeedStorage, storage)\n \n         class TestSpider(scrapy.Spider):\n@@ -1567,22 +1572,14 @@ class BatchDeliveriesTest(FeedExportTestBase):\n                 for item in items:\n                     yield item\n \n-        s3 = boto3.resource('s3')\n-        my_bucket = s3.Bucket(s3_test_bucket_name)\n-        batch_size = settings.getint('FEED_EXPORT_BATCH_ITEM_COUNT')\n-\n-        with MockServer() as s:\n+        with MockServer() as server:\n             runner = CrawlerRunner(Settings(settings))\n-            TestSpider.start_urls = [s.url('/')]\n+            TestSpider.start_urls = [server.url('/')]\n             yield runner.crawl(TestSpider)\n \n-        for file_uri in my_bucket.objects.filter(Prefix=prefix):\n-            content = get_s3_content_and_delete(s3_test_bucket_name, file_uri.key)\n-            if not content and not items:\n-                break\n-            content = json.loads(content.decode('utf-8'))\n-            expected_batch, items = items[:batch_size], items[batch_size:]\n-            self.assertEqual(expected_batch, content)\n+        self.assertEqual(len(CustomS3FeedStorage.stubs), len(items)+1)\n+        for stub in CustomS3FeedStorage.stubs[:-1]:\n+            stub.assert_no_pending_responses()\n \n \n class FeedExportInitTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35726da434a9bc61dcb1ad5ef475c7d030023c87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3294 | Contributors (this commit): 26 | Commits (past 90d): 20 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ from logging import getLogger\n from pathlib import Path\n from string import ascii_letters, digits\n from unittest import mock\n-from urllib.parse import urljoin, urlparse, quote\n+from urllib.parse import urljoin, quote\n from urllib.request import pathname2url\n \n import lxml.etree\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3b740f07814107e643bb0d073ea116049e37cba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 29 | Files Changed: 2 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 105 | Churn Cumulative: 1098 | Contributors (this commit): 24 | Commits (past 90d): 7 | Contributors (cumulative): 35 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,15 +13,6 @@ from twisted.trial.unittest import SkipTest\n from scrapy.utils.boto import is_botocore_available\n \n \n-def assert_aws_environ():\n-    \"\"\"Asserts the current environment is suitable for running AWS testsi.\n-    Raises SkipTest with the reason if it's not.\n-    \"\"\"\n-    skip_if_no_boto()\n-    if 'AWS_ACCESS_KEY_ID' not in os.environ:\n-        raise SkipTest(\"AWS keys not found\")\n-\n-\n def assert_gcs_environ():\n     if 'GCS_PROJECT_ID' not in os.environ:\n         raise SkipTest(\"GCS_PROJECT_ID not found\")\n\n@@ -1,6 +1,7 @@\n import os\n import random\n import time\n+from datetime import datetime\n from io import BytesIO\n from shutil import rmtree\n from tempfile import mkdtemp\n@@ -23,11 +24,11 @@ from scrapy.pipelines.files import (\n )\n from scrapy.settings import Settings\n from scrapy.utils.test import (\n-    assert_aws_environ,\n     assert_gcs_environ,\n     get_ftp_content_and_delete,\n     get_gcs_content_and_delete,\n     get_s3_content_and_delete,\n+    skip_if_no_boto,\n )\n \n \n@@ -414,32 +415,87 @@ class FilesPipelineTestCaseCustomSettings(unittest.TestCase):\n \n \n class TestS3FilesStore(unittest.TestCase):\n+\n     @defer.inlineCallbacks\n     def test_persist(self):\n-        assert_aws_environ()\n-        uri = os.environ.get('S3_TEST_FILE_URI')\n-        if not uri:\n-            raise unittest.SkipTest(\"No S3 URI available for testing\")\n-        data = b\"TestS3FilesStore: \\xe2\\x98\\x83\"\n-        buf = BytesIO(data)\n+        skip_if_no_boto()\n+\n+        bucket = 'mybucket'\n+        key = 'export.csv'\n+        uri = f's3://{bucket}/{key}'\n+        buffer = mock.MagicMock()\n         meta = {'foo': 'bar'}\n         path = ''\n+        content_type = 'image/png'\n+\n         store = S3FilesStore(uri)\n+        from botocore.stub import Stubber\n+        with Stubber(store.s3_client) as stub:\n+            stub.add_response(\n+                'put_object',\n+                expected_params={\n+                    'ACL': S3FilesStore.POLICY,\n+                    'Body': buffer,\n+                    'Bucket': bucket,\n+                    'CacheControl': S3FilesStore.HEADERS['Cache-Control'],\n+                    'ContentType': content_type,\n+                    'Key': key,\n+                    'Metadata': meta,\n+                },\n+                service_response={},\n+            )\n+\n             yield store.persist_file(\n-            path, buf, info=None, meta=meta,\n-            headers={'Content-Type': 'image/png'})\n-        s = yield store.stat_file(path, info=None)\n-        self.assertIn('last_modified', s)\n-        self.assertIn('checksum', s)\n-        self.assertEqual(s['checksum'], '3187896a9657a28163abb31667df64c8')\n-        u = urlparse(uri)\n-        content, key = get_s3_content_and_delete(\n-            u.hostname, u.path[1:], with_key=True)\n-        self.assertEqual(content, data)\n-        self.assertEqual(key['Metadata'], {'foo': 'bar'})\n+                path,\n+                buffer,\n+                info=None,\n+                meta=meta,\n+                headers={'Content-Type': content_type},\n+            )\n+\n+            stub.assert_no_pending_responses()\n             self.assertEqual(\n-            key['CacheControl'], S3FilesStore.HEADERS['Cache-Control'])\n-        self.assertEqual(key['ContentType'], 'image/png')\n+                buffer.method_calls,\n+                [\n+                    mock.call.seek(0),\n+                    # The call to read does not happen with Stubber\n+                ]\n+            )\n+\n+    @defer.inlineCallbacks\n+    def test_stat(self):\n+        skip_if_no_boto()\n+\n+        bucket = 'mybucket'\n+        key = 'export.csv'\n+        uri = f's3://{bucket}/{key}'\n+        checksum = '3187896a9657a28163abb31667df64c8'\n+\n+        store = S3FilesStore(uri)\n+        from botocore.stub import Stubber\n+        with Stubber(store.s3_client) as stub:\n+            stub.add_response(\n+                'head_object',\n+                expected_params={\n+                    'Bucket': bucket,\n+                    'Key': key,\n+                },\n+                service_response={\n+                    'ETag': f'\"{checksum}\"',\n+                    'LastModified': datetime(2019, 12, 1),\n+                },\n+            )\n+\n+            file_stats = yield store.stat_file('', info=None)\n+            self.assertEqual(\n+                file_stats,\n+                {\n+                    'checksum': checksum,\n+                    'last_modified': 1575154800,\n+                },\n+            )\n+\n+            stub.assert_no_pending_responses()\n \n \n class TestGCSFilesStore(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8f46e845190b7affb7a02c6842b465f0c5c3b76c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 14 | Churn Cumulative: 4406 | Contributors (this commit): 38 | Commits (past 90d): 30 | Contributors (cumulative): 61 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,18 +23,6 @@ def skip_if_no_boto():\n         raise SkipTest('missing botocore library')\n \n \n-def get_s3_content_and_delete(bucket, path, with_key=False):\n-    \"\"\" Get content from s3 key, and delete key afterwards.\n-    \"\"\"\n-    import botocore.session\n-    session = botocore.session.get_session()\n-    client = session.create_client('s3')\n-    key = client.get_object(Bucket=bucket, Key=path)\n-    content = key['Body'].read()\n-    client.delete_object(Bucket=bucket, Key=path)\n-    return (content, key) if with_key else content\n-\n-\n def get_gcs_content_and_delete(bucket, path):\n     from google.cloud import storage\n     client = storage.Client(project=os.environ.get('GCS_PROJECT_ID'))\n\n@@ -41,7 +41,6 @@ from scrapy.extensions.feedexport import (\n from scrapy.settings import Settings\n from scrapy.utils.python import to_unicode\n from scrapy.utils.test import (\n-    get_s3_content_and_delete,\n     get_crawler,\n     mock_google_cloud_storage,\n     skip_if_no_boto,\n\n@@ -27,7 +27,6 @@ from scrapy.utils.test import (\n     assert_gcs_environ,\n     get_ftp_content_and_delete,\n     get_gcs_content_and_delete,\n-    get_s3_content_and_delete,\n     skip_if_no_boto,\n )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c22e810658b227095ea516ed61e51e6be41068ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 19 | Methods Changed: 18 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 36 | Churn Cumulative: 285 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,11 +18,14 @@ except ImportError:\n class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n+    def __init__(self, stats):\n+        self.stats = stats\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool('COMPRESSION_ENABLED'):\n             raise NotConfigured\n-        return cls()\n+        return cls(crawler.stats)\n \n     def process_request(self, request, spider):\n         request.headers.setdefault('Accept-Encoding',\n@@ -37,8 +40,8 @@ class HttpCompressionMiddleware:\n             if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n-                spider.crawler.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n-                spider.crawler.stats.inc_value('httpcompression/response_count', spider=spider)\n+                self.stats.inc_value('httpcompression/response_bytes', len(decoded_body), spider=spider)\n+                self.stats.inc_value('httpcompression/response_count', spider=spider)\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n\n@@ -8,6 +8,7 @@ from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.gz import gunzip\n+from scrapy.utils.test import get_crawler\n from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n@@ -26,8 +27,10 @@ FORMAT = {\n class HttpCompressionTest(TestCase):\n \n     def setUp(self):\n-        self.spider = Spider('foo')\n-        self.mw = HttpCompressionMiddleware()\n+        self.crawler = get_crawler(Spider)\n+        self.spider = self.crawler._create_spider('scrapytest.org')\n+        self.mw = HttpCompressionMiddleware(self.crawler.stats)\n+        self.crawler.stats.open_spider(self.spider)\n \n     def _getresponse(self, coding):\n         if coding not in FORMAT:\n@@ -50,6 +53,13 @@ class HttpCompressionTest(TestCase):\n         response.request = Request('http://scrapytest.org', headers={'Accept-Encoding': 'gzip, deflate'})\n         return response\n \n+    def assertStatsEqual(self, key, value):\n+        self.assertEqual(\n+            self.crawler.stats.get_value(key, spider=self.spider),\n+            value,\n+            str(self.crawler.stats.get_stats(self.spider))\n+        )\n+\n     def test_process_request(self):\n         request = Request('http://scrapytest.org')\n         assert 'Accept-Encoding' not in request.headers\n@@ -66,6 +76,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse is not response\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_br(self):\n         try:\n@@ -79,6 +90,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse is not response\n         assert newresponse.body.startswith(b\"<!DOCTYPE\")\n         assert 'Content-Encoding' not in newresponse.headers\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_rawdeflate(self):\n         response = self._getresponse('rawdeflate')\n@@ -89,6 +101,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse is not response\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_zlibdelate(self):\n         response = self._getresponse('zlibdeflate')\n@@ -99,6 +112,7 @@ class HttpCompressionTest(TestCase):\n         assert newresponse is not response\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n         assert 'Content-Encoding' not in newresponse.headers\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_plain(self):\n         response = Response('http://scrapytest.org', body=b'<!DOCTYPE...')\n@@ -108,6 +122,7 @@ class HttpCompressionTest(TestCase):\n         newresponse = self.mw.process_response(request, response, self.spider)\n         assert newresponse is response\n         assert newresponse.body.startswith(b'<!DOCTYPE')\n+        self.assertStatsEqual('httpcompression/response_count', None)\n \n     def test_multipleencodings(self):\n         response = self._getresponse('gzip')\n@@ -135,6 +150,7 @@ class HttpCompressionTest(TestCase):\n         assert isinstance(newresponse, HtmlResponse)\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_force_recalculate_encoding(self):\n         headers = {\n@@ -154,6 +170,7 @@ class HttpCompressionTest(TestCase):\n         assert isinstance(newresponse, HtmlResponse)\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_no_content_type_header(self):\n         headers = {\n@@ -169,6 +186,7 @@ class HttpCompressionTest(TestCase):\n         assert isinstance(newresponse, respcls)\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_gzipped_contenttype(self):\n         response = self._getresponse('gzip')\n@@ -179,6 +197,7 @@ class HttpCompressionTest(TestCase):\n         self.assertIsNot(newresponse, response)\n         self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n         self.assertNotIn('Content-Encoding', newresponse.headers)\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_gzip_app_octetstream_contenttype(self):\n         response = self._getresponse('gzip')\n@@ -189,6 +208,7 @@ class HttpCompressionTest(TestCase):\n         self.assertIsNot(newresponse, response)\n         self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n         self.assertNotIn('Content-Encoding', newresponse.headers)\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_gzip_binary_octetstream_contenttype(self):\n         response = self._getresponse('x-gzip')\n@@ -199,6 +219,7 @@ class HttpCompressionTest(TestCase):\n         self.assertIsNot(newresponse, response)\n         self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n         self.assertNotIn('Content-Encoding', newresponse.headers)\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_gzipped_gzip_file(self):\n         \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n@@ -241,6 +262,7 @@ class HttpCompressionTest(TestCase):\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertEqual(gunzip(newresponse.body), plainbody)\n+        self.assertStatsEqual('httpcompression/response_count', 1)\n \n     def test_process_response_head_request_no_decode_required(self):\n         response = self._getresponse('gzip')\n@@ -251,3 +273,4 @@ class HttpCompressionTest(TestCase):\n         newresponse = self.mw.process_response(request, response, self.spider)\n         self.assertIs(newresponse, response)\n         self.assertEqual(response.body, b'')\n+        self.assertStatsEqual('httpcompression/response_count', None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ef7c44061f74281e7a977c9b1de4892cf84caf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 739 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: 1.0\n\nDIFF:\n@@ -469,6 +469,7 @@ class TestS3FilesStore(unittest.TestCase):\n         key = 'export.csv'\n         uri = f's3://{bucket}/{key}'\n         checksum = '3187896a9657a28163abb31667df64c8'\n+        last_modified = datetime(2019, 12, 1)\n \n         store = S3FilesStore(uri)\n         from botocore.stub import Stubber\n@@ -481,7 +482,7 @@ class TestS3FilesStore(unittest.TestCase):\n                 },\n                 service_response={\n                     'ETag': f'\"{checksum}\"',\n-                    'LastModified': datetime(2019, 12, 1),\n+                    'LastModified': last_modified,\n                 },\n             )\n \n@@ -490,7 +491,7 @@ class TestS3FilesStore(unittest.TestCase):\n                 file_stats,\n                 {\n                     'checksum': checksum,\n-                    'last_modified': 1575154800,\n+                    'last_modified': last_modified.timestamp(),\n                 },\n             )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#894b509d7a4262cfc7548d78e9ff4831c6551c34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 16 | Methods Changed: 11 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 47 | Churn Cumulative: 926 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,14 +6,11 @@ See documentation in docs/topics/spiders.rst\n \"\"\"\n \n import copy\n-import warnings\n from typing import Sequence\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, HtmlResponse\n from scrapy.linkextractors import LinkExtractor\n from scrapy.spiders import Spider\n-from scrapy.utils.python import get_func_args\n from scrapy.utils.spider import iterate_spider_output\n \n \n@@ -37,15 +34,22 @@ _default_link_extractor = LinkExtractor()\n \n class Rule:\n \n-    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None,\n-                 process_links=None, process_request=None, errback=None):\n+    def __init__(\n+        self,\n+        link_extractor=None,\n+        callback=None,\n+        cb_kwargs=None,\n+        follow=None,\n+        process_links=None,\n+        process_request=None,\n+        errback=None,\n+    ):\n         self.link_extractor = link_extractor or _default_link_extractor\n         self.callback = callback\n         self.errback = errback\n         self.cb_kwargs = cb_kwargs or {}\n         self.process_links = process_links or _identity\n         self.process_request = process_request or _identity_process_request\n-        self.process_request_argcount = None\n         self.follow = follow if follow is not None else not callback\n \n     def _compile(self, spider):\n@@ -53,22 +57,6 @@ class Rule:\n         self.errback = _get_method(self.errback, spider)\n         self.process_links = _get_method(self.process_links, spider)\n         self.process_request = _get_method(self.process_request, spider)\n-        self.process_request_argcount = len(get_func_args(self.process_request))\n-        if self.process_request_argcount == 1:\n-            warnings.warn(\n-                \"Rule.process_request should accept two arguments \"\n-                \"(request, response), accepting only one is deprecated\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n-\n-    def _process_request(self, request, response):\n-        \"\"\"\n-        Wrapper around the request processing function to maintain backward\n-        compatibility with functions that do not take a Response object\n-        \"\"\"\n-        args = [request] if self.process_request_argcount == 1 else [request, response]\n-        return self.process_request(*args)\n \n \n class CrawlSpider(Spider):\n@@ -111,7 +99,7 @@ class CrawlSpider(Spider):\n             for link in rule.process_links(links):\n                 seen.add(link)\n                 request = self._build_request(rule_index, link)\n-                yield rule._process_request(request, response)\n+                yield rule.process_request(request, response)\n \n     def _callback(self, response):\n         rule = self._rules[response.meta['rule']]\n\n@@ -1,8 +1,8 @@\n import gzip\n import inspect\n-from unittest import mock\n import warnings\n from io import BytesIO\n+from unittest import mock\n \n from testfixtures import LogCapture\n from twisted.trial import unittest\n@@ -20,7 +20,6 @@ from scrapy.spiders import (\n     XMLFeedSpider,\n )\n from scrapy.linkextractors import LinkExtractor\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.test import get_crawler\n \n \n@@ -280,7 +279,7 @@ class CrawlSpiderTest(SpiderTest):\n \n         response = HtmlResponse(\"http://example.org/somepage/index.html\", body=self.test_body)\n \n-        def process_request_change_domain(request):\n+        def process_request_change_domain(request, response):\n             return request.replace(url=request.url.replace('.org', '.com'))\n \n         class _CrawlSpider(self.spider_class):\n@@ -290,7 +289,6 @@ class CrawlSpiderTest(SpiderTest):\n                 Rule(LinkExtractor(), process_request=process_request_change_domain),\n             )\n \n-        with warnings.catch_warnings(record=True) as cw:\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n@@ -299,8 +297,6 @@ class CrawlSpiderTest(SpiderTest):\n                          ['http://example.com/somepage/item/12.html',\n                           'http://example.com/about.html',\n                           'http://example.com/nofollow.html'])\n-            self.assertEqual(len(cw), 1)\n-            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n \n     def test_process_request_with_response(self):\n \n@@ -339,10 +335,9 @@ class CrawlSpiderTest(SpiderTest):\n                 Rule(LinkExtractor(), process_request='process_request_upper'),\n             )\n \n-            def process_request_upper(self, request):\n+            def process_request_upper(self, request, response):\n                 return request.replace(url=request.url.upper())\n \n-        with warnings.catch_warnings(record=True) as cw:\n         spider = _CrawlSpider()\n         output = list(spider._requests_to_follow(response))\n         self.assertEqual(len(output), 3)\n@@ -351,8 +346,6 @@ class CrawlSpiderTest(SpiderTest):\n                          ['http://EXAMPLE.ORG/SOMEPAGE/ITEM/12.HTML',\n                           'http://EXAMPLE.ORG/ABOUT.HTML',\n                           'http://EXAMPLE.ORG/NOFOLLOW.HTML'])\n-            self.assertEqual(len(cw), 1)\n-            self.assertEqual(cw[0].category, ScrapyDeprecationWarning)\n \n     def test_process_request_instance_method_with_response(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9661a8dcfc79249e8e3e117eb70682ebc8e57a92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1788 | Contributors (this commit): 28 | Commits (past 90d): 3 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -180,9 +180,9 @@ class CrawlerRunner:\n         :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n             :class:`~scrapy.spiders.Spider` subclass or string\n \n-        :param list args: arguments to initialize the spider\n+        :param args: arguments to initialize the spider\n \n-        :param dict kwargs: keyword arguments to initialize the spider\n+        :param kwargs: keyword arguments to initialize the spider\n         \"\"\"\n         if isinstance(crawler_or_spidercls, Spider):\n             raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#744f352d09a9ec519c84ec5243e62eef78c66f08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 22 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 28 | Churn Cumulative: 516 | Contributors (this commit): 14 | Commits (past 90d): 6 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -97,27 +97,8 @@ class CookiesMiddleware:\n \n     def _get_request_cookies(self, jar, request):\n         \"\"\"\n-        Extract cookies from a Request. Values from the `Request.cookies` attribute\n-        take precedence over values from the `Cookie` request header.\n+        Extract cookies from the Request.cookies attribute\n         \"\"\"\n-        def get_cookies_from_header(jar, request):\n-            cookie_header = request.headers.get(\"Cookie\")\n-            if not cookie_header:\n-                return []\n-            cookie_gen_bytes = (s.strip() for s in cookie_header.split(b\";\"))\n-            cookie_list_unicode = []\n-            for cookie_bytes in cookie_gen_bytes:\n-                try:\n-                    cookie_unicode = cookie_bytes.decode(\"utf8\")\n-                except UnicodeDecodeError:\n-                    logger.warning(\"Non UTF-8 encoded cookie found in request %s: %s\",\n-                                   request, cookie_bytes)\n-                    cookie_unicode = cookie_bytes.decode(\"latin1\", errors=\"replace\")\n-                cookie_list_unicode.append(cookie_unicode)\n-            response = Response(request.url, headers={\"Set-Cookie\": cookie_list_unicode})\n-            return jar.make_cookies(response, request)\n-\n-        def get_cookies_from_attribute(jar, request):\n         if not request.cookies:\n             return []\n         elif isinstance(request.cookies, dict):\n@@ -127,5 +108,3 @@ class CookiesMiddleware:\n         formatted = filter(None, (self._format_cookie(c, request) for c in cookies))\n         response = Response(request.url, headers={\"Set-Cookie\": formatted})\n         return jar.make_cookies(response, request)\n-\n-        return get_cookies_from_header(jar, request) + get_cookies_from_attribute(jar, request)\n\n@@ -2,6 +2,8 @@ import logging\n from testfixtures import LogCapture\n from unittest import TestCase\n \n+import pytest\n+\n from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n from scrapy.exceptions import NotConfigured\n@@ -243,6 +245,7 @@ class CookiesMiddlewareTest(TestCase):\n         self.assertIn('Cookie', request.headers)\n         self.assertEqual(b'currencyCookie=USD', request.headers['Cookie'])\n \n+    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_from_default_request_headers_middleware(self):\n         DEFAULT_REQUEST_HEADERS = dict(Cookie='default=value; asdf=qwerty')\n         mw_default_headers = DefaultHeadersMiddleware(DEFAULT_REQUEST_HEADERS.items())\n@@ -257,6 +260,7 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req2, self.spider) is None\n         self.assertCookieValEqual(req2.headers['Cookie'], b'default=value; a=b; asdf=qwerty')\n \n+    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_keep_cookie_header(self):\n         # keep only cookies from 'Cookie' request header\n         req1 = Request('http://scrapytest.org', headers={'Cookie': 'a=b; c=d'})\n@@ -291,6 +295,7 @@ class CookiesMiddlewareTest(TestCase):\n         assert self.mw.process_request(req3, self.spider) is None\n         self.assertCookieValEqual(req3.headers['Cookie'], b'a=\\xc3\\xa1')\n \n+    @pytest.mark.xfail(reason=\"Cookie header is not currently being processed\")\n     def test_request_headers_cookie_encoding(self):\n         # 1) UTF8-encoded bytes\n         req1 = Request('http://example.org', headers={'Cookie': 'a=á'.encode('utf8')})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#44f0fde9057256dc16f24f22cfebc61626a94450", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 7 | Churn Cumulative: 218 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,18 +55,11 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n             set_tlsext_host_name(connection, self._hostnameBytes)\n         elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n             if self.verbose_logging:\n-                if hasattr(connection, 'get_cipher_name'):  # requires pyOPenSSL 0.15\n-                    if hasattr(connection, 'get_protocol_version_name'):  # requires pyOPenSSL 16.0.0\n                 logger.debug('SSL connection to %s using protocol %s, cipher %s',\n                              self._hostnameASCII,\n                              connection.get_protocol_version_name(),\n                              connection.get_cipher_name(),\n                              )\n-                    else:\n-                        logger.debug('SSL connection to %s using cipher %s',\n-                                     self._hostnameASCII,\n-                                     connection.get_cipher_name(),\n-                                     )\n                 server_cert = connection.get_peer_certificate()\n                 logger.debug('SSL connection certificate: issuer \"%s\", subject \"%s\"',\n                              x509name_to_string(server_cert.get_issuer()),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e42d82526a1a519ff1305b189acde064a40fbd8d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 228 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,7 +5,6 @@ from service_identity.exceptions import CertificateError\n from twisted.internet._sslverify import ClientTLSOptions, verifyHostname, VerificationError\n from twisted.internet.ssl import AcceptableCiphers\n \n-from scrapy import twisted_version\n from scrapy.utils.ssl import x509name_to_string, get_temp_key_info\n \n \n@@ -28,13 +27,6 @@ openssl_methods = {\n }\n \n \n-if twisted_version < (17, 0, 0):\n-    from twisted.internet._sslverify import _maybeSetHostNameIndication as set_tlsext_host_name\n-else:\n-    def set_tlsext_host_name(connection, hostNameBytes):\n-        connection.set_tlsext_host_name(hostNameBytes)\n-\n-\n class ScrapyClientTLSOptions(ClientTLSOptions):\n     \"\"\"\n     SSL Client connection creator ignoring certificate verification errors\n@@ -52,7 +44,7 @@ class ScrapyClientTLSOptions(ClientTLSOptions):\n \n     def _identityVerifyingInfoCallback(self, connection, where, ret):\n         if where & SSL.SSL_CB_HANDSHAKE_START:\n-            set_tlsext_host_name(connection, self._hostnameBytes)\n+            connection.set_tlsext_host_name(self._hostnameBytes)\n         elif where & SSL.SSL_CB_HANDSHAKE_DONE:\n             if self.verbose_logging:\n                 logger.debug('SSL connection to %s using protocol %s, cipher %s',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f47b120e2b67be7821a06ff77786c0bab2cfece8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 138 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,22 @@ its documentation in: docs/topics/link-extractors.rst\n \n \n class Link:\n-    \"\"\"Link objects represent an extracted link by the LinkExtractor.\"\"\"\n+    \"\"\"Link objects represent an extracted link by the LinkExtractor.\n+\n+    Using the anchor tag sample below to illustrate the parameters::\n+\n+            <a href=\"https://example.com/nofollow.html#foo\" rel=\"nofollow\">Dont follow this one</a>\n+\n+    :param url: the absolute url being linked to in the anchor tag.\n+                From the sample, this is ``https://example.com/nofollow.html``.\n+\n+    :param text: the text in the anchor tag. From the sample, this is ``Dont follow this one``.\n+\n+    :param fragment: the part of the url after the hash symbol. From the sample, this is ``foo``.\n+\n+    :param nofollow: an indication of the presence or absence of a nofollow value in the ``rel`` attribute\n+                    of the anchor tag.\n+    \"\"\"\n \n     __slots__ = ['url', 'text', 'fragment', 'nofollow']\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#159e2b2e2fbbcbb2b083a79e83e37cf4e60ee5ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 14 | Methods Changed: 18 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 91 | Churn Cumulative: 1653 | Contributors (this commit): 29 | Commits (past 90d): 17 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,7 @@ def _import_file(filepath):\n     abspath = os.path.abspath(filepath)\n     dirname, file = os.path.split(abspath)\n     fname, fext = os.path.splitext(file)\n-    if fext != '.py':\n+    if fext not in ('.py', '.pyw'):\n         raise ValueError(f\"Not a Python source file: {abspath}\")\n     if dirname:\n         sys.path = [dirname] + sys.path\n\n@@ -496,6 +496,8 @@ class MiscCommandsTest(CommandTest):\n \n class RunSpiderCommandTest(CommandTest):\n \n+    spider_filename = 'myspider.py'\n+\n     debug_log_spider = \"\"\"\n import scrapy\n \n@@ -507,11 +509,23 @@ class MySpider(scrapy.Spider):\n         return []\n \"\"\"\n \n+    badspider = \"\"\"\n+import scrapy\n+\n+class BadSpider(scrapy.Spider):\n+    name = \"bad\"\n+    def start_requests(self):\n+        raise Exception(\"oops!\")\n+        \"\"\"\n+\n     @contextmanager\n-    def _create_file(self, content, name):\n+    def _create_file(self, content, name=None):\n         tmpdir = self.mktemp()\n         os.mkdir(tmpdir)\n+        if name:\n             fname = abspath(join(tmpdir, name))\n+        else:\n+            fname = abspath(join(tmpdir, self.spider_filename))\n         with open(fname, 'w') as f:\n             f.write(content)\n         try:\n@@ -519,12 +533,12 @@ class MySpider(scrapy.Spider):\n         finally:\n             rmtree(tmpdir)\n \n-    def runspider(self, code, name='myspider.py', args=()):\n+    def runspider(self, code, name=None, args=()):\n         with self._create_file(code, name) as fname:\n             return self.proc('runspider', fname, *args)\n \n-    def get_log(self, code, name='myspider.py', args=()):\n-        p, stdout, stderr = self.runspider(code, name=name, args=args)\n+    def get_log(self, code, name=None, args=()):\n+        p, stdout, stderr = self.runspider(code, name, args=args)\n         return stderr\n \n     def test_runspider(self):\n@@ -556,7 +570,7 @@ class MySpider(scrapy.Spider):\n         # which is intended,\n         # but this should not be because of DNS lookup error\n         # assumption: localhost will resolve in all cases (true?)\n-        log = self.get_log(\"\"\"\n+        dnscache_spider = \"\"\"\n import scrapy\n \n class MySpider(scrapy.Spider):\n@@ -565,23 +579,20 @@ class MySpider(scrapy.Spider):\n \n     def parse(self, response):\n         return {'test': 'value'}\n-\"\"\",\n-                           args=('-s', 'DNSCACHE_ENABLED=False'))\n-        print(log)\n+\"\"\"\n+        log = self.get_log(dnscache_spider, args=('-s', 'DNSCACHE_ENABLED=False'))\n         self.assertNotIn(\"DNSLookupError\", log)\n         self.assertIn(\"INFO: Spider opened\", log)\n \n     def test_runspider_log_short_names(self):\n         log1 = self.get_log(self.debug_log_spider,\n                             args=('-s', 'LOG_SHORT_NAMES=1'))\n-        print(log1)\n         self.assertIn(\"[myspider] DEBUG: It Works!\", log1)\n         self.assertIn(\"[scrapy]\", log1)\n         self.assertNotIn(\"[scrapy.core.engine]\", log1)\n \n         log2 = self.get_log(self.debug_log_spider,\n                             args=('-s', 'LOG_SHORT_NAMES=0'))\n-        print(log2)\n         self.assertIn(\"[myspider] DEBUG: It Works!\", log2)\n         self.assertNotIn(\"[scrapy]\", log2)\n         self.assertIn(\"[scrapy.core.engine]\", log2)\n@@ -599,15 +610,7 @@ class MySpider(scrapy.Spider):\n         self.assertIn('Unable to load', log)\n \n     def test_start_requests_errors(self):\n-        log = self.get_log(\"\"\"\n-import scrapy\n-\n-class BadSpider(scrapy.Spider):\n-    name = \"bad\"\n-    def start_requests(self):\n-        raise Exception(\"oops!\")\n-        \"\"\", name=\"badspider.py\")\n-        print(log)\n+        log = self.get_log(self.badspider, name='badspider.py')\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n@@ -696,6 +699,54 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n \n \n+class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n+\n+    spider_filename = 'myspider.pyw'\n+\n+    def setUp(self):\n+        super(WindowsRunSpiderCommandTest, self).setUp()\n+\n+    def test_start_requests_errors(self):\n+        log = self.get_log(self.badspider, name='badspider.pyw')\n+        self.assertIn(\"start_requests\", log)\n+        self.assertIn(\"badspider.pyw\", log)\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_run_good_spider(self):\n+        super().test_run_good_spider()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_runspider(self):\n+        super().test_runspider()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_runspider_dnscache_disabled(self):\n+        super().test_runspider_dnscache_disabled()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_runspider_log_level(self):\n+        super().test_runspider_log_level()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_runspider_log_short_names(self):\n+        super().test_runspider_log_short_names()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_runspider_no_spider_found(self):\n+        super().test_runspider_no_spider_found()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_output(self):\n+        super().test_output()\n+\n+    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n+    def test_overwrite_output(self):\n+        super().test_overwrite_output()\n+\n+    def test_runspider_unable_to_load(self):\n+        raise unittest.SkipTest(\"Already Tested in 'RunSpiderCommandTest' \")\n+\n+\n class BenchCommandTest(CommandTest):\n \n     def test_run(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
