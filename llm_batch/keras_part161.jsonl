{"custom_id": "keras#16e8b93d3852958aa7d28bb127de34681136c5b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 10252 | Contributors (this commit): 33 | Commits (past 90d): 32 | Contributors (cumulative): 34 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -830,12 +830,6 @@ class SeparableConv2D(Conv2D):\n             It defaults to the `image_data_format` value found in your\n             Keras config file at `~/.keras/keras.json`.\n             If you never set it, then it will be \"channels_last\".\n-        dilation_rate: an integer or tuple/list of 2 integers, specifying\n-            the dilation rate to use for dilated convolution.\n-            Can be a single integer to specify the same value for\n-            all spatial dimensions.\n-            Currently, specifying any `dilation_rate` value != 1 is\n-            incompatible with specifying any stride value != 1.\n         depth_multiplier: The number of depthwise convolution output channels\n             for each input channel.\n             The total number of depthwise convolution output\n@@ -890,7 +884,6 @@ class SeparableConv2D(Conv2D):\n                  strides=(1, 1),\n                  padding='valid',\n                  data_format=None,\n-                 dilation_rate=(1, 1),\n                  depth_multiplier=1,\n                  activation=None,\n                  use_bias=True,\n@@ -911,7 +904,6 @@ class SeparableConv2D(Conv2D):\n             strides=strides,\n             padding=padding,\n             data_format=data_format,\n-            dilation_rate=dilation_rate,\n             activation=activation,\n             use_bias=use_bias,\n             bias_regularizer=bias_regularizer,\n@@ -979,8 +971,7 @@ class SeparableConv2D(Conv2D):\n             self.pointwise_kernel,\n             data_format=self.data_format,\n             strides=self.strides,\n-            padding=self.padding,\n-            dilation_rate=self.dilation_rate)\n+            padding=self.padding)\n \n         if self.bias:\n             outputs = K.bias_add(\n@@ -1002,12 +993,10 @@ class SeparableConv2D(Conv2D):\n \n         rows = conv_utils.conv_output_length(rows, self.kernel_size[0],\n                                              self.padding,\n-                                             self.strides[0],\n-                                             dilation=self.dilation_rate[0])\n+                                             self.strides[0])\n         cols = conv_utils.conv_output_length(cols, self.kernel_size[1],\n                                              self.padding,\n-                                             self.strides[1],\n-                                             dilation=self.dilation_rate[1])\n+                                             self.strides[1])\n         if self.data_format == 'channels_first':\n             return (input_shape[0], self.filters, rows, cols)\n         elif self.data_format == 'channels_last':\n\n@@ -191,15 +191,6 @@ def test_separable_conv_2d():\n                                    'strides': strides,\n                                    'depth_multiplier': multiplier},\n                            input_shape=(num_samples, num_row, num_col, stack_size))\n-    # Test dilation\n-    layer_test(convolutional.SeparableConv2D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': (3, 3),\n-                       'strides': strides,\n-                       'depth_multiplier': multiplier,\n-                       'data_format': 'channels_first',\n-                       'dilation_rate': 2},\n-               input_shape=(num_samples, stack_size, num_row, num_col))\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#af0d18f6164952a6610f1c9c14d2dded7e6c488e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 155 | Lines Deleted: 91 | Files Changed: 4 | Hunks: 75 | Methods Changed: 57 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 246 | Churn Cumulative: 4535 | Contributors (this commit): 39 | Commits (past 90d): 36 | Contributors (cumulative): 48 | DMM Complexity: 0.8571428571428571\n\nDIFF:\n@@ -24,7 +24,7 @@ from ..utils.generic_utils import Progbar\n from .. import callbacks as cbks\n \n \n-def standardize_input_data(data, names, shapes=None,\n+def _standardize_input_data(data, names, shapes=None,\n                             check_batch_axis=True,\n                             exception_prefix=''):\n     \"\"\"Normalizes inputs and targets provided by users.\n@@ -42,6 +42,9 @@ def standardize_input_data(data, names, shapes=None,\n             the batch axis of the arrays matches the expected\n             value found in `shapes`.\n         exception_prefix: String prefix used for exception formatting.\n+\n+    # Returns\n+        List of standardized input arrays (one array per model input).\n     \"\"\"\n     if isinstance(data, dict):\n         arrays = []\n@@ -91,14 +94,14 @@ def standardize_input_data(data, names, shapes=None,\n                              'Found: array with shape ' + str(data.shape))\n         arrays = [data]\n \n-    # make arrays at least 2D\n+    # Make arrays at least 2D.\n     for i in range(len(names)):\n         array = arrays[i]\n         if len(array.shape) == 1:\n             array = np.expand_dims(array, 1)\n             arrays[i] = array\n \n-    # check shapes compatibility\n+    # Check shapes compatibility.\n     if shapes:\n         for i in range(len(names)):\n             if shapes[i] is None:\n@@ -125,7 +128,9 @@ def standardize_input_data(data, names, shapes=None,\n     return arrays\n \n \n-def standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n+def _standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n+    \"\"\"Maps `sample_weight` or `class_weight` to model outputs.\n+    \"\"\"\n     if x_weight is None or len(x_weight) == 0:\n         return [None for _ in output_names]\n     if len(output_names) == 1:\n@@ -158,19 +163,29 @@ def standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n                         str(x_weight))\n \n \n-def standardize_class_weights(class_weight, output_names):\n-    return standardize_sample_or_class_weights(class_weight,\n+def _standardize_class_weights(class_weight, output_names):\n+    return _standardize_sample_or_class_weights(class_weight,\n                                                 output_names,\n                                                 'class_weight')\n \n \n-def standardize_sample_weights(sample_weight, output_names):\n-    return standardize_sample_or_class_weights(sample_weight,\n+def _standardize_sample_weights(sample_weight, output_names):\n+    return _standardize_sample_or_class_weights(sample_weight,\n                                                 output_names,\n                                                 'sample_weight')\n \n \n-def check_array_lengths(inputs, targets, weights):\n+def _check_array_lengths(inputs, targets, weights):\n+    \"\"\"Does user input validation for numpy arrays.\n+\n+    # Arguments\n+        inputs: list of Numpy arrays of inputs.\n+        targets: list of Numpy arrays of targets.\n+        weights: list of Numpy arrays of sample weights.\n+\n+    # Raises:\n+        ValueError: in case of incorrectly formatted data.\n+    \"\"\"\n     x_lengths = [x.shape[0] for x in inputs]\n     y_lengths = [y.shape[0] for y in targets]\n     w_lengths = [w.shape[0] for w in weights]\n@@ -198,7 +213,20 @@ def check_array_lengths(inputs, targets, weights):\n                          str(list(set_w)[0]) + ' target samples.')\n \n \n-def check_loss_and_target_compatibility(targets, losses, output_shapes):\n+def _check_loss_and_target_compatibility(targets, losses, output_shapes):\n+    \"\"\"Does validation on the compatiblity of targets and loss functions.\n+\n+    This helps prevent users from using loss functions incorrectly.\n+\n+    # Arguments\n+        targets: list of Numpy arrays of targets.\n+        losses: list of loss functions.\n+        output_shapes: list of shapes of model outputs.\n+\n+    # Raises\n+        ValueError: if a loss function or target array\n+            is incompatible with an output.\n+    \"\"\"\n     key_losses = {'mean_square_error',\n                   'binary_crossentropy',\n                   'categorical_crossentropy'}\n@@ -233,7 +261,24 @@ def check_loss_and_target_compatibility(targets, losses, output_shapes):\n                         'as the output.')\n \n \n-def collect_metrics(metrics, output_names):\n+def _collect_metrics(metrics, output_names):\n+    \"\"\"Maps metric functions to model outputs.\n+\n+    # Arguments\n+        metrics: a list or dict of metric functions.\n+        output_names: a list of the names (strings) of model outputs.\n+\n+    # Returns\n+        A list (one entry per model output) of lists of metric functions.\n+        For instance, if the model has 2 outputs, and for the first output\n+        we want to compute \"binary_accuracy\" and \"binary_crossentropy\",\n+        and just \"binary_accuracy\" for the second output,\n+        the list would look like:\n+            `[[binary_accuracy, binary_crossentropy], [binary_accuracy]]`\n+\n+    # Raises\n+        TypeError: if an incorrect type is passed for the `metrics` argument.\n+    \"\"\"\n     if not metrics:\n         return [[] for _ in output_names]\n     if isinstance(metrics, list):\n@@ -253,10 +298,18 @@ def collect_metrics(metrics, output_names):\n                         str(metrics))\n \n \n-def batch_shuffle(index_array, batch_size):\n-    \"\"\"This shuffles an array in a batch-wise fashion.\n+def _batch_shuffle(index_array, batch_size):\n+    \"\"\"Shuffles an array in a batch-wise fashion.\n+\n     Useful for shuffling HDF5 arrays\n     (where one cannot access arbitrary indices).\n+\n+    # Arguments\n+        index_array: array of indices to be shuffled.\n+        batch_size: integer.\n+\n+    # Returns\n+        The `index_array` array, shuffled in a batch-wise fashion.\n     \"\"\"\n     batch_count = int(len(index_array) / batch_size)\n     # to reshape we need to be cleanly divisible by batch size\n@@ -269,7 +322,7 @@ def batch_shuffle(index_array, batch_size):\n     return np.append(index_array, last_batch)\n \n \n-def make_batches(size, batch_size):\n+def _make_batches(size, batch_size):\n     \"\"\"Returns a list of batch indices (tuples of indices).\n     \"\"\"\n     num_batches = int(np.ceil(size / float(batch_size)))\n@@ -277,13 +330,13 @@ def make_batches(size, batch_size):\n             for i in range(0, num_batches)]\n \n \n-def slice_X(X, start=None, stop=None):\n+def _slice_arrays(arrays, start=None, stop=None):\n     \"\"\"This takes an array-like, or a list of\n     array-likes, and outputs:\n-        - X[start:stop] if X is an array-like\n-        - [x[start:stop] for x in X] if X in a list\n+        - arrays[start:stop] if `arrays` is an array-like\n+        - [x[start:stop] for x in arrays] if `arrays` is a list\n \n-    Can also work on list/array of indices: `slice_X(x, indices)`\n+    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n \n     # Arguments\n         start: can be an integer index (start index)\n@@ -291,24 +344,24 @@ def slice_X(X, start=None, stop=None):\n         stop: integer (stop index); should be None if\n             `start` was a list.\n     \"\"\"\n-    if isinstance(X, list):\n+    if isinstance(arrays, list):\n         if hasattr(start, '__len__'):\n             # hdf5 datasets only support list objects as indices\n             if hasattr(start, 'shape'):\n                 start = start.tolist()\n-            return [x[start] for x in X]\n+            return [x[start] for x in arrays]\n         else:\n-            return [x[start:stop] for x in X]\n+            return [x[start:stop] for x in arrays]\n     else:\n         if hasattr(start, '__len__'):\n             if hasattr(start, 'shape'):\n                 start = start.tolist()\n-            return X[start]\n+            return arrays[start]\n         else:\n-            return X[start:stop]\n+            return arrays[start:stop]\n \n \n-def weighted_objective(fn):\n+def _weighted__masked_objective(fn):\n     \"\"\"Transforms an objective function `fn(y_true, y_pred)`\n     into a sample-weighted, cost-masked objective function\n     `fn(y_true, y_pred, weights, mask)`.\n@@ -338,7 +391,28 @@ def weighted_objective(fn):\n     return weighted\n \n \n-def standardize_weights(y, sample_weight=None, class_weight=None,\n+def _masked_objective(fn):\n+    \"\"\"Transforms an objective function `fn(y_true, y_pred)`\n+    into a cost-masked objective function\n+    `fn(y_true, y_pred, mask)`.\n+    \"\"\"\n+    def masked(y_true, y_pred, mask=None):\n+        # score_array has ndim >= 2\n+        score_array = fn(y_true, y_pred)\n+        if mask is not None:\n+            # Cast the mask to floatX to avoid float64 upcasting in theano\n+            mask = K.cast(mask, K.floatx())\n+            # mask should have the same shape as score_array\n+            score_array *= mask\n+            #  the loss per batch should be proportional\n+            #  to the number of unmasked samples.\n+            score_array /= K.mean(mask)\n+\n+        return K.mean(score_array)\n+    return masked\n+\n+\n+def _standardize_weights(y, sample_weight=None, class_weight=None,\n                          sample_weight_mode=None):\n     \"\"\"Performs weight input validation and standardization\n     to a single sample-wise (or timestep-wise) weight array.\n@@ -374,7 +448,6 @@ def standardize_weights(y, sample_weight=None, class_weight=None,\n \n     if sample_weight is not None:\n         assert len(sample_weight.shape) <= len(y.shape)\n-        # TODO: proper error message\n         assert y.shape[:sample_weight.ndim] == sample_weight.shape\n         return sample_weight\n     elif isinstance(class_weight, dict):\n@@ -414,7 +487,7 @@ class GeneratorEnqueuer(object):\n         self.queue = None\n \n     def start(self, workers=1, max_q_size=10, wait_time=0.05):\n-        \"\"\"Kick off threads which add data from the generator into the queue.\n+        \"\"\"Kicks off threads which add data from the generator into the queue.\n \n         # Arguments\n             workers: number of worker threads\n@@ -571,7 +644,7 @@ class Model(Container):\n             loss_function = losses.get(loss)\n             loss_functions = [loss_function for _ in range(len(self.outputs))]\n         self.loss_functions = loss_functions\n-        weighted_losses = [weighted_objective(fn) for fn in loss_functions]\n+        weighted_losses = [_weighted__masked_objective(fn) for fn in loss_functions]\n \n         # prepare output masks\n         masks = self.compute_mask(self.inputs, mask=None)\n@@ -648,7 +721,6 @@ class Model(Container):\n                                               name=name + '_target',\n                                               sparse=K.is_sparse(self.outputs[i]),\n                                               dtype=K.dtype(self.outputs[i])))\n-\n         # prepare metrics\n         self.metrics = metrics\n         self.metrics_names = ['loss']\n@@ -680,13 +752,12 @@ class Model(Container):\n \n         # list of same size as output_names.\n         # contains tuples (metrics for output, names of metrics)\n-        nested_metrics = collect_metrics(metrics, self.output_names)\n+        nested_metrics = _collect_metrics(metrics, self.output_names)\n \n         def append_metric(layer_num, metric_name, metric_tensor):\n-            \"\"\"Helper function, used in loop below\"\"\"\n+            \"\"\"Helper function used in loop below.\"\"\"\n             if len(self.output_names) > 1:\n                 metric_name = self.output_layers[layer_num].name + '_' + metric_name\n-\n             self.metrics_names.append(metric_name)\n             self.metrics_tensors.append(metric_tensor)\n \n@@ -710,16 +781,15 @@ class Model(Container):\n                     else:\n                         acc_fn = metrics_module.categorical_accuracy\n \n-                    append_metric(i, 'acc', acc_fn(y_true, y_pred))\n+                    masked_fn = _masked_objective(acc_fn)\n+                    append_metric(i, 'acc', masked_fn(y_true, y_pred, mask=masks[i]))\n                 else:\n                     metric_fn = metrics_module.get(metric)\n-                    metric_result = metric_fn(y_true, y_pred)\n-\n-                    if not isinstance(metric_result, dict):\n+                    masked_metric_fn = _masked_objective(metric_fn)\n+                    metric_result = masked_metric_fn(y_true, y_pred, mask=masks[i])\n                     metric_result = {\n                         metric_fn.__name__: metric_result\n                     }\n-\n                     for name, tensor in six.iteritems(metric_result):\n                         append_metric(i, name, tensor)\n \n@@ -867,20 +937,20 @@ class Model(Container):\n         for epoch in range(initial_epoch, epochs):\n             callbacks.on_epoch_begin(epoch)\n             if shuffle == 'batch':\n-                index_array = batch_shuffle(index_array, batch_size)\n+                index_array = _batch_shuffle(index_array, batch_size)\n             elif shuffle:\n                 np.random.shuffle(index_array)\n \n-            batches = make_batches(num_train_samples, batch_size)\n+            batches = _make_batches(num_train_samples, batch_size)\n             epoch_logs = {}\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 try:\n                     if isinstance(ins[-1], float):\n                         # do not slice the training phase flag\n-                        ins_batch = slice_X(ins[:-1], batch_ids) + [ins[-1]]\n+                        ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n                     else:\n-                        ins_batch = slice_X(ins, batch_ids)\n+                        ins_batch = _slice_arrays(ins, batch_ids)\n                 except TypeError:\n                     raise TypeError('TypeError while preparing batch. '\n                                     'If using HDF5 input data, '\n@@ -933,15 +1003,15 @@ class Model(Container):\n         outs = []\n         if verbose == 1:\n             progbar = Progbar(target=samples)\n-        batches = make_batches(samples, batch_size)\n+        batches = _make_batches(samples, batch_size)\n         index_array = np.arange(samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n             if isinstance(ins[-1], float):\n                 # do not slice the training phase flag\n-                ins_batch = slice_X(ins[:-1], batch_ids) + [ins[-1]]\n+                ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n             else:\n-                ins_batch = slice_X(ins, batch_ids)\n+                ins_batch = _slice_arrays(ins, batch_ids)\n \n             batch_outs = f(ins_batch)\n             if not isinstance(batch_outs, list):\n@@ -978,15 +1048,15 @@ class Model(Container):\n         outs = []\n         if verbose == 1:\n             progbar = Progbar(target=samples)\n-        batches = make_batches(samples, batch_size)\n+        batches = _make_batches(samples, batch_size)\n         index_array = np.arange(samples)\n         for batch_index, (batch_start, batch_end) in enumerate(batches):\n             batch_ids = index_array[batch_start:batch_end]\n             if isinstance(ins[-1], float):\n                 # do not slice the training phase flag\n-                ins_batch = slice_X(ins[:-1], batch_ids) + [ins[-1]]\n+                ins_batch = _slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n             else:\n-                ins_batch = slice_X(ins, batch_ids)\n+                ins_batch = _slice_arrays(ins, batch_ids)\n \n             batch_outs = f(ins_batch)\n             if isinstance(batch_outs, list):\n@@ -1024,23 +1094,23 @@ class Model(Container):\n                 output_shapes.append(None)\n             else:\n                 output_shapes.append(output_shape)\n-        x = standardize_input_data(x, self.input_names,\n+        x = _standardize_input_data(x, self.input_names,\n                                     self.internal_input_shapes,\n                                     check_batch_axis=False,\n                                     exception_prefix='model input')\n-        y = standardize_input_data(y, self.output_names,\n+        y = _standardize_input_data(y, self.output_names,\n                                     output_shapes,\n                                     check_batch_axis=False,\n                                     exception_prefix='model target')\n-        sample_weights = standardize_sample_weights(sample_weight,\n+        sample_weights = _standardize_sample_weights(sample_weight,\n                                                      self.output_names)\n-        class_weights = standardize_class_weights(class_weight,\n+        class_weights = _standardize_class_weights(class_weight,\n                                                    self.output_names)\n-        sample_weights = [standardize_weights(ref, sw, cw, mode)\n+        sample_weights = [_standardize_weights(ref, sw, cw, mode)\n                           for (ref, sw, cw, mode)\n                           in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n-        check_array_lengths(x, y, sample_weights)\n-        check_loss_and_target_compatibility(y, self.loss_functions, self.internal_output_shapes)\n+        _check_array_lengths(x, y, sample_weights)\n+        _check_loss_and_target_compatibility(y, self.loss_functions, self.internal_output_shapes)\n         if self.stateful and batch_size:\n             if x[0].shape[0] % batch_size != 0:\n                 raise ValueError('In a stateful network, '\n@@ -1103,7 +1173,6 @@ class Model(Container):\n             initial_epoch: epoch at which to start training\n                 (useful for resuming a previous training run)\n \n-\n         # Returns\n             A `History` instance. Its `history` attribute contains\n             all information collected during training.\n@@ -1145,11 +1214,11 @@ class Model(Container):\n         elif validation_split and 0. < validation_split < 1.:\n             do_validation = True\n             split_at = int(len(x[0]) * (1. - validation_split))\n-            x, val_x = (slice_X(x, 0, split_at), slice_X(x, split_at))\n-            y, val_y = (slice_X(y, 0, split_at), slice_X(y, split_at))\n+            x, val_x = (_slice_arrays(x, 0, split_at), _slice_arrays(x, split_at))\n+            y, val_y = (_slice_arrays(y, 0, split_at), _slice_arrays(y, split_at))\n             sample_weights, val_sample_weights = (\n-                slice_X(sample_weights, 0, split_at),\n-                slice_X(sample_weights, split_at))\n+                _slice_arrays(sample_weights, 0, split_at),\n+                _slice_arrays(sample_weights, split_at))\n             self._make_test_function()\n             val_f = self.test_function\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n@@ -1250,7 +1319,7 @@ class Model(Container):\n             A Numpy array of predictions.\n         \"\"\"\n         # validate user data\n-        x = standardize_input_data(x, self.input_names,\n+        x = _standardize_input_data(x, self.input_names,\n                                     self.internal_input_shapes,\n                                     check_batch_axis=False)\n         if self.stateful:\n@@ -1368,7 +1437,7 @@ class Model(Container):\n     def predict_on_batch(self, x):\n         \"\"\"Returns predictions for a single batch of samples.\n         \"\"\"\n-        x = standardize_input_data(x, self.input_names,\n+        x = _standardize_input_data(x, self.input_names,\n                                     self.internal_input_shapes)\n         if self.uses_learning_phase and not isinstance(K.learning_phase, int):\n             ins = x + [0.]\n\n@@ -5,78 +5,78 @@ from .utils.generic_utils import deserialize_keras_object\n \n \n def binary_accuracy(y_true, y_pred):\n-    return K.mean(K.equal(y_true, K.round(y_pred)))\n+    return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)\n \n \n def categorical_accuracy(y_true, y_pred):\n-    return K.mean(K.equal(K.argmax(y_true, axis=-1),\n-                          K.argmax(y_pred, axis=-1)))\n+    return K.equal(K.argmax(y_true, axis=-1),\n+                   K.argmax(y_pred, axis=-1))\n \n \n def sparse_categorical_accuracy(y_true, y_pred):\n-    return K.mean(K.equal(K.max(y_true, axis=-1),\n-                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())))\n+    return K.equal(K.max(y_true, axis=-1),\n+                   K.cast(K.argmax(y_pred, axis=-1), K.floatx()))\n \n \n def top_k_categorical_accuracy(y_true, y_pred, k=5):\n-    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k))\n+    return K.mean(K.in_top_k(y_pred, K.argmax(y_true, axis=-1), k), axis=-1)\n \n \n def mean_squared_error(y_true, y_pred):\n-    return K.mean(K.square(y_pred - y_true))\n+    return K.mean(K.square(y_pred - y_true), axis=-1)\n \n \n def mean_absolute_error(y_true, y_pred):\n-    return K.mean(K.abs(y_pred - y_true))\n+    return K.mean(K.abs(y_pred - y_true), axis=-1)\n \n \n def mean_absolute_percentage_error(y_true, y_pred):\n     diff = K.abs((y_true - y_pred) / K.clip(K.abs(y_true),\n                                             K.epsilon(),\n                                             None))\n-    return 100. * K.mean(diff)\n+    return 100. * K.mean(diff, axis=-1)\n \n \n def mean_squared_logarithmic_error(y_true, y_pred):\n     first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n     second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n-    return K.mean(K.square(first_log - second_log))\n+    return K.mean(K.square(first_log - second_log), axis=-1)\n \n \n def hinge(y_true, y_pred):\n-    return K.mean(K.maximum(1. - y_true * y_pred, 0.))\n+    return K.mean(K.maximum(1. - y_true * y_pred, 0.), axis=-1)\n \n \n def squared_hinge(y_true, y_pred):\n-    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)))\n+    return K.mean(K.square(K.maximum(1. - y_true * y_pred, 0.)), axis=-1)\n \n \n def categorical_crossentropy(y_true, y_pred):\n-    return K.mean(K.categorical_crossentropy(y_pred, y_true))\n+    return K.categorical_crossentropy(y_pred, y_true)\n \n \n def sparse_categorical_crossentropy(y_true, y_pred):\n-    return K.mean(K.sparse_categorical_crossentropy(y_pred, y_true))\n+    return K.sparse_categorical_crossentropy(y_pred, y_true)\n \n \n def binary_crossentropy(y_true, y_pred):\n-    return K.mean(K.binary_crossentropy(y_pred, y_true))\n+    return K.mean(K.binary_crossentropy(y_pred, y_true), axis=-1)\n \n \n def kullback_leibler_divergence(y_true, y_pred):\n     y_true = K.clip(y_true, K.epsilon(), 1)\n     y_pred = K.clip(y_pred, K.epsilon(), 1)\n-    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n+    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1), axis=-1)\n \n \n def poisson(y_true, y_pred):\n-    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()))\n+    return K.mean(y_pred - y_true * K.log(y_pred + K.epsilon()), axis=-1)\n \n \n def cosine_proximity(y_true, y_pred):\n     y_true = K.l2_normalize(y_true, axis=-1)\n     y_pred = K.l2_normalize(y_pred, axis=-1)\n-    return - K.mean(y_true * y_pred)\n+    return - K.mean(y_true * y_pred, axis=-1)\n \n \n # Aliases\n\n@@ -170,21 +170,15 @@ def test_model_methods():\n     assert trained_epochs == [2, 3, 4]\n \n     # test with a custom metric function\n-    mse = lambda y_true, y_pred: K.mean(K.pow(y_true - y_pred, 2))\n+    def mse(y_true, y_pred):\n+        return K.mean(K.pow(y_true - y_pred, 2))\n \n-    def mse_powers(y_true, y_pred):\n-        m = mse(y_true, y_pred)\n-        return {\n-            'mse_squared': K.pow(m, 2),\n-            'mse_cubed': K.pow(m, 3)\n-        }\n-\n-    model.compile(optimizer, loss, metrics=[mse, mse_powers],\n+    model.compile(optimizer, loss, metrics=[mse],\n                   sample_weight_mode=None)\n \n     out = model.train_on_batch([input_a_np, input_b_np],\n                                [output_a_np, output_b_np])\n-    out_len = 1 + 2 * 4  # total loss, per layer: loss + 3 metrics\n+    out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)\n     assert len(out) == out_len\n     out = model.test_on_batch([input_a_np, input_b_np],\n                               [output_a_np, output_b_np])\n\n@@ -30,14 +30,15 @@ def test_metrics():\n     y_b = K.variable(np.random.random((6, 7)))\n     for metric in all_metrics:\n         output = metric(y_a, y_b)\n-        assert K.eval(output).shape == ()\n+        print(metric.__name__)\n+        assert K.eval(output).shape == (6,)\n \n \n def test_sparse_metrics():\n     for metric in all_sparse_metrics:\n         y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n         y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n-        assert K.eval(metric(y_a, y_b)).shape == ()\n+        assert K.eval(metric(y_a, y_b)).shape == (6,)\n \n \n def test_top_k_categorical_accuracy():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c0809c125521f77b4d850264c4dca072ebb0f67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 25 | Files Changed: 6 | Hunks: 23 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 21757 | Contributors (this commit): 84 | Commits (past 90d): 96 | Contributors (cumulative): 109 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1287,7 +1287,7 @@ class Container(Layer):\n         # Entries are unique. Includes input and output layers.\n         self.layers = []\n \n-        # this is for performance optimization\n+        # This is for performance optimization\n         # when calling the Container on new inputs.\n         # every time the Container is called on a set on input tensors,\n         # we compute the output tensors,\n@@ -1298,7 +1298,7 @@ class Container(Layer):\n         self._output_tensor_cache = {}\n         self._output_shape_cache = {}\n \n-        # Arguments validation.\n+        # User-provided arguments validation.\n         for x in self.inputs:\n             # Check that x is a Keras tensor.\n             if not hasattr(x, '_keras_history'):\n\n@@ -45,6 +45,9 @@ def _standardize_input_data(data, names, shapes=None,\n \n     # Returns\n         List of standardized input arrays (one array per model input).\n+\n+    # Raises\n+        ValueError: in case of improperly formatted user-provided data.\n     \"\"\"\n     if isinstance(data, dict):\n         arrays = []\n@@ -183,7 +186,7 @@ def _check_array_lengths(inputs, targets, weights):\n         targets: list of Numpy arrays of targets.\n         weights: list of Numpy arrays of sample weights.\n \n-    # Raises:\n+    # Raises\n         ValueError: in case of incorrectly formatted data.\n     \"\"\"\n     x_lengths = [x.shape[0] for x in inputs]\n@@ -213,14 +216,14 @@ def _check_array_lengths(inputs, targets, weights):\n                          str(list(set_w)[0]) + ' target samples.')\n \n \n-def _check_loss_and_target_compatibility(targets, losses, output_shapes):\n+def _check_loss_and_target_compatibility(targets, loss_fns, output_shapes):\n     \"\"\"Does validation on the compatiblity of targets and loss functions.\n \n     This helps prevent users from using loss functions incorrectly.\n \n     # Arguments\n         targets: list of Numpy arrays of targets.\n-        losses: list of loss functions.\n+        loss_fns: list of loss functions.\n         output_shapes: list of shapes of model outputs.\n \n     # Raises\n@@ -230,7 +233,7 @@ def _check_loss_and_target_compatibility(targets, losses, output_shapes):\n     key_losses = {'mean_square_error',\n                   'binary_crossentropy',\n                   'categorical_crossentropy'}\n-    for y, loss, shape in zip(targets, losses, output_shapes):\n+    for y, loss, shape in zip(targets, loss_fns, output_shapes):\n         if loss.__name__ == 'categorical_crossentropy':\n             if y.shape[-1] == 1:\n                 raise ValueError(\n@@ -361,7 +364,7 @@ def _slice_arrays(arrays, start=None, stop=None):\n             return arrays[start:stop]\n \n \n-def _weighted__masked_objective(fn):\n+def _weighted_masked_objective(fn):\n     \"\"\"Transforms an objective function `fn(y_true, y_pred)`\n     into a sample-weighted, cost-masked objective function\n     `fn(y_true, y_pred, weights, mask)`.\n@@ -414,8 +417,12 @@ def _masked_objective(fn):\n \n def _standardize_weights(y, sample_weight=None, class_weight=None,\n                          sample_weight_mode=None):\n-    \"\"\"Performs weight input validation and standardization\n-    to a single sample-wise (or timestep-wise) weight array.\n+    \"\"\"Performs sample weight validation and standardization.\n+\n+    Everything gets normalized to a single sample-wise (or timestep-wise)\n+    weight array.\n+\n+    # \n     \"\"\"\n     if sample_weight_mode is not None:\n         if sample_weight_mode != 'temporal':\n@@ -644,7 +651,7 @@ class Model(Container):\n             loss_function = losses.get(loss)\n             loss_functions = [loss_function for _ in range(len(self.outputs))]\n         self.loss_functions = loss_functions\n-        weighted_losses = [_weighted__masked_objective(fn) for fn in loss_functions]\n+        weighted_losses = [_weighted_masked_objective(fn) for fn in loss_functions]\n \n         # prepare output masks\n         masks = self.compute_mask(self.inputs, mask=None)\n\n@@ -730,9 +730,9 @@ class Conv2DTranspose(Conv2D):\n         input_shape = K.shape(inputs)\n         batch_size = input_shape[0]\n         if self.data_format == 'channels_first':\n-            c_axis, h_axis, w_axis = 1, 2, 3\n+            h_axis, w_axis = 2, 3\n         else:\n-            c_axis, h_axis, w_axis = 3, 1, 2\n+            h_axis, w_axis = 1, 2\n \n         height, width = input_shape[h_axis], input_shape[w_axis]\n         kernel_h, kernel_w = self.kernel_size\n@@ -775,7 +775,6 @@ class Conv2DTranspose(Conv2D):\n         else:\n             c_axis, h_axis, w_axis = 3, 1, 2\n \n-        height, width = input_shape[h_axis], input_shape[w_axis]\n         kernel_h, kernel_w = self.kernel_size\n         stride_h, stride_w = self.strides\n \n\n@@ -4,7 +4,7 @@ from numpy.testing import assert_allclose\n \n from keras.layers import Dense, Dropout\n from keras.engine.topology import Input\n-from keras.engine.training import Model, check_loss_and_target_compatibility\n+from keras.engine.training import Model, _check_loss_and_target_compatibility\n from keras.models import Sequential\n from keras import backend as K\n from keras.utils.test_utils import keras_test\n@@ -222,15 +222,15 @@ def test_trainable_argument():\n @keras_test\n def test_check_not_failing():\n     a = np.random.random((2, 1, 3))\n-    check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n-    check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, None, 3)])\n+    _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n+    _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, None, 3)])\n \n \n @keras_test\n def test_check_last_is_one():\n     a = np.random.random((2, 3, 1))\n     with pytest.raises(Exception) as exc:\n-        check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n+        _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n \n     assert \"You are passing a target array\" in str(exc)\n \n@@ -239,7 +239,7 @@ def test_check_last_is_one():\n def test_check_bad_shape():\n     a = np.random.random((2, 3, 5))\n     with pytest.raises(Exception) as exc:\n-        check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, 3, 6)])\n+        _check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, 3, 6)])\n \n     assert \"targets to have the same shape\" in str(exc)\n \n\n@@ -6,13 +6,12 @@ import numpy as np\n \n from keras import backend as K\n from keras.models import Sequential\n-from keras.layers import Dense, Activation, Lambda\n+from keras.layers import Dense, Activation\n from keras.utils import np_utils\n from keras.utils.test_utils import get_test_data, keras_test\n from keras.models import model_from_json, model_from_yaml\n from keras import losses\n-from keras.engine.training import make_batches\n-from keras.legacy.layers import Merge\n+from keras.engine.training import _make_batches\n \n \n input_dim = 16\n@@ -98,7 +97,7 @@ def test_sequential():\n     def data_generator(x, y, batch_size=50):\n         index_array = np.arange(len(x))\n         while 1:\n-            batches = make_batches(len(x_test), batch_size)\n+            batches = _make_batches(len(x_test), batch_size)\n             for batch_index, (batch_start, batch_end) in enumerate(batches):\n                 batch_ids = index_array[batch_start:batch_end]\n                 x_batch = x[batch_ids]\n\n@@ -2,7 +2,7 @@ import numpy as np\n import pytest\n \n from keras.models import Sequential\n-from keras.engine.training import weighted_objective\n+from keras.engine.training import _weighted_masked_objective\n from keras.layers import TimeDistributed, Masking, Dense\n from keras.utils.test_utils import keras_test\n from keras import losses\n@@ -26,7 +26,7 @@ def test_masking():\n \n @keras_test\n def test_loss_masking():\n-    weighted_loss = weighted_objective(losses.get('mae'))\n+    weighted_loss = _weighted_masked_objective(losses.get('mae'))\n     shape = (3, 4, 2)\n     x = np.arange(24).reshape(shape)\n     y = 2 * x\n@@ -40,7 +40,6 @@ def test_loss_masking():\n                                K.variable(y),\n                                K.variable(weights),\n                                K.variable(mask)))\n-    # TODO\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#871809bb8596a50057a1a97a795db16afe9a32ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 41 | Files Changed: 1 | Hunks: 38 | Methods Changed: 17 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 144 | Churn Cumulative: 6939 | Contributors (this commit): 27 | Commits (past 90d): 32 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -119,23 +119,35 @@ class Node(object):\n         # and for each layer, which node and which\n         # tensor output of each node.\n \n-        self.inbound_layers = inbound_layers  # List of layer instances\n-        self.node_indices = node_indices  # List of integers, 1:1 mapping with inbound_layers.\n-        self.tensor_indices = tensor_indices  # List of integers, 1:1 mapping with inbound_layers.\n+        # List of layer instances.\n+        self.inbound_layers = inbound_layers\n+        # List of integers, 1:1 mapping with inbound_layers.\n+        self.node_indices = node_indices\n+        # List of integers, 1:1 mapping with inbound_layers.\n+        self.tensor_indices = tensor_indices\n \n-        # Tensor inputs and outputs of outbound_layer.\n-        self.input_tensors = input_tensors  # List of tensors. 1:1 mapping with inbound_layers.\n-        self.output_tensors = output_tensors  # List of tensors, created by outbound_layer.call().\n+        # Following 2 properties:\n+        # tensor inputs and outputs of outbound_layer.\n \n-        # input and output masks\n-        self.input_masks = input_masks  # List of tensors, 1:1 mapping with input_tensor.\n-        self.output_masks = output_masks  # List of tensors, created by outbound_layer.compute_mask().\n+        # List of tensors. 1:1 mapping with inbound_layers.\n+        self.input_tensors = input_tensors\n+        # List of tensors, created by outbound_layer.call().\n+        self.output_tensors = output_tensors\n \n-        # input and output shapes\n-        self.input_shapes = input_shapes  # List of shape tuples, shapes of input_tensors.\n-        self.output_shapes = output_shapes  # List of shape tuples, shapes of output_tensors.\n+        # Following 2 properties: input and output masks.\n+        # List of tensors, 1:1 mapping with input_tensor.\n+        self.input_masks = input_masks\n+        # List of tensors, created by outbound_layer.compute_mask().\n+        self.output_masks = output_masks\n \n-        # Optional keyword arguments to layer's `call`:\n+        # Following 2 properties: input and output shapes.\n+\n+        # List of shape tuples, shapes of input_tensors.\n+        self.input_shapes = input_shapes\n+        # List of shape tuples, shapes of output_tensors.\n+        self.output_shapes = output_shapes\n+\n+        # Optional keyword arguments to layer's `call`.\n         self.arguments = arguments\n \n         # Add nodes to all layers involved.\n@@ -322,13 +334,15 @@ class Layer(object):\n         # Arguments\n             shape: The shape tuple of the weight.\n             initializer: An Initializer instance (callable).\n+            name: String, the name for the weight variable.\n             trainable: A boolean, whether the weight should\n                 be trained via backprop or not (assuming\n                 that the layer itself is also trainable).\n             regularizer: An optional Regularizer instance.\n+            constraint: An optional Constraint instance.\n         \"\"\"\n         initializer = initializers.get(initializer)\n-        weight = K.variable(initializer(shape), dtype=K.floatx())\n+        weight = K.variable(initializer(shape), dtype=K.floatx(), name=name)\n         if regularizer is not None:\n             self.add_loss(regularizer(weight))\n         if constraint is not None:\n@@ -340,9 +354,15 @@ class Layer(object):\n         return weight\n \n     def assert_input_compatibility(self, inputs):\n-        \"\"\"This checks that the tensor(s) `input`\n+        \"\"\"Checks compatibility between the layer and provided inputs.\n+\n+        This checks that the tensor(s) `input`\n         verify the input assumptions of the layer\n         (if any). If not, exceptions are raised.\n+\n+        # Raises\n+            ValueError: in case of mismatch between\n+                the provided inputs and the expectations of the layer.\n         \"\"\"\n         if not self.input_spec:\n             return True\n@@ -425,20 +445,19 @@ class Layer(object):\n                                     str(spec.shape) + ', found shape=' +\n                                     str(x_shape))\n \n-    def call(self, x):\n+    def call(self, inputs):\n         \"\"\"This is where the layer's logic lives.\n \n         # Arguments\n-            x: input tensor, or list/tuple of input tensors.\n+            inputs: input tensor, or list/tuple of input tensors.\n \n-        # Returns:\n+        # Returns\n             A tensor or list/tuple of tensors.\n         \"\"\"\n-        return x\n+        return inputs\n \n-    def __call__(self, x, **kwargs):\n-        \"\"\"Wrapper around self.call(), for handling\n-        internal Keras references.\n+    def __call__(self, inputs, **kwargs):\n+        \"\"\"Wrapper around self.call(), for handling internal references.\n \n         If a Keras tensor is passed:\n             - We call self._add_inbound_node().\n@@ -454,17 +473,24 @@ class Layer(object):\n         # Arguments\n             x: Can be a tensor or list/tuple of tensors.\n             **kwargs: Additional keyword arguments to be passed to `call()`.\n+\n+        # Returns\n+            Output of the layer's `call` method.\n+\n+        # Raises\n+            ValueError: in case the layer is missing shape information\n+                for its `build` call.\n         \"\"\"\n         with K.name_scope(self.name):\n             # Handle laying building (weight creating, input spec locking).\n             if not self.built:\n                 # Raise exceptions in case the input is not compatible\n                 # with the input_spec specified in the layer constructor.\n-                self.assert_input_compatibility(x)\n+                self.assert_input_compatibility(inputs)\n \n                 # Collect input shapes to build layer.\n                 input_shapes = []\n-                for x_elem in _to_list(x):\n+                for x_elem in _to_list(inputs):\n                     if hasattr(x_elem, '_keras_shape'):\n                         input_shapes.append(x_elem._keras_shape)\n                     elif hasattr(K, 'int_shape'):\n@@ -488,10 +514,10 @@ class Layer(object):\n \n             # Raise exceptions in case the input is not compatible\n             # with the input_spec set at build time.\n-            self.assert_input_compatibility(x)\n+            self.assert_input_compatibility(inputs)\n \n             # Handle mask propagation.\n-            previous_mask = _collect_previous_mask(x)\n+            previous_mask = _collect_previous_mask(inputs)\n             if not _is_all_none(previous_mask):\n                 # The previous layer generated a mask.\n                 if 'mask' in inspect.getargspec(self.call).args:\n@@ -500,11 +526,11 @@ class Layer(object):\n                         # we should override the default mask.\n                         kwargs['mask'] = previous_mask\n             # Handle automatic shape inference (only useful for Theano).\n-            input_shape = _collect_input_shape(x)\n+            input_shape = _collect_input_shape(inputs)\n \n             # Actually call the layer, collecting output(s), mask(s), and shape(s).\n-            output = self.call(x, **kwargs)\n-            output_mask = self.compute_mask(x, previous_mask)\n+            output = self.call(inputs, **kwargs)\n+            output_mask = self.compute_mask(inputs, previous_mask)\n \n             # Infering the output shape is only relevant for Theano.\n             if all([s is not None for s in _to_list(input_shape)]):\n@@ -520,7 +546,7 @@ class Layer(object):\n             # This also updates the layer history of the output tensor(s).\n             # If the input tensor(s) had not previous Keras history,\n             # this does nothing.\n-            self._add_inbound_node(input_tensors=x, output_tensors=output,\n+            self._add_inbound_node(input_tensors=inputs, output_tensors=output,\n                                    input_masks=previous_mask, output_masks=output_mask,\n                                    input_shapes=input_shape, output_shapes=output_shape,\n                                    arguments=kwargs)\n@@ -538,7 +564,14 @@ class Layer(object):\n         \"\"\"Internal method to create an inbound node for the layer.\n \n         # Arguments\n-            TODO\n+            input_tensors: list of input tensors.\n+            output_tensors: list of output tensors.\n+            input_masks: list of input masks (a mask can be a tensor, or None).\n+            output_masks: list of output masks (a mask can be a tensor, or None).\n+            input_shapes: list of input shape tuples.\n+            output_shapes: list of output shape tuples.\n+            arguments: dictionary of keyword arguments that were passed to the\n+                `call` method of the layer at the call that created the node.\n         \"\"\"\n         input_tensors = _to_list(input_tensors)\n         output_tensors = _to_list(output_tensors)\n@@ -588,25 +621,28 @@ class Layer(object):\n                                                 i)\n \n     def compute_output_shape(self, input_shape):\n-        \"\"\"Computes the output shape of the layer given\n-        an input shape (assumes that the layer will be built\n-        to match that input shape).\n+        \"\"\"Computes the output shape of the layer.\n+\n+        Assumes that the layer will be built\n+        to match that input shape provided.\n \n         # Arguments\n             input_shape: Shape tuple (tuple of integers)\n                 or list of shape tuples (one per output tensor of the layer).\n                 Shape tuples can include None for free dimensions,\n                 instead of an integer.\n+\n+        # Returns\n+            An input shape tuple.\n         \"\"\"\n         return input_shape\n \n     def compute_mask(self, inputs, mask=None):\n-        \"\"\"Computes an output masking tensor, given an input tensor\n-        (or list thereof) and an input mask (or list thereof).\n+        \"\"\"Computes an output mask tensor.\n \n         # Arguments\n             inputs: Tensor or list of tensors.\n-            input_mask: Tensor or list of tensors.\n+            mask: Tensor or list of tensors.\n \n         # Returns\n             None or a tensor (or list of tensors,\n@@ -633,6 +669,7 @@ class Layer(object):\n \n     def build(self, input_shape):\n         \"\"\"Creates the layer weights.\n+\n         Must be implemented on all layers that have weights.\n \n         # Arguments\n@@ -656,6 +693,9 @@ class Layer(object):\n                 to retrieve the attribute.\n             attr: Exact node attribute name.\n             attr_name: Human-readable attribute name, for error messages.\n+\n+        # Returns\n+            The layer's attribute `attr` at the node of index `node_index`.\n         \"\"\"\n         if not self.inbound_nodes:\n             raise RuntimeError('The layer has never been called '\n@@ -2279,6 +2319,9 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n         layer: Origin layer of the tensor. Will be\n             determined via tensor._keras_history if not provided.\n         node_index: Origin node index of the tensor.\n+\n+    # Returns\n+        List of input tensors.\n     \"\"\"\n     if not hasattr(tensor, '_keras_history'):\n         return tensor\n@@ -2309,10 +2352,16 @@ def get_source_inputs(tensor, layer=None, node_index=None):\n \n \n def _to_list(x):\n-    \"\"\"This normalizes a list/tensor into a list.\n+    \"\"\"Normalizes a list/tensor into a list.\n \n     If a tensor is passed, we return\n     a list of size 1 containing the tensor.\n+\n+    # Arguments\n+        x: target object to be normalized.\n+\n+    # Returns\n+        A list.\n     \"\"\"\n     if isinstance(x, list):\n         return x\n@@ -2363,7 +2412,14 @@ def _to_snake_case(name):\n \n \n def _collect_input_shape(input_tensors):\n-    # Return the output shape(s) of a list of Keras tensors.\n+    \"\"\"Collects the output shape(s) of a list of Keras tensors.\n+\n+    # Arguments\n+        input_tensors: list of input tensors (or single input tensor).\n+\n+    # Returns\n+        List of shape tuples (or single tuple), one tuple per input.\n+    \"\"\"\n     input_tensors = _to_list(input_tensors)\n     shapes = []\n     for x in input_tensors:\n@@ -2413,7 +2469,7 @@ def load_weights_from_hdf5_group(f, layers):\n     for name in layer_names:\n         g = f[name]\n         weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n-        if len(weight_names):\n+        if weight_names:\n             filtered_layer_names.append(name)\n     layer_names = filtered_layer_names\n     if len(layer_names) != len(filtered_layers):\n@@ -2447,9 +2503,15 @@ def load_weights_from_hdf5_group(f, layers):\n \n \n def load_weights_from_hdf5_group_by_name(f, layers):\n-    \"\"\" Name-based weight loading\n+    \"\"\"Implements name-based weight loading.\n+\n     (instead of topological weight loading).\n+\n     Layers that have no matching name are skipped.\n+\n+    # Arguments\n+        f: A pointer to a HDF5 group.\n+        layers: a list of target layers.\n     \"\"\"\n     # New file format.\n     layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
