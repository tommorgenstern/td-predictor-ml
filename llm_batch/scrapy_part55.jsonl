{"custom_id": "scrapy#ead5466998f6b5237ae69bc57199ec6446e17003", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 8 | Churn Cumulative: 8 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -45,7 +45,7 @@ class ScrapyCommand(object):\n         parser.add_option(\"--nolog\", dest=\"nolog\", action=\"store_true\", help=\"disable all log messages\")\n         parser.add_option(\"--profile\", dest=\"profile\", default=None, help=\"write profiling stats in FILE, to analyze later with: python -m pstats FILE\", metavar=\"FILE\")\n         parser.add_option(\"--pidfile\", dest=\"pidfile\", help=\"Write process pid to file FILE\", metavar=\"FILE\")\n-        parser.add_option(\"--set\", dest=\"settings\", action=\"append\", metavar=\"SETTING:VALUE\", default=[], \n+        parser.add_option(\"--set\", dest=\"settings\", action=\"append\", metavar=\"SETTING=VALUE\", default=[],\n                             help=\"Override Scrapy setting SETTING with VALUE. May be repeated.\")\n         \n     def process_options(self, args, opts):\n@@ -73,11 +73,11 @@ class ScrapyCommand(object):\n             open(opts.pidfile, \"w\").write(str(pid))\n \n         for setting in opts.settings:\n-            if ':' in setting:\n-                name, val = setting.split(':', 1)\n+            if '=' in setting:\n+                name, val = setting.split('=', 1)\n                 settings.overrides[name] = val\n             else:\n-                sys.stderr.write(\"%s: invalid argument --set=%s - proper format is --set=SETTING:VALUE'\\n\" % (sys.argv[0], setting))\n+                sys.stderr.write(\"%s: invalid argument --set %s - proper format is --set SETTING=VALUE'\\n\" % (sys.argv[0], setting))\n                 sys.exit(2)\n \n     def run(self, args, opts):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#584a844e719e5d816738ae07c98a1caff68b7480", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n\n@@ -93,5 +93,5 @@ setup(\n     packages = packages,\n     cmdclass = cmdclasses,\n     data_files = data_files,\n-    scripts = ['scrapy/bin/scrapy-admin.py'],\n+    scripts = ['bin/scrapy-admin.py'],\n )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2c93bc40c6f3647702b4cccf60888a7f2e155720", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ class DecompressionMiddleware(object):\n \n     def is_tar(self, response):\n         try:\n-            tar_file = tarfile.open(name='tar.tmp', fileobj=self.archive)\n+            tar_file = tarfile.open(fileobj=self.archive)\n         except tarfile.ReadError:\n             return False\n         if tar_file.members:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9022f84c05a13d58a03f24cc65439ccc109ee57b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ import tarfile\n import gzip\n import bz2\n from cStringIO import StringIO\n+from tempfile import mktemp\n \n from scrapy import log\n from scrapy.http import Response\n@@ -26,7 +27,7 @@ class DecompressionMiddleware(object):\n \n     def is_tar(self, response):\n         try:\n-            tar_file = tarfile.open(fileobj=self.archive)\n+            tar_file = tarfile.open(name=mktemp(), fileobj=self.archive)\n         except tarfile.ReadError:\n             return False\n         if tar_file.members:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d23dcbb39084b96626f0e46119711a675a198954", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 144 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -320,12 +320,11 @@ class ExecutionEngine(object):\n \n     def _run_starters(self, spider):\n         domain = spider.domain_name\n-        starters = self.starters.get(domain, [])\n+        starters = self.starters.pop(domain, [])\n         while starters:\n             request, deferred = starters.pop(0)\n             schd = self.schedule(request, spider)\n             chain_deferred(schd, deferred)\n-        del self.starters[domain]\n \n     def download(self, request, spider):\n         if self.debug_mode:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e716fad03deed5de5f132a40302c3f1305709c11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 148 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,6 +7,7 @@ For more information see docs/topics/architecture.rst\n from datetime import datetime\n \n from twisted.internet import defer, reactor, task\n+from twisted.internet.error import CannotListenError\n from twisted.python.failure import Failure\n from pydispatch import dispatcher\n \n@@ -126,7 +127,10 @@ class ExecutionEngine(object):\n             for tsk, interval, now in self.tasks:\n                 tsk.start(interval, now)\n             for args, kwargs in [t for t in self.ports if isinstance(t, tuple)]:\n+                try:\n                     reactor.listenTCP(*args, **kwargs)\n+                except CannotListenError:\n+                    log.msg(\"Cannot listen on TCP port %d\" % args[0], level=log.ERROR)\n             self.running = True\n             if control_reactor:\n                 reactor.run() # blocking call\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b040e6a3a74ab47334bb8d7be2220a0f64199bcc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 20 | Churn Cumulative: 24 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,20 +0,0 @@\n-from scrapy.contrib.spiders import CrawlSpider, Rule\n-from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n-\n-class GenericSpider(CrawlSpider):\n-    \"\"\"\n-    This generic crawler crawls an entire site. It can also be used as a default crawler\n-    (see setting \"DEFAULT_SPIDER\")\n-    \"\"\"\n-    \n-    def __init__(self, domain_name):\n-        self.domain_name = domain_name\n-        self.rules = (\n-            Rule(SgmlLinkExtractor(allow_domains=(domain_name,)), self.parse_note, follow=True),\n-        )\n-        super(GenericSpider, self).__init__()\n-        \n-    def parse_note(self, response):\n-        pass\n-\n-# not a singleton\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cb12e5dd3bb957848edc18c2599fcc1a657e668", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 78 | Files Changed: 6 | Hunks: 18 | Methods Changed: 20 | Complexity Δ (Sum/Max): 42/26 | Churn Δ: 137 | Churn Cumulative: 362 | Contributors (this commit): 2 | Commits (past 90d): 21 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,7 +8,7 @@ See documentation in docs/ref/spiders.rst\n import copy\n \n from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n+from scrapy.contrib.spiders.init import InitSpider\n from scrapy.conf import settings\n \n class Rule(object):\n@@ -45,7 +45,7 @@ class Rule(object):\n         else:\n             self.follow = follow\n \n-class CrawlSpider(BaseSpider):\n+class CrawlSpider(InitSpider):\n     \"\"\"\n     Class for spiders that crawl over web pages and extract/parse their links\n     given some crawling rules.\n\n@@ -5,14 +5,14 @@ for scraping from an XML feed.\n See documentation in docs/ref/spiders.rst\n \"\"\"\n \n-from scrapy.spider import BaseSpider\n+from scrapy.contrib.spiders.init import InitSpider\n from scrapy.item import ScrapedItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n from scrapy.core.exceptions import NotConfigured, NotSupported\n \n-class XMLFeedSpider(BaseSpider):\n+class XMLFeedSpider(InitSpider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n     from XML feeds.\n@@ -88,7 +88,7 @@ class XMLFeedSpider(BaseSpider):\n         for (prefix, uri) in self.namespaces:\n             selector.register_namespace(prefix, uri)\n \n-class CSVFeedSpider(BaseSpider):\n+class CSVFeedSpider(InitSpider):\n     \"\"\"Spider for parsing CSV feeds.\n     It receives a CSV file in a response; iterates through each of its rows,\n     and calls parse_row with a dict containing each field's data.\n\n@@ -0,0 +1,44 @@\n+from scrapy.spider import BaseSpider\n+\n+class InitSpider(BaseSpider):\n+    \"\"\"Base Spider with initialization facilities\"\"\"\n+    \n+    def __init__(self):\n+        super(InitSpider, self).__init__()\n+        self._postinit_reqs = []\n+        self._init_complete = False\n+        self._init_started = False\n+\n+    def make_requests_from_url(self, url):\n+        req = super(InitSpider, self).make_requests_from_url(url)\n+        if self._init_complete:\n+            return req\n+        self._postinit_reqs.append(req)\n+        if not self._init_started:\n+            self._init_started = True\n+            return self.init_request()\n+\n+    def initialized(self, response=None):\n+        \"\"\"This method must be set as the callback of your last initialization\n+        request. See self.init_request() docstring for more info.\n+        \"\"\"\n+        self._init_complete = True\n+        reqs = self._postinit_reqs[:]\n+        del self._postinit_reqs\n+        return reqs\n+\n+    def init_request(self):\n+        \"\"\"This function should return one initialization request, with the\n+        self.initialized method as callback. When the self.initialized method\n+        is called this spider is considered initialized. If you need to perform\n+        several requests for initializing your spider, you can do so by using\n+        different callbacks. The only requirement is that the final callback\n+        (of the last initialization request) must be self.initialized. \n+        \n+        The default implementation calls self.initialized immediately, and\n+        means that no initialization is needed. This method should be\n+        overridden only when you need to perform requests to initialize your\n+        spider\n+        \"\"\"\n+        return self.initialized()\n+\n\n@@ -23,7 +23,7 @@ from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipelineManager\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n-from scrapy.utils.defer import chain_deferred, defer_succeed, mustbe_deferred, deferred_imap\n+from scrapy.utils.defer import chain_deferred, deferred_imap\n from scrapy.utils.request import request_info\n \n class ExecutionEngine(object):\n@@ -50,7 +50,6 @@ class ExecutionEngine(object):\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n-        self.initializing = set() # domais in intialization state\n         self.cancelled = set() # domains in cancelation state\n         self.debug_mode = settings.getbool('ENGINE_DEBUG')\n         self.tasks = []\n@@ -216,8 +215,7 @@ class ExecutionEngine(object):\n         pending = self.scheduler.domain_has_pending_requests(domain)\n         downloading = not self.downloader.domain_is_idle(domain)\n         haspipe = not self.pipeline.domain_is_idle(domain)\n-        oninit = domain in self.initializing\n-        return not (pending or downloading or haspipe or oninit or scraping)\n+        return not (pending or downloading or haspipe or scraping)\n \n     def domain_is_open(self, domain):\n         return domain in self.downloader.sites\n@@ -369,57 +367,6 @@ class ExecutionEngine(object):\n         dwld.addBoth(_on_complete)\n         return deferred\n \n-    def initialize(self, spider):\n-        domain = spider.domain_name\n-        if not hasattr(spider, 'init_domain'):\n-            return defer_succeed(True)\n-\n-        def _initialize(req):\n-            if isinstance(req, Request):\n-                _response = None\n-                def _referer(response):\n-                    req.deferred.addCallback(_setreferer, response)\n-                    return response\n-\n-                def _setreferer(result, response):\n-                    if isinstance(result, Request):\n-                        result.headers.setdefault('Referer', response.url)\n-                    return result\n-\n-                def _onerror(_failure):\n-                    ex = _failure.value\n-                    if isinstance(ex, IgnoreRequest):\n-                        log.msg(ex.message, log.DEBUG, domain=domain)\n-                    else:\n-                        return _failure\n-\n-                schd = self.schedule(req, spider)\n-                schd.addCallback(_referer)\n-                chain_deferred(schd, req.deferred)\n-                schd.addErrback(_onerror)\n-                schd.addBoth(_initialize)\n-                return schd\n-            return req\n-\n-        def _bugtrap(_failure):\n-            log.msg(\"Bug in %s init_domain code: %s\" % (domain, _failure), log.ERROR, domain=domain)\n-\n-        def _state(state):\n-            self.initializing.remove(domain)\n-            if state is True:\n-                log.msg('Succeded initialization for %s' % domain, log.INFO, domain=domain)\n-            else:\n-                log.msg('Failed initialization for %s' % domain, log.INFO, domain=domain)\n-            return state\n-\n-        log.msg('Started initialization for %s' % domain, log.INFO, domain=domain)\n-        self.initializing.add(domain)\n-        req = spider.init_domain()\n-        deferred = mustbe_deferred(_initialize, req)\n-        deferred.addErrback(_bugtrap)\n-        deferred.addCallback(_state)\n-        return deferred\n-\n     def open_domain(self, domain, spider=None):\n         log.msg(\"Domain opened\", domain=domain)\n         spider = spider or spiders.fromdomain(domain)\n@@ -431,15 +378,8 @@ class ExecutionEngine(object):\n         self._scraping[domain] = set()\n         signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)\n \n-        # init_domain\n-        dfd = self.initialize(spider)\n-        def _state(state):\n-            if state is True:\n         signals.send_catch_log(signals.domain_opened, sender=self.__class__, domain=domain, spider=spider)\n         self._run_starters(spider)\n-            else:\n-                self._domain_idle(domain)\n-        dfd.addCallback(_state)\n \n     def _domain_idle(self, domain):\n         \"\"\"Called when a domain gets idle. This function is called when there are no\n\n@@ -7,7 +7,7 @@ from scrapy import log\n from scrapy.http import Request\n from scrapy.core.engine import scrapyengine\n from scrapy.spider import spiders\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, arg_to_iter\n from scrapy.utils.url import is_url\n from scrapy.conf import settings\n \n@@ -126,7 +126,7 @@ class ExecutionManager(object):\n         for url in urls:\n             spider = spiders.fromurl(url)\n             if spider:\n-                req = spider.make_request_from_url(url)\n+                for req in arg_to_iter(spider.make_requests_from_url(url)):\n                     perdomain.setdefault(spider.domain_name, []).append(req)\n             else:\n                 log.msg('Could not find spider for <%s>' % url, log.ERROR)\n\n@@ -8,6 +8,7 @@ from twisted.plugin import IPlugin\n \n from scrapy import log\n from scrapy.http import Request\n+from scrapy.utils.misc import arg_to_iter\n \n def _valid_domain_name(obj):\n     \"\"\"Check the domain name specified is valid\"\"\"\n@@ -39,13 +40,6 @@ class ISpider(Interface, IPlugin) :\n     invariant(_valid_domain_name)\n     invariant(_valid_download_delay)\n \n-    def init_domain(self):\n-        \"\"\"This is first called to initialize domain specific quirks, like \n-        session cookies or login stuff\n-        \"\"\"\n-        pass\n-\n-\n class BaseSpider(object):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n     class.\n@@ -64,9 +58,12 @@ class BaseSpider(object):\n         log.msg(message, domain=self.domain_name, level=level)\n \n     def start_requests(self):\n-        return [self.make_request_from_url(url) for url in self.start_urls]\n+        reqs = []\n+        for url in self.start_urls:\n+            reqs.extend(arg_to_iter(self.make_requests_from_url(url)))\n+        return reqs\n \n-    def make_request_from_url(self, url):\n+    def make_requests_from_url(self, url):\n         return Request(url, callback=self.parse, dont_filter=True)\n \n     def parse(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#161335fe7840816c0bd2d5087fa4fb34c6745a74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 120 | Files Changed: 8 | Hunks: 25 | Methods Changed: 26 | Complexity Δ (Sum/Max): 33/43 | Churn Δ: 175 | Churn Cumulative: 650 | Contributors (this commit): 2 | Commits (past 90d): 38 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -54,6 +54,8 @@ DEFAULT_SPIDER = None\n DEPTH_LIMIT = 0\n DEPTH_STATS = True\n \n+DOMAIN_SCHEDULER = 'scrapy.contrib.domainsch.FifoDomainScheduler'\n+\n DOWNLOAD_DELAY = 0\n DOWNLOAD_TIMEOUT = 180      # 3mins\n \n@@ -137,8 +139,6 @@ MYSQL_CONNECTION_SETTINGS = {}\n \n NEWSPIDER_MODULE = ''\n \n-PRIORITIZER = 'scrapy.core.prioritizers.RandomPrioritizer'\n-\n REDIRECT_MAX_METAREFRESH_DELAY = 100\n REDIRECT_MAX_TIMES = 20 # uses Firefox default setting\n \n@@ -149,7 +149,6 @@ REQUESTS_PER_DOMAIN = 8     # max simultaneous requests per domain\n RETRY_TIMES = 2 # initial response + 2 retries = 3 requests\n RETRY_HTTP_CODES = ['500', '503', '504', '400', '408']\n \n-\n ROBOTSTXT_OBEY = False\n \n SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n\n@@ -0,0 +1,36 @@\n+\"\"\"\n+Domain Schedulers keep track of next domains to scrape. They must implement the\n+following methods:\n+\n+* next_domain()\n+  return next domain to scrape and remove it from pending queue\n+\n+* add_domain(domain)\n+  add domain to pending domains to scrape\n+\n+* remove_pending_domain(domain)\n+  remove domain from pendings, do nothing if not pending\n+\n+* has_pending_domain(domain)\n+  Return ``True`` if the domain is pending, ``False`` otherwise\n+\n+\"\"\"\n+\n+class FifoDomainScheduler(object):\n+    \"\"\"Basic domain scheduler based on a FIFO queue\"\"\"\n+\n+    def __init__(self):\n+        self.pending_domains = []\n+\n+    def next_domain(self) :\n+        if self.pending_domains:\n+            return self.pending_domains.pop(0)\n+\n+    def add_domain(self, domain):\n+        self.pending_domains.append(domain)\n+\n+    def remove_pending_domain(self, domain):\n+        self.pending_domains.remove(domain)\n+\n+    def has_pending_domain(self, domain):\n+        return domain in self.pending_domains\n\n@@ -1,37 +0,0 @@\n-import time\n-\n-from scrapy.core.exceptions import NotConfigured\n-from scrapy.store.db import DomainDataHistory\n-from scrapy.conf import settings\n-\n-class LessScrapedPrioritizer(object):\n-    \"\"\"\n-    A spider prioritizer based on these few simple rules:\n-    1. if spider was never scraped, it has top priority\n-    2. if spider was scraped before, then the less recently the spider\n-       has been scraped, the more priority it has\n-    \"\"\"\n-    def __init__(self):\n-        # FIXME this prioritizer must be refactored\n-        raise NotImplemented\n-\n-        if not settings['SCRAPING_DB']:\n-            raise NotConfigured(\"SCRAPING_DB setting is required\")\n-\n-        self.ddh = DomainDataHistory(settings['SCRAPING_DB'], 'domain_data_history')\n-        domains_to_scrape = set(elements)\n-\n-        self.priorities = {}\n-        \n-        for domain in domains_to_scrape:\n-            stat = self.ddh.getlast(domain, path=\"start_time\")\n-            if stat and stat[1]:\n-                last_started = stat[1]\n-                # spider is the timestamp of last start time\n-                self.priorities[domain] = time.mktime(last_started.timetuple())  \n-            else:\n-                # if domain was never scraped, it has top priority\n-                self.priorities[domain] = 1\n-\n-    def get_priority(self, element):\n-        return self.priorities[element]\n\n@@ -155,7 +155,7 @@ class Spiderctl(object):\n         if \"remove_pending_domains\" in args:\n             removed = []\n             for domain in args[\"remove_pending_domains\"]:\n-                if scrapyengine.scheduler.remove_pending_domain(domain):\n+                if scrapyengine.domain_scheduler.remove_pending_domain(domain):\n                     removed.append(domain)\n             if removed:\n                 s += \"<p>\"\n\n@@ -25,6 +25,7 @@ from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, deferred_imap\n from scrapy.utils.request import request_info\n+from scrapy.utils.misc import load_object\n \n class ExecutionEngine(object):\n     \"\"\"\n@@ -64,6 +65,7 @@ class ExecutionEngine(object):\n         \"\"\"\n         self.scheduler = scheduler or Scheduler()\n         self.schedulermiddleware = SchedulerMiddlewareManager(self.scheduler)\n+        self.domain_scheduler = load_object(settings['DOMAIN_SCHEDULER'])()\n         self.downloader = downloader or Downloader(self)\n         self.spidermiddleware = SpiderMiddlewareManager()\n         self._scraping = {}\n@@ -163,7 +165,7 @@ class ExecutionEngine(object):\n         return self.scheduler.is_idle() and self.pipeline.is_idle() and self.downloader.is_idle() and not self._scraping\n \n     def next_domain(self):\n-        domain = self.scheduler.next_domain()\n+        domain = self.domain_scheduler.next_domain()\n         if domain:\n             spider = spiders.fromdomain(domain)\n             self.open_domain(domain, spider)\n@@ -224,7 +226,7 @@ class ExecutionEngine(object):\n     def open_domains(self):\n         return self.downloader.sites.keys()\n \n-    def crawl(self, request, spider, domain_priority=0):\n+    def crawl(self, request, spider):\n         domain = spider.domain_name\n \n         def _process_response(response):\n@@ -282,16 +284,16 @@ class ExecutionEngine(object):\n             request.deferred.addErrback(lambda _:None)\n             request.deferred.errback(_failure) # TODO: merge into spider middleware.\n \n-        schd = self.schedule(request, spider, domain_priority)\n+        schd = self.schedule(request, spider)\n         schd.addCallbacks(_process_response, _cleanfailure)\n         return schd\n \n-    def schedule(self, request, spider, domain_priority=0):\n+    def schedule(self, request, spider):\n         domain = spider.domain_name\n         if not self.scheduler.domain_is_open(domain):\n             if self.debug_mode: \n                 log.msg('Scheduling %s (delayed)' % request_info(request), log.DEBUG)\n-            return self._add_starter(request, spider, domain_priority)\n+            return self._add_starter(request, spider)\n         if self.debug_mode: \n             log.msg('Scheduling %s (now)' % request_info(request), log.DEBUG)\n         schd = self.schedulermiddleware.enqueue_request(domain, request)\n@@ -311,10 +313,10 @@ class ExecutionEngine(object):\n             if not self.next_domain():\n                 return self._stop_if_idle()\n \n-    def _add_starter(self, request, spider, domain_priority):\n+    def _add_starter(self, request, spider):\n         domain = spider.domain_name\n-        if not self.scheduler.domain_is_pending(domain):\n-            self.scheduler.add_domain(domain, priority=domain_priority)\n+        if not self.domain_scheduler.has_pending_domain(domain):\n+            self.domain_scheduler.add_domain(domain)\n             self.starters[domain] = []\n         deferred = defer.Deferred()\n         self.starters[domain] += [(request, deferred)]\n\n@@ -40,7 +40,6 @@ class ExecutionManager(object):\n \n         scheduler = load_object(settings['SCHEDULER'])()\n         scrapyengine.configure(scheduler=scheduler)\n-        self.domainprio = load_object(settings['PRIORITIZER'])()\n         \n     def crawl(self, *args):\n         \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n@@ -49,9 +48,8 @@ class ExecutionManager(object):\n         # schedule initial requests to be scraped at engine start\n         for domain in requests or ():\n             spider = spiders.fromdomain(domain) \n-            priority = self.domainprio.get_priority(domain)\n             for request in requests[domain]:\n-                scrapyengine.crawl(request, spider, domain_priority=priority)\n+                scrapyengine.crawl(request, spider)\n \n     def runonce(self, *args):\n         \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n\n@@ -1,29 +0,0 @@\n-\"\"\"\n-The Prioritizer is a class which receives a list of elements and prioritizes\n-it. It's used for defining the order in which domains are to be scraped. \n-\n-A Prioritizer basically consists of a class which receives a list of elements\n-in its constructs and contains only one method: get_priority() which returns\n-the priority of the given element. The element passed to get_priority() must\n-exists in the list of elements passed in the constructor.\n-\n-This module contains several basic prioritizers.\n-\n-For more advanced prioritizers see: scrapy.contrib.prioritizers\n-\"\"\"\n-import random\n-\n-class NullPrioritizer(object):\n-    \"\"\"\n-    This prioritizer always return the same priority (1)\n-    \"\"\"\n-    def get_priority(self, element):\n-        return 1\n-\n-class RandomPrioritizer(object):\n-    \"\"\"\n-    This prioritizer always return a random priority\n-    \"\"\"\n-    def get_priority(self, element):\n-        return random.randrange(0, 1000)\n-\n\n@@ -16,8 +16,6 @@ class Scheduler(object):\n     \"\"\"\n \n     def __init__(self):\n-        self.pending_domains = set()\n-        self.domains_queue = PriorityQueue()\n         self.pending_requests = {}\n         self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n \n@@ -25,37 +23,20 @@ class Scheduler(object):\n         \"\"\"Check if scheduler's resources were allocated for a domain\"\"\"\n         return domain in self.pending_requests\n \n-    def domain_is_pending(self, domain):\n-        \"\"\"Check if a domain is waiting to be scraped in domain's queue.\"\"\"\n-        return domain in self.pending_domains\n-\n     def domain_has_pending_requests(self, domain):\n         \"\"\"Check if are there pending requests for a domain\"\"\"\n         if domain in self.pending_requests:\n             return bool(self.pending_requests[domain])\n \n-    def next_domain(self) :\n-        \"\"\"Return next domain available to scrape and remove it from available domains queue\"\"\"\n-        if self.pending_domains:\n-            domain = self.domains_queue.pop()[0]\n-            self.pending_domains.remove(domain)\n-            return domain\n-\n-    def add_domain(self, domain, priority=0):\n-        \"\"\"Add a new domain to be scraped, with the given priority. If the\n-        domain is already scheduled, it does nothing.\n-        \"\"\"\n-        if domain not in self.pending_domains:\n-            self.domains_queue.push(domain, priority)\n-            self.pending_domains.add(domain)\n-\n     def open_domain(self, domain):\n         \"\"\"Allocates scheduling resources for the given domain\"\"\"\n         Priority = PriorityStack if self.dfo else PriorityQueue\n         self.pending_requests[domain] = Priority()\n \n     def enqueue_request(self, domain, request):\n-        \"\"\"Enqueue a request to be downloaded for a domain that is currently being scraped.\"\"\"\n+        \"\"\"Enqueue a request to be downloaded for a domain that is currently\n+        being scraped.\n+        \"\"\"\n         dfd = defer.Deferred()\n         self.pending_requests[domain].push((request, dfd), request.priority)\n         return dfd\n@@ -66,8 +47,8 @@ class Scheduler(object):\n         Returns a pair ``(request, deferred)`` where ``deferred`` is the\n         `Deferred` instance returned to the original requester.\n \n-        ``(None, None)`` should be returned if there aren't requests pending\n-        for the domain.\n+        ``(None, None)`` is returned if there aren't any request pending for\n+        the given domain.\n         \"\"\"\n         try:\n             return self.pending_requests[domain].pop()[0] # [1] is priority\n@@ -80,21 +61,6 @@ class Scheduler(object):\n         \"\"\"\n         self.pending_requests.pop(domain, None)\n \n-    def remove_pending_domain(self, domain):\n-        \"\"\"\n-        Remove a pending domain not yet started. If the domain was enqueued\n-        several times, all those instances are removed.\n-\n-        Returns the number of times the domain was enqueued. 0 if domains was\n-        not pending.\n-\n-        If domain is open (not pending) it is not removed and returns None. You\n-        need to call close_domain for open domains.\n-\n-        \"\"\"\n-        if domain in self.pending_domains and not self.domain_is_open(domain):\n-            return self.pending_domains.remove(domain)\n-\n     def is_idle(self):\n         \"\"\"Checks if the schedulers has any request pendings\"\"\"\n         return not self.pending_requests\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#728ec7c5c969288484eeb81dda0832d1240a2ad5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 49 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,18 +1,19 @@\n \"\"\"\n-Domain Schedulers keep track of next domains to scrape. They must implement the\n-following methods:\n+The Domain Scheduler keeps track of next domains to scrape. They must implement\n+the following methods:\n \n * next_domain()\n   return next domain to scrape and remove it from pending queue\n \n * add_domain(domain)\n-  add domain to pending domains to scrape\n+  add domain to pending queue\n \n * remove_pending_domain(domain)\n-  remove domain from pendings, do nothing if not pending\n+  remove (all occurrences) of domain from pending queue, do nothing if not\n+  pending\n \n * has_pending_domain(domain)\n-  Return ``True`` if the domain is pending, ``False`` otherwise\n+  Return ``True`` if the domain is pending to scrape, ``False`` otherwise\n \n \"\"\"\n \n@@ -30,7 +31,7 @@ class FifoDomainScheduler(object):\n         self.pending_domains.append(domain)\n \n     def remove_pending_domain(self, domain):\n-        self.pending_domains.remove(domain)\n+        self.pending_domains = [d for d in self.pending_domains if d != domain]\n \n     def has_pending_domain(self, domain):\n         return domain in self.pending_domains\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#00b49752ceae6b1a8529c8fff48341f762ae48a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 157 | Files Changed: 9 | Hunks: 48 | Methods Changed: 34 | Complexity Δ (Sum/Max): 19/13 | Churn Δ: 236 | Churn Cumulative: 667 | Contributors (this commit): 3 | Commits (past 90d): 35 | Contributors (cumulative): 23 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,14 +2,26 @@\n DuplicatesFilterMiddleware: Filter out already visited urls\n \"\"\"\n \n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.dupefilter import dupefilter\n-\n+from scrapy.core.exceptions import IgnoreRequest, NotConfigured\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n \n class DuplicatesFilterMiddleware(object):\n     \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n+    def __init__(self):\n+        clspath = settings.get('DUPEFILTER_FILTERCLASS')\n+        if not clspath:\n+            raise NotConfigured\n+\n+        self.dupefilter = load_object(clspath)()\n \n     def enqueue_request(self, domain, request):\n-        added = dupefilter.add(domain, request)\n+        added = self.dupefilter.add(domain, request)\n         if not (added or request.dont_filter):\n             raise IgnoreRequest('Skipped (already seen request)')\n+\n+    def open_domain(self, domain):\n+        self.dupefilter.open(domain)\n+\n+    def close_domain(self, domain):\n+        self.dupefilter.close(domain)\n\n@@ -1,23 +0,0 @@\n-\"\"\"\n-DuplicatesFilterMiddleware: Filter out already visited urls\n-\"\"\"\n-\n-from scrapy.http import Request\n-from scrapy.dupefilter import dupefilter\n-from scrapy import log\n-\n-\n-class DuplicatesFilterMiddleware(object):\n-    \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n-\n-    def process_spider_output(self, response, result, spider):\n-        domain = spider.domain_name\n-        for req in result:\n-            if isinstance(req, Request):\n-                has = dupefilter.has(domain, req)\n-                if has and not req.dont_filter:\n-                    log.msg('Skipped (already processed): %s' % req, log.TRACE, domain=domain)\n-                    continue\n-            yield req\n-\n-\n\n@@ -15,7 +15,7 @@ from scrapy import log\n from scrapy.stats import stats\n from scrapy.conf import settings\n from scrapy.core import signals\n-from scrapy.core.scheduler import Scheduler, SchedulerMiddlewareManager\n+from scrapy.core.scheduler import Scheduler\n from scrapy.core.downloader import Downloader\n from scrapy.core.exceptions import IgnoreRequest, DontCloseDomain\n from scrapy.http import Response, Request\n@@ -64,14 +64,11 @@ class ExecutionEngine(object):\n         Configure execution engine with the given scheduling policy and downloader.\n         \"\"\"\n         self.scheduler = scheduler or Scheduler()\n-        self.schedulermiddleware = SchedulerMiddlewareManager(self.scheduler)\n         self.domain_scheduler = load_object(settings['DOMAIN_SCHEDULER'])()\n         self.downloader = downloader or Downloader(self)\n         self.spidermiddleware = SpiderMiddlewareManager()\n         self._scraping = {}\n         self.pipeline = ItemPipelineManager()\n-        # key dictionary of per domain lists of initial requests to scrape\n-        self.starters = {}\n \n         self.configured = True\n \n@@ -291,14 +288,11 @@ class ExecutionEngine(object):\n     def schedule(self, request, spider):\n         domain = spider.domain_name\n         if not self.scheduler.domain_is_open(domain):\n-            if self.debug_mode: \n-                log.msg('Scheduling %s (delayed)' % request_info(request), log.DEBUG)\n-            return self._add_starter(request, spider)\n-        if self.debug_mode: \n-            log.msg('Scheduling %s (now)' % request_info(request), log.DEBUG)\n-        schd = self.schedulermiddleware.enqueue_request(domain, request)\n+            self.scheduler.open_domain(domain)\n+            if not self.downloader.domain_is_open(domain):\n+                self.domain_scheduler.add_domain(domain)\n         self.next_request(spider)\n-        return schd\n+        return self.scheduler.enqueue_request(domain, request)\n \n     def _mainloop(self):\n         \"\"\"Add more domains to be scraped if the downloader has the capacity.\n@@ -313,23 +307,6 @@ class ExecutionEngine(object):\n             if not self.next_domain():\n                 return self._stop_if_idle()\n \n-    def _add_starter(self, request, spider):\n-        domain = spider.domain_name\n-        if not self.domain_scheduler.has_pending_domain(domain):\n-            self.domain_scheduler.add_domain(domain)\n-            self.starters[domain] = []\n-        deferred = defer.Deferred()\n-        self.starters[domain] += [(request, deferred)]\n-        return deferred\n-\n-    def _run_starters(self, spider):\n-        domain = spider.domain_name\n-        starters = self.starters.pop(domain, [])\n-        while starters:\n-            request, deferred = starters.pop(0)\n-            schd = self.schedule(request, spider)\n-            chain_deferred(schd, deferred)\n-\n     def download(self, request, spider):\n         if self.debug_mode:\n             log.msg('Downloading %s' % request_info(request), log.DEBUG)\n@@ -372,16 +349,15 @@ class ExecutionEngine(object):\n     def open_domain(self, domain, spider=None):\n         log.msg(\"Domain opened\", domain=domain)\n         spider = spider or spiders.fromdomain(domain)\n+        self.next_request(spider)\n \n         self.cancelled.discard(domain)\n-        self.scheduler.open_domain(domain)\n         self.downloader.open_domain(domain)\n         self.pipeline.open_domain(domain)\n         self._scraping[domain] = set()\n-        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)\n \n+        signals.send_catch_log(signals.domain_open, sender=self.__class__, domain=domain, spider=spider)\n         signals.send_catch_log(signals.domain_opened, sender=self.__class__, domain=domain, spider=spider)\n-        self._run_starters(spider)\n \n     def _domain_idle(self, domain):\n         \"\"\"Called when a domain gets idle. This function is called when there are no\n@@ -433,7 +409,6 @@ class ExecutionEngine(object):\n         signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)\n         log.msg(\"Domain closed (%s)\" % status, domain=domain) \n         self.cancelled.discard(domain)\n-        self.starters.pop(domain, None)\n         self._mainloop()\n \n     def getstatus(self):\n\n@@ -5,6 +5,7 @@ For more information see the Scheduler Middleware doc in:\n docs/topics/scheduler-middleware.rst\n \n \"\"\"\n+from collections import defaultdict\n from twisted.internet.defer import Deferred\n \n from scrapy import log\n@@ -17,19 +18,13 @@ from scrapy.conf import settings\n \n class SchedulerMiddlewareManager(object):\n \n-    def __init__(self, scheduler):\n-        self.loaded = False\n-        self.scheduler = scheduler\n-        self.mw_enqueue_request = []\n+    def __init__(self):\n+        self.mw_cbs = defaultdict(list)\n         self.load()\n \n-    def _add_middleware(self, mw):\n-        if hasattr(mw, 'enqueue_request'):\n-            self.mw_enqueue_request.append(mw.enqueue_request)\n-\n     def load(self):\n         \"\"\"Load middleware defined in settings module\"\"\"\n-        mws = []\n+        self.mws = []\n         mwlist = build_middleware_list(settings['SCHEDULER_MIDDLEWARES_BASE'],\n                                        settings['SCHEDULER_MIDDLEWARES'])\n         for mwpath in mwlist:\n@@ -37,23 +32,38 @@ class SchedulerMiddlewareManager(object):\n             if cls:\n                 try:\n                     mw = cls()\n-                    self._add_middleware(mw)\n-                    mws.append(mw)\n                 except NotConfigured:\n                     pass\n-        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n-        self.loaded = True\n+                else:\n+                    self._add_middleware(mw)\n+                    self.mws.append(mw)\n \n-    def enqueue_request(self, domain, request):\n+        log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in self.mws]))\n+\n+    def _add_middleware(self, mw):\n+        for name in ('enqueue_request', 'open_domain', 'close_domain'):\n+            mwfunc = getattr(mw, name, None)\n+            if mwfunc:\n+                self.mw_cbs[name].append(mwfunc)\n+\n+    def enqueue_request(self, wrappedfunc, domain, request):\n         def _enqueue_request(request):\n-            for method in self.mw_enqueue_request:\n-                result = method(domain=domain, request=request)\n+            for mwfunc in self.mw_cbs['enqueue_request']:\n+                result = mwfunc(domain=domain, request=request)\n                 assert result is None or isinstance(result, (Response, Deferred)), \\\n                         'Middleware %s.enqueue_request must return None, Response or Deferred, got %s' % \\\n-                        (method.im_self.__class__.__name__, result.__class__.__name__)\n+                        (mwfunc.im_self.__class__.__name__, result.__class__.__name__)\n                 if result:\n                     return result\n-            return self.scheduler.enqueue_request(domain=domain, request=request)\n+            return wrappedfunc(domain=domain, request=request)\n \n         deferred = mustbe_deferred(_enqueue_request, request)\n         return deferred\n+\n+    def open_domain(self, domain):\n+        for mwfunc in self.mw_cbs['open_domain']:\n+            mwfunc(domain)\n+\n+    def close_domain(self, domain):\n+        for mwfunc in self.mw_cbs['close_domain']:\n+            mwfunc(domain)\n\n@@ -6,6 +6,7 @@ from twisted.internet import defer\n \n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n from scrapy.conf import settings\n+from .middleware import SchedulerMiddlewareManager\n \n class Scheduler(object):\n     \"\"\"The scheduler decides what to scrape next. In other words, it defines the\n@@ -18,6 +19,7 @@ class Scheduler(object):\n     def __init__(self):\n         self.pending_requests = {}\n         self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n+        self.middleware = SchedulerMiddlewareManager()\n \n     def domain_is_open(self, domain):\n         \"\"\"Check if scheduler's resources were allocated for a domain\"\"\"\n@@ -30,13 +32,27 @@ class Scheduler(object):\n \n     def open_domain(self, domain):\n         \"\"\"Allocates scheduling resources for the given domain\"\"\"\n+        if domain in self.pending_requests:\n+            raise RuntimeError('Scheduler domain already opened: %s' % domain)\n+\n         Priority = PriorityStack if self.dfo else PriorityQueue\n         self.pending_requests[domain] = Priority()\n+        self.middleware.open_domain(domain)\n+\n+    def close_domain(self, domain):\n+        \"\"\"Called when a spider has finished scraping to free any resources\n+        associated with the domain.\n+        \"\"\"\n+        if domain not in self.pending_requests:\n+            raise RuntimeError('Scheduler domain is not open: %s' % domain)\n+        self.middleware.close_domain(domain)\n+        self.pending_requests.pop(domain, None)\n \n     def enqueue_request(self, domain, request):\n-        \"\"\"Enqueue a request to be downloaded for a domain that is currently\n-        being scraped.\n-        \"\"\"\n+        \"\"\"Enqueue a request to be downloaded for a domain that is currently being scraped.\"\"\"\n+        return self.middleware.enqueue_request(self._enqueue_request, domain, request)\n+\n+    def _enqueue_request(self, domain, request):\n         dfd = defer.Deferred()\n         self.pending_requests[domain].push((request, dfd), request.priority)\n         return dfd\n@@ -55,12 +71,6 @@ class Scheduler(object):\n         except (KeyError, IndexError):\n             return (None, None)\n \n-    def close_domain(self, domain):\n-        \"\"\"Called when a spider has finished scraping to free any resources\n-        associated with the domain.\n-        \"\"\"\n-        self.pending_requests.pop(domain, None)\n-\n     def is_idle(self):\n         \"\"\"Checks if the schedulers has any request pendings\"\"\"\n         return not self.pending_requests\n\n@@ -1,19 +1,9 @@\n-from pydispatch import dispatcher\n-\n-from scrapy.core import signals\n from scrapy.utils.request import request_fingerprint\n-from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n \n \n class BaseFilter(dict):\n     \"\"\"Base class defining the duplicates requests filtering api\"\"\"\n \n-    def __init__(self, *args, **kwargs):\n-        super(BaseFilter, self).__init__(*args, **kwargs)\n-        dispatcher.connect(self.open, signals.domain_open)\n-        dispatcher.connect(self.close, signals.domain_closed)\n-\n     def open(self, domain):\n         \"\"\"Called when a domain starts\"\"\"\n         raise NotImplementedError()\n@@ -74,10 +64,3 @@ class NullFilter(dict):\n         return None\n \n \n-try:\n-    dupefilter\n-except NameError:\n-    clspath = settings.get('DUPEFILTER_FILTERCLASS')\n-    cls = load_object(clspath) if clspath else NullFilter\n-    dupefilter = cls()\n-\n\n@@ -1,7 +1,6 @@\n import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware\n-from scrapy.dupefilter import dupefilter\n from scrapy.spider import spiders\n from scrapy.http import Request, Response, Headers\n \n@@ -11,12 +10,8 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         spiders.spider_modules = ['scrapy.tests.test_spiders']\n         spiders.reload()\n         self.spider = spiders.fromdomain('scrapytest.org')\n-        dupefilter.open('scrapytest.org')\n         self.mw = RedirectMiddleware()\n \n-    def tearDown(self):\n-        dupefilter.close('scrapytest.org')\n-\n     def test_redirect_301(self):\n         url = 'http://www.example.com/301'\n         url2 = 'http://www.example.com/redirected'\n\n@@ -3,28 +3,27 @@ import unittest\n from scrapy.http import Request\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.contrib.schedulermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n-from scrapy.dupefilter import dupefilter\n \n \n+DOMAIN = 'scrapytest.org'\n+\n class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        dupefilter.open('scrapytest.org')\n+        self.mw = DuplicatesFilterMiddleware()\n+        self.mw.open_domain(DOMAIN)\n \n     def tearDown(self):\n-        dupefilter.close('scrapytest.org')\n+        self.mw.close_domain(DOMAIN)\n \n     def test_process_spider_output(self):\n-        domain = 'scrapytest.org'\n-\n-        mw = DuplicatesFilterMiddleware()\n \n         r1 = Request('http://scrapytest.org/1')\n         r2 = Request('http://scrapytest.org/2')\n         r3 = Request('http://scrapytest.org/2')\n         r4 = Request('http://scrapytest.org/1')\n \n-        assert not mw.enqueue_request(domain, r1)\n-        assert not mw.enqueue_request(domain, r2)\n-        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r3)\n-        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r4)\n+        assert not self.mw.enqueue_request(DOMAIN, r1)\n+        assert not self.mw.enqueue_request(DOMAIN, r2)\n+        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, DOMAIN, r3)\n+        self.assertRaises(IgnoreRequest, self.mw.enqueue_request, DOMAIN, r4)\n\n@@ -1,39 +0,0 @@\n-import unittest\n-\n-from scrapy.spider import spiders\n-from scrapy.http import Request, Response\n-from scrapy.core.exceptions import IgnoreRequest\n-from scrapy.contrib.spidermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n-from scrapy.dupefilter import dupefilter\n-\n-class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n-\n-    def setUp(self):\n-        spiders.spider_modules = ['scrapy.tests.test_spiders']\n-        spiders.reload()\n-        self.spider = spiders.fromdomain('scrapytest.org')\n-        dupefilter.open('scrapytest.org')\n-\n-    def tearDown(self):\n-        dupefilter.close('scrapytest.org')\n-\n-    def test_process_spider_output(self):\n-        mw = DuplicatesFilterMiddleware()\n-\n-        response = Response('http://scrapytest.org/')\n-        response.request = Request('http://scrapytest.org/')\n-\n-        r0 = Request('http://scrapytest.org/')\n-        r1 = Request('http://scrapytest.org/1')\n-        r2 = Request('http://scrapytest.org/2')\n-        r3 = Request('http://scrapytest.org/2')\n-\n-        dupefilter.add('scrapytest.org', r0)\n-        dupefilter.add('scrapytest.org', r2)\n-\n-        filtered = list(mw.process_spider_output(response, [r0, r1, r2, r3], self.spider))\n-\n-        assert r0 not in filtered\n-        assert r1 in filtered\n-        assert r2 not in filtered\n-        assert r3 not in filtered\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
