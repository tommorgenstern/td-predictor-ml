{"custom_id": "keras#de96a5dd05f8d06c15c317e070b9746e1d9a6ce9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1075 | Contributors (this commit): 8 | Commits (past 90d): 31 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -132,7 +132,7 @@ class RandomCrop(TFDataLayer):\n             return self.backend.cast(outputs, self.compute_dtype)\n \n         outputs = self.backend.cond(\n-            self.backend.all((training, h_diff >= 0, w_diff >= 0)),\n+            self.backend.ops.all((training, h_diff >= 0, w_diff >= 0)),\n             random_crop,\n             resize,\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1e3e0de9b3dcf4654c90d63e1bcaeaf32c5f9c59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 157 | Lines Deleted: 128 | Files Changed: 9 | Hunks: 43 | Methods Changed: 26 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 285 | Churn Cumulative: 10494 | Contributors (this commit): 16 | Commits (past 90d): 194 | Contributors (cumulative): 60 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1 @@\n+from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -0,0 +1,24 @@\n+import torch\n+\n+from keras_core.optimizers.base_optimizer import BaseOptimizer\n+\n+\n+class TorchOptimizer(BaseOptimizer):\n+    def __new__(cls, *args, **kwargs):\n+        # Import locally to avoid circular imports.\n+        from keras_core import optimizers\n+        from keras_core.backend.torch.optimizers import torch_sgd\n+\n+        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n+        if cls in OPTIMIZERS:\n+            return OPTIMIZERS[cls](*args, **kwargs)\n+        return super().__new__(cls)\n+\n+    def _apply_weight_decay(self, variables):\n+        if self.weight_decay is None:\n+            return\n+\n+        torch._foreach_mul_(\n+            [v.value for v in variables if self._use_weight_decay(v)],\n+            1 - self.weight_decay * self._get_current_learning_rate(),\n+        )\n\n@@ -0,0 +1,43 @@\n+import torch\n+\n+from keras_core import optimizers\n+\n+\n+class SGD(optimizers.SGD):\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        grads, trainable_variables = zip(*grads_and_vars)\n+\n+        self._parallel_update_step(\n+            grads,\n+            [v.value for v in trainable_variables],\n+            self._get_current_learning_rate(),\n+        )\n+        self.iterations.assign(self.iterations + 1)\n+\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        if self.momentum != 0:\n+            bufs = [\n+                self.momentums[self._get_variable_index(variable.value)]\n+                for variable in variables\n+            ]\n+\n+            for i in range(len(bufs)):\n+                if bufs[i] is None:\n+                    bufs[i] = torch.clone(grads[i]).detach()\n+\n+            torch._foreach_mul_(bufs, self.momentum)\n+            torch._foreach_add_(bufs, grads, alpha=-learning_rate)\n+\n+            if self.nesterov:\n+                torch._foreach_add_(variables, grads, alpha=-learning_rate)\n+                torch._foreach_add_(variables, bufs, alpha=self.momentum)\n+            else:\n+                torch._foreach_add_(variables, bufs)\n+\n+        else:\n+            torch._foreach_add_(variables, grads, alpha=-learning_rate)\n\n@@ -84,8 +84,8 @@ class Layer(BackendLayer, Operation):\n \n     Attributes:\n         name: The name of the layer (string).\n-        dtype: The dtype of the layer's weights.\n-        variable_dtype: Dtype of the layer's variables.\n+        dtype: Dtype of the layer's weights. Alias of `layer.variable_dtype`.\n+        variable_dtype: Dtype of the layer's weights.\n         compute_dtype: The dtype of the layer's computations.\n             Layers automatically cast inputs to this dtype, which causes\n             the computations and output to also be in this dtype.\n@@ -374,21 +374,19 @@ class Layer(BackendLayer, Operation):\n         constraint=None,\n         name=None,\n     ):\n-        # TODO: handle layout\n-        self._check_super_called()\n-        initializer = initializers.get(initializer)\n-        variable = backend.Variable(\n-            initializer=initializer,\n+        \"\"\"Add a weight variable to the layer.\n+\n+        Alias of `add_weight()`.\n+        \"\"\"\n+        return self.add_weight(\n             shape=shape,\n-            dtype=dtype or self.variable_dtype,\n+            initializer=initializer,\n+            dtype=dtype,\n             trainable=trainable,\n+            regularizer=regularizer,\n+            constraint=constraint,\n             name=name,\n         )\n-        # Will be added to layer.losses\n-        variable.regularizer = regularizer\n-        variable.constraint = constraint\n-        self._track_variable(variable)\n-        return variable\n \n     def add_weight(\n         self,\n@@ -402,8 +400,6 @@ class Layer(BackendLayer, Operation):\n     ):\n         \"\"\"Add a weight variable to the layer.\n \n-        Alias of `add_variable()`.\n-\n         Args:\n             shape: Shape tuple for the variable.\n                 Must be fully-defined (no `None` entries).\n@@ -422,15 +418,21 @@ class Layer(BackendLayer, Operation):\n             name: String name of the variable. Useful\n                 for debugging purposes.\n         \"\"\"\n-        return self.add_variable(\n-            shape=shape,\n+        # TODO: handle layout\n+        self._check_super_called()\n+        initializer = initializers.get(initializer)\n+        variable = backend.Variable(\n             initializer=initializer,\n-            dtype=dtype,\n+            shape=shape,\n+            dtype=dtype or self.variable_dtype,\n             trainable=trainable,\n-            regularizer=regularizer,\n-            constraint=constraint,\n             name=name,\n         )\n+        # Will be added to layer.losses\n+        variable.regularizer = regularizer\n+        variable.constraint = constraint\n+        self._track_variable(variable)\n+        return variable\n \n     @property\n     def trainable(self):\n@@ -459,9 +461,13 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def variables(self):\n-        # Return only weights/rng state/metric variables\n-        # of all Layers, recursively.\n-        # Also deduplicate them.\n+        \"\"\"List of all layer state, including metric variables and random seeds.\n+\n+        This extends `layer.weights` to include all state used by the layer\n+        including state for metrics and `SeedGenerator`s.\n+        \"\"\"\n+        # Return all `Variables` associate with the layer including metrics\n+        # and random seeds. Also deduplicate them.\n         variables = []\n         seen_ids = set()\n         for v in self._trainable_variables + self._non_trainable_variables:\n@@ -481,20 +487,32 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_variables(self):\n+        \"\"\"List of all trainable layer state.\n+\n+        This is equivalent to `layer.trainable_weights`.\n+        \"\"\"\n         if not self.trainable:\n             return []\n         return [v for v in self.variables if v.trainable]\n \n     @property\n     def non_trainable_variables(self):\n+        \"\"\"List of all non-trainable layer state.\n+\n+        This extends `layer.non_trainable_weights` to include all state used by\n+        the layer including state for metrics and `SeedGenerator`s.\n+        \"\"\"\n         if not self.trainable:\n             return self.variables\n         return [v for v in self.variables if not v.trainable]\n \n     @property\n     def weights(self):\n-        \"\"\"List of weight variables of the layer.\"\"\"\n-        # Return only \"own weights\" of all Layers, recursively.\n+        \"\"\"List of all weight variables of the layer.\n+\n+        Unlike, `layer.variables` this excludes metric state and random seeds.\n+        \"\"\"\n+        # Return only `Variables` directly owned by layers and sub-layers.\n         # Also deduplicate them.\n         weights = []\n         seen_ids = set()\n@@ -511,10 +529,9 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_weights(self):\n-        \"\"\"List of trainable weight variables of the layer.\n+        \"\"\"List of all trainable weight variables of the layer.\n \n-        These are the weights that get updated by the optimizer\n-        during training.\n+        These are the weights that get updated by the optimizer during training.\n         \"\"\"\n         if not self.trainable:\n             return []\n@@ -522,10 +539,11 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def non_trainable_weights(self):\n-        \"\"\"List of non-trainable weight variables of the layer.\n+        \"\"\"List of all non-trainable weight variables of the layer.\n \n-        Non-trainable weights may include batch normalization statistics,\n-        metric variables, or RNG seed variables.\n+        These are the weights that should not be updated by the optimizer during\n+        training. Unlike, `layer.non_trainable_variables` this excludes metric\n+        state and random seeds.\n         \"\"\"\n         if not self.trainable:\n             return self.weights\n@@ -555,7 +573,7 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def dtype(self):\n-        \"\"\"The dtype of the state (weights) of the layer.\"\"\"\n+        \"\"\"Alias of `layer.variable_dtype`.\"\"\"\n         return self.variable_dtype\n \n     @property\n\n@@ -1,17 +1,14 @@\n import numpy as np\n \n from keras_core import backend\n-from keras_core import random\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -55,9 +52,7 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -65,19 +60,13 @@ class RandomCrop(TFDataLayer):\n             )\n \n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n@@ -85,79 +74,19 @@ class RandomCrop(TFDataLayer):\n \n     def call(self, inputs, training=True):\n         if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = self.backend.convert_to_tensor(\n-                backend.convert_to_numpy(inputs)\n-            )\n-\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            dtype = input_shape.dtype\n-            rands = random.uniform(\n-                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n-            )\n-            h_start = rands[0] % (h_diff + 1)\n-            w_start = rands[1] % (w_diff + 1)\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.ops.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n         if (\n-            self.backend.backend() != \"tensorflow\"\n+            backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = self.backend.convert_to_tensor(outputs)\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.gru(\n+                out = backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,6 +536,11 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.lstm(\n+                out = backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,6 +516,11 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -3,9 +3,13 @@ from keras_core.api_export import keras_core_export\n from keras_core.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n+    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n \n-    BackendOptimizer = tf_optimizer.TFOptimizer\n+    BackendOptimizer = TFOptimizer\n+elif backend.backend() == \"torch\":\n+    from keras_core.backend.torch.optimizers import TorchOptimizer\n+\n+    BackendOptimizer = TorchOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -21,7 +21,7 @@ class SGDTest(testing.TestCase):\n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n         self.assertEqual(len(optimizer.variables), 2)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n@@ -32,7 +32,7 @@ class SGDTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -56,8 +56,8 @@ class SGDTest(testing.TestCase):\n         optimizer = SGD(nesterov=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dea6947044b01df0c0913761c3330e48624562b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 158 | Files Changed: 9 | Hunks: 43 | Methods Changed: 26 | Complexity Δ (Sum/Max): -11/5 | Churn Δ: 283 | Churn Cumulative: 10777 | Contributors (this commit): 16 | Commits (past 90d): 203 | Contributors (cumulative): 61 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1 +0,0 @@\n-from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -1,24 +0,0 @@\n-import torch\n-\n-from keras_core.optimizers.base_optimizer import BaseOptimizer\n-\n-\n-class TorchOptimizer(BaseOptimizer):\n-    def __new__(cls, *args, **kwargs):\n-        # Import locally to avoid circular imports.\n-        from keras_core import optimizers\n-        from keras_core.backend.torch.optimizers import torch_sgd\n-\n-        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n-        if cls in OPTIMIZERS:\n-            return OPTIMIZERS[cls](*args, **kwargs)\n-        return super().__new__(cls)\n-\n-    def _apply_weight_decay(self, variables):\n-        if self.weight_decay is None:\n-            return\n-\n-        torch._foreach_mul_(\n-            [v.value for v in variables if self._use_weight_decay(v)],\n-            1 - self.weight_decay * self._get_current_learning_rate(),\n-        )\n\n@@ -1,43 +0,0 @@\n-import torch\n-\n-from keras_core import optimizers\n-\n-\n-class SGD(optimizers.SGD):\n-    def _internal_apply_gradients(self, grads_and_vars):\n-        grads, trainable_variables = zip(*grads_and_vars)\n-\n-        self._parallel_update_step(\n-            grads,\n-            [v.value for v in trainable_variables],\n-            self._get_current_learning_rate(),\n-        )\n-        self.iterations.assign(self.iterations + 1)\n-\n-    def _parallel_update_step(\n-        self,\n-        grads,\n-        variables,\n-        learning_rate,\n-    ):\n-        if self.momentum != 0:\n-            bufs = [\n-                self.momentums[self._get_variable_index(variable.value)]\n-                for variable in variables\n-            ]\n-\n-            for i in range(len(bufs)):\n-                if bufs[i] is None:\n-                    bufs[i] = torch.clone(grads[i]).detach()\n-\n-            torch._foreach_mul_(bufs, self.momentum)\n-            torch._foreach_add_(bufs, grads, alpha=-learning_rate)\n-\n-            if self.nesterov:\n-                torch._foreach_add_(variables, grads, alpha=-learning_rate)\n-                torch._foreach_add_(variables, bufs, alpha=self.momentum)\n-            else:\n-                torch._foreach_add_(variables, bufs)\n-\n-        else:\n-            torch._foreach_add_(variables, grads, alpha=-learning_rate)\n\n@@ -84,8 +84,8 @@ class Layer(BackendLayer, Operation):\n \n     Attributes:\n         name: The name of the layer (string).\n-        dtype: Dtype of the layer's weights. Alias of `layer.variable_dtype`.\n-        variable_dtype: Dtype of the layer's weights.\n+        dtype: The dtype of the layer's weights.\n+        variable_dtype: Dtype of the layer's variables.\n         compute_dtype: The dtype of the layer's computations.\n             Layers automatically cast inputs to this dtype, which causes\n             the computations and output to also be in this dtype.\n@@ -374,19 +374,21 @@ class Layer(BackendLayer, Operation):\n         constraint=None,\n         name=None,\n     ):\n-        \"\"\"Add a weight variable to the layer.\n-\n-        Alias of `add_weight()`.\n-        \"\"\"\n-        return self.add_weight(\n-            shape=shape,\n+        # TODO: handle layout\n+        self._check_super_called()\n+        initializer = initializers.get(initializer)\n+        variable = backend.Variable(\n             initializer=initializer,\n-            dtype=dtype,\n+            shape=shape,\n+            dtype=dtype or self.variable_dtype,\n             trainable=trainable,\n-            regularizer=regularizer,\n-            constraint=constraint,\n             name=name,\n         )\n+        # Will be added to layer.losses\n+        variable.regularizer = regularizer\n+        variable.constraint = constraint\n+        self._track_variable(variable)\n+        return variable\n \n     def add_weight(\n         self,\n@@ -400,6 +402,8 @@ class Layer(BackendLayer, Operation):\n     ):\n         \"\"\"Add a weight variable to the layer.\n \n+        Alias of `add_variable()`.\n+\n         Args:\n             shape: Shape tuple for the variable.\n                 Must be fully-defined (no `None` entries).\n@@ -418,21 +422,15 @@ class Layer(BackendLayer, Operation):\n             name: String name of the variable. Useful\n                 for debugging purposes.\n         \"\"\"\n-        # TODO: handle layout\n-        self._check_super_called()\n-        initializer = initializers.get(initializer)\n-        variable = backend.Variable(\n-            initializer=initializer,\n+        return self.add_variable(\n             shape=shape,\n-            dtype=dtype or self.variable_dtype,\n+            initializer=initializer,\n+            dtype=dtype,\n             trainable=trainable,\n+            regularizer=regularizer,\n+            constraint=constraint,\n             name=name,\n         )\n-        # Will be added to layer.losses\n-        variable.regularizer = regularizer\n-        variable.constraint = constraint\n-        self._track_variable(variable)\n-        return variable\n \n     @property\n     def trainable(self):\n@@ -461,13 +459,9 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def variables(self):\n-        \"\"\"List of all layer state, including metric variables and random seeds.\n-\n-        This extends `layer.weights` to include all state used by the layer\n-        including state for metrics and `SeedGenerator`s.\n-        \"\"\"\n-        # Return all `Variables` associate with the layer including metrics\n-        # and random seeds. Also deduplicate them.\n+        # Return only weights/rng state/metric variables\n+        # of all Layers, recursively.\n+        # Also deduplicate them.\n         variables = []\n         seen_ids = set()\n         for v in self._trainable_variables + self._non_trainable_variables:\n@@ -487,32 +481,20 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_variables(self):\n-        \"\"\"List of all trainable layer state.\n-\n-        This is equivalent to `layer.trainable_weights`.\n-        \"\"\"\n         if not self.trainable:\n             return []\n         return [v for v in self.variables if v.trainable]\n \n     @property\n     def non_trainable_variables(self):\n-        \"\"\"List of all non-trainable layer state.\n-\n-        This extends `layer.non_trainable_weights` to include all state used by\n-        the layer including state for metrics and `SeedGenerator`s.\n-        \"\"\"\n         if not self.trainable:\n             return self.variables\n         return [v for v in self.variables if not v.trainable]\n \n     @property\n     def weights(self):\n-        \"\"\"List of all weight variables of the layer.\n-\n-        Unlike, `layer.variables` this excludes metric state and random seeds.\n-        \"\"\"\n-        # Return only `Variables` directly owned by layers and sub-layers.\n+        \"\"\"List of weight variables of the layer.\"\"\"\n+        # Return only \"own weights\" of all Layers, recursively.\n         # Also deduplicate them.\n         weights = []\n         seen_ids = set()\n@@ -529,9 +511,10 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_weights(self):\n-        \"\"\"List of all trainable weight variables of the layer.\n+        \"\"\"List of trainable weight variables of the layer.\n \n-        These are the weights that get updated by the optimizer during training.\n+        These are the weights that get updated by the optimizer\n+        during training.\n         \"\"\"\n         if not self.trainable:\n             return []\n@@ -539,11 +522,10 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def non_trainable_weights(self):\n-        \"\"\"List of all non-trainable weight variables of the layer.\n+        \"\"\"List of non-trainable weight variables of the layer.\n \n-        These are the weights that should not be updated by the optimizer during\n-        training. Unlike, `layer.non_trainable_variables` this excludes metric\n-        state and random seeds.\n+        Non-trainable weights may include batch normalization statistics,\n+        metric variables, or RNG seed variables.\n         \"\"\"\n         if not self.trainable:\n             return self.weights\n@@ -573,7 +555,7 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def dtype(self):\n-        \"\"\"Alias of `layer.variable_dtype`.\"\"\"\n+        \"\"\"The dtype of the state (weights) of the layer.\"\"\"\n         return self.variable_dtype\n \n     @property\n\n@@ -1,14 +1,17 @@\n import numpy as np\n \n from keras_core import backend\n+from keras_core import random\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n+from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n+from keras_core.random.seed_generator import SeedGenerator\n from keras_core.utils import backend_utils\n+from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(Layer):\n+class RandomCrop(TFDataLayer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,7 +55,9 @@ class RandomCrop(Layer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+    def __init__(\n+        self, height, width, seed=None, data_format=None, name=None, **kwargs\n+    ):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -60,33 +65,95 @@ class RandomCrop(Layer):\n             )\n \n         super().__init__(name=name, **kwargs)\n+        self.height = height\n+        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomCrop(\n-            height=height,\n-            width=width,\n-            seed=self.seed,\n-            name=name,\n-        )\n+        self.seed_generator = SeedGenerator(seed)\n+        self.data_format = backend.standardize_data_format(data_format)\n+\n+        if self.data_format == \"channels_first\":\n+            self.heigh_axis = -2\n+            self.width_axis = -1\n+        elif self.data_format == \"channels_last\":\n+            self.height_axis = -3\n+            self.width_axis = -2\n+\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n-        outputs = self.layer.call(inputs, training=training)\n+        inputs = self.backend.cast(inputs, self.compute_dtype)\n+        input_shape = self.backend.shape(inputs)\n+        is_batched = len(input_shape) > 3\n+        inputs = (\n+            self.backend.expand_dims(inputs, axis=0)\n+            if not is_batched\n+            else inputs\n+        )\n+\n+        h_diff = input_shape[self.height_axis] - self.height\n+        w_diff = input_shape[self.width_axis] - self.width\n+\n+        def random_crop():\n+            dtype = input_shape.dtype\n+            rands = random.uniform(\n+                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n+            )\n+            h_start = rands[0] % (h_diff + 1)\n+            w_start = rands[1] % (w_diff + 1)\n+            if self.data_format == \"channels_last\":\n+                return inputs[\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+            else:\n+                return inputs[\n+                    :,\n+                    :,\n+                    h_start : h_start + self.height,\n+                    w_start : w_start + self.width,\n+                ]\n+\n+        def resize():\n+            outputs = image_utils.smart_resize(\n+                inputs,\n+                [self.height, self.width],\n+                data_format=self.data_format,\n+                backend_module=self.backend,\n+            )\n+            # smart_resize will always output float32, so we need to re-cast.\n+            return self.backend.cast(outputs, self.compute_dtype)\n+\n+        outputs = self.backend.cond(\n+            self.backend.ops.all((training, h_diff >= 0, w_diff >= 0)),\n+            random_crop,\n+            resize,\n+        )\n+\n         if (\n-            backend.backend() != \"tensorflow\"\n+            self.backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = backend.convert_to_tensor(outputs)\n+            outputs = self.backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n+    def compute_output_shape(self, input_shape, *args, **kwargs):\n+        input_shape = list(input_shape)\n+        input_shape[self.height_axis] = self.height\n+        input_shape[self.width_axis] = self.width\n+        return tuple(input_shape)\n \n     def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n+        config = super().get_config()\n+        config.update(\n+            {\n+                \"height\": self.height,\n+                \"width\": self.width,\n+                \"seed\": self.seed,\n+                \"data_format\": self.data_format,\n+            }\n+        )\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                out = backend.gru(\n+                return backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,11 +536,6 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n-                # We disable jit_compile for the model in this case,\n-                # since cuDNN ops aren't XLA compatible.\n-                if backend.backend() == \"tensorflow\":\n-                    self.supports_jit = False\n-                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                out = backend.lstm(\n+                return backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,11 +516,6 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n-                # We disable jit_compile for the model in this case,\n-                # since cuDNN ops aren't XLA compatible.\n-                if backend.backend() == \"tensorflow\":\n-                    self.supports_jit = False\n-                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -3,13 +3,9 @@ from keras_core.api_export import keras_core_export\n from keras_core.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n+    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n \n-    BackendOptimizer = TFOptimizer\n-elif backend.backend() == \"torch\":\n-    from keras_core.backend.torch.optimizers import TorchOptimizer\n-\n-    BackendOptimizer = TorchOptimizer\n+    BackendOptimizer = tf_optimizer.TFOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -21,7 +21,7 @@ class SGDTest(testing.TestCase):\n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n         self.assertEqual(len(optimizer.variables), 2)\n-        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n@@ -32,7 +32,7 @@ class SGDTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            ops.zeros(()),\n+            np.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -56,8 +56,8 @@ class SGDTest(testing.TestCase):\n         optimizer = SGD(nesterov=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = ops.arange(0.1, 1.1, 0.1)\n-        first_grads = ops.full((10,), 0.01)\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#416b15ae22a25a3412fadd28383f8122a51e7054", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1285 | Contributors (this commit): 9 | Commits (past 90d): 34 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import numpy as np\n-\n from keras_core import backend\n from keras_core import random\n from keras_core.api_export import keras_core_export\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c479df3248848f9cf39e1909c1a04a0e1e39e72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 560 | Lines Deleted: 125 | Files Changed: 10 | Hunks: 45 | Methods Changed: 36 | Complexity Δ (Sum/Max): 28/17 | Churn Δ: 685 | Churn Cumulative: 11464 | Contributors (this commit): 17 | Commits (past 90d): 213 | Contributors (cumulative): 71 | DMM Complexity: 0.6015037593984962\n\nDIFF:\n@@ -0,0 +1,400 @@\n+\"\"\"\n+Title: Compact Convolutional Transformers\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/06/30\n+Last modified: 2023/07/17\n+Description: Compact Convolutional Transformers for efficient image classification.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+As discussed in the [Vision Transformers (ViT)](https://arxiv.org/abs/2010.11929) paper,\n+a Transformer-based architecture for vision typically requires a larger dataset than\n+usual, as well as a longer pre-training schedule. [ImageNet-1k](http://imagenet.org/)\n+(which has about a million images) is considered to fall under the medium-sized data regime with\n+respect to ViTs. This is primarily because, unlike CNNs, ViTs (or a typical\n+Transformer-based architecture) do not have well-informed inductive biases (such as\n+convolutions for processing images). This begs the question: can't we combine the\n+benefits of convolution and the benefits of Transformers\n+in a single network architecture? These benefits include parameter-efficiency, and\n+self-attention to process long-range and global dependencies (interactions between\n+different regions in an image).\n+\n+In [Escaping the Big Data Paradigm with Compact Transformers](https://arxiv.org/abs/2104.05704),\n+Hassani et al. present an approach for doing exactly this. They proposed the\n+**Compact Convolutional Transformer** (CCT) architecture. In this example, we will work on an\n+implementation of CCT and we will see how well it performs on the CIFAR-10 dataset.\n+\n+If you are unfamiliar with the concept of self-attention or Transformers, you can read\n+[this chapter](https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11/r-3/312)\n+from  François Chollet's book *Deep Learning with Python*. This example uses\n+code snippets from another example,\n+[Image classification with Vision Transformer](https://keras.io/examples/vision/image_classification_with_vision_transformer/).\n+\n+\n+\"\"\"\n+\n+\n+\"\"\"\n+## Imports\n+\"\"\"\n+\n+import os\n+os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n+\n+from keras_core import layers\n+import keras_core as keras\n+\n+import matplotlib.pyplot as plt\n+import tensorflow as tf\n+import numpy as np\n+\n+\"\"\"\n+## Hyperparameters and constants\n+\"\"\"\n+\n+positional_emb = True\n+conv_layers = 2\n+projection_dim = 128\n+\n+num_heads = 2\n+transformer_units = [\n+    projection_dim,\n+    projection_dim,\n+]\n+transformer_layers = 2\n+stochastic_depth_rate = 0.1\n+\n+learning_rate = 0.001\n+weight_decay = 0.0001\n+batch_size = 128\n+num_epochs = 30\n+image_size = 32\n+\n+\"\"\"\n+## Load CIFAR-10 dataset\n+\"\"\"\n+\n+num_classes = 10\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n+\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\"\"\"\n+## The CCT tokenizer\n+\n+The first recipe introduced by the CCT authors is the tokenizer for processing the\n+images. In a standard ViT, images are organized into uniform *non-overlapping* patches.\n+This eliminates the boundary-level information present in between different patches. This\n+is important for a neural network to effectively exploit the locality information. The\n+figure below presents an illustration of how images are organized into patches.\n+\n+![](https://i.imgur.com/IkBK9oY.png)\n+\n+We already know that convolutions are quite good at exploiting locality information. So,\n+based on this, the authors introduce an all-convolution mini-network to produce image\n+patches.\n+\"\"\"\n+\n+\n+class CCTTokenizer(layers.Layer):\n+    def __init__(\n+        self,\n+        kernel_size=3,\n+        stride=1,\n+        padding=1,\n+        pooling_kernel_size=3,\n+        pooling_stride=2,\n+        num_conv_layers=conv_layers,\n+        num_output_channels=[64, 128],\n+        positional_emb=positional_emb,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        # This is our tokenizer.\n+        self.conv_model = keras.Sequential()\n+        for i in range(num_conv_layers):\n+            self.conv_model.add(\n+                layers.Conv2D(\n+                    num_output_channels[i],\n+                    kernel_size,\n+                    stride,\n+                    padding=\"valid\",\n+                    use_bias=False,\n+                    activation=\"relu\",\n+                    kernel_initializer=\"he_normal\",\n+                )\n+            )\n+            self.conv_model.add(layers.ZeroPadding2D(padding))\n+            self.conv_model.add(\n+                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n+            )\n+\n+        self.positional_emb = positional_emb\n+\n+    def call(self, images):\n+        outputs = self.conv_model(images)\n+        # After passing the images through our mini-network the spatial dimensions\n+        # are flattened to form sequences.\n+        reshaped = tf.reshape(\n+            outputs,\n+            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n+        )\n+        return reshaped\n+\n+    def positional_embedding(self, image_size):\n+        # Positional embeddings are optional in CCT. Here, we calculate\n+        # the number of sequences and initialize an `Embedding` layer to\n+        # compute the positional embeddings later.\n+        if self.positional_emb:\n+            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n+            dummy_outputs = self.call(dummy_inputs)\n+            sequence_length = tf.shape(dummy_outputs)[1]\n+            projection_dim = tf.shape(dummy_outputs)[-1]\n+\n+            embed_layer = layers.Embedding(\n+                input_dim=sequence_length, output_dim=projection_dim\n+            )\n+            return embed_layer, sequence_length\n+        else:\n+            return None\n+\n+\"\"\"\n+## Sequence Pooling\n+Another recipe introduced in CCT is attention pooling or sequence pooling. In ViT, only\n+the feature map corresponding to the class token is pooled and is then used for the\n+subsequent classification task (or any other downstream task).\n+\"\"\"\n+\n+class SequencePooling(layers.Layer):\n+    def __init__(self):\n+        super().__init__()\n+        self.attention = layers.Dense(1)\n+\n+    def call(self, x):\n+        attention_weights = tf.nn.softmax(self.attention(x), axis=1)\n+        weighted_representation = tf.matmul(\n+        attention_weights, x, transpose_a=True\n+    )\n+        return tf.squeeze(weighted_representation, -2)\n+\n+\n+\"\"\"\n+## Stochastic depth for regularization\n+\n+[Stochastic depth](https://arxiv.org/abs/1603.09382) is a regularization technique that\n+randomly drops a set of layers. During inference, the layers are kept as they are. It is\n+very much similar to [Dropout](https://jmlr.org/papers/v15/srivastava14a.html) but only\n+that it operates on a block of layers rather than individual nodes present inside a\n+layer. In CCT, stochastic depth is used just before the residual blocks of a Transformers\n+encoder.\n+\"\"\"\n+\n+\n+# Referred from: github.com:rwightman/pytorch-image-models.\n+class StochasticDepth(layers.Layer):\n+    def __init__(self, drop_prop, **kwargs):\n+        super().__init__(**kwargs)\n+        self.drop_prob = drop_prop\n+\n+    def call(self, x, training=None):\n+        if training:\n+            keep_prob = 1 - self.drop_prob\n+            shape = (tf.shape(x)[0],) + (1,) * (tf.shape(x).shape[0] - 1)\n+            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n+            random_tensor = tf.floor(random_tensor)\n+            return (x / keep_prob) * random_tensor\n+        return x\n+\n+\n+\"\"\"\n+## MLP for the Transformers encoder\n+\"\"\"\n+\n+\n+def mlp(x, hidden_units, dropout_rate):\n+    for units in hidden_units:\n+        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n+        x = layers.Dropout(dropout_rate)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Data augmentation\n+\n+In the [original paper](https://arxiv.org/abs/2104.05704), the authors use\n+[AutoAugment](https://arxiv.org/abs/1805.09501) to induce stronger regularization. For\n+this example, we will be using the standard geometric augmentations like random cropping\n+and flipping.\n+\"\"\"\n+\n+# Note the rescaling layer. These layers have pre-defined inference behavior.\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Rescaling(scale=1.0 / 255),\n+        layers.RandomCrop(image_size, image_size),\n+        layers.RandomFlip(\"horizontal\"),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+\n+\"\"\"\n+## The final CCT model\n+\n+In CCT, outputs from the Transformers encoder are weighted and then passed on to the final task-specific layer (in\n+this example, we do classification).\n+\"\"\"\n+\n+\n+\n+\n+def create_cct_model(\n+    image_size=image_size,\n+    input_shape=input_shape,\n+    num_heads=num_heads,\n+    projection_dim=projection_dim,\n+    transformer_units=transformer_units,\n+):\n+    inputs = layers.Input(input_shape)\n+\n+    # Augment data.\n+    augmented = data_augmentation(inputs)\n+\n+    # Encode patches.\n+    cct_tokenizer = CCTTokenizer()\n+    encoded_patches = cct_tokenizer(augmented)\n+\n+    # Apply positional embedding.\n+    if positional_emb:\n+        pos_embed, seq_length = cct_tokenizer.positional_embedding(image_size)\n+        positions = tf.range(start=0, limit=seq_length, delta=1)\n+        position_embeddings = pos_embed(positions)\n+        encoded_patches += position_embeddings\n+\n+    # Calculate Stochastic Depth probabilities.\n+    dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n+\n+    # Create multiple layers of the Transformer block.\n+    for i in range(transformer_layers):\n+        # Layer normalization 1.\n+        x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n+\n+        # Create a multi-head attention layer.\n+        attention_output = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n+        )(x1, x1)\n+\n+        # Skip connection 1.\n+        attention_output = StochasticDepth(dpr[i])(attention_output)\n+        x2 = layers.Add()([attention_output, encoded_patches])\n+\n+        # Layer normalization 2.\n+        x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n+\n+        # MLP.\n+        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n+\n+        # Skip connection 2.\n+        x3 = StochasticDepth(dpr[i])(x3)\n+        encoded_patches = layers.Add()([x3, x2])\n+\n+    # Apply sequence pooling.\n+    representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n+    weighted_representation = SequencePooling()(representation)\n+\n+    # Classify outputs.\n+    logits = layers.Dense(num_classes)(weighted_representation)\n+    # Create the Keras model.\n+    model = keras.Model(inputs=inputs, outputs=logits)\n+    return model\n+\n+\n+\"\"\"\n+## Model training and evaluation\n+\"\"\"\n+\n+\n+def run_experiment(model):\n+    optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n+\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=keras.losses.CategoricalCrossentropy(\n+            from_logits=True, label_smoothing=0.1\n+        ),\n+        metrics=[\n+            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n+            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+        ],\n+    )\n+\n+    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n+    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+        checkpoint_filepath,\n+        monitor=\"val_accuracy\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    history = model.fit(\n+        x=x_train,\n+        y=y_train,\n+        batch_size=batch_size,\n+        epochs=num_epochs,\n+        validation_split=0.1,\n+        callbacks=[checkpoint_callback],\n+    )\n+\n+    model.load_weights(checkpoint_filepath)\n+    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+    return history\n+\n+\n+cct_model = create_cct_model()\n+history = run_experiment(cct_model)\n+\n+\"\"\"\n+Let's now visualize the training progress of the model.\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"], label=\"train_loss\")\n+plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n+plt.xlabel(\"Epochs\")\n+plt.ylabel(\"Loss\")\n+plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n+plt.legend()\n+plt.grid()\n+plt.show()\n+\n+\"\"\"\n+The CCT model we just trained has just **0.4 million** parameters, and it gets us to\n+~78% top-1 accuracy within 30 epochs. The plot above shows no signs of overfitting as\n+well. This means we can train this network for longer (perhaps with a bit more\n+regularization) and may obtain even better performance. This performance can further be\n+improved by additional recipes like cosine decay learning rate schedule, other data augmentation\n+techniques like [AutoAugment](https://arxiv.org/abs/1805.09501),\n+[MixUp](https://arxiv.org/abs/1710.09412) or\n+[Cutmix](https://arxiv.org/abs/1905.04899). With these modifications, the authors present\n+95.1% top-1 accuracy on the CIFAR-10 dataset. The authors also present a number of\n+experiments to study how the number of convolution blocks, Transformers layers, etc.\n+affect the final performance of CCTs.\n+\n+For a comparison, a ViT model takes about **4.7 million** parameters and **100\n+epochs** of training to reach a top-1 accuracy of 78.22% on the CIFAR-10 dataset. You can\n+refer to\n+[this notebook](https://colab.research.google.com/gist/sayakpaul/1a80d9f582b044354a1a26c5cb3d69e5/image_classification_with_vision_transformer.ipynb)\n+to know about the experimental setup.\n+\n+The authors also demonstrate the performance of Compact Convolutional Transformers on\n+NLP tasks and they report competitive results there.\n+\"\"\"\n\n@@ -0,0 +1 @@\n+from keras_core.backend.torch.optimizers.torch_optimizer import TorchOptimizer\n\n@@ -0,0 +1,24 @@\n+import torch\n+\n+from keras_core.optimizers.base_optimizer import BaseOptimizer\n+\n+\n+class TorchOptimizer(BaseOptimizer):\n+    def __new__(cls, *args, **kwargs):\n+        # Import locally to avoid circular imports.\n+        from keras_core import optimizers\n+        from keras_core.backend.torch.optimizers import torch_sgd\n+\n+        OPTIMIZERS = {optimizers.SGD: torch_sgd.SGD}\n+        if cls in OPTIMIZERS:\n+            return OPTIMIZERS[cls](*args, **kwargs)\n+        return super().__new__(cls)\n+\n+    def _apply_weight_decay(self, variables):\n+        if self.weight_decay is None:\n+            return\n+\n+        torch._foreach_mul_(\n+            [v.value for v in variables if self._use_weight_decay(v)],\n+            1 - self.weight_decay * self._get_current_learning_rate(),\n+        )\n\n@@ -0,0 +1,43 @@\n+import torch\n+\n+from keras_core import optimizers\n+\n+\n+class SGD(optimizers.SGD):\n+    def _internal_apply_gradients(self, grads_and_vars):\n+        grads, trainable_variables = zip(*grads_and_vars)\n+\n+        self._parallel_update_step(\n+            grads,\n+            [v.value for v in trainable_variables],\n+            self._get_current_learning_rate(),\n+        )\n+        self.iterations.assign(self.iterations + 1)\n+\n+    def _parallel_update_step(\n+        self,\n+        grads,\n+        variables,\n+        learning_rate,\n+    ):\n+        if self.momentum != 0:\n+            bufs = [\n+                self.momentums[self._get_variable_index(variable.value)]\n+                for variable in variables\n+            ]\n+\n+            for i in range(len(bufs)):\n+                if bufs[i] is None:\n+                    bufs[i] = torch.clone(grads[i]).detach()\n+\n+            torch._foreach_mul_(bufs, self.momentum)\n+            torch._foreach_add_(bufs, grads, alpha=-learning_rate)\n+\n+            if self.nesterov:\n+                torch._foreach_add_(variables, grads, alpha=-learning_rate)\n+                torch._foreach_add_(variables, bufs, alpha=self.momentum)\n+            else:\n+                torch._foreach_add_(variables, bufs)\n+\n+        else:\n+            torch._foreach_add_(variables, grads, alpha=-learning_rate)\n\n@@ -84,8 +84,8 @@ class Layer(BackendLayer, Operation):\n \n     Attributes:\n         name: The name of the layer (string).\n-        dtype: The dtype of the layer's weights.\n-        variable_dtype: Dtype of the layer's variables.\n+        dtype: Dtype of the layer's weights. Alias of `layer.variable_dtype`.\n+        variable_dtype: Dtype of the layer's weights.\n         compute_dtype: The dtype of the layer's computations.\n             Layers automatically cast inputs to this dtype, which causes\n             the computations and output to also be in this dtype.\n@@ -374,21 +374,19 @@ class Layer(BackendLayer, Operation):\n         constraint=None,\n         name=None,\n     ):\n-        # TODO: handle layout\n-        self._check_super_called()\n-        initializer = initializers.get(initializer)\n-        variable = backend.Variable(\n-            initializer=initializer,\n+        \"\"\"Add a weight variable to the layer.\n+\n+        Alias of `add_weight()`.\n+        \"\"\"\n+        return self.add_weight(\n             shape=shape,\n-            dtype=dtype or self.variable_dtype,\n+            initializer=initializer,\n+            dtype=dtype,\n             trainable=trainable,\n+            regularizer=regularizer,\n+            constraint=constraint,\n             name=name,\n         )\n-        # Will be added to layer.losses\n-        variable.regularizer = regularizer\n-        variable.constraint = constraint\n-        self._track_variable(variable)\n-        return variable\n \n     def add_weight(\n         self,\n@@ -402,8 +400,6 @@ class Layer(BackendLayer, Operation):\n     ):\n         \"\"\"Add a weight variable to the layer.\n \n-        Alias of `add_variable()`.\n-\n         Args:\n             shape: Shape tuple for the variable.\n                 Must be fully-defined (no `None` entries).\n@@ -422,15 +418,21 @@ class Layer(BackendLayer, Operation):\n             name: String name of the variable. Useful\n                 for debugging purposes.\n         \"\"\"\n-        return self.add_variable(\n-            shape=shape,\n+        # TODO: handle layout\n+        self._check_super_called()\n+        initializer = initializers.get(initializer)\n+        variable = backend.Variable(\n             initializer=initializer,\n-            dtype=dtype,\n+            shape=shape,\n+            dtype=dtype or self.variable_dtype,\n             trainable=trainable,\n-            regularizer=regularizer,\n-            constraint=constraint,\n             name=name,\n         )\n+        # Will be added to layer.losses\n+        variable.regularizer = regularizer\n+        variable.constraint = constraint\n+        self._track_variable(variable)\n+        return variable\n \n     @property\n     def trainable(self):\n@@ -459,9 +461,13 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def variables(self):\n-        # Return only weights/rng state/metric variables\n-        # of all Layers, recursively.\n-        # Also deduplicate them.\n+        \"\"\"List of all layer state, including metric variables and random seeds.\n+\n+        This extends `layer.weights` to include all state used by the layer\n+        including state for metrics and `SeedGenerator`s.\n+        \"\"\"\n+        # Return all `Variables` associate with the layer including metrics\n+        # and random seeds. Also deduplicate them.\n         variables = []\n         seen_ids = set()\n         for v in self._trainable_variables + self._non_trainable_variables:\n@@ -481,20 +487,32 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_variables(self):\n+        \"\"\"List of all trainable layer state.\n+\n+        This is equivalent to `layer.trainable_weights`.\n+        \"\"\"\n         if not self.trainable:\n             return []\n         return [v for v in self.variables if v.trainable]\n \n     @property\n     def non_trainable_variables(self):\n+        \"\"\"List of all non-trainable layer state.\n+\n+        This extends `layer.non_trainable_weights` to include all state used by\n+        the layer including state for metrics and `SeedGenerator`s.\n+        \"\"\"\n         if not self.trainable:\n             return self.variables\n         return [v for v in self.variables if not v.trainable]\n \n     @property\n     def weights(self):\n-        \"\"\"List of weight variables of the layer.\"\"\"\n-        # Return only \"own weights\" of all Layers, recursively.\n+        \"\"\"List of all weight variables of the layer.\n+\n+        Unlike, `layer.variables` this excludes metric state and random seeds.\n+        \"\"\"\n+        # Return only `Variables` directly owned by layers and sub-layers.\n         # Also deduplicate them.\n         weights = []\n         seen_ids = set()\n@@ -511,10 +529,9 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def trainable_weights(self):\n-        \"\"\"List of trainable weight variables of the layer.\n+        \"\"\"List of all trainable weight variables of the layer.\n \n-        These are the weights that get updated by the optimizer\n-        during training.\n+        These are the weights that get updated by the optimizer during training.\n         \"\"\"\n         if not self.trainable:\n             return []\n@@ -522,10 +539,11 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def non_trainable_weights(self):\n-        \"\"\"List of non-trainable weight variables of the layer.\n+        \"\"\"List of all non-trainable weight variables of the layer.\n \n-        Non-trainable weights may include batch normalization statistics,\n-        metric variables, or RNG seed variables.\n+        These are the weights that should not be updated by the optimizer during\n+        training. Unlike, `layer.non_trainable_variables` this excludes metric\n+        state and random seeds.\n         \"\"\"\n         if not self.trainable:\n             return self.weights\n@@ -555,7 +573,7 @@ class Layer(BackendLayer, Operation):\n \n     @property\n     def dtype(self):\n-        \"\"\"The dtype of the state (weights) of the layer.\"\"\"\n+        \"\"\"Alias of `layer.variable_dtype`.\"\"\"\n         return self.variable_dtype\n \n     @property\n\n@@ -1,15 +1,14 @@\n+import numpy as np\n+\n from keras_core import backend\n-from keras_core import random\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -53,9 +52,7 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n         if not tf.available:\n             raise ImportError(\n                 \"Layer RandomCrop requires TensorFlow. \"\n@@ -63,95 +60,33 @@ class RandomCrop(TFDataLayer):\n             )\n \n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.seed_generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            dtype = input_shape.dtype\n-            rands = random.uniform(\n-                [2], 0, dtype.max, dtype, seed=self.self.seed_generator\n-            )\n-            h_start = rands[0] % (h_diff + 1)\n-            w_start = rands[1] % (w_diff + 1)\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.ops.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n         if (\n-            self.backend.backend() != \"tensorflow\"\n+            backend.backend() != \"tensorflow\"\n             and not backend_utils.in_tf_graph()\n         ):\n-            outputs = self.backend.convert_to_tensor(outputs)\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -522,7 +522,7 @@ class GRU(RNN):\n                 # implementation of the inner GRU loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.gru(\n+                out = backend.gru(\n                     sequences,\n                     initial_state,\n                     mask,\n@@ -536,6 +536,11 @@ class GRU(RNN):\n                     unroll=self.unroll,\n                     reset_after=self.cell.reset_after,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -502,7 +502,7 @@ class LSTM(RNN):\n                 # implementation of the inner LSTM loop. In the case of\n                 # TF for instance, it will leverage cuDNN when feasible, and\n                 # it will raise NotImplementedError otherwise.\n-                return backend.lstm(\n+                out = backend.lstm(\n                     sequences,\n                     initial_state[0],\n                     initial_state[1],\n@@ -516,6 +516,11 @@ class LSTM(RNN):\n                     go_backwards=self.go_backwards,\n                     unroll=self.unroll,\n                 )\n+                # We disable jit_compile for the model in this case,\n+                # since cuDNN ops aren't XLA compatible.\n+                if backend.backend() == \"tensorflow\":\n+                    self.supports_jit = False\n+                return out\n             except NotImplementedError:\n                 pass\n         return super().inner_loop(\n\n@@ -3,9 +3,13 @@ from keras_core.api_export import keras_core_export\n from keras_core.optimizers import base_optimizer\n \n if backend.backend() == \"tensorflow\":\n-    from keras_core.backend.tensorflow import optimizer as tf_optimizer\n+    from keras_core.backend.tensorflow.optimizer import TFOptimizer\n \n-    BackendOptimizer = tf_optimizer.TFOptimizer\n+    BackendOptimizer = TFOptimizer\n+elif backend.backend() == \"torch\":\n+    from keras_core.backend.torch.optimizers import TorchOptimizer\n+\n+    BackendOptimizer = TorchOptimizer\n else:\n     BackendOptimizer = base_optimizer.BaseOptimizer\n \n\n@@ -21,7 +21,7 @@ class SGDTest(testing.TestCase):\n     def test_single_step(self):\n         optimizer = SGD(learning_rate=0.5)\n         self.assertEqual(len(optimizer.variables), 2)\n-        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        grads = ops.array([1.0, 6.0, 7.0, 2.0])\n         vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n         optimizer.build([vars])\n         optimizer.apply_gradients(zip([grads], [vars]))\n@@ -32,7 +32,7 @@ class SGDTest(testing.TestCase):\n \n     def test_weight_decay(self):\n         grads, var1, var2, var3 = (\n-            np.zeros(()),\n+            ops.zeros(()),\n             backend.Variable(2.0),\n             backend.Variable(2.0, name=\"exclude\"),\n             backend.Variable(2.0),\n@@ -56,8 +56,8 @@ class SGDTest(testing.TestCase):\n         optimizer = SGD(nesterov=True)\n \n         x = backend.Variable(np.ones([10]))\n-        grads = np.arange(0.1, 1.1, 0.1)\n-        first_grads = np.full((10,), 0.01)\n+        grads = ops.arange(0.1, 1.1, 0.1)\n+        first_grads = ops.full((10,), 0.01)\n \n         # fmt: off\n         golden = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
