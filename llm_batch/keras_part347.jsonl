{"custom_id": "keras#f630ad87a01ed2b4d08f91e5553b50c6a85601f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 10 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 34 | Churn Cumulative: 18534 | Contributors (this commit): 75 | Commits (past 90d): 7 | Contributors (cumulative): 75 | DMM Complexity: 0.0\n\nDIFF:\n@@ -16,6 +16,7 @@\n # pylint: disable=g-bad-import-order\n import tensorflow.compat.v2 as tf\n \n+import functools\n from keras import activations\n from keras import backend\n from keras import constraints\n@@ -23,6 +24,7 @@ from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n from keras.engine.input_spec import InputSpec\n+# imports for backwards namespace compatibility\n # pylint: disable=unused-import\n from keras.layers.pooling import AveragePooling1D\n from keras.layers.pooling import AveragePooling2D\n@@ -33,7 +35,6 @@ from keras.layers.pooling import MaxPooling3D\n # pylint: enable=unused-import\n from keras.utils import conv_utils\n from keras.utils import tf_utils\n-\n from tensorflow.python.util.tf_export import keras_export\n # pylint: disable=g-classes-have-attributes\n \n@@ -156,6 +157,7 @@ class Conv(Layer):\n     self.input_spec = InputSpec(min_ndim=self.rank + 2)\n \n     self._validate_init()\n+    self._is_causal = self.padding == 'causal'\n     self._channels_first = self.data_format == 'channels_first'\n     self._tf_data_format = conv_utils.convert_data_format(\n         self.data_format, self.rank + 2)\n@@ -212,35 +214,39 @@ class Conv(Layer):\n     else:\n       self.bias = None\n     channel_axis = self._get_channel_axis()\n-    self.input_spec = InputSpec(\n-        min_ndim=self.rank + 2, axes={channel_axis: input_channel})\n-    self.built = True\n-\n-  def call(self, inputs):\n-    input_shape = inputs.shape\n+    self.input_spec = InputSpec(min_ndim=self.rank + 2,\n+                                axes={channel_axis: input_channel})\n \n     # Convert Keras formats to TF native formats.\n     if self.padding == 'causal':\n-      # Apply causal padding to inputs for Conv1D.\n-      inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n       tf_padding = 'VALID'  # Causal padding handled in `call`.\n     elif isinstance(self.padding, str):\n       tf_padding = self.padding.upper()\n     else:\n       tf_padding = self.padding\n+    tf_dilations = list(self.dilation_rate)\n+    tf_strides = list(self.strides)\n \n     tf_op_name = self.__class__.__name__\n     if tf_op_name == 'Conv1D':\n       tf_op_name = 'conv1d'  # Backwards compat.\n \n-    outputs = tf.nn.convolution(\n-        inputs,\n-        self.kernel,\n+    self._convolution_op = functools.partial(\n+        tf.nn.convolution,\n+        strides=tf_strides,\n         padding=tf_padding,\n-        strides=list(self.strides),\n-        dilations=list(self.dilation_rate),\n+        dilations=tf_dilations,\n         data_format=self._tf_data_format,\n         name=tf_op_name)\n+    self.built = True\n+\n+  def call(self, inputs):\n+    input_shape = inputs.shape\n+\n+    if self._is_causal:  # Apply causal padding to inputs for Conv1D.\n+      inputs = tf.pad(inputs, self._compute_causal_padding(inputs))\n+\n+    outputs = self._convolution_op(inputs, self.kernel)\n \n     if self.use_bias:\n       output_rank = outputs.shape.rank\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba8f81ae45bfa8beb38d1629810262c3b391598c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 35 | Churn Cumulative: 2099 | Contributors (this commit): 30 | Commits (past 90d): 2 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -175,13 +175,26 @@ def print_summary(model, line_length=None, positions=None, print_fn=None):\n       relevant_nodes += v\n \n   def print_row(fields, positions):\n+    left_to_print = [str(x) for x in fields]\n+    while any(left_to_print):\n       line = ''\n-    for i in range(len(fields)):\n-      if i > 0:\n-        line = line[:-1] + ' '\n-      line += str(fields[i])\n-      line = line[:positions[i]]\n-      line += ' ' * (positions[i] - len(line))\n+      for col in range(len(left_to_print)):\n+        if col > 0:\n+          start_pos = positions[col-1]\n+        else:\n+          start_pos = 0\n+        end_pos = positions[col]\n+        # Leave room for a space to delineate columns\n+        # we don't need one if we are printing the last column\n+        space = 1 if col != len(positions) - 1 else 0\n+        delta = end_pos - start_pos - space\n+        fit_into_line = left_to_print[col][:delta]\n+        line += fit_into_line\n+        line += ' ' if space else ''\n+        left_to_print[col] = left_to_print[col][delta:]\n+\n+        # Pad out to the next position\n+        line += ' ' * (positions[col] - len(line))\n       print_fn(line)\n \n   print_fn('Model: \"{}\"'.format(model.name))\n@@ -234,19 +247,11 @@ def print_summary(model, line_length=None, positions=None, print_fn=None):\n \n     name = layer.name\n     cls_name = layer.__class__.__name__\n-    if not connections:\n-      first_connection = ''\n-    else:\n-      first_connection = connections[0]\n     fields = [\n         name + ' (' + cls_name + ')', output_shape,\n-        layer.count_params(), first_connection\n+        layer.count_params(), connections\n     ]\n     print_row(fields, positions)\n-    if len(connections) > 1:\n-      for i in range(1, len(connections)):\n-        fields = ['', '', '', connections[i]]\n-        print_row(fields, positions)\n \n   layers = model.layers\n   for i in range(len(layers)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9badb805177dfb041fda173e64b1cf2bc985abb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 2 | Churn Cumulative: 18978 | Contributors (this commit): 76 | Commits (past 90d): 9 | Contributors (cumulative): 76 | DMM Complexity: None\n\nDIFF:\n@@ -2295,6 +2295,8 @@ class DepthwiseConv2D(Conv2D):\n     depth_multiplier: The number of output channels for the depthwise kernels.\n       It can be understood as the amount of filters that are applied to\n       each input channel.\n+      The total number of depthwise convolution output\n+      channels will be equal to `filters_in * depth_multiplier`.\n     data_format: A string,\n       one of `channels_last` (default) or `channels_first`.\n       The ordering of the dimensions in the inputs.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#34081f74aefb90cb6c89432c792dc390b8f1a132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 18980 | Contributors (this commit): 76 | Commits (past 90d): 10 | Contributors (cumulative): 76 | DMM Complexity: None\n\nDIFF:\n@@ -2267,7 +2267,7 @@ class DepthwiseConv2D(Conv2D):\n \t\n   - Split the input into individual channels.\n   - Convolve each channel with an individual depthwise kernel having 1 input\n-    and `depth_multiplier` many output channels.\n+    and `depth_multiplier` output channels.\n   - Concatenate the convolved outputs (along the channels axis).\n \n   Unlike a regular 2D convolution, depthwise convolution does not mix\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30952af776cfb3afb8e59c71b18bde9df48f2364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 24 | Churn Cumulative: 2123 | Contributors (this commit): 30 | Commits (past 90d): 3 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -184,14 +184,24 @@ def print_summary(model, line_length=None, positions=None, print_fn=None):\n         else:\n           start_pos = 0\n         end_pos = positions[col]\n-        # Leave room for a space to delineate columns\n-        # we don't need one if we are printing the last column\n-        space = 1 if col != len(positions) - 1 else 0\n-        delta = end_pos - start_pos - space\n-        fit_into_line = left_to_print[col][:delta]\n+        # Leave room for 2 spaces to delineate columns\n+        # we don't need any if we are printing the last column\n+        space = 2 if col != len(positions) - 1 else 0\n+        cutoff = end_pos - start_pos - space\n+        fit_into_line = left_to_print[col][:cutoff]\n+        # For nicer formatting we line-break on seeing end of \n+        # tuple/dict etc.\n+        line_break_conditions = (\"),\", \"},\", \"],\", \"',\")\n+        candidate_cutoffs = [fit_into_line.find(x) + len(x) \n+                              for x in line_break_conditions \n+                              if fit_into_line.find(x) >= 0]\n+        if candidate_cutoffs:\n+          cutoff = min(candidate_cutoffs)\n+          fit_into_line = fit_into_line[:cutoff]\n+        \n         line += fit_into_line\n-        line += ' ' if space else ''\n-        left_to_print[col] = left_to_print[col][delta:]\n+        line += ' '*space if space else ''\n+        left_to_print[col] = left_to_print[col][cutoff:]\n \n         # Pad out to the next position\n         line += ' ' * (positions[col] - len(line))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1b22d1f69e0be546b459ba91daaf3cdaeacd800", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 380 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -77,7 +77,7 @@ class SidecarEvaluator(object):\n       name=\"eval_metrics\"))\n   data = tf.data.Dataset.from_tensor_slices(...)\n \n-  SidecarEvaluator(\n+  tf.keras.experimental.SidecarEvaluator(\n       model=model,\n       data=data,\n       checkpoint_dir='/tmp/checkpoint_dir',  # dir for training-saved checkpoint\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c281df898e7e842b94c451afc94fc1e5cf3014a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 36 | Churn Cumulative: 19279 | Contributors (this commit): 105 | Commits (past 90d): 21 | Contributors (cumulative): 109 | DMM Complexity: 1.0\n\nDIFF:\n@@ -782,6 +782,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       values of the `Model`'s metrics are returned. Example:\n       `{'loss': 0.2, 'accuracy': 0.7}`.\n \n+    Raises:\n+      TypeError: In a situation where a model is compiled with loss and no\n+      target is given.\n+\n     \"\"\"\n     # These are the only transformations `Model.fit` applies to user-input\n     # data when a `tf.data.Dataset` is provided.\n@@ -792,6 +796,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       y_pred = self(x, training=True)\n       loss = self.compiled_loss(\n           y, y_pred, sample_weight, regularization_losses=self.losses)\n+    if self.loss and y is None:\n+      raise TypeError(\n+          'Target value is missing. Please ensure `y` value is set '\n+          'in `Model.fit()` or `x` contains `data` and target value in `Model.fit()`.'\n+      )\n     # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n     self.compiled_metrics.update_state(y, y_pred, sample_weight)\n\n@@ -1607,7 +1607,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     outputs = layers_module.Dense(1)(inputs)\n     model = MyModel(inputs, outputs)\n     model.add_loss(tf.reduce_sum(outputs))\n-    model.compile('sgd', 'mse')\n+    model.compile('sgd')\n     model.fit(x, batch_size=batch_size)\n     model.evaluate(x, batch_size=batch_size)\n     model.predict(x, batch_size=batch_size)\n@@ -1704,6 +1704,31 @@ class TrainingTest(keras_parameterized.TestCase):\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n+  @keras_parameterized.run_all_keras_modes\n+  @keras_parameterized.run_with_all_model_types\n+  def test_fit_on_no_output(self):\n+    inputs = layers_module.Input((3,))\n+    outputs = layers_module.Dense(2)(inputs)\n+    model = training_module.Model(inputs, outputs)\n+    model.compile('rmsprop', 'mse')\n+    x = np.zeros((32, 3))\n+    with self.assertRaisesRegex(TypeError, 'Target value is missing..*'):\n+      model.fit(x)\n+\n+  @keras_parameterized.run_all_keras_modes\n+  @keras_parameterized.run_with_all_model_types\n+  def test_fit_on_wrong_output_type(self):\n+    inputs1 = layers_module.Input((3,), name='a')\n+    inputs2 = layers_module.Input((3,), name='b')\n+    x = layers_module.Concatenate()([inputs1, inputs2])\n+    outputs = layers_module.Dense(2, name='c')(x)\n+    model = training_module.Model([inputs1, inputs2], outputs)\n+    model.compile('rmsprop', 'mse')\n+    x = np.zeros((32, 3))\n+    y = np.zeros((32, 2))\n+    with self.assertRaisesRegex(TypeError, 'Target value is missing..*'):\n+      model.fit({'a': x, 'b': x, 'c': y})\n+\n   @keras_parameterized.run_all_keras_modes\n   def test_compile_warning_for_loss_missing_output(self):\n     with self.cached_session():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#522109b242e269c141db2cae9a6885b6d607009f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 97 | Files Changed: 2 | Hunks: 38 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 177 | Churn Cumulative: 19456 | Contributors (this commit): 105 | Commits (past 90d): 23 | Contributors (cumulative): 109 | DMM Complexity: 0.0\n\nDIFF:\n@@ -65,45 +65,6 @@ except ImportError:\n # pylint: enable=g-import-not-at-top\n \n \n-def disable_multi_worker(method):\n-  \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"\n-\n-  def _method_wrapper(self, *args, **kwargs):\n-    if self._in_multi_worker_mode():  # pylint: disable=protected-access\n-      raise ValueError('{} is not supported in multi-worker mode.'.format(\n-          method.__name__))\n-    return method(self, *args, **kwargs)\n-\n-  return tf.__internal__.decorator.make_decorator(\n-      target=method, decorator_func=_method_wrapper)\n-\n-\n-def inject_functional_model_class(cls):\n-  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n-  from keras.engine import functional  # pylint: disable=g-import-not-at-top\n-  from keras.engine import training_v1  # pylint: disable=g-import-not-at-top\n-  if cls == Model or cls == training_v1.Model:\n-    return functional.Functional\n-  # In case there is any multiple inheritance, we stop injecting the\n-  # class if keras model is not in its class hierarchy.\n-  if cls == object:\n-    return object\n-\n-  cls.__bases__ = tuple(inject_functional_model_class(base)\n-                        for base in cls.__bases__)\n-  # Trigger any `__new__` class swapping that needed to happen on `Functional`\n-  # but did not because functional was not in the class hierarchy.\n-  cls.__new__(cls)\n-\n-  return cls\n-\n-\n-def is_functional_model_init_params(args, kwargs):\n-  return (len(args) == 2 or\n-          len(args) == 1 and 'outputs' in kwargs or\n-          'inputs' in kwargs and 'outputs' in kwargs)\n-\n-\n @keras_export('keras.Model', 'keras.models.Model')\n class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   \"\"\"`Model` groups layers into an object with training and inference features.\n@@ -133,8 +94,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   inputs are not supported (e.g. lists of list or dicts of dict).\n \n   2 - By subclassing the `Model` class: in that case, you should define your\n-  layers in `__init__` and you should implement the model's forward pass\n-  in `call`.\n+  layers in `__init__()` and you should implement the model's forward pass\n+  in `call()`.\n \n   ```python\n   import tensorflow as tf\n@@ -154,7 +115,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   ```\n \n   If you subclass `Model`, you can optionally have\n-  a `training` argument (boolean) in `call`, which you can use to specify\n+  a `training` argument (boolean) in `call()`, which you can use to specify\n   a different behavior in training and inference:\n \n   ```python\n@@ -335,15 +296,16 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     never throw unexpected errors in an unrelated workflow).\n \n     Args:\n-     input_shape: Single tuple, TensorShape, or list/dict of shapes, where\n-         shapes are tuples, integers, or TensorShapes.\n+     input_shape: Single tuple, `TensorShape` instance,\n+       or list/dict of shapes, where shapes are tuples, integers, or\n+       `TensorShape` instances.\n \n     Raises:\n       ValueError:\n         1. In case of invalid user-provided data (not of type tuple,\n-           list, TensorShape, or dict).\n+           list, `TensorShape`, or dict).\n         2. If the model requires call arguments that are agnostic\n-           to the input shapes (positional or kwarg in call signature).\n+           to the input shapes (positional or keyword arg in call signature).\n         3. If not all layers were properly built.\n         4. If float type inputs are not supported within the layers.\n \n@@ -407,12 +369,13 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                   'Currently, you cannot build your model if it has '\n                   'positional or keyword arguments that are not '\n                   'inputs to the model, but are required for its '\n-                  '`call` method. Instead, in order to instantiate '\n-                  'and build your model, `call` your model on real '\n+                  '`call()` method. Instead, in order to instantiate '\n+                  'and build your model, `call()` your model on real '\n                   'tensor data with all expected call arguments.')\n         elif len(call_args) < 2:\n           # Signature without `inputs`.\n-          raise ValueError('You can only call `build` on a model if its `call` '\n+          raise ValueError(\n+              'You can only call `build()` on a model if its `call()` '\n               'method accepts an `inputs` argument.')\n         try:\n           self.call(x, **kwargs)\n@@ -420,7 +383,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           raise ValueError('You cannot build your model by calling `build` '\n                            'if your layers do not support float type inputs. '\n                            'Instead, in order to instantiate and build your '\n-                           'model, `call` your model on real tensor data (of '\n+                           'model, call your model on real tensor data (of '\n                            'the correct dtype).')\n     super(Model, self).build(input_shape)\n \n@@ -428,14 +391,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n   def call(self, inputs, training=None, mask=None):\n     \"\"\"Calls the model on new inputs.\n \n-    In this case `call` just reapplies\n+    In this case `call()` just reapplies\n     all ops in the graph to the new inputs\n     (e.g. build a new computational graph from the provided inputs).\n \n     Note: This method should not be called directly. It is only meant to be\n     overridden when subclassing `tf.keras.Model`.\n-    To call a model on an input, always use the `__call__` method,\n-    i.e. `model(inputs)`, which relies on the underlying `call` method.\n+    To call a model on an input, always use the `__call__()` method,\n+    i.e. `model(inputs)`, which relies on the underlying `call()` method.\n \n     Args:\n         inputs: Input tensor, or dict/list/tuple of input tensors.\n@@ -449,7 +412,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         a list of tensors if there are more than one outputs.\n     \"\"\"\n     raise NotImplementedError('When subclassing the `Model` class, you should '\n-                              'implement a `call` method.')\n+                              'implement a `call()` method.')\n \n   def compile(self,\n               optimizer='rmsprop',\n@@ -538,10 +501,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           will only be called every `N` batches\n           (i.e. before/after each `tf.function` execution).\n         **kwargs: Arguments supported for backwards compatibility only.\n-\n-    Raises:\n-        ValueError: In case of invalid arguments for\n-            `optimizer`, `loss` or `metrics`.\n     \"\"\"\n     base_layer.keras_api_gauge.get_cell('compile').set(True)\n     with self.distribute_strategy.scope():\n@@ -572,8 +531,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       # Initializes attrs that are reset each time `compile` is called.\n       self._reset_compile_cache()\n       self._is_compiled = True\n-\n-      self.loss = loss or {}  # Backwards compat.\n+      self.loss = loss or {}\n \n   def _get_optimizer(self, optimizer):\n     \"\"\"Wraps `optimizer` in `LossScaleOptimizer` if necessary.\"\"\"\n@@ -627,7 +585,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n   @property\n   def metrics(self):\n-    \"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.\n+    \"\"\"Returns the model's metrics added using `compile()`, `add_metric()` APIs.\n \n     Note: Metrics passed to `compile()` are available only after a `keras.Model`\n     has been trained/evaluated on actual data.\n@@ -781,11 +739,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       `tf.keras.callbacks.CallbackList.on_train_batch_end`. Typically, the\n       values of the `Model`'s metrics are returned. Example:\n       `{'loss': 0.2, 'accuracy': 0.7}`.\n-\n-    Raises:\n-      TypeError: In a situation where a model is compiled with loss and no\n-      target is given.\n-\n     \"\"\"\n     # These are the only transformations `Model.fit` applies to user-input\n     # data when a `tf.data.Dataset` is provided.\n@@ -798,8 +751,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           y, y_pred, sample_weight, regularization_losses=self.losses)\n     if self.loss and y is None:\n       raise TypeError(\n-          'Target value is missing. Please ensure `y` value is set '\n-          'in `Model.fit()` or `x` contains `data` and target value in `Model.fit()`.'\n+          'Target data is missing. Your model was compiled with a `loss` '\n+          'argument and therefore expects target data to be passed in `fit()`.'\n       )\n     # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n@@ -1454,8 +1407,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         the display labels for the scalar outputs.\n \n     Raises:\n-        RuntimeError: If `model.evaluate` is wrapped in `tf.function`.\n-        ValueError: in case of invalid arguments.\n+        RuntimeError: If `model.evaluate` is wrapped in a `tf.function`.\n     \"\"\"\n     base_layer.keras_api_gauge.get_cell('evaluate').set(True)\n     version_utils.disallow_legacy_graph('Model', 'evaluate')\n@@ -1634,7 +1586,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     Computation is done in batches. This method is designed for performance in\n     large scale inputs. For small amount of inputs that fit in one batch,\n-    directly using `__call__` is recommended for faster execution, e.g.,\n+    directly using `__call__()` is recommended for faster execution, e.g.,\n     `model(x)`, or `model(x, training=False)` if you have layers such as\n     `tf.keras.layers.BatchNormalization` that behaves differently during\n     inference. Also, note the fact that test loss is not affected by\n@@ -1661,7 +1613,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         steps: Total number of steps (batches of samples)\n             before declaring the prediction round finished.\n             Ignored with the default value of `None`. If x is a `tf.data`\n-            dataset and `steps` is None, `predict` will\n+            dataset and `steps` is None, `predict()` will\n             run until the input dataset is exhausted.\n         callbacks: List of `keras.callbacks.Callback` instances.\n             List of callbacks to apply during prediction.\n@@ -1689,7 +1641,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Numpy array(s) of predictions.\n \n     Raises:\n-        RuntimeError: If `model.predict` is wrapped in `tf.function`.\n+        RuntimeError: If `model.predict` is wrapped in a `tf.function`.\n         ValueError: In case of mismatch between the provided\n             input data and the model's expectations,\n             or in case a stateful model receives a number of samples\n@@ -1855,8 +1807,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         the display labels for the scalar outputs.\n \n     Raises:\n-      RuntimeError: If `model.train_on_batch` is wrapped in `tf.function`.\n-      ValueError: In case of invalid user-provided arguments.\n+      RuntimeError: If `model.train_on_batch` is wrapped in a `tf.function`.\n     \"\"\"\n     self._assert_compile_was_called()\n     self._check_call_args('train_on_batch')\n@@ -1915,8 +1866,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         the display labels for the scalar outputs.\n \n     Raises:\n-        RuntimeError: If `model.test_on_batch` is wrapped in `tf.function`.\n-        ValueError: In case of invalid user-provided arguments.\n+        RuntimeError: If `model.test_on_batch` is wrapped in a `tf.function`.\n     \"\"\"\n     self._assert_compile_was_called()\n     self._check_call_args('test_on_batch')\n@@ -1949,9 +1899,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         Numpy array(s) of predictions.\n \n     Raises:\n-        RuntimeError: If `model.predict_on_batch` is wrapped in `tf.function`.\n-        ValueError: In case of mismatch between given number of inputs and\n-          expectations of the model.\n+        RuntimeError: If `model.predict_on_batch` is wrapped in a `tf.function`.\n     \"\"\"\n     self._check_call_args('predict_on_batch')\n     _disallow_inside_tf_function('predict_on_batch')\n@@ -2202,9 +2150,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     is the `Checkpoint` even if the `Checkpoint` has a model attached. This\n     means saving a `tf.keras.Model` using `save_weights` and loading into a\n     `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match\n-    the `Model`'s variables. See the [guide to training\n-    checkpoints](https://www.tensorflow.org/guide/checkpoint) for details\n-    on the TensorFlow format.\n+    the `Model`'s variables. See the\n+    [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint)\n+    for details on the TensorFlow format.\n \n     Args:\n         filepath: String or PathLike, path to the file to save the weights to.\n@@ -2220,9 +2168,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             options for saving weights.\n \n     Raises:\n-        ImportError: If h5py is not available when attempting to save in HDF5\n+        ImportError: If `h5py` is not available when attempting to save in HDF5\n             format.\n-        ValueError: For invalid/unknown format arguments.\n     \"\"\"\n     self._assert_weights_created()\n     filepath = path_to_string(filepath)\n@@ -2325,7 +2272,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         When loading weights in HDF5 format, returns `None`.\n \n     Raises:\n-        ImportError: If h5py is not available and the weight file is in HDF5\n+        ImportError: If `h5py` is not available and the weight file is in HDF5\n             format.\n         ValueError: If `skip_mismatch` is set to `True` when `by_name` is\n           `False`.\n@@ -2451,7 +2398,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         A YAML string.\n \n     Raises:\n-        ImportError: if yaml module is not found.\n+        ImportError: if the `yaml` module is not found.\n     \"\"\"\n     if yaml is None:\n       raise ImportError(\n@@ -2554,9 +2501,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     Returns:\n         A layer instance.\n-\n-    Raises:\n-        ValueError: In case of invalid layer name or index.\n     \"\"\"\n     # TODO(fchollet): We could build a dictionary based on layer names\n     # since they are constant, but we have not done that yet.\n@@ -2667,7 +2611,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     the user would just get an empty list, which is misleading.\n \n     Raises:\n-      ValueError: if the weights of the network has not yet been created.\n+      ValueError: if the weights of the network have not yet been created.\n     \"\"\"\n     if self.dynamic:\n       return\n@@ -2684,7 +2628,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                        self.name)\n \n   def _check_call_args(self, method_name):\n-    \"\"\"Check that `call` has only one positional arg.\"\"\"\n+    \"\"\"Check that `call()` has only one positional arg.\"\"\"\n     # Always allow first arg, regardless of arg name.\n     fullargspec = self._call_full_argspec\n     if fullargspec.defaults:\n@@ -2699,11 +2643,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       extra_args = positional_args[2:]\n       raise ValueError(\n           'Models passed to `' + method_name + '` can only have `training` '\n-          'and the first argument in `call` as positional arguments, '\n+          'and the first argument in `call()` as positional arguments, '\n           'found: ' + str(extra_args) + '.')\n \n   def _validate_compile(self, optimizer, metrics, **kwargs):\n-    \"\"\"Performs validation checks for the default `compile`.\"\"\"\n+    \"\"\"Performs validation checks for the default `compile()`.\"\"\"\n     if any(\n         isinstance(opt, optimizer_v1.Optimizer)\n         for opt in tf.nest.flatten(optimizer)):\n@@ -2724,7 +2668,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           'target_tensors argument is not supported when executing eagerly.')\n     invalid_kwargs = set(kwargs) - {'sample_weight_mode'}\n     if invalid_kwargs:\n-      raise TypeError('Invalid keyword argument(s) in `compile`: %s' %\n+      raise TypeError('Invalid keyword argument(s) in `compile()`: %s' %\n                       (invalid_kwargs,))\n \n     # Model must be created and compiled with the same DistStrat.\n@@ -3076,3 +3020,42 @@ def saver_with_op_caching(obj):\n   return tf.__internal__.tracking.TrackableSaver(\n       tf.__internal__.tracking.ObjectGraphView(\n           weakref.ref(obj), saveables_cache=saveables_cache))\n+\n+\n+def disable_multi_worker(method):\n+  \"\"\"Decorator that disallows multi-worker use of `method`.\"\"\"\n+\n+  def _method_wrapper(self, *args, **kwargs):\n+    if self._in_multi_worker_mode():  # pylint: disable=protected-access\n+      raise ValueError('{} is not supported in multi-worker mode.'.format(\n+          method.__name__))\n+    return method(self, *args, **kwargs)\n+\n+  return tf.__internal__.decorator.make_decorator(\n+      target=method, decorator_func=_method_wrapper)\n+\n+\n+def inject_functional_model_class(cls):\n+  \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n+  from keras.engine import functional  # pylint: disable=g-import-not-at-top\n+  from keras.engine import training_v1  # pylint: disable=g-import-not-at-top\n+  if cls == Model or cls == training_v1.Model:\n+    return functional.Functional\n+  # In case there is any multiple inheritance, we stop injecting the\n+  # class if keras model is not in its class hierarchy.\n+  if cls == object:\n+    return object\n+\n+  cls.__bases__ = tuple(inject_functional_model_class(base)\n+                        for base in cls.__bases__)\n+  # Trigger any `__new__` class swapping that needed to happen on `Functional`\n+  # but did not because functional was not in the class hierarchy.\n+  cls.__new__(cls)\n+\n+  return cls\n+\n+\n+def is_functional_model_init_params(args, kwargs):\n+  return (len(args) == 2 or\n+          len(args) == 1 and 'outputs' in kwargs or\n+          'inputs' in kwargs and 'outputs' in kwargs)\n\n@@ -1712,7 +1712,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     model = training_module.Model(inputs, outputs)\n     model.compile('rmsprop', 'mse')\n     x = np.zeros((32, 3))\n-    with self.assertRaisesRegex(TypeError, 'Target value is missing..*'):\n+    with self.assertRaisesRegex(TypeError, 'Target data is missing..*'):\n       model.fit(x)\n \n   @keras_parameterized.run_all_keras_modes\n@@ -1726,7 +1726,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     model.compile('rmsprop', 'mse')\n     x = np.zeros((32, 3))\n     y = np.zeros((32, 2))\n-    with self.assertRaisesRegex(TypeError, 'Target value is missing..*'):\n+    with self.assertRaisesRegex(TypeError, 'Target data is missing..*'):\n       model.fit({'a': x, 'b': x, 'c': y})\n \n   @keras_parameterized.run_all_keras_modes\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fe04bf89f94c7175c4ab4bc389e8220ff70c49d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 313 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -204,7 +204,7 @@ def image_dataset_from_directory(directory,\n   image_paths, labels = dataset_utils.get_training_or_validation_split(\n       image_paths, labels, validation_split, subset)\n   if not image_paths:\n-    raise ValueError('No images found.')\n+    raise ValueError(f'No images found. Allowed formats: {ALLOWLIST_FORMATS}')\n \n   dataset = paths_and_labels_to_dataset(\n       image_paths=image_paths,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f43c792c129ffba27943c00f11c73c7d8db2838f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 746 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -22,17 +22,17 @@ from tensorflow.python.util.tf_export import keras_export\n def to_categorical(y, num_classes=None, dtype='float32'):\n   \"\"\"Converts a class vector (integers) to binary class matrix.\n \n-  E.g. for use with categorical_crossentropy.\n+  E.g. for use with `categorical_crossentropy`.\n \n   Args:\n-      y: class vector to be converted into a matrix\n-          (integers from 0 to num_classes).\n-      num_classes: total number of classes. If `None`, this would be inferred\n-        as the (largest number in `y`) + 1.\n+      y: Array-like with class values to be converted into a matrix\n+          (integers from 0 to `num_classes - 1`).\n+      num_classes: Total number of classes. If `None`, this would be inferred\n+        as `max(y) + 1`.\n       dtype: The data type expected by the input. Default: `'float32'`.\n \n   Returns:\n-      A binary matrix representation of the input. The classes axis is placed\n+      A binary matrix representation of the input. The class axis is placed\n       last.\n \n   Example:\n@@ -58,10 +58,6 @@ def to_categorical(y, num_classes=None, dtype='float32'):\n   >>> loss = tf.keras.backend.categorical_crossentropy(a, a)\n   >>> print(np.around(loss, 5))\n   [0. 0. 0. 0.]\n-\n-  Raises:\n-      Value Error: If input contains string value\n-\n   \"\"\"\n   y = np.array(y, dtype='int')\n   input_shape = y.shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4e4f2bf5499352bca19b97851c7ba167028d990", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 62 | Churn Cumulative: 19518 | Contributors (this commit): 105 | Commits (past 90d): 25 | Contributors (cumulative): 109 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,7 +51,6 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n-\n # pylint: disable=g-import-not-at-top\n try:\n   import h5py\n@@ -205,9 +204,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'\n     })\n     super(Model, self).__init__(**kwargs)\n+    self._check_step_methods_type()\n     # By default, Model is a subclass model, which is not in graph network.\n     self._is_graph_network = False\n-\n     self.inputs = None\n     self.outputs = None\n     self.input_names = None\n@@ -708,8 +707,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.\n     # (3) Not explicitly setting run_eagerly defaults to TF's global setting.\n     return (self.dynamic or self._run_eagerly or\n-            (tf.config.functions_run_eagerly() and\n-             self._run_eagerly is None))\n+            (tf.config.functions_run_eagerly() and self._run_eagerly is None))\n \n   @run_eagerly.setter\n   def run_eagerly(self, value):\n@@ -2627,6 +2625,25 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                        'inputs or `build()` is called with an `input_shape`.' %\n                        self.name)\n \n+  def _check_step_methods_type(self):\n+    \"\"\"Checks that step methods are not decorated with `@tf.function`.\n+\n+    Raises:\n+       TypeError: if any of `self.train_step`, `self.test_step`, or\n+       `self.predict_step` is decorated in `tf.function`.\n+\n+    \"\"\"\n+    steps = {\n+        self.train_step: 'Model.train_step',\n+        self.test_step: 'Model.test_step',\n+        self.predict_step: 'Model.predict_step'\n+    }\n+    for step in steps:\n+      if isinstance(step, tf.__internal__.function.Function):\n+        raise TypeError(\n+            'Make sure method {} when overridden is not decorated with '\n+            '`@tf.function`.'.format(steps[step]))\n+\n   def _check_call_args(self, method_name):\n     \"\"\"Check that `call()` has only one positional arg.\"\"\"\n     # Always allow first arg, regardless of arg name.\n\n@@ -49,6 +49,43 @@ except ImportError:\n \n class TrainingTest(keras_parameterized.TestCase):\n \n+  @keras_parameterized.run_all_keras_modes\n+  @keras_parameterized.run_with_all_model_types\n+  def test_init_type_error(self):\n+\n+    class ModelWithTrainStepOverridden(training_module.Model):\n+\n+      @tf.function\n+      def train_step(self, data):\n+        return 0\n+\n+    class ModelWithTestStepOverridden(training_module.Model):\n+\n+      @tf.function\n+      def test_step(self, data):\n+        return 0\n+\n+    class ModelWithPredictStepOverridden(training_module.Model):\n+\n+      @tf.function\n+      def predict_step(self, data):\n+        return 0\n+\n+    inputs = layers_module.Input(shape=(1,), name='my_input')\n+    outputs = layers_module.Dense(1)(inputs)\n+    with self.assertRaisesRegex(\n+        TypeError, 'Make sure method Model.train_step when overridden is not'\n+        ' decorated with `@tf.function`.'):\n+      ModelWithTrainStepOverridden(inputs, outputs)\n+    with self.assertRaisesRegex(\n+        TypeError, 'Make sure method Model.test_step when overridden is not'\n+        ' decorated with `@tf.function`.'):\n+      ModelWithTestStepOverridden(inputs, outputs)\n+    with self.assertRaisesRegex(\n+        TypeError, 'Make sure method Model.predict_step when overridden is not'\n+        ' decorated with `@tf.function`.'):\n+      ModelWithPredictStepOverridden(inputs, outputs)\n+\n   @keras_parameterized.run_all_keras_modes\n   @keras_parameterized.run_with_all_model_types\n   def test_model_instrumentation(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#376e9eaa50a441a800b199cb9893d104a03809b1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 11 | Files Changed: 6 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 22 | Churn Cumulative: 27682 | Contributors (this commit): 98 | Commits (past 90d): 43 | Contributors (cumulative): 117 | DMM Complexity: None\n\nDIFF:\n@@ -68,14 +68,14 @@ _TF_OP_LAYER_NAME_PREFIX = 'tf_op_layer_'\n _AUTOCAST_TYPES = (tf.Tensor, tf.SparseTensor,\n                    tf.RaggedTensor)\n \n-keras_layers_gauge = tf.__internal__.monitoring.BoolGauge('/tensorflow/api/oss-keras/layers',\n-                                          'keras layers usage', 'method')\n+keras_layers_gauge = tf.__internal__.monitoring.BoolGauge(\n+    '/tensorflow/api/keras/layers', 'keras layers usage', 'method')\n keras_models_gauge = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/oss-keras/models', 'keras model usage', 'method')\n-keras_api_gauge = tf.__internal__.monitoring.BoolGauge('/tensorflow/api/oss-keras',\n-                                       'keras api usage', 'method')\n+    '/tensorflow/api/keras/models', 'keras model usage', 'method')\n+keras_api_gauge = tf.__internal__.monitoring.BoolGauge(\n+    '/tensorflow/api/keras', 'keras api usage', 'method')\n keras_premade_model_gauge = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/oss-keras/premade_models', 'premade keras model usage', 'type')\n+    '/tensorflow/api/keras/premade_models', 'premade keras model usage', 'type')\n \n _is_name_scope_on_model_declaration_enabled = False\n \n\n@@ -27,7 +27,7 @@ from tensorflow.tools.docs import doc_controls\n \n \n keras_kpl_gauge = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/oss-keras/layers/preprocessing',\n+    '/tensorflow/api/keras/layers/preprocessing',\n     'keras preprocessing layers usage', 'method')\n \n \n\n@@ -34,7 +34,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n keras_data_adapter_gauge = tf.__internal__.monitoring.BoolGauge(\n-    \"/tensorflow/api/oss-keras/data_adapters\", \"keras data adapter usage\", \"method\")\n+    \"/tensorflow/api/keras/data_adapters\", \"keras data adapter usage\", \"method\")\n \n \n class DataAdapter(object, metaclass=abc.ABCMeta):\n\n@@ -23,7 +23,7 @@ from tensorflow.python.util.tf_export import keras_export\n # everything will work as normal.\n \n _model_to_estimator_usage_gauge = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/oss-keras/model_to_estimator',\n+    '/tensorflow/api/keras/model_to_estimator',\n     'Whether tf.keras.estimator.model_to_estimator() is called.', 'version')\n \n \n\n@@ -46,7 +46,7 @@ from tensorflow.python.util.tf_export import keras_export\n # TODO(b/168039935): track dropout rate to decide whether/how to make a\n # dropout rate fastpath.\n keras_temporary_dropout_rate = tf.__internal__.monitoring.BoolGauge(\n-    '/tensorflow/api/oss-keras/dropout/temp_rate_is_zero',\n+    '/tensorflow/api/keras/dropout/temp_rate_is_zero',\n     'Temporarily record if Keras dropout layer was created w/'\n     'constant rate = 0')\n \n\n@@ -34,7 +34,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n-    \"/tensorflow/api/oss-keras/optimizers\", \"keras optimizer usage\", \"method\")\n+    \"/tensorflow/api/keras/optimizers\", \"keras optimizer usage\", \"method\")\n \n _DEFAULT_VALID_DTYPES = frozenset([\n     tf.float16, tf.bfloat16, tf.float32, tf.float64,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#089a1fad7704de48b475882ca98a98e704e545c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 58 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 62 | Churn Cumulative: 19580 | Contributors (this commit): 105 | Commits (past 90d): 27 | Contributors (cumulative): 109 | DMM Complexity: 0.0\n\nDIFF:\n@@ -51,6 +51,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n+\n # pylint: disable=g-import-not-at-top\n try:\n   import h5py\n@@ -204,9 +205,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         'trainable', 'dtype', 'dynamic', 'name', 'autocast', 'inputs', 'outputs'\n     })\n     super(Model, self).__init__(**kwargs)\n-    self._check_step_methods_type()\n     # By default, Model is a subclass model, which is not in graph network.\n     self._is_graph_network = False\n+\n     self.inputs = None\n     self.outputs = None\n     self.input_names = None\n@@ -707,7 +708,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # (2) Explicitly setting run_eagerly causes a Model to be run eagerly.\n     # (3) Not explicitly setting run_eagerly defaults to TF's global setting.\n     return (self.dynamic or self._run_eagerly or\n-            (tf.config.functions_run_eagerly() and self._run_eagerly is None))\n+            (tf.config.functions_run_eagerly() and\n+             self._run_eagerly is None))\n \n   @run_eagerly.setter\n   def run_eagerly(self, value):\n@@ -2625,25 +2627,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                        'inputs or `build()` is called with an `input_shape`.' %\n                        self.name)\n \n-  def _check_step_methods_type(self):\n-    \"\"\"Checks that step methods are not decorated with `@tf.function`.\n-\n-    Raises:\n-       TypeError: if any of `self.train_step`, `self.test_step`, or\n-       `self.predict_step` is decorated in `tf.function`.\n-\n-    \"\"\"\n-    steps = {\n-        self.train_step: 'Model.train_step',\n-        self.test_step: 'Model.test_step',\n-        self.predict_step: 'Model.predict_step'\n-    }\n-    for step in steps:\n-      if isinstance(step, tf.__internal__.function.Function):\n-        raise TypeError(\n-            'Make sure method {} when overridden is not decorated with '\n-            '`@tf.function`.'.format(steps[step]))\n-\n   def _check_call_args(self, method_name):\n     \"\"\"Check that `call()` has only one positional arg.\"\"\"\n     # Always allow first arg, regardless of arg name.\n\n@@ -49,43 +49,6 @@ except ImportError:\n \n class TrainingTest(keras_parameterized.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n-  def test_init_type_error(self):\n-\n-    class ModelWithTrainStepOverridden(training_module.Model):\n-\n-      @tf.function\n-      def train_step(self, data):\n-        return 0\n-\n-    class ModelWithTestStepOverridden(training_module.Model):\n-\n-      @tf.function\n-      def test_step(self, data):\n-        return 0\n-\n-    class ModelWithPredictStepOverridden(training_module.Model):\n-\n-      @tf.function\n-      def predict_step(self, data):\n-        return 0\n-\n-    inputs = layers_module.Input(shape=(1,), name='my_input')\n-    outputs = layers_module.Dense(1)(inputs)\n-    with self.assertRaisesRegex(\n-        TypeError, 'Make sure method Model.train_step when overridden is not'\n-        ' decorated with `@tf.function`.'):\n-      ModelWithTrainStepOverridden(inputs, outputs)\n-    with self.assertRaisesRegex(\n-        TypeError, 'Make sure method Model.test_step when overridden is not'\n-        ' decorated with `@tf.function`.'):\n-      ModelWithTestStepOverridden(inputs, outputs)\n-    with self.assertRaisesRegex(\n-        TypeError, 'Make sure method Model.predict_step when overridden is not'\n-        ' decorated with `@tf.function`.'):\n-      ModelWithPredictStepOverridden(inputs, outputs)\n-\n   @keras_parameterized.run_all_keras_modes\n   @keras_parameterized.run_with_all_model_types\n   def test_model_instrumentation(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
