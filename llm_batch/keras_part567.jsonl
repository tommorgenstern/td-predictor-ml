{"custom_id": "keras#32c021387ea1bef3dbab4afaf9b41f878dd49c5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 96 | Lines Deleted: 79 | Files Changed: 4 | Hunks: 45 | Methods Changed: 4 | Complexity Δ (Sum/Max): 24/24 | Churn Δ: 175 | Churn Cumulative: 3028 | Contributors (this commit): 19 | Commits (past 90d): 14 | Contributors (cumulative): 44 | DMM Complexity: 0.0\n\nDIFF:\n@@ -67,47 +67,55 @@ def audio_dataset_from_directory(\n     Only `.wav` files are supported at this time.\n \n     Args:\n-      directory: Directory where the data is located. If `labels` is \"inferred\",\n-        it should contain subdirectories, each containing audio files for a\n-        class. Otherwise, the directory structure is ignored.\n+        directory: Directory where the data is located.\n+            If `labels` is `\"inferred\"`, it should contain subdirectories,\n+            each containing audio files for a class. Otherwise, the directory\n+            structure is ignored.\n         labels: Either \"inferred\" (labels are generated from the directory\n-        structure), None (no labels), or a list/tuple of integer labels of the\n-        same size as the number of audio files found in the directory. Labels\n-        should be sorted according to the alphanumeric order of the audio file\n-        paths (obtained via `os.walk(directory)` in Python).\n+            structure), `None` (no labels), or a list/tuple of integer labels\n+            of the same size as the number of audio files found in\n+            the directory. Labels should be sorted according to the\n+            alphanumeric order of the audio file paths\n+            (obtained via `os.walk(directory)` in Python).\n         label_mode: String describing the encoding of `labels`. Options are:\n-          - 'int': means that the labels are encoded as integers (e.g. for\n-            `sparse_categorical_crossentropy` loss). - 'categorical' means that\n-            the labels are encoded as a categorical vector (e.g. for\n-            `categorical_crossentropy` loss). - 'binary' means that the labels\n-            (there can be only 2) are encoded as `float32` scalars with values 0\n-            or 1 (e.g. for `binary_crossentropy`). - None (no labels).\n-      class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n-        list of class names (must match names of subdirectories). Used to\n-        control the order of the classes (otherwise alphanumerical order is\n-        used).\n-      batch_size: Size of the batches of data. Default: 32. If `None`, the data\n-        will not be batched (the dataset will yield individual samples).\n+            - `\"int\"`: means that the labels are encoded as integers (e.g. for\n+              `sparse_categorical_crossentropy` loss).\n+            - `\"categorical\"` means that the labels are encoded as a categorical\n+              vector (e.g. for `categorical_crossentropy` loss)\n+            - `\"binary\"` means that the labels (there can be only 2)\n+              are encoded as `float32` scalars with values 0\n+              or 1 (e.g. for `binary_crossentropy`).\n+            - `None` (no labels).\n+        class_names: Only valid if \"labels\" is `\"inferred\"`.\n+            This is the explicit list of class names\n+            (must match names of subdirectories). Used to control the order\n+            of the classes (otherwise alphanumerical order is used).\n+        batch_size: Size of the batches of data. Default: 32. If `None`,\n+            the data will not be batched\n+            (the dataset will yield individual samples).\n         sampling_rate: Audio sampling rate (in samples per second).\n         output_sequence_length: Maximum length of an audio sequence. Audio files\n-        longer than this will be truncated to `output_sequence_length`. If set\n-        to `None`, then all sequences in the same batch will be padded to the\n+            longer than this will be truncated to `output_sequence_length`.\n+            If set to `None`, then all sequences in the same batch will\n+            be padded to the\n             length of the longest sequence in the batch.\n         ragged: Whether to return a Ragged dataset (where each sequence has its\n-        own length). Default: False.\n-      shuffle: Whether to shuffle the data. Default: True. If set to False,\n-        sorts the data in alphanumeric order.\n+            own length). Defaults to `False`.\n+        shuffle: Whether to shuffle the data. Defaults to `True`.\n+            If set to `False`, sorts the data in alphanumeric order.\n         seed: Optional random seed for shuffling and transformations.\n         validation_split: Optional float between 0 and 1, fraction of data to\n             reserve for validation.\n-      subset: Subset of the data to return. One of \"training\", \"validation\" or\n-        \"both\". Only used if `validation_split` is set.\n+        subset: Subset of the data to return. One of `\"training\"`,\n+            `\"validation\"` or `\"both\"`. Only used if `validation_split` is set.\n         follow_links: Whether to visits subdirectories pointed to by symlinks.\n-        Defaults to False.\n+            Defaults to `False`.\n \n     Returns:\n+\n     A `tf.data.Dataset` object.\n-        - If `label_mode` is None, it yields `string` tensors of shape\n+\n+    - If `label_mode` is `None`, it yields `string` tensors of shape\n       `(batch_size,)`, containing the contents of a batch of audio files.\n     - Otherwise, it yields a tuple `(audio, labels)`, where `audio`\n       has shape `(batch_size, sequence_length, num_channels)` and `labels`\n@@ -115,6 +123,7 @@ def audio_dataset_from_directory(\n       below.\n \n     Rules regarding labels format:\n+\n     - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n       `(batch_size,)`.\n     - if `label_mode` is `binary`, the labels are a `float32` tensor of\n\n@@ -68,60 +68,62 @@ def image_dataset_from_directory(\n     images from the subdirectories `class_a` and `class_b`, together with labels\n     0 and 1 (0 corresponding to `class_a` and 1 corresponding to `class_b`).\n \n-    Supported image formats: jpeg, png, bmp, gif.\n+    Supported image formats: `.jpeg`, `.jpg`, `.png`, `.bmp`, `.gif`.\n     Animated gifs are truncated to the first frame.\n \n     Args:\n         directory: Directory where the data is located.\n-            If `labels` is \"inferred\", it should contain\n+            If `labels` is `\"inferred\"`, it should contain\n             subdirectories, each containing images for a class.\n             Otherwise, the directory structure is ignored.\n-      labels: Either \"inferred\"\n+        labels: Either `\"inferred\"`\n             (labels are generated from the directory structure),\n-            None (no labels),\n+            `None` (no labels),\n             or a list/tuple of integer labels of the same size as the number of\n             image files found in the directory. Labels should be sorted\n             according to the alphanumeric order of the image file paths\n             (obtained via `os.walk(directory)` in Python).\n         label_mode: String describing the encoding of `labels`. Options are:\n-          - 'int': means that the labels are encoded as integers\n+            - `\"int\"`: means that the labels are encoded as integers\n                 (e.g. for `sparse_categorical_crossentropy` loss).\n-          - 'categorical' means that the labels are\n+            - `\"categorical\"` means that the labels are\n                 encoded as a categorical vector\n                 (e.g. for `categorical_crossentropy` loss).\n-          - 'binary' means that the labels (there can be only 2)\n+            - `\"binary\"` means that the labels (there can be only 2)\n                 are encoded as `float32` scalars with values 0 or 1\n                 (e.g. for `binary_crossentropy`).\n-          - None (no labels).\n-      class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n-          list of class names (must match names of subdirectories). Used\n-          to control the order of the classes (otherwise alphanumerical order\n-          is used).\n-      color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n-          Whether the images will be converted to have 1, 3, or 4 channels.\n-      batch_size: Size of the batches of data. Default: 32.\n+            - `None` (no labels).\n+        class_names: Only valid if `labels` is `\"inferred\"`.\n+            This is the explicit list of class names\n+            (must match names of subdirectories). Used to control the order\n+            of the classes (otherwise alphanumerical order is used).\n+        color_mode: One of `\"grayscale\"`, `\"rgb\"`, `\"rgba\"`.\n+            Defaults to `\"rgb\"`. Whether the images will be converted to\n+            have 1, 3, or 4 channels.\n+        batch_size: Size of the batches of data. Defaults to 32.\n             If `None`, the data will not be batched\n             (the dataset will yield individual samples).\n         image_size: Size to resize images to after they are read from disk,\n             specified as `(height, width)`. Defaults to `(256, 256)`.\n             Since the pipeline processes batches of images that must all have\n             the same size, this must be provided.\n-      shuffle: Whether to shuffle the data. Default: True.\n-          If set to False, sorts the data in alphanumeric order.\n+        shuffle: Whether to shuffle the data. Defaults to `True`.\n+            If set to `False`, sorts the data in alphanumeric order.\n         seed: Optional random seed for shuffling and transformations.\n         validation_split: Optional float between 0 and 1,\n             fraction of data to reserve for validation.\n         subset: Subset of the data to return.\n-          One of \"training\", \"validation\" or \"both\".\n+            One of `\"training\"`, `\"validation\"`, or `\"both\"`.\n             Only used if `validation_split` is set.\n             When `subset=\"both\"`, the utility returns a tuple of two datasets\n             (the training and validation datasets respectively).\n-      interpolation: String, the interpolation method used when resizing images.\n-            Defaults to `bilinear`. Supports `bilinear`, `nearest`, `bicubic`,\n-            `area`, `lanczos3`, `lanczos5`, `gaussian`, `mitchellcubic`.\n+        interpolation: String, the interpolation method used when\n+            resizing images. Defaults to `\"bilinear\"`.\n+            Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n+            `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n         follow_links: Whether to visit subdirectories pointed to by symlinks.\n-          Defaults to False.\n-      crop_to_aspect_ratio: If True, resize the images without aspect\n+            Defaults to `False`.\n+        crop_to_aspect_ratio: If `True`, resize the images without aspect\n             ratio distortion. When the original aspect ratio differs from the\n             target aspect ratio, the output image will be cropped so as to\n             return the largest possible window in the image\n@@ -131,9 +133,10 @@ def image_dataset_from_directory(\n         **kwargs: Legacy keyword arguments.\n \n     Returns:\n+\n     A `tf.data.Dataset` object.\n \n-        - If `label_mode` is None, it yields `float32` tensors of shape\n+    - If `label_mode` is `None`, it yields `float32` tensors of shape\n         `(batch_size, image_size[0], image_size[1], num_channels)`,\n         encoding images (see below for rules regarding `num_channels`).\n     - Otherwise, it yields a tuple `(images, labels)`, where `images` has\n@@ -142,21 +145,21 @@ def image_dataset_from_directory(\n \n     Rules regarding labels format:\n \n-      - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n+    - if `label_mode` is `\"int\"`, the labels are an `int32` tensor of shape\n         `(batch_size,)`.\n-      - if `label_mode` is `binary`, the labels are a `float32` tensor of\n+    - if `label_mode` is `\"binary\"`, the labels are a `float32` tensor of\n         1s and 0s of shape `(batch_size, 1)`.\n-      - if `label_mode` is `categorical`, the labels are a `float32` tensor\n+    - if `label_mode` is `\"categorical\"`, the labels are a `float32` tensor\n         of shape `(batch_size, num_classes)`, representing a one-hot\n         encoding of the class index.\n \n     Rules regarding number of channels in the yielded images:\n \n-      - if `color_mode` is `grayscale`,\n+    - if `color_mode` is `\"grayscale\"`,\n         there's 1 channel in the image tensors.\n-      - if `color_mode` is `rgb`,\n+    - if `color_mode` is `\"rgb\"`,\n         there are 3 channels in the image tensors.\n-      - if `color_mode` is `rgba`,\n+    - if `color_mode` is `\"rgba\"`,\n         there are 4 channels in the image tensors.\n     \"\"\"\n     if \"smart_resize\" in kwargs:\n\n@@ -64,57 +64,60 @@ def text_dataset_from_directory(\n \n     Args:\n         directory: Directory where the data is located.\n-            If `labels` is \"inferred\", it should contain\n+            If `labels` is `\"inferred\"`, it should contain\n             subdirectories, each containing text files for a class.\n             Otherwise, the directory structure is ignored.\n-        labels: Either \"inferred\"\n+        labels: Either `\"inferred\"`\n             (labels are generated from the directory structure),\n-            None (no labels),\n+            `None` (no labels),\n             or a list/tuple of integer labels of the same size as the number of\n             text files found in the directory. Labels should be sorted according\n             to the alphanumeric order of the text file paths\n             (obtained via `os.walk(directory)` in Python).\n         label_mode: String describing the encoding of `labels`. Options are:\n-            - 'int': means that the labels are encoded as integers\n+            - `\"int\"`: means that the labels are encoded as integers\n                 (e.g. for `sparse_categorical_crossentropy` loss).\n-            - 'categorical' means that the labels are\n+            - `\"categorical\"` means that the labels are\n                 encoded as a categorical vector\n                 (e.g. for `categorical_crossentropy` loss).\n-            - 'binary' means that the labels (there can be only 2)\n+            - `\"binary\"` means that the labels (there can be only 2)\n                 are encoded as `float32` scalars with values 0 or 1\n                 (e.g. for `binary_crossentropy`).\n-            - None (no labels).\n-        class_names: Only valid if \"labels\" is \"inferred\". This is the explicit\n-            list of class names (must match names of subdirectories). Used\n-            to control the order of the classes\n-            (otherwise alphanumerical order is used).\n-        batch_size: Size of the batches of data. Default: 32.\n+            - `None` (no labels).\n+        class_names: Only valid if `\"labels\"` is `\"inferred\"`.\n+            This is the explicit list of class names\n+            (must match names of subdirectories). Used to control the order\n+            of the classes (otherwise alphanumerical order is used).\n+        batch_size: Size of the batches of data. Defaults to 32.\n             If `None`, the data will not be batched\n             (the dataset will yield individual samples).\n         max_length: Maximum size of a text string. Texts longer than this will\n             be truncated to `max_length`.\n-        shuffle: Whether to shuffle the data. Default: True.\n-            If set to False, sorts the data in alphanumeric order.\n+        shuffle: Whether to shuffle the data. Defaults to `True`.\n+            If set to `False`, sorts the data in alphanumeric order.\n         seed: Optional random seed for shuffling and transformations.\n         validation_split: Optional float between 0 and 1,\n             fraction of data to reserve for validation.\n         subset: Subset of the data to return.\n-            One of \"training\", \"validation\" or \"both\".\n+            One of `\"training\"`, `\"validation\"` or `\"both\"`.\n             Only used if `validation_split` is set.\n             When `subset=\"both\"`, the utility returns a tuple of two datasets\n             (the training and validation datasets respectively).\n         follow_links: Whether to visits subdirectories pointed to by symlinks.\n-            Defaults to False.\n+            Defaults to `False`.\n \n     Returns:\n+\n     A `tf.data.Dataset` object.\n-        - If `label_mode` is None, it yields `string` tensors of shape\n+\n+    - If `label_mode` is `None`, it yields `string` tensors of shape\n         `(batch_size,)`, containing the contents of a batch of text files.\n     - Otherwise, it yields a tuple `(texts, labels)`, where `texts`\n         has shape `(batch_size,)` and `labels` follows the format described\n         below.\n \n     Rules regarding labels format:\n+\n     - if `label_mode` is `int`, the labels are an `int32` tensor of shape\n         `(batch_size,)`.\n     - if `label_mode` is `binary`, the labels are a `float32` tensor of\n\n@@ -53,9 +53,10 @@ def timeseries_dataset_from_array(\n             `targets[i]` should be the target\n             corresponding to the window that starts at index `i`\n             (see example 2 below).\n-        Pass None if you don't have target data (in this case the dataset will\n-        only yield the input data).\n-      sequence_length: Length of the output sequences (in number of timesteps).\n+            Pass `None` if you don't have target data (in this case the dataset\n+            will only yield the input data).\n+        sequence_length: Length of the output sequences\n+            (in number of timesteps).\n         sequence_stride: Period between successive output sequences.\n             For stride `s`, output samples would\n             start at index `data[i]`, `data[i + s]`, `data[i + 2 * s]`, etc.\n@@ -78,7 +79,8 @@ def timeseries_dataset_from_array(\n             This is useful to reserve part of the data for test or validation.\n \n     Returns:\n-      A tf.data.Dataset instance. If `targets` was passed, the dataset yields\n+\n+    A `tf.data.Dataset` instance. If `targets` was passed, the dataset yields\n     tuple `(batch_of_sequences, batch_of_targets)`. If not, the dataset yields\n     only `batch_of_sequences`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f48b98802333537f25ac6a4e776e4bb17e976c56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 76/76 | Churn Δ: 11 | Churn Cumulative: 1053 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -338,6 +338,7 @@ def build_pip_package(\n     src_directory,\n     dist_directory,\n     is_nightly=False,\n+    rc=None,\n ):\n     # Build Keras with Bazel to get the protobuf .py files\n     os.chdir(keras_root_directory)\n@@ -383,6 +384,8 @@ def build_pip_package(\n     if is_nightly:\n         date = datetime.datetime.now()\n         version += f\".dev{date.strftime('%Y%m%d%H')}\"\n+    elif rc:\n+        version += rc\n     with open(os.path.join(package_directory, \"__init__.py\")) as f:\n         init_contents = f.read()\n     with open(os.path.join(package_directory, \"__init__.py\"), \"w\") as f:\n@@ -455,8 +458,14 @@ if __name__ == \"__main__\":\n         action=\"store_true\",\n         help=\"Whether this is for the `keras-nightly` package.\",\n     )\n+    parser.add_argument(\n+        \"--RC\",\n+        type=str,\n+        help=\"Whether this is for the release candidate.\",\n+    )\n     args = parser.parse_args()\n     is_nightly = args.nightly\n+    rc = args.RC\n \n     build_directory = os.path.join(tempfile.gettempdir(), TMP_BUILD_DIRNAME)\n     keras_root_directory = pathlib.Path(__file__).parent.resolve()\n@@ -472,6 +481,7 @@ if __name__ == \"__main__\":\n             f\"package_directory={package_directory}\\n\"\n             f\"src_directory={src_directory}\\n\"\n             f\"is_nightly={is_nightly}\"\n+            f\"rc={rc}\"\n         )\n     if os.path.exists(build_directory):\n         raise ValueError(f\"Directory already exists: {build_directory}\")\n@@ -487,6 +497,7 @@ if __name__ == \"__main__\":\n             src_directory,\n             dist_directory,\n             is_nightly,\n+            rc,\n         )\n         wheel_filename = [f for f in saved_filenames if f.endswith(\".whl\")][0]\n         if VERBOSE:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#117a514bed5fd2f1124b7d78790cc72a63679ac7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 0 | Files Changed: 6 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 102/44 | Churn Δ: 54 | Churn Cumulative: 16266 | Contributors (this commit): 61 | Commits (past 90d): 43 | Contributors (cumulative): 103 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,6 +16,8 @@\n \n \"\"\"Constraints: functions that impose constraints on weight values.\"\"\"\n \n+import warnings\n+\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n@@ -357,6 +359,13 @@ unitnorm = unit_norm\n \n @keras_export(\"keras.constraints.serialize\")\n def serialize(constraint, use_legacy_format=False):\n+    if not isinstance(constraint, Constraint):\n+        warnings.warn(\n+            \"The `keras.constraints.serialize()` API should only be used for \"\n+            \"objects of type `keras.constraints.Constraint`. Found an instance \"\n+            f\"of type {type(constraint)}, which may lead to improper \"\n+            \"serialization.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(constraint)\n     return serialize_keras_object(constraint)\n\n@@ -15,6 +15,7 @@\n \"\"\"Keras initializer serialization / deserialization.\"\"\"\n \n import threading\n+import warnings\n \n import tensorflow.compat.v2 as tf\n \n@@ -136,6 +137,14 @@ globals().update(LOCAL.ALL_OBJECTS)\n \n @keras_export(\"keras.initializers.serialize\")\n def serialize(initializer, use_legacy_format=False):\n+    populate_deserializable_objects()\n+    if not isinstance(initializer, tuple(LOCAL.ALL_OBJECTS.values())):\n+        warnings.warn(\n+            \"The `keras.initializers.serialize()` API should only be used for \"\n+            \"objects of type `keras.initializers.Initializer`. Found an \"\n+            f\"instance of type {type(initializer)}, which may lead to improper \"\n+            \"serialization.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(initializer)\n \n\n@@ -2854,6 +2854,12 @@ def serialize(loss, use_legacy_format=False):\n     Returns:\n       Loss configuration dictionary.\n     \"\"\"\n+    if not isinstance(loss, Loss):\n+        warnings.warn(\n+            \"The `keras.losses.serialize()` API should only be used for \"\n+            \"objects of type `keras.losses.Loss`. Found an instance of type \"\n+            f\"{type(loss)}, which may lead to improper serialization.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(loss)\n     return serialize_keras_object(loss)\n\n@@ -15,6 +15,7 @@\n \"\"\"All Keras metrics.\"\"\"\n \n # isort: off\n+import warnings\n from tensorflow.python.util.tf_export import keras_export\n \n # Base classes and utilities\n@@ -138,6 +139,12 @@ def serialize(metric, use_legacy_format=False):\n     Returns:\n       Metric configuration dictionary.\n     \"\"\"\n+    if not isinstance(metric, Metric):\n+        warnings.warn(\n+            \"The `keras.metrics.serialize()` API should only be used for \"\n+            \"objects of type `keras.metrics.Metric`. Found an instance of \"\n+            f\"type {type(metric)}, which may lead to improper serialization.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(metric)\n     return serialize_keras_object(metric)\n\n@@ -20,6 +20,7 @@ For more examples see the base class `tf.keras.optimizers.Optimizer`.\n # Imports needed for deserialization.\n \n import platform\n+import warnings\n \n import tensorflow.compat.v2 as tf\n from absl import logging\n@@ -86,6 +87,20 @@ def serialize(optimizer, use_legacy_format=False):\n     Returns:\n       Python dict which contains the configuration of the input optimizer.\n     \"\"\"\n+    if not isinstance(\n+        optimizer,\n+        (\n+            base_optimizer.Optimizer,\n+            Optimizer,\n+            base_optimizer_legacy.OptimizerV2,\n+        ),\n+    ):\n+        warnings.warn(\n+            \"The `keras.optimizers.serialize()` API should only be used for \"\n+            \"objects of type `keras.optimizers.Optimizer`. Found an instance \"\n+            f\"of type {type(optimizer)}, which may lead to improper \"\n+            \"serialization.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(optimizer)\n     return serialize_keras_object(optimizer)\n\n@@ -16,6 +16,7 @@\n \n \n import math\n+import warnings\n \n import tensorflow.compat.v2 as tf\n \n@@ -419,6 +420,13 @@ orthogonal_regularizer = OrthogonalRegularizer\n \n @keras_export(\"keras.regularizers.serialize\")\n def serialize(regularizer, use_legacy_format=False):\n+    if not isinstance(regularizer, Regularizer):\n+        warnings.warn(\n+            \"The `keras.regularizers.serialize()` API should only be used for \"\n+            \"objects of type `keras.regularizers.Regularizer`. Found an \"\n+            f\"instance of type {type(regularizer)}, which may lead to improper \"\n+            \"serialization.\"\n+        )\n     if use_legacy_format:\n         return legacy_serialization.serialize_keras_object(regularizer)\n     return serialize_keras_object(regularizer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6600a5797dc9e57d95500b32b74e5f0663e0423", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 400 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 11 | Methods Changed: 12 | Complexity Δ (Sum/Max): 23/17 | Churn Δ: 409 | Churn Cumulative: 5551 | Contributors (this commit): 9 | Commits (past 90d): 48 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1347,7 +1347,7 @@ def categorical_crossentropy(\n     >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n     >>> loss = keras_core.losses.categorical_crossentropy(y_true, y_pred)\n     >>> assert loss.shape == (2,)\n-    >>> loss.numpy()\n+    >>> loss\n     array([0.0513, 2.303], dtype=float32)\n     \"\"\"\n     if isinstance(axis, bool):\n\n@@ -8,15 +8,19 @@ from keras_core.metrics.hinge_metrics import CategoricalHinge\n from keras_core.metrics.hinge_metrics import Hinge\n from keras_core.metrics.hinge_metrics import SquaredHinge\n from keras_core.metrics.metric import Metric\n+from keras_core.metrics.probabilistic_metrics import BinaryCrossentropy\n+from keras_core.metrics.probabilistic_metrics import CategoricalCrossentropy\n from keras_core.metrics.probabilistic_metrics import KLDivergence\n from keras_core.metrics.probabilistic_metrics import Poisson\n+from keras_core.metrics.probabilistic_metrics import (\n+    SparseCategoricalCrossentropy,\n+)\n from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n from keras_core.metrics.regression_metrics import mean_squared_error\n from keras_core.saving import serialization_lib\n-from keras_core.utils import naming\n \n ALL_OBJECTS = {\n     Metric,\n@@ -30,6 +34,9 @@ ALL_OBJECTS = {\n     CategoricalHinge,\n     KLDivergence,\n     Poisson,\n+    BinaryCrossentropy,\n+    CategoricalCrossentropy,\n+    SparseCategoricalCrossentropy,\n }\n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n ALL_OBJECTS_DICT.update(\n\n@@ -1,6 +1,9 @@\n from keras_core.api_export import keras_core_export\n+from keras_core.losses.losses import binary_crossentropy\n+from keras_core.losses.losses import categorical_crossentropy\n from keras_core.losses.losses import kl_divergence\n from keras_core.losses.losses import poisson\n+from keras_core.losses.losses import sparse_categorical_crossentropy\n from keras_core.metrics import reduction_metrics\n \n \n@@ -62,6 +65,8 @@ class Poisson(reduction_metrics.MeanMetricWrapper):\n         name: (Optional) string name of the metric instance.\n         dtype: (Optional) data type of the metric result.\n \n+    Examples:\n+\n     Standalone usage:\n \n     >>> m = keras_core.metrics.Poisson()\n@@ -89,3 +94,244 @@ class Poisson(reduction_metrics.MeanMetricWrapper):\n \n     def get_config(self):\n         return {\"name\": self.name, \"dtype\": self.dtype}\n+\n+\n+@keras_core_export(\"keras_core.metrics.BinaryCrossentropy\")\n+class BinaryCrossentropy(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the crossentropy metric between the labels and predictions.\n+\n+    This is the crossentropy metric class to be used when there are only two\n+    label classes (0 and 1).\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        from_logits: (Optional) Whether output is expected\n+            to be a logits tensor. By default, we consider\n+            that output encodes a probability distribution.\n+        label_smoothing: (Optional) Float in `[0, 1]`.\n+            When > 0, label values are smoothed,\n+            meaning the confidence on label values are relaxed.\n+            e.g. `label_smoothing=0.2` means that we will use\n+            a value of 0.1 for label \"0\" and 0.9 for label \"1\".\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.BinaryCrossentropy()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result()\n+    0.81492424\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result()\n+    0.9162905\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.BinaryCrossentropy()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name=\"binary_crossentropy\",\n+        dtype=None,\n+        from_logits=False,\n+        label_smoothing=0,\n+    ):\n+        super().__init__(\n+            binary_crossentropy,\n+            name,\n+            dtype=dtype,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+        )\n+        self.from_logits = from_logits\n+        self.label_smoothing = label_smoothing\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"from_logits\": self.from_logits,\n+            \"label_smoothing\": self.label_smoothing,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.metrics.CategoricalCrossentropy\")\n+class CategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the crossentropy metric between the labels and predictions.\n+\n+    This is the crossentropy metric class to be used when there are multiple\n+    label classes (2 or more). It assumes that labels are one-hot encoded,\n+    e.g., when labels values are `[2, 0, 1]`, then\n+    `y_true` is `[[0, 0, 1], [1, 0, 0], [0, 1, 0]]`.\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        from_logits: (Optional) Whether output is expected to be\n+            a logits tensor. By default, we consider that output\n+            encodes a probability distribution.\n+        label_smoothing: (Optional) Float in `[0, 1]`.\n+            When > 0, label values are smoothed, meaning the confidence\n+            on label values are relaxed. e.g. `label_smoothing=0.2` means\n+            that we will use a value of 0.1 for label\n+            \"0\" and 0.9 for label \"1\".\n+        axis: (Optional) Defaults to -1.\n+            The dimension along which entropy is computed.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> # EPSILON = 1e-7, y = y_true, y` = y_pred\n+    >>> # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+    >>> # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+    >>> # xent = -sum(y * log(y'), axis = -1)\n+    >>> #      = -((log 0.95), (log 0.1))\n+    >>> #      = [0.051, 2.302]\n+    >>> # Reduced xent = (0.051 + 2.302) / 2\n+    >>> m = keras_core.metrics.CategoricalCrossentropy()\n+    >>> m.update_state([[0, 1, 0], [0, 0, 1]],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+    >>> m.result()\n+    1.1769392\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1, 0], [0, 0, 1]],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],\n+    ...                sample_weight=np.array([0.3, 0.7]))\n+    >>> m.result()\n+    1.6271976\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.CategoricalCrossentropy()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name=\"categorical_crossentropy\",\n+        dtype=None,\n+        from_logits=False,\n+        label_smoothing=0,\n+        axis=-1,\n+    ):\n+        super().__init__(\n+            categorical_crossentropy,\n+            name,\n+            dtype=dtype,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.label_smoothing = label_smoothing\n+        self.axis = axis\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"from_logits\": self.from_logits,\n+            \"label_smoothing\": self.label_smoothing,\n+            \"axis\": self.axis,\n+        }\n+\n+\n+@keras_core_export(\"keras_core.metrics.SparseCategoricalCrossentropy\")\n+class SparseCategoricalCrossentropy(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Computes the crossentropy metric between the labels and predictions.\n+\n+    Use this crossentropy metric when there are two or more label classes.\n+    It expects labels to be provided as integers. If you want to provide labels\n+    that are one-hot encoded, please use the `CategoricalCrossentropy`\n+    metric instead.\n+\n+    There should be `num_classes` floating point values per feature for `y_pred`\n+    and a single floating point value per feature for `y_true`.\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        from_logits: (Optional) Whether output is expected\n+            to be a logits tensor. By default, we consider that output\n+            encodes a probability distribution.\n+        axis: (Optional) Defaults to -1.\n+            The dimension along which entropy is computed.\n+\n+    Examples:\n+\n+    Standalone usage:\n+\n+    >>> # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n+    >>> # logits = log(y_pred)\n+    >>> # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+    >>> # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+    >>> # xent = -sum(y * log(softmax), 1)\n+    >>> # log(softmax) = [[-2.9957, -0.0513, -16.1181],\n+    >>> #                [-2.3026, -0.2231, -2.3026]]\n+    >>> # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n+    >>> # xent = [0.0513, 2.3026]\n+    >>> # Reduced xent = (0.0513 + 2.3026) / 2\n+    >>> m = keras_core.metrics.SparseCategoricalCrossentropy()\n+    >>> m.update_state([1, 2],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+    >>> m.result()\n+    1.1769392\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([1, 2],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],\n+    ...                sample_weight=np.array([0.3, 0.7]))\n+    >>> m.result()\n+    1.6271976\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[keras_core.metrics.SparseCategoricalCrossentropy()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name=\"sparse_categorical_crossentropy\",\n+        dtype=None,\n+        from_logits=False,\n+        axis=-1,\n+    ):\n+        super().__init__(\n+            sparse_categorical_crossentropy,\n+            name=name,\n+            dtype=dtype,\n+            from_logits=from_logits,\n+            axis=axis,\n+        )\n+        self.from_logits = from_logits\n+        self.axis = axis\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"from_logits\": self.from_logits,\n+            \"axis\": self.axis,\n+        }\n\n@@ -65,13 +65,7 @@ class PoissonTest(testing.TestCase):\n         )\n \n     def test_config(self):\n-        poisson_obj = metrics.Poisson(name=\"poisson\", dtype=\"float32\")\n-        self.assertEqual(poisson_obj.name, \"poisson\")\n-        self.assertEqual(poisson_obj._dtype, \"float32\")\n-\n-        poisson_obj2 = metrics.Poisson.from_config(poisson_obj.get_config())\n-        self.assertEqual(poisson_obj2.name, \"poisson\")\n-        self.assertEqual(poisson_obj2._dtype, \"float32\")\n+        self.run_class_serialization_test(metrics.Poisson(name=\"poisson\"))\n \n     def test_unweighted(self):\n         self.setup()\n@@ -96,3 +90,147 @@ class PoissonTest(testing.TestCase):\n         expected_result = np.multiply(self.expected_results, sample_weight)\n         expected_result = np.sum(expected_result) / np.sum(sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+\n+class BinaryCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            metrics.BinaryCrossentropy(\n+                name=\"bce\", dtype=\"int32\", label_smoothing=0.2\n+            )\n+        )\n+\n+    def test_unweighted(self):\n+        bce_obj = metrics.BinaryCrossentropy()\n+        y_true = np.array([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.array([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        result = bce_obj(y_true, y_pred)\n+        self.assertAllClose(result, 3.9855, atol=1e-3)\n+\n+    def test_unweighted_with_logits(self):\n+        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n+\n+        y_true = np.array([[1, 0, 1], [0, 1, 1]])\n+        y_pred = np.array([[10.0, -10.0, 10.0], [10.0, 10.0, -10.0]])\n+        result = bce_obj(y_true, y_pred)\n+        self.assertAllClose(result, 3.333, atol=1e-3)\n+\n+    def test_weighted(self):\n+        bce_obj = metrics.BinaryCrossentropy()\n+        y_true = np.array([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.array([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        sample_weight = np.array([1.5, 2.0])\n+        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(result, 3.4162, atol=1e-3)\n+\n+    def test_weighted_from_logits(self):\n+        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n+        y_true = np.array([[1, 0, 1], [0, 1, 1]])\n+        y_pred = np.array([[10.0, -10.0, 10.0], [10.0, 10.0, -10.0]])\n+        sample_weight = np.array([2.0, 2.5])\n+        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(result, 3.7037, atol=1e-3)\n+\n+    def test_label_smoothing(self):\n+        logits = np.array(((10.0, -10.0, -10.0)))\n+        y_true = np.array(((1, 0, 1)))\n+        label_smoothing = 0.1\n+        bce_obj = metrics.BinaryCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        result = bce_obj(y_true, logits)\n+        expected_value = (10.0 + 5.0 * label_smoothing) / 3.0\n+        self.assertAllClose(expected_value, result, atol=1e-3)\n+\n+\n+class CategoricalCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            metrics.CategoricalCrossentropy(\n+                name=\"cce\", dtype=\"int32\", label_smoothing=0.2\n+            )\n+        )\n+\n+    def test_unweighted(self):\n+        cce_obj = metrics.CategoricalCrossentropy()\n+        y_true = np.array([[0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        result = cce_obj(y_true, y_pred)\n+        self.assertAllClose(result, 1.176, atol=1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n+\n+        y_true = np.array([[0, 1, 0], [0, 0, 1]])\n+        logits = np.array([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        result = cce_obj(y_true, logits)\n+        self.assertAllClose(result, 3.5011, atol=1e-3)\n+\n+    def test_weighted(self):\n+        cce_obj = metrics.CategoricalCrossentropy()\n+\n+        y_true = np.array([[0, 1, 0], [0, 0, 1]])\n+        y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        sample_weight = np.array([1.5, 2.0])\n+        result = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(result, 1.338, atol=1e-3)\n+\n+    def test_weighted_from_logits(self):\n+        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n+\n+        y_true = np.array([[0, 1, 0], [0, 0, 1]])\n+        logits = np.array([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        sample_weight = np.array([1.5, 2.0])\n+        result = cce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAllClose(result, 4.0012, atol=1e-3)\n+\n+    def test_label_smoothing(self):\n+        y_true = np.array([[0, 1, 0], [0, 0, 1]])\n+        logits = np.array([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        label_smoothing = 0.1\n+        cce_obj = metrics.CategoricalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose(loss, 3.667, atol=1e-3)\n+\n+\n+class SparseCategoricalCrossentropyTest(testing.TestCase):\n+    def test_config(self):\n+        self.run_class_serialization_test(\n+            metrics.SparseCategoricalCrossentropy(name=\"scce\", dtype=\"int32\")\n+        )\n+\n+    def test_unweighted(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy()\n+\n+        y_true = np.array([1, 2])\n+        y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        result = scce_obj(y_true, y_pred)\n+        self.assertAllClose(result, 1.176, atol=1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n+\n+        y_true = np.array([1, 2])\n+        logits = np.array([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        result = scce_obj(y_true, logits)\n+        self.assertAllClose(result, 3.5011, atol=1e-3)\n+\n+    def test_weighted(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy()\n+\n+        y_true = np.array([1, 2])\n+        y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        sample_weight = np.array([1.5, 2.0])\n+        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(result, 1.338, atol=1e-3)\n+\n+    def test_weighted_from_logits(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n+\n+        y_true = np.array([1, 2])\n+        logits = np.array([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        sample_weight = np.array([1.5, 2.0])\n+        result = scce_obj(y_true, logits, sample_weight=sample_weight)\n+        self.assertAllClose(result, 4.0012, atol=1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d6f77ab84a27c818577c8d51d29684c39ccaa01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 8 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 218 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -19,30 +19,51 @@ from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.regression_metrics import MeanSquaredError\n-from keras_core.metrics.regression_metrics import mean_squared_error\n+from keras_core.metrics.accuracy_metrics import Accuracy\n+from keras_core.metrics.accuracy_metrics import BinaryAccuracy\n+from keras_core.metrics.accuracy_metrics import CategoricalAccuracy\n+from keras_core.metrics.accuracy_metrics import SparseCategoricalAccuracy\n+from keras_core.metrics.accuracy_metrics import TopKCategoricalAccuracy\n+from keras_core.metrics.accuracy_metrics import SparseTopKCategoricalAccuracy\n from keras_core.saving import serialization_lib\n \n ALL_OBJECTS = {\n+    # Base\n     Metric,\n     Mean,\n     Sum,\n-    MeanSquaredError,\n     MeanMetricWrapper,\n+    # Regression\n+    MeanSquaredError,\n+    # Classification\n+    FalseNegatives,\n     FalsePositives,\n+    Precision,\n+    TrueNegatives,\n+    TruePositives,\n+    # Hinge\n     Hinge,\n     SquaredHinge,\n     CategoricalHinge,\n+    # Probabilistic\n     KLDivergence,\n     Poisson,\n     BinaryCrossentropy,\n     CategoricalCrossentropy,\n     SparseCategoricalCrossentropy,\n+    # Accuracy\n+    Accuracy,\n+    BinaryAccuracy,\n+    CategoricalAccuracy,\n+    SparseCategoricalAccuracy,\n+    TopKCategoricalAccuracy,\n+    SparseTopKCategoricalAccuracy,\n }\n ALL_OBJECTS_DICT = {cls.__name__: cls for cls in ALL_OBJECTS}\n ALL_OBJECTS_DICT.update(\n     {\n-        \"mse\": mean_squared_error,\n-        \"MSE\": mean_squared_error,\n+        \"mse\": MeanSquaredError,\n+        \"MSE\": MeanSquaredError,\n     }\n )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a31e96fed71ebfbf1a947e09dc3cd61a5ff169a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 184 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 185 | Churn Cumulative: 310 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,8 @@ from keras_core.layers.core.input_layer import InputLayer\n from keras_core.layers.layer import Layer\n from keras_core.layers.merging.add import Add\n from keras_core.layers.merging.add import add\n+from keras_core.layers.merging.subtract import Subtract\n+from keras_core.layers.merging.subtract import subtract\n from keras_core.layers.regularization.activity_regularization import (\n     ActivityRegularization,\n )\n\n@@ -24,7 +24,7 @@ class MergingLayersTest(testing.TestCase):\n \n     @pytest.mark.skipif(\n         not backend.DYNAMIC_SHAPES_OK,\n-        reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+        reason=\"Backend does not support dynamic shapes.\",\n     )\n     def test_add_correctness_dynamic(self):\n         x1 = np.random.rand(2, 4, 5)\n@@ -99,3 +99,103 @@ class MergingLayersTest(testing.TestCase):\n             ValueError, \" should have the same length.\"\n         ):\n             add_layer.compute_mask([input_1, input_2], [None])\n+\n+    def test_subtract_basic(self):\n+        self.run_layer_test(\n+            layers.Subtract,\n+            init_kwargs={},\n+            input_shape=[(2, 3), (2, 3)],\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes.\",\n+    )\n+    def test_subtract_correctness_dynamic(self):\n+        x1 = np.random.rand(2, 4, 5)\n+        x2 = np.random.rand(2, 4, 5)\n+        x3 = ops.convert_to_tensor(x1 - x2)\n+\n+        input_1 = layers.Input(shape=(4, 5))\n+        input_2 = layers.Input(shape=(4, 5))\n+        subtract_layer = layers.Subtract()\n+        out = subtract_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (2, 4, 5))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            subtract_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                subtract_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_subtract_correctness_static(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+        x2 = np.random.rand(batch_size, *shape)\n+        x3 = ops.convert_to_tensor(x1 - x2)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        subtract_layer = layers.Subtract()\n+        out = subtract_layer([input_1, input_2])\n+        model = models.Model([input_1, input_2], out)\n+        res = model([x1, x2])\n+\n+        self.assertEqual(res.shape, (batch_size, *shape))\n+        self.assertAllClose(res, x3, atol=1e-4)\n+        self.assertIsNone(\n+            subtract_layer.compute_mask([input_1, input_2], [None, None])\n+        )\n+        self.assertTrue(\n+            np.all(\n+                subtract_layer.compute_mask(\n+                    [input_1, input_2],\n+                    [backend.Variable(x1), backend.Variable(x2)],\n+                )\n+            )\n+        )\n+\n+    def test_subtract_errors(self):\n+        batch_size = 2\n+        shape = (4, 5)\n+        x1 = np.random.rand(batch_size, *shape)\n+\n+        input_1 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_2 = layers.Input(shape=shape, batch_size=batch_size)\n+        input_3 = layers.Input(shape=shape, batch_size=batch_size)\n+        subtract_layer = layers.Subtract()\n+\n+        with self.assertRaisesRegex(ValueError, \"`mask` should be a list.\"):\n+            subtract_layer.compute_mask([input_1, input_2], x1)\n+\n+        with self.assertRaisesRegex(ValueError, \"`inputs` should be a list.\"):\n+            subtract_layer.compute_mask(input_1, [None, None])\n+\n+        with self.assertRaisesRegex(\n+            ValueError, \" should have the same length.\"\n+        ):\n+            subtract_layer.compute_mask([input_1, input_2], [None])\n+        with self.assertRaisesRegex(\n+            ValueError, \"layer should be called on exactly 2 inputs\"\n+        ):\n+            layers.Subtract()([input_1, input_2, input_3])\n+        with self.assertRaisesRegex(\n+            ValueError, \"layer should be called on exactly 2 inputs\"\n+        ):\n+            layers.Subtract()([input_1])\n\n@@ -0,0 +1,81 @@\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.merging.base_merge import Merge\n+\n+\n+@keras_core_export(\"keras_core.layers.Subtract\")\n+class Subtract(Merge):\n+    \"\"\"Performs elementwise subtraction.\n+\n+    It takes as input a list of tensors of size 2 both of the\n+    same shape, and returns a single tensor (inputs[0] - inputs[1])\n+    of same shape.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.Subtract()([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> # equivalent to `subtracted = keras_core.layers.subtract([x1, x2])`\n+    >>> subtracted = keras_core.layers.Subtract()([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(subtracted)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+\n+    def build(self, input_shape):\n+        super().build(input_shape)\n+        if len(input_shape) != 2:\n+            raise ValueError(\n+                \"A `Subtract` layer should be called on exactly 2 inputs. \"\n+                f\"Received: input_shape={input_shape}\"\n+            )\n+\n+    def _merge_function(self, inputs):\n+        if len(inputs) != 2:\n+            raise ValueError(\n+                \"A `Subtract` layer should be called on exactly 2 inputs. \"\n+                f\"Received: inputs={inputs}\"\n+            )\n+        return inputs[0] - inputs[1]\n+\n+\n+@keras_core_export(\"keras_core.layers.subtract\")\n+def subtract(inputs, **kwargs):\n+    \"\"\"Functional interface to the `keras_core.layers.Subtract` layer.\n+\n+    Args:\n+        inputs: A list of input tensors of size 2, each tensor of\n+            the same shape.\n+        **kwargs: Standard layer keyword arguments.\n+\n+    Returns:\n+        A tensor as the difference of the inputs. It has the same shape\n+        as the inputs.\n+\n+    Examples:\n+\n+    >>> input_shape = (2, 3, 4)\n+    >>> x1 = np.random.rand(*input_shape)\n+    >>> x2 = np.random.rand(*input_shape)\n+    >>> y = keras_core.layers.subtract([x1, x2])\n+\n+    Usage in a Keras model:\n+\n+    >>> input1 = keras_core.layers.Input(shape=(16,))\n+    >>> x1 = keras_core.layers.Dense(8, activation='relu')(input1)\n+    >>> input2 = keras_core.layers.Input(shape=(32,))\n+    >>> x2 = keras_core.layers.Dense(8, activation='relu')(input2)\n+    >>> subtracted = keras_core.layers.subtract([x1, x2])\n+    >>> out = keras_core.layers.Dense(4)(subtracted)\n+    >>> model = keras_core.models.Model(inputs=[input1, input2], outputs=out)\n+\n+    \"\"\"\n+    return Subtract(**kwargs)(inputs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
