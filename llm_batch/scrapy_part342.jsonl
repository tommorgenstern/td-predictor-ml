{"custom_id": "scrapy#1a3db8149224d6875a90513b52437fb1fa916e10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 385 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -106,8 +106,8 @@ class KeywordArgumentsSpider(MockServerSpider):\n             self.crawler.stats.inc_value(\"boolean_checks\", 3)\n         elif response.url.endswith(\"/general_without\"):\n             self.checks.append(\n-                kwargs == {}\n-            )  # pylint: disable=use-implicit-booleaness-not-comparison\n+                kwargs == {}  # pylint: disable=use-implicit-booleaness-not-comparison\n+            )\n             self.crawler.stats.inc_value(\"boolean_checks\")\n \n     def parse_no_kwargs(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6757973b61766cdb28225733f1c12702fe0886db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 20 | Churn Cumulative: 1757 | Contributors (this commit): 24 | Commits (past 90d): 4 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,14 +2,16 @@ import codecs\n import unittest\n from unittest import mock\n \n+from pkg_resources import parse_version\n+from w3lib import __version__ as w3lib_version\n from w3lib.encoding import resolve_encoding\n \n-from scrapy.http import (Request, Response, TextResponse, HtmlResponse,\n-                         XmlResponse, Headers)\n+from scrapy.exceptions import NotSupported\n+from scrapy.http import (Headers, HtmlResponse, Request, Response,\n+                         TextResponse, XmlResponse)\n+from scrapy.link import Link\n from scrapy.selector import Selector\n from scrapy.utils.python import to_unicode\n-from scrapy.exceptions import NotSupported\n-from scrapy.link import Link\n from tests import get_testdata\n \n \n@@ -180,12 +182,18 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertRaises(ValueError, r.follow, None)\n \n     def test_follow_whitespace_url(self):\n+        target_url = 'http://example.com/foo'\n+        if parse_version(w3lib_version) < parse_version(\"2.1.1\"):\n+            target_url += '%20'\n         self._assert_followed_url('foo ',\n-                                  'http://example.com/foo%20')\n+                                  target_url)\n \n     def test_follow_whitespace_link(self):\n+        target_url = 'http://example.com/foo'\n+        if parse_version(w3lib_version) < parse_version(\"2.1.1\"):\n+            target_url += '%20'\n         self._assert_followed_url(Link('http://example.com/foo '),\n-                                  'http://example.com/foo%20')\n+                                  target_url)\n \n     def test_follow_flags(self):\n         res = self.response_class('http://example.com/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#087334009c2adcf45cb224b28c9c40857253657a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 59 | Churn Cumulative: 8204 | Contributors (this commit): 43 | Commits (past 90d): 8 | Contributors (cumulative): 71 | DMM Complexity: 1.0\n\nDIFF:\n@@ -350,11 +350,11 @@ class FeedExporter:\n         return defer.DeferredList(deferred_list) if deferred_list else None\n \n     def _close_slot(self, slot, spider):\n+        slot.finish_exporting()\n         if not slot.itemcount and not slot.store_empty:\n             # We need to call slot.storage.store nonetheless to get the file\n             # properly closed.\n             return defer.maybeDeferred(slot.storage.store, slot.file)\n-        slot.finish_exporting()\n         logmsg = f\"{slot.format} feed ({slot.itemcount} items) in: {slot.uri}\"\n         d = defer.maybeDeferred(slot.storage.store, slot.file)\n \n\n@@ -33,8 +33,9 @@ from zope.interface.verify import verifyObject\n \n import scrapy\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.exporters import CsvItemExporter\n+from scrapy.exporters import CsvItemExporter, JsonItemExporter\n from scrapy.extensions.feedexport import (\n+    _FeedSlot,\n     BlockingFeedStorage,\n     FeedExporter,\n     FileFeedStorage,\n@@ -890,6 +891,60 @@ class FeedExportTest(FeedExportTestBase):\n             data = yield self.exported_no_data(settings)\n             self.assertEqual(b'', data[fmt])\n \n+    @defer.inlineCallbacks\n+    def test_finish_exporting_is_called(self):\n+        # for each format, keep track of when start_exporting\n+        # has been called but finish_exporting hasn't been called\n+        startRecordingTracker = {}\n+        # we expect finish_recording to be called, setting this to false\n+        expected = {'json': False}\n+\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+        ]\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'json'},\n+            },\n+            'FEED_EXPORT_INDENT': None,\n+        }\n+\n+        # override export_item to raise exception\n+        class FakeJsonItemExporter(JsonItemExporter):\n+            def export_item(self, item):\n+                raise Exception('foo')\n+\n+        # override start/stop_exporting to modify startRecordingTracker\n+        class FakeFeedSlot(_FeedSlot):\n+            def start_exporting(self):\n+                startRecordingTracker[self.format] = True\n+                if not self._exporting:\n+                    self.exporter.start_exporting()\n+                    self._exporting = True\n+            \n+            def finish_exporting(self):\n+                print('finish export called')\n+                startRecordingTracker[self.format] = False\n+                if self._exporting:\n+                    self.exporter.finish_exporting()\n+                    self._exporting = False\n+            \n+\n+        with ExitStack() as stack:\n+            stack.enter_context(\n+                mock.patch(\n+                    'scrapy.exporters.JsonItemExporter', FakeJsonItemExporter\n+                )\n+            )\n+            stack.enter_context(\n+                mock.patch(\n+                    'scrapy.extensions.feedexport._FeedSlot', FakeFeedSlot\n+                )\n+            )\n+            _ = yield self.exported_data(items, settings)\n+            self.assertDictEqual(startRecordingTracker, expected)\n+\n+\n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n         formats = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e92e201b192609259c01b8686bb10f188e40be36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 572 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -46,6 +46,9 @@ DEFAULT_LOGGING = {\n     'version': 1,\n     'disable_existing_loggers': False,\n     'loggers': {\n+        'filelock': {\n+            'level': 'ERROR',\n+        },\n         'hpack': {\n             'level': 'ERROR',\n         },\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dbd1d9b81399b624d8f8081dbc4ba08e66993e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 23 | Churn Cumulative: 1780 | Contributors (this commit): 24 | Commits (past 90d): 5 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,8 @@ import codecs\n import unittest\n from unittest import mock\n \n-from pkg_resources import parse_version\n+from packaging.version import Version as parse_version\n+from pytest import mark\n from w3lib import __version__ as w3lib_version\n from w3lib.encoding import resolve_encoding\n \n@@ -181,19 +182,23 @@ class BaseResponseTest(unittest.TestCase):\n         r = self.response_class(\"http://example.com\")\n         self.assertRaises(ValueError, r.follow, None)\n \n+    @mark.xfail(\n+        parse_version(w3lib_version) >= parse_version(\"2.1.1\"),\n+        reason=\"https://github.com/scrapy/w3lib/pull/207\",\n+        strict=True,\n+    )\n     def test_follow_whitespace_url(self):\n-        target_url = 'http://example.com/foo'\n-        if parse_version(w3lib_version) < parse_version(\"2.1.1\"):\n-            target_url += '%20'\n         self._assert_followed_url('foo ',\n-                                  target_url)\n+                                  'http://example.com/foo%20')\n \n+    @mark.xfail(\n+        parse_version(w3lib_version) >= parse_version(\"2.1.1\"),\n+        reason=\"https://github.com/scrapy/w3lib/pull/207\",\n+        strict=True,\n+    )\n     def test_follow_whitespace_link(self):\n-        target_url = 'http://example.com/foo'\n-        if parse_version(w3lib_version) < parse_version(\"2.1.1\"):\n-            target_url += '%20'\n         self._assert_followed_url(Link('http://example.com/foo '),\n-                                  target_url)\n+                                  'http://example.com/foo%20')\n \n     def test_follow_flags(self):\n         res = self.response_class('http://example.com/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e099572cec6bb9aa7e164b3bff9fd8723286fbad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1788 | Contributors (this commit): 24 | Commits (past 90d): 6 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -183,22 +183,22 @@ class BaseResponseTest(unittest.TestCase):\n         self.assertRaises(ValueError, r.follow, None)\n \n     @mark.xfail(\n-        parse_version(w3lib_version) >= parse_version(\"2.1.1\"),\n+        parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n         reason=\"https://github.com/scrapy/w3lib/pull/207\",\n         strict=True,\n     )\n     def test_follow_whitespace_url(self):\n         self._assert_followed_url('foo ',\n-                                  'http://example.com/foo%20')\n+                                  'http://example.com/foo')\n \n     @mark.xfail(\n-        parse_version(w3lib_version) >= parse_version(\"2.1.1\"),\n+        parse_version(w3lib_version) < parse_version(\"2.1.1\"),\n         reason=\"https://github.com/scrapy/w3lib/pull/207\",\n         strict=True,\n     )\n     def test_follow_whitespace_link(self):\n         self._assert_followed_url(Link('http://example.com/foo '),\n-                                  'http://example.com/foo%20')\n+                                  'http://example.com/foo')\n \n     def test_follow_flags(self):\n         res = self.response_class('http://example.com/')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66f127eb37ac7d2d85d641f9d8f49fa6e3130a92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 46 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 84 | Churn Cumulative: 6761 | Contributors (this commit): 36 | Commits (past 90d): 6 | Contributors (cumulative): 36 | DMM Complexity: 0.0\n\nDIFF:\n@@ -680,6 +680,45 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n                     break\n         return result\n \n+class InstrumentedFeedSlot(_FeedSlot):\n+    \"\"\"Instrumented _FeedSlot subclass for keeping track of calls to\n+    start_exporting and finish_exporting.\"\"\"\n+    def start_exporting(self):\n+        self.update_listener('start')\n+        super().start_exporting()\n+    \n+    def finish_exporting(self):\n+        self.update_listener('finish')\n+        super().start_exporting()\n+    \n+    @classmethod\n+    def subscribe__listener(cls, listener):\n+        cls.update_listener = listener.update\n+\n+class IsExportingListener:\n+    \"\"\"When subscribed to InstrumentedFeedSlot, keeps track of when\n+    a call to start_exporting has been made without a closing call to\n+    finish_exporting and when a call to finis_exporting has been made\n+    before a call to start_exporting.\"\"\"\n+    def __init__(self):\n+        self.start_without_finish = False\n+        self.finish_without_start = False\n+\n+    def update(self, method):\n+        if method == 'start':\n+            self.start_without_finish = True\n+        elif method == 'finish':\n+            if self.start_without_finish:\n+                self.start_without_finish = False\n+            else:\n+                self.finish_before_start = True\n+\n+\n+class ExceptionJsonItemExporter(JsonItemExporter):\n+    \"\"\"JsonItemExporter that throws an exception every time export_item is called.\"\"\"\n+    def export_item(self, _):\n+        raise Exception('foo')\n+\n \n class FeedExportTest(FeedExportTestBase):\n     __test__ = True\n@@ -893,12 +932,6 @@ class FeedExportTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_finish_exporting_is_called(self):\n-        # for each format, keep track of when start_exporting\n-        # has been called but finish_exporting hasn't been called\n-        startRecordingTracker = {}\n-        # we expect finish_recording to be called, setting this to false\n-        expected = {'json': False}\n-\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n         ]\n@@ -906,43 +939,18 @@ class FeedExportTest(FeedExportTestBase):\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'json'},\n             },\n+            'FEED_EXPORTERS': {'json': ExceptionJsonItemExporter},\n             'FEED_EXPORT_INDENT': None,\n         }\n \n-        # override export_item to raise exception\n-        class FakeJsonItemExporter(JsonItemExporter):\n-            def export_item(self, item):\n-                raise Exception('foo')\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n \n-        # override start/stop_exporting to modify startRecordingTracker\n-        class FakeFeedSlot(_FeedSlot):\n-            def start_exporting(self):\n-                startRecordingTracker[self.format] = True\n-                if not self._exporting:\n-                    self.exporter.start_exporting()\n-                    self._exporting = True\n-            \n-            def finish_exporting(self):\n-                print('finish export called')\n-                startRecordingTracker[self.format] = False\n-                if self._exporting:\n-                    self.exporter.finish_exporting()\n-                    self._exporting = False\n-            \n-\n-        with ExitStack() as stack:\n-            stack.enter_context(\n-                mock.patch(\n-                    'scrapy.exporters.JsonItemExporter', FakeJsonItemExporter\n-                )\n-            )\n-            stack.enter_context(\n-                mock.patch(\n-                    'scrapy.extensions.feedexport._FeedSlot', FakeFeedSlot\n-                )\n-            )\n+        with mock.patch('scrapy.extensions.feedexport._FeedSlot',\n+                        InstrumentedFeedSlot):\n             _ = yield self.exported_data(items, settings)\n-            self.assertDictEqual(startRecordingTracker, expected)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n \n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8d67a08155cfd0b745a2b62538d7fd15c033184e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 61 | Churn Cumulative: 6822 | Contributors (this commit): 36 | Commits (past 90d): 7 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -931,7 +931,47 @@ class FeedExportTest(FeedExportTestBase):\n             self.assertEqual(b'', data[fmt])\n \n     @defer.inlineCallbacks\n-    def test_finish_exporting_is_called(self):\n+    def test_start_finish_exporting_items(self):\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+        ]\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'json'},\n+            },\n+            'FEED_EXPORT_INDENT': None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch('scrapy.extensions.feedexport._FeedSlot',\n+                        InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n+\n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_no_items(self):\n+        items = []\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'json'},\n+            },\n+            'FEED_EXPORT_INDENT': None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch('scrapy.extensions.feedexport._FeedSlot',\n+                        InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n+\n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_items_exception(self):\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n         ]\n@@ -952,6 +992,25 @@ class FeedExportTest(FeedExportTestBase):\n             self.assertFalse(listener.start_without_finish)\n             self.assertFalse(listener.finish_without_start)\n \n+    @defer.inlineCallbacks\n+    def test_start_finish_exporting_no_items_exception(self):\n+        items = []\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {'format': 'json'},\n+            },\n+            'FEED_EXPORTERS': {'json': ExceptionJsonItemExporter},\n+            'FEED_EXPORT_INDENT': None,\n+        }\n+\n+        listener = IsExportingListener()\n+        InstrumentedFeedSlot.subscribe__listener(listener)\n+\n+        with mock.patch('scrapy.extensions.feedexport._FeedSlot',\n+                        InstrumentedFeedSlot):\n+            _ = yield self.exported_data(items, settings)\n+            self.assertFalse(listener.start_without_finish)\n+            self.assertFalse(listener.finish_without_start)\n \n     @defer.inlineCallbacks\n     def test_export_no_items_store_empty(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40f4b262d2046f8b64d55c477f1a8c9897d2c838", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6824 | Contributors (this commit): 36 | Commits (past 90d): 8 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -680,6 +680,7 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n                     break\n         return result\n \n+\n class InstrumentedFeedSlot(_FeedSlot):\n     \"\"\"Instrumented _FeedSlot subclass for keeping track of calls to\n     start_exporting and finish_exporting.\"\"\"\n@@ -695,6 +696,7 @@ class InstrumentedFeedSlot(_FeedSlot):\n     def subscribe__listener(cls, listener):\n         cls.update_listener = listener.update\n \n+\n class IsExportingListener:\n     \"\"\"When subscribed to InstrumentedFeedSlot, keeps track of when\n     a call to start_exporting has been made without a closing call to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b09b0e0d7eb133db648dfd02a8a531219db0a15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 32 | Churn Cumulative: 1141 | Contributors (this commit): 29 | Commits (past 90d): 6 | Contributors (cumulative): 38 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,7 @@ from scrapy.utils.console import DEFAULT_PYTHON_SHELLS, start_python_console\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.misc import load_object\n from scrapy.utils.response import open_in_browser\n+from scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\n \n \n class Shell:\n@@ -76,6 +77,10 @@ class Shell:\n                                  banner=self.vars.pop('banner', ''))\n \n     def _schedule(self, request, spider):\n+        if is_asyncio_reactor_installed():\n+            # set the asyncio event loop for the current thread\n+            event_loop_path = self.crawler.settings['ASYNCIO_EVENT_LOOP']\n+            set_asyncio_event_loop(event_loop_path)\n         spider = self._open_spider(request, spider)\n         d = _request_deferred(request)\n         d.addCallback(lambda x: (x, spider))\n\n@@ -71,14 +71,7 @@ def install_reactor(reactor_path, event_loop_path=None):\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n-            policy = get_asyncio_event_loop_policy()\n-            if event_loop_path is not None:\n-                event_loop_class = load_object(event_loop_path)\n-                event_loop = event_loop_class()\n-                asyncio.set_event_loop(event_loop)\n-            else:\n-                event_loop = policy.get_event_loop()\n-\n+            event_loop = set_asyncio_event_loop(event_loop_path)\n             asyncioreactor.install(eventloop=event_loop)\n     else:\n         *module, _ = reactor_path.split(\".\")\n@@ -87,6 +80,24 @@ def install_reactor(reactor_path, event_loop_path=None):\n         with suppress(error.ReactorAlreadyInstalledError):\n             installer()\n \n+def set_asyncio_event_loop(event_loop_path):\n+    \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n+    policy = get_asyncio_event_loop_policy()\n+    if event_loop_path is not None:\n+        event_loop_class = load_object(event_loop_path)\n+        event_loop = event_loop_class()\n+        asyncio.set_event_loop(event_loop)\n+    else:\n+        try:\n+            event_loop = policy.get_event_loop()\n+        except RuntimeError:\n+            # `get_event_loop` is expected to fail when called from a new thread\n+            # with no asyncio event loop yet installed. Such is the case when\n+            # called from `scrapy shell`\n+            event_loop = policy.new_event_loop()\n+            asyncio.set_event_loop(event_loop)\n+    return event_loop\n+\n \n def verify_installed_reactor(reactor_path):\n     \"\"\"Raises :exc:`Exception` if the installed\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c2de9372a27120abf81f2aaf6609e5acae82549e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 911 | Contributors (this commit): 19 | Commits (past 90d): 8 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -80,6 +80,7 @@ def install_reactor(reactor_path, event_loop_path=None):\n         with suppress(error.ReactorAlreadyInstalledError):\n             installer()\n \n+\n def set_asyncio_event_loop(event_loop_path):\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     policy = get_asyncio_event_loop_policy()\n\n@@ -115,3 +115,14 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n         errcode, out, err = yield self.execute([url, '-c', 'item'], check_code=False)\n         self.assertEqual(errcode, 1, out or err)\n         self.assertIn(b'DNS lookup failed', err)\n+\n+    @defer.inlineCallbacks\n+    def test_shell_fetch_async(self):\n+        reactor_path = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n+        url = self.url('/html')\n+        code = f\"fetch('{url}')\"\n+        args = [\"-c\", code, \"--set\", f\"TWISTED_REACTOR={reactor_path}\"]\n+        _, _, err = yield self.execute(args, check_code=True)\n+        self.assertNotIn(\n+            b\"RuntimeError: There is no current event loop in thread\", err\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#334f844e58edfbd791fee6697e3259dbb5a741a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 782 | Contributors (this commit): 22 | Commits (past 90d): 4 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -24,7 +24,7 @@ class ScrapyArgumentParser(argparse.ArgumentParser):\n \n \n def _iter_command_classes(module_name):\n-    # TODO: add `name` attribute to commands and and merge this function with\n+    # TODO: add `name` attribute to commands and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n     for module in walk_modules(module_name):\n         for obj in vars(module).values():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1300c1c8816c86e8ce9cff114dcefdf785138018", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 222 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -69,7 +69,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertIn('foo.NewClass', str(w[1].message))\n         self.assertIn('bar.OldClass', str(w[1].message))\n \n-    def test_subclassing_warns_only_on_direct_childs(self):\n+    def test_subclassing_warns_only_on_direct_children(self):\n         Deprecated = create_deprecated_class('Deprecated', NewName,\n                                              warn_once=False,\n                                              warn_category=MyWarning)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87fc92441f39df4eb1a43482f1475e2ba5415130", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1179 | Contributors (this commit): 25 | Commits (past 90d): 3 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -46,7 +46,7 @@ def parse_url(url, encoding=None):\n \n def escape_ajax(url):\n     \"\"\"\n-    Return the crawleable url according to:\n+    Return the crawlable url according to:\n     https://developers.google.com/webmasters/ajax-crawling/docs/getting-started\n \n     >>> escape_ajax(\"www.example.com/ajax.html#!key=value\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6ebadcd54e018dc1f90505bcdde024dba813a80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 553 | Contributors (this commit): 18 | Commits (past 90d): 5 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -190,7 +190,7 @@ class RFC2616Policy:\n         if response.status in (300, 301, 308):\n             return self.MAXAGE\n \n-        # Insufficient information to compute fresshness lifetime\n+        # Insufficient information to compute freshness lifetime\n         return 0\n \n     def _compute_current_age(self, response, request, now):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a26b6b0607026e4ea266ed3e30d2643d5990bf8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1790 | Contributors (this commit): 25 | Commits (past 90d): 7 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -425,7 +425,7 @@ class TextResponseTest(BaseResponseTest):\n     def test_bom_is_removed_from_body(self):\n         # Inferring encoding from body also cache decoded body as sideeffect,\n         # this test tries to ensure that calling response.encoding and\n-        # response.text in indistint order doesn't affect final\n+        # response.text in indistinct order doesn't affect final\n         # values for encoding and decoded body.\n         url = 'http://example.com'\n         body = b\"\\xef\\xbb\\xbfWORD\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e894db2f3f7d0873c38542744fde946a6db0d659", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1413 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -177,7 +177,7 @@ class CrawlTestCase(TestCase):\n         self.assertIs(record.exc_info[0], ZeroDivisionError)\n \n     @defer.inlineCallbacks\n-    def test_start_requests_lazyness(self):\n+    def test_start_requests_laziness(self):\n         settings = {\"CONCURRENT_REQUESTS\": 1}\n         crawler = get_crawler(BrokenStartRequestsSpider, settings)\n         yield crawler.crawl(mockserver=self.mockserver)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fff2f2db20c7a147fb2631bd6deca03a74e7e208", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 91 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-A spider that generate light requests to meassure QPS throughput\n+A spider that generate light requests to measure QPS throughput\n \n usage:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a839b61147da92ad7b175919fb31fae8293cec97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 7548 | Contributors (this commit): 54 | Commits (past 90d): 19 | Contributors (cumulative): 97 | DMM Complexity: None\n\nDIFF:\n@@ -150,7 +150,7 @@ class CrawlTestCase(TestCase):\n             raise unittest.SkipTest(\"Non-existing hosts are resolvable\")\n         crawler = get_crawler(SimpleSpider)\n         with LogCapture() as log:\n-            # try to fetch the homepage of a non-existent domain\n+            # try to fetch the homepage of a nonexistent domain\n             yield crawler.crawl(\"http://dns.resolution.invalid./\", mockserver=self.mockserver)\n         self._assert_retried(log)\n \n\n@@ -1050,8 +1050,8 @@ class BaseFTPTestCase(unittest.TestCase):\n \n         return self._add_test_callbacks(d, _test)\n \n-    def test_ftp_download_notexist(self):\n-        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/notexist.txt\",\n+    def test_ftp_download_nonexistent(self):\n+        request = Request(url=f\"ftp://127.0.0.1:{self.portNum}/nonexistent.txt\",\n                           meta=self.req_meta)\n         d = self.download_handler.download_request(request, None)\n \n\n@@ -873,7 +873,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n \n-    def test_from_response_formname_notexist(self):\n+    def test_from_response_formname_nonexistent(self):\n         response = _buildresponse(\n             \"\"\"<form name=\"form1\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n@@ -912,7 +912,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n \n-    def test_from_response_formname_notexists_fallback_formid(self):\n+    def test_from_response_formname_nonexistent_fallback_formid(self):\n         response = _buildresponse(\n             \"\"\"<form action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n@@ -927,7 +927,7 @@ class FormRequestTest(RequestTest):\n         fs = _qs(r1)\n         self.assertEqual(fs, {b'four': [b'4'], b'three': [b'3']})\n \n-    def test_from_response_formid_notexist(self):\n+    def test_from_response_formid_nonexistent(self):\n         response = _buildresponse(\n             \"\"\"<form id=\"form1\" action=\"post.php\" method=\"POST\">\n             <input type=\"hidden\" name=\"one\" value=\"1\">\n\n@@ -160,7 +160,7 @@ class UtilsPythonTestCase(unittest.TestCase):\n         b = Obj()\n         # no attributes given return False\n         self.assertFalse(equal_attributes(a, b, []))\n-        # not existent attributes\n+        # nonexistent attributes\n         self.assertFalse(equal_attributes(a, b, ['x', 'y']))\n \n         a.x = 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1a5cf00db78dc5173e75b85951f67c323b81ad96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 2987 | Contributors (this commit): 37 | Commits (past 90d): 14 | Contributors (cumulative): 71 | DMM Complexity: None\n\nDIFF:\n@@ -98,7 +98,7 @@ class ScrapyHTTPPageGetter(HTTPClient):\n # This class used to inherit from Twisted’s\n # twisted.web.client.HTTPClientFactory. When that class was deprecated in\n # Twisted (https://github.com/twisted/twisted/pull/643), we merged its\n-# non-overriden code into this class.\n+# non-overridden code into this class.\n class ScrapyHTTPClientFactory(ClientFactory):\n \n     protocol = ScrapyHTTPPageGetter\n\n@@ -147,8 +147,8 @@ class ImagesPipeline(FilesPipeline):\n         if self._deprecated_convert_image is None:\n             self._deprecated_convert_image = 'response_body' not in get_func_args(self.convert_image)\n             if self._deprecated_convert_image:\n-                warnings.warn(f'{self.__class__.__name__}.convert_image() method overriden in a deprecated way, '\n-                              'overriden method does not accept response_body argument.',\n+                warnings.warn(f'{self.__class__.__name__}.convert_image() method overridden in a deprecated way, '\n+                              'overridden method does not accept response_body argument.',\n                               category=ScrapyDeprecationWarning)\n \n         if self._deprecated_convert_image:\n\n@@ -173,8 +173,8 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                 self.assertEqual(orig_im.getcolors(), thumb_img.getcolors())\n                 self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n \n-                expected_warning_msg = ('.convert_image() method overriden in a deprecated way, '\n-                                        'overriden method does not accept response_body argument.')\n+                expected_warning_msg = ('.convert_image() method overridden in a deprecated way, '\n+                                        'overridden method does not accept response_body argument.')\n                 self.assertEqual(len([warning for warning in w if expected_warning_msg in str(warning.message)]), 1)\n \n     def test_convert_image_old(self):\n\n@@ -11,12 +11,12 @@ from tests.mockserver import MockServer\n from tests.spiders import SingleRequestSpider\n \n \n-OVERRIDEN_URL = \"https://example.org\"\n+OVERRIDDEN_URL = \"https://example.org\"\n \n \n class ProcessResponseMiddleware:\n     def process_response(self, request, response, spider):\n-        return response.replace(request=Request(OVERRIDEN_URL))\n+        return response.replace(request=Request(OVERRIDDEN_URL))\n \n \n class RaiseExceptionRequestMiddleware:\n@@ -30,7 +30,7 @@ class CatchExceptionOverrideRequestMiddleware:\n         return Response(\n             url=\"http://localhost/\",\n             body=b\"Caught \" + exception.__class__.__name__.encode(\"utf-8\"),\n-            request=Request(OVERRIDEN_URL),\n+            request=Request(OVERRIDDEN_URL),\n         )\n \n \n@@ -52,7 +52,7 @@ class AlternativeCallbacksSpider(SingleRequestSpider):\n class AlternativeCallbacksMiddleware:\n     def process_response(self, request, response, spider):\n         new_request = request.replace(\n-            url=OVERRIDEN_URL,\n+            url=OVERRIDDEN_URL,\n             callback=spider.alt_callback,\n             cb_kwargs={\"foo\": \"bar\"},\n         )\n@@ -127,13 +127,13 @@ class CrawlTestCase(TestCase):\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n \n         response = crawler.spider.meta[\"responses\"][0]\n-        self.assertEqual(response.request.url, OVERRIDEN_URL)\n+        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n \n         self.assertEqual(signal_params[\"response\"].url, url)\n-        self.assertEqual(signal_params[\"request\"].url, OVERRIDEN_URL)\n+        self.assertEqual(signal_params[\"request\"].url, OVERRIDDEN_URL)\n \n         log.check_present(\n-            (\"scrapy.core.engine\", \"DEBUG\", f\"Crawled (200) <GET {OVERRIDEN_URL}> (referer: None)\"),\n+            (\"scrapy.core.engine\", \"DEBUG\", f\"Crawled (200) <GET {OVERRIDDEN_URL}> (referer: None)\"),\n         )\n \n     @defer.inlineCallbacks\n@@ -154,7 +154,7 @@ class CrawlTestCase(TestCase):\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n         self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n-        self.assertEqual(response.request.url, OVERRIDEN_URL)\n+        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n \n     @defer.inlineCallbacks\n     def test_downloader_middleware_do_not_override_in_process_exception(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ae77fdb34d862b4ad466f7f9764132502787c47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1161 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ class ZeroDivisionErrorPipeline:\n         return item\n \n \n-class ProcessWithZeroDivisionErrorPipiline:\n+class ProcessWithZeroDivisionErrorPipeline:\n \n     def process_item(self, item, spider):\n         1 / 0\n\n@@ -109,7 +109,7 @@ class DataClassItemsSpider(TestSpider):\n class ItemZeroDivisionErrorSpider(TestSpider):\n     custom_settings = {\n         \"ITEM_PIPELINES\": {\n-            \"tests.pipelines.ProcessWithZeroDivisionErrorPipiline\": 300,\n+            \"tests.pipelines.ProcessWithZeroDivisionErrorPipeline\": 300,\n         }\n     }\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66ab82f12631e82c2c0426d68e52ac5e59344402", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2617 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -434,17 +434,17 @@ class TestRequestMetaUnsafeUrl(MixinUnsafeUrl, TestRefererMiddleware):\n     req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n \n \n-class TestRequestMetaPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n+class TestRequestMetaPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n     req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n \n \n-class TestRequestMetaPredecence002(MixinNoReferrer, TestRefererMiddleware):\n+class TestRequestMetaPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n     req_meta = {'referrer_policy': POLICY_NO_REFERRER}\n \n \n-class TestRequestMetaPredecence003(MixinUnsafeUrl, TestRefererMiddleware):\n+class TestRequestMetaPrecedence003(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n     req_meta = {'referrer_policy': POLICY_UNSAFE_URL}\n \n@@ -561,22 +561,22 @@ class TestSettingsPolicyByName(TestCase):\n             RefererMiddleware(settings)\n \n \n-class TestPolicyHeaderPredecence001(MixinUnsafeUrl, TestRefererMiddleware):\n+class TestPolicyHeaderPrecedence001(MixinUnsafeUrl, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.SameOriginPolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_UNSAFE_URL.upper()}\n \n \n-class TestPolicyHeaderPredecence002(MixinNoReferrer, TestRefererMiddleware):\n+class TestPolicyHeaderPrecedence002(MixinNoReferrer, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER.swapcase()}\n \n \n-class TestPolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+class TestPolicyHeaderPrecedence003(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n \n \n-class TestPolicyHeaderPredecence004(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+class TestPolicyHeaderPrecedence004(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n     \"\"\"\n     The empty string means \"no-referrer-when-downgrade\"\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#826e0ee6111739d154176bfdabb32cbf13f1dd0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2380 | Contributors (this commit): 28 | Commits (past 90d): 9 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -316,7 +316,7 @@ class StartprojectTemplatesTest(ProjectTest):\n         self.assertEqual(actual_permissions, expected_permissions)\n \n     def test_startproject_permissions_unchanged_in_destination(self):\n-        \"\"\"Check that pre-existing folders and files in the destination folder\n+        \"\"\"Check that preexisting folders and files in the destination folder\n         do not see their permissions modified.\"\"\"\n         scrapy_path = scrapy.__path__[0]\n         project_template = Path(scrapy_path, 'templates', 'project')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6aa5374bd3924dc98047a704569f5e1fc87559f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1463 | Contributors (this commit): 22 | Commits (past 90d): 6 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -209,7 +209,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n             self.assertEqual(converted.mode, 'RGB')\n             self.assertEqual(converted.getcolors(), [(10000, (205, 230, 255))])\n \n-            # ensure that we recieved deprecation warnings\n+            # ensure that we received deprecation warnings\n             expected_warning_msg = '.convert_image() method called in a deprecated way'\n             self.assertTrue(len([warning for warning in w if expected_warning_msg in str(warning.message)]) == 4)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d27c611cc0d2fd9786752c8996960dd23fd8e2ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2619 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -147,7 +147,7 @@ class MixinSameOrigin:\n         ('http://example.com:81/page.html', 'http://example.com/not-page.html', None),\n         ('http://example.com/page.html', 'http://example.com:81/not-page.html', None),\n \n-        # Different protocols: do NOT send refferer\n+        # Different protocols: do NOT send referrer\n         ('https://example.com/page.html', 'http://example.com/not-page.html', None),\n         ('https://example.com/page.html', 'http://not.example.com/', None),\n         ('ftps://example.com/urls.zip', 'https://example.com/not-page.html', None),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
