{"custom_id": "keras#b20fcd56c3eafb1773548a6ddf7e3060d7d0a1d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 352 | Contributors (this commit): 11 | Commits (past 90d): 16 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -124,7 +124,7 @@ def deserialize(config, custom_objects=None, **kwargs):\n         loss_scale_optimizer,\n     )\n \n-    use_legacy_optimizer = kwargs.pop(\"use_legacy_optimizer\", True)\n+    use_legacy_optimizer = kwargs.pop(\"use_legacy_optimizer\", False)\n     if len(config[\"config\"]) > 0:\n         # If the optimizer config is not empty, then we use the value of\n         # `is_legacy_optimizer` to override `use_legacy_optimizer`. If\n@@ -245,7 +245,7 @@ def get(identifier, **kwargs):\n     Raises:\n         ValueError: If `identifier` cannot be interpreted.\n     \"\"\"\n-    use_legacy_optimizer = kwargs.pop(\"use_legacy_optimizer\", True)\n+    use_legacy_optimizer = kwargs.pop(\"use_legacy_optimizer\", False)\n     if isinstance(\n         identifier,\n         (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c54caac1bd46c911899a45f610af11b63407c2f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 4375 | Contributors (this commit): 42 | Commits (past 90d): 9 | Contributors (cumulative): 42 | DMM Complexity: 0.0\n\nDIFF:\n@@ -86,6 +86,7 @@ if True:  # This gets transformed to `if sys.version_info[0] == 2:` in OSS.\n             for chunk in chunk_read(response, reporthook=reporthook):\n                 fd.write(chunk)\n \n+\n else:\n     from urllib.request import urlretrieve\n \n@@ -220,6 +221,9 @@ def get_file(\n         )\n \n     if cache_dir is None:\n+        if \"KERAS_HOME\" in os.environ:\n+            cache_dir = os.environ.get(\"KERAS_HOME\")\n+        else:\n             cache_dir = os.path.join(os.path.expanduser(\"~\"), \".keras\")\n     if md5_hash is not None and file_hash is None:\n         file_hash = md5_hash\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc2cd013436c70793b778dec0aa2901662257450", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 4376 | Contributors (this commit): 42 | Commits (past 90d): 10 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -86,7 +86,6 @@ if True:  # This gets transformed to `if sys.version_info[0] == 2:` in OSS.\n             for chunk in chunk_read(response, reporthook=reporthook):\n                 fd.write(chunk)\n \n-\n else:\n     from urllib.request import urlretrieve\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50705a10eb65ba3ae842dbe6e561d35adfb87cd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2299 | Contributors (this commit): 31 | Commits (past 90d): 6 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -107,8 +107,8 @@ class BaseWrapper:\n             else:\n                 if params_name != \"nb_epoch\":\n                     raise ValueError(\n-                        \"{} is not a legal parameter\".format(params_name)\n-                    )\n+                        f\"{params_name} is not a legal parameter\"\n+                    )  # noqa: E501\n \n     def get_params(self, **params):\n         \"\"\"Gets parameters for this estimator.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be73ac1a1e25d9abd4d793cba9707098d7adf231", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 830 | Lines Deleted: 922 | Files Changed: 185 | Hunks: 665 | Methods Changed: 428 | Complexity Δ (Sum/Max): 2/12 | Churn Δ: 1752 | Churn Cumulative: 286397 | Contributors (this commit): 466 | Commits (past 90d): 1296 | Contributors (cumulative): 2105 | DMM Complexity: 0.3944954128440367\n\nDIFF:\n@@ -89,8 +89,7 @@ def softmax(x, axis=-1):\n             output = e / s\n     else:\n         raise ValueError(\n-            \"Cannot apply softmax to a tensor that is 1D. \"\n-            f\"Received input: {x}\"\n+            f\"Cannot apply softmax to a tensor that is 1D. Received input: {x}\"\n         )\n \n     # Cache the logits to use for crossentropy loss.\n\n@@ -172,10 +172,10 @@ class ApplicationsLoadWeightTest(tf.test.TestCase, parameterized.TestCase):\n     def assertShapeEqual(self, shape1, shape2):\n         if len(shape1) != len(shape2):\n             raise AssertionError(\n-                \"Shapes are different rank: %s vs %s\" % (shape1, shape2)\n+                f\"Shapes are different rank: {shape1} vs {shape2}\"\n             )\n         if shape1 != shape2:\n-            raise AssertionError(\"Shapes differ: %s vs %s\" % (shape1, shape2))\n+            raise AssertionError(f\"Shapes differ: {shape1} vs {shape2}\")\n \n     def test_application_pretrained_weights_loading(self):\n         app_module = ARG_TO_MODEL[FLAGS.module][0]\n\n@@ -138,13 +138,11 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n     def assertShapeEqual(self, shape1, shape2):\n         if len(shape1) != len(shape2):\n             raise AssertionError(\n-                \"Shapes are different rank: %s vs %s\" % (shape1, shape2)\n+                f\"Shapes are different rank: {shape1} vs {shape2}\"\n             )\n         for v1, v2 in zip(shape1, shape2):\n             if v1 != v2:\n-                raise AssertionError(\n-                    \"Shapes differ: %s vs %s\" % (shape1, shape2)\n-                )\n+                raise AssertionError(f\"Shapes differ: {shape1} vs {shape2}\")\n \n     @parameterized.parameters(*MODEL_LIST)\n     def test_application_base(self, app, _):\n\n@@ -33,7 +33,7 @@ from keras.utils import layer_utils\n from tensorflow.python.util.tf_export import keras_export\n \n BASE_WEIGHTS_PATH = (\n-    \"https://storage.googleapis.com/tensorflow/\" \"keras-applications/densenet/\"\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/densenet/\"\n )\n DENSENET121_WEIGHT_PATH = (\n     BASE_WEIGHTS_PATH + \"densenet121_weights_tf_dim_ordering_tf_kernels.h5\"\n\n@@ -386,7 +386,7 @@ def EfficientNet(\n \n     b = 0\n     blocks = float(sum(round_repeats(args[\"repeats\"]) for args in blocks_args))\n-    for (i, args) in enumerate(blocks_args):\n+    for i, args in enumerate(blocks_args):\n         assert args[\"repeats\"] > 0\n         # Update block input and output filters based on depth multiplier.\n         args[\"filters_in\"] = round_filters(args[\"filters_in\"])\n@@ -402,8 +402,8 @@ def EfficientNet(\n                 x,\n                 activation,\n                 drop_connect_rate * b / blocks,\n-                name=\"block{}{}_\".format(i + 1, chr(j + 97)),\n-                **args\n+                name=f\"block{i + 1}{chr(j + 97)}_\",\n+                **args,\n             )\n             b += 1\n \n@@ -593,7 +593,7 @@ def EfficientNetB0(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.0,\n@@ -608,7 +608,7 @@ def EfficientNetB0(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -624,7 +624,7 @@ def EfficientNetB1(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.0,\n@@ -639,7 +639,7 @@ def EfficientNetB1(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -655,7 +655,7 @@ def EfficientNetB2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.1,\n@@ -670,7 +670,7 @@ def EfficientNetB2(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -686,7 +686,7 @@ def EfficientNetB3(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.2,\n@@ -701,7 +701,7 @@ def EfficientNetB3(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -717,7 +717,7 @@ def EfficientNetB4(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.4,\n@@ -732,7 +732,7 @@ def EfficientNetB4(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -748,7 +748,7 @@ def EfficientNetB5(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.6,\n@@ -763,7 +763,7 @@ def EfficientNetB5(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -779,7 +779,7 @@ def EfficientNetB6(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         1.8,\n@@ -794,7 +794,7 @@ def EfficientNetB6(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -810,7 +810,7 @@ def EfficientNetB7(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     return EfficientNet(\n         2.0,\n@@ -825,7 +825,7 @@ def EfficientNetB7(\n         pooling=pooling,\n         classes=classes,\n         classifier_activation=classifier_activation,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n\n@@ -999,7 +999,7 @@ def EfficientNetV2(\n     b = 0\n     blocks = float(sum(args[\"num_repeat\"] for args in blocks_args))\n \n-    for (i, args) in enumerate(blocks_args):\n+    for i, args in enumerate(blocks_args):\n         assert args[\"num_repeat\"] > 0\n \n         # Update block input and output filters based on depth multiplier.\n@@ -1032,7 +1032,7 @@ def EfficientNetV2(\n                 activation=activation,\n                 bn_momentum=bn_momentum,\n                 survival_probability=drop_connect_rate * b / blocks,\n-                name=\"block{}{}_\".format(i + 1, chr(j + 97)),\n+                name=f\"block{i + 1}{chr(j + 97)}_\",\n                 **args,\n             )(x)\n             b += 1\n\n@@ -76,7 +76,7 @@ def write_ckpt_to_h5(path_h5, path_ckpt, keras_model, use_ema=True):\n             tf_weight_names,\n             model_name_tf,\n         )\n-        io_utils.print_msg(\"{} and {} match.\".format(tf_block, keras_block))\n+        io_utils.print_msg(f\"{tf_block} and {keras_block} match.\")\n \n     block_mapping = {x[0]: x[1] for x in zip(keras_blocks, tf_blocks)}\n \n@@ -106,7 +106,7 @@ def write_ckpt_to_h5(path_h5, path_ckpt, keras_model, use_ema=True):\n             )\n             continue\n         else:\n-            raise ValueError(\"{} failed to parse.\".format(w.name))\n+            raise ValueError(f\"{w.name} failed to parse.\")\n \n         try:\n             w_tf = tf.train.load_variable(path_ckpt, tf_name)\n@@ -121,9 +121,7 @@ def write_ckpt_to_h5(path_h5, path_ckpt, keras_model, use_ema=True):\n                     stacklevel=2,\n                 )\n             else:\n-                raise ValueError(\n-                    \"Fail to load {} from {}\".format(w.name, tf_name)\n-                )\n+                raise ValueError(f\"Fail to load {w.name} from {tf_name}\")\n \n     total_weights = len(keras_model.weights)\n     io_utils.print_msg(f\"{changed_weights}/{total_weights} weights updated\")\n@@ -212,18 +210,18 @@ def keras_name_to_tf_name_stem_top(\n         tf_name = \"{}/stem/tpu_batch_normalization/{}{}\".format(\n             model_name_tf, bn_weights, ema\n         )\n-        stem_top_dict[\"stem_bn/{}:0\".format(bn_weights)] = tf_name\n+        stem_top_dict[f\"stem_bn/{bn_weights}:0\"] = tf_name\n \n     # top / head batch normalization\n     for bn_weights in [\"beta\", \"gamma\", \"moving_mean\", \"moving_variance\"]:\n         tf_name = \"{}/head/tpu_batch_normalization/{}{}\".format(\n             model_name_tf, bn_weights, ema\n         )\n-        stem_top_dict[\"top_bn/{}:0\".format(bn_weights)] = tf_name\n+        stem_top_dict[f\"top_bn/{bn_weights}:0\"] = tf_name\n \n     if keras_name in stem_top_dict:\n         return stem_top_dict[keras_name]\n-    raise KeyError(\"{} from h5 file cannot be parsed\".format(keras_name))\n+    raise KeyError(f\"{keras_name} from h5 file cannot be parsed\")\n \n \n def keras_name_to_tf_name_block(\n@@ -253,9 +251,7 @@ def keras_name_to_tf_name_block(\n     \"\"\"\n \n     if keras_block not in keras_name:\n-        raise ValueError(\n-            \"block name {} not found in {}\".format(keras_block, keras_name)\n-        )\n+        raise ValueError(f\"block name {keras_block} not found in {keras_name}\")\n \n     # all blocks in the first group will not have expand conv and bn\n     is_first_blocks = keras_block[5] == \"1\"\n\n@@ -52,7 +52,7 @@ def InceptionResNetV2(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Instantiates the Inception-ResNet v2 architecture.\n \n@@ -122,7 +122,7 @@ def InceptionResNetV2(\n     else:\n         layers = VersionAwareLayers()\n     if kwargs:\n-        raise ValueError(\"Unknown argument(s): %s\" % (kwargs,))\n+        raise ValueError(f\"Unknown argument(s): {kwargs}\")\n     if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n\n@@ -74,7 +74,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n BASE_WEIGHT_PATH = (\n-    \"https://storage.googleapis.com/tensorflow/\" \"keras-applications/mobilenet/\"\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/\"\n )\n layers = None\n \n@@ -302,7 +302,7 @@ def MobileNet(\n         inputs = img_input\n \n     # Create model.\n-    model = training.Model(inputs, x, name=\"mobilenet_%0.2f_%s\" % (alpha, rows))\n+    model = training.Model(inputs, x, name=f\"mobilenet_{alpha:0.2f}_{rows}\")\n \n     # Load weights.\n     if weights == \"imagenet\":\n\n@@ -88,8 +88,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n BASE_WEIGHT_PATH = (\n-    \"https://storage.googleapis.com/tensorflow/\"\n-    \"keras-applications/mobilenet_v2/\"\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/\"\n )\n layers = None\n \n@@ -450,9 +449,7 @@ def MobileNetV2(\n         inputs = img_input\n \n     # Create model.\n-    model = training.Model(\n-        inputs, x, name=\"mobilenetv2_%0.2f_%s\" % (alpha, rows)\n-    )\n+    model = training.Model(inputs, x, name=f\"mobilenetv2_{alpha:0.2f}_{rows}\")\n \n     # Load weights.\n     if weights == \"imagenet\":\n@@ -498,7 +495,7 @@ def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n     # 8.\n     pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n     x = inputs\n-    prefix = \"block_{}_\".format(block_id)\n+    prefix = f\"block_{block_id}_\"\n \n     if block_id:\n         # Expand with a pointwise 1x1 convolution.\n\n@@ -31,8 +31,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n # TODO(scottzhu): Change this to the GCS path.\n BASE_WEIGHT_PATH = (\n-    \"https://storage.googleapis.com/tensorflow/\"\n-    \"keras-applications/mobilenet_v3/\"\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/\"\n )\n WEIGHTS_HASHES = {\n     \"large_224_0.75_float\": (\n@@ -611,7 +610,7 @@ def _inverted_res_block(\n     infilters = backend.int_shape(x)[channel_axis]\n     if block_id:\n         # Expand\n-        prefix = \"expanded_conv_{}/\".format(block_id)\n+        prefix = f\"expanded_conv_{block_id}/\"\n         x = layers.Conv2D(\n             _depth(infilters * expansion),\n             kernel_size=1,\n\n@@ -51,7 +51,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n BASE_WEIGHTS_PATH = (\n-    \"https://storage.googleapis.com/tensorflow/\" \"keras-applications/nasnet/\"\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/nasnet/\"\n )\n NASNET_MOBILE_WEIGHT_PATH = BASE_WEIGHTS_PATH + \"NASNet-mobile.h5\"\n NASNET_MOBILE_WEIGHT_PATH_NO_TOP = BASE_WEIGHTS_PATH + \"NASNet-mobile-no-top.h5\"\n@@ -546,12 +546,12 @@ def _separable_conv_block(\n     \"\"\"\n     channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n \n-    with backend.name_scope(\"separable_conv_block_%s\" % block_id):\n+    with backend.name_scope(f\"separable_conv_block_{block_id}\"):\n         x = layers.Activation(\"relu\")(ip)\n         if strides == (2, 2):\n             x = layers.ZeroPadding2D(\n                 padding=imagenet_utils.correct_pad(x, kernel_size),\n-                name=\"separable_conv_1_pad_%s\" % block_id,\n+                name=f\"separable_conv_1_pad_{block_id}\",\n             )(x)\n             conv_pad = \"valid\"\n         else:\n@@ -560,7 +560,7 @@ def _separable_conv_block(\n             filters,\n             kernel_size,\n             strides=strides,\n-            name=\"separable_conv_1_%s\" % block_id,\n+            name=f\"separable_conv_1_{block_id}\",\n             padding=conv_pad,\n             use_bias=False,\n             kernel_initializer=\"he_normal\",\n@@ -569,13 +569,13 @@ def _separable_conv_block(\n             axis=channel_dim,\n             momentum=0.9997,\n             epsilon=1e-3,\n-            name=\"separable_conv_1_bn_%s\" % (block_id),\n+            name=f\"separable_conv_1_bn_{block_id}\",\n         )(x)\n         x = layers.Activation(\"relu\")(x)\n         x = layers.SeparableConv2D(\n             filters,\n             kernel_size,\n-            name=\"separable_conv_2_%s\" % block_id,\n+            name=f\"separable_conv_2_{block_id}\",\n             padding=\"same\",\n             use_bias=False,\n             kernel_initializer=\"he_normal\",\n@@ -584,7 +584,7 @@ def _separable_conv_block(\n             axis=channel_dim,\n             momentum=0.9997,\n             epsilon=1e-3,\n-            name=\"separable_conv_2_bn_%s\" % (block_id),\n+            name=f\"separable_conv_2_bn_{block_id}\",\n         )(x)\n     return x\n \n@@ -616,22 +616,22 @@ def _adjust_block(p, ip, filters, block_id=None):\n             p = ip\n \n         elif p_shape[img_dim] != ip_shape[img_dim]:\n-            with backend.name_scope(\"adjust_reduction_block_%s\" % block_id):\n-                p = layers.Activation(\n-                    \"relu\", name=\"adjust_relu_1_%s\" % block_id\n-                )(p)\n+            with backend.name_scope(f\"adjust_reduction_block_{block_id}\"):\n+                p = layers.Activation(\"relu\", name=f\"adjust_relu_1_{block_id}\")(\n+                    p\n+                )\n                 p1 = layers.AveragePooling2D(\n                     (1, 1),\n                     strides=(2, 2),\n                     padding=\"valid\",\n-                    name=\"adjust_avg_pool_1_%s\" % block_id,\n+                    name=f\"adjust_avg_pool_1_{block_id}\",\n                 )(p)\n                 p1 = layers.Conv2D(\n                     filters // 2,\n                     (1, 1),\n                     padding=\"same\",\n                     use_bias=False,\n-                    name=\"adjust_conv_1_%s\" % block_id,\n+                    name=f\"adjust_conv_1_{block_id}\",\n                     kernel_initializer=\"he_normal\",\n                 )(p1)\n \n@@ -641,14 +641,14 @@ def _adjust_block(p, ip, filters, block_id=None):\n                     (1, 1),\n                     strides=(2, 2),\n                     padding=\"valid\",\n-                    name=\"adjust_avg_pool_2_%s\" % block_id,\n+                    name=f\"adjust_avg_pool_2_{block_id}\",\n                 )(p2)\n                 p2 = layers.Conv2D(\n                     filters // 2,\n                     (1, 1),\n                     padding=\"same\",\n                     use_bias=False,\n-                    name=\"adjust_conv_2_%s\" % block_id,\n+                    name=f\"adjust_conv_2_{block_id}\",\n                     kernel_initializer=\"he_normal\",\n                 )(p2)\n \n@@ -657,18 +657,18 @@ def _adjust_block(p, ip, filters, block_id=None):\n                     axis=channel_dim,\n                     momentum=0.9997,\n                     epsilon=1e-3,\n-                    name=\"adjust_bn_%s\" % block_id,\n+                    name=f\"adjust_bn_{block_id}\",\n                 )(p)\n \n         elif p_shape[channel_dim] != filters:\n-            with backend.name_scope(\"adjust_projection_block_%s\" % block_id):\n+            with backend.name_scope(f\"adjust_projection_block_{block_id}\"):\n                 p = layers.Activation(\"relu\")(p)\n                 p = layers.Conv2D(\n                     filters,\n                     (1, 1),\n                     strides=(1, 1),\n                     padding=\"same\",\n-                    name=\"adjust_conv_projection_%s\" % block_id,\n+                    name=f\"adjust_conv_projection_{block_id}\",\n                     use_bias=False,\n                     kernel_initializer=\"he_normal\",\n                 )(p)\n@@ -676,7 +676,7 @@ def _adjust_block(p, ip, filters, block_id=None):\n                     axis=channel_dim,\n                     momentum=0.9997,\n                     epsilon=1e-3,\n-                    name=\"adjust_bn_%s\" % block_id,\n+                    name=f\"adjust_bn_{block_id}\",\n                 )(p)\n     return p\n \n@@ -695,7 +695,7 @@ def _normal_a_cell(ip, p, filters, block_id=None):\n     \"\"\"\n     channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n \n-    with backend.name_scope(\"normal_A_block_%s\" % block_id):\n+    with backend.name_scope(f\"normal_A_block_{block_id}\"):\n         p = _adjust_block(p, ip, filters, block_id)\n \n         h = layers.Activation(\"relu\")(ip)\n@@ -704,7 +704,7 @@ def _normal_a_cell(ip, p, filters, block_id=None):\n             (1, 1),\n             strides=(1, 1),\n             padding=\"same\",\n-            name=\"normal_conv_1_%s\" % block_id,\n+            name=f\"normal_conv_1_{block_id}\",\n             use_bias=False,\n             kernel_initializer=\"he_normal\",\n         )(h)\n@@ -712,7 +712,7 @@ def _normal_a_cell(ip, p, filters, block_id=None):\n             axis=channel_dim,\n             momentum=0.9997,\n             epsilon=1e-3,\n-            name=\"normal_bn_1_%s\" % block_id,\n+            name=f\"normal_bn_1_{block_id}\",\n         )(h)\n \n         with backend.name_scope(\"block_1\"):\n@@ -720,56 +720,56 @@ def _normal_a_cell(ip, p, filters, block_id=None):\n                 h,\n                 filters,\n                 kernel_size=(5, 5),\n-                block_id=\"normal_left1_%s\" % block_id,\n+                block_id=f\"normal_left1_{block_id}\",\n             )\n             x1_2 = _separable_conv_block(\n-                p, filters, block_id=\"normal_right1_%s\" % block_id\n+                p, filters, block_id=f\"normal_right1_{block_id}\"\n             )\n-            x1 = layers.add([x1_1, x1_2], name=\"normal_add_1_%s\" % block_id)\n+            x1 = layers.add([x1_1, x1_2], name=f\"normal_add_1_{block_id}\")\n \n         with backend.name_scope(\"block_2\"):\n             x2_1 = _separable_conv_block(\n-                p, filters, (5, 5), block_id=\"normal_left2_%s\" % block_id\n+                p, filters, (5, 5), block_id=f\"normal_left2_{block_id}\"\n             )\n             x2_2 = _separable_conv_block(\n-                p, filters, (3, 3), block_id=\"normal_right2_%s\" % block_id\n+                p, filters, (3, 3), block_id=f\"normal_right2_{block_id}\"\n             )\n-            x2 = layers.add([x2_1, x2_2], name=\"normal_add_2_%s\" % block_id)\n+            x2 = layers.add([x2_1, x2_2], name=f\"normal_add_2_{block_id}\")\n \n         with backend.name_scope(\"block_3\"):\n             x3 = layers.AveragePooling2D(\n                 (3, 3),\n                 strides=(1, 1),\n                 padding=\"same\",\n-                name=\"normal_left3_%s\" % (block_id),\n+                name=f\"normal_left3_{block_id}\",\n             )(h)\n-            x3 = layers.add([x3, p], name=\"normal_add_3_%s\" % block_id)\n+            x3 = layers.add([x3, p], name=f\"normal_add_3_{block_id}\")\n \n         with backend.name_scope(\"block_4\"):\n             x4_1 = layers.AveragePooling2D(\n                 (3, 3),\n                 strides=(1, 1),\n                 padding=\"same\",\n-                name=\"normal_left4_%s\" % (block_id),\n+                name=f\"normal_left4_{block_id}\",\n             )(p)\n             x4_2 = layers.AveragePooling2D(\n                 (3, 3),\n                 strides=(1, 1),\n                 padding=\"same\",\n-                name=\"normal_right4_%s\" % (block_id),\n+                name=f\"normal_right4_{block_id}\",\n             )(p)\n-            x4 = layers.add([x4_1, x4_2], name=\"normal_add_4_%s\" % block_id)\n+            x4 = layers.add([x4_1, x4_2], name=f\"normal_add_4_{block_id}\")\n \n         with backend.name_scope(\"block_5\"):\n             x5 = _separable_conv_block(\n-                h, filters, block_id=\"normal_left5_%s\" % block_id\n+                h, filters, block_id=f\"normal_left5_{block_id}\"\n             )\n-            x5 = layers.add([x5, h], name=\"normal_add_5_%s\" % block_id)\n+            x5 = layers.add([x5, h], name=f\"normal_add_5_{block_id}\")\n \n         x = layers.concatenate(\n             [p, x1, x2, x3, x4, x5],\n             axis=channel_dim,\n-            name=\"normal_concat_%s\" % block_id,\n+            name=f\"normal_concat_{block_id}\",\n         )\n     return x, ip\n \n@@ -788,7 +788,7 @@ def _reduction_a_cell(ip, p, filters, block_id=None):\n     \"\"\"\n     channel_dim = 1 if backend.image_data_format() == \"channels_first\" else -1\n \n-    with backend.name_scope(\"reduction_A_block_%s\" % block_id):\n+    with backend.name_scope(f\"reduction_A_block_{block_id}\"):\n         p = _adjust_block(p, ip, filters, block_id)\n \n         h = layers.Activation(\"relu\")(ip)\n@@ -797,7 +797,7 @@ def _reduction_a_cell(ip, p, filters, block_id=None):\n             (1, 1),\n             strides=(1, 1),\n             padding=\"same\",\n-            name=\"reduction_conv_1_%s\" % block_id,\n+            name=f\"reduction_conv_1_{block_id}\",\n             use_bias=False,\n             kernel_initializer=\"he_normal\",\n         )(h)\n@@ -805,11 +805,11 @@ def _reduction_a_cell(ip, p, filters, block_id=None):\n             axis=channel_dim,\n             momentum=0.9997,\n             epsilon=1e-3,\n-            name=\"reduction_bn_1_%s\" % block_id,\n+            name=f\"reduction_bn_1_{block_id}\",\n         )(h)\n         h3 = layers.ZeroPadding2D(\n             padding=imagenet_utils.correct_pad(h, 3),\n-            name=\"reduction_pad_1_%s\" % block_id,\n+            name=f\"reduction_pad_1_{block_id}\",\n         )(h)\n \n         with backend.name_scope(\"block_1\"):\n@@ -818,74 +818,74 @@ def _reduction_a_cell(ip, p, filters, block_id=None):\n                 filters,\n                 (5, 5),\n                 strides=(2, 2),\n-                block_id=\"reduction_left1_%s\" % block_id,\n+                block_id=f\"reduction_left1_{block_id}\",\n             )\n             x1_2 = _separable_conv_block(\n                 p,\n                 filters,\n                 (7, 7),\n                 strides=(2, 2),\n-                block_id=\"reduction_right1_%s\" % block_id,\n+                block_id=f\"reduction_right1_{block_id}\",\n             )\n-            x1 = layers.add([x1_1, x1_2], name=\"reduction_add_1_%s\" % block_id)\n+            x1 = layers.add([x1_1, x1_2], name=f\"reduction_add_1_{block_id}\")\n \n         with backend.name_scope(\"block_2\"):\n             x2_1 = layers.MaxPooling2D(\n                 (3, 3),\n                 strides=(2, 2),\n                 padding=\"valid\",\n-                name=\"reduction_left2_%s\" % block_id,\n+                name=f\"reduction_left2_{block_id}\",\n             )(h3)\n             x2_2 = _separable_conv_block(\n                 p,\n                 filters,\n                 (7, 7),\n                 strides=(2, 2),\n-                block_id=\"reduction_right2_%s\" % block_id,\n+                block_id=f\"reduction_right2_{block_id}\",\n             )\n-            x2 = layers.add([x2_1, x2_2], name=\"reduction_add_2_%s\" % block_id)\n+            x2 = layers.add([x2_1, x2_2], name=f\"reduction_add_2_{block_id}\")\n \n         with backend.name_scope(\"block_3\"):\n             x3_1 = layers.AveragePooling2D(\n                 (3, 3),\n                 strides=(2, 2),\n                 padding=\"valid\",\n-                name=\"reduction_left3_%s\" % block_id,\n+                name=f\"reduction_left3_{block_id}\",\n             )(h3)\n             x3_2 = _separable_conv_block(\n                 p,\n                 filters,\n                 (5, 5),\n                 strides=(2, 2),\n-                block_id=\"reduction_right3_%s\" % block_id,\n+                block_id=f\"reduction_right3_{block_id}\",\n             )\n-            x3 = layers.add([x3_1, x3_2], name=\"reduction_add3_%s\" % block_id)\n+            x3 = layers.add([x3_1, x3_2], name=f\"reduction_add3_{block_id}\")\n \n         with backend.name_scope(\"block_4\"):\n             x4 = layers.AveragePooling2D(\n                 (3, 3),\n                 strides=(1, 1),\n                 padding=\"same\",\n-                name=\"reduction_left4_%s\" % block_id,\n+                name=f\"reduction_left4_{block_id}\",\n             )(x1)\n             x4 = layers.add([x2, x4])\n \n         with backend.name_scope(\"block_5\"):\n             x5_1 = _separable_conv_block(\n-                x1, filters, (3, 3), block_id=\"reduction_left4_%s\" % block_id\n+                x1, filters, (3, 3), block_id=f\"reduction_left4_{block_id}\"\n             )\n             x5_2 = layers.MaxPooling2D(\n                 (3, 3),\n                 strides=(2, 2),\n                 padding=\"valid\",\n-                name=\"reduction_right5_%s\" % block_id,\n+                name=f\"reduction_right5_{block_id}\",\n             )(h3)\n-            x5 = layers.add([x5_1, x5_2], name=\"reduction_add4_%s\" % block_id)\n+            x5 = layers.add([x5_1, x5_2], name=f\"reduction_add4_{block_id}\")\n \n         x = layers.concatenate(\n             [x2, x3, x4, x5],\n             axis=channel_dim,\n-            name=\"reduction_concat_%s\" % block_id,\n+            name=f\"reduction_concat_{block_id}\",\n         )\n         return x, ip\n \n\n@@ -826,7 +826,7 @@ def Stage(block_type, depth, group_width, filters_in, filters_out, name=None):\n         else:\n             raise NotImplementedError(\n                 f\"Block type `{block_type}` not recognized.\"\n-                f\"block_type must be one of (`X`, `Y`, `Z`). \"\n+                \"block_type must be one of (`X`, `Y`, `Z`). \"\n             )\n         return x\n \n\n@@ -85,7 +85,7 @@ def ResNet(\n     pooling=None,\n     classes=1000,\n     classifier_activation=\"softmax\",\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture.\n \n@@ -140,7 +140,7 @@ def ResNet(\n     else:\n         layers = VersionAwareLayers()\n     if kwargs:\n-        raise ValueError(\"Unknown argument(s): %s\" % (kwargs,))\n+        raise ValueError(f\"Unknown argument(s): {kwargs}\")\n     if not (weights in {\"imagenet\", None} or tf.io.gfile.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n@@ -508,7 +508,7 @@ def ResNet50(\n     input_shape=None,\n     pooling=None,\n     classes=1000,\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Instantiates the ResNet50 architecture.\"\"\"\n \n@@ -529,7 +529,7 @@ def ResNet50(\n         input_shape,\n         pooling,\n         classes,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -543,7 +543,7 @@ def ResNet101(\n     input_shape=None,\n     pooling=None,\n     classes=1000,\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Instantiates the ResNet101 architecture.\"\"\"\n \n@@ -564,7 +564,7 @@ def ResNet101(\n         input_shape,\n         pooling,\n         classes,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n@@ -578,7 +578,7 @@ def ResNet152(\n     input_shape=None,\n     pooling=None,\n     classes=1000,\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Instantiates the ResNet152 architecture.\"\"\"\n \n@@ -599,7 +599,7 @@ def ResNet152(\n         input_shape,\n         pooling,\n         classes,\n-        **kwargs\n+        **kwargs,\n     )\n \n \n\n@@ -39,7 +39,7 @@ from keras.utils import layer_utils\n from tensorflow.python.util.tf_export import keras_export\n \n BASE_WEIGHTS_URL = (\n-    \"https://storage.googleapis.com/tensorflow/\" \"keras-applications/resnet_rs/\"\n+    \"https://storage.googleapis.com/tensorflow/keras-applications/resnet_rs/\"\n )\n \n WEIGHT_HASHES = {\n@@ -619,9 +619,9 @@ def ResNetRS(\n \n     if weights in weights_allow_list and include_top and classes != 1000:\n         raise ValueError(\n-            f\"If using `weights` as `'imagenet'` or any \"\n+            \"If using `weights` as `'imagenet'` or any \"\n             f\"of {weights_allow_list} \"\n-            f\"with `include_top` as true, `classes` should be 1000. \"\n+            \"with `include_top` as true, `classes` should be 1000. \"\n             f\"Received classes={classes}\"\n         )\n \n\n@@ -689,7 +689,7 @@ def _current_graph(op_input_list, graph=None):\n \n     op_input_list = tuple(op_input_list)  # Handle generators correctly\n     if graph and not isinstance(graph, tf.Graph):\n-        raise TypeError(\"Input graph needs to be a Graph: %s\" % (graph,))\n+        raise TypeError(f\"Input graph needs to be a Graph: {graph}\")\n \n     # 1. We validate that all of the inputs are from the same graph. This is\n     #    either the supplied graph parameter, or the first one selected from one\n@@ -718,7 +718,7 @@ def _current_graph(op_input_list, graph=None):\n                 _assert_same_graph(original_graph_element, graph_element)\n             elif graph_element.graph is not graph:\n                 raise ValueError(\n-                    \"%s is not from the passed-in graph.\" % graph_element\n+                    f\"{graph_element} is not from the passed-in graph.\"\n                 )\n \n     # 2. If all else fails, we use the default graph, which is always there.\n@@ -2576,8 +2576,8 @@ def batch_dot(x, y, axes=None):\n             + str(y_shape)\n             + \" with axes=\"\n             + str(axes)\n-            + \". x.shape[%d] != \"\n-            \"y.shape[%d] (%d != %d).\" % (axes[0], axes[1], d1, d2)\n+            + \". x.shape[%d] != y.shape[%d] (%d != %d).\"\n+            % (axes[0], axes[1], d1, d2)\n         )\n \n     # backup ndims. Need them later.\n@@ -3661,7 +3661,7 @@ def resize_images(\n     elif data_format == \"channels_last\":\n         rows, cols = 1, 2\n     else:\n-        raise ValueError(\"Invalid `data_format` argument: %s\" % (data_format,))\n+        raise ValueError(f\"Invalid `data_format` argument: {data_format}\")\n \n     new_shape = x.shape[rows : cols + 1]\n     if new_shape.is_fully_defined():\n@@ -4446,8 +4446,8 @@ class GraphExecutionFunction:\n \n         if session_kwargs:\n             raise ValueError(\n-                \"Some keys in session_kwargs are not supported at this \"\n-                \"time: %s\" % (session_kwargs.keys(),)\n+                \"Some keys in session_kwargs are not supported at this time: %s\"\n+                % (session_kwargs.keys(),)\n             )\n \n         self._callable_fn = None\n@@ -4640,8 +4640,8 @@ def function(inputs, outputs, updates=None, name=None, **kwargs):\n             ] and key not in [\"inputs\", \"outputs\", \"updates\", \"name\"]:\n                 msg = (\n                     'Invalid argument \"%s\" passed to K.function with '\n-                    \"TensorFlow backend\"\n-                ) % key\n+                    \"TensorFlow backend\" % key\n+                )\n                 raise ValueError(msg)\n     return GraphExecutionFunction(\n         inputs, outputs, updates=updates, name=name, **kwargs\n@@ -4809,11 +4809,11 @@ def rnn(\n     def _expand_mask(mask_t, input_t, fixed_dim=1):\n         if tf.nest.is_nested(mask_t):\n             raise ValueError(\n-                \"mask_t is expected to be tensor, but got %s\" % mask_t\n+                f\"mask_t is expected to be tensor, but got {mask_t}\"\n             )\n         if tf.nest.is_nested(input_t):\n             raise ValueError(\n-                \"input_t is expected to be tensor, but got %s\" % input_t\n+                f\"input_t is expected to be tensor, but got {input_t}\"\n             )\n         rank_diff = len(input_t.shape) - len(mask_t.shape)\n         for _ in range(rank_diff):\n@@ -4931,7 +4931,7 @@ def rnn(\n             tf.TensorArray(\n                 dtype=inp.dtype,\n                 size=time_steps_t,\n-                tensor_array_name=\"input_ta_%s\" % i,\n+                tensor_array_name=f\"input_ta_{i}\",\n             )\n             for i, inp in enumerate(flatted_inputs)\n         )\n@@ -4960,7 +4960,7 @@ def rnn(\n                 dtype=out.dtype,\n                 size=output_ta_size,\n                 element_shape=out.shape,\n-                tensor_array_name=\"output_ta_%s\" % i,\n+                tensor_array_name=f\"output_ta_{i}\",\n             )\n             for i, out in enumerate(tf.nest.flatten(output_time_zero))\n         )\n@@ -5224,8 +5224,8 @@ def switch(condition, then_expression, else_expression):\n                 \" equal to rank of `then_expression` and \"\n                 \"`else_expression`. ndim(condition)=\"\n                 + str(cond_ndim)\n-                + \", ndim(then_expression)\"\n-                \"=\" + str(expr_ndim)\n+                + \", ndim(then_expression)=\"\n+                + str(expr_ndim)\n             )\n         if cond_ndim > 1:\n             ndim_diff = expr_ndim - cond_ndim\n\n@@ -67,8 +67,12 @@ def compare_single_input_op_to_numpy(\n         np.testing.assert_allclose(keras_output, np_output, atol=1e-4)\n     except AssertionError:\n         raise AssertionError(\n-            \"Test for op `\" + str(keras_op.__name__) + \"` failed; \"\n-            \"Expected \" + str(np_output) + \" but got \" + str(keras_output)\n+            \"Test for op `\"\n+            + str(keras_op.__name__)\n+            + \"` failed; Expected \"\n+            + str(np_output)\n+            + \" but got \"\n+            + str(keras_output)\n         )\n \n \n@@ -93,7 +97,7 @@ def compare_two_inputs_op_to_numpy(\n         backend.variable(input_a, dtype=dtype),\n         backend.variable(input_b, dtype=dtype),\n         *keras_args,\n-        **keras_kwargs\n+        **keras_kwargs,\n     )\n     keras_output = backend.eval(keras_output)\n     np_output = np_op(\n@@ -103,8 +107,12 @@ def compare_two_inputs_op_to_numpy(\n         np.testing.assert_allclose(keras_output, np_output, atol=1e-4)\n     except AssertionError:\n         raise AssertionError(\n-            \"Test for op `\" + str(keras_op.__name__) + \"` failed; \"\n-            \"Expected \" + str(np_output) + \" but got \" + str(keras_output)\n+            \"Test for op `\"\n+            + str(keras_op.__name__)\n+            + \"` failed; Expected \"\n+            + str(np_output)\n+            + \" but got \"\n+            + str(keras_output)\n         )\n \n \n@@ -295,7 +303,7 @@ class BackendUtilsTest(tf.test.TestCase):\n         # we cannot test correctness.\n         # The message gets correctly printed in practice.\n         x = backend.placeholder(shape=())\n-        y = backend.print_tensor(x, \"eager=%s\" % tf.executing_eagerly())\n+        y = backend.print_tensor(x, f\"eager={tf.executing_eagerly()}\")\n         f = backend.function(x, y)\n         f(0)\n \n@@ -1445,7 +1453,7 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n             state_list[i].append(backend.eval(new_states[0]))\n \n             def assert_list_pairwise(z_list, atol=1e-05):\n-                for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n+                for z1, z2 in zip(z_list[1:], z_list[:-1]):\n                     self.assertAllClose(z1, z2, atol=atol)\n \n             assert_list_pairwise(last_output_list[0], atol=1e-04)\n@@ -1557,7 +1565,7 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n             additional_state_list[i].append(backend.eval(new_states[1]))\n \n             def assert_list_pairwise(z_list, atol=1e-05):\n-                for (z1, z2) in zip(z_list[1:], z_list[:-1]):\n+                for z1, z2 in zip(z_list[1:], z_list[:-1]):\n                     self.assertAllClose(z1, z2, atol=atol)\n \n             assert_list_pairwise(last_output_list[0], atol=1e-04)\n\n@@ -128,8 +128,7 @@ def get_distribution_strategy(\n             return tf.distribute.OneDeviceStrategy(\"device:CPU:0\")\n         if num_gpus > 1:\n             raise ValueError(\n-                \"`OneDeviceStrategy` can not be used for more than \"\n-                \"one device.\"\n+                \"`OneDeviceStrategy` can not be used for more than one device.\"\n             )\n         return tf.distribute.OneDeviceStrategy(\"device:GPU:0\")\n \n\n@@ -248,12 +248,12 @@ class CustomMnistBenchmark(tf.test.Benchmark):\n \n         if not isinstance(loss_fn, tf.keras.losses.Loss):\n             raise ValueError(\n-                \"`tf.keras.losses.Loss` instance \" \"for loss_fn is required.\"\n+                \"`tf.keras.losses.Loss` instance for loss_fn is required.\"\n             )\n \n         if not isinstance(optimizer, tf.keras.optimizers.Optimizer):\n             raise ValueError(\n-                \"`tf.keras.optimizers` instance \" \"for optimizer is required.\"\n+                \"`tf.keras.optimizers` instance for optimizer is required.\"\n             )\n \n         avg_epoch_time_list, train_step_time_list = [], []\n\n@@ -327,7 +327,7 @@ class CallbackList:\n \n     def _call_batch_begin_hook(self, mode, batch, logs):\n         \"\"\"Helper function for `on_*_batch_begin` methods.\"\"\"\n-        hook_name = \"on_{mode}_batch_begin\".format(mode=mode)\n+        hook_name = f\"on_{mode}_batch_begin\"\n         self._call_batch_hook_helper(hook_name, batch, logs)\n \n         if self._check_timing:\n@@ -335,7 +335,7 @@ class CallbackList:\n \n     def _call_batch_end_hook(self, mode, batch, logs):\n         \"\"\"Helper function for `on_*_batch_end` methods.\"\"\"\n-        hook_name = \"on_{mode}_batch_end\".format(mode=mode)\n+        hook_name = f\"on_{mode}_batch_end\"\n \n         if self._check_timing and batch >= 1:\n             batch_time = time.time() - self._batch_start_time\n@@ -345,7 +345,7 @@ class CallbackList:\n \n         if len(self._batch_times) >= self._num_batches_for_timing_check:\n             end_hook_name = hook_name\n-            begin_hook_name = \"on_{mode}_batch_begin\".format(mode=mode)\n+            begin_hook_name = f\"on_{mode}_batch_begin\"\n             avg_batch_time = sum(self._batch_times) / len(self._batch_times)\n             avg_end_hook_time = sum(self._hook_times[end_hook_name]) / len(\n                 self._hook_times[end_hook_name]\n@@ -1361,7 +1361,7 @@ class ModelCheckpoint(Callback):\n             else:\n                 raise TypeError(\n                     \"If save_weights_only is True, then `options` must be \"\n-                    f\"either None or a tf.train.CheckpointOptions. \"\n+                    \"either None or a tf.train.CheckpointOptions. \"\n                     f\"Got {options}.\"\n                 )\n         else:\n@@ -1372,7 +1372,7 @@ class ModelCheckpoint(Callback):\n             else:\n                 raise TypeError(\n                     \"If save_weights_only is False, then `options` must be \"\n-                    f\"either None or a tf.saved_model.SaveOptions. \"\n+                    \"either None or a tf.saved_model.SaveOptions. \"\n                     f\"Got {options}.\"\n                 )\n \n@@ -1402,7 +1402,7 @@ class ModelCheckpoint(Callback):\n \n         if mode not in [\"auto\", \"min\", \"max\"]:\n             logging.warning(\n-                \"ModelCheckpoint mode %s is unknown, \" \"fallback to auto mode.\",\n+                \"ModelCheckpoint mode %s is unknown, fallback to auto mode.\",\n                 mode,\n             )\n             mode = \"auto\"\n@@ -1984,7 +1984,7 @@ class EarlyStopping(Callback):\n \n         if mode not in [\"auto\", \"min\", \"max\"]:\n             logging.warning(\n-                \"EarlyStopping mode %s is unknown, \" \"fallback to auto mode.\",\n+                \"EarlyStopping mode %s is unknown, fallback to auto mode.\",\n                 mode,\n             )\n             mode = \"auto\"\n@@ -2138,8 +2138,8 @@ class RemoteMonitor(Callback):\n                 )\n         except requests.exceptions.RequestException:\n             logging.warning(\n-                \"Warning: could not reach RemoteMonitor \"\n-                \"root server at \" + str(self.root)\n+                \"Warning: could not reach RemoteMonitor root server at \"\n+                + str(self.root)\n             )\n \n \n@@ -2857,7 +2857,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         embeddings_ckpt = os.path.join(\n             self._log_write_dir,\n             \"train\",\n-            \"keras_embedding.ckpt-{}\".format(epoch),\n+            f\"keras_embedding.ckpt-{epoch}\",\n         )\n         self.model.save_weights(embeddings_ckpt)\n \n@@ -2947,7 +2947,7 @@ class ReduceLROnPlateau(Callback):\n         self.monitor = monitor\n         if factor >= 1.0:\n             raise ValueError(\n-                f\"ReduceLROnPlateau does not support \"\n+                \"ReduceLROnPlateau does not support \"\n                 f\"a factor >= 1.0. Got {factor}\"\n             )\n         if \"epsilon\" in kwargs:\n@@ -3023,7 +3023,7 @@ class ReduceLROnPlateau(Callback):\n                         if self.verbose > 0:\n                             io_utils.print_msg(\n                                 f\"\\nEpoch {epoch +1}: \"\n-                                f\"ReduceLROnPlateau reducing \"\n+                                \"ReduceLROnPlateau reducing \"\n                                 f\"learning rate to {new_lr}.\"\n                             )\n                         self.cooldown_counter = self.cooldown\n@@ -3084,7 +3084,7 @@ class CSVLogger(Callback):\n                 isinstance(k, collections.abc.Iterable)\n                 and not is_zero_dim_ndarray\n             ):\n-                return '\"[%s]\"' % (\", \".join(map(str, k)))\n+                return f\"\\\"[{', '.join(map(str, k))}]\\\"\"\n             else:\n                 return k\n \n\n@@ -1585,7 +1585,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n \n         with self.assertRaisesRegex(\n             IOError,\n-            \"Please specify a non-directory \" \"filepath for ModelCheckpoint.\",\n+            \"Please specify a non-directory filepath for ModelCheckpoint.\",\n         ):\n             model.fit(train_ds, epochs=1, callbacks=[callback])\n \n@@ -1602,7 +1602,7 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         callback = keras.callbacks.ModelCheckpoint(filepath=filepath)\n \n         with self.assertRaisesRegex(\n-            KeyError, \"Failed to format this callback \" \"filepath.*\"\n+            KeyError, \"Failed to format this callback filepath.*\"\n         ):\n             model.fit(train_ds, epochs=1, callbacks=[callback])\n \n@@ -2763,7 +2763,7 @@ def list_summaries(logdir):\n       ValueError: If an event file contains an summary of unexpected kind.\n     \"\"\"\n     result = _SummaryFile()\n-    for (dirpath, _, filenames) in os.walk(logdir):\n+    for dirpath, _, filenames in os.walk(logdir):\n         for filename in filenames:\n             if not filename.startswith(\"events.out.\"):\n                 continue\n@@ -2930,7 +2930,7 @@ class TestTensorBoardV2(test_combinations.TestCase):\n         model.fit(x, y, batch_size=2, epochs=2, callbacks=[tb_cbk])\n \n         events_file_run_basenames = set()\n-        for (dirpath, _, filenames) in os.walk(self.train_dir):\n+        for dirpath, _, filenames in os.walk(self.train_dir):\n             if any(fn.startswith(\"events.out.\") for fn in filenames):\n                 events_file_run_basenames.add(os.path.basename(dirpath))\n         self.assertEqual(events_file_run_basenames, {\"train\"})\n@@ -3153,11 +3153,9 @@ class TestTensorBoardV2(test_combinations.TestCase):\n                 f.readlines(),\n                 [\n                     \"embeddings {\\n\",\n-                    (\n                     \"  tensor_name: \"\n                     '\"layer_with_weights-0/embeddings/.ATTRIBUTES/'\n-                        'VARIABLE_VALUE\"\\n'\n-                    ),\n+                    'VARIABLE_VALUE\"\\n',\n                     '  metadata_path: \"metadata.tsv\"\\n',\n                     \"}\\n\",\n                 ],\n@@ -3236,7 +3234,7 @@ class TestTensorBoardV2(test_combinations.TestCase):\n         result = set()\n         for summary in summaries:\n             if \"/\" not in summary.tag:\n-                raise ValueError(\"tag has no layer name: %r\" % summary.tag)\n+                raise ValueError(f\"tag has no layer name: {summary.tag!r}\")\n             start_from = 2 if \"subclass\" in model_type else 1\n             new_tag = \"/\".join(summary.tag.split(\"/\")[start_from:])\n             result.add(summary._replace(tag=new_tag))\n@@ -3307,7 +3305,7 @@ class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n     def _count_trace_file(self, logdir):\n         profile_dir = os.path.join(logdir, \"plugins\", \"profile\")\n         count = 0\n-        for (dirpath, dirnames, filenames) in os.walk(profile_dir):\n+        for dirpath, dirnames, filenames in os.walk(profile_dir):\n             del dirpath  # unused\n             del dirnames  # unused\n             for filename in filenames:\n@@ -3875,7 +3873,7 @@ def events_from_logdir(logdir):\n     \"\"\"\n     assert tf.compat.v1.gfile.Exists(logdir)\n     files = tf.compat.v1.gfile.ListDirectory(logdir)\n-    assert len(files) == 1, \"Found not exactly one file in logdir: %s\" % files\n+    assert len(files) == 1, f\"Found not exactly one file in logdir: {files}\"\n     return events_from_file(os.path.join(logdir, files[0]))\n \n \n\n@@ -228,18 +228,18 @@ class TensorBoard(callbacks.TensorBoard):\n                             for grad in grads\n                         ]\n                         tf.compat.v1.summary.histogram(\n-                            \"{}_grad\".format(mapped_weight_name), grads\n+                            f\"{mapped_weight_name}_grad\", grads\n                         )\n \n                 if hasattr(layer, \"output\"):\n                     if isinstance(layer.output, list):\n                         for i, output in enumerate(layer.output):\n                             tf.compat.v1.summary.histogram(\n-                                \"{}_out_{}\".format(layer.name, i), output\n+                                f\"{layer.name}_out_{i}\", output\n                             )\n                     else:\n                         tf.compat.v1.summary.histogram(\n-                            \"{}_out\".format(layer.name), layer.output\n+                            f\"{layer.name}_out\", layer.output\n                         )\n \n     def set_model(self, model):\n@@ -456,7 +456,7 @@ class TensorBoard(callbacks.TensorBoard):\n \n         if self.embeddings_data is None and self.embeddings_freq:\n             raise ValueError(\n-                \"To visualize embeddings, embeddings_data must \" \"be provided.\"\n+                \"To visualize embeddings, embeddings_data must be provided.\"\n             )\n \n         if self.embeddings_freq and self.embeddings_data is not None:\n\n@@ -67,7 +67,9 @@ def load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113):\n     path = get_file(\n         path,\n         origin=origin_folder + \"boston_housing.npz\",\n-        file_hash=\"f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5\",  # noqa: E501\n+        file_hash=(  # noqa: E501\n+            \"f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5\"\n+        ),\n     )\n     with np.load(path, allow_pickle=True) as f:\n         x = f[\"x\"]\n\n@@ -82,7 +82,9 @@ def load_data():\n         dirname,\n         origin=origin,\n         untar=True,\n-        file_hash=\"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\",  # noqa: E501\n+        file_hash=(  # noqa: E501\n+            \"6d958be074577803d12ecdefd02955f39262c83c16fe9348329d7fe0b5c001ce\"\n+        ),\n     )\n \n     num_train_samples = 50000\n\n@@ -79,7 +79,9 @@ def load_data(label_mode=\"fine\"):\n         dirname,\n         origin=origin,\n         untar=True,\n-        file_hash=\"85cd44d02ba6437773c5bbd22e183051d648de2e7d6b014e1ef29b855ba677a7\",  # noqa: E501\n+        file_hash=(  # noqa: E501\n+            \"85cd44d02ba6437773c5bbd22e183051d648de2e7d6b014e1ef29b855ba677a7\"\n+        ),\n     )\n \n     fpath = os.path.join(path, \"train\")\n\n@@ -111,7 +111,9 @@ def load_data(\n     path = get_file(\n         path,\n         origin=origin_folder + \"imdb.npz\",\n-        file_hash=\"69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f\",  # noqa: E501\n+        file_hash=(  # noqa: E501\n+            \"69664113be75683a8fe16e3ed0ab59fda8886cb3cd7ada244f7d9544e4676b9f\"\n+        ),\n     )\n     with np.load(path, allow_pickle=True) as f:\n         x_train, labels_train = f[\"x_train\"], f[\"y_train\"]\n\n@@ -75,7 +75,9 @@ def load_data(path=\"mnist.npz\"):\n     path = get_file(\n         path,\n         origin=origin_folder + \"mnist.npz\",\n-        file_hash=\"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\",  # noqa: E501\n+        file_hash=(  # noqa: E501\n+            \"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"\n+        ),\n     )\n     with np.load(path, allow_pickle=True) as f:\n         x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n\n@@ -117,7 +117,9 @@ def load_data(\n     path = get_file(\n         path,\n         origin=origin_folder + \"reuters.npz\",\n-        file_hash=\"d6586e694ee56d7a4e65172e12b3e987c03096cb01eab99753921ef915959916\",  # noqa: E501\n+        file_hash=(  # noqa: E501\n+            \"d6586e694ee56d7a4e65172e12b3e987c03096cb01eab99753921ef915959916\"\n+        ),\n     )\n     with np.load(path, allow_pickle=True) as f:\n         xs, labels = f[\"x\"], f[\"y\"]\n\n@@ -755,7 +755,7 @@ def run_distribute_coordinator(\n             )\n         else:\n             if task_type != _TaskType.PS:\n-                raise ValueError(\"Unexpected task_type: %r\" % task_type)\n+                raise ValueError(f\"Unexpected task_type: {task_type!r}\")\n             server.join()\n \n \n\n@@ -456,7 +456,7 @@ class TestDistributionStrategyWithNumpyArrays(\n                 # Computed global batch size can not be sharded across replicas\n                 with self.assertRaisesRegex(\n                     ValueError,\n-                    \"could not be sharded evenly \" \"across the sync replicas\",\n+                    \"could not be sharded evenly across the sync replicas\",\n                 ):\n                     distributed_training_utils_v1.get_input_params(\n                         distribution, 63, steps=1, batch_size=None\n\n@@ -162,8 +162,7 @@ def _on_gcp():\n         # issue. There is not default timeout, which means it might block\n         # forever.\n         response = requests.get(\n-            \"%s/computeMetadata/v1/%s\"\n-            % (gce_metadata_endpoint, \"instance/hostname\"),\n+            f\"{gce_metadata_endpoint}/computeMetadata/v1/{'instance/hostname'}\",\n             headers=GCP_METADATA_HEADER,\n             timeout=5,\n         )\n\n@@ -1057,8 +1057,8 @@ def _make_graph_execution_function(model, mode):\n             all_inputs,\n             all_outputs,\n             updates=all_updates,\n-            name=\"distributed_{}_function\".format(mode),\n-            **all_session_args\n+            name=f\"distributed_{mode}_function\",\n+            **all_session_args,\n         )\n \n \n@@ -1105,7 +1105,7 @@ def _make_eager_execution_function(model, mode):\n         return backend.function(\n             all_inputs,\n             all_outputs,\n-            name=\"eager_distributed_{}_function\".format(mode),\n+            name=f\"eager_distributed_{mode}_function\",\n         )\n \n \n\n@@ -310,7 +310,7 @@ def fit_eval_and_predict(\n         if is_stateful_model:\n             predict_length = 3\n         for i in range(predict_length):\n-            result_key = \"predict_result_{}\".format(i)\n+            result_key = f\"predict_result_{i}\"\n             result[result_key] = model.predict(**predict_inputs)\n \n     # Train and eval again to mimic user's flow.\n@@ -397,7 +397,7 @@ def compare_results(\n             results_without_ds[key],\n             atol=tolerance,\n             rtol=tolerance,\n-            msg=\"Fail to assert {}.\".format(key),\n+            msg=f\"Fail to assert {key}.\",\n         )\n \n \n\n@@ -139,7 +139,7 @@ class DistributionStrategyLstmModelCorrectnessTest(\n             ),\n         ):\n             self.skipTest(\n-                \"CentralStorageStrategy is not supported by \" \"mixed precision.\"\n+                \"CentralStorageStrategy is not supported by mixed precision.\"\n             )\n         if isinstance(\n             distribution,\n\n@@ -268,7 +268,7 @@ class MinimizeLossStepTest(tf.test.TestCase, parameterized.TestCase):\n                     variables = VAR_MAP_V1[name]\n \n                 extended_variables = [\n-                    v + \"/replica_{}\".format(replica)\n+                    v + f\"/replica_{replica}\"\n                     for v in variables\n                     for replica in range(1, num_parameter_devices)\n                 ]\n\n@@ -373,7 +373,7 @@ class KerasCallbackMultiProcessTest(parameterized.TestCase, tf.test.TestCase):\n \n             saving_filepath = os.path.join(\n                 test_obj.get_temp_dir(),\n-                \"logfile_%s\" % (get_tf_config_task()[\"type\"]),\n+                f\"logfile_{get_tf_config_task()['type']}\",\n             )\n \n             saving_filepath_for_temp = os.path.join(\n\n@@ -145,14 +145,14 @@ def _create_cluster(\n     cluster_dict = {}\n     if num_workers > 0:\n         cluster_dict[worker_name] = [\n-            \"localhost:%s\" % port for port in worker_ports\n+            f\"localhost:{port}\" for port in worker_ports\n         ]\n     if num_ps > 0:\n-        cluster_dict[ps_name] = [\"localhost:%s\" % port for port in ps_ports]\n+        cluster_dict[ps_name] = [f\"localhost:{port}\" for port in ps_ports]\n     if has_eval:\n-        cluster_dict[\"evaluator\"] = [\"localhost:%s\" % pick_unused_port()]\n+        cluster_dict[\"evaluator\"] = [f\"localhost:{pick_unused_port()}\"]\n     if has_chief:\n-        cluster_dict[chief_name] = [\"localhost:%s\" % pick_unused_port()]\n+        cluster_dict[chief_name] = [f\"localhost:{pick_unused_port()}\"]\n \n     cs = tf.train.ClusterSpec(cluster_dict)\n \n\n@@ -293,7 +293,7 @@ class SidecarEvaluator:\n                 \"End of evaluation. Metrics: %s\",\n                 \" \".join(\n                     [\n-                        \"{}={}\".format(name, value.numpy())\n+                        f\"{name}={value.numpy()}\"\n                         for name, value in return_metrics.items()\n                     ]\n                 ),\n\n@@ -79,8 +79,7 @@ class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n         summary_files = tf.io.gfile.listdir(log_dir)\n         self.assertNotEmpty(\n             summary_files,\n-            \"Summary should have been written and \"\n-            \"log_dir should not be empty.\",\n+            \"Summary should have been written and log_dir should not be empty.\",\n         )\n \n         # Asserts the content of the summary file.\n@@ -138,7 +137,7 @@ class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         with self.assertRaisesRegex(\n             RuntimeError,\n-            \"`iterations` cannot be loaded \" \"from the checkpoint file.\",\n+            \"`iterations` cannot be loaded from the checkpoint file.\",\n         ):\n             sidecar_evaluator.start()\n \n@@ -342,7 +341,7 @@ class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n             sidecar_evaluator_lib.SidecarEvaluatorExperimental(None, None, None)\n \n         warning_msg = (\n-            \"`tf.keras.experimental.SidecarEvaluator` \" \"endpoint is deprecated\"\n+            \"`tf.keras.experimental.SidecarEvaluator` endpoint is deprecated\"\n         )\n         self.assertIn(warning_msg, \"\\n\".join(warning_messages))\n \n\n@@ -121,8 +121,7 @@ class LayoutMap(collections.abc.MutableMapping):\n             )\n         if not isinstance(layout, dtensor.Layout):\n             raise ValueError(\n-                f\"{layout} should be a dtensor.Layout type, \"\n-                f\"got {type(layout)}\"\n+                f\"{layout} should be a dtensor.Layout type, got {type(layout)}\"\n             )\n \n         self._layout_map[key] = layout\n@@ -488,7 +487,7 @@ def _config_dvariable_regularization(\n         ID and newly created DVariable.\n     \"\"\"\n \n-    for (name, variable, regualarizer) in layer._captured_weight_regularizer:\n+    for name, variable, regualarizer in layer._captured_weight_regularizer:\n         if not _is_lazy_init_variable(variable):\n             raise ValueError(\n                 \"Expect the regularization loss are created from \"\n\n@@ -141,7 +141,7 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n \n         if constraint is not None and not callable(constraint):\n             raise ValueError(\n-                f\"Argument `constraint` must be None or a callable. \"\n+                \"Argument `constraint` must be None or a callable. \"\n                 f\"a callable. Got a {type(constraint)}:  {constraint}\"\n             )\n \n@@ -186,9 +186,9 @@ class LazyInitVariable(resource_variable_ops.BaseResourceVariable):\n \n                 if not initial_value.shape.is_compatible_with(self._shape):\n                     raise ValueError(\n-                        f\"In this `tf.Variable` creation, the initial value's \"\n+                        \"In this `tf.Variable` creation, the initial value's \"\n                         f\"shape ({initial_value.shape}) is not compatible with \"\n-                        f\"the explicitly supplied `shape` \"\n+                        \"the explicitly supplied `shape` \"\n                         f\"argument ({self._shape}).\"\n                     )\n                 assert self._dtype is initial_value.dtype.base_dtype\n\n@@ -1075,8 +1075,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n             call_fn = traceback_utils.inject_argument_info_in_traceback(\n                 call_fn,\n-                object_name=f'layer \"{self.name}\" \" \\\n-                f\"(type {self.__class__.__name__})',\n+                object_name=(\n+                    f'layer \"{self.name}\" \"                 f\"(type'\n+                    f\" {self.__class__.__name__})\"\n+                ),\n             )\n             with contextlib.ExitStack() as namescope_stack:\n                 if _is_name_scope_on_model_declaration_enabled:\n@@ -1439,7 +1441,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         \"\"\"\n         kwargs.pop(\"inputs\", None)\n         if kwargs:\n-            raise TypeError(\"Unknown keyword arguments: %s\" % (kwargs.keys(),))\n+            raise TypeError(f\"Unknown keyword arguments: {kwargs.keys()}\")\n \n         def _tag_callable(loss):\n             \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\"\n@@ -1612,8 +1614,8 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n         if not in_call_context and not is_symbolic:\n             raise ValueError(\n-                \"Expected a symbolic Tensor for the metric value, \"\n-                \"received: \" + str(value)\n+                \"Expected a symbolic Tensor for the metric value, received: \"\n+                + str(value)\n             )\n \n         # If a metric was added in a Layer's `call` or `build`.\n@@ -2268,7 +2270,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n             self.__class__, api_name=\"keras\", add_prefix_to_v1_names=True\n         )\n         if canonical_name is not None:\n-            return \"tf.{}\".format(canonical_name)\n+            return f\"tf.{canonical_name}\"\n         return self.__class__.__module__ + \".\" + self.__class__.__name__\n \n     def _instrument_layer_creation(self):\n@@ -3609,7 +3611,7 @@ def _apply_name_scope_on_model_declaration(enable):\n     \"\"\"\n     if not isinstance(enable, bool):\n         raise TypeError(\n-            \"`enable` argument must be `True` or `False`, got {}\".format(enable)\n+            f\"`enable` argument must be `True` or `False`, got {enable}\"\n         )\n \n     global _is_name_scope_on_model_declaration_enabled\n\n@@ -1530,12 +1530,12 @@ class NameScopingTest(test_combinations.TestCase):\n                 \"MatMul/ReadVariableOp/resource\",\n                 \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n                 \"MatMul/ReadVariableOp\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/\" \"MatMul\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/MatMul\",\n                 \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n                 \"BiasAdd/ReadVariableOp/resource\",\n                 \"call_scope/model/outer/ThreeDenses/NestedDense3/\"\n                 \"BiasAdd/ReadVariableOp\",\n-                \"call_scope/model/outer/ThreeDenses/NestedDense3/\" \"BiasAdd\",\n+                \"call_scope/model/outer/ThreeDenses/NestedDense3/BiasAdd\",\n                 \"call_scope/model/OuterDense/MatMul/ReadVariableOp/resource\",\n                 \"call_scope/model/OuterDense/MatMul/ReadVariableOp\",\n                 \"call_scope/model/OuterDense/MatMul\",\n\n@@ -2135,8 +2135,9 @@ class Layer(base_layer.Layer):\n         \"\"\"\n         if not self._inbound_nodes:\n             raise RuntimeError(\n-                \"The layer has never been called \"\n-                \"and thus has no defined \" + attr_name + \".\"\n+                \"The layer has never been called and thus has no defined \"\n+                + attr_name\n+                + \".\"\n             )\n         if not len(self._inbound_nodes) > node_index:\n             raise ValueError(\n\n@@ -118,9 +118,7 @@ class DataAdapter(object, metaclass=abc.ABCMeta):\n             provided.\n         \"\"\"\n         if not self.can_handle(x, y):\n-            raise ValueError(\n-                \"{} Cannot handle input {}, {}\".format(self.__class__, x, y)\n-            )\n+            raise ValueError(f\"{self.__class__} Cannot handle input {x}, {y}\")\n \n     @abc.abstractmethod\n     def get_dataset(self):\n@@ -241,7 +239,7 @@ class TensorLikeDataAdapter(DataAdapter):\n         epochs=1,\n         steps=None,\n         shuffle=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(x, y, **kwargs)\n         x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n@@ -617,7 +615,7 @@ class CompositeTensorDataAdapter(DataAdapter):\n         batch_size=None,\n         steps=None,\n         shuffle=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(x, y, **kwargs)\n         x, y, sample_weights = _process_tensorlike((x, y, sample_weights))\n@@ -701,7 +699,7 @@ class ListsOfScalarsDataAdapter(DataAdapter):\n         sample_weight_modes=None,\n         batch_size=None,\n         shuffle=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(x, y, **kwargs)\n         x = np.asarray(x)\n@@ -720,7 +718,7 @@ class ListsOfScalarsDataAdapter(DataAdapter):\n             sample_weight_modes=sample_weight_modes,\n             batch_size=batch_size,\n             shuffle=shuffle,\n-            **kwargs\n+            **kwargs,\n         )\n \n     def get_dataset(self):\n@@ -797,7 +795,7 @@ class DatasetAdapter(DataAdapter):\n         # Arguments that shouldn't be passed.\n         if not is_none_or_empty(y):\n             raise ValueError(\n-                \"`y` argument is not supported when using \" \"dataset as input.\"\n+                \"`y` argument is not supported when using dataset as input.\"\n             )\n         if not is_none_or_empty(sample_weights):\n             raise ValueError(\n@@ -845,7 +843,7 @@ class GeneratorDataAdapter(DataAdapter):\n         use_multiprocessing=False,\n         max_queue_size=10,\n         model=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         # Generators should never shuffle as exhausting the generator in order\n         # to shuffle the batches is inefficient.\n@@ -991,7 +989,7 @@ class KerasSequenceAdapter(GeneratorDataAdapter):\n         use_multiprocessing=False,\n         max_queue_size=10,\n         model=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         if not is_none_or_empty(y):\n             raise ValueError(\n@@ -1014,7 +1012,7 @@ class KerasSequenceAdapter(GeneratorDataAdapter):\n             use_multiprocessing=use_multiprocessing,\n             max_queue_size=max_queue_size,\n             model=model,\n-            **kwargs\n+            **kwargs,\n         )\n \n     @staticmethod\n@@ -1081,8 +1079,9 @@ def select_data_adapter(x, y):\n     if not adapter_cls:\n         # TODO(scottzhu): This should be a less implementation-specific error.\n         raise ValueError(\n-            \"Failed to find data adapter that can handle \"\n-            \"input: {}, {}\".format(_type_name(x), _type_name(y))\n+            \"Failed to find data adapter that can handle input: {}, {}\".format(\n+                _type_name(x), _type_name(y)\n+            )\n         )\n     elif len(adapter_cls) > 1:\n         raise RuntimeError(\n@@ -1100,12 +1099,10 @@ def _type_name(x):\n     if isinstance(x, dict):\n         key_types = set(_type_name(key) for key in x.keys())\n         val_types = set(_type_name(key) for key in x.values())\n-        return \"({} containing {} keys and {} values)\".format(\n-            type(x), key_types, val_types\n-        )\n+        return f\"({type(x)} containing {key_types} keys and {val_types} values)\"\n     if isinstance(x, (list, tuple)):\n         types = set(_type_name(val) for val in x)\n-        return \"({} containing values of types {})\".format(type(x), types)\n+        return f\"({type(x)} containing values of types {types})\"\n     return str(type(x))\n \n \n@@ -1621,7 +1618,7 @@ def _make_class_weight_map_fn(class_weight):\n \n         if y.shape.rank > 2:\n             raise ValueError(\n-                \"`class_weight` not supported for \" \"3+ dimensional targets.\"\n+                \"`class_weight` not supported for 3+ dimensional targets.\"\n             )\n \n         y_classes = tf.__internal__.smart_cond.smart_cond(\n\n@@ -422,7 +422,7 @@ class Functional(training_lib.Model):\n             proposal = layer.name\n             while proposal in output_names:\n                 existing_count = prefix_count.get(layer.name, 1)\n-                proposal = \"{}_{}\".format(layer.name, existing_count)\n+                proposal = f\"{layer.name}_{existing_count}\"\n                 prefix_count[layer.name] = existing_count + 1\n             output_names.add(proposal)\n             uniquified.append(proposal)\n@@ -589,14 +589,14 @@ class Functional(training_lib.Model):\n                     for j, shape in enumerate(\n                         tf.nest.flatten(layer_output_shapes)\n                     ):\n-                        shape_key = layer.name + \"_%s_%s\" % (node_index, j)\n+                        shape_key = layer.name + f\"_{node_index}_{j}\"\n                         layers_to_output_shapes[shape_key] = shape\n \n             # Read final output shapes from layers_to_output_shapes.\n             output_shapes = []\n             for i in range(len(self._output_layers)):\n                 layer, node_index, tensor_index = self._output_coordinates[i]\n-                shape_key = layer.name + \"_%s_%s\" % (node_index, tensor_index)\n+                shape_key = layer.name + f\"_{node_index}_{tensor_index}\"\n                 output_shapes.append(layers_to_output_shapes[shape_key])\n             output_shapes = tf.nest.pack_sequence_as(\n                 self._nested_outputs, output_shapes\n@@ -916,7 +916,7 @@ class Functional(training_lib.Model):\n             # Model are being relied on.\n             if i > 10000:\n                 raise ValueError(\n-                    \"Layers could not be added due to missing \" \"dependencies.\"\n+                    \"Layers could not be added due to missing dependencies.\"\n                 )\n \n             node = unprocessed_nodes.pop(0)\n@@ -1126,7 +1126,7 @@ def _map_graph_network(inputs, outputs):\n                 for x in tf.nest.flatten(node.keras_inputs):\n                     if id(x) not in computable_tensors:\n                         raise ValueError(\n-                            f\"Graph disconnected: cannot obtain value for \"\n+                            \"Graph disconnected: cannot obtain value for \"\n                             f'tensor {x} at layer \"{layer.name}\". '\n                             \"The following previous layers were accessed \"\n                             f\"without issue: {layers_with_complete_input}\"\n@@ -1205,7 +1205,7 @@ def _build_map_helper(\n     # Prevent cycles.\n     if node in nodes_in_progress:\n         raise ValueError(\n-            f'Tensor {tensor} from layer \"{layer.name}\" ' \"is part of a cycle.\"\n+            f'Tensor {tensor} from layer \"{layer.name}\" is part of a cycle.'\n         )\n \n     # Store the traversal order for layer sorting.\n@@ -1639,9 +1639,7 @@ class ModuleWrapper(base_layer.Layer):\n             elif hasattr(module, \"call\"):\n                 method_name = \"call\"\n         if method_name is None or not hasattr(module, method_name):\n-            raise ValueError(\n-                \"{} is not defined on object {}\".format(method_name, module)\n-            )\n+            raise ValueError(f\"{method_name} is not defined on object {module}\")\n \n         self._module = module\n         self._method_name = method_name\n\n@@ -1702,7 +1702,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n         self.assertTrue(model.built, \"Model should be built\")\n         self.assertTrue(\n             model.weights,\n-            \"Model should have its weights created as it \" \"has been built\",\n+            \"Model should have its weights created as it has been built\",\n         )\n         sample_input = tf.ones((1, 10, 10, 1))\n         output = model(sample_input)\n@@ -1739,7 +1739,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n         self.assertTrue(model.built, \"Model should be built\")\n         self.assertTrue(\n             model.weights,\n-            \"Model should have its weights created as it \" \"has been built\",\n+            \"Model should have its weights created as it has been built\",\n         )\n         sample_input = tf.ones((1, 10, 10, 1))\n         output = model(sample_input)\n@@ -1772,7 +1772,7 @@ class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n         self.assertTrue(model.built, \"Model should be built\")\n         self.assertTrue(\n             model.weights,\n-            \"Model should have its weights created as it \" \"has been built\",\n+            \"Model should have its weights created as it has been built\",\n         )\n         sample_input = tf.ones((1, 10, 10, 1))\n         output = model(sample_input)\n\n@@ -126,7 +126,7 @@ class InputSpec:\n             (\"min_ndim=\" + str(self.min_ndim)) if self.min_ndim else \"\",\n             (\"axes=\" + str(self.axes)) if self.axes else \"\",\n         ]\n-        return \"InputSpec(%s)\" % \", \".join(x for x in spec if x)\n+        return f\"InputSpec({', '.join(x for x in spec if x)})\"\n \n     def get_config(self):\n         return {\n\n@@ -323,9 +323,9 @@ class KerasTensor:\n                 layer.name,\n             )\n         if self._inferred_value is not None:\n-            inferred_value_string = \", inferred_value=%s\" % self._inferred_value\n+            inferred_value_string = f\", inferred_value={self._inferred_value}\"\n         if self.name is not None:\n-            name_string = \", name='%s'\" % self._name\n+            name_string = f\", name='{self._name}'\"\n         return \"KerasTensor(type_spec=%s%s%s%s)\" % (\n             self.type_spec,\n             inferred_value_string,\n@@ -337,18 +337,15 @@ class KerasTensor:\n         symbolic_description = \"\"\n         inferred_value_string = \"\"\n         if isinstance(self.type_spec, tf.TensorSpec):\n-            type_spec_string = \"shape=%s dtype=%s\" % (\n-                self.shape,\n-                self.dtype.name,\n-            )\n+            type_spec_string = f\"shape={self.shape} dtype={self.dtype.name}\"\n         else:\n-            type_spec_string = \"type_spec=%s\" % self.type_spec\n+            type_spec_string = f\"type_spec={self.type_spec}\"\n \n         if hasattr(self, \"_keras_history\"):\n             layer = self._keras_history.layer\n-            symbolic_description = \" (created by layer '%s')\" % (layer.name,)\n+            symbolic_description = f\" (created by layer '{layer.name}')\"\n         if self._inferred_value is not None:\n-            inferred_value_string = \" inferred_value=%s\" % self._inferred_value\n+            inferred_value_string = f\" inferred_value={self._inferred_value}\"\n         return \"<KerasTensor: %s%s%s>\" % (\n             type_spec_string,\n             inferred_value_string,\n@@ -595,8 +592,8 @@ class UserRegisteredTypeKerasTensor(KerasTensor):\n     @classmethod\n     def from_type_spec(cls, type_spec, name=None):\n         raise NotImplementedError(\n-            \"You cannot instantiate a KerasTensor \"\n-            \"directly from TypeSpec: %s\" % type_spec\n+            \"You cannot instantiate a KerasTensor directly from TypeSpec: %s\"\n+            % type_spec\n         )\n \n     def _to_placeholder(self):\n\n@@ -227,8 +227,7 @@ class Node:\n                 + \" was passed non-JSON-serializable arguments. \"\n                 + \"Arguments had types: \"\n                 + str(kwarg_types)\n-                + \". They cannot be serialized out \"\n-                \"when saving the model.\"\n+                + \". They cannot be serialized out when saving the model.\"\n             )\n \n         # `kwargs` is added to each Tensor in the first arg. This should be\n\n@@ -3277,7 +3277,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 f\"{list(layer.name for layer in self.layers)}.\"\n             )\n         raise ValueError(\n-            \"Provide either a layer name or layer index at \" \"`get_layer`.\"\n+            \"Provide either a layer name or layer index at `get_layer`.\"\n         )\n \n     def get_weight_paths(self):\n\n@@ -59,7 +59,7 @@ def model_iteration(\n     validation_in_fit=False,\n     prepared_feed_values_from_dataset=False,\n     steps_name=\"steps\",\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Loop function for arrays of data with modes TRAIN/TEST/PREDICT.\n \n@@ -124,7 +124,7 @@ def model_iteration(\n     if \"steps\" in kwargs:\n         steps_per_epoch = kwargs.pop(\"steps\")\n     if kwargs:\n-        raise TypeError(\"Unknown arguments: %s\" % (kwargs,))\n+        raise TypeError(f\"Unknown arguments: {kwargs}\")\n \n     # In case we were passed a dataset, we extract symbolic tensors from it.\n     reset_dataset_after_each_epoch = False\n@@ -520,13 +520,9 @@ def _get_model_feed(model, mode):\n \n def _print_train_info(num_samples_or_steps, val_samples_or_steps, is_dataset):\n     increment = \"steps\" if is_dataset else \"samples\"\n-    msg = \"Train on {0} {increment}\".format(\n-        num_samples_or_steps, increment=increment\n-    )\n+    msg = f\"Train on {num_samples_or_steps} {increment}\"\n     if val_samples_or_steps:\n-        msg += \", validate on {0} {increment}\".format(\n-            val_samples_or_steps, increment=increment\n-        )\n+        msg += f\", validate on {val_samples_or_steps} {increment}\"\n     io_utils.print_msg(msg)\n \n \n@@ -693,7 +689,7 @@ class ArrayLikeTrainingLoop(training_utils_v1.TrainingLoop):\n         steps_per_epoch=None,\n         validation_steps=None,\n         validation_freq=1,\n-        **kwargs\n+        **kwargs,\n     ):\n         batch_size = model._validate_or_infer_batch_size(\n             batch_size, steps_per_epoch, x\n@@ -765,7 +761,7 @@ class ArrayLikeTrainingLoop(training_utils_v1.TrainingLoop):\n         sample_weight=None,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)\n         x, y, sample_weights = model._standardize_user_data(\n@@ -796,7 +792,7 @@ class ArrayLikeTrainingLoop(training_utils_v1.TrainingLoop):\n         verbose=0,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)\n         x, _, _ = model._standardize_user_data(\n\n@@ -52,7 +52,7 @@ def model_iteration(\n     mode=ModeKeys.TRAIN,\n     batch_size=None,\n     steps_name=\"steps\",\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Loop function for arrays of data with modes TRAIN/TEST/PREDICT.\n \n@@ -457,8 +457,7 @@ def _validate_arguments(\n     if steps_per_epoch is None and not is_dataset:\n         arg_name = \"steps_per_epoch\" if mode == ModeKeys.TRAIN else \"steps\"\n         raise ValueError(\n-            \"Please specify the number of steps via the \"\n-            \"`{}` argument.\".format(arg_name)\n+            f\"Please specify the number of steps via the `{arg_name}` argument.\"\n         )\n \n     val_gen = data_utils.is_generator_or_sequence(\n@@ -473,9 +472,7 @@ def _validate_arguments(\n \n     if any(k != \"steps\" for k in kwargs):\n         raise ValueError(\n-            \"Invalid arguments passed: {}\".format(\n-                [k for k in kwargs if k != \"steps\"]\n-            )\n+            f\"Invalid arguments passed: {[k for k in kwargs if k != 'steps']}\"\n         )\n \n \n@@ -540,7 +537,7 @@ def convert_to_generator_like(\n             if shuffle:\n                 np.random.shuffle(index_array)\n             batches = generic_utils.make_batches(num_samples, batch_size)\n-            for (batch_start, batch_end) in batches:\n+            for batch_start, batch_end in batches:\n                 batch_ids = index_array[batch_start:batch_end]\n                 flat_batch_data = training_utils.slice_arrays(\n                     tf.nest.flatten(data), batch_ids, contiguous=(not shuffle)\n@@ -739,7 +736,7 @@ class EagerDatasetOrIteratorTrainingLoop(training_utils_v1.TrainingLoop):\n         steps_per_epoch=None,\n         validation_steps=None,\n         validation_freq=1,\n-        **kwargs\n+        **kwargs,\n     ):\n         model._validate_or_infer_batch_size(batch_size, steps_per_epoch, x)\n         # Make sure that y, sample_weights, validation_split are not passed.\n@@ -779,7 +776,7 @@ class EagerDatasetOrIteratorTrainingLoop(training_utils_v1.TrainingLoop):\n         sample_weight=None,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         model._validate_or_infer_batch_size(batch_size, steps, x)\n         # Make sure that y, sample_weights, validation_split are not passed.\n@@ -801,7 +798,7 @@ class EagerDatasetOrIteratorTrainingLoop(training_utils_v1.TrainingLoop):\n         verbose=0,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         model._validate_or_infer_batch_size(batch_size, steps, x)\n         return predict_generator(\n@@ -841,7 +838,7 @@ class GeneratorLikeTrainingLoop(training_utils_v1.TrainingLoop):\n         steps_per_epoch=None,\n         validation_steps=None,\n         validation_freq=1,\n-        **kwargs\n+        **kwargs,\n     ):\n         batch_size = model._validate_or_infer_batch_size(\n             batch_size, steps_per_epoch, x\n@@ -909,7 +906,7 @@ class GeneratorLikeTrainingLoop(training_utils_v1.TrainingLoop):\n         sample_weight=None,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)\n         x, y, sample_weights = model._standardize_user_data(\n@@ -939,7 +936,7 @@ class GeneratorLikeTrainingLoop(training_utils_v1.TrainingLoop):\n         verbose=0,\n         steps=None,\n         callbacks=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         batch_size = model._validate_or_infer_batch_size(batch_size, steps, x)\n         x, _, _ = model._standardize_user_data(\n\n@@ -181,9 +181,7 @@ class CoreLayerIntegrationTest(test_combinations.TestCase):\n         for x in [layer_result, model_result]:\n             if not isinstance(x, tf.Tensor):\n                 raise ValueError(\n-                    \"Tensor or EagerTensor expected, got type {}\".format(\n-                        type(x)\n-                    )\n+                    f\"Tensor or EagerTensor expected, got type {type(x)}\"\n                 )\n \n             if (\n@@ -196,9 +194,7 @@ class CoreLayerIntegrationTest(test_combinations.TestCase):\n                     else tf.Tensor\n                 )\n                 raise ValueError(\n-                    \"Expected type {}, got type {}\".format(\n-                        expected_type, type(x)\n-                    )\n+                    f\"Expected type {expected_type}, got type {type(x)}\"\n                 )\n \n     def _run_fit_eval_predict(\n\n@@ -194,14 +194,10 @@ def _append_ragged_tensor_value(target, to_append):\n     \"\"\"Append ragged tensor value objects.\"\"\"\n     # Make sure the ragged tensors are of the same size (save for the 0th dim).\n     if len(target.shape) != len(to_append.shape):\n-        raise RuntimeError(\n-            \"Unable to concatenate %s and %s\" % (target, to_append)\n-        )\n+        raise RuntimeError(f\"Unable to concatenate {target} and {to_append}\")\n \n     if target.shape[1:] != to_append.shape[1:]:\n-        raise RuntimeError(\n-            \"Unable to concatenate %s and %s\" % (target, to_append)\n-        )\n+        raise RuntimeError(f\"Unable to concatenate {target} and {to_append}\")\n \n     adjusted_row_splits = to_append.row_splits[1:] + target.row_splits[-1]\n     new_row_splits = np.append(target.row_splits, adjusted_row_splits)\n@@ -238,7 +234,7 @@ def _append_composite_tensor(target, to_append):\n     \"\"\"\n     if type(target) is not type(to_append):\n         raise RuntimeError(\n-            \"Unable to concatenate %s and %s\" % (type(target), type(to_append))\n+            f\"Unable to concatenate {type(target)} and {type(to_append)}\"\n         )\n \n     # Perform type-specific concatenation.\n@@ -263,7 +259,7 @@ def _append_composite_tensor(target, to_append):\n         return _append_ragged_tensor_value(target, to_append)\n     else:\n         raise RuntimeError(\n-            \"Attempted to concatenate unsupported object %s.\" % type(target)\n+            f\"Attempted to concatenate unsupported object {type(target)}.\"\n         )\n \n \n@@ -555,7 +551,7 @@ def standardize_single_array(x, expected_shape=None):\n \n     if isinstance(x, int):\n         raise ValueError(\n-            \"Expected an array data type but received an integer: {}\".format(x)\n+            f\"Expected an array data type but received an integer: {x}\"\n         )\n \n     if (\n@@ -612,8 +608,9 @@ def standardize_input_data(\n     if not names:\n         if data_len and not isinstance(data, dict):\n             raise ValueError(\n-                \"Error when checking model \" + exception_prefix + \": \"\n-                \"expected no data, but got:\",\n+                \"Error when checking model \"\n+                + exception_prefix\n+                + \": expected no data, but got:\",\n                 data,\n             )\n         return []\n@@ -630,8 +627,10 @@ def standardize_input_data(\n             ]\n         except KeyError as e:\n             raise ValueError(\n-                'No data provided for \"' + e.args[0] + '\". Need data '\n-                \"for each key in: \" + str(names)\n+                'No data provided for \"'\n+                + e.args[0]\n+                + '\". Need data for each key in: '\n+                + str(names)\n             )\n     elif isinstance(data, (list, tuple)):\n         if isinstance(data[0], (list, tuple)):\n@@ -667,8 +666,7 @@ def standardize_input_data(\n                 + \" array(s), \"\n                 + \"for inputs \"\n                 + str(names)\n-                + \" but instead got the \"\n-                \"following list of \"\n+                + \" but instead got the following list of \"\n                 + str(len(data))\n                 + \" arrays: \"\n                 + str(data)[:200]\n@@ -718,8 +716,8 @@ def standardize_input_data(\n                         + names[i]\n                         + \" to have \"\n                         + str(len(shape))\n-                        + \" dimensions, but got array \"\n-                        \"with shape \" + str(data_shape)\n+                        + \" dimensions, but got array with shape \"\n+                        + str(data_shape)\n                     )\n                 if not check_batch_axis:\n                     data_shape = data_shape[1:]\n@@ -778,9 +776,9 @@ def standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n                 + str(len(x_weight))\n                 + \" elements, but the model has \"\n                 + str(len(output_names))\n-                + \" outputs. \"\n-                \"You should provide one `\" + weight_type + \"`\"\n-                \"array per model output.\"\n+                + \" outputs. You should provide one `\"\n+                + weight_type\n+                + \"`array per model output.\"\n             )\n         return x_weight\n     if isinstance(x_weight, collections.abc.Mapping):\n@@ -793,9 +791,9 @@ def standardize_sample_or_class_weights(x_weight, output_names, weight_type):\n         return x_weights\n     else:\n         raise TypeError(\n-            \"The model has multiple outputs, so `\" + weight_type + \"` \"\n-            \"should be either a list or a dict. \"\n-            \"Provided `\"\n+            \"The model has multiple outputs, so `\"\n+            + weight_type\n+            + \"` should be either a list or a dict. Provided `\"\n             + weight_type\n             + \"` type not understood: \"\n             + str(x_weight)\n@@ -862,8 +860,11 @@ def check_array_lengths(inputs, targets, weights=None):\n         raise ValueError(\n             \"Input arrays should have \"\n             \"the same number of samples as target arrays. \"\n-            \"Found \" + str(list(set_x)[0]) + \" input samples \"\n-            \"and \" + str(list(set_y)[0]) + \" target samples.\"\n+            \"Found \"\n+            + str(list(set_x)[0])\n+            + \" input samples and \"\n+            + str(list(set_y)[0])\n+            + \" target samples.\"\n         )\n     if len(set_w) > 1:\n         raise ValueError(\n@@ -1110,14 +1111,14 @@ def standardize_weights(\n     if sample_weight_mode is not None and sample_weight_mode != \"samplewise\":\n         if sample_weight_mode != \"temporal\":\n             raise ValueError(\n-                '\"sample_weight_mode '\n-                'should be None or \"temporal\". '\n-                \"Found: \" + str(sample_weight_mode)\n+                '\"sample_weight_mode should be None or \"temporal\". Found: '\n+                + str(sample_weight_mode)\n             )\n         if len(y.shape) < 3:\n             raise ValueError(\n-                \"Found a sample_weight array for \"\n-                \"an input with shape \" + str(y.shape) + \". \"\n+                \"Found a sample_weight array for an input with shape \"\n+                + str(y.shape)\n+                + \". \"\n                 \"Timestep-wise sample weighting (use of \"\n                 'sample_weight_mode=\"temporal\") is restricted to '\n                 \"outputs that are at least 3D, i.e. that have \"\n@@ -1148,9 +1149,8 @@ def standardize_weights(\n             raise ValueError(\n                 \"Found a sample_weight with shape\"\n                 + str(sample_weight.shape)\n-                + \".\"\n-                \"Expected sample_weight with rank \"\n-                \"less than or equal to \" + str(len(y.shape))\n+                + \".Expected sample_weight with rank less than or equal to \"\n+                + str(len(y.shape))\n             )\n \n         if (\n@@ -1162,8 +1162,7 @@ def standardize_weights(\n                 + str(sample_weight.shape)\n                 + \" for an input with shape \"\n                 + str(y.shape)\n-                + \". \"\n-                \"sample_weight cannot be broadcast.\"\n+                + \". sample_weight cannot be broadcast.\"\n             )\n \n     # Class weights applied per-sample.\n@@ -1171,7 +1170,7 @@ def standardize_weights(\n     if isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n             raise ValueError(\n-                \"`class_weight` not supported for \" \"3+ dimensional targets.\"\n+                \"`class_weight` not supported for 3+ dimensional targets.\"\n             )\n \n         if tf.is_tensor(y):\n@@ -1447,7 +1446,7 @@ def validate_input_types(inp, orig_inp, allow_dict=True, field_name=\"inputs\"):\n     elif isinstance(inp, dict):\n         if not allow_dict:\n             raise ValueError(\n-                \"You cannot pass a dictionary as model {}.\".format(field_name)\n+                f\"You cannot pass a dictionary as model {field_name}.\"\n             )\n     elif not isinstance(inp, np.ndarray) and not tf.is_tensor(inp):\n         raise ValueError(\n\n@@ -232,7 +232,7 @@ class Model(training_lib.Model):\n         weighted_metrics=None,\n         target_tensors=None,\n         distribute=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         \"\"\"Configures the model for training.\n \n@@ -304,8 +304,7 @@ class Model(training_lib.Model):\n         unknown_kwargs = set(kwargs.keys()) - allowed_kwargs\n         if unknown_kwargs:\n             raise TypeError(\n-                \"Invalid keyword argument(s) in `compile`: %s\"\n-                % (unknown_kwargs,)\n+                f\"Invalid keyword argument(s) in `compile`: {unknown_kwargs}\"\n             )\n         self._function_kwargs = kwargs\n         if self._function_kwargs:\n@@ -687,7 +686,7 @@ class Model(training_lib.Model):\n         max_queue_size=10,\n         workers=1,\n         use_multiprocessing=False,\n-        **kwargs\n+        **kwargs,\n     ):\n         \"\"\"Trains the model for a fixed number of epochs (iterations on a dataset).\n \n@@ -2052,10 +2051,7 @@ class Model(training_lib.Model):\n             # want to prepend the output name even if we are loading a\n             # serialized model.\n             if not getattr(metric_fn, \"_from_serialized\", False):\n-                metric_name = \"%s_%s\" % (\n-                    self.output_names[output_index],\n-                    metric_name,\n-                )\n+                metric_name = f\"{self.output_names[output_index]}_{metric_name}\"\n \n         j = 1\n         base_metric_name = metric_name\n@@ -2257,7 +2253,7 @@ class Model(training_lib.Model):\n         self._check_trainable_weights_consistency()\n         if isinstance(self.optimizer, list):\n             raise ValueError(\n-                \"The `optimizer` in `compile` should be a single \" \"optimizer.\"\n+                \"The `optimizer` in `compile` should be a single optimizer.\"\n             )\n         # If we have re-compiled the loss/weighted metric sub-graphs then create\n         # train function even if one exists already. This is because\n@@ -2301,7 +2297,7 @@ class Model(training_lib.Model):\n                     [self.total_loss] + metrics_tensors,\n                     updates=updates,\n                     name=\"train_function\",\n-                    **self._function_kwargs\n+                    **self._function_kwargs,\n                 )\n                 setattr(self, \"train_function\", fn)\n \n@@ -2337,7 +2333,7 @@ class Model(training_lib.Model):\n                     [self.total_loss] + metrics_tensors,\n                     updates=updates,\n                     name=\"test_function\",\n-                    **self._function_kwargs\n+                    **self._function_kwargs,\n                 )\n                 setattr(self, \"test_function\", fn)\n \n@@ -2355,7 +2351,7 @@ class Model(training_lib.Model):\n                     self.outputs,\n                     updates=self.state_updates,\n                     name=\"predict_function\",\n-                    **kwargs\n+                    **kwargs,\n                 )\n \n     def _make_execution_function(self, mode):\n@@ -2711,7 +2707,7 @@ class Model(training_lib.Model):\n             flat_inputs = tf.nest.flatten(x)\n             flat_expected_inputs = tf.nest.flatten(self.inputs)\n             converted_x = []\n-            for (a, b) in zip(flat_inputs, flat_expected_inputs):\n+            for a, b in zip(flat_inputs, flat_expected_inputs):\n                 converted_x.append(_convert_scipy_sparse_tensor(a, b))\n             x = tf.nest.pack_sequence_as(x, converted_x)\n \n@@ -2728,7 +2724,7 @@ class Model(training_lib.Model):\n         flat_expected_inputs = tf.nest.flatten(\n             tf_utils.convert_variables_to_tensors(self.inputs)\n         )\n-        for (a, b) in zip(flat_inputs, flat_expected_inputs):\n+        for a, b in zip(flat_inputs, flat_expected_inputs):\n             tf.nest.assert_same_structure(a, b, expand_composites=True)\n \n         if y is not None:\n\n@@ -82,7 +82,7 @@ class Initializer:\n           **kwargs: Additional keyword arguments.\n         \"\"\"\n         raise NotImplementedError(\n-            \"Initializer subclasses must implement the \" \"`__call__()` method.\"\n+            \"Initializer subclasses must implement the `__call__()` method.\"\n         )\n \n     def get_config(self):\n@@ -573,7 +573,7 @@ class VarianceScaling(Initializer):\n     ):\n         if scale <= 0.0:\n             raise ValueError(\n-                \"`scale` must be positive float. \" f\"Received: scale={scale}.\"\n+                f\"`scale` must be positive float. Received: scale={scale}.\"\n             )\n         allowed_modes = {\"fan_in\", \"fan_out\", \"fan_avg\"}\n         if mode not in allowed_modes:\n\n@@ -134,7 +134,7 @@ def _test_gradients(\n     gradients.\"\"\"\n     if order < 1:\n         raise ValueError(\n-            \"`order` should be a positive integer, got '{}'.\".format(order)\n+            f\"`order` should be a positive integer, got '{order}'.\"\n         )\n     if order > 1:\n         _test_gradients(\n\n@@ -85,7 +85,7 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n                 raise\n \n     def mnist_dataset(self):\n-        path_to_use = \"mnist_{}.npz\".format(str(uuid.uuid4()))\n+        path_to_use = f\"mnist_{str(uuid.uuid4())}.npz\"\n         with self.skip_fetch_failure_exception():\n             (x_train, y_train), _ = tf.keras.datasets.mnist.load_data(\n                 path=path_to_use\n\n@@ -39,11 +39,9 @@ class ParameterServerCustomTrainingLoopTest(tf.test.TestCase):\n         ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n \n         cluster_dict = {}\n-        cluster_dict[\"worker\"] = [\n-            \"localhost:%s\" % port for port in worker_ports\n-        ]\n+        cluster_dict[\"worker\"] = [f\"localhost:{port}\" for port in worker_ports]\n         if num_ps > 0:\n-            cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\n+            cluster_dict[\"ps\"] = [f\"localhost:{port}\" for port in ps_ports]\n \n         cluster_spec = tf.train.ClusterSpec(cluster_dict)\n \n\n@@ -46,9 +46,9 @@ def create_in_process_cluster(num_workers, num_ps):\n     ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n \n     cluster_dict = {}\n-    cluster_dict[\"worker\"] = [\"localhost:%s\" % port for port in worker_ports]\n+    cluster_dict[\"worker\"] = [f\"localhost:{port}\" for port in worker_ports]\n     if num_ps > 0:\n-        cluster_dict[\"ps\"] = [\"localhost:%s\" % port for port in ps_ports]\n+        cluster_dict[\"ps\"] = [f\"localhost:{port}\" for port in ps_ports]\n \n     cluster_spec = tf.train.ClusterSpec(cluster_dict)\n \n\n@@ -125,7 +125,7 @@ class TpuStrategyTest(tf.test.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Trying to run metric.update_state \" \"in replica context\",\n+            \"Trying to run metric.update_state in replica context\",\n         ):\n             with strategy.scope():\n                 for i in dataset:\n\n@@ -118,7 +118,7 @@ def _build_proj_equation(free_dims, bound_dims, output_dims):\n         kernel_str += char\n         output_str += char\n         bias_axes += char\n-    equation = \"%s,%s->%s\" % (input_str, kernel_str, output_str)\n+    equation = f\"{input_str},{kernel_str}->{output_str}\"\n \n     return equation, bias_axes, len(output_str)\n \n@@ -246,7 +246,7 @@ class MultiHeadAttention(Layer):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n-        **kwargs\n+        **kwargs,\n     ):\n         super().__init__(**kwargs)\n         self.supports_masking = True\n@@ -362,7 +362,7 @@ class MultiHeadAttention(Layer):\n                 ),\n                 bias_axes=bias_axes if self._use_bias else None,\n                 name=\"query\",\n-                **self._get_common_kwargs_for_sublayer()\n+                **self._get_common_kwargs_for_sublayer(),\n             )\n             einsum_equation, bias_axes, output_rank = _build_proj_equation(\n                 self._key_shape.rank - 1, bound_dims=1, output_dims=2\n@@ -374,7 +374,7 @@ class MultiHeadAttention(Layer):\n                 ),\n                 bias_axes=bias_axes if self._use_bias else None,\n                 name=\"key\",\n-                **self._get_common_kwargs_for_sublayer()\n+                **self._get_common_kwargs_for_sublayer(),\n             )\n             einsum_equation, bias_axes, output_rank = _build_proj_equation(\n                 self._value_shape.rank - 1, bound_dims=1, output_dims=2\n@@ -386,7 +386,7 @@ class MultiHeadAttention(Layer):\n                 ),\n                 bias_axes=bias_axes if self._use_bias else None,\n                 name=\"value\",\n-                **self._get_common_kwargs_for_sublayer()\n+                **self._get_common_kwargs_for_sublayer(),\n             )\n \n             # Builds the attention computations for multi-head dot product\n@@ -446,7 +446,7 @@ class MultiHeadAttention(Layer):\n             output_shape=_get_output_shape(output_rank - 1, output_shape),\n             bias_axes=bias_axes if self._use_bias else None,\n             name=name,\n-            **common_kwargs\n+            **common_kwargs,\n         )\n \n     def _build_attention(self, rank):\n\n@@ -174,14 +174,14 @@ class Conv(Layer):\n \n         if not all(self.kernel_size):\n             raise ValueError(\n-                \"The argument `kernel_size` cannot contain 0(s). \"\n-                \"Received: %s\" % (self.kernel_size,)\n+                \"The argument `kernel_size` cannot contain 0(s). Received: %s\"\n+                % (self.kernel_size,)\n             )\n \n         if not all(self.strides):\n             raise ValueError(\n-                \"The argument `strides` cannot contains 0(s). \"\n-                \"Received: %s\" % (self.strides,)\n+                \"The argument `strides` cannot contains 0(s). Received: %s\"\n+                % (self.strides,)\n             )\n \n         if self.padding == \"causal\":\n@@ -345,12 +345,12 @@ class Conv(Layer):\n \n         except ValueError:\n             raise ValueError(\n-                f\"One of the dimensions in the output is <= 0 \"\n+                \"One of the dimensions in the output is <= 0 \"\n                 f\"due to downsampling in {self.name}. Consider \"\n-                f\"increasing the input size. \"\n+                \"increasing the input size. \"\n                 f\"Received input shape {input_shape} which would produce \"\n-                f\"output shape with a zero or negative value in a \"\n-                f\"dimension.\"\n+                \"output shape with a zero or negative value in a \"\n+                \"dimension.\"\n             )\n \n     def _recreate_conv_op(self, inputs):\n\n@@ -119,7 +119,7 @@ class Dense(Layer):\n         self.units = int(units) if not isinstance(units, int) else units\n         if self.units < 0:\n             raise ValueError(\n-                f\"Received an invalid value for `units`, expected \"\n+                \"Received an invalid value for `units`, expected \"\n                 f\"a positive integer. Received: units={units}\"\n             )\n         self.activation = activations.get(activation)\n\n@@ -228,9 +228,7 @@ class Lambda(Layer):\n             v for v in created_variables if v.ref() not in tracked_weights\n         ]\n         if untracked_new_vars:\n-            variable_str = \"\\n\".join(\n-                \"  {}\".format(i) for i in untracked_new_vars\n-            )\n+            variable_str = \"\\n\".join(f\"  {i}\" for i in untracked_new_vars)\n             error_str = textwrap.dedent(\n                 \"\"\"\n           The following Variables were created within a Lambda layer ({name})\n@@ -248,9 +246,7 @@ class Lambda(Layer):\n             v for v in accessed_variables if v.ref() not in tracked_weights\n         ]\n         if untracked_used_vars and not self._already_warned:\n-            variable_str = \"\\n\".join(\n-                \"  {}\".format(i) for i in untracked_used_vars\n-            )\n+            variable_str = \"\\n\".join(f\"  {i}\" for i in untracked_used_vars)\n             self._warn(\n                 textwrap.dedent(\n                     \"\"\"\n@@ -316,7 +312,7 @@ class Lambda(Layer):\n             module = None\n         else:\n             raise ValueError(\n-                \"Invalid input for serialization, type: %s \" % type(inputs)\n+                f\"Invalid input for serialization, type: {type(inputs)} \"\n             )\n \n         return output, output_type, module\n@@ -399,7 +395,7 @@ class Lambda(Layer):\n         else:\n             supported_types = [\"function\", \"lambda\", \"raw\"]\n             raise TypeError(\n-                f\"Unsupported value for `function_type` argument. Received: \"\n+                \"Unsupported value for `function_type` argument. Received: \"\n                 f\"function_type={function_type}. \"\n                 f\"Expected one of {supported_types}\"\n             )\n\n@@ -292,9 +292,7 @@ class TFOpLambda(Layer):\n             v for v in created_variables if v.ref() not in tracked_weights\n         ]\n         if untracked_new_vars:\n-            variable_str = \"\\n\".join(\n-                \"  {}\".format(i) for i in untracked_new_vars\n-            )\n+            variable_str = \"\\n\".join(f\"  {i}\" for i in untracked_new_vars)\n             raise ValueError(\n                 \"The following Variables were created within a Lambda layer \"\n                 f\"({self.name}) but are not tracked by said layer: \"\n@@ -311,9 +309,7 @@ class TFOpLambda(Layer):\n             v for v in accessed_variables if v.ref() not in tracked_weights\n         ]\n         if untracked_used_vars and not self._already_warned:\n-            variable_str = \"\\n\".join(\n-                \"  {}\".format(i) for i in untracked_used_vars\n-            )\n+            variable_str = \"\\n\".join(f\"  {i}\" for i in untracked_used_vars)\n             self._warn(\n                 \"The following Variables were used in a Lambda layer's call \"\n                 f\"({self.name}), but are not present in its tracked objects: \"\n\n@@ -171,7 +171,7 @@ class LocallyConnected1D(Layer):\n \n         if input_dim is None:\n             raise ValueError(\n-                \"Axis 2 of input should be fully-defined. \" \"Found shape:\",\n+                \"Axis 2 of input should be fully-defined. Found shape:\",\n                 input_shape,\n             )\n         self.output_length = conv_utils.conv_output_length(\n@@ -180,12 +180,12 @@ class LocallyConnected1D(Layer):\n \n         if self.output_length <= 0:\n             raise ValueError(\n-                f\"One of the dimensions in the output is <= 0 \"\n+                \"One of the dimensions in the output is <= 0 \"\n                 f\"due to downsampling in {self.name}. Consider \"\n-                f\"increasing the input size. \"\n+                \"increasing the input size. \"\n                 f\"Received input shape {input_shape} which would produce \"\n-                f\"output shape with a zero or negative value in a \"\n-                f\"dimension.\"\n+                \"output shape with a zero or negative value in a \"\n+                \"dimension.\"\n             )\n \n         if self.implementation == 1:\n\n@@ -202,12 +202,12 @@ class LocallyConnected2D(Layer):\n \n         if self.output_row <= 0 or self.output_col <= 0:\n             raise ValueError(\n-                f\"One of the dimensions in the output is <= 0 \"\n+                \"One of the dimensions in the output is <= 0 \"\n                 f\"due to downsampling in {self.name}. Consider \"\n-                f\"increasing the input size. \"\n+                \"increasing the input size. \"\n                 f\"Received input shape {input_shape} which would produce \"\n-                f\"output shape with a zero or negative value in a \"\n-                f\"dimension.\"\n+                \"output shape with a zero or negative value in a \"\n+                \"dimension.\"\n             )\n \n         if self.implementation == 1:\n\n@@ -228,7 +228,7 @@ class BatchNormalizationBase(Layer):\n             keys = [\"rmax\", \"rmin\", \"dmax\"]\n             if set(renorm_clipping) - set(keys):\n                 raise ValueError(\n-                    f\"Received invalid keys for `renorm_clipping` argument: \"\n+                    \"Received invalid keys for `renorm_clipping` argument: \"\n                     f\"{renorm_clipping}. Supported values: {keys}.\"\n                 )\n             self.renorm_clipping = renorm_clipping\n@@ -250,8 +250,7 @@ class BatchNormalizationBase(Layer):\n         # when no virtual batch size or adjustment is used.\n         if self.renorm:\n             raise ValueError(\n-                \"Passing both `fused=True` and `renorm=True` is \"\n-                \"not supported\"\n+                \"Passing both `fused=True` and `renorm=True` is not supported\"\n             )\n         axis = [self.axis] if isinstance(self.axis, int) else self.axis\n         # Axis -3 is equivalent to 1, and axis -1 is equivalent to 3, when the\n@@ -328,8 +327,8 @@ class BatchNormalizationBase(Layer):\n         if self.virtual_batch_size is not None:\n             if self.virtual_batch_size <= 0:\n                 raise ValueError(\n-                    f\"`virtual_batch_size` must be a positive integer that \"\n-                    f\"divides the true batch size of the input tensor. \"\n+                    \"`virtual_batch_size` must be a positive integer that \"\n+                    \"divides the true batch size of the input tensor. \"\n                     f\"Received: virtual_batch_size={self.virtual_batch_size}\"\n                 )\n             # If using virtual batches, the first dimension must be the batch\n\n@@ -75,7 +75,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"bucketized|dense|batch_%s\" % batch\n+            name = f\"bucketized|dense|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -79,7 +79,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"hash|dense|batch_%s\" % batch\n+            name = f\"hash|dense|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -79,7 +79,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"hash|varlen|batch_%s\" % batch\n+            name = f\"hash|varlen|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -98,7 +98,7 @@ class BenchmarkLayer(tf.test.TestCase, fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"vocab_list|dense|batch_%s\" % batch\n+            name = f\"vocab_list|dense|batch_{batch}\"\n             k_time, f_time = self.embedding_varlen(\n                 batch_size=batch, max_length=256\n             )\n\n@@ -91,7 +91,7 @@ class BenchmarkLayer(tf.test.TestCase, fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"vocab_list|varlen|batch_%s\" % batch\n+            name = f\"vocab_list|varlen|batch_{batch}\"\n             k_time, f_time = self.embedding_varlen(\n                 batch_size=batch, max_length=256\n             )\n\n@@ -77,7 +77,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"vocab_list|dense|batch_%s\" % batch\n+            name = f\"vocab_list|dense|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -86,7 +86,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"vocab_list_indicator|dense|batch_%s\" % batch\n+            name = f\"vocab_list_indicator|dense|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -86,7 +86,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"vocab_list_indicator|varlen|batch_%s\" % batch\n+            name = f\"vocab_list_indicator|varlen|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -77,7 +77,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"vocab_list|varlen|batch_%s\" % batch\n+            name = f\"vocab_list|varlen|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -76,7 +76,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"embedding|dense|batch_%s\" % batch\n+            name = f\"embedding|dense|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -79,7 +79,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"embedding|varlen|batch_%s\" % batch\n+            name = f\"embedding|varlen|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -80,7 +80,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"hashed_cross|dense|batch_%s\" % batch\n+            name = f\"hashed_cross|dense|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -84,7 +84,7 @@ class BenchmarkLayer(tf.test.Benchmark):\n             ends.append(time.time())\n \n         avg_time = np.mean(np.array(ends) - np.array(starts)) / num_batches\n-        name = \"hashing|batch_%s\" % batch_size\n+        name = f\"hashing|batch_{batch_size}\"\n         baseline = self.run_dataset_implementation(batch_size)\n         extras = {\n             \"dataset implementation baseline\": baseline,\n\n@@ -138,7 +138,7 @@ class BenchmarkLayer(tf.test.Benchmark):\n                 ends.append(time.time())\n \n         avg_time = np.mean(np.array(ends) - np.array(starts)) / count\n-        name = \"image_preprocessing|batch_%s\" % batch_size\n+        name = f\"image_preprocessing|batch_{batch_size}\"\n         baseline = self.run_dataset_implementation(batch_size)\n         extras = {\n             \"dataset implementation baseline\": baseline,\n\n@@ -101,10 +101,7 @@ class BenchmarkAdapt(tf.test.Benchmark):\n             ends.append(time.time())\n \n         avg_time = np.mean(np.array(ends) - np.array(starts))\n-        name = \"normalization_adapt|%s_elements|batch_%s\" % (\n-            num_elements,\n-            batch_size,\n-        )\n+        name = f\"normalization_adapt|{num_elements}_elements|batch_{batch_size}\"\n         baseline = self.run_dataset_implementation(num_elements, batch_size)\n         extras = {\n             \"tf.data implementation baseline\": baseline,\n\n@@ -89,7 +89,7 @@ class BenchmarkLayer(fc_bm.LayerBenchmark):\n \n     def benchmark_layer(self):\n         for batch in BATCH_SIZES:\n-            name = \"weighted_embedding|varlen|batch_%s\" % batch\n+            name = f\"weighted_embedding|varlen|batch_{batch}\"\n             k_time, f_time = embedding_varlen(batch_size=batch, max_length=256)\n             self.report(name, k_time, f_time, NUM_REPEATS)\n \n\n@@ -264,8 +264,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n \n         if sparse and output_mode == INT:\n             raise ValueError(\n-                f\"`sparse` may only be true if `output_mode` is \"\n-                f\"`'one_hot'`, `'multi_hot'`, or `'count'`. \"\n+                \"`sparse` may only be true if `output_mode` is \"\n+                \"`'one_hot'`, `'multi_hot'`, or `'count'`. \"\n                 f\"Received: sparse={sparse} and \"\n                 f\"output_mode={output_mode}\"\n             )\n\n@@ -165,7 +165,7 @@ class Hashing(base_layer.Layer):\n     ):\n         if num_bins is None or num_bins <= 0:\n             raise ValueError(\n-                f\"The `num_bins` for `Hashing` cannot be `None` or \"\n+                \"The `num_bins` for `Hashing` cannot be `None` or \"\n                 f\"non-positive values. Received: num_bins={num_bins}.\"\n             )\n \n@@ -205,8 +205,8 @@ class Hashing(base_layer.Layer):\n \n         if sparse and output_mode == INT:\n             raise ValueError(\n-                f\"`sparse` may only be true if `output_mode` is \"\n-                f'`\"one_hot\"`, `\"multi_hot\"`, or `\"count\"`. '\n+                \"`sparse` may only be true if `output_mode` is \"\n+                '`\"one_hot\"`, `\"multi_hot\"`, or `\"count\"`. '\n                 f\"Received: sparse={sparse} and \"\n                 f\"output_mode={output_mode}\"\n             )\n\n@@ -1265,7 +1265,7 @@ class RandomRotation(BaseImageAugmentationLayer):\n             self.upper = factor\n         if self.upper < self.lower:\n             raise ValueError(\n-                \"Factor cannot have negative values, \" \"got {}\".format(factor)\n+                f\"Factor cannot have negative values, got {factor}\"\n             )\n         check_fill_mode_and_interpolation(fill_mode, interpolation)\n         self.fill_mode = fill_mode\n@@ -1909,8 +1909,7 @@ class RandomHeight(BaseImageAugmentationLayer):\n             )\n         if self.height_lower < -1.0 or self.height_upper < -1.0:\n             raise ValueError(\n-                \"`factor` must have values larger than -1, \"\n-                \"got {}\".format(factor)\n+                f\"`factor` must have values larger than -1, got {factor}\"\n             )\n         self.interpolation = interpolation\n         self._interpolation_method = image_utils.get_interpolation(\n@@ -2033,8 +2032,7 @@ class RandomWidth(BaseImageAugmentationLayer):\n             )\n         if self.width_lower < -1.0 or self.width_upper < -1.0:\n             raise ValueError(\n-                \"`factor` must have values larger than -1, \"\n-                \"got {}\".format(factor)\n+                f\"`factor` must have values larger than -1, got {factor}\"\n             )\n         self.interpolation = interpolation\n         self._interpolation_method = image_utils.get_interpolation(\n\n@@ -179,19 +179,19 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         # are creating a 0-element vocab, which doesn't make sense.\n         if max_tokens is not None and max_tokens <= 1:\n             raise ValueError(\n-                f\"If set, `max_tokens` must be greater than 1. \"\n+                \"If set, `max_tokens` must be greater than 1. \"\n                 f\"Received: max_tokens={max_tokens}\"\n             )\n \n         if pad_to_max_tokens and max_tokens is None:\n             raise ValueError(\n-                f\"If pad_to_max_tokens is True, must set `max_tokens`. \"\n+                \"If pad_to_max_tokens is True, must set `max_tokens`. \"\n                 f\"Received: max_tokens={max_tokens}\"\n             )\n \n         if num_oov_indices < 0:\n             raise ValueError(\n-                f\"`num_oov_indices` must be greater than or equal to 0. \"\n+                \"`num_oov_indices` must be greater than or equal to 0. \"\n                 f\"Received: num_oov_indices={num_oov_indices}\"\n             )\n \n@@ -210,21 +210,21 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n         if invert and output_mode != INT:\n             raise ValueError(\n-                f\"`output_mode` must be `'int'` when `invert` is true. \"\n+                \"`output_mode` must be `'int'` when `invert` is true. \"\n                 f\"Received: output_mode={output_mode}\"\n             )\n \n         if sparse and output_mode == INT:\n             raise ValueError(\n-                f\"`sparse` may only be true if `output_mode` is \"\n-                f\"`'one_hot'`, `'multi_hot'`, `'count'` or `'tf_idf'`. \"\n+                \"`sparse` may only be true if `output_mode` is \"\n+                \"`'one_hot'`, `'multi_hot'`, `'count'` or `'tf_idf'`. \"\n                 f\"Received: sparse={sparse} and \"\n                 f\"output_mode={output_mode}\"\n             )\n \n         if idf_weights is not None and output_mode != TF_IDF:\n             raise ValueError(\n-                f\"`idf_weights` should only be set if `output_mode` is \"\n+                \"`idf_weights` should only be set if `output_mode` is \"\n                 f\"`'tf_idf'`. Received: idf_weights={idf_weights} and \"\n                 f\"output_mode={output_mode}\"\n             )\n@@ -462,7 +462,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         \"\"\"\n         if self.output_mode != TF_IDF and idf_weights is not None:\n             raise ValueError(\n-                f\"`idf_weights` should only be set if output_mode is \"\n+                \"`idf_weights` should only be set if output_mode is \"\n                 f\"`'tf_idf'`. Received: output_mode={self.output_mode} \"\n                 f\"and idf_weights={idf_weights}\"\n             )\n@@ -470,7 +470,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         if isinstance(vocabulary, str):\n             if not tf.io.gfile.exists(vocabulary):\n                 raise ValueError(\n-                    \"Vocabulary file {} does not exist.\".format(vocabulary)\n+                    f\"Vocabulary file {vocabulary} does not exist.\"\n                 )\n             if self.output_mode == TF_IDF:\n                 raise ValueError(\n@@ -504,9 +504,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n         if vocabulary.size == 0:\n             raise ValueError(\n-                \"Cannot set an empty vocabulary, you passed {}.\".format(\n-                    vocabulary\n-                )\n+                f\"Cannot set an empty vocabulary, you passed {vocabulary}.\"\n             )\n \n         oov_start = self._oov_start_index()\n\n@@ -375,14 +375,14 @@ class IntegerLookup(index_lookup.IndexLookup):\n         # are creating a 0-element vocab, which doesn't make sense.\n         if max_tokens is not None and max_tokens <= 1:\n             raise ValueError(\n-                f\"If `max_tokens` is set for `IntegerLookup`, it must be \"\n+                \"If `max_tokens` is set for `IntegerLookup`, it must be \"\n                 f\"greater than 1. Received: max_tokens={max_tokens}.\"\n             )\n \n         if num_oov_indices < 0:\n             raise ValueError(\n-                f\"The value of `num_oov_indices` argument for `IntegerLookup` \"\n-                f\"must >= 0. Received num_oov_indices=\"\n+                \"The value of `num_oov_indices` argument for `IntegerLookup` \"\n+                \"must >= 0. Received num_oov_indices=\"\n                 f\"{num_oov_indices}.\"\n             )\n \n\n@@ -199,11 +199,9 @@ class NormalizationTest(\n     def test_list_input(self):\n         with self.assertRaisesRegex(\n             ValueError,\n-            (\n             \"Normalization only accepts a single input. If you are \"\n             \"passing a python list or tuple as a single input, \"\n-                \"please convert to a numpy array or `tf.Tensor`.\"\n-            ),\n+            \"please convert to a numpy array or `tf.Tensor`.\",\n         ):\n             normalization.Normalization()([1, 2, 3])\n \n\n@@ -53,7 +53,7 @@ class PreprocessingStage(\n             data, (tf.data.Dataset, np.ndarray, tf.__internal__.EagerTensor)\n         ):\n             raise ValueError(\n-                f\"`adapt()` requires a batched Dataset, an EagerTensor, or a \"\n+                \"`adapt()` requires a batched Dataset, an EagerTensor, or a \"\n                 f\"Numpy array as input. Received data={data}\"\n             )\n         if isinstance(data, tf.data.Dataset):\n\n@@ -46,7 +46,7 @@ class PreprocessingLayerTest(tf.test.TestCase):\n             self.assertEqual(len(a), len(b))\n             for key, a_value in a.items():\n                 b_value = b[key]\n-                error_message = \"{} ({})\".format(msg, key) if msg else None\n+                error_message = f\"{msg} ({key})\" if msg else None\n                 self.assertAllCloseOrEqual(a_value, b_value, error_message)\n         elif (\n             isinstance(a, float)\n@@ -71,7 +71,7 @@ class PreprocessingLayerTest(tf.test.TestCase):\n         identical.\"\"\"\n         if len(data) < 4:\n             raise AssertionError(\n-                f\"Data must have at least 4 elements. Received \"\n+                \"Data must have at least 4 elements. Received \"\n                 f\"len(data)={len(data)}.\"\n             )\n         data_0 = np.array([data[0]])\n@@ -104,8 +104,10 @@ class PreprocessingLayerTest(tf.test.TestCase):\n         self.compare_accumulators(\n             all_merge,\n             unordered_all_merge,\n-            msg=\"The order of merge arguments should not change the data \"\n-            \"output.\",\n+            msg=(\n+                \"The order of merge arguments should not change the data \"\n+                \"output.\"\n+            ),\n         )\n \n         hierarchical_merge = combiner.merge(\n@@ -140,8 +142,10 @@ class PreprocessingLayerTest(tf.test.TestCase):\n         self.compare_accumulators(\n             all_merge,\n             mixed_compute,\n-            msg=\"Mixing merge and compute calls should not change the data \"\n-            \"output.\",\n+            msg=(\n+                \"Mixing merge and compute calls should not change the data \"\n+                \"output.\"\n+            ),\n         )\n \n         single_merge = combiner.merge(\n@@ -153,15 +157,16 @@ class PreprocessingLayerTest(tf.test.TestCase):\n         self.compare_accumulators(\n             all_merge,\n             single_merge,\n-            msg=\"Calling merge with a data length of 1 should not change \"\n-            \"the data output.\",\n+            msg=(\n+                \"Calling merge with a data length of 1 should not change \"\n+                \"the data output.\"\n+            ),\n         )\n \n         self.compare_accumulators(\n             expected,\n             all_merge,\n-            msg=\"Calculated accumulators \"\n-            \"did not match expected accumulator.\",\n+            msg=\"Calculated accumulators did not match expected accumulator.\",\n         )\n \n     def validate_accumulator_extract(self, combiner, data, expected):\n\n@@ -118,7 +118,7 @@ def encode_categorical_inputs(\n     # TODO(b/190445202): remove output rank restriction.\n     if inputs.shape.rank > 2:\n         raise ValueError(\n-            f\"When output_mode is not `'int'`, maximum supported output rank \"\n+            \"When output_mode is not `'int'`, maximum supported output rank \"\n             f\"is 2. Received output_mode {output_mode} and input shape \"\n             f\"{original_shape}, \"\n             f\"which would result in output rank {inputs.shape.rank}.\"\n@@ -139,7 +139,7 @@ def encode_categorical_inputs(\n \n     if idf_weights is None:\n         raise ValueError(\n-            f\"When output mode is `'tf_idf'`, idf_weights must be provided. \"\n+            \"When output mode is `'tf_idf'`, idf_weights must be provided. \"\n             f\"Received: output_mode={output_mode} and idf_weights={idf_weights}\"\n         )\n \n\n@@ -265,7 +265,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n         # a dtype of 'string'.\n         if \"dtype\" in kwargs and kwargs[\"dtype\"] != tf.string:\n             raise ValueError(\n-                f\"`TextVectorization` may only have a dtype of string. \"\n+                \"`TextVectorization` may only have a dtype of string. \"\n                 f\"Received dtype: {kwargs['dtype']}.\"\n             )\n         elif \"dtype\" not in kwargs:\n@@ -319,7 +319,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n             and all(isinstance(item, int) for item in ngrams)\n         ):\n             raise ValueError(\n-                f\"`ngrams` must be None, an integer, or a tuple of \"\n+                \"`ngrams` must be None, an integer, or a tuple of \"\n                 f\"integers. Received: ngrams={ngrams}\"\n             )\n \n@@ -330,28 +330,28 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n             or (output_sequence_length is None)\n         ):\n             raise ValueError(\n-                f\"`output_sequence_length` must be either None or an \"\n-                f\"integer when `output_mode` is 'int'. Received: \"\n+                \"`output_sequence_length` must be either None or an \"\n+                \"integer when `output_mode` is 'int'. Received: \"\n                 f\"output_sequence_length={output_sequence_length}\"\n             )\n \n         if output_mode != INT and output_sequence_length is not None:\n             raise ValueError(\n-                f\"`output_sequence_length` must not be set if `output_mode` is \"\n-                f\"not 'int'. \"\n+                \"`output_sequence_length` must not be set if `output_mode` is \"\n+                \"not 'int'. \"\n                 f\"Received output_sequence_length={output_sequence_length}.\"\n             )\n \n         if ragged and output_mode != INT:\n             raise ValueError(\n-                f\"`ragged` must not be true if `output_mode` is \"\n+                \"`ragged` must not be true if `output_mode` is \"\n                 f\"`'int'`. Received: ragged={ragged} and \"\n                 f\"output_mode={output_mode}\"\n             )\n \n         if ragged and output_sequence_length is not None:\n             raise ValueError(\n-                f\"`output_sequence_length` must not be set if ragged \"\n+                \"`output_sequence_length` must not be set if ragged \"\n                 f\"is True. Received: ragged={ragged} and \"\n                 f\"output_sequence_length={output_sequence_length}\"\n             )\n@@ -585,11 +585,9 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n                 inputs = self._split(inputs)\n             else:\n                 raise ValueError(\n-                    (\n                     \"%s is not a supported splitting.\"\n                     \"TextVectorization supports the following options \"\n                     \"for `split`: None, 'whitespace', or a Callable.\"\n-                    )\n                     % self._split\n                 )\n \n\n@@ -82,7 +82,7 @@ class Dropout(base_layer.BaseRandomLayer):\n         if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n             raise ValueError(\n                 f\"Invalid value {rate} received for \"\n-                f\"`rate`, expected a value between 0 and 1.\"\n+                \"`rate`, expected a value between 0 and 1.\"\n             )\n         self.rate = rate\n         self.noise_shape = noise_shape\n\n@@ -65,7 +65,7 @@ class SpatialDropout2D(Dropout):\n             data_format = backend.image_data_format()\n         if data_format not in {\"channels_last\", \"channels_first\"}:\n             raise ValueError(\n-                f'`data_format` must be \"channels_last\" or \"channels_first\". '\n+                '`data_format` must be \"channels_last\" or \"channels_first\". '\n                 f\"Received: data_format={data_format}.\"\n             )\n         self.data_format = data_format\n\n@@ -65,7 +65,7 @@ class SpatialDropout3D(Dropout):\n             data_format = backend.image_data_format()\n         if data_format not in {\"channels_last\", \"channels_first\"}:\n             raise ValueError(\n-                f'`data_format` must be \"channels_last\" or \"channels_first\". '\n+                '`data_format` must be \"channels_last\" or \"channels_first\". '\n                 f\"Received: data_format={data_format}.\"\n             )\n         self.data_format = data_format\n\n@@ -91,8 +91,7 @@ class Cropping3D(Layer):\n         elif hasattr(cropping, \"__len__\"):\n             if len(cropping) != 3:\n                 raise ValueError(\n-                    \"`cropping` should have 3 elements. \"\n-                    f\"Received: {cropping}.\"\n+                    f\"`cropping` should have 3 elements. Received: {cropping}.\"\n                 )\n             dim1_cropping = conv_utils.normalize_tuple(\n                 cropping[0], 2, \"1st entry of cropping\", allow_zero=True\n\n@@ -103,7 +103,7 @@ class Reshape(Layer):\n                     unknown = index\n                 else:\n                     raise ValueError(\n-                        f\"There must be at most one unknown dimension in \"\n+                        \"There must be at most one unknown dimension in \"\n                         f\"output_shape. Received: output_shape={output_shape}.\"\n                     )\n             else:\n\n@@ -101,8 +101,7 @@ class ZeroPadding2D(Layer):\n         elif hasattr(padding, \"__len__\"):\n             if len(padding) != 2:\n                 raise ValueError(\n-                    \"`padding` should have two elements. \"\n-                    f\"Received: {padding}.\"\n+                    f\"`padding` should have two elements. Received: {padding}.\"\n                 )\n             height_padding = conv_utils.normalize_tuple(\n                 padding[0], 2, \"1st entry of padding\", allow_zero=True\n\n@@ -92,7 +92,7 @@ class ZeroPadding3D(Layer):\n         elif hasattr(padding, \"__len__\"):\n             if len(padding) != 3:\n                 raise ValueError(\n-                    \"`padding` should have 3 elements. \" f\"Received: {padding}.\"\n+                    f\"`padding` should have 3 elements. Received: {padding}.\"\n                 )\n             dim1_padding = conv_utils.normalize_tuple(\n                 padding[0], 2, \"1st entry of padding\", allow_zero=True\n\n@@ -188,9 +188,9 @@ class Bidirectional(Wrapper):\n             raise ValueError(\n                 \"Forward layer and backward layer should have different \"\n                 \"`go_backwards` value.\"\n-                f\"forward_layer.go_backwards = \"\n+                \"forward_layer.go_backwards = \"\n                 f\"{self.forward_layer.go_backwards},\"\n-                f\"backward_layer.go_backwards = \"\n+                \"backward_layer.go_backwards = \"\n                 f\"{self.backward_layer.go_backwards}\"\n             )\n \n\n@@ -663,8 +663,9 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_Bidirectional_last_output_with_masking(self):\n         rnn = keras.layers.LSTM\n@@ -696,8 +697,9 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     @parameterized.parameters([keras.layers.LSTM, keras.layers.GRU])\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_Bidirectional_sequence_output_with_masking(self, rnn):\n         samples = 2\n@@ -925,8 +927,9 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n     @parameterized.parameters([\"ave\", \"concat\", \"mul\"])\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm RNN does not support ragged \"\n-        \"tensors yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm RNN does not support ragged tensors yet.\"\n+        ),\n     )\n     def test_Bidirectional_ragged_input(self, merge_mode):\n         np.random.seed(100)\n\n@@ -263,9 +263,9 @@ class DropoutWrapper(_RNNCellWrapper):\n                             f\"Parameter {attr} must be between 0 and 1. \"\n                             f\"Received {const_prob}\"\n                         )\n-                    setattr(self, \"_%s\" % attr, float(const_prob))\n+                    setattr(self, f\"_{attr}\", float(const_prob))\n                 else:\n-                    setattr(self, \"_%s\" % attr, tensor_prob)\n+                    setattr(self, f\"_{attr}\", tensor_prob)\n \n         # Set variational_recurrent, seed before running the code below\n         self._variational_recurrent = variational_recurrent\n\n@@ -137,7 +137,7 @@ class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n     ):\n         if units < 0:\n             raise ValueError(\n-                f\"Received an invalid value for argument `units`, \"\n+                \"Received an invalid value for argument `units`, \"\n                 f\"expected a positive integer, got {units}.\"\n             )\n         # By default use cached variable under v2 mode, see b/143699808.\n\n@@ -213,8 +213,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_with_masking_layer_GRU(self):\n         layer_class = keras.layers.GRU\n@@ -232,8 +233,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_masking_with_stacking_GRU(self):\n         inputs = np.random.random((2, 3, 4))\n@@ -283,8 +285,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_return_states_GRU(self):\n         layer_class = keras.layers.GRU\n@@ -370,8 +373,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_statefulness_GRU(self):\n         num_samples = 2\n@@ -481,8 +485,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask(self):\n@@ -630,8 +635,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_GRU_runtime_with_mask(self):\n\n@@ -40,8 +40,9 @@ _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n class GRUGraphRewriteTest(test_combinations.TestCase):\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_gru_feature_parity_v1_v2(self):\n@@ -143,8 +144,9 @@ class GRUGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask_v1(self):\n\n@@ -298,9 +298,9 @@ class DropoutWrapper(_RNNCellWrapperV1):\n                             f\"Parameter {attr} must be between 0 and 1. \"\n                             f\"Received {const_prob}\"\n                         )\n-                    setattr(self, \"_%s\" % attr, float(const_prob))\n+                    setattr(self, f\"_{attr}\", float(const_prob))\n                 else:\n-                    setattr(self, \"_%s\" % attr, tensor_prob)\n+                    setattr(self, f\"_{attr}\", tensor_prob)\n \n         # Set variational_recurrent, seed before running the code below\n         self._variational_recurrent = variational_recurrent\n\n@@ -265,7 +265,7 @@ class RNNCell(base_layer.Layer):\n                 if inputs.shape.dims[0].value != static_batch_size:\n                     raise ValueError(\n                         \"batch size from input tensor is different from the \"\n-                        f\"input param. Input tensor batch: \"\n+                        \"input param. Input tensor batch: \"\n                         f\"{inputs.shape.dims[0].value}, \"\n                         f\"batch_size: {batch_size}\"\n                     )\n@@ -575,12 +575,12 @@ class GRUCell(LayerRNNCell):\n         _check_supported_dtypes(self.dtype)\n         input_depth = inputs_shape[-1]\n         self._gate_kernel = self.add_weight(\n-            \"gates/%s\" % _WEIGHTS_VARIABLE_NAME,\n+            f\"gates/{_WEIGHTS_VARIABLE_NAME}\",\n             shape=[input_depth + self._num_units, 2 * self._num_units],\n             initializer=self._kernel_initializer,\n         )\n         self._gate_bias = self.add_weight(\n-            \"gates/%s\" % _BIAS_VARIABLE_NAME,\n+            f\"gates/{_BIAS_VARIABLE_NAME}\",\n             shape=[2 * self._num_units],\n             initializer=(\n                 self._bias_initializer\n@@ -589,12 +589,12 @@ class GRUCell(LayerRNNCell):\n             ),\n         )\n         self._candidate_kernel = self.add_weight(\n-            \"candidate/%s\" % _WEIGHTS_VARIABLE_NAME,\n+            f\"candidate/{_WEIGHTS_VARIABLE_NAME}\",\n             shape=[input_depth + self._num_units, self._num_units],\n             initializer=self._kernel_initializer,\n         )\n         self._candidate_bias = self.add_weight(\n-            \"candidate/%s\" % _BIAS_VARIABLE_NAME,\n+            f\"candidate/{_BIAS_VARIABLE_NAME}\",\n             shape=[self._num_units],\n             initializer=(\n                 self._bias_initializer\n@@ -1075,7 +1075,7 @@ class LSTMCell(LayerRNNCell):\n                 else None\n             )\n             self._proj_kernel = self.add_weight(\n-                \"projection/%s\" % _WEIGHTS_VARIABLE_NAME,\n+                f\"projection/{_WEIGHTS_VARIABLE_NAME}\",\n                 shape=[self._num_units, self._num_proj],\n                 initializer=self._initializer,\n                 partitioner=maybe_proj_partitioner,\n@@ -1311,7 +1311,7 @@ class MultiRNNCell(RNNCell):\n                 if self._state_is_tuple:\n                     if not tf.nest.is_nested(state):\n                         raise ValueError(\n-                            f\"Expected state to be a tuple of length \"\n+                            \"Expected state to be a tuple of length \"\n                             f\"{len(self.state_size)}\"\n                             f\", but received: {state}\"\n                         )\n\n@@ -141,7 +141,7 @@ class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n     ):\n         if units < 0:\n             raise ValueError(\n-                f\"Received an invalid value for argument `units`, \"\n+                \"Received an invalid value for argument `units`, \"\n                 f\"expected a positive integer, got {units}.\"\n             )\n         # By default use cached variable under v2 mode, see b/143699808.\n\n@@ -265,8 +265,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_return_state(self):\n         num_states = 2\n@@ -349,8 +350,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n     @parameterized.named_parameters((\"v0\", 0), (\"v1\", 1), (\"v2\", 2))\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_implementation_mode_LSTM(self, implementation_mode):\n         num_samples = 2\n@@ -396,8 +398,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_masking_with_stacking_LSTM(self):\n         inputs = np.random.random((2, 3, 4))\n@@ -532,8 +535,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     def test_statefulness_LSTM(self):\n         num_samples = 2\n@@ -680,8 +684,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask(self):\n@@ -834,8 +839,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_LSTM_runtime_with_mask(self):\n\n@@ -44,8 +44,9 @@ _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n class LSTMGraphRewriteTest(test_combinations.TestCase):\n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_lstm_feature_parity_v1_v2(self):\n@@ -172,8 +173,9 @@ class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n     @tf.test.disable_with_predicate(\n         pred=tf.test.is_built_with_rocm,\n-        skip_message=\"Skipping as ROCm MIOpen does not support padded \"\n-        \"input yet.\",\n+        skip_message=(\n+            \"Skipping as ROCm MIOpen does not support padded input yet.\"\n+        ),\n     )\n     @test_utils.run_v2_only\n     def test_explicit_device_with_go_backward_and_mask_v1(self):\n\n@@ -123,7 +123,7 @@ class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n     ):\n         if units < 0:\n             raise ValueError(\n-                f\"Received an invalid value for argument `units`, \"\n+                \"Received an invalid value for argument `units`, \"\n                 f\"expected a positive integer, got {units}.\"\n             )\n         # By default use cached variable under v2 mode, see b/143699808.\n\n@@ -219,7 +219,7 @@ def _float64_op():\n     inputs = keras.Input(shape=(10,))\n     x = keras.layers.Dense(10, dtype=\"float64\")(inputs)\n     x = tf.nn.relu(x)\n-    assert x.dtype == \"float64\", \"x has dtype: %s\" % x.dtype\n+    assert x.dtype == \"float64\", f\"x has dtype: {x.dtype}\"\n     outputs = keras.layers.Dense(10)(x)\n     return keras.Model(inputs, outputs)\n \n\n@@ -382,19 +382,19 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n                 core_layers.dense(inputs, 2, name=\"my_dense\")\n                 var_dict = _get_variable_dict_from_varstore()\n                 var_key = \"test/my_dense/kernel\"\n-                self.assertEqual(var_dict[var_key].name, \"%s:0\" % var_key)\n+                self.assertEqual(var_dict[var_key].name, f\"{var_key}:0\")\n             with tf.compat.v1.variable_scope(\"test1\") as scope:\n                 inputs = tf.random.uniform((5, 3), seed=1)\n                 core_layers.dense(inputs, 2, name=scope)\n                 var_dict = _get_variable_dict_from_varstore()\n                 var_key = \"test1/kernel\"\n-                self.assertEqual(var_dict[var_key].name, \"%s:0\" % var_key)\n+                self.assertEqual(var_dict[var_key].name, f\"{var_key}:0\")\n             with tf.compat.v1.variable_scope(\"test2\"):\n                 inputs = tf.random.uniform((5, 3), seed=1)\n                 core_layers.dense(inputs, 2)\n                 var_dict = _get_variable_dict_from_varstore()\n                 var_key = \"test2/dense/kernel\"\n-                self.assertEqual(var_dict[var_key].name, \"%s:0\" % var_key)\n+                self.assertEqual(var_dict[var_key].name, f\"{var_key}:0\")\n \n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n\n@@ -51,7 +51,7 @@ class DeterministicRandomTestTool(object):\n         if mode not in {\"constant\", \"num_random_ops\"}:\n             raise ValueError(\n                 \"Mode arg must be 'constant' or 'num_random_ops'. \"\n-                + \"Got: {}\".format(mode)\n+                + f\"Got: {mode}\"\n             )\n         self.seed_implementation = sys.modules[tf.compat.v1.get_seed.__module__]\n         self._mode = mode\n@@ -91,7 +91,7 @@ class DeterministicRandomTestTool(object):\n                     raise ValueError(\n                         \"This `DeterministicRandomTestTool` \"\n                         \"object is trying to re-use the \"\n-                        + \"already-used operation seed {}. \".format(op_seed)\n+                        + f\"already-used operation seed {op_seed}. \"\n                         + \"It cannot guarantee random numbers will match \"\n                         + \"between eager and sessions when an operation seed \"\n                         + \"is reused. You most likely set \"\n\n@@ -209,7 +209,7 @@ class BNTest(tf.test.TestCase):\n         )\n \n         checkpoint_path_a = os.path.join(\n-            self.get_temp_dir(), \"checkpoint_a_%s\" % base_path\n+            self.get_temp_dir(), f\"checkpoint_a_{base_path}\"\n         )\n         self._train(\n             checkpoint_path_a,\n@@ -220,7 +220,7 @@ class BNTest(tf.test.TestCase):\n             freeze_mode=freeze_mode,\n         )\n         checkpoint_path_b = os.path.join(\n-            self.get_temp_dir(), \"checkpoint_b_%s\" % base_path\n+            self.get_temp_dir(), f\"checkpoint_b_{base_path}\"\n         )\n         self._train(\n             checkpoint_path_b,\n\n@@ -64,9 +64,7 @@ def _has_kwargs(fn):\n         fn = fn.__call__\n     elif not callable(fn):\n         raise TypeError(\n-            \"fn should be a function-like object, but is of type {}.\".format(\n-                type(fn)\n-            )\n+            f\"fn should be a function-like object, but is of type {type(fn)}.\"\n         )\n     return tf_inspect.getfullargspec(fn).varkw is not None\n \n@@ -291,8 +289,7 @@ class _EagerVariableStore(tf.Module):\n         \"\"\"\n         if custom_getter is not None and not callable(custom_getter):\n             raise ValueError(\n-                \"Passed a custom_getter which is not callable: %s\"\n-                % custom_getter\n+                f\"Passed a custom_getter which is not callable: {custom_getter}\"\n             )\n \n         with tf.init_scope():\n@@ -352,7 +349,7 @@ class _EagerVariableStore(tf.Module):\n                 )\n \n             # Single variable case\n-            if \"%s/part_0\" % name in self._vars:\n+            if f\"{name}/part_0\" in self._vars:\n                 raise ValueError(\n                     \"No partitioner was provided, but a partitioned version of \"\n                     \"the variable was found: %s/part_0. Perhaps a variable of \"\n\n@@ -97,7 +97,7 @@ class VariableScopeTest(tf.test.TestCase):\n         vs = variable_scope._get_default_variable_store()\n         vs.get_variable(\"v1\", [2])\n         vs.get_variable(\"v2\", [2])\n-        expected_names = [\"%s:0\" % name for name in [\"v1\", \"v2\"]]\n+        expected_names = [f\"{name}:0\" for name in [\"v1\", \"v2\"]]\n         self.assertEqual(\n             set(expected_names), set(v.name for v in vs._vars.values())\n         )\n@@ -174,7 +174,7 @@ class VariableScopeTest(tf.test.TestCase):\n         ]\n \n         # Use different variable_name to distinguish various dtypes\n-        for (i, dtype) in enumerate(types):\n+        for i, dtype in enumerate(types):\n             x = tf.compat.v1.get_variable(\n                 name=\"xx%d\" % i, shape=(3, 4), dtype=dtype\n             )\n\n@@ -1970,7 +1970,7 @@ def categorical_crossentropy(\n     \"\"\"\n     if isinstance(axis, bool):\n         raise ValueError(\n-            f\"`axis` must be of type `int`. \"\n+            \"`axis` must be of type `int`. \"\n             f\"Received: axis={axis} of type {type(axis)}\"\n         )\n     y_pred = tf.convert_to_tensor(y_pred)\n\n@@ -816,7 +816,7 @@ class MeanTensor(Metric):\n         elif values.shape != self._shape:\n             raise ValueError(\n                 \"MeanTensor input values must always have the same \"\n-                f\"shape. Expected shape (set during the first call): \"\n+                \"shape. Expected shape (set during the first call): \"\n                 f\"{self._shape}. \"\n                 f\"Got: {values.shape}.\"\n             )\n\n@@ -1650,7 +1650,7 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n             result = auc_obj(labels, logits)\n             self.assertEqual(self.evaluate(result), 0.5)\n         except ImportError as e:\n-            tf_logging.warning(\"Cannot test special functions: %s\" % str(e))\n+            tf_logging.warning(f\"Cannot test special functions: {str(e)}\")\n \n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n\n@@ -1747,7 +1747,7 @@ class AUC(base_metric.Metric):\n             summation_method, metrics_utils.AUCSummationMethod\n         ) and summation_method not in list(metrics_utils.AUCSummationMethod):\n             raise ValueError(\n-                f\"Invalid `summation_method` \"\n+                \"Invalid `summation_method` \"\n                 f'argument value \"{summation_method}\". '\n                 f\"Expected one of: {list(metrics_utils.AUCSummationMethod)}\"\n             )\n@@ -2845,7 +2845,7 @@ class IoU(_IoUBase):\n         if max(target_class_ids) >= num_classes:\n             raise ValueError(\n                 f\"Target class id {max(target_class_ids)} \"\n-                f\"is out of range, which is \"\n+                \"is out of range, which is \"\n                 f\"[{0}, {num_classes}).\"\n             )\n         self.target_class_ids = list(target_class_ids)\n\n@@ -72,13 +72,13 @@ class AutoCastVariable(tf.Variable, tf.__internal__.types.Tensor):\n         \"\"\"\n         if not isinstance(variable, tf.Variable):\n             raise ValueError(\n-                \"variable must be of type tf.ResourceVariable, but got: \"\n-                \"%s\" % variable\n+                \"variable must be of type tf.ResourceVariable, but got: %s\"\n+                % variable\n             )\n         if not variable.dtype.is_floating:\n             raise ValueError(\n-                \"variable must be a floating point variable but has \"\n-                \"type: %s\" % variable.dtype.name\n+                \"variable must be a floating point variable but has type: %s\"\n+                % variable.dtype.name\n             )\n         self._variable = variable\n         # 'delegate' means AutoCastVariable.op return self._variable.op, which\n\n@@ -77,7 +77,7 @@ def _log_device_compatibility_check(policy_name, gpu_details_list):\n         name = details.get(\"device_name\", \"Unknown GPU\")\n         cc = details.get(\"compute_capability\")\n         if cc:\n-            device_str = \"%s, compute capability %s.%s\" % (name, cc[0], cc[1])\n+            device_str = f\"{name}, compute capability {cc[0]}.{cc[1]}\"\n             if cc >= (7, 0):\n                 supported_device_strs.append(device_str)\n             else:\n\n@@ -340,9 +340,9 @@ class LossScaleOptimizerMetaclass(type):\n \n         # Raise TypeError because inner_optimizer is not an optimizer\n         msg = (\n-            f'\"inner_optimizer\" must be an instance of '\n-            f\"`tf.keras.optimizers.Optimizer` or \"\n-            f\"`tf.keras.optimizers.experimental.Optimizer`, but got: \"\n+            '\"inner_optimizer\" must be an instance of '\n+            \"`tf.keras.optimizers.Optimizer` or \"\n+            \"`tf.keras.optimizers.experimental.Optimizer`, but got: \"\n             f\"{inner_optimizer}.\"\n         )\n         if isinstance(inner_optimizer, legacy_optimizer.OptimizerV2):\n@@ -607,16 +607,16 @@ class LossScaleOptimizer(\n                 # Give better error message if the new experimental optimizer is\n                 # passed.\n                 raise TypeError(\n-                    f\"You passed an instance of the new experimental \"\n-                    f\"optimizer, `optimizer_experimental.Optimizer`, \"\n-                    f\"to LossScaleOptimizer, but \"\n-                    f\"only the classic optimizers subclassing from \"\n-                    f\"`tf.keras.optimizers.Optimizer` can be passed. Please \"\n-                    f\"use `loss_scale_optimizer.LossScaleOptimizerV3` \"\n-                    f\"instead of \"\n-                    f\"`tf.keras.mixed_precision.LossScaleOptimizer`, \"\n-                    f\"as the former supports wrapping \"\n-                    f\"instances of the new experimental optimizer. \"\n+                    \"You passed an instance of the new experimental \"\n+                    \"optimizer, `optimizer_experimental.Optimizer`, \"\n+                    \"to LossScaleOptimizer, but \"\n+                    \"only the classic optimizers subclassing from \"\n+                    \"`tf.keras.optimizers.Optimizer` can be passed. Please \"\n+                    \"use `loss_scale_optimizer.LossScaleOptimizerV3` \"\n+                    \"instead of \"\n+                    \"`tf.keras.mixed_precision.LossScaleOptimizer`, \"\n+                    \"as the former supports wrapping \"\n+                    \"instances of the new experimental optimizer. \"\n                     f\"Got optimizer: {inner_optimizer}\"\n                 )\n             msg = (\n@@ -679,7 +679,7 @@ class LossScaleOptimizer(\n         else:\n             if initial_scale is None:\n                 raise ValueError(\n-                    '\"initial_scale\" must be specified if \"dynamic\" is ' \"False\"\n+                    '\"initial_scale\" must be specified if \"dynamic\" is False'\n                 )\n             self._loss_scale = float(initial_scale)\n             if dynamic_growth_steps is not None:\n@@ -1125,18 +1125,18 @@ class LossScaleOptimizerV3(\n                 # Give better error message if the OptimizerV2 class is passed\n                 # instead of the new experimental optimizer.\n                 raise TypeError(\n-                    f\"You passed a `tf.keras.optimizer.Optimizer` instance to \"\n-                    f\"LossScaleOptimizerV3, but only the new experimental \"\n-                    f\"optimizer defined in \"\n-                    f\"keras/optimizer_expeirmental/optimizer.py can be \"\n-                    f\"passed. Please use \"\n-                    f\"`tf.keras.mixed_precision.LossScaleOptimizer` \"\n-                    f\"instead of LossScaleOptimizerV3, as the former supports \"\n-                    f\"`tf.keras.optimizer.Optimizer`s. Got optimizer: \"\n+                    \"You passed a `tf.keras.optimizer.Optimizer` instance to \"\n+                    \"LossScaleOptimizerV3, but only the new experimental \"\n+                    \"optimizer defined in \"\n+                    \"keras/optimizer_expeirmental/optimizer.py can be \"\n+                    \"passed. Please use \"\n+                    \"`tf.keras.mixed_precision.LossScaleOptimizer` \"\n+                    \"instead of LossScaleOptimizerV3, as the former supports \"\n+                    \"`tf.keras.optimizer.Optimizer`s. Got optimizer: \"\n                     f\"{inner_optimizer}\"\n                 )\n             raise TypeError(\n-                f'\"inner_optimizer\" must be an instance of '\n+                '\"inner_optimizer\" must be an instance of '\n                 f\"Optimizer, but got: {inner_optimizer}.\"\n             )\n         if not isinstance(dynamic, bool):\n@@ -1144,12 +1144,12 @@ class LossScaleOptimizerV3(\n             # second argument argument, as this was commonly done for the\n             # now-removed LossScaleOptimizerV1.\n             raise TypeError(\n-                f'\"dynamic\" argument to LossScaleOptimizer.__init__ must '\n+                '\"dynamic\" argument to LossScaleOptimizer.__init__ must '\n                 f\"be a bool, but got: {repr(dynamic)}\"\n             )\n         if isinstance(inner_optimizer, LossScaleOptimizerV3):\n             raise TypeError(\n-                f\"LossScaleOptimizer cannot wrap another \"\n+                \"LossScaleOptimizer cannot wrap another \"\n                 f\"LossScaleOptimizer, but got: {inner_optimizer}\"\n             )\n         _raise_if_strategy_unsupported()\n@@ -1186,12 +1186,12 @@ class LossScaleOptimizerV3(\n         else:\n             if initial_scale is None:\n                 raise ValueError(\n-                    '\"initial_scale\" must be specified if \"dynamic\" is ' \"False\"\n+                    '\"initial_scale\" must be specified if \"dynamic\" is False'\n                 )\n             self._loss_scale = float(initial_scale)\n             if dynamic_growth_steps is not None:\n                 raise ValueError(\n-                    f'\"dynamic_growth_steps\" must be None if \"dynamic\" '\n+                    '\"dynamic_growth_steps\" must be None if \"dynamic\" '\n                     f\"is False, but got: {dynamic_growth_steps}\"\n                 )\n \n@@ -1482,8 +1482,8 @@ def _create_loss_scale_optimizer_from_v1_loss_scale(optimizer, loss_scale):\n     elif isinstance(loss_scale, tf.compat.v1.mixed_precision.DynamicLossScale):\n         if loss_scale.multiplier != 2:\n             raise ValueError(\n-                f'When passing a DynamicLossScale to \"loss_scale\", '\n-                f\"DynamicLossScale.multiplier must be 2. Got: \"\n+                'When passing a DynamicLossScale to \"loss_scale\", '\n+                \"DynamicLossScale.multiplier must be 2. Got: \"\n                 f\"{loss_scale}\"\n             )\n         return LossScaleOptimizer(\n@@ -1493,14 +1493,14 @@ def _create_loss_scale_optimizer_from_v1_loss_scale(optimizer, loss_scale):\n         )\n     elif isinstance(loss_scale, tf.compat.v1.mixed_precision.LossScale):\n         raise TypeError(\n-            f\"Passing a LossScale that is not a FixedLossScale or a \"\n+            \"Passing a LossScale that is not a FixedLossScale or a \"\n             f\"DynamicLossScale is not supported. Got: {loss_scale}\"\n         )\n     else:\n         raise ValueError(\n-            f\"Invalid value passed to loss_scale. loss_scale \"\n-            f'must be the string \"dynamic\" (recommended), an int, '\n-            f\"a float, a FixedLossScale, or a DynamicLossScale. Got \"\n+            \"Invalid value passed to loss_scale. loss_scale \"\n+            'must be the string \"dynamic\" (recommended), an int, '\n+            \"a float, a FixedLossScale, or a DynamicLossScale. Got \"\n             f\"value: {loss_scale}\"\n         )\n \n@@ -1573,8 +1573,8 @@ def _raise_if_strategy_unsupported():\n             )\n         else:\n             raise ValueError(\n-                f\"Loss scaling is not supported with the \"\n-                f\"tf.distribute.Strategy: \"\n+                \"Loss scaling is not supported with the \"\n+                \"tf.distribute.Strategy: \"\n                 f\"{strategy.__class__.__name__}. Try using a different \"\n-                f\"Strategy, e.g. a MirroredStrategy\"\n+                \"Strategy, e.g. a MirroredStrategy\"\n             )\n\n@@ -147,7 +147,7 @@ class MixedPrecisionTest(test_combinations.TestCase):\n         opt = loss_scale_optimizer_v2.LossScaleOptimizer(opt)\n         with self.assertRaisesRegex(\n             ValueError,\n-            '\"opt\" must not already be an instance of a ' \"LossScaleOptimizer.\",\n+            '\"opt\" must not already be an instance of a LossScaleOptimizer.',\n         ):\n             tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n                 opt\n\n@@ -75,16 +75,14 @@ class KerasModelTest(test_combinations.TestCase):\n             and test_utils.get_model_type() == \"subclass\"\n         ):\n             self.skipTest(\n-                \"Non-default strategies are unsupported with subclassed \"\n-                \"models\"\n+                \"Non-default strategies are unsupported with subclassed models\"\n             )\n \n     def _skip_if_save_format_unsupported(self, save_format):\n         model_type = test_utils.get_model_type()\n         if save_format == \"h5\" and model_type == \"subclass\":\n             self.skipTest(\n-                \"Saving subclassed models with the HDF5 format is \"\n-                \"unsupported\"\n+                \"Saving subclassed models with the HDF5 format is unsupported\"\n             )\n         if (\n             save_format == \"tf\"\n@@ -92,8 +90,7 @@ class KerasModelTest(test_combinations.TestCase):\n             and not tf.executing_eagerly()\n         ):\n             self.skipTest(\n-                \"b/148820505: This combination of features is currently \"\n-                \"broken.\"\n+                \"b/148820505: This combination of features is currently broken.\"\n             )\n \n     @test_combinations.run_with_all_model_types\n\n@@ -194,7 +194,7 @@ class Policy:\n                 \"Instead, pass DType.name. Got: %s\" % (name.name,)\n             )\n         elif not isinstance(name, str):\n-            raise TypeError(\"'name' must be a string, but got: %s\" % (name,))\n+            raise TypeError(f\"'name' must be a string, but got: {name}\")\n         self._name = name\n         self._compute_dtype, self._variable_dtype = self._parse_name(name)\n         if name in (\"mixed_float16\", \"mixed_bloat16\"):\n@@ -223,7 +223,7 @@ class Policy:\n                 error_msg += \" Please use the 'mixed_float16' policy instead.\"\n             elif name == \"bfloat16_with_float32_vars\":\n                 error_msg += \" Please use the 'mixed_bfloat16' policy instead.\"\n-            error_msg += \" Got policy name: '%s'\" % name\n+            error_msg += f\" Got policy name: '{name}'\"\n             raise ValueError(error_msg)\n \n         if name == \"mixed_float16\":\n@@ -306,7 +306,7 @@ class Policy:\n         return self._name\n \n     def __repr__(self):\n-        return '<Policy \"%s\">' % self._name\n+        return f'<Policy \"{self._name}\">'\n \n     def get_config(self):\n         return {\"name\": self.name}\n\n@@ -61,7 +61,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n             \"_infer\",\n         ):\n             self.assertEqual(\n-                repr(mp_policy.Policy(policy)), '<Policy \"%s\">' % policy\n+                repr(mp_policy.Policy(policy)), f'<Policy \"{policy}\">'\n             )\n \n     @test_utils.enable_v2_dtype_behavior\n\n@@ -49,10 +49,7 @@ def create_identity_with_grad_check_fn(expected_gradient, expected_dtype=None):\n             if expected_dtype:\n                 assert (\n                     dx.dtype == expected_dtype\n-                ), \"dx.dtype should be %s but is: %s\" % (\n-                    expected_dtype,\n-                    dx.dtype,\n-                )\n+                ), f\"dx.dtype should be {expected_dtype} but is: {dx.dtype}\"\n             expected_tensor = tf.convert_to_tensor(\n                 expected_gradient, dtype=dx.dtype, name=\"expected_gradient\"\n             )\n@@ -143,7 +140,7 @@ class MultiplyLayer(AssertTypeLayer):\n         activity_regularizer=None,\n         use_operator=False,\n         var_name=\"v\",\n-        **kwargs\n+        **kwargs,\n     ):\n         \"\"\"Initializes the MultiplyLayer.\n \n\n@@ -77,7 +77,7 @@ class SharpnessAwareMinimization(Model):\n \n         gradients_all_batches = []\n         pred_all_batches = []\n-        for (x_batch, y_batch) in zip(x_split, y_split):\n+        for x_batch, y_batch in zip(x_split, y_split):\n             epsilon_w_cache = []\n             with tf.GradientTape() as tape:\n                 pred = self.model(x_batch)\n@@ -89,7 +89,7 @@ class SharpnessAwareMinimization(Model):\n             gradients_order2_norm = self._gradients_order2_norm(gradients)\n             scale = self.rho / (gradients_order2_norm + 1e-12)\n \n-            for (gradient, variable) in zip(gradients, trainable_variables):\n+            for gradient, variable in zip(gradients, trainable_variables):\n                 epsilon_w = gradient * scale\n                 self._distributed_apply_epsilon_w(\n                     variable, epsilon_w, tf.distribute.get_strategy()\n@@ -104,11 +104,11 @@ class SharpnessAwareMinimization(Model):\n                 for gradient in gradients:\n                     gradients_all_batches.append([gradient])\n             else:\n-                for (gradient, gradient_all_batches) in zip(\n+                for gradient, gradient_all_batches in zip(\n                     gradients, gradients_all_batches\n                 ):\n                     gradient_all_batches.append(gradient)\n-            for (variable, epsilon_w) in zip(\n+            for variable, epsilon_w in zip(\n                 trainable_variables, epsilon_w_cache\n             ):\n                 # Restore the variable to its original value before\n\n@@ -180,7 +180,7 @@ def piecewise_constant(x, boundaries, values, name=None):\n     for v in values[1:]:\n         if v.dtype.base_dtype != values[0].dtype.base_dtype:\n             raise ValueError(\n-                f\"`values` must have elements all with the same dtype \"\n+                \"`values` must have elements all with the same dtype \"\n                 f\"({values[0].dtype.base_dtype} vs {v.dtype.base_dtype}).\"\n             )\n     decayed_lr = learning_rate_schedule.PiecewiseConstantDecay(\n\n@@ -128,7 +128,7 @@ class Ftrl(optimizer.Optimizer):\n         if initial_accumulator_value < 0.0:\n             raise ValueError(\n                 \"`initial_accumulator_value` needs to be positive or zero. \"\n-                f\"Received: initial_accumulator_value=\"\n+                \"Received: initial_accumulator_value=\"\n                 f\"{initial_accumulator_value}.\"\n             )\n         if learning_rate_power > 0.0:\n@@ -139,13 +139,13 @@ class Ftrl(optimizer.Optimizer):\n         if l1_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l1_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l1_regularization_strength=\"\n+                \"Received: l1_regularization_strength=\"\n                 f\"{l1_regularization_strength}.\"\n             )\n         if l2_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l2_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l2_regularization_strength=\"\n+                \"Received: l2_regularization_strength=\"\n                 f\"{l2_regularization_strength}.\"\n             )\n         if l2_shrinkage_regularization_strength < 0.0:\n\n@@ -70,7 +70,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             ):\n                 raise ValueError(\n                     \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n-                    f\"Received: ema_overwrite_frequency=\"\n+                    \"Received: ema_overwrite_frequency=\"\n                     f\"{ema_overwrite_frequency}\"\n                 )\n         self.ema_momentum = ema_momentum\n@@ -78,7 +78,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n \n         if self.clipnorm is not None and self.global_clipnorm is not None:\n             raise ValueError(\n-                f\"At most one of `clipnorm` and `global_clipnorm` can \"\n+                \"At most one of `clipnorm` and `global_clipnorm` can \"\n                 f\"be set. Received: clipnorm={self.clipnorm}, \"\n                 f\"global_clipnorm={self.global_clipnorm}.\"\n             )\n@@ -195,8 +195,8 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         if self._var_key(variable) not in self._index_dict:\n             raise KeyError(\n                 f\"The optimizer cannot recognize variable {variable.name}. \"\n-                f\"This usually means that you're reusing an optimizer \"\n-                f\"previously created for a different model. Try creating a \"\n+                \"This usually means that you're reusing an optimizer \"\n+                \"previously created for a different model. Try creating a \"\n                 \"new optimizer instance.\"\n             )\n         self.update_step(gradient, variable)\n@@ -549,7 +549,7 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n     def _update_model_variables_moving_average(self, var_list):\n         \"\"\"Update the stored moving average using the latest value.\"\"\"\n         if self.use_ema:\n-            for (var, average) in zip(\n+            for var, average in zip(\n                 var_list, self._model_variables_moving_average\n             ):\n                 average.assign(\n@@ -561,10 +561,10 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n         if len(var_list) != len(self._model_variables_moving_average):\n             raise ValueError(\n                 f\"The length of model variables ({len(var_list)}) to \"\n-                f\"override does not match the length of model variables \"\n-                f\"stored in the optimizer \"\n+                \"override does not match the length of model variables \"\n+                \"stored in the optimizer \"\n                 f\"({len(self._model_variables_moving_average)}). Please \"\n-                f\"check if the optimizer was called on your model.\"\n+                \"check if the optimizer was called on your model.\"\n             )\n         self._overwrite_model_variables_with_average_value_helper(var_list)\n \n@@ -975,7 +975,7 @@ class Optimizer(_BaseOptimizer):\n                     self.ema_momentum * average + (1 - self.ema_momentum) * var\n                 )\n \n-            for (var, average) in zip(\n+            for var, average in zip(\n                 var_list, self._model_variables_moving_average\n             ):\n                 self._distribution_strategy.extended.update(\n\n@@ -43,8 +43,7 @@ class Optimizer:\n         for k in kwargs:\n             if k not in allowed_kwargs:\n                 raise TypeError(\n-                    \"Unexpected keyword argument \"\n-                    \"passed to optimizer: \" + str(k)\n+                    \"Unexpected keyword argument passed to optimizer: \" + str(k)\n                 )\n             # checks that clipnorm >= 0 and clipvalue >= 0\n             if kwargs[k] < 0:\n@@ -123,8 +122,9 @@ class Optimizer:\n             raise ValueError(\n                 \"Length of the specified weight list (\"\n                 + str(len(weights))\n-                + \") does not match the number of weights \"\n-                \"of the optimizer (\" + str(len(params)) + \")\"\n+                + \") does not match the number of weights of the optimizer (\"\n+                + str(len(params))\n+                + \")\"\n             )\n         weight_value_tuples = []\n         param_values = backend.batch_get_value(params)\n@@ -133,8 +133,8 @@ class Optimizer:\n                 raise ValueError(\n                     \"Optimizer weight shape \"\n                     + str(pv.shape)\n-                    + \" not compatible with \"\n-                    \"provided weight shape \" + str(w.shape)\n+                    + \" not compatible with provided weight shape \"\n+                    + str(w.shape)\n                 )\n             weight_value_tuples.append((p, w))\n         backend.batch_set_value(weight_value_tuples)\n\n@@ -132,13 +132,13 @@ class Ftrl(optimizer_v2.OptimizerV2):\n         if l1_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l1_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l1_regularization_strength=\"\n+                \"Received: l1_regularization_strength=\"\n                 f\"{l1_regularization_strength}.\"\n             )\n         if l2_regularization_strength < 0.0:\n             raise ValueError(\n                 \"`l2_regularization_strength` needs to be positive or zero. \"\n-                f\"Received: l2_regularization_strength=\"\n+                \"Received: l2_regularization_strength=\"\n                 f\"{l2_regularization_strength}.\"\n             )\n         if l2_shrinkage_regularization_strength < 0.0:\n\n@@ -123,7 +123,7 @@ class SGD(optimizer_v2.OptimizerV2):\n             momentum < 0 or momentum > 1\n         ):\n             raise ValueError(\n-                f\"`momentum` must be between [0, 1]. Received: \"\n+                \"`momentum` must be between [0, 1]. Received: \"\n                 f\"momentum={momentum} (of type {type(momentum)}).\"\n             )\n         self._set_hyper(\"momentum\", momentum)\n\n@@ -753,7 +753,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n             \"\"\"Apply gradient to variable.\"\"\"\n             if isinstance(var, tf.Tensor):\n                 raise NotImplementedError(\n-                    f\"Updating a `Tensor` is not implemented. \"\n+                    \"Updating a `Tensor` is not implemented. \"\n                     f\"Received: var={var}.\"\n                 )\n \n@@ -1420,7 +1420,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n           An `Operation` which updates the value of the variable.\n         \"\"\"\n         raise NotImplementedError(\n-            \"`_resource_apply_dense` must be implemented in \" \"subclasses.\"\n+            \"`_resource_apply_dense` must be implemented in subclasses.\"\n         )\n \n     def _resource_apply_sparse_duplicate_indices(\n@@ -1474,7 +1474,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n           An `Operation` which updates the value of the variable.\n         \"\"\"\n         raise NotImplementedError(\n-            \"`_resource_apply_sparse` Must be implemented in \" \"subclasses.\"\n+            \"`_resource_apply_sparse` Must be implemented in subclasses.\"\n         )\n \n     def _resource_scatter_add(self, x, i, v):\n\n@@ -153,7 +153,7 @@ class RMSprop(optimizer_v2.OptimizerV2):\n             momentum < 0 or momentum > 1\n         ):\n             raise ValueError(\n-                f\"`momentum` must be between [0, 1]. Received: \"\n+                \"`momentum` must be between [0, 1]. Received: \"\n                 f\"momentum={momentum} (of type {type(momentum)}).\"\n             )\n         self._set_hyper(\"momentum\", momentum)\n\n@@ -80,11 +80,9 @@ def filter_empty_gradients(grads_and_vars):\n         )\n     if vars_with_empty_grads:\n         logging.warning(\n-            (\n             \"Gradients do not exist for variables %s when minimizing the \"\n             \"loss. If you're using `model.compile()`, did you forget to \"\n-                \"provide a `loss` argument?\"\n-            ),\n+            \"provide a `loss` argument?\",\n             ([v.name for v in vars_with_empty_grads]),\n         )\n     return filtered\n\n@@ -764,13 +764,13 @@ class NumpyArrayIterator(Iterator):\n         channels_axis = 3 if data_format == \"channels_last\" else 1\n         if self.x.shape[channels_axis] not in {1, 3, 4}:\n             warnings.warn(\n-                \"NumpyArrayIterator is set to use the \"\n-                'data format convention \"' + data_format + '\" '\n-                \"(channels on axis \"\n+                'NumpyArrayIterator is set to use the data format convention \"'\n+                + data_format\n+                + '\" (channels on axis '\n                 + str(channels_axis)\n-                + \"), i.e. expected either 1, 3, or 4 \"\n-                \"channels on axis \" + str(channels_axis) + \". \"\n-                \"However, it was passed an array with shape \"\n+                + \"), i.e. expected either 1, 3, or 4 channels on axis \"\n+                + str(channels_axis)\n+                + \". However, it was passed an array with shape \"\n                 + str(self.x.shape)\n                 + \" (\"\n                 + str(self.x.shape[channels_axis])\n@@ -1028,7 +1028,7 @@ class DataFrameIterator(BatchFromFilesMixin, Iterator):\n         # check that filenames/filepaths column values are all strings\n         if not all(df[x_col].apply(lambda x: isinstance(x, str))):\n             raise TypeError(\n-                \"All values in column x_col={} must be strings.\".format(x_col)\n+                f\"All values in column x_col={x_col} must be strings.\"\n             )\n         # check labels are string if class_mode is binary or sparse\n         if self.class_mode in {\"binary\", \"sparse\"}:\n@@ -1075,9 +1075,7 @@ class DataFrameIterator(BatchFromFilesMixin, Iterator):\n             )\n         # check that if weight column that the values are numerical\n         if weight_col and not issubclass(df[weight_col].dtype.type, np.number):\n-            raise TypeError(\n-                \"Column weight_col={} must be numeric.\".format(weight_col)\n-            )\n+            raise TypeError(f\"Column weight_col={weight_col} must be numeric.\")\n \n     def get_classes(self, df, y_col):\n         labels = []\n@@ -2087,8 +2085,8 @@ class ImageDataGenerator:\n         x = np.asarray(x, dtype=self.dtype)\n         if x.ndim != 4:\n             raise ValueError(\n-                \"Input to `.fit()` should have rank 4. \"\n-                \"Got array with shape: \" + str(x.shape)\n+                \"Input to `.fit()` should have rank 4. Got array with shape: \"\n+                + str(x.shape)\n             )\n         if x.shape[self.channel_axis] not in {1, 3, 4}:\n             warnings.warn(\n@@ -2097,11 +2095,9 @@ class ImageDataGenerator:\n                 + self.data_format\n                 + '\" (channels on axis '\n                 + str(self.channel_axis)\n-                + \"), i.e. expected \"\n-                \"either 1, 3 or 4 channels on axis \"\n+                + \"), i.e. expected either 1, 3 or 4 channels on axis \"\n                 + str(self.channel_axis)\n-                + \". \"\n-                \"However, it was passed an array with shape \"\n+                + \". However, it was passed an array with shape \"\n                 + str(x.shape)\n                 + \" (\"\n                 + str(x.shape[self.channel_axis])\n@@ -2347,8 +2343,8 @@ def random_zoom(\n     \"\"\"\n     if len(zoom_range) != 2:\n         raise ValueError(\n-            \"`zoom_range` should be a tuple or list of two\"\n-            \" floats. Received: %s\" % (zoom_range,)\n+            \"`zoom_range` should be a tuple or list of two floats. Received: %s\"\n+            % (zoom_range,)\n         )\n \n     if zoom_range[0] == 1 and zoom_range[1] == 1:\n@@ -2425,7 +2421,7 @@ def apply_brightness_shift(x, brightness, scale=True):\n     \"\"\"\n     if ImageEnhance is None:\n         raise ImportError(\n-            \"Using brightness shifts requires PIL. \" \"Install PIL or Pillow.\"\n+            \"Using brightness shifts requires PIL. Install PIL or Pillow.\"\n         )\n     x_min, x_max = np.min(x), np.max(x)\n     local_scale = (x_min < 0) or (x_max > 255)\n@@ -2527,16 +2523,14 @@ def apply_affine_transform(\n         ImportError: if SciPy is not available.\n     \"\"\"\n     if scipy is None:\n-        raise ImportError(\n-            \"Image transformations require SciPy. \" \"Install SciPy.\"\n-        )\n+        raise ImportError(\"Image transformations require SciPy. Install SciPy.\")\n \n     # Input sanity checks:\n     # 1. x must 2D image with one or more channels (i.e., a 3D tensor)\n     # 2. channels must be either first or last dimension\n     if np.unique([row_axis, col_axis, channel_axis]).size != 3:\n         raise ValueError(\n-            \"'row_axis', 'col_axis', and 'channel_axis'\" \" must be distinct\"\n+            \"'row_axis', 'col_axis', and 'channel_axis' must be distinct\"\n         )\n \n     # shall we support negative indices?\n\n@@ -203,7 +203,7 @@ class TestImage(test_combinations.TestCase):\n         # create folders and subfolders\n         paths = []\n         for cl in range(num_classes):\n-            class_directory = \"class-{}\".format(cl)\n+            class_directory = f\"class-{cl}\"\n             classpaths = [\n                 class_directory,\n                 os.path.join(class_directory, \"subfolder-1\"),\n@@ -225,7 +225,7 @@ class TestImage(test_combinations.TestCase):\n                 classpaths = paths[im_class]\n                 filename = os.path.join(\n                     classpaths[count % len(classpaths)],\n-                    \"image-{}.jpg\".format(count),\n+                    f\"image-{count}.jpg\",\n                 )\n                 filenames.append(filename)\n                 im.save(os.path.join(temp_dir, filename))\n@@ -294,7 +294,7 @@ class TestImage(test_combinations.TestCase):\n         # create folders and subfolders\n         paths = []\n         for cl in range(num_classes):\n-            class_directory = \"class-{}\".format(cl)\n+            class_directory = f\"class-{cl}\"\n             classpaths = [\n                 class_directory,\n                 os.path.join(class_directory, \"subfolder-1\"),\n@@ -316,7 +316,7 @@ class TestImage(test_combinations.TestCase):\n                 classpaths = paths[im_class]\n                 filename = os.path.join(\n                     classpaths[count % len(classpaths)],\n-                    \"image-{}.jpg\".format(count),\n+                    f\"image-{count}.jpg\",\n                 )\n                 filenames.append(filename)\n                 im.save(os.path.join(tmp_folder, filename))\n@@ -426,7 +426,7 @@ class TestDirectoryIterator(test_combinations.TestCase):\n         # create folders and subfolders\n         paths = []\n         for cl in range(num_classes):\n-            class_directory = \"class-{}\".format(cl)\n+            class_directory = f\"class-{cl}\"\n             classpaths = [\n                 class_directory,\n                 os.path.join(class_directory, \"subfolder-1\"),\n@@ -448,7 +448,7 @@ class TestDirectoryIterator(test_combinations.TestCase):\n                 classpaths = paths[im_class]\n                 filename = os.path.join(\n                     classpaths[count % len(classpaths)],\n-                    \"image-{}.png\".format(count),\n+                    f\"image-{count}.png\",\n                 )\n                 filenames.append(filename)\n                 im.save(os.path.join(tmpdir.full_path, filename))\n@@ -509,9 +509,7 @@ class TestDirectoryIterator(test_combinations.TestCase):\n         count = 0\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = os.path.join(\n-                    tmpdir, \"class-1\", \"image-{}.png\".format(count)\n-                )\n+                filename = os.path.join(tmpdir, \"class-1\", f\"image-{count}.png\")\n                 im.save(filename)\n                 count += 1\n \n@@ -549,7 +547,7 @@ class TestDirectoryIterator(test_combinations.TestCase):\n         # create folders and subfolders\n         paths = []\n         for cl in range(num_classes):\n-            class_directory = \"class-{}\".format(cl)\n+            class_directory = f\"class-{cl}\"\n             classpaths = [\n                 class_directory,\n                 os.path.join(class_directory, \"subfolder-1\"),\n@@ -571,7 +569,7 @@ class TestDirectoryIterator(test_combinations.TestCase):\n                 classpaths = paths[im_class]\n                 filename = os.path.join(\n                     classpaths[count % len(classpaths)],\n-                    \"image-{}.png\".format(count),\n+                    f\"image-{count}.png\",\n                 )\n                 filenames.append(filename)\n                 im.save(os.path.join(tmpdir.full_path, filename))\n@@ -856,8 +854,8 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames_without = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n-                filename_without = \"image-{}\".format(count)\n+                filename = f\"image-{count}.png\"\n+                filename_without = f\"image-{count}\"\n                 filenames.append(filename)\n                 filepaths.append(os.path.join(tmpdir.full_path, filename))\n                 filenames_without.append(filename_without)\n@@ -954,7 +952,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n@@ -977,7 +975,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n@@ -1021,7 +1019,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n@@ -1071,7 +1069,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         count = 0\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n@@ -1126,7 +1124,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         count = 0\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n@@ -1205,7 +1203,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         count = 0\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n@@ -1264,8 +1262,8 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames_without = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n-                filename_without = \"image-{}\".format(count)\n+                filename = f\"image-{count}.png\"\n+                filename_without = f\"image-{count}\"\n                 filenames.append(filename)\n                 filenames_without.append(filename_without)\n                 im.save(os.path.join(tmpdir.full_path, filename))\n@@ -1315,7 +1313,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 filenames.append(filename)\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 count += 1\n@@ -1364,7 +1362,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 filenames.append(filename)\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 count += 1\n@@ -1401,7 +1399,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         file_paths = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{:0>5}.png\".format(count)\n+                filename = f\"image-{count:0>5}.png\"\n                 file_path = os.path.join(tmpdir.full_path, filename)\n                 file_paths.append(file_path)\n                 im.save(file_path)\n@@ -1490,7 +1488,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         # create folders and subfolders\n         paths = []\n         for cl in range(num_classes):\n-            class_directory = \"class-{}\".format(cl)\n+            class_directory = f\"class-{cl}\"\n             classpaths = [\n                 class_directory,\n                 os.path.join(class_directory, \"subfolder-1\"),\n@@ -1512,7 +1510,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n                 classpaths = paths[im_class]\n                 filename = os.path.join(\n                     classpaths[count % len(classpaths)],\n-                    \"image-{}.png\".format(count),\n+                    f\"image-{count}.png\",\n                 )\n                 filenames.append(filename)\n                 im.save(os.path.join(tmpdir.full_path, filename))\n@@ -1541,7 +1539,7 @@ class TestDataFrameIterator(test_combinations.TestCase):\n         filenames = []\n         for test_images in all_test_images:\n             for im in test_images:\n-                filename = \"image-{}.png\".format(count)\n+                filename = f\"image-{count}.png\"\n                 im.save(os.path.join(tmpdir.full_path, filename))\n                 filenames.append(filename)\n                 count += 1\n\n@@ -136,9 +136,9 @@ class TimeseriesGenerator(data_utils.Sequence):\n \n         if len(data) != len(targets):\n             raise ValueError(\n-                \"Data and targets have to be\" + \" of same length. \"\n-                \"Data length is {}\".format(len(data))\n-                + \" while target length is {}\".format(len(targets))\n+                \"Data and targets have to be\"\n+                + f\" of same length. Data length is {len(data)}\"\n+                + f\" while target length is {len(targets)}\"\n             )\n \n         self.data = data\n\n@@ -491,7 +491,7 @@ class Tokenizer(object):\n \n         if mode == \"tfidf\" and not self.document_count:\n             raise ValueError(\n-                \"Fit the Tokenizer on some data \" \"before using tfidf mode.\"\n+                \"Fit the Tokenizer on some data before using tfidf mode.\"\n             )\n \n         x = np.zeros((len(sequences), num_words))\n\n@@ -815,7 +815,7 @@ def load_weights_from_hdf5_group(f, model):\n     layer_names = filtered_layer_names\n     if len(layer_names) != len(filtered_layers):\n         raise ValueError(\n-            f\"Layer count mismatch when loading weights from file. \"\n+            \"Layer count mismatch when loading weights from file. \"\n             f\"Model expected {len(filtered_layers)} layers, found \"\n             f\"{len(layer_names)} saved layers.\"\n         )\n@@ -849,8 +849,8 @@ def load_weights_from_hdf5_group(f, model):\n         )\n         if len(weight_values) != len(symbolic_weights):\n             raise ValueError(\n-                f\"Weight count mismatch for top-level weights when loading \"\n-                f\"weights from file. \"\n+                \"Weight count mismatch for top-level weights when loading \"\n+                \"weights from file. \"\n                 f\"Model expects {len(symbolic_weights)} top-level weight(s). \"\n                 f\"Received {len(weight_values)} saved top-level weight(s)\"\n             )\n@@ -937,7 +937,7 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                             f\"{layer.name}) due to mismatch in shape for \"\n                             f\"weight {symbolic_weights[i].name}. \"\n                             f\"Weight expects shape {expected_shape}. \"\n-                            f\"Received saved weight \"\n+                            \"Received saved weight \"\n                             f\"with shape {received_shape}\"\n                         )\n                         continue\n@@ -945,7 +945,7 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                         f\"Shape mismatch in layer #{k} (named {layer.name}) \"\n                         f\"for weight {symbolic_weights[i].name}. \"\n                         f\"Weight expects shape {expected_shape}. \"\n-                        f\"Received saved weight \"\n+                        \"Received saved weight \"\n                         f\"with shape {received_shape}\"\n                     )\n                 else:\n@@ -964,17 +964,17 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n         if len(weight_values) != len(symbolic_weights):\n             if skip_mismatch:\n                 logging.warning(\n-                    f\"Skipping loading top-level weights for model due to \"\n-                    f\"mismatch in number of weights. \"\n+                    \"Skipping loading top-level weights for model due to \"\n+                    \"mismatch in number of weights. \"\n                     f\"Model expects {len(symbolic_weights)} \"\n-                    f\"top-level weight(s). \"\n+                    \"top-level weight(s). \"\n                     f\"Received {len(weight_values)} saved top-level weight(s)\"\n                 )\n             else:\n                 raise ValueError(\n-                    f\"Weight count mismatch for top-level weights of model. \"\n+                    \"Weight count mismatch for top-level weights of model. \"\n                     f\"Model expects {len(symbolic_weights)} \"\n-                    f\"top-level weight(s). \"\n+                    \"top-level weight(s). \"\n                     f\"Received {len(weight_values)} saved top-level weight(s)\"\n                 )\n         else:\n@@ -984,19 +984,19 @@ def load_weights_from_hdf5_group_by_name(f, model, skip_mismatch=False):\n                 if expected_shape != received_shape:\n                     if skip_mismatch:\n                         logging.warning(\n-                            f\"Skipping loading top-level weight for model due \"\n-                            f\"to mismatch in shape for \"\n+                            \"Skipping loading top-level weight for model due \"\n+                            \"to mismatch in shape for \"\n                             f\"weight {symbolic_weights[i].name}. \"\n                             f\"Weight expects shape {expected_shape}. \"\n-                            f\"Received saved weight \"\n+                            \"Received saved weight \"\n                             f\"with shape {received_shape}\"\n                         )\n                     else:\n                         raise ValueError(\n-                            f\"Shape mismatch in model for top-level weight \"\n+                            \"Shape mismatch in model for top-level weight \"\n                             f\"{symbolic_weights[i].name}. \"\n                             f\"Weight expects shape {expected_shape}. \"\n-                            f\"Received saved weight \"\n+                            \"Received saved weight \"\n                             f\"with shape {received_shape}\"\n                         )\n                 else:\n@@ -1109,8 +1109,8 @@ def _legacy_weights(layer):\n     weights = layer.trainable_weights + layer.non_trainable_weights\n     if any(not isinstance(w, tf.Variable) for w in weights):\n         raise NotImplementedError(\n-            f\"Save or restore weights that is not an instance of `tf.Variable` \"\n-            f\"is not supported in h5, use `save_format='tf'` instead. Received \"\n+            \"Save or restore weights that is not an instance of `tf.Variable` \"\n+            \"is not supported in h5, use `save_format='tf'` instead. Received \"\n             f\"a model or layer {layer.__class__.__name__} \"\n             f\"with weights {weights}\"\n         )\n\n@@ -57,7 +57,7 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n         if h5py is not None:\n             self.assertTrue(\n                 h5py.is_hdf5(path),\n-                \"Model saved at path {} is not a valid hdf5 file.\".format(path),\n+                f\"Model saved at path {path} is not a valid hdf5 file.\",\n             )\n \n     def assert_saved_model(self, path):\n\n@@ -333,8 +333,10 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n             )\n             with self.assertRaises(\n                 ValueError,\n-                msg=\"Weight count mismatch for layer #0 (named d1). \"\n-                \"Layer expects 1 weight(s). Received 2 saved weight(s)\",\n+                msg=(\n+                    \"Weight count mismatch for layer #0 (named d1). \"\n+                    \"Layer expects 1 weight(s). Received 2 saved weight(s)\"\n+                ),\n             ):\n                 hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n \n@@ -388,9 +390,11 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n             )\n             with self.assertRaises(\n                 ValueError,\n-                msg=\"Shape mismatch in layer #0 (named d1) for weight \"\n+                msg=(\n+                    \"Shape mismatch in layer #0 (named d1) for weight \"\n                     \"d1_1/kernel:0. Weight expects shape (3, 10). \"\n-                \"Received saved weight with shape (3, 5)\",\n+                    \"Received saved weight with shape (3, 5)\"\n+                ),\n             ):\n                 hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model)\n \n\n@@ -249,9 +249,7 @@ def _generate_object_paths(object_graph_def):\n         for reference in object_graph_def.nodes[current_node].children:\n             if reference.node_id in paths:\n                 continue\n-            paths[reference.node_id] = \"{}.{}\".format(\n-                current_path, reference.local_name\n-            )\n+            paths[reference.node_id] = f\"{current_path}.{reference.local_name}\"\n             nodes_to_visit.append(reference.node_id)\n \n     return paths\n@@ -384,7 +382,7 @@ class KerasObjectLoader:\n                     )\n                     children.append((metric, reference.node_id, metric_path))\n \n-        for (obj_child, child_id, child_name) in children:\n+        for obj_child, child_id, child_name in children:\n             child_proto = self._proto.nodes[child_id]\n \n             if not isinstance(obj_child, tf.__internal__.tracking.Trackable):\n@@ -428,7 +426,7 @@ class KerasObjectLoader:\n             ):\n                 setter = lambda *args: None\n \n-            child_path = \"{}.{}\".format(parent_path, child_name)\n+            child_path = f\"{parent_path}.{child_name}\"\n             self._node_paths[child_id] = child_path\n             self._add_children_recreated_from_config(\n                 obj_child, child_proto, child_id\n@@ -598,7 +596,7 @@ class KerasObjectLoader:\n             if builtin_layer:\n                 raise RuntimeError(\n                     f\"Unable to restore object of class '{class_name}' likely \"\n-                    f\"due to name conflict with built-in Keras class \"\n+                    \"due to name conflict with built-in Keras class \"\n                     f\"'{builtin_layer}'. To override the built-in Keras \"\n                     \"definition of the object, decorate your class with \"\n                     \"`@keras.utils.register_keras_serializable` and include \"\n@@ -760,8 +758,8 @@ class KerasObjectLoader:\n                 for model_id in uninitialized_model_ids\n             ]\n             raise ValueError(\n-                f\"Error loading model(s) in the SavedModel format. \"\n-                f\"The following model(s) could not be initialized: \"\n+                \"Error loading model(s) in the SavedModel format. \"\n+                \"The following model(s) could not be initialized: \"\n                 f\"{uninitialized_model_names}\"\n             )\n \n@@ -1136,9 +1134,9 @@ def revive_custom_object(identifier, metadata):\n     else:\n         raise ValueError(\n             f\"Unable to restore custom object of type {identifier}. \"\n-            f\"Please make sure that any custom layers are included in the \"\n-            f\"`custom_objects` arg when calling `load_model()` and make sure \"\n-            f\"that all layers implement `get_config` and `from_config`.\"\n+            \"Please make sure that any custom layers are included in the \"\n+            \"`custom_objects` arg when calling `load_model()` and make sure \"\n+            \"that all layers implement `get_config` and `from_config`.\"\n         )\n \n \n@@ -1285,7 +1283,7 @@ def recursively_deserialize_keras_object(config, module_objects=None):\n         ]\n     else:\n         raise ValueError(\n-            f\"Unable to decode Keras layer config. Config should be a \"\n+            \"Unable to decode Keras layer config. Config should be a \"\n             f\"dictionary, tuple or list. Received: config={config}\"\n         )\n \n\n@@ -120,9 +120,7 @@ def generate_keras_metadata(saved_nodes, node_paths):\n             if not path:\n                 node_path = \"root\"\n             else:\n-                node_path = \"root.{}\".format(\n-                    \".\".join([ref.name for ref in path])\n-                )\n+                node_path = f\"root.{'.'.join([ref.name for ref in path])}\"\n \n             metadata.nodes.add(\n                 node_id=node_id,\n\n@@ -175,7 +175,7 @@ def wrap_layer_functions(layer, serialization_cache):\n     call_collection = LayerCallCollection(layer)\n     call_fn_with_losses = call_collection.add_function(\n         _wrap_call_and_conditional_losses(layer),\n-        \"{}_layer_call_and_return_conditional_losses\".format(layer.name),\n+        f\"{layer.name}_layer_call_and_return_conditional_losses\",\n         # If any of this layer's child layers use the training arg, the traced\n         # call functions of this layer will have a training keyword argument. If\n         # the original layer does not expect the training arg, then it will have\n@@ -184,7 +184,7 @@ def wrap_layer_functions(layer, serialization_cache):\n     )\n     call_fn = call_collection.add_function(\n         _extract_outputs_from_fn(layer, call_fn_with_losses),\n-        \"{}_layer_call_fn\".format(layer.name),\n+        f\"{layer.name}_layer_call_fn\",\n         # Since `call_fn` wraps call_fn_with_losses and not the original call\n         # function, `match_layer_training_arg` should be set to False.\n         match_layer_training_arg=False,\n@@ -203,9 +203,7 @@ def wrap_layer_functions(layer, serialization_cache):\n             _append_activity_regularizer_loss(\n                 layer, call_fn_with_losses, fns[\"activity_regularizer_fn\"]\n             ),\n-            \"{}_layer_call_and_return_all_conditional_losses\".format(\n-                layer.name\n-            ),\n+            f\"{layer.name}_layer_call_and_return_all_conditional_losses\",\n             match_layer_training_arg=False,\n         )\n     else:\n@@ -757,7 +755,7 @@ def _wrap_unconditional_loss(loss_fn, index):\n         return fn\n     else:\n         return tf.__internal__.function.Function(\n-            fn, \"loss_fn_{}\".format(index), input_signature=[]\n+            fn, f\"loss_fn_{index}\", input_signature=[]\n         )\n \n \n@@ -770,7 +768,7 @@ def _wrap_activity_regularizer(layer):\n         return layer._activity_regularizer\n     return tf.__internal__.function.Function(\n         layer._activity_regularizer,\n-        \"{}_activity_regularizer\".format(layer.name),\n+        f\"{layer.name}_activity_regularizer\",\n         input_signature=[\n             tf.TensorSpec(None, layer._compute_dtype or backend.floatx())\n         ],\n\n@@ -249,7 +249,7 @@ class SerializedAttributes:\n                         \"The object dictionary contained a non-trackable \"\n                         f\"object: {object_dict[key]} (for key {key}). \"\n                         \"Only trackable objects are \"\n-                        f\"allowed, such as Keras layers/models or \"\n+                        \"allowed, such as Keras layers/models or \"\n                         \"tf.Module instances.\"\n                     )\n                 self._object_dict[key] = object_dict[key]\n\n@@ -273,9 +273,7 @@ def _import_and_infer(save_dir, inputs):\n         ]\n         assert set(inputs.keys()) == set(\n             signature.inputs.keys()\n-        ), \"expected {}, found {}\".format(\n-            signature.inputs.keys(), inputs.keys()\n-        )\n+        ), f\"expected {signature.inputs.keys()}, found {inputs.keys()}\"\n         feed_dict = {}\n         for arg_name in inputs.keys():\n             feed_dict[\n\n@@ -54,9 +54,7 @@ class ExportOutput:\n \n         if not isinstance(key, str):\n             raise ValueError(\n-                \"{} output key must be a string; got {}.\".format(\n-                    error_label, key\n-                )\n+                f\"{error_label} output key must be a string; got {key}.\"\n             )\n         return key\n \n@@ -91,9 +89,7 @@ class ExportOutput:\n             key = self._check_output_key(key, error_name)\n             if not isinstance(value, tf.Tensor):\n                 raise ValueError(\n-                    \"{} output value must be a Tensor; got {}.\".format(\n-                        error_name, value\n-                    )\n+                    f\"{error_name} output value must be a Tensor; got {value}.\"\n                 )\n \n             output_dict[key] = value\n@@ -138,16 +134,14 @@ class ClassificationOutput(ExportOutput):\n             isinstance(scores, tf.Tensor) and scores.dtype.is_floating\n         ):\n             raise ValueError(\n-                \"Classification scores must be a float32 Tensor; \"\n-                \"got {}\".format(scores)\n+                f\"Classification scores must be a float32 Tensor; got {scores}\"\n             )\n         if classes is not None and not (\n             isinstance(classes, tf.Tensor)\n             and tf.as_dtype(classes.dtype) == tf.string\n         ):\n             raise ValueError(\n-                \"Classification classes must be a string Tensor; \"\n-                \"got {}\".format(classes)\n+                f\"Classification classes must be a string Tensor; got {classes}\"\n             )\n         if scores is None and classes is None:\n             raise ValueError(\n@@ -207,8 +201,7 @@ class RegressionOutput(ExportOutput):\n         \"\"\"\n         if not (isinstance(value, tf.Tensor) and value.dtype.is_floating):\n             raise ValueError(\n-                \"Regression output value must be a float32 Tensor; \"\n-                \"got {}\".format(value)\n+                f\"Regression output value must be a float32 Tensor; got {value}\"\n             )\n         self._value = value\n \n@@ -391,9 +384,7 @@ class _SupervisedOutput(ExportOutput):\n             op_name = key + self._SEPARATOR_CHAR + self.METRIC_UPDATE_SUFFIX\n             if not isinstance(metric_val, tf.Tensor):\n                 raise ValueError(\n-                    \"{} output value must be a Tensor; got {}.\".format(\n-                        key, metric_val\n-                    )\n+                    f\"{key} output value must be a Tensor; got {metric_val}.\"\n                 )\n             if not (\n                 tf.is_tensor(metric_op) or isinstance(metric_op, tf.Operation)\n\n@@ -104,7 +104,7 @@ def build_all_signature_defs(\n     signature_def_map = {}\n     excluded_signatures = {}\n     for output_key, export_output in export_outputs.items():\n-        signature_name = \"{}\".format(output_key or \"None\")\n+        signature_name = f\"{output_key or 'None'}\"\n         try:\n             signature = export_output.as_signature_def(receiver_tensors)\n             signature_def_map[signature_name] = signature\n@@ -188,7 +188,7 @@ def _log_signature_report(signature_def_map, excluded_signatures):\n             \"be served via TensorFlow Serving APIs:\"\n         )\n         for signature_name, message in excluded_signatures.items():\n-            logging.info(\"'{}' : {}\".format(signature_name, message))\n+            logging.info(f\"'{signature_name}' : {message}\")\n \n     if not signature_def_map:\n         logging.warning(\"Export includes no signatures!\")\n@@ -273,7 +273,7 @@ def get_temp_export_dir(timestamped_export_dir):\n         str_name = str(basename)\n     temp_export_dir = tf.io.gfile.join(\n         tf.compat.as_bytes(dirname),\n-        tf.compat.as_bytes(\"temp-{}\".format(str_name)),\n+        tf.compat.as_bytes(f\"temp-{str_name}\"),\n     )\n     return temp_export_dir\n \n\n@@ -95,7 +95,7 @@ class ModeKeyMap(collections.abc.Mapping):\n             return KerasModeKeys.TEST\n         if is_predict(key):\n             return KerasModeKeys.PREDICT\n-        raise ValueError(\"Invalid mode key: {}.\".format(key))\n+        raise ValueError(f\"Invalid mode key: {key}.\")\n \n     def __getitem__(self, key):\n         return self._internal_dict[self._get_internal_key(key)]\n\n@@ -128,9 +128,7 @@ class KerasDoctestOutputCheckerTest(parameterized.TestCase):\n                 )\n             )\n         except AssertionError as e:\n-            msg = \"\\n\\n  expected: {}\\n  found:     {}\".format(\n-                text_with_wildcards, text\n-            )\n+            msg = f\"\\n\\n  expected: {text_with_wildcards}\\n  found:     {text}\"\n             e.args = (e.args[0] + msg,)\n             raise e\n \n\n@@ -127,7 +127,7 @@ def run_with_all_saved_model_formats(test_or_class=None, exclude_formats=None):\n         exclude_formats.append([\"h5\"])\n     saved_model_formats = [\"h5\", \"tf\", \"tf_no_traces\"]\n     params = [\n-        (\"_%s\" % saved_format, saved_format)\n+        (f\"_{saved_format}\", saved_format)\n         for saved_format in saved_model_formats\n         if saved_format not in tf.nest.flatten(exclude_formats)\n     ]\n@@ -147,7 +147,7 @@ def run_with_all_saved_model_formats(test_or_class=None, exclude_formats=None):\n             elif saved_format == \"tf_no_traces\":\n                 _test_tf_saved_model_format_no_traces(f, self, *args, **kwargs)\n             else:\n-                raise ValueError(\"Unknown model type: %s\" % (saved_format,))\n+                raise ValueError(f\"Unknown model type: {saved_format}\")\n \n         return decorated\n \n@@ -270,7 +270,7 @@ def run_with_all_model_types(test_or_class=None, exclude_models=None):\n     \"\"\"\n     model_types = [\"functional\", \"subclass\", \"sequential\"]\n     params = [\n-        (\"_%s\" % model, model)\n+        (f\"_{model}\", model)\n         for model in model_types\n         if model not in tf.nest.flatten(exclude_models)\n     ]\n@@ -290,7 +290,7 @@ def run_with_all_model_types(test_or_class=None, exclude_models=None):\n             elif model_type == \"sequential\":\n                 _test_sequential_model_type(f, self, *args, **kwargs)\n             else:\n-                raise ValueError(\"Unknown model type: %s\" % (model_type,))\n+                raise ValueError(f\"Unknown model type: {model_type}\")\n \n         return decorated\n \n@@ -317,7 +317,7 @@ def run_all_keras_modes(\n     config=None,\n     always_skip_v1=False,\n     always_skip_eager=False,\n-    **kwargs\n+    **kwargs,\n ):\n     \"\"\"Execute the decorated test with all keras execution modes.\n \n@@ -389,7 +389,7 @@ def run_all_keras_modes(\n         a target dependency.\n     \"\"\"\n     if kwargs:\n-        raise ValueError(\"Unrecognized keyword args: {}\".format(kwargs))\n+        raise ValueError(f\"Unrecognized keyword args: {kwargs}\")\n \n     params = [(\"_v2_function\", \"v2_function\")]\n     if not always_skip_eager:\n@@ -413,7 +413,7 @@ def run_all_keras_modes(\n             elif run_mode == \"v2_function\":\n                 _v2_function_test(f, self, *args, **kwargs)\n             else:\n-                return ValueError(\"Unknown run mode %s\" % run_mode)\n+                return ValueError(f\"Unknown run mode {run_mode}\")\n \n         return decorated\n \n\n@@ -553,7 +553,7 @@ def get_small_mlp(num_hidden, num_classes, input_dim):\n         return get_small_sequential_mlp(num_hidden, num_classes, input_dim)\n     if model_type == \"functional\":\n         return get_small_functional_mlp(num_hidden, num_classes, input_dim)\n-    raise ValueError(\"Unknown model type {}\".format(model_type))\n+    raise ValueError(f\"Unknown model type {model_type}\")\n \n \n class _SubclassModel(models.Model):\n@@ -582,7 +582,7 @@ class _SubclassModel(models.Model):\n             self._set_inputs(inputs)\n \n     def _layer_name_for_i(self, i):\n-        return \"layer{}\".format(i)\n+        return f\"layer{i}\"\n \n     def call(self, inputs, **kwargs):\n         x = inputs\n@@ -691,7 +691,7 @@ def get_model_from_layers(\n             outputs = layer(outputs)\n         return models.Model(inputs, outputs, name=name)\n \n-    raise ValueError(\"Unknown model type {}\".format(model_type))\n+    raise ValueError(f\"Unknown model type {model_type}\")\n \n \n class Bias(layers.Layer):\n@@ -902,7 +902,7 @@ def get_multi_io_model(\n \n     if model_type == \"sequential\":\n         raise ValueError(\n-            \"Cannot use `get_multi_io_model` to construct \" \"sequential models\"\n+            \"Cannot use `get_multi_io_model` to construct sequential models\"\n         )\n \n     if model_type == \"functional\":\n@@ -927,7 +927,7 @@ def get_multi_io_model(\n \n         return models.Model(inputs, outputs)\n \n-    raise ValueError(\"Unknown model type {}\".format(model_type))\n+    raise ValueError(f\"Unknown model type {model_type}\")\n \n \n _V2_OPTIMIZER_MAP = {\n@@ -1168,8 +1168,7 @@ def generate_combinations_with_testcase_name(**kwargs):\n         )\n         named_combinations.append(\n             collections.OrderedDict(\n-                list(combination.items())\n-                + [(\"testcase_name\", \"_test{}\".format(name))]\n+                list(combination.items()) + [(\"testcase_name\", f\"_test{name}\")]\n             )\n         )\n \n\n@@ -32,7 +32,7 @@ class KerasIntegrationTest(test_combinations.TestCase):\n     def _save_and_reload_model(self, model):\n         self.temp_dir = self.get_temp_dir()\n         fpath = os.path.join(\n-            self.temp_dir, \"test_model_%s\" % (random.randint(0, 1e7),)\n+            self.temp_dir, f\"test_model_{random.randint(0, 10000000.0)}\"\n         )\n         if tf.executing_eagerly():\n             save_format = \"tf\"\n\n@@ -58,7 +58,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         test_model(dummy_data)\n         self.assertTrue(\n             test_model.uses_custom_build,\n-            \"Model should use user \" \"defined build when called.\",\n+            \"Model should use user defined build when called.\",\n         )\n \n     def test_attribute_conflict_error(self):\n@@ -119,7 +119,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         with self.assertRaisesRegex(\n             ValueError, \"input shape is not one of the valid types\"\n@@ -161,7 +161,7 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         with self.assertRaisesRegex(\n             ValueError, \"if your layers do not support float type inputs\"\n@@ -186,15 +186,12 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build(batch_input_shape)\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -213,15 +210,12 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build(input_shape=(batch_size, input_dim))\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -240,15 +234,12 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build(input_shape=(batch_size, input_dim))\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -265,16 +256,13 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         batch_input_shape = (batch_size,) + input_shape\n         model.build(input_shape=batch_input_shape)\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -292,15 +280,12 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build(input_shape=tf.TensorShape((batch_size,) + input_shape))\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -318,15 +303,12 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build(input_shape=tf.TensorShape((batch_size,) + input_shape))\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -355,16 +337,13 @@ class ModelSubclassingTest(test_combinations.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         batch_input_shape = tf.TensorShape((batch_size, input_dim))\n         model.build(input_shape=[batch_input_shape, batch_input_shape])\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -697,15 +676,12 @@ class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build((None, input_dim))\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -718,15 +694,12 @@ class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         model.build((None, input_dim))\n         self.assertTrue(\n             model.weights,\n-            (\n-                \"Model should have weights now that it \"\n-                \"has been properly built.\"\n-            ),\n+            \"Model should have weights now that it has been properly built.\",\n         )\n         self.assertTrue(\n             model.built, \"Model should be built after calling `build`.\"\n@@ -740,7 +713,7 @@ class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n         self.assertFalse(model.built, \"Model should not have been built\")\n         self.assertFalse(\n             model.weights,\n-            (\"Model should have no weights since it \" \"has not been built.\"),\n+            \"Model should have no weights since it has not been built.\",\n         )\n         with self.assertRaisesRegex(\n             ValueError, \"cannot build your model if it has positional\"\n\n@@ -108,20 +108,15 @@ def verify_python_files_in_pip(pip_root, bazel_root):\n             file_excluded = file_name.lstrip(\"./\") in PIP_EXCLUDED_FILES\n             if not path_exists and not file_excluded:\n                 raise PipPackagingError(\n-                    (\n                     \"Pip package missing the file %s. If this is expected, \"\n                     \"add it to PIP_EXCLUDED_FILES in \"\n                     \"create_pip_helper.py. Otherwise, \"\n                     \"make sure it is a build dependency of the pip package\"\n-                    )\n                     % file_name\n                 )\n             if path_exists and file_excluded:\n                 raise PipPackagingError(\n-                    (\n-                        \"File in PIP_EXCLUDED_FILES included in pip. %s\"\n-                        % file_name\n-                    )\n+                    f\"File in PIP_EXCLUDED_FILES included in pip. {file_name}\"\n                 )\n \n \n\n@@ -164,7 +164,7 @@ def audio_dataset_from_directory(\n \n         if sampling_rate <= 0:\n             raise ValueError(\n-                f\"`sampling_rate` should be higher than 0. \"\n+                \"`sampling_rate` should be higher than 0. \"\n                 f\"Received: sampling_rate={sampling_rate}\"\n             )\n \n@@ -198,7 +198,7 @@ def audio_dataset_from_directory(\n \n     if label_mode == \"binary\" and len(class_names) != 2:\n         raise ValueError(\n-            f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n+            'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n             f\"class_names. Received: class_names={class_names}\"\n         )\n \n\n@@ -59,7 +59,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         # Generate paths to class subdirectories\n         paths = []\n         for class_index in range(num_classes):\n-            class_directory = \"class_%s\" % (class_index,)\n+            class_directory = f\"class_{class_index}\"\n             if nested_dirs:\n                 class_paths = [\n                     class_directory,\n@@ -82,7 +82,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         ):\n             path = paths[i % len(paths)]\n             ext = \"wav\"\n-            filename = os.path.join(path, \"audio_%s.%s\" % (i, ext))\n+            filename = os.path.join(path, f\"audio_{i}.{ext}\")\n             with open(os.path.join(temp_dir, filename), \"wb\") as f:\n                 f.write(audio.numpy())\n             i += 1\n@@ -94,7 +94,7 @@ class AudioDatasetFromDirectoryTest(test_combinations.TestCase):\n         # Save a few extra audio in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n         for i, audio in enumerate(self._get_audio_samples(3)):\n-            filename = \"audio_%s.wav\" % (i,)\n+            filename = f\"audio_{i}.wav\"\n             with open(os.path.join(directory, filename), \"wb\") as f:\n                 f.write(audio.numpy())\n \n\n@@ -52,7 +52,7 @@ class ToDense(Layer):\n         elif isinstance(inputs, tf.Tensor):\n             output = inputs\n         else:\n-            raise TypeError(\"Unexpected tensor type %s\" % type(inputs).__name__)\n+            raise TypeError(f\"Unexpected tensor type {type(inputs).__name__}\")\n \n         # Return a float so that we can compile models with this as the final\n         # layer.\n@@ -97,7 +97,7 @@ class _SubclassModel(keras.Model):\n             self._set_inputs(i_layer)\n \n     def _layer_name_for_i(self, i):\n-        return \"layer{}\".format(i)\n+        return f\"layer{i}\"\n \n     def call(self, inputs, **kwargs):\n         x = inputs\n@@ -143,7 +143,7 @@ def get_model_from_layers_with_input(\n             outputs = layer(outputs)\n         return keras.Model(inputs, outputs)\n \n-    raise ValueError(\"Unknown model type {}\".format(model_type))\n+    raise ValueError(f\"Unknown model type {model_type}\")\n \n \n def get_test_mode_kwargs():\n@@ -355,7 +355,7 @@ class SparseTensorInputTest(test_combinations.TestCase):\n             optimizer=\"sgd\",\n             loss=\"mse\",\n             metrics=[\"accuracy\"],\n-            **get_test_mode_kwargs()\n+            **get_test_mode_kwargs(),\n         )\n         kwargs = get_kwargs(use_dataset, action)\n \n@@ -543,7 +543,7 @@ class RaggedTensorInputTest(test_combinations.TestCase, tf.test.TestCase):\n             optimizer=\"sgd\",\n             loss=\"mse\",\n             metrics=[\"accuracy\"],\n-            **get_test_mode_kwargs()\n+            **get_test_mode_kwargs(),\n         )\n \n         # Prepare the input data\n@@ -599,7 +599,7 @@ class RaggedTensorInputValidationTest(\n             optimizer=\"sgd\",\n             loss=\"mse\",\n             metrics=[\"accuracy\"],\n-            **get_test_mode_kwargs()\n+            **get_test_mode_kwargs(),\n         )\n \n         for data_element in data:\n@@ -638,7 +638,7 @@ class RaggedTensorInputValidationTest(\n             optimizer=\"sgd\",\n             loss=\"mse\",\n             metrics=[\"accuracy\"],\n-            **get_test_mode_kwargs()\n+            **get_test_mode_kwargs(),\n         )\n         kwargs = get_kwargs(use_dataset)\n \n\n@@ -260,7 +260,7 @@ def get_file(\n                 io_utils.print_msg(\n                     \"A local file was found, but it seems to be \"\n                     f\"incomplete or outdated because the {hash_algorithm} \"\n-                    f\"file hash does not match the original value of \"\n+                    \"file hash does not match the original value of \"\n                     f\"{file_hash} \"\n                     \"so we will re-download the data.\"\n                 )\n@@ -309,9 +309,9 @@ def get_file(\n         if os.path.exists(fpath) and file_hash is not None:\n             if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n                 raise ValueError(\n-                    f\"Incomplete or corrupted file detected. \"\n+                    \"Incomplete or corrupted file detected. \"\n                     f\"The {hash_algorithm} \"\n-                    f\"file hash does not match the provided value \"\n+                    \"file hash does not match the provided value \"\n                     f\"of {file_hash}.\"\n                 )\n \n@@ -836,7 +836,7 @@ def init_pool_generator(gens, random_seed=None, id_queue=None):\n \n     # name isn't used for anything, but setting a more descriptive name is\n     # helpful when diagnosing orphaned processes.\n-    worker_proc.name = \"Keras_worker_{}\".format(worker_proc.name)\n+    worker_proc.name = f\"Keras_worker_{worker_proc.name}\"\n \n     if random_seed is not None:\n         np.random.seed(random_seed + worker_proc.ident)\n\n@@ -180,8 +180,8 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                         f\"lengths. Mismatch found at index {i}, \"\n                         f\"Expected shape={expected_shape} \"\n                         f\"Received shape={np.array(element).shape}.\"\n-                        f\"Please provide a list of NumPy arrays with \"\n-                        f\"the same length.\"\n+                        \"Please provide a list of NumPy arrays with \"\n+                        \"the same length.\"\n                     )\n         else:\n             raise ValueError(\n@@ -206,7 +206,7 @@ def _get_data_iterator_from_dataset(dataset, dataset_type_spec):\n                         f\"lengths. Mismatch found at index {i}, \"\n                         f\"Expected shape={expected_shape} \"\n                         f\"Received shape={np.array(element).shape}.\"\n-                        f\"Please provide a tuple of NumPy arrays with \"\n+                        \"Please provide a tuple of NumPy arrays with \"\n                         \"the same length.\"\n                     )\n         else:\n@@ -358,7 +358,7 @@ def _rescale_dataset_split_sizes(left_size, right_size, total_length):\n     # check right_size is a integer or float\n     if right_size is not None and right_size_type not in [int, float]:\n         raise TypeError(\n-            f\"Invalid `right_size` Type. \"\n+            \"Invalid `right_size` Type. \"\n             \"Expected: int or float or None,\"\n             f\"Received: type(right_size)={right_size_type}.\"\n         )\n\n@@ -61,14 +61,14 @@ class SplitDatasetTest(tf.test.TestCase):\n     def test_dataset_with_invalid_shape(self):\n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Received a list of NumPy arrays \" \"with different lengths\",\n+            \"Received a list of NumPy arrays with different lengths\",\n         ):\n             dataset = [np.ones(shape=(200, 32)), np.zeros(shape=(100, 32))]\n             dataset_utils.split_dataset(dataset, left_size=4)\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Received a tuple of NumPy arrays \" \"with different lengths\",\n+            \"Received a tuple of NumPy arrays with different lengths\",\n         ):\n             dataset = (np.ones(shape=(200, 32)), np.zeros(shape=(201, 32)))\n             dataset_utils.split_dataset(dataset, left_size=4)\n\n@@ -784,7 +784,7 @@ def deserialize_keras_object(\n         return identifier\n     else:\n         raise ValueError(\n-            f\"Could not interpret serialized \"\n+            \"Could not interpret serialized \"\n             f\"{printable_module_name}: {identifier}\"\n         )\n \n@@ -975,7 +975,7 @@ class Progbar:\n \n         message = \"\"\n         now = time.time()\n-        info = \" - %.0fs\" % (now - self._start)\n+        info = f\" - {now - self._start:.0f}s\"\n         if current == self.target:\n             self._time_at_epoch_end = now\n         if self.verbose == 1:\n@@ -1025,20 +1025,20 @@ class Progbar:\n                 else:\n                     eta_format = \"%ds\" % eta\n \n-                info = \" - ETA: %s\" % eta_format\n+                info = f\" - ETA: {eta_format}\"\n \n             for k in self._values_order:\n-                info += \" - %s:\" % k\n+                info += f\" - {k}:\"\n                 if isinstance(self._values[k], list):\n                     avg = np.mean(\n                         self._values[k][0] / max(1, self._values[k][1])\n                     )\n                     if abs(avg) > 1e-3:\n-                        info += \" %.4f\" % avg\n+                        info += f\" {avg:.4f}\"\n                     else:\n-                        info += \" %.4e\" % avg\n+                        info += f\" {avg:.4e}\"\n                 else:\n-                    info += \" %s\" % self._values[k]\n+                    info += f\" {self._values[k]}\"\n \n             self._total_width += len(info)\n             if prev_total_width > self._total_width:\n@@ -1057,14 +1057,14 @@ class Progbar:\n                 count = (\"%\" + str(numdigits) + \"d/%d\") % (current, self.target)\n                 info = count + info\n                 for k in self._values_order:\n-                    info += \" - %s:\" % k\n+                    info += f\" - {k}:\"\n                     avg = np.mean(\n                         self._values[k][0] / max(1, self._values[k][1])\n                     )\n                     if avg > 1e-3:\n-                        info += \" %.4f\" % avg\n+                        info += f\" {avg:.4f}\"\n                     else:\n-                        info += \" %.4e\" % avg\n+                        info += f\" {avg:.4e}\"\n                 if self._time_at_epoch_end:\n                     time_per_epoch = (\n                         self._time_at_epoch_end - self._time_at_epoch_start\n@@ -1099,11 +1099,11 @@ class Progbar:\n         \"\"\"\n         formatted = \"\"\n         if time_per_unit >= 1 or time_per_unit == 0:\n-            formatted += \" %.0fs/%s\" % (time_per_unit, unit_name)\n+            formatted += f\" {time_per_unit:.0f}s/{unit_name}\"\n         elif time_per_unit >= 1e-3:\n-            formatted += \" %.0fms/%s\" % (time_per_unit * 1e3, unit_name)\n+            formatted += f\" {time_per_unit * 1000.0:.0f}ms/{unit_name}\"\n         else:\n-            formatted += \" %.0fus/%s\" % (time_per_unit * 1e6, unit_name)\n+            formatted += f\" {time_per_unit * 1000000.0:.0f}us/{unit_name}\"\n         return formatted\n \n     def _estimate_step_duration(self, current, now):\n\n@@ -206,7 +206,7 @@ class SerializeKerasObjectTest(tf.test.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            \"Cannot register a class that does \" \"not have a get_config.*\",\n+            \"Cannot register a class that does not have a get_config.*\",\n         ):\n \n             @keras.utils.generic_utils.register_keras_serializable(\n\n@@ -216,7 +216,7 @@ def image_dataset_from_directory(\n \n     if label_mode == \"binary\" and len(class_names) != 2:\n         raise ValueError(\n-            f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n+            'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n             f\"class_names. Received: class_names={class_names}\"\n         )\n \n\n@@ -65,7 +65,7 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n         # Generate paths to class subdirectories\n         paths = []\n         for class_index in range(num_classes):\n-            class_directory = \"class_%s\" % (class_index,)\n+            class_directory = f\"class_{class_index}\"\n             if nested_dirs:\n                 class_paths = [\n                     class_directory,\n@@ -89,7 +89,7 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n                 ext = \"jpg\"\n             else:\n                 ext = \"png\"\n-            filename = os.path.join(path, \"image_%s.%s\" % (i, ext))\n+            filename = os.path.join(path, f\"image_{i}.{ext}\")\n             img.save(os.path.join(temp_dir, filename))\n             i += 1\n         return temp_dir\n@@ -103,7 +103,7 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n         # Save a few extra images in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n         for i, img in enumerate(self._get_images(3)):\n-            filename = \"image_%s.jpg\" % (i,)\n+            filename = f\"image_{i}.jpg\"\n             img.save(os.path.join(directory, filename))\n \n         dataset = image_dataset.image_dataset_from_directory(\n@@ -417,7 +417,7 @@ class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            '`subset` must be either \"training\", ' '\"validation\" or \"both\"',\n+            '`subset` must be either \"training\", \"validation\" or \"both\"',\n         ):\n             _ = image_dataset.image_dataset_from_directory(\n                 directory, validation_split=0.2, subset=\"other\"\n\n@@ -131,7 +131,7 @@ def smart_resize(x, size, interpolation=\"bilinear\"):\n     \"\"\"\n     if len(size) != 2:\n         raise ValueError(\n-            \"Expected `size` to be a tuple of 2 integers, \" f\"but got: {size}.\"\n+            f\"Expected `size` to be a tuple of 2 integers, but got: {size}.\"\n         )\n     img = tf.convert_to_tensor(x)\n     if img.shape.rank is not None:\n@@ -355,7 +355,7 @@ def save_img(path, x, data_format=None, file_format=None, scale=True, **kwargs):\n     img = array_to_img(x, data_format=data_format, scale=scale)\n     if img.mode == \"RGBA\" and (file_format == \"jpg\" or file_format == \"jpeg\"):\n         warnings.warn(\n-            \"The JPG format does not support \" \"RGBA images, converting to RGB.\"\n+            \"The JPG format does not support RGBA images, converting to RGB.\"\n         )\n         img = img.convert(\"RGB\")\n     img.save(path, format=file_format, **kwargs)\n@@ -407,12 +407,12 @@ def load_img(\n     \"\"\"\n     if grayscale:\n         warnings.warn(\n-            \"grayscale is deprecated. Please use \" 'color_mode = \"grayscale\"'\n+            'grayscale is deprecated. Please use color_mode = \"grayscale\"'\n         )\n         color_mode = \"grayscale\"\n     if pil_image is None:\n         raise ImportError(\n-            \"Could not import PIL.Image. \" \"The use of `load_img` requires PIL.\"\n+            \"Could not import PIL.Image. The use of `load_img` requires PIL.\"\n         )\n     if isinstance(path, io.BytesIO):\n         img = pil_image.open(path)\n@@ -423,8 +423,7 @@ def load_img(\n             img = pil_image.open(io.BytesIO(f.read()))\n     else:\n         raise TypeError(\n-            \"path should be path-like or io.BytesIO\"\n-            \", not {}\".format(type(path))\n+            f\"path should be path-like or io.BytesIO, not {type(path)}\"\n         )\n \n     if color_mode == \"grayscale\":\n\n@@ -113,13 +113,13 @@ def ask_to_proceed_with_overwrite(filepath):\n         True if we can proceed with overwrite, False otherwise.\n     \"\"\"\n     overwrite = (\n-        input(\"[WARNING] %s already exists - overwrite? \" \"[y/n]\" % (filepath))\n+        input(f\"[WARNING] {filepath} already exists - overwrite? [y/n]\")\n         .strip()\n         .lower()\n     )\n     while overwrite not in (\"y\", \"n\"):\n         overwrite = (\n-            input('Enter \"y\" (overwrite) or \"n\" ' \"(cancel).\").strip().lower()\n+            input('Enter \"y\" (overwrite) or \"n\" (cancel).').strip().lower()\n         )\n     if overwrite == \"n\":\n         return False\n\n@@ -22,8 +22,7 @@ def _to_matrix(u):\n     u_rank = len(u.shape)\n     if u_rank not in [1, 2]:\n         raise ValueError(\n-            \"The input tensor should have rank 1 or 2. \"\n-            f\"Received rank: {u_rank}\"\n+            f\"The input tensor should have rank 1 or 2. Received rank: {u_rank}\"\n         )\n     if u_rank == 1:\n         return tf.expand_dims(u, 0)\n\n@@ -86,9 +86,7 @@ def validate_string_arg(\n     else:\n         allowed_args = \"`None`, \" if allow_none else \"\"\n         allowed_args += \"a `Callable`, \" if allow_callables else \"\"\n-        allowed_args += \"or one of the following values: %s\" % (\n-            allowable_strings,\n-        )\n+        allowed_args += f\"or one of the following values: {allowable_strings}\"\n         if allow_callables:\n             callable_note = (\n                 f\"If restoring a model and `{arg_name}` is a custom callable, \"\n@@ -317,7 +315,7 @@ def print_summary(\n             line += \"|\" * nested_level\n             print_fn(line)\n \n-    print_fn('Model: \"{}\"'.format(model.name))\n+    print_fn(f'Model: \"{model.name}\"')\n     print_fn(\"_\" * line_length)\n     print_row(to_display, positions)\n     print_fn(\"=\" * line_length)\n@@ -377,9 +375,7 @@ def print_summary(\n                 _,\n             ) in node.iterate_inbound():\n                 connections.append(\n-                    \"{}[{}][{}]\".format(\n-                        inbound_layer.name, node_index, tensor_index\n-                    )\n+                    f\"{inbound_layer.name}[{node_index}][{tensor_index}]\"\n                 )\n \n         name = layer.name\n@@ -440,9 +436,9 @@ def print_summary(\n \n     non_trainable_count = count_params(model.non_trainable_weights)\n \n-    print_fn(\"Total params: {:,}\".format(trainable_count + non_trainable_count))\n-    print_fn(\"Trainable params: {:,}\".format(trainable_count))\n-    print_fn(\"Non-trainable params: {:,}\".format(non_trainable_count))\n+    print_fn(f\"Total params: {trainable_count + non_trainable_count:,}\")\n+    print_fn(f\"Trainable params: {trainable_count:,}\")\n+    print_fn(f\"Non-trainable params: {non_trainable_count:,}\")\n     print_fn(\"_\" * line_length)\n \n \n\n@@ -213,7 +213,7 @@ def assert_thresholds_range(thresholds):\n         ]\n         if invalid_thresholds:\n             raise ValueError(\n-                f\"Threshold values must be in [0, 1]. \"\n+                \"Threshold values must be in [0, 1]. \"\n                 f\"Received: {invalid_thresholds}\"\n             )\n \n\n@@ -40,7 +40,7 @@ class _ObjectIdentityWrapper:\n         if not isinstance(other, _ObjectIdentityWrapper):\n             raise TypeError(\n                 \"Cannot compare wrapped object with unwrapped object. \"\n-                f\"Expect the object to be `_ObjectIdentityWrapper`. \"\n+                \"Expect the object to be `_ObjectIdentityWrapper`. \"\n                 f\"Got: {other}\"\n             )\n \n@@ -68,7 +68,7 @@ class _ObjectIdentityWrapper:\n         return id(self._wrapped)\n \n     def __repr__(self):\n-        return \"<{} wrapping {!r}>\".format(type(self).__name__, self._wrapped)\n+        return f\"<{type(self).__name__} wrapping {self._wrapped!r}>\"\n \n \n class _WeakObjectIdentityWrapper(_ObjectIdentityWrapper):\n@@ -152,7 +152,7 @@ class ObjectIdentityDictionary(collections.abc.MutableMapping):\n             yield key.unwrapped\n \n     def __repr__(self):\n-        return \"ObjectIdentityDictionary(%s)\" % repr(self._storage)\n+        return f\"ObjectIdentityDictionary({repr(self._storage)})\"\n \n \n class ObjectIdentityWeakKeyDictionary(ObjectIdentityDictionary):\n\n@@ -167,7 +167,7 @@ def text_dataset_from_directory(\n \n     if label_mode == \"binary\" and len(class_names) != 2:\n         raise ValueError(\n-            f'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n+            'When passing `label_mode=\"binary\"`, there must be exactly 2 '\n             f\"class_names. Received: class_names={class_names}\"\n         )\n \n@@ -187,12 +187,12 @@ def text_dataset_from_directory(\n         if not file_paths_train:\n             raise ValueError(\n                 f\"No training text files found in directory {directory}. \"\n-                f\"Allowed format: .txt\"\n+                \"Allowed format: .txt\"\n             )\n         if not file_paths_val:\n             raise ValueError(\n                 f\"No validation text files found in directory {directory}. \"\n-                f\"Allowed format: .txt\"\n+                \"Allowed format: .txt\"\n             )\n         train_dataset = paths_and_labels_to_dataset(\n             file_paths=file_paths_train,\n@@ -235,7 +235,7 @@ def text_dataset_from_directory(\n         if not file_paths:\n             raise ValueError(\n                 f\"No text files found in directory {directory}. \"\n-                f\"Allowed format: .txt\"\n+                \"Allowed format: .txt\"\n             )\n         dataset = paths_and_labels_to_dataset(\n             file_paths=file_paths,\n\n@@ -41,7 +41,7 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n         # Generate paths to class subdirectories\n         paths = []\n         for class_index in range(num_classes):\n-            class_directory = \"class_%s\" % (class_index,)\n+            class_directory = f\"class_{class_index}\"\n             if nested_dirs:\n                 class_paths = [\n                     class_directory,\n@@ -59,7 +59,7 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n \n         for i in range(count):\n             path = paths[i % len(paths)]\n-            filename = os.path.join(path, \"text_%s.txt\" % (i,))\n+            filename = os.path.join(path, f\"text_{i}.txt\")\n             f = open(os.path.join(temp_dir, filename), \"w\")\n             text = \"\".join(\n                 [random.choice(string.printable) for _ in range(length)]\n@@ -73,7 +73,7 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n         # subdirs. Save a few extra files in the parent directory.\n         directory = self._prepare_directory(count=7, num_classes=2)\n         for i in range(3):\n-            filename = \"text_%s.txt\" % (i,)\n+            filename = f\"text_{i}.txt\"\n             f = open(os.path.join(directory, filename), \"w\")\n             text = \"\".join([random.choice(string.printable) for _ in range(20)])\n             f.write(text)\n@@ -292,7 +292,7 @@ class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            '`subset` must be either \"training\", ' '\"validation\" or \"both\"',\n+            '`subset` must be either \"training\", \"validation\" or \"both\"',\n         ):\n             _ = text_dataset.text_dataset_from_directory(\n                 directory, validation_split=0.2, subset=\"other\"\n\n@@ -116,7 +116,7 @@ def get_reachable_from_inputs(inputs, targets=None):\n             outputs = x.consumers()\n         else:\n             raise TypeError(\n-                f\"Expected tf.Operation, tf.Variable, or tf.Tensor. \"\n+                \"Expected tf.Operation, tf.Variable, or tf.Tensor. \"\n                 f\"Received: {x}\"\n             )\n \n\n@@ -150,25 +150,25 @@ def timeseries_dataset_from_array(\n     if start_index:\n         if start_index < 0:\n             raise ValueError(\n-                f\"`start_index` must be 0 or greater. Received: \"\n+                \"`start_index` must be 0 or greater. Received: \"\n                 f\"start_index={start_index}\"\n             )\n         if start_index >= len(data):\n             raise ValueError(\n-                f\"`start_index` must be lower than the length of the \"\n+                \"`start_index` must be lower than the length of the \"\n                 f\"data. Received: start_index={start_index}, for data \"\n                 f\"of length {len(data)}\"\n             )\n     if end_index:\n         if start_index and end_index <= start_index:\n             raise ValueError(\n-                f\"`end_index` must be higher than `start_index`. \"\n+                \"`end_index` must be higher than `start_index`. \"\n                 f\"Received: start_index={start_index}, and \"\n                 f\"end_index={end_index} \"\n             )\n         if end_index >= len(data):\n             raise ValueError(\n-                f\"`end_index` must be lower than the length of the \"\n+                \"`end_index` must be lower than the length of the \"\n                 f\"data. Received: end_index={end_index}, for data of \"\n                 f\"length {len(data)}\"\n             )\n@@ -181,23 +181,23 @@ def timeseries_dataset_from_array(\n     # Validate strides\n     if sampling_rate <= 0:\n         raise ValueError(\n-            f\"`sampling_rate` must be higher than 0. Received: \"\n+            \"`sampling_rate` must be higher than 0. Received: \"\n             f\"sampling_rate={sampling_rate}\"\n         )\n     if sampling_rate >= len(data):\n         raise ValueError(\n-            f\"`sampling_rate` must be lower than the length of the \"\n+            \"`sampling_rate` must be lower than the length of the \"\n             f\"data. Received: sampling_rate={sampling_rate}, for data \"\n             f\"of length {len(data)}\"\n         )\n     if sequence_stride <= 0:\n         raise ValueError(\n-            f\"`sequence_stride` must be higher than 0. Received: \"\n+            \"`sequence_stride` must be higher than 0. Received: \"\n             f\"sequence_stride={sequence_stride}\"\n         )\n     if sequence_stride >= len(data):\n         raise ValueError(\n-            f\"`sequence_stride` must be lower than the length of the \"\n+            \"`sequence_stride` must be lower than the length of the \"\n             f\"data. Received: sequence_stride={sequence_stride}, for \"\n             f\"data of length {len(data)}\"\n         )\n\n@@ -216,9 +216,9 @@ def model_to_dot(\n                 sub_w_last_node[layer.layer.name] = sub_w_nodes[-1]\n                 dot.add_subgraph(submodel_wrapper)\n             else:\n-                layer_name = \"{}({})\".format(layer_name, layer.layer.name)\n+                layer_name = f\"{layer_name}({layer.layer.name})\"\n                 child_class_name = layer.layer.__class__.__name__\n-                class_name = \"{}({})\".format(class_name, child_class_name)\n+                class_name = f\"{class_name}({child_class_name})\"\n \n         if expand_nested and isinstance(layer, functional.Functional):\n             submodel_not_wrapper = model_to_dot(\n@@ -255,7 +255,7 @@ def model_to_dot(\n \n         # Rebuild the label as a table including the layer's name.\n         if show_layer_names:\n-            label = \"%s|%s\" % (layer_name, label)\n+            label = f\"{layer_name}|{label}\"\n \n         # Rebuild the label as a table including the layer's dtype.\n         if show_dtype:\n@@ -266,7 +266,7 @@ def model_to_dot(\n                 else:\n                     return str(dtype)\n \n-            label = \"%s|%s\" % (label, format_dtype(layer.dtype))\n+            label = f\"{label}|{format_dtype(layer.dtype)}\"\n \n         # Rebuild the label as a table including input/output shapes.\n         if show_shapes:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
