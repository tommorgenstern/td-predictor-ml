{"custom_id": "scrapy#d7da298e0637d105dcec379f6cbb3196e752ae72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 11 | Files Changed: 4 | Hunks: 16 | Methods Changed: 10 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 7620 | Contributors (this commit): 67 | Commits (past 90d): 7 | Contributors (cumulative): 124 | DMM Complexity: 1.0\n\nDIFF:\n@@ -366,7 +366,8 @@ class ExecutionEngine:\n         self.slot = Slot(start_requests, close_if_idle, nextcall, scheduler)\n         self.spider = spider\n         if hasattr(scheduler, \"open\"):\n-            yield scheduler.open(spider)\n+            if d := scheduler.open(spider):\n+                yield d\n         yield self.scraper.open_spider(spider)\n         assert self.crawler.stats\n         self.crawler.stats.open_spider(spider)\n\n@@ -322,6 +322,7 @@ class Scheduler(BaseScheduler):\n \n     def _mq(self):\n         \"\"\"Create a new priority queue instance, with in-memory storage\"\"\"\n+        assert self.crawler\n         return build_from_crawler(\n             self.pqclass,\n             self.crawler,\n@@ -331,6 +332,7 @@ class Scheduler(BaseScheduler):\n \n     def _dq(self):\n         \"\"\"Create a new priority queue instance, with disk storage\"\"\"\n+        assert self.crawler\n         assert self.dqdir\n         state = self._read_dqs_state(self.dqdir)\n         q = build_from_crawler(\n\n@@ -445,7 +445,9 @@ class CrawlerProcess(CrawlerRunner):\n             d.addBoth(self._stop_reactor)\n \n         resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n-        resolver = build_from_crawler(resolver_class, self, reactor=reactor)\n+        # We pass self, which is CrawlerProcess, instead of Crawler here,\n+        # which works because the default resolvers only use crawler.settings.\n+        resolver = build_from_crawler(resolver_class, self, reactor=reactor)  # type: ignore[arg-type]\n         resolver.install_on_reactor()\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint(\"REACTOR_THREADPOOL_MAXSIZE\"))\n\n@@ -1,5 +1,7 @@\n \"\"\"Helper functions which don't fit anywhere else\"\"\"\n \n+from __future__ import annotations\n+\n import ast\n import hashlib\n import inspect\n@@ -22,6 +24,8 @@ from typing import (\n     Iterable,\n     List,\n     Optional,\n+    Type,\n+    TypeVar,\n     Union,\n     cast,\n )\n@@ -32,9 +36,11 @@ from scrapy.utils.datatypes import LocalWeakReferencedCache\n \n if TYPE_CHECKING:\n     from scrapy import Spider\n-\n+    from scrapy.crawler import Crawler\n+    from scrapy.settings import BaseSettings\n \n _ITERABLE_SINGLE_VALUES = dict, Item, str, bytes\n+T = TypeVar(\"T\")\n \n \n def arg_to_iter(arg: Any) -> Iterable[Any]:\n@@ -177,7 +183,9 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n     return instance\n \n \n-def build_from_crawler(objcls, crawler, /, *args, **kwargs):\n+def build_from_crawler(\n+    objcls: Type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n+) -> T:\n     \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n@@ -185,20 +193,22 @@ def build_from_crawler(objcls, crawler, /, *args, **kwargs):\n     Raises ``TypeError`` if the resulting instance is ``None``.\n     \"\"\"\n     if hasattr(objcls, \"from_crawler\"):\n-        instance = objcls.from_crawler(crawler, *args, **kwargs)\n+        instance = objcls.from_crawler(crawler, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_crawler\"\n     elif hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(crawler.settings, *args, **kwargs)\n+        instance = objcls.from_settings(crawler.settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n         instance = objcls(*args, **kwargs)\n         method_name = \"__new__\"\n     if instance is None:\n         raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return instance\n+    return cast(T, instance)\n \n \n-def build_from_settings(objcls, settings, /, *args, **kwargs):\n+def build_from_settings(\n+    objcls: Type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n+) -> T:\n     \"\"\"Construct a class instance using its ``from_settings`` constructor.\n \n     ``*args`` and ``**kwargs`` are forwarded to the constructor.\n@@ -206,14 +216,14 @@ def build_from_settings(objcls, settings, /, *args, **kwargs):\n     Raises ``TypeError`` if the resulting instance is ``None``.\n     \"\"\"\n     if hasattr(objcls, \"from_settings\"):\n-        instance = objcls.from_settings(settings, *args, **kwargs)\n+        instance = objcls.from_settings(settings, *args, **kwargs)  # type: ignore[attr-defined]\n         method_name = \"from_settings\"\n     else:\n         instance = objcls(*args, **kwargs)\n         method_name = \"__new__\"\n     if instance is None:\n         raise TypeError(f\"{objcls.__qualname__}.{method_name} returned None\")\n-    return instance\n+    return cast(T, instance)\n \n \n @contextmanager\n@@ -290,7 +300,7 @@ def is_generator_with_return_value(callable: Callable) -> bool:\n     return bool(_generator_callbacks_cache[callable])\n \n \n-def warn_on_generator_with_return_value(spider: \"Spider\", callable: Callable) -> None:\n+def warn_on_generator_with_return_value(spider: Spider, callable: Callable) -> None:\n     \"\"\"\n     Logs a warning if a callable is a generator function and includes\n     a 'return' statement with a value different than None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9eea22fb0ca99193b7f38f9f9398b278d64ea977", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 16 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 47 | Churn Cumulative: 858 | Contributors (this commit): 24 | Commits (past 90d): 1 | Contributors (cumulative): 24 | DMM Complexity: 0.8666666666666667\n\nDIFF:\n@@ -4,18 +4,22 @@ import inspect\n import os\n import sys\n from importlib.metadata import entry_points\n+from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Type\n \n import scrapy\n from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n from scrapy.crawler import CrawlerProcess\n from scrapy.exceptions import UsageError\n+from scrapy.settings import BaseSettings, Settings\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import get_project_settings, inside_project\n from scrapy.utils.python import garbage_collect\n \n \n class ScrapyArgumentParser(argparse.ArgumentParser):\n-    def _parse_optional(self, arg_string):\n+    def _parse_optional(\n+        self, arg_string: str\n+    ) -> Optional[Tuple[Optional[argparse.Action], str, Optional[str]]]:\n         # if starts with -: it means that is a parameter not a argument\n         if arg_string[:2] == \"-:\":\n             return None\n@@ -23,7 +27,7 @@ class ScrapyArgumentParser(argparse.ArgumentParser):\n         return super()._parse_optional(arg_string)\n \n \n-def _iter_command_classes(module_name):\n+def _iter_command_classes(module_name: str) -> Iterable[Type[ScrapyCommand]]:\n     # TODO: add `name` attribute to commands and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n     for module in walk_modules(module_name):\n@@ -37,8 +41,8 @@ def _iter_command_classes(module_name):\n                 yield obj\n \n \n-def _get_commands_from_module(module, inproject):\n-    d = {}\n+def _get_commands_from_module(module: str, inproject: bool) -> Dict[str, ScrapyCommand]:\n+    d: Dict[str, ScrapyCommand] = {}\n     for cmd in _iter_command_classes(module):\n         if inproject or not cmd.requires_project:\n             cmdname = cmd.__module__.split(\".\")[-1]\n@@ -46,8 +50,10 @@ def _get_commands_from_module(module, inproject):\n     return d\n \n \n-def _get_commands_from_entry_points(inproject, group=\"scrapy.commands\"):\n-    cmds = {}\n+def _get_commands_from_entry_points(\n+    inproject: bool, group: str = \"scrapy.commands\"\n+) -> Dict[str, ScrapyCommand]:\n+    cmds: Dict[str, ScrapyCommand] = {}\n     if sys.version_info >= (3, 10):\n         eps = entry_points(group=group)\n     else:\n@@ -61,7 +67,9 @@ def _get_commands_from_entry_points(inproject, group=\"scrapy.commands\"):\n     return cmds\n \n \n-def _get_commands_dict(settings, inproject):\n+def _get_commands_dict(\n+    settings: BaseSettings, inproject: bool\n+) -> Dict[str, ScrapyCommand]:\n     cmds = _get_commands_from_module(\"scrapy.commands\", inproject)\n     cmds.update(_get_commands_from_entry_points(inproject))\n     cmds_module = settings[\"COMMANDS_MODULE\"]\n@@ -70,16 +78,17 @@ def _get_commands_dict(settings, inproject):\n     return cmds\n \n \n-def _pop_command_name(argv):\n+def _pop_command_name(argv: List[str]) -> Optional[str]:\n     i = 0\n     for arg in argv[1:]:\n         if not arg.startswith(\"-\"):\n             del argv[i]\n             return arg\n         i += 1\n+    return None\n \n \n-def _print_header(settings, inproject):\n+def _print_header(settings: BaseSettings, inproject: bool) -> None:\n     version = scrapy.__version__\n     if inproject:\n         print(f\"Scrapy {version} - active project: {settings['BOT_NAME']}\\n\")\n@@ -88,7 +97,7 @@ def _print_header(settings, inproject):\n         print(f\"Scrapy {version} - no active project\\n\")\n \n \n-def _print_commands(settings, inproject):\n+def _print_commands(settings: BaseSettings, inproject: bool) -> None:\n     _print_header(settings, inproject)\n     print(\"Usage:\")\n     print(\"  scrapy <command> [options] [args]\\n\")\n@@ -103,13 +112,17 @@ def _print_commands(settings, inproject):\n     print('Use \"scrapy <command> -h\" to see more info about a command')\n \n \n-def _print_unknown_command(settings, cmdname, inproject):\n+def _print_unknown_command(\n+    settings: BaseSettings, cmdname: str, inproject: bool\n+) -> None:\n     _print_header(settings, inproject)\n     print(f\"Unknown command: {cmdname}\\n\")\n     print('Use \"scrapy\" to see available commands')\n \n \n-def _run_print_help(parser, func, *a, **kw):\n+def _run_print_help(\n+    parser: argparse.ArgumentParser, func: Callable, *a: Any, **kw: Any\n+) -> None:\n     try:\n         func(*a, **kw)\n     except UsageError as e:\n@@ -120,7 +133,9 @@ def _run_print_help(parser, func, *a, **kw):\n         sys.exit(2)\n \n \n-def execute(argv=None, settings=None):\n+def execute(\n+    argv: Optional[List[str]] = None, settings: Optional[Settings] = None\n+) -> None:\n     if argv is None:\n         argv = sys.argv\n \n@@ -162,14 +177,16 @@ def execute(argv=None, settings=None):\n     sys.exit(cmd.exitcode)\n \n \n-def _run_command(cmd, args, opts):\n+def _run_command(cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace) -> None:\n     if opts.profile:\n         _run_command_profiled(cmd, args, opts)\n     else:\n         cmd.run(args, opts)\n \n \n-def _run_command_profiled(cmd, args, opts):\n+def _run_command_profiled(\n+    cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace\n+) -> None:\n     if opts.profile:\n         sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n     loc = locals()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc1a83e7c42dc5142eb9190fdca887385254a7a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 17 | Methods Changed: 20 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 573 | Contributors (this commit): 15 | Commits (past 90d): 1 | Contributors (cumulative): 15 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -4,14 +4,20 @@ Scrapy Item\n See documentation in docs/topics/item.rst\n \"\"\"\n \n+from __future__ import annotations\n+\n from abc import ABCMeta\n from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n-from typing import Dict\n+from typing import TYPE_CHECKING, Any, Dict, Iterator, KeysView, NoReturn, Tuple\n \n from scrapy.utils.trackref import object_ref\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n class Field(dict):\n     \"\"\"Container of field metadata\"\"\"\n@@ -23,7 +29,9 @@ class ItemMeta(ABCMeta):\n     .. _metaclass: https://realpython.com/python-metaclasses\n     \"\"\"\n \n-    def __new__(mcs, class_name, bases, attrs):\n+    def __new__(\n+        mcs, class_name: str, bases: Tuple[type, ...], attrs: Dict[str, Any]\n+    ) -> ItemMeta:\n         classcell = attrs.pop(\"__classcell__\", None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n         _class = super().__new__(mcs, \"x_\" + class_name, new_bases, attrs)\n@@ -44,7 +52,7 @@ class ItemMeta(ABCMeta):\n         return super().__new__(mcs, class_name, bases, new_attrs)\n \n \n-class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n+class Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n     \"\"\"\n     Base class for scraped items.\n \n@@ -69,51 +77,51 @@ class Item(MutableMapping, object_ref, metaclass=ItemMeta):\n \n     fields: Dict[str, Field]\n \n-    def __init__(self, *args, **kwargs):\n-        self._values = {}\n+    def __init__(self, *args: Any, **kwargs: Any):\n+        self._values: Dict[str, Any] = {}\n         if args or kwargs:  # avoid creating dict for most common case\n             for k, v in dict(*args, **kwargs).items():\n                 self[k] = v\n \n-    def __getitem__(self, key):\n+    def __getitem__(self, key: str) -> Any:\n         return self._values[key]\n \n-    def __setitem__(self, key, value):\n+    def __setitem__(self, key: str, value: Any) -> None:\n         if key in self.fields:\n             self._values[key] = value\n         else:\n             raise KeyError(f\"{self.__class__.__name__} does not support field: {key}\")\n \n-    def __delitem__(self, key):\n+    def __delitem__(self, key: str) -> None:\n         del self._values[key]\n \n-    def __getattr__(self, name):\n+    def __getattr__(self, name: str) -> NoReturn:\n         if name in self.fields:\n             raise AttributeError(f\"Use item[{name!r}] to get field value\")\n         raise AttributeError(name)\n \n-    def __setattr__(self, name, value):\n+    def __setattr__(self, name: str, value: Any) -> None:\n         if not name.startswith(\"_\"):\n             raise AttributeError(f\"Use item[{name!r}] = {value!r} to set field value\")\n         super().__setattr__(name, value)\n \n-    def __len__(self):\n+    def __len__(self) -> int:\n         return len(self._values)\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator[str]:\n         return iter(self._values)\n \n     __hash__ = object_ref.__hash__\n \n-    def keys(self):\n+    def keys(self) -> KeysView[str]:\n         return self._values.keys()\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return pformat(dict(self))\n \n-    def copy(self):\n+    def copy(self) -> Self:\n         return self.__class__(self)\n \n-    def deepcopy(self):\n+    def deepcopy(self) -> Self:\n         \"\"\"Return a :func:`~copy.deepcopy` of this item.\"\"\"\n         return deepcopy(self)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#08a265b6ff9bc47774173238b06715154b39e534", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 68 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,10 @@ The Extension Manager\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n+from typing import Any, List\n+\n from scrapy.middleware import MiddlewareManager\n+from scrapy.settings import Settings\n from scrapy.utils.conf import build_component_list\n \n \n@@ -12,5 +15,5 @@ class ExtensionManager(MiddlewareManager):\n     component_name = \"extension\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings):\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n         return build_component_list(settings.getwithbase(\"EXTENSIONS\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38020e0b0481d2b15792757669272d6d3bf4b14f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 62 | Lines Deleted: 36 | Files Changed: 1 | Hunks: 15 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 98 | Churn Cumulative: 645 | Contributors (this commit): 21 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,11 +14,23 @@ from email.mime.nonmultipart import MIMENonMultipart\n from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n-from typing import TYPE_CHECKING, Optional\n+from typing import (\n+    IO,\n+    TYPE_CHECKING,\n+    Any,\n+    Callable,\n+    Dict,\n+    List,\n+    Optional,\n+    Sequence,\n+    Tuple,\n+    Union,\n+)\n \n from twisted import version as twisted_version\n from twisted.internet import ssl\n from twisted.internet.defer import Deferred\n+from twisted.python.failure import Failure\n from twisted.python.versions import Version\n \n from scrapy.settings import BaseSettings\n@@ -26,6 +38,9 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n if TYPE_CHECKING:\n+    # imports twisted.internet.reactor\n+    from twisted.mail.smtp import ESMTPSenderFactory\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -37,7 +52,7 @@ logger = logging.getLogger(__name__)\n COMMASPACE = \", \"\n \n \n-def _to_bytes_or_none(text):\n+def _to_bytes_or_none(text: Union[str, bytes, None]) -> Optional[bytes]:\n     if text is None:\n         return None\n     return to_bytes(text)\n@@ -46,23 +61,23 @@ def _to_bytes_or_none(text):\n class MailSender:\n     def __init__(\n         self,\n-        smtphost=\"localhost\",\n-        mailfrom=\"scrapy@localhost\",\n-        smtpuser=None,\n-        smtppass=None,\n-        smtpport=25,\n-        smtptls=False,\n-        smtpssl=False,\n-        debug=False,\n+        smtphost: str = \"localhost\",\n+        mailfrom: str = \"scrapy@localhost\",\n+        smtpuser: Optional[str] = None,\n+        smtppass: Optional[str] = None,\n+        smtpport: int = 25,\n+        smtptls: bool = False,\n+        smtpssl: bool = False,\n+        debug: bool = False,\n     ):\n-        self.smtphost = smtphost\n-        self.smtpport = smtpport\n-        self.smtpuser = _to_bytes_or_none(smtpuser)\n-        self.smtppass = _to_bytes_or_none(smtppass)\n-        self.smtptls = smtptls\n-        self.smtpssl = smtpssl\n-        self.mailfrom = mailfrom\n-        self.debug = debug\n+        self.smtphost: str = smtphost\n+        self.smtpport: int = smtpport\n+        self.smtpuser: Optional[bytes] = _to_bytes_or_none(smtpuser)\n+        self.smtppass: Optional[bytes] = _to_bytes_or_none(smtppass)\n+        self.smtptls: bool = smtptls\n+        self.smtpssl: bool = smtpssl\n+        self.mailfrom: str = mailfrom\n+        self.debug: bool = debug\n \n     @classmethod\n     def from_settings(cls, settings: BaseSettings) -> Self:\n@@ -78,14 +93,14 @@ class MailSender:\n \n     def send(\n         self,\n-        to,\n-        subject,\n-        body,\n-        cc=None,\n-        attachs=(),\n-        mimetype=\"text/plain\",\n-        charset=None,\n-        _callback=None,\n+        to: Union[str, List[str]],\n+        subject: str,\n+        body: str,\n+        cc: Union[str, List[str], None] = None,\n+        attachs: Sequence[Tuple[str, str, IO]] = (),\n+        mimetype: str = \"text/plain\",\n+        charset: Optional[str] = None,\n+        _callback: Optional[Callable[..., None]] = None,\n     ) -> Optional[Deferred]:\n         from twisted.internet import reactor\n \n@@ -142,13 +157,15 @@ class MailSender:\n         dfd.addCallbacks(\n             callback=self._sent_ok,\n             errback=self._sent_failed,\n-            callbackArgs=[to, cc, subject, len(attachs)],\n-            errbackArgs=[to, cc, subject, len(attachs)],\n+            callbackArgs=(to, cc, subject, len(attachs)),\n+            errbackArgs=(to, cc, subject, len(attachs)),\n         )\n         reactor.addSystemEventTrigger(\"before\", \"shutdown\", lambda: dfd)\n         return dfd\n \n-    def _sent_ok(self, result, to, cc, subject, nattachs):\n+    def _sent_ok(\n+        self, result: Any, to: List[str], cc: List[str], subject: str, nattachs: int\n+    ) -> None:\n         logger.info(\n             \"Mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n             'Subject=\"%(mailsubject)s\" Attachs=%(mailattachs)d',\n@@ -160,7 +177,14 @@ class MailSender:\n             },\n         )\n \n-    def _sent_failed(self, failure, to, cc, subject, nattachs):\n+    def _sent_failed(\n+        self,\n+        failure: Failure,\n+        to: List[str],\n+        cc: List[str],\n+        subject: str,\n+        nattachs: int,\n+    ) -> Failure:\n         errstr = str(failure.value)\n         logger.error(\n             \"Unable to send mail: To=%(mailto)s Cc=%(mailcc)s \"\n@@ -176,13 +200,13 @@ class MailSender:\n         )\n         return failure\n \n-    def _sendmail(self, to_addrs, msg):\n+    def _sendmail(self, to_addrs: List[str], msg: bytes) -> Deferred:\n         from twisted.internet import reactor\n \n-        msg = BytesIO(msg)\n-        d = Deferred()\n+        msg_io = BytesIO(msg)\n+        d: Deferred = Deferred()\n \n-        factory = self._create_sender_factory(to_addrs, msg, d)\n+        factory = self._create_sender_factory(to_addrs, msg_io, d)\n \n         if self.smtpssl:\n             reactor.connectSSL(\n@@ -193,10 +217,12 @@ class MailSender:\n \n         return d\n \n-    def _create_sender_factory(self, to_addrs, msg, d):\n+    def _create_sender_factory(\n+        self, to_addrs: List[str], msg: IO, d: Deferred\n+    ) -> ESMTPSenderFactory:\n         from twisted.mail.smtp import ESMTPSenderFactory\n \n-        factory_keywords = {\n+        factory_keywords: Dict[str, Any] = {\n             \"heloFallback\": True,\n             \"requireAuthentication\": False,\n             \"requireTransportSecurity\": self.smtptls,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0c8e21b8acfcac2d6d057f3c67b2252d0fa660e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 40 | Files Changed: 2 | Hunks: 35 | Methods Changed: 35 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 140 | Churn Cumulative: 2323 | Contributors (this commit): 20 | Commits (past 90d): 5 | Contributors (cumulative): 28 | DMM Complexity: 0.88\n\nDIFF:\n@@ -143,7 +143,7 @@ class Downloader:\n \n         return key, self.slots[key]\n \n-    def _get_slot_key(self, request: Request, spider: Spider) -> str:\n+    def _get_slot_key(self, request: Request, spider: Any) -> str:\n         if self.DOWNLOAD_SLOT in request.meta:\n             return cast(str, request.meta[self.DOWNLOAD_SLOT])\n \n\n@@ -1,12 +1,32 @@\n+from __future__ import annotations\n+\n import hashlib\n import logging\n+from typing import (\n+    TYPE_CHECKING,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Protocol,\n+    Tuple,\n+    Type,\n+    cast,\n+)\n \n+from scrapy import Request\n+from scrapy.core.downloader import Downloader\n+from scrapy.crawler import Crawler\n from scrapy.utils.misc import build_from_crawler\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n logger = logging.getLogger(__name__)\n \n \n-def _path_safe(text):\n+def _path_safe(text: str) -> str:\n     \"\"\"\n     Return a filesystem-safe version of a string ``text``\n \n@@ -24,6 +44,18 @@ def _path_safe(text):\n     return \"-\".join([pathable_slot, unique_slot])\n \n \n+class QueueProtocol(Protocol):\n+    \"\"\"Protocol for downstream queues of ``ScrapyPriorityQueue``.\"\"\"\n+\n+    def push(self, request: Request) -> None: ...\n+\n+    def pop(self) -> Optional[Request]: ...\n+\n+    def close(self) -> None: ...\n+\n+    def __len__(self) -> int: ...\n+\n+\n class ScrapyPriorityQueue:\n     \"\"\"A priority queue implemented using multiple internal queues (typically,\n     FIFO queues). It uses one internal queue for each priority value. The internal\n@@ -51,18 +83,30 @@ class ScrapyPriorityQueue:\n     \"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        downstream_queue_cls: Type[QueueProtocol],\n+        key: str,\n+        startprios: Iterable[int] = (),\n+    ) -> Self:\n         return cls(crawler, downstream_queue_cls, key, startprios)\n \n-    def __init__(self, crawler, downstream_queue_cls, key, startprios=()):\n-        self.crawler = crawler\n-        self.downstream_queue_cls = downstream_queue_cls\n-        self.key = key\n-        self.queues = {}\n-        self.curprio = None\n+    def __init__(\n+        self,\n+        crawler: Crawler,\n+        downstream_queue_cls: Type[QueueProtocol],\n+        key: str,\n+        startprios: Iterable[int] = (),\n+    ):\n+        self.crawler: Crawler = crawler\n+        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n+        self.key: str = key\n+        self.queues: Dict[int, QueueProtocol] = {}\n+        self.curprio: Optional[int] = None\n         self.init_prios(startprios)\n \n-    def init_prios(self, startprios):\n+    def init_prios(self, startprios: Iterable[int]) -> None:\n         if not startprios:\n             return\n \n@@ -71,17 +115,17 @@ class ScrapyPriorityQueue:\n \n         self.curprio = min(startprios)\n \n-    def qfactory(self, key):\n+    def qfactory(self, key: int) -> QueueProtocol:\n         return build_from_crawler(\n             self.downstream_queue_cls,\n             self.crawler,\n             self.key + \"/\" + str(key),\n         )\n \n-    def priority(self, request):\n+    def priority(self, request: Request) -> int:\n         return -request.priority\n \n-    def push(self, request):\n+    def push(self, request: Request) -> None:\n         priority = self.priority(request)\n         if priority not in self.queues:\n             self.queues[priority] = self.qfactory(priority)\n@@ -90,9 +134,9 @@ class ScrapyPriorityQueue:\n         if self.curprio is None or priority < self.curprio:\n             self.curprio = priority\n \n-    def pop(self):\n+    def pop(self) -> Optional[Request]:\n         if self.curprio is None:\n-            return\n+            return None\n         q = self.queues[self.curprio]\n         m = q.pop()\n         if not q:\n@@ -102,7 +146,7 @@ class ScrapyPriorityQueue:\n             self.curprio = min(prios) if prios else None\n         return m\n \n-    def peek(self):\n+    def peek(self) -> Optional[Request]:\n         \"\"\"Returns the next object to be returned by :meth:`pop`,\n         but without removing it from the queue.\n \n@@ -112,30 +156,32 @@ class ScrapyPriorityQueue:\n         if self.curprio is None:\n             return None\n         queue = self.queues[self.curprio]\n-        return queue.peek()\n+        # Protocols can't declare optional members\n+        return cast(Request, queue.peek())  # type: ignore[attr-defined]\n \n-    def close(self):\n-        active = []\n+    def close(self) -> List[int]:\n+        active: List[int] = []\n         for p, q in self.queues.items():\n             active.append(p)\n             q.close()\n         return active\n \n-    def __len__(self):\n+    def __len__(self) -> int:\n         return sum(len(x) for x in self.queues.values()) if self.queues else 0\n \n \n class DownloaderInterface:\n-    def __init__(self, crawler):\n-        self.downloader = crawler.engine.downloader\n+    def __init__(self, crawler: Crawler):\n+        assert crawler.engine\n+        self.downloader: Downloader = crawler.engine.downloader\n \n-    def stats(self, possible_slots):\n+    def stats(self, possible_slots: Iterable[str]) -> List[Tuple[int, str]]:\n         return [(self._active_downloads(slot), slot) for slot in possible_slots]\n \n-    def get_slot_key(self, request):\n+    def get_slot_key(self, request: Request) -> str:\n         return self.downloader._get_slot_key(request, None)\n \n-    def _active_downloads(self, slot):\n+    def _active_downloads(self, slot: str) -> int:\n         \"\"\"Return a number of requests in a Downloader for a given slot\"\"\"\n         if slot not in self.downloader.slots:\n             return 0\n@@ -149,10 +195,22 @@ class DownloaderAwarePriorityQueue:\n     \"\"\"\n \n     @classmethod\n-    def from_crawler(cls, crawler, downstream_queue_cls, key, startprios=()):\n+    def from_crawler(\n+        cls,\n+        crawler: Crawler,\n+        downstream_queue_cls: Type[QueueProtocol],\n+        key: str,\n+        startprios: Optional[Dict[str, Iterable[int]]] = None,\n+    ) -> Self:\n         return cls(crawler, downstream_queue_cls, key, startprios)\n \n-    def __init__(self, crawler, downstream_queue_cls, key, slot_startprios=()):\n+    def __init__(\n+        self,\n+        crawler: Crawler,\n+        downstream_queue_cls: Type[QueueProtocol],\n+        key: str,\n+        slot_startprios: Optional[Dict[str, Iterable[int]]] = None,\n+    ):\n         if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n             raise ValueError(\n                 f'\"{self.__class__}\" does not support CONCURRENT_REQUESTS_PER_IP'\n@@ -169,16 +227,18 @@ class DownloaderAwarePriorityQueue:\n                 \"queue class can be resumed.\"\n             )\n \n-        self._downloader_interface = DownloaderInterface(crawler)\n-        self.downstream_queue_cls = downstream_queue_cls\n-        self.key = key\n-        self.crawler = crawler\n+        self._downloader_interface: DownloaderInterface = DownloaderInterface(crawler)\n+        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n+        self.key: str = key\n+        self.crawler: Crawler = crawler\n \n-        self.pqueues = {}  # slot -> priority queue\n+        self.pqueues: Dict[str, ScrapyPriorityQueue] = {}  # slot -> priority queue\n         for slot, startprios in (slot_startprios or {}).items():\n             self.pqueues[slot] = self.pqfactory(slot, startprios)\n \n-    def pqfactory(self, slot, startprios=()):\n+    def pqfactory(\n+        self, slot: str, startprios: Iterable[int] = ()\n+    ) -> ScrapyPriorityQueue:\n         return ScrapyPriorityQueue(\n             self.crawler,\n             self.downstream_queue_cls,\n@@ -186,11 +246,11 @@ class DownloaderAwarePriorityQueue:\n             startprios,\n         )\n \n-    def pop(self):\n+    def pop(self) -> Optional[Request]:\n         stats = self._downloader_interface.stats(self.pqueues)\n \n         if not stats:\n-            return\n+            return None\n \n         slot = min(stats)[1]\n         queue = self.pqueues[slot]\n@@ -199,14 +259,14 @@ class DownloaderAwarePriorityQueue:\n             del self.pqueues[slot]\n         return request\n \n-    def push(self, request):\n+    def push(self, request: Request) -> None:\n         slot = self._downloader_interface.get_slot_key(request)\n         if slot not in self.pqueues:\n             self.pqueues[slot] = self.pqfactory(slot)\n         queue = self.pqueues[slot]\n         queue.push(request)\n \n-    def peek(self):\n+    def peek(self) -> Optional[Request]:\n         \"\"\"Returns the next object to be returned by :meth:`pop`,\n         but without removing it from the queue.\n \n@@ -220,13 +280,13 @@ class DownloaderAwarePriorityQueue:\n         queue = self.pqueues[slot]\n         return queue.peek()\n \n-    def close(self):\n+    def close(self) -> Dict[str, List[int]]:\n         active = {slot: queue.close() for slot, queue in self.pqueues.items()}\n         self.pqueues.clear()\n         return active\n \n-    def __len__(self):\n+    def __len__(self) -> int:\n         return sum(len(x) for x in self.pqueues.values()) if self.pqueues else 0\n \n-    def __contains__(self, slot):\n+    def __contains__(self, slot: str) -> bool:\n         return slot in self.pqueues\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21fa0761818c158ae9fc35b49ea8d2300f0fa510", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 585 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -7,10 +7,18 @@ See documentation in docs/topics/item.rst\n from __future__ import annotations\n \n from abc import ABCMeta\n-from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n-from typing import TYPE_CHECKING, Any, Dict, Iterator, KeysView, NoReturn, Tuple\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    Dict,\n+    Iterator,\n+    KeysView,\n+    MutableMapping,\n+    NoReturn,\n+    Tuple,\n+)\n \n from scrapy.utils.trackref import object_ref\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad35ffdb0da052d0df194ce5dc1ba7e8d823190f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 21 | Methods Changed: 23 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 462 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,12 @@\n-from typing import Any\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, List, Optional, Sequence, Type\n \n from twisted.internet import defer\n-from twisted.internet.base import ThreadedResolver\n+from twisted.internet.base import ReactorBase, ThreadedResolver\n+from twisted.internet.defer import Deferred\n from twisted.internet.interfaces import (\n+    IAddress,\n     IHostnameResolver,\n     IHostResolution,\n     IResolutionReceiver,\n@@ -12,6 +16,12 @@ from zope.interface.declarations import implementer, provider\n \n from scrapy.utils.datatypes import LocalCache\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+    from scrapy.crawler import Crawler\n+\n # TODO: cache misses\n dnscache: LocalCache[str, Any] = LocalCache(10000)\n \n@@ -22,65 +32,66 @@ class CachingThreadedResolver(ThreadedResolver):\n     Default caching resolver. IPv4 only, supports setting a timeout value for DNS requests.\n     \"\"\"\n \n-    def __init__(self, reactor, cache_size, timeout):\n+    def __init__(self, reactor: ReactorBase, cache_size: int, timeout: float):\n         super().__init__(reactor)\n         dnscache.limit = cache_size\n         self.timeout = timeout\n \n     @classmethod\n-    def from_crawler(cls, crawler, reactor):\n+    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n         if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n             cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n         else:\n             cache_size = 0\n         return cls(reactor, cache_size, crawler.settings.getfloat(\"DNS_TIMEOUT\"))\n \n-    def install_on_reactor(self):\n+    def install_on_reactor(self) -> None:\n         self.reactor.installResolver(self)\n \n-    def getHostByName(self, name: str, timeout=None):\n+    def getHostByName(self, name: str, timeout: Sequence[int] = ()) -> Deferred[str]:\n         if name in dnscache:\n             return defer.succeed(dnscache[name])\n         # in Twisted<=16.6, getHostByName() is always called with\n         # a default timeout of 60s (actually passed as (1, 3, 11, 45) tuple),\n         # so the input argument above is simply overridden\n         # to enforce Scrapy's DNS_TIMEOUT setting's value\n-        timeout = (self.timeout,)\n+        # The timeout arg is typed as Sequence[int] but supports floats.\n+        timeout = (self.timeout,)  # type: ignore[assignment]\n         d = super().getHostByName(name, timeout)\n         if dnscache.limit:\n             d.addCallback(self._cache_result, name)\n         return d\n \n-    def _cache_result(self, result, name):\n+    def _cache_result(self, result: Any, name: str) -> Any:\n         dnscache[name] = result\n         return result\n \n \n @implementer(IHostResolution)\n class HostResolution:\n-    def __init__(self, name):\n-        self.name = name\n+    def __init__(self, name: str):\n+        self.name: str = name\n \n-    def cancel(self):\n+    def cancel(self) -> None:\n         raise NotImplementedError()\n \n \n @provider(IResolutionReceiver)\n class _CachingResolutionReceiver:\n-    def __init__(self, resolutionReceiver, hostName):\n-        self.resolutionReceiver = resolutionReceiver\n-        self.hostName = hostName\n-        self.addresses = []\n+    def __init__(self, resolutionReceiver: IResolutionReceiver, hostName: str):\n+        self.resolutionReceiver: IResolutionReceiver = resolutionReceiver\n+        self.hostName: str = hostName\n+        self.addresses: List[IAddress] = []\n \n-    def resolutionBegan(self, resolution):\n+    def resolutionBegan(self, resolution: IHostResolution) -> None:\n         self.resolutionReceiver.resolutionBegan(resolution)\n         self.resolution = resolution\n \n-    def addressResolved(self, address):\n+    def addressResolved(self, address: IAddress) -> None:\n         self.resolutionReceiver.addressResolved(address)\n         self.addresses.append(address)\n \n-    def resolutionComplete(self):\n+    def resolutionComplete(self) -> None:\n         self.resolutionReceiver.resolutionComplete()\n         if self.addresses:\n             dnscache[self.hostName] = self.addresses\n@@ -93,30 +104,30 @@ class CachingHostnameResolver:\n     does not support setting a timeout value for DNS requests.\n     \"\"\"\n \n-    def __init__(self, reactor, cache_size):\n-        self.reactor = reactor\n-        self.original_resolver = reactor.nameResolver\n+    def __init__(self, reactor: ReactorBase, cache_size: int):\n+        self.reactor: ReactorBase = reactor\n+        self.original_resolver: IHostnameResolver = reactor.nameResolver\n         dnscache.limit = cache_size\n \n     @classmethod\n-    def from_crawler(cls, crawler, reactor):\n+    def from_crawler(cls, crawler: Crawler, reactor: ReactorBase) -> Self:\n         if crawler.settings.getbool(\"DNSCACHE_ENABLED\"):\n             cache_size = crawler.settings.getint(\"DNSCACHE_SIZE\")\n         else:\n             cache_size = 0\n         return cls(reactor, cache_size)\n \n-    def install_on_reactor(self):\n+    def install_on_reactor(self) -> None:\n         self.reactor.installNameResolver(self)\n \n     def resolveHostName(\n         self,\n-        resolutionReceiver,\n+        resolutionReceiver: IResolutionReceiver,\n         hostName: str,\n-        portNumber=0,\n-        addressTypes=None,\n-        transportSemantics=\"TCP\",\n-    ):\n+        portNumber: int = 0,\n+        addressTypes: Optional[Sequence[Type[IAddress]]] = None,\n+        transportSemantics: str = \"TCP\",\n+    ) -> IHostResolution:\n         try:\n             addresses = dnscache[hostName]\n         except KeyError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b749db92e5b15c974b0d77280c22b63000ad4263", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 22 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 65 | Churn Cumulative: 315 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -3,9 +3,10 @@ from __future__ import annotations\n import logging\n import sys\n from abc import ABCMeta, abstractmethod\n-from typing import TYPE_CHECKING, Union\n+from typing import TYPE_CHECKING, Optional, Union\n from warnings import warn\n \n+from scrapy import Spider\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.python import to_unicode\n \n@@ -18,12 +19,14 @@ if TYPE_CHECKING:\n logger = logging.getLogger(__name__)\n \n \n-def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n+def decode_robotstxt(\n+    robotstxt_body: bytes, spider: Optional[Spider], to_native_str_type: bool = False\n+) -> str:\n     try:\n         if to_native_str_type:\n-            robotstxt_body = to_unicode(robotstxt_body)\n+            body_decoded = to_unicode(robotstxt_body)\n         else:\n-            robotstxt_body = robotstxt_body.decode(\"utf-8\", errors=\"ignore\")\n+            body_decoded = robotstxt_body.decode(\"utf-8\", errors=\"ignore\")\n     except UnicodeDecodeError:\n         # If we found garbage or robots.txt in an encoding other than UTF-8, disregard it.\n         # Switch to 'allow all' state.\n@@ -33,8 +36,8 @@ def decode_robotstxt(robotstxt_body, spider, to_native_str_type=False):\n             exc_info=sys.exc_info(),\n             extra={\"spider\": spider},\n         )\n-        robotstxt_body = \"\"\n-    return robotstxt_body\n+        body_decoded = \"\"\n+    return body_decoded\n \n \n class RobotParser(metaclass=ABCMeta):\n@@ -66,82 +69,80 @@ class RobotParser(metaclass=ABCMeta):\n \n \n class PythonRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body, spider):\n+    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n         from urllib.robotparser import RobotFileParser\n \n-        self.spider = spider\n-        robotstxt_body = decode_robotstxt(\n-            robotstxt_body, spider, to_native_str_type=True\n-        )\n-        self.rp = RobotFileParser()\n-        self.rp.parse(robotstxt_body.splitlines())\n+        self.spider: Optional[Spider] = spider\n+        body_decoded = decode_robotstxt(robotstxt_body, spider, to_native_str_type=True)\n+        self.rp: RobotFileParser = RobotFileParser()\n+        self.rp.parse(body_decoded.splitlines())\n \n     @classmethod\n-    def from_crawler(cls, crawler, robotstxt_body):\n+    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url, user_agent):\n+    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.can_fetch(user_agent, url)\n \n \n class ReppyRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body, spider):\n+    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n         warn(\"ReppyRobotParser is deprecated.\", ScrapyDeprecationWarning, stacklevel=2)\n         from reppy.robots import Robots\n \n-        self.spider = spider\n+        self.spider: Optional[Spider] = spider\n         self.rp = Robots.parse(\"\", robotstxt_body)\n \n     @classmethod\n-    def from_crawler(cls, crawler, robotstxt_body):\n+    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url, user_agent):\n+    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n         return self.rp.allowed(url, user_agent)\n \n \n class RerpRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body, spider):\n+    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n         from robotexclusionrulesparser import RobotExclusionRulesParser\n \n-        self.spider = spider\n-        self.rp = RobotExclusionRulesParser()\n-        robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n-        self.rp.parse(robotstxt_body)\n+        self.spider: Optional[Spider] = spider\n+        self.rp: RobotExclusionRulesParser = RobotExclusionRulesParser()\n+        body_decoded = decode_robotstxt(robotstxt_body, spider)\n+        self.rp.parse(body_decoded)\n \n     @classmethod\n-    def from_crawler(cls, crawler, robotstxt_body):\n+    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url, user_agent):\n+    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.is_allowed(user_agent, url)\n \n \n class ProtegoRobotParser(RobotParser):\n-    def __init__(self, robotstxt_body, spider):\n+    def __init__(self, robotstxt_body: bytes, spider: Optional[Spider]):\n         from protego import Protego\n \n-        self.spider = spider\n-        robotstxt_body = decode_robotstxt(robotstxt_body, spider)\n-        self.rp = Protego.parse(robotstxt_body)\n+        self.spider: Optional[Spider] = spider\n+        body_decoded = decode_robotstxt(robotstxt_body, spider)\n+        self.rp = Protego.parse(body_decoded)\n \n     @classmethod\n-    def from_crawler(cls, crawler, robotstxt_body):\n+    def from_crawler(cls, crawler: Crawler, robotstxt_body: bytes) -> Self:\n         spider = None if not crawler else crawler.spider\n         o = cls(robotstxt_body, spider)\n         return o\n \n-    def allowed(self, url, user_agent):\n+    def allowed(self, url: Union[str, bytes], user_agent: Union[str, bytes]) -> bool:\n         user_agent = to_unicode(user_agent)\n         url = to_unicode(url)\n         return self.rp.can_fetch(url, user_agent)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5f7fd2a653407da3eb3e53c853209d9bfb6b275f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 22 | Methods Changed: 27 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 58 | Churn Cumulative: 396 | Contributors (this commit): 12 | Commits (past 90d): 2 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,20 +2,28 @@\n Scheduler queues\n \"\"\"\n \n+from __future__ import annotations\n+\n import marshal\n import pickle  # nosec\n from os import PathLike\n from pathlib import Path\n-from typing import Union\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Type, Union\n \n from queuelib import queue\n \n+from scrapy import Request\n+from scrapy.crawler import Crawler\n from scrapy.utils.request import request_from_dict\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n-def _with_mkdir(queue_class):\n+\n+def _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n     class DirectoriesCreated(queue_class):\n-        def __init__(self, path: Union[str, PathLike], *args, **kwargs):\n+        def __init__(self, path: Union[str, PathLike], *args: Any, **kwargs: Any):\n             dirname = Path(path).parent\n             if not dirname.exists():\n                 dirname.mkdir(parents=True, exist_ok=True)\n@@ -24,18 +32,23 @@ def _with_mkdir(queue_class):\n     return DirectoriesCreated\n \n \n-def _serializable_queue(queue_class, serialize, deserialize):\n+def _serializable_queue(\n+    queue_class: Type[queue.BaseQueue],\n+    serialize: Callable[[Any], bytes],\n+    deserialize: Callable[[bytes], Any],\n+) -> Type[queue.BaseQueue]:\n     class SerializableQueue(queue_class):\n-        def push(self, obj):\n+        def push(self, obj: Any) -> None:\n             s = serialize(obj)\n             super().push(s)\n \n-        def pop(self):\n+        def pop(self) -> Optional[Any]:\n             s = super().pop()\n             if s:\n                 return deserialize(s)\n+            return None\n \n-        def peek(self):\n+        def peek(self) -> Optional[Any]:\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n             but without removing it from the queue.\n \n@@ -50,31 +63,36 @@ def _serializable_queue(queue_class, serialize, deserialize):\n                 ) from ex\n             if s:\n                 return deserialize(s)\n+            return None\n \n     return SerializableQueue\n \n \n-def _scrapy_serialization_queue(queue_class):\n+def _scrapy_serialization_queue(\n+    queue_class: Type[queue.BaseQueue],\n+) -> Type[queue.BaseQueue]:\n     class ScrapyRequestQueue(queue_class):\n-        def __init__(self, crawler, key):\n+        def __init__(self, crawler: Crawler, key: str):\n             self.spider = crawler.spider\n             super().__init__(key)\n \n         @classmethod\n-        def from_crawler(cls, crawler, key, *args, **kwargs):\n+        def from_crawler(\n+            cls, crawler: Crawler, key: str, *args: Any, **kwargs: Any\n+        ) -> Self:\n             return cls(crawler, key)\n \n-        def push(self, request):\n-            request = request.to_dict(spider=self.spider)\n-            return super().push(request)\n+        def push(self, request: Request) -> None:\n+            request_dict = request.to_dict(spider=self.spider)\n+            super().push(request_dict)\n \n-        def pop(self):\n+        def pop(self) -> Optional[Request]:\n             request = super().pop()\n             if not request:\n                 return None\n             return request_from_dict(request, spider=self.spider)\n \n-        def peek(self):\n+        def peek(self) -> Optional[Request]:\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n             but without removing it from the queue.\n \n@@ -89,13 +107,15 @@ def _scrapy_serialization_queue(queue_class):\n     return ScrapyRequestQueue\n \n \n-def _scrapy_non_serialization_queue(queue_class):\n+def _scrapy_non_serialization_queue(\n+    queue_class: Type[queue.BaseQueue],\n+) -> Type[queue.BaseQueue]:\n     class ScrapyRequestQueue(queue_class):\n         @classmethod\n-        def from_crawler(cls, crawler, *args, **kwargs):\n+        def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n             return cls()\n \n-        def peek(self):\n+        def peek(self) -> Optional[Any]:\n             \"\"\"Returns the next object to be returned by :meth:`pop`,\n             but without removing it from the queue.\n \n@@ -113,7 +133,7 @@ def _scrapy_non_serialization_queue(queue_class):\n     return ScrapyRequestQueue\n \n \n-def _pickle_serialize(obj):\n+def _pickle_serialize(obj: Any) -> bytes:\n     try:\n         return pickle.dumps(obj, protocol=4)\n     # Both pickle.PicklingError and AttributeError can be raised by pickle.dump(s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
