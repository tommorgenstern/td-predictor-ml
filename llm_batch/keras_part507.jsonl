{"custom_id": "keras#45515a39f0f8ae2c811838aa8c3160ee0bc29a13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 40 | Churn Cumulative: 426 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: 0.72\n\nDIFF:\n@@ -151,6 +151,7 @@ class Embedding(Layer):\n         # self.dtype before casting to int32 might cause the int32 values to be\n         # different due to a loss of precision.\n         kwargs[\"autocast\"] = False\n+        use_one_hot_matmul = kwargs.pop(\"use_one_hot_matmul\", False)\n         super().__init__(**kwargs)\n \n         self.input_dim = input_dim\n@@ -169,6 +170,9 @@ class Embedding(Layer):\n                 \"`tf.keras.layers.Embedding` is used with `tf.SparseTensor` \"\n                 \"input.\"\n             )\n+        # Make this flag private and do not serialize it for now.\n+        # It will be part of the public API after further testing.\n+        self._use_one_hot_matmul = use_one_hot_matmul\n \n     @tf_utils.shape_type_conversion\n     def build(self, input_shape=None):\n@@ -255,6 +259,15 @@ class Embedding(Layer):\n                     sparse_ids=sparse_inputs_expanded,\n                     default_id=0,\n                 )\n+        elif self._use_one_hot_matmul:\n+            # Note that we change the dtype of the one_hot to be same as the\n+            # weight tensor, since the input data are usually ints, and weights\n+            # are floats. The nn.embedding_lookup support ids as ints, but\n+            # the one_hot matmul need both inputs and weights to be same dtype.\n+            one_hot_data = tf.one_hot(\n+                inputs, depth=self.input_dim, dtype=self.dtype\n+            )\n+            out = tf.matmul(one_hot_data, self.embeddings)\n         else:\n             out = tf.nn.embedding_lookup(self.embeddings, inputs)\n \n\n@@ -226,6 +226,33 @@ class EmbeddingTest(test_combinations.TestCase):\n         self.assertAllClose(output.values, expected_output.values)\n         self.assertAllClose(output.dense_shape, expected_output.dense_shape)\n \n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_use_one_hot(self):\n+        batch = 8\n+        input_length = 10\n+        layer = keras.layers.Embedding(input_dim=100, output_dim=16)\n+        self.assertFalse(layer._use_one_hot_matmul)\n+\n+        inputs = tf.random.uniform(\n+            shape=[batch, input_length], minval=0, maxval=9, dtype=tf.int64\n+        )\n+        output_1 = layer(inputs)\n+\n+        layer._use_one_hot_matmul = True\n+        output_2 = layer(inputs)\n+\n+        self.assertAllClose(output_1, output_2)\n+        self.assertEqual(output_1.dtype, output_2.dtype)\n+\n+        # Make sure the layer can be created with hidden kwargs, and not\n+        # serialize it into config (for now).\n+        layer = keras.layers.Embedding(\n+            input_dim=100, output_dim=16, use_one_hot_matmul=True\n+        )\n+        self.assertTrue(layer._use_one_hot_matmul)\n+\n+        self.assertNotIn(\"use_one_hot_matmul\", layer.get_config())\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee72ff03ca2dfcc43509d5a934e1d3a4a032cdc6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4419 | Contributors (this commit): 45 | Commits (past 90d): 4 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -991,22 +991,22 @@ def pad_sequences(\n     default.\n \n     >>> sequence = [[1], [2, 3], [4, 5, 6]]\n-    >>> tf.keras.preprocessing.sequence.pad_sequences(sequence)\n+    >>> tf.keras.utils.pad_sequences(sequence)\n     array([[0, 0, 1],\n            [0, 2, 3],\n            [4, 5, 6]], dtype=int32)\n \n-    >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, value=-1)\n+    >>> tf.keras.utils.pad_sequences(sequence, value=-1)\n     array([[-1, -1,  1],\n            [-1,  2,  3],\n            [ 4,  5,  6]], dtype=int32)\n \n-    >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, padding='post')\n+    >>> tf.keras.utils.pad_sequences(sequence, padding='post')\n     array([[1, 0, 0],\n            [2, 3, 0],\n            [4, 5, 6]], dtype=int32)\n \n-    >>> tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=2)\n+    >>> tf.keras.utils.pad_sequences(sequence, maxlen=2)\n     array([[0, 1],\n            [2, 3],\n            [5, 6]], dtype=int32)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16ffd34ea2b867f028f4ca781584d92892cd33f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 9545 | Contributors (this commit): 116 | Commits (past 90d): 10 | Contributors (cumulative): 116 | DMM Complexity: None\n\nDIFF:\n@@ -1897,9 +1897,7 @@ class BackupAndRestore(Callback):\n             self._batches_count += 1\n             if self._batches_count >= self.save_freq:\n                 self._batches_count = 0\n-                self._training_state.back_up(\n-                    epoch=self._current_epoch, batch=batch\n-                )\n+                self._back_up(epoch=self._current_epoch, batch=batch)\n \n     def _implements_train_batch_hooks(self):\n         return self.save_freq != \"epoch\"\n@@ -1920,7 +1918,10 @@ class BackupAndRestore(Callback):\n     def on_epoch_end(self, epoch, logs=None):\n         # Back up the model and current epoch for possible future recovery.\n         if self.save_freq == \"epoch\":\n-            self._training_state.back_up(epoch=epoch)\n+            self._back_up(epoch=epoch)\n+\n+    def _back_up(self, epoch, batch=0):\n+        self._training_state.back_up(epoch=epoch, batch=batch)\n \n \n @keras_export(\"keras.callbacks.experimental.BackupAndRestore\", v1=[])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ea250574af6d2a93c98f0bf6138b6aaad6ecc5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 165 | Lines Deleted: 38 | Files Changed: 9 | Hunks: 52 | Methods Changed: 43 | Complexity Δ (Sum/Max): 21/4 | Churn Δ: 203 | Churn Cumulative: 12996 | Contributors (this commit): 80 | Commits (past 90d): 26 | Contributors (cumulative): 148 | DMM Complexity: 0.8252427184466019\n\nDIFF:\n@@ -20,7 +20,7 @@ import tensorflow.compat.v2 as tf\n \n import keras.layers.activation as activation_layers\n from keras import backend\n-from keras.saving.legacy import serialization\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.utils import generic_utils\n \n # isort: off\n@@ -487,7 +487,7 @@ def linear(x):\n \n @keras_export(\"keras.activations.serialize\")\n @tf.__internal__.dispatch.add_dispatch_support\n-def serialize(activation):\n+def serialize(activation, use_legacy_format=False):\n     \"\"\"Returns the string identifier of an activation function.\n \n     Args:\n@@ -515,7 +515,12 @@ def serialize(activation):\n         and activation.__name__ in _TF_ACTIVATIONS_V2\n     ):\n         return _TF_ACTIVATIONS_V2[activation.__name__]\n-    return serialization.serialize_keras_object(activation)\n+\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(activation)\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.serialize_keras_object(activation)\n \n \n # Add additional globals so that deserialize can find these common activation\n@@ -528,7 +533,7 @@ silu = tf.nn.silu\n \n @keras_export(\"keras.activations.deserialize\")\n @tf.__internal__.dispatch.add_dispatch_support\n-def deserialize(name, custom_objects=None):\n+def deserialize(name, custom_objects=None, use_legacy_format=False):\n     \"\"\"Returns activation function given a string identifier.\n \n     Args:\n@@ -565,7 +570,16 @@ def deserialize(name, custom_objects=None):\n         obj_filter=callable,\n     )\n \n-    return serialization.deserialize_keras_object(\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            name,\n+            module_objects=activation_functions,\n+            custom_objects=custom_objects,\n+            printable_module_name=\"activation function\",\n+        )\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.deserialize_keras_object(\n         name,\n         module_objects=activation_functions,\n         custom_objects=custom_objects,\n@@ -606,7 +620,12 @@ def get(identifier):\n     if identifier is None:\n         return linear\n     if isinstance(identifier, (str, dict)):\n-        return deserialize(identifier)\n+        use_legacy_format = (\n+            \"module\" not in identifier\n+            if isinstance(identifier, dict)\n+            else False\n+        )\n+        return deserialize(identifier, use_legacy_format=use_legacy_format)\n     elif callable(identifier):\n         return identifier\n     else:\n\n@@ -19,6 +19,7 @@\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n \n@@ -355,12 +356,21 @@ unitnorm = unit_norm\n \n \n @keras_export(\"keras.constraints.serialize\")\n-def serialize(constraint):\n+def serialize(constraint, use_legacy_format=False):\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(constraint)\n     return serialize_keras_object(constraint)\n \n \n @keras_export(\"keras.constraints.deserialize\")\n-def deserialize(config, custom_objects=None):\n+def deserialize(config, custom_objects=None, use_legacy_format=False):\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=globals(),\n+            custom_objects=custom_objects,\n+            printable_module_name=\"constraint\",\n+        )\n     return deserialize_keras_object(\n         config,\n         module_objects=globals(),\n@@ -375,7 +385,8 @@ def get(identifier):\n     if identifier is None:\n         return None\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        use_legacy_format = \"module\" not in identifier\n+        return deserialize(identifier, use_legacy_format=use_legacy_format)\n     elif isinstance(identifier, str):\n         config = {\"class_name\": str(identifier), \"config\": {}}\n         return deserialize(config)\n\n@@ -20,7 +20,7 @@ import tensorflow.compat.v2 as tf\n \n from keras.initializers import initializers_v1\n from keras.initializers import initializers_v2\n-from keras.saving.legacy import serialization\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n \n@@ -134,15 +134,28 @@ globals().update(LOCAL.ALL_OBJECTS)\n \n \n @keras_export(\"keras.initializers.serialize\")\n-def serialize(initializer):\n-    return serialization.serialize_keras_object(initializer)\n+def serialize(initializer, use_legacy_format=False):\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(initializer)\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.serialize_keras_object(initializer)\n \n \n @keras_export(\"keras.initializers.deserialize\")\n-def deserialize(config, custom_objects=None):\n+def deserialize(config, custom_objects=None, use_legacy_format=False):\n     \"\"\"Return an `Initializer` object from its config.\"\"\"\n     populate_deserializable_objects()\n-    return serialization.deserialize_keras_object(\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=LOCAL.ALL_OBJECTS,\n+            custom_objects=custom_objects,\n+            printable_module_name=\"initializer\",\n+        )\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.deserialize_keras_object(\n         config,\n         module_objects=LOCAL.ALL_OBJECTS,\n         custom_objects=custom_objects,\n@@ -187,7 +200,8 @@ def get(identifier):\n     if identifier is None:\n         return None\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        use_legacy_format = \"module\" not in identifier\n+        return deserialize(identifier, use_legacy_format=use_legacy_format)\n     elif isinstance(identifier, str):\n         identifier = str(identifier)\n         return deserialize(identifier)\n\n@@ -50,7 +50,7 @@ from keras.layers.preprocessing import text_vectorization\n from keras.layers.rnn import cell_wrappers\n from keras.layers.rnn import gru\n from keras.layers.rnn import lstm\n-from keras.saving.legacy import serialization\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.saved_model import json_utils\n from keras.utils import generic_utils\n from keras.utils import tf_inspect as inspect\n@@ -187,7 +187,7 @@ def populate_deserializable_objects():\n \n \n @keras_export(\"keras.layers.serialize\")\n-def serialize(layer):\n+def serialize(layer, use_legacy_format=False):\n     \"\"\"Serializes a `Layer` object into a JSON-compatible representation.\n \n     Args:\n@@ -207,11 +207,15 @@ def serialize(layer):\n     pprint(tf.keras.layers.serialize(model))\n     # prints the configuration of the model, as a dict.\n     \"\"\"\n-    return serialization.serialize_keras_object(layer)\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(layer)\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.serialize_keras_object(layer)\n \n \n @keras_export(\"keras.layers.deserialize\")\n-def deserialize(config, custom_objects=None):\n+def deserialize(config, custom_objects=None, use_legacy_format=False):\n     \"\"\"Instantiates a layer from a config dictionary.\n \n     Args:\n@@ -249,7 +253,16 @@ def deserialize(config, custom_objects=None):\n     ```\n     \"\"\"\n     populate_deserializable_objects()\n-    return serialization.deserialize_keras_object(\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=LOCAL.ALL_OBJECTS,\n+            custom_objects=custom_objects,\n+            printable_module_name=\"layer\",\n+        )\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.deserialize_keras_object(\n         config,\n         module_objects=LOCAL.ALL_OBJECTS,\n         custom_objects=custom_objects,\n\n@@ -24,6 +24,7 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras.saving import saving_lib\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n from keras.utils import losses_utils\n@@ -2568,7 +2569,7 @@ def is_categorical_crossentropy(loss):\n \n \n @keras_export(\"keras.losses.serialize\")\n-def serialize(loss):\n+def serialize(loss, use_legacy_format=False):\n     \"\"\"Serializes loss function or `Loss` instance.\n \n     Args:\n@@ -2577,11 +2578,13 @@ def serialize(loss):\n     Returns:\n       Loss configuration dictionary.\n     \"\"\"\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(loss)\n     return serialize_keras_object(loss)\n \n \n @keras_export(\"keras.losses.deserialize\")\n-def deserialize(name, custom_objects=None):\n+def deserialize(name, custom_objects=None, use_legacy_format=False):\n     \"\"\"Deserializes a serialized loss class/function instance.\n \n     Args:\n@@ -2593,6 +2596,13 @@ def deserialize(name, custom_objects=None):\n     Returns:\n         A Keras `Loss` instance or a loss function.\n     \"\"\"\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            name,\n+            module_objects=globals(),\n+            custom_objects=custom_objects,\n+            printable_module_name=\"loss function\",\n+        )\n     return deserialize_keras_object(\n         name,\n         module_objects=globals(),\n@@ -2639,7 +2649,8 @@ def get(identifier):\n         return None\n     if isinstance(identifier, str):\n         identifier = str(identifier)\n-        return deserialize(identifier)\n+        use_legacy_format = \"module\" not in identifier\n+        return deserialize(identifier, use_legacy_format=use_legacy_format)\n     if isinstance(identifier, dict):\n         return deserialize(identifier)\n     if callable(identifier):\n\n@@ -91,6 +91,7 @@ from keras.metrics.metrics import sparse_categorical_crossentropy\n from keras.metrics.metrics import sparse_top_k_categorical_accuracy\n from keras.metrics.metrics import squared_hinge\n from keras.metrics.metrics import top_k_categorical_accuracy\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n \n@@ -109,7 +110,7 @@ cosine_proximity = cosine_similarity\n \n \n @keras_export(\"keras.metrics.serialize\")\n-def serialize(metric):\n+def serialize(metric, use_legacy_format=False):\n     \"\"\"Serializes metric function or `Metric` instance.\n \n     Args:\n@@ -118,11 +119,13 @@ def serialize(metric):\n     Returns:\n       Metric configuration dictionary.\n     \"\"\"\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(metric)\n     return serialize_keras_object(metric)\n \n \n @keras_export(\"keras.metrics.deserialize\")\n-def deserialize(config, custom_objects=None):\n+def deserialize(config, custom_objects=None, use_legacy_format=False):\n     \"\"\"Deserializes a serialized metric class/function instance.\n \n     Args:\n@@ -133,6 +136,13 @@ def deserialize(config, custom_objects=None):\n     Returns:\n         A Keras `Metric` instance or a metric function.\n     \"\"\"\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=globals(),\n+            custom_objects=custom_objects,\n+            printable_module_name=\"metric function\",\n+        )\n     return deserialize_keras_object(\n         config,\n         module_objects=globals(),\n@@ -176,7 +186,8 @@ def get(identifier):\n       ValueError: If `identifier` cannot be interpreted.\n     \"\"\"\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        use_legacy_format = \"module\" not in identifier\n+        return deserialize(identifier, use_legacy_format=use_legacy_format)\n     elif isinstance(identifier, str):\n         return deserialize(str(identifier))\n     elif callable(identifier):\n\n@@ -56,6 +56,7 @@ from keras.optimizers.legacy.rmsprop import RMSprop\n from keras.optimizers.optimizer_v1 import Optimizer\n from keras.optimizers.optimizer_v1 import TFOptimizer\n from keras.optimizers.schedules import learning_rate_schedule\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n \n@@ -64,7 +65,7 @@ from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export(\"keras.optimizers.serialize\")\n-def serialize(optimizer):\n+def serialize(optimizer, use_legacy_format=False):\n     \"\"\"Serialize the optimizer configuration to JSON compatible python dict.\n \n     The configuration can be used for persistence and reconstruct the\n@@ -81,11 +82,13 @@ def serialize(optimizer):\n     Returns:\n       Python dict which contains the configuration of the input optimizer.\n     \"\"\"\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(optimizer)\n     return serialize_keras_object(optimizer)\n \n \n @keras_export(\"keras.optimizers.deserialize\")\n-def deserialize(config, custom_objects=None, **kwargs):\n+def deserialize(config, custom_objects=None, use_legacy_format=False, **kwargs):\n     \"\"\"Inverse of the `serialize` function.\n \n     Args:\n@@ -104,6 +107,8 @@ def deserialize(config, custom_objects=None, **kwargs):\n     )\n \n     use_legacy_optimizer = kwargs.pop(\"use_legacy_optimizer\", False)\n+    if kwargs:\n+        raise TypeError(f\"Invalid keyword arguments: {kwargs}\")\n     if len(config[\"config\"]) > 0:\n         # If the optimizer config is not empty, then we use the value of\n         # `is_legacy_optimizer` to override `use_legacy_optimizer`. If\n@@ -158,6 +163,15 @@ def deserialize(config, custom_objects=None, **kwargs):\n     # Make deserialization case-insensitive for built-in optimizers.\n     if config[\"class_name\"].lower() in all_classes:\n         config[\"class_name\"] = config[\"class_name\"].lower()\n+\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=all_classes,\n+            custom_objects=custom_objects,\n+            printable_module_name=\"optimizer\",\n+        )\n+\n     return deserialize_keras_object(\n         config,\n         module_objects=all_classes,\n@@ -245,6 +259,8 @@ def get(identifier, **kwargs):\n         ValueError: If `identifier` cannot be interpreted.\n     \"\"\"\n     use_legacy_optimizer = kwargs.pop(\"use_legacy_optimizer\", False)\n+    if kwargs:\n+        raise TypeError(f\"Invalid keyword arguments: {kwargs}\")\n     if isinstance(\n         identifier,\n         (\n@@ -266,12 +282,18 @@ def get(identifier, **kwargs):\n         backend.track_tf_optimizer(opt)\n         return opt\n     elif isinstance(identifier, dict):\n+        use_legacy_format = \"module\" not in identifier\n         return deserialize(\n-            identifier, use_legacy_optimizer=use_legacy_optimizer\n+            identifier,\n+            use_legacy_optimizer=use_legacy_optimizer,\n+            use_legacy_format=use_legacy_format,\n         )\n     elif isinstance(identifier, str):\n         config = {\"class_name\": str(identifier), \"config\": {}}\n-        return deserialize(config, use_legacy_optimizer=use_legacy_optimizer)\n+        return deserialize(\n+            config,\n+            use_legacy_optimizer=use_legacy_optimizer,\n+        )\n     else:\n         raise ValueError(\n             f\"Could not interpret optimizer identifier: {identifier}\"\n\n@@ -20,7 +20,7 @@ import math\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n-from keras.saving.legacy import serialization\n+from keras.saving.legacy import serialization as legacy_serialization\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -1092,7 +1092,7 @@ class NoisyLinearCosineDecay(LearningRateSchedule):\n \n \n @keras_export(\"keras.optimizers.schedules.serialize\")\n-def serialize(learning_rate_schedule):\n+def serialize(learning_rate_schedule, use_legacy_format=False):\n     \"\"\"Serializes a `LearningRateSchedule` into a JSON-compatible dict.\n \n     Args:\n@@ -1108,11 +1108,17 @@ def serialize(learning_rate_schedule):\n     >>> tf.keras.optimizers.schedules.serialize(lr_schedule)\n     {'class_name': 'ExponentialDecay', 'config': {...}}\n     \"\"\"\n-    return serialization.serialize_keras_object(learning_rate_schedule)\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(\n+            learning_rate_schedule\n+        )\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.serialize_keras_object(learning_rate_schedule)\n \n \n @keras_export(\"keras.optimizers.schedules.deserialize\")\n-def deserialize(config, custom_objects=None):\n+def deserialize(config, custom_objects=None, use_legacy_format=False):\n     \"\"\"Instantiates a `LearningRateSchedule` object from a serialized form.\n \n     Args:\n@@ -1139,7 +1145,16 @@ def deserialize(config, custom_objects=None):\n     lr_schedule = tf.keras.optimizers.schedules.deserialize(config)\n     ```\n     \"\"\"\n-    return serialization.deserialize_keras_object(\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=globals(),\n+            custom_objects=custom_objects,\n+            printable_module_name=\"decay\",\n+        )\n+\n+    # To be replaced by new serialization_lib\n+    return legacy_serialization.deserialize_keras_object(\n         config,\n         module_objects=globals(),\n         custom_objects=custom_objects,\n\n@@ -20,6 +20,7 @@ import math\n import tensorflow.compat.v2 as tf\n \n from keras import backend\n+from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n \n@@ -417,16 +418,25 @@ orthogonal_regularizer = OrthogonalRegularizer\n \n \n @keras_export(\"keras.regularizers.serialize\")\n-def serialize(regularizer):\n+def serialize(regularizer, use_legacy_format=False):\n+    if use_legacy_format:\n+        return legacy_serialization.serialize_keras_object(regularizer)\n     return serialize_keras_object(regularizer)\n \n \n @keras_export(\"keras.regularizers.deserialize\")\n-def deserialize(config, custom_objects=None):\n+def deserialize(config, custom_objects=None, use_legacy_format=False):\n     if config == \"l1_l2\":\n         # Special case necessary since the defaults used for \"l1_l2\" (string)\n         # differ from those of the L1L2 class.\n         return L1L2(l1=0.01, l2=0.01)\n+    if use_legacy_format:\n+        return legacy_serialization.deserialize_keras_object(\n+            config,\n+            module_objects=globals(),\n+            custom_objects=custom_objects,\n+            printable_module_name=\"regularizer\",\n+        )\n     return deserialize_keras_object(\n         config,\n         module_objects=globals(),\n@@ -441,7 +451,8 @@ def get(identifier):\n     if identifier is None:\n         return None\n     if isinstance(identifier, dict):\n-        return deserialize(identifier)\n+        use_legacy_format = \"module\" not in identifier\n+        return deserialize(identifier, use_legacy_format=use_legacy_format)\n     elif isinstance(identifier, str):\n         return deserialize(str(identifier))\n     elif callable(identifier):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b96ba57c7b3cd200ff571c56443230166f79708", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 15 | Churn Cumulative: 134 | Contributors (this commit): 5 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1352,6 +1352,8 @@ class RevivedNetwork(RevivedLayer):\n                 revived_obj.activity_regularizer = regularizers.deserialize(\n                     metadata[\"activity_regularizer\"]\n                 )\n+            if metadata.get(\"autocast\") is not None:\n+                revived_obj._autocast = metadata[\"autocast\"]\n \n         return revived_obj, _revive_setter\n \n\n@@ -1115,16 +1115,25 @@ class TestSavedModelFormatAllModes(test_combinations.TestCase):\n             def __init__(self):\n                 super().__init__(autocast=autocast)\n \n-        x = tf.constant(3, dtype=tf.float64)\n+        class CustomModel(keras.Model):\n+            def __init__(self):\n+                super().__init__(autocast=autocast)\n \n-        x_in = keras.Input(tensor=x)\n+            def call(self, inputs):\n+                return inputs\n+\n+        x = tf.constant([3], dtype=tf.float64)\n+\n+        x_in = keras.Input((1,))\n         output = CustomLayer()(x_in)\n+        output = CustomModel()(output)\n         model = keras.Model(inputs=x_in, outputs=output)\n \n         saved_model_dir = self._save_model_dir()\n         model.save(saved_model_dir, save_format=\"tf\")\n         loaded = keras_load.load(saved_model_dir)\n         self.assertEqual(autocast, loaded.layers[-1]._autocast)\n+        self.assertEqual(autocast, loaded.layers[-2]._autocast)\n         self.assertEqual(self.evaluate(model(x)), self.evaluate(loaded(x)))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bfdcf3c48e313012d7a47645cad07f39ba33cb2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 40 | Churn Cumulative: 4036 | Contributors (this commit): 14 | Commits (past 90d): 9 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,8 +66,8 @@ class TestText(tf.test.TestCase):\n         sequences = []\n         for seq in tokenizer.texts_to_sequences_generator(sample_texts):\n             sequences.append(seq)\n-        self.assertLess(np.max(np.max(sequences)), 10)\n-        self.assertEqual(np.min(np.min(sequences)), 1)\n+        self.assertLess(np.max(np.max(np.asarray(sequences, dtype=object))), 10)\n+        self.assertEqual(np.min(np.min(np.asarray(sequences, dtype=object))), 1)\n \n         tokenizer.fit_on_sequences(sequences)\n \n\n@@ -151,7 +151,13 @@ def _convert_dataset_to_list(\n         start_time,\n     ):\n         if dataset_type_spec in [tuple, list]:\n-            dataset_as_list.append(np.array(sample))\n+            # The try-except here is for NumPy 1.24 compatibility, see:\n+            # https://numpy.org/neps/nep-0034-infer-dtype-is-object.html\n+            try:\n+                arr = np.array(sample)\n+            except ValueError:\n+                arr = np.array(sample, dtype=object)\n+            dataset_as_list.append(arr)\n         else:\n             dataset_as_list.append(sample)\n \n\n@@ -47,14 +47,20 @@ class SplitDatasetTest(tf.test.TestCase):\n             dataset, left_size=0.3\n         )\n \n-        self.assertEqual(np.array(list(left_split)).shape, (2, 2))\n-        self.assertEqual(np.array(list(right_split)).shape, (3, 2))\n+        self.assertEqual(np.array(list(left_split), dtype=object).shape, (2, 2))\n+        self.assertEqual(\n+            np.array(list(right_split), dtype=object).shape, (3, 2)\n+        )\n \n-        self.assertEqual(np.array(list(left_split)[0]).shape, (2,))\n+        self.assertEqual(\n+            np.array(list(left_split)[0], dtype=object).shape, (2,)\n+        )\n         self.assertEqual(np.array(list(left_split)[0][0]).shape, (3,))\n         self.assertEqual(np.array(list(left_split)[0][1]).shape, ())\n \n-        self.assertEqual(np.array(list(right_split)[0]).shape, (2,))\n+        self.assertEqual(\n+            np.array(list(right_split)[0], dtype=object).shape, (2,)\n+        )\n         self.assertEqual(np.array(list(right_split)[0][0]).shape, (3,))\n         self.assertEqual(np.array(list(right_split)[0][1]).shape, ())\n \n@@ -118,14 +124,20 @@ class SplitDatasetTest(tf.test.TestCase):\n         self.assertIsInstance(left_split, tf.data.Dataset)\n         self.assertIsInstance(right_split, tf.data.Dataset)\n \n-        self.assertEqual(np.array(list(left_split)).shape, (2, 2))\n-        self.assertEqual(np.array(list(right_split)).shape, (3, 2))\n+        self.assertEqual(np.array(list(left_split), dtype=object).shape, (2, 2))\n+        self.assertEqual(\n+            np.array(list(right_split), dtype=object).shape, (3, 2)\n+        )\n \n-        self.assertEqual(np.array(list(left_split)[0]).shape, (2,))\n+        self.assertEqual(\n+            np.array(list(left_split)[0], dtype=object).shape, (2,)\n+        )\n         self.assertEqual(np.array(list(left_split)[0][0]).shape, (32, 32))\n         self.assertEqual(np.array(list(left_split)[0][1]).shape, ())\n \n-        self.assertEqual(np.array(list(right_split)[0]).shape, (2,))\n+        self.assertEqual(\n+            np.array(list(right_split)[0], dtype=object).shape, (2,)\n+        )\n         self.assertEqual(np.array(list(right_split)[0][0]).shape, (32, 32))\n         self.assertEqual(np.array(list(right_split)[0][1]).shape, ())\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b29e088f51adcac1aabb9de82d786b809d322c72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 987 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,6 +68,18 @@ def set_random_seed(seed):\n     backend._SEED_GENERATOR.generator = random.Random(seed)\n \n \n+def get_random_seed():\n+    \"\"\"Retrieve a seed value to seed a random generator.\n+\n+    Returns:\n+      the random seed as an integer.\n+    \"\"\"\n+    if getattr(backend._SEED_GENERATOR, \"generator\", None):\n+        return backend._SEED_GENERATOR.generator.randint(1, 1e9)\n+    else:\n+        return random.randint(1, 1e9)\n+\n+\n def is_tensor_or_tensor_list(v):\n     v = tf.nest.flatten(v)\n     if v and isinstance(v[0], tf.Tensor):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9de24b0beac97d1ef8c34e427beb895ef791d254", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 44 | Churn Cumulative: 21405 | Contributors (this commit): 140 | Commits (past 90d): 17 | Contributors (cumulative): 141 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3391,6 +3391,37 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             # Create optimizer variables.\n             self.optimizer.build(self.trainable_variables)\n \n+    def export(self, filepath):\n+        \"\"\"Create a SavedModel artifact for inference (e.g. via TF-Serving).\n+\n+        This method lets you export a model to a lightweight SavedModel artifact\n+        that contains the model's forward pass only (its `call()` method)\n+        and can be served via e.g. TF-Serving. The forward pass is registered\n+        under the name `serve()` (see example below).\n+\n+        The original code of the model (including any custom layers you may\n+        have used) is *no longer* necessary to reload the artifact -- it is\n+        entirely standalone.\n+\n+        Args:\n+            filepath: `str` or `pathlib.Path` object. Path where to save\n+                the artifact.\n+\n+        Example:\n+\n+        ```python\n+        # Create the artifact\n+        model.export(\"path/to/location\")\n+\n+        # Later, in a different process / environment...\n+        reloaded_artifact = tf.saved_model.load(\"path/to/location\")\n+        predictions = reloaded_artifact.serve(input_data)\n+        ```\n+        \"\"\"\n+        from keras.saving import export_lib\n+\n+        export_lib.export_model(self, filepath)\n+\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _set_save_spec(self, inputs, args=None, kwargs=None):\n         \"\"\"Defines the save spec so that serialization can trace `call()`.\n\n@@ -368,6 +368,19 @@ class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n                 my_endpoint,\n             )\n \n+    @test_combinations.run_with_all_model_types\n+    def test_model_export_method(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = self._get_model()\n+        ref_input = tf.random.normal((3, 10))\n+        ref_output = model(ref_input).numpy()\n+\n+        model.export(temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(\n+            ref_output, revived_model.serve(ref_input).numpy(), atol=1e-6\n+        )\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#67a8e6b96b1731ec732084f443345c70d87c31bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 22 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 626 | Contributors (this commit): 13 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -187,8 +187,8 @@ class MultiHeadAttention(Layer):\n         dropout: Dropout probability.\n         use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n         output_shape: The expected shape of an output tensor, besides the batch\n-        and sequence dims. If not specified, projects back to the key feature\n-        dim.\n+            and sequence dims. If not specified, projects back to the key\n+            feature dim.\n         attention_axes: axes over which the attention is applied. `None` means\n             attention over all axes, but batch, heads, and features.\n         kernel_initializer: Initializer for dense layer kernels.\n@@ -202,29 +202,31 @@ class MultiHeadAttention(Layer):\n     Call arguments:\n         query: Query `Tensor` of shape `(B, T, dim)`.\n         value: Value `Tensor` of shape `(B, S, dim)`.\n-      key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will use\n-        `value` for both `key` and `value`, which is the most common case.\n+        key: Optional key `Tensor` of shape `(B, S, dim)`. If not given, will\n+            use `value` for both `key` and `value`, which is the most common\n+            case.\n         attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n-        attention to certain positions. The boolean mask specifies which query\n-        elements can attend to which key elements, 1 indicates attention and 0\n-        indicates no attention. Broadcasting can happen for the missing batch\n-        dimensions and the head dimension.\n+            attention to certain positions. The boolean mask specifies which\n+            query elements can attend to which key elements, 1 indicates\n+            attention and 0 indicates no attention. Broadcasting can happen for\n+            the missing batch dimensions and the head dimension.\n         return_attention_scores: A boolean to indicate whether the output should\n             be `(attention_output, attention_scores)` if `True`, or\n             `attention_output` if `False`. Defaults to `False`.\n         training: Python boolean indicating whether the layer should behave in\n             training mode (adding dropout) or in inference mode (no dropout).\n-        Defaults to either using the training mode of the parent layer/model,\n-        or False (inference) if there is no parent layer.\n+            Defaults to either using the training mode of the parent\n+            layer/model, or False (inference) if there is no parent layer.\n         use_causal_mask: A boolean to indicate whether to apply a causal mask to\n-        prevent tokens from attending to future tokens (e.g., used in a decoder\n-        Transformer).\n+            prevent tokens from attending to future tokens (e.g., used in a\n+            decoder Transformer).\n \n     Returns:\n         attention_output: The result of the computation, of shape `(B, T, E)`,\n-        where `T` is for target sequence shapes and `E` is the query input last\n-        dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n-        are projected to the shape specified by `output_shape`.\n+            where `T` is for target sequence shapes and `E` is the query input\n+            last dimension if `output_shape` is `None`. Otherwise, the\n+            multi-head outputs are projected to the shape specified by\n+            `output_shape`.\n         attention_scores: [Optional] multi-head attention coefficients over\n             attention axes.\n     \"\"\"\n@@ -505,10 +507,11 @@ class MultiHeadAttention(Layer):\n             key: Projected key `Tensor` of shape `(B, S, N, key_dim)`.\n             value: Projected value `Tensor` of shape `(B, S, N, value_dim)`.\n             attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n-            attention to certain positions. It is generally not needed if the\n-            `query` and `value` (and/or `key`) are masked.\n-          training: Python boolean indicating whether the layer should behave in\n-            training mode (adding dropout) or in inference mode (doing nothing).\n+                attention to certain positions. It is generally not needed if\n+                the `query` and `value` (and/or `key`) are masked.\n+            training: Python boolean indicating whether the layer should behave\n+                in training mode (adding dropout) or in inference mode (doing\n+                nothing).\n \n         Returns:\n           attention_output: Multi-headed outputs of attention computation.\n@@ -629,9 +632,9 @@ class MultiHeadAttention(Layer):\n             value: Projected value `Tensor` of shape `(B, T, N, value_dim)`.\n             attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n                 attention to certain positions.\n-          use_causal_mask: A boolean to indicate whether to apply a causal mask\n-            to prevent tokens from attending to future tokens (e.g., used in a\n-            decoder Transformer).\n+            use_causal_mask: A boolean to indicate whether to apply a causal\n+                mask to prevent tokens from attending to future tokens (e.g.,\n+                used in a decoder Transformer).\n \n         Returns:\n             attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n@@ -682,6 +685,7 @@ class MultiHeadAttention(Layer):\n           [True,  True,  True,  False],\n           [True,  True,  True,  True]]]\n         ```\n+\n         Args:\n             query: query `Tensor` of shape `(B, T, ...)`.\n             value: value `Tensor` of shape `(B, S, ...)` (optional, defaults to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f64c727037faf8f9d43a16139cc9f869270b018", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 7288 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 41 | DMM Complexity: 0.6\n\nDIFF:\n@@ -489,6 +489,7 @@ class ConvLSTM(ConvRNN):\n             bias_constraint=bias_constraint,\n             dropout=dropout,\n             recurrent_dropout=recurrent_dropout,\n+            name='cell',\n             dtype=kwargs.get(\"dtype\"),\n         )\n         super().__init__(\n\n@@ -583,6 +583,7 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             reset_after=reset_after,\n             dtype=kwargs.get(\"dtype\"),\n             trainable=kwargs.get(\"trainable\", True),\n+            name='cell',\n             **cell_kwargs,\n         )\n         super().__init__(\n\n@@ -556,6 +556,7 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n             implementation=implementation,\n             dtype=kwargs.get(\"dtype\"),\n             trainable=kwargs.get(\"trainable\", True),\n+            name='cell',\n             **cell_kwargs,\n         )\n         super().__init__(\n\n@@ -269,6 +269,7 @@ class LSTM(RNN):\n             implementation=implementation,\n             dtype=kwargs.get(\"dtype\"),\n             trainable=kwargs.get(\"trainable\", True),\n+            name='cell',\n             **cell_kwargs\n         )\n         super().__init__(\n\n@@ -392,6 +392,7 @@ class SimpleRNN(RNN):\n             recurrent_dropout=recurrent_dropout,\n             dtype=kwargs.get(\"dtype\"),\n             trainable=kwargs.get(\"trainable\", True),\n+            name='cell',\n             **cell_kwargs,\n         )\n         super().__init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f24d733a26b847ca7a82707bdab4749a86b446fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 693 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -175,7 +175,7 @@ class Bidirectional(Wrapper):\n         self.return_sequences = layer.return_sequences\n         self.return_state = layer.return_state\n         self.supports_masking = True\n-        self._trainable = True\n+        self._trainable = kwargs.get('trainable', True)\n         self._num_constants = 0\n         self.input_spec = layer.input_spec\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2727df09aa284a94ce8234ad1279d9659cdf2064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 304 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 8 | Methods Changed: 14 | Complexity Δ (Sum/Max): 25/20 | Churn Δ: 310 | Churn Cumulative: 1071 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: 0.7\n\nDIFF:\n@@ -362,6 +362,149 @@ def export_model(model, filepath):\n     export_archive.write_out(filepath)\n \n \n+class ReloadedLayer(base_layer.Layer):\n+    \"\"\"Reload a Keras model/layer that was saved via SavedModel / ExportArchive.\n+\n+    Arguments:\n+        filepath: `str` or `pathlib.Path` object. The path to the SavedModel.\n+        call_endpoint: Name of the endpoint to use as the `call()` method\n+            of the reloaded layer. If the SavedModel was created\n+            via `model.export()`,\n+            then the default endpoint name is `'serve'`. In other cases\n+            it may be named `'serving_default'`.\n+\n+    Example:\n+\n+    ```python\n+    model.export(\"path/to/artifact\")\n+    reloaded_layer = ReloadedLayer(\"path/to/artifact\")\n+    outputs = reloaded_layer(inputs)\n+    ```\n+\n+    The reloaded object can be used like a regular Keras layer, and supports\n+    training/fine-tuning of its trainable weights. Note that the reloaded\n+    object retains none of the internal structure or custom methods of the\n+    original object -- it's a brand new layer created around the saved\n+    function.\n+\n+    **Limitations:**\n+\n+    * Only call endpoints with a single `inputs` tensor argument\n+    (which may optionally be a dict/tuple/list of tensors) are supported.\n+    For endpoints with multiple separate input tensor arguments, consider\n+    subclassing `ReloadedLayer` and implementing a `call()` method with a\n+    custom signature.\n+    * If you need training-time behavior to differ from inference-time behavior\n+    (i.e. if you need the reloaded object to support a `training=True` argument\n+    in `__call__()`), make sure that the training-time call function is\n+    saved as a standalone endpoint in the artifact, and provide its name\n+    to the `ReloadedLayer` via the `call_training_endpoint` argument.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        filepath,\n+        call_endpoint=\"serve\",\n+        call_training_endpoint=None,\n+        trainable=True,\n+        name=None,\n+        dtype=None,\n+    ):\n+        # Initialize an empty layer, then add_weight() etc. as needed.\n+        super().__init__(trainable=trainable, name=name, dtype=dtype)\n+\n+        self._reloaded_obj = tf.saved_model.load(filepath)\n+\n+        self.filepath = filepath\n+        self.call_endpoint = call_endpoint\n+        self.call_training_endpoint = call_training_endpoint\n+\n+        # Resolve the call function.\n+        if hasattr(self._reloaded_obj, call_endpoint):\n+            # Case 1: it's set as an attribute.\n+            self.call_endpoint_fn = getattr(self._reloaded_obj, call_endpoint)\n+        elif call_endpoint in self._reloaded_obj.signatures:\n+            # Case 2: it's listed in the `signatures` field.\n+            self.call_endpoint_fn = self._reloaded_obj.signatures[call_endpoint]\n+        else:\n+            raise ValueError(\n+                f\"The endpoint '{call_endpoint}' is neither an \"\n+                \"attribute of the reloaded SavedModel, nor an entry \"\n+                \"in the `signatures` field of the reloaded SavedModel. \"\n+            )\n+\n+        # Resolving the training function.\n+        if call_training_endpoint:\n+            if hasattr(self._reloaded_obj, call_training_endpoint):\n+                self.call_training_endpoint_fn = getattr(\n+                    self._reloaded_obj, call_training_endpoint\n+                )\n+            elif call_training_endpoint in self._reloaded_obj.signatures:\n+                self.call_training_endpoint_fn = self._reloaded_obj.signatures[\n+                    call_training_endpoint\n+                ]\n+            else:\n+                raise ValueError(\n+                    f\"The endpoint '{call_training_endpoint}' is \"\n+                    \"neither an attribute of the reloaded SavedModel, \"\n+                    \"nor an entry in the `signatures` field of \"\n+                    \"the reloaded SavedModel. \"\n+                )\n+\n+        # Add trainable and non-trainable weights from the call_endpoint_fn.\n+        all_fns = [self.call_endpoint_fn]\n+        if call_training_endpoint:\n+            all_fns.append(self.call_training_endpoint_fn)\n+        trainable_variables_ids = set()\n+        non_trainable_variables_ids = set()\n+        for fn in all_fns:\n+            # The function may or may not be already a concrete function\n+            if hasattr(fn, \"concrete_functions\"):\n+                concrete_functions = fn.concrete_functions\n+            else:\n+                concrete_functions = [fn]\n+            for concrete_fn in concrete_functions:\n+                for v in concrete_fn.trainable_variables:\n+                    if id(v) not in trainable_variables_ids:\n+                        self._add_existing_weight(v, trainable=True)\n+                        trainable_variables_ids.add(id(v))\n+\n+                for v in concrete_fn.variables:\n+                    if (\n+                        id(v) not in trainable_variables_ids\n+                        and id(v) not in non_trainable_variables_ids\n+                    ):\n+                        self._add_existing_weight(v, trainable=False)\n+                        non_trainable_variables_ids.add(id(v))\n+        self.built = True\n+\n+    def _add_existing_weight(self, weight, trainable):\n+        \"\"\"Calls add_weight() to register but not create an existing weight.\"\"\"\n+        self.add_weight(\n+            name=weight.name,\n+            shape=weight.shape,\n+            dtype=weight.dtype,\n+            trainable=trainable,\n+            getter=lambda *_, **__: weight,\n+        )\n+\n+    def call(self, inputs, training=False, **kwargs):\n+        if training:\n+            if self.call_training_endpoint:\n+                return self.call_training_endpoint_fn(inputs, **kwargs)\n+        return self.call_endpoint_fn(inputs, **kwargs)\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        config = {\n+            # Note: this is not intended to be portable.\n+            \"filepath\": self.filepath,\n+            \"call_endpoint\": self.call_endpoint,\n+            \"call_training_endpoint\": self.call_training_endpoint,\n+        }\n+        return {**base_config, **config}\n+\n+\n def _make_tensor_spec(x):\n     return tf.TensorSpec(x.shape, dtype=x.dtype)\n \n\n@@ -15,6 +15,7 @@\n \"\"\"Tests for inference-only model/layer exporting utilities.\"\"\"\n import os\n \n+import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n@@ -24,9 +25,7 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n \n-@test_utils.run_v2_only\n-class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n-    def _get_model(self):\n+def get_model():\n     layers = [\n         keras.layers.Dense(10, activation=\"relu\"),\n         keras.layers.BatchNormalization(),\n@@ -35,10 +34,13 @@ class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n     model = test_utils.get_model_from_layers(layers, input_shape=(10,))\n     return model\n \n+\n+@test_utils.run_v2_only\n+class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n     @test_combinations.run_with_all_model_types\n     def test_standard_model_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-        model = self._get_model()\n+        model = get_model()\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input).numpy()\n \n@@ -52,7 +54,7 @@ class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n     def test_low_level_model_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n \n-        model = self._get_model()\n+        model = get_model()\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input).numpy()\n \n@@ -371,7 +373,7 @@ class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n     @test_combinations.run_with_all_model_types\n     def test_model_export_method(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-        model = self._get_model()\n+        model = get_model()\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input).numpy()\n \n@@ -382,5 +384,158 @@ class ExportArchiveTest(tf.test.TestCase, parameterized.TestCase):\n         )\n \n \n+@test_utils.run_v2_only\n+class TestReloadedLayer(tf.test.TestCase, parameterized.TestCase):\n+    @test_combinations.run_with_all_model_types\n+    def test_reloading_export_archive(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = get_model()\n+        ref_input = tf.random.normal((3, 10))\n+        ref_output = model(ref_input).numpy()\n+\n+        export_lib.export_model(model, temp_filepath)\n+        reloaded_layer = export_lib.ReloadedLayer(temp_filepath)\n+        self.assertAllClose(\n+            reloaded_layer(ref_input).numpy(), ref_output, atol=1e-7\n+        )\n+        self.assertLen(reloaded_layer.weights, len(model.weights))\n+        self.assertLen(\n+            reloaded_layer.trainable_weights, len(model.trainable_weights)\n+        )\n+        self.assertLen(\n+            reloaded_layer.non_trainable_weights,\n+            len(model.non_trainable_weights),\n+        )\n+\n+        # Test fine-tuning\n+        new_model = keras.Sequential([reloaded_layer])\n+        new_model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+        x = tf.random.normal((32, 10))\n+        y = tf.random.normal((32, 1))\n+        new_model.train_on_batch(x, y)\n+        new_output = reloaded_layer(ref_input).numpy()\n+        self.assertNotAllClose(new_output, ref_output, atol=1e-5)\n+\n+        # Test that trainable can be set to False\n+        reloaded_layer.trainable = False\n+        new_model.compile(optimizer=\"rmsprop\", loss=\"mse\")\n+        x = tf.random.normal((32, 10))\n+        y = tf.random.normal((32, 1))\n+        new_model.train_on_batch(x, y)\n+        # The output must not have changed\n+        self.assertAllClose(\n+            reloaded_layer(ref_input).numpy(), new_output, atol=1e-7\n+        )\n+\n+    @test_combinations.run_with_all_model_types\n+    def test_reloading_default_saved_model(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = get_model()\n+        ref_input = tf.random.normal((3, 10))\n+        ref_output = model(ref_input).numpy()\n+\n+        tf.saved_model.save(model, temp_filepath)\n+        reloaded_layer = export_lib.ReloadedLayer(\n+            temp_filepath, call_endpoint=\"serving_default\"\n+        )\n+        # The output is a dict, due to the nature of SavedModel saving.\n+        new_output = reloaded_layer(ref_input)\n+        self.assertAllClose(\n+            new_output[list(new_output.keys())[0]].numpy(),\n+            ref_output,\n+            atol=1e-7,\n+        )\n+        self.assertLen(reloaded_layer.weights, len(model.weights))\n+        self.assertLen(\n+            reloaded_layer.trainable_weights, len(model.trainable_weights)\n+        )\n+        self.assertLen(\n+            reloaded_layer.non_trainable_weights,\n+            len(model.non_trainable_weights),\n+        )\n+\n+    def test_call_training(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        keras.utils.set_random_seed(1337)\n+        model = keras.Sequential(\n+            [\n+                keras.Input((10,)),\n+                keras.layers.Dense(10),\n+                keras.layers.Dropout(0.99999),\n+            ]\n+        )\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\n+            name=\"call_inference\",\n+            fn=lambda x: model(x, training=False),\n+            input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n+        )\n+        export_archive.add_endpoint(\n+            name=\"call_training\",\n+            fn=lambda x: model(x, training=True),\n+            input_signature=[tf.TensorSpec(shape=(None, 10), dtype=tf.float32)],\n+        )\n+        export_archive.write_out(temp_filepath)\n+        reloaded_layer = export_lib.ReloadedLayer(\n+            temp_filepath,\n+            call_endpoint=\"call_inference\",\n+            call_training_endpoint=\"call_training\",\n+        )\n+        inference_output = reloaded_layer(\n+            tf.random.normal((1, 10)), training=False\n+        )\n+        training_output = reloaded_layer(\n+            tf.random.normal((1, 10)), training=True\n+        )\n+        self.assertAllClose(np.mean(training_output), 0.0, atol=1e-7)\n+        self.assertNotAllClose(np.mean(inference_output), 0.0, atol=1e-7)\n+\n+    @test_combinations.run_with_all_model_types\n+    def test_serialization(self):\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = get_model()\n+        ref_input = tf.random.normal((3, 10))\n+        ref_output = model(ref_input).numpy()\n+\n+        export_lib.export_model(model, temp_filepath)\n+        reloaded_layer = export_lib.ReloadedLayer(temp_filepath)\n+\n+        # Test reinstantiation from config\n+        config = reloaded_layer.get_config()\n+        rereloaded_layer = export_lib.ReloadedLayer.from_config(config)\n+        self.assertAllClose(\n+            rereloaded_layer(ref_input).numpy(), ref_output, atol=1e-7\n+        )\n+\n+        # Test whole model saving with reloaded layer inside\n+        model = keras.Sequential([reloaded_layer])\n+        temp_model_filepath = os.path.join(self.get_temp_dir(), \"m.keras\")\n+        model.save(temp_model_filepath, save_format=\"keras_v3\")\n+        reloaded_model = keras.models.load_model(\n+            temp_model_filepath,\n+            custom_objects={\"ReloadedLayer\": export_lib.ReloadedLayer},\n+        )\n+        self.assertAllClose(\n+            reloaded_model(ref_input).numpy(), ref_output, atol=1e-7\n+        )\n+\n+    def test_errors(self):\n+        # Test missing call endpoint\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+        model = keras.Sequential([keras.Input((2,)), keras.layers.Dense(3)])\n+        export_lib.export_model(model, temp_filepath)\n+        with self.assertRaisesRegex(ValueError, \"The endpoint 'wrong'\"):\n+            export_lib.ReloadedLayer(temp_filepath, call_endpoint=\"wrong\")\n+\n+        # Test missing call training endpoint\n+        with self.assertRaisesRegex(ValueError, \"The endpoint 'wrong'\"):\n+            export_lib.ReloadedLayer(\n+                temp_filepath,\n+                call_endpoint=\"serve\",\n+                call_training_endpoint=\"wrong\",\n+            )\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
